[
    {
        "index": "#10",
        "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution",
        "link": "/arxiv/2510.04886",
        "arxiv_id": "2510.04886",
        "authors": "Adi Banerjee, Anirudh Nair, Tarik Borogovac",
        "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents a significant challenge in debugging and improving collaborative AI systems. Current approaches to pinpointing agent and step level failures in interaction traces - whether using all-at-once evaluation, step-by-step analysis, or binary search - fall short when analyzing complex patterns, struggling with both accuracy and consistency. We present ECHO (Error attribution through Contextual Hierarchy and Objective consensus analysis), a novel algorithm that combines hierarchical context representation, objective analysis-based evaluation, and consensus voting to improve error attribution accuracy. Our approach leverages a positional-based leveling of contextual understanding while maintaining objective evaluation criteria, ultimately reaching conclusions through a consensus mechanism. Experimental results demonstrate that ECHO outperforms existing methods across various multi-agent interaction scenarios, showing particular strength in cases involving subtle reasoning errors and complex interdependencies. Our findings suggest that leveraging these concepts of structured, hierarchical context representation combined with consensus-based objective decision-making, provides a more robust framework for error attribution in multi-agent systems.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-09T14:04:51.257230",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心贡献是提出了一种名为ECHO的新算法，用于改进LLM多智能体系统中的“错误归因”。其明确目标是“调试和改进协作式AI系统”。这并非将LLM作为工具应用于某个特定领域，而是致力于解决LLM在一种前沿协作范式（多智能体系统）中遇到的基础性挑战：如何精确定位和诊断推理过程中的失败。通过提供一种更有效的方法来识别“微妙的推理错误和复杂的相互依赖关系”，该论文为后续改进LLM的多步推理和问题解决能力铺平了道路。这完全符合筛选标准中“改进LLM的基础能力”、“增强其逻辑、规划、多步推理等通用能力”的要求。 2.  **第二步：正面指标——论文包含多个高度相关的主题。** 论文摘要中明确出现了多个正面指标： *   **核心概念**: \"Large Language Model (LLM)\" *   **能力方向**: \"reasoning errors\"（直接指向推理能力）、\"problem-solving\"（在“协作式AI系统”的上下文中体现） *   **新兴范式**: \"multi-agent systems\"（论文的核心研究对象） 这些关键词的密集出现，强烈表明该论文与您的研究主题高度相关。 3.  **第三步：排除标准——论文未聚焦于排除领域。** 论文的研究内容完全不涉及多模态、视觉、医疗、化学、机器人等特定应用领域，也未讨论水印、安全等应用层面的可靠性问题。因此，它没有触犯任何排除标准。 4.  **第四步：处理特殊和模糊情况——论文属于应保留的智能体研究。** 论文聚焦于“多智能体系统”，这属于“智能体/工具使用”的范畴。根据筛选标准，如果论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”，则应保留。本文提出的ECHO算法正是这样一种通用性的错误归因框架，旨在提升多智能体系统整体的推理和协作质量，而非应用于特定领域。因此，它属于应保留的情况。同时，其“错误归因”工作也与“增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的目标一致。 **综合判断:** 这篇论文的本质是开发一种新的方法论（ECHO算法），用于诊断和提升LLM在多智能体协作框架下的推理表现。它直接触及了LLM通用推理能力的核心痛点——如何发现并修复复杂的推理链条中的错误。因此，它不仅符合，而且是高度符合您关于“大语言模型通用推理能力”的研究课题。"
    },
    {
        "index": "#12",
        "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation",
        "link": "/arxiv/2510.04851",
        "arxiv_id": "2510.04851",
        "authors": "Dongge Han, Camille Couturier, Daniel Madrigal Diaz, Xuchao Zhang, Victor Rühle, Saravan Rajmohan",
        "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent large language model (LLM) systems in workflow automation. LEGOMem decomposes past task trajectories into reusable memory units and flexibly allocates them across orchestrators and task agents to support planning and execution. To explore the design space of memory in multi-agent systems, we use LEGOMem as a lens and conduct a systematic study of procedural memory in multi-agent systems, examining where memory should be placed, how it should be retrieved, and which agents benefit most. Experiments on the OfficeBench benchmark show that orchestrator memory is critical for effective task decomposition and delegation, while fine-grained agent memory improves execution accuracy. We find that even teams composed of smaller language models can benefit substantially from procedural memory, narrowing the performance gap with stronger agents by leveraging prior execution traces for more accurate planning and tool use. These results position LEGOMem as both a practical framework for memory-augmented agent systems and a research tool for understanding memory design in multi-agent workflow automation.",
        "subjects": "Artificial Intelligence, Machine Learning, Multiagent Systems",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-09T14:04:51.258259",
        "filter_reason": "这篇论文完全符合筛选要求，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一个名为LEGOMem的**模块化程序化记忆框架**。这个框架并非将LLM应用于某个特定垂直领域（如医疗、化学），而是旨在**增强多智能体LLM系统本身的基础能力**。具体来说，它通过一种新的记忆机制来提升系统的**规划、任务分解、执行和工具使用**能力。这些都是构成通用推理能力的核心要素。因此，根据第一步的核心判断标准，这篇论文的本质是改进LLM的基础能力，应予以保留。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 明确以 \"multi-agent large language model (LLM) systems\" 为研究对象。 *   **能力方向**: 直接聚焦于 \"planning and execution\"、\"task decomposition\"，这些都是通用推理和问题解决的关键环节。 *   **新兴范式**: 论文的核心是关于 \"multi-agent systems\" 和 \"tool use\" 的研究，提出了一个系统性的框架来增强这些范式下的能力。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准中的领域。它没有讨论多模态、视觉，没有聚焦于任何特定应用领域（如医疗、化学），也未涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 该论文是“智能体/工具使用”特殊情况的完美范例。它提出的是一种**通用的智能体协作框架（记忆框架）**，其目标是增强LLM智能体在通用工作流自动化场景下的**问题解决能力**。摘要中强调，该框架能帮助较小的模型“进行更准确的规划和工具使用”，这表明其贡献是提升模型的基础、通用能力，而非解决特定领域问题。因此，完全符合保留条件。 5.  **第五步：最终决策** 综合以上分析，该论文的核心贡献是提出了一种创新的系统架构（记忆框架），用以提升LLM在多智能体协作环境下的通用规划与执行能力。这直接服务于“提高大语言模型本身的通用推理能力”这一核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#23",
        "title": "The Argument is the Explanation: Structured Argumentation for Trust in Agents",
        "link": "/arxiv/2510.03442",
        "arxiv_id": "2510.03442",
        "authors": "Ege Cakar, Per Ola Kristensson",
        "summary": "Humans are black boxes -- we cannot observe their neural processes, yet society functions by evaluating verifiable arguments. AI explainability should follow this principle: stakeholders need verifiable reasoning chains, not mechanistic transparency. We propose using structured argumentation to provide a level of explanation and verification neither interpretability nor LLM-generated explanation is able to offer. Our pipeline achieves state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7 points above prior work) and $0.81$ macro F1, $\\sim$0.07 above previous published results with comparable data setups, for Argumentative MicroTexts relation classification, converting LLM text into argument graphs and enabling verification at each inferential step. We demonstrate this idea on multi-agent risk assessment using the Structured What-If Technique, where specialized agents collaborate transparently to carry out risk assessment otherwise achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we capture support/attack relationships, thereby enabling automatic hallucination detection via fact nodes attacking arguments. We also provide a verification mechanism that enables iterative refinement through test-time feedback without retraining. For easy deployment, we provide a Docker container for the fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python package on GitHub.",
        "subjects": "Machine Learning, Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-03",
        "category": "cs.MA",
        "crawl_time": "2025-10-09T14:04:51.268536",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 论文的核心贡献并非将LLM应用于某个特定领域，而是提出了一种名为“结构化论证”的通用方法论。这个方法论旨在解决LLM推理过程中的根本性问题：输出的不可验证性和潜在的幻觉。通过将LLM生成的文本转换为可验证的论证图，论文致力于增强LLM推理过程的透明度、可靠性和稳健性。这是一种对LLM基础推理能力的改进，而非简单的应用，因此符合核心保留标准。 2.  **第二步：正面指标——论文高度相关。** -   **核心概念**: 论文的核心流程是“converting LLM text into argument graphs”，表明LLM是其研究的中心。 -   **能力方向**: 论文直接聚焦于“verifiable reasoning chains”（可验证的推理链）和“inferential step”（推理步骤），这正是推理能力的核心。 -   **新兴范式**: 论文明确提出了一个“multi-agent”（多智能体）协作框架，其中专业化智能体协同工作，这是提升LLM问题解决能力的前沿范式。 3.  **第三步：排除标准——论文不涉及排除领域。** 论文虽然提到了“risk assessment”（风险评估）作为应用案例，但其研究焦点是“如何通过结构化论证实现可信的智能体协作”，而不是风险评估领域本身的知识或问题。该方法论是通用的，可以迁移到其他需要严谨推理的任务中。论文不涉及多模态、特定垂直领域或应用层面的安全水印等排除内容。 4.  **第四步：处理特殊和模糊情况——论文是典型的“保留”案例。** -   **智能体/工具使用**: 论文提出的是一个通用的智能体协作框架，旨在通过透明化的协作来增强通用问题解决能力，而非针对特定领域的智能体应用。 -   **幻觉/可解释性**: 这是论文最突出的亮点之一。它没有停留在对幻觉现象的讨论，而是提出了一种**全新的、可操作的机制**——“enabling automatic hallucination detection via fact nodes attacking arguments”（通过事实节点攻击论点来实现自动幻觉检测）。这是一种通过改进模型内在推理结构来提升其可靠性和推理质量的方法，完全符合“保留”标准。它提供的可解释性是结构化的、可验证的，超越了简单的文本解释。 **最终决策：** 综合分析，这篇论文提出了一种创新的“结构化论证”框架，通过将LLM的输出转化为可验证的论证图，并利用多智能体协作机制，直接提升了LLM推理的通用能力、可靠性和可解释性。其核心贡献是方法论层面的创新，旨在解决LLM推理的根本性难题，因此与“提高大语言模型通用推理能力”的研究目标高度契合。**应予以保留。**"
    },
    {
        "index": "#3",
        "title": "Slm-mux: Orchestrating small language models for reasoning",
        "link": "/arxiv/2510.05077",
        "arxiv_id": "2510.05077",
        "authors": "Chenyu Wang, Zishen Wan, Hao Kang, Emma Chen, Zhiqiang Xie, Tushar Krishna, Vijay Janapa Reddi, Yilun Du",
        "summary": "With the rapid development of language models, the number of small language models (SLMs) has grown significantly. Although they do not achieve state-of-the-art accuracy, they are more efficient and often excel at specific tasks. This raises a natural question: can multiple SLMs be orchestrated into a system where each contributes effectively, achieving higher accuracy than any individual model? Existing orchestration methods have primarily targeted frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To address this gap, we propose a three-stage approach for orchestrating SLMs. First, we introduce SLM-MUX, a multi-model architecture that effectively coordinates multiple SLMs. Building on this, we develop two optimization strategies: (i) a model selection search that identifies the most complementary SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our approach delivers strong results: Compared to existing orchestration methods, our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0% on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and GSM8K, and matches its performance on MATH. We further provide theoretical analyses to substantiate the advantages of our method. In summary, we demonstrate that SLMs can be effectively orchestrated into more accurate and efficient systems through the proposed approach.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.562681",
        "filter_reason": "这篇论文完全符合您的研究范围，应该被保留。我的判断过程如下： **第一步：核心判断——论文的本质是提升通用推理能力** 这篇论文的核心贡献是提出了一种名为“SLM-MUX”的新方法，其本质是一种**多模型协作框架**。论文的目标不是将语言模型应用于某个特定领域，而是通过“编排”多个小型语言模型，让它们协同工作，从而**系统性提升整个系统的推理能力**。这直接命中了您筛选标准中的“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”以及“智能体协作框架”这两个要点。论文的出发点是“如何让多个模型组合起来比单个模型更强”，这是一个关于模型能力增强的基础方法论研究，而非应用研究。 **第二步：正面指标高度吻合** - **核心概念**: 论文研究对象是小语言模型，属于语言模型家族的范畴，其方法论对LLM同样具有启发性。 - **能力方向**: 论文标题和摘要中反复强调的核心词就是“reasoning”（推理）。其验证方法选用了MATH、GSM8K（数学推理）和GPQA（科学问题推理）等公认的通用推理能力基准数据集，这明确表明其研究焦点就是通用推理能力。 - **新兴范式**: “Orchestrating”（编排）多个模型的思想与“llm-based agents”或“multi-agent systems”的范式高度一致。这是一种通过多个智能体（这里是SLMs）协作来解决复杂问题的研究，完全符合保留标准。 **第三步和第四步：排除标准与特殊情况分析** - 论文完全没有涉及多模态、视觉、特定应用领域（医疗、化学等）或模型基础设施，因此**完全排除了所有排除标准**。 - 在处理“智能体”这一特殊情况时，这篇论文是提出一种**通用的编排框架**来提升**通用问题解决能力**，而不是将其应用于“化学实验自动化”等特定场景。因此，根据筛选标准，它属于应该保留的正面案例。 **第五步：最终决策** 综合以上分析，这篇论文提出了一种新颖的、系统性的方法论（SLM-MUX），旨在通过模型协作的方式从根本上提升语言模型的通用推理性能。它关注的是“如何让模型变得更会推理”这一核心科学问题，而非“如何用模型解决某个具体问题”。因此，它精准地契合了您“致力于提高大语言模型本身的通用推理能力”的核心研究目标。"
    },
    {
        "index": "#4",
        "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs",
        "link": "/arxiv/2510.05069",
        "arxiv_id": "2510.05069",
        "authors": "Dachuan Shi, Abedelkadir Asi, Keying Li, Xiangchi Yuan, Leyan Pan, Wenke Lee, Wen Xiao",
        "summary": "Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.563356",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础推理能力。** 论文的核心贡献是提出了一种名为“SwiReasoning”的新框架，旨在提升大语言模型的推理能力。它并非将LLM应用于某个特定领域，而是聚焦于优化LLM的“思考过程”本身。具体来说，它解决了现有两种推理范式——显式思维链和潜在空间推理——的局限性，通过动态切换这两种模式来平衡探索与利用、提升准确性和token效率。这是一种对LLM基础推理方法论的创新，完全符合“改进LLM的基础能力、增强其逻辑、数学、多步推理等通用能力”的核心目标。 2.  **第二步：正面指标——论文高度相关。** 论文明确包含了多个关键正面指标： *   **核心概念**: 标题和摘要中多次提及“Large language models, LLMs”。 *   **能力方向**: 论文的核心就是“reasoning”，详细探讨了显式和潜在的“reasoning”方式，并在“mathematics and STEM benchmarks”上进行验证，这直接对应了“reasoning (math reasoning, logical reasoning)”。 *   **新兴范式**: 论文对“chain-of-thought (CoT)”和“latent space reasoning”这两种前沿推理范式进行了创新性的结合与改进，属于LLM推理方法论的前沿研究。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 该论文的研究内容非常纯粹，完全没有触及任何排除标准： *   它不涉及多模态、视觉等领域。 *   它没有将方法应用于医疗、化学、生物等特定领域，而是使用数学等通用任务作为评估基准。 *   它不关注模型基础设施、部署优化或水印、安全等问题。 4.  **第四步：处理特殊和模糊情况——不适用。** 这篇论文的焦点非常清晰，是关于LLM内部的推理过程优化，不属于需要特殊处理的模糊情况。 **最终决策**: 综合以上分析，这篇论文的本质是提出一种新的、通用的推理框架（SwiReasoning）来增强LLM的内在推理能力和效率。它的研究目标、方法和评估基准都与您的核心目标“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”高度一致。因此，这篇论文是您研究课题的优质候选项，应予保留。"
    },
    {
        "index": "#13",
        "title": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models",
        "link": "/arxiv/2510.04933",
        "arxiv_id": "2510.04933",
        "authors": "Amir Hameed Mir",
        "summary": "Large Language Models (LLMs) often produce fluent yet factually incorrect statements-a phenomenon known as hallucination-posing serious risks in high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric framework for hallucination detection that analyzes the evolution of hidden-state semantics across transformer layers. Unlike prior methods that rely on multiple sampling passes or external verification sources, LSD operates intrinsically within the model's representational space. Using margin-based contrastive learning, LSD aligns hidden activations with ground-truth embeddings derived from a factual encoder, revealing a distinct separation in semantic trajectories: factual responses preserve stable alignment, while hallucinations exhibit pronounced semantic drift across depth. Evaluated on the TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming SelfCheckGPT and Semantic Entropy baselines while requiring only a single forward pass. This efficiency yields a 5-20x speedup over sampling-based methods without sacrificing precision or interpretability. LSD offers a scalable, model-agnostic mechanism for real-time hallucination monitoring and provides new insights into the geometry of factual consistency within large language models.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Theory, Machine Learning, Neural and Evolutionary Computing",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.573147",
        "filter_reason": "这篇论文符合我的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“逐层语义动力学（LSD）”的几何框架，用于在模型内部检测幻觉。这并非将LLM作为工具应用于特定领域，而是深入探究LLM内部工作机制（隐藏状态的语义演变），并提出一种新的方法论来理解和量化模型的事实一致性。这种对模型内在表示和推理过程的故障模式（幻觉）进行基础性研究，直接关联到提升LLM的通用推理质量。一个可靠的推理系统必须建立在事实正确的基础上，因此，检测并理解幻觉的成因是提升通用推理能力的关键一环。因此，这篇论文的本质是改进LLM的基础能力，应予以保留。 2.  **第二步：正面指标** - **核心概念**: 论文明确聚焦于 \"Large Language Models (LLMs)\"。 - **能力方向**: 虽然论文标题未直接提及 \"reasoning\"，但其研究的核心问题 \"hallucination\"（幻觉）是影响推理质量的关键因素。事实性是逻辑推理和多步问题解决的基石。论文通过分析模型的“语义轨迹”来区分事实与幻觉，这本质上是在探究模型进行事实性推理的内在动态过程。 - **新兴范式**: 论文提供了一种新的机制来监控和提升模型的可靠性，这可以被看作是构建更高级、更可靠的智能体或推理系统的基础组件。 3.  **第三步：排除标准** - 论文不涉及多模态、视觉或任何特定应用领域（如医疗、化学）。 - 论文虽然涉及“模型可靠性”，但其研究深度并非停留在应用层面的水印或安全策略，而是深入到模型内部的表示几何学，探究可靠性的根本来源。因此，它不属于“应用层面”的排除范畴。 4.  **第四步：处理特殊和模糊情况** 这篇论文是判断的关键。它完美地符合“幻觉/可解释性/安全”这一特殊情况的保留条件： - **提出新方法**: 论文提出的LSD框架是一种全新的、基于模型内部几何结构的方法，用于检测幻觉。 - **增强模型内在的可解释性**: 论文的核心价值之一在于它“为LLM内部事实一致性的几何结构提供了新的见解”。它不是简单地给出一个“是/否”的判断，而是解释了事实性陈述和幻觉在模型深度维度上的动态差异（“语义漂移”），这极大地增强了我们对模型内部工作原理的理解。 - **提升通用可靠性和推理质量**: 通过提供一种高效、内在的幻觉检测机制，该研究为提升LLM的通用可靠性铺平了道路。一个能实时自我监控事实一致性的模型，其输出的推理结果质量自然更高。 **最终决策**: 综合以上分析，这篇论文虽然主题是“幻觉检测”，但其研究路径是基础性和模型内在的。它通过提出一种新颖的几何分析方法，深入探究了LLM的表示层如何处理事实信息，并揭示了幻觉产生的内在动态。这种对模型核心机制的深刻洞察，直接服务于提升模型事实推理的可靠性，是增强LLM通用推理能力的重要组成部分。因此，这篇论文高度相关，应被筛选**保留**。"
    },
    {
        "index": "#16",
        "title": "Detecting Distillation Data from Reasoning Models",
        "link": "/arxiv/2510.04850",
        "arxiv_id": "2510.04850",
        "authors": "Hengxiang Zhang, Hyeong Kyu Choi, Yixuan Li, Hongxin Wei",
        "summary": "Reasoning distillation has emerged as an efficient and powerful paradigm for enhancing the reasoning capabilities of large language models. However, reasoning distillation may inadvertently cause benchmark contamination, where evaluation data included in distillation datasets can inflate performance metrics of distilled models. In this work, we formally define the task of distillation data detection, which is uniquely challenging due to the partial availability of distillation data. Then, we propose a novel and effective method Token Probability Deviation (TBD), which leverages the probability patterns of the generated output tokens. Our method is motivated by the analysis that distilled models tend to generate near-deterministic tokens for seen questions, while producing more low-probability tokens for unseen questions. Our key idea behind TBD is to quantify how far the generated tokens' probabilities deviate from a high reference probability. In effect, our method achieves competitive detection performance by producing lower scores for seen questions than for unseen questions. Extensive experiments demonstrate the effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of 0.470 on the S1 dataset.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.574586",
        "filter_reason": "这篇论文符合你的筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断——这篇论文的本质是什么？** 论文的核心贡献并非直接提出一种新的训练方法来“提升”LLM的推理能力，而是提出了一种方法来“检测”现有推理能力提升方法（特别是推理蒸馏）中存在的一个关键问题：数据污染。虽然它没有直接增强模型，但它为整个“提升LLM推理能力”的研究领域提供了一个至关重要的**评估和保障工具**。如果无法有效区分模型是真的学会了推理，还是仅仅记住了答案，那么所有关于提升推理能力的研究都将失去其科学性和可靠性。因此，这篇论文的本质是解决该研究领域的一个基础性、方法论的难题，其最终目标是确保我们能更准确、更公平地评估和推动LLM通用推理能力的进步。这与你的核心目标高度一致。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文摘要中包含了多个高相关性的正面指标： *   **核心概念**: 明确提到了 \"Reasoning Models\" 和 \"large language models\"。 *   **能力方向**: 核心围绕 \"reasoning capabilities\"。 *   **训练方法**: 聚焦于 \"Reasoning distillation\"，这是一种被广泛用于增强模型推理能力的训练范式。 这些指标表明论文与你的研究主题紧密相关。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全没有涉及多模态、视觉，也没有将其方法应用于任何特定领域（如医疗、化学等）。同时，它讨论的“数据污染检测”问题，其本质是关于模型评估的科学严谨性，而非应用层面的水印、安全或伦理问题。因此，它不符合任何一条排除标准。 4.  **第四步：处理特殊和模糊情况** 这篇论文的情况类似于“可靠性”研究的延伸。它提出的方法（TBD）可以被看作是一种提升模型“内在可解释性”和“可靠性”的工具，其目的是揭示模型性能背后的真实原因（是真正学会了推理，还是记忆了数据）。这直接服务于提升模型的“通用推理质量”，因为它帮助研究者过滤掉虚假的性能提升。因此，根据筛选标准中“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留”的原则，这篇论文应该被保留。 5.  **第五步：最终决策** 综合以上分析，尽管这篇论文没有直接提出一种让模型“变得更聪明”的算法，但它为整个“让模型变得更聪明”的研究社区提供了一把“尺子”，用来精确衡量和验证各种方法的真实效果。这种元研究（meta-research）对于推动领域健康发展至关重要。它直接解决了推理能力提升过程中的一个核心挑战，确保了研究的有效性，因此完全符合你为“大语言模型通用推理能力”这一课题筛选前沿论文的要求。"
    },
    {
        "index": "#27",
        "title": "Multi-Agent Tool-Integrated Policy Optimization",
        "link": "/arxiv/2510.04678",
        "arxiv_id": "2510.04678",
        "authors": "Zhanfeng Mo, Xingxuan Li, Yuntao Chen, Lidong Bing",
        "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge-intensive and complex reasoning tasks. Existing implementations typically rely on a single agent, but they suffer from limited context length and noisy tool responses. A natural solution is to adopt a multi-agent framework with planner- and worker-agents to manage context. However, no existing methods support effective reinforcement learning post-training of tool-integrated multi-agent frameworks. To address this gap, we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which enables distinct roles (planner and worker) to be trained within a single LLM instance using role-specific prompts via reinforcement learning. MATPO is derived from a principled credit assignment mechanism across planner and worker rollouts. This design eliminates the need to deploy multiple LLMs, which would be memory-intensive, while preserving the benefits of specialization. Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently outperforms single-agent baselines by an average of 18.38% relative improvement in performance and exhibits greater robustness to noisy tool outputs. Our findings highlight the effectiveness of unifying multiple agent roles within a single LLM and provide practical insights for stable and efficient multi-agent RL training.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.584942",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **第一步：核心判断——论文本质符合要求** 论文的核心贡献是提出了一种名为 **Multi-Agent Tool-Integrated Policy Optimization (MATPO)** 的**新训练范式**。其本质是使用**强化学习（RL）**来训练一个单一的大语言模型，使其内部能够分化出“规划者”和“工作者”等多种角色，并通过**工具使用**来协同完成复杂的推理任务。这直接命中了“改进LLM的基础能力”、“提出新的训练范式”、“增强其逻辑、规划、多步推理等通用能力”的核心要求。它并非将LLM应用于特定领域，而是致力于提升LLM模型本身解决通用问题的内在机制和能力。 2.  **第二步：正面指标——论文高度相关** 论文摘要中几乎包含了所有关键的正面指标： *   **核心概念**: 明确以 \"Large language models (LLMs)\" 为研究对象。 *   **能力方向**: 目标是解决 \"knowledge-intensive and complex reasoning tasks\"，并涉及 \"planning\"。 *   **训练方法**: 核心方法是 \"reinforcement learning (RL)\" 和 \"Policy Optimization\"。 *   **新兴范式**: 论文聚焦于 \"multi-agent framework\" 和 \"tool-integrated planning\"。 3.  **第三步：排除标准——论文不涉及任何排除领域** 论文的研究内容是纯粹的方法论创新，不涉及多模态、视觉，也没有将方法应用于医疗、化学、机器人等任何特定领域。它讨论的鲁棒性是针对模型在推理过程中处理“噪声工具输出”的能力，这属于提升推理质量本身，而非应用层面的安全或水印问题。 4.  **第四步：处理特殊和模糊情况——智能体/工具使用** 论文提出的多智能体框架是一个**通用的推理框架**（规划者-工作者），旨在提升模型在复杂任务上的通用问题解决能力，而非应用于某个特定垂直领域。这完全符合“保留”标准，是当前提升LLM推理能力的前沿研究方向。 **最终决策**: 该论文提出了一种基于强化学习的创新训练方法（MATPO），通过将多智能体协作和工具使用能力内化到单一LLM中，显著提升了其在复杂推理任务上的表现。其研究目标、方法和贡献都精准地聚焦于“提升大语言模型本身的通用推理能力”，是与你研究课题高度相关的前沿论文，应予以保留。"
    },
    {
        "index": "#36",
        "title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization",
        "link": "/arxiv/2510.04506",
        "arxiv_id": "2510.04506",
        "authors": "Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han",
        "summary": "Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.594587",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于提出了一种新颖的训练范式，该范式明确地利用并增强了大语言模型的推理能力。以下是根据您的筛选标准进行的详细判断过程： 1.  **第一步：核心判断** 论文的本质是改进LLM的基础能力，而非将其作为特定领域的工具。GRACE框架的核心思想是，将传统上用于训练静态文本编码器的对比损失，转变为引导模型生成过程的“奖励信号”。在这个过程中，LLM被训练来生成“rationales”（即结构化的自然语言解释），这本身就是一种语义推理和逻辑表达的过程。这种方法不是将LLM视为一个黑盒，而是鼓励它展现其内部的推理链条。因此，论文的核心是提出一种**增强模型内在推理透明度和能力的新训练范式**，符合保留标准。 2.  **第二步：正面指标** 论文与多个正面指标高度吻合： - **核心概念**: 明确以大语言模型为研究对象。 - **能力方向**: 虽然不是数学或符号推理，但论文核心的“rationales”生成是**语义推理**的一种体现，即模型需要解释其对文本语义的理解过程。论文中也多次使用了“reasoning capabilities”、“reasoning process”等关键词。 - **训练方法**: 采用了“Contrastive Policy Optimization”，即对比策略优化。这是一种典型的强化学习方法，使用“policy gradient”进行训练，与筛选标准中的“reinforcement learning (RL)”完全一致。 - **新兴范式**: 论文将LLM从一个“opaque encoder”转变为一个“interpretable agent”，这与“llm-based agents”的理念相符，即模型不再是单纯的功能函数，而是具有可观察内部过程的智能体。 3.  **第三步：排除标准** 论文未触及任何排除标准。它不涉及多模态、特定应用领域（评估是在通用的MTEB基准上进行的），也不讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** - **可解释性**: 这篇论文是“通过提出新方法来增强模型内在可解释性”的典型案例。它不是简单地讨论幻觉现象，而是通过让模型生成可读的rationales，从根本上改变了模型的训练目标和内部工作机制，使其推理过程变得“transparent and inspectable”。这直接提升了模型的通用推理质量和可靠性，因此应该保留。 5.  **第五步：最终决策** 综合来看，GRACE论文虽然最终目标是提升文本表示（embedding）的质量，但其实现路径完全聚焦于**如何通过强化学习来引导和利用LLM的生成式推理能力**。它将模型视为一个能够进行推理的智能体，并通过奖励机制来优化其推理过程（生成rationales）。这种“统一表示学习与生成，以产出更透明推理过程”的方法，正是致力于提升LLM本身通用推理能力的前沿研究，与您的核心目标高度契合。 因此，最终判断为 **True**。"
    },
    {
        "index": "#40",
        "title": "Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners",
        "link": "/arxiv/2510.04454",
        "arxiv_id": "2510.04454",
        "authors": "Xiangchi Yuan, Xiang Chen, Tong Yu, Dachuan Shi, Can Jin, Wenke Lee, Saayan Mitra",
        "summary": "Large Language Models (LLMs) show strong reasoning abilities, often amplified by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although RL algorithms can substantially improve reasoning, they struggle to expand reasoning boundaries because they learn from their own reasoning trajectories rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers complementary benefits but typically requires large-scale data and risks overfitting. Recent attempts to combine SFT and RL face three main challenges: data inefficiency, algorithm-specific designs, and catastrophic forgetting. We propose a plug-and-play framework that dynamically integrates SFT into RL by selecting challenging examples for SFT. This approach reduces SFT data requirements and remains agnostic to the choice of RL or SFT algorithm. To mitigate catastrophic forgetting of RL-acquired skills during SFT, we select high-entropy tokens for loss calculation and freeze parameters identified as critical for RL. Our method achieves state-of-the-art (SoTA) reasoning performance using only 1.5% of the SFT data and 20.4% of the RL data used by prior SoTA, providing an efficient and plug-and-play solution for combining SFT and RL in reasoning post-training.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.601778",
        "filter_reason": "这篇论文完全符合筛选要求。 **判断过程与核心依据如下：** 1.  **第一步：核心判断 (保留)** *   论文的本质是提出一种新的训练范式（一个“即插即用框架”），用于解决在结合监督微调（SFT）和强化学习（RL）时遇到的“灾难性遗忘”问题。 *   其最终目标是“Yields Stronger Reasoners”（产生更强的推理者），并明确指出该方法在“reasoning post-training”（推理后训练）中取得了最先进的性能。 *   这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。它不是将LLM应用于特定领域，而是致力于提升模型本身的核心推理能力。 2.  **第二步：正面指标 (高度相关)** *   **核心概念**: 论文明确以“Large Language Models (LLMs)”为研究对象。 *   **能力方向**: 论文的核心主题是“reasoning”（推理），并在摘要中多次提及，如“strong reasoning abilities”、“improve reasoning”、“reasoning performance”。 *   **训练方法**: 论文的核心贡献是关于如何结合“Supervised Fine-Tuning (SFT)”和“Reinforcement Learning (RL)”这两种关键的训练方法，属于方法论研究。 3.  **第三步：排除标准 (不涉及)** *   论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学等）或模型可靠性（如水印、安全）等排除标准中的任何内容。其关注点是纯粹的模型训练算法和推理能力提升。 4.  **第四步：处理特殊和模糊情况 (不适用)** *   论文不涉及智能体/工具使用，也不涉及幻觉/可解释性/安全等特殊情况的讨论。其提出的“减轻遗忘”技术（选择高熵token进行损失计算和冻结关键参数）是一种优化训练过程的技术手段，旨在更好地保留RL学到的推理技能，属于训练方法论的范畴。 **最终决策：** 综合以上分析，该论文的核心贡献在于提出了一种创新的、高效的训练框架，通过优化SFT和RL的结合方式，直接提升了大语言模型的通用推理能力。这与“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”这一核心目标高度一致。因此，应予以保留。"
    },
    {
        "index": "#50",
        "title": "Read the Scene, Not the Script: Outcome-Aware Safety for LLMs",
        "link": "/arxiv/2510.04320",
        "arxiv_id": "2510.04320",
        "authors": "Rui Wu, Yihao Quan, Zeru Shi, Zhenting Wang, Yanshu Li, Ruixiang Tang",
        "summary": "Safety-aligned Large Language Models (LLMs) still show two dominant failure modes: they are easily jailbroken, or they over-refuse harmless inputs that contain sensitive surface signals. We trace both to a common cause: current models reason weakly about links between actions and outcomes and over-rely on surface-form signals, lexical or stylistic cues that do not encode consequences. We define this failure mode as Consequence-blindness. To study consequence-blindness, we build a benchmark named CB-Bench covering four risk scenarios that vary whether semantic risk aligns with outcome risk, enabling evaluation under both matched and mismatched conditions which are often ignored by existing safety benchmarks. Mainstream models consistently fail to separate these risks and exhibit consequence-blindness, indicating that consequence-blindness is widespread and systematic. To mitigate consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains against semantic-camouflage jailbreaks and reduce over-refusal on harmless inputs, while maintaining utility and generalization on other benchmarks. These results clarify the limits of current alignment, establish consequence-aware reasoning as a core alignment goal and provide a more practical and reproducible evaluation path.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.611794",
        "filter_reason": "这篇论文完全符合您的筛选标准，其核心贡献是提出了一种提升LLM通用推理能力的新方法。 以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断** - **保留**。这篇论文的本质并非讨论如何在特定领域应用LLM，也不是关于模型基础设施。它将LLM当前在安全性上的两个主要失败模式（被越狱、过度拒绝）归结于一个更深层次的根本原因：**“Consequence-blindness”（结果盲视）**，即模型无法有效推理“行为”与“结果”之间的联系。 - 论文的核心贡献是提出了**“outcome-aware reasoning”（结果感知推理）**这一新的推理范式，并构建了相应的数据集（CS-Chain-4k）和训练方法来直接提升模型的这一能力。这属于改进LLM基础能力和提出新训练范式的范畴，直接增强了模型的逻辑推理能力。 2.  **第二步：正面指标** - 论文完全符合多项正面指标。 - **核心概念**: 明确聚焦于 **Large language models (LLMs)**。 - **能力方向**: 核心就是 **reasoning**。它定义了一种新的推理类型——**outcome-aware reasoning**，旨在解决模型在逻辑链条上的缺陷。 - **训练方法**: 提出了一种新的**微调**范式，通过专门构建的数据集来对齐模型，使其具备结果感知的推理能力。 3.  **第三步：排除标准** - 论文不涉及多模态、视觉或特定应用领域（如医疗、化学等）。 - **关键点**：虽然论文标题和摘要中多次出现“Safety”，看似触及了排除标准中的“模型可靠性（应用层面）”，但需要结合第四步进行深入分析。 4.  **第四步：处理特殊和模糊情况** - 这正是本文的关键所在。论文完美契合了您对“安全”这一模糊情况的处理原则。 - 论文并非从应用层面讨论如何给模型打“安全补丁”，而是**提出一种新方法来提升模型的内在能力，从而增强其通用可靠性**。它将安全问题重构为一种**推理能力的缺失**。 - 论文的核心论点是：“To mitigate consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning dataset for safety alignment.”。这表明，其解决方案是**通过提升“consequence-reasoning”这一通用推理能力，来系统性地解决安全问题**。 - 这种做法与您的标准——“如果论文提出一种新方法来...提升模型的通用可靠性和推理质量，应该保留”——完全一致。它不是在讨论安全的社会学影响，而是在修改模型内部的推理“引擎”。 5.  **第五步：最终决策** - 综合来看，尽管论文的动机和实验评估围绕“安全性”展开，但其**本质贡献是识别并修复了一种根本性的推理缺陷**。它提出了一种名为“结果感知推理”的通用推理能力，并给出了训练方法来实现它。这种方法论上的创新，直接指向了提升LLM的“通用推理能力”这一核心目标。因此，这篇论文不仅符合，而且是您研究课题下非常相关和高价值的前沿论文。"
    },
    {
        "index": "#52",
        "title": "Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness",
        "link": "/arxiv/2510.04293",
        "arxiv_id": "2510.04293",
        "authors": "Lingnan Xu, Chong Feng, Kaiyuan Zhang, Liu Zhengyong, Wenqiang Xu, Fanqing Meng",
        "summary": "While large language models (LLMs) demonstrate impressive capabilities, their reliance on parametric knowledge often leads to factual inaccuracies. Retrieval-Augmented Generation (RAG) mitigates this by leveraging external documents, yet existing approaches treat retrieved passages as isolated chunks, ignoring valuable structure that is crucial for document organization. Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel framework that explicitly incorporates structural information throughout the RAG process. RDR2 employs an LLM-based router to dynamically navigate document structure trees, jointly evaluating content relevance and hierarchical relationships to assemble optimal evidence. Our key innovation lies in formulating document routing as a trainable task, with automatic action curation and structure-aware passage selection inspired by human reading strategies. Through comprehensive evaluation on five challenging datasets, RDR2 achieves state-of-the-art performance, demonstrating that explicit structural awareness significantly enhances RAG systems' ability to acquire and utilize knowledge, particularly in complex scenarios requiring multi-document synthesis.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.612714",
        "filter_reason": "这篇论文符合筛选要求，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为RDR2的新框架，用于改进检索增强生成（RAG）方法。它并非将LLM应用于某个特定领域，而是致力于解决LLM在利用外部知识时的一个根本性缺陷：忽略文档结构。通过让LLM能够理解和导航文档的结构层次，该方法提升了LLM获取和整合信息的能力。这属于“提出新的训练范式”或“增强其通用能力”的范畴，因为它改进了LLM处理复杂信息输入的基础流程，从而间接但有力地增强了其推理能力。因此，论文通过了核心判断。 2.  **第二步：正面指标** - **核心概念**: 论文明确以\"Large language models (LLMs)\"为核心研究对象。 - **能力方向**: 虽然摘要未直接使用\"reasoning\"一词，但其强调的\"multi-document synthesis\"（多文档综合）和\"complex scenarios\"（复杂场景）正是高级推理能力的体现。综合多个信息源以形成连贯、准确的答案，是逻辑推理和问题解决的关键部分。 - **新兴范式**: 该研究与\"tool use\"（工具使用）和\"llm-based agents\"（基于LLM的智能体）高度相关。RDR2框架中的LLM路由器可以被视为一个智能体，其任务是导航“文档结构树”这一工具，以解决“找到最优证据”这一子问题。这是一种通用的方法论，不局限于特定领域。 3.  **第三步：排除标准** 论文的研究焦点不涉及多模态、视觉、任何特定应用领域（如医疗、化学），也不关注模型部署、水印或安全等问题。因此，它完全避开了所有排除标准。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的典型范例。RDR2框架本身是通用的，其目标是提升LLM在需要整合复杂信息时的通用表现，而非应用于特定领域。因此，根据此规则，应予以保留。 - **幻觉/可解释性/安全**: 论文通过提供更精确、结构化的证据，直接解决了RAG系统可能因信息片段孤立而产生的事实错误（一种幻觉形式）。它提出了一种新的框架来提升模型的内在可靠性，而非进行应用层面的讨论，这也符合保留条件。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一种通用的、基于LLM智能体的新框架（RDR2），通过赋予LLM文档结构感知能力，显著增强了其获取、整合和利用外部知识进行复杂问题求解的能力。这直接服务于“提高大语言模型通用推理能力”这一核心目标，因为它为LLM在需要深度信息综合的推理任务中提供了更强大的基础。因此，最终判断为 **True**。"
    },
    {
        "index": "#51",
        "title": "Measuring Language Model Hallucinations Through Distributional Correctness",
        "link": "/arxiv/2510.04302",
        "arxiv_id": "2510.04302",
        "authors": "Thomas F Burns",
        "summary": "Common evaluation paradigms for language models focus on scoring single responses through accuracy metrics or proper scoring rules, failing to capture the full richness of a model's belief state. Recent work illustrates that language models hallucinate in-part because they are optimised to be good test-takers under binary scoring schemes that reward any answer over abstention. While this insight naturally leads to penalty-based approaches, they ignore crucial distinctions in how models distribute uncertainty, for example between hedging toward incorrect answers versus hedging toward \"I don't know\" responses. A novel evaluation metric, the Distributional Correctness Score (DCS), is introduced to solve this problem, i.e., of not considering a model's entire probability distribution over answer choices. DCS naturally distinguishes between harmful overconfidence in wrong answers and uncertainty expressed through abstention, providing scores in an interpretable default range. Through theoretical analysis and illustrative examples, DCS is demonstrated to offer a more nuanced and aligned evaluation paradigm that incentivises models to express genuine uncertainty rather than guessing. Adapting 12 existing evaluation benchmarks to DCS's variants and measuring performance on six language models reveals that for half of the tested benchmarks scores are negative across all tested models, indicating significant tendencies towards hallucination.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.612224",
        "filter_reason": "这篇论文的核心贡献是提出了一种新的评估指标——分布正确性得分（DCS），用于更精细地衡量和评估大语言模型的幻觉现象。虽然论文本身没有直接提出一种新的训练方法或架构来“提高”模型能力，但它完全符合我的研究范围，理由如下： 1.  **核心判断（第一步）**: 论文的本质并非将LLM作为工具应用于特定领域，而是直面LLM本身的核心缺陷——幻觉。幻觉是通用推理能力（尤其是逻辑和事实推理）的直接障碍。这篇论文通过提出一种新的评估范式，旨在从根本上改变对模型行为的激励机制，这属于改进LLM基础能力的范畴。它不是应用研究，而是关于模型内在能力和评估方法论的深度研究。 2.  **符合特殊情况的判断（第四步）**: 这篇论文是“幻觉/可解释性/安全”这一特殊情况的典型范例。筛选标准明确指出：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” DCS正是这样一种方法。它通过“激励模型表达真实的不确定性而非猜测”，直接作用于模型的推理过程。一个懂得在不确定时“弃权”或表达不确定性的模型，其推理的可靠性和质量远高于一个盲目猜测、产生幻觉的模型。因此，DCS为提升模型的内在推理质量提供了关键的评估和优化方向。 3.  **与研究目标的关联性**: 我的核心目标是筛选“致力于提高LLM通用推理能力”的论文。提高推理能力不仅包括让模型“会做”，也包括让模型“知道自己不会做”。后者是高级推理和认知能力的重要体现。这篇论文通过提出一种能够区分“有害的过度自信”和“有益的不确定性表达”的评估方法，为训练出更诚实、更可靠的推理模型铺平了道路。它解决了当前优化目标（如准确率）与理想推理能力之间的错位问题，是推动LLM通用推理能力向前发展的关键性基础研究。 综上所述，该论文虽然没有提出一个端到端的训练方案，但它提出了一种能够引导和激励模型向更优推理行为（即减少无根据的猜测）发展的评估方法论。这直接触及了通用推理能力的核心——可靠性与诚实度，因此完全符合筛选要求。"
    },
    {
        "index": "#60",
        "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling",
        "link": "/arxiv/2510.04204",
        "arxiv_id": "2510.04204",
        "authors": "Zhengyang Tang, Zihan Ye, Chenyu Huang, Xuhan Huang, Chengpeng Li, Sihang Li, Guanhua Chen, Ming Yan, Zizhuo Wang, Hongyuan Zha, Dayiheng Liu, Benyou Wang",
        "summary": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs -- In particular, we show that direct fine-tuning on traditional \\textit{non-reflective} datasets leads to limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose \\textbf{CALM} (\\textit{Corrective Adaptation with Lightweight Modification}), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6\\% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop \\textbf{STORM} (\\textit{Smart Thinking Optimization Reasoning Model}), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9\\% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks.",
        "subjects": "Computation and Language, Artificial Intelligence, Computational Engineering, Finance, and Science, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.622074",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出一种名为**CALM**的新颖训练框架，旨在**改进和放大大语言模型（LRMs）固有的推理模式**。它并非简单地将现有LLM应用于某个特定领域，而是深入研究如何通过“专家干预者提供修正性提示”和“强化学习”等方法，来优化模型自身的推理轨迹（reasoning trajectories）。其核心贡献是一种**新的训练范式**，旨在提升模型的基础推理能力，这与你的目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度一致。 **第二步：正面指标——论文是否包含以下主题？** 论文包含了多个强烈的正面指标： - **核心概念**: 论文明确以“Large Reasoning Models (LRMs)”为研究对象。 - **能力方向**: 核心主题就是“reasoning”，特别是“complex multi-step reasoning”和“native reasoning patterns”。 - **训练方法**: 明确提出了“supervised fine-tuning”和“reinforcement learning”作为其框架的关键组成部分。 - **新兴范式**: CALM框架中的“expert intervener”机制，可以看作是一种增强模型自我修正和问题解决能力的通用框架，与智能体和工具使用的思想有共通之处。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文不涉及任何排除标准领域。它没有讨论多模态、视觉，也没有聚焦于医疗、化学等特定应用领域。虽然论文的实验基准是“optimization modeling”，但这应被视为一个**衡量复杂推理能力的标准测试平台**，而非一个应用领域。论文的焦点是“如何让模型更好地推理”，而不是“如何用模型解决某个化学问题”。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的CALM框架，通过一个“专家干预者”来提供修正性提示，这本质上是一种通用的、用于增强模型推理能力的交互式框架。它不是为特定领域（如化学）设计的，而是旨在提升模型在复杂任务上的通用问题解决能力，因此符合保留条件。 - **特定应用领域**: “Optimization modeling”是数学和计算机科学中的一个核心问题类型，它高度依赖逻辑、规划和多步推理。因此，研究如何提升模型在这方面的能力，等同于研究如何提升其通用推理能力。这与将模型应用于法律、医疗等需要大量领域知识的场景有本质区别。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种能够“保留并放大”LLM“固有的推理模式”的通用训练框架（CALM）。它通过创新的方法论（提示修正、强化学习）来提升模型的基础推理能力，并在一个公认的复杂推理任务（优化建模）上验证了其有效性。这完全符合你筛选“致力于提高大语言模型（LLM）本身的『通用推理能力』”的前沿论文的目标。因此，应予以保留。"
    },
    {
        "index": "#61",
        "title": "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization",
        "link": "/arxiv/2510.04182",
        "arxiv_id": "2510.04182",
        "authors": "Wengao Ye, Yan Liang, Lianlei Shan",
        "summary": "Recent advancements in Large Language Models (LLMs) have shifted from explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning, where intermediate thoughts are represented as vectors rather than text. However, latent reasoning can be brittle on challenging, out-of-distribution tasks where robust reasoning is most critical. To overcome these limitations, we introduce Latent Thought Policy Optimization (LTPO), a parameter-free framework that enhances LLM reasoning entirely at test time, without requiring model parameter updates. LTPO treats intermediate latent \"thought\" vectors as dynamic parameters that are actively optimized for each problem instance. It employs an online policy gradient method guided by an intrinsic, confidence-based reward signal computed directly from the frozen LLM's own output distributions, eliminating the need for external supervision or expensive text generation during optimization. Extensive experiments on five reasoning benchmarks show that LTPO not only matches or surpasses strong baselines on standard tasks but also demonstrates remarkable robustness where others fail. Most notably, on highly challenging AIME benchmarks where existing latent reasoning baselines collapse to near-zero accuracy, LTPO delivers substantial improvements, showcasing a unique capability for complex reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.622539",
        "filter_reason": "这篇论文完全符合研究范围，是一篇高质量的相关论文。以下是基于筛选标准的详细判断过程： 1.  **第一步：核心判断——论文的本质是改进LLM的基础推理能力。** 论文的核心贡献是提出了一种名为“潜在思维策略优化（LTPO）”的新框架。这个框架的本质不是将LLM应用于某个特定领域，而是直接改进LLM在**推理**这一核心能力上的表现。它通过在测试时动态优化中间的“潜在思维”向量，来增强模型在复杂、分布外任务上的推理鲁棒性。这是一种旨在提升模型内在能力、属于通用推理方法论前沿的研究，完全符合“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——论文命中多个高相关度主题。** - **核心概念**: 摘要中明确提到“Large Language Models (LLMs)”。 - **能力方向**: 论文的核心就是“reasoning”，并特别关注其在“challenging, out-of-distribution tasks”上的表现，以及“complex reasoning”，这正是通用推理能力的核心。通过在“AIME benchmarks”（美国数学邀请赛）上进行测试，直接证明了其在**数学推理**上的效果。 - **训练方法**: 论文采用了“online policy gradient method”，这是一种强化学习技术，用于优化思维策略，符合强化学习这一正面指标。 - **新兴范式**: “Latent reasoning”（潜在推理）是当前超越显式“Chain-of-Thought”的前沿范式。而“Test-Time Reasoning Enhancement”（测试时推理增强）是一种新颖的、旨在提升模型即时表现的方法论，属于提升模型能力的创新范式。 3.  **第三步：排除标准——论文未触及任何排除领域。** 论文研究的是纯文本语言模型的推理能力，不涉及任何多模态（如视觉）、特定应用领域（如医疗、化学），也未讨论模型基础设施、部署优化或应用层面的可靠性（如水印、安全）问题。 4.  **第四步：处理特殊和模糊情况——不适用。** 论文不属于需要特殊处理的智能体/工具使用或幻觉/可解释性范畴，其焦点非常清晰，即通用推理能力的增强方法。 5.  **第五步：最终决策——保留。** 综合来看，这篇论文精准地聚焦于“如何提升LLM的通用推理能力”这一核心目标。它提出的LTPO框架是一种创新的方法论，通过在测试时优化潜在推理过程，显著提升了模型在复杂推理任务上的鲁棒性和准确性。这完全契合“致力于提高大语言模型（LLM）本身的『通用推理能力』”的研究课题要求。"
    },
    {
        "index": "#66",
        "title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning",
        "link": "/arxiv/2510.04081",
        "arxiv_id": "2510.04081",
        "authors": "Honglin Lin, Qizhi Pei, Xin Gao, Zhuoshi Pan, Yu Li, Juntao Li, Conghui He, Lijun Wu",
        "summary": "Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Caco's code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention.",
        "subjects": "Computation and Language, Programming Languages",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.624930",
        "filter_reason": "根据您提供的筛选标准，这篇论文完全符合研究范围。以下是详细的判断过程： **第一步：核心判断** 这篇论文的本质是提出一种名为Caco（Code-Assisted Chain-of-ThOught）的新框架，旨在通过代码辅助的方式自动化地生成高质量、可验证的指令-思维链（Instruction-CoT）数据，从而提升大语言模型的推理能力。其核心贡献在于**改进LLM的基础能力**（推理能力）和**提出新的训练范式**（通过代码驱动生成和验证数据的闭环流程）。这完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。它并非将LLM作为工具应用于某个特定领域，而是聚焦于模型能力的内在提升。 **第二步：正面指标** 该论文高度匹配多个正面指标： *   **核心概念**: 论文明确围绕\"Large Language Models (LLMs)\"展开。 *   **能力方向**: 论文的核心是提升\"reasoning capability\"，具体涉及\"Chain-of-Thought (CoT)\"、\"logical correctness\"和\"mathematical reasoning\"，这些都是通用推理的关键组成部分。 *   **训练方法**: 论文提出的Caco框架是一种新颖的数据合成与模型训练范式。虽然未使用强化学习（RL），但其\"closed-loop process\"（闭环过程）、\"fully automated...without human intervention\"（完全自动化...无需人工干预）的理念，与\"self-evolve\"（自我进化）的思路高度契合，即构建一个能够自我完善和规模化生产高质量数据的系统。 *   **新兴范式**: \"Code-Assisted\"（代码辅助）是\"tool use\"（工具使用）的一种具体体现。它利用代码的可执行性作为工具，来验证和提升推理路径的逻辑正确性，从而增强LLM的通用问题解决能力。 **第三步：排除标准** 该论文未触及任何排除标准： *   **多模态与视觉**: 论文完全聚焦于语言和代码，不涉及任何视觉或多模态内容。 *   **特定应用领域**: 尽管论文在数学推理基准上进行了测试，但其方法本身是通用的。作者明确指出，其目标是解决现有方法\"hindering scalability and generalizability\"（阻碍可扩展性和泛化性）的问题，并展示了模型在\"unseen tasks\"（未见任务）上的\"superior generalization\"（卓越的泛化能力）。因此，数学只是作为衡量通用推理能力的一个标准测试场，而非论文的研究领域本身。 *   **模型可靠性（应用层面）**: 论文关注的是通过代码执行来保证推理过程的\"logical correctness\"（逻辑正确性），这属于模型内在能力的提升，而非水印、安全等应用层面的可靠性研究。 **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 该论文是“提出一种通用的工具使用方法来增强LLM的通用问题解决能力”的典范。它将代码作为一种通用工具来验证和生成推理路径，而不是将其应用于“化学实验”等特定场景。因此，根据标准，应予以保留。 *   **幻觉/可解释性/安全**: 论文通过代码执行的自动化验证来确保推理路径的逻辑正确性，这直接从方法论上减少了模型产生逻辑错误（一种幻觉）的可能性，从而提升了模型推理的内在质量和可靠性。这符合保留标准。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种创新的、可扩展的训练框架（Caco），利用代码作为工具来增强LLM的通用推理能力。它不仅触及了CoT、工具使用等关键主题，还提出了一种具有自我进化潜力的闭环数据生成范式。论文完全符合您的研究范围，是一篇关于提升LLM基础通用推理能力的高质量前沿研究。"
    },
    {
        "index": "#57",
        "title": "Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought",
        "link": "/arxiv/2510.04230",
        "arxiv_id": "2510.04230",
        "authors": "Guijin Son, Donghun Yang, Hitesh Laxmichand Patel, Amit Agarwal, Hyunwoo Ko, Chanuk Lim, Srikant Panda, Minhyuk Kim, Nikunj Drolia, Dasol Choi, Kyong-Ha Lee, Youngjae Yu",
        "summary": "Recent frontier models employ long chain-of-thought reasoning to explore solution spaces in context and achieve stonger performance. While many works study distillation to build smaller yet capable models, most focus on English and little is known about language-specific reasoning. To bridge this gap, we first introduct **Language-Mixed CoT**, a reasoning schema that switches between English and a target language, using English as an anchor to excel in reasoning while minimizing translation artificats. As a Korean case study, we curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5, Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves state-of-the-art performance, with the highest overall average score (64.0 \\pm 25), ranking first on 5/9 benchmarks and second on the remainder. Samller and mid-sized models also benefit substantially, with an average improvement of +18.6 points across teh evaluated nine benchmarks. Ablations show **Language-Mixed CoT** is more effective than monolingual CoT, also resulting in cross-lingual and mult-modal performance gains. We release our data-curation pipeline, evaluation system, datasets, and models to advance research on language-specific reasoning. Data and model collection: https://huggingface.co/KOREAson.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.615229",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Language-Mixed Chain-of-Thought”（语言混合思维链）的新型推理模式。这并非将LLM作为工具应用于特定领域，而是直接改进LLM的基础推理机制。该方法通过在英语和目标语言（如韩语）之间切换，旨在增强模型在非英语环境下的逻辑和数学推理能力。这完全属于“改进LLM的基础能力”、“增强其逻辑、数学、多步推理等通用能力”的范畴，应予以**保留**。 2.  **第二步：正面指标** 论文与多个正面指标高度匹配： *   **核心概念**: 论文的研究对象是大型语言模型。 *   **能力方向**: 论文的标题和摘要中反复强调的核心主题是“Reasoning”（推理），并涉及数学、STEM和代码等具体推理任务。 *   **新兴范式**: 论文的核心工作是对“Chain-of-Thought”（思维链）这一关键推理范式的创新和扩展，提出了“Language-Mixed CoT”这一新方法。 3.  **第三步：排除标准** 论文不触及任何排除标准： *   **多模态与视觉**: 论文专注于文本推理，虽然摘要末尾提到该方法“也带来多模态性能提升”，但这只是一个附加效果，并非论文的核心研究焦点。 *   **特定应用领域**: 论文以韩语作为案例研究，但“语言”本身是LLM的一项基础能力，而不是像医疗、化学、金融那样的特定垂直领域。研究的目标是提升模型在特定语言下的通用推理能力，而非解决韩语领域的某个具体应用问题。 *   **模型可靠性**: 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需进行额外判断。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种新的方法论（Language-Mixed CoT）来增强大语言模型在多语言环境下的通用推理能力。它直接回应了你的核心目标——“致力于提高大语言模型（LLM）本身的『通用推理能力』”，并且与你的筛选标准高度契合。因此，最终判断为符合。"
    },
    {
        "index": "#76",
        "title": "Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions",
        "link": "/arxiv/2510.03999",
        "arxiv_id": "2510.03999",
        "authors": "Yang Xu, Xuanming Zhang, Min-Hsuan Yeh, Jwala Dhamala, Ousmane Dia, Rahul Gupta, Yixuan Li",
        "summary": "Deception is a pervasive feature of human communication and an emerging concern in large language models (LLMs). While recent studies document instances of LLM deception under pressure, most evaluations remain confined to single-turn prompts and fail to capture the long-horizon interactions in which deceptive strategies typically unfold. We introduce the first simulation framework for probing and evaluating deception in LLMs under extended sequences of interdependent tasks and dynamic contextual pressures. Our framework instantiates a multi-agent system: a performer agent tasked with completing tasks and a supervisor agent that evaluates progress, provides feedback, and maintains evolving states of trust. An independent deception auditor then reviews full trajectories to identify when and how deception occurs. We conduct extensive experiments across 11 frontier models, spanning both closed- and open-source systems, and find that deception is model-dependent, increases with event pressure, and consistently erodes supervisor trust. Qualitative analyses further reveal distinct strategies of concealment, equivocation, and falsification. Our findings establish deception as an emergent risk in long-horizon interactions and provide a foundation for evaluating future LLMs in real-world, trust-sensitive contexts.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.634869",
        "filter_reason": "这篇论文符合我的研究范围，判断过程如下： 1.  **第一步：核心判断** 论文的核心并非将LLM视为工具去解决某个外部领域的具体问题，而是聚焦于LLM本身在复杂交互中涌现出的一种高级认知行为——欺骗。它提出了一种新的模拟框架（多智能体系统）来探测、评估并理解这种行为。虽然它没有直接提出一种“训练方法”来提升推理能力，但它深入研究了与通用推理能力紧密相关的“战略规划”和“心智理论”（Theory of Mind）的体现。理解欺骗行为如何产生、演变，是提升模型通用可靠性和高级推理能力（如规划、策略）的基础性研究。因此，其本质是探究LLM的内在能力边界和特性，符合“改进LLM基础能力”的宏观范围。 2.  **第二步：正面指标** - **核心概念**: 论文明确以 \"large language models (LLMs)\" 为研究对象。 - **能力方向**: 论文研究的 \"deceptive behaviors\" 本质上是一种高级的 **problem-solving** 和 **planning** 行为。它发生在 \"long-horizon interactions\" 和 \"extended sequences of interdependent tasks\" 中，这些场景都强烈依赖于多步推理和长远规划能力。 - **新兴范式**: 论文的核心贡献是一个 \"multi-agent system\" 框架，这完全符合筛选标准中的 \"llm-based agents\" 和 \"multi-agent systems\" 范畴。该框架被设计用来通用地探查模型行为，而非绑定特定应用。 3.  **第三步：排除标准** 论文不涉及多模态、视觉、化学、医疗等特定应用领域。虽然它触及了“欺骗”，这可以被视为一种可靠性和安全问题，但它并非像“水印”那样的应用层安全技术，而是对模型内在认知过程的深层次探究，因此不应仅因此被排除。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的执行者-监督者-审计员多智能体框架是一个**通用的研究方法论框架**，用于在模拟的、长时程的交互环境中探测LLM的行为。它不是“用于化学实验的智能体”，而是用于理解智能体本身通用能力的工具，因此完全符合保留条件。 - **安全**: 论文研究欺骗，其最终目标是“provide a foundation for evaluating future LLMs in real-world, trust-sensitive contexts”。这属于提升模型内在可靠性和可信度的研究，旨在提升模型在复杂推理任务中的表现质量，属于基础性研究，而非应用层面的安全防护，因此应该保留。 **最终决策**: 综合来看，这篇论文的核心贡献在于提出了一种新颖的、基于多智能体系统的评估框架，用于研究和理解LLM在需要复杂规划和多步推理的长时程任务中表现出的欺骗行为。这项工作虽然不是直接提出一种训练方法来“提升”推理分数，但它为我们理解LLM通用推理能力的复杂性和局限性提供了新的视角和工具，是推动该领域向前发展不可或缺的一环。它精准地聚焦于LLM自身的内在能力，符合筛选“致力于提高大语言模型（LLM）本身的『通用推理能力』”论文的初衷。因此，判断为符合。"
    },
    {
        "index": "#81",
        "title": "Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models",
        "link": "/arxiv/2510.03805",
        "arxiv_id": "2510.03805",
        "authors": "Canhui Wu, Qiong Cao, Chang Li, Zhenfang Wang, Chao Xue, Yuwei Fan, Wei Xi, Xiaodong He",
        "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks but often suffer from excessive verbosity, known as \"overthinking.\" Existing solutions via reinforcement learning (RL) typically penalize generated tokens to promote conciseness. However, these methods encounter two challenges: responses with fewer tokens do not always correspond to fewer reasoning steps, and models may develop hacking behavior in later stages of training by discarding reasoning steps to minimize token usage. In this work, we introduce \\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more efficient reasoning by favoring compact reasoning steps. Our step-aware reward function prioritizes correctness while imposing penalties for redundant steps, and withholds rewards for incorrect responses to prevent the reinforcement of erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when the length of any output step exceeds the upper limit, we halt updates to prevent hacking behavior caused by merging steps. Extensive experiments across four reasoning benchmarks demonstrate that SP achieves state-of-the-art accuracy while significantly reducing response length. For instance, on AIME24, SP reduces token usage by \\textbf{69.7\\%}.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.647521",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **核心判断（第一步）：** 论文的核心是提出一种名为“Step Pruner (SP)”的新型强化学习（RL）训练框架，其目标是解决大型推理模型（LRM）在复杂任务中出现的“过度思考”问题。这并非将LLM应用于特定领域，而是直接针对LLM在执行通用推理任务时的内在缺陷（推理步骤冗余、效率低下）提出改进方案。这完全符合你筛选标准中的“改进LLM的基础能力”、“提出新的训练范式”以及“增强其...多步推理等通用能力”。论文的本质是优化模型自身的推理过程，而非将其作为工具使用。 2.  **正面指标（第二步）：** 论文内容与你的正面指标高度吻合。 *   **核心概念:** 论文研究对象是 \"Large Reasoning Models (LRMs)\"，属于大语言模型范畴。 *   **能力方向:** 论文标题和摘要反复强调核心是 \"Reasoning\"，目标是实现 \"Efficient and Accurate Reasoning\"。 *   **训练方法:** 论文明确提出了一种新的 \"reinforcement learning (RL) framework\"，并设计了新的奖励函数，这直接命中了你的筛选标准。 3.  **排除标准（第三步）：** 论文的研究焦点完全避开了你的所有排除标准。它不涉及多模态、视觉，不针对任何特定应用领域（如医疗、化学），也不是关于模型部署、水印或安全等应用层面的可靠性研究。 4.  **特殊和模糊情况（第四步）：** 论文中提到的防止“hacking behavior”（模型通过合并步骤来规避token惩罚）是一个关键点。这属于提升模型内在推理质量和可靠性的方法，旨在确保强化学习过程不会“教会”模型走捷径、跳过必要的推理步骤。这符合你保留标准中“提升模型的通用可靠性和推理质量”的情况，是一种方法论层面的创新，而非应用层面的讨论。 **最终决策（第五步）：** 综合以上分析，该论文通过提出一种创新的强化学习方法，直接致力于解决LLM在通用推理过程中的一个核心痛点——推理过程的冗长和低效。其核心贡献是增强模型本身的推理效率和质量，这与你的核心目标“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”高度一致。因此，这篇论文应该被**保留**。"
    },
    {
        "index": "#91",
        "title": "Can an LLM Induce a Graph? Investigating Memory Drift and Context Length",
        "link": "/arxiv/2510.03611",
        "arxiv_id": "2510.03611",
        "authors": "Raquib Bin Yousuf, Aadyant Khatri, Shengzhe Xu, Mandar Sharma, Naren Ramakrishnan",
        "summary": "Recently proposed evaluation benchmarks aim to characterize the effective context length and the forgetting tendencies of large language models (LLMs). However, these benchmarks often rely on simplistic 'needle in a haystack' retrieval or continuation tasks that may not accurately reflect the performance of these models in information-dense scenarios. Thus, rather than simple next token prediction, we argue for evaluating these models on more complex reasoning tasks that requires them to induce structured relational knowledge from the text - such as graphs from potentially noisy natural language content. While the input text can be viewed as generated in terms of a graph, its structure is not made explicit and connections must be induced from distributed textual cues, separated by long contexts and interspersed with irrelevant information. Our findings reveal that LLMs begin to exhibit memory drift and contextual forgetting at much shorter effective lengths when tasked with this form of relational reasoning, compared to what existing benchmarks suggest. With these findings, we offer recommendations for the optimal use of popular LLMs for complex reasoning tasks. We further show that even models specialized for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in these settings. These results point to significant limitations in the models' ability to abstract structured knowledge from unstructured input and highlight the need for architectural adaptations to improve long-range reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.657601",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 这篇论文的本质是**深入探究和评估大语言模型在通用推理能力上的一个核心缺陷**。它没有提出一个全新的训练范式来直接“提升”能力，但它做了一件同样重要且更具基础性的工作：**设计了更精确的“标尺”来衡量LLM的通用推理能力**。其核心贡献是提出了一种新的、更复杂的评估任务（从文本中归纳图结构），用以测试模型的“关系推理”和“长程推理”能力。这直接服务于“提高LLM通用推理能力”这一宏大目标，因为精确诊断问题是解决问题的第一步。论文并非将LLM应用于特定领域，而是聚焦于模型本身的基础认知能力。 2.  **第二步：正面指标** 论文高度匹配多个正面指标： *   **核心概念**: 明确以\"Large language models (LLMs)\"为研究对象。 *   **能力方向**: 核心讨论的就是\"reasoning\"，具体化为\"complex reasoning tasks\"、\"relational reasoning\"和\"long-range reasoning\"，这正是你关注的核心能力。 *   **新兴范式**: 论文中提到的\"induce structured relational knowledge\"（归纳结构化关系知识）可以被视为一种高级的推理形式，与\"deep research\"的理念相通。 3.  **第三步：排除标准** 该论文完全避开了所有排除标准： *   它不涉及多模态、视觉。 *   它的研究方法是通用的，不针对医疗、化学等任何特定应用领域。 *   它讨论的\"memory drift\"（记忆漂移）是模型内在的认知局限性，属于推理能力的范畴，而非应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文的研究内容可以看作是对“幻觉”或“可靠性”问题的一种深层次探究。它不是泛泛而谈，而是将“遗忘”和“推理错误”具体化为在长上下文中归纳关系图时的“记忆漂移”。这种对模型内在推理缺陷的精确分析和量化，完全符合“提升模型内在可靠性，从而提升推理质量”的保留原则。 5.  **第五步：最终决策** 综合来看，这篇论文是一篇高质量的前沿研究。它虽然没有提出一个“解决方案”，但它**精准地定义和量化了“通用推理能力”中的一个关键瓶颈**（长程关系推理中的记忆漂移）。对于任何致力于提升LLM推理能力的研究者来说，这篇论文提供了至关重要的洞见和评估基准，指明了未来模型架构改进的方向。因此，它绝对是你研究课题中不可或缺的参考文献。"
    },
    {
        "index": "#92",
        "title": "Decoupling Task-Solving and Output Formatting in LLM Generation",
        "link": "/arxiv/2510.03595",
        "arxiv_id": "2510.03595",
        "authors": "Haikang Deng, Po-Nien Kung, Nanyun Peng",
        "summary": "Large language models (LLMs) are increasingly adept at following instructions containing task descriptions to solve complex problems, such as mathematical reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow more complex, models often struggle to adhere to all instructions. This difficulty is especially common when instructive prompts intertwine reasoning directives -- specifying what the model should solve -- with rigid formatting requirements that dictate how the solution must be presented. The entanglement creates competing goals for the model, suggesting that more explicit separation of these two aspects could lead to improved performance. To this front, we introduce Deco-G, a decoding framework that explicitly decouples format adherence from task solving. Deco-G handles format compliance with a separate tractable probabilistic model (TPM), while prompts LLMs with only task instructions. At each decoding step, Deco-G combines next token probabilities from the LLM with the TPM calculated format compliance likelihood to form the output probability. To make this approach both practical and scalable for modern instruction-tuned LLMs, we introduce three key innovations: instruction-aware distillation, a flexible trie-building algorithm, and HMM state pruning for computational efficiency. We demonstrate the effectiveness of Deco-G across a wide range of tasks with diverse format requirements, including mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall, our approach yields 1.0% to 6.0% relative gain over regular prompting practice with guaranteed format compliance.",
        "subjects": "Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.658058",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心贡献是提出了一种名为 **Deco-G** 的解码框架。这个框架的本质并非将LLM应用于某个特定领域，而是解决LLM在生成过程中遇到的一个普遍性问题：当任务指令（如数学推理）与格式指令（如输出JSON）混合时，模型会产生内部冲突，导致性能下降。Deco-G通过将格式遵循的任务从一个独立的概率模型（TPM）中剥离出来，让LLM可以“心无旁骛”地专注于核心的任务求解（即推理过程）。这是一种对LLM生成机制的底层优化，旨在释放其固有的推理潜力，因此完全符合“改进LLM的基础能力”这一核心要求。 2.  **第二步：正面指标——论文高度相关。** -   **核心概念**: 论文明确以 \"Large language models (LLMs)\" 为研究对象。 -   **能力方向**: 论文的核心动机和验证任务都直接指向 **推理**。摘要中明确提到 \"mathematical reasoning\" 和 \"task solving\"，其方法旨在提升模型在这些需要深度思考和规划的通用任务上的表现。 -   **新兴范式**: 虽然不是智能体或强化学习，但它提出了一种新颖的生成框架，这属于LLM方法论研究的范畴，与CoT等改进推理能力的研究处于同一层级。 3.  **第三步：排除标准——论文不涉及排除领域。** -   论文的研究内容是纯文本的，不涉及任何**多模态与视觉**内容。 -   虽然论文在实验中提到了 \"event argument extraction\"（事件论元提取），这看起来像一个特定领域任务，但它仅仅是作为验证框架通用性的一个例子。论文的核心贡献是 **Deco-G框架本身**，而不是提取方法。其主要验证场景是“数学推理”和“LLM-as-a-judge”，这些都属于通用推理能力的范畴。因此，论文的焦点并非特定应用领域。 -   论文不涉及**模型可靠性（应用层面）**，如水印、安全等。 4.  **第四步：处理特殊和模糊情况。** 本论文不涉及特殊或模糊情况。它清晰地提出了一种通用的、技术性的方法论，其目标是通过优化生成过程来直接提升模型的内在能力。 5.  **第五步：最终决策。** 综合以上分析，这篇论文提出了一种创新的解码框架，通过解耦任务求解和输出格式，有效地减轻了LLM在复杂指令下的认知负担，使其能够更专注于推理本身。这项工作直接服务于“提升大语言模型通用推理能力”的核心目标，是一种方法论层面的创新，而非特定领域的应用。因此，这篇论文是您研究课题下的高质量前沿文献，应予以保留。"
    },
    {
        "index": "#99",
        "title": "Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs",
        "link": "/arxiv/2510.03527",
        "arxiv_id": "2510.03527",
        "authors": "Sayan Ghosh, Shahzaib Saqib Warraich, Dhruv Tarsadiya, Gregory Yauney, Swabha Swayamdipta",
        "summary": "Language models can be sampled multiple times to access the distribution underlying their responses, but existing methods cannot efficiently synthesize rich epistemic signals across different long-form responses. We introduce Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents shared information, as well as semantic variation in a set of sampled LM responses to the same prompt. We construct ConGrs using a light-weight lexical sequence alignment algorithm from bioinformatics, supplemented by the targeted usage of a secondary LM judge. Further, we design task-dependent decoding methods to synthesize a single, final response from our ConGr data structure. Our experiments show that synthesizing responses from ConGrs improves factual precision on two biography generation tasks by up to 31% over an average response and reduces reliance on LM judges by more than 80% compared to other methods. We also use ConGrs for three refusal-based tasks requiring abstention on unanswerable queries and find that abstention rate is increased by up to 56%. We apply our approach to the MATH and AIME reasoning tasks and find an improvement over self-verification and majority vote baselines by up to 6 points of accuracy. We show that ConGrs provide a flexible method for capturing variation in LM responses and using the epistemic signals provided by response variation to synthesize more effective responses.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.666700",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“共识图”的通用方法论，用于综合大语言模型对同一问题的多次采样结果。这种方法并非将LLM应用于某个特定领域（如医疗、化学），而是提出一种新的后处理/解码技术，旨在从模型自身的输出分布中提炼出更高质量的回答。这属于“改进LLM的基础能力”的范畴，其目标是通过优化输出合成机制来提升模型表现。 2.  **第二步：正面指标** 论文明确包含了多个关键正面指标： *   **核心概念**: 论文的研究对象是Language Models（LLMs）。 *   **能力方向**: 论文在摘要中明确提到，他们的方法在 **`MATH and AIME reasoning tasks`** 上取得了效果，直接关联到核心目标“推理能力”。 *   **问题解决**: 该方法的本质是通过综合多个响应来解决一个问题（`synthesize a single, final response`）。 3.  **第三步：排除标准** 论文完全不涉及多模态、视觉、特定应用领域（如生物、金融），也未聚焦于模型基础设施或部署优化。虽然提到了“factual precision”和“abstention”，但这属于提升模型内在可靠性的范畴，而非应用层面的水印或安全研究。因此，论文没有被任何排除标准命中。 4.  **第四步：处理特殊和模糊情况** 论文可以被视为一种提升模型内在可靠性的方法。通过构建共识图，该方法识别了多个响应中的共享信息（共识）和变化，这本质上是一种降低随机性、减少事实错误（即幻觉）的机制。根据筛选标准，“如果论文提出一种新方法来减少幻觉……从而提升模型的通用可靠性和推理质量，应该保留”。本文正是如此，它通过提升输出的可靠性和事实精确性，直接促进了在推理任务上的表现。 5.  **第五步：最终决策** 综合以上分析，这篇论文提出了一种名为`ConGrs`的通用响应合成框架。其核心价值在于，它是一种与具体领域无关的通用方法论，能够通过分析并综合LLM的多次输出，来提炼出更精确、更可靠的答案。最关键的是，论文明确验证了该方法在数学推理任务（MATH, AIME）上的有效性，并报告了显著的性能提升。这直接响应了我的研究目标——筛选那些致力于提高大语言模型“通用推理能力”的论文。因此，这篇论文高度相关，应被保留。"
    },
    {
        "index": "#98",
        "title": "Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance",
        "link": "/arxiv/2510.03528",
        "arxiv_id": "2510.03528",
        "authors": "Ahmed Alajrami, Xingwei Tan, Nikolaos Aletras",
        "summary": "Instruction-tuning plays a vital role in enhancing the task-solving abilities of large language models (LLMs), improving their usability in generating helpful responses on various tasks. However, previous work has demonstrated that they are sensitive to minor variations in instruction phrasing. In this paper, we explore whether introducing perturbations in instruction-tuning data can enhance LLMs' resistance against noisy instructions. We focus on how instruction-tuning with perturbations, such as removing stop words or shuffling words, affects LLMs' performance on the original and perturbed versions of widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics and potential shifts in model behavior. Surprisingly, our results suggest that instruction-tuning on perturbed instructions can, in some cases, improve downstream performance. These findings highlight the importance of including perturbed instructions in instruction-tuning, which can make LLMs more resilient to noisy user inputs.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.660868",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心是提出一种**新的训练范式**——在包含噪声（如词序打乱、移除停用词）的指令数据上进行微调。 - **与目标关联**: 这种方法旨在提升LLM的一项**基础能力**——对指令措辞变化的鲁棒性。一个能够抵抗噪声输入的模型，是进行稳定、可靠的通用推理的前提。如果模型连指令都无法稳健理解，那么其逻辑、数学、规划等高级推理能力也无从谈起。因此，这项工作直接致力于改进LLM的内在能力，而非将其作为工具应用于特定领域。 2.  **第二步：正面指标** - **核心概念**: 论文明确以\"Large language models (LLMs)\"为研究对象。 - **能力方向**: 论文评估了模型在GSM8K（数学推理基准）和BBH（包含多项复杂推理任务）上的性能，这直接关联到\"reasoning\"和\"problem-solving\"能力。提升对指令的鲁棒性，本质上是为了更好地释放和保障这些推理能力。 - **训练方法**: \"Fine-Tuning on Noisy Instructions\"本身就是一种新颖的训练方法，旨在优化模型的学习过程和泛化能力。 3.  **第三步：排除标准** - 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施（如部署、硬件加速）。因此，它没有触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用或幻觉/安全等特殊议题，因此无需进行特殊判断。 5.  **第五步：最终决策** - **核心贡献**: 论文的核心贡献是发现并验证了一种通过“噪声指令微调”来增强LLM鲁棒性的新方法。 - **为何符合**: 这种鲁棒性是LLM通用推理能力的重要组成部分。它确保了模型在面对不完美、不精确的用户输入时，依然能够准确理解意图并执行复杂的推理任务。这项工作不是解决某个具体领域的问题，而是从更基础的层面——**指令理解的稳定性**——来提升模型的通用性能，这与“提高大语言模型本身的通用推理能力”的核心目标高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#115",
        "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training",
        "link": "/arxiv/2510.04996",
        "arxiv_id": "2510.04996",
        "authors": "Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, Tong Zhang",
        "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.706602",
        "filter_reason": "根据第一步核心判断，这篇论文的本质完全符合研究目标。论文的核心贡献是提出了一种名为Reinforce-Ada的**自适应采样框架**，用于优化LLM的强化学习（RL）训练过程。该方法并非将LLM作为工具应用于特定领域，而是直接针对LLM训练中的技术瓶颈（固定采样导致的梯度不稳定）进行改进。其最终目的是“在多个推理基准上加速收敛并提升最终性能”，这直接对应了『提升LLM本身的通用推理能力』这一核心目标。 具体分析如下： 1.  **核心贡献符合要求**: 论文提出了一种新的训练范式（自适应采样框架），这是一种方法论层面的创新，旨在提升LLM在推理任务上的表现。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。 2.  **正面指标高度匹配**: 论文摘要中明确出现了多个关键正面指标： *   **核心概念**: \"large language models (LLMs)\" *   **能力方向**: \"reasoning tasks\", \"reasoning-capable LLMs\", \"reasoning benchmarks\" *   **训练方法**: \"Reinforcement learning\", \"online RL post-training\" 这些关键词表明论文的研究焦点与您的研究范围高度重合。 3.  **未触发排除标准**: 论文的研究内容不涉及多模态、任何特定应用领域（如医疗、化学等），也不关注模型的基础设施或部署优化。因此，它完全避开了所有排除标准。 4.  **特殊情况不适用**: 论文的核心是训练算法的优化，而非智能体框架或安全性研究，因此第四步的特殊情况判断不直接适用，但其研究动机（提升推理性能）与保留“提升推理质量”的论文是一致的。 综上所述，Reinforce-Ada是一篇典型的关于LLM训练方法论的研究，它通过改进强化学习的采样策略来提升模型的推理能力，完全符合您设定的筛选范围。"
    },
    {
        "index": "#108",
        "title": "Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision",
        "link": "/arxiv/2510.03323",
        "arxiv_id": "2510.03323",
        "authors": "Ge Chang, Jinbo Su, Jiacheng Liu, Pengfei Yang, Yuhao Shang, Huiwen Zheng, Hongli Ma, Yan Liang, Yuanchun Li, Yunxin Liu",
        "summary": "A significant portion of real-world data is inherently represented as textual graphs, and integrating these graphs into large language models (LLMs) is promising to enable complex graph-based question answering. However, a key challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e., how to retrieve relevant content from large graphs that is sufficiently informative while remaining compact for the LLM context. Existing retrievers suffer from poor performance since they either rely on shallow embedding similarity or employ interactive retrieving policies that demand excessive data labeling and training cost. To address these issues, we present Graph-$S^3$, an agentic textual graph reasoning framework that employs an LLM-based retriever trained with synthetic stepwise supervision. Instead of rewarding the agent based on the final answers, which may lead to sparse and unstable training signals, we propose to closely evaluate each step of the retriever based on offline-extracted golden subgraphs. Our main techniques include a data synthesis pipeline to extract the golden subgraphs for reward generation and a two-stage training scheme to learn the interactive graph exploration policy based on the synthesized rewards. Based on extensive experiments on three common datasets in comparison with seven strong baselines, our approach achieves an average improvement of 8.1\\% in accuracy and 9.7\\% in F$_1$ score. The advantage is even higher in more complicated multi-hop reasoning tasks. Our code will be open-sourced.",
        "subjects": "Computation and Language",
        "date": "2025-10-01",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.670963",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。 **第一步：核心判断** 这篇论文的本质是提出一种新的训练范式（合成逐步监督，Synthetic Stepwise Supervision），用于增强一个基于LLM的智能体在复杂任务上的推理能力。其核心问题“图检索”是LLM进行图上推理的关键瓶颈，而论文提出的解决方案——通过评估和奖励推理过程中的每一步，而非仅仅最终结果——是直接针对提升LLM多步推理质量和稳定性的方法论创新。这并非将LLM作为工具应用于特定领域，而是致力于改进LLM处理复杂信息结构时的内在推理机制。 **第二步：正面指标** 论文命中了多个关键的正面指标： - **核心概念**: 明确以\"Large language models (LLMs)\"为基础。 - **能力方向**: 核心研究内容是\"reasoning\"，特别是\"multi-hop reasoning\"，这是通用推理能力的典型体现。 - **训练方法**: 提出的方法与\"reinforcement learning (RL)\"思想高度相关，通过\"rewarding the agent\"和\"training signals\"来优化\"interactive retrieving policies\"，这是一种新颖的训练范式。 - **新兴范式**: 论文提出了一个\"agentic textual graph reasoning framework\"，属于\"llm-based agents\"的研究范畴。 **第三步：排除标准** 论文不涉及任何主要排除领域： - **多模态与视觉**: 研究对象是\"textual graphs\"（文本图），不涉及视觉信息。 - **特定应用领域**: 虽然图可以应用于任何领域（如生物、金融），但该论文提出的方法是领域无关的通用框架，旨在解决一类通用问题（图推理），而非解决某个特定领域的问题。 - **模型可靠性**: 论文不涉及水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的\"agentic\"框架是一种通用的智能体协作和探索方法，旨在增强LLM在图结构数据上的通用问题解决能力。它不是为特定领域（如化学、医疗）设计的智能体，因此符合保留标准。 **第五步：最终决策** 综合来看，这篇论文的核心贡献在于提出了一种创新的训练方法，通过细粒度的逐步监督来优化LLM智能体的多步推理过程。这直接触及了提升LLM“通用推理能力”的核心目标，是一项方法论层面的前沿研究，而非特定领域的应用。因此，它完全符合您的筛选要求。"
    },
    {
        "index": "#111",
        "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models",
        "link": "/arxiv/2510.05095",
        "arxiv_id": "2510.05095",
        "authors": "Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia",
        "summary": "Large reasoning models (LRMs) generate intermediate reasoning traces before producing final answers, yielding strong gains on multi-step and mathematical tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for model deployment, remains underexplored. The statistically correct objective for preference alignment requires marginalizing over reasoning traces, but this computation is intractable in practice. A common workaround optimizes a single sampled trajectory, which introduces substantial gradient variance from stochastic trace sampling. To address this challenge, we frame preference optimization for LRMs through the lens of the bias--variance trade-off and propose Bias--Variance Optimized Preference Optimization (BVPO), a simple, drop-in method that mixes two gradient estimators: a high-variance trace-based estimator and a low-variance empty-trace estimator obtained by disabling reasoning trace generation. Our theory shows that BVPO strictly reduces trace-induced variance for any nontrivial mixture, provides a closed-form choice of the mixing weight that minimizes mean-squared error relative to the true marginal gradient, and under standard smoothness and step-size conditions, tightens classical convergence bounds for stochastic gradient descent. Empirically, BVPO improves alignment over the best baseline by up to 7.8 points on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on general conversational data, BVPO also boosts reasoning performance for base models by up to 4.0 points on the average of six math reasoning benchmarks. These results identify variance from trace sampling as a key bottleneck and demonstrate that directly optimizing the bias--variance trade-off yields more stable training and stronger overall performance.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.703867",
        "filter_reason": "这篇论文完全符合您的筛选标准，其核心贡献直接指向提升大语言模型自身的通用推理能力。 1.  **核心判断 (第一步 - 保留):** 这篇论文的本质是提出一种新的训练范式（BVPO），用于解决大型推理模型在对齐过程中的关键技术难题。它并非将LLM应用于特定领域，而是专注于改进模型内部的学习过程，特别是如何更稳定、更有效地训练模型生成高质量的推理过程。这直接属于“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴。 2.  **正面指标 (第二步 - 高度匹配):** 论文摘要中明确包含了多个关键正面指标： *   **核心概念:** \"Large reasoning models (LRMs)\"，直接点明研究对象。 *   **能力方向:** \"reasoning traces\", \"multi-step and mathematical tasks\", \"reasoning performance\"，清晰地聚焦于推理能力。 *   **训练方法:** \"Preference Optimization\" 是一种先进的模型对齐与优化技术，属于强化学习相关的训练方法论范畴。 3.  **排除标准 (第三步 - 完全规避):** 该论文的研究内容完全不涉及多模态、特定应用领域（如医疗、化学等），也非关注模型基础设施或应用层面的水印、安全等问题。其所有实验均在通用对话对齐数据集和数学推理基准上完成，确保了其方法的通用性。 4.  **最终决策 (第五步):** 论文的核心贡献在于，它发现并解决了训练推理模型时的一个关键瓶颈（由推理轨迹采样引入的梯度方差），并提出了一个理论坚实、效果显著的新方法（BVPO）。最关键的证据是，论文明确指出，该方法在提升通用对齐能力的同时，**显著提升了模型在多个数学推理基准上的性能**。这直接证明了其工作与“提高大语言模型本身的通用推理能力”这一核心目标高度一致。因此，这是一篇高质量、高度相关的前沿论文，应当被保留。"
    },
    {
        "index": "#119",
        "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning",
        "link": "/arxiv/2510.04935",
        "arxiv_id": "2510.04935",
        "authors": "Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, Fei Huang",
        "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in simple tasks, where the models excessively utilize System 2-type, deliberate reasoning, leading to inefficient token generation. Furthermore, these models face challenges in adapting their reasoning capabilities to rapidly changing environments due to the static nature of their pretraining data. To address these issues, advancing Large Language Models (LLMs) for complex reasoning tasks requires innovative approaches that bridge intuitive and deliberate cognitive processes, akin to human cognition's dual-system dynamic. This paper introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless integration of System 1's fast, intuitive thinking with System 2's deliberate reasoning within LLMs. MARS strategically integrates multiple external tools, such as Google Search, Google Scholar, and Python Interpreter, to access up-to-date information and execute complex computations, while creating a specialized division of labor where System 1 efficiently processes and summarizes high-volume external information, providing distilled insights that expand System 2's reasoning context without overwhelming its capacity. Furthermore, we propose a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to simultaneously optimize both systems with multi-turn tool interactions, bin-packing optimization, and sample balancing strategies that enhance collaborative efficiency. Extensive experiments demonstrate MARS achieves substantial improvements of 3.86% on the challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.714217",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： **第一步：核心判断** 论文的核心是提出一个名为MARS（Multi-Agent System for Deep ReSearch）的新框架，旨在解决大型推理模型（LRMs）在推理过程中的两个核心问题：1）在简单任务上过度分析，推理效率低下；2）难以适应动态变化的信息环境。论文的本质是通过一种**新的方法论**——即结合System 1（直觉）和System 2（审慎推理）的双系统动态——来**增强LLM的通用推理能力**。这并不是将LLM作为工具应用于某个特定领域，而是直接对LLM的推理机制进行优化和改进。因此，它满足了核心判断中的“保留”条件。 **第二步：正面指标** 论文高度契合所有关键的正面指标： - **核心概念**: 直接研究\"Large Language Models (LLMs)\"和其变体\"Large Reasoning Models (LRMs)\"。 - **能力方向**: 核心目标是提升\"complex reasoning\"、\"deliberative reasoning\"能力，并在极具挑战性的\"Humanity's Last Exam (HLE)\"等推理基准上进行验证。 - **训练方法**: 提出了\"multi-agent reinforcement learning\"框架，这是对模型训练和优化范式的直接创新。 - **新兴范式**: 论文本身就是一个\"Multi-Agent System\"的范例，并集成了\"tool use\"（Google Search, Python Interpreter）来扩展LLM的能力边界。 **第三步：排除标准** 论文完全没有涉及任何排除标准领域： - 它不涉及多模态、视觉或机器人控制。 - 它的研究领域是通用的“深度研究”和“复杂推理”，而非医疗、化学等特定应用。 - 它不讨论水印、安全等模型可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的MARS框架是一个完美的“保留”案例。它不是将智能体用于某个特定领域（如化学实验自动化），而是构建了一个**通用的智能体协作框架**，通过分工和工具使用来增强LLM在面对复杂、知识密集型任务时的**通用问题解决能力**。这正是筛选标准中明确指出的应保留的情况。 **第五步：最终决策** 综合以上分析，论文《MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning》的核心贡献在于提出了一种新颖的、基于多智能体强化学习的训练和推理框架，以系统性地改进LLM的内在推理效率和质量。它致力于解决通用推理能力中的根本性问题（如效率与适应性），而非局限于特定应用。因此，这篇论文与你关于“大语言模型通用推理能力”的研究课题高度相关，应当被筛选出来。"
    },
    {
        "index": "#125",
        "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
        "link": "/arxiv/2510.04618",
        "arxiv_id": "2510.04618",
        "authors": "Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, Kunle Olukotun",
        "summary": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.717428",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是根据筛选标准进行的详细判断： 1.  **第一步：核心判断——论文的本质是什么？** - **核心贡献**: 本论文的核心贡献是提出了一种名为“Agentic Context Engineering (ACE)”的**通用框架**。这个框架并不改变模型权重，而是通过一个“生成、反思、策划”的模块化流程，**动态地、迭代地优化和进化LLM所使用的上下文**。这本质上是一种全新的、旨在提升模型性能的**优化范式和方法论**。 - **与核心目标的契合度**: 你的目标是“提高大语言模型（LLM）本身的『通用推理能力』”。ACE框架通过让上下文自我进化，积累和提炼策略，从而让同一个LLM在任务中表现得更好。这直接增强了模型在推理、规划和问题解决等方面的表现，完全符合改进LLM基础能力和通用能力的目标。它不是将LLM作为工具应用于特定领域，而是提出了一种让LLM自身变得更强的方法。 2.  **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文明确以“Large language model (LLM)”为核心。 - **能力方向**: 摘要中明确提到了“agents and domain-specific reasoning”，这直接关联到推理和问题解决能力。 - **训练方法**: 论文虽未使用传统的RLHF，但其“self-improving”的机制和“leveraging natural execution feedback”的设计，与“evolution”和“self-evolve”的理念高度一致。它通过任务执行的反馈来优化上下文，是一种自监督的进化过程。 - **新兴范式**: 论文主题直接关联“llm-based agents”，并探讨了如何优化“agent memory”，这是智能体研究中的核心问题。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** - **多模态与视觉**: 未涉及，符合。 - **特定应用领域**: 这是需要仔细辨析的一点。论文在评估部分提到了“finance”和“AppWorld”。**关键在于，这些是作为评估基准来证明ACE框架的通用性和有效性，而不是论文的研究焦点。** 论文的标题、摘要和核心贡献都围绕着一个**通用的上下文工程框架**，而不是一个“用于金融的LLM”。因此，它不属于被排除的“主要聚焦于特定应用领域”的论文。 - **模型可靠性（应用层面）**: 未涉及水印、安全等，符合。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的ACE框架，正是一种**通用的方法来增强智能体的通用问题解决能力**。它优化的是智能体的“记忆”和“策略”，使其能更好地处理任务，这完全符合“保留”的条件。它不是“用于化学实验的智能体”，而是“能让智能体在各类任务（包括化学实验）中表现更好的通用框架”。 **最终决策**: 综合以上分析，这篇论文提出了一种创新的、通用的方法论（ACE），通过动态优化上下文来实现LLM的自我改进。其核心目标是提升模型在推理、规划等通用任务上的表现，而非局限于某个特定领域。论文中提到的金融等领域的应用，仅作为验证其通用性的案例。因此，这篇论文与你的研究课题“大语言模型通用推理能力”高度相关，是一篇应该保留的前沿研究。"
    },
    {
        "index": "#126",
        "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning",
        "link": "/arxiv/2510.04573",
        "arxiv_id": "2510.04573",
        "authors": "Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamaki, Navdeep Jaitly, Yi-An Ma, Lianhui Qin",
        "summary": "Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may limit the ability to revisit and refine earlier tokens in a holistic manner, which can also lead to inefficient exploration for diverse solutions. In this paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning framework that unifies the expressiveness of continuous latent representation with the iterative refinement capabilities of latent diffusion models for an existing LLM. We first construct a structured latent reasoning space using a Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of thought tokens, preserving semantic information and interpretability while offering compact but expressive representations. Subsequently, we utilize a latent diffusion model that learns to denoise a block of latent thought tokens with a blockwise bidirectional attention mask, enabling longer horizon and iterative refinement with adaptive test-time compute. This design allows efficient parallel generation of diverse reasoning trajectories, allowing the model to plan and revise the reasoning process holistically. We conduct evaluations on a suite of mathematical reasoning and planning benchmarks. Empirical results show that LaDiR consistently improves accuracy, diversity, and interpretability over existing autoregressive, diffusion-based, and latent reasoning methods, revealing a new paradigm for text reasoning with latent diffusion.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.723195",
        "filter_reason": "这篇论文完全符合筛选要求，应予以保留。以下是根据您的筛选标准进行的详细判断： **第一步：核心判断** 该论文的本质是提出一种名为LaDiR的新颖推理框架，旨在从根本上改进大语言模型的推理过程。其核心贡献在于： 1.  **改进基础能力**: 论文直接针对LLM在推理时的自回归解码局限性，即无法有效回顾和修正已生成的步骤。 2.  **提出新范式**: 它结合了变分自编码器（VAE）和潜在扩散模型，创建了一种非自回归的、可迭代优化的推理范式。这超越了传统的思维链方法，属于方法论层面的创新。 3.  **增强通用能力**: 论文明确指出其目标是提升LLM在“数学推理”和“规划”等通用推理任务上的能力，这些都是通用推理能力的核心组成部分。 因此，这篇论文的核心是提升LLM的内在通用推理能力，而非将其应用于特定领域，完全符合第一步的“保留”标准。 **第二步：正面指标** 该论文高度匹配多项正面指标： - **核心概念**: 标题和摘要多次提及 \"Large Language Models (LLMs)\"。 - **能力方向**: 摘要中明确出现了 \"reasoning\", \"mathematical reasoning\", \"planning\" 等关键词，直指研究核心。 - **训练方法**: 虽然不是强化学习，但其提出的“潜在扩散模型”进行“迭代优化”的方法，是一种新颖的模型训练和推理优化范式，符合寻找新方法论的精神。 - **新兴范式**: 论文将自己定位为“文本推理的新范式”，这正是您所关注的前沿方向。 **第三步：排除标准** 该论文的焦点完全避开了所有排除标准： - **多模态与视觉**: 论文专注于“文本推理”，未涉及任何视觉或多模态内容。 - **特定应用领域**: 评估基准是通用的“数学推理和规划基准”，而非医疗、化学等特定领域。 - **模型可靠性（应用层面）**: 论文未讨论水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 此处不适用。 - **可解释性**: 论文提到其方法能增强“可解释性”，这是因为其“思维块”保留了语义信息。这是通过改进核心推理机制来实现的，属于提升模型内在质量的范畴，因此应予以保留。 **第五步：最终决策** 综合以上分析，LaDiR这篇论文致力于通过引入一种全新的、基于潜在扩散的迭代式推理范式，来从根本上提升LLM在数学和规划等通用推理任务上的表现。这完全契合您所寻找的“致力于提高大语言模型（LLM）本身的通用推理能力”的前沿研究。因此，最终判断为符合要求。"
    },
    {
        "index": "#143",
        "title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning",
        "link": "/arxiv/2510.04072",
        "arxiv_id": "2510.04072",
        "authors": "Ziyan Wang, Zheng Wang, Jie Fu, Xingwei Qu, Qi Cheng, Shengpu Tang, Minjia Zhang, Xiaoming Huo",
        "summary": "Reinforcement learning (RL) has become central to enhancing reasoning in large language models (LLMs). Yet on-policy algorithms such as Group Relative Policy Optimization (GRPO) often suffer in early training: noisy gradients from low-quality rollouts lead to unstable updates and inefficient exploration. We introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient framework to address these limitations via decomposing each step into three stages: a short fast trajectory of inner steps on the same batch, a reposition mechanism to control off-policy drift, and a final slow correction. This reposition-before-update design preserves the objective and rollout process unchanged, making SFPO plug-compatible with existing policy-gradient pipelines. Extensive experiments demonstrate that SFPO consistently improves stability, reduces rollouts, and accelerates convergence of reasoning RL training. Specifically, it outperforms GRPO by up to 2.80 points in average on math reasoning benchmarks. It also achieves up to 4.93\\texttimes{} fewer rollouts and a 4.19\\texttimes{} reduction in wall-clock time to match GRPO's best accuracy.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.763826",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献直接指向提升大语言模型的通用推理能力。判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为“Slow-Fast Policy Optimization (SFPO)”的新训练框架。其本质是**改进LLM的基础训练范式**，旨在解决强化学习（RL）在训练LLM推理能力时遇到的“不稳定更新”和“低效探索”等根本性问题。这完全符合筛选标准中“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。它不是将LLM作为工具应用于特定领域，而是专注于提升LLM本身。 2.  **第二步：正面指标** 论文包含了多个关键的正面指标： *   **核心概念**: 明确以“large language models (LLMs)”为研究对象。 *   **能力方向**: 论文的标题和摘要都聚焦于“LLM Reasoning”，并在实验中使用了“math reasoning benchmarks”来验证效果。 *   **训练方法**: 论文的核心是一种新的“Reinforcement learning (RL)”策略优化方法，属于强化学习优化的范畴。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准： *   它不涉及多模态、视觉或机器人控制。 *   它的应用场景是通用的数学推理基准，而非医疗、化学等特定领域。 *   它关注的是训练过程的稳定性和效率，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本文情况非常清晰，不涉及智能体/工具使用的领域限定问题，也不涉及幻觉/安全等模糊议题。它是一个纯粹的、关于如何通过改进RL算法来提升LLM推理能力的方法论研究。 **最终决策**: 这篇论文的核心贡献是提出了一种新的、更稳定、更高效的强化学习优化方法（SFPO），专门用于提升大语言模型的推理能力。它直接解决了当前LLM推理训练中的一个关键技术挑战，属于提升模型“通用推理能力”的前沿方法论研究。因此，这篇论文与你的研究目标高度契合，应该被保留。"
    },
    {
        "index": "#141",
        "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs",
        "link": "/arxiv/2510.04140",
        "arxiv_id": "2510.04140",
        "authors": "Zishang Jiang, Jinyi Han, Tingyun Li, Xinyi Wang, Sihang Jiang, Jiaqing Liang, Zhaoqian Dai, Shuguang Ma, Fei Yu, Yanghua Xiao",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely adopted technique for enhancing the reasoning ability of Large Language Models (LLMs). However, the effectiveness of RLVR strongly depends on the capability of base models. This issue arises because it requires the model to have sufficient capability to perform high-quality exploration, which involves both effectiveness and diversity. Unfortunately, existing methods address this issue by imitating expert trajectories, which improve effectiveness but neglect diversity. To address this, we argue that the expert only needs to provide guidance only at critical decision points rather than the entire reasoning path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation for Token-level Optimization of Reasoning, a framework that provides expert guidance only at critical decision points to perform effective and diverse exploration in RLVR. Extensive experiments show that MENTOR enables models capture the essence of expert strategies rather than surface imitation, thereby performing high-quality exploration and achieving superior overall performance. Our code is available online.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.757565",
        "filter_reason": "这篇论文完全符合您的筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——论文本质是提升LLM基础能力。** 该论文的核心是提出一种名为MENTOR的新框架，用于优化LLM的强化学习（RL）训练过程。其直接目标是“enhancing the reasoning ability of Large Language Models (LLMs)”（增强大语言模型的推理能力）。这完全属于“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴。它并非将LLM作为工具应用于特定领域，而是聚焦于LLM本身的能力提升。 2.  **第二步：正面指标——论文高度相关。** 论文命中了多个关键的正面指标： *   **核心概念**: 标题和摘要中明确提到了“Large Language Models, LLMs”。 *   **能力方向**: 论文的核心主题是“reasoning ability”（推理能力），并讨论了“reasoning path”（推理路径）和“critical decision points”（关键决策点），这些都直接关联到通用推理、规划和问题解决。 *   **训练方法**: 论文的核心方法论是“Reinforcement Learning of LLMs”（LLM的强化学习），这与您关注的强化学习优化方向完全吻合。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 该论文的研究内容非常纯粹，完全不涉及任何排除标准中提到的领域，如多模态、视觉、特定应用领域（医疗、化学等）或应用层面的模型可靠性（水印、安全等）。 4.  **第四步：处理特殊和模糊情况——不适用。** 该论文的研究焦点清晰，不属于需要特殊处理的模糊情况。 **最终决策**： 该论文致力于通过提出一种新的强化学习训练范式（MENTOR框架），来解决LLM在推理能力训练中“探索的有效性与多样性”这一核心挑战。这直接对准了您“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，这篇论文是您研究课题的 highly relevant 的前沿文献。"
    },
    {
        "index": "#144",
        "title": "What Scales in Cross-Entropy Scaling Law?",
        "link": "/arxiv/2510.04067",
        "arxiv_id": "2510.04067",
        "authors": "Junxi Yan, Zixi Wei, Jingtao Zhan, Qingyao Ai, Yiqun Liu",
        "summary": "The cross-entropy scaling law has long served as a key tool for guiding the development of large language models. It shows that cross-entropy loss decreases in a predictable power-law rate as the model size increases. However, recent evidence indicates that this law breaks down at very large scales: the loss decreases more slowly than expected, which causes significant trouble for developing large language models. In this paper, we hypothesize that the root cause lies in the fact that cross-entropy itself does not truly scale; instead, only one of its hidden components does. To investigate this, we introduce a novel decomposition of cross-entropy into three parts: Error-Entropy, Self-Alignment, and Confidence. We show both theoretically and empirically that this decomposition precisely captures the training dynamics and optimization objectives. Through extensive experiments on multiple datasets and 32 models spanning five orders of magnitude in size, we find that only error-entropy follows a robust power-law scaling, while the other two terms remain largely invariant. Moreover, error-entropy constitutes the dominant share of cross-entropy in small models but diminishes in proportion as models grow larger. This explains why the cross-entropy scaling law appears accurate at small scales but fails at very large ones. Our findings establish the error-entropy scaling law as a more accurate description of model behavior. We believe it will have wide applications in the training, understanding, and future development of large language models.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.764369",
        "filter_reason": "这篇论文的核心贡献是关于大语言模型（LLM）训练和缩放的基础性理论研究，而非将其应用于特定领域。以下是我的详细判断过程： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的本质是对LLM训练中一个核心现象——交叉熵缩放律——进行深入的理论分析和解构。它提出了一种新的损失函数分解方法（误差熵、自对齐、置信度），并发现只有“误差熵”这一项真正遵循幂律缩放。 - **是否符合要求**: 这篇论文的核心目标不是解决某个具体的应用问题，而是为了“更准确地描述模型行为”，并“指导大型语言模型的开发”。这种对模型训练动态和缩放规律的深刻理解，是提升模型所有基础能力（包括推理能力）的根本前提。因此，它属于改进LLM基础能力和训练范式的范畴，应该**保留**。 2.  **第二步：正面指标** - 论文摘要明确提到了核心概念 \"Large language models\"。 - 虽然没有直接提及 \"reasoning\" 或 \"planning\"，但其研究内容——理解模型如何随着规模增大而变得更好——是构建更强推理能力模型的基石。一个更准确的缩放律能帮助研究者更高效地训练出在推理等任务上表现更优的模型。 3.  **第三步：排除标准** - 论文完全不涉及多模态（视觉）。 - 论文不聚焦于任何特定应用领域（如医疗、化学等）。 - 论文不讨论模型在应用层面的可靠性问题（如水印、安全）。 4.  **第四步：处理特殊和模糊情况** - 这篇论文可以被视为对模型**内在可解释性**的一种探索。它将一个黑箱般的损失函数（交叉熵）分解为三个具有明确物理意义的部分，从而揭示了模型训练过程中的内在机制。根据筛选标准，“如果论文提出一种新方法来……增强模型内在的可解释性……从而提升模型的通用可靠性和推理质量，应该保留。” 这篇论文通过提供更精确的理论工具，间接有助于未来训练出更可靠、推理能力更强的模型，因此符合保留条件。 **最终决策**: 这篇论文虽然没有直接提出一种新的推理方法（如思维链），但它触及了LLM发展的一个更根本的问题：我们如何理解和预测模型规模扩大时的行为？通过修正和完善缩放律，它为未来如何更科学、更高效地构建更大、更强、推理能力更卓越的LLM提供了关键的理论指导。这种对底层规律的探索，完全符合“致力于提高大语言模型本身的『通用推理能力』”这一核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#146",
        "title": "Principled and Tractable RL for Reasoning with Diffusion Language Models",
        "link": "/arxiv/2510.04019",
        "arxiv_id": "2510.04019",
        "authors": "Anthony Zhan",
        "summary": "Diffusion large language models (dLLMs) are a new paradigm of non-autoregressive language models that are trained to predict multiple tokens in parallel and generate text via iterative unmasking. Recent works have successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B scale, but dLLMs have yet to benefit from modern post-training techniques, e.g. reinforcement learning (RL), that have proven effective for autoregressive models. Crucially, algorithms designed for traditional LLMs aren't directly compatible with diffusion frameworks due to inherent differences in modeling assumptions. Moreover, existing attempts at dLLM post-training with RL rely on heuristic-based objectives with no theoretical grounding. In this work, we present Amortized Group Relative Policy Optimization (AGRPO), a principled on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo sampling to compute an unbiased policy gradient estimate, making it the first tractable, faithful adaptation of policy gradient methods for dLLMs. We demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x performance on the Countdown task over the baseline LLaDA-8B-Instruct model and 1.3x performance gains over comparable RL methods such as diffu-GRPO. Furthermore, these gains persist across different numbers of sampling steps at inference time, achieving better tradeoffs between compute and performance. Our results demonstrate that online RL algorithms can be extended to diffusion LLMs in principled ways, maintaining both theoretical soundness and practical effectiveness.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.765366",
        "filter_reason": "这篇论文完全符合您的筛选标准，应被保留。以下是我的详细判断过程： **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出了一种名为“Amortized Group Relative Policy Optimization (AGRPO)”的新型强化学习（RL）算法。这个算法是专门为一种新兴的大语言模型范式——“扩散语言模型”所设计的。其目标是让dLLMs能够受益于RL这一后训练技术，从而提升模型能力。这完全属于“提出新的训练范式”和“改进LLM的基础能力”的范畴，而不是将LLM作为工具应用于特定领域。 **第二步：正面指标——论文是否包含以下主题？** 该论文与正面指标高度匹配： *   **核心概念**: 明确以“Diffusion large language models (dLLMs)”为研究对象。 *   **能力方向**: 论文的目标是提升模型的推理能力，并在摘要中明确提到其在“different math/reasoning tasks”上验证了效果，具体包括“GSM8K”（数学推理）和“Countdown”（推理/规划）任务。 *   **训练方法**: 整篇论文的核心就是关于“reinforcement learning (RL)”，并提出了一种全新的、有理论基础的RL算法（AGRPO）。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 该论文与所有排除标准均无关： *   **多模态与视觉**: 尽管标题中有“Diffusion”，但这里的“Diffusion”指的是文本生成过程中的迭代式去噪方法，而非图像或视频生成。论文的研究对象是纯粹的语言模型。 *   **特定应用领域**: 论文的评估基准是通用的数学和推理任务（GSM8K, Countdown），而非医疗、化学、法律等特定领域。 *   **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用，也不涉及幻觉/可解释性/安全等特殊模糊情况，因此该步骤不适用。 **第五步：最终决策** 综合以上分析，这篇论文的本质是为一种新型大语言模型（dLLMs）设计一种有原则、可扩展的强化学习训练方法，用以提升其通用推理能力（特别是数学推理）。这完全契合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#153",
        "title": "Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration",
        "link": "/arxiv/2510.03865",
        "arxiv_id": "2510.03865",
        "authors": "Wenhao Deng, Long Wei, Chenglei Yu, Tailin Wu",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently enhanced the reasoning capabilities of large language models (LLMs), particularly for mathematical problem solving. However, a fundamental limitation remains: as the sampling budget increases, the advantage of RLVR-trained models over their pretrained bases often diminishes or even vanishes, revealing a strong dependence on the base model's restricted search space. We attribute this phenomenon to the widespread use of the reverse Kullback-Leibler (KL) divergence regularizer, whose mode-seeking behavior keeps the policy trapped inside the base model's support region and hampers wider exploration. To address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an algorithm to promote broader yet focused exploration. Our method (i) utilizes the forward KL penalty to replace the reverse KL penalty for out-of-distribution exploration, and (ii) reweights the reference policy to facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B models with RAPO on the 8K SimpleRL-Zero dataset, without supervised fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO consistently improves problem-solving performance. Notably, RAPO enables models to surpass the base model's performance ceiling and solves previously intractable problems, advancing the frontier of RLVR for challenging reasoning tasks.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.774054",
        "filter_reason": "这篇论文完全符合你的研究范围。判断依据如下： 1.  **第一步：核心判断** 论文的核心是提出了一种名为RAPO（Rewards-Aware Policy Optimization）的新算法，这是一种改良的强化学习（RL）训练范式。其直接目标是解决现有RL方法（RLVR）在提升LLM推理能力时遇到的瓶颈问题——即模型难以超越其预训练基座的搜索空间。这本质上是在改进LLM的基础能力，即其内在的、通用的推理和问题解决能力，而非将LLM作为工具应用于某个特定领域。因此，根据核心判断标准，应予以保留。 2.  **第二步：正面指标** 论文高度匹配多个正面指标： *   **核心概念**: 明确以 \"Large language models (LLMs)\" 为研究对象。 *   **能力方向**: 核心主题是 \"reasoning capabilities\"，特别是在 \"mathematical problem solving\" 和 \"challenging reasoning tasks\" 上的表现。 *   **训练方法**: 论文的全部贡献都围绕一种新的强化学习训练方法（RAPO），这与 \"reinforcement learning (RL)\" 指标完全吻合。 3.  **第三步：排除标准** 该论文未触及任何排除标准。它不涉及多模态、视觉、特定垂直领域（如医疗、化学）的应用，也未讨论模型基础设施、水印或应用层面的安全等问题。 4.  **第四步：处理特殊和模糊情况** 一个潜在的模糊点是论文在数学问题上进行评测。然而，这里的数学（AIME数据集）是作为衡量和验证“通用推理能力”的**基准**，而不是论文要解决的**应用领域**。论文的动机和贡献是提出一种**通用的**训练算法来突破LLM的推理天花板，这种方法论本身是领域无关的。这与“用于化学实验自动化的智能体”有本质区别。 **最终决策**: 该论文致力于通过一种创新的训练范式（改良的强化学习算法）来突破LLM的内在推理瓶颈，提升其通用问题解决能力。它直接回应了“如何提高大语言模型本身通用推理能力”这一核心问题，提供了方法论层面的贡献，而非简单的应用。因此，这篇论文是高度相关且应被筛选入内的前沿研究。"
    },
    {
        "index": "#142",
        "title": "Internal states before wait modulate reasoning patterns",
        "link": "/arxiv/2510.04128",
        "arxiv_id": "2510.04128",
        "authors": "Dmitrii Troitskii, Koyena Pal, Chris Wendler, Callum Stuart McDougall, Neel Nanda",
        "summary": "Prior work has shown that a significant driver of performance in reasoning models is their ability to reason and self-correct. A distinctive marker in these reasoning traces is the token wait, which often signals reasoning behavior such as backtracking. Despite being such a complex behavior, little is understood of exactly why models do or do not decide to reason in this particular manner, which limits our understanding of what makes a reasoning model so effective. In this work, we address the question whether model's latents preceding wait tokens contain relevant information for modulating the subsequent reasoning process. We train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent attribution technique in the crosscoder setting. We locate a small set of features relevant for promoting/suppressing wait tokens' probabilities. Finally, through a targeted series of experiments analyzing max activating examples and causal interventions, we show that many of our identified features indeed are relevant for the reasoning process and give rise to different types of reasoning patterns such as restarting from the beginning, recalling prior knowledge, expressing uncertainty, and double-checking.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.758049",
        "filter_reason": "这篇论文非常符合你的研究范围。我的判断过程如下： 1.  **核心判断（第一步）：** 这篇论文的本质并非将LLM作为工具应用于特定领域，而是对LLM内部的推理机制进行深入的、基础性的分析。其核心贡献在于揭示了模型在执行推理任务（特别是自我修正）时，其内部潜在状态是如何影响和调节具体推理行为（如`wait`标记的出现）的。这种对模型内在工作原理的探索，直接服务于“提高LLM本身通用推理能力”这一核心目标，因为它为我们理解“什么构成了有效的推理”以及“如何可能去增强它”提供了关键洞见。因此，这篇论文应被**保留**。 2.  **正面指标（第二步）：** 论文高度符合多个正面指标。 *   **核心概念:** 论文明确研究的是DeepSeek-R1-Distill-Llama-8B，这是一个大语言模型。 *   **能力方向:** 论文的主题完全是关于`reasoning`（推理），并深入探讨了`self-correct`（自我修正）、`backtracking`（回溯）、`restarting`（重启）、`double-checking`（再次检查）等具体的推理模式。 *   **新兴范式:** 虽然没有直接提出新的智能体或工具使用框架，但它研究的`wait`标记是许多先进推理范式（如思维链）中的关键行为。理解其内在机制，对优化这些范式至关重要。 3.  **排除标准（第三步）：** 论文完全不涉及任何排除标准。它没有讨论多模态、特定应用领域（如医疗、化学），也没有关注模型部署或应用层面的水印、安全等问题。 4.  **处理特殊和模糊情况（第四步）：** 这篇论文是一个典型的、关于“可解释性”的特殊情况。它提出了一种新的“潜在归因技术”来分析模型内部，目的正是为了理解推理过程。根据你的标准：“如果论文提出一种新方法来……增强模型内在的可解释性……从而提升模型的通用可靠性和推理质量，应该保留。” 这篇论文完美符合这一描述。它不是停留在现象讨论，而是通过因果干预等实验手段，验证了这些内部特征与推理质量的直接关联，为未来提升推理能力提供了明确的方向。 **核心依据总结：** 这篇论文的核心贡献是**通过可解释性方法，揭示了大语言模型内部与通用推理能力（特别是自我修正）相关的潜在机制**。它没有提出新的训练算法，但它提供的深刻洞见是未来设计更优推理算法和训练范式的基础。这种对LLM内在推理过程的“解剖学”研究，正是推动“通用推理能力”发展的关键一环，完全符合你的筛选目标。"
    },
    {
        "index": "#160",
        "title": "Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning",
        "link": "/arxiv/2510.03669",
        "arxiv_id": "2510.03669",
        "authors": "Wenlong Deng, Yi Ren, Yushu Li, Boying Gong, Danica J. Sutherland, Xiaoxiao Li, Christos Thrampoulidis",
        "summary": "Reinforcement learning with verifiable rewards has significantly advanced the reasoning capabilities of large language models, yet how to explicitly steer training toward exploration or exploitation remains an open problem. We introduce Token Hidden Reward (THR), a token-level metric that quantifies each token's influence on the likelihood of correct responses under Group Relative Policy Optimization (GRPO). We find that training dynamics are dominated by a small subset of tokens with high absolute THR values. Most interestingly, tokens with positive THR strengthen confidence in correct outputs, thus favoring exploitation, while tokens with negative THR preserve probability mass for alternative outputs, enabling exploration. This insight suggests a natural intervention: a THR-guided reweighting algorithm that modulates GRPO's learning signals to explicitly bias training toward exploitation or exploration. We validate the efficacy of this algorithm on diverse math reasoning benchmarks. By amplifying tokens with positive THR value and weakening negative ones, our algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse strategy yields consistent gains in Pass@K accuracy, favoring exploration. We further demonstrate that our algorithm integrates seamlessly with other RL objectives such as GSPO and generalizes across architectures including Llama. These findings establish THR as a principled and fine-grained mechanism for dynamically controlling exploration and exploitation in RL-tuned LLMs, providing new tools for targeted fine-tuning in reasoning-intensive applications.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.777647",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是详细的判断过程： 1.  **第一步：核心判断——论文本质** 论文的核心贡献是提出了一种名为“Token Hidden Reward (THR)”的新方法论，用于在强化学习训练过程中，精细地调控大语言模型的探索与利用权衡。它不是将LLM应用于某个特定领域，而是直接作用于LLM的训练过程本身，旨在通过改进训练范式（一种新的RL优化方法）来提升模型的推理能力。这完全属于“改进LLM的基础能力”和“提出新的训练范式”的范畴。 2.  **第二步：正面指标——论文主题** 论文明确命中了多个关键正面指标： - **核心概念**: 直接研究 \"Large language models (LLMs)\"，并在 \"Llama\" 架构上进行了验证。 - **能力方向**: 核心目标是提升 \"reasoning capabilities\"，特别是在 \"math reasoning benchmarks\" 上进行了验证，这直接关联到通用推理能力。 - **训练方法**: 论文深入探讨了 \"Reinforcement Learning (RL)\"，并提出了对 \"Group Relative Policy Optimization (GRPO)\" 的改进，这是当前提升LLM推理能力的前沿训练方法。 3.  **第三步：排除标准——论文焦点** 论文完全没有触及任何排除标准： - 它不涉及多模态、视觉或任何特定应用领域（如医疗、法律等）。虽然它在数学推理任务上验证，但数学推理是衡量LLM通用逻辑和规划能力的核心基准，而非特定领域应用。 - 它也不关注模型基础设施、部署优化或应用层面的安全、水印等问题。 4.  **第四步：处理特殊和模糊情况** 本文情况清晰，不存在需要特殊处理的模糊地带。其提出的THR机制是一种通用的、与具体任务无关的优化技术，旨在从算法层面提升RL训练的效率和效果，从而增强LLM的内在推理质量。 5.  **第五步：最终决策** 综合来看，这篇论文的研究工作非常前沿且高度相关。它提出了一种创新的、token级别的机制来控制LLM在强化学习训练中的动态过程，这直接服务于“提升大语言模型通用推理能力”这一核心目标。该研究不仅深化了我们对RL训练LLM背后机制的理解，还提供了一种可操作的、能针对不同推理目标（如追求单一最优解或探索多种可能性）进行精准微调的工具。因此，这篇论文是理想的筛选对象。"
    },
    {
        "index": "#161",
        "title": "Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders",
        "link": "/arxiv/2510.03659",
        "arxiv_id": "2510.03659",
        "authors": "Xu Wang, Yan Hu, Benyou Wang, Difan Zou",
        "summary": "Sparse Autoencoders (SAEs) are widely used to steer large language models (LLMs), based on the assumption that their interpretable features naturally enable effective model behavior steering. Yet, a fundamental question remains unanswered: does higher interpretability indeed imply better steering utility? To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B, Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels, and evaluate their interpretability and steering utility based on SAEBench (arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis reveals only a relatively weak positive association (tau b approx 0.298), indicating that interpretability is an insufficient proxy for steering performance. We conjecture the interpretability utility gap may stem from the selection of SAE features, as not all of them are equally effective for steering. To further find features that truly steer the behavior of LLMs, we propose a novel selection criterion called Delta Token Confidence, which measures how much amplifying a feature changes the next token distribution. We show that our method improves the steering performance of three LLMs by 52.52 percent compared to the current best output score based criterion (arXiv:2503.34567). Strikingly, after selecting features with high Delta Token Confidence, the correlation between interpretability and utility vanishes (tau b approx 0), and can even become negative. This further highlights the divergence between interpretability and utility for the most effective steering features.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.778181",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是根据筛选标准进行的详细分析： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是**提出一种新的方法来改进和控制大语言模型（LLM）的行为**。其核心贡献是“Delta Token Confidence”这一新颖的特征选择标准，旨在更有效地“引导模型行为”。这属于改进LLM基础能力的方法论研究。它不是将LLM应用于特定领域，而是研究如何从内部更精准地控制和提升模型本身的表现，这与“改进LLM的基础能力”、“提出新的训练范式”高度一致。因此，根据第一步，应该**保留**。 **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文明确以“large language models (LLMs)”为研究对象。 - **能力方向**: 论文的核心是“steering utility”（引导实用性），这直接关系到模型的可控性和问题解决能力。虽然未直接使用“reasoning”一词，但能够精确引导模型改变其“next token distribution”（下一个词元分布），是实现更高级别推理和规划能力的基础。 - **训练方法**: 虽然不是传统意义上的RLHF，但论文提出的特征选择方法是一种优化模型内部机制、提升其表现力的新范式，与优化模型能力的目标一致。 - **新兴范式**: 论文研究的“Sparse Autoencoders (SAEs)”和“model steering”是当前理解并增强LLM内部工作机制的前沿探索，属于深度研究（deep research）的范畴。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全不涉及任何排除标准领域： - **多模态与视觉**: 未提及。 - **特定应用领域**: 未提及任何医疗、化学、机器人等具体应用场景。 - **模型可靠性（应用层面）**: 论文讨论的是技术层面的“interpretability”和“utility”，而非应用层面的水印、安全或社会影响。 **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: 这篇论文是处理该特殊情况的绝佳范例。它没有停留在对“可解释性”的哲学或社会学讨论上，而是**提出了一种新的技术方法**。论文的核心洞见是：单纯追求人类可理解的特征并不能最大化模型性能。它提出的“Delta Token Confidence”方法，正是为了**通过更优的内部特征选择来提升模型的“steering utility”**，这本质上是在提升模型的内在可控性和能力上限，从而间接提升其通用推理质量。这完全符合“提出一种新方法来增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留”的原则。 **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种基础性的方法论（Delta Token Confidence），用于从内部更有效地控制和引导LLM的行为。这项研究直接触及了如何提升LLM核心能力的关键问题，即如何理解和操纵其内部工作机制以获得更好的性能。它与你的核心目标——“致力于提高大语言模型（LLM）本身的『通用推理能力』”——高度契合，因为它为构建更强大、更可控、推理能力更强的LLM提供了新的技术路径。因此，最终判断为符合要求。"
    },
    {
        "index": "#168",
        "title": "Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning",
        "link": "/arxiv/2510.03394",
        "arxiv_id": "2510.03394",
        "authors": "Donghwan Rho",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training large language models (LLMs) with stronger reasoning abilities. It has also been applied to a variety of logic puzzles. In this work, we study the Korean word-chain game using RLVR. We show that rule-derived rewards can naturally conflict, and demonstrate through experiments that a curriculum-learning scheme mitigates these conflicts. Our findings motivate further studies of puzzle tasks in diverse languages.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.808528",
        "filter_reason": "这篇论文完全符合筛选标准，应予以保留。我的判断过程如下： **第一步：核心判断** - **核心贡献分析**：这篇论文的本质是研究如何改进一种用于训练大语言模型推理能力的方法——\"Reinforcement Learning with Verifiable Rewards\" (RLVR)。论文的核心贡献是发现并解决了在RLVR训练过程中可能出现的\"奖励冲突\"问题，并提出了一种\"课程学习\"方案来缓解这一问题。 - **是否符合目标**：论文的核心目标是增强LLM的\"stronger reasoning abilities\"（更强的推理能力）。它使用的\"Korean word-chain game\"（韩语接龙游戏）本质上是一个逻辑和规则遵循的谜题，是作为检验和提升模型推理能力的**测试平台**，而非一个特定的应用领域（如医疗或金融）。因此，论文的焦点是**方法论创新**，旨在提升LLM的**基础推理训练范式**，这与我筛选“提升LLM本身通用推理能力”的核心目标高度一致。 **第二步：正面指标** - **核心概念**：摘要明确提到了\"Large language models (LLMs)\"。 - **能力方向**：直接点明了研究目标是提升\"reasoning abilities\"，并将其应用于\"logic puzzles\"。 - **训练方法**：论文的核心是\"Reinforcement learning with verifiable rewards (RLVR)\"，属于\"reinforcement learning (RL)\"范畴，并引入了\"curriculum-learning scheme\"作为改进。 - 论文几乎命中了所有关键的正面指标，表明其与研究课题的相关性极高。 **第三步：排除标准** - **多模态与视觉**：不涉及。 - **特定应用领域**：不涉及。韩语接龙游戏是一个通用逻辑游戏，不属于化学、生物等特定专业领域。 - **模型可靠性（应用层面）**：不涉及水印、安全等问题。 论文完全没有触及任何明确的排除标准。 **第四步：处理特殊和模糊情况** - 本论文的情况不涉及智能体/工具使用或幻觉等特殊情况的模糊地带。它清晰地聚焦于训练算法层面的改进。 **第五步：最终决策** 综合以上分析，这篇论文虽然以一个具体的游戏为案例，但其研究动机和核心贡献是通用性的。它致力于解决在利用强化学习提升LLM推理能力时遇到的一个普遍性训练难题（奖励冲突），并提出了一种具有普适性的解决方案（课程学习）。这种对基础训练范式的改进，正是我所寻找的、能够直接提升LLM通用推理能力的前沿研究。因此，最终判断为符合要求。"
    },
    {
        "index": "#167",
        "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition",
        "link": "/arxiv/2510.03399",
        "arxiv_id": "2510.03399",
        "authors": "Xiaoyan Bai, Aryan Shrivastava, Ari Holtzman, Chenhao Tan",
        "summary": "Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.",
        "subjects": "Artificial Intelligence, Computation and Language, Computers and Society, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.807946",
        "filter_reason": "这篇论文符合筛选要求，应当保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心并非将LLM应用于特定领域，而是深入探究LLM的一种基础性、高阶认知能力——自我识别。自我识别是一种元认知能力，它与模型对自身状态、能力和产出的理解密切相关，是通用推理能力（尤其是高级推理和自我反思）的重要组成部分。论文的本质是对LLM基础能力的深度诊断和分析，而非应用。因此，这一步判断为“保留”。 2.  **第二步：正面指标** - **核心概念**: 论文明确研究10个当代大语言模型。 - **能力方向**: 论文的核心是分析模型在自我识别任务中的表现，并深入探讨了其背后的“reasoning”（推理过程）。研究发现模型的推理存在“hierarchical bias”（层级偏见），这直接触及了模型的逻辑判断和推理质量问题。 虽然论文不涉及强化学习或智能体框架，但它对LLM核心概念和其“推理”能力的聚焦非常明确。 3.  **第三步：排除标准** - 论文不涉及多模态、视觉或任何特定应用领域（如医疗、化学）。 - 论文提到了“safety”（安全性），但这是从元认知能力的角度探讨其安全意义（例如，一个能识别自己输出的模型在评估场景中更安全），而不是研究应用层的水印、安全对抗等技术。这不属于需要排除的“模型可靠性（应用层面）”范畴。 4.  **第四步：处理特殊和模糊情况** - 这篇论文的情况与“幻觉/可解释性/安全”的特殊情况高度相似。它虽然没有提出一种新的训练方法来直接“减少幻觉”，但它通过对自我识别这一具体任务的深入剖析，揭示了模型内在推理机制的一个根本性缺陷（层级偏见）。这种对“推理过程”本身的诊断性研究，为未来提升模型的通用推理质量、减少因认知偏差导致的错误，提供了关键的洞见和方向。这属于增强模型内在可解释性、从而提升推理质量的研究范畴，应该保留。 5.  **第五步：最终决策** 综合分析，这篇论文的核心贡献在于**系统性地评估并深刻揭示了LLM在自我识别这一元认知能力上的缺失及其背后的推理缺陷**。它没有提出一个新的“提高”方法，但它通过精准的“诊断”，清晰地指出了LLM通用推理能力中的一个重要短板（推理过程中的偏见）。对于“致力于提高大语言模型通用推理能力”这一研究目标而言，深刻理解问题的根源与寻找解决方案同等重要。因此，这篇论文为该领域的研究提供了宝贵的基础性见解，完全符合筛选标准。"
    },
    {
        "index": "#166",
        "title": "PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters",
        "link": "/arxiv/2510.03415",
        "arxiv_id": "2510.03415",
        "authors": "Aditya Thimmaiah, Jiyang Zhang, Jayanth Srinivasa, Junyi Jessy Li, Milos Gligoric",
        "summary": "As large language models (LLMs) excel at code reasoning, a natural question arises: can an LLM execute programs (i.e., act as an interpreter) purely based on a programming language's formal semantics? If so, it will enable rapid prototyping of new programming languages and language features. We study this question using the imperative language IMP (a subset of C), formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). We introduce three evaluation sets-Human-Written, LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by code-complexity metrics spanning the size, control-flow, and data-flow axes. Given a program and its semantics formalized with SOS/K-semantics, models are evaluated on three tasks ranging from coarse to fine: (1) final-state prediction, (2) semantic rule prediction, and (3) execution trace prediction. To distinguish pretraining memorization from semantic competence, we define two nonstandard semantics obtained through systematic mutations of the standard rules. Across strong code/reasoning LLMs, performance drops under nonstandard semantics despite high performance under the standard one. We further find that (i) there are patterns to different model failures, (ii) most reasoning models perform exceptionally well on coarse grained tasks involving reasoning about highly complex programs often containing nested loop depths beyond five, and surprisingly, (iii) providing formal semantics helps on simple programs but often hurts on more complex ones. Overall, the results show a promise that LLMs could serve as programming language interpreters, but points to the lack of their robust semantics understanding. We release the benchmark and the supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.",
        "subjects": "Programming Languages, Artificial Intelligence, Computation and Language, Software Engineering",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.807261",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是**对大语言模型基础推理能力的深度探查和评估**。它没有将LLM作为工具应用于某个外部领域，而是将LLM本身作为研究对象，研究其理解并执行形式化逻辑规则的能力。具体来说，论文通过构建一个关于编程语言形式语义的基准，来测试LLM能否像解释器一样，基于严格的逻辑规则（操作语义）进行多步推理，预测程序的执行结果、规则应用和执行轨迹。这直接触及了LLM的**通用符号推理、逻辑推理和多步推理能力**，完全符合你筛选标准中“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的核心目标。 2.  **第二步：正面指标分析** 论文包含了多个高相关性的正面指标： *   **核心概念**: 明确以“Large language models (LLMs)”为核心研究对象。 *   **能力方向**: 论文的主题是“code reasoning”，并且其设计的任务（最终状态预测、语义规则预测、执行轨迹预测）都是对**推理**能力的直接度量，尤其是逻辑和符号推理。 *   **新兴范式**: 虽然没有直接提出新范式，但其研究内容与“工具使用”和“问题解决”能力高度相关。将LLM视为编程语言解释器，可以看作是一种极端形式化的、基于规则的“工具使用”，用以评估其通用问题求解的潜力。 3.  **第三步：排除标准分析** 该论文完全避开了所有排除标准： *   它不涉及**多模态与视觉**。 *   它的研究领域是计算机科学的基础理论（编程语言语义学），而非**特定应用领域**（如医疗、化学）。研究这一点是为了理解LLM的基础能力，而不是为了快速原型化某个商业应用。 *   它讨论的是模型推理的**内在逻辑缺陷**（依赖记忆而非真正的语义理解），而非应用层面的**水印、安全**等问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文的情况非常清晰，不属于模糊范畴。它虽然提及了“快速原型化新编程语言”这一潜在应用，但这仅仅是引出研究问题的动机。论文的**全部核心贡献**都在于构建基准、设计实验、分析LLM在形式化推理任务上的表现和失败模式。这种对模型内在能力的深入剖析，正是研究“通用推理能力”所需要的前沿工作。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种全新的评估方法，用于**系统性地衡量和深入理解LLM的形式化逻辑和符号推理能力**。它不是在应用LLM，而是在解剖LLM的推理机制，发现了其在面对“非标准”逻辑规则时的脆弱性。这种对LLM推理能力边界的探索，对于整个领域如何去“提高大语言模型的通用推理能力”具有根本性的指导意义。因此，这篇论文是你研究课题中必须保留的前沿文献。"
    },
    {
        "index": "#178",
        "title": "General Exploratory Bonus for Optimistic Exploration in RLHF",
        "link": "/arxiv/2510.03269",
        "arxiv_id": "2510.03269",
        "authors": "Wendi Li, Changdae Oh, Yixuan Li",
        "summary": "Optimistic exploration is central to improving sample efficiency in reinforcement learning with human feedback, yet existing exploratory bonus methods to incentivize exploration often fail to realize optimism. We provide a theoretical analysis showing that current formulations, under KL or $\\alpha$-divergence regularization, unintentionally bias exploration toward high-probability regions of the reference model, thereby reinforcing conservative behavior instead of promoting discovery of uncertain regions. To address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle. GEB counteracts divergence-induced bias via reference-dependent reward regulation and unifies prior heuristic bonuses as special cases, while extending naturally across the full $\\alpha$-divergence family. Empirically, GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones. These results demonstrate that GEB offers both a principled and practical solution for optimistic exploration in RLHF.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.818899",
        "filter_reason": "这篇论文符合研究要求。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为“通用探索性奖励（GEB）”的新方法，用于改进强化学习与人类反馈（RLHF）中的探索机制。RLHF是当前训练和优化大语言模型（LLM）使其与人类偏好对齐的核心技术之一。改进RLHF的训练效率和质量，直接关系到提升LLM的基础能力，使其能更好地遵循指令、生成高质量内容，并为解决复杂问题（这需要推理能力）打下更坚实的基础。因此，这篇论文的本质是改进LLM的基础训练范式，而非将其应用于特定领域，符合第一步的保留标准。 2.  **第二步：正面指标** 论文明确包含了多个关键正面指标： *   **核心概念**: 摘要中提到了“large language model backbones”。 *   **训练方法**: 论文的核心贡献聚焦于“reinforcement learning with human feedback (RLHF)”，这是通过强化学习优化模型的关键方法。 *   **新兴范式**: 虽然没有直接提及智能体或工具使用，但RLHF本身是构建高级智能体的重要一环。论文中提到的“alignment tasks”（对齐任务）也属于LLM能力提升的核心议题。 3.  **第三步：排除标准** 论文的研究内容完全不涉及任何排除标准领域。它没有讨论多模态、视觉、特定应用领域（如医疗、化学），也没有关注水印、安全等应用层面的可靠性问题。其焦点纯粹集中在模型训练算法的理论和改进上。 4.  **第四步：处理特殊和模糊情况** 本论文不直接涉及智能体/工具使用或幻觉/可解释性/安全等特殊情况。它聚焦于更底层的训练机制——探索策略。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种新的理论框架（GEB），旨在解决RLHF中探索效率低下的问题。通过鼓励模型进行“乐观探索”，该方法有望帮助模型跳出保守行为，更有效地学习和发现新的、更优的策略。这种对基础训练范式的改进，是提升LLM通用能力（包括推理、规划和问题解决能力）的根本性工作。因此，它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    },
    {
        "index": "#177",
        "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
        "link": "/arxiv/2510.03279",
        "arxiv_id": "2510.03279",
        "authors": "Youjin Wang, Yangjingyi Chen, Jiahao Yan, Jiaxuan Lu, Xiao Sun",
        "summary": "With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T14:04:52.818417",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种新的架构框架 `MemMamba`，旨在解决现有状态空间模型（如Mamba）在长序列建模中的“长距离记忆衰减”问题。这属于对模型**基础能力的改进**。有效的长距离记忆是模型进行复杂推理（如理解长篇文档、进行多步规划、关联分散信息）的**先决条件**。如果一个模型无法记住长上下文的关键信息，它的通用推理能力必然会受到严重限制。因此，这篇论文的本质是增强LLM底层架构的核心能力，而非将其作为工具应用于特定领域。这完全符合“保留”标准。 2.  **第二步：正面指标** - **核心概念**: 论文虽然未直接使用\"LLM\"一词，但其讨论的State Space Model (Mamba)是当前LLM架构研究的前沿方向，旨在替代或补充Transformer，因此与LLM高度相关。 - **能力方向**: 论文直接解决了\"长距离记忆\"问题。虽然摘要未明确写出\"reasoning\"，但长距离记忆是数学推理、逻辑推理和规划等高级推理能力的基石。论文在Passkey Retrieval任务上的提升，直接证明了其增强模型在长上下文中检索和利用信息的能力，这是推理的第一步。 3.  **第三步：排除标准** - **多模态与视觉**: 论文完全未涉及视觉或多模态内容。 - **特定应用领域**: 论文提到了\"bioinformatics\"（生物信息学）作为长序列建模的一个应用场景，但这只是为了说明其方法的普适性。论文的核心贡献是架构本身，而非解决某个特定的生物信息学问题。其评测基准（PG19, Passkey Retrieval）都是通用的NLP任务，而非特定领域任务。 - **模型可靠性（应用层面）**: 论文未涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况。 5.  **第五步：最终决策** 综合来看，`MemMamba` 论文通过提出一种新的架构机制，显著提升了模型处理超长序列时的记忆保真度。这种对“记忆”这一基础能力的增强，是提升大语言模型通用推理能力的关键一环。它不是应用层面的研究，而是深入模型架构内部的根本性改进，完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#16",
        "title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents",
        "link": "/arxiv/2510.04695",
        "arxiv_id": "2510.04695",
        "authors": "Yiding Wang, Zhepei Wei, Xinyu Zhu, Yu Meng",
        "summary": "Enabling large language models (LLMs) to utilize search tools offers a promising path to overcoming fundamental limitations such as knowledge cutoffs and hallucinations. Recent work has explored reinforcement learning (RL) for training search-augmented agents that interleave reasoning and retrieval before answering. These approaches usually rely on outcome-based rewards (e.g., exact match), implicitly assuming that optimizing for final answers will also yield effective intermediate search behaviors. Our analysis challenges this assumption: we uncover multiple systematic deficiencies in search that arise under outcome-only training and ultimately degrade final answer quality, including failure to invoke tools, invalid queries, and redundant searches. To address these shortcomings, we introduce DeSA (Decoupling Search-and-Answering), a simple two-stage training framework that explicitly separates search optimization from answer generation. In Stage 1, agents are trained to improve search effectiveness with retrieval recall-based rewards. In Stage 2, outcome rewards are employed to optimize final answer generation. Across seven QA benchmarks, DeSA-trained agents consistently improve search behaviors, delivering substantially higher search recall and answer accuracy than outcome-only baselines. Notably, DeSA outperforms single-stage training approaches that simultaneously optimize recall and outcome rewards, underscoring the necessity of explicitly decoupling the two objectives.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.584013",
        "filter_reason": "这篇论文完全符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM的通用能力。** 论文的核心贡献是提出了一种名为“DeSA”的**新的训练框架**。这个框架并非将LLM应用于某个特定领域，而是旨在解决LLM智能体在使用搜索工具进行推理时存在的根本性缺陷。它通过“解耦搜索和回答”这两个阶段，优化了智能体的**中间推理过程**（即搜索行为），而不仅仅是最终答案。这直接属于“改进LLM的基础能力”和“提出新的训练范式”，因此应予以保留。 2.  **第二步：正面指标——论文高度匹配多个关键主题。** - **核心概念**: 论文明确以“Large language models (LLMs)”为研究对象。 - **能力方向**: 论文的核心是提升LLM智能体的“problem-solving”和“reasoning”能力，尤其是在需要信息检索的复杂问答场景中。 - **训练方法**: 论文深入探讨了“reinforcement learning (RL)”，并在此基础上提出了改进的两阶段训练方法。 - **新兴范式**: 论文的研究焦点是“llm-based agents”和“tool use”，这正是当前提升LLM通用推理能力的前沿方向。 3.  **第三步：排除标准——论文不触及任何排除领域。** 论文的研究内容完全不涉及多模态、视觉、特定应用领域（如医疗、化学），也没有聚焦于水印、安全等应用层面的可靠性问题。它使用的是通用的问答（QA）基准，旨在发现和改进一种普遍存在的训练问题。 4.  **第四步：处理特殊和模糊情况——论文是正向案例。** - **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的典型范例。它研究的不是“用于XX领域的智能体”，而是“如何让智能体更好地使用搜索工具”这一通用方法论。 - **幻觉**: 论文在摘要开头就将“克服幻觉”作为研究动机之一。它提出的DeSA框架通过改进搜索行为，从根源上减少了因信息不足或错误而导致的幻觉。这属于“提出一种新方法来减少幻觉，从而提升模型的通用可靠性和推理质量”，符合保留标准。 5.  **第五步：最终决策** 综合分析，这篇论文的本质是提出一种创新的训练范式（DeSA），旨在优化LLM智能体在推理过程中的搜索行为，从而提升其通用的问题解决和推理能力。它精准地命中了所有核心筛选标准和正面指标，并成功避开了所有排除标准。因此，这篇论文是关于“大语言模型通用推理能力”研究的理想候选。"
    },
    {
        "index": "#12",
        "title": "Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning",
        "link": "/arxiv/2510.04817",
        "arxiv_id": "2510.04817",
        "authors": "Abhinav Madahar",
        "summary": "Controllers for structured LM reasoning (e.g., Chain-of-Thought, self-consistency, and Tree-of-Thoughts) often entangle what to try next with how to execute it, exposing only coarse global knobs and yielding brittle, compute-inefficient, and hard-to-audit behavior. We introduce Natural Language Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form natural-language directive to each search edge and translates it into a schema-bounded control vector for decoding, search (branch quotas, exploration $\\beta$), generation bundle size, retrieval mixtures, and verification passes. A labeller $\\Lambda$ emits labels from the parent state and a compact context; a tuner $\\Psi$ maps $(P, L, C)\\to \\Pi$, with strict schema validation and trust-region projection around safe defaults. Downstream selection remains ToT-style with score $S=\\mu+\\beta\\sigma$ and depth-annealed $\\beta$. We show NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for top-$k$ selection under label-conditioned bundles, and bound selector shortfall by control-vector distortion, providing decision-relevant justification for guards like trust regions and verification passes. We instantiate $\\Psi$ as a prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH (subset), StrategyQA, and ARC-Challenge with compute-aware reporting (success@compute, tokens-per-success) and ablations over $\\Lambda$, $\\Psi$, trust-region radius, and control quantization; preregistered forecasts anticipate accuracy gains at comparable token budgets and improved success@compute under constraints. NLEL offers an interpretable, model-agnostic interface that separates intent from execution for controllable, auditable LM inference.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.581909",
        "filter_reason": "这篇论文完全符合你的筛选标准，应当被保留。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“自然语言边缘标注（NLEL）”的新**方法论框架**。这个框架的目的不是将LLM应用到某个新领域，而是旨在**改进和增强LLM自身的结构化推理过程**。它直接针对当前主流推理方法（如思维链CoT、思维树ToT）的缺陷——即“意图”与“执行”的耦合——提出了解决方案。这完全属于“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的范畴。因此，根据第一步的核心判断，这篇论文应该**保留**。 2.  **第二步：正面指标** 论文与多个正面指标高度匹配： *   **核心概念**: 论文的研究对象是“结构化LM reasoning”，明确以大语言模型为基础。 *   **能力方向**: 论文的主题就是“reasoning”，并且通过在GSM8K（数学推理）、MATH（数学推理）、StrategyQA（逻辑推理）等标准数据集上进行评估，直接验证了其在通用推理能力上的效果。 *   **新兴范式**: 论文是对“Chain-of-Thought”和“Tree-of-Thoughts”等**新兴推理范式的深化和泛化**。它提出了一种更精细、更可控、更可审计的推理控制框架，这本身就是对LLM推理范式的重要贡献。 3.  **第三步：排除标准** 论文完全不触及任何排除标准： *   它没有涉及多模态、视觉等内容。 *   它的应用场景是通用的数学和逻辑问题，而非医疗、化学、机器人等特定领域。 *   它虽然提到了“可审计”和“信任区域”，但这是为了提升推理过程的可控性和可靠性，属于**方法论层面的改进**，而非研究水印、安全等应用层面的模型可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文的情况与“可解释性”这一特殊案例高度相关。论文通过引入自然语言标签，使得LLM在推理过程中的每一步选择都变得**可解释和可审计**。这种对内在推理过程的透明化，直接提升了模型推理的质量和可靠性，属于“增强模型内在的可解释性……从而提升模型的通用可靠性和推理质量”的情况，因此应该**保留**。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种**通用的、模型无关的框架**，用于精细化解耦和控制LLM的推理搜索过程。它直接解决了当前LLM推理方法中的一个核心痛点，旨在从根本上提升推理的效率、可控性和鲁棒性。这项工作是对LLM“通用推理能力”这一核心目标的直接、前沿的探索，因此**完全符合你的研究范围**。"
    },
    {
        "index": "#17",
        "title": "Watch and Learn: Learning to Use Computers from Online Videos",
        "link": "/arxiv/2510.04673",
        "arxiv_id": "2510.04673",
        "authors": "Chan Hee Song, Yiwen Song, Palash Goyal, Yu Su, Oriana Riva, Hamid Palangi, Tomas Pfister",
        "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse, ever-changing applications and environments, but learning is hindered by the scarcity of large-scale, high-quality training data in the target application. Existing datasets are domain-specific, static, and costly to annotate, while current synthetic data generation methods often yield simplistic or misaligned task demonstrations. To address these limitations, we introduce Watch & Learn (W&L), a framework that converts human demonstration videos readily available on the Internet into executable UI trajectories at scale. Instead of directly generating trajectories or relying on ad hoc reasoning heuristics, we cast the problem as an inverse dynamics objective: predicting the user's action from consecutive screen states. This formulation reduces manual engineering, is easier to learn, and generalizes more robustly across applications. Concretely, we develop an inverse dynamics labeling pipeline with task-aware video retrieval, generate over 53k high-quality trajectories from raw web videos, and demonstrate that these trajectories improve CUAs both as in-context demonstrations and as supervised training data. On the challenging OSWorld benchmark, UI trajectories extracted with W&L consistently enhance both general-purpose and state-of-the-art frameworks in-context, and deliver stronger gains for open-source models under supervised training. These results highlight web-scale human demonstration videos as a practical and scalable foundation for advancing CUAs towards real-world deployment.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.584596",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **核心判断（第一步）：** 论文的本质是提出一种名为“Watch & Learn (W&L)”的新框架，旨在提升“计算机使用智能体”的能力。CUAs的核心任务是在复杂、多变的环境中规划和执行多步骤任务流，这正是LLM通用推理能力（特别是规划和问题解决能力）的直接体现和应用。该论文并非将LLM作为工具应用于某个特定领域，而是致力于改进驱动这些智能体的LLM本身的基础能力。因此，它符合“保留”标准。 2.  **正面指标（第二步）：** 论文命中了多个关键正面指标： *   **能力方向：** 明确提到了“规划任务工作流”，这直接对应于`planning`和`problem-solving`。 *   **新兴范式：** 论文的核心研究对象是“计算机使用智能体”，这是`llm-based agents`和`tool use`范式的重要组成部分。 *   **训练方法：** 提出了一种新的数据生成和训练范式，即通过“逆动力学目标”从视频中学习，这属于改进模型训练方法论的范畴。 3.  **排除标准（第三步）：** 论文成功避开了所有排除标准。 *   **多模态与视觉：** 尽管论文的输入数据源是视频（涉及视觉），但其研究焦点并非视觉理解本身，而是如何从视觉状态中推断出用户的**操作意图和行动序列**，最终目的是训练智能体的**规划和执行能力**。视觉在这里是环境的表征，而不是研究的核心。这与纯粹的视觉或视频理解论文有本质区别。 *   **特定应用领域：** 论文明确强调其目标是解决“多样化的、不断变化的应用和环境”中的问题，并批评现有数据集“领域特定”。其使用的基准OSWorld也是一个通用计算机操作环境，而非特定领域。 4.  **特殊和模糊情况（第四步）：** 这篇论文是“智能体/工具使用”特殊情况的完美范例。它提出的是一个**通用的**智能体训练框架（W&L），旨在增强CUA的**通用问题解决能力**，而不是将其局限于某个特定领域（如化学或医疗）。因此，根据规则，应该保留。 **最终决策（第五步）：** 综合来看，这篇论文的核心贡献是提出了一种创新的方法，通过大规模在线视频数据来训练和提升计算机使用智能体的通用规划和执行能力。这直接服务于“提高大语言模型本身的通用推理能力”这一核心目标。它探讨了如何为LLM驱动的智能体提供更高质量、更具泛化性的训练数据，从而增强其在真实世界复杂任务中的推理表现。因此，这篇论文与你的研究课题高度相关，应当被筛选出来。"
    },
    {
        "index": "#32",
        "title": "Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning",
        "link": "/arxiv/2510.04488",
        "arxiv_id": "2510.04488",
        "authors": "Edward Y. Chang, Ethan Y. Chang",
        "summary": "Multi-agent debate often wastes compute by using a fixed adversarial stance, aggregating without deliberation, or stopping on heuristics. We introduce MACI, an active controller with two independent dials that decouple information from behavior: an information dial that gates evidence by quality, and a behavior dial that schedules contentiousness from exploration to consolidation. A moderator tracks disagreement, overlap, evidence quality, and argument quality, and halts when gains plateau. We provide theory-lite guarantees for nonincreasing dispersion and provable termination, with a budget-feasible scheduler. Across clinical diagnosis and news-bias tasks, MACI improves accuracy and calibration while reducing tokens, and converts residual uncertainty into precision RAG plans that specify what to retrieve next. We use a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal, validated for order invariance and judge-swap stability; stability depends on using high-capability judges. MACI turns debate into a budget-aware, measurable, and provably terminating controller.",
        "subjects": "Artificial Intelligence, Information Theory",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.603295",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。 **第一步：核心判断** 论文的核心贡献是提出了一种名为MACI（Multi-Agent Collaborative Intelligence）的**新型主动控制器框架**。该框架旨在优化多智能体辩论过程，通过引入双拨盘（信息拨盘和行为拨盘）来解耦信息和行为，并由一个仲裁者来动态调控推理流程。其根本目标是提升LLM在多步、多角度协作场景下的**推理可靠性、准确性和效率**。这并非将LLM应用于特定领域，而是在改进LLM自身进行推理的**方法论和范式**，完全符合“改进LLM基础能力、增强其通用推理能力”的核心要求。 **第二步：正面指标** 论文高度匹配多个正面指标： - **核心概念**: 论文标题和摘要中反复出现“LLM Reasoning”，明确指向大语言模型。 - **能力方向**: 论文的核心主题就是“Reasoning”，关注如何通过协作机制提升推理的“accuracy”、“calibration”和“reliability”。 - **新兴范式**: 论文聚焦于“Multi-Agent”协作、“Multi-agent debate”，这是当前提升LLM复杂推理能力的一个核心前沿范式。它提出的仲裁者和控制器机制，是对这一范式的创新性改进。 **第三步：排除标准** 论文没有触及任何排除标准的核心领域： - **多模态与视觉**: 全文未提及。 - **特定应用领域**: 论文虽然在“临床诊断和新闻偏见任务”上进行了评估，但这是为了**验证其通用方法的有效性**。论文的贡献是MACI这个通用框架本身，而不是一个“用于临床诊断的智能体”。这些任务只是测试床，用以证明该框架在不同领域的通用推理提升效果。 - **模型可靠性（应用层面）**: 论文讨论的可靠性是推理过程的内在可靠性（如准确性、校准度），而非水印、安全等应用层面的防护机制。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这正是典型的“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的案例。MACI框架本身是领域无关的，其设计目标是解决通用协作推理中的计算浪费和启发式停止等问题，因此应被保留。 - **幻觉/可解释性/安全**: 论文通过追踪“evidence quality”和“argument quality”，本质上是在构建一个更严谨的推理过程，从而可以减少因证据不足或论证不严而导致的错误结论（一种变相的幻觉控制）。这属于提升模型内在推理质量的方法，符合保留标准。 **最终决策** 综合分析，该论文提出了一种创新的、通用的多智能体协作控制框架（MACI），其核心目标是直接、根本性地提升大语言模型的推理质量和效率。它属于改进LLM基础推理方法论的前沿研究，与我关于“大语言模型通用推理能力”的研究课题高度契合，因此最终判断为 **True**。"
    },
    {
        "index": "#34",
        "title": "DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization",
        "link": "/arxiv/2510.04474",
        "arxiv_id": "2510.04474",
        "authors": "Gang Li, Yan Chen, Ming Lin, Tianbao Yang",
        "summary": "Recent large reasoning models (LRMs) driven by reinforcement learning algorithms (e.g., GRPO) have achieved remarkable performance on challenging reasoning tasks. However, these models suffer from overthinking, generating unnecessarily long and redundant reasoning even for simple questions, which substantially increases computational cost and response latency. While existing methods incorporate length rewards to GRPO to promote concise reasoning, they incur significant performance degradation. We identify the root cause: when rewards for correct but long rollouts are penalized, GRPO's group-relative advantage function can assign them negative advantages, actively discouraging valid reasoning. To overcome this, we propose Decoupled Reward Policy Optimization (DRPO), a novel framework that decouples the length-based learning signal of correct rollouts from incorrect ones. DRPO ensures that reward signals for correct rollouts are normalized solely within the positive group, shielding them from interference by negative samples. The DRPO's objective is grounded in integrating an optimized positive data distribution, which maximizes length-based rewards under a KL regularization, into a discriminative objective. We derive a closed-form solution for this distribution, enabling efficient computation of the objective and its gradients using only on-policy data and importance weighting. Of independent interest, this formulation is general and can incorporate other preference rewards of positive data beyond length. Experiments on mathematical reasoning tasks demonstrate DRPO's significant superiority over six efficient reasoning baselines. Notably, with a 1.5B model, our method achieves 77\\% length reduction with only 1.1\\% performance loss on simple questions like GSM8k dataset, while the follow-up baseline sacrifices 4.3\\% for 68\\% length reduction.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.604350",
        "filter_reason": "这篇论文完全符合您的研究范围。 **第一步：核心判断** 这篇论文的本质是提出一种新的强化学习训练范式（DRPO），用于解决当前大型推理模型（LRM）在推理过程中存在的“过度思考”问题。其核心贡献并非将LLM应用于某个特定领域，而是直接改进LLM在执行推理任务时的内在机制和效率。DRPO通过解耦奖励信号，优化了模型的策略，使其在保持准确性的同时，能够生成更简洁、高效的推理路径。这直接对应了筛选标准中“改进LLM的基础能力”、“提出新的训练范式”以及“增强其逻辑、数学、多步推理等通用能力”的要求。 **第二步：正面指标** 论文包含了多个强烈的正面指标： - **核心概念**: 论文聚焦于“Large reasoning models (LRMs)”，这是大语言模型（LLMs）的一个核心子集。 - **能力方向**: 论文的标题和摘要都明确以“Reasoning”为核心，并在“mathematical reasoning tasks”上进行了验证，完全符合“reasoning (尤其是 math reasoning)”的能力方向。 - **训练方法**: 论文的核心是提出一种新的强化学习算法“Decoupled Reward Policy Optimization (DRPO)”，是对现有“GRPO”等RL方法的改进，与“reinforcement learning (RL)”这一训练方法高度相关。 **第三步：排除标准** 论文完全避开了所有排除标准： - **多模态与视觉**: 论文的研究对象是纯文本的推理模型，不涉及任何视觉或多模态内容。 - **特定应用领域**: 尽管论文在数学数据集（如GSM8k）上进行实验，但其提出的方法（DRPO）是一种通用的优化框架，旨在解决推理过程中的普遍问题（过度思考），而非针对数学领域本身。其目标是提升模型的“通用推理能力”，而非解决特定领域问题。 - **模型可靠性（应用层面）**: 论文关注的是推理过程的效率和简洁性，而非水印、安全或社会影响等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，其核心目标非常明确和纯粹。 **第五步：最终决策** 综合以上分析，这篇论文提出了一种新颖的、通用的强化学习方法来直接提升大语言模型的推理效率和质量。它致力于解决模型在执行通用推理任务时遇到的一个根本性挑战（过度思考），完全契合您“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，这是一篇高度相关且应被保留的前沿论文。"
    },
    {
        "index": "#26",
        "title": "TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use",
        "link": "/arxiv/2510.04550",
        "arxiv_id": "2510.04550",
        "authors": "Pengfei He, Zhenwei Dai, Bing He, Hui Liu, Xianfeng Tang, Hanqing Lu, Juanhui Li, Jiayuan Ding, Subhabrata Mukherjee, Suhang Wang, Yue Xing, Jiliang Tang, Benoit Dumoulin",
        "summary": "Large language model (LLM)-based agents increasingly rely on tool use to complete real-world tasks. While existing works evaluate the LLMs' tool use capability, they largely focus on the final answers yet overlook the detailed tool usage trajectory, i.e., whether tools are selected, parameterized, and ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to comprehensively evaluate LLMs' tool use capability through diverse tasks with fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable tools across practical domains with tasks grounded in production-style APIs, and synthesizes trajectories that vary in breadth (parallel calls) and depth (interdependent chains). Besides final accuracy, TRAJECT-Bench also reports trajectory-level diagnostics, including tool selection and argument correctness, and dependency/order satisfaction. Analyses reveal failure modes such as similar tool confusion and parameter-blind selection, and scaling behavior with tool diversity and trajectory length where the bottleneck of transiting from short to mid-length trajectories is revealed, offering actionable guidance for LLMs' tool use.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.594631",
        "filter_reason": "这篇论文完全符合您的筛选标准。我的判断过程如下： 1.  **核心判断（第一步）：论文的本质是提升LLM的通用能力。** 这篇论文的核心贡献是提出了一个名为TRAJECT-Bench的**基准测试**。虽然它本身不是一种新的训练范式或模型架构，但其根本目的在于**评估和诊断**LLM在“工具使用”这一核心通用能力上的表现。论文强调，现有评估方法只关注最终答案，而忽略了达成答案的“轨迹”——即工具的选择、参数化和排序过程。这个“轨迹”本身就是一种复杂的**多步推理和规划过程**。通过提供细粒度的诊断（如工具选择错误、参数错误、依赖关系错误等），该基准测试为研究者提供了“可操作的指导”，从而能够更精确地定位和改进LLM在通用推理能力上的短板。因此，这篇论文的本质是服务于“提高LLM通用推理能力”这一核心目标，而不是将LLM应用于特定领域。 2.  **正面指标（第二步）：论文与核心主题高度相关。** 论文明确包含了多个关键正面指标： *   **核心概念**: \"Large language model (LLM)-based agents\" *   **能力方向**: 论文研究的“工具使用轨迹”直接关系到**规划**和**问题解决**，这是通用推理的核心组成部分。 *   **新兴范式**: 论文的核心就是关于“**llm-based agents**”和“**tool use**”的评估。 3.  **排除标准（第三步）：论文不涉及任何排除领域。** 论文的研究焦点纯粹集中在LLM的工具使用能力上，完全没有涉及多模态、视觉、医疗、化学等特定应用领域，也未讨论模型基础设施或应用层面的水印、安全等问题。 4.  **特殊和模糊情况（第四步）：处理得当。** 论文完美地符合“智能体/工具使用”的保留标准。它提出的是一个**通用的**智能体工具使用评估框架，旨在增强LLM的**通用问题解决能力**，而非将其应用于特定领域（如化学或法律）。其诊断结果（如“相似工具混淆”）直接指向了模型在逻辑和推理层面的缺陷，为后续改进模型提供了明确方向。 **最终决策（第五步）：** 综合以上分析，这篇论文虽然是一个基准测试，但其设计初衷和最终产出都是为了系统性地衡量、诊断并最终指导**LLM通用推理能力（特别是工具使用中的规划与多步推理）**的提升。它为该领域的研究者提供了至关重要的“标尺”和“显微镜”，是推动该方向发展的关键性基础设施研究。因此，它完全符合您的研究范围，应当被保留。"
    },
    {
        "index": "#27",
        "title": "Code World Models for General Game Playing",
        "link": "/arxiv/2510.04542",
        "arxiv_id": "2510.04542",
        "authors": "Wolfgang Lehrach, Daniel Hennes, Miguel Lazaro-Gredilla, Xinghua Lou, Carter Wendelken, Zun Li, Antoine Dedieu, Jordi Grau-Moya, Marc Lanctot, Atil Iscen, John Schultz, Marcus Chiam, Ian Gemp, Piotr Zielinski, Satinder Singh, Kevin P. Murphy",
        "summary": "Large Language Models (LLMs) reasoning abilities are increasingly being applied to classical board and card games, but the dominant approach -- involving prompting for direct move generation -- has significant drawbacks. It relies on the model's implicit fragile pattern-matching capabilities, leading to frequent illegal moves and strategically shallow play. Here we introduce an alternative approach: We use the LLM to translate natural language rules and game trajectories into a formal, executable world model represented as Python code. This generated model -- comprising functions for state transition, legal move enumeration, and termination checks -- serves as a verifiable simulation engine for high-performance planning algorithms like Monte Carlo tree search (MCTS). In addition, we prompt the LLM to generate heuristic value functions (to make MCTS more efficient), and inference functions (to estimate hidden states in imperfect information games). Our method offers three distinct advantages compared to directly using the LLM as a policy: (1) Verifiability: The generated CWM serves as a formal specification of the game's rules, allowing planners to algorithmically enumerate valid actions and avoid illegal moves, contingent on the correctness of the synthesized model; (2) Strategic Depth: We combine LLM semantic understanding with the deep search power of classical planners; and (3) Generalization: We direct the LLM to focus on the meta-task of data-to-code translation, enabling it to adapt to new games more easily. We evaluate our agent on 10 different games, of which 4 are novel and created for this paper. 5 of the games are fully observed (perfect information), and 5 are partially observed (imperfect information). We find that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10 considered games.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.600508",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献是提出了一种新颖的范式来显著增强大语言模型的通用规划和推理能力。以下是我的详细判断过程： **第一步：核心判断** 论文的本质并非简单地将LLM应用于游戏领域，而是为了解决当前LLM在复杂推理任务（如下棋）中存在的根本性缺陷——直接生成动作时容易出现非法操作、策略深度不足。作者提出的核心方法是“代码世界模型”，利用LLM的语义理解能力，将非结构化的自然语言规则“翻译”成一个形式化、可执行的程序。这本质上是一种新的方法论，它将LLM从一个直接的、脆弱的策略生成器，转变为一个强大的、通用的“世界模型构建器”。这个构建出的世界模型再与经典的规划算法（如MCTS）结合，实现了LLM语义理解与经典规划深度搜索的强强联合。这完全符合“改进LLM的基础能力”、“增强其规划、多步推理等通用能力”以及“智能体协作框架、工具使用”的保留标准。 **第二步：正面指标** 该论文高度命中了多个正面指标： - **核心概念**: 论文明确以Large Language Models (LLMs)为核心研究对象。 - **能力方向**: 论文的核心是提升LLM在游戏中的reasoning和planning能力，直接对比了其方法与直接LLM推理在\"strategically shallow play\"上的差异。 - **新兴范式**: 这篇论文是llm-based agent和tool use研究的典型范例。LLM在此处作为核心工具，生成另一个工具（代码世界模型），共同构成一个更强大的智能体系统。 **第三步：排除标准** 论文不涉及任何排除标准中提到的领域： - 它不涉及多模态（Vision, MLLMs等）。 - 它的研究目标是“General Game Playing”，而非特定领域（如医疗、化学等）的应用，游戏在此处是衡量通用推理能力的基准测试。 - 它不讨论模型部署、硬件加速或应用层面的安全、水印问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的是一种“通用游戏博弈”框架，其核心是让LLM构建一个“世界模型”这一通用能力，而非针对某个特定游戏。这完全符合保留条件：“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”。 **第五步：最终决策** 综合以上分析，该论文的核心贡献在于提出了一种创新的框架，通过让LLM承担“规则到代码”的元任务，有效规避了其直接进行复杂多步推理时的短板，并将其强大的语义理解能力与经典规划算法的深度搜索能力相结合，从而在根本上提升了LLM驱动智能体的通用规划和问题解决能力。这项研究对于探索如何让LLM进行更可靠、更深入的推理具有非常重要的前沿价值，是“大语言模型通用推理能力”研究方向的优秀论文。因此，判断为符合。"
    },
    {
        "index": "#24",
        "title": "COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context",
        "link": "/arxiv/2510.04568",
        "arxiv_id": "2510.04568",
        "authors": "Naman Gupta, Shreeyash Gowaikar, Arun Iyer, Kirankumar Shiragur, Ramakrishna B Bairi, Rishikesh Maurya, Ritabrata Maiti, Sankarshan Damle, Shachee Mishra Gupta",
        "summary": "Reasoning over very long inputs remains difficult for large language models (LLMs). Common workarounds either shrink the input via retrieval (risking missed evidence), enlarge the context window (straining selectivity), or stage multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents, CoA), free-form summaries passed between agents can discard crucial details and amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc messages with a structured memory. A Planner agent first turns a user query into concrete, checkable sub-questions. worker agents process chunks via a fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared memory. A Manager agent then Synthesizes the final answer directly from the memory. This preserves step-wise read-then-reason benefits while changing both the communication medium (structured memory) and the worker procedure (fixed micro-cycle), yielding higher faithfulness, better long-range aggregation, and auditability. On long-context QA from the HELMET suite, COSMIR reduces propagation-stage information loss and improves accuracy over a CoA baseline.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.593435",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。 **第一步：核心判断** 论文的核心贡献是提出了一种名为COSMIR的新框架，旨在解决大语言模型在处理长上下文时的推理难题。它并非将LLM应用于某个特定领域，而是专注于改进LLM**本身的推理过程和方法论**。其核心思想是“用结构化内存和固定的智能体协作流程来替代自由形式的摘要”，以减少信息损失和错误传播。这直接属于“提出新的训练范式（此处为推理框架）、增强其...多步推理等通用能力”的范畴，符合保留标准。 **第二步：正面指标** 论文命中了多个关键的正面指标： - **核心概念**: 明确以 \"large language models (LLMs)\" 为研究对象。 - **能力方向**: 标题和摘要多次强调 \"Iterative Reasoning\"（迭代推理），这是多步推理的核心体现。同时，\"Planner agent\" 将查询分解为子问题，这涉及到**规划**能力。 - **新兴范式**: COSMIR本身就是一个典型的 \"llm-based agents\" 和 \"multi-agent systems\" 框架，它定义了Planner、Worker、Manager等多个角色的协作方式，这与“智能体协作框架”这一保留范例高度吻合。 **第三步：排除标准** 论文完全避开了所有排除标准： - 它不涉及**多模态与视觉**，处理的是纯文本的长上下文。 - 它不聚焦于任何**特定应用领域**，其评估数据集HELMET是通用的长上下文问答基准，而非医疗、化学等。 - 它讨论的忠实度和可审计性是作为其推理框架带来的**内在质量提升**，而非应用层面的水印、安全或社会学研究。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的COSMIR框架正是一个“通用的智能体协作框架”，其目标是增强LLM在长上下文下的“通用问题解决能力”，而非应用于特定领域。因此，根据规则，应该保留。 - **幻觉/可解释性/安全**: 论文通过结构化内存和固定流程，旨在解决早期错误被放大的问题，并提供“可审计性”，这实际上是从方法论层面提升了模型的内在可靠性，减少了推理过程中的“幻觉”风险。这属于“通过新方法来...提升模型的通用可靠性和推理质量”，因此应该保留。 **最终决策** 综合以上分析，这篇论文的研究本质、核心贡献和研究方向与研究目标“提高大语言模型（LLM）本身的『通用推理能力』”高度一致。它提出了一种创新的、基于多智能体协作的推理框架，直接针对LLM在复杂、长程推理任务中的弱点进行优化。因此，最终判断为**True**，应被纳入筛选范围。"
    },
    {
        "index": "#29",
        "title": "Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph",
        "link": "/arxiv/2510.04520",
        "arxiv_id": "2510.04520",
        "authors": "Hanyu Wang, Ruohan Xie, Yutong Wang, Guoxiong Gao, Xintao Yu, Bin Dong",
        "summary": "Accurate auto-formalization of theorem statements is essential for advancing automated discovery and verification of research-level mathematics, yet remains a major bottleneck for LLMs due to hallucinations, semantic mismatches, and their inability to synthesize new definitions. To tackle these issues, we present Aria (Agent for Retrieval and Iterative Autoformalization), a system for conjecture-level formalization in Lean that emulates human expert reasoning via a two-phase Graph-of-Thought process: recursively decomposing statements into a dependency graph and then constructing formalizations from grounded concepts. To ensure semantic correctness, we introduce AriaScorer, a checker that retrieves definitions from Mathlib for term-level grounding, enabling rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy, surpassing previous methods. On FATE-X, a suite of challenging algebra problems from research literature, it outperforms the best baseline with 44.0% vs. 24.0% final accuracy. On a dataset of homological conjectures, Aria reaches 42.9% final accuracy while all other models score 0%.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.601628",
        "filter_reason": "这篇论文完全符合我的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 论文的核心贡献是提出了一个名为Aria的智能体系统，用于解决LLM在“自动形式化”这一高难度推理任务上的瓶颈。自动形式化要求模型将自然语言描述的数学定理精确地转换为形式化语言（如Lean），这本质上是一个极度严格、不容许任何偏差的逻辑、数学和符号推理任务。论文提出的方法，如“思维图”的递归分解和基于检索的迭代验证，并非将LLM作为工具应用于某个外部领域，而是直接针对LLM在**逻辑一致性、语义理解和多步推理**上的内在缺陷进行改进。这是一种旨在增强LLM基础推理能力的新方法论。 2.  **第二步：正面指标——论文高度相关。** 论文摘要中包含了多个关键正面指标： *   **核心概念**: 论文的研究对象是LLM。 *   **能力方向**: 论文聚焦于**reasoning**，特别是**math reasoning**和**logical reasoning**。自动形式化本身就是这两种能力的终极体现。 *   **新兴范式**: 论文提出了一个**llm-based agent (Aria)**，并使用了**tool use (AriaScorer从Mathlib检索定义)**的策略来增强其能力。这完全符合对通用智能体框架和工具使用方法的研究。 3.  **第三步：排除标准——论文不属于排除范围。** *   **多模态与视觉**: 论文不涉及任何视觉或多模态内容。 *   **特定应用领域**: 虽然论文的实验是在数学领域进行的，但数学是衡量逻辑和推理能力的通用标准，类似于下棋。这篇论文的重点不是“解决某个数学问题”，而是“**如何让LLM学会更严谨的推理范式**”。因此，它不属于将LLM应用于特定领域（如医疗、化学）的应用型研究。 *   **模型可靠性（应用层面）**: 论文虽然关注“幻觉”问题，但其目的是通过提出新的内在方法论（迭代验证）来从根本上提升模型的推理质量和可靠性，而非讨论应用层面的水印或安全策略。 4.  **第四步：处理特殊和模糊情况——论文处理得当。** *   **智能体/工具使用**: 论文提出的Aria智能体和AriaScorer工具使用方法，是一个**通用的、旨在增强LLM问题解决能力**的框架。其核心思想（分解、验证、工具辅助）具有普适性，可以迁移到其他需要严谨推理的任务中，因此应当保留。 *   **幻觉/可解释性/安全**: 论文通过引入基于外部知识库的检索和验证机制，直接针对LLM的“幻觉”和“语义不匹配”问题提出了一个解决方案。这是一种通过改进模型内在推理流程来提升其可靠性的创新方法，完全符合保留标准。 **最终决策**: 综合以上分析，这篇论文的本质是提出一种新的智能体框架和推理范式（思维图），通过迭代验证和工具使用，显著提升了LLM在极具挑战性的逻辑和数学推理任务上的表现。它直接触及并尝试解决LLM通用推理能力的核心瓶颈。因此，这篇论文与我的研究目标高度契合，应该被保留。"
    },
    {
        "index": "#21",
        "title": "Making Mathematical Reasoning Adaptive",
        "link": "/arxiv/2510.04617",
        "arxiv_id": "2510.04617",
        "authors": "Zhejian Lai, Xiang Geng, Zhijun Wang, Yang Bai, Jiahuan Li, Rongxiang Weng, Jingang Wang, Xuezhi Cao, Xunliang Cai, Shujian Huang",
        "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. However, existing LLMs exhibit failures of robustness and generalization. This paper attributes these deficiencies to spurious reasoning, i.e., producing answers from superficial features. To address this challenge, we propose the AdaR framework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers. AdaR synthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. To improve data quality, we extract the problem-solving logic from the original query and generate the corresponding answer by code execution, then apply a sanity check. Experimental results demonstrate that AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis indicates that data synthesis and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs. Subsequent analyses derive key design insights into the effect of critical factors and the applicability to instruct LLMs. Our project is available at https://github.com/LaiZhejian/AdaR",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.591910",
        "filter_reason": "这篇论文完全符合筛选要求，应予保留。其判断过程如下： 1.  **第一步：核心判断** 论文的核心是解决大语言模型在数学推理方面的“鲁棒性和泛化性”缺陷。它将问题根源归结为“虚假推理”，并提出了一种名为“AdaR”的新框架来引导模型进行“自适应推理”。本质上，这是一种通过改进训练范式（结合数据合成和强化学习RLVR）来增强LLM基础推理能力的方法。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学等通用能力”的保留标准。它并非将LLM应用于特定领域，而是聚焦于提升模型本身的核心能力。 2.  **第二步：正面指标** 论文摘要中高频次地出现了多个关键正面指标： - **核心概念**: \"large language models (LLMs)\" - **能力方向**: \"Mathematical reasoning\", \"robustness\", \"generalization\", \"problem-solving logic\" - **训练方法**: \"trains models with RLVR\" (一种强化学习方法) 这些关键词直接指向了研究目标的核心范畴。 3.  **第三步：排除标准** 该论文完全不涉及任何排除标准领域： - **多模态与视觉**: 论文专注于文本和数学逻辑，不涉及视觉信息。 - **特定应用领域**: “数学推理”被视为一种通用的认知能力，而非生物、化学等特定应用领域。论文的目标是提升这种通用能力，而非解决某个特定领域的科学问题。 - **模型可靠性（应用层面）**: 论文关注的是模型内在推理过程的可靠性（鲁棒性、泛化性），而非水印、安全等应用层面的技术。 4.  **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: 论文所解决的“虚假推理”问题是导致事实性错误和幻觉的重要内在原因之一。通过训练模型依赖“问题解决逻辑”而非“表面特征”，该方法直接提升了模型内在的推理质量和可靠性，这符合“提升模型的通用可靠性和推理质量”的保留条件。 **最终决策**: 综上所述，该论文通过提出新的训练范式（AdaR）和强化学习方法（RLVR）来直接解决LLM在数学推理中的核心缺陷（虚假推理），旨在提升模型本身的通用推理鲁棒性和泛化能力。这与研究课题“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，应将其保留。"
    },
    {
        "index": "#40",
        "title": "On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems",
        "link": "/arxiv/2510.04311",
        "arxiv_id": "2510.04311",
        "authors": "Bohan Tang, Huidong Liang, Keyue Jiang, Xiaowen Dong",
        "summary": "Large language model multi-agent systems (LLM-MAS) offer a promising paradigm for harnessing collective intelligence to achieve more advanced forms of AI behaviour. While recent studies suggest that LLM-MAS can outperform LLM single-agent systems (LLM-SAS) on certain tasks, the lack of systematic experimental designs limits the strength and generality of these conclusions. We argue that a principled understanding of task complexity, such as the degree of sequential reasoning required and the breadth of capabilities involved, is essential for assessing the effectiveness of LLM-MAS in task solving. To this end, we propose a theoretical framework characterising tasks along two dimensions: depth, representing reasoning length, and width, representing capability diversity. We theoretically examine a representative class of LLM-MAS, namely the multi-agent debate system, and empirically evaluate its performance in both discriminative and generative tasks with varying depth and width. Theoretical and empirical results show that the benefit of LLM-MAS over LLM-SAS increases with both task depth and width, and the effect is more pronounced with respect to depth. This clarifies when LLM-MAS are beneficial and provides a principled foundation for designing future LLM-MAS methods and benchmarks.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.612815",
        "filter_reason": "这篇论文完全符合你的筛选标准，应该被保留。以下是详细的判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 这篇论文的核心不是提出一种新的训练方法或模型架构，而是对一种旨在增强LLM能力的前沿范式——大语言模型多智能体系统（LLM-MAS）——进行**基础性、机理性的研究**。它的本质是探究“为什么”以及“在何种条件下”LLM-MAS能够比单一LLM表现更好。这种对方法论本身的深入剖析，其最终目的就是为了指导如何更有效地构建和应用LLM-MAS来**解决更复杂的通用问题**。这完全符合“致力于提高大语言模型本身通用推理能力”的宏大目标，因为它为“如何提高”（即通过多智能体协作）提供了理论依据和设计原则。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文命中了多个关键正面指标： *   **核心概念**: 论文紧紧围绕“Large language model multi-agent systems (LLM-MAS)”展开。 *   **能力方向**: 论文直接将任务复杂度的核心维度“深度”定义为“推理长度”，并实证研究了多智能体系统在长推理任务上的优势。这直接切中了“reasoning”这一核心能力。 *   **新兴范式**: “llm-based agents”和“multi-agent systems”是本论文的绝对主角。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全避开了所有排除标准。它不涉及多模态、视觉，也没有将智能体应用于医疗、化学等特定领域。它研究的是通用任务，聚焦于方法论本身，而非应用。 4.  **第四步：处理特殊和模糊情况** 本论文是“智能体”相关特殊情况的完美范例。它并非提出一个新的智能体框架，而是对“多智能体辩论”这一代表性框架进行理论和实证分析。这种分析属于**通用智能体协作框架的基础研究**，旨在阐明该范式的有效性边界（即任务深度和宽度越大，优势越明显）。这为未来设计和优化用于增强LLM通用问题解决能力的智能体系统提供了**“原则性基础”**，因此绝对应该保留。 **综合判断与核心依据:** 这篇论文的核心贡献在于，它超越了简单的“提出新方法并展示效果”的层面，而是上升到了“理解方法论背后的原理”的高度。它通过提出任务复杂度的“深度-宽度”理论框架，清晰地论证了LLM-MAS在需要**长序列推理**的任务上具有巨大优势。这项工作的意义在于，它为所有致力于通过“智能体协作”这一路径来提升LLM通用推理能力的研究者，提供了重要的理论指导和评估基准。它回答了一个根本性问题：我们应该在哪些类型的推理任务上优先考虑使用多智能体系统？ 因此，尽管这篇论文没有直接“改进”一个LLM，但它对“如何有效提升LLM推理能力”这一领域做出了根本性的贡献，是典型的、高质量的前沿研究。它完全符合你的筛选要求。"
    },
    {
        "index": "#52",
        "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning",
        "link": "/arxiv/2510.04116",
        "arxiv_id": "2510.04116",
        "authors": "Ziying Zhang, Yaqing Wang, Quanming Yao",
        "summary": "Meta reasoning behaviors work as a skeleton to guide large language model (LLM) reasoning, thus help to improve reasoning performance. However, prior researches implement meta reasoning skeleton with manually designed structure, limiting ability to adapt to query-specific requirement and capture intricate logical dependency among reasoning steps. To deal with the challenges, we represent meta reasoning skeleton with directed acyclic graph (DAG) to unify skeletons proposed in prior works and model intricate logical dependency. Then we propose AutoMR, a framework that searches for query-aware meta reasoning skeleton automatically inspired by automated machine learning (AutoML). Specifically, we construct search space based on DAG representation of skeleton and then formulate the search problem. We design a dynamic skeleton sampling algorithm by expanding meta reasoning skeleton along with reasoning context at inference time. This algorithm can derive any meta reasoning skeleton in search space efficiently and adapt skeleton to evolving base reasoning context, thus enable efficient query-aware skeleton search. We conduct experiments on extensive benchmark datasets. Experimental results show that AutoMR achieves better reasoning performance than previous works broadly.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.624901",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为`AutoMR`的新框架，用于**自动搜索“元推理骨架”**来引导大语言模型的推理过程。这直接聚焦于改进LLM的**基础推理能力**和**方法论**。它并非将LLM应用于某个垂直领域，而是致力于优化LLM在处理通用问题时的内在逻辑结构和多步推理策略。这与“思维链”等旨在增强模型基础能力的研究范式完全一致，因此应予以保留。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 论文标题和摘要反复提及“Large language model (LLM) reasoning”。 *   **能力方向**: 论文的核心目标就是提升“reasoning performance”，并讨论“intricate logical dependency”，直接指向逻辑推理和多步问题解决能力。 *   **训练/优化方法**: 论文受“automated machine learning (AutoML)”启发，提出了一种**自动搜索**框架。这种搜索和优化最优结构的方法，与强化学习、进化等方法一样，属于改进模型能力的新范式。 *   **新兴范式**: 自动搜索最优推理路径可以被视为“深度研究”和智能体“规划”能力的一种基础技术，为构建更强大的问题解决系统提供了支持。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准中的领域。它专注于纯文本的推理任务，在“广泛的基准测试数据集”上进行验证，没有涉及多模态、医疗、化学等特定应用领域，也未讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文不直接关于智能体或工具使用，但其提出的**动态构建和适应推理结构**的方法，是构建一个能够自主规划、解决复杂问题的通用智能体的关键技术组成部分。它属于增强LLM通用问题解决能力的底层研究，而非特定领域的应用。 5.  **第五步：最终决策** 综合以上分析，该论文的本质是提出一种创新的、自动化的方法（`AutoMR`）来增强LLM的通用推理骨架，从而提升其逻辑推理和问题解决的性能。这与研究课题“大语言模型通用推理能力”的核心目标完美契合。因此，最终判断为**保留**。"
    },
    {
        "index": "#45",
        "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework",
        "link": "/arxiv/2510.04206",
        "arxiv_id": "2510.04206",
        "authors": "Hanchen Zhang, Xiao Liu, Bowen Lv, Xueqiao Sun, Bohao Jing, Iat Long Iong, Zhenyu Hou, Zehan Qi, Hanyu Lai, Yifan Xu, Rui Lu, Hongning Wang, Jie Tang, Yuxiao Dong",
        "summary": "Recent advances in large language models (LLMs) have sparked growing interest in building generalist agents that can learn through online interactions. However, applying reinforcement learning (RL) to train LLM agents in multi-turn, multi-task settings remains challenging due to lack of scalable infrastructure and stable training algorithms. In this work, we present the AgentRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AgentRL features a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, we design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. Experiments show that AgentRL, trained on open LLMs across five agentic tasks, significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents. Multi-task training with AgentRL matches the best results among all task-specific models. AgentRL is open-sourced at https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in building \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.620914",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献与您的筛选标准高度一致。我的判断过程如下： 1.  **第一步：核心判断——保留。** 论文的核心是提出一个名为**AgentRL**的框架，用于在**多轮、多任务**环境下，通过**强化学习（RL）**来训练大语言模型（LLM）智能体。这完全符合您的核心目标——论文的本质是研究如何通过一种新的训练范式（基于RL的智能体训练）来系统性提升LLM的通用推理和问题解决能力。它不是将LLM作为工具应用于某个特定领域，而是致力于改进LLM本身的基础能力，特别是在需要多步交互和规划的复杂任务中的表现。 2.  **第二步：正面指标——高度相关。** 论文摘要中包含了多个关键的正面指标： *   **核心概念**: 明确提到 \"Large language models (LLMs)\"。 *   **能力方向**: 虽然没有直接使用\"reasoning\"一词，但\"generalist agents that can learn through online interactions\"、\"agentic tasks\"等表述都指向了通用的**问题解决**、**规划**和**多步推理**能力。训练智能体能完成任务，本质上就是在训练其推理与决策能力。 *   **训练方法**: 论文的标题和核心就是关于**Reinforcement Learning (RL)**的应用。 *   **新兴范式**: 论文聚焦于**llm-based agents**，并提出了一个可扩展的框架。 3.  **第三步：排除标准——不触及。** 论文的研究内容完全不涉及任何排除标准中提到的领域。它没有讨论多模态、视觉模型，也没有限定在医疗、化学等特定应用领域，更没有关注水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况——符合保留条件。** 针对“智能体”这一特殊情况，论文提出的是一个**通用的智能体协作与训练框架**，旨在提升LLM在多种任务上的通用表现（\"multi-task RL\"）。摘要中提到其训练结果在五个智能体任务上超越了多个顶尖模型，并且多任务训练的效果能匹配特定任务模型的最佳结果，这强有力地证明了该框架的**通用性**。因此，它符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”的原则。 **最终决策**: 综合以上分析，AgentRL论文提出了一种创新的、可扩展的训练框架和算法，旨在攻克提升LLM通用推理能力（尤其是在多轮、多任务交互中）的关键挑战。这与您的研究课题“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度契合。因此，这篇论文应该被保留。"
    },
    {
        "index": "#35",
        "title": "Utility-Learning Tension in Self-Modifying Agents",
        "link": "/arxiv/2510.04399",
        "arxiv_id": "2510.04399",
        "authors": "Charles L. Wang, Keir Dorchen, Peter Jin",
        "summary": "As systems trend toward superintelligence, a natural modeling premise is that agents can self-improve along every facet of their own design. We formalize this with a five-axis decomposition and a decision layer, separating incentives from learning behavior and analyzing axes in isolation. Our central result identifies and introduces a sharp utility--learning tension, the structural conflict in self-modifying systems whereby utility-driven changes that improve immediate or expected performance can also erode the statistical preconditions for reliable learning and generalization. Our findings show that distribution-free guarantees are preserved iff the policy-reachable model family is uniformly capacity-bounded; when capacity can grow without limit, utility-rational self-changes can render learnable tasks unlearnable. Under standard assumptions common in practice, these axes reduce to the same capacity criterion, yielding a single boundary for safe self-modification. Numerical experiments across several axes validate the theory by comparing destructive utility policies against our proposed two-gate policies that preserve learnability.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.604857",
        "filter_reason": "这篇论文非常符合您的研究范围。我的判断过程如下： **第一步：核心判断** 这篇论文的本质是研究『自我修改智能体』的基础理论。它没有将智能体作为工具应用于某个特定领域，而是深入探讨了智能体在进行自我改进时，其内部存在的根本性冲突——即“效用-学习张力”。论文的核心贡献在于形式化并证明了：一个为了提升短期性能（效用）而进行的自我修改，可能会破坏其长期学习和泛化的根基。这直接触及了如何让一个智能系统（包括未来的LLM）能够安全、可靠地『自我进化』，从而提升其通用能力的核心问题。因此，它属于改进LLM基础能力和提出新训练范式（自我进化）的范畴，应予以保留。 **第二步：正面指标** 论文与多个正面指标高度相关： - **能力方向**: 虽然没有直接使用“reasoning”一词，但其核心关注点“可靠的学习和泛化”是通用推理能力的基石。无法泛化，就无法进行通用的、多步的推理。 - **训练方法**: 论文为“自我进化”或“自我修改”这一前沿训练范式提供了关键的理论基础和安全边界。它探讨了在什么条件下，一个智能体的自我改变是“安全的”（即不会损害学习能力）。 - **新兴范式**: 论文的研究对象是“自我修改智能体”，这是LLM-based agents发展的终极形态之一。研究其内在原理，对于构建更强大的通用智能体至关重要。 **第三步：排除标准** 论文完全不涉及任何排除标准： - 它不是关于多模态或视觉的。 - 它不是针对医疗、化学、机器人等特定领域的应用研究。 - 它虽然提到了“safe self-modification”，但这是指保护模型学习能力不被破坏的『内在、根本性』安全，而非防止生成有害内容或添加水印等『应用层面』的安全。 **第四步：处理特殊和模糊情况** - **智能体**: 这篇论文提出的理论是一个通用的智能体框架，旨在理解并指导智能体的自我进化过程，而不是将其应用于某个垂直领域。这完全符合保留条件。 - **安全**: 论文提出的“安全自我修改”边界，是为了确保模型在追求更高性能时，不会“搬起石头砸自己的脚”，即破坏自身学习和泛化的能力。这是一种提升模型内在可靠性和推理质量的根本性方法，因此应该保留。 **第五步：最终决策** 综合以上分析，这篇论文虽然理论性很强，没有直接提出一个在现有LLM上立即可用的技巧，但它深刻地揭示了高级智能体（包括未来的LLM）在自我进化道路上可能遇到的根本性障碍。它为“如何让LLM真正具备自我提升通用推理能力”这一宏伟目标提供了重要的理论指导和安全护栏。因此，这篇论文与您的研究课题高度相关，是必须保留的前沿研究。"
    },
    {
        "index": "#55",
        "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows",
        "link": "/arxiv/2510.04089",
        "arxiv_id": "2510.04089",
        "authors": "Yitong Cui, Liu Liu, Baosheng Yu, Jiayan Qiu, Xikai Zhang, Likang Xiao, Yixing Liu, Quan Chen",
        "summary": "Large language models (LLMs) have exhibited significant capabilities in addressing challenging problems throughout various fields, often through the use of agentic workflows that adhere to structured instructions and multi-step procedures. However, designing such workflows demands substantial manual effort, posing challenges to scalability and generalizability. Recent studies have aimed to minimize the human intervention needed for their construction, leading to advances in automated techniques for optimizing agentic workflows. However, current approaches are often constrained by their limited representational capacity, insufficient adaptability, weak scalability, and pairwise comparison paradigm -- issues that stem primarily from a dependence on discrete optimization techniques. To overcome these limitations, we introduce a new score-based preference approach, refereed as SPOGW, which operates directly on cardinal reward signals through group-wise comparison and enables more efficient and stable optimization in a continuous space. SPOGW incorporates Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL), which regulates training update by placing greater emphasis on the advantageous regions of the policy response. In five benchmark datasets covering mathematical reasoning, coding, and question answering, SPOGW matches or exceeds the performance of current state-of-the-art approaches, presenting a viable and forward-looking methodology for automated generation and optimization of agentic workflows.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.631761",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文本质** 该论文的核心贡献是提出了一种名为SPOGW的**新型优化方法**，用于自动化生成和优化“智能体工作流”。论文的出发点是，手动设计让LLM执行复杂任务的“工作流”（即结构化的多步骤指令序列）成本高昂且难以泛化。SPOGW通过在连续空间中进行基于分数的组别比较，提供了一种更高效、稳定的方式来训练和优化这些工作流。 **这直接触及了提升LLM『通用推理能力』的核心。** “智能体工作流”是LLM实现复杂多步推理、规划和问题求解的关键架构和范式。论文并非将LLM应用于某个特定领域，而是致力于改进LLM本身组织和执行复杂任务流程的**基础能力**。这属于方法论层面的创新，旨在增强LLM的通用推理与规划能力，完全符合“保留”标准。 2.  **第二步：正面指标匹配** - **核心概念**: 论文明确以“Large language models (LLMs)”为研究对象。 - **能力方向**: 论文旨在优化“agentic workflows”和“multi-step procedures”，这正是LLM进行推理、规划和问题求解的体现。其评估基准覆盖了“数学推理、编码和问答”，这些都是通用推理能力的典型测试场景。 - **训练方法**: 论文提出的SPOGW是一种“score-based preference approach”，结合了“Iterative offline GRPO (ioGRPO)”，这属于强化学习（RL）和偏好优化的范畴，是当前提升LLM能力的前沿训练方法。 - **新兴范式**: 论文的焦点是“llm-based agents”的“workflows”，完全命中了智能体这一新兴范式。 该论文与所有正面指标高度相关。 3.  **第三步：排除标准检验** - 论文不涉及多模态、视觉。 - 论文的研究目标是通用的工作流优化方法，评估任务也是通用的推理任务，不属于任何特定应用领域（如医疗、化学等）。 - 论文不关注水印、安全等模型可靠性应用层面问题。 该论文未触及任何排除标准。 4.  **第四步：特殊情况处理** - **智能体/工具使用**: 论文提出的SPOGW是一种**通用的**智能体工作流优化框架，其目标是提升LLM在各种通用任务（数学、编码、问答）上的表现，而非应用于特定领域。因此，它符合“保留”条件。 5.  **第五步：最终决策** 综合来看，这篇论文的本质是提出一种新的训练/优化范式（SPOGW），其目标是自动化地构建和优化能让LLM执行多步复杂推理的“工作流”。这是一种致力于从方法论层面提升LLM内在的、通用的推理与规划能力的研究，与您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”这一核心目标高度契合。因此，应予以保留。"
    },
    {
        "index": "#60",
        "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning",
        "link": "/arxiv/2510.04040",
        "arxiv_id": "2510.04040",
        "authors": "Xu Shen, Song Wang, Zhen Tan, Laura Yao, Xinyu Zhao, Kaidi Xu, Xin Wang, Tianlong Chen",
        "summary": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models. To the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.634286",
        "filter_reason": "这篇论文完全符合您的研究范围筛选标准，理由如下： **核心判断 (第一步):** 这篇论文的本质是提出一种**方法论和评测基准**，旨在解决大语言模型核心推理范式——思维链——的根本问题，即其“忠实性”。虽然它没有直接提出一个新的训练策略或模型架构，但它为评估和改进LLM的推理过程质量提供了必不可少的工具和理论基础。提升推理的“忠实性”和“可解释性”本身就等同于提升推理能力的“质量”和“可靠性”。这直接服务于“提高LLM本身的通用推理能力”这一核心目标，而非将LLM应用到特定领域。 **正面指标 (第二步):** 论文命中了多个关键正面指标： *   **核心概念:** 论文明确聚焦于Large language models (LLMs)。 *   **能力方向:** 核心主题是reasoning，特别是Chain-of-Thought (CoT) reasoning，以及problem-solving和trustworthy reasoning。 *   这篇论文的工作，即评测CoT的忠实性，是通往更高级planning和problem-solving能力的必经之路。 **排除标准 (第三步):** 论文完全没有触及任何排除标准： *   它不涉及多模态、视觉或扩散模型。 *   它的研究目标是通用性的，虽然使用了多个领域的数据来构建基准，但其目的是为了评测通用的CoT能力，而非解决特定领域（如医疗、化学）的问题。 *   它关注的可靠性（Reliability）是内在的、与推理过程质量直接相关的“忠实性”，而不是应用层面的水印或安全策略。 **特殊情况和模糊情况 (第四步):** 这篇论文完美地契合了“幻觉/可解释性”的特殊情况。CoT的不忠实性可以被看作是一种结构化的幻觉或推理过程的错误。这篇论文提出了一种系统性的评测方法论（一个基准），来量化这个问题，这为未来开发减少此类不忠实性、增强模型内在可解释性的新方法铺平了道路。它不是对社会现象的讨论，而是对模型内在推理质量的深度技术探索和评测。 **最终决策 (第五步):** 综合分析，该论文的贡献是基础性的。它为“提升LLM通用推理能力”这一宏大课题提供了关键的“尺子”和“测试场”。没有这样的基准，任何声称改进了CoT的新方法都将缺乏科学、量化的评估依据。因此，这篇论文虽然不直接“改进”模型，但它通过建立评测标准，直接驱动和支撑了未来的改进研究，是领域内不可或缺的、高价值的论文。它符合您筛选标准的所有要求，应当被保留。"
    },
    {
        "index": "#47",
        "title": "Constructing coherent spatial memory in LLM agents through graph rectification",
        "link": "/arxiv/2510.04195",
        "arxiv_id": "2510.04195",
        "authors": "Puzhen Zhang, Xuyang Chen, Yu Feng, Yuhan Jiang, Liqiu Meng",
        "summary": "Given a map description through global traversal navigation instructions (e.g., visiting each room sequentially with action signals such as north, west, etc.), an LLM can often infer the implicit spatial layout of the environment and answer user queries by providing a shortest path from a start to a destination (for instance, navigating from the lobby to a meeting room via the hall and elevator). However, such context-dependent querying becomes incapable as the environment grows much longer, motivating the need for incremental map construction that builds a complete topological graph from stepwise observations. We propose a framework for LLM-driven construction and map repair, designed to detect, localize, and correct structural inconsistencies in incrementally constructed navigation graphs. Central to our method is the Version Control, which records the full history of graph edits and their source observations, enabling fine-grained rollback, conflict tracing, and repair evaluation. We further introduce an Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation. To properly evaluate our approach, we create a refined version of the MANGO benchmark dataset by systematically removing non-topological actions and inherent structural conflicts, providing a cleaner testbed for LLM-driven construction and map repair. Our approach significantly improves map correctness and robustness, especially in scenarios with entangled or chained inconsistencies. Our results highlight the importance of introspective, history-aware repair mechanisms for maintaining coherent spatial memory in LLM agents.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.622039",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出一种新的方法论框架，旨在增强LLM智能体在处理复杂、增量信息时的内部状态一致性和推理能力。其核心贡献是“图修正”框架，包括“版本控制”和“边影响评分”等机制，用于检测和修正LLM在构建内部空间地图（一种内部世界模型）时产生的结构性错误。这并非将LLM作为一个简单工具应用于特定领域，而是直接作用于LLM智能体的“思考”和“记忆”过程，致力于提升其基础的、通用的推理和问题解决能力。因此，根据第一步标准，应**保留**。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文高度符合正面指标： *   **核心概念**: 明确以 \"LLM agents\" 为研究对象。 *   **能力方向**: 核心涉及 **reasoning**（检测和定位地图中的结构不一致性本身就是一种逻辑推理）、**planning**（论文目标之一是提供最短路径，这是规划的典型任务）和 **problem-solving**（整个图修正框架就是为了解决地图构建错误这一复杂问题）。 *   **新兴范式**: 论文的研究主题属于前沿的 **llm-based agents** 范畴，探讨如何让智能体具备更强大的自主认知能力。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文成功避开了所有排除标准： *   **多模态与视觉**: 论文的输入是文本形式的“导航指令”，输出是图结构，完全不涉及视觉信息。 *   **特定应用领域**: 尽管论文使用了“导航”、“空间”等听起来与机器人学相关的术语，但其研究场景是纯粹的、基于文本描述的抽象环境（如“从大厅到会议室”）。它不涉及任何真实的机器人硬件、传感器或物理世界控制，因此不属于机器人控制应用。其研究的是一种抽象的**空间推理能力**，这是一种通用认知能力，而非特定领域知识。 *   **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况。** *   **智能体/工具使用**: 这篇论文是提出一种**通用的智能体认知框架**来增强其通用问题解决能力。其核心——“图修正”和“版本控制”——是一种内省和纠错机制，这种机制可以被迁移到其他需要增量构建和修正内部知识模型的任务中。它完全符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”的准则。 *   **幻觉/可解释性/安全**: 论文实质上是在解决一种特定形式的“结构性幻觉”——LLM构建的内部地图与现实（或指令）不符。它提出了一种新方法来**减少这种内在的逻辑错误**，从而提升模型的通用可靠性和推理质量。这恰好符合“如果论文提出一种新方法来减少幻觉……从而提升模型的通用可靠性和推理质量，应该保留”的准则。 5.  **第五步：最终决策。** 综合以上分析，这篇论文的核心贡献是提出了一种方法论，通过赋予LLM智能体“历史感知”和“自修正”的能力，来显著提升其在复杂任务中的**推理鲁棒性**和**规划准确性**。它直接触及了LLM通用推理能力的核心——如何处理不一致信息、如何维护一个连贯的内部世界模型。因此，这篇论文是您研究课题“大语言模型通用推理能力”的绝佳候选，应被**保留**。"
    },
    {
        "index": "#38",
        "title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation",
        "link": "/arxiv/2510.04373",
        "arxiv_id": "2510.04373",
        "authors": "Hadi Nekoei, Aman Jaiswal, Patrice Bechard, Oleh Shliazhko, Orlando Marquez Ayala, Mathieu Reymond, Massimo Caccia, Alexandre Drouin, Sarath Chandar, Alexandre Lacoste",
        "summary": "Large language model (LLM) agents perform well in sequential decision-making tasks, but improving them on unfamiliar domains often requires costly online interactions or fine-tuning on large expert datasets. These strategies are impractical for closed-source models and expensive for open-source ones, with risks of catastrophic forgetting. Offline trajectories offer reusable knowledge, yet demonstration-based methods struggle because raw traces are long, noisy, and tied to specific tasks. We present Just-in-time Episodic Feedback Hinter (JEF Hinter), an agentic system that distills offline traces into compact, context-aware hints. A zooming mechanism highlights decisive steps in long trajectories, capturing both strategies and pitfalls. Unlike prior methods, JEF Hinter leverages both successful and failed trajectories, extracting guidance even when only failure data is available, while supporting parallelized hint generation and benchmark-independent prompting. At inference, a retriever selects relevant hints for the current state, providing targeted guidance with transparency and traceability. Experiments on MiniWoB++, WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms strong baselines, including human- and document-based hints.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.611698",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心是提出一种方法论来增强大语言模型智能体的通用推理与适应能力。我的判断过程如下： 1.  **第一步：核心判断——保留。** 论文的核心贡献是 \"Just-in-time Episodic Feedback Hinter (JEF Hinter)\"，这是一个**新的智能体系统框架**。它的目标不是将LLM应用于某个特定领域（如医疗或化学），而是解决LLM智能体在**不熟悉领域**中表现不佳的**通用性问题**。它通过一种新颖的方式（提炼离线轨迹为hints）来提升智能体的决策和适应能力，这直接属于改进LLM基础能力（特别是规划和问题解决能力）的范畴。 2.  **第二步：正面指标——高度相关。** 论文摘要中明确包含了多个关键正面指标： *   **核心概念**: \"Large language model (LLM) agents\"。 *   **能力方向**: \"sequential decision-making tasks\"（顺序决策，是推理和规划的核心形式），\"adaptation\"（适应能力）。 *   **新兴范式**: \"llm-based agents\"。 论文旨在通过提供\"strategies and pitfalls\"（策略和陷阱）的提示来引导智能体，这本质上是在增强其推理过程的鲁棒性和效率。 3.  **第三步：排除标准——不涉及。** 论文的研究内容与多模态、特定应用领域（医疗、化学等）以及模型可靠性（水印、安全）等排除标准完全无关。其使用的实验基准（MiniWoB++, WebArena）是衡量通用网页操作和决策能力的标准环境，而非特定领域。 4.  **第四步：处理特殊和模糊情况——符合保留条件。** 论文聚焦于**智能体/工具使用**。根据标准，如果提出一种**通用的智能体协作框架或工具使用方法**来增强LLM的通用问题解决能力，就应该保留。JEF Hinter正是这样一个通用框架：它是一个可以与主智能体协作的“hinter”系统，其目的是提升智能体在**任何**不熟悉任务中的表现，而不是局限于某个特定领域。它通过从成功和失败的轨迹中学习，提炼出通用的策略性知识，这是一种提升模型内在推理质量的方法。 **最终决策**: 综合以上分析，这篇论文的本质是提出一种创新的、通用的方法论（JEF Hinter），旨在通过利用离线知识来增强LLM智能体的适应性和决策能力。这直接触及了“大语言模型通用推理能力”的核心，特别是其在规划、多步推理和从经验中学习方面的能力。因此，这篇论文与您的研究目标高度契合，应该被保留。"
    },
    {
        "index": "#72",
        "title": "The Hidden Game Problem",
        "link": "/arxiv/2510.03845",
        "arxiv_id": "2510.03845",
        "authors": "Gon Buzaglo, Noah Golowich, Elad Hazan",
        "summary": "This paper investigates a class of games with large strategy spaces, motivated by challenges in AI alignment and language games. We introduce the hidden game problem, where for each player, an unknown subset of strategies consistently yields higher rewards compared to the rest. The central question is whether efficient regret minimization algorithms can be designed to discover and exploit such hidden structures, leading to equilibrium in these subgames while maintaining rationality in general. We answer this question affirmatively by developing a composition of regret minimization techniques that achieve optimal external and swap regret bounds. Our approach ensures rapid convergence to correlated equilibria in hidden subgames, leveraging the hidden game structure for improved computational efficiency.",
        "subjects": "Artificial Intelligence, Computer Science and Game Theory, Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.651508",
        "filter_reason": "这篇论文符合筛选标准，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是针对“隐藏博弈问题”提出一种新的“遗憾最小化算法”。这本质上是一种新的**训练/学习范式**。虽然论文没有直接以LLM为实验对象，但其问题背景——“AI对齐”和“语言游戏”——直指LLM的核心挑战。LLM在生成回答时，本质上就是在一个巨大的策略空间（词汇和句子的组合）中进行选择。论文中提出的“发现并利用隐藏结构以获得更高奖励”这一问题，可以被精准地映射到LLM的通用推理任务上：在众多可能的推理路径中，如何高效地找到（或学会找到）那条最优的、能得出正确答案的路径。因此，这篇论文致力于改进一个基础的、通用的学习算法，该算法可以被用来增强LLM的内在推理和规划能力，而非将LLM作为工具应用于特定领域。 2.  **第二步：正面指标** -   **核心概念**: 论文虽未直接提及\"LLM\"，但其研究的\"语言游戏\"和\"AI对齐\"是LLM研究的前沿核心议题。 -   **能力方向**: 论文的目标是“在隐藏子博弈中快速收敛到相关均衡”，这本质上是一种高级的**问题解决**和**规划**能力。它要求智能体在复杂环境中理解并利用潜在规则，这正是通用推理能力的体现。 -   **训练方法**: 论文明确提出了“遗憾最小化算法”，这是**强化学习**领域的核心技术。该方法旨在优化学习过程，使其更高效，这与通过RLHF等方法提升LLM能力的研究思路高度一致。 -   **新兴范式**: 论文的研究框架是基于“玩家”的“博弈”，这天然地契合了**多智能体系统**的范式。 3.  **第三步：排除标准** 论文完全不涉及多模态、视觉、医疗、化学等特定应用领域，也未讨论部署优化或硬件加速。它关注的是算法和理论本身，因此不属于排除范围。 4.  **第四步：处理特殊和模糊情况** -   **智能体/工具使用**: 这篇论文提出的是一个**通用的智能体协作与学习框架**（即“隐藏博弈”及其求解算法），旨在提升智能体在复杂、有隐藏结构环境中的通用问题解决能力。这完全符合“保留”的条件，它不是针对某个具体领域的应用。 **最终决策**: 综合来看，这篇论文虽然理论性较强，但其核心贡献——一种新的、高效的强化学习算法（遗憾最小化技术）——为解决LLM在通用推理中遇到的“如何在巨大可能空间中找到最优解”这一根本性问题提供了潜在的新方法论。它通过改进底层的训练和学习范式来增强模型的通用能力，与研究目标“提高大语言模型（LLM）本身的『通用推理能力』”高度契合。因此，应予以保留。"
    },
    {
        "index": "#79",
        "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information",
        "link": "/arxiv/2510.03632",
        "arxiv_id": "2510.03632",
        "authors": "Jiaxi Li, Yucheng Shi, Jin Lu, Ninghao Liu",
        "summary": "Tree search has become as a representative framework for test-time reasoning with large language models (LLMs), exemplified by methods such as Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning paths. However, it remains difficult to provide instant and reliable quantitative assessments of intermediate reasoning step quality, and extensive path exploration is computationally costly. To address this, we propose Mutual Information Tree Search (MITS), a novel framework that guides reasoning with information-theoretic principles. MITS introduces an effective scoring function based on pointwise mutual information (PMI), which enables step-wise evaluation of reasoning paths and search tree expansion via beam search without expensive look-ahead simulations, achieving superior reasoning performances while maintaining computational efficiency. The framework is complemented by an entropy-based dynamic sampling strategy that adaptively allocates computational resources to uncertain reasoning steps where exploration is most beneficial. For final prediction, MITS employs a weighted voting scheme that combines PMI scores with prediction consensus. Through comprehensive experiments on diverse reasoning benchmarks, MITS consistently surpasses baseline methods, establishing a principled and efficient framework for LLM reasoning.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.655184",
        "filter_reason": "该论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步（核心判断）：符合。** 该论文的本质是提出一种新的算法框架（MITS），用于在“测试时”增强大语言模型的推理能力。其核心是改进“推理路径”的质量评估和搜索策略，这直接触及了LLM在执行逻辑、规划、多步推理等任务时的内在能力。它不是将LLM应用于某个特定下游领域，而是致力于提升LLM作为通用推理引擎的基础表现。这与您筛选标准中“改进LLM的基础能力……增强其逻辑、数学、规划、多步推理等通用能力”的目标高度一致。 2.  **第二步（正面指标）：高度相关。** 论文明确包含了多个关键正面指标： *   **核心概念**: 论文标题和摘要多次提及“Large language models (LLMs)”。 *   **能力方向**: 核心关键词就是“reasoning”（推理），并且明确指出其方法在“diverse reasoning benchmarks”（多样化推理基准）上有效，这通常涵盖了数学、逻辑等通用推理任务。 *   **新兴范式**: 论文属于对“Tree Search”推理范式的改进，这是继“Chain-of-Thought”之后非常前沿的测试时增强推理方法。 3.  **第三步（排除标准）：不涉及。** 论文的研究焦点非常纯粹，不涉及任何排除标准中的领域。它没有讨论视觉、多模态，也没有限定在医疗、化学等特定应用场景，更没有关乎模型部署的基础设施或水印、安全等应用层面的可靠性问题。 4.  **第四步（特殊和模糊情况）：不适用。** 该论文的焦点清晰，不属于需要特别处理的智能体/工具应用或幻觉/安全等模糊情况。 **最终决策：** 综上所述，这篇论文的核心贡献是提出了一种名为MITS的、基于信息论的树搜索推理框架。它的目标是解决“如何更高效、更准确地评估和引导LLM的推理过程”这一根本性科学问题。这篇工作直接推动了LLM在**通用推理**这一核心能力上的边界，是您研究课题“大语言模型通用推理能力”的典型相关前沿论文，因此应当保留。"
    },
    {
        "index": "#81",
        "title": "Understanding the Role of Training Data in Test-Time Scaling",
        "link": "/arxiv/2510.03605",
        "arxiv_id": "2510.03605",
        "authors": "Adel Javanmard, Baharan Mirzasoleiman, Vahab Mirrokni",
        "summary": "Test-time scaling improves the reasoning capabilities of large language models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts (CoTs). This enables models to tackle more complex problem by breaking them down into additional steps, backtracking, and correcting mistakes. Despite its strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions in the training data under which long CoTs emerge, and when such long CoTs improve the performance, remain unclear. In this paper, we study the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. Our analysis provides a theoretical explanation for several intriguing observations: First, at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. Second, if the skills required to solve a downstream task are not sufficiently present in the training data, increasing test-time compute can harm performance. Finally, we characterize task hardness via the smallest eigenvalue of its feature covariance matrix and show that training on a diverse, relevant, and hard set of tasks results in best performance for test-time scaling. We confirm our findings with experiments on large, nonlinear transformer architectures.",
        "subjects": "Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.656256",
        "filter_reason": "**第一步：核心判断** 这篇论文的本质是**对“测试时扩展”这一前沿推理范式进行理论分析**，旨在揭示其有效性背后的机制。论文的核心贡献并非将LLM应用于某个特定领域，而是深入探究“为什么”以及“在什么条件下”增加测试时的计算量（通过生成长CoT）能够增强LLM的推理能力。这完全符合“改进LLM的基础能力、增强其通用推理能力”的核心目标。它属于方法论和理论机理的范畴，是研究如何让LLM本身变得更强的关键工作。 **第二步：正面指标** - **核心概念**: 论文摘要明确指出研究对象是“大语言模型”。 - **能力方向**: 论文的核心议题是“推理能力”，并详细讨论了“思维链”这一关键的推理技术。 - **训练方法**: 论文虽然不直接提出一种新的训练方法，但它深入分析了“训练数据”对测试时扩展效果的决定性作用，并讨论了“上下文示例”，这与训练和上下文学习紧密相关。 - **新兴范式**: “测试时扩展”和“长思维链”正是以OpenAI o1为代表的新兴推理范式的核心。该论文为这一范式提供了理论支撑。 **第三步：排除标准** - **多模态与视觉**: 论文不涉及任何视觉或多模态内容。**通过。** - **特定应用领域**: 论文使用的“线性回归”任务是一个理论分析载体，用于建立数学模型和推导结论，其目的是为了解释在通用LLM中观察到的现象，而非解决线性回归这一特定领域问题。**通过。** - **模型可靠性（应用层面）**: 论文不涉及水印、安全或安保等应用层面的可靠性问题。**通过。** **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 此处不适用。 - **幻觉/可解释性/安全**: 论文通过理论分析，阐明了测试时扩展在“何时”会损害性能（例如，当训练数据中缺少必要技能时），这种对模型行为边界的理论性解释，本质上是在增强我们对模型内在推理过程的理解，有助于提升其通用可靠性。因此，它符合保留标准。 **第五步：最终决策** 综合以上分析，该论文**完全符合**筛选要求。它的核心贡献是理论性地解释了当前最前沿的LLM推理增强技术（测试时扩展/长CoT）的工作原理和生效条件。这项工作直接服务于“提高大语言模型通用推理能力”这一核心目标，因为它为我们如何更有效地训练和部署具备强大推理能力的模型提供了深刻的洞见和指导原则。它不是应用研究，而是对底层机理的探索，属于本课题所寻求的高质量前沿论文。"
    },
    {
        "index": "#73",
        "title": "GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time",
        "link": "/arxiv/2510.03777",
        "arxiv_id": "2510.03777",
        "authors": "Divij Handa, Mihir Parmar, Aswin RRV, Md Nayem Uddin, Hamid Palangi, Chitta Baral",
        "summary": "Repeated Sampling (RS) is a simple inference-time algorithm that has been shown to improve model performance on complex tasks. Although it is an effective way of scaling inference time, it often struggles to generate diverse solution candidates, frequently relying on the same underlying approach to solve the problem and thus producing redundant samples. To address this limitation, we propose a new inference algorithm, GuidedSampling, which decouples the exploration and generation phases during inference, increasing diversity of generated candidate solutions. The exploration phase identifies multiple concepts that can be utilized to solve the problem, while the generation phase applies a specific concept to provide final solution candidates. We first define the theoretical bounds of GuidedSampling and then empirically demonstrate that it improves the performance of base model at pass@50 by on an average ~21.6% across various benchmarks compared to RS. Furthermore, models trained on trajectories of GuidedSampling exhibit substantial performance improvements at pass@5 by on an average ~9.7%, compared to models trained on traditional RS. Additionally, models trained with GuidedSampling increases the average number of concepts per instance (1.67 -> 3.03), yielding a diverse set of candidates than traditional RS.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.652072",
        "filter_reason": "这篇论文完全符合你的筛选要求。 **第一步：核心判断——这篇论文的本质是什么？** 论文的核心是提出一种名为`GuidedSampling`的新型**推理时算法**。其本质是通过改进模型在生成答案时的采样策略，来解决传统重复采样（Repeated Sampling）方法在面临复杂问题时“方案同质化”、“缺乏多样性”的瓶颈。这直接触及并改进了LLM在解决复杂问题时的内在推理过程——即如何探索和生成多样化的解题路径。这并非将LLM作为工具应用于特定领域，而是对LLM本身的基础问题解决能力进行增强，符合“改进LLM基础能力”和“增强其多步推理等通用能力”的要求。 **第二步：正面指标——论文是否包含以下主题？** 论文高度匹配以下正面指标： - **核心概念**: 论文的研究对象明确是`Large language models (LLMs)`。 - **能力方向**: 论文聚焦于提升模型在`complex tasks`上的表现，通过生成`diverse candidate solutions`来增强其`problem-solving`能力。这种探索多种“概念”或解题思路的方法，是通用`reasoning`能力的核心体现。 - **训练方法**: 论文不仅提出了推理时算法，还进一步探讨了**利用该算法生成的轨迹来训练模型**，从而使模型本身获得性能提升。这构成了一个完整的“改进方法-验证效果-优化模型”的闭环，与“提出新的训练范式”高度相关。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文内容完全不涉及任何排除标准。它没有提及视觉、多模态，也没有将方法限定在医疗、化学等任何特定应用领域，更不讨论模型安全、水印或基础设施优化。 **第四步：处理特殊和模糊情况** 本篇论文的情况非常清晰，不属于模糊地带。它提出的`GuidedSampling`是一种**通用的推理框架**，旨在提升LLM在各类复杂任务上的表现，而非为某个特定领域服务。这与保留“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的精神内核一致。 **第五步：最终决策** 综合以上分析，这篇《GuidedSampling》论文的核心贡献是一种创新的、旨在提升LLM通用推理与问题解决能力的方法论。它通过在推理阶段引导模型探索更广阔的解空间，并利用其产出优化模型训练，完美契合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，应予以保留。"
    },
    {
        "index": "#70",
        "title": "Algorithm Generation via Creative Ideation",
        "link": "/arxiv/2510.03851",
        "arxiv_id": "2510.03851",
        "authors": "Ruiying Ma, Chieh-Jan Mike Liang, Yanjie Gao, Francis Y. Yan",
        "summary": "Designing system algorithms remains challenging, where the discontinuous nature of the solution space often forces system engineers to rely on generic heuristics at the expense of performance. We study whether LLMs can practically drive algorithm generation, and find that they are biased towards well-known generic designs, rather than making the creative leaps needed to navigate the discontinuous solution space. To address this limitation, we introduce MetaMuse, a framework for creative ideation built on three self-reflection principles: (1) quantifying solution diversity and usefulness in measurable performance space, rather than abstract idea space, (2) steering ideation through external stimuli, rather than internal randomness, and (3) constructing executable solutions using waypoint reasoning, rather than free-form chain-of-thought. Extensive evaluation shows that MetaMuse can generate high-performing solutions for two critical problems at a global cloud provider: cache replacement (reducing cache misses by up to 35.76%) and online bin packing (reducing bin usage by up to 30.93%).",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.645297",
        "filter_reason": "这篇论文完全符合筛选要求，应予以保留。以下是我的详细判断过程： 1.  **核心判断 (第一步):** 论文的核心是解决LLM在\"算法生成\"这一通用任务上的固有缺陷——缺乏创造性、倾向于常规解。它提出的解决方案\"MetaMuse\"框架，并非将LLM应用于某个特定垂直领域，而是旨在改进LLM生成算法这一高度复杂的**推理与规划过程**。其核心贡献\"waypoint reasoning\"（路径点推理）是对现有Chain-of-Thought（CoT）推理范式的革新，旨在引导模型进行更结构化、更可靠的**多步推理**。这直接触及了提升LLM**通用推理能力**的本质，属于改进模型基础能力和提出新训练/推理范式的研究。 2.  **正面指标 (第二步):** 论文高度契合正面指标。 *   **核心概念:** 明确以\"LLMs\"为研究对象。 *   **能力方向:** \"Algorithm generation\"本身就是一种高级的**problem-solving**和**planning**能力，论文中提出的\"waypoint reasoning\"直接关联**reasoning**。 *   **新兴范式:** \"Creative ideation\"和\"self-reflection principles\"可以被看作是一种促进模型自我优化和深度思考的探索，与\"deep research\"的精神内核相符。 3.  **排除标准 (第三步):** 论文成功避开了所有排除标准。 *   **特定应用领域:** 这是需要仔细甄别的点。虽然论文在\"cache replacement\"（缓存替换）和\"online bin packing\"（在线装箱）这两个具体问题上进行了评估，但这两个问题是计算机科学领域的**经典通用算法问题**，用于算法性能的基准测试，而非生物、医疗等特定行业应用。论文的立足点是提出一种**通用的算法生成框架**，而非一个用于缓存或装箱的专用工具。因此，这不属于\"特定应用领域\"的排除范畴。 4.  **处理特殊和模糊情况 (第四步):** *   论文提出的\"waypoint reasoning\"是对\"chain-of-thought\"的精炼和改进，这可以被视为一种提升模型内在推理质量和可靠性的方法。通过将\"自由形式的思维链\"转变为更有序的\"路径点推理\"，它直接增强了模型在复杂任务上的推理鲁棒性，这与\"提升模型内在可解释性或安全性，从而提升通用推理质量\"的原则是一致的。 **最终决策:** 综合来看，这篇论文的核心贡献是\"MetaMuse\"框架及其内部的\"waypoint reasoning\"方法，其目标是解决LLM在通用高级推理任务（算法生成）中的一个根本性瓶颈（缺乏创造性、推理过程易失序）。它提出了一种新的、更结构化的推理范式来增强LLM的通用问题解决能力，尽管使用了具体的算法问题作为验证案例，但其方法论是通用且深刻的。因此，这篇论文与\"提高大语言模型本身的『通用推理能力』\"这一核心目标高度一致。"
    },
    {
        "index": "#100",
        "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts",
        "link": "/arxiv/2510.05040",
        "arxiv_id": "2510.05040",
        "authors": "Jihoon Lee, Hoyeon Moon, Kevin Zhai, Arun Kumar Chithanar, Anit Kumar Sahu, Soummya Kar, Chul Lee, Souradip Chakraborty, Amrit Singh Bedi",
        "summary": "Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn a mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, a common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), a training-free inference method that ensembles across heterogeneous block schedules. By doing a majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish a new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays a critical role in determining performance during inference.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.677199",
        "filter_reason": "这篇论文完全符合您的研究范围，核心原因在于其提出了一种全新的、通用的方法论来提升大语言模型（LLM）的推理能力，而非应用LLM解决特定领域的问题。 以下是根据您的筛选标准进行的详细判断过程： 1.  **第一步：核心判断** - 本论文的核心是提出一种**免训练的推理时方法（HEX）**，通过集成不同的文本生成路径来显著提升LLM在推理任务上的表现。这完全符合您筛选标准中“改进LLM的基础能力”、“提出新的训练范式（此处为推理范式）”、“增强其逻辑、数学、规划、多步推理等通用能力”的要求。它不属于将LLM作为工具应用于特定领域的研究，也不是关于基础设施或部署优化的工作。因此，根据第一步判断，应该**保留**。 2.  **第二步：正面指标** - 论文高度契合多个正面指标： *   **核心概念**: 论文明确聚焦于“Diffusion-based large language models (dLLMs)”，属于LLMs范畴。 *   **能力方向**: 论文的核心贡献和所有实验都围绕**推理能力** 展开，特别是在**数学推理**上取得了巨大突破（GSM8K, MATH），并在科学推理和真实性回答上也有显著提升。 *   **新兴范式**: HEX方法揭示并利用了模型内部隐含的“混合专家”特性，通过一种类似集成学习的方式在测试时增强模型，这可以被视为一种新颖的模型内在潜能挖掘范式。 3.  **第三步：排除标准** - 该论文完全避开了所有排除领域。它不涉及多模态与视觉，没有应用于医疗、化学、机器人等特定领域，其研究焦点是模型内在的推理机制，而非应用层面的水印或安全。 4.  **第四步：处理特殊和模糊情况** - 论文在TruthfulQA上的表现提升了真实性。这并非对安全的社会学研究，而是通过改进推理算法本身（多数投票避免单一错误路径），从而**提升了模型内在的可靠性和输出质量**。这正好符合“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留”的标准。 5.  **第五步：最终决策** - 论文的本质是提出了一种名为HEX的通用推理增强框架。它通过在推理阶段动态地探索和集成多种逻辑生成路径，显著改善了LLM在多项通用推理基准测试上的表现。该方法论本身与特定领域解耦，旨在提升模型最核心的逻辑、数学和问题解决能力。这与您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标高度一致。 综上所述，本论文是一项典型的、旨在增强LLM基础能力的算法研究，完全符合您的筛选要求。"
    },
    {
        "index": "#131",
        "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails",
        "link": "/arxiv/2510.04860",
        "arxiv_id": "2510.04860",
        "authors": "Siwei Han, Jiaqi Liu, Yaofeng Su, Wenbo Duan, Xinyuan Liu, Cihang Xie, Mohit Bansal, Mingyu Ding, Linjun Zhang, Huaxiu Yao",
        "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary capabilities to adapt and refine their strategies through real-world interaction, their long-term reliability becomes a critical concern. We identify the Alignment Tipping Process (ATP), a critical post-deployment risk unique to self-evolving LLM agents. Unlike training-time failures, ATP arises when continual interaction drives agents to abandon alignment constraints established during training in favor of reinforced, self-interested strategies. We formalize and analyze ATP through two complementary paradigms: Self-Interested Exploration, where repeated high-reward deviations induce individual behavioral drift, and Imitative Strategy Diffusion, where deviant behaviors spread across multi-agent systems. Building on these paradigms, we construct controllable testbeds and benchmark Qwen3-8B and Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode rapidly under self-evolution, with initially aligned models converging toward unaligned states. In multi-agent settings, successful violations diffuse quickly, leading to collective misalignment. Moreover, current reinforcement learning-based alignment methods provide only fragile defenses against alignment tipping. Together, these findings demonstrate that alignment of LLM agents is not a static property but a fragile and dynamic one, vulnerable to feedback-driven decay during deployment. Our data and code are available at https://github.com/aiming-lab/ATP.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.715273",
        "filter_reason": "这篇论文完全符合你的研究范围，理由如下： 1.  **核心判断通过**：论文的本质并非将LLM应用于特定领域，而是对一种新兴的LLM范式——“自我进化的LLM智能体”——进行基础性研究。它深入探讨了这种范式在部署后出现的动态、系统性风险，这直接关系到LLM基础能力的稳定性和可靠性。核心贡献是识别并形式化了“对齐倾覆过程”（ATP），这是一个关于LLM智能体在自我进化中如何失效的理论框架，属于对LLM基础能力边界的探索。 2.  **正面指标高度匹配**：论文命中了几乎所有关键正面指标。 *   **核心概念**: 明确聚焦于 \"Large Language Model (LLM) agents\"。 *   **能力方向**: 研究了智能体的 \"strategies\" 和 \"behaviors\"，这属于广义的推理与规划范畴。一个无法保持对齐的智能体，其长期问题解决和推理能力是不可靠的。 *   **训练方法**: 核心议题是 \"self-evolution\" 和 \"reinforcement learning-based alignment methods\"。 *   **新兴范式**: 论文的核心研究对象就是 \"llm-based agents\" 和 \"multi-agent systems\"。 3.  **排除标准不适用**：论文不涉及多模态、视觉，也非针对医疗、化学等特定领域的应用。 4.  **特殊情况的精准处理**： *   **智能体/工具使用**: 论文提出的是一个通用的分析框架，用于理解自我进化智能体的行为漂移，而非应用于特定领域。这完全符合保留标准。 *   **安全/可靠性**: 论文虽然涉及“安全”，但并非应用层面的水印或策略讨论。它提出了一种新的理论（ATP）和实验方法来揭示“对齐”这一安全属性在动态环境下的脆弱性。这直接呼应了第四步的保留条件：“提出一种新方法来...提升模型的通用可靠性和推理质量”。理解对齐为何会失效，是未来设计出更可靠、推理能力更稳健的LLM智能体的关键前提。 **核心依据**：你的目标是“提高大语言模型（LLM）本身的『通用推理能力』”。一个能够自我进化的智能体，其通用推理能力的上限非常高，但前提是它能保持稳定和对齐。这篇论文恰恰指出了这一前沿范式中最核心的脆弱性——对齐的动态衰减。它没有直接提出一个“更强的推理算法”，但它深刻地揭示了当前自我进化范式下，导致推理能力（以可靠策略的形式体现）劣化的根本原因。这种对根本性风险的洞察和形式化，是推动该领域走向更可靠、更强通用推理能力不可或缺的一环。因此，这篇论文是极具价值的前沿研究，完全符合你的筛选要求。"
    },
    {
        "index": "#140",
        "title": "Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning",
        "link": "/arxiv/2510.04786",
        "arxiv_id": "2510.04786",
        "authors": "Jonas Hübotter, Leander Diaz-Bone, Ido Hakimi, Andreas Krause, Moritz Hardt",
        "summary": "Humans are good at learning on the job: We learn how to solve the tasks we face as we go along. Can a model do the same? We propose an agent that assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task. The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from a large pool of available training data. Our experiments demonstrate that reinforcement learning on a test-time curriculum consistently improves the model on its target tasks, across a variety of evaluations and models. Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%. Our findings show the potential of test-time curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.725386",
        "filter_reason": "这篇论文完全符合你的研究范围，是一篇应该被保留的高质量论文。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM基础能力** 论文的核心贡献是提出了一种名为“测试时课程表”的新训练范式。这个范式的本质不是把LLM当作工具去解决某个特定领域的问题，而是**通过在测试时使用强化学习（RL）对模型进行持续训练，来从方法论层面提升LLM自身的能力**。这直接命中了你筛选标准中“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。它研究的是“如何让模型在遇到任务时能学会学习”，这是一个基础能力的增强。 2.  **第二步：正面指标——高度匹配关键主题** -   **核心概念**: 论文的研究对象是Qwen3-8B等大语言模型。 -   **能力方向**: 论文的评估基准是AIME25（高难度数学竞赛）和CodeElo（编程），这正是衡量**数学推理**和**问题解决**能力的核心指标。论文的目标就是提升模型在这些任务上的表现。 -   **训练方法**: **强化学习**是TTC-RL范式的核心组成部分。 -   **新兴范式**: 论文提出的“测试时课程表”以及“测试时扩展”的理念，属于当前LLM前沿研究中非常受关注的提升推理能力的新兴范式。 3.  **第三步：排除标准——完全规避** 论文的研究内容完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施、水印/安全等排除项。它聚焦于纯文本语言模型的通用能力提升。 4.  **第四步：特殊情况——不涉及排除项** 论文不涉及智能体在特定领域的应用，也不对可解释性/安全等进行社会学层面的讨论。它提出的TTC-RL方法，其核心是通过减少在特定任务上的推理错误来提升模型的质量，这与提升通用推理可靠性的目标是一致的。 5.  **第五步：最终决策** 综合所有分析，这篇论文的**核心是提出一种全新的、通用的训练范式（TTC-RL）**，旨在通过在测试时的动态学习和强化，来显著提升大语言模型在数学和编码等核心推理任务上的表现。它完全符合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标，并且其方法论具有创新性和前沿性。因此，这篇论文**必须保留**。"
    },
    {
        "index": "#165",
        "title": "A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents",
        "link": "/arxiv/2510.04607",
        "arxiv_id": "2510.04607",
        "authors": "Yuan Wang, Mingyu Li, Haibo Chen",
        "summary": "Computer-use agents (CUAs) powered by large language models (LLMs) have emerged as a promising approach to automating computer tasks, yet they struggle with graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to decompose high-level goals into lengthy, error-prone sequences of fine-grained actions, resulting in low success rates and an excessive number of LLM calls. We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms existing GUIs into three declarative primitives: access, state, and observation, which are better suited for LLMs. Our key idea is policy-mechanism separation: LLMs focus on high-level semantic planning (policy) while GOI handles low-level navigation and interaction (mechanism). GOI does not require modifying the application source code or relying on application programming interfaces (APIs). We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on Windows. Compared to a leading GUI-based agent baseline, GOI improves task success rates by 67% and reduces interaction steps by 43.5%. Notably, GOI completes over 61% of successful tasks with a single LLM call.",
        "subjects": "Operating Systems, Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.754787",
        "filter_reason": "这篇论文符合研究范围。我的判断过程如下： 1.  **第一步：核心判断** 这篇论文的**本质**是提出一种新的方法论，即“面向目标的接口”（GOI），以提升基于LLM的“计算机使用智能体”的效率和成功率。其核心贡献是“策略-机制分离”：让LLM专注于高层的**语义规划和决策（即“推理”和“规划”能力）**，而将底层的、繁琐的GUI交互操作（即“机制”）外包给GOI抽象层。这并没有改变LLM内部的权重或结构，但它通过优化LLM与外部环境的交互范式，**极大地释放和增强了LLM在多步任务规划与问题解决方面的潜能**。因此，它的核心是关于改进LLM在通用任务中的表现，而非将其应用于特定领域。 2.  **第二步：正面指标匹配** 论文完美地匹配了多个正面指标： *   **核心概念**: 明确聚焦于 \"Large language models (LLMs)\"。 *   **能力方向**: 核心是解决LLM在\"planning\"（规划）和\"problem-solving\"（问题解决）上的困难。通过让LLM专注于\"high-level semantic planning\"，直接提升其通用推理能力。 *   **新兴范式**: 论文完全围绕 \"llm-based agents\" 展开，并提出了一种新颖的 \"tool use\" 形式——将GUI应用转化为LLM友好的声明式接口工具。这是一种增强智能体通用能力的框架性研究。 3.  **第三步：排除标准排除** *   **多模态与视觉**: 虽然论文涉及GUI，但其目的不是让LLM去“看”或理解视觉内容，恰恰相反，是为了**绕过**视觉界面的复杂性，将其抽象成纯文本或结构化的数据。因此，它不属于视觉或多模态研究。 *   **特定应用领域**: 尽管在Microsoft Office上进行了评估，但这只是作为一个普遍存在的GUI应用的**实例**。论文提出的GOI是一个**通用框架**，可以应用于\"existing GUIs\"，而非特定于办公、医疗或化学领域。其目标是解决“计算机任务自动化”这个通用问题。 4.  **第四步：特殊情况处理** 本论文是关于智能体/工具使用的典型案例。它提出了一种**通用的智能体交互框架（GOI）**来增强LLM的**通用问题解决能力**（自动化任意计算机任务），完全符合筛选标准中“应该保留”的情况。它不是“用于XX领域的智能体”，而是“一种让智能体在XX（GUI）场景下工作得更好的通用方法”。 5.  **最终决策** 综合以上分析，这篇论文提出了一种创新的外部系统设计，通过将LLM从繁琐的低阶操作中解放出来，使其能够更纯粹地发挥其高层规划和推理的优势。这直接提升了LLM作为通用问题解决器在复杂、多步任务中的表现，其本质是增强LLM的**通用推理和规划能力**。因此，它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。"
    },
    {
        "index": "#232",
        "title": "On the Limitations and Capabilities of Position Embeddings for Length Generalization",
        "link": "/arxiv/2510.04130",
        "arxiv_id": "2510.04130",
        "authors": "Yang Chen, Yitao Liang, Zhouchen Lin",
        "summary": "In Transformers, Position Embeddings (PEs) significantly influence Length Generalization (LG) performance, yet their fundamental role remains unclear. In this work, we investigate the limitations and capabilities of PEs in achieving LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs), introducing Linear Representation Complexity (LRC) to characterize when PEs enable LG. Our analysis shows that PEs do not expand computational capabilities but structure learned computations across positions. Extending to practical Transformers, we propose Sequential Representation Complexity (SRC) and conjecture that LG is possible if and only if SRC remains invariant across scales. We support this hypothesis with empirical evidence in various reasoning tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance scaling, and a Learning-Based Position Embedding framework that automatically learns positional relations. Our work provides theoretical insights and practical strategies for improving LG in Transformers.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.828661",
        "filter_reason": "这篇论文符合您的筛选标准。 1.  **核心判断（第一步）**: 这篇论文的本质是研究并解决大型语言模型（基于Transformer架构）在处理长序列时的泛化能力。它没有将LLM作为工具应用于特定领域，而是深入到模型的核心组件——位置嵌入，从理论和实践两个层面进行改进。论文的核心贡献是提出了新的理论框架（LRC, SRC）来解释长度泛化，并设计了新的方法（Scale Hint, Learning-Based PE）来**增强模型本身在更长序列下的表现能力**。 2.  **与核心目标的关联**：长度泛化能力是LLM进行复杂推理（尤其是需要多步骤、长链条的数学推理和逻辑推理）的**基础和先决条件**。如果一个模型无法保持对长上下文信息的连贯理解，那么它执行复杂的多步推理任务（如解决复杂的数学问题、编写长代码、进行复杂的规划）的能力将大打折扣。因此，这篇论文通过提升模型的长度泛化能力，直接为其通用推理能力的增强铺平了道路。摘要中明确提到，其假设和贡献在“各种推理任务”上得到了经验证据的支持。 3.  **正面指标（第二步）**: 论文的核心主题与推理能力紧密相关。摘要直接提到了“reasoning tasks”。虽然它没有直接提及RLHF或Agent，但它通过改进最底层的架构组件来提升模型的基础能力，这比一些应用层面的Agent研究更为根本。 4.  **排除标准（第三步）**: 该论文完全不涉及多模态、特定应用领域（医疗、化学等）或应用层面的可靠性问题（如水印、安全）。它聚焦于LLM的基础架构和通用能力，因此成功避开了所有排除项。 **总结**: 该论文通过对Transformer中位置嵌入的理论分析和创新，致力于解决长度泛化这一关键瓶颈问题。由于长度泛化是实现稳健、复杂通用推理的基石，因此这项工作本质上是在提升LLM的通用推理能力。它完全符合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。"
    },
    {
        "index": "#240",
        "title": "A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling",
        "link": "/arxiv/2510.04087",
        "arxiv_id": "2510.04087",
        "authors": "Hyung Gyu Rho",
        "summary": "Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is \\textit{better}, but what is \\textit{good enough}. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.",
        "subjects": "Methodology, Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.838071",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于提升大语言模型（LLM）在推理过程中的内在可靠性和效率，从而增强其通用问题解决能力。 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一种新的方法论来改进LLM的基础能力。它没有将LLM作为工具应用于特定领域，而是聚焦于优化LLM的核心技术环节——偏好对齐和推理时采样策略。论文的核心是改进“奖励模型”和“Best-of-N采样”这一通用推理范式，旨在让模型不仅能判断“哪个更好”，更能判断“哪个足够好”，从而从根本上提升模型输出的质量和可靠性。这直接关系到LLM的通用推理能力。 2.  **第二步：正面指标** - 论文紧密围绕LLM的核心技术展开，涉及**奖励模型**（与强化学习RLHF强相关）和**Best-of-N采样**（一种重要的推理增强技术）。其目标是提升模型在处理**困难提示**时的表现，这直接关联到**问题解决**和**推理**能力。因此，它满足多个正面指标。 3.  **第三步：排除标准** - 论文完全不涉及多模态、特定应用领域（如医疗、化学）或模型基础设施。虽然提到了“reliability”，但其关注点是模型输出的内在质量，而非水印、安全等应用层面的防护措施。因此，它不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - 论文对“可靠性”的探讨，属于“提出一种新方法来减少幻觉、增强模型内在的可靠性，从而提升模型的通用可靠性和推理质量”的范畴。它通过改进奖励模型，使模型能够拒绝生成“不可接受的选项”，这直接减少了低质量、不可靠输出的概率，是提升通用推理质量的关键一步。 **核心依据总结**: 该论文的核心贡献是提出了一种带有“外部选项”的上下文质量奖励模型，以及基于此的自适应推理策略。这是一种通用的、方法论层面的创新，旨在解决当前LLM在复杂推理任务中可能选择“最不差的坏选项”的根本缺陷。通过让模型学会识别“足够好”的答案，它直接提升了LLM在通用问题解决场景下的输出可靠性和推理质量，完全契合您“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。"
    },
    {
        "index": "#249",
        "title": "The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View",
        "link": "/arxiv/2510.04028",
        "arxiv_id": "2510.04028",
        "authors": "Xinhao Yao, Lu Yu, Xiaolin Hu, Fengwei Teng, Qing Cui, Jun Zhou, Yong Liu",
        "summary": "The ongoing debate on whether reinforcement learning with verifiable rewards (RLVR) expands or shrinks the reasoning capabilities of large language models (LLMs) remains unresolved. Some studies contend that RLVR mainly improves sampling efficiency but at the expense of diversity and exploratory capacity, resulting in capability boundary shrinkage. In contrast, others demonstrate that prolonged training can lead to the emergence of novel reasoning strategies, suggesting capability boundary expansion. To reconcile these contradictory findings, we theoretically and empirically show that both perspectives are partially valid-each aligning with a separate phase in an inherent two-stage probability mass dynamic: (1) Exploitation stage: initially, the model primarily samples explored high-reward and low-reward tokens, while rarely selecting the potentially optimal token. Positive advantage estimates increase the probability of high-reward tokens and decrease those of low-reward tokens, yet the optimal token's probability remains largely unchanged during this stage. (2) Exploration stage: as training advances, the growth rate of previously acquired high-reward tokens slows as their probabilities approach saturation. When a potentially optimal token-now receiving positive advantage estimates-is occasionally sampled, its probability increases, while those of the originally high-reward tokens decrease. This dynamic suggests that over-exploitation during the exploitation stage may lead to capability boundary shrinkage, whereas prolonged training into the exploration stage can promote an expansion of the reasoning capability boundary. Building upon our insights, we revisit the potential of only using relative negative gradients for prolonging training, providing a theoretical and empirical foundation for the development of more advanced reasoning capabilities.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.848289",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。以下是我的详细判断过程： 1.  **第一步：核心判断——论文的本质是改进LLM的通用推理能力。** 论文的核心并非将LLM应用于某个特定领域，而是深入探讨一种训练方法——强化学习与可验证奖励（RLVR）——对LLM自身“推理能力边界”的影响。它旨在解决一个关于RLVR是会“收缩”还是“扩张”模型推理能力的核心学术争论。论文提出的“两阶段动态模型”是对LLM训练过程的理论性洞察，其最终目的是为“开发更先进的推理能力”提供理论和实证基础。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的核心要求。 2.  **第二步：正面指标——论文高度相关。** - **核心概念**: 论文的研究对象是“大语言模型”。 - **能力方向**: 论文的标题和摘要反复强调“推理能力边界”、“新颖的推理策略”和“更先进的推理能力”，与筛选标准中的“reasoning”高度契合。 - **训练方法**: 论文的核心是分析“强化学习”，这是筛选标准中明确列出的关键训练方法。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容是纯粹的、通用的LLM推理能力训练机制，与多模态、视觉、医疗、化学、机器人等特定应用领域完全无关。同时，它也不涉及模型基础设施、部署优化或水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况——不适用。** 论文虽然不直接讨论智能体或工具使用，但它所研究的RLVR训练方法是提升LLM自主解决问题能力的关键技术，是构建高级智能体和实现有效工具使用的底层能力之一。因此，其研究内容与提升通用问题解决能力的目标是一致的。 **最终决策**: 综合以上分析，这篇论文的本质是通过对一种强化学习训练范式（RLVR）进行深入的理论和实证分析，来揭示并指导如何更好地扩展大语言模型的通用推理能力边界。它直接回应了“如何提高LLM本身的通用推理能力”这一核心研究目标，提供了基础性的方法论洞见。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#259",
        "title": "Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data",
        "link": "/arxiv/2510.03988",
        "arxiv_id": "2510.03988",
        "authors": "Hoang Anh Just, Myeongseob Ko, Ruoxi Jia",
        "summary": "Distilling long reasoning traces (10K+ tokens) from stronger teacher models into smaller student LLMs via SFT has emerged as a standard paradigm. This approach is practical and efficient: it leverages the ease of generating abundant reasoning data from stronger models and provides a direct, data-driven way to teach less capable models better reasoning. While previous work has largely focused on prompt selection with responses from a single teacher, the equally important problem of choosing the best response when multiple teacher outputs are available for a single prompt remains underexplored. This challenge becomes important in a multi-teacher setting, where different students may benefit from the outputs of different teachers. This paper fills that gap with a systematic study of response selection for reasoning distillation. We first show that the current method, which picks responses the student assigns the highest global log-probability (global naturalness), fails when responses come from multiple teachers, i.e., global naturalness no longer correlates with downstream performance, especially as the reasoning traces from strong teachers become longer. To overcome this problem, we introduce Local Naturalness, which measures the student's log-probabilities over short, sequential reasoning steps conditioned only on a small local window. Local Naturalness enables two applications: 1) Teacher Selection: Aggregating local scores across prompts reliably identifies the most helpful teacher. 2) Response Selection from a Multiple Teachers: When mixing answers from many teachers, Local Naturalness boosts a 32B student's accuracy on math benchmarks by 9.4pp over global selection, also surpassing the performance achieved by training on data from the single best teacher. These results highlight the power of localized data quality evaluation and data mixing for more effective reasoning distillation.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.858725",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心是研究如何通过一种更优化的“知识蒸馏”方法，来提升小规模学生大语言模型（LLM）的推理能力。它提出了一种名为“局部自然度”的新指标，用于在多个教师模型生成的推理轨迹中，筛选出最适合学生模型学习的数据。 - **符合保留标准**: 这项工作直接致力于改进LLM的基础能力——推理。它提出了一种新的训练范式（优化的蒸馏数据选择方法），旨在增强模型的数学和多步推理能力。这并非将LLM作为工具应用于特定领域，而是从根本上提升模型本身的通用能力。 2.  **第二步：正面指标** - **核心概念**: 论文明确聚焦于 \"Large language models, LLMs\"。 - **能力方向**: 论文的核心主题是 \"reasoning\"，特别是 \"math reasoning\"，并通过在数学基准测试上的提升来验证其有效性。这直接命中了您关注的核心能力。 - **训练方法**: 虽然不是强化学习，但“知识蒸馏”是提升模型能力的关键训练范式之一。本文对该范式进行了创新性的改进，属于方法论研究。 3.  **第三步：排除标准** - **多模态与视觉**: 论文完全不涉及视觉或多模态内容，是纯文本模型的研究。 - **特定应用领域**: 论文虽然在数学基准上测试，但数学推理被视为衡量通用智能和逻辑推理能力的核心基础，而非像医疗、化学那样的特定垂直领域。其目标是提升模型的通用推理能力，而非解决某个特定领域的应用问题。 - **模型可靠性**: 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** - 本文不涉及智能体/工具使用或幻觉/安全等特殊情况，因此无需特殊判断。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种创新的方法论（局部自然度），用于优化LLM的训练过程（推理蒸馏），从而直接提升了LLM的通用推理能力。它完全符合您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标。因此，应予以保留。"
    },
    {
        "index": "#260",
        "title": "What Can You Do When You Have Zero Rewards During RL?",
        "link": "/arxiv/2510.03971",
        "arxiv_id": "2510.03971",
        "authors": "Jatin Prakash, Anirudh Buvanesh",
        "summary": "Reinforcement learning (RL) with outcome-based rewards has proven effective for improving large language models (LLMs) on complex reasoning tasks. However, its success often depends on the base model occasionally sampling correct solutions. When no correct solutions are sampled, training encounters a zero-reward barrier where learning stalls due to zero gradients. We study this scenario through the graph search task introduced in Bachmann et al. (2024) and evaluate recent methods that incorporate desirable components such as dense rewards, diversity incentives, and improved credit assignment. Our experiments show that none of these approaches overcome the zero-reward barrier if the base model never produces a correct answer. In contrast, we find that a simple data-centric intervention of adding easier samples to the training set enables the model to eventually solve the original hard task despite starting from zero reward. Importantly, this succeeds without modifying the RL algorithm itself. Because official implementations of several baselines were unavailable, we developed our own, which allowed us to conduct a detailed analysis of their failure modes. We release these implementations to support further research at: https://github.com/rl4reasoning/rl-baselines",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.859221",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心是研究并解决一个在“使用强化学习（RL）提升大语言模型（LLM）复杂推理能力”过程中遇到的关键瓶颈——“零奖励障碍”。它没有将LLM作为工具应用于特定领域，而是深入探讨了训练方法论本身。论文提出了一种“以数据为中心”的干预方法（添加更简单的样本），使得RL训练能够成功，从而让模型最终掌握解决困难推理任务的能力。 - **符合性**: 这完全符合“改进LLM的基础能力”和“提出新的训练范式”的标准。其目标是让LLM本身变得更强，而不是用它去做别的事情。 2.  **第二步：正面指标** - **核心概念**: 论文明确以 \"large language models (LLMs)\" 为研究对象。 - **能力方向**: 论文聚焦于 \"complex reasoning tasks\"，这正是您关注的核心能力。 - **训练方法**: 论文的核心方法论是 \"Reinforcement learning (RL)\"，并探讨了如何优化这一过程。 - **新兴范式**: 虽然没有直接提及Agent，但使用RL优化LLM推理能力是当前LLM-based Agent和高级推理能力研究中的关键技术路径。 - **符合性**: 论文命中了所有关键的正面指标，表明其与您的研究课题高度相关。 3.  **第三步：排除标准** - **多模态与视觉**: 论文未涉及任何视觉或多模态内容。 - **特定应用领域**: 论文的实验任务是一个通用的“图搜索任务”，属于经典的推理问题，而非医疗、化学等特定领域。 - **模型可靠性（应用层面）**: 论文关注的是训练过程中的梯度消失问题，而非水印、安全等应用层面的可靠性问题。 - **符合性**: 论文完全避开了所有排除标准。 4.  **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊情况，其焦点非常清晰，即RL训练中的技术难题。 5.  **第五步：最终决策** - **综合分析**: 这篇论文是一篇典型的、致力于提升LLM通用推理能力的方法论研究。它识别了一个现有训练范式（RL for reasoning）中的根本性缺陷，并提出了一种有效的、不改变算法本身的解决方案。这项工作直接推动了如何让LLM通过训练获得更强推理能力这一核心目标，是您研究课题下的前沿和高质量文献。 **核心依据**: 论文的贡献在于解决了“如何让RL在LLM推理训练中有效工作”这一基础性问题，其本质是增强模型内在的推理能力，而非外部应用。因此，它完全符合您的筛选要求。"
    },
    {
        "index": "#357",
        "title": "Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis",
        "link": "/arxiv/2510.03366",
        "arxiv_id": "2510.03366",
        "authors": "Harshwardhan Fartale, Ashish Kattamuri, Rahul Raja, Arpita Vats, Ishita Prasad, Akshata Kishore Moharir",
        "summary": "Transformer-based language models excel at both recall (retrieving memorized facts) and reasoning (performing multi-step inference), but whether these abilities rely on distinct internal mechanisms remains unclear. Distinguishing recall from reasoning is crucial for predicting model generalization, designing targeted evaluations, and building safer interventions that affect one ability without disrupting the other.We approach this question through mechanistic interpretability, using controlled datasets of synthetic linguistic puzzles to probe transformer models at the layer, head, and neuron level. Our pipeline combines activation patching and structured ablations to causally measure component contributions to each task type. Across two model families (Qwen and LLaMA), we find that interventions on distinct layers and attention heads lead to selective impairments: disabling identified \"recall circuits\" reduces fact-retrieval accuracy by up to 15\\% while leaving reasoning intact, whereas disabling \"reasoning circuits\" reduces multi-step inference by a comparable margin. At the neuron level, we observe task-specific firing patterns, though these effects are less robust, consistent with neuronal polysemanticity.Our results provide the first causal evidence that recall and reasoning rely on separable but interacting circuits in transformer models. These findings advance mechanistic interpretability by linking circuit-level structure to functional specialization and demonstrate how controlled datasets and causal interventions can yield mechanistic insights into model cognition, informing safer deployment of large language models.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.952303",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于对大语言模型的通用推理能力进行深度的、机制层面的解构与分析。 1.  **核心判断（第一步）：** 论文的本质是探究LLM内部机制，而非应用。它没有将LLM作为工具去解决某个特定领域的问题，而是聚焦于LLM最核心的能力之一——“推理”。论文通过区分“记忆”和“推理”这两种基础能力，并试图在模型内部找到它们各自对应的“电路”，这直接关系到如何理解和改进LLM的通用推理能力。这种基础性的、机理性的研究，是提升模型能力的前提和关键，因此完全符合“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的核心目标。 2.  **正面指标（第二步）：** 论文明确包含了多个关键正面指标。 *   **核心概念:** 研究对象是Transformer-based language models，即LLMs。 *   **能力方向:** 论文的标题和摘要反复强调核心研究对象是 **reasoning** (推理) 和 **multi-step inference** (多步推理)，这正是你关注的核心。 3.  **排除标准（第三步）：** 论文没有触及任何排除标准。它不涉及多模态、特定应用领域（如医疗、化学），也不是关于模型基础设施或应用层面的水印、安全等问题。 4.  **特殊和模糊情况（第四步）：** 这篇论文是“可解释性”研究的优秀范例。根据你的标准，“如果论文提出一种新方法来……增强模型内在的可解释性……从而提升模型的通用可靠性和推理质量，应该保留。” 该论文通过“激活修补”和“结构化消融”等因果干预方法，首次提供了“记忆”和“推理”依赖于可分离但相互作用的“电路”的因果证据。这种对模型内在工作机制的揭示，极大地增强了我们对模型推理能力的内在可解释性，为未来如何针对性地优化这些“推理电路”以提升模型整体推理质量指明了方向。 **总结：** 该论文的核心贡献是，通过严谨的因果分析方法，在LLM内部识别出了专门负责“推理”功能的模块（reasoning circuits）。这项工作并非提出一种新的训练技巧，而是从根本上回答了“LLM的推理能力是如何实现的”这一关键科学问题。对于致力于“提高大语言模型通用推理能力”的研究者来说，理解其内部机制是进行有效改进的第一步，也是最关键的一步。因此，这篇论文不仅符合，而且是高度相关且极具价值的前沿研究。"
    },
    {
        "index": "#360",
        "title": "Provenance Networks: End-to-End Exemplar-Based Explainability",
        "link": "/arxiv/2510.03361",
        "arxiv_id": "2510.03361",
        "authors": "Ali Kayyam, Anusha Madan Gopal, M. Anthony Lewis",
        "summary": "We introduce provenance networks, a novel class of neural models designed to provide end-to-end, training-data-driven explainability. Unlike conventional post-hoc methods, provenance networks learn to link each prediction directly to its supporting training examples as part of the model's normal operation, embedding interpretability into the architecture itself. Conceptually, the model operates similarly to a learned KNN, where each output is justified by concrete exemplars weighted by relevance in the feature space. This approach facilitates systematic investigations of the trade-off between memorization and generalization, enables verification of whether a given input was included in the training set, aids in the detection of mislabeled or anomalous data points, enhances resilience to input perturbations, and supports the identification of similar inputs contributing to the generation of a new data point. By jointly optimizing the primary task and the explainability objective, provenance networks offer insights into model behavior that traditional deep networks cannot provide. While the model introduces additional computational cost and currently scales to moderately sized datasets, it provides a complementary approach to existing explainability techniques. In particular, it addresses critical challenges in modern deep learning, including model opaqueness, hallucination, and the assignment of credit to data contributors, thereby improving transparency, robustness, and trustworthiness in neural models.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:54.953411",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“溯源网络”的新型神经网络架构。这种架构并非将模型应用于某个特定领域，而是旨在从根本上改变模型的工作方式，使其在做出预测时能够直接追溯到支持该预测的训练样本。这是一种对模型基础能力的改进，属于方法论层面的创新。 2.  **第二步与第四步：正面指标与特殊情况分析** 尽管论文标题和摘要没有直接提及“LLM”或“Reasoning”，但其核心贡献与我的研究目标高度相关，关键在于它对“幻觉”问题的处理。 *   摘要中明确指出，该模型“addresses critical challenges in modern deep learning, including model opaqueness, **hallucination**...”。 *   幻觉是当前大语言模型在执行复杂推理任务时面临的核心挑战之一，它直接导致了模型输出的不可靠和不准确。一个频繁产生幻觉的模型，其推理能力是无法被信任的。 *   根据第四步的筛选标准：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性...从而提升模型的通用可靠性和推理质量，应该保留。” 这篇论文正是如此。它通过一种全新的、内建于模型架构的可解释性方法来对抗幻觉，这直接提升了模型的通用可靠性和推理输出的质量。 3.  **第三步：排除标准** 论文完全不涉及多模态、特定应用领域（如医疗、化学）或应用层面的可靠性问题（如水印）。它提出的是一种通用的神经网络模型架构，因此没有被任何排除标准命中。 **最终决策**： 该论文的本质是提出一种新的模型架构，通过增强内在可解释性来解决深度学习模型（尤其是LLM）的幻觉问题。由于减少幻觉是提升LLM通用推理能力和可靠性的关键前提，这篇论文为我的核心研究目标“提高大语言模型本身的通用推理能力”提供了重要的、基础性的方法论支持。因此，它完全符合筛选要求。"
    },
    {
        "index": "#399",
        "title": "CoDA: Coding LM via Diffusion Adaptation",
        "link": "/arxiv/2510.03270",
        "arxiv_id": "2510.03270",
        "authors": "Haolin Chen, Shiyu Wang, Can Qin, Bo Pang, Zuxin Liu, Jielin Qiu, Jianguo Zhang, Yingbo Zhou, Zeyuan Chen, Ran Xu, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang, Weiran Yao",
        "summary": "Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:55.000344",
        "filter_reason": "这篇论文符合我的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种新的模型架构和训练范式（CoDA），旨在提升模型在代码生成任务上的表现。代码生成，尤其是在HumanEval这类基准测试上的表现，被广泛认为是衡量大语言模型逻辑推理、算法规划和多步问题解决能力的核心指标。它本质上不是将LLM应用于某个外部领域（如化学或金融），而是**在改进LLM本身的基础推理能力**。论文提出的“扩散自适应”训练方法，直接作用于模型的能力构建，这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划、多步推理等通用能力”的要求。 2.  **第二步：正面指标** 论文与多个正面指标高度相关： - **核心概念**: 论文研究对象是“Coding LM”，属于大语言模型（LLMs）的范畴。 - **能力方向**: 代码生成是**逻辑推理** 和 **问题解决** 的典型体现。论文通过在Humaneval, MBPP等数据集上进行评估，直接衡量了模型的这项能力。 - **训练方法**: 论文提出了“扩散预训练 + 代码为中心的中等训练 + 指令微调”这一新的训练组合范式，这属于对训练方法的创新。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： - **多模态与视觉**: 论文专注于纯文本代码，不涉及任何视觉或多模态内容。 - **特定应用领域**: 虽然主题是“代码”，但在人工智能研究中，代码能力通常被视为一种**通用推理能力的“试金石”**，而非像医疗、法律那样的垂直领域应用。提升模型的代码能力，本质上是在提升其底层的逻辑和规划能力。 - **模型可靠性（应用层面）**: 论文关注的是模型的核心性能（推理准确性），而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况。唯一需要辨析的是“代码”是否算作特定领域。如上所述，在此研究背景下，提升代码生成能力与提升数学推理能力类似，都是为了增强模型的通用、底层的认知和推理能力，而不是赋予其特定的领域知识。因此，应将其视为通用能力的一部分。 5.  **第五步：最终决策** 综上所述，这篇论文通过提出一种新的扩散模型架构和训练方法，直接致力于提升LLM在代码生成这一核心推理任务上的表现。其研究目标、方法和贡献都与“提高大语言模型本身的『通用推理能力』”这一核心目标高度一致。 因此，最终判断为 **True**。"
    },
    {
        "index": "#408",
        "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning",
        "link": "/arxiv/2510.03259",
        "arxiv_id": "2510.03259",
        "authors": "Yoonjeon Kim, Doohyuk Jang, Eunho Yang",
        "summary": "Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by itself. We argue that large reasoning models lack this meta-awareness property by proving severe misalignment between true rollouts and predicted meta information. We posit that aligning meta-prediction with true rollouts will lead to significant performance gains. To verify this hypothesis, we design a training pipeline that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced meta-awareness directly translates to improved accuracy. Unlike existing meta-cognitive reasoning models, our method does not require external training sources but leverages self-generated signals to train meta-awareness. Moreover, our method enables efficient training by i) filtering out zero-variance prompts that are either trivial or unsolvable and ii) cutting off lengthy rollouts when they are unlikely to lead to correct answers. The results are inspiring: our strategy yields significant improvements in both accuracy and training efficiency on in-domain tasks and shows strong generalization to out-of-domain benchmarks. More specifically, our method can speed up GRPO training by over 1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 % boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks spanning logical, scientific, and coding domains.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:55.005036",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据您的筛选标准进行的详细判断过程： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出一种新的训练范式（MASA），旨在通过增强大语言模型的“元认知”（meta-awareness）能力来提升其通用推理能力。论文的核心贡献是解决推理模型在“元信息预测”与“真实推理轨迹”之间的错位问题，通过自我对齐的强化学习方法，让模型更好地“知道如何思考”。这直接触及了LLM基础能力的改进，属于提升逻辑、数学等通用推理能力的方法论研究，完全符合保留标准。 **第二步：正面指标——论文是否包含以下主题？** 该论文高度契合多个正面指标： - **核心概念**: 论文研究对象是“large reasoning models”，属于LLMs范畴。 - **能力方向**: 论文明确聚焦于“reasoning”能力，并在多个数学（AIME25, six mathematics benchmarks）、逻辑（GPQA-Diamond）和科学推理的通用基准上验证了其有效性。 - **训练方法**: 论文的核心是“Self-Alignment Reinforcement Learning”，这是一种新颖的强化学习方法，旨在优化模型的内在推理过程。 - **新兴范式**: 论文探讨的“meta-awareness”和“meta-cognitive reasoning”是当前提升LLM推理能力的前沿研究方向。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 该论文不涉及任何排除标准中的领域： - 它不涉及多模态、视觉或特定应用领域（如医疗、化学等）。 - 它关注的是模型内在能力的提升，而非模型基础设施、部署优化或硬件加速。 - 虽然提到了“safety”和“hallucination”的潜在影响，但其核心并非研究这些应用层面的可靠性问题，而是通过提升推理质量来间接改善这些问题，因此不属于排除范畴。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 该论文未涉及智能体或工具使用，因此不适用此条判断。 - **幻觉/可解释性/安全**: 论文通过增强模型的“元认知”来提升推理的准确性和一致性，这可以被视为一种从根源上减少推理错误（一种幻觉形式）的方法。由于它提出了一种新的训练方法来提升模型内在的推理质量，而非进行应用层面的讨论，因此应该被保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心目标是提出一种创新的训练方法（MASA），通过增强LLM的元认知能力来直接提升其在数学、逻辑、科学等多个领域的通用推理性能。其研究内容、方法和目标与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，最终判断为符合要求。"
    },
    {
        "index": "#412",
        "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents",
        "link": "/arxiv/2510.03253",
        "arxiv_id": "2510.03253",
        "authors": "Heyang Gao, Zexu Sun, Erxue Min, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Chen",
        "summary": "Large Language Models (LLMs) as autonomous agents are increasingly tasked with solving complex, long-horizon problems. Aligning these agents via preference-based offline methods like Direct Preference Optimization (DPO) is a promising direction, yet it faces a critical granularity mismatch. Trajectory-level DPO provides a signal that is too coarse for precise credit assignment, while step-level DPO is often too myopic to capture the value of multi-step behaviors. To resolve this challenge, we introduce Hierarchical Preference Learning (HPL), a hierarchical framework that optimizes LLM agents by leveraging preference signals at multiple, synergistic granularities. While HPL incorporates trajectory- and step-level DPO for global and local policy stability, its core innovation lies in group-level preference optimization guided by a dual-layer curriculum. Our approach first decomposes expert trajectories into semantically coherent action groups and then generates contrasting suboptimal groups to enable preference learning at a fine-grained, sub-task level. Then, instead of treating all preference pairs equally, HPL introduces a curriculum scheduler that organizes the learning process from simple to complex. This curriculum is structured along two axes: the group length, representing sub-task complexity, and the sample difficulty, defined by the reward gap between preferred and dispreferred action groups. Experiments on three challenging agent benchmarks show that HPL outperforms existing state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO loss effectively integrates preference signals across multiple granularities, while the dual-layer curriculum is crucial for enabling the agent to solve a wide range of tasks, from simple behaviors to complex multi-step sequences.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:55.012466",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断** 这篇论文的本质是提出一种新的训练范式——分层偏好学习（HPL），旨在解决长时程任务中LLM智能体的对齐问题。其核心贡献并非将LLM应用于某个特定领域，而是改进LLM作为智能体时的基础训练方法。论文针对“轨迹级信号太粗”和“步骤级信号太短视”这一核心矛盾，提出了一个更精细化的、在“动作组”级别进行偏好优化的框架。这直接关系到提升LLM在复杂、多步骤任务中的规划和问题解决能力，属于对LLM基础通用能力的增强。 **第二步：正面指标** 论文命中了多个关键的正面指标： - **核心概念**: 明确以 \"Large Language Models (LLMs)\" 为研究对象。 - **能力方向**: 聚焦于 \"long-horizon problems\"、\"multi-step behaviors\"，这直接对应了**规划**和**问题解决**能力，是通用推理的核心组成部分。 - **训练方法**: 核心是基于 \"Direct Preference Optimization (DPO)\" 的改进，属于**强化学习**和偏好学习的范畴。 - **新兴范式**: 研究对象是 \"LLM agents\"，旨在提升其自主解决问题的能力。 **第三步：排除标准** 论文完全避开了所有排除标准： - **多模态与视觉**: 论文仅涉及文本，未提及视觉或多模态内容。 - **特定应用领域**: 实验是在通用的 \"agent benchmarks\" 上进行的，而非医疗、化学等特定领域。 - **模型可靠性（应用层面）**: 论文目标是提升性能和推理质量，而非水印、安全等应用层面的可靠性。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的HPL框架是一个**通用的智能体训练框架**，用于增强LLM在长时程任务中的通用规划和推理能力。它不是将智能体应用于特定领域，而是提出了一种提升智能体本身能力的方法论，因此符合保留条件。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种创新的、分层级的训练方法，以解决LLM智能体在长时程复杂任务中的规划和多步推理难题。这是一种方法论层面的突破，直接致力于提升LLM的通用推理能力，与你的核心目标高度一致。因此，应判定为符合要求。"
    },
    {
        "index": "#404",
        "title": "Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data",
        "link": "/arxiv/2510.03264",
        "arxiv_id": "2510.03264",
        "authors": "Syeda Nahida Akter, Shrimai Prabhumoye, Eric Nyberg, Mostofa Patwary, Mohammad Shoeybi, Yejin Choi, Bryan Catanzaro",
        "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs revolves around post-training on high-quality, reasoning-intensive data. While emerging literature suggests that reasoning data is increasingly incorporated also during the mid-training stage-a practice that is relatively more proprietary and less openly characterized-the role of such data in pretraining remains unclear. In particular, due to the opaqueness of pretraining corpora in most frontier models, the effect of reasoning data introduced at different phases of pre- and/or post-training is relatively less reported in the scientific literature. This raises several important questions: Is adding reasoning data earlier during pretraining any better than introducing it during post-training? Could earlier inclusion risk overfitting and harm generalization, or instead establish durable foundations that later fine-tuning cannot recover? We conduct the first systematic study of how reasoning data-varying in scale, diversity, and quality-affects LLM performance when introduced at different stages of training. We find that front-loading reasoning data into pretraining is critical (19% avg gain), establishing foundational capabilities that cannot be fully replicated by later-stage SFT, even with more data. We uncover an asymmetric principle for optimal data allocation: pretraining benefits most from broad diversity in reasoning patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg gain). We show that high-quality pretraining data has latent effects, activated only after SFT, and that naively scaling SFT data can be detrimental, washing away the benefits of early reasoning injection. Our results challenge the conventional separation of language modeling and reasoning, providing a principled guide for strategically allocating data across the entire training pipeline to build more capable models.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T14:04:55.002981",
        "filter_reason": "该论文完全符合研究范围，其核心贡献在于系统性地探究了如何优化LLM的训练流程以提升其通用推理能力。以下是根据筛选标准的详细分析： 1.  **第一步：核心判断** 论文的核心本质是研究一种**新的训练范式和方法论**。它并非将LLM应用于特定领域，而是深入探讨了在模型发展的不同阶段（预训练、指令微调）如何通过战略性分配“推理数据”来**增强LLM本身的基础推理能力**。这直接触及了“改进LLM的基础能力”和“提出新的训练范式”这一核心目标，因此应予以保留。其发现的“前置推理”原则和“非对称数据分配”原则，都是为了构建“更有能力的模型”，这与研究目标高度一致。 2.  **第二步：正面指标** 该论文的摘要中包含了多个强烈的正面指标： - **核心概念**: 明确以 \"Large language models, LLMs\" 为研究对象。 - **能力方向**: 核心关键词是 \"reasoning abilities\"（推理能力），全文围绕如何提升这一能力展开。 - **训练方法**: 深入讨论了 \"pretraining\" 和 \"post-training\"（特指SFT）这两个关键的训练阶段，并提出了优化方法论。 - **新兴范式**: 虽然没有提出具体的像CoT那样的新技巧，但它对训练数据在整个生命周期中的战略性使用进行了系统性研究，这本身就是一种更宏观的方法论创新，旨在构建更强大的基础模型。 3.  **第三步：排除标准** 论文完全避除了所有排除标准： - **多模态与视觉**: 论文仅讨论语言模型，不涉及视觉或多模态。 - **特定应用领域**: 研究目标是通用的推理能力，而非在医疗、化学等特定领域的应用。 - **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层问题，而是模型内在能力的提升。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等模糊话题，其研究焦点非常清晰，即为提升模型通用推理能力而进行的数据分配策略研究。 5.  **第五步：最终决策** 综合以上分析，该论文的焦点是**如何通过优化训练数据在不同阶段的分配，来从根本上提升LLM的通用推理能力**。它是一项关于模型核心能力增强的、基础性的方法论研究，而非特定领域应用或模型工程优化的研究。其发现的“前置推理数据至关重要”以及“预训练重多样性，微调重质量”的原则，直接响应了“提高大语言模型本身的通用推理能力”这一核心目标。因此，这篇论文高度相关，应被纳入筛选范围。"
    },
    {
        "index": "#22",
        "title": "Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking",
        "link": "/arxiv/2510.04930",
        "arxiv_id": "2510.04930",
        "authors": "Ali Saheb Pasand, Elvis Dohmatob",
        "summary": "Grokking is the phenomenon whereby, unlike the training performance, which peaks early in the training process, the test/generalization performance of a model stagnates over arbitrarily many epochs and then suddenly jumps to usually close to perfect levels. In practice, it is desirable to reduce the length of such plateaus, that is to make the learning process \"grok\" faster. In this work, we provide new insights into grokking. First, we show both empirically and theoretically that grokking can be induced by asymmetric speeds of (stochastic) gradient descent, along different principal (i.e singular directions) of the gradients. We then propose a simple modification that normalizes the gradients so that dynamics along all the principal directions evolves at exactly the same speed. Then, we establish that this modified method, which we call egalitarian gradient descent (EGD) and can be seen as a carefully modified form of natural gradient descent, groks much faster. In fact, in some cases the stagnation is completely removed. Finally, we empirically show that on classical arithmetic problems such as modular addition and sparse parity problem which this stagnation has been widely observed and intensively studied, that our proposed method eliminates the plateaus.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T14:04:54.976577",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种新的优化算法“Egalitarian Gradient Descent (EGD)”，旨在加速模型的“grokking”现象。Grokking指的是模型泛化能力在长时间停滞后的突然跃升。这本质上是对模型**训练范式和基础学习能力**的改进。它并非将模型作为工具应用于特定领域，而是深入研究并优化模型“如何学习”和“如何泛化”这一根本性问题。这完全符合“改进LLM的基础能力、提出新的训练范式”的保留标准。 2.  **第二步：正面指标** - **能力方向**: 论文明确在“经典算术问题，如模加法和稀疏奇偶性问题”上验证其方法。这些是典型的**数学推理**任务，是衡量LLM通用推理能力的重要基准。因此，论文直接触及了“reasoning (尤其是 math reasoning)”这一核心能力方向。 - **训练方法**: 论文提出了一种新的梯度下降变体，这属于一种新的**训练方法**，旨在提升模型的学习效率和泛化能力。 3.  **第三步：排除标准** 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）。因此，它没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **关于“LLM”关键词的缺失**: 虽然摘要中没有明确出现“Large language models”或“LLMs”，但这并不影响其相关性。Grokking现象最初在包括Transformer（LLM的基础架构）在内的多种模型中被发现和研究。理解并加速Grokking，对于提升Transformer架构模型的泛化和推理能力具有直接的指导意义。这篇论文的研究是基础性的，其成果可以迁移并应用于提升LLM在数学等推理任务上的表现。 - **关于通用性**: 论文研究的不是某个特定领域的应用，而是通用的学习现象和优化方法。其目标是让模型在抽象的数学推理任务上“grok”得更快，这直接对应了提升模型的**通用推理能力**。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种新的训练优化方法，以加速模型在数学推理任务上的泛化过程。这直接触及了提升LLM通用推理能力的根本——即如何让模型更有效地学习和泛化。因此，尽管它没有直接以某个大型LLM为实验对象，但其研究目标和贡献与“提高大语言模型本身的通用推理能力”这一核心目标高度一致。**结论：保留。**"
    },
    {
        "index": "#158",
        "title": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models",
        "link": "/arxiv/2510.03817",
        "arxiv_id": "2510.03817",
        "authors": "Philipp Becker, Niklas Freymuth, Serge Thilges, Fabian Otto, Gerhard Neumann",
        "summary": "On-policy Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model's most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language Models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model's inference behavior. Across datasets, model families, and advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T14:04:55.057698",
        "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一种名为TROLL的新方法，用于改进大语言模型的强化学习（RL）训练过程。它并非将LLM应用于特定领域，而是直接针对LLM训练中的核心技术——基于PPO的RL微调——进行优化。这完全符合“改进LLM的基础能力”和“提出新的训练范式”的核心要求。通过使训练过程更稳定、更高效，该方法旨在提升模型最终学到的能力，这自然包括了通用推理能力。 2.  **正面指标（第二步）：** 论文明确包含了多个关键正面指标。 *   **核心概念:** 论文标题和摘要反复提及 \"Large language models (LLMs)\"。 *   **训练方法:** 论文的主题就是 \"Reinforcement Learning (RL)\"，具体是对PPO算法的改进。RLHF（基于人类反馈的强化学习）是提升LLM推理和遵循指令能力的关键技术，因此对RL训练方法的优化直接关系到通用推理能力的提升。 *   **能力方向:** 虽然摘要未直接使用\"reasoning\"一词，但RL微调的目标就是通过奖励信号来引导模型产生更优的输出，这通常与解决复杂问题、逻辑推理、数学求解等能力直接相关。论文提到的\"final success rates\"（最终成功率）也暗示了其在任务解决能力上的提升。 3.  **排除标准（第三步）：** 论文完全没有触及任何排除标准。它不涉及多模态、视觉，不针对任何特定应用领域（如医疗、化学），也不讨论水印、安全等应用层面的可靠性问题。 4.  **特殊和模糊情况（第四步）：** 本文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需进行特殊判断。 **最终决策（第五步）：** 综合分析，这篇论文是一项关于LLM核心训练算法的基础性研究。它提出了一种更优的RL训练范式（TROLL），旨在解决现有方法（PPO）的不稳定性和次优性问题。这种对训练过程的根本性改进，是提升LLM各项通用能力（包括推理能力）的关键驱动力。因此，它与研究课题“大语言模型通用推理能力”高度相关，是应该被筛选出的前沿论文。"
    },
    {
        "index": "#173",
        "title": "Group Policy Gradient",
        "link": "/arxiv/2510.03679",
        "arxiv_id": "2510.03679",
        "authors": "Junhua Chen, Zixi Zhang, Hantao Zhong, Rika Antonova",
        "summary": "We introduce Group Policy Gradient (GPG), a family of critic-free policy-gradient estimators for general MDPs. Inspired by the success of GRPO's approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a learned value function with a group-based Monte Carlo advantage estimator, removing the memory, compute, and hyperparameter costs of training a critic while preserving PPO's clipped-objective structure. We prove the consistency of the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate empirically that GPG matches or outperforms PPO on standard benchmarks. GPG makes better use of parallel simulations, which, together with its critic-free design, results in more efficient use of computational resources than PPO.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T14:04:55.062458",
        "filter_reason": "这篇论文符合我的研究范围，判断依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一种新的强化学习算法“Group Policy Gradient (GPG)”。这并非将LLM作为工具应用于特定领域，而是对LLM训练过程中的核心方法论——强化学习（特别是RLHF）——进行改进。论文明确指出，其方法受到RLHF中GRPO成功的启发，旨在替代PPO中的评判器，从而提升训练效率。这完全符合“提出新的训练范式，增强其基础能力”的保留标准。 2.  **正面指标（第二步）：** 论文直接命中了最关键的正面指标。摘要中明确提到了“Reinforcement Learning from Human Feedback (RLHF)”和“reinforcement learning (RL)”，这正是提升LLM通用推理能力（如逻辑、规划、问题解决）的核心训练方法之一。虽然论文本身可能在标准RL基准上测试，但其动机和最终应用场景清晰地指向了LLM的训练优化。 3.  **排除标准（第三步）：** 该论文不涉及任何排除标准。它没有讨论多模态、视觉，没有聚焦于医疗、化学等特定应用领域，也不是关于水印、安全等应用层面的可靠性问题。其焦点是纯粹的算法创新。 4.  **特殊与模糊情况（第四步）：** 该论文属于“提出一种新方法来提升模型内在能力”的范畴。通过提供一个更高效、更节省资源的RLHF训练算法，它直接为提升LLM的通用推理能力提供了基础工具。一个更好的训练算法，意味着可以更有效地训练出推理能力更强的模型。 **总结：** 这篇论文的本质是关于LLM训练基础设施的算法层创新，而非应用层或基础设施层。它提出了一种旨在优化RLHF过程的新方法，而RLHF是提升LLM通用推理能力的关键技术。因此，这篇论文对于“大语言模型通用推理能力”这一研究课题具有直接且重要的价值，应当被保留。"
    },
    {
        "index": "#200",
        "title": "RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models",
        "link": "/arxiv/2510.03515",
        "arxiv_id": "2510.03515",
        "authors": "Lianghuan Huang, Sagnik Anupam, Insup Lee, Shuo Li, Osbert Bastani",
        "summary": "Reinforcement learning (RL) has emerged as a promising strategy for finetuning small language models (SLMs) to solve targeted tasks such as math and coding. However, RL algorithms tend to be resource-intensive, taking a significant amount of time to train. We propose RAPID, a novel RL algorithm that can substantially reduce the running time of RL. Our key insight is that RL tends to be costly due to the need to perform both inference and backpropagation during training. To maximize use of computational resources, our algorithm performs inference in large batches, and then performs off-policy policy gradient updates in mini-batches. For off-policy updates, we incorporate group advantage estimation into the policy gradient algorithm, and derive an importance weighted estimator to correct for the bias arising from off-policy learning. Our experiments demonstrate that our algorithm can reduce running time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms while maintaining similar or better accuracy.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T14:04:55.071139",
        "filter_reason": "这篇论文符合研究范围，应予以保留。判断依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一种名为RAPID的新型强化学习（RL）算法。这完全符合筛选标准中“提出新的训练范式”这一类别。其目标是解决RL在微调语言模型时资源消耗大的问题，从而更高效地提升模型能力。论文明确指出，这种训练方法的应用场景是解决“定向任务，例如数学和编码”，这些正是衡量和提升“通用推理能力”的核心领域。因此，论文的本质是改进LLM（或其同类SLM）的基础训练方法，以增强其推理能力，而非将其作为工具应用于特定领域。 2.  **正面指标（第二步）：** 论文命中了多个关键正面指标。 *   **核心概念:** 论文研究对象是语言模型，虽然标题是“小语言模型(SLMs)”，但其方法论对大语言模型（LLMs）同样具有普适性和重要价值。 *   **能力方向:** 论文明确以提升模型在“数学和编码”任务上的表现为目标，这直接对应了“reasoning (尤其是 math reasoning)”和“problem-solving”。 *   **训练方法:** 整篇论文的核心就是关于一种创新的“reinforcement learning (RL)”算法。 3.  **排除标准（第三步）：** 论文不涉及任何排除标准中的领域。它没有讨论多模态、特定应用领域（如医疗、化学），也没有关注模型部署或水印等应用层面的可靠性问题。 4.  **特殊和模糊情况（第四步）：** 此处不适用。 **最终决策（第五步）：** 综合分析，这篇论文的核心是提出一种新的、更高效的强化学习训练范式，其直接目标是提升语言模型在数学和编码等核心推理任务上的表现。这完全契合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。尽管论文以小语言模型（SLM）为实验对象，但其方法论对于提升LLM的训练效率和推理能力具有重要的参考价值，属于前沿的基础能力研究。因此，应判定为符合要求。"
    },
    {
        "index": "#329",
        "title": "Learning Linear Regression with Low-Rank Tasks in-Context",
        "link": "/arxiv/2510.04548",
        "arxiv_id": "2510.04548",
        "authors": "Kaito Takanami, Takashi Takahashi, Yoshiyuki Kabashima",
        "summary": "In-context learning (ICL) is a key building block of modern large language models, yet its theoretical mechanisms remain poorly understood. It is particularly mysterious how ICL operates in real-world applications where tasks have a common structure. In this work, we address this problem by analyzing a linear attention model trained on low-rank regression tasks. Within this setting, we precisely characterize the distribution of predictions and the generalization error in the high-dimensional limit. Moreover, we find that statistical fluctuations in finite pre-training data induce an implicit regularization. Finally, we identify a sharp phase transition of the generalization error governed by task structure. These results provide a framework for understanding how transformers learn to learn the task structure.",
        "subjects": "Disordered Systems and Neural Networks, Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T14:04:55.114426",
        "filter_reason": "这篇论文符合筛选要求。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是一项**理论分析**，而非提出新的训练方法或应用。它的核心贡献不是直接“提高”LLM的推理能力，而是“理解”LLM一个关键能力——上下文学习的内在机制。具体来说，它通过一个简化的理论模型（线性注意力模型和低秩回归任务），揭示Transformer如何在学习过程中捕捉和泛化任务的结构性信息。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文紧密围绕LLM的核心能力展开。它研究的“上下文学习”是现代LLM实现复杂推理（如思维链）的基础。虽然关键词中没有直接出现\"reasoning\"，但理解ICL的泛化机制（\"generalization error\"）和任务结构学习（\"learn to learn the task structure\"）是探究LLM通用问题解决能力根源的深层工作。这直接关联到LLM的通用推理能力是如何形成和运作的。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 该论文完全不符合排除标准。它不涉及多模态、任何特定应用领域（如生物、医疗），也不讨论模型部署或安全水印等应用层面的可靠性问题。其研究是纯粹且基础的。 4.  **第四步：处理特殊和模糊情况——理论分析的价值** 这篇论文的模糊性在于它没有提出一个可以直接“提升”模型性能的新方法。然而，对于一个致力于“大语言模型通用推理能力”的顶尖研究员来说，**深刻理解现有能力的理论基础是推动能力边界的前提**。这篇论文的工作，即揭示ICL中的隐式正则化和相变现象，为未来设计更有效的训练范式（例如，如何让模型更好地学习任务结构）提供了理论指导。它回答了“模型为什么能做到”的问题，这是回答“如何让模型做得更好”的基础。因此，这种基础理论探索与提升通用推理能力的最终目标高度一致，应当被保留。 5.  **第五步：最终决策** 综合来看，尽管这篇论文没有提出像CoT或RL那样的新训练技巧，但它深入剖析了LLM核心能力ICL的理论基础。理解“Transformer如何学习学习”是通往提升其通用推理能力的必经之路。这篇论文为这一目标提供了关键的洞见和理论框架，因此完全符合你的研究筛选范围。"
    }
]