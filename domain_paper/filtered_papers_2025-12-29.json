[
    {
        "index": "#19",
        "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
        "link": "/arxiv/2512.22322",
        "arxiv_id": "2512.22322",
        "authors": "Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun",
        "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.",
        "subjects": "Computation and Language, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning, Multiagent Systems",
        "date": "2025-12-26",
        "category": "cs.MA",
        "crawl_time": "2026-01-01T11:00:03.458811",
        "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心研究。 1.  **核心贡献符合构建与改进 LLM 智能体的目标**： 论文提出了 **SmartSnap** 这一新范式，并定义了一种新型的 **Self-Verifying Agent（自验证智能体）**。这不仅仅是应用现有智能体去解决特定问题，而是对智能体架构和机制的实质性改进。它解决了 Agentic RL 中任务验证的瓶颈问题，将验证从“被动、事后”转变为“主动、原位”。 2.  **符合“单智能体”与“自我反思”的核心关注点**： 论文的核心在于赋予智能体双重使命：完成任务 + 证明完成。这直接对应筛选标准中的 **自我反思** 和 **自我修正** 能力。智能体通过主动寻找证据来验证自身行为，这是一种高级的元认知能力，属于 Agentic AI 的关键能力范畴。 3.  **不属于排除项**： *   虽然论文在 GUI 和移动任务上进行了实验，但其核心贡献是通用的“自验证”机制，而非针对特定领域的应用解决方案，因此不属于“非演化型应用”。 *   虽然涉及 GUI，但视觉仅作为智能体感知环境的一部分，并非研究视觉模型本身，因此不违反多模态排除规则。 *   论文关注的是智能体的任务完成验证机制，而非安全对齐或基础设施优化。 综上所述，该论文通过提出新的智能体框架来增强智能体的自主性和可靠性，是关于 LLM 智能体构建与演化的高质量研究。"
    },
    {
        "index": "#81",
        "title": "FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents",
        "link": "/arxiv/2512.22733",
        "arxiv_id": "2512.22733",
        "authors": "Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo",
        "summary": "Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \\textbf{FoldAct}\\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\\times$ speedup.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-12-28",
        "category": "cs.LG",
        "crawl_time": "2026-01-01T11:00:05.639484",
        "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心方法论研究。 1.  **核心判断（第一步）：** *   论文的核心贡献是提出了 **FoldAct** 这一新框架，旨在解决 **Long-horizon search agents**（长视界搜索智能体）在强化学习训练中面临的上下文无限增长和非平稳观测分布问题。 *   这属于 **构建和改进 LLM 智能体** 的范畴。它关注的是智能体如何处理记忆（上下文折叠）以及如何在长任务中保持稳定性和效率，这是智能体架构和训练机制的关键部分，而非简单的应用或基础设施优化。 2.  **正面指标匹配（第二步）：** *   **核心范式**：论文明确针对 **LLM-based Agents**（特别是搜索智能体）。 *   **智能体能力**：论文重点解决了智能体的 **Memory**（通过上下文折叠压缩交互历史）和 **Planning/Search**（长视界搜索）能力。它提出的“分离损失计算”和“全上下文一致性损失”是为了优化智能体的训练过程，使其具备更好的长程任务处理能力。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然提到了计算效率（5.19倍加速），但这属于算法层面的优化（选择性片段训练），而非硬件或部署基础设施的研究。 4.  **特殊情况处理（第四步）：** *   论文涉及 **Reasoning/Planning**（长视界搜索），且其核心是改进智能体在执行过程中的状态管理和训练稳定性，符合保留条件。 综上所述，FoldAct 提出了一种改进智能体记忆机制和训练稳定性的新方法，直接贡献于 LLM 智能体的构建与优化，因此被保留。"
    },
    {
        "index": "#208",
        "title": "Memento-II: Learning by Stateful Reflective Memory",
        "link": "/arxiv/2512.22716",
        "arxiv_id": "2512.22716",
        "authors": "Jun Wang",
        "summary": "We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-12-27",
        "category": "cs.LG",
        "crawl_time": "2026-01-01T11:00:05.760774",
        "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”和“单智能体”方向的核心研究。 1.  **核心贡献符合第一步判断**： 论文的核心贡献是提出了一种名为“Memento-II”的理论框架，旨在解决LLM智能体的持续学习和经验学习问题。这直接对应了您筛选条件中的“构建、改进或演化 LLM智能体”以及“自我演化”的方法论。它不是将智能体作为工具应用到特定领域，而是研究智能体本身如何学习和适应的底层机制。 2.  **高度匹配正面指标**： *   **自我演化**：论文明确提出了“continual and experiential learning”（持续和经验学习），并强调智能体可以在“without back propagation or model fine tuning”（无需反向传播或模型微调）的情况下通过交互进行适应。这是典型的自我演化特征，即通过经验而非参数更新来迭代改进。 *   **智能体能力**：论文重点研究了“Memory”（情景记忆）和“Self-Reflection”（反思），将其作为智能体适应环境的关键机制。这直接命中了您关注的核心能力列表。 *   **核心范式**：该研究属于“LLM-based Agents”和“Agentic AI”范畴，特别是关于“Retrieval-based language model agents”（基于检索的语言模型智能体）。 3.  **不涉及排除标准**： 论文主要关注智能体的学习机制和理论框架，不涉及安全对齐、多模态视觉处理或图神经网络等排除领域。 综上所述，该论文通过引入“有状态反思决策过程”和结合强化学习，为LLM智能体在不更新模型参数的情况下实现自我完善和持续适应提供了理论基础，精准契合您关于“自我演化”和“单智能体”的研究目标。"
    }
]