[
    {
        "index": "#3",
        "title": "A Multi-Agent Framework for Stateful Inference-Time Search",
        "link": "/arxiv/2510.07147",
        "arxiv_id": "2510.07147",
        "authors": "Arshika Lalan, Rajat Ghosh, Aditya Kolsur, Debojyoti Dutta",
        "summary": "Recent work explores agentic inference-time techniques to perform structured, multi-step reasoning. However, stateless inference often struggles on multi-step tasks due to the absence of persistent state. Moreover, task-specific fine-tuning or instruction-tuning often achieve surface-level code generation but remain brittle on tasks requiring deeper reasoning and long-horizon dependencies. To address these limitations, we propose stateful multi-agent evolutionary search, a training-free framework that departs from prior stateless approaches by combining (i) persistent inference-time state, (ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate its effectiveness in automated unit test generation through the generation of edge cases. We generate robust edge cases using an evolutionary search process, where specialized agents sequentially propose, mutate, and score candidates. A controller maintains persistent state across generations, while evolutionary preservation ensures diversity and exploration across all possible cases. This yields a generalist agent capable of discovering robust, high-coverage edge cases across unseen codebases. Experiments show our stateful multi-agent inference framework achieves substantial gains in coverage over stateless single-step baselines, evaluated on prevalent unit-testing benchmarks such as HumanEval and TestGenEvalMini and using three diverse LLM families - Llama, Gemma, and GPT. These results indicate that combining persistent inference-time state with evolutionary search materially improves unit-test generation.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Multiagent Systems, Software Engineering",
        "date": "2025-10-08",
        "category": "cs.MA",
        "crawl_time": "2025-10-09T19:59:12.660182",
        "filter_reason": "这篇论文完全符合筛选要求，应当保留。我的判断过程如下： 1.  **核心判断（第一步）**：该论文的核心是提出一种名为“有状态多智能体进化搜索”的**免训练框架**，旨在解决现有LLM在“多步任务”和“长时依赖”推理中的根本性缺陷。它的本质是提出一种**新的推理范式或方法论**，通过引入持久状态、对抗性变异和进化保留等机制，来优化LLM在推理时的搜索和问题解决过程。这完全属于“改进LLM的基础能力、增强其多步推理”的范畴，而不是将LLM作为工具应用于特定领域。 2.  **正面指标（第二步）**：论文涵盖了多个关键正面指标： *   **核心概念**: 明确以大语言模型为基础。 *   **能力方向**: 直指LLM的“multi-step reasoning”(多步推理)和“deeper reasoning”(更深推理)能力。 *   **训练方法**: 提出了“evolutionary search”(进化搜索)这一新的计算范式。 *   **新兴范式**: 核心内容就是一个“Multi-Agent Framework”（多智能体框架），设计了具有不同角色的智能体来进行协作推理。 3.  **排除标准与特殊情况处理（第三、四步）**：关键在于如何理解论文中的“单元测试生成”。 *   这不是一个“特定应用领域”研究，而是选择了一个极具挑战性、需要深度探索和长时规划的任务作为**评估基准**。论文的目标是构建一个“generalist agent”（通用智能体），并证明了它在“unseen codebases”（未见过的代码库）上的泛化能力。 *   根据**智能体/工具使用的特殊处理规则**，这篇论文提出的是一种**通用的智能体协作框架和推理时搜索方法**，旨在增强LLM的**通用问题解决能力**。它利用单元测试生成这个任务来验证其方法的有效性，而非仅仅针对该任务本身。因此，应该保留。 4.  **最终决策（第五步）**：该论文的核心贡献是方法论层面的创新，即通过在推理时引入状态化和进化搜索机制，来系统性地提升LLM的通用推理能力。尽管其验证场景是单元测试，但论文的定位、贡献和所解决的问题都具有高度的通用性，与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度契合。"
    },
    {
        "index": "#1",
        "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
        "link": "/arxiv/2510.07318",
        "arxiv_id": "2510.07318",
        "authors": "Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei",
        "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.054678",
        "filter_reason": "这篇论文符合筛选要求，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 该论文的本质是提出了一种新的神经网络架构框架——人工海马网络（AHN），用于解决大语言模型在长上下文建模中的核心瓶颈。这个工作的核心贡献是**改进LLM的基础能力——长程记忆与信息整合能力**。它并非简单地将现有LLM应用于某个领域，而是通过修改模型结构本身，使其能够更高效、更准确地处理和利用超长序列信息。通用推理，尤其是多步推理、规划和复杂问题解决，都强烈依赖于模型对长距离上下文的精确捕捉和记忆。因此，提升长上下文建模能力是增强LLM通用推理能力的**关键基础设施和前置条件**。论文的实验证明，通过引入AHN，模型在长上下文基准测试上的表现得到了提升，这直接说明了其核心能力（信息检索与利用）的增强，从而为更复杂的推理任务奠定了基础。 2.  **第二步：正面指标** - **核心概念**: 论文的研究对象是Transformer架构的LLM（如Qwen2.5），完全符合。 - **能力方向**: 虽然摘要未直接使用\"reasoning\"一词，但\"长上下文建模\"（Long-context modeling）是实现深度推理的先决条件。其评测基准（LV-Eval, InfiniteBench）中包含了大量的需要长程依赖的问答和任务，这些都属于广义的问题解决范畴。 - **训练方法**: 不涉及，但这不是排除项。 - **新兴范式**: 论文提出的是一种底层的架构创新，它能够赋能上层的智能体、工具使用等范式。例如，一个需要回忆几十万字前信息的智能体，如果搭载了AHN，其推理和规划能力将得到质的飞跃。 3.  **第三步：排除标准** - **多模态与视觉**: 完全不涉及，论文聚焦于纯文本序列。 - **特定应用领域**: 不涉及，该方法是一个通用框架，而非针对化学、医疗等特定领域。 - **模型可靠性（应用层面）**: 不涉及水印、安全等。 4.  **第四步：处理特殊和模糊情况** 关键点在于区分“模型基础设施研究”和“模型基础能力研究”。虽然论文提到了“减少计算和内存需求”，但这更像是一个核心能力提升带来的**附加收益**，而非研究的主要目标。一篇关于基础设施的论文（如模型量化、蒸馏）的主要目标是在**不损失或略微损失**模型性能的前提下，提升效率。而本论文的核心是**通过创新架构，在提升模型处理长上下文任务性能的同时，也带来了效率的巨大提升**。它的落脚点在于“性能增强”，效率是其优秀设计的体现。因此，这应被视为对模型“通用问题解决能力”的一次基础性增强，而非单纯的工程或部署优化。 5.  **第五步：最终决策** 综合以上分析，该论文通过创新的“人工海马网络”架构，直接解决了LLM在长程信息记忆和检索上的根本性挑战。这项基础能力的提升，对于实现真正复杂的、多步骤的通用推理至关重要。它并非领域应用，也非纯粹的工程优化，而是对LLM核心机制的一次深刻探索与改进。因此，它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    },
    {
        "index": "#5",
        "title": "On the Convergence of Moral Self-Correction in Large Language Models",
        "link": "/arxiv/2510.07290",
        "arxiv_id": "2510.07290",
        "authors": "Guangliang Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, Xitong Zhang, Rongrong Wang, Kristen Marie Johnson",
        "summary": "Large Language Models (LLMs) are able to improve their responses when instructed to do so, a capability known as self-correction. When instructions provide only a general and abstract goal without specific details about potential issues in the response, LLMs must rely on their internal knowledge to improve response quality, a process referred to as intrinsic self-correction. The empirical success of intrinsic self-correction is evident in various applications, but how and why it is effective remains unknown. Focusing on moral self-correction in LLMs, we reveal a key characteristic of intrinsic self-correction: performance convergence through multi-round interactions; and provide a mechanistic analysis of this convergence behavior. Based on our experimental results and analysis, we uncover the underlying mechanism of convergence: consistently injected self-correction instructions activate moral concepts that reduce model uncertainty, leading to converged performance as the activated moral concepts stabilize over successive rounds. This paper demonstrates the strong potential of moral self-correction by showing that it exhibits a desirable property of converged performance.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.057416",
        "filter_reason": "根据您提供的筛选标准，我的判断过程如下： **第一步：核心判断** 这篇论文的核心是研究大语言模型（LLM）的一种内在能力——“内在自我修正”。它没有将LLM作为工具应用于某个特定领域（如医疗、法律），而是深入探究了LLM在接收到抽象修正指令后，如何通过多轮交互来改进自身响应质量的内在机制。论文揭示了“性能收敛”这一关键特性，并从机制上解释了其成因（通过激活特定概念来减少模型不确定性）。这完全符合“改进LLM的基础能力、增强其通用能力”的保留标准。自我修正能力是LLM进行多步推理和问题解决时的关键一环，理解其工作机制是提升模型通用推理能力的重要基础。 **第二步：正面指标** - **核心概念**: 论文明确以\"Large Language Models (LLMs)\"为研究对象。 - **能力方向**: 论文研究的\"self-correction\"（自我修正）是一种高级的问题解决能力。虽然它聚焦于\"moral\"（道德）领域作为案例，但其分析的“内在自我修正”机制具有通用性，旨在提升模型的响应质量和推理鲁棒性，这与\"problem-solving\"和通用推理能力紧密相关。 - **新兴范式**: \"self-correction\"（自我修正）与\"self-evolve\"（自我进化）范式高度相关，都是指模型在推理时动态地改进自身输出的能力。 **第三步：排除标准** - **多模态与视觉**: 论文不涉及任何视觉或多模态内容。 - **特定应用领域**: 尽管论文以\"moral\"（道德）为切入点，但其研究目标是揭示自我修正的通用机制，而非解决某个具体的道德伦理应用问题。它属于对模型内在认知过程的探索，而非领域应用。 - **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性研究。 **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: 这篇论文可以被看作是对模型内在可解释性的一种探索。它通过机制分析，揭示了模型如何通过减少不确定性来提升输出质量。这属于“提出一种新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量”的范畴，因此应该保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于**揭示了LLM内在自我修正能力的内在机制**。它不是应用研究，而是对LLM一项基础认知能力的深入探索。理解这种自我修正的“收敛”机制，对于未来设计更具鲁棒性和更强推理能力的LLM具有重要意义。因此，这篇论文完全符合您为“大语言模型通用推理能力”设定的研究范围。"
    },
    {
        "index": "#4",
        "title": "Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning",
        "link": "/arxiv/2510.07300",
        "arxiv_id": "2510.07300",
        "authors": "Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Kaiyu Huang, Yufeng Chen, Jinan Xu, Jie Zhou",
        "summary": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex reasoning tasks by adopting the \"think-then-answer\" paradigm, which enhances both accuracy and interpretability. However, current LRMs exhibit two critical limitations when processing non-English languages: (1) They often struggle to maintain input-output language consistency; (2) They generally perform poorly with wrong reasoning paths and lower answer accuracy compared to English. These limitations significantly degrade the user experience for non-English speakers and hinder the global deployment of LRMs. To address these limitations, we propose M-Thinker, which is trained by the GRPO algorithm that involves a Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment (CTA) reward. Specifically, the LC reward defines a strict constraint on the language consistency between the input, thought, and answer. Besides, the CTA reward compares the model's non-English reasoning paths with its English reasoning path to transfer its own reasoning capability from English to non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B models not only achieve nearly 100% language consistency and superior performance on two multilingual benchmarks (MMATH and PolyMath), but also exhibit excellent generalization on out-of-domain languages.",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.056931",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 论文的核心贡献是提出了一种名为M-Thinker的新模型和一种新的训练范式（GRPO算法）。其根本目标是解决现有大型推理模型（LRMs）在非英语环境下推理能力下降的问题。这并非将LLM应用于某个特定领域，而是直接针对LLM的**基础能力——通用推理**——进行改进和增强。论文通过设计新的奖励机制（语言一致性LC和跨语言思维对齐CTA）来优化模型的推理过程，使其在不同语言下都能保持高质量的推理路径和答案准确性。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 **第二步：正面指标——论文高度相关。** 论文明确包含了多个关键的正面指标： - **核心概念**: 论文研究对象是\"Large Reasoning Models (LRMs)\"，这是LLM的一个子集，专注于推理。 - **能力方向**: 论文的核心主题是\"reasoning\"，并具体在数学推理基准上进行评估。 - **训练方法**: 论文的核心方法论是\"Reinforcement Learning\"（GRPO算法），这是一种前沿的训练范式。 - **新兴范式**: 论文基于\"think-then-answer\"（思维链）范式进行改进。 **第三步：排除标准——论文未触及任何排除领域。** 论文的研究焦点非常纯粹，不涉及以下任何排除领域： - **多模态与视觉**: 论文仅处理文本。 - **特定应用领域**: 研究的是多语言这一通用能力，而非医疗、化学、机器人等具体应用。 - **模型可靠性（应用层面）**: 论文不讨论水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况——不适用。** 论文不涉及智能体/工具使用，也不以幻觉或安全性为主要议题，因此无需进行特殊情况的判断。 **第五步：最终决策——保留。** 综合以上分析，这篇论文的本质是方法论创新，旨在通过一种新颖的强化学习框架来**系统性地提升大语言模型在多语言环境下的通用推理能力**。它直接解决了模型推理过程中的核心问题（推理路径质量、语言一致性），并在通用推理基准上验证了其有效性。这与您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标高度一致。因此，这篇论文是您研究课题下的高质量前沿文献。"
    },
    {
        "index": "#2",
        "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
        "link": "/arxiv/2510.07315",
        "arxiv_id": "2510.07315",
        "authors": "Ming Zhong, Xiang Zhou, Ting-Yun Chang, Qingze Wang, Nan Xu, Xiance Si, Dan Garrette, Shyam Upadhyay, Jeremiah Liu, Jiawei Han, Benoit Schillings, Jiao Sun",
        "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models' code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Software Engineering",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.055572",
        "filter_reason": "这篇论文符合您的筛选标准，应予以保留。我的判断过程如下： **第一步：核心判断——这篇论文的本质是什么？** 这篇论文的本质并非将LLM作为工具应用于特定领域，而是提出了一种新的**评估方法论**来衡量LLM在特定通用能力上的表现。其核心贡献是： 1.  **提出假设**：LLM在编码领域的“通用推理能力”不仅包括功能正确性，还包括遵循非功能性指令的能力（如代码风格、可读性、意图保持等），后者是区分模型优劣的关键。 2.  **构建评测基准**：为了验证该假设，论文创建了一个名为“Vibe Checker”的测试床，它超越了传统的`pass@k`指标，能够量化评估LLM遵循代码指令的能力。 尽管这篇论文的直接产出是“评测基准”而非“训练范式”，但它精准地定义和度量了“通用推理能力”的一个重要子集——**在复杂、多约束条件下遵循指令并进行规划和生成的能力**。这属于对LLM基础能力的深刻洞察和方法论革新，是“提高”模型能力不可或缺的一环。因此，它符合核心判断中“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的要求。 **第二步：正面指标——论文是否包含以下主题？** -   **核心概念**: 论文完全围绕**Large language models (LLMs)**在代码生成任务上的表现展开。 -   **能力方向**: 论文的核心是**reasoning**和**problem-solving**。代码生成本身就是一种复杂的逻辑推理过程。论文进一步探讨的“遵循多重指令”更是对模型**规划**和**多步推理**能力的直接考验。 -   **新兴范式**: 论文虽然没有直接提出新的智能体或工具使用框架，但其研究的“vibe check”和“instruction following”是构建高级LLM智能体（能够理解并执行复杂自然语言指令）的核心基础。 论文在多个关键正面指标上表现出强相关性。 **第三步：排除标准——论文是否主要聚焦于以下领域？** -   **多模态与视觉**: 论文完全不涉及视觉或多模态内容。 -   **特定应用领域**: “编码”是一项通用技能，而非特定垂直领域（如生物、化学、法律）。论文研究的是普适性的编程能力，因此不属于特定应用领域的范畴。 -   **模型可靠性（应用层面）**: 论文不涉及水印、安全等议题。 论文成功避开了所有主要的排除标准。 **第四步：处理特殊和模糊情况** -   **智能体/工具使用**: 不直接适用，但论文所研究的“遵循指令”能力是构建通用智能体的基石。 -   **幻觉/可解释性/安全**: 这一点是判断的关键。论文所指出的模型在遵循多重指令时出现的“功能回归”问题，本质上是一种**复杂的推理失败**。模型无法将多个约束条件同时纳入其逻辑生成路径，导致顾此失彼。论文提出的方法正是为了**诊断和量化这种特定的推理缺陷**。这符合“如果论文提出一种新方法来...增强模型内在的可解释性...从而提升模型的通用可靠性和推理质量，应该保留”的标准。它提供了一个更精细的透镜来审视模型的推理过程，指明了提升方向。 **第五步：最终决策** 综合以上分析，这篇论文“Vibe Checker”虽然表面上是一篇评测工作，但其内核是**对LLM通用推理能力的深刻解构**。它识别出“遵循指令”是当前模型在真实世界推理任务中的一个关键短板，并提供了一套行之有效的方法论来量化这一短板。这种对基础能力的精准定义和度量，是推动整个领域向前发展的关键一步，完全符合您筛选“致力于提高大语言模型（LLM）本身的『通用推理能力』”的前沿论文的目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#7",
        "title": "Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models",
        "link": "/arxiv/2510.07248",
        "arxiv_id": "2510.07248",
        "authors": "Jonggeun Lee, Woojung Song, Jongwook Han, Haesung Pyun, Yohan Jo",
        "summary": "Small language models (SLMs) offer significant computational advantages for tool-augmented AI systems, yet they struggle with tool-use tasks, particularly in selecting appropriate tools and identifying correct parameters. A common failure mode is schema misalignment: models hallucinate plausible but non-existent tool names that reflect naming conventions internalized during pretraining but absent from the provided tool schema. Rather than forcing models to adapt to arbitrary schemas, we propose adapting schemas to align with models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool Schema Generation), a training-free method that leverages peakedness-a signal from contamination detection indicating pretraining familiarity-to automatically rename tool components. By generating multiple candidates and selecting those with highest output concentration across samples, PA-Tool identifies pretrain-aligned naming patterns. Experiments on MetaTool and RoTBench show improvements of up to 17% points, with schema misalignment errors reduced by 80%. PA-Tool enables small models to approach state-of-the-art performance while maintaining computational efficiency for adaptation to new tools without retraining. Our work demonstrates that schema-level interventions can unlock the tool-use potential of resource-efficient models by adapting schemas to models rather than models to schemas.",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.058370",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心贡献是提出了一种名为PA-Tool的新方法，旨在解决小语言模型（SLM）在工具使用任务上的核心缺陷——“模式错位”。它没有将模型应用于某个特定领域，而是专注于提升模型本身执行“工具使用”这一通用任务的能力。工具使用被您明确列为增强通用推理能力的关键方法论之一。因此，这篇论文的本质是改进LLM/SLM的基础能力，应予以保留。 2.  **第二步：正面指标——论文高度相关。** 论文明确包含了多个正面指标： *   **核心概念**: 虽然聚焦于SLMs，但其原理和发现对LLMs同样具有启发性，属于同一研究范畴。 *   **能力方向**: 论文的核心就是“tool use”，这是通用问题解决和推理能力的重要组成部分。模型需要推理出哪个工具是合适的，以及正确的参数是什么。 *   **新兴范式**: 论文直接探讨了“tool use”这一前沿范式，并提出了创新的解决方案。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容与多模态、特定应用领域（如医疗、化学）或应用层面的模型可靠性（如水印）完全无关。其方法具有通用性，实验也是在通用的工具使用基准上进行的。 4.  **第四步：处理特殊和模糊情况——论文符合保留条件。** *   **智能体/工具使用**: 论文提出了一种通用的、与领域无关的工具使用方法（PA-Tool）来增强模型的通用问题解决能力，这完全符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”的标准。 *   **幻觉/可解释性/安全**: 论文直接针对模型在工具使用中的一种特定“幻觉”现象（即幻觉出不存在的工具名），并提出了一种根本性的解决方法。这属于“提出一种新方法来减少幻觉，从而提升模型的通用可靠性和推理质量，应该保留”的情况。 **最终决策**: 该论文通过提出“让工具适应模型，而非模型适应工具”的创新思路，解决了模型在通用推理能力（特别是工具使用）中的一个关键瓶颈。其方法论贡献（PA-Tool）直接提升了模型执行通用任务的精确性和可靠性，与您追求“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，这篇论文应被筛选为符合要求。"
    },
    {
        "index": "#6",
        "title": "Online Rubrics Elicitation from Pairwise Comparisons",
        "link": "/arxiv/2510.07284",
        "arxiv_id": "2510.07284",
        "authors": "MohammadHossein Rezaei, Robert Vacareanu, Zihao Wang, Clinton Wang, Yunzhong He, Afra Feyza Akyürek",
        "summary": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers where verifiable rewards are not applicable and human preferences provide coarse signals. Prior work shows that reinforcement learning with rubric-based rewards leads to consistent gains in LLM post-training. Most existing approaches rely on rubrics that remain static over the course of training. Such static rubrics, however, are vulnerable to reward-hacking type behaviors and fail to capture emergent desiderata that arise during training. We introduce Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates evaluation criteria in an online manner through pairwise comparisons of responses from current and reference policies. This online process enables continuous identification and mitigation of errors as training proceeds. Empirically, this approach yields consistent improvements of up to 8% over training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as well as the validation sets of expert questions and rubrics. We qualitatively analyze the elicited criteria and identify prominent themes such as transparency, practicality, organization, and reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.057921",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。以下是我的详细判断过程： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心贡献是提出了一种名为“Online Rubrics Elicitation”的新方法。这并非将LLM应用于某个特定领域，而是专注于改进LLM的训练过程本身，特别是在后训练阶段。它解决了现有强化学习方法中“静态评分标准”的局限性，通过动态更新评估标准来引导模型持续进步。这完全符合“改进LLM的基础能力”和“提出新的训练范式”的保留标准。其最终目标是提升模型在开放式、长答案任务上的表现，这本质上是对模型通用推理和问题解决能力的增强。 2.  **第二步：正面指标——论文高度相关。** - **核心概念**: 论文明确以训练“LLMs”为核心。 - **能力方向**: 论文旨在提升模型在“开放式长-form answers”上的表现，并在GPQA（一个需要深度推理的基准）等数据集上验证了效果。摘要中明确指出，定性分析发现了“reasoning”是动态涌现出的关键评估维度之一。 - **训练方法**: 论文直接建立在“reinforcement learning with rubric-based rewards”之上，并对其进行了创新，属于“强化学习优化”的范畴。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文内容完全不涉及多模态、视觉、特定应用领域（如医疗、化学），也不是关于模型基础设施或应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况——论文提升了模型的内在推理质量。** 论文提出的动态评分标准方法，其核心动机之一是防止“reward-hacking”（奖励破解）。奖励破解是模型没有真正学会任务，而是学会了利用奖励函数漏洞的行为。通过动态更新标准，该方法迫使模型不断提升其回答的真实质量，包括逻辑的严谨性、内容的组织性和推理的深度。这可以被看作是一种提升模型内在可靠性和推理质量的基础方法，而非应用层面的讨论，因此应该保留。 **最终决策**: 综合以上分析，这篇论文提出了一种创新的训练范式（动态评分标准），旨在通过改进强化学习过程来提升大语言模型在复杂、开放式任务上的表现。其核心贡献直接关联到提升模型的通用推理和问题解决能力，与“大语言模型通用推理能力”的研究课题高度契合。因此，最终判断为 **True**。"
    },
    {
        "index": "#9",
        "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
        "link": "/arxiv/2510.07242",
        "arxiv_id": "2510.07242",
        "authors": "Leitian Tao, Ilia Kulikov, Swarnadeep Saha, Tianlu Wang, Jing Xu, Yixuan Li, Jason E Weston, Ping Yu",
        "summary": "Post-training for reasoning of large language models (LLMs) increasingly relies on verifiable rewards: deterministic checkers that provide 0-1 correctness signals. While reliable, such binary feedback is brittle--many tasks admit partially correct or alternative answers that verifiers under-credit, and the resulting all-or-nothing supervision limits learning. Reward models offer richer, continuous feedback, which can serve as a complementary supervisory signal to verifiers. We introduce HERO (Hybrid Ensemble Reward Optimization), a reinforcement learning framework that integrates verifier signals with reward-model scores in a structured way. HERO employs stratified normalization to bound reward-model scores within verifier-defined groups, preserving correctness while refining quality distinctions, and variance-aware weighting to emphasize challenging prompts where dense signals matter most. Across diverse mathematical reasoning benchmarks, HERO consistently outperforms RM-only and verifier-only baselines, with strong gains on both verifiable and hard-to-verify tasks. Our results show that hybrid reward design retains the stability of verifiers while leveraging the nuance of reward models to advance reasoning.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.064543",
        "filter_reason": "这篇论文完全符合你的研究范围。 **第一步：核心判断——这篇论文的本质是什么？** 这篇论文的本质是提出一种名为**HERO（Hybrid Ensemble Reward Optimization）的新型强化学习训练框架**。其目标是改进大语言模型（LLM）的后训练过程，具体来说，是解决在数学推理这类任务中，由于奖励信号过于稀疏（只有0或1）而导致学习效率低下的问题。这完全符合**「改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力」**的标准。其核心贡献是方法论层面的创新，旨在提升模型本身的推理能力，而不是将其作为工具应用于某个特定领域。 **第二步：正面指标——论文是否包含以下主题？** 这篇论文命中了所有关键正面指标： - **核心概念**: 论文明确研究对象是 \"large language models (LLMs)\"。 - **能力方向**: 论文的核心是提升LLM的 \"reasoning\" 能力，特别是 \"mathematical reasoning\"。 - **训练方法**: 论文的核心贡献是一个 \"reinforcement learning (RL)\" 框架，这是优化LLM推理能力的关键训练范式。 - **新兴范式**: 虽然未直接提及 Agents 或 Tool Use，但其通过改进奖励模型来增强模型决策和输出的方法，与提升基于LLM的智能体或推理系统的核心能力思想是一致的。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全避开了所有排除标准： - 它不涉及多模态、视觉或机器人控制。 - 它不以医疗、化学等特定应用领域为目标。尽管它在数学推理基准上测试，但数学能力被视为衡量**通用推理能力**的核心基石，而非一个狭窄的应用领域。论文的研究重点是“如何更好地训练”，而不是“如何解决某个数学难题”。 - 它不涉及水印、安全等应用层面的可靠性议题。 **第四步：处理特殊和模糊情况** 本篇论文不存在模糊情况。它清晰地聚焦于**提升模型内在推理能力的训练方法论**。虽然它通过数学问题来验证效果，但这与研究如何用LLM解决生物问题的论文有本质区别。前者是改进引擎本身，后者是驾驶引擎去某个地方。 **第五步：最终决策** 综合分析，这篇论文通过提出一种新颖的混合奖励强化学习框架，直接针对LLM在通用推理训练中的一个核心痛点（稀疏奖励）提出了解决方案。它的工作旨在提升模型底层的逻辑和数学推理能力，这正是你寻找的、致力于提高大语言模型本身『通用推理能力』的前沿研究。因此，这篇论文应被保留。"
    },
    {
        "index": "#23",
        "title": "More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning",
        "link": "/arxiv/2510.07169",
        "arxiv_id": "2510.07169",
        "authors": "Yike Zhao, Simin Guo, Ziqing Yang, Shifan Han, Dahua Lin, Fei Tan",
        "summary": "The reasoning capabilities of Large Language Models (LLMs) play a critical role in many downstream tasks, yet depend strongly on the quality of training data. Despite various proposed data construction methods, their practical utility in real-world pipelines remains underexplored. In this work, we conduct a comprehensive analysis of open-source datasets and data synthesis techniques for mathematical reasoning, evaluating them under a unified pipeline designed to mirror training and deployment scenarios. We further distill effective data selection strategies and identify practical methods suitable for industrial applications. Our findings highlight that structuring data in more interpretable formats, or distilling from stronger models often outweighs simply scaling up data volume. This study provides actionable guidance for integrating training data to enhance LLM capabilities, supporting both cost-effective data curation and scalable model enhancement. We hope this work will inspire further research on how to balance \"more data\" versus \"better data\" for real-world reasoning tasks.",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.081450",
        "filter_reason": "这篇论文完全符合您的筛选标准，应当保留。以下是我的详细判断过程： 1.  **第一步：核心判断（符合保留条件）** 论文的核心并非将LLM应用于某个特定领域，而是对提升LLM本身能力的关键因素——训练数据——进行批判性分析。它致力于回答一个根本性问题：为了提升LLM的数学推理能力，我们应该如何选择和构造训练数据？这直接关系到LLM基础能力的改进。论文的核心贡献是“提炼有效的数据选择策略”和“提供可操作的指导来增强LLM能力”，这属于改进LLM内在通用能力的范畴。 2.  **第二步：正面指标（高度匹配）** 论文命中了所有关键正面指标： *   **核心概念**: 明确以 \"Large Language Models (LLMs)\" 为研究对象。 *   **能力方向**: 精准聚焦于 \"reasoning capabilities\"，特别是 \"mathematical reasoning\"，这正是通用推理能力的核心支柱之一。 *   **训练方法**: 论文深入分析了 \"data synthesis techniques\"（数据合成技术），这是提升模型能力（尤其是在监督学习和强化学习微调阶段）的关键方法论。它提出的“从更强的模型中提炼”也是一种重要的训练范式。 3.  **第三步：排除标准（完全规避）** 论文未触及任何明确的排除领域： *   它不涉及多模态（Vision, MLLMs）。 *   它的应用背景是“数学推理”，但研究目的并非解决某个数学难题，而是将数学推理作为一个衡量和提升**通用能力**的标杆，这与“医疗、化学”等特定领域应用有本质区别。 *   它不关注模型部署、基础设施或应用层面的安全问题（如Watermarking）。 4.  **第四步：特殊和模糊情况（不适用）** 该论文不涉及智能体/工具使用，也不是从可解释性或安全性的角度切入。它非常纯粹地聚焦于“数据”这一训练核心要素，以提升模型的“推理”质量。 **最终决策**: 这篇论文的本质是一项旨在提升大语言模型**通用数学推理能力**的方法论研究。它通过分析“数据选择与合成”这一训练环节的根本问题，为如何更有效地提升LLM的核心推理能力提供了实证依据和实用策略。这与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，它是一篇非常相关且有价值的前沿论文，应当被保留。"
    },
    {
        "index": "#27",
        "title": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning",
        "link": "/arxiv/2510.07105",
        "arxiv_id": "2510.07105",
        "authors": "Taylor Sorensen, Yejin Choi",
        "summary": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity, or legitimate disagreement between annotators. In this paper, we outline our system for modeling human variation. Our system leverages language models' (LLMs) in-context learning abilities, along with a two-step meta-learning training procedure for 1) post-training on many datasets requiring in-context learning and 2) specializing the model via in-context meta-learning to the particular data distribution of interest. We also evaluate the performance of our system submission to the Learning With Disagreements (LeWiDi) competition, where it was the overall winner on both tasks. Additionally, we perform an ablation study to measure the importance of each system component. We find that including rater examples in-context is crucial for our system's performance, dataset-specific fine-tuning is helpful on the larger datasets, post-training on other in-context datasets is helpful on one of the competition datasets, and that performance improves with model scale.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.083194",
        "filter_reason": "这篇论文符合你的研究范围，核心判断依据如下： 1.  **核心判断（第一步）：论文的本质是提升LLM的基础能力。** 论文的核心贡献并非将LLM应用于某个特定领域，而是提出了一种**新的训练范式**（两步元学习训练过程）来**增强LLM的上下文学习能力**。ICL（In-Context Learning）本身就是LLM的一项核心基础能力。论文的目标是让模型更好地处理“主观性、模糊性或标注者分歧”，这本质上是在提升模型处理复杂、非结构化信息的**通用推理和问题解决能力**。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑...多步推理等通用能力”的保留标准。 2.  **正面指标（第二步）：论文包含多个高度相关的主题。** -   **核心概念**: 论文明确以“language models (LLMs)”为核心。 -   **能力方向**: 论文聚焦于提升模型处理“人类变异”的能力，这是一种高级的**问题解决**和**推理**能力，要求模型理解并模拟复杂的人类判断逻辑，而非简单的模式匹配。 -   **训练方法**: 论文的核心创新点之一是“元学习”，这是一种与强化学习（RL）并列的、旨在提升模型学习能力和泛化性的前沿训练方法论。 3.  **排除标准（第三步）：论文未触及任何排除领域。** 论文的研究内容与多模态、特定应用领域（如医疗、化学）或模型基础设施（如部署、硬件加速）完全无关。它关注的是通用的NLP任务和模型本身的内在能力。 4.  **处理特殊和模糊情况（第四步）：不适用，但方向正确。** 论文虽然不直接研究幻觉或可解释性，但其提升模型对模糊和分歧信息的处理能力，客观上会提高模型输出的**内在可靠性**和**推理质量**，这与你的研究目标在精神上是一致的。 **最终决策（第五步）：** 综合来看，这篇论文是一篇典型的、致力于提升LLM自身通用能力的方法论研究。它通过提出一种创新的元学习框架，强化了LLM的ICL能力，使其能更好地进行涉及主观性和模糊性的复杂推理。这完全契合你筛选“致力于提高大语言模型（LLM）本身的『通用推理能力』”论文的核心目标。因此，应予以保留。"
    },
    {
        "index": "#35",
        "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models",
        "link": "/arxiv/2510.07048",
        "arxiv_id": "2510.07048",
        "authors": "Yuntao Gui, James Cheng",
        "summary": "Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMs' chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the model's ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: https://github.com/ytgui/Search-R3",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.128981",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出一种新的训练范式（Search-R3框架）来增强大语言模型的基础能力。其核心贡献并非简单地将LLM应用于检索任务，而是将“推理过程”与“嵌入生成”这两个功能进行深度统一和相互增强。论文通过利用并优化LLM的思维链能力，来让模型在逐步的语义分析推理中，产生更高质量的搜索嵌入。这直接触及了如何改进LLM的内在工作机制，特别是其多步推理能力，属于提升模型基础能力的范畴，而非特定领域的应用。 **第二步：正面指标——论文是否包含相关主题？** 论文命中了多个关键的正面指标： - **核心概念**: 明确以“Large Language Models (LLMs)”为核心研究对象。 - **能力方向**: 核心聚焦于“reasoning”，特别是“chain-of-thought capabilities”和“step-by-step”的推理过程，旨在解决“complex knowledge-intensive tasks”。 - **训练方法**: 明确提出了包含“reinforcement learning (RL)”在内的多阶段训练方法，用于优化推理和嵌入生成。这与您关注点中的“强化学习优化”高度一致。 **第三步：排除标准——论文是否聚焦于排除领域？** 该论文没有落入任何排除标准中： - 非多模态研究，专注于文本。 - 非特定应用领域（如医疗、法律），其应用场景是通用的信息检索（retrieval tasks），这是LLM的一项基础能力，而非垂直领域应用。 - 非模型基础设施或应用层面的可靠性研究。 **第四步：处理特殊和模糊情况** 本篇论文的情况非常清晰，不属于需要特殊处理的模糊地带。它提出的框架是一种通用的方法论，旨在提升模型在需要推理和检索的复杂任务上的表现，这与“用于化学实验自动化的智能体”等特定领域应用有本质区别。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种通过强化学习来优化LLM推理过程，并将其与嵌入生成功效统一的新范式。这直接提升了LLM的通用推理能力和语义理解深度，完全符合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为**True**。"
    },
    {
        "index": "#45",
        "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
        "link": "/arxiv/2510.06915",
        "arxiv_id": "2510.06915",
        "authors": "Zecheng Tang, Baibei Ji, Quantong Qiu, Haitian Wang, Xiaobo Liang, Juntao Li, Min Zhang",
        "summary": "Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.139782",
        "filter_reason": "这篇论文完全符合筛选标准，应予以保留。 **判断过程和核心依据如下：** 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心是改进LLM训练流程中的关键组件——**奖励模型**。它并非将LLM应用于特定领域，而是提出了一种新的**训练范式**，旨在解决现有奖励模型在长上下文场景下的“上下文-响应一致性”判断能力不足这一根本性问题。一个能准确判断长历史轨迹中一致性的奖励模型，是训练出具备复杂规划、多步推理和长期记忆能力的LLM或智能体的基础。因此，这篇论文的本质是增强LLM的基础能力，属于筛选标准中“保留”的范畴。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文触及了多个关键的正面指标： *   **核心概念**: 明确以“Large language models (LLMs)”为研究对象。 *   **能力方向**: 虽未直接使用\"reasoning\"一词，但其研究的“long history trajectories”和“context-aware preference judgments”是实现复杂推理和规划的先决条件。缺乏上下文一致性，高质量的推理和规划就无从谈起。 *   **训练方法**: 论文的核心是提出一种新的训练策略来构建奖励模型，而奖励模型是**强化学习**的核心。 *   **新兴范式**: 论文的直接动机是服务于“LLM agent”，并提升其在真实世界长任务中的表现。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 该论文完全不涉及任何排除标准中的领域。它没有讨论多模态、特定领域应用（如医疗、化学），也未将研究焦点放在应用层面的安全、水印或模型基础设施上。论文中提到的\"safety\"只是作为当前短上下文奖励模型关注的一个例子，用以反衬其研究的“long-context consistency”这一新维度。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 论文是典型的“提出一种通用的方法来增强LLM的通用问题解决能力”的案例。它通过改进奖励模型这一核心组件，来赋能通用的LLM智能体，使其能更好地处理长上下文任务。这完全符合保留条件，而非将其应用于特定化学或生物领域的智能体。 5.  **第五步：最终决策** 综合来看，这篇论文精准地定位了一个限制LLM通用能力发展（特别是长程推理和规划）的核心瓶颈——长上下文奖励建模。它提出了一种通用的解决方案，通过改进训练范式，直接提升了LLM在复杂任务中的基础能力。这项工作是方法论层面的创新，旨在推动LLM本身的通用推理能力上限，与您的核心研究目标高度契合。因此，应判定为 **True**。"
    },
    {
        "index": "#44",
        "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
        "link": "/arxiv/2510.06917",
        "arxiv_id": "2510.06917",
        "authors": "Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang",
        "summary": "Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user's turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally \"think while listening.\" In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at https://d223302.github.io/SHANKS/",
        "subjects": "Computation and Language, Audio and Speech Processing",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.139251",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是基于筛选标准的详细判断过程： 1.  **第一步：核心判断** - 论文的核心贡献是**SHANKS，一个通用的推理框架**。这个框架并非将LLM应用于某个特定领域，而是致力于改进LLM（或SLM）的推理过程本身。它提出的“边听边想”范式，是对当前LLM“听完再想”模式的根本性变革，旨在让模型进行实时的、连续的推理。这种对推理过程（何时思考、如何思考）的优化，直接触及了LLM的**基础能力和通用推理机制**。 - 论文中提到的“生成未言明的思维链推理”和“决定是否打断用户以及调用工具”，都是对模型逻辑、规划和问题解决能力的直接增强。这完全符合“改进LLM的基础能力……增强其逻辑、数学、规划、多步推理等通用能力”的定义。 2.  **第二步：正面指标** - 该论文命中了多个关键的正面指标： - **核心概念**: 明确提到了`Large language models (LLMs)`。 - **能力方向**: 核心主题是`reasoning`，特别是`chain-of-thought reasoning`。其实验案例直接评估了`math reasoning`（数学推理）能力。 - **新兴范式**: 论文涉及了`tool use`（工具调用），其整体框架可以催生出更具交互性和实时性的`llm-based agents`（基于LLM的智能体）。 3.  **第三步：排除标准** - **多模态与视觉**: 论文虽然涉及语音，但其核心并非处理视觉信息或复杂的多模态融合。它解决的是**语言模型在流式音频输入下的推理时机问题**，本质仍然是语言和符号推理，不属于应被排除的视觉或多模态研究范畴。 - **特定应用领域**: 尽管论文使用了“数学问题”作为示例，但其目的并非解决数学领域的某个具体问题，而是将数学推理作为一个公认的、衡量通用推理能力的标准测试床。这是方法论论文的常见做法，不属于特定应用领域的研究。 - **模型可靠性**: 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的框架是**通用的**，它让模型具备了在对话流中实时推理、决策（打断用户）和行动（调用工具）的能力。这是一种增强LLM通用问题解决能力的元框架，而不是针对特定领域（如化学）的智能体应用，因此完全符合保留标准。 **最终决策** 综上所述，论文SHANKS的本质是提出一种新的、用于增强大语言模型**通用推理能力**的框架。它通过改变推理的时间范式（从“听完再想”到“边听边想”），直接提升了模型在逻辑、规划和多步推理等任务上的表现和能力边界。这与您“提高大语言模型本身的『通用推理能力』”的核心目标高度一致，因此应被保留。"
    },
    {
        "index": "#47",
        "title": "$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences",
        "link": "/arxiv/2510.06870",
        "arxiv_id": "2510.06870",
        "authors": "Yining Wang, Jinman Zhao, Chuangxin Zhao, Shuhao Guan, Gerald Penn, Shinan Liu",
        "summary": "Reinforcement Learning with Human Feedback (RLHF) has been the dominant approach for improving the reasoning capabilities of Large Language Models (LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has simplified this paradigm by replacing the reward and value models with rule-based verifiers. A prominent example is Group Relative Policy Optimization (GRPO). However, GRPO inherently suffers from a length bias, since the same advantage is uniformly assigned to all tokens of a response. As a result, longer responses distribute the reward over more tokens and thus contribute disproportionately to gradient updates. Several variants, such as DAPO and Dr. GRPO, modify the token-level aggregation of the loss, yet these methods remain heuristic and offer limited interpretability regarding their implicit token preferences. In this work, we explore the possibility of allowing the model to learn its own token preference during optimization. We unify existing frameworks under a single formulation and introduce a learnable parameter $\\lambda$ that adaptively controls token-level weighting. We use $\\lambda$-GRPO to denote our method, and we find that $\\lambda$-GRPO achieves consistent improvements over vanilla GRPO and DAPO on multiple mathematical reasoning benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\\lambda$-GRPO improves average accuracy by $+1.9\\%$, $+1.0\\%$, and $+1.7\\%$ compared to GRPO, respectively. Importantly, these gains come without any modifications to the training data or additional computational cost, highlighting the effectiveness and practicality of learning token preferences.",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.145823",
        "filter_reason": "这篇论文完全符合您的研究主题，应该被保留。以下是详细的判断过程： **第一步：核心判断——这篇论文的本质是什么？** 这篇论文的本质是**改进大语言模型的训练范式**，以增强其通用推理能力。它的核心贡献是针对现有强化学习对齐方法（GRPO）中的“长度偏差”问题，提出了一种名为$\\lambda$-GRPO的新框架。通过引入一个可学习的参数来动态调整token级别的奖励权重，该方法优化了模型的训练过程，使其能更有效地学习如何进行推理。这直接属于“提出新的训练范式、增强其...多步推理等通用能力”的范畴，而非将LLM作为特定领域的工具。 **第二步：正面指标——论文是否包含以下主题？** 该论文高度匹配多个核心正面指标： - **核心概念**: 明确以“Large Language Models (LLMs)”为研究对象。 - **能力方向**: 核心目标是“improving the reasoning capabilities”，并在“multiple mathematical reasoning benchmarks”上验证效果。 - **训练方法**: 论文的核心内容就是对“Reinforcement Learning with Human Feedback (RLHF)”及其衍生框架（如GRPO）的改进，属于强化学习领域的创新。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 该论文不涉及任何排除标准中的领域。它没有讨论多模态、视觉，也没有针对医疗、化学等特定应用领域。虽然它在数学推理任务上进行了测试，但这通常被认为是评估LLM**通用推理**能力的标准基准，而非一个“特定领域应用”。论文的核心方法（可学习的token偏好）是通用的，可以应用于其他推理任务。 **第四步：处理特殊和模糊情况** - **关于评估领域**: 论文使用“数学推理”作为验证基准。这与将LLM应用于“化学”或“法律”等领域有本质区别。数学推理被广泛认为是衡量LLM逻辑、符号操作和复杂问题分解等**通用推理**能力的关键指标。因此，这不被视为特定应用领域，而是对核心能力的度量。 - **关于模型可靠性**: 论文通过优化训练过程来提升推理准确性，这间接提升了模型的可靠性。但它不是从应用层面（如添加水印）进行研究，而是从模型**内在的学习机制**上进行改进，因此符合保留条件。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是一种创新的强化学习训练框架，旨在解决LLM在多步推理中的一个普遍性问题（长度偏差），从而提升其内在的、通用的推理能力。它完全符合您筛选关于“大语言模型通用推理能力”前沿论文的核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#53",
        "title": "Adaptive Tool Generation with Models as Tools and Reinforcement Learning",
        "link": "/arxiv/2510.06825",
        "arxiv_id": "2510.06825",
        "authors": "Chenpeng Wang, Xiaojie Cheng, Chunye Wang, Linfeng Yang, Lei Zhang",
        "summary": "Tool-augmented language models have demonstrated strong capabilities, but their reliance on live API access creates scalability and reliability challenges during training and deployment. We propose MTR, a simulation-first training framework for tool-augmented reasoning. Instead of relying on live APIs, MTR learns from complete ReAct traces with schema-validated, simulated observations. Our approach operates through a multi-agent architecture where a ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an AutoAgent produces structured think-act-observe sequences, and a ToolActor simulates realistic responses. Training proceeds in two stages: Stage-1 Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy with a composite trace reward that balances answer correctness and internal consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue, 2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to live-API systems and excels on reasoning-intensive tasks, suggesting that effective tool reasoning can be learned from structured traces without live interactions.",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.148586",
        "filter_reason": "这篇论文完全符合研究范围，其核心贡献直接指向提升大语言模型的通用推理能力。 1.  **核心判断 (第一步):** 论文的本质是提出一种名为MTR的**全新训练框架**，旨在解决当前工具增强语言模型在训练和部署中依赖实时API的瓶颈。这并非将LLM作为工具应用于特定领域，而是从根本上改进LLM使用工具进行推理的**基础能力和训练范式**。其核心是增强模型自身的“tool-augmented reasoning”（工具增强推理）能力，这是一种通用能力。 2.  **正面指标 (第二步):** 论文高度契合多个正面指标： *   **能力方向:** 明确聚焦于 **reasoning**，特别是通过工具进行的 **multi-hop reasoning**（多跳推理）。 *   **训练方法:** 提出了一个包含监督微调（SFT）和强化学习（GRPO）的两阶段训练方法，直接命中了 **reinforcement learning (RL)** 这一关键主题。 *   **新兴范式:** 论文的核心是一个 **multi-agent systems**（多智能体系统）架构，包含ToolMaker、AutoAgent和ToolActor，这属于前沿的 **llm-based agents** 范式。同时，整个研究都围绕 **tool use** 展开。 3.  **排除标准 (第三步):** 论文完全不涉及任何排除标准。它没有讨论多模态、特定应用领域（如医疗、化学），也未关注水印、安全等应用层面的可靠性问题。其评估基准（HotpotQA等）是通用的多跳问答数据集，进一步证明了其通用性。 4.  **特殊和模糊情况 (第四步):** 论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的典型范例。其多智能体框架（ToolMaker生成工具、AutoAgent执行推理、ToolActor模拟环境）是一个通用的方法论，旨在让模型学会如何更好地与工具交互以完成复杂推理任务，而非针对某个特定领域。 **核心依据总结:** 该论文的核心贡献是MTR框架，一种通过模拟和强化学习来训练LLM掌握工具推理能力的新方法。它直接解决了如何提升LLM在通用、复杂任务上的推理表现这一核心问题，完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”的研究目标。"
    },
    {
        "index": "#50",
        "title": "SID: Multi-LLM Debate Driven by Self Signals",
        "link": "/arxiv/2510.06843",
        "arxiv_id": "2510.06843",
        "authors": "Xuhang Chen, Zhifan Song, Deyi Ji, Shuo Gao, Lanyun Zhu",
        "summary": "Large Language Models (LLMs) have exhibited impressive capabilities across diverse application domains. Recent work has explored Multi-LLM Agent Debate (MAD) as a way to enhance performance by enabling multiple LLMs to discuss and refine responses iteratively. Nevertheless, existing MAD methods predominantly focus on utilizing external structures, such as debate graphs, using LLM-as-a-Judge, while neglecting the application of self signals, such as token logits and attention, that arise during generation. This omission leads to redundant computation and potential performance degradation. In this paper, we shift the focus to the self signals of multi-LLM debate and introduce a Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of self-signals: model-level confidence and token-level semantic focus, to adaptively guide the debate process. Our approach enables high-confidence agents to exit early at the model level and compress the redundant debate contents based on the attention mechanism. We evaluate our method on various LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental results demonstrate that our method not only outperforms existing MAD techniques in accuracy but also reduces token consumption, highlighting the effectiveness of utilizing self signals in enhancing both the performance and efficiency of multi-agent debate systems. Our code will be available at~\\href{https://github.com/xuhang2019/SID}{\\texttt{https://github.com/xuhang2019/SID}}.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.147202",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心是提出一种名为“SID”的新方法，用于改进“多LLM智能体辩论”这一过程。MAD本身是一种旨在通过多个模型协作、讨论和迭代优化来提升最终答案质量的通用技术，其本质就是增强LLM的推理和问题解决能力。这篇论文并非将LLM应用于某个特定领域（如医疗、化学），而是专注于优化LLM之间协作推理的**方法论**。它通过利用模型内部的“self signals”（如置信度和注意力）来引导辩论，这是一种对LLM基础推理过程的改进和优化。因此，根据第一步的核心判断标准，这篇论文应该**保留**。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文包含了多个强相关的正面指标： *   **核心概念**: 明确提及 \"Large Language Models (LLMs)\"。 *   **能力方向**: 虽然摘要未直接使用\"reasoning\"一词，但\"Multi-LLM Agent Debate\"（多智能体辩论）本身就是一种提升模型推理能力的经典范式。论文的目标是\"enhance performance\"和\"outperforms existing MAD techniques in accuracy\"，这直接指向了提升模型的通用问题解决和推理质量。 *   **新兴范式**: 论文的主题就是 \"llm-based agents\" 和 \"multi-agent systems\"，完全命中。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文的主要焦点不涉及任何排除标准： *   **多模态与视觉**: 摘要中提到在\"Multimodal LLMs\"上进行了评估，但这只是为了验证方法的普适性。论文的核心方法SID是基于LLM的\"token logits\"和\"attention\"等通用信号，并非为多模态任务设计。因此，其核心贡献不属于多模态研究。 *   **特定应用领域**: 论文在\"multiple challenging benchmarks\"上评估，没有限定在任何特定应用领域。 *   **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 这篇论文是提出一种**通用的智能体协作框架（SID）**，其目的是通过更高效的辩论机制来增强LLM的通用问题解决能力。这完全符合“保留”的条件，是典型的提升LLM通用推理能力的研究。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种新颖的、通用的方法论（SID），通过利用LLM的内部信号来优化多智能体辩论过程，从而提升LLM在通用任务上的性能和效率。这直接命中了你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为**符合**。"
    },
    {
        "index": "#58",
        "title": "Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition",
        "link": "/arxiv/2510.06774",
        "arxiv_id": "2510.06774",
        "authors": "Lei Xu, Pierre Beckmann, Marco Valentino, André Freitas",
        "summary": "Neuro-symbolic NLP methods aim to leverage the complementary strengths of large language models and formal logical solvers. However, current approaches are mostly static in nature, i.e., the integration of a target solver is predetermined at design time, hindering the ability to employ diverse formal inference strategies. To address this, we introduce an adaptive, multi-paradigm, neuro-symbolic inference framework that: (1) automatically identifies formal reasoning strategies from problems expressed in natural language; and (2) dynamically selects and applies specialized formal logical solvers via autoformalization interfaces. Extensive experiments on individual and multi-paradigm reasoning tasks support the following conclusions: LLMs are effective at predicting the necessary formal reasoning strategies with an accuracy above 90 percent. This enables flexible integration with formal logical solvers, resulting in our framework outperforming competing baselines by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively. Moreover, adaptive reasoning can even positively impact pure LLM methods, yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT settings with GPT-4o. Finally, although smaller models struggle with adaptive neuro-symbolic reasoning, post-training offers a viable path to improvement. Overall, this work establishes the foundations for adaptive LLM-symbolic reasoning, offering a path forward for unifying material and formal inferences on heterogeneous reasoning challenges.",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.156107",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接致力于提升大语言模型的通用推理能力。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM基础能力。** 论文的核心是提出一个“自适应、多范式、神经符号推理框架”。这个框架的本质不是将LLM应用于某个特定领域，而是提出一种新的方法论，通过让LLM动态地识别推理策略并调用形式化逻辑求解器，来增强其自身的逻辑推理能力。这完全符合“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。论文明确指出，其方法“优于竞争基线”，并且“自适应推理甚至能对纯LLM方法产生积极影响”，这直接证明了其工作是在提升LLM的内在推理性能。 2.  **第二步：正面指标——论文高度相关。** 论文命中了多个关键的正面指标： *   **核心概念**: 论文标题和摘要反复提及 \"Large language models (LLMs)\"。 *   **能力方向**: 论文的核心是 \"reasoning\"，特别是 \"logical reasoning\" 和 \"formal inferences\"，这正是您关注的核心。 *   **新兴范式**: 该研究属于 \"tool use\" 范畴，它将“形式化逻辑求解器”作为一种通用工具，由LLM动态选择和使用，以解决复杂的推理问题。这是一种增强LLM通用问题解决能力的典型范式。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容与所有排除标准均无关系。它不涉及多模态、视觉，不针对医疗、化学等特定应用领域，也不讨论水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况——属于应保留的通用工具使用。** 论文提出的框架是一个通用的智能体/工具使用方法。它不是“用于化学实验自动化的智能体”，而是一个能够根据自然语言问题，自适应地选择和组合“形式化逻辑求解器”这一通用工具的框架。其目标是解决“异构推理挑战”，这完全符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”的原则。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种创新的、自适应的神经符号推理框架，旨在从根本上提升LLM的逻辑推理和问题解决能力。它不是应用型研究，而是方法论层面的突破，直接对准了“大语言模型通用推理能力”这一核心目标。因此，这篇论文是您研究课题下的高度相关且应保留的前沿文献。"
    },
    {
        "index": "#59",
        "title": "Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs",
        "link": "/arxiv/2510.06750",
        "arxiv_id": "2510.06750",
        "authors": "Jaeseong Lee, Dayoung Kwon, seung-won hwang",
        "summary": "Large Reasoning Models (LRMs) excel in structured tasks by emulating deliberate human reasoning but often suffer from overthinking, degrading performance and wasting resources. One possible baseline is to deploy both LLM and LRM, then route input by predicting whether it requires reasoning and may cause overthinking. However, deploying multiple models can be costly or impractical. We propose a superposed deployment strategy with a lightweight, training-free regulation to optimize inference by switching one model on and off. Instead of routing, we selectively unlearn from LRM at inference, scaling down computation while preserving reasoning. By analyzing the cumulative energy of singular values, we identify optimal low-rank projections to adjust reasoning just right.",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.178028",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Gold-Switch”的**推理优化方法**。它并非将LLM应用于特定领域，而是直接针对大型推理模型（LRM）在通用推理任务中存在的“过度思考”这一核心问题。通过一种免训练的、在推理时动态调整模型行为的技术（选择性遗忘、低秩投影），它旨在优化模型的推理过程，使其在保持推理能力的同时，提升效率和性能。这本质上是在**改进LLM的通用推理能力本身**，属于您筛选标准中“增强其逻辑、数学、规划、多步推理等通用能力”的范畴。因此，根据第一步，应予以**保留**。 2.  **第二步：正面指标** 论文高度契合多个正面指标： *   **核心概念**: 论文明确以“Large Reasoning Models (LRMs)”和“LLMs”为研究对象。 *   **能力方向**: 整篇论文都围绕“reasoning”展开，具体解决的是推理过程中的“overthinking”问题，目标是优化推理的“just right”程度。 *   **新兴范式**: 提出的“superposed deployment strategy”是一种新颖的推理范式，与思维链（CoT）等方法类似，都是探索如何更好地激发和调控LLM的推理能力。 3.  **第三步：排除标准** 论文完全没有触及任何排除标准： *   它不涉及多模态、视觉或特定应用领域（如医疗、化学）。 *   它关注的是模型推理的内在效率和效果，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 该论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，其焦点非常清晰：**如何优化LLM自身的推理过程**。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种创新的、通用的方法论，用于解决LLM在推理过程中的一个根本性挑战（过度思考）。它通过在推理阶段动态调控模型，直接提升了LLM的通用推理能力和效率。这与您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标完全一致。因此，最终判断为**True**。"
    },
    {
        "index": "#52",
        "title": "Mid-Training of Large Language Models: A Survey",
        "link": "/arxiv/2510.06826",
        "arxiv_id": "2510.06826",
        "authors": "Kaixiang Mo, Yuxin Shi, Weiwei Weng, Zhiqiang Zhou, Shuman Liu, Haibo Zhang, Anxiang Zeng",
        "summary": "Large language models (LLMs) are typically developed through large-scale pre-training followed by task-specific fine-tuning. Recent advances highlight the importance of an intermediate mid-training stage, where models undergo multiple annealing-style phases that refine data quality, adapt optimization schedules, and extend context length. This stage mitigates diminishing returns from noisy tokens, stabilizes convergence, and expands model capability in late training. Its effectiveness can be explained through gradient noise scale, the information bottleneck, and curriculum learning, which together promote generalization and abstraction. Despite widespread use in state-of-the-art systems, there has been no prior survey of mid-training as a unified paradigm. We introduce the first taxonomy of LLM mid-training spanning data distribution, learning-rate scheduling, and long-context extension. We distill practical insights, compile evaluation benchmarks, and report gains to enable structured comparisons across models. We also identify open challenges and propose avenues for future research and practice.",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.148137",
        "filter_reason": "这篇论文符合你的研究范围，判断过程如下： **第一步：核心判断** 这篇论文的核心是关于一种名为“中间训练”的LLM训练范式。它并非将LLM作为工具应用于特定领域，而是深入探讨和系统化了LLM在预训练和微调之间的一个关键训练阶段。论文提出，通过优化该阶段的数据质量、优化调度和上下文长度，可以“扩展模型能力”、“促进泛化和抽象”。这本质上是在改进LLM的基础构建过程，旨在打造一个能力更强、更通用的基础模型。这完全符合你筛选标准中“改进LLM的基础能力、提出新的训练范式”的要求。 **第二步：正面指标** - **核心概念**: 论文标题和摘要明确聚焦于“Large language models (LLMs)”。 - **能力方向**: 虽然摘要没有直接使用“reasoning”或“planning”等词汇，但它强调了“泛化”、“抽象”和“扩展模型能力”。这些是通用推理能力的基石。一个具有更强泛化和抽象能力的模型，其内在的推理潜力必然会得到提升。因此，这篇综述的内容与提升通用推理能力高度相关。 - **训练方法**: 论文的核心“中间训练”本身就是一种新颖的训练方法论探讨，涉及“学习率调度”和“课程学习”，这与“新的训练范式”紧密相关。 **第三步：排除标准** 该论文的焦点完全集中在LLM自身的训练过程上，没有涉及任何多模态、视觉内容，也没有深入到任何特定应用领域（如医疗、化学等），更不关乎模型水印、安全等应用层可靠性问题。因此，它成功地避开了所有的排除标准。 **第四步：处理特殊和模糊情况** 本文不涉及智能体/工具使用或幻觉/可解释性等特殊议题，无需额外判断。 **第五步：最终决策** 综合以上分析，这篇论文是一篇关于LLM基础训练方法的综述。它的核心贡献在于首次系统性地提出了“中间训练”这一范式，并论证了其对提升模型泛化和抽象能力的作用。这种对模型内在能力和训练机理的深挖，直接服务于构建更强大的基础LLM，而强大的基础能力是实现卓越通用推理能力的前提。因此，它完全符合你“致力于提高大语言模型（LLM）本身『通用推理能力』”的核心研究目标。"
    },
    {
        "index": "#66",
        "title": "How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects",
        "link": "/arxiv/2510.06700",
        "arxiv_id": "2510.06700",
        "authors": "Leonardo Bertolazzi, Sandro Pezzelle, Raffaelle Bernardi",
        "summary": "Both humans and large language models (LLMs) exhibit content effects: biases in which the plausibility of the semantic content of a reasoning problem influences judgments regarding its logical validity. While this phenomenon in humans is best explained by the dual-process theory of reasoning, the mechanisms behind content effects in LLMs remain unclear. In this work, we address this issue by investigating how LLMs encode the concepts of validity and plausibility within their internal representations. We show that both concepts are linearly represented and strongly aligned in representational geometry, leading models to conflate plausibility with validity. Using steering vectors, we demonstrate that plausibility vectors can causally bias validity judgements, and vice versa, and that the degree of alignment between these two concepts predicts the magnitude of behavioral content effects across models. Finally, we construct debiasing vectors that disentangle these concepts, reducing content effects and improving reasoning accuracy. Our findings advance understanding of how abstract logical concepts are represented in LLMs and highlight representational interventions as a path toward more logical systems.",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.187879",
        "filter_reason": "根据您提供的筛选标准，我对这篇论文进行了严格的评估，判断其完全符合研究范围。详细判断过程如下： **第一步：核心判断——这篇论文的本质是什么？** 这篇论文的本质不是应用LLM解决特定领域问题，而是深入探究并试图修复LLM在**通用推理能力**上的一个核心缺陷。其核心贡献如下： 1.  **诊断问题**: 论文精确指出了LLM在逻辑推理中存在的“内容效应”问题，即模型会将语义的“合理性”与逻辑的“有效性”相混淆，这是一种根本性的推理偏差。 2.  **揭示机制**: 通过表征分析，论文揭示了这种混淆的内在机制——两种概念在模型的内部表示空间中被高度对齐。 3.  **提出解决方案**: 最关键的是，论文没有止步于分析，而是提出了一种新的干预方法——**“解偏置导向向量”**。通过这种方法，论文成功地**在模型内部解耦了“合理性”与“有效性”这两个概念，从而减少了推理偏差并提升了模型的逻辑推理准确性**。 这完全符合“改进LLM的基础能力、增强其逻辑、通用能力”的核心要求。它提出了一种新的、在表征层面进行干预的方法论来提升模型的内在推理质量。 **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文研究的主题明确是 \"Large language models (LLMs)\"。 - **能力方向**: 论文的主题直指 \"logical reasoning\"，并对推理中的偏差进行了深入分析和修正，是高度相关的论文。 - **训练方法**: 论文虽然未使用RLHF或进化训练，但提出了基于表征操作的“解偏置向量”这一新型范式，属于提升模型能力的前沿方法论探索。 - **新兴范式**: 论文虽然不直接涉及智能体或工具使用，但其对模型内部概念的精细操控和因果分析，为构建更强大、更可靠的LLM系统（包括智能体）提供了重要基础。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施（如硬件加速、水印）。它是一个纯粹聚焦于LLM内在通用推理机制的研究。 **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: “内容效应”可以被看作是LLM推理过程中的一种系统性错误或“幻觉”。这篇论文提出了一种新方法（解偏置向量）来从根本上减少此类错误，从而**提升了模型的通用可靠性和推理质量**。同时，其表征分析本身就是一种增强模型内在可解释性的工作。因此，它完全符合“提出新方法来减少幻觉，从而提升模型的通用可靠性和推理质量，应该保留”的标准。 **第五步：最终决策** 综合以上所有分析，这篇论文不仅深刻分析了LLM在逻辑推理上的一个关键弱点，更重要的是提出了一种新颖且有效的内在干预方法来修正这一弱点，从而直接提升了模型的**通用推理能力**。其研究深度、方法论创新性和与核心目标的契合度都非常高，因此应被筛选保留。"
    },
    {
        "index": "#71",
        "title": "Aligning Large Language Models via Fully Self-Synthetic Data",
        "link": "/arxiv/2510.06652",
        "arxiv_id": "2510.06652",
        "authors": "Shangjian Yin, Zhepei Wei, Xinyu Zhu, Wei-Lin Chen, Yu Meng",
        "summary": "Traditional reinforcement learning from human feedback (RLHF) for large language models (LLMs) relies on expensive human-annotated datasets, while Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs, requiring the collection of diverse prompts and corresponding responses, often necessitating external reward models or proprietary models like GPT-4 to annotate preference pairs. In this work, we introduce Self-Alignment Optimization (SAO), a fully self-synthetic framework for LLM alignment, where all training data, including prompts (i.e., user queries), responses, and preferences, are generated by the model itself. Specifically, SAO first instructs the LLM to engage in persona role-play and generate diverse prompts and responses, which are then self-evaluated for preference optimization. Extensive experiments demonstrate that SAO effectively enhances the model's chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining strong performance on downstream objective tasks (e.g., question-answering, math reasoning). Our work provides a practical solution for self-improvement in aligning LLMs, and the code for reproducing our results is available at: https://github.com/SJY8460/SAO.",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.190278",
        "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **论文核心贡献**: 该论文提出了一种名为“Self-Alignment Optimization (SAO)”的全新训练范式。其核心思想是让LLM完全依靠自身生成的数据（包括提示、回答和偏好）来进行自我对齐和优化。 - **符合性分析**: 这完全符合“改进LLM的基础能力”和“提出新的训练范式”的标准。SAO是一种方法论层面的创新，旨在让模型实现“自我进化”，这是提升模型通用能力的关键路径。它不是将LLM应用于特定领域，而是直接作用于LLM本身，因此通过了核心判断。 2.  **第二步：正面指标** - **核心概念**: 论文明确以“Large language models (LLMs)”为核心研究对象。 - **能力方向**: 摘要中明确指出，该方法在提升模型对话能力的同时，能“maintaining strong performance on downstream objective tasks (e.g., question-answering, **math reasoning**)”。这直接关联到“通用推理能力”中的数学推理方向。 - **训练方法**: 论文直接对标并改进了RLHF和RLAIF，属于“reinforcement learning (RL)”范畴。其“fully self-synthetic”和“self-improvement”的特性，完美契合了“evolution, self-evolve”这一前沿主题。 - **新兴范式**: SAO框架本身就是一种“自我进化”的新兴范式，旨在让模型摆脱对外部数据（人类或强模型标注）的依赖，实现自主能力提升。 3.  **第三步：排除标准** - 论文内容完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施（如水印、安全部署）。因此，它没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **对齐与推理**: 虽然论文的主题是“对齐”，但它提出的方法（SAO）是一种底层的训练技术。一个对齐更好的模型，其遵循指令、生成逻辑连贯、事实准确内容的能力会更强，这直接提升了其通用推理的质量和可靠性。论文通过在数学推理等任务上保持性能来证明这一点，表明其方法并非牺牲核心能力去换取对话的流畅性，而是从根本上提升模型的综合能力。这符合“提升模型的通用可靠性和推理质量”的保留原则。 **最终决策**: 该论文提出了一种创新的、通用的LLM自我进化训练框架（SAO）。它通过让模型自我生成和评估数据来优化自身，这是一种直接作用于LLM基础能力的方法论。论文明确验证了该方法在保持数学推理等核心能力上的有效性，这与“提升大语言模型通用推理能力”的核心目标高度一致。因此，这篇论文是高质量的前沿研究，应被纳入筛选范围。"
    },
    {
        "index": "#65",
        "title": "Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management",
        "link": "/arxiv/2510.06727",
        "arxiv_id": "2510.06727",
        "authors": "Miao Lu, Weiwei Sun, Weihua Du, Zhan Ling, Xuesong Yao, Kang Liu, Jiecao Chen",
        "summary": "We study reinforcement learning (RL) fine-tuning of large language model (LLM) agents for long-horizon multi-turn tool use, where context length quickly becomes a fundamental bottleneck. Existing RL pipelines can suffer from degraded instruction following, excessive rollout costs, and most importantly, strict context limits. To address these challenges, we introduce summarization-based context management to training. In specific, it periodically compresses the tool using history by LLM-generated summaries that retain task-relevant information to keep a compact context while enabling the agent to scale beyond the fixed context window. Building on this formulation, we derive a policy gradient representation that seamlessly enables standard LLM RL infrastructures to optimize both tool-use behaviors as well as summarization strategies in an end-to-end fashion. We instantiate this framework with \\underline{SU}mmarization augmented \\underline{P}olicy \\underline{O}ptimization (\\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond a fixed context limit. Experiments on interactive function calling and searching tasks demonstrate that \\texttt{SUPO} significantly improves the success rate while maintaining the same or even lower working context length compared to baselines. We also demonstrate that for complex searching tasks, \\texttt{SUPO} can further improve the evaluation performance when scaling test-time maximum round of summarization beyond that of training time. Our results establish summarization-based context management as a principled and scalable approach for training RL agents beyond a fixed context length limit.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.182153",
        "filter_reason": "这篇论文完全符合你的筛选标准，应该被保留。以下是我的详细判断过程： 1.  **核心判断（第一步）**: 论文的核心贡献是提出了一种名为 `SUPO` 的新训练范式，它通过“基于摘要的端到端上下文管理”方法，解决了大语言模型智能体在长时程、多轮任务中面临的上下文窗口瓶颈问题。这本质上是一种**改进LLM自身基础能力**的研究。它致力于让LLM能够处理更长的任务历史，从而进行更复杂的规划和多步推理。这完全符合筛选标准中“改进LLM的基础能力、提出新的训练范式、增强其...规划、多步推理等通用能力”的要求。它不是将LLM作为工具应用于某个特定领域。 2.  **正面指标（第二步）**: 论文高度符合多个正面指标： *   **核心概念**: 明确聚焦于 \"Large language models (LLMs)\" 和 \"LLM agents\"。 *   **能力方向**: 研究目标是提升 \"long-horizon multi-turn tool use\" 能力，这直接关系到模型的 \"planning\" 和 \"problem-solving\" 能力，是通用推理的核心组成部分。 *   **训练方法**: 核心方法论是 \"reinforcement learning (RL) fine-tuning\"，并提出了新的算法 `SUPO`，这属于筛选标准中的关键方法。 *   **新兴范式**: 论文的研究对象是 \"llm-based agents\" 和 \"tool use\"，这是当前提升LLM推理能力的热门范式。 3.  **排除标准（第三步）**: 论文没有触及任何主要的排除领域。它不涉及多模态、视觉，也不针对医疗、化学等特定应用领域。论文的实验是基于通用的“交互式函数调用”和“搜索任务”，这些都是通用型任务，而非领域特定问题。 4.  **特殊和模糊情况（第四步）**: 这篇论文是“智能体/工具使用”特殊情况的完美例证。它提出的是一种**通用的智能体训练框架**（`SUPO`），旨在通过优化上下文管理来增强LLM智能体的**通用问题解决能力**，而不是将其限制在某一特定领域（如化学实验自动化）。因此，它应该被保留。 **最终决策（第五步）**: 综上所述，这篇论文的本质是提出一种创新的训练框架（`SUPO`），通过解决上下文长度这一根本性瓶颈，来增强大语言模型智能体在长时程任务中的规划和多步推理能力。它的核心贡献、研究方法和目标方向都与“提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标高度一致。因此，这是一篇非常相关且有价值的前沿论文，应予以保留。"
    },
    {
        "index": "#75",
        "title": "The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law",
        "link": "/arxiv/2510.06559",
        "arxiv_id": "2510.06559",
        "authors": "Cheonkam Jeong, Sungdo Kim, Jewoo Park",
        "summary": "Contemporary language models are fluent yet routinely mis-handle the types of meaning their outputs entail. We argue that hallucination, brittle moderation, and opaque compliance outcomes are symptoms of missing type-theoretic semantics rather than data or scale limitations. Building on Montague's view of language as typed, compositional algebra, we recast alignment as a parsing problem: natural-language inputs must be compiled into structures that make explicit their descriptive, normative, and legal dimensions under context. We present Savassan, a neuro-symbolic architecture that compiles utterances into Montague-style logical forms and maps them to typed ontologies extended with deontic operators and jurisdictional contexts. Neural components extract candidate structures from unstructured inputs; symbolic components perform type checking, constraint reasoning, and cross-jurisdiction mapping to produce compliance-aware guidance rather than binary censorship. In cross-border scenarios, the system \"parses once\" (e.g., defect claim(product x, company y)) and projects the result into multiple legal ontologies (e.g., defamation risk in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into a single, explainable decision. This paper contributes: (i) a diagnosis of hallucination as a type error; (ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii) a production-oriented design that embeds typed interfaces across the pipeline. We outline an evaluation plan using legal reasoning benchmarks and synthetic multi-jurisdiction suites. Our position is that trustworthy autonomy requires compositional typing of meaning, enabling systems to reason about what is described, what is prescribed, and what incurs liability within a unified algebra of meaning.",
        "subjects": "Computation and Language, Artificial Intelligence, Logic in Computer Science",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.192133",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于提出了一种从根本上提升大语言模型通用推理能力的新范式。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心并非将LLM应用于法律领域，而是诊断并试图解决LLM的一个根本性缺陷：对“意义”的结构化处理能力不足。作者明确指出，幻觉等问题是“缺少类型论语义”的症状，而非数据或规模的限制。这直接触及了LLM推理能力的根基。论文提出的Savassan神经符号架构，旨在通过将自然语言编译为形式化的逻辑结构，来增强模型内在的逻辑、约束和规范推理能力。这是一种对模型基础架构和认知范式的革新，完全符合“改进LLM的基础能力”和“增强其逻辑、多步推理等通用能力”的标准。 2.  **第二步：正面指标——论文高度相关。** - **核心概念**: 论文直接针对Contemporary language models (LLMs)。 - **能力方向**: 论文的核心是**reasoning**，特别是**logical reasoning**和**constraint reasoning**。它旨在让模型能够推理“被描述的内容、被规定的内容以及产生责任的内容”，这是一种高级的通用问题解决能力。 - **新兴范式**: 论文提出了一个**neuro-symbolic architecture**，这是当前提升模型推理能力的一个前沿研究方向。它将神经网络的感知能力与符号逻辑的严谨推理相结合，是增强模型通用性的有力途径。 3.  **第三步：排除标准——论文并未被排除。** - **特定应用领域**: 虽然论文以法律和商业场景作为示例和评测基准，但这只是为了展示其框架在复杂、高要求任务上的有效性。论文的焦点是那个“统一的、组合式的意义代数”框架本身，而不是某个特定的法律应用。它强调的是框架的通用性（如“跨司法管辖映射”），因此不应被视为“特定应用领域”的研究。这与“用于化学实验自动化的智能体”有着本质区别，后者是领域驱动的，而本文是方法驱动的。 - **模型可靠性**: 论文虽然涉及了安全与合规，但它不是从应用层面（如水印、内容过滤）去解决，而是从模型内在的语义表示和推理机制入手，通过提升推理质量来根除问题。这符合“提升模型的通用可靠性和推理质量”的保留标准。 4.  **第四步：处理特殊和模糊情况——论文是正面典型。** 论文是“提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的绝佳范例。它将“幻觉”重新定义为“类型错误”，并提出了一套形式化的系统来纠正它，这正是从根源上提升模型推理质量和可靠性的研究。 **最终决策**: 综合来看，这篇论文的本质是提出一种基于形式语义学和神经符号结合的新架构，以解决LLM在通用推理（尤其是逻辑和规范推理）上的根本性缺陷。它使用法律领域作为其强大推理能力的“试验场”，但其贡献是具有普适性的方法论。因此，这篇论文与“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度契合，应当被保留。"
    },
    {
        "index": "#72",
        "title": "A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures",
        "link": "/arxiv/2510.06640",
        "arxiv_id": "2510.06640",
        "authors": "Nhat M. Hoang, Do Xuan Long, Cong-Duy Nguyen, Min-Yen Kan, Luu Anh Tuan",
        "summary": "State Space Models (SSMs) have recently emerged as efficient alternatives to Transformer-Based Models (TBMs) for long-sequence processing, offering linear scaling and lower memory use. Yet, how contextual information flows across layers and tokens in these architectures remains understudied. We present the first unified, token- and layer-level analysis of representation propagation in SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing, we characterize how representations evolve within and across layers. We find a key divergence: TBMs rapidly homogenize token representations, with diversity reemerging only in later layers, while SSMs preserve token uniqueness early but converge to homogenization deeper. Theoretical analysis and parameter randomization further reveal that oversmoothing in TBMs stems from architectural design, whereas in SSMs it arises mainly from training dynamics. These insights clarify the inductive biases of both architectures and inform future model and training designs for long-context reasoning.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.190753",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心并非提出一种新的、可直接应用的推理方法（如新的CoT变体），而是对两种主流架构（Transformer和State-Space Models）在处理长序列信息时的内在机制进行深入、基础性的分析。它探究的是“上下文信息如何流动”以及“表征如何演化”这一根本性问题。虽然它不是一种“方法论”研究，但它直接触及了影响LLM通用推理能力（尤其是长上下文推理）的核心瓶颈——例如，信息是如何在模型深层被保留或丢失的（过平滑问题）。这种对模型内在机理的剖析，是“改进LLM的基础能力”和“增强其通用能力”的必要前提和理论基石。因此，其本质是服务于提升模型能力的基础研究，应予以保留。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文明确提到了与LLM推理能力高度相关的主题。虽然标题和摘要中没有直接出现\"reasoning\"这一高频词，但其研究目标——\"inform future model and training designs for long-context reasoning\"（为未来的长上下文推理模型和训练设计提供信息）——直接点明了其与推理能力的强关联。长上下文推理是通用推理能力的关键组成部分。论文通过分析表征流，揭示了不同架构在处理长程依赖时的优缺点，这为设计更擅长推理的模型提供了关键洞见。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 该论文完全没有涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（水印、安全）等排除标准。它聚焦于模型架构本身的理论分析，完全避开了这些排除项。 4.  **第四步：处理特殊和模糊情况** 本论文不属于智能体/工具使用或幻觉/可解释性等特殊情况的范畴。它的模糊之处在于其“分析性”而非“方法论性”的定位。然而，正如第一步所分析的，对于旨在“提高LLM本身通用推理能力”的研究者而言，理解现有模型的内在缺陷和优势（如本文揭示的过平滑问题及其根源）是提出更优模型和方法的第一步。这种基础性分析工作为未来的方法论创新指明了方向，其价值与提出新方法同等重要，甚至更为根本。 5.  **第五步：最终决策** 综合来看，这篇论文虽然不是一篇直接提出新推理技巧的“方法论文”，但它是一篇深刻的“机理分析论文”。它通过严谨的实验和理论分析，揭示了影响LLM长上下文推理能力的核心架构特性（表征同质化/过平滑）。这些发现直接为“如何设计出推理能力更强的模型”这一核心目标提供了理论指导和设计依据。对于一位顶尖的人工智能研究员来说，这类能够揭示模型本质、启发未来设计方向的基础性研究是极具价值的，完全符合“致力于提高大语言模型本身的通用推理能力”这一研究课题的范畴。因此，最终判断为保留。"
    },
    {
        "index": "#78",
        "title": "Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels",
        "link": "/arxiv/2510.06499",
        "arxiv_id": "2510.06499",
        "authors": "Zhepeng Cen, Haolin Chen, Shiyu Wang, Zuxin Liu, Zhiwei Liu, Ding Zhao, Silvio Savarese, Caiming Xiong, Huan Wang, Weiran Yao",
        "summary": "Large Language Models (LLMs) have achieved remarkable success through imitation learning on vast text corpora, but this paradigm creates a training-generation gap and limits robust reasoning. Reinforcement learning (RL) offers a more data-efficient solution capable of bridging this gap, yet its application has been constrained by a critical data bottleneck: existing RL datasets are orders of magnitude smaller and less diverse than web-scale pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a scalable data engine that systematically converts large-scale pre-training documents into millions of diverse, verifiable question-answer pairs for RL. Using this pipeline, we construct the Webscale-RL dataset, containing 1.2 million examples across more than 9 domains. Our experiments show that the model trained on this dataset significantly outperforms continual pretraining and strong data refinement baselines across a suite of benchmarks. Notably, RL training with our dataset proves substantially more efficient, achieving the performance of continual pre-training with up to 100$\\times$ fewer tokens. Our work presents a viable path toward scaling RL to pre-training levels, enabling more capable and efficient language models.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-07",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.198682",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM基础能力。** 这篇论文的核心贡献是提出了一个名为“Webscale-RL”的自动化数据管道。其目的不是将LLM应用于某个特定领域，而是为了解决强化学习（RL）在训练LLM时面临的“数据瓶颈”问题。论文明确指出，当前基于模仿学习的范式限制了模型的“鲁棒推理”能力，而RL是缩小“训练-生成差距”的关键。因此，这篇论文的本质是提出一种**新的训练范式和数据工程方法**，旨在通过大规模RL数据来从根本上提升LLM的推理能力。这完全符合“改进LLM的基础能力”、“提出新的训练范式”和“增强其逻辑...多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——论文高度相关。** 论文摘要中包含了多个关键的正面指标： *   **核心概念**: 明确提到 \"Large Language Models (LLMs)\"。 *   **能力方向**: 直接点出研究动机是提升 \"robust reasoning\"（鲁棒推理）。 *   **训练方法**: 核心内容就是 \"Reinforcement learning (RL)\"，并致力于将其扩展到预训练规模。 这些指标都强烈表明该论文与“大语言模型通用推理能力”这一主题高度相关。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容是通用的数据管道和训练方法，完全没有涉及多模态、视觉、医疗、化学、机器人等任何特定应用领域。同时，它也不关注模型部署、硬件加速或水印、安全等应用层面的可靠性问题。因此，它没有触犯任何排除标准。 4.  **第四步：处理特殊和模糊情况——不适用。** 这篇论文不涉及智能体/工具使用或幻觉/安全等特殊情况的讨论，因此无需进行特殊判断。 5.  **第五步：最终决策。** 综合以上分析，这篇论文是一项基础性的方法论研究。它没有将LLM视为工具，而是聚焦于如何通过解决RL的数据瓶颈问题，来**直接提升LLM内核的通用推理能力和训练效率**。其目标是“enabling more capable and efficient language models”（实现更强大、更高效的语言模型），这与你的核心目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#82",
        "title": "MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning",
        "link": "/arxiv/2510.06430",
        "arxiv_id": "2510.06430",
        "authors": "Neeraja Kirtane, Yuvraj Khanna, Peter Relan",
        "summary": "Large language models excel on math benchmarks, but their math reasoning robustness to linguistic variation is underexplored. While recent work increasingly treats high-difficulty competitions like the IMO as the gold standard for evaluating reasoning, we believe in comprehensive benchmarking of high school-level math problems in real educational settings. We introduce MathRobust-LV, a test set and evaluation methodology that mirrors how instructors rephrase problems across assessments while keeping difficulty constant: we change surface details (names, contexts, variables) while preserving numerical structure and answers. In contrast to prior efforts that alter problem content or emphasize IMO-level tasks, we focus on high-school-level dataset problems at the difficulty level where models are currently deployed in educational settings: tutoring and assessment systems. In these applications, instructors rephrase identical concepts in varied ways, making linguistic robustness essential for reliable deployment. Although MATH data benchmarking is often regarded as saturated, our experiment on 34 models reveals that accuracy declines when moving from the baseline to the variants. These drops are severe for smaller models (9-11%) while stronger models also show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain comparatively stable. Our results highlight that robustness to linguistic variation is a fundamental challenge, exposing reasoning vulnerabilities in models.",
        "subjects": "Computation and Language",
        "date": "2025-10-07",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.200467",
        "filter_reason": "这篇论文符合筛选要求，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一个新的评估基准（MathRobust-LV）和评估方法论，用于衡量大语言模型在数学推理任务中对语言变化的鲁棒性。虽然它没有直接提出一种新的训练范式或架构来“提高”模型能力，但它精准地“诊断”了现有LLM在通用推理能力（特别是数学推理）上的一个根本性缺陷——对表面语言变化的脆弱性。这种诊断性工作是推动领域进步的关键一步，它为未来如何“提高”模型通用推理能力（例如，通过设计新的训练数据或目标函数来增强这种鲁棒性）指明了明确的方向和提供了衡量标准。因此，这篇论文的本质是致力于理解和衡量LLM的通用推理能力，属于该研究范围的核心。 2.  **第二步：正面指标** 论文明确包含了多个关键的正面指标： *   **核心概念**: \"Large language models\" (LLMs)。 *   **能力方向**: \"mathematical reasoning\"，这是通用推理能力的一个核心子集。 论文的核心内容紧密围绕这些主题展开。 3.  **第三步：排除标准** 论文不触及任何主要的排除领域： *   它不涉及多模态或视觉。 *   它虽然使用“高中数学”作为测试场景，但其目的并非解决某个特定的教育或科学领域问题，而是评估模型的通用认知能力，因此不属于“特定应用领域”的排除范畴。 *   它研究的是推理的内在脆弱性，而非应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文的情况与“幻觉/可解释性/安全”的模糊情况类似。它没有提出一种新方法来“减少”某种推理缺陷，但它提出了一种新方法来“暴露”这种缺陷。根据筛选标准，“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” 这篇论文通过创建一个严谨的测试集，揭示了模型在看似不变的问题上因语言变化而导致的推理质量下降，这直接关系到“通用可靠性和推理质量”。它为提升这一质量提供了必要的评估工具和深刻洞见，因此应该被保留。 5.  **第五步：最终决策** 综合以上分析，该论文通过创建一个新的评估基准，深入探究并量化了LLM在通用推理能力（数学推理）上的一个关键弱点。这项工作对于理解当前LLM能力的边界、以及未来如何系统性地提升其通用推理能力具有重要的指导意义。它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。因此，最终判断为保留。"
    },
    {
        "index": "#96",
        "title": "A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation",
        "link": "/arxiv/2510.06265",
        "arxiv_id": "2510.06265",
        "authors": "Aisha Alansari, Hamzah Luqman",
        "summary": "Large language models (LLMs) have transformed natural language processing, achieving remarkable performance across diverse tasks. However, their impressive fluency often comes at the cost of producing false or fabricated information, a phenomenon known as hallucination. Hallucination refers to the generation of content by an LLM that is fluent and syntactically correct but factually inaccurate or unsupported by external evidence. Hallucinations undermine the reliability and trustworthiness of LLMs, especially in domains requiring factual accuracy. This survey provides a comprehensive review of research on hallucination in LLMs, with a focus on causes, detection, and mitigation. We first present a taxonomy of hallucination types and analyze their root causes across the entire LLM development lifecycle, from data collection and architecture design to inference. We further examine how hallucinations emerge in key natural language generation tasks. Building on this foundation, we introduce a structured taxonomy of detection approaches and another taxonomy of mitigation strategies. We also analyze the strengths and limitations of current detection and mitigation approaches and review existing evaluation benchmarks and metrics used to quantify LLMs hallucinations. Finally, we outline key open challenges and promising directions for future research, providing a foundation for the development of more truthful and trustworthy LLMs.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.211858",
        "filter_reason": "**第一步：核心判断** 这篇论文的本质是针对大语言模型（LLM）中一个普遍存在的核心缺陷——“幻觉”——进行全面的综述、分类和分析。它并非将LLM作为工具应用于某个特定领域（如医疗、法律），而是直指LLM本身在生成内容时的可靠性和真实性问题。一个可靠的推理过程必须建立在事实准确和逻辑连贯的基础上，而“幻觉”正是对这一基础的直接破坏。因此，研究如何理解和减轻幻觉，本质上是在探索如何提升LLM推理输出的**质量与可靠性**，这完全符合“改进LLM的基础能力”的大方向。 **第二步：正面指标** 论文的核心是关于**Large language models (LLMs)**。其主题“hallucination”与**reasoning**能力高度负相关，因为幻觉是推理失败的一种极端表现。论文旨在通过分析原因、检测方法和缓解策略，最终目标是“为开发更真实和更可信的LLMs奠定基础”，这与提升模型整体问题解决能力的目标一致。虽然它没有提出新的训练方法，但它系统性地梳理了现有旨在提升模型可靠性的方法。 **第三步：排除标准** 该论文完全避开了所有排除标准。它的研究范围不涉及**多模态与视觉**，不聚焦于任何**特定应用领域**，也不是关于模型部署的**水印**等应用层面的可靠性技术。它关注的是模型内在的生成质量。 **第四步：处理特殊和模糊情况** 这篇论文是关于“幻觉”的，属于筛选标准中的特殊情况。标准指出：“如果论文提出一种新方法来减少幻觉...从而提升模型的通用可靠性和推理质量，应该保留。” 虽然这篇论文是**综述（Survey）**而非提出新方法，但它通过系统性地梳理幻觉的成因、检测和缓解策略，为研究者构建了完整的知识图谱。它清晰地指出了现有方法的优缺点和未来的研究方向。任何想要“提出一种新方法来减少幻觉以提升推理质量”的研究，都必须首先建立在这类综述工作的基础上。因此，这篇综述直接服务于提升LLM通用推理能力和可靠性的研究目标，是相关研究中不可或缺的基石性文献。 **第五步：最终决策** 综合以上分析，这篇论文虽然是一篇综述，但其核心议题“幻觉”是制约LLM通用推理能力的关键瓶颈。论文通过对该问题的系统性梳理，为未来提升模型推理的真实性和可靠性指明了道路，与研究课题“提高大语言模型本身的通用推理能力”高度契合。因此，这篇论文应当被**保留**。"
    },
    {
        "index": "#112",
        "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces",
        "link": "/arxiv/2510.06953",
        "arxiv_id": "2510.06953",
        "authors": "Minju Gwak, Guijin Son, Jaehyung Kim",
        "summary": "The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stable flow of information. In this work, we revisit this principle in the context of large language model (LLM) reasoning traces, asking whether step-level uniformity reflects reasoning quality. To this end, we propose an entropy-based stepwise information density metric and introduce two complementary measures of uniformity, local and global uniformity scores. Across the experiments on six different reasoning benchmarks, we find that step-level uniformity not only provides a strong theoretical lens but also yields practical performance benefits; for example, selecting reasoning traces with more uniform information density at the step-level improves accuracy by 10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals that correct reasoning traces tend to avoid sharp information density spikes, while incorrect traces exhibit irregular information bursts. These results demonstrate that UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality. Results highlight the uniformity of the information density as a robust diagnostic and selection criterion for building more reliable and accurate reasoning systems.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.252549",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是根据筛选标准进行的详细判断： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心是提出一种新的方法论来评估和提升大语言模型的推理能力。它没有将LLM作为工具应用于特定领域，而是深入分析了LLM在执行推理任务时产生的“推理轨迹”。论文的核心贡献是提出了一种基于“均匀信息密度”假设的度量标准，用于判断推理过程的质量，并证明了通过选择信息密度更均匀的推理轨迹，可以显著提升模型在多个通用推理基准上的准确率。这直接属于“改进LLM的基础能力”和“增强其逻辑、数学、多步推理等通用能力”的范畴。因此，根据第一步，应**保留**。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文高度相关，包含了多个关键正面指标： *   **核心概念**: 明确以 \"Large language models (LLMs)\" 为研究对象。 *   **能力方向**: 论文的核心就是 \"reasoning\"，并在六个不同的 \"reasoning benchmarks\" 上进行了验证。 *   **新兴范式**: 虽然没有直接提及智能体或工具使用，但其研究内容——分析和优化“推理轨迹”——是构建高效LLM智能体和复杂问题解决系统的基础。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全不涉及任何排除标准领域。它没有研究多模态、视觉，也没有聚焦于医疗、化学等特定应用领域，更不是关于水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文可以被视为对“模型可靠性”和“可解释性”的深入研究，但其角度完全符合你的要求。它不是从应用层面讨论安全，而是提出一种新方法（信息密度度量）来增强模型**内在的推理质量和可靠性**。通过揭示“正确推理轨迹倾向于避免信息密度尖峰”这一规律，论文为理解LLM的推理过程提供了新的可解释性视角，并利用这一洞察来构建更准确的推理系统。这完全符合“提出一种新方法来减少幻觉、增强模型内在的可解释性...从而提升模型的通用可靠性和推理质量，应该保留”的原则。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种创新的、理论驱动的度量标准，用于评估和筛选LLM的推理过程，从而直接提升其通用推理能力。它不涉及特定应用，而是聚焦于LLM核心能力的改进，与你的研究目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度契合。因此，最终判断为**符合**。"
    },
    {
        "index": "#109",
        "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas",
        "link": "/arxiv/2510.07091",
        "arxiv_id": "2510.07091",
        "authors": "Baixuan Xu, Tianshi Zheng, Zhaowei Wang, Hong Ting Tsang, Weiqi Wang, Tianqing Fang, Yangqiu Song",
        "summary": "Enabling LLMs to effectively operate long-horizon task which requires long-term planning and multiple interactions is essential for open-world autonomy. Conventional methods adopt planning with actions where a executable action list would be provided as reference. However, this action representation choice would be impractical when the environment action space is combinatorial exploded (e.g., open-ended real world). This naturally leads to a question: As environmental action space scales, what is the optimal action representation for long-horizon agents? In this paper, we systematically study the effectiveness of two different action representations. The first one is conventional planning with actions (PwA) which is predominantly adopted for its effectiveness on existing benchmarks. The other one is planning with schemas (PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ] to [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable scalability. This alternative is motivated by its alignment with human cognition and its compliance with environment-imposed action format restriction. We propose cognitive bandwidth perspective as a conceptual framework to qualitatively understand the differences between these two action representations and empirically observe a representation-choice inflection point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve as evidence of the need for scalable representations. We further conduct controlled experiments to study how the location of this inflection point interacts with different model capacities: stronger planning proficiency shifts the inflection rightward, whereas better schema instantiation shifts it leftward. Finally, noting the suboptimal performance of PwS agents, we provide an actionable guide for building more capable PwS agents for better scalable autonomy.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.251040",
        "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断——论文本质分析** 论文的核心贡献是提出并系统研究了一种新的规划范式——“规划与模式”（Planning with Schemas, PwS），以解决现有“规划与行动”方法在长时程、大空间任务中遇到的瓶颈。这直接对应了你筛选标准中的“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。论文的本质是提升LLM作为智能体**核心规划能力**的方法论研究，而非将LLM应用于特定领域。 2.  **第二步：正面指标——主题高度相关** 论文摘要中充斥着与你研究目标高度相关的正面指标： *   **核心概念**: 论文的研究对象是基于LLM的智能体，旨在提升其自主性。 *   **能力方向**: 论文的核心是 **planning** 和 **problem-solving**，特别是针对复杂的 **long-horizon task**。这些都是通用推理能力的关键组成部分。 *   **新兴范式**: 论文聚焦于 **llm-based agents**，探讨了如何构建更强大的智能体以实现“可扩展的自主性”。 3.  **第三步：排除标准——无触及** 论文的研究内容完全不涉及任何排除标准。它不是关于多模态、视觉或特定应用领域（如医疗、化学），也未聚焦于模型部署优化或应用层面的安全水印等问题。 4.  **第四步：特殊和模糊情况处理——智能体/工具使用** 这篇论文是关于智能体研究的典型**保留案例**。根据你的标准：“如果是提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留。” 该论文所提出的PwS框架，正是一种旨在增强LLM在开放世界下、面对组合爆炸的任务空间时，进行通用规划和问题解决的**通用方法论**。它研究的是智能体“如何规划”这一根本性问题，而不是“用智能体做什么具体的事”。 **总结**: 该论文的切入点是解决LLM在长时程规划中的“认知带宽瓶颈”，提出的PwS方法是一种旨在增强模型通用规划和可扩展能力的全新框架。其研究目标、方法和贡献都精准地落在“提高大语言模型本身的通用推理能力”这一核心范畴内，因此是一篇高度相关且值得保留的前沿论文。"
    },
    {
        "index": "#121",
        "title": "The Markovian Thinker",
        "link": "/arxiv/2510.06557",
        "arxiv_id": "2510.06557",
        "authors": "Milad Aghajohari, Kamran Chitsaz, Amirhossein Kazemnejad, Sarath Chandar, Alessandro Sordoni, Aaron Courville, Siva Reddy",
        "summary": "Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL \"thinking environment\", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.314428",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接指向提升大语言模型的通用推理能力。 1.  **核心判断（第一步）：** 论文的本质是提出一种全新的训练范式——“马尔可夫思考”，旨在解决当前长链思维推理中的一个根本性瓶颈：计算成本随推理长度呈二次方增长。这并非将LLM应用于特定领域，而是直接改进LLM进行多步推理的内在机制和效率。它通过重新设计“思考环境”，使得模型能够以线性的计算成本进行超长距离的推理。这完全符合“改进LLM的基础能力”和“提出新的训练范式”的标准，其目标是增强模型的“多步推理”这一核心通用能力。 2.  **正面指标（第二步）：** 论文高度契合多个正面指标。 *   **核心概念:** 明确研究 \"reasoning LLMs\"。 *   **能力方向:** 核心主题是 \"long chains of thought (LongCoT)\"，即长链推理。 *   **训练方法:** 核心方法是 \"Reinforcement learning (RL)\"，并利用其来优化推理过程。 *   **新兴范式:** 提出的 \"Markovian Thinking\" 和 \"Delethink\" 环境本身就是一种关于如何组织和管理LLM推理过程的新兴范式，与智能体框架中设计环境的思想一脉相承，但更聚焦于内在的思考过程。 3.  **排除标准（第三步）：** 论文完全不涉及任何排除标准。它专注于纯文本的语言模型，没有涉及视觉、多模态，也没有将方法应用于医疗、化学等特定领域。同时，其研究内容是模型的核心推理能力，而非水印、安全等应用层面的可靠性问题。 4.  **特殊和模糊情况（第四步）：** 论文提出的“Delethink”环境可以被视为一种通用的、用于增强LLM内在推理能力的框架。它不是为特定任务设计的，而是为了让LLM本身能够更高效、更长程地思考，因此属于应该保留的情况。 **总结：** 该论文的核心贡献在于通过一种创新的、基于强化学习的范式，解决了LLM进行长链推理时的计算效率问题，从而显著提升了其通用推理能力的上限和可扩展性。这项工作直接服务于“提高大语言模型本身的通用推理能力”这一核心目标，是典型的、高质量的前沿研究，因此应被保留。"
    },
    {
        "index": "#124",
        "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning",
        "link": "/arxiv/2510.06261",
        "arxiv_id": "2510.06261",
        "authors": "Zhanke Zhou, Chentao Cao, Xiao Feng, Xuan Li, Zongze Li, Xiangyu Lu, Jiangchao Yao, Weikai Huang, Linrui Xu, Tian Cheng, Guanyu Jiang, Yiming Zheng, Brando Miranda, Tongliang Liu, Sanmi Koyejo, Masashi Sugiyama, Bo Han",
        "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.321759",
        "filter_reason": "这篇论文完全符合你的筛选标准，应被保留。以下是我的详细判断过程： 1.  **核心判断（第一步）：** 论文的核心本质是提出一种名为\"AlphaApollo\"的**自进化智能体推理系统**，其目标是解决基础模型在推理能力上的两个根本瓶颈：内在能力有限和测试时迭代不可靠。论文的核心贡献并非将LLM应用于某个特定领域，而是设计了一套**通用的方法论和系统框架**，通过编排模型与专业工具（计算、检索）、以及建立一个多轮迭代的自进化机制来**系统性地增强LLM的通用推理能力**。这完全符合“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的核心要求。 2.  **正面指标（第二步）：** 该论文命中了多个关键正面指标： *   **核心概念:** 论文明确以\"Foundation Models (FMs)\"为核心研究对象。 *   **能力方向:** 标题和摘要多次强调\"Deep Agentic Reasoning\"。评估基准是AIME（美国数学邀请赛），这是一个典型的**数学推理**测试，直接对应了筛选标准中的\"reasoning (尤其是 math reasoning)\"。 *   **训练方法:** 论文提出了\"self-evolving system\"，这是一种新颖的测试时优化范式，虽然不等同于RLHF，但符合通过迭代学习来提升模型性能的广义\"进化\"范畴。 *   **新兴范式:** 整篇论文都是关于\"llm-based agents\"、\"tool use\"的。它提出的编排多个模型和工具的方法，正是当前智能体研究的前沿。 3.  **排除标准（第三步）：** 该论文没有聚焦于任何排除标准中的领域： *   它不涉及任何**多模态与视觉**内容。 *   它的评估基准是通用的数学推理问题，而非**特定应用领域**（如医疗、化学、机器人）。 *   它的研究重点是提升推理的正确性和可靠性，而非**模型可靠性（应用层面）**的水印、安全等问题。 4.  **特殊和模糊情况处理（第四步）：** *   **智能体/工具使用:** 这正是本论文的核心亮点。论文提出的是一个**通用的智能体协作和工具使用框架**，旨在解决基础的推理瓶颈，而非应用于特定领域（如“用于化学实验的智能体”）。它通过工具使用来执行“可验证的推理”，这是一种提升通用推理质量的有效手段。因此，根据标准，应该保留。 *   **幻觉/可解释性/安全:** 论文通过引入计算和检索工具，并对推理过程进行迭代优化，目标是实现“可验证的推理”，这直接针对了LLM产生幻觉和推理不可靠的问题。这是一种从方法论上**内在提升模型可靠性**的研究，而非应用层面的讨论，因此应该保留。 5.  **最终决策（第五步）：** 综合以上分析，论文“AlphaApollo”的核心贡献是提出了一种创新的、系统性的框架，该框架通过智能体编排、工具使用和自进化机制，显著提升了大语言模型在数学等复杂推理任务上的表现。其研究目标、方法论和评估方式都高度聚焦于提升“LLM的通用推理能力”，是“大语言模型通用推理能力”研究课题下的典型前沿论文。 因此，最终判断为 **True**。"
    },
    {
        "index": "#1",
        "title": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning",
        "link": "/arxiv/2510.07312",
        "arxiv_id": "2510.07312",
        "authors": "Sumeet Ramesh Motwani, Alesia Ivanova, Ziyang Cai, Philip Torr, Riashat Islam, Shital Shah, Christian Schroeder de Witt, Charles London",
        "summary": "Large language models excel at short-horizon reasoning tasks, but performance drops as reasoning horizon lengths increase. Existing approaches to combat this rely on inference-time scaffolding or costly step-level supervision, neither of which scales easily. In this work, we introduce a scalable method to bootstrap long-horizon reasoning capabilities using only existing, abundant short-horizon data. Our approach synthetically composes simple problems into complex, multi-step dependency chains of arbitrary length. We train models on this data using outcome-only rewards under a curriculum that automatically increases in complexity, allowing RL training to be scaled much further without saturating. Empirically, our method generalizes remarkably well: curriculum training on composed 6th-grade level math problems (GSM8K) boosts accuracy on longer, competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x. Importantly, our long-horizon improvements are significantly higher than baselines even at high pass@k, showing that models can learn new reasoning paths under RL. Theoretically, we show that curriculum RL with outcome rewards achieves an exponential improvement in sample complexity over full-horizon training, providing training signal comparable to dense supervision. h1 therefore introduces an efficient path towards scaling RL for long-horizon problems using only existing data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.408045",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据您的筛选标准进行的详细判断过程： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心是提出一种新的训练范式（基于强化学习的课程学习），旨在解决大语言模型在长程推理（long-horizon reasoning）上的根本性弱点。它不是将LLM应用于某个特定领域，而是直接改进LLM的**通用推理能力**。论文通过合成更复杂的数据，并使用仅基于结果的奖励进行训练，来“引导”模型学会处理更长的推理链。这直接触及了LLM基础能力的增强，属于方法论层面的创新，因此符合保留标准。 **第二步：正面指标——论文是否包含以下主题？** 该论文高度匹配多个正面指标： - **核心概念**: 论文明确以“Large language models (LLMs)”为研究对象。 - **能力方向**: 论文的核心就是提升“reasoning”能力，特别是“long-horizon reasoning”，并具体在数学推理（math reasoning）任务上进行了验证。 - **训练方法**: 论文的核心贡献是一种新的“reinforcement learning (RL)”方法，结合了“curriculum”学习，这是一种新颖的训练范式。 - **新兴范式**: 虽然不直接涉及智能体或工具使用，但其解决长程推理问题的方法论，是构建高级智能体和解决复杂问题（deep research）的关键基础。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 该论文完全不涉及任何排除标准领域： - 它不涉及多模态、视觉或特定应用领域（如医疗、化学等）。 - 它虽然关注模型性能，但焦点在于通过改进训练方法来提升其内在的推理能力，而非应用层面的水印、安全或可靠性问题。 **第四步：处理特殊和模糊情况** 本论文的情况非常清晰，不属于需要特殊处理的模糊情况。它提出的强化学习方法，其目标是提升模型的**通用**长程推理能力，而非针对特定领域。论文中使用的数学问题（GSM8K, MATH等）是衡量通用推理能力的标准基准，而非特定应用。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种通过强化学习来增强大语言模型**通用长程推理能力**的、可扩展的新方法。它直接回应了您研究课题的核心目标——“提高大语言模型（LLM）本身的『通用推理能力』”。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#122",
        "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles",
        "link": "/arxiv/2510.06475",
        "arxiv_id": "2510.06475",
        "authors": "Yitao Long, Yuru Jiang, Hongjun Liu, Yilun Zhao, Jingchen Sun, Yiqiu Shen, Chen Zhao, Arman Cohan, Dennis Shasha",
        "summary": "This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-07",
        "category": "cs.CL",
        "crawl_time": "2025-10-09T19:59:14.315195",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是基于筛选标准的详细判断过程： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心贡献是提出了一个名为 \"PuzzlePlex\" 的**基准**，用于**评估和衡量**基础模型（主要是LLM）在**通用推理和规划**方面的能力。虽然它没有提出一种全新的训练范式来直接“提高”模型能力，但它为“提高”这一目标提供了至关重要的**评估标准和实验平台**。在学术研究中，创建一个高质量、有针对性的基准，本身就是推动领域进步的核心贡献之一。它明确界定了问题（推理和规划），提供了衡量标准，并能揭示现有模型的局限性，从而**直接指导未来的改进方向**。因此，这篇论文的本质是服务于“提高LLM通用推理能力”这一宏大目标的基础性工作，符合保留标准。 **第二步：正面指标——论文是否包含以下主题？** 该论文命中了多个关键的正面指标： - **核心概念**: 论文明确研究对象是 \"Foundation Models\"，在当前语境下主要指LLM。 - **能力方向**: 论文的标题和摘要反复强调 \"reasoning\" (推理) 和 \"planning\" (规划)，这正是你关注的核心能力。 - **新兴范式**: 论文对比了 \"instruction-based\" 和 \"code-based\" 两种设置，后者与工具使用和代码生成能力密切相关，是当前提升LLM推理能力的重要范式。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 该论文成功地避开了所有排除标准： - **多模态与视觉**: 论文完全聚焦于抽象的\"puzzles\"（谜题），不涉及任何视觉或多模态内容。 - **特定应用领域**: \"Puzzles\"是通用的逻辑和策略问题，不属于生物、医疗、化学等任何特定应用领域。 - **模型可靠性（应用层面）**: 论文关注的是模型的推理性能和规划能力，而非水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文探讨的 \"code-based execution\" 可以视为一种广义上的工具使用，即将代码作为与环境交互和解决问题的工具。由于这是在一个通用的、非特定领域的基准中进行的比较研究，其目的是为了探索提升通用推理能力的有效路径，因此符合保留条件。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献——为LLM的通用推理与规划能力创建一个新颖、可扩展的评估基准——与你的研究目标高度契合。它不涉及特定应用，专注于核心能力，并且其成果（基准和发现）将直接为该领域的后续研究（包括你自己的研究）提供明确的评估依据和改进方向。因此，这是一篇非常值得保留的前沿论文。"
    },
    {
        "index": "#24",
        "title": "COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization",
        "link": "/arxiv/2510.07043",
        "arxiv_id": "2510.07043",
        "authors": "Tian Qin, Felix Bai, Ting-Yao Hu, Raviteja Vemulapalli, Hema Swetha Koppula, Zhiyang Xu, Bowen Jin, Mert Cemri, Jiarui Lu, Zirui Wang, Meng Cao",
        "summary": "Real-world large language model (LLM) agents must master strategic tool use and user preference optimization through multi-turn interactions to assist users with complex planning tasks. We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel-planning scenarios. We cast travel planning as a constrained preference optimization problem, where agents must satisfy hard constraints while simultaneously optimizing soft user preferences. To support this, we build a realistic travel database covering transportation, accommodation, and ticketing for 20 U.S. National Parks, along with a comprehensive tool ecosystem that mirrors commercial booking platforms. Evaluating state-of-the-art models, we uncover two critical gaps: (i) an acceptable-optimal gap, where agents reliably meet constraints but fail to optimize preferences, and (ii) a plan-coordination gap, where performance collapses on multi-service (flight and hotel) coordination tasks, especially for open-source models. By grounding reasoning and planning in a practical, user-facing domain, COMPASS provides a benchmark that directly measures an agent's ability to optimize user preferences in realistic tasks, bridging theoretical advances with real-world impact.",
        "subjects": "Machine Learning",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.430118",
        "filter_reason": "这篇论文符合筛选要求，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一个名为COMPASS的**基准**，用于评估LLM智能体在多轮交互中的工具使用、规划和偏好优化能力。虽然它没有直接提出一种新的训练范式或模型架构来“改进”LLM，但它精准地定义和衡量了LLM通用推理能力中的一个关键子集——**在复杂、多步骤任务中进行规划和优化**的能力。一个好的基准是推动领域进步的基石，它通过揭示现有模型的不足（如论文中发现的“可接受-最优差距”和“规划-协调差距”），为未来的研究指明了方向。因此，这篇论文的本质是服务于“提高LLM通用推理能力”这一核心目标的，属于基础性研究，应予以保留。 2.  **第二步：正面指标** 论文高度符合多个正面指标： - **核心概念**: 明确以 \"large language model (LLM) agents\" 为研究对象。 - **能力方向**: 核心聚焦于 \"planning\" 和 \"preference optimization\"，这些都是通用推理和问题解决能力的关键组成部分。 - **新兴范式**: 论文主题是 \"tool-mediated\" 的智能体行为，属于当前LLM研究的前沿范式。它探讨的是智能体如何通过工具和多轮交互来解决复杂问题，这正是通用问题解决能力的体现。 3.  **第三步：排除标准** 论文不触及任何排除标准： - **多模态与视觉**: 论文未涉及视觉或多模态内容。 - **特定应用领域**: 这是最需要辨析的一点。论文虽然使用了“旅行规划”作为具体场景，但其目的并非解决旅游行业的问题。摘要明确指出，该场景是为了“将推理和规划建立在一个实用的、面向用户的领域”，其最终目标是“直接衡量智能体在现实任务中优化用户偏好的能力”。这里的“旅行规划”是**载体**而非**目的**，研究的是底层的通用规划与优化能力，而非旅游领域的专业知识。这与“用于化学实验的智能体”有本质区别，后者以解决化学问题为核心。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文完全符合“保留”条件。它提出的是一个通用的评估框架，用于衡量智能体通过工具使用进行规划和优化的通用能力，而不是将智能体应用于某个特定垂直领域。它所揭示的“规划-协调差距”是LLM在通用推理层面的普遍性弱点。 5.  **第五步：最终决策** 综合来看，这篇论文虽然是一篇基准测试论文，但其核心是定义、衡量和揭示LLM在**通用规划与偏好优化**这一高级推理能力上的现状与不足。它为未来如何提升LLM的这些能力提供了清晰的评估标准和研究方向。因此，它与“致力于提高大语言模型（LLM）本身的『通用推理能力』”的研究目标高度一致，应当被纳入筛选范围。"
    },
    {
        "index": "#31",
        "title": "From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics",
        "link": "/arxiv/2510.06954",
        "arxiv_id": "2510.06954",
        "authors": "Zheng-An Chen, Tao Luo",
        "summary": "Although transformer-based models have shown exceptional empirical performance, the fundamental principles governing their training dynamics are inadequately characterized beyond configuration-specific studies. Inspired by empirical evidence showing improved reasoning capabilities under small initialization scales in language models, we employ the gradient flow analytical framework established in [Zhou et al. NeurIPS 2022] to systematically investigate linearized Transformer training dynamics. Our theoretical analysis dissects the dynamics of attention modules into two distinct stages. In the first stage, asymmetric weight perturbations from random initialization sustain non-degenerate gradient dynamics in parameter matrices, facilitating systematic escape from small initialization regimes. Subsequently, these matrices undergo condensation, progressively aligning toward the target orientation. In the second stage, the previously static key-query matrices actively participate in training, driving the normalized matrices toward asymptotic rank collapse. This two-stage framework generalizes classical directional convergence results.",
        "subjects": "Machine Learning",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.438397",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断**—— 论文的核心是关于改进LLM的基础能力。这篇论文的标题和摘要明确指出，它是一项关于Transformer训练动态的**理论分析**。其本质不是应用LLM去解决某个外部问题，而是深入探究Transformer模型**内部的训练原理**。一个关键点是，它的研究动机是“受经验证据启发，即语言模型在小初始化尺度下推理能力得到提升”。这表明论文的理论分析与提升模型的“推理能力”这一核心目标直接挂钩。它试图从一个更基础的层面（训练动态和权重演化）来解释**为何**某些训练条件能带来更好的推理性能。这完全属于“改进LLM的基础能力”和“提出新的训练范式（的理解）”的范畴。 2.  **第二步：正面指标**—— 论文高度相关。摘要中明确提到了“language models”和“reasoning capabilities”。虽然它没有直接提出像CoT或RLHF这样的新方法，但它通过理论分析**解构**了训练过程（分为两个阶段：condensation和rank collapse），这正是理解和优化这些高级方法所需的基础研究。这种对训练动态的深入剖析，对于未来设计出能有效提升推理能力的训练算法至关重要。 3.  **第三步：排除标准**—— 论文不涉及任何排除领域。论文焦点纯粹是Transformer模型的训练理论，没有提及多模态、视觉、医疗、化学等任何特定应用领域，也没有讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：特殊和模糊情况**—— 本论文不涉及智能体/工具，也不涉及幻觉/安全等特殊情况，因此无需特殊处理。 **结论**： 这篇论文的核心贡献在于提供了一个理论框架，用以解释Transformer在训练过程中其内部参数（特别是注意力模块的权重矩阵）是如何演化的，并且这一理论分析与提升模型推理能力的经验现象紧密相连。它从“第一性原理”的视角出发，揭示了训练动态与模型内在能力（如推理）之间的潜在联系。这种基础性的理论研究，正是推动“提高大语言模型本身的通用推理能力”这一目标向前发展的关键基石。因此，它非常符合您的筛选要求。"
    },
    {
        "index": "#32",
        "title": "Grouped Differential Attention",
        "link": "/arxiv/2510.06949",
        "arxiv_id": "2510.06949",
        "authors": "Junghwan Lim, Sungmin Lee, Dongseok Kim, Wai Ting Cheung, Beomgyu Kim, Taehwan Kim, Haesol Lee, Junhyeok Lee, Dongpin Oh, Eunhwan Park",
        "summary": "The self-attention mechanism, while foundational to modern Transformer architectures, suffers from a critical inefficiency: it frequently allocates substantial attention to redundant or noisy context. Differential Attention addressed this by using subtractive attention maps for signal and noise, but its required balanced head allocation imposes rigid constraints on representational flexibility and scalability. To overcome this, we propose Grouped Differential Attention (GDA), a novel approach that introduces unbalanced head allocation between signal-preserving and noise-control groups. GDA significantly enhances signal focus by strategically assigning more heads to signal extraction and fewer to noise-control, stabilizing the latter through controlled repetition (akin to GQA). This design achieves stronger signal fidelity with minimal computational overhead. We further extend this principle to group-differentiated growth, a scalable strategy that selectively replicates only the signal-focused heads, thereby ensuring efficient capacity expansion. Through large-scale pretraining and continual training experiments, we demonstrate that moderate imbalance ratios in GDA yield substantial improvements in generalization and stability compared to symmetric baselines. Our results collectively establish that ratio-aware head allocation and selective expansion offer an effective and practical path toward designing scalable, computation-efficient Transformer architectures.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.438919",
        "filter_reason": "这篇论文符合你的研究范围。判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“分组差分注意力”的新方法，旨在改进Transformer架构中最基础、最核心的组件——自注意力机制。论文的本质是**对LLM的基础架构进行优化**，通过更有效地分配注意力资源（区分信号与噪声），来提升模型处理信息的能力。这完全符合“改进LLM的基础能力”的标准，因为它直接作用于模型本身，而非将其作为工具应用于特定领域。一个能够在上下文中更好地区分信号和噪声的模型，是进行高质量逻辑推理和问题解决的先决条件。 2.  **第二步：正面指标** 虽然标题和摘要中没有频繁出现“reasoning”等直接词汇，但其贡献与推理能力密切相关： *   **核心概念**: 论文研究的是Transformer架构，这是所有现代LLM的基础。 *   **能力方向**: 论文声称通过GDA实现了“泛化和稳定性的显著改进”。更强的泛化能力意味着模型能更好地将学到的知识应用到新的、未见过的推理问题上；而稳定性则保证了推理过程的一致性和可靠性。这些都是通用推理能力的重要组成部分。 *   **新兴范式**: 论文虽然不直接研究Agent，但它提出了一种更高效的架构，这种架构是实现复杂LLM-based Agents或工具使用的底层基础。 3.  **第三步：排除标准** 论文完全避开了所有的排除标准： *   它不属于多模态或视觉领域。 *   它没有聚焦于任何特定的应用领域（如医疗、化学等）。 *   它讨论的不是应用层面的水印或安全问题。 4.  **第四步：处理特殊和模糊情况** 本文提出的GDA方法，在机制上与“减少幻觉”有异曲同工之妙。幻觉的一个重要来源就是模型对输入中的噪声或细微细节进行了错误的关联。GDA通过“差分”和“分组”策略，让模型更专注于核心“信号”，抑制“噪声”，这从根本上提升了模型内在的可靠性，从而能够提升其推理质量。这符合“提出一种新方法来减少幻觉，从而提升模型的通用可靠性和推理质量”的保留标准。 **最终决策**: 综合来看，这篇论文虽然表面上是关于注意力机制的工程优化，但其本质是**一种基础性的、旨在提升模型核心信息处理能力的研究**。通过改进模型区分上下文中重要与次要信息的能力，它为模型进行更准确、更稳健的通用推理（如数学、逻辑、规划等）打下了坚实的架构基础。因此，它完全符合你筛选“致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标。"
    },
    {
        "index": "#47",
        "title": "Efficient numeracy in language models through single-token number embeddings",
        "link": "/arxiv/2510.06824",
        "arxiv_id": "2510.06824",
        "authors": "Linus Kreitner, Paul Hager, Jonathan Mengedoht, Georgios Kaissis, Daniel Rueckert, Martin J. Menten",
        "summary": "To drive progress in science and engineering, large language models (LLMs) must be able to process large amounts of numerical data and solve long calculations efficiently. This is currently only possible through the use of external tools or extensive reasoning chains, either limiting the numerical intuition of LLMs or limiting the length of problems they can solve. We show that frontier LLMs require excessive amounts of reasoning tokens to solve even basic calculations, which is exacerbated by their tokenization strategies that split single numbers into multiple tokens. This motivates the need for efficient and effective single-token number encodings. We introduce a set of desiderata for such encodings and show that existing approaches fail to fulfill them. To address these shortcomings, we propose BitTokens, a novel tokenization strategy that embeds any number into a single token using its IEEE 754 binary floating-point representation. Through extensive experiments we show that our BitTokens allow even small language models to learn algorithms that solve basic arithmetic operations nearly perfectly. This newly gained efficiency could expand the length and complexity of problems language models can solve.",
        "subjects": "Machine Learning",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.447152",
        "filter_reason": "这篇论文完全符合筛选要求，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“BitTokens”的新型分词策略，旨在解决大语言模型在处理数字时的根本性效率问题。它通过将任何数字编码为单个token，直接改进了LLM处理数值信息的基础能力。这并非将LLM作为工具应用于某个特定领域，而是对LLM本身的一种底层增强，旨在提升其固有的数学计算和推理能力。这完全符合“改进LLM的基础能力、增强其逻辑、数学...等通用能力”的保留标准。 2.  **第二步：正面指标** 论文高度符合正面指标： *   **核心概念**: 论文明确以“large language models (LLMs)”为研究对象。 *   **能力方向**: 论文的核心是提升“numeracy”（计算能力）和解决“long calculations”（长计算）的能力，这直接隶属于“math reasoning”和“problem-solving”的范畴。 *   论文的目标是“expand the length and complexity of problems language models can solve”，这与提升通用推理能力的目标高度一致。 3.  **第三步：排除标准** 论文完全不触及任何排除标准。它没有涉及多模态、视觉，也没有将研究限定在医疗、化学等特定应用领域，更不是关于模型部署、水印或安全等基础设施或应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体、工具使用、幻觉或安全等特殊情况，因此无需进行特殊判断。值得注意的是，论文中提到其方法可以减少对外部工具的依赖，这恰恰是从内部增强模型能力，符合筛选标准。 **最终决策**: 综合以上分析，这篇论文的本质是提出一种创新的方法论，通过优化LLM的数字表示方式，从根本上提升其数学计算这一核心推理能力。它直接作用于模型本身，旨在增强其通用问题解决的潜力，而非应用于特定场景。因此，这篇论文与“提高大语言模型通用推理能力”的研究课题高度相关，应被判定为符合要求。"
    },
    {
        "index": "#46",
        "title": "Recurrence-Complete Frame-based Action Models",
        "link": "/arxiv/2510.06828",
        "arxiv_id": "2510.06828",
        "authors": "Michael Keiblinger",
        "summary": "In recent years, attention-like mechanisms have been used to great success in the space of large language models, unlocking scaling potential to a previously unthinkable extent. \"Attention Is All You Need\" famously claims RNN cells are not needed in conjunction with attention. We challenge this view. In this paper, we point to existing proofs that architectures with fully parallelizable forward or backward passes cannot represent classes of problems specifically interesting for long-running agentic tasks. We further conjecture a critical time t beyond which non-recurrence-complete models fail to aggregate inputs correctly, with concrete implications for agentic systems (e.g., software engineering agents). To address this, we introduce a recurrence-complete architecture and train it on GitHub-derived action sequences. Loss follows a power law in the trained sequence length while the parameter count remains fixed. Moreover, longer-sequence training always amortizes its linearly increasing wall-time cost, yielding lower loss as a function of wall time.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.446685",
        "filter_reason": "这篇论文完全符合我的研究范围，其核心贡献是提出了一种名为“Recurrence-Complete”的新架构，旨在解决现有基于注意力机制的模型（如Transformer）在处理长期、序列化智能体任务时的根本性局限。这直接命中了我的核心目标——『提高大语言模型本身的通用推理能力』。 我的判断过程如下： 1.  **第一步（核心判断）**: 论文的核心并非将LLM应用于特定领域，而是对LLM的主流架构（Attention机制）提出了挑战，并给出了一个旨在提升其基础能力的替代方案。论文中提到的“长期运行的智能体任务”（如软件工程智能体）本质上是一种复杂的多步规划和问题求解过程，是通用推理能力的核心体现。因此，该论文属于改进LLM基础能力以增强其通用推理能力的研究，应予以保留。 2.  **第二步（正面指标）**: 论文虽然未直接使用“LLM”一词，但其讨论的“attention-like mechanisms”和挑战“Attention Is All You Need”明确指向了LLM的核心架构。它聚焦于“agentic tasks”，这高度关联“planning”和“problem-solving”等能力方向。提出新架构并在其上进行训练，本身就是一种新的训练范式和模型基础探索。 3.  **第三步（排除标准）**: 论文未涉及任何多模态、视觉或特定应用领域（如医疗、化学），也非关于模型可靠性（水印、安全等）的研究。因此，不触发任何排除标准。 4.  **第四步（处理特殊情况）**: 这篇论文是“智能体”相关研究的典型范例。它不是将智能体作为工具应用在某个领域，而是从底层架构层面探讨如何让智能体更擅长处理长程任务。这完全符合“提出一种通用的...方法来增强LLM的通用问题解决能力，应该保留”的原则。其目的是增强智能体内在的、通用的规划和执行能力，这正是通用推理的一部分。 **最终决策**: 该论文通过改进LLM的基础架构来增强其在复杂、长程任务上的推理和规划能力，从根本上提升了模型的潜能，而非在应用层面进行优化。它直面了当前LLM架构在通用推理上的一个潜在瓶颈，并提出了创新的解决方案。因此，这篇论文与我的研究课题高度相关，应当被筛选出来。"
    },
    {
        "index": "#59",
        "title": "XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation",
        "link": "/arxiv/2510.06672",
        "arxiv_id": "2510.06672",
        "authors": "Udbhav Bamba, Minghao Fang, Yifan Yu, Haizhong Zheng, Fan Lai",
        "summary": "Reinforcement learning algorithms such as GRPO have driven recent advances in large language model (LLM) reasoning. While scaling the number of rollouts stabilizes training, existing approaches suffer from limited exploration on challenging prompts and leave informative feedback signals underexploited, due to context-independent rollout allocation across prompts (e.g., generating 16 rollouts per prompt) and relying heavily on sparse rewards. This paper presents XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy optimization through the principled lens of rollout exploration-exploitation. To enhance exploration, XRPO introduces a mathematically grounded rollout allocator that adaptively prioritizes prompts with higher potential for uncertainty reduction. It further addresses stagnation on zero-reward prompts through an in-context seeding strategy that injects curated exemplars, steering the model into more difficult reasoning trajectories. To strengthen exploitation, XRPO develops a group-relative, novelty-aware advantage sharpening mechanism that leverages sequence likelihoods to amplify low-probability yet correct responses, thereby extending the policy's reach beyond sparse rewards. Experiments across diverse math and coding benchmarks on both reasoning and non-reasoning models demonstrate that XRPO outperforms existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while accelerating training convergence by up to 2.7X.",
        "subjects": "Machine Learning",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.451579",
        "filter_reason": "根据您提供的筛选标准，我对这篇论文进行了严格、精准的分析，判断其完全符合您的研究范围。 1.  **核心判断 (第一步): 论文本质是改进LLM的基础推理能力。** 该论文的核心贡献是提出了一种名为XRPO的新强化学习（RL）算法，它是现有GRPO算法的改进。论文明确指出，其目标是解决当前RL方法在提升LLM推理能力时遇到的瓶颈（如对困难问题的探索不足、奖励信号利用不充分）。这完全符合筛选标准中“保留”的条件——即论文的核心是“提出新的训练范式”、“增强其逻辑、数学、规划、多步推理等通用能力”。它并非将LLM作为工具应用于特定领域，而是直接作用于LLM本身，通过优化其训练过程来提升其内在的推理性能。 2.  **正面指标 (第二步): 论文高度匹配所有关键主题。** -   **核心概念**: 摘要开篇即点明研究主体是“Large language models (LLMs)”。 -   **能力方向**: 论文的核心目标是提升“LLM reasoning”，并在“math and coding benchmarks”上进行验证，这些都是通用推理能力的典型体现。 -   **训练方法**: 论文的标题和摘要都围绕“Reinforcement learning (RL)”展开，提出了一种新的RL策略优化方法，这与“reinforcement learning (RLHF, RL)”这一正面指标完全一致。 3.  **排除标准 (第三步): 论文完全不涉及任何排除领域。** -   **多模态**: 论文未提及任何视觉或多模态内容，专注于纯文本的推理任务。 -   **特定应用领域**: 尽管在数学和编码基准上测试，但这些被普遍视为评估LLM通用能力的基准，而非像医疗、化学那样的专业领域应用。 -   **模型可靠性（应用层面）**: 论文未讨论Watermarking, Safety, Security等应用层面的可靠性问题。 4.  **特殊和模糊情况 (第四步): 不适用，但论文基础性更强。** 该论文不涉及智能体/工具使用，或从可靠性角度讨论幻觉。它关注的是更为底层的“策略优化”机制，这是强化学习训练LLM的核心。这种对基础训练方法的改进，比构建上层的应用框架更为根本，直接服务于提升模型“通用推理能力”这一核心目标。 **最终决策:** 综合以上分析，这篇论文的本质是通过提出一种新颖的强化学习策略优化方法（XRPO），来直接提升大语言模型的数学和代码推理能力。它是一项方法论上的创新，完全聚焦于增强LLM的“通用推理能力”，而非特定领域应用。因此，该论文是您研究课题“大语言模型通用推理能力”的完美匹配，应被**保留**。"
    },
    {
        "index": "#60",
        "title": "The Effect of Attention Head Count on Transformer Approximation",
        "link": "/arxiv/2510.06662",
        "arxiv_id": "2510.06662",
        "authors": "Penghao Yu, Haotian Jiang, Zeyu Bao, Ruoxi Yu, Qianxiao Li",
        "summary": "Transformer has become the dominant architecture for sequence modeling, yet a detailed understanding of how its structural parameters influence expressive power remains limited. In this work, we study the approximation properties of transformers, with particular emphasis on the role of the number of attention heads. Our analysis begins with the introduction of a generalized $D$-retrieval task, which we prove to be dense in the space of continuous functions, thereby providing the basis for our theoretical framework. We then establish both upper and lower bounds on the parameter complexity required for $\\epsilon$-approximation. Specifically, we show that transformers with sufficiently many heads admit efficient approximation, whereas with too few heads, the number of parameters must scale at least as $O(1/\\epsilon^{cT})$, for some constant $c$ and sequence length $T$. To the best of our knowledge, this constitutes the first rigorous lower bound of this type in a nonlinear and practically relevant setting. We further examine the single-head case and demonstrate that an embedding dimension of order $O(T)$ allows complete memorization of the input, where approximation is entirely achieved by the feed-forward block. Finally, we validate our theoretical findings with experiments on both synthetic data and real-world tasks, illustrating the practical relevance of our results.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.451905",
        "filter_reason": "这篇论文符合你的研究范围。我的判断过程如下： 1.  **核心判断 (第一步):** 这篇论文的本质是对Transformer模型的基础架构进行理论分析，而非应用。它没有将LLM作为工具解决特定领域问题，也没有关注部署优化。论文的核心是探究“注意力头的数量”这一关键结构参数如何影响模型的表达能力和函数近似效率。虽然它没有提出一种新的推理*训练方法*，但它直接研究了构成推理能力的**根本前提——模型的表达力**。一个模型的表达力是其能否完成复杂推理任务的理论基础。因此，这篇论文致力于“改进LLM的基础能力”，符合筛选核心。 2.  **正面指标 (第二步):** *   **核心概念:** 论文研究对象是Transformer，这是当前所有主流LLM的基础架构，研究它等同于研究LLM的本质。 *   **能力方向:** 论文虽未直接使用\"reasoning\"一词，但其核心贡献——建立Transformer近似复杂函数的上下界——是实现数学推理和逻辑推理等高级能力的数学基础。论文证明，足够多的注意力头能使模型“高效近似”复杂函数，这正是通用问题解决能力的体现。单头情况下模型只能依赖“记忆”（通过Feed-Forward网络块），而多头则允许更高效的“表达”，这与模型从死记硬背到真正理解推理的飞跃在理论上是一致的。 3.  **排除标准 (第三步):** 论文完全未涉及多模态、医疗、化学、机器人等特定应用领域，也未讨论水印、安全等应用层面的可靠性问题。因此，不触犯任何排除标准。 4.  **最终决策 (第五步):** 综合来看，虽然这篇论文的风格是理论分析而非提出一个具体的“技巧”（如CoT），但它深刻揭示了一个核心架构设计（注意力头数量）如何从底层决定模型的上限。对于一位致力于提升LLM通用推理能力的顶尖研究员来说，理解“为什么”某些设计更有效与“如何”实现新技巧同等重要。这篇论文提供了关于“为什么”的关键洞见，指导着未来如何从结构层面设计出天生就具备更强推理潜力的模型。因此，它是一篇非常前沿且高度相关的基础性研究，应当被保留。"
    },
    {
        "index": "#69",
        "title": "POME: Post Optimization Model Edit via Muon-style Projection",
        "link": "/arxiv/2510.06627",
        "arxiv_id": "2510.06627",
        "authors": "Yong Liu, Di Fu, Yang Luo, Zirui Zhu, Minhao Cheng, Cho-Jui Hsieh, Yang You",
        "summary": "We introduce Post-Optimization Model Edit (POME), a new algorithm that enhances the performance of fine-tuned large language models using only their pretrained and fine-tuned checkpoints, without requiring extra data or further optimization. The core idea is to apply a muon-style projection to $\\Delta W$, the difference between the fine-tuned and pretrained weights. This projection uses truncated singular value decomposition (SVD) to equalize the influence of dominant update directions and prune small singular values, which often represent noise. As a simple post-processing step, POME is completely decoupled from the training pipeline. It requires zero modifications and imposes no overhead, making it universally compatible with any optimizer or distributed framework. POME delivers consistent gains, boosting average performance by +2.5\\% on GSM8K and +1.0\\% on code generation. Its broad applicability -- from 7B foundation models to 72B RLHF-instructed models -- establishes it as a practical, zero-cost enhancement for any fine-tuning pipeline. Code is available at https://github.com/NUS-HPC-AI-Lab/POME.",
        "subjects": "Machine Learning",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.454903",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - 论文的核心贡献是提出了一种名为POME的后处理算法，用于编辑和优化已经微调好的大语言模型权重。 - 该方法并非将LLM作为工具应用于特定领域，而是直接作用于模型本身，旨在提升模型的内在性能。 - 论文明确指出，该方法在GSM8K（数学推理基准）和代码生成（逻辑推理的一种形式）上带来了性能提升。这直接关联到提升LLM的『通用推理能力』这一核心目标。 - 虽然POME不是一种全新的训练范式（如RLHF或CoT），但它是一种通用的、与训练过程解耦的模型增强方法论，其本质是改进模型的能力，而非应用模型。因此，它符合“保留”标准。 2.  **第二步：正面指标** - **核心概念**: 论文明确研究`Large language models, LLMs`。 - **能力方向**: 论文在`math reasoning` (GSM8K)和`logical reasoning` (代码生成)任务上验证了其有效性，这是非常强的正面指标。 - **训练方法**: 论文提到其方法适用于`RLHF-instructed models`，虽然其本身不是RL方法，但与主流训练范式的兼容性表明其定位是基础能力增强。 3.  **第三步：排除标准** - 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）。它是一种纯粹的、通用的模型权重优化技术，因此成功避开了所有排除标准。 4.  **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需进行特殊判断。 5.  **第五步：最终决策** - 综合来看，POME论文提出了一种通用的、零成本的模型编辑方法，其直接效果是提升了LLM在数学和逻辑推理任务上的表现。这完全契合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一研究课题的核心目标。它是一种方法论层面的创新，旨在从模型权重层面优化和增强模型的基础推理能力，而非将其应用于特定场景。因此，最终判断为 **True**。"
    },
    {
        "index": "#73",
        "title": "Incoherence in goal-conditioned autoregressive models",
        "link": "/arxiv/2510.06545",
        "arxiv_id": "2510.06545",
        "authors": "Jacek Karwowski, Raymond Douglas",
        "summary": "We investigate mathematically the notion of incoherence: a structural issue with reinforcement learning policies derived by naive goal-conditioning of autoregressive models. We focus on the process of re-training models on their own actions, that is, fine-tuning offline-learned policies with online RL. We prove that it decreases incoherence and leads to an improvement in return, and we aim to characterize the resulting trajectory of policies. By re-framing standard notions of control-as-inference and soft Q learning, we establish a three-way correspondence with two other ways of understanding the iterative re-training process: as folding the posterior into the reward and, in the deterministic case, as decreasing the temperature parameter; the correspondence has computational content via the training-inference trade-off. Through soft-conditioning generative models, we discuss the link between incoherence and the effective horizon.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.456224",
        "filter_reason": "这篇论文完全符合你的研究范围，是一篇关于提升大语言模型（LLM）基础推理能力的核心理论论文。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心研究对象是“目标条件自回归模型”。自回归模型是LLM的基础架构，而“目标条件化”是引导模型完成特定任务（如规划、问题求解）的关键技术。论文指出了在这种训练范式下存在的一个根本性结构问题——“不连贯性”，并提出了通过“在线强化学习对离线策略进行微调”的迭代再训练方法来解决这个问题。这本质上是在提出一种新的、更优的训练范式，旨在提升模型在多步决策和规划任务中的内在一致性和表现，完全符合“改进LLM基础能力、增强其规划、多步推理等通用能力”的保留标准。它并非将LLM应用于特定领域，而是研究模型本身的学习机制。 2.  **第二步：正面指标——论文高度相关。** -   **核心概念**: 论文研究的“自回归模型”是LLM的核心。 -   **能力方向**: “目标条件化”、“控制”、“策略”和“有效视界”等概念直接指向模型的**规划**和**问题解决**能力。解决“不连贯性”问题，就是为了让模型生成的行为序列（推理链）更加逻辑自洽、目标一致。 -   **训练方法**: 论文的核心就是关于**强化学习（RL）**的理论分析，探讨了如何通过迭代再训练（一种自我进化的形式）来优化模型策略。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文完全没有提及多模态、视觉、任何特定应用领域（如医疗、化学），也没有讨论水印、安全等应用层面的可靠性问题。其焦点纯粹在于模型的理论和训练方法。 4.  **第四步：处理特殊和模糊情况——论文触及了推理能力的根本。** 论文研究的“不连贯性”可以看作是模型在复杂推理任务中“逻辑跳跃”或“行为矛盾”的理论根源。通过解决这个根本问题，论文旨在从源头上提升模型推理的可靠性和质量，这与“减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的保留方向高度一致。它不是在讨论应用，而是在夯实理论基础。 **最终决策**: 这篇论文是一篇理论性极强的研究，它深入剖析了LLM在执行目标导向任务（如规划和推理）时的一个核心理论缺陷，并提出了一种基于强化学习的改进训练范式。其工作直接致力于提升LLM的通用规划和多步推理能力，是“大语言模型通用推理能力”研究领域的前沿和核心贡献。因此，应**保留**。"
    },
    {
        "index": "#95",
        "title": "SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation",
        "link": "/arxiv/2510.06303",
        "arxiv_id": "2510.06303",
        "authors": "Shuang Cheng, Yihan Bian, Dawei Liu, Yuhua Jiang, Yihao Liu, Linfeng Zhang, Wenhai Wang, Qipeng Guo, Kai Chen, Biqing Qi, Bowen Zhou",
        "summary": "We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies the training efficiency of autoregressive models with the parallel inference capability of diffusion. Instead of costly end-to-end diffusion training, SDAR performs a lightweight paradigm conversion that transforms a well-trained autoregressive (AR) model into a blockwise diffusion model through brief, data-efficient adaptation. During inference, SDAR generates sequences autoregressively across blocks for global coherence while decoding all tokens within each block in parallel via a discrete diffusion process. Extensive experiments show that AR models remain substantially more compute-efficient than masked diffusion models, providing a strong foundation for adaptation. Building on this insight, SDAR achieves efficient AR-to-diffusion conversion with minimal cost, preserving AR-level performance while enabling parallel generation. Scaling studies across dense and Mixture-of-Experts architectures confirm that SDAR scales without compromise: larger models exhibit stronger robustness to block size and decoding thresholds, yielding greater speedups without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning and domain adaptability. Our 30B MoE model surpasses its AR counterpart on challenging scientific reasoning benchmarks such as GPQA and ChemBench, and gains further improvements under test-time scaling methods like majority voting and pass@k. Together, these results establish SDAR as a practical paradigm that combines the strengths of autoregression and diffusion for scalable, high-throughput reasoning.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-07",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.463008",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： **第一步：核心判断** 论文的核心是提出一种名为SDAR的全新**序列生成范式**。它并非将LLM作为工具应用于特定领域，而是从根本上改进了LLM（以自回归模型为代表）的生成和推理机制。该范式通过将自回归模型转换为分块扩散模型，旨在结合两者的优点，实现更高效、并行的推理。这是一种对LLM基础能力的**方法论创新**，旨在提升其内在的生成效率和推理质量，因此完全符合“保留”标准。 **第二步：正面指标** 论文高度契合多个正面指标： - **核心概念**: 论文的研究对象是“autoregressive (AR) model”和“Mixture-of-Experts (MoE)”，这直接对应大语言模型的核心架构。 - **能力方向**: 论文标题和摘要中多次明确提及“reasoning”。例如，摘要中直接指出SDAR“demonstrates enhanced reasoning”，并在“challenging scientific reasoning benchmarks”上验证了其效果，最终目标是实现“scalable, high-throughput reasoning”。这直接命中了您研究的核心能力方向。 - **训练方法**: 论文提出的“lightweight paradigm conversion”和“data-efficient adaptation”是一种新的模型训练/转换范式，旨在高效地改造现有模型。 - **新兴范式**: SDAR本身就是一个被提出的新兴范式，它探索了结合自回归与扩散模型的混合生成方法，旨在提升模型的基础能力。 **第三步：排除标准** 论文不触及任何主要的排除领域： - **多模态与视觉**: 论文完全专注于文本序列的生成，未涉及视觉或多模态内容。 - **特定应用领域**: 尽管论文在“ChemBench”等基准上进行了测试，但这并非论文的焦点。其核心是提出一个**通用的**生成范式SDAR，而这些科学推理基准仅被用作衡量该通用范式效果的“试金石”。论文的标题、摘要和核心贡献都围绕SDAR范式本身，而非化学应用，因此不应被排除。 - **模型可靠性（应用层面）**: 论文未讨论水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** 论文中“ChemBench”的出现是一个需要精确判断的点。根据筛选标准，如果论文是“一种用于化学的智能体”，则应排除。但本论文的论述逻辑是：我们提出了一个提升**通用推理能力**的新范式SDAR，作为证明，该范式在**包括化学在内的多个科学推理基准**上都取得了提升。因此，ChemBench是验证其**通用性**的证据，而不是其**特定性**的目标。这符合“保留”的原则。 **第五步：最终决策** 综合分析，该论文的核心贡献是一种创新的、旨在提升大语言模型生成效率和推理质量的通用范式（SDAR）。它通过改进模型底层的生成机制，直接作用于LLM的通用推理能力，并在高难度的推理基准上验证了其有效性。这与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，应判定为符合要求。"
    },
    {
        "index": "#157",
        "title": "Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them",
        "link": "/arxiv/2510.06534",
        "arxiv_id": "2510.06534",
        "authors": "Jiahe Jin, Abhijay Paladugu, Chenyan Xiong",
        "summary": "Agentic search leverages large language models (LLMs) to interpret complex user information needs and execute a multi-step process of planning, searching, and synthesizing information to provide answers. This paradigm introduces unique challenges for LLMs' reasoning and agentic capabilities when interacting with retrieval systems and the broader web. In this paper, we propose a reasoning-driven LLM-based pipeline to study effective reasoning behavior patterns in agentic search. Using this pipeline, we analyze successful agentic search trajectories and identify four beneficial reasoning behaviors: Information Verification, Authority Evaluation, Adaptive Search, and Error Recovery. Based on these findings, we propose a technique called Behavior Priming to train more effective agentic search models. It synthesizes agentic search trajectories that exhibit these four behaviors and integrates them into the agentic search model through supervised fine-tuning (SFT), followed by standard reinforcement learning (RL). Experiments on three benchmarks (GAIA, WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models with RL. Crucially, we demonstrate that the desired reasoning behaviors in the SFT data, rather than the correctness of the final answer, is the critical factor for achieving strong final performance after RL: fine-tuning on trajectories with desirable reasoning behaviors but incorrect answers leads to better performance than fine-tuning on trajectories with correct answers. Our analysis further reveals the underlying mechanism: the introduced reasoning behaviors endow models with more effective exploration (higher pass@k and entropy) and test-time scaling (longer trajectories) capabilities, providing a strong foundation for RL. Our code will be released as open source.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.538185",
        "filter_reason": "这篇论文完全符合您关于“大语言模型通用推理能力”的研究范围。我的判断过程如下： **第一步：核心判断** 论文的本质是研究如何提升LLM在智能体搜索任务中的推理能力。它并非将LLM作为工具应用于某个特定领域，而是深入分析了LLM在执行通用任务（搜索、规划、信息整合）时所展现的推理行为模式。其核心贡献是提出了一种名为“行为启动”的**新训练范式**，通过向模型注入有益的推理行为（信息验证、权威评估、自适应搜索、错误恢复）来增强其内在能力。这直接属于改进LLM基础能力和通用推理能力的范畴，因此应**保留**。 **第二步：正面指标** 论文高度契合所有正面指标： - **核心概念**: 明确以 \"large language models (LLMs)\" 为研究对象。 - **能力方向**: 核心主题是 \"reasoning\"，并深入探讨了 \"planning\"、\"problem-solving\" 和 \"error recovery\" 等具体能力。 - **训练方法**: 提出了结合 \"supervised fine-tuning (SFT)\" 和 \"reinforcement learning (RL)\" 的新方法。 - **新兴范式**: 研究的核心是 \"Agentic search\"，即基于LLM的智能体框架。 **第三步：排除标准** 论文完全避开了所有排除标准： - **多模态与视觉**: 论文仅涉及文本和网络信息，不涉及视觉或多模态内容。 - **特定应用领域**: 实验基准（GAIA, WebWalker, HLE）是通用的，研究目标是通用的智能体搜索能力，而非医疗、化学等特定领域。 - **模型可靠性（应用层面）**: 论文讨论的“错误恢复”是一种内在的推理能力，而非应用层面的水印或安全策略。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文将“智能体搜索”作为一个**通用框架**来研究和提升LLM的推理与规划能力，而非将其应用于特定垂直领域。这完全符合保留条件。 - **幻觉/可解释性/安全**: 论文提出的“信息验证”和“错误恢复”行为，本质上是提升模型内在推理质量和可靠性的方法。它不是在讨论这些现象的社会影响，而是在提出一种**新的训练方法来从根源上改善**这些行为，因此符合保留条件。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于：1）识别并定义了LLM在通用任务中展现出的四种有益推理行为；2）提出了一种创新的训练方法（行为启动）来系统性地将这些行为赋予模型；3）通过严谨的实验证明，**推理过程的质量比最终答案的正确性更能决定模型的上限**。这项工作直接推动了我们对如何提升LLM通用推理能力的理解，并提供了一套行之有效的方法论，是您研究课题下的高质量前沿论文。因此，最终判断为 **True**。"
    },
    {
        "index": "#173",
        "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization",
        "link": "/arxiv/2510.06274",
        "arxiv_id": "2510.06274",
        "authors": "Mohammad Mahdi Samiei Paqaleh, Arash Marioriyad, Arman Tahmasebi-Zadeh, Mohamadreza Fereydooni, Mahdi Ghaznavai, Mahdieh Soleymani Baghshah",
        "summary": "Recent progress has pushed AI frontiers from pattern recognition tasks toward problems that require step by step, System2 style reasoning, especially with large language models. Yet, unlike learning, where generalization and out of distribution (OoD) evaluation concepts are well formalized, there is no clear, consistent definition or metric for reasoning ability. We propose Complexity Out of Distribution (Complexity OoD) generalization as a framework and problem setting to define and measure reasoning. A model exhibits Complexity OoD generalization when it maintains performance on test instances whose minimal required solution complexity, either representational (richer solution structure) or computational (more reasoning steps/program length), exceeds that of all training examples. We formalize complexity via solution description Kolmogorov complexity and operational proxies (e.g., object/relation counts; reasoning step counts), clarifying how Complexity OoD differs from length and compositional OoD. This lens unifies learning and reasoning: many cases solvable with System1 like processing at low complexity become System2 like under complexity pressure, while System2 can be viewed as generalization over solution structures. We translate this perspective into practice with recommendations for operationalizing Complexity OoD across the stack: incorporating complexity into benchmark and evaluation metric design, rethinking supervision to target solution traces, seeking and designing inductive biases for Complexity OoD generalization, addressing learning to reason spillovers such as spurious shortcuts, semantic robustness, catastrophic forgetting, and step wise calibration. Because Complexity OoD cannot be solved by scaling data alone, progress toward robust reasoning will require architectures and training regimes that explicitly model and allocate computation with respect to complexity.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-09T19:59:14.551543",
        "filter_reason": "这篇论文完全符合筛选标准，是关于“大语言模型通用推理能力”的核心前沿研究。 我的判断过程如下： 1.  **第一步：核心判断——完全符合。** 这篇论文的本质并非将LLM作为工具应用于特定领域，而是直击LLM研究的核心难题：如何定义、度量并最终提升其『通用推理能力』。论文的核心贡献是提出了一个名为“Complexity Out of Distribution (Complexity OoD) generalization”的新框架。这个框架旨在为“推理”这个模糊的概念提供一个清晰的、可操作的定义和评估标准。这直接属于“改进LLM的基础能力”和“提出新的训练范式”的范畴，因为它为如何设计更好的训练目标、评估基准和模型架构以增强推理能力指明了方向。 2.  **第二步：正面指标——高度相关。** 论文摘要中明确包含了多个核心正面指标： -   **核心概念**: 明确提到 \"large language models\"。 -   **能力方向**: 整篇论文都围绕 \"reasoning\" 展开，特别是 \"System2 style reasoning\" 和 \"step by step reasoning\"，这正是通用推理能力的核心。 -   **新兴范式**: 虽然没有直接提及agents或tool use，但其提出的“learning to reason”和“generalization over solution structures”等观点，为构建更强大的推理智能体提供了理论基础。 3.  **第三步：排除标准——未触发。** 论文完全没有涉及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施（如部署、硬件加速）。它讨论的是模型内在的、领域无关的能力。 4.  **第四步：处理特殊和模糊情况——强化了保留决策。** 论文提到了“spurious shortcuts”和“semantic robustness”等问题。根据筛选标准，如果论文提出新方法来解决这些问题以提升通用推理质量，就应该保留。这篇论文正是如此：它将这些问题视为“learning to reason spillovers”（学习推理的溢出效应），并提出通过Complexity OoD框架来系统性地解决它们，从而实现“robust reasoning”（稳健的推理）。这完全符合提升模型内在可靠性和推理质量的目标。 **最终决策：** 这篇论文的价值在于它没有停留在“如何让模型在某个推理任务上表现更好”，而是上升到了“我们到底该如何定义和衡量推理本身”的元问题。它提出的“Complexity OoD generalization”框架，为整个LLM推理研究领域提供了一个新的、深刻的理论视角和评估基准。这正是一项致力于“提高大语言模型（LLM）本身的『通用推理能力』”的研究所需要的基础性、前沿性工作。因此，这篇论文高度符合我的研究范围，应予以保留。"
    },
    {
        "index": "#10",
        "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning",
        "link": "/arxiv/2510.07038",
        "arxiv_id": "2510.07038",
        "authors": "Wenxun Wu, Yuanyang Li, Guhan Chen, Linyue Wang, Hongyang Chen",
        "summary": "Recent advances in large language models (LLMs) have popularized test-time scaling, where models generate additional reasoning tokens before producing final answers. These approaches have demonstrated significant performance improvements on benchmarks involving mathematical reasoning. However, language models relying solely on direct inference still struggle with tasks demanding up-to-date knowledge or computational tools such as calculators and code interpreters for complex arithmetic operations. To overcome these limitations, we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement learning framework that systematically integrates multi-hop reasoning with adaptive tool-calling capabilities. Our approach employs a modified version of Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm, which we adapt specifically for tool invocation scenarios, enabling models to dynamically interleave complex reasoning with on-demand tool usage (including search APIs and Python interpreters). To support this research, we introduce two new datasets: TAPO-easy-60K and TAPO-hard-18K, specifically designed to train and evaluate both fact-based reasoning and mathematical calculation capabilities. Our experiments on Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach, with both models achieving state-of-the-art performance on tasks requiring external knowledge and mathematical computation among methods with comparable parameters. Notably, TAPO achieves more efficient tool utilization than baseline methods while preventing excessive calls caused by reward hacking. These results highlight the significant potential of combining advanced reasoning with tool usage to enhance model performance in knowledge-intensive and computationally demanding tasks.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T19:59:14.475848",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： **第一步：核心判断** 论文的核心是提出一种名为“工具增强策略优化（TAPO）”的新型强化学习框架。其本质并非将LLM应用于特定领域，而是致力于改进LLM本身的基础能力。具体来说，它通过一种新的训练范式，系统性地将多步推理（multi-hop reasoning）与自适应工具调用（adaptive tool-calling）能力结合起来，从而增强LLM在需要外部知识和复杂计算的通用任务上的表现。这直接命中了您“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。 **第二步：正面指标** 该论文包含了多个关键的正面指标： - **核心概念**: 论文明确以大语言模型（LLMs）为研究对象。 - **能力方向**: 核心关注点是“reasoning”（推理），特别是“mathematical reasoning”（数学推理）和“fact-based reasoning”（基于事实的推理）。 - **训练方法**: 提出了一种新的强化学习（RL）方法，即修改版的DAPO，用于优化模型的策略。 - **新兴范式**: 论文的核心贡献之一就是“tool use”（工具使用），并探讨了如何让模型动态地、自适应地使用工具（如搜索API、Python解释器）来增强其推理能力。 **第三步：排除标准** 论文的主要焦点完全不在排除标准所列的任何领域。它不涉及多模态、视觉，也没有将方法限定在医疗、化学、机器人等特定应用领域。同时，它也不是关于模型基础设施、部署优化或应用层面的水印、安全等问题。 **第四步：处理特殊和模糊情况** 论文恰好是“智能体/工具使用”这一特殊情况的正面范例。它提出的是一种**通用的**工具使用框架（TAPO），旨在增强LLM的**通用问题解决能力**（尤其是在知识密集型和计算密集型任务上），而不是将工具应用在某个特定领域。因此，根据筛选标准，应当保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种通过强化学习来协同LLM的推理能力和自适应工具使用能力的新方法。这直接回应了您对“提高LLM通用推理能力”的研究需求，属于方法论层面的前沿探索。因此，最终判断为符合要求。"
    },
    {
        "index": "#13",
        "title": "TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs",
        "link": "/arxiv/2510.06878",
        "arxiv_id": "2510.06878",
        "authors": "Daria Ozerova, Ekaterina Trofimova",
        "summary": "Iterative refinement has been a promising paradigm to enable large language models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of the key challenges, however, is how to effectively search through the enormous search space of possible refinements. Existing methods typically fall back on predefined heuristics, which are troubled by the exploration-exploitation dilemma and cannot adapt based on past refinement outcomes. We introduce Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with a Thompson-Sampling-based tree search. TGPR explores both failed and successful refinement paths actively, with denser training trajectories and more adaptive policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to +4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to +12.51 percentage points absolute improvement in pass@10 (on APPS) compared to a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a principled approach to combining learned policies with structured search methods, offering a general framework for enhancing iterative refinement and stateful reasoning in LLMs.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T19:59:14.477222",
        "filter_reason": "这篇论文完全符合您的研究范围。判断过程如下： 1.  **第一步：核心判断（保留）** 论文的核心贡献是提出了一种名为TGPR（Tree-Guided Policy Refinement）的新框架。这个框架通过将强化学习（GRPO）与树搜索相结合，旨在解决大语言模型在进行“迭代式精炼”（iterative refinement）时面临的搜索空间巨大这一核心挑战。这直接关系到提升LLM解决复杂推理和问题任务的能力。因此，这篇论文的本质是**改进LLM的基础能力**（迭代式精炼和有状态推理）和**提出新的训练范式**（结合强化学习的树搜索策略），完全符合第一步的保留标准。 2.  **第二步：正面指标（高度相关）** - **核心概念**: 论文明确关注 \"Large language models (LLMs)\"。 - **能力方向**: 摘要多次提到 \"difficult reasoning and problem-solving tasks\", \"iterative refinement\", 和 \"stateful reasoning\"，这些都是通用推理能力的核心。 - **训练方法**: 论文的核心技术是基于 \"reinforcement learning (GRPO)\" 的，这是筛选标准中明确列出的训练方法。 - **新兴范式**: \"Iterative refinement\" 是当前提升LLM复杂问题解决能力的一个重要研究方向。 3.  **第三步：排除标准（未触发）** - 论文不涉及多模态、视觉等内容。 - 尽管论文的实验基准是代码数据集，但其研究焦点并非“代码生成”这个特定领域，而是探索一种通用的优化方法。这将在下一步详细说明。 - 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况（适用并满足保留条件）** 论文的实验设置看似聚焦于“代码”这个特定领域，这需要仔细甄别。然而，论文摘要的最后一句做出了清晰的界定：“Apart from debugging code, TGPR focuses on a principled approach to combining learned policies with structured search methods, offering a **general framework for enhancing iterative refinement and stateful reasoning in LLMs**.” 这句话明确指出，论文的价值不在于解决了“代码调试”这个具体问题，而在于提出了一种可以**增强LLM通用迭代式精炼和有状态推理能力的通用框架**。代码调试只是用来验证该通用框架有效性的一个测试平台。这完全符合“如果提出一种通用的...方法来增强LLM的通用问题解决能力，应该保留”的原则。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种创新的、基于强化学习和搜索的训练优化框架，其目标是提升LLM在解决复杂任务时的迭代精炼和有状态推理能力。作者明确将该工作定位为一个通用方法论，而非特定领域的应用。因此，这篇论文是关于提升“大语言模型通用推理能力”的高质量前沿研究，**应当被保留**。"
    },
    {
        "index": "#14",
        "title": "Autoformalizer with Tool Feedback",
        "link": "/arxiv/2510.06857",
        "arxiv_id": "2510.06857",
        "authors": "Qi Guo, Jianing Wang, Jianfei Zhang, Deyang Kong, Xiangzhou Huang, Xiangyu Xi, Wei Wang, Jingang Wang, Xunliang Cai, Shikun Zhang, Wei Ye",
        "summary": "Autoformalization addresses the scarcity of data for Automated Theorem Proving (ATP) by translating mathematical problems from natural language into formal statements. Efforts in recent work shift from directly prompting large language models to training an end-to-end formalizer model from scratch, achieving remarkable advancements. However, existing formalizer still struggles to consistently generate valid statements that meet syntactic validity and semantic consistency. To address this issue, we propose the Autoformalizer with Tool Feedback (ATF), a novel approach that incorporates syntactic and consistency information as tools into the formalization process. By integrating Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge approach for consistency validation, the model is able to adaptively refine generated statements according to the tool feedback, enhancing both syntactic validity and semantic consistency. The training of ATF involves a cold-start phase on synthetic tool-calling data, an expert iteration phase to improve formalization capabilities, and Direct Preference Optimization to alleviate ineffective revisions. Experimental results show that ATF markedly outperforms a range of baseline formalizer models, with its superior performance further validated by human evaluations. Subsequent analysis reveals that ATF demonstrates excellent inference scaling properties. Moreover, we open-source Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate advancements in autoformalization and ATP research.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T19:59:14.477747",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据您的筛选标准进行的详细判断过程： **第一步：核心判断——这篇论文的本质是什么？** 这篇论文的本质是提出一种名为“Autoformalizer with Tool Feedback (ATF)”的新方法，旨在提升大语言模型将自然语言数学问题转换为形式化语言（如Lean 4）的能力。其核心贡献并非解决某个特定的数学或科学问题，而是**改进LLM在执行一项高度复杂的推理任务——数学形式化——时的基础能力**。论文通过引入工具反馈机制，让模型能够自我修正和迭代优化，这直接增强了模型的逻辑严谨性和多步推理能力。因此，它属于“改进LLM的基础能力、增强其逻辑、数学、多步推理等通用能力”的范畴，应予以保留。 **第二步：正面指标——论文是否包含以下主题？** 该论文高度符合多个正面指标： - **核心概念**: 论文的核心研究对象是Large Language Models (LLMs)。 - **能力方向**: 论文聚焦于**reasoning**，特别是**math reasoning**。将自然语言数学问题形式化，是数学推理能力的极致体现，要求模型具备深刻的逻辑理解和精确的符号转换能力。 - **训练方法**: 论文采用了先进的训练范式，包括**Direct Preference Optimization (DPO)**，这是一种与强化学习相关的对齐技术，用于优化模型的决策过程。 - **新兴范式**: 论文的核心是**Tool Use**。它创新地将编译器（Lean 4）和多个LLM作为“工具”来提供反馈，指导模型进行自我修正。这是一种通用的、可迁移的增强模型推理能力的方法论，而非特定领域的应用。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 该论文不触及任何排除标准： - 它不涉及多模态、视觉或特定应用领域（如医疗、化学等）。虽然研究对象是数学，但其目标是提升模型的**通用数学推理能力**，而非解决某个具体的数学分支问题。 - 它不关注模型基础设施、部署优化或硬件加速。 - 它不涉及水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这篇论文是“提出一种通用的工具使用方法来增强LLM的通用问题解决能力”的绝佳范例。它使用工具（编译器、LLM评委）来提升模型在形式化推理这一通用任务上的表现，而不是将工具应用于特定领域。因此，完全符合保留条件。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种通过工具反馈来增强大语言模型数学推理能力的新范式。它直接作用于提升LLM的通用推理能力，方法论具有创新性和通用性，完全符合您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标。因此，最终判断为符合要求。"
    },
    {
        "index": "#21",
        "title": "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks",
        "link": "/arxiv/2510.06587",
        "arxiv_id": "2510.06587",
        "authors": "Jingbo Yang, Bairu Hou, Wei Wei, Shiyu Chang, Yujia Bao",
        "summary": "Large language model (LLM) agents are becoming competent at straightforward web tasks, such as opening an item page or submitting a form, but still struggle with objectives that require long horizon navigation, large scale information extraction, and reasoning under constraints. We present WebDART, a general framework that enables a single LLM to handle such complex chores. WebDART (i) dynamically decomposes each objective into three focused subtasks: navigation, information extraction, and execution, so the model concentrates on one skill at a time, and (ii) continuously replans the decomposition as new webpages are revealed, taking advantage of newly discovered filters or shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena, WebDART lifts success rates by up to 13.7 percentage points over previous SOTA agents, while matching their performance on the easier WebArena suite and completing tasks with up to 14.7 fewer navigation steps.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T19:59:14.486232",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM的通用能力。** 论文的核心贡献是提出了一个名为WebDART的**通用框架**。这个框架旨在解决LLM在处理“长视距导航、大规模信息提取和约束条件下推理”等复杂任务时的能力短板。其核心技术是“动态分解”和“重新规划”，这二者都是对LLM**通用推理和规划能力**的直接增强方法论。它不是将LLM应用于某个特定垂直领域，而是在一个相对通用和开放的环境（网页）中，研究如何让LLM的“大脑”变得更会思考、更会规划。因此，这完全符合“改进LLM的基础能力”和“增强其...规划、多步推理等通用能力”的要求。 2.  **第二步：正面指标——论文高度相关。** 论文摘要中明确包含了多个关键正面指标： *   **核心概念**: \"Large language model (LLM) agents\" *   **能力方向**: \"reasoning under constraints\" (约束条件下推理), \"long horizon navigation\" (长视距规划) *   **新兴范式**: \"LLM agents\" (智能体) 这些关键词都直接指向了您的核心研究目标——“大语言模型通用推理能力”。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究对象是网页任务，这是一个通用领域，而非化学、医疗、生物等特定应用领域。论文也完全不涉及多模态视觉、模型部署优化或水印、安全等被排除的主题。 4.  **第四步：处理特殊和模糊情况——智能体框架。** 本文是“智能体/工具使用”情况的典型范例。WebDART是一个**通用的智能体协作框架**，其设计目的是为了增强LLM在“复杂任务”中的“通用问题解决能力”。它不是一个“用于XX领域的智能体”，而是一个“提升LLM规划与推理能力的智能体方法论”。因此，根据筛选标准，它应该被明确**保留**。 **最终决策**: 综合以上分析，论文《WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks》的核心贡献在于提出了一种新的方法论（动态分解与重新规划），以实质性提升LLM在复杂任务中的规划与多步推理能力。它完全聚焦于LLM本身的通用能力增强，而非特定领域的应用，因此与您的研究课题高度契合。"
    },
    {
        "index": "#26",
        "title": "Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?",
        "link": "/arxiv/2510.06410",
        "arxiv_id": "2510.06410",
        "authors": "Aochong Oliver Li, Tanya Goyal",
        "summary": "Reasoning LLMs are trained to verbalize their reasoning process, yielding strong gains on complex tasks. This transparency also opens a promising direction: multiple reasoners can directly collaborate on each other's thinking within a shared trajectory, yielding better inference efficiency and exploration. A key prerequisite, however, is the ability to assess the usefulness and build on another model's partial thinking -- we call this off-trajectory reasoning. Our paper investigates a critical question: can standard solo-reasoning training pipelines deliver desired off-trajectory behaviors? We propose twin tests that capture the two extremes of the off-trajectory spectrum, namely Recoverability, which tests whether LLMs can backtrack from \"distractions\" induced by misleading reasoning traces, and Guidability, which tests their ability to build upon correct reasoning from stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and reveals a counterintuitive finding -- \"stronger\" LLMs on benchmarks are often more fragile under distraction. Moreover, all models tested fail to effectively leverage guiding steps from collaborators on problems beyond their inherent capabilities with solve rates remaining under 9.2%. Finally, we conduct control studies to isolate the effects of three factors in post-training on these behaviors: the choice of distillation teacher, the use of RL, and data selection strategy. Our results provide actionable insights for training natively strong reasoning collaborators; e.g., we find that suboptimal recoverability behaviors of teacher models are transferred to distilled students even if the distillation trajectories are correct. Taken together, this work lays the groundwork for evaluating multi-model collaborations in shared reasoning trajectories and highlights the limitations of off-the-shelf reasoning LLMs.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-07",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T19:59:14.488552",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是研究一种名为“Off-Trajectory Reasoning”（偏离轨迹推理）的全新通用推理能力。它并非将LLM应用于特定领域，而是深入探究LLM在协作推理场景下的基础行为和局限性。论文的核心贡献在于：1) 定义并评估了LLM在协作推理中的两种关键能力（Recoverability和Guidability）；2) 揭示了当前顶尖LLM在这些新维度上的不足；3) 通过控制实验分析了蒸馏、强化学习等训练方法对这些能力的影响。这直接关系到如何改进LLM的基础推理范式和训练策略，以提升其通用能力。 2.  **第二步：正面指标** - 论文高度匹配所有正面指标： - **核心概念**: 明确以\"Reasoning LLMs\"为研究对象。 - **能力方向**: 核心主题就是\"reasoning\"，特别是多步推理和协作推理。 - **训练方法**: 直接研究了\"distillation\"（蒸馏）和\"RL\"（强化学习）对模型推理行为的影响。 - **新兴范式**: 提出的\"multiple reasoners can directly collaborate\"（多个推理器直接协作）框架，本质上是一种通用的多智能体系统，旨在增强LLM的通用问题解决能力。 3.  **第三步：排除标准** - 论文完全不涉及任何排除标准领域。它没有讨论多模态、视觉，没有聚焦于医疗、化学等特定应用，也未涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的“多模型协作框架”是一个典型的**通用智能体协作框架**。其目标是增强LLM在通用推理任务上的表现，而非应用于特定领域。因此，根据筛选标准，应该保留。 - **幻觉/可解释性/安全**: 论文研究的“Recoverability”（从误导信息中恢复的能力）与提升模型推理过程的鲁棒性和可靠性密切相关，这可以被视为提升通用推理质量的一个方面，符合保留条件。 5.  **第五步：最终决策** - 综合以上分析，该论文是一篇高质量的前沿研究。它不仅定义了一个新的通用推理能力维度，还通过严谨的实验揭示了现有模型的短板，并为未来如何训练出更擅长协作推理的LLM提供了“actionable insights”（可行的见解）。这完全契合你“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标。"
    },
    {
        "index": "#27",
        "title": "Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks",
        "link": "/arxiv/2510.06307",
        "arxiv_id": "2510.06307",
        "authors": "Wentao Deng, Jiahuan Pei, Zhiwei Xu, Zhaochun Ren, Zhumin Chen, Pengjie Ren",
        "summary": "A multi-agent system (MAS) enhances its capacity to solve complex natural language processing (NLP) tasks through collaboration among multiple agents, where consensus-seeking serves as a fundamental mechanism. However, existing consensus-seeking approaches typically rely on voting mechanisms to judge consensus, overlooking contradictions in system-internal beliefs that destabilize the consensus. Moreover, these methods often involve agents updating their results through indiscriminate collaboration with every other agent. Such uniform interaction fails to identify the optimal collaborators for each agent, hindering the emergence of a stable consensus. To address these challenges, we provide a theoretical framework for selecting optimal collaborators that maximize consensus stability. Based on the theorems, we propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate stable consensus via selecting optimal collaborators and calibrating the consensus judgment by system-internal beliefs. Experimental results on the MATH and MMLU benchmark datasets demonstrate that the proposed BCCS framework outperforms the best existing results by 2.23% and 3.95% of accuracy on challenging tasks, respectively. Our code and data are available at https://github.com/dengwentao99/BCCS.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-07",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T19:59:14.489027",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **第一步：核心判断——本质是方法论研究，而非领域应用** 论文的核心贡献是提出了一种名为“信念校准的共识寻求（BCCS）”的**多智能体协作框架**。这并非将LLM作为工具应用于某个特定垂直领域（如化学、医疗），而是提出了一种**通用的方法论**，旨在通过改进多个LLM智能体之间的协作方式，来提升它们解决复杂任务的集体能力。这直接命中了筛选标准中“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”以及“智能体协作框架”的保留要求。 2.  **第二步：正面指标——高度相关** - **核心概念**: 论文的研究对象是“多智能体系统（MAS）”，在当前AI研究中，这通常指由LLM驱动的智能体。 - **能力方向**: 论文明确在**MATH**和**MMLU**这两个公认的、用于评估**数学推理**和**通用推理**能力的基准数据集上进行了验证。这直接表明其研究目标是提升LLM的通用推理能力。 - **新兴范式**: 论文的主题“多智能体系统”正是筛选标准中明确列出的新兴范式之一。 3.  **第三步：排除标准——完全规避** 论文不涉及任何多模态、视觉内容，也没有聚焦于任何特定应用领域（如医疗、化学、机器人等）。同时，其研究焦点也不是水印、安全等应用层面的可靠性问题。 4.  **第四步：特殊和模糊情况——符合保留条件** - **智能体/工具使用**: 论文提出的BCCS框架是一个**通用的智能体协作框架**，其有效性在通用的推理基准上得到证明，而非应用于特定领域。这完全符合“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的保留条件。 - **幻觉/可解释性/安全**: 论文中提到的“系统内部信念”和“校准共识判断”，可以被视为一种提升模型内在一致性和决策质量的新方法。通过解决“系统内部信念的矛盾”，它旨在提升模型输出的稳定性和可靠性，从而间接提升推理质量，这符合“提升模型的通用可靠性和推理质量”的保留标准。 **最终决策**: 综合以上分析，该论文提出了一种新颖的、通用的多智能体协作框架（BCCS），其核心目标是通过优化智能体间的共识机制来提升LLM在数学和通用推理任务上的表现。这完全契合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心研究目标。因此，这篇论文应该被保留。"
    },
    {
        "index": "#150",
        "title": "VeriEquivBench: An Equivalence Score for Ground-Truth-Free Evaluation of Formally Verifiable Code",
        "link": "/arxiv/2510.06296",
        "arxiv_id": "2510.06296",
        "authors": "Lingfei Zeng, Fengdi Che, Xuhan Huang, Fei Ye, Xu Xu, Binhang Yuan, Jie Fu",
        "summary": "Formal verification is the next frontier for ensuring the correctness of code generated by Large Language Models (LLMs). While methods that co-generate code and formal specifications in formal languages, like Dafny, can, in principle, prove alignment with user intent, progress is bottlenecked by specification quality evaluation. Current benchmarks rely on matching against ground-truth specifications, a manual and expertise-intensive process that has limited existing datasets to a few hundred simple problems and also suffers from a reliability issue. To address this, we introduce VeriEquivBench, a new benchmark with $2,389$ complex algorithmic problems that probe the limitations of current models in both code generation and formal reasoning. Our evaluation framework replaces ground-truth matching with a formally grounded metric, the equivalence score, and rigorously verifies the quality of generated specifications and code. Our results show that generating formally verifiable code remains a profound challenge for state-of-the-art LLMs. This underscores both the difficulty of the task and the need for benchmarks like VeriEquivBench to drive progress toward scalable and reliable coding agents.",
        "subjects": "Programming Languages, Artificial Intelligence",
        "date": "2025-10-07",
        "category": "cs.AI",
        "crawl_time": "2025-10-09T19:59:14.574763",
        "filter_reason": "这篇论文符合我的研究范围，判断依据如下： **第一步：核心判断** 这篇论文的本质是提出一个新的评估基准和度量标准，用于衡量大语言模型在“可形式化验证的代码生成”任务上的表现。虽然它没有直接提出一种新的训练范式或模型架构来“改进”LLM，但它精准地聚焦于LLM的一项核心通用能力——**形式化推理**。论文明确指出，生成可形式化验证的代码对现有LLM是“一个重大挑战”，并旨在通过这个基准“推动……可扩展且可靠的编码智能体的发展”。因此，这篇论文的核心贡献是为我们研究“如何提升LLM通用推理能力”这一课题，提供了关键的、前沿的“标尺”和“试金石”，它定义和量化了问题本身，是推动该领域进步不可或缺的基础性工作。 **第二步：正面指标** 该论文高度符合正面指标： *   **核心概念**: 论文明确以 Large Language Models (LLMs) 为研究对象。 *   **能力方向**: 论文的核心是评估 **reasoning** 能力，特别是更严格的子领域——**formal reasoning**（形式化推理），这比一般的逻辑推理要求更高，是通用推理能力的核心体现。 *   **新兴范式**: 论文的目标是推动 **llm-based agents** 的发展，具体而言是“编码智能体”，这是一种通用的、以代码为媒介解决问题的智能体范式。 **第三步：排除标准** 该论文不触及任何排除标准： *   它不涉及多模态或视觉。 *   它的研究领域是通用的计算机科学和逻辑学，而非医疗、化学等特定应用领域。 *   它讨论的“可靠性”是基于形式化验证的代码正确性，这属于逻辑推理和模型内在能力的范畴，而不是应用层面的水印、安全或伦理问题。 **第四步：处理特殊和模糊情况** 本案例的核心矛盾点在于“评估论文”是否应被保留。对于“致力于提高”这一目标，一个能够精准揭示当前模型能力短板、并指明未来努力方向的评估基准，其重要性不亚于一个具体的改进方法。特别是该论文提出的基准填补了现有方法的不足（如依赖人工标注、规模小、可靠性差），为学术界提供了一个更强大、更可靠的工具来衡量和驱动“形式化推理”这一核心能力的进步。它不是简单应用，而是对基础能力进行深入、严谨的度量。 **第五步：最终决策** 综合以上分析，尽管这篇论文的产出是一个“基准”而非一种“算法”或“范式”，但它所瞄准和度量的“形式化推理能力”正是我研究目标中“通用推理能力”的关键组成部分。作为一个前沿研究者，我需要关注那些定义问题、衡量进展的基础性工作。这篇论文通过提供高质量的评估工具，极大地促进了“如何提升LLM形式化推理能力”这一核心问题的研究。因此，它完全符合我的筛选要求，应被保留。"
    }
]