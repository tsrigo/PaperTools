[
    {
        "index": "#3",
        "title": "ReclAIm: A multi-agent framework for degradation-aware performance tuning of medical imaging AI",
        "link": "/arxiv/2510.17004",
        "arxiv_id": "2510.17004",
        "authors": "Eleftherios Tzanis, Michail E. Klontzas",
        "summary": "Ensuring the long-term reliability of AI models in clinical practice requires continuous performance monitoring and corrective actions when degradation occurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent framework capable of autonomously monitoring, evaluating, and fine-tuning medical image classification models. The system, built on a large language model core, operates entirely through natural language interaction, eliminating the need for programming expertise. ReclAIm successfully trains, evaluates, and maintains consistent performance of models across MRI, CT, and X-ray datasets. Once ReclAIm detects significant performance degradation, it autonomously executes state-of-the-art fine-tuning procedures that substantially reduce the performance gap. In cases with performance drops of up to -41.1% (MRI InceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of the initial model results. ReclAIm enables automated, continuous maintenance of medical imaging AI models in a user-friendly and adaptable manner that facilitates broader adoption in both research and clinical environments.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-10-19",
        "category": "cs.MA",
        "crawl_time": "2025-10-21T11:00:04.146264",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **核心判断 (第一步):** *   **保留**: 论文的核心贡献是提出一个名为 \"ReclAIm\" 的**多智能体框架**。这直接命中了你研究焦点的第二个方向 \"多智能体\"。它不是简单地将一个已有框架应用于医疗领域，而是**构建**了一个新的框架来解决模型性能退化问题。因此，它不属于“非演化型应用”的排除范畴。 2.  **正面指标 (第二步):** *   论文明确包含了多个核心关注点。标题和摘要中直接提到了 `Multi-Agent framework`。 *   更重要的是，该框架的功能是“自主地监控、评估和微调”，这完全符合 `Self-Correction` 和 `Self-Refine` 的定义，属于你第三个研究焦点“自我演化”的范畴。智能体能够根据环境反馈（性能下降）自主采取行动（执行微调）以恢复性能，这是一个典型的自我完善循环。 3.  **排除标准 (第三步):** *   论文的主要贡献不是关于安全、对齐或可解释性，因此不触发排除标准。 *   虽然论文涉及 \"medical imaging AI\"，但根据你的规则，视觉/多模态在这里是智能体**监控和操作的对象**，而不是研究的核心。研究的核心是那个能够管理和维护这些视觉模型的**智能体框架**本身。因此，这不属于被排除的情况。 4.  **特殊和模糊情况处理 (第四步):** *   这篇论文是“自我演化的应用”的一个完美范例。根据你的核心规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。” ReclAIm 提出的正是一种**自主监控并自我校正的演化机制**，并将其应用于医疗成像领域。因此，它不仅应该被保留，而且是你研究范围内的一个高相关度样本。 **最终决策 (第五步):** 综合以上分析，这篇论文的核心贡献在于构建了一个具有自我修正能力的多智能体框架。它完美地结合了你研究范围内的“多智能体”和“自我演化”两个方向。尽管其应用场景是特定的医疗领域，但其方法论和框架本身具有普适性，完全符合你筛选“核心贡献在于构建、改进或演化 LLM 智能体”的论文的目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#6",
        "title": "Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis",
        "link": "/arxiv/2510.16635",
        "arxiv_id": "2510.16635",
        "authors": "Wonduk Seo, Juhyeon Lee, Junseo Koh, Hyunjin An, Jian Park, Seunghyun Lee, Haihua Chen, Yi Bu",
        "summary": "Prompt optimization has emerged as an effective alternative to retraining for improving the performance of Large Language Models (LLMs). However, most existing approaches treat evaluation as a black box, relying solely on numerical scores while offering limited insight into why a prompt succeeds or fails. They also depend heavily on trial-and-error refinements, which are difficult to interpret and control. In this paper, we introduce MA-SAPO, a Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior methods, MA-SAPO explicitly couples evaluation outcomes with structured reasoning to guide systematic edits. The framework specifically consists of two stages: during the Reasoning Phase, agents collaboratively explain metric scores, diagnose weaknesses, and synthesize targeted refinements that are stored as reusable reasoning assets; during the Test Phase, agents retrieve these assets to analyze optimized prompts and apply only evidence-grounded edits. By turning evaluation signals into interpretable reasoning chains, MA-SAPO produces prompt refinements that are more transparent, auditable, and controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent improvements over single-pass prompting, retrieval-augmented baselines, and prior multi-agent strategies, validating the effectiveness of our approach.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Computation and Language, Human-Computer Interaction, Information Retrieval",
        "date": "2025-10-18",
        "category": "cs.MA",
        "crawl_time": "2025-10-21T11:00:04.148450",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM或智能体作为工具去解决一个外部领域（如生物、金融）的问题。它的核心贡献是**构建了一个名为MA-SAPO的全新多智能体框架**，用于解决“提示优化”这个AI内部的问题。这个框架本身就是一个方法论，属于构建和改进LLM智能体的范畴。 2.  **第二步：正面指标** - 论文高度符合你的核心关注点： - **核心范式**: 论文标题和摘要中明确提到了 `Multi-Agent framework`，直接命中“多智能体”方向。 - **智能体能力**: 论文描述了智能体进行 `Reasoning`（推理）、`diagnose weaknesses`（诊断弱点，一种自我反思）、`synthesize targeted refinements`（综合改进，一种自我修正），并使用 `retrieved reasoning assets`（检索推理资产，一种记忆/工具使用形式）。 - **多智能体**: 摘要中明确指出智能体是 `collaboratively`（协作地）工作，这直接命中了多智能体研究中的“协作”子方向。 - **演化机制**: 整个MA-SAPO框架是一个 `Reasoning Phase` 和 `Test Phase` 循环迭代的过程，旨在 `systematic edits`（系统性编辑）和 `refinements`（改进），这完全符合“自我完善和迭代”的演化机制。 3.  **第三步：排除标准** - 论文的主要贡献不是关于安全、对齐或多模态，因此不触犯任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的推理是典型的“保留”情况。它不是在提升LLM的基础数学能力，而是在构建一个**智能体框架**，让智能体们进行结构化推理（`structured reasoning`）来指导行动（`guide systematic edits`），这正是Agentic AI的核心。 **核心依据总结**: 该论文的核心贡献是**提出了一种新颖的多智能体协作框架（MA-SAPO）**。这个框架通过让多个智能体进行协作推理、诊断和反思，并利用记忆（推理资产）来迭代地改进提示。这完美地契合了你研究课题的三个核心方向： - **多智能体**: 智能体间的协作。 - **单智能体**: 智能体的规划、记忆、自我反思与修正。 - **自我演化**: 通过迭代和反馈进行系统性改进。 因此，这篇论文是关于“LLM智能体及其演化”的前沿研究，应该被保留。"
    },
    {
        "index": "#12",
        "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries",
        "link": "/arxiv/2510.17576",
        "arxiv_id": "2510.17576",
        "authors": "Cansu Erdogan, Cesar Alan Contreras, Alireza Rastegarpanah, Manolis Chiou, Rustam Stolkin",
        "summary": "This paper addresses the problem of planning complex manipulation tasks, in which multiple robots with different end-effectors and capabilities, informed by computer vision, must plan and execute concatenated sequences of actions on a variety of objects that can appear in arbitrary positions and configurations in unstructured scenes. We propose an intent-driven planning pipeline which can robustly construct such action sequences with varying degrees of supervisory input from a human using simple language instructions. The pipeline integrates: (i) perception-to-text scene encoding, (ii) an ensemble of large language models (LLMs) that generate candidate removal sequences based on the operator's intent, (iii) an LLM-based verifier that enforces formatting and precedence constraints, and (iv) a deterministic consistency filter that rejects hallucinated objects. The pipeline is evaluated on an example task in which two robot arms work collaboratively to dismantle an Electric Vehicle battery for recycling applications. A variety of components must be grasped and removed in specific sequences, determined by human instructions and/or by task-order feasibility decisions made by the autonomous system. On 200 real scenes with 600 operator prompts across five component classes, we used metrics of full-sequence correctness and next-task correctness to evaluate and compare five LLM-based planners (including ablation analyses of pipeline components). We also evaluated the LLM-based human interface in terms of time to execution and NASA TLX with human participant experiments. Results indicate that our ensemble-with-verification approach reliably maps operator intent to safe, executable multi-robot plans while maintaining low user effort.",
        "subjects": "Robotics, Artificial Intelligence, Human-Computer Interaction, Multiagent Systems",
        "date": "2025-10-20",
        "category": "cs.MA",
        "crawl_time": "2025-10-21T11:00:04.157179",
        "filter_reason": "这篇论文符合筛选标准，应当保留。我的判断过程如下： 1.  **第一步：核心判断** *   **核心贡献分析**: 这篇论文的核心贡献并非“用LLM成功拆卸了电动汽车电池”，而是提出了一种新颖的 **“意图驱动的LLM集成规划流程”**。这个流程包含了一系列组件：LLM集成生成器、LLM验证器、一致性过滤器。这是一个关于如何构建和设计一个多智能体规划系统的**方法论和新框架**。 *   **判断**: 论文的核心是构建一个多智能体系统（Multi-Agent System）的规划方法，而不是简单地将LLM作为工具应用于特定领域。因此，它通过了第一步的筛选，属于 **保留 (Keep)** 范畴。 2.  **第二步：正面指标** *   论文高度符合多个核心关注点： *   `Multi-Agent Systems (MAS)`: 明确涉及“multiple robots”（多机器人）和“collaboratively”（协作地工作），这是典型的多智能体研究。 *   `Planning`: 论文的标题和摘要都反复强调“Planning”，提出的核心框架就是一个“planning pipeline”。 *   `Self-Correction`: 论文中的“LLM-based verifier”（验证器）和“deterministic consistency filter”（一致性过滤器）起到了对生成计划进行校验和修正的作用，这是一种在规划过程中的自我纠正机制。 *   `Tool Use`: 机器人本身是执行物理任务的工具，而计算机视觉模块（perception-to-text scene encoding）是智能体感知环境的工具。 *   这些正面指标强烈表明该论文与我的研究目标高度相关。 3.  **第三步：排除标准** *   **安全与对齐**: 论文提到了“safe”和“hallucinated objects”，但其主要贡献是提出一个能处理这些问题的规划框架，而不是研究安全或幻觉本身。这些是系统需要解决的约束，而不是研究的主题。因此，不适用排除标准。 *   **多模态与视觉**: 论文使用了“computer vision”和“perception-to-text”，但根据核心规则，视觉在这里是作为智能体**感知环境的工具**，其目的是为后续的LLM规划提供输入。论文的核心是规划框架，而非视觉模型本身。因此，不适用排除标准。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 这篇论文是“保留”情况的完美范例。它不是在提升LLM的基础数学逻辑能力，而是在研究**智能体（多机器人）如何在复杂任务中进行规划**，并为此提出了一个包含生成、验证、过滤的完整框架。这完全属于Agentic框架下的规划研究。 5.  **第五步：最终决策** *   综合以上分析，该论文的核心贡献在于提出了一种创新的、用于**多智能体协作规划**的LLM框架。它虽然以机器人拆解为应用场景，但其研究价值在于**方法论本身**，即如何构建一个鲁棒的、能响应人类意图的多智能体规划系统。这完全契合我研究课题中的“**多智能体**”方向，特别是其“**协作**”和“**规划**”子方向。因此，最终判断为保留。"
    },
    {
        "index": "#19",
        "title": "Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration",
        "link": "/arxiv/2510.16645",
        "arxiv_id": "2510.16645",
        "authors": "Zhixuan He, Yue Feng",
        "summary": "Large Language Models (LLMs) demonstrate strong performance but often lack interpretable reasoning. This paper introduces the Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo), which enhances both performance and interpretability by simulating a structured debate among four specialized LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the framework to collaboratively explore diverse cognitive approaches. Through iterative debate, agents challenge and refine initial responses, yielding more robust conclusions and an explicit, auditable reasoning chain. Across six benchmarks and under a unified open-source setup, DiMo improves accuracy over widely used single-model and debate baselines, with the largest gains on math. We position DiMo as a semantics-aware, Web-native multi-agent framework: it models human-machine intelligence with LLM agents that produce semantically typed, URL-annotated evidence chains for explanations and user-friendly interactions. Although our experiments use standard reasoning benchmarks, the framework is designed to be instantiated over Web corpora and knowledge graphs, combining retrieval-augmented reasoning with structured justifications that downstream systems can inspect and reuse.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Multiagent Systems",
        "date": "2025-10-18",
        "category": "cs.MA",
        "crawl_time": "2025-10-21T11:00:04.166212",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献与您的筛选标准高度契合。以下是我的详细判断过程： **第一步：核心判断——论文的本质是什么？** - **保留 (Keep)**。这篇论文的本质是构建一个新的多智能体系统（Multi-Agent Systems）方法论。它提出了一个名为“DiMo”的框架，该框架的核心不是将LLM作为工具去解决某个外部领域的问题，而是研究如何通过多个LLM智能体的协作来改进推理过程本身。这完全符合“构建、改进或演化 LLM智能体”的核心目标。 **第二步：正面指标——论文是否包含我的核心关注点？** - **核心范式**: 论文明确提出了一个 `Multi-Agent Collaboration Framework`，直接命中了 `Multi-Agent Systems (MAS)` 这一核心范式。 - **多智能体**: 摘要中详细描述了智能体间的协作机制，如 `structured debate`（结构化辩论）、`collaboratively explore`（协作探索）、`challenge and refine`（挑战与精炼），这些都属于 `Collaboration` 和 `Communication` 的范畴。 - **智能体能力**: 论文通过让智能体模拟不同的推理范式，实际上是在探索一种高级的 `Planning` 和 `Reasoning` 能力。其 `iterative debate`（迭代辩论）过程也隐含了 `Self-Correction` 或 `Self-Refinement` 的思想，即智能体在交互中修正和完善彼此的输出。 - **演化机制**: `iterative debate` 和 `refine initial responses` 描述了一个 `Iterative Improvement` 的过程，这与自我演化的思想相关，尽管它更偏向于群体内的演化而非单个智能体的自我演化。 **第三步：排除标准——是否为我的研究焦点之外？** - **安全与对齐**: 论文的主要贡献是提升性能和可解释性，而非安全、对齐或可解释性本身。它将可解释性作为其方法带来的一个**结果**（`explicit, auditable reasoning chain`），而不是研究的**主题**。因此，这不属于排除范围。 - **多模态与视觉**: 论文内容完全基于文本，不涉及任何多模态或视觉内容。 **第四步：处理特殊和模糊情况** - **推理/规划 (Reasoning/Planning)**: 这篇论文是典型的“保留”情况。它不是在研究如何通过微调或新数据集来提升LLM的基础数学能力，而是在构建一个**智能体框架**（`Multi-Agent Collaboration Framework`）来处理复杂推理任务。这与 `ReAct`、`ToT` 等Agentic框架属于同一类别，是您研究焦点中的“单智能体/多智能体”方向。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种新颖的**多智能体协作框架（DiMo）**，通过让多个具有不同推理模式的智能体进行结构化辩论，来提升LLM系统的推理性能和可解释性。这完全符合您研究课题中的“**多智能体 (Multi-Agent)**”方向，特别是其子方向“**智能体间的协作、通信**”。因此，这篇论文是高度相关且应该被保留的前沿研究。"
    },
    {
        "index": "#17",
        "title": "Verification-Aware Planning for Multi-Agent Systems",
        "link": "/arxiv/2510.17109",
        "arxiv_id": "2510.17109",
        "authors": "Tianyang Xu, Dan Zhang, Kushan Mitra, Estevam Hruschka",
        "summary": "Large language model (LLM) agents are increasingly deployed to tackle complex tasks, often necessitating collaboration among multiple specialized agents. However, multi-agent collaboration introduces new challenges in planning, coordination, and verification. Execution failures frequently arise not from flawed reasoning alone, but from subtle misalignments in task interpretation, output format, or inter-agent handoffs. To address these challenges, we present VeriMAP, a framework for multi-agent collaboration with verification-aware planning. The VeriMAP planner decomposes tasks, models subtask dependencies, and encodes planner-defined passing criteria as subtask verification functions (VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets, demonstrating that it outperforms both single- and multi-agent baselines while enhancing system robustness and interpretability. Our analysis highlights how verification-aware planning enables reliable coordination and iterative refinement in multi-agent systems, without relying on external labels or annotations.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Multiagent Systems",
        "date": "2025-10-20",
        "category": "cs.MA",
        "crawl_time": "2025-10-21T11:00:04.165132",
        "filter_reason": "这篇论文完全符合您的筛选标准，应被保留。我的判断过程如下： **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为 VeriMAP 的新**框架**，用于改进**多智能体系统**的协作。其核心贡献是方法论层面的创新，即“带验证的规划”，而不是将现有智能体作为工具应用到一个特定领域。这直接命中了您研究范围中的“多智能体”方向。 **第二步：正面指标** - 该论文高度匹配您的核心关注点： - **核心范式**: 明确涉及 `Multi-Agent Systems (MAS)`。 - **智能体能力**: 核心贡献是关于 `Planning`，并且通过验证函数实现了 `Self-Correction` / `Self-Refine` 的迭代优化效果。 - **多智能体**: 论文的核心是解决多智能体间的 `Collaboration`（协作）和 `Coordination`（协调）问题。 - **演化机制**: 摘要中提到的 \"iterative refinement\"（迭代优化）表明该框架具备自我完善的循环机制。 **第三步：排除标准** - **安全与对齐**: 论文虽然提到了 \"enhancing ... interpretability\"（增强可解释性），但这只是其框架带来的一个积极副作用，而非论文的主要研究贡献。论文的核心是验证感知的规划框架，因此不触发排除规则。 - **多模态与视觉**: 论文未涉及视觉或多模态内容。 **第四步：处理特殊和模糊情况** - **推理/规划**: 该论文的研究内容属于“保留”范畴。它研究的是智能体（特别是多智能体）如何进行更可靠的规划，而不是仅仅提升LLM本身的基础数学或逻辑推理能力。`ReAct`、`ToT` 等方法与本文提出的 VeriMAP 在研究范式上是一致的，都属于 Agentic AI 的规划范畴。 **第五步：最终决策** - 综合分析，这篇论文的核心贡献是**构建和改进多智能体系统**。它提出了一个创新的规划框架（VeriMAP）来解决多智能体协作中的关键挑战（如协调失败），并引入了验证机制来提升系统的鲁棒性和迭代优化能力。这与您的研究课题“LLM智能体及其演化”中的“多智能体”方向高度契合，且不触及任何排除标准。因此，最终决策为 **保留**。"
    },
    {
        "index": "#21",
        "title": "Personalized Collaborative Learning with Affinity-Based Variance Reduction",
        "link": "/arxiv/2510.16232",
        "arxiv_id": "2510.16232",
        "authors": "Chenyu Zhang, Navid Azizan",
        "summary": "Multi-agent learning faces a fundamental tension: leveraging distributed collaboration without sacrificing the personalization needed for diverse agents. This tension intensifies when aiming for full personalization while adapting to unknown heterogeneity levels -- gaining collaborative speedup when agents are similar, without performance degradation when they are different. Embracing the challenge, we propose personalized collaborative learning (PCL), a novel framework for heterogeneous agents to collaboratively learn personalized solutions with seamless adaptivity. Through carefully designed bias correction and importance correction mechanisms, our method AffPCL robustly handles both environment and objective heterogeneity. We prove that AffPCL reduces sample complexity over independent learning by a factor of $\\max\\{n^{-1}, \\delta\\}$, where $n$ is the number of agents and $\\delta\\in[0,1]$ measures their heterogeneity. This affinity-based acceleration automatically interpolates between the linear speedup of federated learning in homogeneous settings and the baseline of independent learning, without requiring prior knowledge of the system. Our analysis further reveals that an agent may obtain linear speedup even by collaborating with arbitrarily dissimilar agents, unveiling new insights into personalization and collaboration in the high heterogeneity regime.",
        "subjects": "Machine Learning, Machine Learning, Multiagent Systems, Systems and Control",
        "date": "2025-10-17",
        "category": "cs.MA",
        "crawl_time": "2025-10-21T11:00:04.167254",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。判断依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献是提出了一种名为“个性化协作学习”的新框架（PCL），旨在解决异构多智能体系统中的协作与个性化问题。这完全符合筛选标准中的“保留”条件，即“论文的核心是关于构建...多智能体系统的方法论或新框架”。它不是将已有框架应用于特定领域，而是提出了一个新的协作学习范式。 2.  **正面指标 (第二步):** 论文包含多个核心关注点。 *   **核心范式:** 明确涉及 `Multi-Agent Systems (MAS)`。 *   **多智能体:** 核心内容围绕 `Collaboration`（协作），并深入探讨了 `Heterogeneity`（异构性）这一多智能体系统中的关键挑战。 *   这些关键词和概念都与我的研究焦点“多智能体”高度吻合。 3.  **排除标准 (第三步):** 论文内容不涉及任何安全与对齐（如Safety, Alignment）或多模态（如Vision, MLLMs）等排除领域。其焦点纯粹在于智能体间的学习机制。 4.  **处理模糊情况 (第四步):** *   一个潜在的模糊点是，摘要中并未明确提及“LLM”。然而，在当前AI研究的前沿背景下，一个关于“多智能体协作学习”的基础性框架，其方法论和思想对于构建和演化**LLM智能体社会**具有直接的、至关重要的价值。我的研究目标是“构建、改进或演化LLM智能体”，而一个能解决异构智能体个性化协作问题的框架，正是构建高级LLM多智能体系统的核心技术之一。作为顶尖研究员，我关注的是能够赋能LLM智能体的基础性方法论，而非仅仅寻找标题中包含“LLM”的论文。 *   该论文的贡献是系统层面的方法论创新，而非简单的应用。它解决的是“智能体如何更好地协作”这一核心Agentic问题，这与我的研究目标完全一致。 **最终决策 (第五步):** 综合分析，该论文的核心贡献在于提出了一种创新的多智能体协作框架，直接命中了我的研究焦点之一“多智能体”。尽管没有明确提及LLM，但其提出的方法论对于构建和演化复杂的LLM智能体系统具有基础性和前瞻性的重要意义。因此，这篇论文高度相关，应该被保留。"
    },
    {
        "index": "#22",
        "title": "Agentic AI for Ultra-Modern Networks: Multi-Agent Framework for RAN Autonomy and Assurance",
        "link": "/arxiv/2510.16144",
        "arxiv_id": "2510.16144",
        "authors": "Sukhdeep Singh, Avinash Bhat, Shweta M, Subhash K Singh, Moonki Hong, Madhan Raj K, Kandeepan Sithamparanathan, Sunder A. Khowaja, Kapal Dev",
        "summary": "The increasing complexity of Beyond 5G and 6G networks necessitates new paradigms for autonomy and assur- ance. Traditional O-RAN control loops rely heavily on RIC- based orchestration, which centralizes intelligence and exposes the system to risks such as policy conflicts, data drift, and unsafe actions under unforeseen conditions. In this work, we argue that the future of autonomous networks lies in a multi-agentic architecture, where specialized agents collaborate to perform data collection, model training, prediction, policy generation, verification, deployment, and assurance. By replacing tightly- coupled centralized RIC-based workflows with distributed agents, the framework achieves autonomy, resilience, explainability, and system-wide safety. To substantiate this vision, we design and evaluate a traffic steering use case under surge and drift conditions. Results across four KPIs: RRC connected users, IP throughput, PRB utilization, and SINR, demonstrate that a naive predictor-driven deployment improves local KPIs but destabilizes neighbors, whereas the agentic system blocks unsafe policies, preserving global network health. This study highlights multi- agent architectures as a credible foundation for trustworthy AI- driven autonomy in next-generation RANs.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-17",
        "category": "cs.MA",
        "crawl_time": "2025-10-21T11:00:04.167931",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——保留** - 论文的核心贡献并非简单地将一个已有的智能体框架应用于网络领域，而是**提出并设计了一个全新的、用于实现RAN（无线接入网）自主性的多智能体架构**。论文明确指出，其核心论点是“the future of autonomous networks lies in a multi-agentic architecture”，并详细描述了该架构中专门化智能体的分工与协作。这完全符合“保留”标准中“核心是关于构建……多智能体系统（Multi-Agent Systems）的方法论或新框架”的定义。 - 它不属于“非演化型应用”，因为研究的焦点是智能体架构本身的设计、原理和优势，而不是网络KPI的优化结果。网络场景是用来验证其架构有效性的试验场。 2.  **第二步：正面指标——高度相关** - 论文标题和摘要中明确包含了我的核心关注点：`Agentic AI` 和 `Multi-Agent Systems (MAS)`。 - 摘要中提到了智能体的关键能力：`Collaboration`（“specialized agents collaborate”），以及隐含的`Planning`（“policy generation, verification, deployment”）和`Self-Correction`（“blocks unsafe policies, preserving global network health”）。这些都表明论文深入探讨了智能体的核心机制。 3.  **第三步：排除标准——不触发** - 论文提到了 `Safety` 和 `Explainability`，但它们是作为该多智能体架构所实现的**结果或优势**（“achieves……explainability, and system-wide safety”）来论述的，而非论文的主要研究贡献。论文的核心是“如何通过多智能体架构实现自主性”，而不是“如何为智能体设计一种新的安全对齐方法”。因此，这不触发“安全与对齐”的排除标准。 4.  **第四步：处理特殊和模糊情况——不适用** - 该论文属于多智能体方向，不涉及自我演化的例外情况。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于**构建和设计一个用于复杂系统（RAN）自主控制的多智能体框架**。它探讨了智能体间的协作、分工以及如何通过分布式架构实现比传统中心化方法更优的自主性和安全性。这完全契合我研究课题中的“多智能体”方向，因此应被筛选为**True**。"
    },
    {
        "index": "#20",
        "title": "Ripple Effect Protocol: Coordinating Agent Populations",
        "link": "/arxiv/2510.16572",
        "arxiv_id": "2510.16572",
        "authors": "Ayush Chopra, Aman Sharma, Feroz Ahmad, Luca Muscariello, Vijoy Pandey, Ramesh Raskar",
        "summary": "Modern AI agents can exchange messages using protocols such as A2A and ACP, yet these mechanisms emphasize communication over coordination. As agent populations grow, this limitation produces brittle collective behavior, where individually smart agents converge on poor group outcomes. We introduce the Ripple Effect Protocol (REP), a coordination protocol in which agents share not only their decisions but also lightweight sensitivities - signals expressing how their choices would change if key environmental variables shifted. These sensitivities ripple through local networks, enabling groups to align faster and more stably than with agent-centric communication alone. We formalize REP's protocol specification, separating required message schemas from optional aggregation rules, and evaluate it across scenarios with varying incentives and network topologies. Benchmarks across three domains: (i) supply chain cascades (Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling), and (iii) sustainable resource allocation (Fishbanks) show that REP improves coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly handling multimodal sensitivity signals from LLMs. By making coordination a protocol-level capability, REP provides scalable infrastructure for the emerging Internet of Agents",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-18",
        "category": "cs.MA",
        "crawl_time": "2025-10-21T11:00:04.166761",
        "filter_reason": "这篇论文完全符合您的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断 (保留)** - 论文的本质是提出一种名为“涟漪效应协议”的新方法。其核心贡献不是将现有智能体应用于某个领域，而是**构建了一个用于改进多智能体系统协作能力的协议（方法论/框架）**。论文明确指出，现有协议（如A2A）强调“通信”而非“协调”，而REP正是为了解决这个问题。这完全符合“构建、改进或演化LLM智能体”的核心目标。 - 它不属于“非演化型应用”，因为其创新点在于协议本身，而非在供应链等领域的应用。 - 它不属于“非Agentic的推理”，因为它关注的是智能体群体间的集体行为和对齐。 - 虽然提到了“基础设施”，但这里指的是为“智能体互联网”提供的一种**逻辑层面的协议能力**，而非模型部署、硬件加速等底层技术设施。因此，它应被视为一种方法论贡献。 2.  **第二步：正面指标 (高度相关)** - 论文包含了多个核心关注点： - **多智能体**: 这是论文最核心的范畴，标题和摘要反复提及“Agent Populations”、“Collective Behavior”、“Group Outcomes”、“Local Networks”。 - **协作**: 论文的中心主题就是“Coordination”（协调），旨在让智能体群体“align faster and more stably”。 - **通信**: 论文与现有通信协议（A2A, ACP）进行对比，并提出了改进方案。 - 论文还明确提到了 `LLM-based Agents`，表明其研究是基于LLM的智能体。 3.  **第三步：排除标准 (未触发)** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态模型本身。虽然提到了“multimodal sensitivity signals”，但这指的是信号内容的多样性（如对价格、需求等不同环境变量的敏感度），而非视觉或音频等多模态数据，因此不触犯排除规则。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 本文不涉及单个智能体的推理或规划，而是聚焦于多个智能体间的协调机制，这本身就是您研究焦点（多智能体）的一部分。 - **自我演化的应用**: 本文不直接涉及自我演化，但其提出的协议可以被看作是智能体群体适应环境、达成更优集体行为的演化性一步。 **最终决策**: 论文的核心贡献是提出一种新的多智能体协调协议（REP），以解决大规模智能体群体中的协作效率问题。这直接命中了您研究课题中的**“多智能体”**方向，特别是**“协作”**和**“通信”**子方向。它提供了一种构建更强大、更具可扩展性的多智能体系统的新方法，因此与您的研究目标高度契合。"
    },
    {
        "index": "#1",
        "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics",
        "link": "/arxiv/2510.17797",
        "arxiv_id": "2510.17797",
        "authors": "Akshara Prabhakar, Roshan Ram, Zixiang Chen, Silvio Savarese, Frank Wang, Caiming Xiong, Huan Wang, Weiran Yao",
        "summary": "As information grows exponentially, enterprises face increasing pressure to transform unstructured data into coherent, actionable insights. While autonomous agents show promise, they often struggle with domain-specific nuances, intent alignment, and enterprise integration. We present Enterprise Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning Agent for adaptive query decomposition, (2) four specialized search agents (General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a Visualization Agent for data-driven insights, and (5) a reflection mechanism that detects knowledge gaps and updates research direction with optional human-in-the-loop steering guidance. These components enable automated report generation, real-time streaming, and seamless enterprise deployment, as validated on internal datasets. On open-ended benchmarks including DeepResearch Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without any human steering. We release the EDR framework and benchmark trajectories to advance research on multi-agent reasoning applications. Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and Dataset at https://huggingface.co/datasets/Salesforce/EDR-200",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-20",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:05.846094",
        "filter_reason": "该论文完全符合你的筛选标准，应被保留。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是构建了一个名为EDR（Enterprise Deep Research）的新型多智能体系统框架。它并非简单地将现有智能体应用于企业领域，而是提出了一个包含“主规划智能体”、“专业化搜索智能体”、“可视化智能体”和“反思机制”的全新架构。这直接命中了你“构建、改进或演化LLM智能体”的核心目标。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标——高度相关** 论文摘要中包含了大量你的核心关注点： *   **核心范式**: 明确提出了 `Multi-Agent System`。 *   **智能体能力**: 包含了 `Planning` (Master Planning Agent for adaptive query decomposition)、`Tool Use` (extensible MCP-based tool ecosystem) 和 `Self-Reflection` (a reflection mechanism that detects knowledge gaps and updates research direction)。 *   **多智能体**: 系统由多个专业化智能体（Master, Search, Visualization）协作构成，体现了 `Collaboration` 和 `Agent Society` 的思想。 这些指标表明，论文的研究内容与你的三个核心方向（单智能体、多智能体、自我演化）都紧密相关。 3.  **第三步：排除标准——未触发** 论文的主要贡献是系统架构和推理机制，而非安全、对齐或多模态技术。虽然它处理数据（可能包括文本），但其研究核心是智能体的决策流程和协作方式，而不是视觉或多模态模型本身。因此，它避开了所有主要的排除项。 4.  **第四步：处理特殊和模糊情况——符合保留条件** *   **推理/规划**: 论文中的“Master Planning Agent”负责自适应查询分解，这属于智能体在复杂任务中进行多步规划和推理的范畴，符合保留条件。 *   **自我演化的应用**: 论文提出的“反思机制”能够检测知识空白并更新研究方向，这是一种在单次任务执行中的自我完善和迭代，非常符合“自我演化”的研究精神。尽管它被应用于企业分析这一特定领域，但根据筛选规则第四条的第二点，由于其核心是提出一种新的“自我演化”机制，因此应当被保留。 **最终决策**: 这篇论文的核心是提出一个新的多智能体协作框架，该框架集成了规划、工具使用和自我反思等关键能力。它虽然以企业分析为应用场景，但其贡献在于方法论本身，即如何设计和构建一个更强大、更可控的LLM智能体系统。这完全符合你关于“LLM智能体及其演化”的研究课题，尤其是多智能体和单智能体能力方向。因此，应判定为 **True**。"
    },
    {
        "index": "#20",
        "title": "Deep Self-Evolving Reasoning",
        "link": "/arxiv/2510.17498",
        "arxiv_id": "2510.17498",
        "authors": "Zihan Liu, Shun Zheng, Xumeng Wen, Yang Wang, Jiang Bian, Mao Yang",
        "summary": "Long-form chain-of-thought reasoning has become a cornerstone of advanced reasoning in large language models. While recent verification-refinement frameworks have enabled proprietary models to solve Olympiad-level problems, their effectiveness hinges on strong, reliable verification and correction capabilities, which remain fragile in open-weight, smaller-scale models. This work demonstrates that even with weak verification and refinement capabilities on hard tasks, the reasoning limits of such models can be substantially extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning (DSER). We conceptualize iterative reasoning as a Markov chain, where each step represents a stochastic transition in the solution space. The key insight is that convergence to a correct solution is guaranteed as long as the probability of improvement marginally exceeds that of degradation. By running multiple long-horizon, self-evolving processes in parallel, DSER amplifies these small positive tendencies, enabling the model to asymptotically approach correct answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously unsolvable problems and boosts overall performance, enabling this compact model to surpass the single-turn accuracy of its 600B-parameter teacher through majority voting. Beyond its immediate utility for test-time scaling, the DSER framework serves to diagnose the fundamental limitations of current open-weight reasoners. By clearly delineating their shortcomings in self-verification, refinement, and stability, our findings establish a clear research agenda for developing next-generation models with powerful, intrinsic self-evolving capabilities.",
        "subjects": "Computation and Language",
        "date": "2025-10-20",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:05.866747",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献与您的“自我演化”方向高度契合。 1.  **第一步：核心判断** - **核心贡献**: 论文的核心是提出了一种名为“深度自我演化推理”的新范式/框架。这个框架的核心思想不是简单地应用LLM，而是构建一个能让LLM通过迭代过程进行自我完善的机制。作者将迭代推理过程概念化为一个马尔可夫链，只要改进的概率略大于退化的概率，就能保证收敛到正确答案。这本质上是一种**自我演化**的方法论。 - **排除项分析**: - **非演化型应用**: 论文虽然应用在数学推理（AIME）上，但其核心贡献是DSER这个**自我演化机制本身**，而不是解决数学问题的应用结果。这完全符合第四步中“自我演化的应用”的例外情况。 - **非Agentic的推理**: 这篇论文超越了简单的CoT变体。它不是在优化单次预测的Token级别准确性，而是构建了一个包含长期迭代、自我修正和概率性收敛的完整**推理过程框架**。这符合第四步中“保留”的条件，即“关于智能体如何进行规划或在复杂任务中进行多步推理（如 ReAct、ToT 或新的Agentic框架）”。DSER正是一种新的、以演化为核心的Agentic推理框架。 - **基础设施**: 论文不涉及模型基础设施或部署优化。 2.  **第二步：正面指标** - 论文摘要中明确包含了多个核心关注点的关键词，如：`Self-Evolving` (标题和摘要中反复出现), `Self-Improvement` (隐含在improvement和convergence的概念中), `Iterative Improvement` (DSER的核心机制), `Self-Verification`, `Refinement`。这些指标强烈表明论文与您的“自我演化”研究方向直接相关。 3.  **第三步：排除标准** - 论文的主要贡献不涉及`Safety`、`Alignment`、`Hallucination`等安全与对齐问题。 - 论文是纯文本推理，不涉及`Vision`、`MLLMs`等多模态内容。 4.  **第四步：特殊和模糊情况处理** - **推理/规划**: 如第一步所述，这篇论文虽然主题是“推理”，但它提出的是一个结构化的、迭代的、具有自我修正能力的Agentic框架，而非简单的提示技巧。因此应该保留。 - **自我演化的应用**: 论文的核心是提出一种新的“自我演化”机制，即使它被应用在数学领域，根据您的规则也应该保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献是构建了一个名为DSER的**自我演化框架**，用于提升LLM在复杂任务中的推理能力。其本质是关于智能体如何通过迭代和反馈进行自我完善，这精准地命中了您的研究焦点“自我演化”。因此，这篇论文应被**保留（True）**。"
    },
    {
        "index": "#60",
        "title": "FinSight: Towards Real-World Financial Deep Research",
        "link": "/arxiv/2510.16844",
        "arxiv_id": "2510.16844",
        "authors": "Jiajie Jin, Yuyao Zhang, Yimeng Xu, Hongjin Qian, Yutao Zhu, Zhicheng Dou",
        "summary": "Generating professional financial reports is a labor-intensive and intellectually demanding process that current AI systems struggle to fully automate. To address this challenge, we introduce FinSight (Financial InSight), a novel multi agent framework for producing high-quality, multimodal financial reports. The foundation of FinSight is the Code Agent with Variable Memory (CAVM) architecture, which unifies external data, designed tools, and agents into a programmable variable space, enabling flexible data collection, analysis and report generation through executable code. To ensure professional-grade visualization, we propose an Iterative Vision-Enhanced Mechanism that progressively refines raw visual outputs into polished financial charts. Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis segments into coherent, citation-aware, and multimodal reports, ensuring both analytical depth and structural consistency. Experiments on various company and industry-level tasks demonstrate that FinSight significantly outperforms all baselines, including leading deep research systems in terms of factual accuracy, analytical depth, and presentation quality, demonstrating a clear path toward generating reports that approach human-expert quality.",
        "subjects": "Computation and Language, Artificial Intelligence, Computational Engineering, Finance, and Science",
        "date": "2025-10-19",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:05.916148",
        "filter_reason": "这篇论文符合你的筛选标准，应该被保留。以下是我的详细判断过程： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出一个名为 \"FinSight\" 的**新颖的多智能体框架**，用于生成复杂的金融研究报告。摘要明确指出这是一个 \"novel multi agent framework\"，并详细介绍了其内部架构，如 \"Code Agent with Variable Memory (CAVM) architecture\" 和 \"Iterative Vision-Enhanced Mechanism\"。这表明论文的本质是**构建和改进LLM智能体系统**，而不是简单地将现有框架应用于金融领域。因此，它通过了第一步的核心判断。 2.  **第二步：正面指标——高度相关** 论文包含了多个你的核心关注点，相关性极高： *   **核心范式**: 明确提到了 `Multi-Agent Systems`。 *   **智能体能力**: CAVM架构展示了 `Tool Use`（通过可执行代码调用工具和数据）和 `Memory`（可变内存空间）。迭代视觉增强机制是一种 `Self-Refine` 或 `Iterative Improvement` 的体现。整个框架的运行过程涉及到复杂的 `Planning`。 *   **多智能体**: 论文标题和摘要都强调这是一个 `Multi-Agent` 框架，涉及多个智能体协同完成数据收集、分析、可视化和写作等任务。 这些正面指标有力地证明了论文与你的研究目标高度契合。 3.  **第三步：排除标准——不适用** 论文的主要内容不是关于安全、对齐或可解释性。虽然提到了 `Vision` 和 `Multimodal`，但它们是作为智能体生成报告的**工具和输出形式**（通过迭代机制优化图表），而不是研究的核心（例如，不是提出一个新的视觉模型或多模态融合理论）。根据你的规则，这种情况不应被排除。 4.  **第四步：处理特殊和模糊情况——符合保留规则** *   **推理/规划**: 论文描述了一个完整的多智能体工作流，用于解决一个复杂的多步骤任务（数据分析 -> 可视化 -> 报告撰写）。这完全符合“智能体如何进行规划或在复杂任务中进行多步推理”的保留标准。 *   **自我演化的应用**: 论文中的 \"Iterative Vision-Enhanced Mechanism\" 是一个典型的迭代改进过程，属于自我演化的子范畴（Self-Refine）。更重要的是，如第一步所述，论文的核心是提出一个新的Multi-Agent框架，即使在金融领域应用，其方法论贡献依然是主要的。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心是构建一个具有工具使用、记忆和迭代改进能力的新型多智能体框架。尽管其应用场景是金融领域，但其贡献在于方法论本身，直接命中了你研究的第二个方向“多智能体”以及第一个方向的多个子方向（工具使用、记忆、规划）。因此，这篇论文是极具价值的前沿研究，完全符合你的筛选要求。"
    },
    {
        "index": "#66",
        "title": "Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games",
        "link": "/arxiv/2510.16761",
        "arxiv_id": "2510.16761",
        "authors": "Yikai Zhang, Ye Rong, Siyu Yuan, Jiangjie Chen, Jian Xie, Yanghua Xiao",
        "summary": "Existing language agents often encounter difficulties in dynamic adversarial games due to poor strategic reasoning. To mitigate this limitation, a promising approach is to allow agents to learn from game interactions automatically, without relying on costly expert-labeled data. Unlike static environments where agents receive fixed feedback or rewards, selecting appropriate opponents in dynamic adversarial games can significantly impact learning performance. However, the discussion of opponents in adversarial environments remains an area under exploration. In this paper, we propose a Step-level poliCy Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we conduct a detailed analysis of opponent selection by setting opponents at different levels and find that self-play is the most effective way to improve strategic reasoning in such adversarial environments. Utilizing SCO-PAL with self-play, we increase the average win rate against four opponents by approximately 30% compared to baselines and achieve a 54.76% win rate against GPT-4 in six adversarial games.",
        "subjects": "Computation and Language",
        "date": "2025-10-19",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:05.924658",
        "filter_reason": "这篇论文完全符合您的研究范围，是一篇关于LLM智能体自我演化的高质量研究。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一种新的方法论（SCO-PAL），用于**改进和演化**语言智能体。其核心贡献不是将现有智能体应用于某个领域，而是**构建了一个让智能体通过与环境和自身博弈进行自我完善的学习框架**。这直接命中了您“构建、改进或演化LLM智能体”的核心目标。 2.  **第二步：正面指标分析** - 论文命中了多个核心关注点： - **自我演化**: 这是论文最核心的标签。论文明确指出智能体可以“从游戏交互中自动学习”，而“自我博弈”是实现这种自我演化、自我改进的关键机制。 - **多智能体**: “对抗性游戏”和“对手选择”天然属于多智能体系统研究的范畴。论文探讨了智能体与不同水平对手（包括自身）的互动，这正是多智能体博弈的体现。 - **规划**: 论文旨在提升智能体的“策略推理”能力，这在动态环境中是高级规划和决策能力的体现，远超简单的单步推理。 - **自我改进**: 通过自我博弈不断迭代优化策略，是典型的自我改进过程。 3.  **第三步：排除标准分析** - 论文完全不涉及任何排除标准。其研究焦点是智能体的能力和演化机制，而不是安全、对齐或可解释性。同时，论文也未涉及多模态技术，其核心是基于语言的智能体。 4.  **第四步：特殊和模糊情况分析** - **推理/规划**: 论文研究的是智能体在动态对抗环境中的**策略推理**，这属于Agentic框架下的高级规划能力，完全符合保留条件，而非提升LLM基础推理能力。 - **自我演化的应用**: 这篇论文是“自我演化应用”的完美范例。它的应用领域是“对抗性游戏”，但其**核心贡献是提出了一种名为SCO-PAL的新型自我演化机制**。根据您的筛选规则，这种提出新演化机制的论文应该被保留，即使它被应用在特定领域。 **总结**: 该论文的核心贡献是提出了一种通过自我博弈机制来优化语言智能体策略推理能力的新方法（SCO-PAL）。这直接对应您研究课题中的 **“自我演化”** 方向，并深度融合了 **“多智能体”** (对抗博弈) 和 **“单智能体”** (策略规划) 的关键要素。它不是简单的应用，而是对智能体如何学习和进化这一根本问题的探索。因此，这篇论文与您的研究范围高度契合，应予以保留。"
    },
    {
        "index": "#80",
        "title": "Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection",
        "link": "/arxiv/2510.16499",
        "arxiv_id": "2510.16499",
        "authors": "Michelle Yuan, Khushbu Pahwa, Shuaichen Chang, Mustafa Kaba, Jiarong Jiang, Xiaofei Ma, Yi Zhang, Monica Sunkara",
        "summary": "Designing effective agentic systems requires the seamless composition and integration of agents, tools, and models within dynamic and uncertain environments. Most existing methods rely on static, semantic retrieval approaches for tool or agent discovery. However, effective reuse and composition of existing components remain challenging due to incomplete capability descriptions and the limitations of retrieval methods. Component selection suffers because the decisions are not based on capability, cost, and real-time utility. To address these challenges, we introduce a structured, automated framework for agentic system composition that is inspired by the knapsack problem. Our framework enables a composer agent to systematically identify, select, and assemble an optimal set of agentic components by jointly considering performance, budget constraints, and compatibility. By dynamically testing candidate components and modeling their utility in real-time, our approach streamlines the assembly of agentic systems and facilitates scalable reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five benchmarking datasets shows that our online-knapsack-based composer consistently lies on the Pareto frontier, achieving higher success rates at significantly lower component costs compared to our baselines. In the single-agent setup, the online knapsack composer shows a success rate improvement of up to 31.6% in comparison to the retrieval baselines. In multi-agent systems, the online knapsack composer increases success rate from 37% to 87% when agents are selected from an agent inventory of 100+ agents. The substantial performance gap confirms the robust adaptability of our method across diverse domains and budget constraints.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-18",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:05.942448",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断依据如下： 1.  **第一步：核心判断** 论文的核心贡献是提出一个 **“用于智能体系统组合的自动化框架”**。它不是将现有的智能体作为工具去解决某个特定领域的问题，而是研究 **如何更有效地构建（构建、改进）** 一个由多个智能体、工具和模型组成的复杂系统。这直接命中了您筛选标准中的“保留”条件：核心贡献在于构建或改进LLM智能体的方法论或新框架。它解决的是智能体工程中的一个根本性问题：如何选择和组合组件。 2.  **第二步：正面指标** 论文与您的核心关注点高度契合： *   **核心范式**: 论文标题和摘要中明确提到了 `Agentic Component Selection`、`agentic system composition`，并讨论了 `single-agent setup` 和 `multi-agent systems`，完全符合 `Agentic AI` 和 `Multi-Agent Systems (MAS)` 的范式。 *   **智能体能力**: 该框架的核心是让一个“composer agent”去选择和组装其他组件，这本身就是一种高级的规划（`Planning`）和工具使用（`Tool Use`）能力。 *   **多智能体**: 论文明确在多智能体场景下进行了评估，研究如何从超过100个智能体的库存中选择最优的智能体组合，这直接对应了您关注的“智能体间的协作”。 3.  **第三步：排除标准** 该论文完全不涉及您指定的排除标准。其研究焦点在于提升智能体系统的性能、成本效益和组合效率，而非安全、对齐或可解释性。同时，它也没有涉及多模态或视觉内容，除非这些内容是作为被选择的“工具”之一，但研究的核心是选择机制本身。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 该论文提出的框架可以被看作是一种元规划。它不是在解决一个具体的下游任务，而是在规划“如何构建一个能够解决任务的智能体系统”。这完全符合“关于智能体如何进行规划或在复杂任务中进行多步推理”的保留条件。 *   **自我演化**: 虽然论文的标题没有直接使用“自我演化”，但其“动态测试候选组件并实时建模其效用”的机制，体现了系统根据环境反馈进行动态调整和优化的思想，这与演化的精神内核相通。即便不严格归类为自我演化，其核心的“构建和改进智能体”的贡献也足以使其被保留。 **最终决策**: 这篇论文的核心是提出一种创新的、系统化的方法论来自动构建LLM智能体系统。它直接回应了您研究课题中“构建、改进LLM智能体”的核心目标，并横跨了单智能体（工具选择）和多智能体（智能体选择）两个关键方向。因此，这是一篇高度相关且应被保留的前沿论文。"
    },
    {
        "index": "#105",
        "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
        "link": "/arxiv/2510.17790",
        "arxiv_id": "2510.17790",
        "authors": "Yuhao Yang, Zhen Yang, Zi-Yi Dou, Anh Nguyen, Keen You, Omar Attia, Andrew Szot, Michael Feng, Ram Ramrakhya, Alexander Toshev, Chao Huang, Yinfei Yang, Zhe Gan",
        "summary": "Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid action -- seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing over 17,000 verifiable tasks spanning real-world computer-use scenarios; (3) a large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language",
        "date": "2025-10-20",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:05.981898",
        "filter_reason": "这篇论文完全符合您的筛选标准，应该被保留。我的判断过程如下： **第一步：核心判断——保留** 这篇论文的本质是**构建和改进一个全新的LLM智能体**。它提出了一个名为“UltraCUA”的基础模型，其核心贡献是引入了“混合动作”机制。这并非将现有智能体作为工具去解决某个特定领域（如金融、医疗）的问题，而是直接针对智能体本身的能力进行创新和增强。因此，它通过了第一步的核心判断。 **第二步：正面指标——高度匹配** 论文内容与您的核心关注点高度契合： - **核心范式**: 论文明确研究“Computer Use Agents”，属于`LLM-based Agents`的范畴。 - **智能体能力**: 论文的核心创新点“Hybrid Action”（混合动作）直接解决了`Tool Use / Tool Augmentation`（工具使用）这一关键能力。它通过让智能体能够策略性地在低级GUI操作（点击、滚动）和高级程序化工具调用（API）之间进行切换，极大地提升了智能体的效率和鲁棒性。这可以被看作是一种高级的规划和决策能力。 **第三步：排除标准——未触及** - **安全与对齐**: 论文全文未提及`Safety`, `Alignment`, `Hallucination`等，其目标是提升智能体的任务执行性能和效率。 - **多模态与视觉**: 摘要中提到了“Multimodal agents”和“visual grounding”，但这并未成为排除的理由。根据您的特殊规则，视觉在这里是智能体感知和操作计算机环境的**工具**，而不是论文研究的核心。论文的核心贡献是**动作机制**的创新，而非视觉模型本身。它恰恰是为了解决纯视觉依赖带来的问题而提出的。 **第四步：处理特殊和模糊情况——符合保留条件** - **推理/规划**: 论文中的“strategic alternation between low-level and high-level actions”（在低级和高级动作之间进行策略性切换）本质上是一种智能体在复杂任务中的决策和规划过程。它不是在提升LLM的基础数学或逻辑推理能力，而是在构建一个能让智能体在真实环境中更有效行动的框架，这完全符合“保留”的条件。 **第五步：最终决策** 综合以上分析，论文《UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action》的核心贡献是提出了一种构建和改进LLM智能体的新方法。它通过创新的“混合动作”机制，显著增强了智能体的工具使用和规划决策能力，直接命中了您研究课题中“单智能体”方向下的“工具使用”和“规划”子方向。因此，这篇论文与您的研究范围高度相关，应予以保留。"
    },
    {
        "index": "#101",
        "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs",
        "link": "/arxiv/2510.16062",
        "arxiv_id": "2510.16062",
        "authors": "Guiyao Tie, Zenghui Yuan, Zeli Zhao, Chaoran Hu, Tianhe Gu, Ruihang Zhang, Sizhe Zhang, Junran Wu, Xiaoyue Tu, Ming Jin, Qingsong Wen, Lixing Chen, Pan Zhou, Lichao Sun",
        "summary": "Self-correction of large language models (LLMs) emerges as a critical component for enhancing their reasoning performance. Although various self-correction methods have been proposed, a comprehensive evaluation of these methods remains largely unexplored, and the question of whether LLMs can truly correct themselves is a matter of significant interest and concern. In this study, we introduce CorrectBench, a benchmark developed to evaluate the effectiveness of self-correction strategies, including intrinsic, external, and fine-tuned approaches, across three tasks: commonsense reasoning, mathematical reasoning, and code generation. Our findings reveal that: 1) Self-correction methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing different self-correction strategies yields further improvements, though it reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited optimization under additional self-correction methods and have high time costs. Interestingly, a comparatively simple chain-of-thought (CoT) baseline demonstrates competitive accuracy and efficiency. These results underscore the potential of self-correction to enhance LLM's reasoning performance while highlighting the ongoing challenge of improving their efficiency. Consequently, we advocate for further research focused on optimizing the balance between reasoning capabilities and operational efficiency. Project Page: https://correctbench.github.io/",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:05.974361",
        "filter_reason": "这篇论文的核心贡献是构建了一个名为 CorrectBench 的基准测试（Benchmark），用于系统性评估和比较不同的LLM自我修正（Self-Correction）策略。我的判断过程如下： 1.  **第一步：核心判断** - **保留 (Keep)**。这篇论文的本质不是将LLM作为工具去解决某个外部领域的问题，也不是单纯提升LLM的基础推理能力。它的核心是**研究“自我修正”这一智能体关键能力的机制和效果**。自我修正（Self-Correction）与自我反思（Self-Reflection）和自我完善（Self-Improvement）紧密相关，是“自我演化”（Self-Evolving）方向的一个核心子集。论文通过构建基准来衡量不同自我修正方法论的优劣，这直接服务于“改进LLM智能体”的目标。 2.  **第二步：正面指标** - 论文高度符合我的核心关注点。摘要中明确提到了 `Self-Correction`，这与我筛选标准中的 `Self-Correction`、`Self-Reflection`、`Self-Improvement` 等演化机制直接对应。论文评估的内在（intrinsic）、外在（external）和微调（fine-tuned）自我修正方法，都是构建和改进智能体自我演化能力的具体技术路径。虽然它没有提出一个全新的智能体框架，但它为该领域的研究提供了关键的评估工具和深刻的洞见，这对于推动“自我演化”方向的发展至关重要。 3.  **第三步：排除标准** - 论文不涉及安全与对齐（Safety, Alignment等），也不以多模态或视觉为核心。它的焦点完全集中在LLM的推理和自我修正能力上，因此没有被排除。 4.  **第四步：处理特殊和模糊情况** - **推理/规划 (Reasoning/Planning)**：这篇论文虽然涉及推理任务（常识、数学、代码），但其重点**不是**提出一种新的CoT变体来提升LLM的基础推理能力。相反，它将推理任务作为**测试平台**，来评估“自我修正”这一**Agentic能力**的有效性。这完全符合“保留”的条件，即研究智能体如何在复杂任务中进行多步推理和自我完善。 - **自我演化的应用**：此条不适用，因为论文是方法论/基准研究，而非特定领域应用。 5.  **第五步：最终决策** - 综合来看，这篇论文的核心贡献在于对LLM智能体的“自我演化”能力（具体为自我修正）进行了系统性的评估和分析。它回答了“LLM能否有效自我修正”以及“哪种修正策略更优”等关键问题，为后续构建更强大的自我演化智能体提供了实证基础和方向指引。因此，它完全符合我关于“LLM智能体及其演化”的研究范围，特别是“自我演化”这一核心方向。"
    },
    {
        "index": "#100",
        "title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle",
        "link": "/arxiv/2510.16079",
        "arxiv_id": "2510.16079",
        "authors": "Rong Wu, Xiaoman Wang, Jianbiao Mei, Pinlong Cai, Daocheng Fu, Cheng Yang, Licheng Wen, Xuemeng Yang, Yufan Shen, Yuxin Wang, Botian Shi",
        "summary": "Current Large Language Model (LLM) agents show strong performance in tool use, but lack the crucial capability to systematically learn from their own experiences. While existing frameworks mainly focus on mitigating external knowledge gaps, they fail to address a more fundamental limitation: the inability to iteratively refine problem-solving strategies. In this work, we introduce EvolveR, a framework designed to enable agent to self-improve through a complete, closed-loop experience lifecycle. This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively retrieves distilled principles to guide its decision-making, accumulating a diverse set of behavioral trajectories. This loop employs a policy reinforcement mechanism to iteratively update the agent based on its performance. We demonstrate the effectiveness of EvolveR on complex multi-hop question-answering benchmarks, where it achieves superior performance over strong agentic baselines. Our work presents a comprehensive blueprint for agents that learn not only from external data but also from the consequences of their own actions, paving the way for more autonomous and continuously improving systems. Code is available at https://github.com/Edaizi/EvolveR.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:05.973729",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断 (保留)** - 论文的本质是**构建和改进LLM智能体**。其核心贡献是提出了一种名为**EvolveR的新框架**，该框架通过一个“经验驱动的生命周期”来实现智能体的**自我演化**和**持续完善**。这完全符合“核心贡献在于构建、改进或演化LLM智能体”的保留标准。 - 它不属于排除类型：它不是将已有智能体简单应用于特定领域（虽然用QA任务做验证，但核心是方法论）；它不是非Agentic的基础推理提升，而是关注智能体如何迭代优化其解决问题策略；它也不是基础设施研究。 2.  **第二步：正面指标 (高度相关)** - 论文标题和摘要中包含大量符合您核心关注点的关键词： - **核心范式**: `Self-Evolving`, `LLM-based Agents` - **演化机制**: `Self-Improvement`, `Iterative Improvement` - 摘要详细描述了其创新机制，如`Offline Self-Distillation`（离线自我蒸馏）和`Online Interaction`（在线交互），构成一个完整的闭环，这直接体现了“自我演化”的核心思想。 3.  **第三步：排除标准 (未触发)** - 论文的贡献重点在于智能体的演化能力，而非安全、对齐、可解释性或幻觉。 - 论文未涉及视觉或多模态内容。 4.  **第四步：处理特殊和模糊情况 (符合保留规则)** - 论文虽然在一个具体的“多跳问答”任务上进行了评估，但这属于**“自我演化的应用”**这一特殊情况。根据您的规则，**只要论文的核心是提出一种新的“自我演化”机制，即使它被应用在特定领域，也应该保留**。EvolveR框架本身是普适的，其价值在于方法论，而非应用领域。 **最终决策**: 该论文的核心贡献是提出一个让LLM智能体通过自身经验进行迭代学习和自我完善的新框架。这精准地命中了您研究焦点中的**“自我演化”**方向。论文不仅提出了理论框架，还设计了具体的实现机制（离线蒸馏与在线交互的闭环），是Agentic AI领域前沿且高度相关的工作。因此，最终判断为 **True**，应保留该论文。"
    },
    {
        "index": "#113",
        "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning",
        "link": "/arxiv/2510.17590",
        "arxiv_id": "2510.17590",
        "authors": "Mir Nafis Sharear Shopnil, Sharad Duwal, Abhishek Tyagi, Adiba Mahbub Proma",
        "summary": "Misinformation spreads across web platforms through billions of daily multimodal posts that combine text and images, overwhelming manual fact-checking capacity. Supervised detection models require domain-specific training data and fail to generalize across diverse manipulation tactics. We present MIRAGE, an inference-time, model-pluggable agentic framework that decomposes multimodal verification into four sequential modules: visual veracity assessment detects AI-generated images, cross-modal consistency analysis identifies out-of-context repurposing, retrieval-augmented factual checking grounds claims in web evidence through iterative question generation, and a calibrated judgment module integrates all signals. MIRAGE orchestrates vision-language model reasoning with targeted web retrieval, outputs structured and citation-linked rationales. On MMFakeBench validation set (1,000 samples), MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65 points while maintaining 34.3% false positive rate versus 97.3% for a judge-only baseline. Test set results (5,000 samples) confirm generalization with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97 points. Our results demonstrate that decomposed agentic reasoning with web retrieval can match supervised detector performance without domain-specific training, enabling misinformation detection across modalities where labeled data remains scarce.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Computers and Society, Machine Learning",
        "date": "2025-10-20",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:05.986203",
        "filter_reason": "这篇论文符合你的研究范围，应予以保留。以下是详细的判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为MIRAGE的**智能体框架**。其核心贡献并非简单地将LLM应用于虚假信息检测这一特定领域，而是构建了一套结构化的方法论，即如何让智能体通过规划、分解任务、使用工具（网络检索）和多步推理来解决复杂的多模态问题。摘要中明确指出其是一个“inference-time, model-pluggable agentic framework”，并详细描述了其四个顺序模块，这表明其核心是关于智能体的构建方法论，而非仅仅是应用。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: 论文标题和摘要中多次强调 `Agentic Framework`，完全命中。 - **智能体能力**: - `Planning`: 论文将复杂的验证任务“decomposes into four sequential modules”，这是典型的任务规划能力。 - `Tool Use / Tool Augmentation`: 框架明确集成了“targeted web retrieval”和“iterative question generation”，这是智能体使用外部工具进行信息获取和处理的体现。 - `ReAct`: 虽然未直接提及ReAct，但其“推理（视觉语言模型）+ 行动（网络检索）+ 整合判断”的流程与ReAct范式高度一致。 3.  **第三步：排除标准** - **安全与对齐**: 尽管论文的应用领域是“虚假信息检测”，与安全相关，但其**主要贡献**是提出一个通用的智能体推理框架，而不是一种新的安全算法、对齐理论或可解释性方法。论文的重点在于“如何构建智能体”来完成任务，而非任务本身的安全属性。因此，不适用此排除标准。 - **多模态与视觉**: 论文确实涉及“Multimodal”和“vision-language model”，但根据筛选规则，只要它们是**被用作智能体感知环境的工具，而不是研究的核心**，就不应排除。在本论文中，视觉模型是智能体框架中的一个模块（“visual veracity assessment”），用于完成特定子任务。研究的核心是这个智能体框架的架构和推理流程，而非改进视觉模型本身。因此，这符合例外情况，不应排除。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是“关于智能体如何进行规划或在复杂任务中进行多步推理”的**绝佳范例**。它通过任务分解和工具编排，展示了智能体超越简单问答的复杂推理能力。因此，完全符合保留条件。 5.  **第五步：最终决策** - 综合分析，这篇论文的核心贡献在于构建了一个具有规划和工具使用能力的单智能体框架。它虽然应用在特定领域，但其方法论具有通用性，并且完美地展示了Agentic AI的核心理念。因此，它完全符合你研究目标中的“单智能体”方向，应该被保留。"
    },
    {
        "index": "#125",
        "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents",
        "link": "/arxiv/2510.16907",
        "arxiv_id": "2510.16907",
        "authors": "Kangrui Wang, Pingyue Zhang, Zihan Wang, Yaning Gao, Linjie Li, Qineng Wang, Hanyang Chen, Chi Wan, Yiping Lu, Zhengyuan Yang, Lijuan Wang, Ranjay Krishna, Jiajun Wu, Li Fei-Fei, Yejin Choi, Manling Li",
        "summary": "A key challenge in training Vision-Language Model (VLM) agents, compared to Language Model (LLM) agents, lies in the shift from textual states to complex visual observations. This transition introduces partial observability and demands robust world modeling. We ask: Can VLM agents construct internal world models through explicit visual state reasoning? To address this question, we architecturally enforce and reward the agent's reasoning process via reinforcement learning (RL), formulating it as a Partially Observable Markov Decision Process (POMDP). We find that decomposing the agent's reasoning into State Estimation (\"what is the current state?\") and Transition Modeling (\"what comes next?\") is critical for success, as demonstrated through five reasoning strategies. Our investigation into how agents represent internal beliefs reveals that the optimal representation is task-dependent: Natural Language excels at capturing semantic relationships in general tasks, while Structured formats are indispensable for precise manipulation and control. Building on these insights, we design a World Modeling Reward that provides dense, turn-level supervision for accurate state prediction, and introduce Bi-Level General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment. Through this form of visual state reasoning, a 3B-parameter model achieves a score of 0.82 across five diverse agent benchmarks, representing a 3$\\times$ improvement over its untrained counterpart (0.21) and outperforming proprietary reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5 (0.62). All experiments are conducted within our VAGEN framework, a scalable system for training and analyzing multi-turn VLM agents in diverse visual environments. Code and data are publicly available at https://vagen-ai.github.io.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-19",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:06.003142",
        "filter_reason": "这篇论文完全符合你的研究范围，核心贡献在于构建和改进LLM智能体。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心是提出一个名为 **VAGEN** 的新框架，用于训练和改进 **VLM智能体**。它并非简单地将现有智能体应用于某个领域，而是专注于解决智能体在视觉环境中进行多轮推理的根本性挑战——即如何构建内部世界模型。论文提出的方法论（如将推理过程建模为POMDP、设计世界建模奖励、引入Bi-Level GAE算法）都是为了**构建和改进智能体本身的能力**，因此符合“核心贡献在于构建、改进或演化 LLM智能体”的保留标准。 2.  **第二步：正面指标——高度相关** 论文包含了多个你关注的核心指标： *   **核心范式**: 论文明确研究 `LLM-based Agents` (具体为VLM Agents)。 *   **智能体能力**: 论文的核心是智能体的 **`Planning`** 和 **`Reasoning`** 能力。它通过强化学习来强化智能体的“世界模型推理”，这直接关系到智能体在复杂环境中的规划和决策。`Multi-Turn` 的设定也暗示了对 **`Memory`** 的依赖，智能体需要记住历史观测来预测未来状态。通过RL进行训练和改进，也触及了 **`Self-Correction`** 和 **`Self-Improvement`** 的范畴。 3.  **第三步：排除标准——不适用** *   **安全与对齐**: 论文的主要贡献是提升智能体的推理性能，而非安全、对齐或可解释性，因此不在此排除范围内。 *   **多模态与视觉**: 这是最需要辨析的一点。虽然论文标题和摘要中都提到了 `VLM` (Vision-Language Model)，但它完全符合你设定的**例外情况**。论文的研究核心**不是**提出一个新的视觉模型或改进视觉感知技术，而是**将视觉作为智能体感知环境的工具**，研究智能体如何利用这些视觉信息进行**“世界模型推理”**。其贡献（VAGEN框架、世界建模奖励、Bi-Level GAE）是关于智能体的**推理框架和训练机制**，而非视觉技术本身。因此，这篇论文是关于“Agentic AI”的研究，而不是“视觉”研究。 4.  **第四步：处理特殊和模糊情况——符合保留条件** *   **推理/规划**: 这篇论文是“智能体如何进行规划或在复杂任务中进行多步推理”的绝佳范例。它将智能体的推理过程分解为“状态估计”和“状态转移建模”，并将其置于POMDP框架下，这正是智能体规划的核心。它远超提升LLM基础推理能力的范畴，是典型的Agentic Reasoning研究。 **结论**: 该论文的核心贡献是提出了一种新的方法论（VAGEN框架）和训练机制（世界建模奖励、Bi-Level GAE），用于**显著提升LLM智能体在视觉环境中的多步规划和推理能力**。这完全符合你“构建、改进或演化 LLM智能体”的核心目标，并聚焦于“单智能体”方向下的规划与推理子方向。因此，应**保留**此论文。"
    },
    {
        "index": "#128",
        "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science",
        "link": "/arxiv/2510.16872",
        "arxiv_id": "2510.16872",
        "authors": "Shaolei Zhang, Ju Fan, Meihao Fan, Guoliang Li, Xiaoyong Du",
        "summary": "Autonomous data science, from raw data sources to analyst-grade deep research reports, has been a long-standing challenge, and is now becoming feasible with the emergence of powerful large language models (LLMs). Recent workflow-based data agents have shown promising results on specific data tasks but remain fundamentally limited in achieving fully autonomous data science due to their reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B, the first agentic LLM designed for autonomous data science, capable of automatically completing the end-toend pipeline from data sources to analyst-grade deep research reports. To tackle high-complexity data science tasks, we propose a curriculum-based agentic training paradigm that emulates the learning trajectory of human data scientists, enabling LLMs to progressively acquire and integrate multiple capabilities in real-world environments. We also introduce a data-grounded trajectory synthesis framework that constructs high-quality training data. Through agentic training, DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data question answering and specialized analytical tasks to open-ended data research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze outperforms previous workflow-based agents built on most advanced proprietary LLMs. The model, code, and training data of DeepAnalyze are open-sourced, paving the way toward autonomous data science.",
        "subjects": "Artificial Intelligence, Computation and Language, Databases",
        "date": "2025-10-19",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:06.004895",
        "filter_reason": "这篇论文的核心贡献完全符合您的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——保留** 这篇论文的本质是**构建和改进一个全新的LLM智能体**，而非简单应用。其核心贡献在于： 1.  **构建了一个新的智能体LLM (DeepAnalyze-8B)**：论文明确指出这是“第一个为自主数据科学设计的智能体LLM”，其目标是完成端到端的复杂任务流，这直接对应了“构建LLM智能体”的目标。 2.  **提出了一种新的智能体训练范式**：论文的核心创新点是“基于课程的智能体训练范式”，这是一种旨在让LLM在真实环境中逐步学习和整合多种能力的**方法论**。这属于对LLM智能体“改进”和使其“演化”的范畴。 这与您要求排除的“非演化型应用”有本质区别。论文的重点不是“我们用智能体解决了数据科学问题”，而是“我们如何创造和训练一个能自主解决数据科学问题的智能体”。 **第二步：正面指标——高度匹配** 论文摘要中包含了大量您核心关注点的关键词和概念： *   **核心范式**: `Agentic Large Language Models`, `data agents` *   **自我演化**: 论文的精髓在于`curriculum-based agentic training paradigm`（基于课程的智能体训练范式）。这个概念直接与您的“自我演化”方向高度契合，它描述了一个结构化的、渐进式的智能体能力提升过程，是一种精巧的`Self-Improvement`或`Iterative Improvement`机制。 *   **智能体能力**: 虽然没有直接列出所有关键词，但“自主完成从数据源到深度研究报告的端到端流程”这一描述，内在地包含了复杂的`Planning`（规划整个流程）、`Tool Use`（使用数据分析工具）和执行能力。 **第三步：排除标准——未触发** 论文内容聚焦于智能体的构建和训练，完全没有涉及`Safety`, `Alignment`, `Interpretability`等安全与对齐主题，也未涉及`Vision`等多模态内容。因此，它通过了排除标准的检验。 **第四步：处理特殊和模糊情况——适用例外规则** 这篇论文是“自我演化的应用”这一特殊情况的完美例证。 *   **应用领域**: 论文的应用领域是“自主数据科学”，这是一个特定领域。 *   **核心贡献**: 根据您的规则，“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。” 本文的“基于课程的智能体训练范式”正是一种**新的自我演化/学习机制**。因此，尽管它应用于数据科学领域，但由于其核心贡献在于演化机制本身，必须保留。 **最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一种新颖的LLM智能体架构（DeepAnalyze）和一种使其能力演化的训练范式（课程学习）。这精准地命中了您研究课题中的“构建LLM智能体”和“自我演化”两个核心方向。它不是简单的应用，而是对Agentic AI基础能力的推进，因此完全符合您的筛选要求。"
    },
    {
        "index": "#135",
        "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications",
        "link": "/arxiv/2510.16724",
        "arxiv_id": "2510.16724",
        "authors": "Minhua Lin, Zongyu Wu, Zhichao Xu, Hui Liu, Xianfeng Tang, Qi He, Charu Aggarwal, Hui Liu, Xiang Zhang, Suhang Wang",
        "summary": "The advent of large language models (LLMs) has transformed information access and reasoning through open-ended natural language interaction. However, LLMs remain limited by static knowledge, factual hallucinations, and the inability to retrieve real-time or domain-specific information. Retrieval-Augmented Generation (RAG) mitigates these issues by grounding model outputs in external evidence, but traditional RAG pipelines are often single turn and heuristic, lacking adaptive control over retrieval and reasoning. Recent advances in agentic search address these limitations by enabling LLMs to plan, retrieve, and reflect through multi-step interaction with search environments. Within this paradigm, reinforcement learning (RL) offers a powerful mechanism for adaptive and self-improving search behavior. This survey provides the first comprehensive overview of \\emph{RL-based agentic search}, organizing the emerging field along three complementary dimensions: (i) What RL is for (functional roles), (ii) How RL is used (optimization strategies), and (iii) Where RL is applied (scope of optimization). We summarize representative methods, evaluation protocols, and applications, and discuss open challenges and future directions toward building reliable and scalable RL driven agentic search systems. We hope this survey will inspire future research on the integration of RL and agentic search. Our repository is available at https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-19",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:06.013844",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** - **保留 (Keep)**。这篇论文的核心贡献是对“基于强化学习的智能体搜索”（RL-based agentic search）这一新兴领域进行全面综述。其本质是系统性地梳理和构建一个关于如何利用强化学习（RL）来优化和驱动LLM智能体进行搜索、规划和反思的框架。这直接对应了您研究目标中的“构建、改进或演化LLM智能体”，特别是通过RL这种机制来实现智能体的自适应和自我完善。它不是将智能体作为工具应用到某个垂直领域，而是研究智能体本身的方法论。 **第二步：正面指标——论文是否包含我的核心关注点？** - **高度相关**。摘要中明确包含了多个核心关键词和概念： - **核心范式**: `Agentic AI`, `LLM-based Agents`。论文标题和摘要反复强调“agentic search”。 - **智能体能力**: `Planning`, `Self-Reflection`。摘要明确指出智能体能够“plan, retrieve, and reflect through multi-step interaction”。 - **演化机制**: `Self-Improvement`, `Iterative Improvement`。强化学习（RL）的核心就是通过与环境的交互和反馈进行自我优化，论文将其描述为“adaptive and self-improving search behavior”，这与“自我演化”的核心理念高度契合。 **第三步：排除标准——是否为我的研究焦点之外？** - **不排除**。这篇论文是一篇综述（Survey），其焦点是方法论和系统框架的整合，而不是安全、对齐或多模态技术。虽然它可能提及这些方面，但它们不是论文的核心贡献，因此不触及排除标准。 **第四步：处理特殊和模糊情况** - **推理/规划**: 论文明确将“agentic search”与传统的、启发式的RAG区分开来，强调其核心在于“multi-step interaction with search environments”，这完全符合您对“智能体如何进行规划或在复杂任务中进行多步推理”的保留标准。它研究的不是LLM的基础推理能力，而是智能体在搜索环境中的规划与行动能力。 - **自我演化的应用**: 此处不适用，因为论文本身是方法论综述，而非特定领域的应用。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是系统性地梳理了“基于强化学习的智能体搜索”这一前沿领域。它直接聚焦于如何通过RL机制来增强LLM智能体的规划、反思和自我改进能力，完美地覆盖了您研究范围中的“单智能体”（规划、反思）和“自我演化”（通过RL实现自适应和自我完善）两个核心方向。因此，这篇论文是您研究课题中极具价值的参考文献，应予以保留。"
    },
    {
        "index": "#148",
        "title": "SIADAFIX: issue description response for adaptive program repair",
        "link": "/arxiv/2510.16059",
        "arxiv_id": "2510.16059",
        "authors": "Xin Cao, Nan Yu",
        "summary": "We propose utilizing fast and slow thinking to enhance the capabilities of large language model-based agents on complex tasks such as program repair. In particular, we design an adaptive program repair method based on issue description response, called SIADAFIX. The proposed method utilizes slow thinking bug fix agent to complete complex program repair tasks, and employs fast thinking workflow decision components to optimize and classify issue descriptions, using issue description response results to guide the orchestration of bug fix agent workflows. SIADAFIX adaptively selects three repair modes, i.e., easy, middle and hard mode, based on problem complexity. It employs fast generalization for simple problems and test-time scaling techniques for complex problems. Experimental results on the SWE-bench Lite show that the proposed method achieves 60.67% pass@1 performance using the Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source methods. SIADAFIX effectively balances repair efficiency and accuracy, providing new insights for automated program repair. Our code is available at https://github.com/liauto-siada/siada-cli.",
        "subjects": "Software Engineering, Computation and Language",
        "date": "2025-10-17",
        "category": "cs.CL",
        "crawl_time": "2025-10-21T11:00:06.031142",
        "filter_reason": "根据严格的筛选标准，这篇论文符合您的要求，应被保留。判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将LLM应用于程序修复领域。其核心贡献是提出了一种名为 **SIADAFIX 的新颖方法论/框架**。该框架设计了包含“慢思考bug修复智能体”和“快思考工作流决策组件”的复杂系统，并通过“自适应地选择三种修复模式”和“指导...工作流的编排”来完成任务。这完全符合“构建、改进LLM智能体”的定义，它是一个关于智能体如何工作的机制，而不是一个简单的应用。 2.  **第二步：正面指标** - 论文明确匹配了多个核心关注点： - **核心范式**: `LLM-based Agents` (摘要中直接提及)。 - **智能体能力**: 论文的核心在于`Planning`。它通过“工作流决策组件”来“自适应地选择三种修复模式”，这是一种明确的基于问题复杂度的规划和工作流编排能力。这与ReAct、ToT等Agentic框架的思想一脉相承，即智能体如何根据情况决定下一步行动。 3.  **第三步：排除标准** - 论文的主要贡献是关于提升智能体在特定任务上的性能框架，不涉及`Safety`、`Alignment`、`Interpretability`等安全与对齐议题。 - 论文处理的是代码（文本），不涉及`Vision`、`MLLMs`等多模态内容。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是“保留”的典型案例。它研究的不是LLM本身的基础数学或逻辑推理能力，而是**智能体如何进行规划和多步推理**来完成一个复杂任务（程序修复）。其提出的“快慢思考”结合、自适应模式选择和工作流编排，正是Agentic AI中规划能力的核心体现。 **最终决策**: 这篇论文的核心贡献是构建了一个名为SIADAFIX的LLM智能体框架，该框架通过引入快慢思考机制和自适应工作流编排，显著提升了智能体在复杂任务（程序修复）上的表现。这完全符合您“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的核心目标，特别是与您的研究方向“单智能体”中的“规划”高度契合。因此，应判定为 **True**。"
    },
    {
        "index": "#65",
        "title": "Learning After Model Deployment",
        "link": "/arxiv/2510.17160",
        "arxiv_id": "2510.17160",
        "authors": "Derda Kaymak, Gyuhak Kim, Tomoya Kaichi, Tatsuya Konishi, Bing Liu",
        "summary": "In classic supervised learning, once a model is deployed in an application, it is fixed. No updates will be made to it during the application. This is inappropriate for many dynamic and open environments, where unexpected samples from unseen classes may appear. In such an environment, the model should be able to detect these novel samples from unseen classes and learn them after they are labeled. We call this paradigm Autonomous Learning after Model Deployment (ALMD). The learning here is continuous and involves no human engineers. Labeling in this scenario is performed by human co-workers or other knowledgeable agents, which is similar to what humans do when they encounter an unfamiliar object and ask another person for its name. In ALMD, the detection of novel samples is dynamic and differs from traditional out-of-distribution (OOD) detection in that the set of in-distribution (ID) classes expands as new classes are learned during application, whereas ID classes is fixed in traditional OOD detection. Learning is also different from classic supervised learning because in ALMD, we learn the encountered new classes immediately and incrementally. It is difficult to retrain the model from scratch using all the past data from the ID classes and the novel samples from newly discovered classes, as this would be resource- and time-consuming. Apart from these two challenges, ALMD faces the data scarcity issue because instances of new classes often appear sporadically in real-life applications. To address these issues, we propose a novel method, PLDA, which performs dynamic OOD detection and incremental learning of new classes on the fly. Empirical evaluations will demonstrate the effectiveness of PLDA.",
        "subjects": "Machine Learning",
        "date": "2025-10-20",
        "category": "cs.LG",
        "crawl_time": "2025-10-21T11:00:07.494139",
        "filter_reason": "这篇论文符合我的研究范围，核心依据在于其贡献完全契合“自我演化”这一核心方向。 我的判断过程如下： 1.  **第一步：核心判断** - **论文核心贡献**: 论文的核心是提出了一种名为“部署后自主学习”的新范式（ALMD）和一种具体方法（PLDA）。该方法的核心目标是让一个已部署的模型能够在开放环境中**自主地、增量地学习新知识**（新类别），而不是一个固定的模型。 - **判断**: 这完全符合“构建、改进或演化 LLM智能体”中的“自我演化”方向。论文的本质不是将一个固定模型应用于某个领域，而是提出一种让模型能够**自我完善和迭代**的机制。因此，它不属于“非演化型应用”，应予以保留。 2.  **第二步：正面指标** - 论文的摘要中充满了与我的研究焦点高度相关的关键词： - **自我演化**: 整个ALMD范式就是关于自我演化的。 - **自我改进**: “学习是连续的，不涉及人类工程师”、“ID类别的集合会随着新类别的学习而扩展”等描述，都直接指向模型的自我改进能力。 - **迭代改进**: “学习我们遇到的新类别是立即和增量的”描述了迭代改进的过程。 - 这些正面指标进一步确认了论文与“自我演化”方向的强相关性。 3.  **第三步：排除标准** - 论文摘要中没有提及`Safety`, `Alignment`, `Hallucination`等安全与对齐相关的话题。 - 论文也没有涉及`Vision`, `MLLMs`等多模态内容。 - 因此，论文没有触犯任何明确的排除标准。 4.  **第四步：处理特殊和模糊情况** - 论文的研究内容可以被视为一种通用的“自我演化机制”的应用。根据我设定的特殊规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域...也应该保留。” 这篇论文的核心贡献正是ALMD这种自我演化机制，因此，即使它没有明确限定在LLM智能体或某个特定领域，也应该被保留。它所描述的“动态OOD检测”和“增量学习”是构建能够在真实世界中持续学习和演化的智能体的关键技术。 **最终决策**: 综合以上分析，该论文的核心贡献是提出了一种让模型在部署后能够自主、持续、增量学习新知识的机制。这完全符合我研究目标中的“自我演化”方向。它不是关于模型的基础推理能力或特定领域的非演化型应用，而是聚焦于智能体如何适应环境并进行自我完善。因此，这篇论文高度相关，应该被**保留 (Keep)**。"
    },
    {
        "index": "#75",
        "title": "Consistent Zero-Shot Imitation with Contrastive Goal Inference",
        "link": "/arxiv/2510.17059",
        "arxiv_id": "2510.17059",
        "authors": "Kathryn Wantlin, Chongyi Zheng, Benjamin Eysenbach",
        "summary": "In the same way that generative models today conduct most of their training in a self-supervised fashion, how can agentic models conduct their training in a self-supervised fashion, interactively exploring, learning, and preparing to quickly adapt to new tasks? A prerequisite for embodied agents deployed in real world interactions ought to be training with interaction, yet today's most successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion of action. The problem of pure exploration (which assumes no data as input) is well studied in the reinforcement learning literature and provides agents with a wide array of experiences, yet it fails to prepare them for rapid adaptation to new tasks. Today's language and vision models are trained on data provided by humans, which provides a strong inductive bias for the sorts of tasks that the model will have to solve (e.g., modeling chords in a song, phrases in a sonnet, sentences in a medical record). However, when they are prompted to solve a new task, there is a faulty tacit assumption that humans spend most of their time in the most rewarding states. The key contribution of our paper is a method for pre-training interactive agents in a self-supervised fashion, so that they can instantly mimic human demonstrations. Our method treats goals (i.e., observations) as the atomic construct. During training, our method automatically proposes goals and practices reaching them, building off prior work in reinforcement learning exploration. During evaluation, our method solves an (amortized) inverse reinforcement learning problem to explain demonstrations as optimal goal-reaching behavior. Experiments on standard benchmarks (not designed for goal-reaching) show that our approach outperforms prior methods for zero-shot imitation.",
        "subjects": "Machine Learning",
        "date": "2025-10-20",
        "category": "cs.LG",
        "crawl_time": "2025-10-21T11:00:07.504248",
        "filter_reason": "这篇论文完全符合您的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出一种**自监督地预训练交互式智能体**的新方法，使其能够通过自主探索和学习来快速模仿人类演示。这完全符合您筛选标准中的“保留”条件：论文的核心是关于**构建和改进智能体的方法论**。它并非将已有智能体应用于特定领域（非演化型应用），也非单纯提升LLM的基础推理能力，而是关注智能体如何通过交互进行学习。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了多个您关注的正面指标，且与其核心贡献紧密相关： *   **核心范式**: 论文明确提出了“agentic models”和“embodied agents”，其研究内容正是关于Agentic AI。 *   **自我演化**: 这是最强的匹配点。论文的核心是让智能体进行“自监督”学习，通过“交互式探索、学习和准备”，从而“快速适应新任务”。这正是您“自我演化”研究方向的精髓，即智能体通过经验和环境反馈进行自我完善和迭代。其方法“automatically proposes goals and practices reaching them”是一种典型的迭代改进机制。 *   **规划与行动**: 论文的方法围绕“目标构建”、“尝试达成目标”等行为，这涉及到智能体的自主规划和行动能力，属于“单智能体”范畴。 3.  **第三步：排除标准——未触及** 论文的主要贡献不涉及安全、对齐、可解释性等排除项。虽然论文提到了视觉模型（VLMs）作为背景对比，但其研究的核心是智能体的训练范式，而非新的视觉模型本身。这符合您“除非它们被用作智能体感知环境的工具，而不是研究的核心”的特殊规则。 4.  **第四步：处理特殊和模糊情况——符合保留条件** *   **推理/规划**: 论文关注的是智能体如何通过与环境交互来规划并达成一系列目标，从而实现快速模仿，这属于智能体的规划范畴，而非提升LLM本身的数学或逻辑推理能力。 *   **自我演化的应用**: 这不是一个特定领域的应用，而是一个通用的智能体演化框架，因此直接符合“自我演化”的核心研究方向。 5.  **第五步：最终决策** 综合以上分析，该论文提出了一种让智能体自我演化、自主学习和快速适应的新训练框架。它的本质贡献直指“LLM智能体及其演化”这一课题的核心，特别是“自我演化”和“单智能体”两大方向。因此，这篇论文是您需要筛选的前沿论文，应予以保留。"
    },
    {
        "index": "#93",
        "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search",
        "link": "/arxiv/2510.16916",
        "arxiv_id": "2510.16916",
        "authors": "Dong Li, Xujiang Zhao, Linlin Yu, Yanchi Liu, Wei Cheng, Zhengzhang Chen, Zhong Chen, Feng Chen, Chen Zhao, Haifeng Chen",
        "summary": "Large Language Models (LLMs) offer promising capabilities for tackling complex reasoning tasks, including optimization problems. However, existing methods either rely on prompt engineering, which leads to poor generalization across problem types, or require costly supervised training. We introduce SolverLLM, a training-free framework that leverages test-time scaling to solve diverse optimization problems. Rather than solving directly, SolverLLM generates mathematical formulations and translates them into solver-ready code, guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the search process, we modify classical MCTS with (1) dynamic expansion for adaptive formulation generation, (2) prompt backpropagation to guide exploration via outcome-driven feedback, and (3) uncertainty backpropagation to incorporate reward reliability into decision-making. Experiments on six standard benchmark datasets demonstrate that SolverLLM outperforms both prompt-based and learning-based baselines, achieving strong generalization without additional training.",
        "subjects": "Machine Learning",
        "date": "2025-10-19",
        "category": "cs.LG",
        "crawl_time": "2025-10-21T11:00:07.516635",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于构建了一种新颖的LLM智能体框架。以下是详细的判断依据： 1.  **第一步：核心判断 (保留)** 论文的核心是构建和改进一个LLM智能体。它提出了一个名为`SolverLLM`的“训练自由的框架”，其本质不是一个简单的应用，而是一个方法论，指导LLM如何通过一个结构化的过程来解决复杂问题。这与“非演化型应用”的排除规则有明显区别，因为论文的重点是**智能体的工作机制本身**，而不是它解决了哪个领域的优化问题。 2.  **第二步：正面指标 (高度匹配)** 论文包含了多个你关注的核心关注点，这证明了它的高度相关性： *   **规划:** 论文的核心是一个新颖的“LLM引导的蒙特卡洛树搜索 (MCTS)”策略。MCTS是一种经典的规划和搜索算法，在这里被用作智能体决策和探索解决方案空间的蓝图。这完全符合“智能体如何进行规划或在复杂任务中进行多步推理”的要求。 *   **工具使用:** 摘要明确指出，智能体“生成数学公式并将其转换为求解器就绪的代码”。在这里，外部的数学求解器是智能体使用的**工具**，而LLM负责生成调用该工具的代码。这是一个非常标准和经典的工具使用场景。 *   **自我修正/反思:** 论文提出了“提示反向传播”机制，该机制利用“结果驱动的反馈”来引导搜索过程。这意味着智能体在执行一个行动（生成代码并求解）后，会根据结果（求解的成功与否、质量高低）来反思和调整其下一步的行动（即优化搜索策略或提示），这是一种明确的自我修正和迭代改进机制。 3.  **第三步：排除标准 (不触发)** 论文完全没有涉及安全、对齐、可解释性或多模态等内容，因此不触发任何排除标准。 4.  **第四步：处理特殊和模糊情况 (符合保留条件)** *   **推理/规划:** 这篇论文是“保留”的典型案例。它不是在提升LLM的数学基础能力，而是在构建一个**智能体框架**来利用LLM进行复杂的规划和多步推理。MCTS的引入、工具的使用以及反馈循环，都使其超越了单纯的“LLM推理”，进入了“Agentic AI”的范畴。 *   **自我演化的应用:** 虽然论文应用于优化问题领域，但它的核心贡献是一种新的、可泛化的智能体工作机制（框架），而不是该应用本身。因此，它符合研究焦点。 **总结:** `SolverLLM`论文的核心贡献是设计并实现了一个集成了**规划（MCTS）**、**工具使用（代码生成与求解）**和**自我修正（基于反馈的提示反向传播）**等关键能力的LLM智能体框架。它完美契合了你研究课题中关于“构建、改进LLM智能体”的核心目标，特别是属于“单智能体”方向下的规划、工具使用和自我反思等子方向。因此，应**保留**。"
    },
    {
        "index": "#132",
        "title": "LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs",
        "link": "/arxiv/2510.16552",
        "arxiv_id": "2510.16552",
        "authors": "Ang Li, Yifei Wang, Zhihang Yuan, Stefanie Jegelka, Yisen Wang",
        "summary": "Reinforcement learning in large language models (LLMs) often relies on scalar rewards, a practice that discards valuable textual rationale buried in the rollouts, forcing the model to explore \\textit{de novo} with each attempt and hindering sample efficiency. While LLMs can uniquely learn from language feedback provided in-context, naively integrating on-line experiences into RL training presents a paradox: feedback from the same problem risks information leakage and memorization, while feedback from different problems often leads to behavior collapse due to irrelevant context. To resolve this tension, we propose \\textbf{Language-And-Numerical Policy Optimization (LANPO)}, a framework that cleanly separates the roles of feedback: language guides exploration, while numerical rewards drive optimization. LANPO builds a dynamic experience pool from past trials and introduces two principles to ensure feedback is effective: \\emph{Reward-Agnostic Reflection} for safe intra-sample self-correction and \\emph{Relevant Abstraction} to distill generalizable lessons from inter-sample experiences. Across mathematical reasoning benchmarks, LANPO enables 7B and 14B models to significantly outperform strong baselines trained with GRPO in test accuracy. Our work provides a robust method for integrating historical experiences into the LLM RL loop, creating more effective and data-efficient learning agents.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-18",
        "category": "cs.LG",
        "crawl_time": "2025-10-21T11:00:07.569046",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献直接命中了你的“自我演化”研究焦点。 **判断过程分析:** 1.  **第一步：核心判断 (保留)** - 论文的核心是提出一个名为 **LANPO (Language-And-Numerical Policy Optimization)** 的新**框架**。 - 这个框架的目标是让LLM通过强化学习（RL）从历史经验中更有效地学习，从而创建“更有效和更高效的数据学习智能体”。 - 这不是在特定领域（如生物、金融）简单应用LLM，也不是在提升LLM的基础推理能力（如一个新CoT变体）。它是在**构建和改进一个能让LLM自我演化的方法论**。因此，根据第一步的规则，应予以保留。 2.  **第二步：正面指标 (高度匹配)** - 论文摘要中包含了大量与你研究焦点直接相关的核心关键词，这进一步确认了它的相关性： - **自我演化**: 论文的核心就是让智能体从过去的经验中学习，实现迭代改进。 - **自我修正/自我反思**: 明确提出了 `Reward-Agnostic Reflection`（与奖励无关的反思）机制，这是一个典型的智能体自我修正能力。 - **迭代改进**: 整个框架通过构建动态经验池，从历史试验中提炼经验教训，这正是迭代改进和自我完善的过程。 - **学习智能体**: 摘要最后明确指出，其工作旨在创建“学习智能体”。 3.  **第三步：排除标准 (未触发)** - 论文的主要贡献不是关于安全、对齐、可解释性或幻觉。 - 论文不涉及多模态或视觉内容。 - 因此，论文没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况 (确认保留)** - **推理/规划**: 论文在数学推理基准上测试。这看起来像是“推理”，但其核心贡献**不是**一个新的推理技巧（如一种新的思维链），而是一个**让智能体学会如何推理的RL框架**。根据规则“如果论文是关于智能体如何进行规划或在复杂任务中进行多步推理……应保留”，LANPO正是这样一个关于智能体学习过程的框架，因此符合保留条件。 - **自我演化的应用**: 这篇论文是提出一种新的“自我演化”机制（LANPO），并将其应用于数学推理领域。根据规则“如果论文的核心是提出一种新的‘自我演化’机制……也应该保留”，这篇论文是典型的此类情况，应被保留。 **最终决策:** 综合以上分析，这篇论文的核心贡献是提出了一种名为LANPO的新框架，该框架通过结合语言和数值反馈，让LLM智能体能够从历史经验中进行自我反思和迭代改进，从而实现更高效的自我演化。这完全契合你研究课题中的“自我演化”方向，是一篇高质量的前沿论文，应被**保留**。"
    },
    {
        "index": "#209",
        "title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems",
        "link": "/arxiv/2510.15969",
        "arxiv_id": "2510.15969",
        "authors": "Paul-Niklas Ken Kandora, Simon Caspar Zeller, Aaron Jeremias Elsing, Elena Kuss, Steffen Rebennack",
        "summary": "Reformulating nonlinear optimization problems is largely manual and expertise-intensive, yet it remains essential for solving such problems with linear optimization solvers or applying special-purpose algorithms. We introduce \\textit{LinearizeLLM}, an agent-based framework that solves this task by leveraging Large Language Models (LLMs). The framework assigns each nonlinear pattern to a \\textit{reformulation agent} that is explicitly instructed to derive an exact linear reformulation for its nonlinearity pattern, for instance, absolute-value terms or bilinear products of decision variables. The agents then coordinate to assemble a solver-ready linear model equivalent to the original problem. To benchmark the approach, we create a dataset of 20 real-world nonlinear optimization problems derived from the established ComplexOR dataset of linear optimization problems. We evaluate our approach with several LLMs. Our results indicate that specialized LLM agents can automate linearization tasks, opening a path toward fully conversational modeling pipelines for nonlinear optimization.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-12",
        "category": "cs.LG",
        "crawl_time": "2025-10-21T11:00:07.632394",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献是提出了一个名为 `LinearizeLLM` 的 **agent-based framework** (基于智能体的框架)。这直接命中了你筛选条件的“保留”项——“构建、改进或演化 LLM智能体的论文”。它不是简单地将LLM作为工具应用于一个领域，而是**设计了一个新的智能体架构**来解决问题。论文详细描述了如何将复杂任务分解，并分配给不同的“reformulation agent”，这本身就是对智能体构建方法论的贡献。 2.  **正面指标 (第二步):** 论文包含了多个你的核心关注点： *   **多智能体:** 论文明确使用了多个`reformulation agent`，每个智能体负责一种特定的非线性模式，这构成了一个多智能体系统。 *   **协作:** 摘要中提到“The agents then **coordinate** to assemble a solver-ready linear model”，这直接对应了多智能体间的协作与协调。 *   **Agentic AI:** 整个框架的设计思想就是Agentic AI，通过多个智能体分工合作，完成一个复杂的、需要多步推理和规划的任务。 3.  **排除标准 (第三步):** 论文内容不涉及安全、对齐、多模态等排除项，因此没有触发任何排除规则。 4.  **特殊和模糊情况 (第四步):** *   **推理/规划:** 论文的研究内容是关于智能体如何进行规划和协作来解决一个复杂问题（线性化），而不是提升LLM本身的基础数学或逻辑推理能力。这完全符合“保留”关于智能体规划/推理框架的规则。 *   **自我演化的应用:** 虽然这篇论文不直接关于“自我演化”，但它符合该规则的逻辑精神。它的核心是提出一种**新的Agentic方法论**（`LinearizeLLM`框架），而不是将已有框架简单应用到特定领域。因此，它属于“构建智能体”的范畴，而非“非演化型应用”。 **最终决策:** 综合来看，这篇论文的核心贡献在于**构建了一个新颖的多智能体协作框架**来解决一个特定领域的复杂问题。其研究焦点是智能体的架构和协作机制，这与你的“多智能体”研究方向高度契合。因此，这篇论文应被保留。"
    },
    {
        "index": "#205",
        "title": "Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning",
        "link": "/arxiv/2510.15979",
        "arxiv_id": "2510.15979",
        "authors": "Zexu Sun, Yongcheng Zeng, Erxue Min, Heyang Gao, Bokai Ji, Xu Chen",
        "summary": "Contemporary progress in large language models (LLMs) has revealed notable inferential capacities via reinforcement learning (RL) employing verifiable reward, facilitating the development of O1 and R1-like reasoning models. Directly training from base models with RL is called zero-RL. However, previous works rely upon activating LLMs' inherent capacities through fixed prompt templates. This strategy introduces substantial sampling inefficiencies for weak LLMs, as the majority of problems generate invalid outputs during accuracy-driven filtration in reasoning tasks, which causes a waste of samples. To solve this issue, we propose Cog-Rethinker, a novel hierarchical metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses on the rollout procedure in RL training. After the direct rollout, our Cog-Rethinker improves sample utilization in a hierarchical metacognitive two-stage framework. By leveraging human cognition during solving problems, firstly, it prompts policy to decompose zero-accuracy problems into subproblems to produce final reasoning results. Secondly, with zero-accuracy problems in previous rollout stage, it further prompts policy to refine these answers by referencing previous wrong solutions. Moreover, to enable cold-start of the two new reasoning patterns and maintain train-test consistency across prompt templates, our Cog-Rethinker applies supervised fine-tuning on the policy using correct samples of the two stages with direct rollout template. Experimental results demonstrate Cog-Rethinker's superior performance on various mathematical reasoning benchmarks, we also analyzed its improved sample efficiency that accelerates convergence compared to baseline methods.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.LG",
        "crawl_time": "2025-10-21T11:00:07.631055",
        "filter_reason": "这篇论文的核心贡献是提出了一个名为 Cog-Rethinker 的分层元认知强化学习框架，用于提升LLM的推理能力。经过严格筛选，该论文符合研究范围。 判断过程如下： 1.  **第一步：核心判断 (保留)** 论文的本质是构建和改进LLM智能体的方法论。它并非将现有智能体框架简单应用于某个领域，而是提出了一个全新的、包含特定机制的框架（Cog-Rethinker）来优化LLM的推理过程。该框架的核心在于改变RL训练中的rollout策略，使其更高效。这完全符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **第二步：正面指标 (高度相关)** 论文包含了多个核心关注点： *   **智能体能力**: 论文明确提到了两个关键阶段：1) 将问题分解为子问题（`Planning`）；2) 参考错误答案进行优化（`Self-Correction` / `Self-Reflection`）。这些都是单智能体研究的核心能力。 *   **演化机制**: 整个分层迭代优化的过程，就是一种 `Iterative Improvement` 和 `Self-Refine` 的机制，旨在让智能体通过反思和修正来演化其推理能力。 3.  **第三步：排除标准 (未触发)** 论文的主要贡献不涉及安全、对齐、可解释性，也未涉及多模态或视觉内容。因此，未触发任何排除标准。 4.  **第四步：处理特殊和模糊情况 (符合保留条件)** 这篇论文是“推理/规划”模糊情况的典型范例。它并非简单地通过微调或新数据集来提升LLM的基础数学能力，而是构建了一个包含**规划（分解问题）**和**自我反思（修正错误）**的完整Agentic框架。这正是我们研究范围中“智能体如何进行规划或在复杂任务中进行多步推理”的核心议题。论文提出的元认知框架，本质上是一种让智能体学会“如何思考”和“如何修正思考”的结构化方法，属于Agentic AI的范畴。 **最终决策**：综合分析，尽管论文的评测基准是数学推理，但其核心贡献在于提出了一种新颖的、具有规划和自我修正能力的智能体框架。这直接对应了研究焦点中的“单智能体”方向，特别是“规划”和“自我反思”子方向。因此，这篇论文与“LLM智能体及其演化”的研究课题高度相关，应当保留。"
    },
    {
        "index": "#288",
        "title": "More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents",
        "link": "/arxiv/2510.16786",
        "arxiv_id": "2510.16786",
        "authors": "Pengfei Gao, Chao Peng",
        "summary": "LLM-powered coding agents, which operate in iterative loops (turns) to solve software engineering tasks, are becoming increasingly powerful. However, their practical deployment is hindered by significant and unpredictable costs. This challenge arises from a combination of factors: quadratically growing token counts with each turn, the high price of models, the large number of turns required for real-world tasks, and the tendency of agents to take inefficient or unnecessary actions. While existing research focuses on optimizing individual turns, the strategic control of the total number of turns remains an underexplored area for managing agent performance and cost. To address this gap, we conduct a comprehensive empirical study on SWE-bench using three state-of-the-art models and evaluate the impact of three distinct turn-control strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a novel dynamic-turn strategy that grants extensions on-demand. Our findings first reveal a fundamental trade-off in the unrestricted setting, where no single model excels across performance, cost, and turn efficiency. We then show that a fixed-turn limit, specifically at the 75th percentile of the baseline, serves as a \"sweet spot\", substantially reducing costs (by 24%-68%) with minimal impact on solve rates. Most significantly, the dynamic-turn strategy consistently outperforms fixed-limit approaches, achieving comparable or better solve rates while further reducing costs by an additional 12%-24% by intelligently allocating resources only to tasks that need them. This work provides the first systematic analysis of turn-control strategies, offering simple yet effective guidelines for developers to balance cost and efficacy. We demonstrate that dynamic resource allocation is a superior, easy-to-implement approach for deploying powerful yet economically viable coding agents.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning",
        "date": "2025-10-19",
        "category": "cs.LG",
        "crawl_time": "2025-10-21T11:00:07.720213",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于**改进LLM智能体的运作效率和成本控制**，属于“构建、改进或演化LLM智能体”这一核心目标。 以下是详细的判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将一个已有的LLM智能体应用到编码领域去完成任务。它的核心贡献是提出并系统性地研究了**“轮次控制策略”**，这是一种管理和优化智能体行为（即其迭代循环）的新方法论。论文通过引入“动态轮次策略”，直接改进了智能体的资源分配和任务执行效率。这完全符合“改进LLM智能体”的定义，而非“非演化型应用”。 2.  **第二步：正面指标** - 论文命中了多个核心关注点： - **核心范式**: `LLM-based Agents` (论文明确研究 \"LLM-powered coding agents\")。 - **智能体能力**: 论文研究的“迭代循环”是智能体进行`Planning`（规划）、`Self-Correction`（自我纠正）和`ReAct`（推理+行动）的核心机制。论文提出的策略正是为了优化这一关键能力。 - **演化机制**: “动态轮次策略”本身可以看作一种简单的自适应机制，它根据任务需求动态调整资源（轮次数），这体现了智能体在执行层面的`Iterative Improvement`（迭代改进）思想。 3.  **第三步：排除标准** - 论文的研究焦点是智能体的**效率和成本**，不涉及`Safety`、`Alignment`、`Hallucination`等安全与对齐问题。 - 论文研究对象是基于文本的编码智能体，不涉及`Vision`、`MLLMs`等多模态内容。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文研究的“迭代循环”正是智能体进行多步推理和规划以解决复杂任务的体现。论文提出的“动态轮次策略”是对这一规划/执行过程的直接优化和控制，完全符合“保留”的条件。它不是在提升LLM的基础推理能力，而是在提升智能体框架的推理效率和成本效益。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种新的、有效的策略来**改进LLM智能体的运行机制**，使其在保持性能的同时大幅降低成本。这直接对应了您研究课题中“改进LLM智能体”的核心目标，特别是与单智能体的规划和执行效率优化高度相关。因此，这篇论文应该被**保留**。"
    },
    {
        "index": "#332",
        "title": "Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification",
        "link": "/arxiv/2510.16281",
        "arxiv_id": "2510.16281",
        "authors": "Yilin Wu, Anqi Li, Tucker Hermans, Fabio Ramos, Andrea Bajcsy, Claudia P'erez-D'Arpino",
        "summary": "Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2025-10-18",
        "category": "cs.LG",
        "crawl_time": "2025-10-21T11:00:07.789731",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为“运行时推理-行动对齐验证”的训练无关方法，用于引导视觉-语言-动作（VLA）模型。该方法的核心机制是：在智能体生成一个文本计划后，采样多个候选动作序列，通过模拟和视觉语言模型（VLM）作为工具来评估每个序列的结果，并选择与原始计划最一致的动作序列来执行。 以下是详细的判断过程： 1.  **第一步：核心判断** - **保留**: 论文的核心是构建一个**改进LLM智能体**的新框架。它没有发明一个全新的智能体，而是为现有的“推理型VLA智能体”增加了一个关键的**自我验证和自我纠正**模块。这个模块（即“运行时引导方法”）是一个关于如何让智能体更忠实地执行其自身规划的通用方法论。因此，它不是简单地将LLM应用到机器人领域，而是在研究智能体本身的运行机制，符合“构建、改进LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **自我反思/自我纠正**: 论文的整个框架就是一种动态的自我纠正机制。智能体在执行前，会“反思”其潜在行动是否符合其“思考”（文本计划），并选择最优方案。这完全符合`Self-Correction`和`Self-Reflection`的定义。 - **规划**: 论文的出发点是解决智能体的规划（文本计划）与行动不一致的问题，整个过程都围绕`Planning`展开。 - **工具使用**: 智能体明确使用了两个外部工具：1) **模拟器**来预测动作序列的结果；2) **预训练的VLM**来判断结果与文本计划的对齐程度。这是非常典型的`Tool Use`范例。 3.  **第三步：排除标准** - **安全与对齐**: 论文虽然提到了“对齐”，但这里的“对齐”是指智能体内部“推理与行动”的一致性，而不是AI伦理层面的“与人类价值观对齐”。其主要贡献是提升性能和鲁棒性，而非安全或可解释性，因此不在此排除范围内。 - **多模态与视觉**: 这是本案例中最关键的判断点。论文确实涉及了视觉语言模型（VLA, VLM）。但是，根据筛选规则的例外情况：“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在这篇论文中，**VLM是作为“验证工具”来使用的**，是作者提出的“对齐验证框架”的一个组成部分。研究的**核心贡献**是这个框架本身，而不是一个新的VLM模型或视觉理解算法。因此，视觉部分是智能体感知和评估环境的方式，符合例外情况，不应被排除。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文明确属于“保留”情况。它不是在提升LLM的基础数学能力，而是在研究一个**智能体如何进行规划并确保行动与规划一致**，这是一个典型的Agentic AI问题，其方法论（运行时验证和选择）可以被视为一个新的Agentic框架。 **最终决策**: 综合以上分析，这篇论文完全符合研究范围。它属于**单智能体**方向，其核心贡献在于提出了一种新颖的、利用工具进行自我反思和自我纠正的机制，以提升智能体规划的执行效果。尽管应用场景是机器人，但其方法论具有通用性，是关于智能体本身的核心研究。因此，应予以保留。"
    },
    {
        "index": "#345",
        "title": "Interpretable RNA-Seq Clustering with an LLM-Based Agentic Evidence-Grounded Framework",
        "link": "/arxiv/2510.16082",
        "arxiv_id": "2510.16082",
        "authors": "Elias Hossain, Mehrdad Shoeibi, Ivan Garibay, Niloofar Yousefi",
        "summary": "We propose CITE V.1, an agentic, evidence-grounded framework that leverages Large Language Models (LLMs) to provide transparent and reproducible interpretations of RNA-seq clusters. Unlike existing enrichment-based approaches that reduce results to broad statistical associations and LLM-only models that risk unsupported claims or fabricated citations, CITE V.1 transforms cluster interpretation by producing biologically coherent explanations explicitly anchored in the biomedical literature. The framework orchestrates three specialized agents: a Retriever that gathers domain knowledge from PubMed and UniProt, an Interpreter that formulates functional hypotheses, and Critics that evaluate claims, enforce evidence grounding, and qualify uncertainty through confidence and reliability indicators. Applied to Salmonella enterica RNA-seq data, CITE V.1 generated biologically meaningful insights supported by the literature, while an LLM-only Gemini baseline frequently produced speculative results with false citations. By moving RNA-seq analysis from surface-level enrichment to auditable, interpretable, and evidence-based hypothesis generation, CITE V.1 advances the transparency and reliability of AI in biomedicine.",
        "subjects": "Quantitative Methods, Artificial Intelligence, Machine Learning",
        "date": "2025-10-17",
        "category": "cs.LG",
        "crawl_time": "2025-10-21T11:00:07.802396",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献并非将LLM作为工具简单应用于生物领域，而是**构建了一个全新的、名为CITE V.1的多智能体框架**。摘要明确指出，该框架“orchestrates three specialized agents”（编排三个专业化智能体）：Retriever、Interpreter和Critics。这表明论文的重点在于**方法论创新**，即如何设计和组织多个智能体协同工作以完成复杂任务，而不是在特定领域（生物医学）中应用已有技术。因此，它不属于“非演化型应用”的排除范围。 2.  **第二步：正面指标——高度匹配** 该论文包含了多个核心关注点： *   **核心范式**: `Agentic AI`, `LLM-based Agents`, `Multi-Agent Systems (MAS)`。标题和摘要反复强调“agentic framework”和“three specialized agents”。 *   **多智能体**: 论文描述了不同角色智能体之间的`协作`（Collaboration）与`通信`（Communication，隐含在“orchestrates”和“evaluate claims”中）。Retriever负责工具使用（`Tool Use`），Interpreter负责生成假设，而Critics则扮演了审查和评估的角色。 *   **智能体能力**: Critic智能体的功能（evaluate claims, enforce evidence grounding）是一种高级的`自我修正`（Self-Correction）或`自我反思`（Self-Reflection）机制，确保了最终输出的可靠性。 3.  **第三步：排除标准——未触达** *   **安全与对齐**: 尽管论文提到了“interpretable”（可解释）和“transparent”（透明），但这并非论文的**主要贡献**。论文的核心是**提出一个能够实现可解释性的智能体架构**，而不是研究一种新的可解释性理论或对齐方法。可解释性是该多智能体框架设计所带来的优良特性，是其方法论成功的结果，而非研究目标本身。因此，它不适用此排除规则。 *   **多模态与视觉**: 论文完全基于文本，不涉及视觉或多模态内容。 4.  **第四步：处理特殊和模糊情况——适用“保留”规则** 论文描述了一个多智能体如何通过分工协作、工具使用和内部批判（Critics）来完成一个复杂的多步推理任务（从数据检索到假设生成再到验证）。这完全符合“关于智能体如何进行规划或在复杂任务中进行多步推理”的保留条件。 **最终决策**: 这篇论文的本质是提出了一种新颖的多智能体协作框架，该框架通过明确的角色分工（检索、解释、批判）和证据锚定机制，解决了LLM在特定领域应用中易产生幻觉和缺乏可解释性的问题。其核心贡献在于**智能体系统的构建和交互范式**，完全契合您研究范围中的“多智能体”方向。尽管其应用场景是生物医学，但这仅仅是验证其框架有效性的试验场，论文的焦点始终在方法论本身。因此，应予以保留。"
    },
    {
        "index": "#13",
        "title": "Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis",
        "link": "/arxiv/2510.17235",
        "arxiv_id": "2510.17235",
        "authors": "Chong Chen, Ze Liu, Lingfeng Bao, Yanlin Wang, Ting Chen, Daoyuan Wu, Jiachi Chen",
        "summary": "The cryptocurrency market offers significant investment opportunities but faces challenges including high volatility and fragmented information. Data integration and analysis are essential for informed investment decisions. Currently, investors use three main approaches: (1) Manual analysis across various sources, which depends heavily on individual experience and is time-consuming and prone to bias; (2) Data aggregation platforms-limited in functionality and depth of analysis; (3) Large language model agents-based on static pretrained models, lacking real-time data integration and multi-step reasoning capabilities. To address these limitations, we present Coinvisor, a reinforcement learning-based chatbot that provides comprehensive analytical support for cryptocurrency investment through a multi-agent framework. Coinvisor integrates diverse analytical capabilities through specialized tools. Its key innovation is a reinforcement learning-based tool selection mechanism that enables multi-step planning and flexible integration of diverse data sources. This design supports real-time interaction and adaptive analysis of dynamic content, delivering accurate and actionable investment insights. We evaluated Coinvisor through automated benchmarks on tool calling accuracy and user studies with 20 cryptocurrency investors using our interface. Results show that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base model in tool orchestration. User studies show high satisfaction (4.64/5), with participants preferring Coinvisor to both general LLMs and existing crypto platforms (4.62/5).",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-20",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.532054",
        "filter_reason": "这篇论文符合您的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** - **保留 (Keep)**。这篇论文的核心贡献是构建了一个名为 Coinvisor 的 LLM 智能体。它并非简单地将现有模型应用于金融领域，而是提出了一个**新的方法论和框架**来解决现有 LLM 智能体在特定任务中的局限性。论文明确指出现有 LLM 智能体“缺乏实时数据集成和多步推理能力”，而 Coinvisor 的核心创新正是为了解决这些问题。因此，它属于“构建、改进 LLM 智能体”的范畴，符合第一步的保留标准。 **第二步：正面指标——论文是否包含我的核心关注点？** - **是，论文包含了多个核心关注点。** - **核心范式**: 论文明确提到了 `LLM-based Agents` 和 `Multi-Agent Framework`。 - **智能体能力**: 论文的核心创新点之一是 `multi-step planning`（多步规划）和 `flexible integration of diverse data sources`（灵活集成多样化数据源），这直接对应了智能体的 `Planning`（规划）和 `Tool Use / Tool Augmentation`（工具使用）能力。其 `reinforcement learning-based tool selection mechanism`（基于强化学习的工具选择机制）是对智能体工具使用能力的一种改进。 - **多智能体**: 论文提到其框架是 `multi-agent framework`，这表明它涉及了多智能体系统的构建。 **第三步：排除标准——是否为我的研究焦点之外？** - **否，论文的主要贡献不属于排除标准。** - **安全与对齐**: 论文的研究目标是提升投资分析的准确性和效率，而非安全、对齐或可解释性。 - **多模态与视觉**: 论文专注于文本和结构化数据（加密货币市场数据），不涉及视觉或多模态内容。 **第四步：处理特殊和模糊情况** - **推理/规划**: 论文明确提出了 `multi-step planning` 能力，并通过强化学习机制来实现。这完全符合“保留”条件，即“关于智能体如何进行规划或在复杂任务中进行多步推理”。它不是在提升 LLM 的基础数学或逻辑能力，而是在构建一个能够进行规划和工具编排的智能体框架。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于**构建并改进了一个 LLM 智能体**。它通过引入一个基于强化学习的工具选择机制，显著增强了智能体的**多步规划**和**工具使用**能力，使其能够在一个动态、复杂的环境（加密货币市场）中进行自适应分析。这完全符合您研究课题中“单智能体 (Agentic)”方向下的“规划”和“工具使用”子方向。尽管其应用领域是金融投资，但其核心是提出了一种新的智能体框架，而非简单的应用，因此不应被排除。最终判断为 **True**。"
    },
    {
        "index": "#17",
        "title": "Which LLM Multi-Agent Protocol to Choose?",
        "link": "/arxiv/2510.17149",
        "arxiv_id": "2510.17149",
        "authors": "Hongyi Du, Jiaqi Su, Jisen Li, Lijie Ding, Yingxuan Yang, Peixuan Han, Xiangru Tang, Kunlun Zhu, Jiaxuan You",
        "summary": "As large-scale multi-agent systems evolve, the communication protocol layer has become a critical yet under-evaluated factor shaping performance and reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora, etc.), selection is often intuition-driven and lacks standardized guidance. We introduce ProtocolBench, a benchmark that systematically compares agent protocols along four measurable axes: task success, end-to-end latency, message or byte overhead, and robustness under failures. On ProtocolBench, protocol choice significantly influences system behavior. In the Streaming Queue scenario, overall completion time varies by up to 36.5% across protocols, and mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery, resilience also differs consistently across protocols. Beyond evaluation, we present ProtocolRouter, a learnable protocol router that selects per-scenario (or per-module) protocols from requirement and runtime signals. ProtocolRouter reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol baseline, and achieves scenario-specific gains such as higher success in GAIA. We also release ProtocolRouterBench to standardize protocol evaluation and improve reliability at scale.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-20",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.534184",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步核心判断**: 论文的本质是**保留**。 *   论文的核心贡献并非将已有的多智能体框架应用于某个特定领域（如金融、生物），而是聚焦于多智能体系统（Multi-Agent Systems）本身的一个基础且关键的组成部分：**通信协议**。 *   它提出了一个全新的基准 `ProtocolBench` 来系统性地评估不同协议，并提出了一个可学习的自适应路由器 `ProtocolRouter` 来动态选择最优协议。这直接属于**改进LLM多智能体系统**的方法论和新框架，完全符合您的核心目标。 2.  **第二步正面指标**: 论文包含了多个核心关注点。 *   **核心范式**: 论文明确围绕 `LLM Multi-Agent Systems` 展开。 *   **多智能体**: 研究的核心是智能体间的 `Communication`（通信），并延伸到了系统整体的 `Collaboration`（协作）效率和 `Robustness`（鲁棒性），这些都是多智能体研究的关键子方向。 *   **演化机制**: `ProtocolRouter` 是一个能够根据场景和运行时信号进行学习和选择的组件。这可以被视为一种**系统级的自适应和演化机制**，它使得整个多智能体社会能够动态地优化其通信策略，从而更好地演化以适应不同任务和环境，这与您关注的“自我演化”方向在系统层面是高度契合的。 3.  **第三步排除标准**: 论文不涉及任何排除标准。 *   论文的主旨是性能、效率和可靠性评估，而非安全、对齐、可解释性。 *   论文不涉及任何多模态或视觉内容。 4.  **第四步特殊和模糊情况**: 此处不适用。 *   论文的研究内容（通信协议）本身就是多智能体框架的一部分，而不是在某个领域的应用。`ProtocolRouter` 的提出是系统层面的创新，而非简单的应用。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于为LLM多智能体系统提供了一个新的评估基准和一个能够提升系统性能与可靠性的自适应框架。它直接解决了多智能体领域中“如何高效通信与协作”这一基础性问题，属于对**多智能体框架的构建和改进**。`ProtocolRouter` 的引入更是一种系统级的演化尝试，使得多智能体社会能够“演化”出更优的沟通策略。因此，这篇论文是您研究课题“LLM智能体及其演化”中“多智能体”方向的优质前沿论文，应予以保留。"
    },
    {
        "index": "#22",
        "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems",
        "link": "/arxiv/2510.17052",
        "arxiv_id": "2510.17052",
        "authors": "Hassan Hamad, Yingru Xu, Liang Zhao, Wenbo Yan, Narendra Gyanchandani",
        "summary": "Tool-augmented large language models (LLMs) are increasingly employed in real-world applications, but tool usage errors still hinder their reliability. We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight distinct error types specific to tool-calling (e.g., premature invocation, argument misalignment, and misinterpretation of tool outputs) and provides targeted feedback to the main LLM. The main LLM, assumed to have strong reasoning, task understanding and orchestration capabilities, then revises its response based on ToolCritic's feedback. We systematically define these error categories and construct a synthetic dataset to train ToolCritic. Experimental results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic improves tool-calling accuracy by up to 13% over baselines, including zero-shot prompting and self-correction techniques. This represents a promising step toward more robust LLM integration with external tools in real-world dialogue applications.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-19",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.536970",
        "filter_reason": "这篇论文完全符合您的筛选标准，应该被保留。以下是我的详细判断过程： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出了一个名为 `ToolCritic` 的新框架。这个框架的本质不是将LLM应用到一个新领域，而是**构建了一个用于改进现有LLM智能体能力的方法论**。它通过一个“批评-修正”的循环，专门解决LLM智能体在使用工具时出现的错误。这直接命中了您“构建、改进或演化LLM智能体”的核心目标，属于对智能体能力的“改进”，因此必须保留。 2.  **第二步：正面指标——高度相关** 论文命中了多个核心正面指标： *   **智能体能力**: 论文的核心是 `Tool Use / Tool Augmentation`（工具使用/工具增强）。它定义了8种工具调用错误类型，并提供了修正方案。 *   **自我反思/修正**: `ToolCritic` 的“检测错误并提供反馈，主LLM据此修正响应”机制，是一种非常具体且可操作的 `Self-Correction`（自我修正）或 `Self-Reflection`（自我反思）的实现。它将自我反思过程外部化和结构化，但最终目的还是为了让智能体自身变得更好。 *   **核心范式**: 整篇论文都在讨论如何让 `LLM-based Agents` 在使用工具时更加可靠，完全属于 `Agentic AI` 的研究范畴。 3.  **第三步：排除标准——未触发** 论文的研究焦点是提升智能体执行任务的成功率和可靠性，而非安全性、伦理或对齐问题。它不涉及 `Safety`, `Security`, `Alignment` 等关键词。同时，论文的研究对象是基于文本的对话系统，不涉及视觉或多模态内容，因此也未触发多模态排除标准。 4.  **第四步：处理特殊和模糊情况——符合保留规则** *   **推理/规划**: 论文讨论的是智能体在执行任务过程中的工具使用环节，这是智能体规划和行动循环的一部分，而不是提升LLM底层的数学或逻辑推理能力。因此，它符合“保留”关于智能体推理/规划的规则。 *   **自我演化**: `ToolCritic` 提出的“检测-反馈-修正”循环，可以被视为一种**迭代的自我改进机制**。虽然修正依赖于外部模块，但整个系统（主LLM + ToolCritic）作为一个整体，通过经验（错误反馈）在性能上得到了提升和演化。这完全符合您对“自我演化”方向的关注。 **最终决策**: 这篇论文的核心贡献是提出了一种新颖的框架（`ToolCritic`），用于**改进LLM智能体的工具使用能力**，并实现了一种结构化的**自我修正机制**。它精准地切中了您研究范围中的“单智能体”方向，特别是“工具使用”和“自我反思/修正”这两个子方向。因此，这篇论文与您的研究课题高度相关，应予以保留。"
    },
    {
        "index": "#18",
        "title": "Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation",
        "link": "/arxiv/2510.17146",
        "arxiv_id": "2510.17146",
        "authors": "Subin Lin, Chuanbo Hua",
        "summary": "Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a substantial share of global building energy use, making reliable anomaly detection essential for improving efficiency and reducing emissions. Classical rule-based approaches offer explainability but lack adaptability, while deep learning methods provide predictive power at the cost of transparency, efficiency, and physical plausibility. Recent attempts to use Large Language Models (LLMs) for anomaly detection improve interpretability but largely ignore the physical principles that govern HVAC operations. We present PILLM, a Physics-Informed LLM framework that operates within an evolutionary loop to automatically generate, evaluate, and refine anomaly detection rules. Our approach introduces physics-informed reflection and crossover operators that embed thermodynamic and control-theoretic constraints, enabling rules that are both adaptive and physically grounded. Experiments on the public Building Fault Detection dataset show that PILLM achieves state-of-the-art performance while producing diagnostic rules that are interpretable and actionable, advancing trustworthy and deployable AI for smart building systems.",
        "subjects": "Artificial Intelligence, Computational Engineering, Finance, and Science",
        "date": "2025-10-20",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.534662",
        "filter_reason": "这篇论文完全符合您的筛选标准，应被保留。其核心贡献在于构建了一种具备**自我演化**能力的LLM智能体框架，而非简单的领域应用。 以下是根据您的筛选标准进行的详细判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质**不是**简单地将LLM应用于HVAC（暖通空调）领域进行异常检测。它的核心贡献是提出了一个名为PILLM的**演化框架**。摘要中的关键句是 \"a Physics-Informed LLM framework that operates within an **evolutionary loop** to **automatically generate, evaluate, and refine** anomaly detection rules\"。这表明论文的核心是构建一个能够自主迭代、自我完善的机制，这正是“自我演化”的精髓。它不是一次性使用LLM，而是让LLM在一个循环中不断演化出更好的规则，完全符合您对“构建、改进或演化LLM智能体”的核心目标。 2.  **第二步：正面指标** - 该论文包含了大量您关注的核心范式和能力关键词： - **核心范式**: `Self-Evolving` (通过 \"evolutionary loop\" 体现), `Evolutionary Algorithms` (通过 \"crossover operators\" 体现)。 - **智能体能力**: `Self-Reflection` (明确提出了 \"physics-informed reflection\" 算子), `Self-Refine` (通过 \"evaluate, and refine\" 体现)。 - **演化机制**: `Self-Improvement`, `Iterative Improvement` (整个 \"evolutionary loop\" 就是这个机制的体现)。 - 这些指标强烈表明该论文与您的研究方向高度相关。 3.  **第三步：排除标准** - 论文的主要贡献不是关于安全、对齐或多模态。它提到了 \"interpretability\"（可解释性），但这是作为其演化出的规则的一个**优点**，而不是论文要解决的核心研究问题。论文的核心是**如何演化**出这些规则，而非**如何解释**这些规则。因此，不触发排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这是判断本论文的关键。表面上看，这是一个关于HVAC的应用论文，但根据您设定的“保留例外”规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。” - 本论文正是这种情况。其核心创新点是**“演化循环”**以及其中的**“物理信息反思和交叉算子”**。HVAC异常检测只是这个新颖演化机制的**验证平台和应用场景**。研究的是**“如何自我演化”**，而不是**“如何检测HVAC异常”**。因此，它完全符合保留的例外情况。 **最终决策**: 综合以上分析，该论文的核心贡献是提出了一种新颖的、带有反思机制的自我演化框架，用于自动生成和完善规则。这精准地命中了您研究目标中的“**自我演化**”方向。尽管其应用场景是特定的HVAC领域，但其方法论具有通用性和前沿性，是您研究“LLM智能体及其演化”课题的理想候选论文。因此，最终判断为 **True**。"
    },
    {
        "index": "#23",
        "title": "STARK: Strategic Team of Agents for Refining Kernels",
        "link": "/arxiv/2510.16996",
        "arxiv_id": "2510.16996",
        "authors": "Juncheng Dong, Yang Yang, Tao Liu, Yang Wang, Feng Qi, Vahid Tarokh, Kaushik Rangadurai, Shuang Yang",
        "summary": "The efficiency of GPU kernels is central to the progress of modern AI, yet optimizing them remains a difficult and labor-intensive task due to complex interactions between memory hierarchies, thread scheduling, and hardware-specific characteristics. While recent advances in large language models (LLMs) provide new opportunities for automated code generation, existing approaches largely treat LLMs as single-shot generators or naive refinement tools, limiting their effectiveness in navigating the irregular kernel optimization landscape. We introduce an LLM agentic framework for GPU kernel optimization that systematically explores the design space through multi-agent collaboration, grounded instruction, dynamic context management, and strategic search. This framework mimics the workflow of expert engineers, enabling LLMs to reason about hardware trade-offs, incorporate profiling feedback, and refine kernels iteratively. We evaluate our approach on KernelBench, a benchmark for LLM-based kernel optimization, and demonstrate substantial improvements over baseline agents: our system produces correct solutions where baselines often fail, and achieves kernels with up to 16x faster runtime performance. These results highlight the potential of agentic LLM frameworks to advance fully automated, scalable GPU kernel optimization.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-19",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.537471",
        "filter_reason": "这篇论文完全符合您的筛选标准，应予以保留。以下是我的详细判断过程： **第一步：核心判断——保留** 这篇论文的本质不是简单地将LLM作为工具应用于GPU内核优化领域，而是提出了一种**全新的LLM智能体框架（STARK）**来解决该问题。其核心贡献在于构建了一个“Strategic Team of Agents”（智能体战略团队），这是一种方法论上的创新。论文详细阐述了该框架如何通过“多智能体协作”、“有根据的指令”、“动态上下文管理”和“战略性搜索”来工作，这完全符合您要求的“核心贡献在于构建、改进或演化LLM智能体”的保留标准。它超越了“非演化型应用”，因为它提出的是一个可迁移的智能体协作框架，而不仅仅是应用本身。 **第二步：正面指标——高度相关** 论文命中了您的多个核心关注点，且相关性极强： *   **核心范式**: 论文标题和摘要中明确提到“LLM agentic framework”和“Team of Agents”，直接对应`Agentic AI`和`Multi-Agent Systems (MAS)`。 *   **多智能体**: “multi-agent collaboration”和“Strategic Team of Agents”是论文的核心机制，这与您的“协作”、“通信”焦点高度一致。 *   **自我演化**: “refine kernels iteratively”体现了迭代优化的过程，属于`Self-Improvement`或`Self-Refine`的范畴。 *   **智能体能力**: “strategic search”对应`Planning`；“incorporate profiling feedback”对应`Tool Use`（性能剖析工具）；“dynamic context management”对应`Memory`。 **第三步：排除标准——不涉及** 论文的研究焦点是智能体框架的设计和优化效果，完全没有涉及`Safety`, `Alignment`, `Interpretability`等安全对齐主题，也未涉及`Vision`, `VLMs`等多模态内容。 **第四步：处理特殊和模糊情况——符合例外保留规则** 这篇论文是“自我演化的应用”的一个绝佳范例。虽然它被应用在非常具体的“GPU内核优化”领域，但其核心贡献是提出了一种新的**多智能体协作与迭代优化的机制**。根据您的规则“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”，这篇论文应被保留。它展示了智能体如何通过协作和反馈循环在复杂任务中进行自我完善，这正是您关注的演化机制的体现。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是构建了一个新颖的多智能体协作框架（STARK），该框架通过规划、工具使用和迭代优化，实现了在GPU内核优化任务上的卓越表现。它直接对准了您研究课题中的“多智能体”和“自我演化”两个核心方向，是关于如何构建和改进LLM智能体本身的前沿研究。因此，它非常符合您的研究范围。"
    },
    {
        "index": "#33",
        "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI",
        "link": "/arxiv/2510.16720",
        "arxiv_id": "2510.16720",
        "authors": "Jitao Sang, Jinlin Xiao, Jiarun Han, Jilin Chen, Xiaoyi Chen, Shuyu Wei, Yongjie Sun, Yuhang Wang",
        "summary": "The rapid evolution of agentic AI marks a new phase in artificial intelligence, where Large Language Models (LLMs) no longer merely respond but act, reason, and adapt. This survey traces the paradigm shift in building agentic AI: from Pipeline-based systems, where planning, tool use, and memory are orchestrated by external logic, to the emerging Model-native paradigm, where these capabilities are internalized within the model's parameters. We first position Reinforcement Learning (RL) as the algorithmic engine enabling this paradigm shift. By reframing learning from imitating static data to outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task across language, vision and embodied domains. Building on this, the survey systematically reviews how each capability -- Planning, Tool use, and Memory -- has evolved from externally scripted modules to end-to-end learned behaviors. Furthermore, it examines how this paradigm shift has reshaped major agent applications, specifically the Deep Research agent emphasizing long-horizon reasoning and the GUI agent emphasizing embodied interaction. We conclude by discussing the continued internalization of agentic capabilities like Multi-agent collaboration and Reflection, alongside the evolving roles of the system and model layers in future agentic AI. Together, these developments outline a coherent trajectory toward model-native agentic AI as an integrated learning and interaction framework, marking the transition from constructing systems that apply intelligence to developing models that grow intelligence through experience.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-19",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.542382",
        "filter_reason": "这篇论文完全符合您的研究范围，是一篇高度相关的核心文献。我的判断过程如下： 1.  **第一步：核心判断——保留** 这篇论文的本质是一篇综述，其核心贡献并非提出一个全新的智能体，而是系统性地梳理和定义了LLM智能体的**演化范式**。它清晰地描绘了从“基于管道”到“模型原生”的范式转变，这直接对应了您研究目标中的“**演化LLM智能体**”。它不是将智能体作为工具应用到某个领域，而是深入探讨了智能体本身构建方法的演进，因此完全符合保留标准。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了您所有核心关注点的关键词和概念： *   **核心范式**: 明确提到了 `Agentic AI`，并讨论了其范式演进。 *   **智能体能力**: 系统性地回顾了 `Planning`、`Tool use` 和 `Memory` 这三大核心能力的演化。 *   **多智能体**: 在结论部分明确指出，未来的演化方向包括 `Multi-agent collaboration` 的内化。 *   **演化机制**: 整篇论文的主题就是“演化”，标题中的“Paradigm Shift”（范式转变）和摘要中的“evolution”、“grow intelligence through experience”（通过经验发展智能）等表述，与您的“自我演化”方向高度契合。它还提到了 `Reflection`（反思）作为未来的演化方向。 3.  **第三步：排除标准——未触发** 论文摘要完全没有提及安全、对齐、可解释性、水印或幻觉等排除性议题。虽然提到了 `vision` 和 `GUI agent`，但它们是作为“模型原生”范式应用的**例证领域**，而非研究的核心。论文的核心是**范式本身**，而不是视觉或多模态技术，这符合您“除非它们被用作智能体感知环境的工具，而不是研究的核心”的特殊规则。 4.  **第四步：处理特殊和模糊情况——完美契合** *   **推理/规划**: 论文明确将 `Planning` 作为核心能力之一，并回顾了其从外部模块到内部化行为的演化过程。这完全符合您对“智能体如何进行规划”的保留标准。 *   **自我演化的应用**: 这篇综述本身就是对“自我演化”这一宏大主题的宏观审视，它探讨了智能体如何从依赖外部逻辑演化为内部化、通过经验“成长”智能的模型。这比单一的应用案例更具基础性和前瞻性。 **最终决策**: 这篇论文是一篇关于“LLM智能体及其演化”的**纲领性综述**。它的核心贡献在于为整个领域的发展轨迹提供了一个清晰的框架和叙事，即从外部编排的“系统”到内部化学习的“模型”的演化。这与您的研究课题“LLM智能体及其演化”在宏观层面完美对齐。它不仅涵盖了单智能体的核心能力（规划、工具、记忆），还展望了多智能体协作和自我反思的演化，是理解该领域过去、现在和未来的关键文献。因此，必须保留。"
    },
    {
        "index": "#34",
        "title": "An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems",
        "link": "/arxiv/2510.16701",
        "arxiv_id": "2510.16701",
        "authors": "Ni Zhang, Zhiguang Cao, Jianan Zhou, Cong Zhang, Yew-Soon Ong",
        "summary": "Complex vehicle routing problems (VRPs) remain a fundamental challenge, demanding substantial expert effort for intent interpretation and algorithm design. While large language models (LLMs) offer a promising path toward automation, current approaches still rely on external intervention, which restrict autonomy and often lead to execution errors and low solution feasibility. To address these challenges, we propose an Agentic Framework with LLMs (AFL) for solving complex vehicle routing problems, achieving full automation from problem instance to solution. AFL directly extracts knowledge from raw inputs and enables self-contained code generation without handcrafted modules or external solvers. To improve trustworthiness, AFL decomposes the overall pipeline into three manageable subtasks and employs four specialized agents whose coordinated interactions enforce cross-functional consistency and logical soundness. Extensive experiments on 60 complex VRPs, ranging from standard benchmarks to practical variants, validate the effectiveness and generality of our framework, showing comparable performance against meticulously designed algorithms. Notably, it substantially outperforms existing LLM-based baselines in both code reliability and solution feasibility, achieving rates close to 100% on the evaluated benchmarks.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-19",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.542674",
        "filter_reason": "这篇论文符合您的研究范围，应被保留。我的判断过程如下： **第一步：核心判断——保留** 论文的核心贡献并非简单地应用LLM去解决车辆路径问题（VRP）这一特定领域问题。其本质是提出了一种**全新的、自主的“智能体框架”**。摘要明确指出，当前方法依赖外部干预，而他们的框架AFL旨在实现“从问题实例到解决方案的完全自动化”。这表明论文的重点是构建一个具有自主性的智能体系统，而非将其作为工具使用。因此，它不属于“非演化型应用”的排除范畴。 **第二步：正面指标——高度匹配** 论文包含了多个核心关注点，表明其与您的研究方向高度相关： 1.  **核心范式**: 标题和摘要中明确出现了 `Agentic Framework` 和 `LLMs`。 2.  **多智能体**: 这是最关键的一点。摘要中清晰地指出，该框架“ employs **four specialized agents whose coordinated interactions**...”（采用了四个专门化的智能体，它们之间进行协调互动）。这直接命中了您“多智能体”研究方向中的“协作”与“通信”。 3.  **智能体能力**: 框架涉及 `Planning`（将整体流程分解为三个可管理的子任务）和 `Tool Use`（实现自主的代码生成）。 **第三步：排除标准——未触及** 论文的主要贡献不是关于安全、对齐、可解释性或多模态。它专注于提升智能体解决任务的可靠性和可行性，这是对智能体框架本身的改进，而非安全或对齐研究。 **第四步：处理特殊和模糊情况——适用保留规则** 这篇论文的情况与“推理/规划”的保留规则完全吻合。它不是在研究如何提升LLM的基础数学或逻辑推理能力，而是在研究**一个智能体框架（AFL）如何进行规划和多步推理**来解决复杂的现实任务（VRP）。VRP在这里是作为验证框架有效性的一个复杂、有挑战性的**测试平台**，而不是研究的最终目的。 **第五步：最终决策** 综合以上分析，该论文的核心贡献在于**构建和设计了一个多智能体协作框架（AFL）**，以实现解决复杂任务的端到端自动化。它完全符合您“多智能体”研究方向中的“协作、通信”等子方向，并且其研究范式是关于智能体框架本身，而非特定领域的应用。因此，这篇论文与您的研究目标高度契合，应被保留。"
    },
    {
        "index": "#20",
        "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI",
        "link": "/arxiv/2510.17108",
        "arxiv_id": "2510.17108",
        "authors": "Yoonjin Lee, Munhee Kim, Hanbi Choi, Juhyeon Park, Seungho Lyoo, Woojin Park",
        "summary": "Despite advances in financial AI, the automation of evidence-based reasoning remains unresolved in corporate credit assessment, where qualitative non-financial indicators exert decisive influence on loan repayment outcomes yet resist formalization. Existing approaches focus predominantly on numerical prediction and provide limited support for the interpretive judgments required in professional loan evaluation. This study develops and evaluates two operational large language model (LLM)-based systems designed to generate structured reasoning from non-financial evidence. The first is a non-adversarial single-agent system (NAS) that produces bidirectional analysis through a single-pass reasoning pipeline. The second is a debate-based multi-agent system (KPD-MADS) that operationalizes adversarial verification through a ten-step structured interaction protocol grounded in Karl Popper's critical dialogue framework. Both systems were applied to three real corporate cases and evaluated by experienced credit risk professionals. Compared to manual expert reporting, both systems achieved substantial productivity gains (NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The KPD-MADS demonstrated superior reasoning quality, receiving higher median ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs. 3.0), and usability (62.5 vs. 52.5). These findings show that structured multi-agent interaction can enhance reasoning rigor and interpretability in financial AI, advancing scalable and defensible automation in corporate credit assessment.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-20",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.535898",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。 以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断** - **核心贡献**：这篇论文的核心贡献是构建并评估了一个新颖的基于LLM的多智能体系统（KPD-MADS）。该系统通过一个“十步结构化交互协议”来实现智能体间的对抗性验证和结构化辩论，以提升其在企业信用评估任务中的推理质量。 - **判断依据**：这完全符合“保留”标准。论文的本质是提出一种新的**多智能体方法论**，而不是简单地将LLM或已有框架应用到金融领域。它设计了一种新的智能体交互框架（KPD-MADS），并为了凸显其优势，还设计了一个更简单的单智能体基线系统（NAS）进行对比。因此，它不属于“非演化型应用”或“非Agentic的推理”。 2.  **第二步：正面指标** - 论文明确包含了多个核心正面指标： - **核心范式**: `LLM-based Agents`, `Multi-Agent Systems (MAS)` - **多智能体**: `Collaboration` (通过对抗性辩论实现), `Communication` (通过结构化交互协议实现) - **智能体能力**: `Reasoning` (证据推理) - 这些指标强烈表明论文与您的研究焦点（特别是多智能体方向）高度相关。 3.  **第三步：排除标准** - **安全与对齐**：摘要中提到了 \"interpretability\" (可解释性)。这是一个需要仔细辨别的点。然而，从摘要来看，可解释性是其提出的多智能体框架所带来的一个**效果和评估指标**（“enhance reasoning rigor and interpretability”），而非论文的研究核心。论文的核心是构建那个**能够**提升可解释性的多智能体框架本身，而不是研究一种新的可解释性技术。因此，它不触及“主要贡献是关于可解释性”的排除标准。 - **多模态与视觉**：论文内容不涉及，不相关。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**：该论文完美符合“保留”规则。KPD-MADS系统是一个典型的智能体多步推理框架，它通过多个智能体间的结构化辩论来协同完成复杂的推理任务，这属于“智能体如何进行规划或在复杂任务中进行多步推理”的范畴，而非改进LLM本身的基础推理能力。 5.  **第五步：最终决策** - 综合以上分析，该论文的核心是提出并验证了一种新的多智能体协作与推理范式（结构化辩论），属于Agentic AI中**多智能体系统**的前沿研究。尽管它被应用在金融这个特定领域，但其核心贡献在于构建和改进智能体本身的方法论，与您“构建、改进或演化LLM智能体”的核心目标高度一致。因此，最终判断为保留。"
    },
    {
        "index": "#45",
        "title": "RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile",
        "link": "/arxiv/2510.16392",
        "arxiv_id": "2510.16392",
        "authors": "Ao Tian, Yunfeng Lu, Xinxin Fan, Changhao Wang, Lanzhi Zhou, Yeyao Zhang, Yanfang Liu",
        "summary": "Personalized and continuous interactions are the key to enhancing user experience in today's large language model (LLM)-based conversational systems, however, the finite context windows and static parametric memory make it difficult to model the cross-session long-term user states and behavioral consistency. Currently, the existing solutions to this predicament, such as retrieval-augmented generation (RAG) and explicit memory systems, primarily focus on fact-level storage and retrieval, lacking the capability to distill latent preferences and deep traits from the multi-turn dialogues, which limits the long-term and effective user modeling, directly leading to the personalized interactions remaining shallow, and hindering the cross-session continuity. To realize the long-term memory and behavioral consistency for Language Agents in LLM era, we propose a self-evolving memory framework RGMem, inspired by the ideology of classic renormalization group (RG) in physics, this framework enables to organize the dialogue history in multiple scales: it first extracts semantics and user insights from episodic fragments, then through hierarchical coarse-graining and rescaling operations, progressively forms a dynamically-evolved user profile. The core innovation of our work lies in modeling memory evolution as a multi-scale process of information compression and emergence, which accomplishes the high-level and accurate user profiles from noisy and microscopic-level interactions.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-18",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.551453",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献精准地命中了“自我演化”和“单智能体”方向。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心是提出一个名为 **RGMem** 的 **“自我演化的记忆框架”**。其本质不是将现有技术应用于某个领域，而是构建一个全新的方法论，来解决语言智能体的长期记忆和行为一致性问题。这完全符合“构建、改进或演化 LLM智能体”的核心目标。它不属于“非演化型应用”，因为它提出了新的演化机制；也不属于“非Agentic的推理”，因为它聚焦于智能体的“记忆”这一核心能力。 2.  **第二步：正面指标——高度相关** 论文包含了多个核心关注点： *   **核心范式**: `Self-Evolving` (自我演化) 在标题和摘要中被反复强调，是论文的立意之本。`LLM-based Agents` (基于LLM的智能体) 也是其研究对象。 *   **智能体能力**: `Memory` (记忆) 是论文的核心。其提出的“从多轮对话中提炼潜在偏好和深层特质”的过程，本质上是一种高级的 `Self-Reflection` (自我反思)。 *   **演化机制**: 论文的标题和摘要明确提到了 `Memory Evolution` (记忆演化)，其核心创新点“将记忆演化建模为一个多尺度过程”，正是一种 `Iterative Improvement` (迭代改进) 和 `Self-Improvement` (自我完善) 的机制。 3.  **第三步：排除标准——未触发** 论文的研究焦点是智能体的记忆演化机制，没有涉及安全、对齐、可解释性等内容，也未涉及多模态或视觉技术。因此，它完全避开了您设定的排除标准。 4.  **第四步：处理特殊和模糊情况——精准命中例外规则** 这篇论文是“自我演化的应用”这一特殊情况的完美范例。虽然它的应用场景是“用户画像”，属于一个特定领域，但根据您的规则：“**如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。**” *   **核心贡献**: 论文的核心贡献是 **RGMem框架** 本身，即一种受物理学“重整化群”启发的、通过多尺度信息压缩来实现记忆演化的新**机制**。 *   **应用场景**: “用户画像”只是这个新机制的一个具体应用和验证场景。 因此，这篇论文完全符合您设定的“保留”例外情况。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一种创新的、用于LLM智能体的自我演化记忆框架。它直接对齐了您研究目标中的“自我演化”方向，并深化了“单智能体”方向中的“记忆”与“自我反思”能力。因此，这是一篇高度相关且应被保留的前沿论文。"
    },
    {
        "index": "#36",
        "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards",
        "link": "/arxiv/2510.16614",
        "arxiv_id": "2510.16614",
        "authors": "Xuan Zhang, Ruixiao Li, Zhijian Zhou, Long Li, Yulei Qin, Ke Li, Xing Sun, Xiaoyu Tan, Chao Qu, Yuan Qi",
        "summary": "Reinforcement Learning (RL) has become a compelling way to strengthen the multi step reasoning ability of Large Language Models (LLMs). However, prevalent RL paradigms still lean on sparse outcome-based rewards and limited exploration, which often drives LLMs toward repetitive and suboptimal reasoning patterns. In this paper, we study the central question of how to design exploration for LLM reasoning and introduce MERCI (Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that augments policy optimization with a principled intrinsic reward. Building on the idea of count-based exploration, MERCI leverages a lightweight Coin Flipping Network (CFN) to estimate the pseudo count and further epistemic uncertainty over reasoning trajectories, and converts them into an intrinsic reward that values novelty while preserving the learning signal from task rewards. We integrate MERCI into some advanced RL frameworks like Group Relative Policy Optimization (GRPO). Experiments on complex reasoning benchmarks demonstrate that MERCI encourages richer and more varied chains of thought, significantly improves performance over strong baselines, and helps the policy escape local routines to discover better solutions. It indicates that our targeted intrinsic motivation can make exploration reliable for language model reasoning.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-18",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.543381",
        "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **第一步：核心判断——保留** 论文的本质不是将现有智能体框架应用于某个特定领域，而是提出了一种**新的方法论（MERCI算法）**来改进LLM的推理过程。其核心目标是解决现有RL方法在训练LLM进行多步推理时遇到的“探索不足”和“模式固化”问题。这直接属于“构建、改进或演化LLM智能体”的范畴，特别是对智能体核心能力——**规划与推理**的改进。它不是非演化型应用，也不是非Agentic的推理，因为它关注的是智能体如何在推理空间中进行探索和策略优化，而非仅仅提升LLM基础的单步数学或逻辑能力。 2.  **第二步：正面指标——高度匹配** 论文内容与您的核心关注点高度契合： *   **智能体能力**: 论文的核心是改进LLM的**多步推理**能力，这与智能体的**规划**能力直接相关。摘要中提到的“reasoning trajectories”（推理轨迹）、“escape local routines to discover better solutions”（摆脱局部常规以发现更好的解决方案）都是智能体在复杂任务中进行规划和探索的典型表现。 *   **演化机制**: 整个算法基于强化学习，通过引入内在奖励来引导策略迭代优化，这是一种**迭代改进**的形式。它帮助智能体（即LLM策略）从有限的探索模式中演化出来，发现更优的推理路径。 3.  **第三步：排除标准——未触及** 论文的研究焦点纯粹在于提升LLM的推理性能和探索效率，完全不涉及安全、对齐、可解释性或多模态等排除领域。 4.  **第四步：处理特殊情况——明确符合** 根据“推理/规划”的特殊规则，这篇论文是典型的“保留”案例。 *   **保留**: 论文正是关于“智能体如何进行规划或在复杂任务中进行多步推理”的。MERCI可以被视为一个新的Agentic框架或对现有框架（如GRPO）的重要增强，它通过内在奖励机制来优化智能体的规划过程，使其能产生“更丰富多样的思维链”。 *   **排除**: 它并非简单地通过数据集或微调来提升LLM的基础数学能力，而是从智能体学习的角度，优化了其探索和决策的动态过程。 **总结**: 该论文的核心贡献是提出了一种新颖的算法（MERCI），通过内在奖励机制来增强LLM在推理任务中的探索能力，从而改进其多步规划和解决问题的能力。这直接对应了您研究目标中的“单智能体”方向，特别是“规划”这一子方向。因此，这是一篇高度相关且应被保留的前沿论文。"
    },
    {
        "index": "#56",
        "title": "Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration",
        "link": "/arxiv/2510.16194",
        "arxiv_id": "2510.16194",
        "authors": "Guanchen Wu, Zuhui Chen, Yuzhang Xie, Carl Yang",
        "summary": "Protected health information (PHI) de-identification is critical for enabling the safe reuse of clinical notes, yet evaluating and comparing PHI de-identification models typically depends on costly, small-scale expert annotations. We present TEAM-PHI, a multi-agent evaluation and selection framework that uses large language models (LLMs) to automatically measure de-identification quality and select the best-performing model without heavy reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each independently judging the correctness of PHI extractions and outputting structured metrics. Their results are then consolidated through an LLM-based majority voting mechanism that integrates diverse evaluator perspectives into a single, stable, and reproducible ranking. Experiments on a real-world clinical note corpus demonstrate that TEAM-PHI produces consistent and accurate rankings: despite variation across individual evaluators, LLM-based voting reliably converges on the same top-performing systems. Further comparison with ground-truth annotations and human evaluation confirms that the framework's automated rankings closely match supervised evaluation. By combining independent evaluation agents with LLM majority voting, TEAM-PHI offers a practical, secure, and cost-effective solution for automatic evaluation and best-model selection in PHI de-identification, even when ground-truth labels are limited.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.561838",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。 1.  **核心判断 (第一步):** 论文的核心贡献是提出了一个名为 \"TEAM-PHI\" 的 **多智能体评估和选择框架**。它不是简单地将一个已有的智能体框架应用到医疗领域，而是**构建了一个全新的多智能体系统**来解决“自动评估模型”这一特定问题。论文的本质是关于如何设计和组织多个智能体（Evaluation Agents）进行协作，并通过一种基于LLM的投票机制来整合结果。这完全符合“构建、改进LLM智能体”的核心目标，因此不属于“非演化型应用”的排除范畴。 2.  **正面指标 (第二步):** 论文明确包含了多个核心关注点。 *   **核心范式:** 标题和摘要中反复出现 `Multi-Agent Collaboration`，这直接命中了“多智能体”方向。 *   **多智能体:** 论文详细描述了多个 `Evaluation Agents` 的工作方式，它们独立工作并通过 `LLM-based majority voting mechanism` 进行协作与通信，这属于 `Collaboration` 和 `Communication` 的范畴。 3.  **排除标准 (第三步):** 论文不涉及任何排除标准。 *   **安全与对齐:** 虽然论文的应用背景是医疗信息安全（PHI去标识化），但论文的**核心贡献**是提出的多智能体评估框架，而不是一种新的安全或对齐技术。这里的“安全”是应用场景，而非研究方法本身。 *   **多模态与视觉:** 论文处理的是文本（临床笔记），不涉及视觉或多模态内容。 4.  **特殊情况和模糊情况 (第四步):** 此处不适用。 **最终决策 (第五步):** 综合来看，这篇论文的核心贡献在于提出了一种新颖的**多智能体协作框架**（TEAM-PHI），用于解决模型评估和选择问题。它详细阐述了多个智能体如何分工、独立评估、并通过投票机制进行协作与整合。这完全符合研究课题中“多智能体”这一核心方向，为智能体间的协作模式提供了新的方法论和实证研究。因此，这篇论文与你的研究范围高度相关，应该被保留。"
    },
    {
        "index": "#47",
        "title": "Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs",
        "link": "/arxiv/2510.16374",
        "arxiv_id": "2510.16374",
        "authors": "Nick Oh",
        "summary": "Current approaches to enhancing LLM reasoning follows two isolated paradigms: Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack mechanisms to verify whether selected strategies succeed; while Generate-Verify approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan et al., 2023) iteratively refine outputs but commence generation blindly without task assessment. This separation creates inefficiencies -- strategies fail without feedback, and refinement occurs without strategic grounding. We address this gap by implementing Flavell's cognitive monitoring model (1979) from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025), operationalising it as a three-phase iterative system. On GSM8K, preliminary results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37% increased inference cost. These initial findings suggest upfront monitoring produces higher-quality initial solutions that reduce refinement needs, though evaluation beyond arithmetic reasoning is needed to establish generalisability.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-18",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.552400",
        "filter_reason": "这篇论文完全符合你的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献并非将LLM应用于某个特定领域，而是提出了一种全新的、结构化的LLM推理框架——`Monitor-Generate-Verify`。这个框架是一个方法论上的创新，旨在改进LLM的自主推理过程。它通过引入一个前置的“监控”阶段，将现有的“规划-生成”和“生成-验证”两种范式融合成一个统一的迭代系统。这本质上是在**构建和改进一个LLM智能体的推理循环**，因此完全符合“构建、改进或演化LLM智能体”的核心目标。它不属于非演化型应用，也不仅仅是提升LLM的基础推理能力，而是设计了一个更高层次的智能体工作流。 2.  **第二步：正面指标** - 论文命中了多个核心正面指标，表明它与你的研究焦点高度相关： - **智能体能力**: 论文的核心是关于`Planning`（规划，体现在Monitor-Generate阶段）和`Self-Correction`/`Self-Reflection`（自我纠正/反思，体现在Verify阶段和整个迭代循环中）。它明确与`SELF-REFINE`等自我反思框架进行对比和改进。 - **演化机制**: 论文提出的“三阶段迭代系统”本身就是一种`Iterative Improvement`（迭代改进）机制。通过监控、生成、验证的循环，智能体能够不断优化其输出，这属于自我演化的范畴。 3.  **第三步：排除标准** - 论文未触发任何排除标准。其研究焦点是提升推理框架的效率和效果，而非安全、对齐或多模态问题。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这是判断的关键。根据规则，应“保留”关于“智能体如何进行规划或在复杂任务中进行多步推理”的论文。本文提出的`Monitor-Generate-Verify`框架正是一个典型的智能体多步推理框架。它超越了简单的Chain-of-Thought（CoT）提示，构建了一个包含任务评估、策略制定、执行和验证的完整、结构化的自主循环。这与ReAct、ToT等Agentic框架在精神上是完全一致的，都属于智能体核心能力的构建。 **核心依据总结**: 该论文的核心贡献是提出了一种新的LLM智能体推理框架（`Monitor-Generate-Verify`），旨在通过结构化的迭代循环来增强智能体的规划和自我纠正能力。这直接对应了你研究范围中的“单智能体”方向，特别是“规划”和“自我反思”这两个子方向。它不是对LLM基础能力的微调，也不是一个简单的应用，而是一个关于“如何构建更智能的智能体”的方法论创新，因此完全符合你的筛选要求。"
    },
    {
        "index": "#67",
        "title": "PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency",
        "link": "/arxiv/2510.15966",
        "arxiv_id": "2510.15966",
        "authors": "Shian Jia, Ziyang Huang, Xinbo Wang, Haofei Zhang, Mingli Song",
        "summary": "Memory systems are fundamental to AI agents, yet existing work often lacks adaptability to diverse tasks and overlooks the constructive and task-oriented role of AI agent memory. Drawing from Piaget's theory of cognitive development, we propose PISA, a pragmatic, psych-inspired unified memory system that addresses these limitations by treating memory as a constructive and adaptive process. To enable continuous learning and adaptability, PISA introduces a trimodal adaptation mechanism (i.e., schema updation, schema evolution, and schema creation) that preserves coherent organization while supporting flexible memory updates. Building on these schema-grounded structures, we further design a hybrid memory access architecture that seamlessly integrates symbolic reasoning with neural retrieval, significantly improving retrieval accuracy and efficiency. Our empirical evaluation, conducted on the existing LOCOMO benchmark and our newly proposed AggQA benchmark for data analysis tasks, confirms that PISA sets a new state-of-the-art by significantly enhancing adaptability and long-term knowledge retention.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-12",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.572297",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献与你的研究目标高度契合。以下是我的详细判断过程： 1.  **第一步：核心判断——保留** 这篇论文的本质是提出一个新的框架（PISA），旨在**构建和改进LLM智能体的核心组件——记忆系统**。摘要明确指出，现有工作缺乏适应性，而PISA通过一个受心理学启发的统一记忆系统来解决这一问题。它不是将现有智能体作为工具去解决某个特定领域的问题，而是直接对智能体本身进行架构层面的创新和改进。因此，论文的核心是构建和改进Agentic LLM，符合“保留”标准。 2.  **第二步：正面指标——高度相关** 论文包含了大量你的核心关注点： *   **核心范式**: 论文标题和摘要中反复强调 “AI Agency”，表明其研究范式完全是 Agentic AI。 *   **智能体能力**: 论文的**核心贡献是关于“Memory”**，这直接命中了你列出的单智能体关键能力之一。一个先进的记忆系统是规划、自我反思等能力的基础。 *   **演化机制**: 这是最关键的亮点。论文提出了一个**“三模态适应机制”（模式更新、模式演化、模式创建）**。这完美对应了你的“自我演化”方向，尤其是其中的 “Self-Improvement”, “Self-Refine”, 和 “Iterative Improvement”。这个机制让智能体能够根据经验动态地调整和完善其内部知识结构，这正是自我演化的核心思想。 3.  **第三步：排除标准——不触及** 论文的主要贡献在于提升智能体的认知架构（记忆），而非安全、对齐或多模态。虽然更好的记忆可能会间接减少幻觉，但论文的焦点是“适应性”和“知识保留”，而不是解决幻觉问题本身。因此，它避开了所有排除标准。 4.  **第四步：特殊/模糊情况处理** 论文的贡献可以被看作是一种特殊的**自我演化机制**。它提出的“模式演化”和“模式创建”机制，使得智能体的记忆结构能够随时间和任务进行迭代和升级。根据你的规则“即使它被应用在特定领域（如‘用于化学实验的自我演化智能体’），也应该保留”，这篇论文提出的“自我演化”记忆机制是普适性的，并且在数据分析任务上进行了验证，更应该被保留。 5.  **第五步：最终决策** **综合判断：保留**。 **核心依据**：这篇论文的核心贡献是**为LLM智能体设计了一个全新的、具备自我演化能力的统一记忆系统（PISA）**。这直接回应了你研究范围中的**“单智能体”**（特别是“记忆”子方向）和**“自我演化”**两大核心方向。其提出的“三模态适应机制”是实现智能体自我完善和迭代的一个创新性方法论，完全符合你筛选“核心贡献在于构建、改进或演化LLM智能体”的论文的目标。"
    },
    {
        "index": "#69",
        "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding",
        "link": "/arxiv/2510.15952",
        "arxiv_id": "2510.15952",
        "authors": "Myung Ho Kim",
        "summary": "Large language models exhibit intelligence without genuine epistemic understanding, exposing a key gap: the absence of epistemic architecture. This paper introduces the Structured Cognitive Loop (SCL) as an executable epistemological framework for emergent intelligence. Unlike traditional AI research asking \"what is intelligence?\" (ontological), SCL asks \"under what conditions does cognition emerge?\" (epistemological). Grounded in philosophy of mind and cognitive phenomenology, SCL bridges conceptual philosophy and implementable cognition. Drawing on process philosophy, enactive cognition, and extended mind theory, we define intelligence not as a property but as a performed process -- a continuous loop of judgment, memory, control, action, and regulation. SCL makes three contributions. First, it operationalizes philosophical insights into computationally interpretable structures, enabling \"executable epistemology\" -- philosophy as structural experiment. Second, it shows that functional separation within cognitive architecture yields more coherent and interpretable behavior than monolithic prompt based systems, supported by agent evaluations. Third, it redefines intelligence: not representational accuracy but the capacity to reconstruct its own epistemic state through intentional understanding. This framework impacts philosophy of mind, epistemology, and AI. For philosophy, it allows theories of cognition to be enacted and tested. For AI, it grounds behavior in epistemic structure rather than statistical regularity. For epistemology, it frames knowledge not as truth possession but as continuous reconstruction within a phenomenologically coherent loop. We situate SCL within debates on cognitive phenomenology, emergence, normativity, and intentionality, arguing that real progress requires not larger models but architectures that realize cognitive principles structurally.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.573241",
        "filter_reason": "这篇论文完全符合你的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断 (保留)** - 论文的核心贡献是提出了一个名为“结构化认知循环”（SCL）的新框架。摘要明确指出，这是一个“可执行的认识论框架”和“认知架构”，旨在解决LLM缺乏“认识论架构”的问题。 - 这直接命中了你筛选标准的核心：**“构建、改进或演化 LLM智能体的方法论或新框架”**。它不是将现有智能体作为工具去解决某个领域问题，而是在智能体的根本架构层面进行创新。 2.  **第二步：正面指标 (高度相关)** - 论文包含了多个核心关注点： - **核心范式**: 它本质上是一个 `LLM-based Agent` 的架构研究。 - **智能体能力**: 摘要中描述的SCL是一个包含“判断、记忆、控制、行动、调节”的连续循环。这直接对应了智能体的 `Planning` (规划/判断)、`Memory` (记忆) 和 `Action` (行动) 能力。此外，其目标“重建其自身认识论状态”的能力，与 `Self-Reflection` (自我反思) 和 `Self-Correction` (自我修正) 高度相关。 - **演化机制**: 论文强调智能体通过这个结构化的循环来“持续重建”其知识状态，这是一种内在的、结构化的自我完善和迭代机制，符合 `Self-Improvement` 和 `Iterative Improvement` 的精神。 3.  **第三步：排除标准 (未触发)** - 论文虽然提到了“interpretable behavior”（可解释的行为），但其主要贡献是**SCL架构本身**，可解释性是这个架构带来的一个积极**结果**，而非研究的核心方法或目标。根据你的规则“只要论文的主要贡献是关于...一律排除”，这篇论文的主要贡献是架构，因此不被排除。 - 论文不涉及安全、对齐、视觉等多模态内容。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文讨论的“判断”和“控制”是智能体在复杂任务中进行多步推理和规划的核心组成部分，而非提升LLM基础的数学或逻辑能力。因此，它符合“保留”的条件。 **总结**: 该论文的核心是提出一个全新的、基于哲学认知理论的LLM智能体架构（SCL），旨在通过结构化的认知循环（包含记忆、规划、行动、反思等）来构建具有更连贯、可解释行为的智能体。这完全契合你“构建、改进或演化LLM智能体”的核心目标，特别是在“单智能体”的规划、记忆和自我反思方向上做出了理论和方法论上的贡献。因此，这是一篇高度相关的前沿论文。"
    },
    {
        "index": "#267",
        "title": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning",
        "link": "/arxiv/2510.16156",
        "arxiv_id": "2510.16156",
        "authors": "Yueqian Lin, Zhengmian Hu, Jayakumar Subramanian, Qinsi Wang, Nikos Vlassis, Hai \"Helen\" Li, Yiran Chen",
        "summary": "Effective human-AI collaboration on complex reasoning tasks requires that users understand and interact with the model's process, not just receive an output. However, the monolithic text from methods like Chain-of-Thought (CoT) prevents this, as current interfaces lack real-time verbalization and robust user barge-in. We present AsyncVoice Agent, a system whose asynchronous architecture decouples a streaming LLM backend from a conversational voice frontend. This design allows narration and inference to run in parallel, empowering users to interrupt, query, and steer the model's reasoning process at any time. Objective benchmarks show this approach reduces interaction latency by more than 600x compared to monolithic baselines while ensuring high fidelity and competitive task accuracy. By enabling a two-way dialogue with a model's thought process, AsyncVoice Agent offers a new paradigm for building more effective, steerable, and trustworthy human-AI systems for high-stakes tasks.",
        "subjects": "Audio and Speech Processing, Artificial Intelligence, Multimedia",
        "date": "2025-10-17",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.784671",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建和改进LLM智能体。我的判断依据如下： 1.  **核心判断 (第一步):** *   **保留:** 论文的核心是提出一个名为“AsyncVoice Agent”的**新系统/框架**。这个框架的本质不是应用，而是对LLM智能体交互模式的根本性改进。它通过一种异步架构，将LLM的推理过程实时、可交互地呈现给用户，并允许用户随时介入和引导。这直接属于“构建、改进LLM智能体”的范畴。 *   **排除:** 它不是将已有智能体应用于特定领域（如医疗、金融），也不是研究LLM本身的基础数学或逻辑推理能力，更不是关于模型部署的基础设施。它的“架构”是为了实现一个核心的Agentic能力（可交互性），而非单纯的工程优化。 2.  **正面指标 (第二步):** *   论文明确符合多个核心关注点：`Agentic AI`, `LLM-based Agents`, `Planning`（规划）, `Reasoning`（推理）。 *   其关键创新点——“让用户能够中断、查询和引导模型的推理过程”——是对智能体**交互式规划与推理**能力的重大增强。这与ReAct等允许智能体与环境（在此案例中是用户）交互的范式高度一致。 3.  **排除标准 (第三步):** *   论文虽然提到了“trustworthy”（值得信赖），但其实现路径是通过**过程透明和用户可控**，而非通过安全约束、对齐或可解释性技术（如XAI）来分析模型内部。因此，它不属于被排除的“安全与对齐”类别。 *   论文不涉及视觉或多模态内容。 4.  **特殊和模糊情况 (第四步):** *   **推理/规划:** 这篇论文是“关于智能体如何进行规划或在复杂任务中进行多步推理”的完美范例。它没有停留在静态的Chain-of-Thought输出，而是构建了一个动态的、可被外部（用户）干预的推理框架。这完全符合保留条件。 **总结:** 论文的**核心贡献**是“AsyncVoice Agent”这一**新的LLM智能体框架**，它通过创新的系统设计，极大地增强了智能体在规划和推理过程中的**可交互性、可引导性和透明度**。这直接对齐了您研究目标中的“单智能体”方向，特别是关于智能体规划与推理能力的子方向。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#280",
        "title": "TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration",
        "link": "/arxiv/2510.16080",
        "arxiv_id": "2510.16080",
        "authors": "Kerem Delikoyun, Qianyu Chen, Win Sen Kuan, John Tshon Yit Soong, Matthew Edward Cove, Oliver Hayden",
        "summary": "Emergency departments worldwide face rising patient volumes, workforce shortages, and variability in triage decisions that threaten the delivery of timely and accurate care. Current triage methods rely primarily on vital signs, routine laboratory values, and clinicians' judgment, which, while effective, often miss emerging biological signals that could improve risk prediction for infection typing or antibiotic administration in acute conditions. To address this challenge, we introduce TriAgent, a large language model (LLM)-based multi-agent framework that couples automated biomarker discovery with deep research for literature-grounded validation and novelty assessment. TriAgent employs a supervisor research agent to generate research topics and delegate targeted queries to specialized sub-agents for evidence retrieval from various data sources. Findings are synthesized to classify biomarkers as either grounded in existing knowledge or flagged as novel candidates, offering transparent justification and highlighting unexplored pathways in acute care risk stratification. Unlike prior frameworks limited to existing routine clinical biomarkers, TriAgent aims to deliver an end-to-end framework from data analysis to literature grounding to improve transparency, explainability and expand the frontier of potentially actionable clinical biomarkers. Given a user's clinical query and quantitative triage data, TriAgent achieved a topic adherence F1 score of 55.7 +/- 5.0%, surpassing the CoT-ReAct agent by over 10%, and a faithfulness score of 0.42 +/- 0.39, exceeding all baselines by more than 50%. Across experiments, TriAgent consistently outperformed state-of-the-art LLM-based agentic frameworks in biomarker justification and literature-grounded novelty assessment. We share our repo: https://github.com/CellFace/TriAgent.",
        "subjects": "Quantitative Methods, Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.795415",
        "filter_reason": "这篇论文符合您的研究范围，应当保留。我的判断过程如下： 1.  **第一步：核心判断** - 论文的核心贡献是构建了一个名为 **TriAgent** 的新颖的 **LLM-based Multi-Agent Framework**。它并非简单地将现有LLM或智能体框架作为工具应用于医疗领域，而是**提出了一种新的多智能体协作架构和方法论**来解决复杂问题。 - 摘要中明确描述了其架构：“employs a supervisor research agent to generate research topics and delegate targeted queries to specialized sub-agents”，这清晰地展示了其在**构建和改进多智能体系统**方面的核心贡献。 - 因此，它不属于“非演化型应用”的排除范畴，应予以**保留**。 2.  **第二步：正面指标** - 论文包含了大量核心关注点： - **核心范式**: `LLM-based Agents`, `Multi-Agent Systems (MAS)` - **多智能体**: `Collaboration` (协作), `Communication` (通信，体现在supervisor与sub-agents的交互中) - 论文还将自己的方法与 `CoT-ReAct agent` 进行了比较，这进一步表明其研究焦点在于Agentic框架的改进，属于您关注的核心领域。 3.  **第三步：排除标准** - **安全与对齐**: 论文提到了“transparency, explainability”（透明度、可解释性）。但是，这些是作为其多智能体框架带来的**结果或优点**，而不是论文的**主要研究贡献**。论文的核心是“如何通过多智能体协作完成任务”，而不是“如何实现一种新的可解释性技术”。因此，这不触发排除规则。 - **多模态与视觉**: 论文未涉及。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: TriAgent框架的工作流程（生成主题 -> 委托查询 -> 检索证据 -> 综合分类）是一个典型的多步、复杂的规划与推理过程。它属于“智能体如何进行规划或在复杂任务中进行多步推理”的范畴，因此符合保留条件。 5.  **第五步：最终决策** - 综合来看，这篇论文的本质是提出了一种创新的**多智能体协作框架**来解决特定领域的复杂问题。其核心贡献在于**智能体架构的设计与协作机制的实现**，完全符合您研究课题中的“多智能体”方向。尽管其应用场景是医疗领域的生物标志物发现，但这并不妨碍其作为一篇关于Agentic AI方法论的前沿论文的价值。因此，最终判断为**符合要求**。"
    },
    {
        "index": "#342",
        "title": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination",
        "link": "/arxiv/2510.15949",
        "arxiv_id": "2510.15949",
        "authors": "Charidimos Papadakis, Angeliki Dimitriou, Giorgos Filandrianos, Maria Lymperaiou, Konstantinos Thomas, Giorgos Stamou",
        "summary": "Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.",
        "subjects": "Trading and Market Microstructure, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.849620",
        "filter_reason": "这篇论文完全符合您的研究范围，应当被保留。我的判断过程如下： **第一步：核心判断——这篇论文的本质是什么？** 这篇论文的本质是**保留**。它并非简单地将LLM作为工具应用于金融领域，而是提出了一个全新的、旨在解决LLM智能体在复杂环境下适应性问题的方法论。论文的核心贡献是： 1.  **ATLAS框架**：一个整合了多智能体协作的统一框架。 2.  **Adaptive-OPRO**：一种新颖的、使智能体能够通过实时反馈动态调整自身指令的提示优化技术。 这属于典型的“构建、改进或演化LLM智能体”的研究，而非“非演化型应用”。其研究重点是智能体本身的机制和演化，而不仅仅是交易这个应用场景。 **第二步：正面指标——论文是否包含我的核心关注点？** 该论文命中了多个核心关注点，相关性极高： *   **多智能体**: 论文标题和摘要都明确提出了`Multi-Agent`框架（ATLAS），涉及智能体间的协作与信息整合。 *   **自我演化**: 这是论文最突出的亮点。`Adaptive-OPRO`技术通过整合实时反馈来`动态调整提示`，实现了`increasing performance over time`，这完全符合`Self-Evolving`、`Self-Improvement`和`Iterative Improvement`的定义。 *   **智能体能力**: 论文探讨了如何让智能体`synthesize heterogeneous information streams`（整合信息，一种高级记忆/感知能力）并产生`executable market actions`（工具使用/行动能力）。它还对比了`reflection-based feedback`，表明其研究范围触及了`Self-Reflection`和`Self-Correction`。 **第三步：排除标准——是否为我的研究焦点之外？** 该论文没有触及任何排除标准。它的核心贡献不是关于安全、对齐、可解释性，也不涉及多模态或视觉模型。 **第四步：处理特殊和模糊情况** 这篇论文是“自我演化的应用”这一特殊情况的完美范例。根据您的规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域（如‘用于化学实验的自我演化智能体’），也应该保留。” 本文的核心贡献是`Adaptive-OPRO`这一**新的自我演化机制**，金融交易是其应用和验证的领域。因此，尽管领域特定，但其方法论贡献是通用且前沿的，完全符合您的收录标准。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一种新颖的多智能体框架和一种关键的自我演化机制（动态提示优化）。它直接命中了您研究课题中的“多智能体”和“自我演化”两个核心方向。虽然应用场景是金融，但其研究焦点和方法论创新完全聚焦于LLM智能体本身的构建与演化，是您课题下的高质量前沿论文。因此，最终判断为 **True**。"
    },
    {
        "index": "#353",
        "title": "FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures",
        "link": "/arxiv/2510.15906",
        "arxiv_id": "2510.15906",
        "authors": "Yunsheng Bai, Ghaith Bany Hamad, Chia-Tung Ho, Syed Suhaib, Haoxing Ren",
        "summary": "Debugging formal verification (FV) failures represents one of the most time-consuming bottlenecks in modern hardware design workflows. When properties fail, engineers must manually trace through complex counter-examples spanning multiple cycles, analyze waveforms, and cross-reference design specifications to identify root causes - a process that can consume hours or days per bug. Existing solutions are largely limited to manual waveform viewers or simple automated tools that cannot reason about the complex interplay between design intent and implementation logic. We present FVDebug, an intelligent system that automates root-cause analysis by combining multiple data sources - waveforms, RTL code, design specifications - to transform failure traces into actionable insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis that structures failure traces into directed acyclic graphs, (2) Graph Scanner using batched Large Language Model (LLM) analysis with for-and-against prompting to identify suspicious nodes, and (3) Insight Rover leveraging agentic narrative exploration to generate high-level causal explanations. FVDebug further provides concrete RTL fixes through its Fix Generator. Evaluated on open benchmarks, FVDebug attains high hypothesis quality and strong Pass@k fix rates. We further report results on two proprietary, production-scale FV counterexamples. These results demonstrate FVDebug's applicability from academic benchmarks to industrial designs.",
        "subjects": "Hardware Architecture, Artificial Intelligence",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-21T11:00:07.853342",
        "filter_reason": "这篇论文符合您的研究范围，核心判断依据如下： 1.  **核心判断 (第一步): 论文的核心贡献是构建了一个新的LLM智能体框架，而非简单的应用。** - 尽管论文的应用领域是“硬件设计中的形式化验证调试”，这看似是一个特定领域的应用，但它并未被排除。关键在于，论文的主要贡献**不是**将一个已有的智能体框架（如ReAct, Auto-GPT）直接应用到此领域，而是**提出并构建了一个名为FVDebug的全新智能系统**。 - 该系统的核心创新点在于其新颖的pipeline，特别是“Insight Rover leveraging agentic narrative exploration”这一部分。这表明论文的本质是探索一种**新的智能体工作范式**（agentic narrative exploration），用以解决复杂问题。这完全符合“核心贡献在于构建、改进LLM智能体”的保留标准。 2.  **正面指标 (第二步): 论文明确包含了您关注的核心Agentic能力。** - **核心范式**: 摘要中直接使用了 `agentic narrative exploration` 这一关键术语，清晰地表明了其Agentic AI的定位。 - **智能体能力**: - **规划/推理**: 整个pipeline（因果图合成 -> 图扫描 -> 洞察探索 -> 修复生成）是一个复杂的多步推理和规划过程，用于解决一个非平凡的复杂任务。 - **工具使用**: 智能体组合并使用了多种数据源（波形、RTL代码、设计规范）作为其分析和推理的“工具”。 - **自我反思**: “for-and-against prompting”是一种显式的自我反思和批判性思维机制，让LLM从正反两方面评估一个假设，这属于智能体高级认知能力的范畴。 - **行动**: “Fix Generator”模块表明智能体不仅能分析问题，还能生成具体的解决方案（RTL修复代码），这是智能体执行具体行动的体现。 3.  **排除标准 (第三步): 论文不触及硬性排除项。** - 论文的主要贡献不是关于安全、对齐、可解释性或多模态视觉。因此，它没有被第三步的排除规则所排除。 4.  **特殊与模糊情况 (第四步): 明确符合保留条件。** - **推理/规划**: 论文的“agentic narrative exploration”和整个pipeline是典型的**智能体如何进行规划和多步推理**的实例，远超于提升LLM基础数学或逻辑能力的范畴。因此应被保留。 - **自我演化的应用**: 虽然这不属于自我演化论文，但它符合第四条规则的精神：即使应用在特定领域（硬件），只要其核心是提出一种新的智能体机制，就应该保留。 **总结**: 该论文的核心是提出了一种名为“agentic narrative exploration”的新颖LLM智能体工作模式，并将其封装在FVDebug系统中，用于自动化地完成复杂的硬件调试任务。它的贡献在于**智能体方法论的创新**，而非特定领域的应用本身。因此，它精准地契合了您关于“构建、改进LLM智能体”的核心研究目标。"
    }
]