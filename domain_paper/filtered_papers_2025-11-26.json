[
    {
        "index": "#1",
        "title": "BAMAS: Structuring Budget-Aware Multi-Agent Systems",
        "link": "/arxiv/2511.21572",
        "arxiv_id": "2511.21572",
        "authors": "Liming Yang, Junyu Luo, Xuanzhe Liu, Yiling Lou, Zhenpeng Chen",
        "summary": "Large language model (LLM)-based multi-agent systems have emerged as a powerful paradigm for enabling autonomous agents to solve complex tasks. As these systems scale in complexity, cost becomes an important consideration for practical deployment. However, existing work rarely addresses how to structure multi-agent systems under explicit budget constraints. In this paper, we propose BAMAS, a novel approach for building multi-agent systems with budget awareness. BAMAS first selects an optimal set of LLMs by formulating and solving an Integer Linear Programming problem that balances performance and cost. It then determines how these LLMs should collaborate by leveraging a reinforcement learning-based method to select the interaction topology. Finally, the system is instantiated and executed based on the selected agents and their collaboration topology. We evaluate BAMAS on three representative tasks and compare it with state-of-the-art agent construction methods. Results show that BAMAS achieves comparable performance while reducing cost by up to 86%.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-11-26",
        "category": "cs.MA",
        "crawl_time": "2025-11-27T11:00:03.689590",
        "filter_reason": "该论文的核心贡献是提出了一种名为BAMAS的新方法，用于构建具有预算感知的多智能体系统。这完全符合你筛选标准中的“保留”类别，即论文的核心是关于构建和改进LLM智能体（特别是多智能体系统）的方法论。 具体分析如下： 1.  **第一步：核心判断** - 论文的核心是**构建和改进多智能体系统**。它不是将已有的智能体框架应用到某个垂直领域，而是提出了一种新的、在预算约束下构建多智能体系统的结构化方法。因此，它属于“保留”范畴，排除了“非演化型应用”和“基础设施”等类别。 2.  **第二步：正面指标** - 论文包含了多个核心关注点。 *   **核心范式**: 明确提到了 `Multi-Agent Systems (MAS)`。 *   **多智能体**: 论文的核心是解决智能体间的协作问题，通过“reinforcement learning-based method to select the interaction topology”来确定智能体如何协作，这直接对应了 `Collaboration` 和 `Communication`。 3.  **第三步：排除标准** - 论文不涉及任何安全对齐或多模态相关的研究，因此没有触发排除标准。 4.  **第四步：特殊和模糊情况** - 本论文不涉及推理/规划的内部机制或自我演化，因此这些特殊情况不直接适用。但其重点在于多智能体系统的**构建和结构优化**，这本身就是你研究目标“构建、改进或演化LLM智能体”的一部分。 **最终决策**：该论文提出了一种新颖的框架（BAMAS）来优化多智能体系统的构建过程，其核心贡献在于方法论创新，旨在解决多智能体系统在规模化部署时的成本和效率问题。这直接命中了你的研究焦点“多智能体”，属于对智能体协作和系统构建的改进。因此，这篇论文完全符合你的研究范围。"
    },
    {
        "index": "#9",
        "title": "$A^2Flow:$ Automating Agentic Workflow Generation via Self-Adaptive Abstraction Operators",
        "link": "/arxiv/2511.20693",
        "arxiv_id": "2511.20693",
        "authors": "Mingming Zhao, Xiaokang Wei, Yuanqi Shao, Kaiwen Zhou, Lin Yang, Siwei Rao, Junhui Zhan, Zhitang Chen",
        "summary": "Large language models (LLMs) have shown strong potential in automating the design of agentic workflows. However, existing methods still rely heavily on manually predefined operators, limiting generalization and scalability. To address this issue, we propose $A^2Flow$, a fully automated framework for agentic workflow generation based on self-adaptive abstraction operators. $A^2Flow$ employs a three-stage operator extraction process: 1) Case-based Initial Operator Generation: leveraging expert demonstrations and LLM reasoning to generate case-specific operators; 2) Operator Clustering and Preliminary Abstraction: grouping similar operators across tasks to form preliminary abstractions; and 3) Deep Extraction for Abstract Execution Operators: applying long chain-of-thought prompting and multi-path reasoning to derive compact and generalizable execution operators. These operators serve as reusable building blocks for workflow construction without manual predefinition. Furthermore, we enhance node-level workflow search with an operator memory mechanism, which retains historical outputs to enrich context and improve decision-making. Experiments on general and embodied benchmarks show that $A^2Flow$ achieves a 2.4\\% and 19.3\\% average performance improvement and reduces resource usage by 37\\% over state-of-the-art baselines. Homepage:https://github.com/pandawei-ele/A2FLOW",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-11-23",
        "category": "cs.MA",
        "crawl_time": "2025-11-27T11:00:03.691870",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为 `$A^2Flow$` 的新框架，其核心贡献在于**自动化地生成和改进LLM智能体的工作流**。这直接对应了您研究目标中的“构建、改进或演化 LLM智能体”。它不是将现有智能体作为工具去解决某个特定领域的问题，而是专注于智能体本身的构建方法论，因此不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标** - 论文包含了大量核心关注点： - **核心范式**: 论文标题和摘要明确提到了 `Agentic Workflow Generation`，这属于 `Agentic AI` 和 `LLM-based Agents` 的核心。 - **智能体能力**: 论文的核心是生成工作流，这直接关联到 `Planning`（规划）。同时，它提出的 `operator memory mechanism` 明确对应了 `Memory`（记忆）能力。 - **演化机制**: 论文的核心创新点 `Self-Adaptive Abstraction Operators`（自适应抽象算子）以及其三阶段的算子提取过程，本质上是一种让智能体系统从具体案例中学习、泛化并形成可复用高级组件的机制。这完全符合 `Self-Evolving`（自我演化）和 `Self-Improvement`（自我完善）的定义，即系统通过经验（案例演示）和环境反馈（任务表现）来迭代和优化自身的工作流构建能力。 3.  **第三步：排除标准** - 论文的主要贡献不涉及 `Safety`、`Alignment`、`Interpretability` 等安全与对齐问题。 - 论文虽然可能在“具身基准测试”上进行评估，但其核心贡献并非视觉或多模态模型本身，而是生成工作流的框架。视觉只是智能体在特定环境中可能使用的一种工具，而非研究焦点。因此，它不触犯多模态与视觉的排除规则。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文明确是关于智能体如何进行规划（`Agentic Workflow Generation`），属于应保留的范畴。它不是在提升LLM的基础数学或逻辑推理能力，而是在构建一个更高层次的、用于任务规划和执行的智能体框架。 - **自我演化的应用**: 这篇论文的核心就是提出一种新的“自我演化/适应”机制（`Self-Adaptive Abstraction`），因此即使它应用在特定领域，也应保留。而本文是在通用和具身基准上验证，更符合要求。 **最终决策**: 综合以上分析，该论文的核心贡献是提出了一种新颖的、自动化的、自适应的框架来构建和演化LLM智能体的工作流。它直接触及了您研究课题的“单智能体”和“自我演化”两个核心方向，并且不涉及任何排除标准。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#6",
        "title": "Prune4Web: DOM Tree Pruning Programming for Web Agent",
        "link": "/arxiv/2511.21398",
        "arxiv_id": "2511.21398",
        "authors": "Jiayuan Zhang, Kaiquan Chen, Zhihao Lu, Enshen Zhou, Qian Yu, Jing Zhang",
        "summary": "Web automation employs intelligent agents to execute high-level tasks by mimicking human interactions with web interfaces. Despite the capabilities of recent Large Language Model (LLM)-based web agents, navigating complex, real-world webpages efficiently remains a significant hurdle due to the prohibitively large size of Document Object Model (DOM) structures, often ranging from 10,000 to 100,000 tokens. Existing strategies typically rely on crude DOM truncation -- risking the loss of critical information -- or employ inefficient heuristics and separate ranking models, failing to achieve an optimal balance between precision and scalability. To address these challenges, we introduce Prune4Web, a novel paradigm that shifts DOM processing from resource-intensive LLM reading to efficient programmatic pruning. Central to our approach is DOM Tree Pruning Programming, where an LLM generates executable Python scoring scripts to dynamically filter DOM elements based on semantic cues from decomposed sub-tasks. This mechanism eliminates the need for LLMs to ingest raw, massive DOMs, instead delegating traversal and scoring to lightweight, interpretable programs. This methodology achieves a 25x to 50x reduction in candidate elements for grounding, thereby facilitating precise action localization while mitigating attention dilution. Furthermore, we propose a specialized data annotation pipeline and a two-turn dialogue training strategy that jointly optimizes the Planner, Programmatic Filter, and Grounder within a unified framework. Extensive experiments demonstrate state-of-the-art performance. Notably, on our low-level grounding task, Prune4Web dramatically improves accuracy from 46.8% to 88.28%, underscoring its efficacy in real-world web automation.",
        "subjects": "Artificial Intelligence, Computation and Language, Human-Computer Interaction, Multiagent Systems",
        "date": "2025-11-26",
        "category": "cs.MA",
        "crawl_time": "2025-11-27T11:00:03.691043",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于**构建和改进LLM智能体**。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将LLM智能体应用于网页自动化领域，而是针对现有LLM Web智能体在处理复杂网页时效率低下的核心痛点，提出了一种全新的方法论和框架——**Prune4Web**。它的核心贡献是“DOM Tree Pruning Programming”，这是一种改进智能体感知和处理环境信息（DOM）能力的创新机制。因此，它属于“构建、改进或演化LLM智能体”的范畴，而非“非演化型应用”。 2.  **第二步：正面指标** - 论文高度符合您的核心关注点： - **核心范式**: 论文明确研究 `LLM-based Agents` (Web Agent)。 - **智能体能力**: - **`Tool Use / Tool Augmentation`**: 这是论文最核心的亮点。它让LLM生成可执行的Python脚本来作为工具，动态地剪枝DOM树，这是一种高级的工具使用形式，极大地提升了智能体的效率。 - **`Planning`**: 论文提到其方法基于“decomposed sub-tasks”（分解的子任务），并且其框架中包含一个“Planner”组件，这直接关联到智能体的规划能力。 - 论文通过优化智能体的感知（过滤DOM）和行动（精确定位）环节，实质性地提升了智能体的整体性能。 3.  **第三步：排除标准** - 论文未触发任何排除标准。其主要贡献是关于智能体的效率和性能，而非安全、对齐或可解释性。虽然处理的是网页，但其处理对象是结构化的DOM树（文本/标签），而非原始像素，因此不属于被排除的多模态或视觉研究范畴。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美符合“保留”条件。它研究的是智能体如何在复杂任务（网页导航）中进行高效的多步操作。其提出的剪枝机制是智能体规划和执行链条中的一个关键环节，旨在解决智能体在真实环境中的推理瓶颈，而不是提升LLM本身的数学或逻辑推理能力。 **最终决策**: 该论文的核心贡献是提出了一种名为Prune4Web的新范式，通过让LLM生成程序化工具（Python脚本）来优化Web智能体的环境感知能力。这直接属于**单智能体**研究方向下的**工具使用**和**规划**能力的改进。它不是对现有智能体的简单应用，而是对智能体架构和能力的实质性创新，完全契合您“构建、改进或演化LLM智能体”的核心目标。因此，应予以保留。"
    },
    {
        "index": "#5",
        "title": "Conversational no-code and multi-agentic disease module identification and drug repurposing prediction with ChatDRex",
        "link": "/arxiv/2511.21438",
        "arxiv_id": "2511.21438",
        "authors": "Simon Süwer, Kester Bagemihl, Sylvie Baier, Lucia Dicunta, Markus List, Jan Baumbach, Andreas Maier, Fernando M. Delgado-Chaves",
        "summary": "Repurposing approved drugs offers a time-efficient and cost-effective alternative to traditional drug development. However, in silico prediction of repurposing candidates is challenging and requires the effective collaboration of specialists in various fields, including pharmacology, medicine, biology, and bioinformatics. Fragmented, specialized algorithms and tools often address only narrow aspects of the overall problem, and heterogeneous, unstructured data landscapes require specialized users to be involved. Hence, these data services do not integrate smoothly across workflows. With ChatDRex, we present a conversation-based, multi-agent system that facilitates the execution of complex bioinformatic analyses aiming for network-based drug repurposing prediction. It builds on the integrated systems medicine knowledge graph NeDRex. ChatDRex provides natural language access to its extensive biomedical KG and integrates bioinformatics agents for network analysis and drug repurposing, complemented by agents for functional coherence evaluation for in silico validation, as well as agents for literature mining and for discussing the obtained results in a scientific context. Its flexible multi-agent design assigns specific tasks to specialized agents, including query routing, data retrieval, algorithm execution, and result visualization. A dedicated reasoning module keeps the user in the loop and allows for hallucination detection. By enabling physicians and researchers without computer science expertise to control complex analyses in natural language, ChatDRex democratizes access to bioinformatics as an important resource for drug repurposing. It enables clinical experts to generate hypotheses and explore drug repurposing opportunities, ultimately accelerating the discovery of novel therapies and advancing personalized medicine and translational research.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-11-26",
        "category": "cs.MA",
        "crawl_time": "2025-11-27T11:00:03.690744",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将一个已有的LLM智能体框架应用到药物重定位领域。它的核心贡献在于**构建了一个新颖的、对话式的、多智能体系统**。摘要中明确指出，这是一个“conversation-based, multi-agent system”，并详细描述了其“flexible multi-agent design”，该设计将特定任务分配给专业化智能体。这表明论文的重点是**提出和实现一个新的多智能体协作框架**，而不仅仅是展示其在特定领域的应用结果。因此，它避开了“非演化型应用”的排除规则。 2.  **第二步：正面指标** - 论文包含了大量核心关注点的正面指标： - **核心范式**: `Multi-Agent Systems (MAS)` 被明确提及。 - **智能体能力**: `Tool Use / Tool Augmentation`（智能体执行网络分析、文献挖掘等算法）、`Planning`（通过“query routing”和“assigns specific tasks”体现）。 - **多智能体**: `Collaboration`（多个专业化智能体协同工作）、`Communication`（通过对话式系统实现）。 - 这些指标强烈表明该论文与我的研究焦点高度相关，特别是“多智能体”方向。 3.  **第三步：排除标准** - 论文提到了“hallucination detection”，但这被描述为系统中的一个“reasoning module”的功能，目的是“keeps the user in the loop”，而不是论文的主要研究贡献。论文的核心是构建智能体系统，而非研究安全或对齐本身。因此，这不触发排除标准。 - 论文不涉及多模态与视觉。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的“reasoning module”和智能体任务的分配（query routing, algorithm execution）属于智能体框架内的规划和推理，符合保留条件。 - **自我演化的应用**: 虽然这不直接是自我演化的论文，但它遵循了同样的原则：**核心贡献在于方法论（多智能体框架），而非应用领域**。正如规则所述，即使应用在特定领域（生物医学），只要核心是提出新的智能体机制，就应该保留。 5.  **第五步：最终决策** - 综合以上分析，这篇论文的核心贡献是**设计和实现了一个用于复杂任务的多智能体协作框架**。尽管其应用场景是药物重定位，但论文的创新点在于智能体系统的架构、任务分配和工具集成机制。这完全符合我研究课题中“多智能体”方向的要求，即关注智能体间的协作、通信和任务执行。因此，这篇论文是高度相关的前沿研究，应被保留。"
    },
    {
        "index": "#2",
        "title": "Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark in Multi-robot Cooperation",
        "link": "/arxiv/2511.21510",
        "arxiv_id": "2511.21510",
        "authors": "Ke Zhang, Xiaoning Zhao, Ce Zheng, Jiahong Ning, Dandan Zhu, Wenqi Zhang, Chen Sun, Toshiharu Sugawara",
        "summary": "This study proposes Tool-RoCo, a novel benchmark for evaluating large language models (LLMs) in long-term multi-agent cooperation based on RoCo, a multi-robot cooperative benchmark. Recent research on LLM-based multi-agent systems has relied on predefined orchestration, while ignoring agent autonomy. Tool-RoCo treats other agents as tools and introduces cooperative tools, leveraging tool usage to evaluate multi-agent cooperation and self-organization. Tool usage means that each agent (LLM) selects a tool from a candidate set based on the current state, receives feedback, and adjusts its selection in subsequent rounds. To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) self-organization, where a randomly chosen initial agent can request collaboration, activating additional agents via tool calls. Tool-RoCo includes three multi-robot tasks, SORT, PACK, and CABINET, to measure format and parameter accuracy and agent coordination through tool usage. The results using several LLMs showed that cooperative tools accounted for only 7.09% of all tools, indicating that LLM-based agents rarely invoked others as assistants. Moreover, activation tools accounted for 96.42%, suggesting that current LLMs tend to maintain active agents while seldom deactivating them for adaptive coordination. Tool-RoCo provides a systematic benchmark to evaluate LLM autonomy and cooperation in multi-agent tasks. Code and Demo: https://github.com/ColaZhang22/Tool-Roco",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-11-26",
        "category": "cs.MA",
        "crawl_time": "2025-11-27T11:00:03.689921",
        "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断——保留** *   **论文本质**: 这篇论文的核心贡献是提出了一个名为 **Tool-RoCo** 的新**基准**。这个基准并非简单地将现有LLM智能体应用到机器人领域，而是构建了一个**新的评估框架和方法论**，专门用于研究和衡量LLM在多智能体环境下的**自主性**和**协作能力**。 *   **符合目标**: 这直接命中了你“构建、改进或演化LLM智能体”的核心目标。它通过提出新的评估范式（四种LLM协作范式）和核心机制（将其他智能体视为工具），推动了如何**构建和评估**更高级的多智能体系统的研究。它不是“非演化型应用”，因为其焦点是智能体本身的组织与协作机制，而非解决机器人任务本身。 2.  **第二步：正面指标——高度相关** *   论文摘要中包含了大量与你研究焦点高度匹配的核心范式和能力关键词： *   **多智能体**: `Multi-agent cooperation`, `Multi-robot cooperation`, `decentralized cooperation`, `agent coordination`。 *   **智能体能力**: `Tool Use` (这是论文的核心机制), `autonomy` (自主性), `self-organization` (自组织)。 *   **演化机制**: `Self-organization` 是自主演化的一个关键表现形式，论文通过评估智能体如何激活/停用自身来适应环境，这与“自我演化”中的迭代改进和适应环境反馈紧密相关。 *   这些正面指标表明，论文的研究内容与你的三个方向（单智能体、多智能体、自我演化）均有交叉，尤其侧重于**多智能体**和**自我演化（自组织）**。 3.  **第三步：排除标准——未触发** *   论文的主要贡献不是关于安全、对齐或可解释性。 *   论文虽然涉及机器人，但其核心是LLM的决策与协作逻辑，而非视觉或多模态感知。机器人任务（SORT, PACK, CABINET）是作为评估智能体能力的载体，而非研究本身。 4.  **第四步：处理特殊和模糊情况——符合保留规则** *   **自我演化的应用**: 这篇论文可以被看作是“自我演化的应用”的一个绝佳范例。虽然它应用在机器人领域，但其核心贡献是提出了一种新的“自组织”机制和评估基准，这完全符合你设定的“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域，也应该保留”的例外规则。 *   **推理/规划**: 论文探讨的是智能体在多步任务中如何通过工具使用进行协作和自组织，这属于智能体层面的规划和推理，而非提升LLM本身的基础数学或逻辑能力。 **最终决策**: 这篇论文的核心贡献在于构建了一个创新的基准（Tool-RoCo），用于推动和评估LLM智能体在多智能体协作中的自主性和自组织能力。它直接服务于你“构建、改进或演化LLM智能体”的研究目标，并且深度契合了“多智能体”和“自我演化”这两个核心方向。因此，这篇论文是高度相关且必须保留的前沿研究。"
    },
    {
        "index": "#9",
        "title": "Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State Detection in a 7B Model",
        "link": "/arxiv/2511.21399",
        "arxiv_id": "2511.21399",
        "authors": "Joshua Fonseca Rivera",
        "summary": "Lindsey (2025) investigates introspective awareness in language models through four experiments, finding that models can sometimes detect and identify injected activation patterns -- but unreliably (~20% success in the best model). We focus on the first of these experiments -- self-report of injected \"thoughts\" -- and ask whether this capability can be directly trained rather than waiting for emergence. Through fine-tuning on transient single-token injections, we transform a 7B parameter model from near-complete failure (0.4% accuracy, 6.7% false positive rate) to reliable detection (85% accuracy on held-out concepts at α=40, 0% false positives). Our model detects fleeting \"thoughts\" injected at a single token position, retains that information, and reports the semantic content across subsequent generation steps. On this task, our trained model satisfies three of Lindsey's criteria: accuracy (correct identification), grounding (0/60 false positives), and internality (detection precedes verbalization). Generalization to unseen concept vectors (7.5pp gap) demonstrates the model learns a transferable skill rather than memorizing specific vectors, though this does not establish metacognitive representation in Lindsey's sense. These results address an open question raised by Lindsey: whether \"training for introspection would help eliminate cross-model differences.\" We show that at least one component of introspective behavior can be directly induced, offering a pathway to built-in AI transparency.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-11-26",
        "category": "cs.CL",
        "crawl_time": "2025-11-27T11:00:03.839569",
        "filter_reason": "这篇论文符合筛选标准，应当保留。我的判断过程如下： 1.  **第一步：核心判断** - **论文的核心贡献**: 这篇论文的核心贡献并非将LLM应用于某个特定领域，也不是提升其基础数学或逻辑推理能力。它的核心是提出了一种**通过微调来训练模型“内省行为”的方法论**。具体来说，它让模型能够可靠地检测、记忆并报告其内部被注入的“思想”（一种内部状态）。 - **是否符合研究目标**: 这完全符合“构建、改进或演化 LLM智能体”的核心目标。内省，即对自身内部状态的感知和报告，是高级智能体实现**自我反思**和**自我修正**的基石。这篇论文的工作，正是在构建这样一个基础的、关键的智能体能力模块。它不是在解决一个外部任务，而是在塑造智能体的内部机制。因此，应**保留**。 2.  **第二步：正面指标** - 论文的研究内容与多个核心关注点高度吻合： - **智能体能力**: `Self-Reflection` (自我反思) 是最直接相关的指标。论文训练的“内省行为”是自我反思的前提。同时，模型能够“retains that information, and reports...across subsequent generation steps”，这体现了`Memory` (记忆) 能力。 - **核心范式**: 研究内容属于 `Agentic AI` 的范畴，因为它在探索如何赋予模型更自主的内部处理能力。 3.  **第三步：排除标准** - **安全与对齐**: 论文摘要最后提到了 \"offering a pathway to built-in AI transparency\"。虽然“透明度”与可解释性相关，但这只是该研究带来的一个**潜在应用或结果**，而非论文的**核心贡献**。论文的核心是“如何训练内省行为”这一方法论，而不是提出一种新的可解释性技术或安全对齐方案。因此，它不应被归为排除类别。 - **多模态与视觉**: 论文不涉及视觉或多模态内容。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文的研究不属于“非Agentic的推理”。它关注的不是模型如何解决一个数学题或逻辑题，而是模型如何**感知和报告自身的内部状态**。这是一种元认知能力，是智能体区别于普通语言模型的关键特征之一，因此属于应保留的范畴。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一种训练LLM实现内省能力的方法。内省能力是实现智能体自我反思和自我修正的关键组成部分，直接服务于“单智能体”研究方向下的“自我反思”和“记忆”子方向。它不是一篇应用型、基础设施型或基础推理型论文，也未触及安全对齐的核心。因此，这篇论文精准地契合了“LLM智能体及其演化”的研究范围，应予以保留。"
    },
    {
        "index": "#22",
        "title": "Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs",
        "link": "/arxiv/2511.20940",
        "arxiv_id": "2511.20940",
        "authors": "Reham Omar, Abdelghny Orogat, Ibrahim Abdelaziz, Omij Mangukiya, Panos Kalnis, Essam Mansour",
        "summary": "Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems. KGs are widely used in enterprise and domain applications to provide structured, evolving, and reliable knowledge. Large language models (LLMs) enable natural and context-aware conversations, but lack direct access to private and dynamic KGs. Retrieval-augmented generation (RAG) systems can retrieve graph content but often serialize structure, struggle with multi-turn context, and require heavy indexing. Traditional KGQA systems preserve structure but typically support only single-turn QA, incur high latency, and struggle with coreference and context tracking. To address these limitations, we propose Chatty-KG, a modular multi-agent system for conversational QA over KGs. Chatty-KG combines RAG-style retrieval with structured execution by generating SPARQL queries through task-specialized LLM agents. These agents collaborate for contextual interpretation, dialogue tracking, entity and relation linking, and efficient query planning, enabling accurate and low-latency translation of natural questions into executable queries. Experiments on large and diverse KGs show that Chatty-KG significantly outperforms state-of-the-art baselines in both single-turn and multi-turn settings, achieving higher F1 and P@1 scores. Its modular design preserves dialogue coherence and supports evolving KGs without fine-tuning or pre-processing. Evaluations with commercial (e.g., GPT-4o, Gemini-2.0) and open-weight (e.g., Phi-4, Gemma 3) LLMs confirm broad compatibility and stable performance. Overall, Chatty-KG unifies conversational flexibility with structured KG grounding, offering a scalable and extensible approach for reliable multi-turn KGQA.",
        "subjects": "Computation and Language",
        "date": "2025-11-26",
        "category": "cs.CL",
        "crawl_time": "2025-11-27T11:00:03.848062",
        "filter_reason": "这篇论文完全符合你的研究范围，核心原因在于其贡献是**构建和改进一个多智能体系统**，而非简单的应用。 以下是详细的判断过程： 1.  **第一步：核心判断** - **保留**。论文的核心贡献是提出了一个名为“Chatty-KG”的**模块化多智能体系统**。摘要明确指出，该系统通过“task-specialized LLM agents”（任务专用的LLM智能体）来协作解决问题。这完全符合你筛选标准中“构建、改进LLM智能体”和“多智能体系统”的要求。它不是简单地将一个现有框架（如ReAct）应用到知识图谱问答上，而是设计了一个全新的、由多个专门智能体构成的架构。 2.  **第二步：正面指标** - 论文命中了多个核心正面指标，表明其与你的研究焦点高度相关： - **核心范式**: `Multi-Agent Systems (MAS)` - 论文标题和摘要反复强调这是一个多智能体系统。 - **多智能体**: `Collaboration` - 摘要明确提到“These agents collaborate for contextual interpretation, dialogue tracking...”（这些智能体协作进行上下文解释、对话跟踪...）。 - **智能体能力**: `Planning` - 论文涉及“efficient query planning”（高效的查询规划），这是智能体规划能力的具体体现。`Tool Use` - 智能体通过生成SPARQL查询来与知识图谱交互，这是一种明确的工具使用行为。`Memory` - 系统通过“dialogue tracking”（对话跟踪）来处理多轮上下文，这体现了智能体的记忆能力。 3.  **第三步：排除标准** - 论文没有触发任何排除标准。其主要贡献是关于系统架构和性能，而非安全、对齐或多模态。虽然它处理的是特定领域（知识图谱问答），但其核心是提出一种新的智能体组织方法，这属于你关注的“构建”范畴，而非“非演化型应用”。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的“query planning”（查询规划）是典型的智能体规划行为。它是指智能体为了达成“回答用户问题”这一目标，而规划出一系列步骤（如识别实体、链接关系、生成SPARQL查询）。这完全符合你“保留”关于智能体如何进行规划的研究的标准，而不是关于提升LLM本身基础推理能力的研究。 **最终决策**: 这篇论文的核心贡献是**设计并实现了一个新颖的多智能体协作框架**，用于解决复杂的对话式知识图谱问答任务。它详细阐述了多个智能体如何分工（上下文解释、对话跟踪、实体链接、查询规划）和协作，这直接命中了你研究课题中的“多智能体”方向，并涉及了“规划”、“工具使用”和“记忆”等关键的单智能体能力。因此，这篇论文是你研究“LLM智能体及其演化”课题的理想筛选对象。"
    },
    {
        "index": "#25",
        "title": "Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory",
        "link": "/arxiv/2511.20857",
        "arxiv_id": "2511.20857",
        "authors": "Tianxin Wei, Noveen Sachdeva, Benjamin Coleman, Zhankui He, Yuanchen Bei, Xuying Ning, Mengting Ai, Yunzhe Li, Jingrui He, Ed H. Chi, Chi Wang, Shuo Chen, Fernando Pereira, Wang-Cheng Kang, Derek Zhiyuan Cheng",
        "summary": "Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-11-25",
        "category": "cs.CL",
        "crawl_time": "2025-11-27T11:00:03.849170",
        "filter_reason": "这篇论文完全符合研究范围，其核心贡献直接命中了“自我演化”和“单智能体”两个核心方向。 1.  **第一步：核心判断 (保留)** 论文的核心不是将LLM智能体作为工具应用于某个特定领域，而是聚焦于智能体本身的一个核心组件——记忆，并提出了一个全新的框架和基准来衡量和实现其“自我演化”。论文的核心贡献是两点：1）提出了`Evo-Memory`，一个用于评估LLM智能体自我演化记忆能力的基准和框架；2）提出了`ReMem`，一个实现智能体在测试时通过经验持续改进的“行动-思考-记忆”精炼管道。这两点都属于构建、改进和演化LLM智能体的方法论范畴，因此应保留。 2.  **第二步：正面指标 (高度匹配)** 论文标题和摘要中包含了大量核心关注点： *   **核心范式**: `LLM-based Agents`, `Self-Evolving` *   **智能体能力**: `Memory`, `Self-Reflection` (体现在`ReMem`的refine pipeline中) *   **演化机制**: `Self-Improvement`, `Iterative Improvement` (摘要中明确提到`continual improvement`和`test-time evolution`) 这些正面指标表明该论文与您的研究焦点高度相关。 3.  **第三步：排除标准 (未触发)** 论文的主要贡献并非关于安全、对齐、可解释性或多模态。它的焦点是智能体的能力机制，完全避开了排除标准所列出的领域。 4.  **第四步：处理特殊和模糊情况 (符合保留规则)** *   **推理/规划**: 论文提出的`ReMem`是一个“action-think-memory refine pipeline”，这是一个典型的智能体自主规划和多步推理框架，将推理、行动和记忆更新紧密结合。这完全符合“保留”关于智能体规划和推理框架的规则，而不是单纯提升LLM的基础推理能力。 *   **自我演化的应用**: 该论文本身就是一个关于“自我演化”机制的研究，而不是一个应用。它提出的`Evo-Memory`基准和`ReMem`方法，正是您所寻找的“自我演化”机制的核心贡献。 **最终决策**: 综合以上分析，该论文的核心贡献在于提出了一套全新的基准（`Evo-Memory`）和方法（`ReMem`），用于研究和实现LLM智能体的自我演化记忆能力。这直接对应了您研究目标中的“自我演化”方向，并深入探讨了“单智能体”方向中的“记忆”和“自我反思”能力。因此，这篇论文是您课题下的高度相关前沿研究，应被筛选保留。"
    },
    {
        "index": "#28",
        "title": "SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models",
        "link": "/arxiv/2511.20820",
        "arxiv_id": "2511.20820",
        "authors": "Jiaojiao Han, Wujiang Xu, Mingyu Jin, Mengnan Du",
        "summary": "Large language models (LLMs) have achieved remarkable progress, yet their internal mechanisms remain largely opaque, posing a significant challenge to their safe and reliable deployment. Sparse autoencoders (SAEs) have emerged as a promising tool for decomposing LLM representations into more interpretable features, but explaining the features captured by SAEs remains a challenging task. In this work, we propose SAGE (SAE AGentic Explainer), an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanation-driven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanationdriven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.",
        "subjects": "Computation and Language",
        "date": "2025-11-25",
        "category": "cs.CL",
        "crawl_time": "2025-11-27T11:00:03.850655",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地应用LLM，而是**提出了一种新的智能体框架（SAGE）**。其核心贡献在于构建了一个能够主动、迭代地完成特定复杂任务的LLM智能体，而不是将LLM作为黑盒工具应用于某个领域。该框架的设计本身就是对Agentic AI的推进。 2.  **第二步：正面指标** - 论文高度符合我的核心关注点： - **核心范式**: 论文标题和摘要明确提出了 `Agentic Explainer Framework` 和 `agent-based framework`，直接命中 `Agentic AI` 和 `LLM-based Agents`。 - **智能体能力**: SAGE框架的实现过程完美体现了多个关键能力： - **规划**: \"systematically formulating multiple explanations for each feature\" (为每个特征系统性地制定多种解释)。 - **工具使用**: \"designing targeted experiments to test them\" (设计针对性实验来测试解释)。这里的“实验”就是智能体用来验证假设的工具。 - **自我反思/自我修正**: \"iteratively refining explanations based on empirical activation feedback\" (根据经验性激活反馈迭代地完善解释)。这是一个典型的基于环境反馈的自我完善循环。 - **演化机制**: 整个 \"formulate -> test -> refine\" 的循环就是一种**迭代改进**和**自我完善**的机制，完全符合“自我演化”的研究方向。 3.  **第三步：排除标准** - **安全与对齐**: 这是本案例中最需要辨析的一点。虽然论文的研究动机（\"safe and reliable deployment\"）和任务（\"Interpreting SAE Features\"）与“可解释性”高度相关，但论文的**主要贡献并非一种新的可解释性理论或安全对齐方法**。它的核心贡献是**提出了一种实现可解释性的智能体方法论**。换言之，这篇论文是关于“如何构建一个能做X的智能体”，而不是“关于X本身的研究”。我的研究焦点是Agentic AI，因此，只要论文的核心是构建智能体，即使其应用场景是可解释性，也应该保留。这篇论文的价值在于它展示了一个通过规划、工具使用和自我反思来解决复杂问题的新颖智能体架构。 - **多模态与视觉**: 论文不涉及此内容。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的推理是封装在智能体的行动循环中的（规划-行动-观察-反思），而不是提升LLM本身的基础推理能力。因此，它符合保留条件。 - **自我演化的应用**: 这篇论文可以被视为一个“自我演化”机制的范例。智能体通过与环境（SAE特征激活）的交互，不断迭代和优化其内部状态（解释），这正是自我演化的体现。 **最终决策**: 综合以上分析，这篇论文的核心贡献是构建了一个名为SAGE的智能体框架。该框架通过规划、工具使用和基于反馈的自我反思/完善循环，来解决“解释SAE特征”这一复杂任务。这完全符合我“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的核心目标，并且精准地命中了“单智能体”和“自我演化”两个研究方向。尽管其应用领域触及了可解释性，但其方法论上的创新属于Agentic AI的范畴，因此应予以保留。"
    },
    {
        "index": "#60",
        "title": "ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training",
        "link": "/arxiv/2511.20718",
        "arxiv_id": "2511.20718",
        "authors": "Chenliang Li, Adel Elmahdy, Alex Boyd, Zhongruo Wang, Alfredo Garcia, Parminder Bhatia, Taha Kass-Hout, Cao Xiao, Mingyi Hong",
        "summary": "PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-11-25",
        "category": "cs.CL",
        "crawl_time": "2025-11-27T11:00:03.880813",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于**改进LLM智能体的训练方法**，属于“构建、改进或演化LLM智能体”的范畴。具体判断过程如下： 1.  **第一步：核心判断 (保留)** *   论文的本质不是将LLM智能体作为工具应用到某个领域，而是提出了一种新的训练算法（ST-PPO）来解决LLM智能体训练过程中的一个核心痛点：不稳定性。 *   它的核心贡献是方法论层面的创新，旨在**构建和改进更稳定、更强大的LLM智能体**，而不是简单地应用一个已有的智能体框架。因此，它通过了第一步的核心判断，应该被保留。 2.  **第二步：正面指标 (高度相关)** *   论文明确提到了 `Multi-Turn Agents Training`，直接命中了 `Agentic AI` 和 `LLM-based Agents` 的核心范式。 *   研究对象是 `multi-turn dialogue and reasoning tasks`，这直接关联到智能体的 `Planning` 和多步 `Reasoning` 能力。论文提出的“turn-level importance sampling”正是为了更好地对齐这种多轮推理的自然结构。 *   使用PPO（一种强化学习算法）来训练智能体，本身就是一种让智能体通过与环境交互进行**自我完善和迭代**的机制。因此，这篇论文与 `Self-Evolving` 方向高度相关，它致力于优化这种演化过程的稳定性。 3.  **第三步：排除标准 (未触发)** *   论文的研究焦点是训练算法的稳定性，完全不涉及 `Safety`, `Alignment`, `Interpretability` 等安全与对齐问题。 *   论文处理的是文本任务，没有涉及 `Vision`, `MLLMs` 等多模态内容。 4.  **第四步：处理特殊和模糊情况 (符合保留规则)** *   **推理/规划**: 这篇论文不是在提出一种新的非Agentic推理技巧（如CoT的变体），而是在研究**如何训练一个能够在多轮任务中稳定进行推理的智能体**。它关注的是智能体训练的优化过程，属于“保留”的范畴。 *   **自我演化的应用**: 论文的核心贡献是提出一种新的、更稳定的训练机制（可以看作是一种演化机制的改进），即使它在QA等特定领域进行验证，其价值在于方法本身，因此符合保留规则。 **总结**: 该论文的核心贡献是提出了一种名为ST-PPO的稳定化训练算法，专门用于解决多轮LLM智能体在训练中性能崩溃的问题。这直接对应了你研究目标中的“**改进LLM智能体**”和“**自我演化**”（通过优化强化学习训练过程）。它是一项关于智能体底层训练机制的方法论研究，而非应用型研究，因此与你的研究课题高度契合。"
    },
    {
        "index": "#56",
        "title": "Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning",
        "link": "/arxiv/2511.20993",
        "arxiv_id": "2511.20993",
        "authors": "Shanwei Fan",
        "summary": "Large language models (LLMs) offer strong high-level planning capabilities for reinforcement learning (RL) by decomposing tasks into subgoals. However, their practical utility is limited by poor planning-execution alignment, which reflects a critical gap between abstract plans and actionable, environment-compatible behaviors. This misalignment arises from two interrelated limitations: (1) LLMs often produce subgoals that are semantically plausible but infeasible or irrelevant in the target environment due to insufficient grounding in environment-specific knowledge, and (2) single-LLM planning conflates generation with self-verification, resulting in overconfident yet unreliable subgoals that frequently fail during execution. To address these challenges, we propose Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR), a framework that integrates an environment-specific subgoal graph and structured entity knowledge with a multi-LLM planning pipeline that explicitly separates generation, critique, and refinement to produce executable and verifiable subgoals. A subgoal tracker further monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain alignment between plans and actions. Experimental results on 22 diverse tasks in the open-world game \"Crafter\" demonstrate the effectiveness of our proposed method.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-11-26",
        "category": "cs.LG",
        "crawl_time": "2025-11-27T11:00:04.764836",
        "filter_reason": "这篇论文完全符合你的研究范围，应被保留。 **判断过程和核心依据如下：** 1.  **第一步：核心判断 (保留)** *   **论文本质**: 这篇论文的核心贡献是提出了一个名为 **SGA-ACR (Subgoal Graph-Augmented Actor-Critic-Refiner)** 的新**框架**。这个框架旨在解决LLM智能体在强化学习任务中“规划-执行对齐”的核心挑战。它不是简单地将LLM或现有智能体框架应用到一个新领域，而是**构建和改进LLM智能体的方法论**。因此，它完全符合“保留”标准。 2.  **第二步：正面指标 (高度匹配)** *   **核心范式**: 论文明确属于 `LLM-based Agents` 的研究范畴。 *   **智能体能力**: 论文的核心是关于 `Planning` (规划)。它不仅研究规划，还通过一个“生成-批判-精炼”的多LLM流程，实现了 `Self-Correction` 和 `Self-Reflection` 的机制。摘要中提到的“subgoal tracker”和“adaptively updates the subgoal graph”也体现了智能体的 `Memory` 和 `Iterative Improvement` 能力。 *   **演化机制**: 框架中的“Refiner”和“adaptive updates”机制，使得智能体能够根据执行反馈和环境变化来调整其规划，这是一种在任务执行过程中的**自我完善和迭代**，与“自我演化”的方向紧密相关。 3.  **第三步：排除标准 (未触发)** *   论文的主要贡献不是关于安全、对齐、可解释性或水印。 *   论文虽然应用于游戏\"Crafter\"（一个视觉环境），但其研究核心是**规划框架本身**，而不是视觉或多模态技术。视觉信息只是智能体感知环境的一部分，而非研究的创新点。 4.  **第四步：处理特殊和模糊情况 (精准定位)** *   **推理/规划**: 这篇论文是“智能体如何进行规划”的典型范例。它提出的SGA-ACR框架是一个全新的Agentic规划架构，超越了简单的ReAct或ToT，引入了子目标图、多LLM协作（生成、批判、精炼）和动态更新机制。这完全符合“保留”关于智能体规划和多步推理框架的论文的要求。 **最终决策**: 该论文的核心是构建一个新颖的LLM智能体框架（SGA-ACR），以解决智能体在复杂环境中的规划和自我修正问题。它直接命中了你的研究焦点“单智能体”下的“规划”和“自我反思”子方向，并触及了“自我演化”的迭代改进思想。因此，这篇论文是高质量、高度相关的前沿研究，**必须保留**。"
    },
    {
        "index": "#136",
        "title": "Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning",
        "link": "/arxiv/2511.20694",
        "arxiv_id": "2511.20694",
        "authors": "Kevin Lee, Russell Spiewak, James Walsh",
        "summary": "Scientific reasoning through Large Language Models in heliophysics involves more than just recalling facts: it requires incorporating physical assumptions, maintaining consistent units, and providing clear scientific formats through coordinated approaches. To address these challenges, we present Reasoning With a Star, a newly contributed heliophysics dataset applicable to reasoning; we also provide an initial benchmarking approach. Our data are constructed from National Aeronautics and Space Administration & University Corporation for Atmospheric Research Living With a Star summer school problem sets and compiled into a readily consumable question-and-answer structure with question contexts, reasoning steps, expected answer type, ground-truth targets, format hints, and metadata. A programmatic grader checks the predictions using unit-aware numerical tolerance, symbolic equivalence, and schema validation. We benchmark a single-shot baseline and four multi-agent patterns, finding that decomposing workflows through systems engineering principles outperforms direct prompting on problems requiring deductive reasoning rather than pure inductive recall.",
        "subjects": "Artificial Intelligence, Solar and Stellar Astrophysics, Machine Learning, Space Physics",
        "date": "2025-11-23",
        "category": "cs.LG",
        "crawl_time": "2025-11-27T11:00:04.861568",
        "filter_reason": "这篇论文符合研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - 论文的标题和摘要明确指出其核心是关于 \"Agentic Scientific Reasoning\"。虽然论文的主要产出是一个数据集和基准测试，但其根本目的并非简单地应用LLM解决太阳物理学问题，而是为了**评估和比较不同的智能体推理模式**。 - 论文的核心贡献在于，它提供了一个标准化的测试平台，并得出了关于“哪种智能体工作流更有效”的结论（例如，“通过系统工程原理分解工作流优于直接提示”）。这直接服务于“构建、改进或演化LLM智能体”的核心目标，因为它为如何设计更优的智能体提供了实证依据。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `Agentic AI` (标题中明确提及)。 - **多智能体**: 摘要中明确提到 \"four multi-agent patterns\"。 - **智能体能力**: `Planning` (通过 \"decomposing workflows\" 体现) 和 `Reasoning` (全文主题)。 - 这些正面指标强烈表明该论文与我的研究焦点高度相关。 3.  **第三步：排除标准** - 论文内容不涉及安全与对齐、多模态与视觉等排除领域。其评估指标（单位感知、符号等价）是服务于科学推理任务的，而非研究可解释性或幻觉本身。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美符合“保留”条件。它不是在研究如何提升LLM的基础数学或逻辑能力，而是在研究**智能体如何进行规划和多步推理**。它比较了单智能体与多智能体模式，以及直接提示与工作流分解的效果，这正是对Agentic框架的探索和改进。 **最终决策**: 综合分析，这篇论文的本质是**一项关于智能体推理方法论的实证研究**，它以太阳物理学为测试领域，提出了一个基准来衡量和指导如何构建更有效的LLM智能体。其关于多智能体模式和工作流分解优于直接提示的发现，是对“多智能体”和“规划”这两个核心方向的直接贡献。因此，尽管其形式是数据集和基准，但其研究内核与我的目标高度一致，应被保留。"
    },
    {
        "index": "#5",
        "title": "MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning",
        "link": "/arxiv/2511.21460",
        "arxiv_id": "2511.21460",
        "authors": "Junjian Wang, Lidan Zhao, Xi Sheryl Zhang",
        "summary": "Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-26",
        "category": "cs.AI",
        "crawl_time": "2025-11-27T11:00:04.824666",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——保留** 论文的核心贡献并非简单应用现有技术，而是提出了两个全新的方法论框架： 1.  **MADRA**：一个用于风险评估的**多智能体辩论**框架。 2.  **一个分层认知协作规划框架**：该框架集成了安全、记忆、规划和**自我演化**机制。 这两个贡献的本质是关于如何**构建**和**改进**LLM智能体系统，特别是多智能体系统和具备自我演化能力的智能体，因此直接命中了您的研究目标。它不属于“非演化型应用”或“非Agentic的推理”。 **第二步：正面指标——高度匹配** 论文包含了大量您关注的核心关键词和概念： *   **多智能体**: `Multi-Agent Debate`, `collective reasoning`, `collaborative planning`。 *   **自我演化**: 明确提出了 `self-evolution mechanisms` 和 `continuous learning`，这是您研究的核心方向之一。 *   **智能体能力**: `Planning` (规划), `Memory` (记忆)。 这些正面指标表明，论文的研究内容与您的焦点高度一致。 **第三步：排除标准——不适用** *   **安全与对齐**: 尽管论文的主题是“Risk-Aware”（风险感知）和“Safety”（安全），但其**主要贡献**并非提出一种新的安全理论或对齐技术，而是构建了一个**多智能体辩论框架**和**自我演化框架**来**实现**安全。安全是框架的目标和应用场景，而框架本身（即Agentic AI的方法论）才是核心贡献。因此，它不属于“主要贡献是关于Safety”的排除范畴。 *   **多模态与视觉**: 论文在具身环境（AI2-THOR, VirtualHome）中验证，其中必然涉及视觉。但视觉是作为智能体感知环境的**工具**，研究的核心是智能体内部的规划、辩论和演化机制，而非视觉模型本身。这符合“除非它们被用作智能体感知环境的工具，而不是研究的核心”的规则。 **第四步：处理特殊和模糊情况——强化保留决策** *   **自我演化的应用**: 论文明确提出了一个包含“自我演化机制”的规划框架。根据您的规则，“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。这篇论文完美地符合这一例外情况，其自我演化机制是核心贡献之一。 **第五步：最终决策** 综合以上分析，这篇论文的核心是构建一个新颖的**多智能体系统**（MADRA）和一个集成了**自我演化**能力的规划框架。虽然其应用目标是提升具身智能体的安全性，但其方法论贡献完全聚焦于Agentic AI的构建和演化，与您“LLM智能体及其演化”的研究课题，特别是“多智能体”和“自我演化”两个方向，高度契合。因此，应判定为符合要求。"
    },
    {
        "index": "#6",
        "title": "EWE: An Agentic Framework for Extreme Weather Analysis",
        "link": "/arxiv/2511.21444",
        "arxiv_id": "2511.21444",
        "authors": "Zhe Jiang, Jiong Wang, Xiaoyu Yue, Zijie Guo, Wenlong Zhang, Fenghua Ling, Wanli Ouyang, Lei Bai",
        "summary": "Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.",
        "subjects": "Artificial Intelligence, Atmospheric and Oceanic Physics",
        "date": "2025-11-26",
        "category": "cs.AI",
        "crawl_time": "2025-11-27T11:00:04.825162",
        "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心判断 (第一步):** - **保留:** 论文的核心贡献是构建了一个名为 EWE (Extreme Weather Expert) 的 **\"intelligent agent framework\"**。它不是简单地将现有LLM或智能体框架应用于气象领域，而是提出了一个全新的、专门用于解决复杂诊断任务的智能体方法论。这直接命中了您“构建、改进LLM智能体”的核心目标。 2.  **正面指标 (第二步):** - 论文摘要中明确包含了多个您关注的核心范式和能力关键词： - **核心范式:** \"intelligent agent framework\" 直接对应 `Agentic AI` 和 `LLM-based Agents`。 - **智能体能力:** \"knowledge-guided planning\" 对应 `Planning`；\"domain-tailored meteorological toolkit\" 对应 `Tool Use / Tool Augmentation`；\"closed-loop reasoning\" 暗示了智能体在执行、观察和思考之间循环，这与 `Self-Correction` 或 `Self-Reflection` 的理念高度相关。 3.  **排除标准 (第三步) 与特殊情况处理 (第四步):** - **非演化型应用:** 尽管论文的应用领域是“极端天气分析”，但其核心贡献是**提出智能体框架本身**，并为此领域创建了新的基准。这属于方法论创新，而非单纯的应用。因此，它不属于“非演化型应用”的排除范畴。 - **多模态与视觉:** 论文提到智能体“produces and interprets multimodal visualizations”。根据您的核心规则，这里的视觉能力是作为智能体**感知和解释环境的工具**，而不是研究的核心。研究的核心是这个智能体框架如何利用这些工具进行规划和推理，因此不应被排除。 - **推理/规划:** 论文明确聚焦于智能体的 \"knowledge-guided planning\" 和 \"closed-loop reasoning\"，这完全符合您对“智能体如何进行规划或在复杂任务中进行多步推理”的保留标准，而非提升LLM基础推理能力。 **总结:** 该论文的本质是提出一个新颖的LLM智能体框架（EWE），其核心贡献在于智能体的**规划、工具使用和闭环推理**机制。虽然它被应用于极端天气这一特定领域，但其方法论贡献是普适的，并且为该新兴领域建立了基准。这完全符合您在“单智能体”方向上筛选关于“构建、改进LLM智能体”的前沿论文的目标。因此，应予以保留。"
    },
    {
        "index": "#11",
        "title": "OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection",
        "link": "/arxiv/2511.21064",
        "arxiv_id": "2511.21064",
        "authors": "Chujie Wang, Jianyu Lu, Zhiyuan Luo, Xi Chen, Chu He",
        "summary": "Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD's lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent's state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-11-26",
        "category": "cs.AI",
        "crawl_time": "2025-11-27T11:00:04.827445",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为“OVOD-Agent”的新颖框架，该框架将传统的开放词汇目标检测任务，转化为一个具有主动视觉推理和自我演化能力的智能体任务。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心并非简单地将现有技术应用于视觉领域，而是**构建了一个全新的智能体框架**。它明确提出了一个具有状态、记忆、行动和奖励的智能体模型（w-MDP），并设计了一个闭环的自我演化机制。这完全符合“构建、改进或演化LLM智能体”的核心目标，特别是命中了“自我演化”这一关键方向。它不是非演化型应用，也不是非Agentic的推理。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了大量我的核心关注点： *   **核心范式**: 论文标题和摘要反复强调 `Agent` 和 `Self-Evolving`。 *   **智能体能力**: 提到了 `Proactive Visual Reasoning`（主动推理，一种规划形式），其 `Visual-CoT` 设计借鉴了智能体推理范式，并且其 `w-MDP` 模型明确包含了 `Memory` 和 `Interaction Dynamics`。 *   **演化机制**: 论文的核心创新点在于其 `Self-Evolving` 机制，具体表现为通过 `Bandit` 探索和 `Reward Model (RM)` 优化形成的 `Self-Improvement` 闭环。 3.  **第三步：排除标准——不适用** *   **安全与对齐**: 论文未涉及安全、对齐等问题。 *   **多模态与视觉**: 这是一个关键点。虽然论文的应用领域是视觉（`OVOD`），但根据筛选规则，只要视觉不是研究的核心，而是智能体感知和交互的环境，就应该保留。本文的核心贡献是**智能体的架构和演化算法**，而不是一个新的视觉模型或视觉-语言模型。视觉任务是智能体施展其“主动推理”和“自我演化”能力的舞台。因此，这不构成排除理由。 4.  **第四步：处理特殊和模糊情况——符合保留条件** *   **推理/规划**: 论文提出的 `Visual-CoT` 是一个典型的智能体推理框架，它将推理过程分解为显式的行动，这完全符合保留条件。 *   **自我演化的应用**: 这篇论文是“自我演化的应用”这一例外情况的完美范例。它的核心贡献是提出了一种**新的自我演化机制**（基于马尔可夫转移矩阵和Bandit轨迹的自监督奖励模型优化闭环），并将其应用于视觉检测领域。根据规则，这种情况应该保留。 **最终决策**: 尽管该智能体并非直接基于LLM（论文提到其轻量级特性不适合LLM管理），但它借鉴了LLM的CoT思想，并且其核心贡献——一个具有自我演化能力的智能体框架——与我的研究课题“LLM智能体及其演化”中的“Agentic AI”和“Self-Evolving”方向高度契合。作为一个顶尖的人工智能研究员，我认为这篇论文提出的自我演化闭环机制对于智能体领域具有重要的启发意义，完全符合我的筛选要求。因此，最终判断为保留。"
    },
    {
        "index": "#12",
        "title": "Towards Trustworthy Legal AI through LLM Agents and Formal Reasoning",
        "link": "/arxiv/2511.21033",
        "arxiv_id": "2511.21033",
        "authors": "Linze Chen, Yufan Cai, Zhe Hou, Jinsong Dong",
        "summary": "The rationality of law manifests in two forms: substantive rationality, which concerns the fairness or moral desirability of outcomes, and formal rationality, which requires legal decisions to follow explicitly stated, general, and logically coherent rules. Existing LLM-based systems excel at surface-level text analysis but lack the guarantees required for principled jurisprudence. We introduce L4M, a novel framework that combines adversarial LLM agents with SMT-solver-backed proofs to unite the interpretive flexibility of natural language with the rigor of symbolic verification. The pipeline consists of three phases: (1) Statute Formalization, where domain-specific prompts convert legal provisions into logical formulae; (2) Dual Fact and Statute Extraction, in which prosecutor- and defense-aligned LLMs independently map case narratives to fact tuples and statutes, ensuring role isolation; and (3) Solver-Centric Adjudication, where an autoformalizer compiles both parties' arguments into logic constraints, and unsat cores trigger iterative self-critique until a satisfiable formula is achieved, which is then verbalized by a Judge-LLM into a transparent verdict and optimized sentence. Experimental results on public benchmarks show that our system surpasses advanced LLMs including GPT-o4-mini, DeepSeek-V3, and Claude 4 as well as state-of-the-art Legal AI baselines, while providing rigorous and explainable symbolic justifications.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-26",
        "category": "cs.AI",
        "crawl_time": "2025-11-27T11:00:04.832997",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于构建了一个新颖的、具有多智能体协作和自我演化特性的LLM智能体框架。我的判断过程如下： 1.  **第一步：核心判断——保留** - 论文的核心不是简单地将LLM应用于法律领域，而是提出了一个名为L4M的**新框架**。这个框架的本质是**构建和组合LLM智能体**来解决复杂问题。它详细描述了智能体的角色（检察官、辩护方、法官）、交互流程和自我修正机制，这完全符合“构建、改进或演化LLM智能体”的核心目标。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标——高度匹配** - **多智能体**: 论文明确提出了“adversarial LLM agents”，并设计了“prosecutor- and defense-aligned LLMs”以及“Judge-LLM”等多个具有不同角色和目标的智能体。它们协同工作，这直接命中了“Multi-Agent Systems”、“Collaboration”、“Communication”和“Negotiation”等核心关注点。 - **自我演化**: 论文的核心机制之一是“iterative self-critique”。当SMT求解器发现逻辑矛盾时，会触发智能体进行迭代式的自我批判和完善，直到生成可满足的逻辑公式。这完美契合了“Self-Evolving”、“Self-Correction”和“Iterative Improvement”的研究方向。 - **智能体能力**: 框架中的智能体使用了SMT求解器作为外部工具来进行形式化验证，这体现了“Tool Use / Tool Augmentation”能力。整个流程也涉及复杂的多步推理和规划。 3.  **第三步：排除标准——未触犯** - **安全与对齐**: 虽然论文标题和摘要中提到了“Trustworthy”和“explainable”，但这并非论文的**主要贡献**。论文的核心是提出一个能产生可解释结果的智能体架构，而不是提出一种新的对齐或安全技术。可解释性是其框架设计带来的一个**结果**，而非研究目标本身。因此，它不属于被排除的安全与对齐研究。 - **多模态与视觉**: 论文未涉及多模态内容。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一例外情况的绝佳范例。它的核心贡献是提出了一种新的“自我演化”机制（基于形式化验证的迭代自我批判），并将其应用在法律领域。根据你的规则，这种情况应该保留。 - **推理/规划**: 论文中的推理是嵌入在智能体框架中的。智能体使用工具（SMT求解器）进行推理和验证，这属于智能体的规划与执行能力，而非提升LLM本身的基础推理能力，因此符合保留标准。 **总结**: 该论文的核心贡献是构建了一个由多个对抗性LLM智能体组成的系统，该系统能够利用外部工具进行形式化验证，并通过迭代式自我批判机制不断演化完善。这精准地覆盖了你研究范围中的“多智能体”和“自我演化”两个核心方向，因此应被保留。"
    },
    {
        "index": "#19",
        "title": "Learning Multi-Access Point Coordination in Agentic AI Wi-Fi with Large Language Models",
        "link": "/arxiv/2511.20719",
        "arxiv_id": "2511.20719",
        "authors": "Yifan Fan, Le Liang, Peng Liu, Xiao Li, Ziyang Guo, Qiao Lan, Shi Jin, Wen Tong",
        "summary": "Multi-access point coordination (MAPC) is a key technology for enhancing throughput in next-generation Wi-Fi within dense overlapping basic service sets. However, existing MAPC protocols rely on static, protocol-defined rules, which limits their ability to adapt to dynamic network conditions such as varying interference levels and topologies. To address this limitation, we propose a novel Agentic AI Wi-Fi framework where each access point, modeled as an autonomous large language model agent, collaboratively reasons about the network state and negotiates adaptive coordination strategies in real time. This dynamic collaboration is achieved through a cognitive workflow that enables the agents to engage in natural language dialogue, leveraging integrated memory, reflection, and tool use to ground their decisions in past experience and environmental feedback. Comprehensive simulation results demonstrate that our agentic framework successfully learns to adapt to diverse and dynamic network environments, significantly outperforming the state-of-the-art spatial reuse baseline and validating its potential as a robust and intelligent solution for future wireless networks.",
        "subjects": "Artificial Intelligence, Information Theory, Signal Processing",
        "date": "2025-11-25",
        "category": "cs.AI",
        "crawl_time": "2025-11-27T11:00:04.836445",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献是**构建一个新颖的Agentic AI框架**，而不是简单地将现有LLM或智能体框架作为工具应用。它提出将每个Wi-Fi接入点（AP）建模为一个自主的LLM智能体，并设计了一个让这些智能体协同工作的系统。这直接命中了“构建LLM智能体”和“多智能体系统”的核心目标。虽然应用领域是Wi-Fi，但其贡献在于**方法论**——即如何构建一个能够协作、推理和适应的多智能体系统，这超越了“非演化型应用”的范畴。 2.  **正面指标 (第二步):** 论文摘要中包含了大量与你研究焦点高度相关的关键词和概念： *   **核心范式:** `Agentic AI`, `LLM-based Agents`, `Multi-Agent Systems`。 *   **多智能体:** `collaboratively reasons`, `negotiates`, `natural language dialogue`, `collaboration`。 *   **智能体能力:** `memory`, `reflection`, `tool use`。 *   **演化机制:** `learns to adapt`, `ground their decisions in past experience and environmental feedback`，这体现了通过经验和反馈进行迭代改进的机制，属于自我演化的范畴。 3.  **排除标准 (第三步):** 论文的主要贡献不涉及安全、对齐、可解释性或视觉多模态等问题，因此没有触发任何排除标准。 4.  **特殊和模糊情况处理 (第四步):** *   **推理/规划:** 论文明确讨论了智能体如何进行规划和多步推理（`negotiates adaptive coordination strategies`），这属于智能体层面的规划，符合保留标准。 *   **自我演化的应用:** 这篇论文是“自我演化的应用”这一例外情况的完美范例。它的核心是提出一种**新的智能体协作与自适应机制**（即“认知工作流”），并将其应用于Wi-Fi领域。因此，即使有具体的应用场景，也应被保留，因为其核心贡献在于智能体本身的构建和演化机制。 **总结:** 该论文的本质是提出一个由多个具备记忆、反思和工具使用能力的LLM智能体组成的系统，用于解决复杂的动态协调问题。它直接贡献于**多智能体协作**和**智能体的自适应演化**这两个核心研究方向，完全符合你为“LLM智能体及其演化”课题设定的筛选标准。"
    }
]