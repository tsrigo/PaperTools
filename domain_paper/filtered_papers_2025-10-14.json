[
    {
        "index": "#4",
        "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics",
        "link": "/arxiv/2510.12787",
        "arxiv_id": "2510.12787",
        "authors": "Marco Del Tredici, Jacob McCarran, Benjamin Breen, Javier Aspuru Mijares, Weichen Winston Yin, Jacob M. Taylor, Frank Koppens, Dirk Englund",
        "summary": "We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperform them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Prover's assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-14",
        "category": "cs.MA",
        "crawl_time": "2025-10-15T11:00:03.200849",
        "filter_reason": "这篇论文完全符合您的筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出了一个名为“Ax-Prover”的**通用智能体框架**。尽管其应用和测试案例是数学和量子物理中的定理证明，但论文的本质并非解决某个具体的数学或物理问题。相反，它提出了一种**方法论**：如何通过将LLM与形式化工具（Lean）相结合，构建一个能够进行深度、严谨、多步推理的通用智能体。论文明确强调其方法是一种“generalizable methodology”（可泛化的方法论），旨在与“specialized systems”（专用系统）区分开来。这完全符合筛选标准中“改进LLM的基础能力”和“提出新的训练范式（此处为推理范式）”的要求，特别是“智能体协作框架”和“工具使用”这两个方向。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文高度匹配多个正面指标： *   **核心概念**: 明确以“Large Language Models (LLMs)”为核心组件。 *   **能力方向**: 论文的核心就是“Deep Reasoning”（深度推理），具体表现为“theorem proving”（定理证明），这是逻辑推理和数学推理的极致体现。 *   **新兴范式**: 论文标题和摘要都清晰地表明这是一个“Agentic Framework”（智能体框架）、“multi-agent system”（多智能体系统），并且核心机制是“tool use”（工具使用）。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 这是最需要仔细辨析的一点。论文虽然涉及“Mathematics”和“Quantum Physics”，但它们被用作**验证框架通用性的测试平台**，而非研究的最终目标。论文的焦点在于框架本身的**通用性和泛化能力**，而不是在数学或物理领域取得突破。因此，它不属于“将LLM作为一种工具，应用到某个特定领域”的排除范畴。论文不涉及多模态、视觉或模型可靠性（水印、安全）等排除领域。 4.  **第四步：处理特殊和模糊情况** 本论文是“智能体/工具使用”这一特殊情况的完美范例。它提出的并非“用于化学的智能体”，而是一个**通用的、基于工具的智能体定理证明方法**，并展示了该方法在数学、抽象代数、量子理论等多个不同领域的有效性。这完全符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”的原则。 **最终决策**: 综合以上分析，Ax-Prover这篇论文的核心是研究如何通过智能体框架和工具使用来**系统性地提升大语言模型在复杂、严谨逻辑推理任务上的通用能力**。它提出的方法论具有跨领域的泛化潜力，其研究目标与您“提高大语言模型本身的『通用推理能力』”的核心目标高度一致。因此，这篇论文是您研究课题的前沿和相关文献，应被筛选出来。"
    },
    {
        "index": "#7",
        "title": "Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations",
        "link": "/arxiv/2510.12699",
        "arxiv_id": "2510.12699",
        "authors": "Sunny Yu, Ahmad Jabbar, Robert Hawkins, Dan Jurafsky, Myra Cheng",
        "summary": "Different open-ended generation tasks require different degrees of output diversity. However, current LLMs are often miscalibrated. They collapse to overly homogeneous outputs for creative tasks and hallucinate diverse but incorrect responses for factual tasks. We argue that these two failure modes are unified by, and can both be addressed by, the notion of effective generation space size (GSS) -- the set of semantically distinct outputs a model considers for a prompt. We present GSSBench, a task suite of prompt pairs with ground-truth GSS relationships to assess different metrics and understand where models diverge from desired behavior. We find that hallucination detection metrics, particularly EigenScore, consistently outperform standard diversity and uncertainty quantification metrics, while using only model internals, providing interpretable insights into a model's internal task representations. We demonstrate three applications of GSS: (1) detecting prompt ambiguity and predicting clarification questions for better grounding, (2) interpreting overthinking and underthinking in reasoning models, and (3) steering models to expand their generation space to yield high-quality and diverse outputs.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.812425",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心并非将LLM应用于某个特定领域，而是致力于理解和改进LLM本身的一种基础性、通用性的能力。论文提出了“有效生成空间大小（GSS）”这一新概念，用以解释和校准LLM在开放性生成任务中的两种核心失败模式：在创造性任务中输出过于同质化，以及在事实性任务中产生多样但错误的幻觉。这本质上是在探究和提升LLM的内在生成机制和可控性，属于改进LLM基础能力的范畴。其最终目标是让模型能够根据任务需求，生成既高质量又具备适当多样性的输出，这直接关系到模型进行高质量、可靠推理的基础。 **第二步：正面指标——论文是否包含相关主题？** 论文高度符合多个正面指标： - **核心概念**: 论文研究的核心对象就是大语言模型（LLMs）。 - **能力方向**: 论文直接探讨了与推理能力密切相关的“过度思考（overthinking）”和“思考不足（underthinking）”现象。通过校准生成空间，论文旨在提升模型输出的质量和可靠性，这是通用推理能力的重要组成部分。 - **新兴范式**: 论文提出的“引导模型扩展其生成空间”的方法，可以被视为一种增强模型问题解决能力的通用技术，与自我进化、深度研究等范式在目标上是一致的。 **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文完全不涉及任何排除标准中的领域。它没有讨论多模态、特定应用场景，也不是从应用层面研究模型可靠性（如水印或安全对策）。虽然它研究“幻觉”，但其切入角度是根本性的——通过模型内部表示来理解和量化幻觉，并提出校准方法，这正是为了从根源上提升模型能力，而非仅仅是应用层面的防御。 **第四步：处理特殊和模糊情况** 论文对“幻觉”的处理方式完美地契合了筛选标准中的“应保留”情况。它没有停留在现象描述或社会影响分析，而是深入到模型内部机制，提出了一种可量化的新方法（GSS）来诊断和缓解幻觉，其直接目的是提升模型的内在可靠性和推理质量。同样，对于“可解释性”，论文通过EigenScore等内部指标提供了对模型任务表示的可解释性洞察，这服务于提升模型能力这一核心目标。 **第五步：最终决策** 综合来看，这篇论文的核心贡献是提出了一种名为“有效生成空间大小（GSS）”的新理论和评估框架，用于理解和校准LLM的开放性生成行为。它从模型的内在生成机制出发，解决了模型在事实性和创造性任务中的两种关键缺陷，并直接应用于提升推理过程的“过度/不足思考”问题的诊断。这项工作深刻地触及了LLM通用推理能力的底层逻辑——如何让模型在保持准确性的同时具备灵活性和可控性。因此，它完全符合“致力于提高大语言模型本身的通用推理能力”这一核心研究目标。"
    },
    {
        "index": "#2",
        "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
        "link": "/arxiv/2510.12773",
        "arxiv_id": "2510.12773",
        "authors": "Ahmed Heakl, Martin Gubri, Salman Khan, Sangdoo Yun, Seong Joon Oh",
        "summary": "Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.809903",
        "filter_reason": "这篇论文完全符合研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为Dr.LLM的**动态层路由框架**。这个框架的本质不是将LLM应用于某个特定领域，而是**改进LLM自身的推理机制**。它通过训练轻量级的“路由器”，让模型能够根据问题的复杂度，动态地决定是跳过、执行还是重复某些transformer层。对于简单的查询，模型可以“抄近路”以节省计算；对于需要“深度推理”的难题，模型可以“多走几步”（重复执行某些层）来进行更充分的思考。这是一种对LLM基础推理过程的根本性增强，完全符合“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的核心要求。 2.  **第二步：正面指标** 论文与多个正面指标高度相关： *   **核心概念**: 论文标题和摘要明确聚焦于“Large Language Models (LLMs)”。 *   **能力方向**: 摘要开篇就点出问题在于LLM对需要“deeper reasoning”（深度推理）的难题处理不足。其方法在ARC（逻辑）和DART（数学）等推理基准上取得了显著的准确性提升，这直接证明了其在增强**推理能力**方面的有效性。 *   **训练方法**: 论文使用“Monte Carlo Tree Search (MCTS)”来生成高质量的层配置作为监督信号，这是一种复杂的搜索和规划算法，与强化学习（RL）的思想一脉相承，旨在优化模型的决策过程。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准： *   它没有涉及视觉或多模态内容。 *   它的研究方法是通用的，并在多个通用领域（MMLU, GSM8k等）的基准上进行了验证，而非聚焦于医疗、化学等特定应用领域。 *   它的研究目标是提升模型的推理准确性和效率，而不是水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 虽然论文不直接讨论智能体或工具使用，但其核心思想——**动态分配计算资源以解决不同难度的问题**——与通用智能体框架中“根据任务情况决定思考时长或使用何种工具”的理念高度一致。Dr.LLM可以被看作是一种增强模型“内部思考”能力的通用框架，因此应当保留。 5.  **第五步：最终决策** 综合以上分析，Dr.LLM这篇论文提出了一种创新的方法，通过让模型自适应地调整其计算深度（即“思考”的深度），来直接提升其在逻辑和数学等通用推理任务上的表现。它不是应用层面的研究，而是对LLM核心推理机制的深刻改进。因此，这篇论文与“提高大语言模型（LLM）本身的『通用推理能力』”这一研究目标高度契合，应当被保留。"
    },
    {
        "index": "#5",
        "title": "Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages",
        "link": "/arxiv/2510.12722",
        "arxiv_id": "2510.12722",
        "authors": "Nadine El-Naggar, Tatsuki Kuribayashi, Ted Briscoe",
        "summary": "Whether language models (LMs) have inductive biases that favor typologically frequent grammatical properties over rare, implausible ones has been investigated, typically using artificial languages (ALs) (White and Cotterell, 2021; Kuribayashi et al., 2024). In this paper, we extend these works from two perspectives. First, we extend their context-free AL formalization by adopting Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover attested but previously overlooked constructions, such as unbounded dependency and mildly context-sensitive structures. Second, our evaluation focuses more on the generalization ability of LMs to process unseen longer test sentences. Thus, our ALs better capture features of natural languages and our experimental paradigm leads to clearer conclusions -- typologically plausible word orders tend to be easier for LMs to productively generalize.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.811317",
        "filter_reason": "这篇论文符合研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是研究语言模型（LMs）的**长度泛化能力**。具体来说，它探究了不同的语法结构（词序）如何影响模型处理未见过的、更长句子的能力。这属于对LLM**基础能力**的深入探究。长度泛化能力是模型进行复杂、多步推理的前提。如果一个模型无法理解并泛化到更长的句子结构，它就难以处理需要长链条逻辑或数学推导的问题。因此，这篇论文的本质是**理解和改进LLM的基础泛化与结构学习能力**，这直接关系到其通用推理能力的上限，符合“改进LLM的基础能力”的保留标准。它并非将LLM应用于特定领域，而是进行基础性研究。 2.  **第二步：正面指标** - **核心概念**: 论文明确研究 \"language models (LMs)\"。 - **能力方向**: 论文的核心评估指标是 \"generalization ability\"，特别是 \"length generalization\"。虽然摘要中没有直接使用 \"reasoning\" 一词，但长度泛化能力是衡量模型能否理解深层语法和逻辑结构的关键指标，是推理能力的重要组成部分。能够泛化到更长的句子，意味着模型掌握了其背后的组合规则，而非简单记忆，这本身就是一种高级的模式识别和逻辑推理过程。 3.  **第三步：排除标准** - 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）。它使用人工语言（ALs）正是为了剥离领域特定知识，专注于模型本身的语言学泛化能力。因此，它不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况。 5.  **第五步：最终决策** 综合来看，这篇论文虽然不是提出像CoT那样的新训练范式，但它通过严谨的实验设计，揭示了影响LLM长度泛化能力的关键因素——语法结构的合理性。这种对模型基础能力的深刻洞察，是未来设计出更强推理能力模型的理论基石。它回答了“LLM的推理能力在结构泛化上存在何种局限以及为何存在”这一根本问题，完全契合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#8",
        "title": "Reasoning Pattern Matters: Learning to Reason without Human Rationales",
        "link": "/arxiv/2510.12643",
        "arxiv_id": "2510.12643",
        "authors": "Chaoxu Pang, Yixuan Cao, Ping Luo",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities under the widely adopted SFT+RLVR paradigm, which first performs Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories (rationales) to establish initial reasoning behaviors, then applies Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model using verifiable signals without golden rationales. However, annotating high-quality rationales for the SFT stage remains prohibitively expensive. This paper investigates when and how rationale annotation costs can be substantially reduced without compromising reasoning performance. We identify a broad class of problems, termed patterned reasoning tasks, where reasoning follows a fixed, procedural strategy consistent across instances. Although instances vary in content such as domain knowledge, factual information, or numeric values, the solution derives from applying a shared reasoning pattern. We argue that the success of SFT+RLVR on such tasks primarily stems from its ability to enable models to internalize these reasoning patterns. Using numerical semantic matching as a representative task, we provide both causal and behavioral evidence showing that reasoning patterns rather than the quantity or quality of rationales are the key determinant of performance. Building on these insights, we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet effective framework that enables LLMs to generate rationales aligned with task-specific reasoning patterns without requiring human rationale annotations. Experiments show that PARO-generated rationales achieve comparable SFT+RLVR performance to human rationales that are 10 times larger. These results suggest that large-scale human rationale annotations can be replaced with LLM-based automatic annotations requiring only limited human supervision over reasoning patterns.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.812884",
        "filter_reason": "这篇论文完全符合您的筛选标准，是关于提升大语言模型通用推理能力的前沿研究。 1.  **核心判断（第一步）：论文的本质是改进LLM的基础推理能力。** 论文的核心贡献并非将LLM应用于某个特定领域，而是深入探究并改进了训练LLM进行推理的方法论。它首先分析了当前主流的SFT+RLVR（监督微调+可验证奖励强化学习）范式，指出了其依赖昂贵人工标注推理轨迹的瓶颈。接着，论文提出了一个关键洞见：对于一大类推理任务，起决定性作用的是通用的“推理模式”，而非具体的推理内容。基于此，它提出了一种名为PARO的新框架，让LLM自己生成符合推理模式的训练数据，从而在不牺牲性能的前提下，大幅降低了对人工标注的依赖。这是一种典型的、旨在增强LLM内在推理能力的**新训练范式**研究，与您的核心目标高度一致。 2.  **正面指标（第二步）：论文命中了多个关键正面指标。** - **核心概念**: 论文明确以“Large Language Models (LLMs)”为研究对象。 - **能力方向**: 论文的标题和摘要通篇围绕“reasoning”（推理）能力展开，深入探讨如何提升“reasoning capabilities”。 - **训练方法**: 论文的核心内容就是对现有训练方法（SFT, RLVR）的分析和改进，提出了一种新的、更高效的训练框架（PARO），这与“强化学习优化”、“自我进化”等方法论研究同属一类。 - **新兴范式**: PARO框架让LLM自主生成训练数据以提升自身，这与“自我进化”的理念不谋而合，是一种非常前沿的探索。 3.  **排除标准（第三步）：论文不涉及任何排除领域。** 论文的研究范围纯粹集中在LLM的文本推理能力上，完全不涉及多模态、视觉、医疗、化学、机器人控制等特定应用领域，也未讨论水印、安全等应用层面的可靠性问题。它所使用的“数值语义匹配”任务仅仅是作为一个代表性的例子来验证其关于“推理模式”的假设，研究焦点始终是通用的方法论，而非该任务本身。 **总结**: 该论文精准地定位在“如何提升LLM通用推理能力”这一核心问题上。它通过深刻的分析，提出了关于推理模式的新见解，并设计出一种创新的、成本更低的训练框架来增强模型的这一基础能力。其贡献是方法论层面的，对整个LLM推理研究社区具有普遍价值，因此完全符合您的筛选要求，应予以保留。"
    },
    {
        "index": "#14",
        "title": "BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)",
        "link": "/arxiv/2510.12516",
        "arxiv_id": "2510.12516",
        "authors": "Tomas Ruiz, Siyao Peng, Barbara Plank, Carsten Schwemmer",
        "summary": "Test-time scaling is a family of techniques to improve LLM outputs at inference time by performing extra computation. To the best of our knowledge, test-time scaling has been limited to domains with verifiably correct answers, like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025 tasks to evaluate annotation disagreements. We experiment with three test-time scaling methods: two benchmark algorithms (Model Averaging and Majority Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM performance consistently on the LeWiDi tasks, but the Best-of-N method does not. Our experiments suggest that the Best-of-N method does not currently transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for this gap.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.820866",
        "filter_reason": "这篇论文符合研究范围。 我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是分析“Best-of-N测试时缩放”这一技术的局限性。测试时缩放是一种通过在推理时增加计算量来提升LLM输出的通用方法论，其本身就是为了增强模型能力。这篇论文并非将LLM作为工具应用于某个特定领域（如化学、医疗），而是研究一种提升LLM自身通用能力的方法。它探讨了为什么一个在数学推理（一种典型的通用推理能力）上有效的方法，在处理“注释分歧”这类更模糊的任务时会失效。这种对技术边界和失效原因的深入分析，是改进和创造更强大通用推理方法的基础性工作，因此其本质是改进LLM的基础能力。 2.  **第二步：正面指标** 论文明确包含了核心概念“LLM”，并聚焦于“reasoning”（特别是作为基准的math reasoning）。它研究的“test-time scaling”是当前提升LLM推理能力的前沿范式之一。这些都表明它与研究目标高度相关。 3.  **第三步：排除标准** 论文不涉及多模态、视觉，也没有聚焦于任何特定的应用领域（如医疗、法律等）。它研究的“注释分歧”问题可以被视为一个通用的自然语言理解和推理挑战，而非领域特定问题。因此，不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** 本论文不直接涉及智能体/工具使用或幻觉/安全等特殊情况。但其研究精神与“分析模型内在能力”是一致的。通过剖析Best-of-N方法为何在特定任务上“水土不服”，论文实际上是在探索LLM推理机制的深层问题，这对于提升模型的通用可靠性和推理质量具有间接但重要的价值。 5.  **第五步：最终决策** 综合来看，这篇论文的核心贡献在于**剖析了一种通用推理增强方法（Best-of-N测试时缩放）的适用边界和失效原因**。它不是简单地应用LLM，而是对LLM能力增强技术本身进行深入的实证研究和分析。理解“什么方法在什么条件下有效/无效”是推动该领域进步的关键。因此，这篇论文完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    },
    {
        "index": "#12",
        "title": "Teaching Language Models to Faithfully Express their Uncertainty",
        "link": "/arxiv/2510.12587",
        "arxiv_id": "2510.12587",
        "authors": "Bryan Eikema, Evgenia Ilia, José G. C. de Souza, Chrysoula Zerva, Wilker Aziz",
        "summary": "Large language models (LLMs) often miscommunicate their uncertainty: repeated queries can produce divergent answers, yet generated responses are typically unhedged or hedged in ways that do not reflect this variability. This conveys unfaithful information about the uncertain state of the LLMs' knowledge, creating a faithfulness gap that affects even strong LLMs. We introduce Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches instruction-tuned LLMs to express uncertainty faithfully without altering their underlying answer distribution. We construct training data by augmenting model samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or 'likely') aligned with sample consistency, requiring no supervision beyond the model and a set of prompts. We evaluate FUT on open-domain question answering (QA) across multiple models and datasets. Our results show that FUT substantially reduces the faithfulness gap, while preserving QA accuracy and introducing minimal semantic distribution shift. Further analyses demonstrate robustness across decoding strategies, choice of hedgers, and other forms of uncertainty expression (i.e. numerical). These findings establish FUT as a simple and effective way to teach LLMs to communicate uncertainty faithfully.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.820005",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Faithful Uncertainty Tuning (FUT)”的新颖微调方法。该方法并非将LLM应用于特定领域，而是直接作用于LLM本身，旨在改进模型的一个基础能力：如何忠实地表达其自身的不确定性。虽然它没有直接提升模型在数学或逻辑问题上的解题正确率，但它通过让模型“知道自己不知道”并能准确表达出来，极大地提升了模型输出的**可靠性**和**推理质量**。一个能够正确评估并传达自身知识边界的模型，是通用推理能力的重要组成部分，因为它避免了无根据的、错误的“幻觉式”推理，使模型作为一个推理整体更加可信和稳健。因此，这篇论文的本质是改进LLM的基础能力，符合核心判断的保留标准。 2.  **第二步：正面指标** 论文明确包含多个正面指标： *   **核心概念**: 论文的核心研究对象是 \"Large language models (LLMs)\"。 *   **能力方向**: 论文在 \"open-domain question answering (QA)\" 任务上进行评估，这属于一种通用的 \"problem-solving\"。虽然不直接等同于数学或逻辑推理，但准确表达不确定性是高质量推理过程的一部分。 *   **训练方法**: 论文提出了一种新的微调范式，这与筛选标准中提到的“提出新的训练范式”完全一致。 3.  **第三步：排除标准** 论文完全避除了所有的排除标准： *   论文研究对象是纯文本语言模型，不涉及多模态与视觉。 *   论文的评估任务是开放域问答，并非医疗、化学等特定应用领域。 *   论文的研究焦点是模型内在的表达机制，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文是符合特殊情况的典型案例。筛选标准中提到：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” *   **减少幻觉**: 论文解决的“faithfulness gap”问题，即模型内心不确定但口头上却很自信，这正是“幻觉”产生的一种根源。FUT通过让模型表达不确定性，有效抑制了“自信的错误答案”，从而减少了幻觉。 *   **增强内在可解释性**: 让模型输出“可能”或“我不确定”等不确定性标记，极大地增强了模型决策过程的透明度和可解释性。用户可以从中了解到模型对答案的置信度，这是内在可解释性的一个重要维度。 *   **提升通用可靠性和推理质量**: 一个可靠的表达不确定性的模型，其输出的整体信息质量更高，用户可以据此做出更明智的判断。这直接提升了模型在通用问题解决场景下的可靠性，使其成为一个更值得信赖的推理伙伴。 5.  **第五步：最终决策** 综合以上分析，这篇论文虽然不直接提升LLM的计算或逻辑推理速度，但它通过一种创新的微调范式，解决了LLM在表达不确定性方面的根本缺陷。这不仅增强了模型的内在可解释性和通用可靠性，更是提升其整体推理质量和可信度的关键一环。一个优秀的通用推理模型，不仅要“会算”，更要“知道自己算得对不对”。因此，该论文完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    },
    {
        "index": "#18",
        "title": "Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation",
        "link": "/arxiv/2510.12460",
        "arxiv_id": "2510.12460",
        "authors": "Linfeng Gao, Baolong Bi, Zheng Yuan, Le Wang, Zerui Chen, Zhimin Wei, Shenghua Liu, Qinggang Zhang, Jinsong Su",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to enhance the factuality of Large Language Models (LLMs). However, existing RAG systems often suffer from an unfaithfulness issue, where the model's response contradicts evidence from the retrieved context. Existing approaches to improving contextual faithfulness largely rely on external interventions, such as prompt engineering, decoding constraints, or reward-based fine-tuning. These works treat the LLM as a black box and overlook a crucial question: how does the LLM internally integrate retrieved evidence with its parametric memory, particularly under knowledge conflicts? To address this gap, we conduct a probing-based analysis of hidden-state representations in LLMs and observe three findings: knowledge integration occurs hierarchically, conflicts manifest as latent signals at the sentence level, and irrelevant context is often amplified when aligned with parametric knowledge. Building on these findings, we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a framework that (i) decomposes context into fine-grained sentence-level knowledge, (ii) employs hidden-state probing to localize conflicting knowledge, and (iii) introduces conflict-aware fine-tuning to guide the model to accurately integrate retrieved evidence. Extensive experiments across three benchmarks demonstrate that CLEAR substantially improves both accuracy and contextual faithfulness, consistently outperforming strong baselines under diverse conflict conditions. The related resources are available at https://github.com/LinfengGao/CLEAR.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.822758",
        "filter_reason": "这篇论文完全符合我的研究范围。以下是我的详细判断过程： **第一步：核心判断——这篇论文的本质是什么？** 这篇论文的本质是改进LLM自身处理信息的能力，特别是当外部信息（检索到的知识）与内部知识（模型参数记忆）发生冲突时的处理机制。其核心贡献是提出了一种新的训练范式（冲突感知微调）和框架（CLEAR），通过分析模型的内部隐藏状态来引导模型更准确地整合信息。这并非将LLM应用于某个特定领域，而是致力于提升LLM的基础能力——即忠实于证据并进行逻辑一致的信息整合。这种能力是通用推理的基石。因此，根据第一步标准，应**保留**。 **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文明确以\"Large Language Models (LLMs)\"为核心研究对象。 - **能力方向**: 论文的核心是解决\"unfaithfulness issue\"（不忠实问题），即模型输出与证据的矛盾。一个能够忠实于证据、解决知识冲突的模型，其逻辑推理和问题解决能力必然得到增强。这直接关联到`reasoning`和`problem-solving`。 - **训练方法**: 论文提出了一种新颖的微调方法，即\"conflict-aware fine-tuning\"（冲突感知微调），这可以被视为一种新的训练范式。 - **新兴范式**: 论文的研究背景是\"Retrieval-Augmented Generation (RAG)\"，这是一种典型的`tool use`范式。论文提出的CLEAR框架旨在通用地增强RAG的效果。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施（如部署、硬件）。因此，根据第三步标准，不应排除。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 本文研究的RAG是工具使用的一种形式。论文提出的CLEAR框架是一种通用的、旨在提升RAG忠实度的方法，而不是应用于特定领域的智能体。这完全符合“提出一种通用的...工具使用方法来增强LLM的通用问题解决能力”的保留标准。 - **幻觉/可解释性/安全**: 本文研究的\"不忠实性\"问题与\"幻觉\"高度相关。但论文没有停留在现象描述或应用层面对策，而是深入到模型的内部机制（\"probing-based analysis of hidden-state representations\"），提出了一种从根本上提升模型可靠性的新方法（冲突感知微调）。这完全符合“提出一种新方法来减少幻觉...从而提升模型的通用可靠性和推理质量，应该保留”的标准。 **第五步：最终决策** 综合以上分析，这篇论文的核心是揭示并解决LLM在整合内外部知识时的根本性矛盾。它通过提出一种创新的、基于模型内部状态的训练框架（CLEAR），来增强模型在复杂信息环境下的逻辑一致性和事实忠实性。这直接提升了LLM的通用推理质量，因为它教会了模型如何“思考”和处理冲突信息，而不是简单地复述其既有知识。因此，这篇论文与我“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度契合。"
    },
    {
        "index": "#19",
        "title": "PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation",
        "link": "/arxiv/2510.12434",
        "arxiv_id": "2510.12434",
        "authors": "Xiangjun Zai, Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Wenjie Zhang",
        "summary": "Knowledge Hypergraphs (KHs) have recently emerged as a knowledge representation for retrieval-augmented generation (RAG), offering a paradigm to model multi-entity relations into a structured form. However, existing KH-based RAG methods suffer from three major limitations: static retrieval planning, non-adaptive retrieval execution, and superficial use of KH structure and semantics, which constrain their ability to perform effective multi-hop question answering. To overcome these limitations, we propose PRoH, a dynamic Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates three core innovations: (i) a context-aware planning module that sketches the local KH neighborhood to guide structurally grounded reasoning plan generation; (ii) a structured question decomposition process that organizes subquestions as a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive, multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided reasoning path retrieval algorithm that prioritizes semantically coherent hyperedge traversals. Experiments across multiple domains demonstrate that PRoH achieves state-of-the-art performance, surpassing the prior SOTA model HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation (G-E) score, while maintaining strong robustness in long-range multi-hop reasoning tasks.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.823209",
        "filter_reason": "这篇论文完全符合您的筛选标准，应被保留。以下是我的详细判断过程： **第一步：核心判断** 论文的核心贡献是提出了一种名为PRoH的新框架，旨在通过动态规划和推理来增强检索增强生成（RAG）的效果。其本质是改进大语言模型在处理需要多步、多跳推理的复杂问题时的能力。这并非将LLM作为工具应用于某个特定领域（如生物或化学），而是直接作用于LLM的推理过程本身，提出了一种通用的方法论来提升其“多跳问题回答”和“长程多跳推理”能力。因此，这篇论文的核心是改进LLM的基础推理能力，符合“保留”标准。 **第二步：正面指标** 论文与多个正面指标高度相关： - **核心概念**: 论文的研究对象是检索增强生成（RAG），这是当前大语言模型（LLMs）研究的一个核心方向。 - **能力方向**: 论文摘要中明确、反复地提到了\"reasoning\"、\"planning\"、\"multi-hop question answering\"和\"long-range multi-hop reasoning\"。这些都是通用推理能力的关键组成部分，直接命中了您的研究目标。 - **新兴范式**: 论文提出的框架具有\"llm-based agents\"的特征。它包含一个\"上下文感知的规划模块\"和一个\"动态演化的DAG\"来组织子问题，这本质上是一个让LLM能够自主规划、分解问题、并自适应探索的智能体框架。这与\"deep research\"的范式也高度契合。 **第三步：排除标准** 论文完全避开了所有排除标准： - **多模态与视觉**: 论文聚焦于文本和知识超图，不涉及任何视觉内容。 - **特定应用领域**: 摘要明确指出其实验在\"多个领域\"进行，表明这是一个通用框架，而非为医疗、化学等特定领域设计的应用。 - **模型可靠性**: 论文关注的是推理性能（F1, G-E分数）和鲁棒性，而非水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的绝佳范例。PRoH框架将知识超图作为一种结构化的“工具”或“环境”，让LLM在其中进行动态规划和推理。其目的是解决通用的多跳推理问题，而不是某个特定领域的问题，因此应当保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种创新的框架（PRoH），通过引入动态规划和自适应探索机制，直接提升大语言模型的通用多步推理能力。它不涉及特定应用领域，也不属于基础设施或排除范畴。其研究内容与方法论与您“提高大语言模型本身的通用推理能力”的核心目标高度一致。因此，最终判断为 **True**，应将该论文纳入研究范围。"
    },
    {
        "index": "#28",
        "title": "DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering",
        "link": "/arxiv/2510.12251",
        "arxiv_id": "2510.12251",
        "authors": "Jiakai Li, Rongzheng Wang, Yizhuo Ma, Shuang Liang, Guangchun Luo, Ke Qin",
        "summary": "While large language models (LLMs) show considerable promise across various fields, they have notable limitations in handling multi-document question answering (Multi-doc QA) tasks. The first challenge is long-range dependency modeling, where LLMs struggle to focus on key information in long texts, which weakens important semantic connections. Second, most LLMs suffer from the ''lost-in-the-middle'' issue, where they have difficulty processing information in the middle of long inputs. Current solutions either truncate global dependencies or demand costly finetuning, ultimately lacking a universal and simple solution for these challenges. To resolve these limitations, we propose Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by assessing paragraph relevance through layer-wise attention tracking and position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS) module enhances focus on critical paragraphs by suppressing information exchange between key and irrelevant texts, thus mitigating the limitations in long-range dependency modeling. Notably, DSAS functions as a plug-and-play solution requiring no architectural modifications or extra training parameters. Extensive experiments on four benchmarks demonstrate DSAS's efficacy across mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of both the CGW and RAS modules. In addition, detailed discussions in the Appendix further validate the robustness and scalability of DSAS.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.832482",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为DSAS（双阶段自适应锐化）的通用框架，用于优化LLM在处理长文本时的注意力机制。它旨在解决LLM在长上下文理解中的两个关键瓶颈：“lost-in-the-middle”问题和长距离依赖建模难题。这两个问题直接限制了模型在需要综合多处信息才能得出答案的任务上的表现。虽然论文的实验设定在“多文档问答”任务上，但其提出的方法是一种**通用且即插即用**的解决方案，不依赖于特定领域的知识或模型架构的修改。改进模型对长上下文信息的聚焦和关联能力，是提升其在规划、逻辑链构建等复杂推理任务上表现的基础。因此，这篇论文的本质是改进LLM处理信息这一**基础能力**，从而间接但有力地增强了其通用推理潜力，而非将其作为工具应用于特定领域。 2.  **第二步：正面指标** - **核心概念**: 论文明确以大语言模型为核心研究对象。 - **能力方向**: 论文聚焦的问题与“reasoning”和“problem-solving”高度相关。多文档问答本身就要求模型具备信息检索、比较、综合和推导的能力。DSAS通过优化注意力，增强了模型解决此类问题的基础能力。 - **训练方法**: 论文并非提出新的训练方法，而是提出一种无需额外训练的推理时优化方法，这属于模型能力增强的一种新范式。 3.  **第三步：排除标准** - 论文不涉及多模态、视觉等方向。 - 论文的应用场景是多文档问答，这是一个通用NLP任务，而非医疗、化学、生物等特定应用领域。 - 论文不涉及模型水印、安全性等应用层面的可靠性研究。 4.  **第四步：处理特殊和模糊情况** 本论文不属于模糊情况的范畴。它既不是特定领域的智能体，也不是对幻觉现象的社会学探讨。它提出的是一个直接的、模型内部的机制优化方案。 5.  **第五步：最终决策** 综合以上分析，尽管论文的标题和摘要将应用场景限定在“多文档问答”，但其提出的DSAS框架具有**通用性**和**普适性**，旨在解决LLM在长上下文理解方面的根本性缺陷。这种缺陷是阻碍LLM实现更高级通用推理能力的关键瓶颈之一。因此，该论文的研究内容与“提高大语言模型本身的通用推理能力”这一核心目标高度契合，应当被筛选出来。"
    },
    {
        "index": "#34",
        "title": "Towards Inference-time Scaling for Continuous Space Reasoning",
        "link": "/arxiv/2510.12167",
        "arxiv_id": "2510.12167",
        "authors": "Minghan Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari",
        "summary": "Inference-time scaling through multiple sample generation in combination with Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective for text-based reasoning in large language models. This paper investigates whether such established techniques can be successfully adapted to reasoning in the continuous space, using COCONUT (Hao et al. 2024) continuous space reasoning LM as the backbone. We demonstrate the feasibility of generating diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on the generated samples reveals the potential that could enable a significant gain in performance akin to observed gain in the discrete space. However, we highlight unique challenges faced for materializing this gain in the continuous thought space. In particular, working recipes for data generation and training PRM and ORM models in the discrete space unlocks only marginal improvements in the continuous space. Through probing various aspects including geometric properties and trajectory dynamics we identify the underlying reasons that prevent effective discrimination between correct and incorrect reasoning (essential for the functioning of PRM and ORM). Our findings reveal that current limitations stem from the absence of key inductive biases in continuous thought representations. We argue that the training frameworks for continuous reasoning LMs require not only to optimize for accuracy but also to explicitly incorporate inductive biases that could be utilized during inference-time for discrimination of correct and incorrect thoughts.\\footnote{Our code and data will be publicly available.}",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.851002",
        "filter_reason": "这篇论文完全符合你的研究范围，是关于提升大语言模型通用推理能力的前沿研究。以下是根据你的筛选标准进行的详细判断： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是**改进LLM的推理方法论**。它没有将LLM作为工具应用于特定领域，而是聚焦于一个核心问题：如何将在离散文本推理中已被证明有效的“推理时扩展”技术，迁移并应用于更具挑战性的“连续空间推理”。论文的核心贡献在于指出了现有方法在新范式下的局限性，分析了其根本原因（缺乏归纳偏置），并为未来的训练框架指明了方向。这完全符合“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的核心目标。 **第二步：正面指标——论文是否包含以下主题？** 该论文命中了多个关键正面指标： - **核心概念**: 明确以 \"Large language models (LLMs)\" 和 \"continuous space reasoning LM\" 为研究对象。 - **能力方向**: 论文的标题和摘要的核心就是 \"reasoning\"，特别是探索 \"text-based reasoning\" 技术在 \"continuous space reasoning\" 上的适用性，这直接关系到模型的数学和逻辑推理能力。 - **训练方法**: 论文深入探讨了 \"Process- or Outcome-Reward Model (PRM or ORM)\" 这一与强化学习（RL）紧密相关的训练/评估范式在连续空间中的表现和挑战。 - **新兴范式**: \"Inference-time scaling\" 和 \"PRM/ORM re-ranking\" 正是当前提升LLM推理能力的前沿研究范式。论文是对这一范式的深化和批判性研究。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 该论文完全避开了所有排除标准： - **多模态与视觉**: 论文研究的是“连续空间”，在本文语境下，它更可能指数学、几何或物理问题的向量表示空间，而非像素、图像等视觉空间。全文未提及Vision, MLLMs等关键词。 - **特定应用领域**: “连续空间推理”是一种通用的基础推理能力，而不是一个特定的应用领域（如医疗、化学）。 - **模型可靠性（应用层面）**: 论文讨论的是模型内在推理路径的“可区分性”，这是提升推理质量的核心问题，而非水印、安全等应用层面的可靠性话题。 **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: 论文的研究内容与“提升模型的通用可靠性和推理质量”高度相关。它通过分析PRM/ORM为何无法有效区分对错，本质上是在探索如何让模型的“思考过程”更可靠、更可判别。这种基础性的方法论研究，旨在从根本上提升推理质量，而非停留在现象的讨论或应用层面的修补，因此应该保留。 **第五步：最终决策** 综合以上分析，这篇论文是一篇高质量的、致力于提升LLM通用推理能力的研究。它没有停留在应用现有技术，而是勇敢地探索了一个新兴的推理范式（连续空间推理），并深刻揭示了现有方法论的根本性缺陷，进而提出了富有洞察力的改进方向（引入归纳偏置）。这种对LLM核心推理能力的深度剖析和探索，正是你所寻找的前沿论文。因此，最终决策为 **True**。"
    },
    {
        "index": "#35",
        "title": "A Survey on Parallel Reasoning",
        "link": "/arxiv/2510.12164",
        "arxiv_id": "2510.12164",
        "authors": "Ziqi Wang, Boye Niu, Zipeng Gao, Zhi Zheng, Tong Xu, Linghui Meng, Zhongli Li, Jing Liu, Yilong Chen, Chen Zhu, Hua Wu, Haifeng Wang, Enhong Chen",
        "summary": "With the increasing capabilities of Large Language Models (LLMs), parallel reasoning has emerged as a new inference paradigm that enhances reasoning robustness by concurrently exploring multiple lines of thought before converging on a final answer. It has become a significant trend to explore parallel reasoning to overcome the fragility of standard sequential methods and improve practical performance. In this paper, we aim to survey and summarize the progress and challenges of parallel reasoning. We first present a formal definition of parallel reasoning and clarify its distinction from related concepts like Chain-of-Thought. Then, we organize and discuss advanced techniques based on a novel taxonomy, including non-interactive reasoning, interactive reasoning, and efficiency-focused decoding strategies. Additionally, we explore various application scenarios, such as solving complex problems and enhancing the reliability of LLM outputs.Finally, we highlight the core challenges of parallel reasoning and suggest potential directions for future research. We hope that our work can provide a useful roadmap for beginners and encourage more research on improving parallel reasoning methods. Related source can be avaliable in https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.851710",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是详细的判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 论文的本质是对“并行推理”这一新兴范式的综述。其核心目标是探讨如何通过“并发探索多条思路”来“增强推理的鲁棒性”并“克服标准顺序方法的脆弱性”。这直接触及了大语言模型通用推理能力的核心——即改进模型进行复杂、多步推理的基础方法论。它并非将LLM应用于特定领域，而是专注于提升LLM自身的推理过程和模式，因此完全符合“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文高度命中了所有关键正面指标： *   **核心概念**: 摘要开篇即点明研究对象是“Large Language Models (LLMs)”。 *   **能力方向**: 论文的标题和摘要反复强调“reasoning”（推理），并且明确指出其目标是提升“reasoning robustness”（推理鲁棒性），这与“通用推理能力”的目标完全一致。 *   **新兴范式**: “并行推理”本身就是一种与“思维链”并列的新兴推理范式，旨在提升模型的问题解决能力。摘要中提到的“solving complex problems”（解决复杂问题）也与此相关。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全避开了所有排除标准。它没有涉及多模态、视觉，也没有将研究限定在医疗、化学等任何特定应用领域。其讨论的“reliability”（可靠性）是内在推理过程的可靠性，而非应用层面的水印或安全策略，因此不属于排除范畴。 4.  **第四步：处理特殊和模糊情况** 本篇论文作为一篇综述，其讨论的“增强LLM输出的可靠性”并非指添加外部安全防护或水印，而是指通过改进内在推理路径（从单一路径到多路并行）来使模型的输出结果本身更稳定、更不容易出错。这完全符合“如果论文提出一种新方法来减少幻觉、增强模型内在的...可靠性，从而提升模型的通用可靠性和推理质量，应该保留”的原则。 **最终决策**: 综合以上分析，这篇题为《A Survey on Parallel Reasoning》的论文，其核心贡献是系统性地梳理和总结了“并行推理”这一旨在提升大语言模型内在推理鲁棒性和通用问题解决能力的前沿范式。它直接服务于“提高大语言模型本身的『通用推理能力』”这一核心目标，是一篇高度相关且极具参考价值的前沿综述论文。因此，应予以保留。"
    },
    {
        "index": "#43",
        "title": "Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models",
        "link": "/arxiv/2510.12044",
        "arxiv_id": "2510.12044",
        "authors": "Yukun Zhang, Qi Dong",
        "summary": "Existing alignment techniques for Large Language Models (LLMs), such as Direct Preference Optimization (DPO), typically treat the model as a monolithic entity, applying uniform optimization pressure across all layers. This approach overlooks the functional specialization within the Transformer architecture, where different layers are known to handle distinct tasks from syntax to abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm by introducing Hierarchical Alignment, a novel method that applies targeted DPO to distinct functional blocks of a model's layers: local (syntax), intermediate (logic), and global (factuality). Through a series of controlled experiments on state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge, demonstrate significant and predictable improvements. Specifically, aligning the local layers (Local-Align) enhances grammatical fluency. More importantly, aligning the global layers (Global-Align) not only improves factual consistency as hypothesized but also proves to be the most effective strategy for enhancing logical coherence, outperforming all baselines. Critically, all hierarchical strategies successfully avoid the \"alignment tax\" observed in standard DPO, where gains in fluency come at the cost of degraded logical reasoning. These findings establish a more resource-efficient, controllable, and interpretable path for model alignment, highlighting the immense potential of shifting from monolithic optimization to structure-aware surgical fine-tuning to build more advanced and reliable LLMs.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.861365",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献是提出了一种新颖的训练范式来直接提升大语言模型的基础推理能力。 **详细判断过程如下:** 1.  **第一步：核心判断——论文的本质是什么？** - **核心贡献**: 论文提出了“分层对齐”方法，这是一种针对LLM内部不同功能层（语法、逻辑、事实性）进行“外科手术式”微调的新训练范式。它不是将LLM作为工具应用到某个领域，而是直接改进LLM的内部工作机制和对齐策略。 - **与核心目标的关联**: 摘要明确指出，该方法的核心优势之一是显著提升了“逻辑连贯性”，并且是“增强逻辑连贯性最有效的策略”。这直接命中了提升LLM“通用推理能力”中的“逻辑推理”这一关键维度。论文还解决了传统对齐方法中常见的“对齐税”问题，即模型流畅性提升但推理能力下降的副作用，这本身就是对模型通用推理能力的一次重要保护和增强。 - **结论**: 论文的核心是改进LLM的基础能力和提出新的训练范式，旨在增强其逻辑推理能力，因此符合第一步的**保留**标准。 2.  **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文标题和摘要中多次提及 \"Large Language Models (LLMs)\"。 - **能力方向**: 论文明确以 \"logical reasoning\" 和 \"logical coherence\" 为核心优化和评估目标，这与筛选标准中的 \"reasoning (尤其是 logical reasoning)\" 高度吻合。 - **训练方法**: 论文基于 \"Direct Preference Optimization (DPO)\" 进行改进，DPO是当前主流的强化学习对齐技术之一，符合筛选标准中的 \"reinforcement learning (RL)\"。 - **结论**: 论文命中了多个关键的正面指标，进一步确认了其相关性。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** - 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或应用层面的模型可靠性（如水印、安全）。其研究对象是通用的文本基础模型（Llama-3.1-8B, Qwen1.5-7B）。 - **结论**: 论文未触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 不适用。 - **模型可靠性**: 论文虽然提到了“可靠”，但其切入点是通过改进模型内在的推理逻辑来实现的，而不是添加外部的安全或可信机制。它解决了“对齐税”这一深层问题，从而提升了模型在推理任务上的内在可靠性。这完全符合“提出一种新方法来……提升模型的通用可靠性和推理质量，应该保留”的准则。 5.  **第五步：最终决策** - 综合以上所有分析，这篇论文的本质是提出一种创新的、结构感知的微调方法，旨在解决现有对齐技术损害模型推理能力的根本问题。它通过实验证明，有针对性地优化与逻辑相关的模型层，可以显著增强LLM的通用逻辑推理能力，且无损于其他能力。这与研究课题“大语言模型通用推理能力”的目标高度一致，是一篇非常有价值的、符合筛选标准的前沿研究。"
    },
    {
        "index": "#47",
        "title": "Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models",
        "link": "/arxiv/2510.12032",
        "arxiv_id": "2510.12032",
        "authors": "Jung-Woo Shim, Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee",
        "summary": "Recent advancements in large language models (LLMs) have shown strong performance in natural language understanding and generation tasks. However, LLMs continue to encounter challenges with hallucinations, where models generate plausible but incorrect information. While several factors contribute to hallucinations, the impact of ill-formed prompts, prompts with ambiguous wording, incorrect grammar, or incomplete information, was relatively under explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a framework designed to systematically improve these ill-formed prompts across multiple stages. Each stage addresses specific errors such as punctuation, typographical mistakes, and misuse of key terms, using small language models (SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of prompts with additional context and employs a self-reflection mechanism with ranking to prioritize the most relevant input. Experimental results on hallucination benchmarks show that prompts refined by MPR achieve over an 85~\\% win rate compared to their original forms, demonstrating its effectiveness in reducing hallucinations and improving LLM output accuracy. Interestingly, we reveal that MPR can be combined with existing post-hoc hallucination mitigation frameworks, further enhancing its versatility. MPR provides a lightweight and adaptable solution for enhancing LLM reliability across various domains.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.863176",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“多阶段提示优化（MPR）”的框架，其目标是系统性地改进输入LLM的提示，从而减少模型产生的“幻觉”。幻觉是LLM在推理过程中出现事实错误或逻辑不一致的直接表现，因此，减少幻觉本质上是提升模型逻辑推理和事实准确性等通用能力的关键环节。这篇论文并非将LLM作为工具应用于某个特定领域，而是致力于改进LLM本身在生成内容时的可靠性，这直接关系到其通用推理能力的质量。因此，根据第一步的核心判断标准，这篇论文应该**保留**。 2.  **第二步：正面指标** 论文明确包含了核心概念“Large language models (LLMs)”。虽然摘要中没有直接使用“reasoning”一词，但其核心目标“mitigating hallucinations”和“improving LLM output accuracy”是高质量推理的直接结果和衡量标准。论文中提到的“self-reflection mechanism”也与推理过程中的自我审视和修正密切相关。这些正面指标都指向了论文与LLM核心能力的关联性。 3.  **第三步：排除标准** 该论文不涉及多模态、视觉或任何特定应用领域（如医疗、化学等）。它关注的是LLM本身的一个普遍性问题（幻觉），而非某个垂直领域的应用。虽然它涉及“模型可靠性”，但根据第四步的特殊情况处理，这种可靠性是内在的、与推理质量直接相关的，而非应用层面的水印或安全策略。因此，它不触犯任何排除标准。 4.  **第四步：处理特殊和模糊情况** 这篇论文是“幻觉/可解释性/安全”这一特殊情况的典型例子。筛选标准明确指出：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” 本文提出的MPR框架正是一种旨在减少幻觉的新技术方法，其最终效果是“improving LLM output accuracy”和“enhancing LLM reliability”，这完全符合保留条件。它不是对幻觉现象的社会学分析，而是一个提升模型内在推理质量的技术方案。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种通用的、非领域特定的方法（MPR），通过优化提示来减少LLM的幻觉。减少幻觉是提升LLM逻辑一致性、事实准确性和整体推理能力的关键步骤。因此，该论文直接服务于“提高大语言模型本身的通用推理能力”这一核心目标，应被判定为符合研究范围。"
    },
    {
        "index": "#48",
        "title": "CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement",
        "link": "/arxiv/2510.12029",
        "arxiv_id": "2510.12029",
        "authors": "Jung-Woo Shim, Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee",
        "summary": "Recent advancements in large language models (LLMs) highlight their fluency in generating responses to diverse prompts. However, these models sometimes generate plausible yet incorrect ``hallucinated\" facts, undermining trust. A frequent but often overlooked cause of such errors is the use of poorly structured or vague prompts by users, leading LLMs to base responses on assumed rather than actual intentions. To mitigate hallucinations induced by these ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a plug-and-play framework for curative prompt refinement that 1) cleans ill-formed prompts, and 2) generates additional informative task descriptions to align the intention of the user and the prompt using a fine-tuned small language model. When applied to language models, we discover that CPR significantly increases the quality of generation while also mitigating hallucination. Empirical studies show that prompts with CPR applied achieves over a 90\\% win rate over the original prompts without any external knowledge.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.863652",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“治疗性提示优化（CPR）”的框架，用于缓解大语言模型因用户提示词不明确而产生的幻觉。这并非将LLM应用于特定领域，而是直接针对LLM本身的一个核心缺陷——幻觉——进行改进。幻觉是阻碍LLM进行可靠、准确推理的关键障碍。通过优化输入（提示词）来提升输出（生成内容）的质量和事实准确性，该方法本质上是在增强LLM的基础能力和可靠性，这直接服务于提升其通用推理能力的目标。因此，它通过了核心判断。 2.  **第二步：正面指标** 论文明确包含核心概念“Large language models, LLMs”。虽然它没有直接提出一种新的推理范式（如CoT），但其目标“mitigating hallucination”（缓解幻觉）和效果“increases the quality of generation”（提升生成质量）与通用推理能力高度相关。一个不产生幻觉、输出可靠的模型是进行有效逻辑推理和问题解决的前提。 3.  **第三步：排除标准** 该论文不涉及多模态、视觉、特定应用领域（如医疗、化学），也不关注模型基础设施或应用层面的水印、安全等问题。因此，它没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** 这篇论文是“幻觉”这一特殊情况的典型例子。筛选标准明确指出：“如果论文提出一种新方法来减少幻觉……从而提升模型的通用可靠性和推理质量，应该保留。” CPR正是这样一种新颖的方法论。它不是对幻觉现象的社会学讨论，而是提出了一套具体的技术框架来从根源上（提示词层面）解决问题，其最终效果是提升了模型输出的整体质量和可靠性，为更高级的推理任务奠定了坚实基础。 **最终决策**：综合以上分析，这篇论文虽然不是直接提出一种新的推理算法，但它通过系统性地解决“幻觉”这一核心瓶颈问题，显著提升了LLM输出的可靠性和事实准确性。一个可靠的模型是展现通用推理能力的先决条件。因此，该论文的研究内容与“提高大语言模型本身的通用推理能力”这一核心目标高度契合，应予以保留。"
    },
    {
        "index": "#36",
        "title": "Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models",
        "link": "/arxiv/2510.12137",
        "arxiv_id": "2510.12137",
        "authors": "Shihao Ji, Zihui Song, Jiajie Huang",
        "summary": "Large Language Models (LLMs) hallucinate, generating factually incorrect yet confident assertions. We argue this stems from the Transformer's Softmax function, which creates \"Artificial Certainty\" by collapsing ambiguous attention scores into a single probability distribution, discarding uncertainty information at each layer. To fix this, we introduce the Credal Transformer, which replaces standard attention with a Credal Attention Mechanism (CAM) based on evidential theory. CAM produces a \"credal set\" (a set of distributions) instead of a single attention vector, with the set's size directly measuring model uncertainty. We implement this by re-conceptualizing attention scores as evidence masses for a Dirichlet distribution: sufficient evidence recovers standard attention, while insufficient evidence yields a diffuse distribution, representing ambiguity. Empirically, the Credal Transformer identifies out-of-distribution inputs, quantifies ambiguity, and significantly reduces confident errors on unanswerable questions by abstaining. Our contribution is a new architecture to mitigate hallucinations and a design paradigm that integrates uncertainty quantification directly into the model, providing a foundation for more reliable AI.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.852314",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出了一种名为“Credal Transformer”的**新架构**，它通过修改Transformer的核心组件——注意力机制，来从根本上解决LLM的幻觉问题。这并非将LLM作为工具应用于特定领域，而是直接对LLM的**基础能力和内在机制**进行改进。幻觉是阻碍LLM进行可靠推理的关键瓶颈之一，因此，从根本上抑制幻觉，等同于在提升模型推理能力的“地基”。这完全符合“改进LLM的基础能力”这一保留标准。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文明确包含了核心正面指标： *   **核心概念**: 论文聚焦于 \"Large Language Models (LLMs)\"。 *   **能力方向**: 虽然摘要没有直接使用\"reasoning\"这个词，但它解决的\"hallucination\"（幻觉）问题是通用推理质量的直接体现。一个能够识别不确定性并拒绝回答不可靠问题的模型，其逻辑推理和问题解决能力得到了本质上的增强。减少“自信的错误”是提升推理质量的核心环节。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全不涉及排除标准中的任何领域。它不是关于多模态、视觉，也不是关于医疗、化学等特定应用，更不是关于模型部署的基础设施。 4.  **第四步：处理特殊和模糊情况——幻觉/可解释性/安全** 这是判断本论文的关键。根据筛选标准：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” 本论文正是这一情况的完美范例： *   它提出了一种**新方法**（Credal Attention Mechanism）。 *   该方法的目的是**减少幻觉**。 *   它通过在模型架构中直接集成“不确定性量化”，**增强了模型内在的可解释性**（模型现在能“说出”自己不确定）。 *   最终效果是**提升了模型的通用可靠性和推理质量**（减少了事实错误，学会了在无把握时保持审慎）。 这与仅仅研究幻觉现象的社会学分析，或在应用层面对输出进行后处理的安全加固有本质区别。本文是从模型架构的根源上解决问题。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种创新的、架构层面的方法，通过量化和管理不确定性来抑制LLM的幻觉。这项工作直接提升了LLM的内在可靠性，而可靠性是进行高质量通用推理的基石。因此，它与你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度契合。"
    },
    {
        "index": "#53",
        "title": "Scaling Long-Horizon LLM Agent via Context-Folding",
        "link": "/arxiv/2510.11967",
        "arxiv_id": "2510.11967",
        "authors": "Weiwei Sun, Miao Lu, Zhan Ling, Kang Liu, Xuesong Yao, Yiming Yang, Jiecao Chen",
        "summary": "Large language model (LLM) agents are fundamentally constrained by context length on long-horizon tasks. We introduce Context-Folding, a framework that empowers agents to actively manage their working context. An agent can procedurally branch into a sub-trajectory to handle a subtask and then fold it upon completion, collapsing the intermediate steps while retaining a concise summary of the outcome. To make this behavior learnable, we develop an end-to-end reinforcement learning framework FoldGRPO with specific process rewards to encourage effective task decomposition and context management. On complex long-horizon tasks (Deep Research and SWE), our folding agent matches or outperforms the ReAct baselines while using an active context 10$\\times$ smaller and significantly outperforms models that rely on summarization-based context management.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.881670",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 论文的核心贡献是提出了一种名为“Context-Folding”的通用框架，以及一个配套的强化学习训练方法“FoldGRPO”。其根本目标是解决LLM智能体在执行长时程任务时面临的上下文长度限制问题。这个问题直接制约了模型进行复杂、多步推理和规划的能力。论文通过让智能体学会主动“折叠”已完成子任务的上下文，从而在有限的上下文窗口内处理更复杂的任务。这本质上是对LLM**基础推理和规划能力**的一种增强，属于提出新的训练范式和方法论来提升模型通用能力的范畴，而非将其应用于特定领域。 2.  **第二步：正面指标——论文高度相关。** 论文命中了多个关键的正面指标： *   **核心概念**: 明确以 \"Large language model (LLM) agents\" 为研究对象。 *   **能力方向**: 直接针对 \"long-horizon tasks\"，这涉及到 \"planning\" 和 \"problem-solving\" 的核心能力。 *   **训练方法**: 提出了新的 \"end-to-end reinforcement learning framework (FoldGRPO)\" 来训练模型，这与“强化学习优化”标准完全吻合。 *   **新兴范式**: 论文主题是关于 \"LLM Agent\"，并提出了一个通用的框架来增强其能力。 3.  **第三步：排除标准——论文未触及任何排除领域。** 论文的研究焦点非常纯粹，没有涉及多模态、视觉、医疗、化学等任何特定应用领域，也未讨论模型基础设施或应用层面的安全水印等问题。虽然它在SWE（软件工程）任务上进行了测试，但SWE在这里被用作一个验证“长时程任务”能力的通用基准，而非论文的研究目标。论文的核心方法“Context-Folding”是领域无关的。 4.  **第四步：处理特殊和模糊情况——完全符合“保留”条件。** 该论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的典型范例。它提出的“Context-Folding”框架是一种通用的上下文管理策略，旨在提升LLM智能体在**任何**长时程任务中的表现，而不是局限于某个特定领域。 **最终决策：** 综合以上分析，这篇论文的本质是提出一种创新的方法论和训练范式，旨在攻克LLM在长程规划和多步推理方面的核心瓶颈。其研究目标、方法和贡献都与“提高大语言模型本身的通用推理能力”这一核心目标高度一致。因此，应判定为符合要求。"
    },
    {
        "index": "#45",
        "title": "Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions",
        "link": "/arxiv/2510.12040",
        "arxiv_id": "2510.12040",
        "authors": "Sungmin Kang, Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Salman Avestimehr",
        "summary": "The rapid advancement of large language models (LLMs) has transformed the landscape of natural language processing, enabling breakthroughs across a wide range of areas including question answering, machine translation, and text summarization. Yet, their deployment in real-world applications has raised concerns over reliability and trustworthiness, as LLMs remain prone to hallucinations that produce plausible but factually incorrect outputs. Uncertainty quantification (UQ) has emerged as a central research direction to address this issue, offering principled measures for assessing the trustworthiness of model generations. We begin by introducing the foundations of UQ, from its formal definition to the traditional distinction between epistemic and aleatoric uncertainty, and then highlight how these concepts have been adapted to the context of LLMs. Building on this, we examine the role of UQ in hallucination detection, where quantifying uncertainty provides a mechanism for identifying unreliable generations and improving reliability. We systematically categorize a wide spectrum of existing methods along multiple dimensions and present empirical results for several representative approaches. Finally, we discuss current limitations and outline promising future research directions, providing a clearer picture of the current landscape of LLM UQ for hallucination detection.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.862235",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是研究如何通过“不确定性量化（UQ）”这一方法论来检测和缓解大语言模型的“幻觉”问题。这并非将LLM作为工具应用到特定领域，而是直接针对LLM本身的核心缺陷之一——生成不可靠、事实错误的内容——进行深入研究。一个模型的推理能力不仅体现在它能多好地“想出”答案，更体现在它能否判断自己生成的答案是否可靠。因此，提升模型对自身输出可靠性的判断能力，是增强其通用推理能力（特别是鲁棒性和可信度）的根本性工作。论文的核心贡献在于系统性地梳理和构建了用于提升LLM内在可靠性的UQ方法论框架，这直接服务于“提高LLM本身的基础能力”这一核心目标。 2.  **第二步与第三步：正面指标与排除标准** - **正面指标**: 论文核心概念明确聚焦于“Large language models (LLMs)”。虽然摘要没有直接提及“reasoning”，但其研究的核心问题“hallucination”是高质量推理的最大障碍。解决幻觉问题，直接提升模型推理输出的准确性和可靠性。 - **排除标准**: 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施（如部署加速）。它关注的是一个普遍存在于所有LLM中的通用性问题，因此顺利通过所有排除标准。 3.  **第四步：处理特殊和模糊情况** 这篇论文是“幻觉/可解释性/安全”这一特殊情况的典型范例。筛选标准中明确指出：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” - 本论文的核心正是“不确定性量化（UQ）”这一用于“减少幻觉”的方法论。 - 通过量化不确定性，模型能够“识别不可靠的生成结果”，这本质上是增强了模型的一种内在“元认知”能力。 - 这种能力对于提升模型的“通用可靠性”和“推理质量”至关重要。一个可靠的推理系统必须知道自己在何时“不知道”或“不确定”，从而避免做出错误的结论。因此，这项研究是提升LLM通用推理能力链条上不可或缺的一环。 **综合判断**: 尽管这篇论文可能是一篇综述性论文，它并非提出某一种具体的、全新的推理算法（如CoT的变体），但它所探讨的“不确定性量化”是构建更可靠、更智能的LLM的基石。一个无法判断自身输出是否可信的模型，其“推理能力”是脆弱和不完整的。该论文系统地梳理了如何让LLM具备这种自我评估能力，为未来的研究指明了方向，这完全符合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。它关注的是推理能力的“质量保障”和“鲁棒性”维度，是顶级研究中不可或缺的一部分。因此，应予以保留。"
    },
    {
        "index": "#52",
        "title": "Conjecturing: An Overlooked Step in Formal Mathematical Reasoning",
        "link": "/arxiv/2510.11986",
        "arxiv_id": "2510.11986",
        "authors": "Jasivan Alex Sivakumar, Philipp Borchert, Ronald Cardenas, Gerasimos Lampouras",
        "summary": "Autoformalisation, the task of expressing informal mathematical statements in formal language, is often viewed as a direct translation process. This, however, disregards a critical preceding step: conjecturing. Many mathematical problems cannot be formalised directly without first conjecturing a conclusion such as an explicit answer, or a specific bound. Since Large Language Models (LLMs) already struggle with autoformalisation, and the evaluation of their conjecturing ability is limited and often entangled within autoformalisation or proof, it is particularly challenging to understand its effect. To address this gap, we augment existing datasets to create ConjectureBench, and redesign the evaluation framework and metric specifically to measure the conjecturing capabilities of LLMs both as a distinct task and within the autoformalisation pipeline. Our evaluation of foundational models, including GPT-4.1 and DeepSeek-V3.1, reveals that their autoformalisation performance is substantially overestimated when the conjecture is accounted for during evaluation. However, the conjecture should not be assumed to be provided. We design an inference-time method, Lean-FIRe to improve conjecturing and autoformalisation, which, to the best of our knowledge, achieves the first successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1 and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite knowledge to generate accurate conjectures, improving autoformalisation performance requires treating conjecturing as an independent task, and investigating further how to correctly integrate it within autoformalisation. Finally, we provide forward-looking guidance to steer future research toward improving conjecturing, an overlooked step of formal mathematical reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.880993",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 论文的核心贡献在于识别并解决了大语言模型在“形式数学推理”这一高级通用能力中的一个关键瓶颈——“猜想”步骤。它没有将LLM用作解决特定领域问题的工具，而是深入探究并改进了LLM进行复杂推理（数学推理）的内在流程和能力。作者提出了一种新的推理时方法“Lean-FIRe”来增强这一能力，这属于改进LLM基础能力和方法论的研究范畴，完全符合“保留”标准。 2.  **第二步：正面指标——论文高度相关。** 论文明确包含了多个正面指标： *   **核心概念**: 论文直接评估和改进了GPT-4.1和DeepSeek-V3.1等大型语言模型。 *   **能力方向**: 论文的核心主题是“形式数学推理”，这是“数学推理”和“逻辑推理”的典型代表，是通用推理能力的核心组成部分。 *   **新兴范式**: 论文提出的“Lean-FIRe”方法，通过与形式化工具Lean交互来解决问题，可以被视为一种增强LLM问题解决能力的“工具使用”范式。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容是纯文本和形式语言的推理，不涉及多模态、视觉。虽然研究主题是“数学”，但数学在这里被视为衡量和提升通用逻辑推理能力的基准领域，而非像医疗、化学那样的特定应用领域。论文也未讨论模型基础设施或应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况——论文的定位清晰。** 论文提出的工具使用方法（与Lean交互）是为了提升模型在数学推理这一通用任务上的表现，而不是应用于某个特定领域，因此应该保留。 **总结**: 该论文的本质是一项方法论研究，它将LLM的数学推理过程分解为“猜想”和“形式化”两个步骤，并针对“猜想”这一被忽视的环节提出了新的评估方法和改进策略。这项工作直接致力于提升LLM在逻辑和数学方面的核心、通用推理能力，与您的“大语言模型通用推理能力”研究课题高度契合。因此，这篇论文应该被**保留**。"
    },
    {
        "index": "#62",
        "title": "PHANTOM RECALL: When Familiar Puzzles Fool Smart Models",
        "link": "/arxiv/2510.11812",
        "arxiv_id": "2510.11812",
        "authors": "Souradeep Mukhopadhyay, Rishabh Baral, Nimeesh Mahajan, Samhitha Harish, Aswin RRV, Mihir Parmar, Mutsumi Nakamura, Chitta Baral",
        "summary": "Large language models (LLMs) such as GPT, Gemini, and Claude often appear adept at solving classic logic puzzles--but how much genuine reasoning underlies their answers? Recent evidence suggests that these models frequently rely on memorized templates rather than reasoning from first principles. When puzzles are slightly modified, their performance collapses, revealing a striking fragility. In particular, we asked: Have LLMs addressed these issues? To what extent? How about perturbations to other puzzles? Is there a general way of reformulating the prompt so that the models do better? To examine these things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25 well-known logic puzzles and 149 carefully designed perturbations that preserve reasoning structure but alter superficial details and solutions. We evaluate eleven leading LLMs and identify a recurring failure mode--phantom recall--where models confidently reproduce memorized solutions or spurious rationales that no longer fit the altered scenario. To probe and mitigate this issue, we contribute three tools: (i) an automated logical-equivalence judge to detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error categories, and (iii) a prompting-based mitigation framework guided by these categories. Despite near-perfect accuracy on unmodified puzzles, models significantly underperform humans on perturbed ones, exhibiting both phantom recall and over-elaboration. Our findings reveal a crucial limitation: LLMs often fail to re-reason when contextual cues shift--highlighting the gap between linguistic fluency and logical understanding.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.891954",
        "filter_reason": "这篇论文完全符合筛选标准，应予以保留。我的判断过程如下： **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 这篇论文的核心并非将LLM应用于某个特定领域，而是深入探究了LLM在通用逻辑推理任务中的一个根本性缺陷：它们常常通过记忆模板而非真正的“从第一性原理出发的推理”来解决问题。论文的核心贡献不仅仅是揭示问题，更重要的是提出了一个“基于提示的缓解框架”来改善这一状况。这直接触及了如何“增强其逻辑、多步推理等通用能力”的核心目标，属于改进LLM基础能力的研究范畴。 **第二步：正面指标——论文高度相关。** - **核心概念**: 论文明确以“Large language models (LLMs)”为研究对象。 - **能力方向**: 论文的核心主题就是“reasoning”，特别是“logical reasoning”和“problem-solving”。它通过设计逻辑谜题的变体来测试模型的“re-reason”（重新推理）能力。 - **训练方法**: 虽然没有涉及RLHF等训练范式，但其提出的“prompting-based mitigation framework”是一种旨在提升模型推理表现的方法论，与目标一致。 **第三步：排除标准——论文不涉及任何排除领域。** - 论文完全不涉及多模态、视觉、机器人控制或任何特定应用领域（如医疗、化学）。 - 论文虽然讨论了模型的一种不可靠性（phantom recall），但其焦点在于通过提出新方法来提升模型的内在推理质量，而非应用层面的水印、安全等。 **第四步：处理特殊和模糊情况——论文是正面案例。** 这篇论文是“幻觉/可解释性”模糊情况下的一个典型正面案例。它所识别的“phantom recall”可以视为一种由记忆引起的推理幻觉。论文没有停留在现象描述，而是进一步贡献了“一个自动化的逻辑等价性判断器”和“一个基于提示的缓解框架”来主动探测和缓解这一问题。这完全符合“提出一种新方法来减少幻觉，从而提升模型的通用可靠性和推理质量，应该保留”的原则。 **第五步：最终决策** 综合以上分析，这篇论文通过构建新的评测基准（PHANTOM RECALL）来精准度量LLM通用推理能力的短板，并提出了针对性的改进方法。其研究目标、方法和贡献都紧密围绕“提升大语言模型本身的通用推理能力”这一核心，是高质量、高相关性的前沿研究，因此应被保留。"
    },
    {
        "index": "#61",
        "title": "R-WoM: Retrieval-augmented World Model For Computer-use Agents",
        "link": "/arxiv/2510.11892",
        "arxiv_id": "2510.11892",
        "authors": "Kai Mei, Jiang Guo, Shuaichen Chang, Mingwen Dong, Dongkyu Lee, Xing Niu, Jiarong Jiang",
        "summary": "Large Language Models (LLMs) can serve as world models to enhance agent decision-making in digital environments by simulating future states and predicting action outcomes, potentially eliminating costly trial-and-error exploration. However, this capability is fundamentally limited by LLMs' tendency toward hallucination and their reliance on static training knowledge, which can lead to compounding errors that inhibit long-horizon simulations. To systematically investigate whether LLMs are appropriate for world modeling, we probe two core capabilities of world models--future state prediction and reward estimation--through three tasks: next-state identification, full-procedure planning alignment, and milestone transition recognition. Our analysis shows that while LLMs effectively capture immediate next states and identify meaningful state transitions, their performance rapidly degrades in full-procedure planning. This highlights LLMs' limitations in reliably modeling environment dynamics over long horizons. To address these limitations, we propose the Retrieval-augmented World Model (R-WoM), which grounds LLM simulations by incorporating factual, up-to-date knowledge retrieved from external tutorials. Experiments show that R-WoM achieves substantial improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to baselines, with particular advantages in longer-horizon simulations.",
        "subjects": "Computation and Language",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.891399",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **核心判断（第一步）**: 论文的核心是解决大语言模型在作为“世界模型”进行长期规划和模拟时的根本性缺陷。论文明确指出，LLM的“幻觉”和“静态知识”限制了其在长视野任务中的可靠性。因此，它并非将LLM作为工具应用于特定领域，而是直面并试图改进LLM在**规划**和**多步推理**这一通用能力上的短板。其提出的R-WoM方法是一种新的方法论，旨在增强LLM内在的推理模拟质量，这直接触及了你研究目标的核心。 2.  **正面指标（第二步）**: 论文高度匹配了多个关键正面指标。 *   **核心概念**: 论文主题是关于 \"Large Language Models (LLMs)\"。 *   **能力方向**: 核心讨论了 \"world models\"（世界模型），这直接关联到 \"reasoning\", \"planning\", 和 \"problem-solving\"。论文通过任务如 \"full-procedure planning\" 来评测LLM的规划能力。 *   **新兴范式**: 论文的研究对象是 \"Computer-use Agents\"，属于 \"llm-based agents\" 的范畴。同时，其核心方法 \"Retrieval-augmented\" 是一种典型的 \"tool use\" 形式，目的是增强模型能力。 3.  **排除标准（第三步）**: 论文不涉及任何主要的排除领域。 *   它不是关于视觉或多模态模型的。 *   它的应用环境是通用的“数字环境”（如操作系统、网页），而非医疗、化学等特定领域。 *   它虽然提到了“幻觉”，但目的是从方法论上解决它以提升推理质量，而非讨论应用层面的水印或安全策略。 4.  **处理特殊和模糊情况（第四步）**: *   **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的绝佳范例。R-WoM框架通过检索外部知识这一“工具”，来增强LLM智能体在通用计算机使用任务中的长期规划和决策能力，因此应该保留。 *   **幻觉**: 论文将“幻觉”视为阻碍LLM进行可靠长期推理的根本原因，并提出了一种新的方法（检索增强）来缓解这一问题，从而提升模型的推理质量。这完全符合“提出一种新方法来减少幻觉...从而提升模型的通用可靠性和推理质量，应该保留”的标准。 **最终决策（第五步）**: 综合来看，这篇论文的核心贡献在于提出了一种名为R-WoM的通用方法，通过检索增强来克服LLM作为世界模型时的幻觉和知识局限性，从而显著提升了其在需要长视野规划和多步推理的通用任务中的表现。这直接且深刻地贡献于“提高大语言模型本身的通用推理能力”这一核心目标，因此应被保留。"
    },
    {
        "index": "#57",
        "title": "TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition",
        "link": "/arxiv/2510.11944",
        "arxiv_id": "2510.11944",
        "authors": "Yupei Li, Philipp Borchert, Gerasimos Lampouras",
        "summary": "Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4) mathematical reasoning but still struggle with autoformalisation, the task of transforming informal into formal mathematical statements. Autoformalisation helps pair the informal reasoning of LLMs with formal proof assistants which enable machine-verifiable generation and mitigate hallucinations. Yet, the performance of current Math LLMs is constrained by the scarcity of large-scale corpora, particularly those containing pairs of informal and formal statements. Although current models are trained to generate code from natural language instructions, structural and syntactic differences between these and formal mathematics limit effective transfer learning. We propose TopoAlign, a framework that unlocks widely available code repositories as training resources for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and dependency functions, and reassembles these components into analogues that structurally mirror formal statements. This produces structurally aligned code data that can be used for training Math LLMs without requiring additional human annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign provides substantial gains for DeepSeek-Math, improving performance by 17.77% on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10 and typecheck@10, respectively, demonstrating that training on aligned code data is beneficial even for specialized models.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.884184",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种名为 **TopoAlign** 的新框架。这个框架并非将LLM应用于某个特定领域，而是致力于解决提升LLM数学推理能力的一个关键瓶颈：高质量训练数据的稀缺性。 - **核心贡献**: TopoAlign通过一种新颖的拓扑分解方法，将广泛可用的代码数据转换为结构上与形式化数学陈述对齐的数据，从而为数学LLM的训练提供了新的、大规模的资源。 - **判断**: 这完全符合“提出新的训练范式，增强其逻辑、数学、多步推理等通用能力”的保留标准。论文的本质是改进LLM的基础能力，而非应用。 2.  **第二步：正面指标** - **核心概念**: 论文明确聚焦于 **Large Language Models (LLMs)**。 - **能力方向**: 论文的核心是 **mathematical reasoning**（数学推理），并具体探讨了 **autoformalisation**（自动形式化）这一高级推理任务。这直接命中了“reasoning (尤其是 math reasoning)”这一关键指标。 - **训练方法**: 论文提出了一种新的数据对齐和训练方法，虽然不是RL或进化，但属于“新的训练范式”的范畴。 3.  **第三步：排除标准** - **多模态**: 论文仅涉及文本（代码和数学语言），不涉及任何视觉或多模态内容。 - **特定应用领域**: 数学是通用推理的基础和核心，而非像医疗、化学那样的特定应用领域。该论文旨在提升模型在数学这一通用领域的推理能力，而非解决某个具体的化学或生物问题。 - **模型可靠性**: 论文虽然提到了“mitigate hallucinations”（减轻幻觉），但这是通过将非形式化推理与形式化证明助手相结合来实现的，属于提升模型内在推理质量和可靠性的方法，而非应用层面的水印或安全策略。 4.  **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: 论文通过自动形式化，将LLM的输出与可验证的形式化证明系统对接，这是一种从根本上提升推理准确性和减少幻觉的内在机制。这完全符合“提出一种新方法来减少幻觉，从而提升模型的通用可靠性和推理质量，应该保留”的原则。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种创新的训练数据生成框架（TopoAlign），旨在通过解决数据瓶颈来直接提升大语言模型的**数学推理能力**。数学推理是通用推理能力的基石之一。该研究不涉及特定应用领域，不涉及多模态，其方法论的贡献是普适性的，旨在增强LLM的基础能力。因此，这篇论文与你的研究目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度契合，应该被保留。"
    },
    {
        "index": "#67",
        "title": "The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation",
        "link": "/arxiv/2510.12668",
        "arxiv_id": "2510.12668",
        "authors": "Minghao Tang, Shiyu Ni, Jingtong Wu, Zengxin Han, Keping Bi",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving external documents. As an emerging form of RAG, parametric retrieval-augmented generation (PRAG) encodes documents as model parameters (i.e., LoRA modules) and injects these representations into the model during inference, enabling interaction between the LLM and documents at parametric level. Compared with directly placing documents in the input context, PRAG is more efficient and has the potential to offer deeper model-document interaction. Despite its growing attention, the mechanism underlying parametric injection remains poorly understood. In this work, we present a systematic study of PRAG to clarify the role of parametric injection, showing that parameterized documents capture only partial semantic information of documents, and relying on them alone yields inferior performance compared to interaction at text level. However, these parametric representations encode high-level document information that can enhance the model's understanding of documents within the input context. When combined parameterized documents with textual documents, the model can leverage relevant information more effectively and become more robust to noisy inputs, achieving better performance than either source alone. We recommend jointly using parameterized and textual documents and advocate for increasing the information content of parametric representations to advance PRAG.",
        "subjects": "Information Retrieval, Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.894399",
        "filter_reason": "这篇论文符合我的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是研究一种名为“参数化检索增强生成（PRAG）”的新方法。这种方法旨在改进LLM如何与外部知识（文档）进行交互。传统的RAG将文本放入上下文，而PRAG将文档编码为模型参数（如LoRA），在更深层次上与模型融合。这本质上是在**改进LLM处理和利用外部信息的基础机制**。一个强大的通用推理能力，不仅依赖于模型内部的逻辑链条，也依赖于其准确、高效地获取和整合外部知识的能力。因此，对RAG这种基础能力的改进，直接关系到LLM的通用问题解决能力，而不是将其应用于某个特定领域。论文的核心贡献是系统性研究PRAG的机制和效果，属于提升LLM基础能力的范畴，应予以保留。 2.  **第二步：正面指标** - **核心概念**: 论文明确以“Large language models (LLMs)”为核心研究对象。 - **能力方向**: 虽然摘要没有直接使用\"reasoning\"一词，但检索增强生成（RAG）是提升LLM事实准确性和知识广度的关键技术，这是进行高质量推理和问题解决的前提。论文中提到的“leverage relevant information more effectively”和“become more robust to noisy inputs”都是提升问题解决能力的直接体现。 - **新兴范式**: PRAG可以被视为一种新颖的**“工具使用”**范式。这里的“工具”是外部知识库，而PRAG提供了一种比传统文本拼接更深层次的“工具-模型”交互方式。这符合筛选标准中关于“工具使用”的正面指标。 3.  **第三步：排除标准** - 论文不涉及多模态、视觉等内容。 - 论文是通用方法研究，没有聚焦于医疗、化学等任何特定应用领域。 - 论文不涉及水印、安全等模型可靠性（应用层面）的研究。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 该论文提出的PRAG方法，正是一种**通用的工具使用方法**，旨在增强LLM与外部知识这一“工具”的交互能力，从而提升其通用问题解决能力。它不是将此方法应用于特定领域的智能体，而是对方法本身进行系统性研究，因此完全符合保留条件。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是通过对一种新兴的、更深层次的检索增强方法（PRAG）进行系统性研究，来探索如何提升大语言模型整合外部信息的能力。这种能力是LLM通用推理和问题解决能力的重要组成部分。因此，该论文精准地契合了“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。"
    },
    {
        "index": "#66",
        "title": "Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?",
        "link": "/arxiv/2510.12680",
        "arxiv_id": "2510.12680",
        "authors": "Shouren Wang, Wang Yang, Xianxuan Long, Qifan Wang, Vipin Chaudhary, Xiaotian Han",
        "summary": "Hybrid thinking enables LLMs to switch between reasoning and direct answering, offering a balance between efficiency and reasoning capability. Yet our experiments reveal that current hybrid thinking LLMs only achieve partial mode separation: reasoning behaviors often leak into the no-think mode. To understand and mitigate this, we analyze the factors influencing controllability and identify four that matter most: (1) larger data scale, (2) using think and no-think answers from different questions rather than the same question, (3) a moderate increase in no-think data number, and (4) a two-phase strategy that first trains reasoning ability and then applies hybrid think training. Building on these findings, we propose a practical recipe that, compared to standard training, can maintain accuracy in both modes while significantly reducing no-think output length (from $1085$ to $585$ on MATH500) and occurrences of reasoning-supportive tokens such as ``\\texttt{wait}'' (from $5917$ to $522$ on MATH500). Our findings highlight the limitations of current hybrid thinking and offer directions for strengthening its controllability.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.893942",
        "filter_reason": "这篇论文完全符合您的筛选要求，核心判断如下： **第一步：核心判断——论文本质分析** 这篇论文的本质是研究并改进大语言模型（LLM）的一种核心推理机制——“混合思维”。该机制旨在让模型能够根据问题复杂度，在“深度推理”和“快速作答”两种模式间自主切换。论文的核心贡献在于： 1.  **发现问题**：揭示了当前“混合思维”方法存在“模式泄露”问题，即推理行为会不必要地出现在本应快速作答的场景中。 2.  **归因分析**：系统性地分析了影响这种模式可控性的关键因素，如数据规模、训练策略等。 3.  **提出方案**：基于分析，提出了一套实用的训练方法，显著提升了模型在两种模式间切换的精确性和效率。 这完全属于对LLM**基础能力**的改进，直接针对其**推理过程**的优化、控制和效率提升，而非将LLM作为工具应用于某个特定领域。因此，根据第一步标准，应予以保留。 **第二步：正面指标匹配** 论文高度匹配所有正面指标： - **核心概念**: 论文研究对象明确为 \"LLMs\"。 - **能力方向**: 核心议题是 \"reasoning\"，并使用 \"MATH500\" 数据集进行验证，直指数学推理能力。 - **训练方法**: 论文提出了新的训练策略 (\"two-phase strategy\", \"practical recipe\")，属于训练范式的创新。 - **新兴范式**: \"Hybrid thinking\" 本身就是一种新兴的、旨在优化模型推理行为和效率的范式，与思维链等思想一脉相承。 **第三步：排除标准判断** 论文完全不涉及任何排除标准所列的领域： - 没有涉及多模态与视觉。 - 虽然使用了数学数据集，但其目的是将数学推理作为**通用的推理能力基准**，而非解决某个特定的数学应用问题。 - 论文讨论的是模型推理过程的“可控性”和“精确性”，这与模型内在的推理质量相关，区别于应用层面的水印、安全等问题。 **第四步：特殊和模糊情况处理** 本论文不涉及智能体/工具使用或安全等特殊模糊情况，其研究焦点非常清晰，即优化LLM内部的推理模式切换机制。 **第五步：最终决策** 综上所述，这篇论文精准地聚焦于“提升大语言模型通用推理能力”这一核心目标。它通过一种创新的“混合思维”视角，深入探究并优化了LLM推理过程的可控性与效率，是典型的对LLM基础能力进行强化的研究。因此，这篇论文与您的研究课题高度相关，应被筛选保留。"
    },
    {
        "index": "#76",
        "title": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization",
        "link": "/arxiv/2510.12063",
        "arxiv_id": "2510.12063",
        "authors": "Sunzhu Li, Zhiyu Lin, Shuling Yang, Jiale Zhao, Wei Chen",
        "summary": "Large Reasoning Models (LRMs) are powerful, but they still suffer from inefficient and off-target reasoning. Currently, training-free methods are limited to either rigid heuristics or descriptive, non-actionable analyses. In this paper, we introduce ThinkPilot, a training-free framework that automatically optimizes LRMs reasoning. It uses an evolutionary process to generate think-prefixes, which are instructions that evolve driven by a taxonomy of reasoning behaviors to guide models toward superior performance. Extensive experiments demonstrate ThinkPilot's broad effectiveness: it significantly improves the accuracy-length trade-off for efficient reasoning, drastically improves safety (for example, cutting the StrongREJECT score of DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction following. It also synergizes with existing training-based methods. Our analysis reveals that think-prefixes can reliably control LRMs' reasoning behaviors, and that different tasks have strong preferences for specific behavioral distributions. By automatically identifying and eliciting these behaviors, ThinkPilot provides a generalizable framework for aligning LRMs reasoning with task demands. Data and code are available at https://github.com/teqkilla/ThinkPilot",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.904130",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种名为 **ThinkPilot** 的**免训练框架**，其目标是**自动优化大语言模型的推理过程**。它通过一种进化算法来生成和优化“思维前缀”，以引导模型进行更高效、更准确的推理。 - **符合目标**: 这直接命中了你“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。它不是将LLM应用于某个特定领域，而是致力于改进LLM的内在推理机制和方法论，属于基础能力和新训练范式的研究。 2.  **第二步：正面指标** - **核心概念**: 论文明确研究 \"Large Reasoning Models (LRMs)\"，即大推理模型，是LLM的一个子集。 - **能力方向**: 论文的核心就是解决 \"inefficient and off-target reasoning\"（低效和偏离目标的推理），并提升 \"efficient reasoning\"（高效推理），完全聚焦于推理能力。 - **训练方法**: 提出了 \"evolutionary process\"（进化过程）作为一种新的优化方法，这符合你对新训练/优化范式的关注。 - **新兴范式**: \"think-prefixes\" 是一种新颖的引导模型推理的范式，与思维链类似但更进一步，旨在更精细地控制模型行为。 3.  **第三步：排除标准** - **多模态**: 论文未涉及视觉、语音等多模态内容。 - **特定应用领域**: 论文是通用性的，旨在提升模型在各类任务上的推理表现，而非局限于医疗、化学等特定领域。 - **模型可靠性（应用层面）**: 论文提到了提升安全性，但这并非其核心。它不是通过添加外部过滤器或水印等应用层手段，而是通过**优化推理过程本身**来减少有害输出。这属于提升模型内在可靠性的范畴，而非应用层面的安全加固。 4.  **第四步：处理特殊和模糊情况** - **安全**: 论文中提到的安全性提升（将StrongREJECT分数从27.0%降至0.7%）是其方法带来的一个**结果**，而非研究的出发点。其核心机制是优化“思维前缀”来控制模型的推理行为，一个更受控、更精确的推理过程自然会减少产生不安全内容的可能性。这完全符合“提出一种新方法来……提升模型的通用可靠性和推理质量，应该保留”的原则。 5.  **第五步：最终决策** - **综合分析**: 该论文提出了一种创新的、通用的方法论，通过进化算法自动优化引导模型推理的指令，从而直接提升了LLM的通用推理效率、准确性和可控性。其核心贡献是方法论层面的，旨在增强LLM的基础推理能力，与你的研究目标高度契合。 因此，这篇论文应该被**保留**。"
    },
    {
        "index": "#60",
        "title": "LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance",
        "link": "/arxiv/2510.11905",
        "arxiv_id": "2510.11905",
        "authors": "Patrick Haller, Mark Ibrahim, Polina Kirichenko, Levent Sagun, Samuel J. Bell",
        "summary": "For Large Language Models (LLMs) to be reliable, they must learn robust knowledge that can be generally applied in diverse settings -- often unlike those seen during training. Yet, extensive research has shown that LLM performance can be brittle, with models exhibiting excessive sensitivity to trivial input variations. In this work, we explore whether this brittleness is a direct result of unstable internal knowledge representations. To explore this question, we build on previous work showing that LLM representations encode statement truthfulness -- i.e., true, factual statements can be easily separated from false, inaccurate ones. Specifically, we test the robustness of learned knowledge by evaluating representation separability on samples that have undergone superficial transformations to drive them out-of-distribution (OOD), such as typos or reformulations. By applying semantically-preserving perturbations, we study how separability degrades as statements become more OOD, across four LLM families, five evaluation datasets, and three knowledge probing methods. Our results reveal that internal representations of statement truthfulness collapse as the samples' presentations become less similar to those seen during pre-training. While LLMs can often distinguish between true and false statements when they closely resemble the pre-training data, this ability is highly dependent on the statement's exact surface form. These findings offer a possible explanation for brittle benchmark performance: LLMs may learn shallow, non-robust knowledge representations that allow for only limited generalizability. Our work presents a fundamental challenge for the utility of truthfulness probes, and more broadly, calls for further research on improving the robustness of learned knowledge representations.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.890918",
        "filter_reason": "这篇论文完全符合你的研究范围，是一篇高质量的基础性研究。我的判断过程如下： 1.  **核心判断（第一步）：** 这篇论文的本质不是将LLM作为工具应用于特定领域，而是对LLM本身进行深入的**诊断性分析**。其核心贡献是揭示了LLM内部知识表示的一个根本性缺陷：**脆弱性**。论文指出，LLM对事实真伪的判断高度依赖于输入文本的表层形式，一旦遇到拼写错误或改写等分布外（OOD）的微小扰动，其内部表示就会崩溃，导致判断能力急剧下降。这种对知识表示稳定性的探究，直接触及了LLM能否进行可靠、通用推理的**核心基础**。如果模型的知识是脆弱的，那么任何基于这些知识的逻辑、数学或多步推理都将是不可靠的。因此，这篇论文是在为“提高LLM通用推理能力”这一目标**揭示关键瓶颈**，属于基础能力研究的范畴。 2.  **正面指标（第二步）：** 论文明确以“Large Language Models (LLMs)”为核心研究对象。虽然摘要中没有直接使用\"reasoning\"一词，但其探讨的“robust knowledge”、“generalizability”以及“brittle benchmark performance”都与通用推理能力息息相关。可靠的推理必须建立在稳定、鲁棒的知识基础之上。这篇论文正是对这一基础进行剖析，因此与“reasoning”和“problem-solving”的能力方向高度相关。 3.  **排除标准（第三步）：** 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施。它关注的是模型内在的知识表示机制，因此没有被任何排除标准命中。 4.  **处理特殊和模糊情况（第四步）：** 这篇论文可以被视为对“模型可靠性”的深入研究，但它属于**模型内在的、根本性的可靠性问题**，而非应用层面的水印或安全策略。它直接解释了“幻觉”等现象可能产生的深层原因——即模型学到的知识是浅层且不鲁棒的。根据筛选标准，如果论文能“提升模型的通用可靠性和推理质量”，就应该保留。这篇论文虽然没提出新方法，但它通过严谨的实验指出了问题的根源，为未来如何提升可靠性（例如，通过新的训练范式来学习更鲁棒的知识表示）指明了方向，其贡献是根本性的。 **最终决策（第五步）：** 综合来看，这篇论文虽然没有提出一种新的训练方法或框架来“直接”提升推理能力，但它通过深刻的诊断分析，揭示了当前LLM在通用推理方面的一个**根本性缺陷**：知识表示的脆弱性。这项工作为我们理解“为什么LLM的推理能力会表现出不稳定性”提供了关键答案，并为未来研究如何构建更鲁棒、更可靠的通用推理模型奠定了重要的理论基础。因此，它对于你“致力于提高大语言模型本身的通用推理能力”的研究课题具有极高的参考价值，应当保留。"
    },
    {
        "index": "#87",
        "title": "TripScore: Benchmarking and rewarding real-world travel planning with fine-grained evaluation",
        "link": "/arxiv/2510.09011",
        "arxiv_id": "2510.09011",
        "authors": "Yincen Qu, Huan Xiao, Feng Li, Hui Zhou, Xiangying Dai",
        "summary": "Travel planning is a valuable yet complex task that poses significant challenges even for advanced large language models (LLMs). While recent benchmarks have advanced in evaluating LLMs' planning capabilities, they often fall short in evaluating feasibility, reliability, and engagement of travel plans. We introduce a comprehensive benchmark for travel planning that unifies fine-grained criteria into a single reward, enabling direct comparison of plan quality and seamless integration with reinforcement learning (RL). Our evaluator achieves moderate agreement with travel-expert annotations (60.75\\%) and outperforms multiple LLM-as-judge baselines. We further release a large-scale dataset of 4,870 queries including 219 real-world, free-form requests for generalization to authentic user intent. Using this benchmark, we conduct extensive experiments across diverse methods and LLMs, including test-time computation, neuro-symbolic approaches, supervised fine-tuning, and RL via GRPO. Across base models, RL generally improves itinerary feasibility over prompt-only and supervised baselines, yielding higher unified reward scores.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-15T11:00:03.972850",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： **第一步：核心判断** 论文的本质是提出一种新的**评估基准和奖励模型**，用于衡量和提升大语言模型的**规划能力**。虽然论文的应用场景是“旅行规划”，但其核心贡献并非解决旅行领域的具体问题，而是： 1.  **构建了一个精细化的评估框架**：该框架将可行性、可靠性等通用规划能力的衡量标准统一为一个奖励信号。 2.  **验证了一种训练范式**：论文利用这个奖励信号，通过强化学习（RL）来优化LLM的规划能力，并证明了其有效性。 因此，这篇论文的核心是**改进LLM的基础能力（规划）**和**提出新的训练范式（基于精细奖励的RL）**，完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。它将“旅行规划”作为一个复杂、多步骤、贴近现实的推理任务来作为研究载体，而非终点。 **第二步：正面指标** 论文包含了多个强相关的正面指标： - **核心概念**: 明确以 \"large language models (LLMs)\" 为研究对象。 - **能力方向**: 核心聚焦于 \"planning capabilities\"，这是通用推理能力的关键组成部分。 - **训练方法**: 深入探讨了 \"reinforcement learning (RL)\"，并与其他方法如\"supervised fine-tuning\"进行对比，旨在通过训练提升模型能力。 **第三步：排除标准** 论文没有触及任何排除标准： - 它不涉及多模态、视觉等内容。 - 虽然使用了“旅行”这一领域，但如第一步所述，其研究焦点是通用的“规划”能力，而非旅行领域的特定应用。 - 它不涉及模型基础设施、部署优化或硬件加速。 **第四步：处理特殊和模糊情况** 本案例恰好是“特殊和模糊情况”的典型代表。乍一看，“旅行规划”似乎是一个特定应用领域，容易被排除。但深入分析摘要后，可以明确其研究范式是： - **提出一种通用的智能体/工具使用方法**：这里的“方法”指的就是“精细化的评估基准和奖励模型”。这个方法是通用的，可以被迁移到其他需要多步规划和可行性评估的任务中。它旨在增强LLM的通用问题解决能力，而非仅限于旅行。 - **研究如何提升模型内在能力**：论文通过RL来提升行程的“可行性”，这直接关系到模型的推理质量和内在可靠性，而非应用层面的水印或安全。 **第五步：最终决策** 综合以上分析，这篇论文的本质是通过创建一个高质量的基准和奖励信号，来研究和提升LLM在复杂、多步骤规划任务上的通用推理能力。它使用“旅行规划”作为一个理想的试验场，但其方法论和发现对于提升LLM的通用规划与问题解决能力具有直接的贡献。因此，这篇论文**完全符合**你的研究范围。"
    },
    {
        "index": "#21",
        "title": "Learning-To-Measure: In-context Active Feature Acquisition",
        "link": "/arxiv/2510.12624",
        "arxiv_id": "2510.12624",
        "authors": "Yuta Kobayashi, Zilin Jing, Jiayu Yao, Hongseok Namkoong, Shalmali Joshi",
        "summary": "Active feature acquisition (AFA) is a sequential decision-making problem where the goal is to improve model performance for test instances by adaptively selecting which features to acquire. In practice, AFA methods often learn from retrospective data with systematic missingness in the features and limited task-specific labels. Most prior work addresses acquisition for a single predetermined task, limiting scalability. To address this limitation, we formalize the meta-AFA problem, where the goal is to learn acquisition policies across various tasks. We introduce Learning-to-Measure (L2M), which consists of i) reliable uncertainty quantification over unseen tasks, and ii) an uncertainty-guided greedy feature acquisition agent that maximizes conditional mutual information. We demonstrate a sequence-modeling or autoregressive pre-training approach that underpins reliable uncertainty quantification for tasks with arbitrary missingness. L2M operates directly on datasets with retrospective missingness and performs the meta-AFA task in-context, eliminating per-task retraining. Across synthetic and real-world tabular benchmarks, L2M matches or surpasses task-specific baselines, particularly under scarce labels and high missingness.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.LG",
        "crawl_time": "2025-10-15T11:00:04.537693",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Learning-to-Measure (L2M)”的新方法，用于解决“元主动特征获取”问题。这并非将LLM应用于特定领域，而是提出了一种**通用的方法论**来增强模型在信息不完整情况下的决策能力。主动特征获取（AFA）本身是一个**序列决策问题**，其核心是“自适应地选择要获取哪些特征”，这本质上是一种规划、推理和问题解决的能力。论文的目标是学习一个可以跨多种任务使用的“元策略”，这正是提升模型**通用能力**的体现，而非针对单一任务的优化。 2.  **第二步：正面指标** 论文包含了多个强烈的正面指标： *   **核心概念**: 论文明确提到其基础是“序列建模或自回归预训练方法”，这直接指向了大语言模型（LLM）的核心技术范式。同时，“in-context”（上下文）执行是现代LLM的关键能力。 *   **能力方向**: 论文解决的问题“序列决策”以及其提出的“uncertainty-guided greedy feature acquisition agent”（由不确定性引导的贪婪特征获取智能体）直接对应了**规划**和**问题解决**能力。最大化“条件互信息”是一种基于信息论的推理决策原则。 *   **新兴范式**: 论文的核心亮点之一是“performs the meta-AFA task in-context”（在上下文中执行元AFA任务），这直接利用了LLM的上下文学习这一新兴范式。同时，它明确提出了一个“agent”框架。 3.  **第三步：排除标准** 该论文完全不符合任何排除标准。它不涉及多模态、视觉；其应用背景是“表格数据”，这是一种通用的数据格式，而非医疗、化学等特定应用领域；研究内容也与水印、安全等模型可靠性应用层面无关。 4.  **第四步：处理特殊和模糊情况** *   **智能体**: 论文提出的“feature acquisition agent”是一种**通用的**决策智能体，其目标是跨任务、跨数据集地学习获取信息的策略。它不是一个应用于特定领域的智能体（如“用于化学实验的智能体”），因此应该被保留。这个智能体框架本身就是增强模型通用规划能力的一种方法。 **最终决策**: 综合以上分析，这篇论文的本质是利用自回归预训练（LLM的基础技术）和上下文学习，提出了一种通用的智能体框架，以解决序列决策问题（主动特征获取）。其核心目标是提升模型在信息不完整环境下的规划、推理和决策能力，并且这种能力是可泛化到多种任务的。这完全契合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，该论文应被**保留**。"
    },
    {
        "index": "#53",
        "title": "Self-Verifying Reflection Helps Transformers with CoT Reasoning",
        "link": "/arxiv/2510.12157",
        "arxiv_id": "2510.12157",
        "authors": "Zhongwei Yu, Wannian Xia, Xue Yan, Bo Xu, Haifeng Zhang, Yali Du, Jun Wang",
        "summary": "Advanced large language models (LLMs) frequently reflect in reasoning chain-of-thoughts (CoTs), where they self-verify the correctness of current solutions and explore alternatives. However, given recent findings that LLMs detect limited errors in CoTs, how reflection contributes to empirical improvements remains unclear. To analyze this issue, in this paper, we present a minimalistic reasoning framework to support basic self-verifying reflection for small transformers without natural language, which ensures analytic clarity and reduces the cost of comprehensive experiments. Theoretically, we prove that self-verifying reflection guarantees improvements if verification errors are properly bounded. Experimentally, we show that tiny transformers, with only a few million parameters, benefit from self-verification in both training and reflective execution, reaching remarkable LLM-level performance in integer multiplication and Sudoku. Similar to LLM results, we find that reinforcement learning (RL) improves in-distribution performance and incentivizes frequent reflection for tiny transformers, yet RL mainly optimizes shallow statistical patterns without faithfully reducing verification errors. In conclusion, integrating generative transformers with discriminative verification inherently facilitates CoT reasoning, regardless of scaling and natural language.",
        "subjects": "Machine Learning",
        "date": "2025-10-14",
        "category": "cs.LG",
        "crawl_time": "2025-10-15T11:00:04.568341",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心是研究并改进大语言模型（LLM）的『思维链』推理机制。它没有将LLM作为工具应用于特定领域，而是深入探究了“反思”这一推理过程中的关键环节，提出了一个“自我验证反思”框架来增强模型的推理能力。 - **判断依据**: 论文旨在回答一个根本性问题——“反思”为何能提升LLM的推理表现。它通过理论证明和实验验证，阐明了将生成式模型与判别式验证相结合如何内在地促进CoT推理。这直接属于“改进LLM的基础能力”和“增强其逻辑、数学、多步推理等通用能力”的范畴，是典型的方法论研究。 2.  **第二步：正面指标** - **核心概念**: 论文摘要明确提到了 \"Advanced large language models (LLMs)\" 和 \"transformers\"。 - **能力方向**: 标题和摘要反复强调 \"reasoning\" 和 \"chain-of-thoughts (CoTs)\"。实验任务 \"integer multiplication\" 和 \"Sudoku\" 分别是数学推理和逻辑推理的经典基准。 - **训练方法**: 摘要中明确提到了 \"reinforcement learning (RL)\" 作为一种训练和优化方法，并分析了其作用机制。 - **结论**: 该论文命中了几乎所有关键的正面指标，进一步确认了其高度相关性。 3.  **第三步：排除标准** - **多模态与视觉**: 论文完全没有涉及视觉、视频或其他模态，专注于文本和符号推理。 - **特定应用领域**: 实验任务是通用的数学和逻辑谜题，而非医疗、化学或任何特定垂直领域。 - **模型可靠性（应用层面）**: 论文讨论的是模型内部的错误检测和验证，属于提升模型内在能力，而非水印、安全等应用层面的可靠性问题。 - **结论**: 该论文未触及任何明确的排除领域。 4.  **第四步：处理特殊和模糊情况** - **幻觉/可解释性**: 论文的核心“自我验证反思”机制，本质上是一种减少推理过程中错误（即推理层面的幻觉）的方法。它通过提出一个新框架来让模型自我验证解决方案的正确性，从而提升推理质量。这完全符合“提出一种新方法来减少幻觉，从而提升模型的通用可靠性和推理质量，应该保留”的标准。 5.  **第五步：最终决策** - 综合以上所有分析，这篇论文是一篇非常典型的、旨在提升LLM核心推理能力的前沿研究。它从理论层面剖析了“反思”的作用，并通过实验验证了“自我验证反思”这一通用方法的有效性。其贡献直接指向了如何让LLM本身“更会思考”，完美契合您“提高大语言模型本身的『通用推理能力』”这一核心目标。因此，最终决策为 **True**。"
    },
    {
        "index": "#65",
        "title": "Mamaba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning",
        "link": "/arxiv/2510.12026",
        "arxiv_id": "2510.12026",
        "authors": "Junsoo Oh, Wei Huang, Taiji Suzuki",
        "summary": "Mamba, a recently proposed linear-time sequence model, has attracted significant attention for its computational efficiency and strong empirical performance. However, a rigorous theoretical understanding of its underlying mechanisms remains limited. In this work, we provide a theoretical analysis of Mamba's in-context learning (ICL) capability by focusing on tasks defined by low-dimensional nonlinear target functions. Specifically, we study in-context learning of a single-index model $y \\approx g_*(\\langle \\boldsymbol{\\beta}, \\boldsymbol{x} \\rangle)$, which depends on only a single relevant direction $\\boldsymbol{\\beta}$, referred to as feature. We prove that Mamba, pretrained by gradient-based methods, can achieve efficient ICL via test-time feature learning, extracting the relevant direction directly from context examples. Consequently, we establish a test-time sample complexity that improves upon linear Transformers -- analyzed to behave like kernel methods -- and is comparable to nonlinear Transformers, which have been shown to surpass the Correlational Statistical Query (CSQ) lower bound and achieve near information-theoretically optimal rate in previous works. Our analysis reveals the crucial role of the nonlinear gating mechanism in Mamba for feature extraction, highlighting it as the fundamental driver behind Mamba's ability to achieve both computational efficiency and high performance.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.LG",
        "crawl_time": "2025-10-15T11:00:04.584484",
        "filter_reason": "这篇论文完全符合你的研究范围，是一篇非常值得保留的高质量论文。以下是根据你的筛选标准进行的详细判断： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心是**对Mamba模型的基础能力——上下文学习——进行理论分析**。它没有将Mamba应用于任何特定领域，而是深入探究了Mamba模型在“测试时”如何学习和泛化的内在机制。具体来说，它分析了模型如何从上下文中的少量示例里提取关键特征（\"test-time feature learning\"）来解决新问题。这直接触及了大模型通用推理能力的核心，即模型如何在没有进行权重更新的情况下进行逻辑归纳和问题解决。因此，这篇论文的本质是改进和理解LLM的基础能力，应予以**保留**。 **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文虽然以Mamba为标题，但Mamba是当前LLM架构研究中的一个重要方向（作为Transformer的潜在替代品）。其研究的上下文学习（ICL）是LLM的核心范式之一。 - **能力方向**: 论文研究的核心是**上下文学习（ICL）**，这正是LLM实现**推理**和**问题解决**能力的关键方式。论文分析了模型如何“学习”并“提取相关方向”，这本身就是一种复杂的推理过程。 - **训练方法**: 论文提到了梯度下降预训练，但其重点在于**测试时的行为**（\"test-time feature learning\"），这与思维链等推理范式在“推理时”进行多步计算的思路有共通之处。 - **新兴范式**: ICL本身就是大模型领域至关重要的新兴范式。这篇论文为理解这一范式提供了坚实的理论基础。 **第三步：排除标准——论文是否主要聚焦于以下领域？** - **多模态与视觉**: 否，论文完全聚焦于序列模型。 - **特定应用领域**: 否，论文使用的是理论上的“低维非线性目标函数”，而非任何真实世界的特定领域。 - **模型可靠性（应用层面）**: 否，论文不涉及水印、安全等内容。 **第四步：处理特殊和模糊情况** 这篇论文的情况非常清晰，不涉及特殊模糊情况。但值得一提的是，它对Mamba内部机制（非线性门控）的理论分析，实际上是一种**深度的可解释性研究**。这种研究揭示了模型为何具备强大的ICL能力，为未来如何设计出推理能力更强的模型指明了方向，完全符合“通过增强模型内在可解释性来提升推理质量”的保留原则。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于**从理论层面揭示了Mamba这一新型LLM架构实现通用推理能力（具体表现为ICL）的内在机理**。它不关注应用，而是深入模型的基础能力，为“如何提升LLM通用推理能力”这一核心目标提供了深刻的见解和理论支撑。因此，这篇论文与你的研究课题高度相关，应被**保留**。"
    },
    {
        "index": "#89",
        "title": "GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving",
        "link": "/arxiv/2510.11769",
        "arxiv_id": "2510.11769",
        "authors": "Ruida Wang, Jiarui Yao, Rui Pan, Shizhe Diao, Tong Zhang",
        "summary": "Solving math problems through verifiable languages such as Lean has significantly impacted both the mathematics and computer science communities. Current state-of-the-art models are often trained with expensive online Reinforcement Learning (RL) or expert iteration. However, these approaches rely on fixed problem sets, which causes inefficient training and limits the model to tackle complex problems. To overcome these limitations, we propose GAR: Generative Adversarial Reinforcement learning, a comprehensive RL training framework that jointly trains the problem composer and solver in an adversarial loop. GAR introduces an implicit curriculum learning mechanism, which aligns task difficulty with the prover's evolving capability. It thereby improves the training efficiency and enables stronger performance of proving advanced theorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and DeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of 4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on ProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR establishes a general RL paradigm for co-evolution of problem generation and solving under verifiable environments.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.LG",
        "crawl_time": "2025-10-15T11:00:04.605804",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 这篇论文的核心贡献并非仅仅是解决数学问题，而是提出了一种名为GAR（生成对抗强化学习）的全新训练范式。其本质是通过一个对抗性的循环，让“问题生成器”和“问题解决器”（即LLM）协同进化。这种方法论的核心目标是提升“解决器”模型的能力，通过动态生成与其能力相匹配的难题，创建一个隐式的课程学习机制。这直接触及了“改进LLM的基础能力”和“提出新的训练范式”的核心，旨在增强其解决复杂、多步问题的通用推理能力，而非将其作为工具应用于特定领域。 **第二步：正面指标——论文高度相关。** 该论文命中了多个关键的正面指标： - **核心概念**: 论文的研究对象是Goedel-Prover和DeepSeek-Prover，这些都是大语言模型（LLMs）。 - **能力方向**: 论文聚焦于“形式化定理证明”，这是逻辑推理和数学推理的巅峰体现，是衡量通用推理能力的黄金标准之一。 - **训练方法**: 论文的核心是“生成对抗强化学习”，属于强化学习（RL）范畴，并明确提出了“协同进化”的概念。 - **新兴范式**: GAR框架本身就是一个“多智能体系统”，由生成器和解决器两个智能体构成，共同提升问题解决能力。 **第三步：排除标准——论文未触及排除领域。** 论文的研究内容完全避开了所有排除标准： - 它不涉及多模态、视觉或特定应用领域（如医疗、化学等）。 - 它不关注模型部署、硬件加速或应用层面的可靠性问题（如水印、安全）。 **第四步：处理特殊和模糊情况——论文提出了通用的智能体框架。** 虽然论文的应用场景是“形式化定理证明”（一个特定领域），但这并不构成排除的理由。关键在于，论文明确指出其贡献“Beyond formal proving, GAR establishes a general RL paradigm for co-evolution of problem generation and solving under verifiable environments.”（超越形式化证明，GAR为在可验证环境中进行问题生成和解决的协同进化建立了一个通用的强化学习范式）。这表明，形式化证明只是验证其通用方法论有效性的一个理想“可验证环境”。其提出的GAR框架是一种通用的智能体协作与进化范式，旨在提升LLM的内在问题解决能力，完全符合“保留”标准。 **第五步：最终决策。** 综合以上分析，这篇论文提出了一种创新的、基于对抗强化学习和协同进化的训练框架，其根本目标是提升大语言模型在复杂推理任务上的通用能力。它不仅解决了当前RL训练在固定数据集上的局限性，更重要的是，它为提升LLM的自主学习和问题解决能力提供了一个具有普适性的新范式。因此，这篇论文与您关于“大语言模型通用推理能力”的研究课题高度契合，是前沿且重要的参考文献。"
    },
    {
        "index": "#125",
        "title": "Tensor Logic: The Language of AI",
        "link": "/arxiv/2510.12269",
        "arxiv_id": "2510.12269",
        "authors": "Pedro Domingos",
        "summary": "Progress in AI is hindered by the lack of a programming language with all the requisite features. Libraries like PyTorch and TensorFlow provide automatic differentiation and efficient GPU implementation, but are additions to Python, which was never intended for AI. Their lack of support for automated reasoning and knowledge acquisition has led to a long and costly series of hacky attempts to tack them on. On the other hand, AI languages like LISP an Prolog lack scalability and support for learning. This paper proposes tensor logic, a language that solves these problems by unifying neural and symbolic AI at a fundamental level. The sole construct in tensor logic is the tensor equation, based on the observation that logical rules and Einstein summation are essentially the same operation, and all else can be reduced to them. I show how to elegantly implement key forms of neural, symbolic and statistical AI in tensor logic, including transformers, formal reasoning, kernel machines and graphical models. Most importantly, tensor logic makes new directions possible, such as sound reasoning in embedding space. This combines the scalability and learnability of neural networks with the reliability and transparency of symbolic reasoning, and is potentially a basis for the wider adoption of AI.",
        "subjects": "Artificial Intelligence, Machine Learning, Neural and Evolutionary Computing, Programming Languages, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.LG",
        "crawl_time": "2025-10-15T11:00:04.661486",
        "filter_reason": "这篇论文符合我的研究范围，尽管它不是一篇典型的“在现有LLM上进行微调”的论文，但其核心贡献与我的目标高度一致。 判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为“tensor logic”的新编程语言，旨在从根本上统一神经和符号AI。其目标是解决现有框架（如PyTorch/TensorFlow）缺乏自动推理能力，以及传统AI语言（如LISP/Prolog）缺乏可扩展性的问题。这种“统一”和“构建新基础”的尝试，直接触及了提升AI（包括LLM）推理能力的根本路径。它不是将LLM作为工具应用于特定领域，而是试图为构建具有更强通用推理能力的AI系统提供一个新的、更底层的范式。因此，根据“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的标准，这篇论文应被**保留**。 2.  **第二步：正面指标** 论文摘要中充满了与筛选标准高度相关的关键词。它反复强调 **reasoning**（“automated reasoning”, “formal reasoning”, “sound reasoning in embedding space”），这正是我研究的核心。虽然它没有直接提及LLMs，但明确提到了 **transformers** 作为可以用其新语言实现的模型之一，表明其研究成果与当前主流LLM架构是兼容且相关的。其目标是结合神经网络的“learnability”和符号推理的“reliability”，这正是提升通用推理能力的关键所在。 3.  **第三步：排除标准** 该论文完全没有涉及任何排除标准中的领域。它不关注多模态，不面向任何特定应用（如医疗、化学），也不讨论模型部署、水印或安全等应用层面的可靠性问题。它纯粹聚焦于AI的基础理论和能力构建。 4.  **第四步：处理特殊和模糊情况** 这篇论文可以被看作是对“通用推理能力”问题的一个更深层次的探索。它没有停留在如何让现有LLM“学会”推理，而是提出了一个能“原生”支持推理的计算框架。这类似于在“自我进化”或“新训练范式”的范畴内，因为它提供了一个全新的构建和训练具备推理能力模型的理论基础。其目标“sound reasoning in embedding space”正是为了解决当前LLM推理不可靠、不透明的核心痛点，这与“提出一种新方法来增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的原则完全吻合。 5.  **第五步：最终决策** 综合以上分析，这篇论文虽然基础且理论性强，但其核心贡献——提出一个旨在从根本上增强AI（特别是类LLM模型）通用推理能力的新范式——与我的研究目标“提高大语言模型（LLM）本身的『通用推理能力』”高度契合。它为如何构建真正具备可靠、透明推理能力的模型提供了全新的视角和理论基础，是一篇极具前瞻性和相关性的前沿论文。因此，最终判断为 **True**。"
    },
    {
        "index": "#139",
        "title": "AI Agents as Universal Task Solvers",
        "link": "/arxiv/2510.12066",
        "arxiv_id": "2510.12066",
        "authors": "Alessandro Achille, Stefano Soatto",
        "summary": "AI reasoning agents are already able to solve a variety of tasks by deploying tools, simulating outcomes of multiple hypotheses and reflecting on them. In doing so, they perform computation, although not in the classical sense -- there is no program being executed. Still, if they perform computation, can AI agents be universal? Can chain-of-thought reasoning solve any computable task? How does an AI Agent learn to reason? Is it a matter of model size? Or training dataset size? In this work, we reinterpret the role of learning in the context of AI Agents, viewing them as compute-capable stochastic dynamical systems, and highlight the role of time in a foundational principle for learning to reason. In doing so, we propose a shift from classical inductive learning to transductive learning -- where the objective is not to approximate the distribution of past data, but to capture their algorithmic structure to reduce the time needed to find solutions to new tasks. Transductive learning suggests that, counter to Shannon's theory, a key role of information in learning is about reduction of time rather than reconstruction error. In particular, we show that the optimal speed-up that a universal solver can achieve using past data is tightly related to their algorithmic information. Using this, we show a theoretical derivation for the observed power-law scaling of inference time versus training time. We then show that scaling model size can lead to behaviors that, while improving accuracy on benchmarks, fail any reasonable test of intelligence, let alone super-intelligence: In the limit of infinite space and time, large models can behave as savants, able to brute-force through any task without any insight. Instead, we argue that the key quantity to optimize when scaling reasoning models is time, whose critical role in learning has so far only been indirectly considered.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.LG",
        "crawl_time": "2025-10-15T11:00:04.673522",
        "filter_reason": "这篇论文完全符合你的研究范围。判断依据如下： 1.  **第一步：核心判断** 论文的核心并非将LLM应用于特定领域，而是深入探讨了**AI智能体（特别是推理智能体）的内在能力和学习机制**。它提出了一种新的学习范式（transductive learning/转导学习），旨在从根本上提升AI智能体的推理效率和解决问题的能力。这正属于“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的范畴，完全符合保留标准。 2.  **第二步：正面指标** 论文摘要中密集地出现了多个高相关性的正面指标： - **核心概念**: \"AI reasoning agents\" - **能力方向**: \"reasoning\", \"solve a variety of tasks\", \"problem-solving\", \"learn to reason\"。论文直接将“推理”和“学习推理”作为研究核心。 - **新兴范式**: \"AI Agents\", \"deploying tools\", \"chain-of-thought reasoning\"。论文不仅研究了智能体，还深入分析了思维链（CoT）等技术的理论基础和局限。 3.  **第三步：排除标准** 论文完全没有涉及任何排除标准中的领域。它不讨论视觉（Vision）、特定医疗或化学应用，也不讨论水印、安全等应用层面的可靠性问题。其讨论是**基础性的和领域无关的**。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文明确以“AI Agents as Universal Task Solvers”（作为通用任务解决者的AI智能体）为题，其目标正是提出一种**通用的**智能体框架来增强LLM的通用问题解决能力，而不是将其应用于特定领域。这完全符合该情况的“保留”标准。 - **幻觉/可解释性/安全**: 虽然论文没有直接讨论幻觉，但其提出的核心议题——区分“brute-force through any task without any insight”（蛮力计算而无洞察）和真正的“intelligence”（智能）——本质上是在探讨**高质量、高效率的推理与低质量、低效率的伪推理之间的区别**。这直接关系到提升模型的内在推理质量，从而在根源上提升通用可靠性，因此符合该情况的“保留”标准。 5.  **最终决策** **核心贡献**: 该论文的核心贡献在于，它为如何提升LLM的通用推理能力提供了一个全新的、根本性的理论视角。它挑战了当前主流的“越大越好”的模型缩放思路，提出**优化推理过程中的“时间”**是发展出真正具备推理能力甚至超智能模型的关键。论文通过“转导学习”这一概念，论证了学习的目标应是捕获算法结构以减少解决新问题的时间，而非仅仅拟合数据分布。 **结论**: 这篇论文直指“大语言模型如何更好地推理”这一核心科学问题，提出了深刻的理论洞见和全新的优化方向，是典型的致力于提升LLM**自身通用推理能力**的前沿研究，与你的研究目标高度契合，必须保留。"
    },
    {
        "index": "#9",
        "title": "HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games",
        "link": "/arxiv/2510.12563",
        "arxiv_id": "2510.12563",
        "authors": "Jingcong Liang, Shijun Wan, Xuehai Wu, Siyuan Wang, Yitong Li, Qianglong Chen, Duyu Tang, Zhongyu Wei",
        "summary": "Large Reasoning Models (LRMs) have demonstrated impressive performance on complex tasks, including logical puzzle games that require deriving solutions satisfying all constraints. However, whether they can flexibly apply appropriate rules to varying conditions, particularly when faced with non-canonical game variants, remains an open question. Existing corpora focus on popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats and memorization of solution patterns, which can mask deficiencies in understanding novel rules or adapting strategies to new variants. To address this, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles across 10 games, designed to test the robustness of LRMs on the \"long-tail\" of logical games. HardcoreLogic systematically transforms canonical puzzles through three dimensions: Increased Complexity (IC), Uncommon Elements (UE), and Unsolvable Puzzles (UP), reducing reliance on shortcut memorization. Evaluations on a diverse set of LRMs reveal significant performance drops, even for models achieving top scores on existing benchmarks, indicating heavy reliance on memorized stereotypes. While increased complexity is the dominant source of difficulty, models also struggle with subtle rule variations that do not necessarily increase puzzle difficulty. Our systematic error analysis on solvable and unsolvable puzzles further highlights gaps in genuine reasoning. Overall, HardcoreLogic exposes the limitations of current LRMs and establishes a benchmark for advancing high-level logical reasoning.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.AI",
        "crawl_time": "2025-10-15T11:00:04.720500",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据您提供的筛选标准进行的详细判断过程： **第一步：核心判断——论文的本质是什么？** - **保留**。这篇论文的核心并非将LLM作为工具应用于某个特定领域，而是直接针对大语言模型（或更精确地说，大型推理模型LRMs）的『通用推理能力』本身进行深入探究。其本质贡献是： 1.  **揭示问题**：指出现有LRMs在逻辑推理任务上可能存在过度依赖记忆和模式匹配，而非真正的灵活推理能力的问题。 2.  **提出解决方案**：为了解决上述评估偏差，作者创建了一个新的、更具挑战性的评测基准（Benchmark）——HardcoreLogic。 3.  **推动进步**：该基准旨在通过测试模型在非标准、长尾逻辑谜题上的表现，来激励和引导未来研究，从而真正提升LLM的通用逻辑推理能力。 这完全符合“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的核心目标。 **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文明确聚焦于“Large Reasoning Models (LRMs)”，这是LLM的一个子集，完全符合。 - **能力方向**: 论文的核心主题就是“logical reasoning”，并深入探讨了模型在“problem-solving”和适应新规则（一种高级推理）方面的能力。这高度匹配。 - **训练方法**: 虽然论文本身没有提出新的训练方法，但它通过严格的评测揭示了现有训练方法的局限性（如过拟合和记忆依赖），为未来的训练范式（如更有效的强化学习、自我进化等）指明了方向。因此，它与您关注的方法论研究紧密相关。 - **新兴范式**: 论文的研究成果对于提升“llm-based agents”的通用问题解决能力至关重要，因为一个能够灵活处理非标准规则的模型是构建强大智能体的基础。 **第三步：排除标准——论文是否主要聚焦于以下领域？** - **多模态与视觉**: 否。论文完全基于文本的逻辑谜题，不涉及任何视觉或多模态内容。 - **特定应用领域**: 否。逻辑谜题是一个通用的认知能力测试平台，不属于生物、医疗、化学等任何特定应用领域。 - **模型可靠性（应用层面）**: 否。论文关注的是模型内在的推理能力缺陷，而非水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 不适用。 - **幻觉/可解释性/安全**: 论文通过分析模型在“可解”与“不可解”谜题上的错误，实际上是在深入探究模型推理失败的原因，这与增强模型内在可解释性、减少“推理幻觉”的目标是一致的。它不是社会学讨论，而是对模型内在能力的深度剖析。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是创建了一个旨在严格评估并推动大语言模型『通用逻辑推理能力』发展的新基准。它直面当前模型在灵活性和泛化性上的核心挑战，完全符合您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标。因此，最终判断为保留。"
    },
    {
        "index": "#8",
        "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks",
        "link": "/arxiv/2510.12635",
        "arxiv_id": "2510.12635",
        "authors": "Yuxiang Zhang, Jiangming Shu, Ye Ma, Xueyuan Lin, Shangxi Wu, Jitao Sang",
        "summary": "Large Language Models face challenges in long-horizon agentic tasks as their constrained memory is easily overwhelmed by distracting or irrelevant context. Existing working memory methods typically rely on external, heuristic mechanisms that are decoupled from the agent's core policy. In this work, we reframe working memory management as a learnable, intrinsic capability. We propose a novel framework, Memory-as-Action, where an agent actively manages its working memory by executing explicit editing operations as part of a unified policy. This formulation allows an agent, trained via reinforcement learning, to balance memory curation against long-term task objectives under given resource constraints. However, such memory editing actions break the standard assumption of a continuously growing prefix in LLM interactions, leading to what we call trajectory fractures. These non-prefix changes disrupt the causal continuity required by standard policy gradient methods, making those methods inapplicable. To address this, we propose a new algorithm, Dynamic Context Policy Optimization, which enables stable end-to-end reinforcement learning by segmenting trajectories at memory action points and applying trajectory-level advantages to the resulting action segments. Our results demonstrate that jointly optimizing for task reasoning and memory management in an end-to-end fashion not only reduces overall computational consumption but also improves task performance, driven by adaptive context curation strategies tailored to the model's intrinsic capabilities.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.AI",
        "crawl_time": "2025-10-15T11:00:04.720202",
        "filter_reason": "这篇论文完全符合我的研究范围，应该被保留。我的判断过程如下： **第一步：核心判断——这篇论文的本质是什么？** 这篇论文的本质是提出一种新的训练范式和方法论，用以增强大语言模型的基础能力。其核心贡献是“Memory-as-Action”框架，它将工作内存管理从一种外部的、启发式的机制，转变为一种可学习的、与智能体核心策略相统一的内在能力。这直接解决了LLM在长视野任务中因上下文过长而推理能力下降的根本性问题。因此，论文的核心是**改进LLM的通用推理和规划能力**，而非将其应用于特定领域。符合保留标准。 **第二步：正面指标——论文是否包含以下主题？** 论文包含了多个强烈的正面指标： - **核心概念**: 明确以 \"Large Language Models\" 为研究对象。 - **能力方向**: 聚焦于 \"long-horizon agentic tasks\"，这直接关联到 \"planning\" 和 \"problem-solving\" 等高级推理能力。 - **训练方法**: 核心方法是 \"reinforcement learning\"，并为此提出了新的算法 \"Dynamic Context Policy Optimization\"。 - **新兴范式**: 研究内容属于 \"llm-based agents\" 的范畴，旨在提升智能体的自主性和能力。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全不涉及任何排除标准中的领域： - 它不涉及多模态、视觉或机器人控制。 - 它的研究是通用的，没有限定在医疗、化学等特定应用领域。 - 它关注的是提升模型核心推理性能，而非水印、安全等应用层面的可靠性。 **第四步：处理特殊和模糊情况** 论文的研究内容恰好是“智能体”这一模糊情况的正面范例。它提出的是一种**通用的智能体框架**（Memory-as-Action），旨在通过优化内存管理来提升LLM在各类长视野任务中的通用问题解决能力，而不是将智能体应用于某个特定垂直领域。这完全符合“保留”的条件。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一种创新的、可学习的内存管理机制，并通过强化学习算法进行端到端优化，从而直接提升了LLM在长视野任务中的推理和规划能力。这完全契合我“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#6",
        "title": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection",
        "link": "/arxiv/2510.12697",
        "arxiv_id": "2510.12697",
        "authors": "Tianyu Hu, Zhen Tan, Song Wang, Huaizhi Qu, Tianlong Chen",
        "summary": "With advancements in reasoning capabilities, Large Language Models (LLMs) are increasingly employed for automated judgment tasks. While LLMs-as-Judges offer promise in automating evaluations, current approaches often rely on simplistic aggregation methods (e.g., majority voting), which can fail even when individual agents provide correct answers. To address this, we propose a multi-agent debate judge framework where agents collaboratively reason and iteratively refine their responses. We formalize the debate process mathematically, analyzing agent interactions and proving that debate amplifies correctness compared to static ensembles. To enhance efficiency, we introduce a stability detection mechanism that models judge consensus dynamics via a time-varying Beta-Binomial mixture, with adaptive stopping based on distributional similarity (Kolmogorov-Smirnov test). This mechanism models the judges' collective correct rate dynamics using a time-varying mixture of Beta-Binomial distributions and employs an adaptive stopping criterion based on distributional similarity (Kolmogorov-Smirnov statistic). Experiments across multiple benchmarks and models demonstrate that our framework improves judgment accuracy over majority voting while maintaining computational efficiency.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.AI",
        "crawl_time": "2025-10-15T11:00:04.719475",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。我的判断过程如下： 1.  **核心判断 (第一步):** *   **核心贡献:** 论文的核心是提出了一种新颖的“多智能体辩论”框架。其本质并非将LLM应用于某个特定外部领域，而是研究如何通过多个LLM智能体之间的交互、辩论和迭代优化，来提升LLM在“判断”这一通用任务上的表现。 *   **符合目标:** 这种“协作推理”和“迭代完善响应”的机制，直接作用于LLM的思维过程和问题解决路径，旨在增强其内在的通用推理能力和决策质量。这与你筛选标准中提到的“智能体协作框架”和方法论研究完全一致。 2.  **正面指标 (第二步):** *   论文摘要中明确包含了多个高相关性的正面指标： *   **核心概念:** \"Large Language Models (LLMs)\" *   **能力方向:** \"reasoning capabilities\", \"collaboratively reason\" *   **新兴范式:** \"multi-agent debate\", 这是一种典型的多智能体系统应用，旨在提升模型能力。 *   这些关键词的出现，强力佐证了论文与“大语言模型通用推理能力”这一主题的高度相关性。 3.  **排除标准 (第三步):** *   论文的研究内容不涉及任何排除标准中的领域。它没有讨论视觉、多模态，也没有聚焦于医疗、化学、机器人等特定应用，更不涉及水印等模型基础设施或应用层面的可靠性问题。其研究对象是LLM本身的推理和判断机制。 4.  **处理特殊情况 (第四步):** *   **智能体框架:** 本论文是“提出一种通用的智能体协作框架”的完美范例。虽然论文的实验设置是“LLM-as-Judges”，但其提出的“多智能体辩论”框架和“自适应稳定性检测”机制具有通用性，其核心目标是提升LLM的推理、辩论和达成共识的能力，这是一种可迁移到多种通用问题解决场景的基础能力框架。因此，它完全符合保留条件，而不是被排除的“用于特定领域的智能体”。 5.  **最终决策 (第五步):** *   综合以上分析，这篇论文的本质是提出一种新的方法论（多智能体辩论框架），旨在通过结构化的交互过程来放大和优化LLM的内在推理能力。它直接触及了LLM通用推理的核心——如何让模型通过反思、辩论和协作来得出更优的结论。因此，这篇论文不仅符合，而且非常契合你关于“大语言模型通用推理能力”的研究课题。"
    },
    {
        "index": "#22",
        "title": "$\\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning",
        "link": "/arxiv/2510.12264",
        "arxiv_id": "2510.12264",
        "authors": "Deyu Zou, Yongqiang Chen, Jianxiang Wang, Haochen Yang, Mufei Li, James Cheng, Pan Li, Yu Gong",
        "summary": "Active reasoning requires large language models (LLMs) to interact with external sources and strategically gather information to solve problems. Central to this process is belief tracking: maintaining a coherent understanding of the problem state and the missing information toward the solution. However, due to limited reasoning capabilities, LLM-based agents often suffer from belief deviation: they struggle to correctly model beliefs, lose track of problem states, and fall into uninformative or repetitive actions. Once this happens, errors compound and reinforcement learning (RL) training fails to properly credit the crucial exploratory steps. To address this issue, we propose to track the deviation of model beliefs and develop $\\mathbf{T^3}$, a simple yet effective method that detects excessive belief deviation and truncates trajectories during training to remove uninformative tails. By preserving credit for informative prefixes, $\\mathbf{T^3}$ systematically improves policy optimization. Across 5 challenging tasks, $\\mathbf{T^3}$ consistently enhances training stability, token efficiency, and final performance, achieving up to 30% gains while cutting rollout tokens by roughly 25%. These results highlight belief control as a key principle for developing robust and generalizable LLM-based active reasoners.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.AI",
        "crawl_time": "2025-10-15T11:00:04.724239",
        "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为 $\\mathbf{T^3}$ 的新方法，用于解决大语言模型在“主动推理”过程中遇到的“信念偏差”问题。主动推理要求模型通过与外部环境交互来策略性地收集信息，这是一种高级的通用推理能力。论文没有将LLM应用于某个特定领域（如医疗或化学），而是聚焦于改进LLM智能体在进行通用问题求解时的内在推理机制和训练过程。其核心贡献是一种新的训练范式（通过截断轨迹来优化强化学习），旨在提升模型的基础推理稳定性和效率。这完全符合“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 明确以 \"Large language models (LLMs)\" 为研究对象。 *   **能力方向**: 核心主题是 \"Active reasoning\"（主动推理），这是推理能力的一个重要分支。同时，它也涉及 \"problem-solving\"（问题解决）。 *   **训练方法**: 论文的核心方法论是基于 \"Reinforcement Learning (RL)\" 的改进，$\\mathbf{T^3}$ 是一种在RL训练过程中提升效果的技巧。 *   **新兴范式**: 论文的研究对象是 \"LLM-based agents\"，探讨如何让这些智能体更好地进行推理。 3.  **第三步：排除标准** 论文完全没有触及任何排除标准： *   它不涉及多模态、视觉或任何特定应用领域。 *   它讨论的“信念偏差”是模型内在推理过程的缺陷，而非应用层面的水印、安全或安保问题。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 论文讨论的LLM智能体是为了进行通用的“主动推理”，而不是应用于特定领域。它提出的 $\\mathbf{T^3}$ 方法是一种通用的训练框架，旨在增强智能体本身的通用问题解决能力，因此应该保留。 *   **幻觉/可解释性/安全**: 论文解决的“信念偏差”问题，与“幻觉”高度相关，都是指模型对世界状态的内部表征出现了错误。论文提出了一种新方法（$\\mathbf{T^3}$）来减少这种偏差，从而提升模型的内在推理质量和可靠性。这完全符合“提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的保留标准。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种新的强化学习训练方法 $\\mathbf{T^3}$，通过控制LLM智能体的“信念偏差”，显著提升了其在“主动推理”这一通用任务上的表现。该研究直接针对并改进了LLM的通用推理能力，与你的核心目标高度契合。因此，这篇论文**符合**你的研究要求。"
    },
    {
        "index": "#19",
        "title": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis",
        "link": "/arxiv/2510.12350",
        "arxiv_id": "2510.12350",
        "authors": "Ayush Khaitan, Vijay Ganesh",
        "summary": "Large language models have recently demonstrated advanced capabilities in solving IMO and Putnam problems; yet their role in research mathematics has remained fairly limited. The key difficulty is verification: suggested proofs may look plausible, but cannot be trusted without rigorous checking. We present a framework, called LLM+CAS, and an associated tool, O-Forge, that couples frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic Feedback loop to produce proofs that are both creative and symbolically verified. Our focus is on asymptotic inequalities, a topic that often involves difficult proofs and appropriate decomposition of the domain into the \"right\" subdomains. Many mathematicians, including Terry Tao, have suggested that using AI tools to find the right decompositions can be very useful for research-level asymptotic analysis. In this paper, we show that our framework LLM+CAS turns out to be remarkably effective at proposing such decompositions via a combination of a frontier LLM and a CAS. More precisely, we use an LLM to suggest domain decomposition, and a CAS (such as Mathematica) that provides a verification of each piece axiomatically. Using this loop, we answer a question posed by Terence Tao: whether LLMs coupled with a verifier can be used to help prove intricate asymptotic inequalities. More broadly, we show how AI can move beyond contest math towards research-level tools for professional mathematicians.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.AI",
        "crawl_time": "2025-10-15T11:00:04.723328",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是详细的判断过程： 1.  **核心判断（第一步）**：这篇论文的本质不是简单地将现有LLM应用于数学领域，而是提出一种全新的**方法论框架**（LLM+CAS, In-Context Symbolic Feedback loop）来**增强LLM本身在复杂任务上的推理和问题解决能力**。其核心贡献在于“耦合”（couple）LLM与计算机代数系统（CAS），并通过一个反馈循环，使LLM能够产出既“有创意”又“经过符号化验证”的证明。这种方法直接触及并提升LLM在处理复杂、多步、需要严格验证的推理任务时的瓶颈，即“不可信的输出”。这完全符合“改进LLM的基础能力、增强其逻辑、数学、多步推理等通用能力”的要求，属于“工具使用”和“智能体协作框架”的前沿研究。 2.  **正面指标（第二步）**：论文高度契合多个正面指标。 *   **核心概念**: 明确以“Large language models”为核心。 *   **能力方向**: 专注于“reasoning”中的“math reasoning”，旨在解决“asymptotic inequalities”的“proof”问题，这属于顶级的数学推理和问题解决范畴。 *   **新兴范式**: 整篇论文都在探讨一种“tool use”（使用CAS作为验证工具）的范式，并构建了一个“llm-based agent”框架，使其能完成从“contest math”向“research-level tools”的跨越。 3.  **排除标准（第三步）**：论文不触及任何排除标准。 *   它不属于多模态或视觉研究。 *   虽然其应用领域是“asymptotic analysis”研究数学，但这并非“医疗、化学、生物”等特定的垂直应用领域。数学是逻辑和推理的基础科学，提升模型在数学上的推理能力，本质上是在提升其最核心的通用推理能力。论文的焦点是**方法论**而非特定数学问题的答案。 4.  **处理模糊情况（第四步）**： *   **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的完美范例。LLM负责提出假设，CAS（作为工具）负责验证，这种“LLM提议者+验证者”的模式具有很高的通用性，可以迁移到其他需要形式化验证的推理任务中。它并非仅仅用于某个特定领域的自动化。 *   **幻觉/可解释性/安全**: 论文的核心动机之一就是解决LLM在数学推理中的“幻觉”问题（“suggested proofs may look plausible, but cannot be trusted”）。它提出的解决方案是一种根本性的、模型内在能力的增强，通过引入外部验证工具来确保输出质量，从而“提升模型的通用可靠性和推理质量”，而非应用层面的讨论。 **最终决策**：综合来看，这篇论文通过提出一个创新的“工具使用”框架，有效解决了LLM在进行高级数学推理时面临的“不可验证性”这一核心挑战。它不仅提升了LLM在数学这一基础学科上的推理深度和可靠性，更重要的是，其所构建的“LLM+验证器”范式为增强大语言模型的通用、严谨推理能力提供了极具价值的思路和实证，完全契合你关于“大语言模型通用推理能力”的研究课题。因此，应予以保留。"
    },
    {
        "index": "#27",
        "title": "ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents",
        "link": "/arxiv/2510.12194",
        "arxiv_id": "2510.12194",
        "authors": "Linyi Yang, Yixuan Weng",
        "summary": "Current deep-research agents run in a ''fire-and-forget'' mode: once started, they give users no way to fix errors or add expert knowledge during execution. We present ResearStudio, the first open-source framework that places real-time human control at its core. The system follows a Collaborative Workshop design. A hierarchical Planner-Executor writes every step to a live ''plan-as-document,'' a fast communication layer streams each action, file change, and tool call to a web interface. At any moment, the user can pause the run, edit the plan or code, run custom commands, and resume -- switching smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In fully autonomous mode, ResearStudio achieves state-of-the-art results on the GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These results show that strong automated performance and fine-grained human control can coexist. The full code, protocol, and evaluation scripts are available at https://github.com/ResearAI/ResearStudio. We will continue to update the repository to encourage further work on safe and controllable research agents. Our live demo is publicly accessible at http://ai-researcher.net:3000/. We support the development of DeepScientist, which can be accessed at https://github.com/ResearAI/DeepScientist.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.AI",
        "crawl_time": "2025-10-15T11:00:04.725622",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。判断依据如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一个名为“ResearStudio”的**通用智能体框架**。这个框架的本质并非将LLM应用于某个特定垂直领域，而是致力于解决当前深度研究智能体（Deep-Research Agents）普遍存在的“黑盒”和“失控”问题。它通过引入实时、细粒度的人机交互与控制机制，旨在**增强智能体在执行复杂、多步推理任务（即深度研究）时的通用能力、可控性和可靠性**。这完全符合“改进LLM的基础能力”和“提出新的智能体协作框架”的核心目标。它不是关于LLM在医疗、化学等领域的应用，而是关于如何让基于LLM的智能体本身变得更强大、更通用。 2.  **第二步：正面指标** 论文高度符合多个正面指标： -   **核心概念**: 论文围绕“Deep-Research Agents”展开，其核心驱动力是大语言模型（LLMs）。 -   **能力方向**: “深度研究”（Deep-Research）本质上是一种复杂的**问题解决（problem-solving）**和**规划（planning）**能力，涉及信息检索、分析、整合和多步推理。论文通过提升智能体的可控性来间接提升其完成这类复杂任务的通用推理能力。 -   **新兴范式**: 论文明确聚焦于**“llm-based agents”**，并提出了一个创新的**“multi-agent”**（Planner-Executor）协作模式。同时，它也深度探讨了**“tool use”**，因为框架将工具调用作为核心交互内容之一进行实时展示和控制。 3.  **第三步：排除标准** 论文不触及任何排除标准： -   它不涉及多模态、视觉内容。 -   它的应用场景是通用的“研究”，而非医疗、化学、机器人等特定领域。虽然其成果可以用于这些领域，但论文本身贡献的是一个通用框架。 -   它不讨论模型基础设施、部署优化或硬件加速。 4.  **第四步：处理特殊和模糊情况** -   **智能体/工具使用**: 这正是论文的核心。它提出的是一种**通用的智能体协作框架（Planner-Executor）和工具使用方法**，其目的是增强LLM在“深度研究”这一通用问题解决场景下的能力。它不是“用于化学实验的智能体”，而是“用于任何需要深度研究的智能体”，因此完全符合保留条件。 -   **可靠性**: 论文通过引入“human-intervenable”机制，直接提升了智能体运行的可靠性。这是一种从方法论层面提升模型系统可靠性的方式，而非应用层面的水印或安全讨论，因此应被视为对通用推理能力的一种增强。 5.  **第五步：最终决策** 综合分析，ResearStudio论文的核心是提出一种创新的、以人为中心的通用智能体框架，旨在提升LLM在复杂、多步推理任务中的表现和可控性。它直接贡献于“大语言模型通用推理能力”这一研究课题，特别是在智能体规划和问题解决的前沿范式上。因此，最终判断为**True**。"
    },
    {
        "index": "#25",
        "title": "GOAT: A Training Framework for Goal-Oriented Agent with Tools",
        "link": "/arxiv/2510.12218",
        "arxiv_id": "2510.12218",
        "authors": "Hyunji Min, Sangwon Jung, Junyoung Sung, Dosung Lee, Leekyeung Han, Paul Hongsuck Seo",
        "summary": "Large language models (LLMs) have recently been extended beyond traditional text generation to serve as interactive agents capable of using external tools based on user intent. However, current LLM agents still show limited ability to handle goal-oriented queries, which require decomposing a high-level objective into multiple interdependent API calls with correct planning and execution. Current approaches mainly rely on zero-shot evaluation due to the absence of training data. While proprietary closed-source models such as GPT-4 demonstrate strong reasoning abilities, smaller open-source models struggle to perform complex tool use effectively. Thus, we propose a novel training framework GOAT, which enables fine-tuning of LLM agents in a human annotation-free setting. GOAT automatically constructs synthetic datasets of goal-oriented API execution tasks directly from given API documents, equipping models with the ability to reason over interdependent calls and generate coherent responses. Through extensive experiments, we show that GOAT-trained agents achieve state-of-the-art performance across multiple existing goal-oriented benchmarks. In addition, we introduce GOATBench, a new goal-oriented API execution benchmark, and demonstrate that agents trained with GOAT also excel in this setting. These results highlight GOAT as a practical path toward building robust open-source LLM agents capable of complex reasoning and tool use.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.AI",
        "crawl_time": "2025-10-15T11:00:04.725098",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断** - **核心贡献**: 该论文的核心是提出了一种名为GOAT的、全新的训练框架，旨在提升大语言模型作为智能体时的通用推理能力。 - **符合保留标准**: 论文并非将LLM应用于特定领域，而是聚焦于改进LLM自身的基础能力。它通过一种新的训练范式（自动合成数据并进行微调），解决了LLM在处理“目标导向型查询”时的核心瓶颈，即“将高层级目标分解为多个相互依赖的API调用”。这个过程本身就是一种高级的**规划**和**多步推理**能力的体现。因此，论文的本质是增强LLM的逻辑、规划和推理等通用能力，符合保留标准。 2.  **第二步：正面指标** - **核心概念**: 论文明确以 `Large language models (LLMs)` 为研究对象。 - **能力方向**: 摘要中反复出现 `reasoning` (`reason over interdependent calls`, `complex reasoning`)、`planning` (`correct planning and execution`)、`problem-solving` (`goal-oriented queries`) 等关键词，这些都是通用推理能力的核心。 - **训练方法**: 论文提出了 `a novel training framework`，这是一种方法论创新，旨在 `fine-tuning of LLM agents`，属于新的训练范式。 - **新兴范式**: 论文的研究主题是 `LLM-based agents` 和 `tool use`，这正是当前提升LLM通用能力的前沿方向。 3.  **第三步：排除标准** - 论文的研究内容完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）等排除领域。其框架是通用的，可以应用于任何给定的API文档，而非局限于某个垂直领域。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的典型范例。GOAT框架不绑定任何特定领域的工具，而是从通用的API文档中学习，其目标是让模型掌握“处理复杂工具使用”的通用能力，这正是为了增强其通用推理和问题解决能力，因此应该保留。 **最终决策**: 综合以上分析，该论文致力于通过创新的训练方法，从底层增强LLM的规划和复杂推理能力，使其能更好地作为通用智能体进行工具使用和问题解决。其研究目标、方法和贡献都与您筛选“致力于提高大语言模型（LLM）本身的『通用推理能力』”论文的核心目标高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#40",
        "title": "Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation",
        "link": "/arxiv/2510.12047",
        "arxiv_id": "2510.12047",
        "authors": "Soohan Lim, Joonghyuk Hahn, Hyunwoo Park, Sang-Ki Ko, Yo-Sub Han",
        "summary": "Prevailing code generation benchmarks, such as HumanEval+ and MBPP+, primarily evaluate large language models (LLMs) with pass@k on functional correctness using well-formed inputs. However, they ignore a crucial aspect of real-world software: adherence to contracts-the preconditions and validity constraints that dictate how ill-formed inputs must be rejected. This critical oversight means that existing benchmarks fail to measure, and models consequently fail to generate, truly robust and reliable code snippets. We introduce PACT, a program assessment and contract-adherence evaluation framework, to bridge this gap. PACT is the first framework designed to systematically evaluate and enhance contract-adherence in LLM-generated code snippets alongside functional correctness. PACT's contributions are threefold: First, it provides a comprehensive test-suite corpus focused on contract violations, extending HumanEval+ and MBPP+. Second, it enables a systematic analysis of code generation under varied prompting conditions. This analysis demonstrates that augmenting prompts with contract-violating test cases significantly enhance a model's ability to respect contracts compared to using contract description alone. Finally, it introduces novel metrics to rigorously quantify contract adherence in both test generation and code generation. By revealing critical errors that conventional benchmarks overlook, PACT provides the rigorous and interpretable metrics to evaluate the robustness of LLM-generated code snippets in both functionality and contract-adherence.Our code and data are available at https://github.com/suhanmen/PACT.",
        "subjects": "Artificial Intelligence, Software Engineering",
        "date": "2025-10-14",
        "category": "cs.AI",
        "crawl_time": "2025-10-15T11:00:04.729583",
        "filter_reason": "这篇论文符合筛选要求，应予以保留。我的判断过程如下： **第一步：核心判断——这篇论文的本质是什么？** 这篇论文的本质是**提出一种新的方法论来提升大语言模型在特定任务上的逻辑严谨性和鲁棒性**。尽管论文的评估背景是“代码生成”，但其核心贡献并非将LLM作为一个工具应用于软件开发领域，而是深入探究并试图修复LLM在逻辑推理中的一个根本性缺陷：对规则/契约的遵守。论文提出的“在提示中加入契约违反测试用例”的方法，是一种通用的提示工程技术，旨在引导模型进行更严谨的逻辑推理，而不是仅仅依赖描述性的契约文本。这符合“改进LLM的基础能力、增强其逻辑、推理等通用能力”的核心要求。 **第二步：正面指标——论文是否包含以下主题？** *   **核心概念**: 论文明确以“Large language models (LLMs)”为核心研究对象。 *   **能力方向**: 论文聚焦于“contract-adherence”（契约遵守），这本质上是一种**逻辑推理**能力。它要求模型理解并执行条件约束（“如果输入无效，则拒绝”），这是高级逻辑推理和问题解决能力的关键组成部分。 *   **新兴范式**: 论文提出了一种新的**提示策略**来增强模型能力，这与思维链（CoT）等通过提示优化推理路径的范式在精神上是一致的，都属于改进模型推理方法论的研究。 **第三步：排除标准——论文是否主要聚焦于以下领域？** *   **多模态与视觉**: 论文完全不涉及。 *   **特定应用领域**: 这是需要仔细辨析的一点。虽然论文在“代码生成”这个领域进行实验，但代码生成本身就是衡量LLM逻辑和推理能力的核心任务之一。它不同于“用于医疗诊断的LLM”这类纯粹的外部领域应用。论文的最终目标不是解决一个具体的软件开发问题，而是通过这个任务载体，提升模型**通用的、可迁移的遵守逻辑规则的能力**。 *   **模型可靠性（应用层面）**: 论文关注的是模型的“robustness”（鲁棒性）和“contract-adherence”（契约遵守），这属于模型内在的、与推理质量直接相关的可靠性问题。它并非讨论水印、内容安全等应用层面的防护措施，而是探究如何从模型内部生成更符合逻辑、更严谨的输出。 **第四步：处理特殊和模糊情况** *   **幻觉/可解释性/安全**: 这篇论文的情况与筛选标准中“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留”的描述高度吻合。论文所解决的“未能拒绝非法输入”问题，可以看作是一种在逻辑推理上的“系统性错误”或“隐性幻觉”。作者提出的新方法（在提示中加入反例）直接提升了模型输出的逻辑正确性和鲁棒性，从而增强了其通用推理质量。 **第五步：最终决策** 综合分析，这篇论文的核心贡献是提出了一种方法论（PACT框架和相应的提示策略），旨在通过增强LLM对逻辑约束的理解和执行能力，来提升其通用推理的严谨性和鲁棒性。虽然其评估场景是代码生成，但所解决问题的本质（遵守规则、处理边界条件）是通用的。因此，它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    }
]