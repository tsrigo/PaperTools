[
    {
        "index": "#2",
        "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards",
        "link": "/arxiv/2509.21319",
        "arxiv_id": "2509.21319",
        "authors": "Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev",
        "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.531588",
        "filter_reason": "这篇论文的核心贡献是提出了一种新的强化学习方法RLBFF（Reinforcement Learning with Binary Flexible Feedback），用于改进大语言模型的后训练过程，特别是奖励模型的训练。该方法结合了人类反馈(RLHF)和可验证奖励(RLVR)的优点，旨在解决RLHF的可解释性和奖励黑客问题，以及RLVR仅关注正确性的局限性。从本质上看，这属于改进LLM基础能力的范畴，目的是提高模型的对齐质量和响应质量，而不是将LLM作为工具应用到特定领域。论文没有涉及多模态内容或特定应用领域，而是关注通用的LLM训练方法。虽然论文没有直接强调推理能力，但改进奖励模型会间接提升模型的整体能力，包括推理能力。此外，论文还展示了使用RLBFF训练的模型在多个基准测试上的优异性能，进一步证明了其有效性。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#4",
        "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective",
        "link": "/arxiv/2509.21134",
        "arxiv_id": "2509.21134",
        "authors": "Yiwen Zhang, Ziang Chen, Fanqi Kong, Yizhe Huang, Xue Feng",
        "summary": "Large Language Models (LLMs) have been used to make decisions in complex scenarios, where they need models to think deeply, reason logically, and decide wisely. Many existing studies focus solely on multi-round conversations in social tasks or simulated environments, neglecting the various types of decisions and their interdependence. Current reinforcement learning methods struggle to consider the strategies of others during training. To address these issues, we first define a strategic decision-making problem that includes two types of decisions and their temporal dependencies. Furthermore, we propose **T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to optimize the perception of other individual strategies and the game situation trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm, ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating rollouts based on reasoning the strategies of other individuals, 2) estimating advantages at both the graph-level and sample-level, and 3) balancing global and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in terms of model output compliance and cooperative outcomes. Additionally, when compared to models with parameter sizes 100 times larger, it shows an 18% improvement. This demonstrates the effectiveness of the ToMPO algorithm in enhancing the model's strategic decision-making capabilities.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-09-25",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T18:47:29.774137",
        "filter_reason": "这篇论文的核心贡献是提出ToMPO (Theory of Mind Policy Optimization)算法，旨在增强大语言模型在复杂场景中的战略决策能力。从本质上看，论文完全符合我们的研究目标，因为它专注于改进LLM的基础能力，特别是提出了一种新的强化学习训练范式来增强模型的推理能力。论文明确关注LLM需要\"think deeply, reason logically, and decide wisely\"的能力，这直接对应通用推理能力的核心要素。从多智能体视角研究LLM的战略决策，论文通过优化LLM对他人策略的感知和推理，增强了模型的逻辑推理和问题解决能力。论文满足所有正面指标：核心概念是LLMs，能力方向涉及reasoning和problem-solving，训练方法采用强化学习，新兴范式涉及multi-agent systems。同时，论文不涉及任何排除标准中的领域，如多模态、特定应用领域或模型基础设施等。因此，该论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究范围。"
    },
    {
        "index": "#6",
        "title": "Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond",
        "link": "/arxiv/2509.21284",
        "arxiv_id": "2509.21284",
        "authors": "Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng",
        "summary": "Existing research indicates that the output of Chain-of-Thought (CoT) is significantly affected by input perturbations. Although many methods aim to mitigate such impact by optimizing prompts, a theoretical explanation of how these perturbations influence CoT outputs remains an open area of research. This gap limits our in-depth understanding of how input perturbations propagate during the reasoning process and hinders further improvements in prompt optimization methods. Therefore, in this paper, we theoretically analyze the effect of input perturbations on the fluctuation of CoT outputs. We first derive an upper bound for input perturbations under the condition that the output fluctuation is within an acceptable range, based on which we prove that: (i) This upper bound is positively correlated with the number of reasoning steps in the CoT; (ii) Even an infinitely long reasoning process cannot eliminate the impact of input perturbations. We then apply these conclusions to the Linear Self-Attention (LSA) model, which can be viewed as a simplified version of the Transformer. For the LSA model, we prove that the upper bound for input perturbation is negatively correlated with the norms of the input embedding and hidden state vectors. To validate this theoretical analysis, we conduct experiments on three mainstream datasets and four mainstream models. The experimental results align with our theoretical analysis, empirically demonstrating the correctness of our findings.",
        "subjects": "Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.534033",
        "filter_reason": "这篇论文的核心是对思维链（Chain-of-Thought, CoT）的鲁棒性进行理论分析，研究输入扰动如何影响CoT输出的波动，并推导出输入扰动的上界及其与推理步骤数量、嵌入向量范数等的关系。从本质上看，这篇论文属于改进LLM基础能力的研究范畴，特别是针对推理过程中的鲁棒性问题。思维链（CoT）是提高大语言模型推理能力的关键技术之一，论文深入分析了CoT推理过程中的稳定性问题，这与增强LLM的通用推理能力直接相关。论文没有涉及任何特定应用领域，其研究是通用的，适用于CoT推理过程本身，不符合任何排除标准。虽然论文没有提出新的训练方法或新兴范式，但它通过理论分析深化了我们对LLM推理过程的理解，这种基础性的理论研究对于未来改进LLM的推理能力和鲁棒性具有重要意义，因此符合研究目标。"
    },
    {
        "index": "#11",
        "title": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents",
        "link": "/arxiv/2509.21212",
        "arxiv_id": "2509.21212",
        "authors": "Yaxiong Wu, Yongyue Zhang, Sheng Liang, Yong Liu",
        "summary": "Long-term conversational agents require effective memory management to handle dialogue histories that exceed the context window of large language models (LLMs). Existing methods based on fact extraction or summarization reduce redundancy but struggle to organize and retrieve relevant information across different granularities of dialogue and generated memory. We introduce SGMem (Sentence Graph Memory), which represents dialogue as sentence-level graphs within chunked units, capturing associations across turn-, round-, and session-level contexts. By combining retrieved raw dialogue with generated memory such as summaries, facts and insights, SGMem supplies LLMs with coherent and relevant context for response generation. Experiments on LongMemEval and LoCoMo show that SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.",
        "subjects": "Computation and Language, Information Retrieval",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.589896",
        "filter_reason": "这篇论文的核心贡献是提出SGMem（句子图记忆）方法，用于改进大语言模型在长期对话中的记忆管理能力。论文专注于解决LLM处理超出上下文窗口的对话历史时的挑战，通过将对话表示为句子级图并有效检索相关信息，增强了LLM的长期记忆能力。虽然论文不直接针对推理、数学或逻辑能力，但长期记忆管理是支持复杂推理和多步对话的基础能力，属于提升LLM通用能力的范畴。论文关注的是通用对话智能体而非特定领域应用，不符合任何排除标准。论文中提到的\"long-term conversational agents\"属于LLM-based agents的新兴范式，符合正面指标。因此，这篇论文符合\"大语言模型通用推理能力\"的研究目标，应予以保留。"
    },
    {
        "index": "#10",
        "title": "Query-Centric Graph Retrieval Augmented Generation",
        "link": "/arxiv/2509.21237",
        "arxiv_id": "2509.21237",
        "authors": "Yaxiong Wu, Jianyuan Bo, Yongyue Zhang, Sheng Liang, Yong Liu",
        "summary": "Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs) with external knowledge for long-context understanding and multi-hop reasoning, but existing methods face a granularity dilemma: fine-grained entity-level graphs incur high token costs and lose context, while coarse document-level graphs fail to capture nuanced relations. We introduce QCG-RAG, a query-centric graph RAG framework that enables query-granular indexing and multi-hop chunk retrieval. Our query-centric approach leverages Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with controllable granularity, improving graph quality and interpretability. A tailored multi-hop retrieval mechanism then selects relevant chunks via the generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG consistently outperforms prior chunk-based and graph-based RAG methods in question answering accuracy, establishing a new paradigm for multi-hop reasoning.",
        "subjects": "Computation and Language, Information Retrieval",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.589226",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。首先，从核心判断来看，这篇论文的本质是提出一种名为QCG-RAG的查询中心图RAG框架，旨在增强大语言模型的多跳推理能力。论文的核心贡献是改进LLM的基础推理能力，而非将其作为工具应用于特定领域。其次，从正面指标看，论文明确涉及大语言模型(LLMs)和推理能力(特别是multi-hop reasoning)，这与研究目标高度一致。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性。虽然论文没有涉及强化学习、智能体等新兴范式，但其核心关注的多跳推理能力正是LLM通用推理能力的重要组成部分。论文提出的查询中心图检索方法是一种通用的方法论，可以增强LLM在各种任务中的推理表现，而非局限于特定应用场景。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#13",
        "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning",
        "link": "/arxiv/2509.21193",
        "arxiv_id": "2509.21193",
        "authors": "Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin",
        "summary": "Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: https://github.com/tangxiangru/Eigen-1.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.596693",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为Eigen-1的统一框架，通过结合隐式检索和结构化协作来增强大语言模型的推理能力。论文提出了两个主要创新点：基于监控器的检索模块（在token级别运作，最小化对外部知识检索的干扰）和分层解决方案细化（HSR，通过迭代指定每个候选作为锚点并由其同伴修复）。虽然论文在科学推理任务（如生物和化学领域）上进行了评估，但其方法论本质上是通用的，旨在提升LLM的基础推理能力，而非解决特定领域问题。论文明确解决了LLM推理中的两个关键瓶颈：显式检索导致的推理碎片化和多智能体管道中的解决方案稀释问题。这些改进直接针对LLM的通用推理能力，符合研究目标。论文还包含多个正面指标，如LLMs、推理能力和多智能体系统。因此，尽管评估领域涉及科学推理，但论文的核心是提升LLM的通用推理能力，应被保留。"
    },
    {
        "index": "#26",
        "title": "Behind RoPE: How Does Causal Mask Encode Positional Information?",
        "link": "/arxiv/2509.21042",
        "arxiv_id": "2509.21042",
        "authors": "Junu Kim, Xiao Liu, Zhenghao Lin, Lei Ji, Yeyun Gong, Edward Choi",
        "summary": "While explicit positional encodings such as RoPE are a primary source of positional information in Transformer decoders, the causal mask also provides positional information. In this work, we prove that the causal mask can induce position-dependent patterns in attention scores, even without parameters or causal dependency in the input. Our theoretical analysis indicates that the induced attention pattern tends to favor nearby query-key pairs, mirroring the behavior of common positional encodings. Empirical analysis confirms that trained models exhibit the same behavior, with learned parameters further amplifying these patterns. Notably, we found that the interaction of causal mask and RoPE distorts RoPE's relative attention score patterns into non-relative ones. We consistently observed this effect in modern large language models, suggesting the importance of considering the causal mask as a source of positional information alongside explicit positional encodings.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.608212",
        "filter_reason": "这篇论文的核心贡献是深入分析了Transformer架构中因果掩码如何编码位置信息，以及它与RoPE的相互作用。根据筛选标准，我认为它符合研究范围，原因如下： 首先，从核心判断来看，这篇论文的本质是研究LLM的基础架构机制，特别是位置编码这一核心组件。位置编码对于LLM的推理能力至关重要，因为它直接影响模型理解序列中元素关系的能力，这是逻辑推理和多步推理的基础。论文不是将LLM作为工具应用到特定领域，而是深入探究LLM内部工作机制。 其次，论文符合正面指标中的\"核心概念\"，因为它直接研究大型语言模型(LLMs)的架构机制。虽然它没有直接讨论推理、规划或问题解决，但位置编码是这些能力的基础性支撑。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 最后，虽然论文没有提出新的训练范式或直接改进推理能力的方法，但它提供了对LLM基础架构的深入理解，这种基础性研究对于未来改进LLM的通用推理能力具有重要价值。理解因果掩码如何提供位置信息，有助于设计更有效的位置编码机制，从而提升模型的整体推理能力。 因此，这篇论文符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#34",
        "title": "WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs",
        "link": "/arxiv/2509.20863",
        "arxiv_id": "2509.20863",
        "authors": "Guowei Xu, Wenxin Xu, Jiawang Zhao, Kaisheng Ma",
        "summary": "Diffusion models have recently shown strong potential in language modeling, offering faster generation compared to traditional autoregressive approaches. However, applying supervised fine-tuning (SFT) to diffusion models remains challenging, as they lack precise probability estimates at each denoising step. While the diffusion mechanism enables the model to reason over entire sequences, it also makes the generation process less predictable and often inconsistent. This highlights the importance of controlling key tokens that guide the direction of generation. To address this issue, we propose WeFT, a weighted SFT method for diffusion language models, where tokens are assigned different weights based on their entropy. Derived from diffusion theory, WeFT delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and MATH-500). The code and models will be made publicly available.",
        "subjects": "Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.611801",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出一种名为WeFT的加权监督微调方法，专门用于改进扩散语言模型(dLLMs)的推理能力。论文的核心贡献不是将LLM作为工具应用到特定领域，而是通过新的训练范式（基于熵值的权重分配）来增强模型本身的推理能力。论文在四个推理基准测试（Sudoku、Countdown、GSM8K和MATH-500）上验证了方法的有效性，表明其提升了模型的数学和逻辑推理能力。这符合\"改进LLM基础能力、提出新训练范式、增强其逻辑和数学推理能力\"的保留标准。 第二步：正面指标 论文包含以下正面指标： - 核心概念：研究扩散语言模型(dLLMs)，属于大语言模型的范畴 - 能力方向：明确关注推理能力，特别是在数学推理(GSM8K, MATH-500)和逻辑推理(Sudoku, Countdown)任务上 - 论文虽然没有涉及强化学习或智能体系统等其他正面指标，但已包含最核心的指标 第三步：排除标准 论文不符合任何排除标准： - 虽然提到扩散模型，但指的是语言模型领域的扩散模型，而非视觉领域的扩散模型 - 没有专注于任何特定应用领域（如医疗、化学等） - 没有主要关注模型可靠性问题（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及明显的特殊或模糊情况。虽然通过基于熵值的权重分配方法间接提高了生成过程的一致性和可预测性，可能有助于减少幻觉，但这不是论文的主要焦点。 综上所述，这篇论文的核心贡献是提出一种新的微调方法来增强大语言模型的通用推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#37",
        "title": "Distilling Many-Shot In-Context Learning into a Cheat Sheet",
        "link": "/arxiv/2509.20820",
        "arxiv_id": "2509.20820",
        "authors": "Ukyo Honda, Soichiro Murakami, Peinan Zhang",
        "summary": "Recent advances in large language models (LLMs) enable effective in-context learning (ICL) with many-shot examples, but at the cost of high computational demand due to longer input tokens. To address this, we propose cheat-sheet ICL, which distills the information from many-shot ICL into a concise textual summary (cheat sheet) used as the context at inference time. Experiments on challenging reasoning tasks show that cheat-sheet ICL achieves comparable or better performance than many-shot ICL with far fewer tokens, and matches retrieval-based ICL without requiring test-time retrieval. These findings demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs in downstream tasks.",
        "subjects": "Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.618353",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础能力，特别是上下文学习(in-context learning)能力。论文提出的\"cheat-sheet ICL\"方法通过将many-shot示例提炼为简洁摘要，有效增强了LLM在推理任务上的性能。这明显属于改进LLM通用推理能力的范畴，而不是将LLM作为工具应用到特定领域。 其次，从正面指标来看，论文明确包含了\"核心概念\"(Large language models, LLMs)和\"能力方向\"(reasoning)，摘要中特别提到在\"challenging reasoning tasks\"上的实验，表明其关注点正是推理能力。 第三，从排除标准来看，论文不涉及多模态与视觉、特定应用领域或模型可靠性等被排除的领域。论文关注的是通用推理任务，而非医疗、化学等特定领域应用。 此外，论文也不涉及特殊或模糊情况需要额外判断。 综上所述，这篇论文的核心贡献是提出了一种新的方法来增强LLM的上下文学习和推理能力，完全符合筛选\"致力于提高大语言模型本身的通用推理能力\"论文的研究目标。"
    },
    {
        "index": "#43",
        "title": "Confidence-guided Refinement Reasoning for Zero-shot Question Answering",
        "link": "/arxiv/2509.20750",
        "arxiv_id": "2509.20750",
        "authors": "Youwon Jang, Woo Suk Choi, Minjoon Jung, Minsu Lee, Byoung-Tak Zhang",
        "summary": "We propose Confidence-guided Refinement Reasoning (C2R), a novel training-free framework applicable to question-answering (QA) tasks across text, image, and video domains. C2R strategically constructs and refines sub-questions and their answers (sub-QAs), deriving a better confidence score for the target answer. C2R first curates a subset of sub-QAs to explore diverse reasoning paths, then compares the confidence scores of the resulting answer candidates to select the most reliable final answer. Since C2R relies solely on confidence scores derived from the model itself, it can be seamlessly integrated with various existing QA models, demonstrating consistent performance improvements across diverse models and benchmarks. Furthermore, we provide essential yet underexplored insights into how leveraging sub-QAs affects model behavior, specifically analyzing the impact of both the quantity and quality of sub-QAs on achieving robust and reliable reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.621320",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Confidence-guided Refinement Reasoning (C2R)\"的新型无需训练框架，用于改进问答任务的推理能力。从本质上看，该论文完全符合研究目标，因为它专注于改进LLM的基础推理能力，特别是通过构建和细化子问题来探索多样化的推理路径，类似于思维链(CoT)的推理方法，但更强调置信度评估。论文的核心是增强模型的多步推理和问题解决能力，这直接符合\"改进LLM的基础能力、增强其逻辑、多步推理等通用能力\"的要求。在正面指标方面，论文明确关注\"reasoning\"和\"problem-solving\"，虽然未明确提到LLMs，但其框架可与各种QA模型集成，这些模型很可能包括LLM。虽然论文提到其框架适用于文本、图像和视频领域，但多模态只是应用场景，而不是核心焦点；论文的核心是提出一种通用的推理框架，而不是专注于多模态技术本身。此外，论文关注提高推理的可靠性和稳健性，这与提高模型内在可靠性和推理质量相关，进一步支持其符合研究目标。因此，这篇论文应该被保留。"
    },
    {
        "index": "#53",
        "title": "Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures",
        "link": "/arxiv/2509.20577",
        "arxiv_id": "2509.20577",
        "authors": "Sampurna Roy, Ayan Sar, Anurag Kaushish, Kanav Gupta, Tanupriya Choudhury, Abhijit Kumar",
        "summary": "Contemporary transformer architectures apply identical processing depth to all inputs, creating inefficiencies and limiting reasoning quality. Simple factual queries are subjected to the same multilayered computation as complex logical problems, wasting resources while constraining deep inference. To overcome this, we came up with a concept of Dynamic Reasoning Chains through Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends the Mixture of Experts paradigm from width-based to depth specialised computation. DS-MoE introduces expert modules optimised for distinct reasoning depths, shallow pattern recognition, compositional reasoning, logical inference, memory integration, and meta-cognitive supervision. A learned routing network dynamically assembles custom reasoning chains, activating only the necessary experts to match input complexity. The dataset on which we trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse domains such as scientific papers, legal texts, programming code, and web content, enabling systematic assessment across reasoning depths. Experimental results demonstrate that DS-MoE achieves up to 16 per cent computational savings and 35 per cent faster inference compared to uniform-depth transformers, while delivering 2.8 per cent higher accuracy on complex multi-step reasoning benchmarks. Furthermore, routing decisions yield interpretable reasoning chains, enhancing transparency and scalability. These findings establish DS-MoE as a significant advancement in adaptive neural architectures, demonstrating that depth-specialised modular processing can simultaneously improve efficiency, reasoning quality, and interpretability in large-scale language models.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.631633",
        "filter_reason": "这篇论文完全符合研究目标，核心贡献是提出一种名为\"Depth Specialised Mixture of Experts (DS-MoE)\"的新框架，用于改进大语言模型的推理能力。具体分析如下： 第一步核心判断：论文本质上是改进LLM的基础能力和推理能力，而非将其作为工具应用于特定领域。DS-MoE框架通过动态构建推理链，根据输入复杂性激活必要的专家模块，明显属于增强模型通用推理能力的研究，特别是多步推理能力。 第二步正面指标：论文包含多个相关主题。核心概念上，它专注于Transformer架构（LLM的基础架构）；能力方向上，明确强调\"reasoning chains\"、\"logical inference\"和\"complex multi-step reasoning benchmarks\"，直接针对推理能力的提升。 第三步排除标准：论文不符合任何排除标准。虽然训练数据包含多个领域内容，但研究本身不是针对特定应用领域，而是提出通用架构改进方法；不涉及多模态与视觉内容；也不主要关注模型可靠性问题。 第四步特殊情况处理：论文提到\"routing decisions yield interpretable reasoning chains, enhancing transparency\"，这表明它通过提供可解释的推理链增强了模型的可解释性，从而提升推理质量，符合保留标准。 综上所述，这篇论文通过创新的架构设计，显著提高了LLM的推理效率和质量，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#56",
        "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning",
        "link": "/arxiv/2509.20502",
        "arxiv_id": "2509.20502",
        "authors": "Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang",
        "summary": "Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\\%. Code is available at https://github.com/xwang97/MARS.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.633219",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文本质上是提出MARS（Multi-Agent Review System）这一新的多智能体协作框架，旨在提高LLM的推理能力，这属于\"改进LLM的基础能力\"和\"智能体协作框架\"的范畴，符合保留标准。其次，论文包含多个正面指标：明确讨论大语言模型(LLMs)、专注于推理能力(reasoning)、提出了基于LLM的多智能体系统(multi-agent systems)。第三，论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。最后，在特殊和模糊情况处理上，MARS是一种通用的智能体协作框架，受评审过程启发，用于增强LLM的通用推理能力，而非针对特定领域的应用。论文的核心贡献是提出了一种更高效的多智能体协作方法，在保持推理质量的同时，显著减少了计算开销，这直接服务于提升LLM通用推理能力的研究目标。"
    },
    {
        "index": "#76",
        "title": "ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning",
        "link": "/arxiv/2509.21070",
        "arxiv_id": "2509.21070",
        "authors": "Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu",
        "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between \"Thinking\" and \"NoThinking\" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.654227",
        "filter_reason": "根据筛选标准，我对这篇论文进行了全面分析： 第一步：核心判断 这篇论文的核心是提出ScaleDiff管道，通过大规模生成困难数学问题来训练大语言模型，从而提高其数学推理能力。论文关注的是增强LLM的基础推理能力，特别是数学推理这一通用能力的重要组成部分。论文提出了新的训练范式（通过生成困难问题进行训练），并展示了这种方法能显著提高模型性能，符合\"改进LLM的基础能力、提出新的训练范式、增强其推理能力\"的保留标准。 第二步：正面指标 - 核心概念：论文明确提到了\"Large Reasoning Models (LRMs)\"，以及Qwen2.5-Math-7B-Instruct等具体模型，符合LLMs核心概念。 - 能力方向：论文直接针对\"mathematical reasoning\"（数学推理），这是推理能力的重要方面，符合reasoning能力方向。 虽然论文未涉及强化学习、智能体系统等其他正面指标，但已满足最关键的两项。 第三步：排除标准 论文不涉及多模态与视觉、特定应用领域（数学推理在此被视为通用能力而非特定应用领域）或模型可靠性（应用层面）等排除标准中的任何内容。 第四步：特殊和模糊情况 论文不涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等模糊情况。 综合判断，这篇论文的核心贡献是提出了一种通过大规模生成困难数学问题来提升LLM推理能力的新方法，直接针对\"大语言模型通用推理能力\"的研究目标，因此符合筛选要求。"
    },
    {
        "index": "#73",
        "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns",
        "link": "/arxiv/2509.21124",
        "arxiv_id": "2509.21124",
        "authors": "Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai",
        "summary": "Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model's reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.652544",
        "filter_reason": "这篇论文完全符合我的研究目标，核心原因如下： 1. **核心判断**：论文的本质是关于改进LLM的基础推理能力。它提出了一种新的训练范式，通过学习多样化的思维链模式来扩展基础模型的推理潜力。论文明确关注如何提高模型在数学推理方面的能力，这属于改进LLM基础能力的范畴，而不是将LLM作为工具应用于特定领域。 2. **正面指标**：论文包含多个关键正面指标： - 核心概念：论文明确研究foundation model（基础模型），通常指LLM - 能力方向：专注于reasoning（推理能力），特别是mathematical reasoning（数学推理） - 训练方法：涉及reinforcement learning（强化学习）作为提高推理能力的方法 - 新兴范式：提出了关于chains of thought patterns的新方法，这与思维链(CoT)相关的研究方向一致 3. **排除标准**：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域（如医疗、化学等），而是关注通用数学推理能力 - 不关注模型可靠性的应用层面问题（如水印、安全等） 4. **特殊和模糊情况**：论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊领域。 论文的核心贡献是定义了\"推理潜力\"的概念，并提出了从CoT序列中抽象原子推理模式，构建富含高价值推理模式的核心参考集的方法，以及双粒度算法来高效选择高价值CoT数据。这些都是直接针对提高LLM本身推理能力的研究，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。"
    },
    {
        "index": "#78",
        "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems",
        "link": "/arxiv/2509.21054",
        "arxiv_id": "2509.21054",
        "authors": "Haodong Zhao, Jidong Li, Zhaomin Wu, Tianjie Ju, Zhuosheng Zhang, Bingsheng He, Gongshen Liu",
        "summary": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to solve complex problems, necessitates a deep understanding of the persuasion dynamics that govern their interactions. This paper challenges the prevailing hypothesis that persuasive efficacy is primarily a function of model scale. We propose instead that these dynamics are fundamentally dictated by a model's underlying cognitive process, especially its capacity for explicit reasoning. Through a series of multi-agent persuasion experiments, we uncover a fundamental trade-off we term the Persuasion Duality. Our findings reveal that the reasoning process in LRMs exhibits significantly greater resistance to persuasion, maintaining their initial beliefs more robustly. Conversely, making this reasoning process transparent by sharing the \"thinking content\" dramatically increases their ability to persuade others. We further consider more complex transmission persuasion situations and reveal complex dynamics of influence propagation and decay within multi-hop persuasion between multiple agent networks. This research provides systematic evidence linking a model's internal processing architecture to its external persuasive behavior, offering a novel explanation for the susceptibility of advanced models and highlighting critical implications for the safety, robustness, and design of future MAS.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.655274",
        "filter_reason": "这篇论文完全符合我的研究目标，因为它专注于研究大语言模型(LLMs)和大型推理模型(LRMs)的推理能力如何影响其在多智能体系统中的行为表现。 首先，从核心判断来看，论文的本质是研究模型的基础推理能力，特别是其\"显式推理能力\"如何影响说服动态。这直接符合\"改进LLM的基础能力、增强其逻辑、多步推理等通用能力\"的研究范围，而不是将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标：核心概念上明确研究LLMs和LRMs；能力方向上聚焦于推理过程，特别是显式推理能力；新兴范式上研究多智能体系统(MAS)的协作与交互。 第三，论文不符合任何排除标准。它不涉及多模态与视觉，不聚焦于特定应用领域，也不主要研究模型可靠性的应用层面问题。 在特殊和模糊情况处理上，该论文研究的是通用的多智能体系统框架，探索模型推理过程与说服效果的关系，而不是将智能体应用于特定领域。虽然论文提到了安全性和鲁棒性，但这是作为研究意义的一部分，而不是主要焦点。 核心贡献在于揭示了\"说服二元性\"(Persuasion Duality)这一基本权衡关系：推理过程使模型对说服更具抵抗力，同时通过使推理过程透明化又能显著提高模型说服他人的能力。这直接关联到模型的内部处理架构与外部行为表现，为提升大语言模型的通用推理能力提供了新的见解。"
    },
    {
        "index": "#80",
        "title": "DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?",
        "link": "/arxiv/2509.21016",
        "arxiv_id": "2509.21016",
        "authors": "Yiyou Sun, Yuhan Cao, Pohao Huang, Haoyue Bai, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song",
        "summary": "It remains an open question whether LLMs can acquire or generalize genuinely new reasoning strategies, beyond the sharpened skills encoded in their parameters during pre-training or post-training. To attempt to answer this debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding, a controlled benchmark of synthetic coding problem families designed to probe two fundamental aspects: learnability -- can LLMs, through reinforcement learning (RL), solve problem families where pretrained models exhibit failure with large enough attempts (pass@K=0)? --and transferrability -- if learnability happens, can such skills transfer systematically to out-of-distribution (OOD) test sets? Unlike prior public coding datasets, DELTA isolates reasoning skills through templated problem generators and introduces fully OOD problem families that demand novel strategies rather than tool invocation or memorized patterns. Our experiments reveal a striking grokking phase transition: after an extended period with near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To enable learnability on previously unsolvable problem families, we explore key training ingredients such as staged warm-up with dense rewards, experience replay, curriculum training, and verification-in-the-loop. Beyond learnability, we use DELTA to evaluate transferability or generalization along exploratory, compositional, and transformative axes, as well as cross-family transfer. Results show solid gains within families and for recomposed skills, but persistent weaknesses in transformative cases. DELTA thus offers a clean testbed for probing the limits of RL-driven reasoning and for understanding how models can move beyond existing priors to acquire new algorithmic skills.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.661440",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文本质是研究LLMs如何通过强化学习(RL)获取和泛化全新的推理策略，而非仅依赖预训练或后训练期间编码的技能。论文提出了DELTA-Code基准来探究LLMs能否通过RL解决预训练模型无法解决的问题，以及这种能力能否迁移到分布外测试集。这明显属于改进LLM基础能力、提出新训练范式、增强其逻辑推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标，包括核心概念\"Large language models, LLMs\"，能力方向\"reasoning\"(明确讨论\"new reasoning strategies\"和\"RL-driven reasoning\")，以及训练方法\"reinforcement learning (RL)\"(论文的核心方法)。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域(虽然使用编程算法作为测试平台，但目的是研究通用推理能力而非解决编程领域问题)或模型可靠性(应用层面)。 第四步特殊和模糊情况：虽然论文使用编程算法作为测试平台，但核心目标是理解LLMs如何通过强化学习超越现有先验知识获取新算法技能，而非解决编程领域的具体问题。这属于研究LLM通用推理能力的工作。 论文的核心贡献是提出了DELTA-Code基准，用于评估LLMs在算法编程问题上的学习能力和迁移能力，揭示了RL训练模型经历的\"grokking\"相变现象，并探索了使模型能够学习以前无法解决问题的关键训练要素。这些研究直接关注如何提升LLM的通用推理能力，符合研究目标。"
    },
    {
        "index": "#81",
        "title": "Mechanism of Task-oriented Information Removal in In-context Learning",
        "link": "/arxiv/2509.21012",
        "arxiv_id": "2509.21012",
        "authors": "Hakaze Cho, Haolin Yang, Gouki Minegishi, Naoya Inoue",
        "summary": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.661955",
        "filter_reason": "这篇论文符合我的研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的本质是研究大语言模型的上下文学习(ICL)内在机制，特别是从\"信息移除\"的新角度解释ICL如何工作。论文不是将LLM作为工具应用到特定领域，而是深入研究LLM本身的基础工作机制——上下文学习能力，这属于LLM的核心推理能力之一。论文揭示了ICL通过选择性移除冗余信息来提高模型在特定任务上表现的机制，这直接关系到提升LLM的通用推理能力。 第二步：正面指标 - 核心概念：论文明确研究现代语言模型(LMs)的上下文学习机制，符合\"Large language models, LLMs\"这一核心概念。 - 能力方向：虽然论文没有直接提到reasoning、planning等词汇，但上下文学习(ICL)本身就是LLM进行推理和问题解决的关键机制。论文研究的信息移除机制直接关系到模型如何聚焦于特定任务并进行有效推理，因此与\"reasoning\"和\"problem-solving\"能力方向高度相关。 第三步：排除标准 论文不涉及任何排除标准中的领域： - 没有研究多模态与视觉相关内容 - 没有将LLM应用到任何特定领域（如医疗、化学等） - 没有关注模型可靠性方面的水印、安全等问题 第四步：特殊和模糊情况 论文属于增强模型内在可解释性的研究。通过揭示ICL的信息移除机制和识别关键的\"去噪头\"，论文提高了我们对LLM如何进行推理的理解，这属于增强模型内在可解释性的研究，应该保留。 核心贡献：论文揭示了上下文学习的一个关键机制——面向任务的信息移除过程，解释了LLM如何通过选择性移除冗余信息来提高在特定任务上的表现。这一发现不仅增进了我们对LLM推理机制的理解，还可能为改进LLM的推理能力提供新思路。 因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标，应该被保留。"
    },
    {
        "index": "#89",
        "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning",
        "link": "/arxiv/2509.20712",
        "arxiv_id": "2509.20712",
        "authors": "Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou",
        "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose \\textbf{C}ontrolling \\textbf{E}ntropy via \\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.671117",
        "filter_reason": "这篇论文的核心贡献是提出CE-GPPO算法，一种改进的强化学习方法，用于优化大语言模型处理复杂推理任务的能力。论文专注于改进LLM的基础训练机制，特别是通过更好地管理策略熵来平衡探索和利用，从而提升模型的推理性能。这完全符合\"改进LLM基础能力、提出新训练范式、增强其推理能力\"的核心标准。论文明确关注LLMs的数学推理能力，并通过强化学习方法进行优化，这些都是正面指标。论文不涉及任何排除标准中的领域，如多模态、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#84",
        "title": "On Theoretical Interpretations of Concept-Based In-Context Learning",
        "link": "/arxiv/2509.20882",
        "arxiv_id": "2509.20882",
        "authors": "Huaze Tang, Tianren Peng, Shao-lun Huang",
        "summary": "In-Context Learning (ICL) has emerged as an important new paradigm in natural language processing and large language model (LLM) applications. However, the theoretical understanding of the ICL mechanism remains limited. This paper aims to investigate this issue by studying a particular ICL approach, called concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs well for predicting query labels in prompts with only a few demonstrations. In addition, the proposed theory quantifies the knowledge that can be leveraged by the LLMs to the prompt tasks, and leads to a similarity measure between the prompt demonstrations and the query input, which provides important insights and guidance for model pre-training and prompt engineering in ICL. Moreover, the impact of the prompt demonstration size and the dimension of the LLM embeddings in ICL are also explored based on the proposed theory. Finally, several real-data experiments are conducted to validate the practical usefulness of CB-ICL and the corresponding theory.",
        "subjects": "Information Theory, Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T18:47:31.663417",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。首先，从核心判断来看，这篇论文的本质是研究大语言模型(LLM)的上下文学习(ICL)机制的理论解释，特别是基于概念的上下文学习(CB-ICL)。论文提出了理论分析来解释CB-ICL为何以及何时能够在少量示例的情况下表现良好，这属于改进LLM基础能力的理论研究范畴，而非将LLM作为工具应用到特定领域。 从正面指标看，论文明确涉及大语言模型(LLMs)这一核心概念，虽然不直接聚焦于推理、规划或问题解决，但上下文学习(ICL)本身与LLM的通用推理能力密切相关，因为理解上下文中的示例并应用它们解决新问题需要一定程度的推理能力。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性等应用层面的研究。 虽然这篇论文的主要贡献是理论性的，而非直接提出改进LLM推理能力的新方法，但它通过深入理解ICL机制，为改进LLM的基础能力（包括推理能力）提供了理论基础和指导。论文提出的理论量化了LLM可以利用的知识，并提出了相似性度量，这些都为模型预训练和提示工程提供了重要见解，有助于提升LLM的通用推理能力。 因此，这篇论文符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#5",
        "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA",
        "link": "/arxiv/2509.21199",
        "arxiv_id": "2509.21199",
        "authors": "Kaiyang Wan, Lang Gao, Honglin Mu, Preslav Nakov, Yuxia Wang, Xiuying Chen",
        "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:31.958503",
        "filter_reason": "这篇论文的核心是研究大语言模型在多步推理任务中的理论限制和解决方案，直接关注LLM的通用推理能力。论文建立了LLM单次推理的准确率上限理论（Fano-style bound），解释了为什么单次推理在复杂任务中会失效，并提出了InfoQA框架来解决这个问题。这属于改进LLM基础能力和推理能力的研究，符合我们的筛选标准。论文不是将LLM作为工具应用到特定领域，而是研究LLM本身的推理机制，没有涉及多模态、特定应用领域或模型可靠性等排除标准的内容。论文关注的是多跳问答中的推理能力，属于逻辑推理和问题解决的范畴，虽然使用了多跳问答作为测试任务，但这不是一个特定应用领域，而是评估推理能力的常见任务。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#9",
        "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
        "link": "/arxiv/2509.21128",
        "arxiv_id": "2509.21128",
        "authors": "Kohsei Matsutani, Shota Takashiro, Gouki Minegishi, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo",
        "summary": "Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:31.960354",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文本质上是研究如何通过强化学习(RL)和监督微调(SFT)这两种训练方法来提高大语言模型的推理能力，属于改进LLM基础能力的研究，而非将LLM作为工具应用到特定领域。论文明确研究Large language models (LLMs)的reasoning能力，特别是在数学领域的推理，并探讨了reinforcement learning和supervised fine-tuning这两种训练方法对推理能力的影响机制。论文不符合任何排除标准，它虽然以数学领域为实验场景，但数学推理通常被视为通用推理能力的重要组成部分，论文的核心贡献在于揭示了不同训练方法如何塑造和影响LLM的推理过程，而非解决数学领域的特定问题。论文提出了新的分析框架来量化推理路径，从轨迹级别和步骤级别两个粒度研究推理过程，发现RL压缩不正确推理轨迹而SFT扩展正确推理轨迹的互补效应，这些发现对于提升LLM的通用推理能力具有重要指导意义。"
    },
    {
        "index": "#4",
        "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns",
        "link": "/arxiv/2509.21224",
        "arxiv_id": "2509.21224",
        "authors": "Stefan Szeider",
        "summary": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:31.958039",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究LLM智能体在没有外部任务时的自发行为模式，特别是元认知模式。论文提出了\"持续推理和行动框架\"，使用持久记忆和自我反馈使智能体能够持续自主运行，这明显属于改进LLM基础能力和增强其通用推理能力的研究，而不是将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标：核心概念方面明确研究\"large language model (LLM) agents\"；能力方向方面关注推理能力，特别是自主推理和元认知；新兴范式方面研究LLM-based agents的自主行为模式。 第三，论文不符合任何排除标准：没有涉及多模态与视觉内容，没有聚焦于特定应用领域，也没有主要关注模型可靠性问题。 最后，在特殊和模糊情况处理上，论文研究的智能体框架是通用的，旨在增强LLM的自主推理和元认知能力，而不是应用在特定领域，因此应该保留。 论文的核心贡献是首次系统地记录了未经提示的LLM智能体行为，特别是它们自发表现出的元认知模式，这直接关系到提升LLM的通用推理能力和自主认知能力，完全符合研究目标。"
    },
    {
        "index": "#14",
        "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
        "link": "/arxiv/2509.21043",
        "arxiv_id": "2509.21043",
        "authors": "Samuel Schapiro, Sumuk Shashidhar, Alexi Gladstone, Jonah Black, Royce Moon, Dilek Hakkani-Tur, Lav R. Varshney",
        "summary": "Artificial intelligence (AI) systems, and large language models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Though in many ways similar to forms of compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, marking a new frontier in generalization abilities.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:31.968115",
        "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)的组合创造力(Combinatorial Creativity)，将其视为泛化能力的新前沿。论文提出了理论框架和算法任务来评估LLM输出的新颖性和实用性，并研究了模型规模、深度和宽度对创造力的影响。创造力可以被视为一种高级的推理和问题解决能力，与通用推理能力密切相关。论文不是将LLM作为工具应用于特定领域，而是专注于理解和改进LLM本身的基础能力。论文没有涉及排除标准中的任何领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。论文的核心贡献是揭示了LLMs创造力的扩展行为、发现最优模型架构参数以及解释\"构想-执行差距\"背后的新颖性-实用性权衡，这些都是对LLM基础能力的深入研究，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#12",
        "title": "Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution",
        "link": "/arxiv/2509.21072",
        "arxiv_id": "2509.21072",
        "authors": "Kaiwen He, Zhiwei Wang, Chenyi Zhuang, Jinjie Gu",
        "summary": "Recent years, multimodal models have made remarkable strides and pave the way for intelligent browser use agents. However, when solving tasks on real world webpages in multi-turn, long-horizon trajectories, current agents still suffer from disordered action sequencing and excessive trial and error during execution. This paper introduces Recon-Act, a self-evolving multi-agent framework grounded in Reconnaissance-Action behavioral paradigm. The system comprises a Reconnaissance Team and an Action Team: the former conducts comparative analysis and tool generation, while the latter handles intent decomposition, tool orchestration, and execution. By contrasting the erroneous trajectories with successful ones, the Reconnaissance Team infers remedies, and abstracts them into a unified notion of generalized tools, either expressed as hints or as rule-based codes, and register to the tool archive in real time. The Action Team reinference the process empowered with these targeting tools, thus establishing a closed-loop training pipeline of data-tools-action-feedback. Following the 6 level implementation roadmap proposed in this work, we have currently reached Level 3 (with limited human-in-the-loop intervention). Leveraging generalized tools obtained through reconnaissance, Recon-Act substantially improves adaptability to unseen websites and solvability on long-horizon tasks, and achieves state-of-the-art performance on the challenging VisualWebArena dataset.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:31.967081",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Recon-Act\"的自进化多智能体框架，通过侦察-行动的循环过程，不断生成和优化工具，以提高在复杂网页环境中的任务执行能力。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，这篇论文不是将LLM作为工具应用到特定领域，而是提出了一种新的多智能体协作框架和工具使用方法，增强了模型在复杂环境中的规划、推理和执行能力。论文中提到的\"intent decomposition\"（意图分解）、通过对比错误轨迹和成功轨迹来推断补救措施，以及建立\"数据-工具-行动-反馈的闭环训练管道\"，都是直接提升LLM通用推理能力的方法论创新。 其次，论文包含多个正面指标：在能力方向上涉及planning、reasoning和problem-solving；在训练方法上明确提出了\"Self-Evolving\"（自进化）机制；在新兴范式上提出了\"multi-agent framework\"和\"tool generation/orchestration\"，这些都是增强LLM通用推理能力的关键要素。 第三，论文不主要聚焦于任何排除领域。虽然论文在VisualWebArena数据集上评估，可能涉及视觉信息，但其核心是多智能体协作和工具使用，而不是多模态处理本身。论文关注的是通用的浏览器使用能力，适用于\"unseen websites\"，表明其具有通用性，不局限于特定应用领域。 最后，论文明确提出了一个通用的智能体协作框架，通过工具使用来增强LLM的通用问题解决能力，符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的保留条件。 综上所述，这篇论文致力于提高大语言模型本身的通用推理能力，特别是通过多智能体协作和工具使用来增强其在复杂环境中的规划和执行能力，因此符合研究目标。"
    },
    {
        "index": "#24",
        "title": "Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning",
        "link": "/arxiv/2509.20744",
        "arxiv_id": "2509.20744",
        "authors": "Qihang Ai, Haiyun Jiang",
        "summary": "We study reasoning tasks through a framework that integrates auto-regressive (AR) and non-autoregressive (NAR) language models. AR models, which generate text sequentially, excel at producing coherent outputs but often suffer from slow inference, particularly in reasoning-intensive domains such as mathematics and code, where lengthy chains of thought are required. In contrast, NAR models, such as discrete diffusion models, allow parallel generation and offer substantial speedups, though typically at the cost of reduced output quality. To address these limitations, we introduce a new paradigm in which an NAR model efficiently produces intermediate reasoning traces, which subsequently guide an AR model to deliver precise final answers. Experiments demonstrate that our approach yields significant 26% improvements over strong baselines while substantially reducing inference cost.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:31.977964",
        "filter_reason": "这篇论文的核心贡献是提出了一种新的推理范式，通过整合自回归(AR)和非自回归(NAR)语言模型来增强大语言模型的推理能力和效率。论文明确关注\"reasoning tasks\"，特别提到了\"mathematics and code\"等推理密集型领域，这正是通用推理能力的重要组成部分。论文不是将LLM作为工具应用于特定领域，而是改进LLM本身的推理机制，通过让NAR模型高效生成中间推理轨迹，然后指导AR模型提供精确的最终答案，从而在保持推理质量的同时显著提高效率。这种方法直接针对LLM的通用推理能力进行改进，符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。论文不涉及多模态、特定应用领域或模型可靠性等排除标准，因此应该被保留。"
    },
    {
        "index": "#29",
        "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection",
        "link": "/arxiv/2509.20562",
        "arxiv_id": "2509.20562",
        "authors": "Yubin Ge, Salvatore Romeo, Jason Cai, Monica Sunkara, Yi Zhang",
        "summary": "Despite the rapid advancements in LLM agents, they still face the challenge of generating meaningful reflections due to inadequate error analysis and a reliance on rare successful trajectories, especially in complex tasks. In this work, we propose SAMULE, a new framework for self-learning agents powered by a retrospective language model that is trained based on Multi-Level Reflection Synthesis. It first synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights based on same typed errors from diverse task failures. Then we fine-tune a language model serving as the retrospective model to generate reflections during inference. We further extend our framework to interactive settings through a foresight-based reflection mechanism, enabling agents to proactively reflect and adapt during user interactions by comparing predicted and actual responses. Extensive experiments on three challenging benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our approach significantly outperforms reflection-based baselines. Our results highlight the critical role of well-designed reflection synthesis and failure-centric learning in building self-improving LLM agents.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:31.980427",
        "filter_reason": "这篇论文的核心贡献是提出SAMULE框架，一种通过多级反思合成来增强LLM智能体自我学习和推理能力的新方法。论文专注于改进LLM的基础能力，特别是通过三个互补的反思层次（单轨迹学习、任务内学习和任务间学习）来提升智能体的错误分析和问题解决能力。这种方法论研究直接关注提升LLM的通用推理能力，而非将LLM作为工具应用于特定领域。论文使用了TravelPlanner、NATURAL PLAN等规划相关的基准测试来评估方法，但其核心是提出一种通用的反思框架，而非针对特定应用。论文明确涉及LLM智能体、自我改进机制和问题解决能力等正面指标，同时不涉及多模态、特定应用领域或模型可靠性等排除标准。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#41",
        "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL",
        "link": "/arxiv/2509.21282",
        "arxiv_id": "2509.21282",
        "authors": "Madeleine Dwyer, Adam Sobey, Adriane Chapman",
        "summary": "Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information and introduces gradient discontinuities. We propose Probability Smoothing Policy Optimisation (PSPO), which smooths the current policy's probabilities toward the old (behaviour) policy before computing the importance ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient signal, while interpolation toward the old policy creates a soft trust region that discourages large, destabilising updates, with formal guarantees. We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO (single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar performance but improves the reasoning leading to clearer and more concise responses which are more logical. Compared to clipped GRPO, GR-PSPO substantially improves performance both the 0.5B and 1.5B models, with a boost of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:31.991189",
        "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是提出一种新的强化学习方法(PSPO)来改进LLM的训练过程，而非将LLM作为工具应用到特定领域。论文专注于通过改进训练方法来提升模型的基础能力，这符合我的核心目标。 其次，论文包含了多个重要的正面指标：(1)核心概念上明确研究LLM，在Qwen2.5模型上进行实验；(2)能力方向上关注推理能力，特别是在GSM8K数学推理数据集上评估，并明确提到\"improves the reasoning leading to clearer and more concise responses which are more logical\"；(3)训练方法上提出了一种新的强化学习技术(PSPO)，用于替代传统的ratio clipping方法。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然使用了数学推理数据集进行评估，但这只是评估通用推理能力的常见基准，而非特定应用领域的研究。 论文的核心贡献是提出了一种概率平滑策略优化方法(PSPO)，通过创建软信任区域来改进LLM的强化学习训练过程，从而提升模型的推理能力和性能。这种方法直接针对LLM的通用推理能力进行改进，完全符合我的研究目标。"
    },
    {
        "index": "#35",
        "title": "LATTS: Locally Adaptive Test-Time Scaling",
        "link": "/arxiv/2509.20368",
        "arxiv_id": "2509.20368",
        "authors": "Theo Uscidda, Matthew Trager, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto",
        "summary": "One common strategy for improving the performance of Large Language Models (LLMs) on downstream tasks involves using a \\emph{verifier model} to either select the best answer from a pool of candidates or to steer the auto-regressive generation process towards better outputs. This class of methods typically results in improved accuracy at the cost of increased computation at test-time, a paradigm known as \\emph{test-time scaling}. However, most existing approaches increase computation uniformly across all samples and generation steps, without considering the complexity of individual instances, leading to inefficient resource use. We address this limitation by proposing an approach, called \\emph{Locally Adaptive Test-Time Scaling (LATTS)}, that allocates variable compute across generation steps. Specifically, at each generation step, LATTS employs a verifier-based acceptance criterion to decide whether to resample, backtrack, restart, or stop the generation process. This criterion effectively adjusts the per-step computational effort based on a precise notion of \\emph{local difficulty} derived from the verifier model. Empirical results show that LATTS achieves significantly superior accuracy--compute tradeoffs compared to standard verifier-based methods.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:31.988333",
        "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，论文的本质是提出一种名为\"Locally Adaptive Test-Time Scaling (LATTS)\"的方法，用于改进大语言模型在推理过程中的计算资源分配。这属于改进LLM基础能力的研究，特别是优化其推理性能，而不是将LLM作为工具应用到特定领域。 论文明确包含正面指标中的核心概念\"Large language models, LLMs\"，并且关注提高LLM在下游任务上的性能，这涉及到推理和问题解决能力。虽然论文没有提到强化学习等训练方法或新兴范式，但其核心贡献是关于推理过程的优化。 论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。相反，它提出的是一种通用的测试时计算缩放方法，适用于各种下游任务。 特别值得注意的是，LATTS方法通过在每个生成步骤使用基于验证器的接受标准，动态决定是否重新采样、回溯、重启或停止生成过程，这实际上是一种增强LLM推理能力的通用方法论。它通过更智能地分配计算资源来提高模型的推理质量和效率，这与提高大语言模型通用推理能力的研究目标高度一致。"
    },
    {
        "index": "#52",
        "title": "Tree Search for LLM Agent Reinforcement Learning",
        "link": "/arxiv/2509.21240",
        "arxiv_id": "2509.21240",
        "authors": "Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu",
        "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:32.007144",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。论文的核心贡献是提出了一种基于树搜索的强化学习方法（Tree-GRPO），用于增强大语言模型（LLM）的代理能力和推理能力。该方法针对长期和多轮代理任务中的稀疏监督问题，通过树搜索采样提高LLM在固定预算内的推理效率，并利用树结构轨迹构建逐步过程监督信号。论文涉及多个正面指标，包括LLM核心概念、推理和问题解决能力方向、强化学习训练方法以及LLM智能体新兴范式。同时，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。论文提出的智能体方法是通用的，旨在提升LLM的通用推理能力，而非针对特定领域的应用，因此完全符合研究目标。"
    },
    {
        "index": "#60",
        "title": "GRPO is Secretly a Process Reward Model",
        "link": "/arxiv/2509.21154",
        "arxiv_id": "2509.21154",
        "authors": "Michael Sullivan",
        "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and show that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:32.011216",
        "filter_reason": "这篇论文的核心贡献是证明GRPO强化学习算法实际上隐含地构建了一个过程奖励模型(PRM)，并基于这一发现提出了改进算法λ-GRPO，从而提高LLM在推理任务上的性能。从本质上看，论文关注的是改进LLM的训练方法，特别是通过强化学习优化来增强模型的推理能力，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文明确涉及LLMs、reasoning和reinforcement learning等正面指标，且没有被排除标准所涵盖（如多模态、特定应用领域或模型可靠性应用层面）。论文不是将LLM作为工具应用于特定领域，而是致力于提升模型本身的基础推理能力，因此符合研究范围。"
    },
    {
        "index": "#68",
        "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
        "link": "/arxiv/2509.21091",
        "arxiv_id": "2509.21091",
        "authors": "Junpei Komiyama, Daisuke Oba, Masafumi Oyamada",
        "summary": "We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:32.020214",
        "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围。根据筛选标准，我的判断过程如下： 第一步：核心判断——论文的本质是改进LLM的基础推理能力。论文提出了Best-of-∞方法，通过多数投票和自适应生成方案来提高LLM在测试时的推理性能。这是一种通用的方法论研究，旨在增强LLM的基础推理能力，而不是将LLM作为工具应用于特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：关注推理能力(reasoning)，通过多数投票机制提高模型的问题解决能力 论文虽然未涉及强化学习或新兴智能体范式，但其核心关注点与通用推理能力直接相关。 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域（如医疗、化学等） - 不主要关注模型可靠性问题（如水印、安全等） 第四步：特殊和模糊情况——论文不涉及需要特殊判断的情况。它提出的是一种通用的测试时间计算分配方法，用于提高LLM的推理能力，而非针对特定领域的应用。 核心贡献：论文提出了一种自适应生成方案，通过基于答案一致性选择N值来有效分配推理时间计算，并将框架扩展到多个LLMs的加权集成，从而提高模型的通用推理能力。这种方法论研究直接服务于提升LLM的通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#76",
        "title": "Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs",
        "link": "/arxiv/2509.21044",
        "arxiv_id": "2509.21044",
        "authors": "Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li",
        "summary": "Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:32.029483",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该保留。 首先，从核心判断来看，这篇论文的本质是研究强化学习(RL)微调如何影响大语言模型(LLM)的内部工作机制，特别是激活强度和多样性的变化。论文不是将LLM作为工具应用到特定领域，而是研究LLM本身的内部机制如何通过RL微调得到改善，这属于改进LLM基础能力的研究，符合保留标准。 其次，从正面指标分析： - 核心概念：论文明确研究Large language models (LLMs)，符合。 - 能力方向：虽然论文没有直接研究推理、规划或问题解决，但它研究了RL微调如何增强LLM的内部机制，并提到这种变化提高了模型的泛化能力，这与通用推理能力相关，部分符合。 - 训练方法：论文核心研究的就是强化学习(RL)微调，包括PPO、GRPO和DPO等方法，完全符合。 第三，论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，在特殊和模糊情况处理上，论文研究了LLM的内部机制，可以看作是对模型可解释性的一种探索，通过分析激活强度和多样性来解释为什么RL微调能提高模型性能，这属于增强模型内在可解释性的研究，应该保留。 论文的核心贡献是揭示了强化学习微调如何系统性地改变LLM的内部电路，特别是增强激活强度和多样性，从而提高模型的泛化能力。这有助于理解为什么RL微调能够提升LLM的通用能力，为改进LLM的推理能力提供了理论基础，符合研究目标。"
    },
    {
        "index": "#176",
        "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments",
        "link": "/arxiv/2509.20386",
        "arxiv_id": "2509.20386",
        "authors": "Nishant Gaurav, Adit Akarsh, Ankit Ranjan, Manoj Bajaj",
        "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef- ficiently operate with extensive Model Control Protocol (MCP) tool sets that exceed the contextual memory limitations of large language models. Our approach addresses the fundamental challenge of tool selection in environments containing hundreds or thousands of available tools, where loading all tools simultaneously is computationally infeasible. We propose and evaluate five distinct architectures that progressively refine the tool selection process, culminating in a search-and-load mechanism that achieves intelligent tool selection with minimal computational overhead. Our experimental results demonstrate that the proposed approach reduces tool loading by up to 50% while maintaining task completion accuracy, advancing the path towards truly general-purpose AI agents capable of dynamically adapting to diverse task environments.",
        "subjects": "Software Engineering, Artificial Intelligence, Information Retrieval",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T18:47:32.130587",
        "filter_reason": "这篇论文的核心贡献是提出Dynamic ReAct方法，用于提升ReAct智能体在大规模工具环境中的工具选择能力。根据第一步核心判断，论文本质上是关于改进LLM的基础能力，特别是增强其工具使用这一通用推理能力，而非将LLM应用到特定领域。论文符合多个正面指标：涉及LLM核心概念、关注推理和问题解决能力、探索LLM-based agents和tool use等新兴范式。同时，论文不符合任何排除标准，没有聚焦于多模态、特定应用领域或模型可靠性的应用层面。特别地，论文提出的是一种通用的工具选择方法，使LLM智能体能够高效处理大量工具，这直接增强了LLM的通用推理和问题解决能力，而非针对特定领域的应用。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#18",
        "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say",
        "link": "/arxiv/2509.21164",
        "arxiv_id": "2509.21164",
        "authors": "Jacob Fein-Ashley, Dhruv Parikh, Rajgopal Kannan, Viktor Prasanna",
        "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain (e.g., math, code, general reasoning), motivating systems that leverage complementary strengths across models. Prior multi-LLM approaches either (i) route a query to one or a few experts and generate independently, (ii) aggregate outputs from each model via costly multi-turn exchanges, or (iii) fuse weights into a single model-typically requiring architectural homogeneity. We introduce Mixture of Thoughts (MoT), a simple method for latent-level collaboration among heterogeneous experts under a global routing scheme. For each query, a lightweight router selects top-$K$ experts and designates a primary expert; uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active (selected) peers. Pre-trained experts remain frozen; only the router and the lightweight interaction layers are trained with a novel joint training objective that improves both the expert selection and inter-expert collaboration. Across five in-distribution (ID) and three out-of-distribution (OOD) benchmarks, MoT surpasses the current routing and aggregation-based state-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further, MoT significantly outperforms the best-performing single model. It achieves this with single-pass inference, runtime comparable to routing baselines, and none of the overheads of iterative aggregation. MoT offers a simple latent-space mechanism for combining heterogeneous LLMs, a practical step toward broader multi-LLM collaboration. Our code is publicly available at https://github.com/jacobfa/mot.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T18:47:32.065779",
        "filter_reason": "这篇论文符合我的研究目标，因为它核心关注的是提升大语言模型本身的通用推理能力。论文提出的Mixture of Thoughts (MoT)方法是一种创新的多LLM协作框架，通过在潜在层面整合不同专家模型的思想来增强推理能力，而非简单路由查询或聚合输出。从核心判断看，MoT属于改进LLM基础能力和提出新训练范式的研究，专注于提升模型在数学、代码和一般推理等通用能力上的表现。从正面指标看，论文明确涉及大语言模型(LLMs)核心概念，关注推理能力(reasoning)，并提出了多智能体系统(multi-agent systems)的新兴范式。论文不涉及任何排除标准中的领域，如多模态、特定应用或模型可靠性等。MoT作为一种通用的智能体协作框架，旨在增强LLM的通用问题解决能力，而非应用于特定领域，完全符合我的研究范围。"
    },
    {
        "index": "#83",
        "title": "Theoretical Bounds for Stable In-Context Learning",
        "link": "/arxiv/2509.20677",
        "arxiv_id": "2509.20677",
        "authors": "Tongxi Wang, Zhuoyang Xia",
        "summary": "In-context learning (ICL) is flexible but its reliability is highly sensitive to prompt length. This paper establishes a non-asymptotic lower bound that links the minimal number of demonstrations to ICL stability under fixed high-dimensional sub-Gaussian representations. The bound gives explicit sufficient conditions in terms of spectral properties of the covariance, providing a computable criterion for practice. Building on this analysis, we propose a two-stage observable estimator with a one-shot calibration that produces practitioner-ready prompt-length estimates without distributional priors. Experiments across diverse datasets, encoders, and generators show close alignment between the predicted thresholds and empirical knee-points, with the theory acting as a conservative but reliable upper bound; the calibrated variant further tightens this gap. These results connect spectral coverage to stable ICL, bridge theory and deployment, and improve the interpretability and reliability of large-scale prompting in realistic finite-sample regimes.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T18:47:32.103786",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究上下文学习(ICL)的稳定性问题，建立了理论界限来连接最小演示数量与ICL稳定性。ICL是大语言模型的核心基础能力之一，论文关注的是如何提高这种能力的稳定性和可靠性，而不是将LLM作为工具应用到特定领域。因此，根据第一步的判断标准，应该保留。 第二步：正面指标 论文虽然没有直接提到\"Large language models\"或\"LLMs\"，但上下文学习(ICL)是LLM的核心能力，与核心概念相关。论文关注ICL的稳定性，这与推理能力有一定关联，因为稳定的上下文学习是有效推理的基础。虽然论文没有涉及强化学习、智能体系统等新兴范式，但ICL本身是LLM通用能力的重要组成部分。 第三步：排除标准 论文不符合任何排除标准。它不涉及多模态与视觉领域，不关注特定应用领域（如医疗、化学等），也不主要聚焦于应用层面的模型可靠性问题（如水印、安全等）。 第四步：特殊和模糊情况 这篇论文不涉及智能体/工具使用，也不直接讨论幻觉/可解释性/安全问题，因此不需要应用特殊情况的判断标准。 核心贡献与判断依据 论文的核心贡献是建立了ICL稳定性的理论界限，并提出了估计提示长度的方法。这属于改进LLM基础能力的研究，因为ICL是LLM展示推理能力的关键机制。通过提高ICL的稳定性，论文间接促进了LLM在推理任务中的表现，这与\"提高大语言模型的通用推理能力\"的研究目标相符。虽然论文没有提出新的训练范式或直接增强推理能力的方法，但它提供了关于如何更有效地利用LLM的ICL能力的理论指导，这对于理解和提升LLM的通用推理能力具有重要价值。"
    },
    {
        "index": "#88",
        "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning",
        "link": "/arxiv/2509.20616",
        "arxiv_id": "2509.20616",
        "authors": "Hanjiang Hu, Changliu Liu, Na Li, Yebin Wang",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in higher multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks with over 30 steps. We also theoretically and empirically validate the strong cross-task generalizability that the models trained on complex tasks can lead to the successful completion of all simpler subtasks.",
        "subjects": "Machine Learning, Systems and Control",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T18:47:32.106168",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式（单轮强化学习），专门用于增强LLM的多轮任务规划和推理能力，而不是将LLM作为工具应用到特定领域。其次，论文包含了所有正面指标：核心概念明确涉及Large Language Models (LLMs)；能力方向聚焦于task reasoning和task planning；训练方法采用了Group Relative Policy Optimization (GRPO)这一强化学习方法；新兴范式方面研究了LLM agents。第三，论文不涉及任何排除标准领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。最后，在特殊和模糊情况处理上，虽然论文涉及LLM agents，但它提出的是一种通用的智能体训练方法来增强LLM的通用任务规划和推理能力，而非应用于特定领域。论文的核心贡献是通过将多轮任务规划转化为单轮任务推理问题，提出了一种新的强化学习方法来提升LLM的通用推理和规划能力，这与研究目标高度一致。"
    }
]