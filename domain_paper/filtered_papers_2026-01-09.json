[
    {
        "index": "#2",
        "title": "From Idea to Co-Creation: A Planner-Actor-Critic Framework for Agent Augmented 3D Modeling",
        "link": "/arxiv/2601.05016",
        "arxiv_id": "2601.05016",
        "authors": "Jin Gao, Saichandu Juluri",
        "summary": "We present a framework that extends the Actor-Critic architecture to creative 3D modeling through multi-agent self-reflection and human-in-the-loop supervision. While existing approaches rely on single-prompt agents that directly execute modeling commands via tools like Blender MCP, our approach introduces a Planner-Actor-Critic architecture. In this design, the Planner coordinates modeling steps, the Actor executes them, and the Critic provides iterative feedback, while human users act as supervisors and advisors throughout the process. Through systematic comparison between single-prompt modeling and our reflective multi-agent approach, we demonstrate improvements in geometric accuracy, aesthetic quality, and task completion rates across diverse 3D modeling scenarios. Our evaluation reveals that critic-guided reflection, combined with human supervisory input, reduces modeling errors and increases complexity and quality of the result compared to direct single-prompt execution. This work establishes that structured agent self-reflection, when augmented by human oversight and advisory guidance, produces higher-quality 3D models while maintaining efficient workflow integration through real-time Blender synchronization.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Graphics, Human-Computer Interaction",
        "date": "2026-01-08",
        "category": "cs.MA",
        "crawl_time": "2026-01-13T11:07:56.121529",
        "filter_reason": "该论文提出了一个 Planner-Actor-Critic 框架，涉及多智能体协作（规划者、行动者、评论家）、自我反思和工具使用（Blender MCP），符合 LLM 智能体的研究范围（规划、工具使用、自我反思、多智能体），且重点在于智能体架构而非纯视觉模型或纯应用。"
    },
    {
        "index": "#3",
        "title": "AI Agents as Policymakers in Simulated Epidemics",
        "link": "/arxiv/2601.04245",
        "arxiv_id": "2601.04245",
        "authors": "Goshi Aoki, Navid Ghaffarzadegan",
        "summary": "AI agents are increasingly deployed as quasi-autonomous systems for specialized tasks, yet their potential as computational models of decision-making remains underexplored. We develop a generative AI agent to study repetitive policy decisions during an epidemic, embedding the agent, prompted to act as a city mayor, within a simulated SEIR environment. Each week, the agent receives updated epidemiological information, evaluates the evolving situation, and sets business restriction levels. The agent is equipped with a dynamic memory that weights past events by recency and is evaluated in both single- and ensemble-agent settings across environments of varying complexity. Across scenarios, the agent exhibits human-like reactive behavior, tightening restrictions in response to rising cases and relaxing them as risk declines. Crucially, providing the agent with brief systems-level knowledge of epidemic dynamics, highlighting feedbacks between disease spread and behavioral responses, substantially improves decision quality and stability. The results illustrate how theory-informed prompting can shape emergent policy behavior in AI agents. These findings demonstrate that generative AI agents, when situated in structured environments and guided by minimal domain theory, can serve as powerful computational models for studying decision-making and policy design in complex social systems.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Computers and Society",
        "date": "2026-01-06",
        "category": "cs.MA",
        "crawl_time": "2026-01-13T11:07:56.121955",
        "filter_reason": "论文研究了生成式AI智能体在模拟环境中的决策机制，涉及单智能体的动态记忆和决策制定，以及多智能体集成设置。虽然背景是流行病模拟，但重点在于智能体的行为建模和能力评估，而非单纯的医疗领域应用，符合LLM智能体的研究范围。"
    },
    {
        "index": "#6",
        "title": "When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail",
        "link": "/arxiv/2601.04748",
        "arxiv_id": "2601.04748",
        "authors": "Xiaoxiao Li",
        "summary": "Multi-agent AI systems have proven effective for complex reasoning. These systems are compounded by specialized agents, which collaborate through explicit communication, but incur substantial computational overhead. A natural question arises: can we achieve similar modularity benefits with a single agent that selects from a library of skills? We explore this question by viewing skills as internalized agent behaviors. From this perspective, a multi-agent system can be compiled into an equivalent single-agent system, trading inter-agent communication for skill selection. Our preliminary experiments suggest this approach can substantially reduce token usage and latency while maintaining competitive accuracy on reasoning benchmarks. However, this efficiency raises a deeper question that has received little attention: how does skill selection scale as libraries grow? Drawing on principles from cognitive science, we propose that LLM skill selection exhibits bounded capacity analogous to human decision-making. We investigate the scaling behavior of skill selection and observe a striking pattern. Rather than degrading gradually, selection accuracy remains stable up to a critical library size, then drops sharply, indicating a phase transition reminiscent of capacity limits in human cognition. Furthermore, we find evidence that semantic confusability among similar skills, rather than library size alone, plays a central role in this degradation. This perspective suggests that hierarchical organization, which has long helped humans manage complex choices, may similarly benefit AI systems. Our initial results with hierarchical routing support this hypothesis. This work opens new questions about the fundamental limits of semantic-based skill selection in LLMs and offers a cognitive-grounded framework and practical guidelines for designing scalable skill-based agents.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2026-01-08",
        "category": "cs.MA",
        "crawl_time": "2026-01-13T11:07:56.123264",
        "filter_reason": "论文直接探讨了单智能体与多智能体系统的架构对比，研究了单智能体如何通过技能库模拟多智能体的协作行为。内容涉及智能体的技能选择、架构设计及可扩展性，符合单智能体和多智能体的研究范围。"
    },
    {
        "index": "#7",
        "title": "Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models",
        "link": "/arxiv/2601.04651",
        "arxiv_id": "2601.04651",
        "authors": "Can Xu, Lingyong Yan, Jiayi Wu, Haosen Wang, Shuaiqiang Wang, Yuchen Li, Jizhou Huang, Dawei Yin, Xiang Li",
        "summary": "Recent advances in synergizing large reasoning models (LRMs) with retrieval-augmented generation (RAG) have shown promising results, yet two critical challenges remain: (1) reasoning models typically operate from a single, unchallenged perspective, limiting their ability to conduct deep, self-correcting reasoning over external documents, and (2) existing training paradigms rely excessively on outcome-oriented rewards, which provide insufficient signal for shaping the complex, multi-step reasoning process. To address these issues, we propose an Reasoner-Verifier framework named Adversarial Reasoning RAG (ARR). The Reasoner and Verifier engage in reasoning on retrieved evidence and critiquing each other's logic while being guided by process-aware advantage that requires no external scoring model. This reward combines explicit observational signals with internal model uncertainty to jointly optimize reasoning fidelity and verification rigor. Experiments on multiple benchmarks demonstrate the effectiveness of our method.",
        "subjects": "Artificial Intelligence, Information Retrieval, Multiagent Systems",
        "date": "2026-01-08",
        "category": "cs.MA",
        "crawl_time": "2026-01-13T11:07:56.123766",
        "filter_reason": "论文提出了一个推理者-验证者框架，其中两个组件（推理者和验证者）相互批评逻辑并进行对抗与合作，这属于多智能体协作与博弈以及自我反思的范畴，符合LLM智能体的定义。"
    },
    {
        "index": "#16",
        "title": "AgentTutor: Empowering Personalized Learning with Multi-Turn Interactive Teaching in Intelligent Education Systems",
        "link": "/arxiv/2601.04219",
        "arxiv_id": "2601.04219",
        "authors": "Yuxin Liu, Zeqing Song, Jiong Lou, Chentao Wu, Jie Li",
        "summary": "The rapid advancement of large-scale language models (LLMs) has shown their potential to transform intelligent education systems (IESs) through automated teaching and learning support applications. However, current IESs often rely on single-turn static question-answering, which fails to assess learners' cognitive levels, cannot adjust teaching strategies based on real-time feedback, and is limited to providing simple one-off responses. To address these issues, we introduce AgentTutor, a multi-turn interactive intelligent education system to empower personalized learning. It features an LLM-powered generative multi-agent system and a learner-specific personalized learning profile environment that dynamically optimizes and delivers teaching strategies based on learners' learning status, personalized goals, learning preferences, and multimodal study materials. It includes five key modules: curriculum decomposition, learner assessment, dynamic strategy, teaching reflection, and knowledge & experience memory. We conducted extensive experiments on multiple benchmark datasets, AgentTutor significantly enhances learners' performance while demonstrating strong effectiveness in multi-turn interactions and competitiveness in teaching quality among other baselines.",
        "subjects": "Computers and Society, Artificial Intelligence, Multiagent Systems",
        "date": "2025-12-24",
        "category": "cs.MA",
        "crawl_time": "2026-01-13T11:07:56.132902",
        "filter_reason": "论文提出了一个由LLM驱动的生成式多智能体系统，包含教学反思、知识记忆等核心智能体能力，并涉及多智能体协作与基于反馈的动态策略优化，符合LLM智能体的研究范围。"
    },
    {
        "index": "#17",
        "title": "Generative Teaching via Code",
        "link": "/arxiv/2601.04204",
        "arxiv_id": "2601.04204",
        "authors": "Yuheng Wang, Runde Yang, Lin Wu, Jie Zhang, Jingru Fan, Ruoyu Fu, Tianle Zhou, Huatao Li, Siheng Chen, Weinan E, Chen Qian",
        "summary": "The scalability of high-quality online education is hindered by the high costs and slow cycles of labor-intensive manual content creation. Despite advancements in video generation, current approaches often fail to ensure pedagogical structure and precise control due to their pixel-level, black-box nature. In this paper, we propose Generative Teaching, a novel paradigm that transitions educators from manual creators to high-level directors, allowing them to focus on pedagogical intent while autonomous agents handle the execution. To realize this vision, we introduce TeachMaster, a multi-agent framework that leverages code as an intermediate semantic medium. Unlike traditional video generation methods, TeachMaster orchestrates a collaborative team of agents--spanning planning, design, and rendering--to automate the production of interpretable, editable, and curriculum-ready educational videos. Experiments validate that TeachMaster significantly boosts production efficiency without compromising structural coherence or visual fidelity, providing a robust solution for scalable education.",
        "subjects": "Computers and Society, Artificial Intelligence, Computation and Language, Human-Computer Interaction, Multiagent Systems",
        "date": "2025-12-07",
        "category": "cs.MA",
        "crawl_time": "2026-01-13T11:07:56.133456",
        "filter_reason": "论文提出了TeachMaster，这是一个多智能体框架，利用代码作为媒介，涉及智能体在规划、设计和渲染方面的协作，符合多智能体协作和规划的研究范围。"
    },
    {
        "index": "#4",
        "title": "Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems",
        "link": "/arxiv/2601.05171",
        "arxiv_id": "2601.05171",
        "authors": "Jihao Zhao, Ding Chen, Zhaoxin Fan, Kerun Xu, Mengting Hu, Bo Tang, Feiyu Xiong, Zhiyu li",
        "summary": "Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.",
        "subjects": "Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.309227",
        "filter_reason": "论文提出了一个包含动态记忆管理的框架，涉及单智能体的记忆机制和自我演化（通过强化学习更新记忆树）。摘要中明确提到了触发“agentic mode”来按需引入细节，符合LLM智能体的研究范围。"
    },
    {
        "index": "#7",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "link": "/arxiv/2601.05163",
        "arxiv_id": "2601.05163",
        "authors": "Qintong Zhang, Xinjie Lv, Jialong Wu, Baixuan Li, Zhengwei Tao, Guochen Yan, Huanyao Zhang, Bin Wang, Jiahao Xu, Haitao Mi, Wentao Zhang",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "subjects": "Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.311088",
        "filter_reason": "该论文提出了一个名为 DocDancer 的工具驱动的智能体框架，专注于解决文档问答中的信息寻求问题。它明确涉及了单智能体的“工具使用”和“文档探索”机制，符合 LLM 智能体的研究范围。"
    },
    {
        "index": "#8",
        "title": "Agent-as-a-Judge",
        "link": "/arxiv/2601.05111",
        "arxiv_id": "2601.05111",
        "authors": "Runyang You, Hongru Cai, Caiqi Zhang, Qiancheng Xu, Meng Liu, Tiezheng Yu, Yongqi Li, Wenjie Li",
        "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.316862",
        "filter_reason": "论文提出的“Agent-as-a-Judge”范式明确涵盖了规划、工具增强验证、持久记忆（单智能体特征）以及多智能体协作（多智能体特征），完全符合LLM智能体的研究范围。"
    },
    {
        "index": "#15",
        "title": "Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei",
        "link": "/arxiv/2601.05004",
        "arxiv_id": "2601.05004",
        "authors": "Peng Wang, Xilin Tao, Siyi Yao, Jiageng Wu, Yuntao Zou, Zhuotao Tian, Libo Qin, Dagang Li",
        "summary": "Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.",
        "subjects": "Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.320401",
        "filter_reason": "论文提出了一个名为SAS（Subcultural Alignment Solver）的多智能体框架，该框架包含自动检索和亚文化对齐机制，旨在解决LLMs在特定任务中的语义错位问题。虽然应用场景涉及心理学/亚文化，但论文的核心贡献在于多智能体框架的设计与实现，符合“多智能体：协作、通信”的研究范围。"
    },
    {
        "index": "#24",
        "title": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis",
        "link": "/arxiv/2601.04879",
        "arxiv_id": "2601.04879",
        "authors": "Mingyue Cheng, Daoyu Wang, Qi Liu, Shuo Yu, Xiaoyu Tao, Yuqian Wang, Chengzhong Chu, Yu Duan, Mingkang Long, Enhong Chen",
        "summary": "Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.",
        "subjects": "Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.330154",
        "filter_reason": "论文提出了一个“认知深度研究智能体”，重点研究了智能体的工作流设计，包括动态记忆、工具使用（搜索）和迭代规划等核心机制。虽然应用场景是商业报告，但其核心贡献在于智能体架构本身，符合单智能体的研究范围。"
    },
    {
        "index": "#29",
        "title": "RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection",
        "link": "/arxiv/2601.04853",
        "arxiv_id": "2601.04853",
        "authors": "Zhiwei Liu, Runteng Guo, Baojie Qu, Yuechen Jiang, Min Peng, Qianqian Xie, Sophia Ananiadou",
        "summary": "Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.",
        "subjects": "Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.348206",
        "filter_reason": "论文提出了RAAR框架，明确利用了“多智能体协作”，即通过特定视角的智能体和摘要智能体的配合来构建推理路径，符合多智能体协作的研究范围。虽然应用于虚假信息检测，但其核心贡献在于智能体推理框架的设计，而非单纯的应用。"
    },
    {
        "index": "#31",
        "title": "Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework",
        "link": "/arxiv/2601.04790",
        "arxiv_id": "2601.04790",
        "authors": "Junhyuk Choi, Jeongyoun Kwon, Heeju Kim, Haeun Cho, Hayeong Jung, Sehee Min, Bugeun Kim",
        "summary": "Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.349441",
        "filter_reason": "该论文研究了利用大语言模型的多智能体系统，分析了权威角色对智能体交互和评估框架的影响，属于多智能体协作与通信的研究范畴。"
    },
    {
        "index": "#32",
        "title": "NC2C: Automated Convexification of Generic Non-Convex Optimization Problems",
        "link": "/arxiv/2601.04789",
        "arxiv_id": "2601.04789",
        "authors": "Xinyue Peng, Yanming Liu, Yihan Cang, Yuwei Zhang, Xinyi Wang, Songhang Deng, Jiannan Cao",
        "summary": "Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\\% execution rate and a 76\\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.350071",
        "filter_reason": "该论文提出了一个基于LLM的自动化框架（NC2C），用于解决非凸优化问题。它集成了符号推理和求解器（工具使用），并采用了迭代验证和错误修正循环（自我反思/反馈）。这符合单智能体的定义，特别是涉及工具使用和通过反馈进行自我完善，不属于纯推理或纯应用。"
    },
    {
        "index": "#37",
        "title": "Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval",
        "link": "/arxiv/2601.04742",
        "arxiv_id": "2601.04742",
        "authors": "Seyeon Jeong, Yeonjun Choi, JongWook Kim, Beakcheol Jang",
        "summary": "Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.",
        "subjects": "Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.378748",
        "filter_reason": "该论文提出了Tool-MAD，这是一个多智能体辩论框架，涉及多个LLM智能体之间的协作、通信以及工具使用（如搜索API和RAG模块），完全符合多智能体和工具使用的研究范围。"
    },
    {
        "index": "#42",
        "title": "Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents",
        "link": "/arxiv/2601.04716",
        "arxiv_id": "2601.04716",
        "authors": "Yonghyun Jun, Junhyuk Choi, Jihyeong Park, Hwanhee Lee",
        "summary": "Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \\textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \\textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \\textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \\textit{\"Fame Fades\"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \\textit{\"Nature Remains\"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.",
        "subjects": "Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.381899",
        "filter_reason": "该论文明确研究基于 LLM 的角色扮演智能体，探讨了智能体的内部身份结构（参数化与属性化）及其在多轮对话中的行为表现。这属于单智能体范畴中关于智能体行为一致性、记忆与上下文利用的研究，且不属于排除的纯应用、纯推理、安全对齐或其他技术领域。"
    },
    {
        "index": "#48",
        "title": "ToolGate: Contract-Grounded and Verified Tool Execution for LLMs",
        "link": "/arxiv/2601.04688",
        "arxiv_id": "2601.04688",
        "authors": "Yanming Liu, Xinyue Peng, Jiannan Cao, Xinyi Wang, Songhang Deng, Jintao Chen, Jianwei Yin, Xuhong Zhang",
        "summary": "Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \\textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.",
        "subjects": "Computation and Language, Artificial Intelligence, Formal Languages and Automata Theory",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.391075",
        "filter_reason": "该论文专注于LLM的工具使用，提出了一个名为ToolGate的框架来验证工具调用和状态演化，属于单智能体研究中的“工具使用”范畴。"
    },
    {
        "index": "#53",
        "title": "Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR",
        "link": "/arxiv/2601.04611",
        "arxiv_id": "2601.04611",
        "authors": "Yihong Tang, Kehai Chen, Xuefeng Bai, Benyou Wang, Zeming Liu, Haifeng Wang, Min Zhang",
        "summary": "Current role-playing agents (RPAs) are typically constructed by imitating surface-level behaviors, but this approach lacks internal cognitive consistency, often causing out-of-character errors in complex situations. To address this, we propose Character-R1, a framework designed to provide comprehensive verifiable reward signals for effective role-aware reasoning, which are missing in recent studies. Specifically, our framework comprises three core designs: (1) Cognitive Focus Reward, which enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward, which utilizes overlap-based metrics with reference responses as optimization anchors to enhance exploration and performance; and (3) Character-Conditioned Reward Normalization, which adjusts reward distributions based on character categories to ensure robust optimization across heterogeneous roles. Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and others.",
        "subjects": "Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.399243",
        "filter_reason": "该论文明确研究角色扮演智能体，旨在通过强化学习框架增强智能体的内部认知、记忆和推理能力，属于单智能体（记忆、自我反思）及自我演化（通过反馈自我完善）的研究范畴。"
    },
    {
        "index": "#62",
        "title": "LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation",
        "link": "/arxiv/2601.04516",
        "arxiv_id": "2601.04516",
        "authors": "Yuxiao Ye, Yiming Zhang, Yiran Ma, Huiyuan Xie, Huining Zhu, Zhiyuan Liu",
        "summary": "Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.",
        "subjects": "Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.403619",
        "filter_reason": "该论文提出了一个基于博弈论的多智能体对话生成框架，专注于LLM智能体之间的通信效率和交互过程，属于多智能体协作与通信的研究范畴。"
    },
    {
        "index": "#66",
        "title": "Beyond Static Summarization: Proactive Memory Extraction for LLM Agents",
        "link": "/arxiv/2601.04463",
        "arxiv_id": "2601.04463",
        "authors": "Chengyuan Yang, Zequn Sun, Wei Wei, Wei Hu",
        "summary": "Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is \"ahead-of-time\", acting as a blind \"feed-forward\" process that misses important details because it doesn't know future tasks. Second, extraction is usually \"one-off\", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.416230",
        "filter_reason": "该论文专注于LLM智能体的记忆管理机制，提出了通过反馈循环和自提问来改进记忆提取的方法。这直接属于研究范围中“单智能体”的“记忆”和“自我反思”能力，且不属于排除项。"
    },
    {
        "index": "#105",
        "title": "Higher-Order Knowledge Representations for Agentic Scientific Reasoning",
        "link": "/arxiv/2601.04878",
        "arxiv_id": "2601.04878",
        "authors": "Isabella A. Stewart, Markus J. Buehler",
        "summary": "Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a \"teacherless\" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.",
        "subjects": "Artificial Intelligence, Materials Science, Computation and Language, Machine Learning",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.466360",
        "filter_reason": "论文明确提出了一个“agentic reasoning system”（智能体推理系统），核心贡献在于为智能体配备了“hypergraph traversal tools”（超图遍历工具）以辅助其进行科学假设生成。这符合单智能体研究中“工具使用”的定义，且重点在于智能体的架构与方法论，而非单纯的应用或纯推理算法。"
    },
    {
        "index": "#109",
        "title": "AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search",
        "link": "/arxiv/2601.04767",
        "arxiv_id": "2601.04767",
        "authors": "Zefang Zong, Dingwei Chen, Yang Li, Qi Yi, Bo Zhou, Chengming Li, Bo Qian, Peng Chen, Jie Jiang",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.468534",
        "filter_reason": "论文明确研究LLM智能体，提出了AT$^2$PO框架用于多轮智能体强化学习，涉及工具交互、策略优化和策略性探索，属于单智能体规划与自我演化的研究范畴，且不属于排除项。"
    },
    {
        "index": "#111",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "link": "/arxiv/2601.04726",
        "arxiv_id": "2601.04726",
        "authors": "Yuyang Hu, Jiongnan Liu, Jiejun Tan, Yutao Zhu, Zhicheng Dou",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.469569",
        "filter_reason": "该论文提出了一种名为CompassMem的以事件为中心的记忆框架，旨在增强LLM智能体在长周期场景中的记忆保留、组织和检索能力，以支持下游决策和推理。这完全符合“单智能体”中关于“记忆”的研究范围。"
    },
    {
        "index": "#121",
        "title": "Advancing Language Models for Code-related Tasks",
        "link": "/arxiv/2601.04526",
        "arxiv_id": "2601.04526",
        "authors": "Zhao Tian",
        "summary": "Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.",
        "subjects": "Software Engineering, Artificial Intelligence, Computation and Language",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.479865",
        "filter_reason": "论文摘要中明确提出了一种基于智能体的技术（Specine）来推进模型推理，这属于单智能体（工具使用/规划）的研究范畴，符合筛选条件。"
    },
    {
        "index": "#122",
        "title": "CircuitLM: A Multi-Agent LLM-Aided Design Framework for Generating Circuit Schematics from Natural Language Prompts",
        "link": "/arxiv/2601.04505",
        "arxiv_id": "2601.04505",
        "authors": "Khandakar Shakib Al Hasan, Syed Rifat Raiyan, Hasin Mahtab Alvee, Wahid Sadik",
        "summary": "Generating accurate circuit schematics from high-level natural language descriptions remains a persistent challenge in electronics design, as large language models (LLMs) frequently hallucinate in granular details, violate electrical constraints, and produce non-machine-readable outputs. We present CircuitLM, a novel multi-agent LLM-aided circuit design pipeline that translates user prompts into structured, visually interpretable CircuitJSON schematics through five sequential stages: (i) LLM-based component identification, (ii) canonical pinout retrieval, (iii) chain-of-thought reasoning by an electronics expert agent, (iv) JSON schematic synthesis, and (v) force-directed SVG visualization. Anchored by a curated, embedding-powered component knowledge base. While LLMs often violate electrical constraints, CircuitLM bridges this gap by grounding generation in a verified and dynamically extensible component database, initially comprising 50 components. To ensure safety, we incorporate a hybrid evaluation framework, namely Dual-Metric Circuit Validation (DMCV), validated against human-expert assessments, which achieves high fidelity in microcontroller-centric designs. We evaluate the system on 100 diverse embedded-systems prompts across six LLMs and introduce DMCV to assess both structural and electrical validity. This work bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts. Our code and data will be made public upon acceptance.",
        "subjects": "Artificial Intelligence, Computation and Language, Systems and Control",
        "date": "2026-01-08",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.480353",
        "filter_reason": "论文提出了一个名为CircuitLM的多智能体LLM辅助设计框架，涉及多个智能体（如电子专家智能体）的协作与流水线处理，符合多智能体协作的研究范围。"
    },
    {
        "index": "#127",
        "title": "The Language of Bargaining: Linguistic Effects in LLM Negotiations",
        "link": "/arxiv/2601.04387",
        "arxiv_id": "2601.04387",
        "authors": "Stuti Sinha, Himanshu Kumar, Aryan Raju Mandapati, Rakshit Sakhuja, Dhruv Kumar",
        "summary": "Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Science and Game Theory",
        "date": "2026-01-07",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.488133",
        "filter_reason": "该论文使用受控的多智能体模拟（最后通牒、买卖、资源交换游戏）来研究LLMs之间的谈判行为，属于研究范围内的“多智能体：协作、通信、博弈”。"
    },
    {
        "index": "#133",
        "title": "SAGE-32B: Agentic Reasoning via Iterative Distillation",
        "link": "/arxiv/2601.04237",
        "arxiv_id": "2601.04237",
        "authors": "Basab Jha, Firoj Paudel, Ujjwal Puri, Ethan Henkel, Zhang Yuting, Mateusz Kowalczyk, Mei Huang, Choi Donghyuk, Wang Junhao",
        "summary": "We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2026-01-04",
        "category": "cs.CL",
        "crawl_time": "2026-01-13T11:07:57.506749",
        "filter_reason": "论文明确提出了专注于智能体推理和长程规划的模型 SAGE-32B。其核心功能包括任务分解、工具使用和错误恢复（自我反思），符合单智能体的研究范围。此外，论文在 AgentBench 等智能体基准上进行了评估，且训练过程涉及通过反馈循环提升性能，符合筛选条件。"
    },
    {
        "index": "#24",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "link": "/arxiv/2601.04786",
        "arxiv_id": "2601.04786",
        "authors": "Lang Feng, Fuchao Yang, Feng Chen, Xin Cheng, Haiyang Xu, Zhenglin Wan, Ming Yan, Bo An",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.LG",
        "crawl_time": "2026-01-13T11:07:57.411623",
        "filter_reason": "该论文提出了AgentOCR框架，旨在解决LLM智能体在多轮交互中面临的记忆（历史记录）膨胀问题。这属于单智能体研究中的“记忆”范畴。论文通过引入视觉token和强化学习训练智能体进行“自我压缩”，以平衡任务成功率和token效率，核心在于改进智能体的记忆机制而非单纯的基础设施优化或纯视觉研究。"
    },
    {
        "index": "#119",
        "title": "Sci-Reasoning: A Dataset Decoding AI Innovation Patterns",
        "link": "/arxiv/2601.04577",
        "arxiv_id": "2601.04577",
        "authors": "Jiachen Liu, Maestro Harmon, Zechen Zhang",
        "summary": "While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2026-01-08",
        "category": "cs.LG",
        "crawl_time": "2026-01-13T11:07:57.542552",
        "filter_reason": "论文明确介绍了“Sci-Reasoning”数据集，旨在捕捉科学推理过程，并指出该数据集为“训练下一代AI研究智能体”提供了结构化的推理轨迹，直接服务于LLM智能体的开发。"
    },
    {
        "index": "#145",
        "title": "Scaling Trends for Multi-Hop Contextual Reasoning in Mid-Scale Language Models",
        "link": "/arxiv/2601.04254",
        "arxiv_id": "2601.04254",
        "authors": "Brady Steele, Micah Katz",
        "summary": "We present a controlled study of multi-hop contextual reasoning in large language models, providing a clean demonstration of the task-method dissociation: rule-based pattern matching achieves 100% success on structured information retrieval but only 6.7% on tasks requiring cross-document reasoning, while LLM-based multi-agent systems show the inverse pattern, achieving up to 80% on reasoning tasks where rule-based methods fail. Using a synthetic evaluation framework with 120 trials across four models (LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B), we report three key findings: (1) Multi-agent amplification depends on base capability: statistically significant gains occur only for models with sufficient reasoning ability (p < 0.001 for LLaMA-3 8B, p = 0.014 for Mixtral), with improvements of up to 46.7 percentage points, while weaker models show no benefit, suggesting amplification rather than compensation; (2) Active parameters predict reasoning performance: Mixtral's performance aligns with its ~12B active parameters rather than 47B total, consistent with the hypothesis that inference-time compute drives reasoning capability in MoE architectures; (3) Architecture quality matters: LLaMA-3 8B outperforms LLaMA-2 13B despite fewer parameters, consistent with known training improvements. Our results provide controlled quantitative evidence for intuitions about multi-agent coordination and MoE scaling, while highlighting the dependence of multi-agent benefits on base model capability. We release our evaluation framework to support reproducible research on reasoning in mid-scale models.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2026-01-06",
        "category": "cs.LG",
        "crawl_time": "2026-01-13T11:07:57.555690",
        "filter_reason": "论文明确研究了基于LLM的多智能体系统，分析了多智能体协调和放大效应对推理性能的影响，符合多智能体协作的研究范围。"
    },
    {
        "index": "#2",
        "title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents",
        "link": "/arxiv/2601.05215",
        "arxiv_id": "2601.05215",
        "authors": "Tamil Sudaravan Mohan Doss, Michael Xu, Sudha Rao, Andrew D. Wilson, Balasaravanan Thoravi Kumaravel",
        "summary": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence. As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.724884",
        "filter_reason": "论文提出了一个用于评估具有记忆能力的LLM智能体的基准。研究内容涵盖了智能体的核心能力，包括规划、记忆（读写）、工具使用（库存/代码执行）以及自我反思（修复尝试），符合单智能体的研究范围。"
    },
    {
        "index": "#3",
        "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
        "link": "/arxiv/2601.05214",
        "arxiv_id": "2601.05214",
        "authors": "Kait Healy, Bharathi Srinivasan, Visakh Madathil, Jing Wu",
        "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.725156",
        "filter_reason": "该论文专注于LLM智能体的工具调用与使用能力，研究如何检测智能体在工具选择和参数生成过程中的幻觉问题，属于单智能体研究中的“工具使用”范畴。"
    },
    {
        "index": "#5",
        "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
        "link": "/arxiv/2601.05187",
        "arxiv_id": "2601.05187",
        "authors": "Yanchang Liang, Xiaowei Zhao",
        "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.725709",
        "filter_reason": "论文提出了SimuAgent，这是一个基于LLM的智能体，采用了规划-执行架构，具备工具使用能力，并引入了Reflection-GRPO机制进行自我反思和强化学习优化，符合单智能体（规划、工具使用、自我反思）及自我演化的研究范围。"
    },
    {
        "index": "#10",
        "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
        "link": "/arxiv/2601.05107",
        "arxiv_id": "2601.05107",
        "authors": "Muzhao Tian, Zisu Huang, Xiaohua Wang, Jingwen Xu, Zhengkang Guo, Qi Qian, Yuanzhe Shen, Kaitao Song, Jiakang Yuan, Changze Lv, Xiaoqing Zheng",
        "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.727177",
        "filter_reason": "该论文提出了“SteeM”框架，专注于解决LLM智能体在长期交互中的记忆管理问题。这属于单智能体研究范畴中的“记忆”机制，旨在优化智能体如何利用历史信息进行个性化交互，符合筛选条件。"
    },
    {
        "index": "#12",
        "title": "Arabic Prompts with English Tools: A Benchmark",
        "link": "/arxiv/2601.05101",
        "arxiv_id": "2601.05101",
        "authors": "Konstantin Kubrak, Ahmed El-Moselhy, Ammar Alsulami, Remaz Altuwaim, Hassan Ismail Fawaz, Faisal Alsaby",
        "summary": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.727821",
        "filter_reason": "论文提出了一个基准，专门用于评估LLM在阿拉伯语环境下的工具调用和智能体能力，这属于单智能体研究中的“工具使用”范畴。"
    },
    {
        "index": "#28",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "link": "/arxiv/2601.04888",
        "arxiv_id": "2601.04888",
        "authors": "Tongyu Wen, Guanting Dong, Zhicheng Dou",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.732488",
        "filter_reason": "该论文明确研究基于LLM的搜索智能体，重点在于优化智能体的工具使用（信息检索）和规划（查询细化）能力，属于单智能体研究范畴。"
    },
    {
        "index": "#33",
        "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models",
        "link": "/arxiv/2601.04861",
        "arxiv_id": "2601.04861",
        "authors": "Jingbo Wang, Sendong Zhao, Jiatong Liu, Haochun Wang, Wanting Li, Bing Qin, Ting Liu",
        "summary": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%.",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.733930",
        "filter_reason": "该论文提出了 OI-MAS 框架，专注于多智能体系统（MAS）的协作与优化。研究内容涉及动态选择智能体角色和模型规模，属于多智能体协作的研究范围。虽然关注计算效率，但核心在于智能体框架的架构设计而非底层基础设施部署，且不涉及被排除的纯推理、特定领域应用或多模态内容。"
    },
    {
        "index": "#44",
        "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions",
        "link": "/arxiv/2601.04745",
        "arxiv_id": "2601.04745",
        "authors": "Tingyu Wu, Zhisheng Chen, Ziyan Weng, Shuhe Wang, Chenglong Li, Shuo Zhang, Sen Hu, Silin Wu, Qizhen Lan, Huacan Wang, Ronghao Chen",
        "summary": "Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in \\href{KnowMeBench}{https://github.com/QuantaAlpha/KnowMeBench}.",
        "subjects": "Artificial Intelligence, Information Retrieval",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.737224",
        "filter_reason": "该论文提出了一个用于评估“终身数字伴侣”的基准，重点在于长时程记忆和人理解。这属于LLM智能体研究中的“记忆”范畴，旨在评估智能体在长期交互中存储、检索和利用用户信息的能力。"
    },
    {
        "index": "#49",
        "title": "Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search",
        "link": "/arxiv/2601.04703",
        "arxiv_id": "2601.04703",
        "authors": "Yiqun Chen, Lingyong Yan, Zixuan Yang, Erhan Zhang, Jiashu Zhao, Shuaiqiang Wang, Dawei Yin, Jiaxin Mao",
        "summary": "Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \\textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.738624",
        "filter_reason": "该论文提出了一个名为M-ASK的多智能体框架，用于解决智能体搜索中的问题。它明确将任务解耦为“搜索行为智能体”（负责规划和执行搜索动作）和“知识管理智能体”（负责维护内部上下文），属于多智能体协作范畴，同时涉及规划、工具使用和记忆等单智能体核心能力，符合筛选标准。"
    },
    {
        "index": "#53",
        "title": "ResMAS: Resilience Optimization in LLM-based Multi-agent Systems",
        "link": "/arxiv/2601.04694",
        "arxiv_id": "2601.04694",
        "authors": "Zhilun Zhou, Zihan Liu, Jiahe Liu, Qingyu Shao, Yihan Wang, Kun Shao, Depeng Jin, Fengli Xu",
        "summary": "Large Language Model-based Multi-Agent Systems (LLM-based MAS), where multiple LLM agents collaborate to solve complex tasks, have shown impressive performance in many areas. However, MAS are typically distributed across different devices or environments, making them vulnerable to perturbations such as agent failures. While existing works have studied the adversarial attacks and corresponding defense strategies, they mainly focus on reactively detecting and mitigating attacks after they occur rather than proactively designing inherently resilient systems. In this work, we study the resilience of LLM-based MAS under perturbations and find that both the communication topology and prompt design significantly influence system resilience. Motivated by these findings, we propose ResMAS: a two-stage framework for enhancing MAS resilience. First, we train a reward model to predict the MAS's resilience, based on which we train a topology generator to automatically design resilient topology for specific tasks through reinforcement learning. Second, we introduce a topology-aware prompt optimization method that refines each agent's prompt based on its connections and interactions with other agents. Extensive experiments across a range of tasks show that our approach substantially improves MAS resilience under various constraints. Moreover, our framework demonstrates strong generalization ability to new tasks and models, highlighting its potential for building resilient MASs.",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.739746",
        "filter_reason": "该论文研究基于LLM的多智能体系统（MAS），重点在于通过优化通信拓扑结构和提示词设计来提升系统的韧性。这属于多智能体协作与通信的研究范畴，符合筛选条件。"
    },
    {
        "index": "#56",
        "title": "Vibe Coding an LLM-powered Theorem Prover",
        "link": "/arxiv/2601.04653",
        "arxiv_id": "2601.04653",
        "authors": "Zhe Hou",
        "summary": "We present Isabellm, an LLM-powered theorem prover for Isabelle/HOL that performs fully automatic proof synthesis. Isabellm works with any local LLM on Ollama and APIs such as Gemini CLI, and it is designed to run on consumer grade computers. The system combines a stepwise prover, which uses large language models to propose proof commands validated by Isabelle in a bounded search loop, with a higher-level proof planner that generates structured Isar outlines and attempts to fill and repair remaining gaps. The framework includes beam search for tactics, tactics reranker ML and RL models, premise selection with small transformer models, micro-RAG for Isar proofs built from AFP, and counter-example guided proof repair. All the code is implemented by GPT 4.1 - 5.2, Gemini 3 Pro, and Claude 4.5. Empirically, Isabellm can prove certain lemmas that defeat Isabelle's standard automation, including Sledgehammer, demonstrating the practical value of LLM-guided proof search. At the same time, we find that even state-of-the-art LLMs, such as GPT 5.2 Extended Thinking and Gemini 3 Pro struggle to reliably implement the intended fill-and-repair mechanisms with complex algorithmic designs, highlighting fundamental challenges in LLM code generation and reasoning. The code of Isabellm is available at https://github.com/zhehou/llm-isabelle",
        "subjects": "Artificial Intelligence, Logic in Computer Science",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.740585",
        "filter_reason": "该论文构建了一个名为Isabellm的系统，该系统结合了高级证明规划器、逐步证明器以及反例引导的修复机制。这符合单智能体研究范围中的“规划”（生成结构化大纲）、“工具使用”（调用Isabelle/HOL进行验证）以及“自我反思/完善”（填补缺口和修复证明），属于具有反馈循环的智能体框架，而非单纯的数学推理。"
    },
    {
        "index": "#59",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "link": "/arxiv/2601.04620",
        "arxiv_id": "2601.04620",
        "authors": "Di Zhang",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.741425",
        "filter_reason": "该论文明确研究“Self-Evolving LLM Agents”（自我演化LLM智能体），提出了一种通过反馈（执行痕迹、症状级质量信号）进行迭代改进的机制，完全符合“自我演化：通过反馈自我完善”的研究范围。虽然引入了“发布工程”的概念，但其核心目标是解决智能体改进过程中的不稳定性和可审计性问题，而非纯基础设施优化或特定领域应用。"
    },
    {
        "index": "#69",
        "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration",
        "link": "/arxiv/2601.04544",
        "arxiv_id": "2601.04544",
        "authors": "Jiuzhou Zhao, Chunrong Chen, Chenqi Qiao, Lebin Zheng, Minqi Han, Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang",
        "summary": "Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.744232",
        "filter_reason": "该论文提出了一个用于多智能体协作的自适应推理路由器（TCAndon-Router），重点解决了多智能体系统中的任务分配、动态智能体接入以及协作执行流水线设计问题，属于多智能体协作与通信的研究范畴。"
    },
    {
        "index": "#98",
        "title": "Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements",
        "link": "/arxiv/2601.04235",
        "arxiv_id": "2601.04235",
        "authors": "Hong Su",
        "summary": "Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.",
        "subjects": "Artificial Intelligence",
        "date": "2026-01-04",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.752178",
        "filter_reason": "该论文提出了一个AI智能体模型，专注于智能体如何主动与环境交互以获取反馈、评估行动并进行自主规划。这符合单智能体研究范围中的规划、自我反思（通过反馈评估）以及自我演化（通过反馈调整行动）的定义。"
    },
    {
        "index": "#132",
        "title": "Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests",
        "link": "/arxiv/2601.04886",
        "arxiv_id": "2601.04886",
        "authors": "Jingzhi Gong, Giovanni Pinna, Yixin Bian, Jie M. Zhang",
        "summary": "Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2026-01-08",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.762460",
        "filter_reason": "该论文明确研究“AI编程智能体”，属于LLM智能体在工具使用（代码生成与修改）和人机协作方面的具体应用研究。论文分析了智能体生成的PR描述与代码变更的一致性，涉及智能体的输出质量和可信度，符合LLM智能体的研究范畴，不属于纯应用（如医疗/金融）、纯推理、安全对齐或基础设施优化等排除项。"
    },
    {
        "index": "#196",
        "title": "3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation",
        "link": "/arxiv/2601.04404",
        "arxiv_id": "2601.04404",
        "authors": "Jusheng Zhang, Yijia Fan, Zimo Wen, Jian Wang, Keze Wang",
        "summary": "Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2026-01-07",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.781179",
        "filter_reason": "论文提出了一个名为Tri MARF的多智能体协作架构，包含三个专门的智能体（VLM智能体、信息聚合智能体和门控智能体）进行协作，符合“多智能体：协作”的研究范围。虽然涉及视觉数据，但核心贡献在于智能体架构而非单纯的视觉模型。"
    },
    {
        "index": "#208",
        "title": "ParaCodex: A Profiling-Guided Autonomous Coding Agent for Reliable Parallel Code Generation and Translation",
        "link": "/arxiv/2601.04327",
        "arxiv_id": "2601.04327",
        "authors": "Erel Kaplan, Tomer Bitan, Lian Ghrayeb, Le Chen, Tom Yotam, Niranjan Hasabnis, Gal Oren",
        "summary": "Parallel programming is central to HPC and AI, but producing code that is correct and fast remains challenging, especially for OpenMP GPU offload, where data movement and tuning dominate. Autonomous coding agents can compile, test, and profile on target hardware, but outputs are brittle without domain scaffolding. We present ParaCodex, an HPC-engineer workflow that turns a Codex-based agent into an autonomous OpenMP GPU offload system using staged hotspot analysis, explicit data planning, correctness gating, and profiling-guided refinement. We evaluate translation from serial CPU kernels to OpenMP GPU offload kernels on HeCBench, Rodinia, and NAS. After excluding five kernels, ParaCodex succeeded on all 31 valid kernels. The generated kernels improved GPU time over reference OpenMP implementations in 25/31 cases, achieving geometric-mean speedups of 3x on HeCBench and 5x on Rodinia, and outperforming a zero-shot Codex baseline on all suites. We also evaluate CUDA to OpenMP offload translation on ParEval, where ParaCodex maintains high compilation and validation rates in code-only and end-to-end settings.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Artificial Intelligence",
        "date": "2026-01-07",
        "category": "cs.AI",
        "crawl_time": "2026-01-13T11:07:57.784601",
        "filter_reason": "论文提出了一个自主编码智能体，明确涉及工具使用（编译、测试、分析）、规划（显式数据规划）和自我反思（分析引导的优化），符合单智能体的研究范围。"
    }
]