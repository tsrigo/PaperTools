[
    {
        "index": "#3",
        "title": "Multi-agent In-context Coordination via Decentralized Memory Retrieval",
        "link": "/arxiv/2511.10030",
        "arxiv_id": "2511.10030",
        "authors": "Tao Jiang, Zichuan Lin, Lihe Li, Yi-Chen Li, Cong Guan, Lei Yuan, Zongzhang Zhang, Yang Yu, Deheng Ye",
        "summary": "Large transformer models, trained on diverse datasets, have demonstrated impressive few-shot performance on previously unseen tasks without requiring parameter updates. This capability has also been explored in Reinforcement Learning (RL), where agents interact with the environment to retrieve context and maximize cumulative rewards, showcasing strong adaptability in complex settings. However, in cooperative Multi-Agent Reinforcement Learning (MARL), where agents must coordinate toward a shared goal, decentralized policy deployment can lead to mismatches in task alignment and reward assignment, limiting the efficiency of policy adaptation. To address this challenge, we introduce Multi-agent In-context Coordination via Decentralized Memory Retrieval (MAICC), a novel approach designed to enhance coordination by fast adaptation. Our method involves training a centralized embedding model to capture fine-grained trajectory representations, followed by decentralized models that approximate the centralized one to obtain team-level task information. Based on the learned embeddings, relevant trajectories are retrieved as context, which, combined with the agents' current sub-trajectories, inform decision-making. During decentralized execution, we introduce a novel memory mechanism that effectively balances test-time online data with offline memory. Based on the constructed memory, we propose a hybrid utility score that incorporates both individual- and team-level returns, ensuring credit assignment across agents. Extensive experiments on cooperative MARL benchmarks, including Level-Based Foraging (LBF) and SMAC (v1/v2), show that MAICC enables faster adaptation to unseen tasks compared to existing methods. Code is available at https://github.com/LAMDA-RL/MAICC.",
        "subjects": "Multiagent Systems, Machine Learning",
        "date": "2025-11-13",
        "category": "cs.MA",
        "crawl_time": "2025-11-14T11:00:03.350984",
        "filter_reason": "这篇论文完全符合您的研究范围，应被保留。以下是详细的判断过程： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出了一种名为MAICC的新方法，用于解决多智能体强化学习（MARL）中的协调与快速适应问题。这并非简单地将已有框架应用于特定领域，而是**构建和改进多智能体系统**本身的方法论。其核心在于设计了一种新的协调机制（去中心化记忆检索）和一种新的记忆平衡机制，这完全符合“构建、改进或演化LLM智能体”的核心目标。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了大量您关注的核心正面指标： *   **核心范式**: `Multi-Agent Systems (MAS)` 是论文的绝对核心。 *   **智能体能力**: `Memory` 是论文的关键创新点，标题和摘要中多次强调。 *   **多智能体**: `Collaboration` (协作) 和 `Coordination` (协调) 是论文要解决的核心问题。摘要中明确提到了 \"cooperative Multi-Agent Reinforcement Learning\" 和 \"team-level task information\"。 *   **演化机制**: 论文的目标是实现 \"fast adaptation\" (快速适应) 和 \"in-context coordination\" (上下文协调)，这本质上是一种**自我演化**或**迭代改进**的能力，使智能体能够根据检索到的经验和在线数据快速调整行为。 3.  **第三步：排除标准——未触发** 论文的研究焦点是算法性能和协调效率，完全没有涉及安全、对齐、可解释性或水印等内容。同时，它也不涉及多模态或视觉模型，其处理的上下文是轨迹数据，而非视觉信息。 4.  **第四步：处理特殊和模糊情况——符合保留规则** *   **推理/规划**: 论文讨论的是多智能体在复杂环境中如何基于检索到的记忆和当前状态进行决策，这属于智能体的规划和推理范畴，而非提升LLM本身的基础推理能力。 *   **自我演化的应用**: 论文的核心贡献正是提出了一种新的“自我演化”机制（通过记忆检索实现快速适应）。即使它被应用在MARL这个特定领域，根据您的规则，这种提出新机制的研究也应该被保留。 **最终决策**: 这篇论文的核心贡献在于为多智能体系统设计了一种新颖的协调与适应框架。它直接命中了您的研究焦点中的**“多智能体”**方向，并且其“快速适应”的目标与**“自我演化”**方向高度契合。论文提出的去中心化记忆检索和混合效用分数等机制，都是对智能体能力的实质性改进。因此，这篇论文是您研究课题下的高质量前沿文献，应被筛选出来。"
    },
    {
        "index": "#1",
        "title": "Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance",
        "link": "/arxiv/2511.10400",
        "arxiv_id": "2511.10400",
        "authors": "Lifan Zheng, Jiawei Chen, Qinghong Yin, Jingyuan Zhang, Xinyi Zeng, Yu Tian",
        "summary": "Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Computation and Language",
        "date": "2025-11-13",
        "category": "cs.MA",
        "crawl_time": "2025-11-14T11:00:03.350388",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM智能体作为工具应用到一个新领域，而是**提出了一种新的方法论（CP-WBFT机制）来改进LLM多智能体系统（MAS）的可靠性**。它的核心贡献在于构建和改进一个多智能体框架，这直接命中了您“构建、改进或演化LLM智能体”的核心目标。它研究的是智能体系统本身的属性（可靠性），而非应用。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `Multi-Agent Systems (MAS)` 和 `LLM-based Agents` 是论文的绝对核心。 - **智能体能力**: 论文明确提到其机制“capitalizes on the intrinsic reflective and discriminative capabilities of LLMs”（利用了LLM固有的反思和判别能力），这与 `Self-Reflection` 高度相关。 - **多智能体**: 论文的研究背景是 `Multi-Agent Systems`，其提出的共识机制直接处理智能体间的 `Communication` 和协作问题，以提升整个系统的稳定性。 3.  **第三步：排除标准** - **安全与对齐**: 论文摘要中提到了“safety assessment tasks”（安全评估任务），但这仅仅是作为**评估其机制有效性的一个测试场景**，而非论文的主要贡献。论文的核心是“Byzantine Fault Tolerance”（拜占庭容错）和“Reliability”（可靠性），这是一个系统鲁棒性和分布式系统领域的问题，与AI安全、对齐、可解释性等研究焦点有本质区别。因此，不应被排除。 - **多模态与视觉**: 论文不涉及多模态内容。 4.  **第四步：处理特殊和模糊情况** - 该论文不涉及推理/规划或自我演化的应用等特殊情况，其定位非常清晰。 5.  **第五步：最终决策** - **综合判断**：该论文的核心贡献是提出了一种名为CP-WBFT的新共识机制，用于**改进LLM多智能体系统的可靠性**。这完全属于您研究范围中的“多智能体”方向，并且是关于“改进”智能体系统的方法论研究。它不是简单的应用，也不是关于安全对齐或基础模型推理的研究。因此，这篇论文与您的研究目标高度契合，应被筛选出来。"
    },
    {
        "index": "#44",
        "title": "MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection",
        "link": "/arxiv/2511.09918",
        "arxiv_id": "2511.09918",
        "authors": "Pritish Sahu, Anirudh Som, Dimitra Vergyri, Ajay Divakaran",
        "summary": "Social norms are implicit, culturally grounded expectations that guide interpersonal communication. Unlike factual commonsense, norm reasoning is subjective, context-dependent, and varies across cultures, posing challenges for computational models. Prior works provide valuable normative annotations but mostly target isolated utterances or synthetic dialogues, limiting their ability to capture the fluid, multi-turn nature of real-world conversations. In this work, we present Norm-RAG, a retrieval-augmented, agentic framework for nuanced social norm inference in multi-turn dialogues. Norm-RAG models utterance-level attributes including communicative intent, speaker roles, interpersonal framing, and linguistic cues and grounds them in structured normative documentation retrieved via a novel Semantic Chunking approach. This enables interpretable and context-aware reasoning about norm adherence and violation across multilingual dialogues. We further introduce MINDS (Multilingual Interactions with Norm-Driven Speech), a bilingual dataset comprising 31 multi-turn Mandarin-English and Spanish-English conversations. Each turn is annotated for norm category and adherence status using multi-annotator consensus, reflecting cross-cultural and realistic norm expression. Our experiments show that Norm-RAG improves norm detection and generalization, demonstrates improved performance for culturally adaptive and socially intelligent dialogue systems.",
        "subjects": "Computation and Language",
        "date": "2025-11-13",
        "category": "cs.CL",
        "crawl_time": "2025-11-14T11:00:04.298659",
        "filter_reason": "这篇论文符合研究范围，应予以保留。 **判断过程如下:** 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将LLM应用到一个新领域，而是提出了一个名为 **Norm-RAG** 的新框架。摘要明确将其定义为一个 **\"retrieval-augmented, agentic framework\"**（检索增强的智能体框架）。这表明其核心贡献在于构建一个具有特定能力的LLM智能体，而不仅仅是应用。它不属于“非演化型应用”或“非Agentic的推理”的排除范畴。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: 明确提到了 `agentic framework`，直接命中 `Agentic AI` 和 `LLM-based Agents`。 - **智能体能力**: `retrieval-augmented` 是一种典型的 **`Tool Use / Tool Augmentation`**（工具使用）形式。框架的目标是进行 \"nuanced social norm inference\" 和 \"context-aware reasoning\"，这属于智能体的复杂 **`Reasoning`** 能力。 - **单智能体方向**: 该论文聚焦于单个智能体如何通过工具（检索）和推理来完成一项复杂任务（社会规范检测），完全符合“单智能体”的研究方向。 3.  **第三步：排除标准** - **安全与对齐**: 虽然论文主题“社会规范”与AI对齐有潜在关联，但论文的**主要贡献**是提出一个用于**检测**规范的智能体框架，而不是一个关于如何对齐LLM的理论或安全机制。因此，它不属于以安全/对齐为核心贡献的论文，不应被排除。 - **多模态与视觉**: 论文处理的是文本对话，不涉及视觉或多模态内容，不触发此项排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 该论文属于“保留”情况。它研究的是智能体（Norm-RAG）如何在一个复杂任务（多轮对话中的规范推理）中进行规划和推理，而不是提升LLM本身的基础数学或逻辑能力。其框架本身就体现了Agentic的推理过程。 **最终决策:** 综合以上分析，这篇论文的核心贡献是构建了一个名为Norm-RAG的LLM智能体框架。该框架通过使用检索工具来增强其推理能力，以解决一个复杂任务（跨文化对话中的社会规范检测）。这完全符合“构建、改进LLM智能体”的核心目标，特别是在“单智能体”方向下的“工具使用”和“推理”子方向。因此，这篇论文高度相关，应被保留。"
    },
    {
        "index": "#42",
        "title": "REAP: Enhancing RAG with Recursive Evaluation and Adaptive Planning for Multi-Hop Question Answering",
        "link": "/arxiv/2511.09966",
        "arxiv_id": "2511.09966",
        "authors": "Yijie Zhu, Haojie Zhou, Wanting Hong, Tailin Liu, Ning Wang",
        "summary": "Retrieval-augmented generation (RAG) has been extensively employed to mitigate hallucinations in large language models (LLMs). However, existing methods for multi-hop reasoning tasks often lack global planning, increasing the risk of falling into local reasoning impasses. Insufficient exploitation of retrieved content and the neglect of latent clues fail to ensure the accuracy of reasoning outcomes. To overcome these limitations, we propose Recursive Evaluation and Adaptive Planning (REAP), whose core idea is to explicitly maintain structured sub-tasks and facts related to the current task through the Sub-task Planner (SP) and Fact Extractor (FE) modules. SP maintains a global perspective, guiding the overall reasoning direction and evaluating the task state based on the outcomes of FE, enabling dynamic optimization of the task-solving trajectory. FE performs fine-grained analysis over retrieved content to extract reliable answers and clues. These two modules incrementally enrich a logically coherent representation of global knowledge, enhancing the reliability and the traceability of the reasoning process. Furthermore, we propose a unified task paradigm design that enables effective multi-task fine-tuning, significantly enhancing SP's performance on complex, data-scarce tasks. We conduct extensive experiments on multiple public multi-hop datasets, and the results demonstrate that our method significantly outperforms existing RAG methods in both in-domain and out-of-domain settings, validating its effectiveness in complex multi-hop reasoning tasks.",
        "subjects": "Computation and Language",
        "date": "2025-11-13",
        "category": "cs.CL",
        "crawl_time": "2025-11-14T11:00:04.297786",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于构建和改进一个LLM智能体。我的判断依据如下： 1.  **核心判断 (第一步):** - **保留**。这篇论文的本质不是简单地将RAG应用于问答任务，而是提出了一种名为REAP的**新方法论和新框架**来解决现有RAG方法在多跳推理中的缺陷。其核心是构建一个由“子任务规划器”和“事实提取器”组成的系统化架构，这完全符合“构建、改进LLM智能体”的核心目标。它不是非演化型应用，因为它贡献的是智能体本身的架构，而非应用领域的成果。 2.  **正面指标 (第二步):** - 论文包含了多个核心关注点： - **Agentic AI / LLM-based Agents**: REAP框架本身就是一个典型的LLM智能体架构。 - **Planning**: 论文明确提出了“Sub-task Planner (SP)”模块，负责“global planning”（全局规划）、“guiding the overall reasoning direction”（引导整体推理方向）和“dynamic optimization of the task-solving trajectory”（动态优化任务解决轨迹）。这完全命中了“规划”这一核心能力。 - **Memory**: 论文提到两个模块“incrementally enrich a logically coherent representation of global knowledge”（增量地丰富一个逻辑连贯的全局知识表示），这实质上是一种在推理过程中构建和更新记忆的机制。 - **Self-Correction / Self-Reflection**: “Recursive Evaluation and Adaptive Planning”中的“Recursive Evaluation”（递归评估）以及SP模块“evaluating the task state”（评估任务状态）的功能，体现了智能体在执行过程中进行自我评估和动态调整的能力，属于自我反思和修正的范畴。 3.  **排除标准 (第三步):** - 论文的主要贡献是提升多跳推理的准确性和可靠性，而非安全、对齐或多模态。因此，它不触及任何排除标准。 4.  **特殊和模糊情况 (第四步):** - **推理/规划**: 这篇论文是“智能体规划”的绝佳范例。它不是在改进LLM的基础推理能力（如微调模型做数学题），而是在LLM之上构建了一个**规划系统**来引导和控制推理过程，使其能够进行更复杂的多步任务。这完全符合保留条件。 **总结**: 该论文的核心贡献是REAP框架，一个具备**规划**、**记忆**和**自我修正**能力的LLM智能体。它通过模块化设计（SP和FE）实现了对复杂推理任务的全局视角和动态优化，这直接对应了你研究课题中的“单智能体”方向，特别是“规划”和“自我反思”子方向。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#63",
        "title": "AgentEvolver: Towards Efficient Self-Evolving Agent System",
        "link": "/arxiv/2511.10395",
        "arxiv_id": "2511.10395",
        "authors": "Yunpeng Zhai, Shuchang Tao, Cheng Chen, Anni Zou, Ziqian Chen, Qingxu Fu, Shinji Mai, Li Yu, Jiaji Deng, Zouying Cao, Zhaoyang Liu, Bolin Ding, Jingren Zhou",
        "summary": "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-11-13",
        "category": "cs.CL",
        "crawl_time": "2025-11-14T11:00:04.318392",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。判断依据如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为 `AgentEvolver` 的新框架，其核心贡献是构建一个能够“自我演化”的智能体系统。这直接命中了你研究目标的第三个核心方向“自我演化”。论文并非将现有智能体作为工具应用到某个领域，而是专注于改进智能体本身的学习和演化机制，因此不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标** - 论文包含了大量你的核心关注点。标题和摘要中明确出现了 `Self-Evolving Agent System`。 - 论文提出的三个核心机制——`self-questioning`（自我提问）、`self-navigating`（自我导航）和 `self-attributing`（自我归因）——都属于 `Self-Improvement` 和 `Iterative Improvement` 的范畴，是“自我演化”的具体实现方式。 - 摘要中提到的 `reasoning`, `tool use`, `executing complex tasks` 也与“单智能体”的能力相关，表明其研究建立在Agentic AI的基础之上。 3.  **第三步：排除标准** - 论文的主要贡献是关于智能体的效率和演化机制，没有涉及 `Safety`, `Alignment`, `Interpretability` 等安全与对齐问题。 - 论文也未提及 `Vision`, `MLLMs` 等多模态内容，因此不触发排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化”方向的典型范例。它不仅应用了演化机制，更是**提出了一种新的自我演化方法论**（即三个“self-”机制）。根据你的规则，即使它被应用在特定领域，只要核心是新的演化机制就应保留。而本文的核心就是提出这个机制本身，因此完全符合要求。 **最终决策**: 该论文的核心贡献在于构建了一个新颖的、高效的LLM智能体自我演化框架 `AgentEvolver`，并提出了具体的协同机制来实现智能体的自主学习和持续改进。这与你研究课题中的“自我演化”方向高度契合，且属于前沿的方法论研究，因此应被筛选为“True”。"
    },
    {
        "index": "#64",
        "title": "Simulating Misinformation Propagation in Social Networks using Large Language Models",
        "link": "/arxiv/2511.10384",
        "arxiv_id": "2511.10384",
        "authors": "Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat",
        "summary": "Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.",
        "subjects": "Social and Information Networks, Artificial Intelligence, Computation and Language, Computers and Society",
        "date": "2025-11-13",
        "category": "cs.CL",
        "crawl_time": "2025-11-14T11:00:04.318940",
        "filter_reason": "这篇论文符合你的研究范围，核心依据如下： 1.  **第一步：核心判断——保留** - **论文的本质**: 这篇论文的核心贡献并非简单地使用LLM作为工具去解决虚假信息问题，而是**构建了一个新颖的多智能体框架**。该框架将具有不同人设的LLM实例化为“synthetic agents”，并让它们在模拟的社交网络中进行交互和信息传播。这完全符合“构建、改进或演化 LLM智能体”中的“构建”和“多智能体系统”的定义。 - **非演化型应用的排除**: 虽然论文的应用领域是“虚假信息传播”，但其核心贡献是方法论层面的——即提出并验证了一个**用于模拟的智能体框架**，而不是应用已有框架得出领域结论。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标——高度相关** - 论文明确包含了多个核心关注点： - **核心范式**: `LLM-based Agents` (LLM personas as synthetic agents), `Multi-Agent Systems (MAS)` (networks of such agents)。 - **多智能体**: `Communication` (信息在智能体间传播), `Agent Society` (模拟具有不同意识形态和身份的智能体社会)。论文研究的正是智能体间的交互如何导致宏观现象（虚假信息加速）。 3.  **第三步：排除标准——未触发** - **安全与对齐**: 尽管论文研究的是“虚假信息”，这是一个与安全相关的议题，但论文的**主要贡献**是提出一个**模拟框架**来理解和分析这一现象，而不是提出一种新的安全、对齐或防御技术。它的目标是“研究”和“模拟”，而非“对齐”或“防御”，因此不属于被排除的类别。 - **多模态与视觉**: 论文完全基于文本，不涉及视觉或多模态内容。 4.  **第四步：处理特殊和模糊情况** - **自我演化的辨析**: 论文中提到的“evolves”是指“misinformation evolves”（虚假信息在传播过程中演化），而不是“agents self-evolve”（智能体自我演化）。智能体本身的人设和能力是固定的。但这并不影响其符合“多智能体”的核心研究方向。你的研究范围包含“多智能体”，因此这篇论文是高度相关的。 **最终决策**: 这篇论文的核心贡献在于**构建了一个由LLM驱动的多智能体系统框架**，用于模拟和分析复杂的社会现象。它直接命中了你研究焦点中的“多智能体”方向，深入探讨了智能体间的通信、交互以及由此产生的涌现行为。因此，这篇论文完全符合你的筛选要求，应该被保留。"
    },
    {
        "index": "#68",
        "title": "ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs",
        "link": "/arxiv/2511.10240",
        "arxiv_id": "2511.10240",
        "authors": "Minbae Park, Hyemin Yang, Jeonghyun Kim, Kunsoo Park, Hyunjoon Kim",
        "summary": "Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-11-13",
        "category": "cs.CL",
        "crawl_time": "2025-11-14T11:00:04.320908",
        "filter_reason": "这篇论文符合你的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出了一个名为 `ProgRAG` 的新框架。这个框架的本质不是简单地将LLM应用于知识图谱问答（KGQA）领域，而是构建了一个具有特定工作流程的**LLM智能体**。该智能体通过“将复杂问题分解为子问题”、“使用外部检索器（工具使用）”、“逐步扩展推理路径”以及“通过不确定性感知剪枝进行优化”等一系列步骤来完成复杂任务。这完全符合“构建、改进LLM智能体的方法论或新框架”的保留标准。它不是一个非演化型的应用，而是一个新的Agentic框架。 2.  **第二步：正面指标——高度相关** 论文包含了多个核心关注点： *   **智能体能力**: 明确涉及了 `Planning`（规划，即问题分解）、`Tool Use`（工具使用，即调用外部检索器）。其“逐步扩展推理路径”和“不确定性感知剪枝”也体现了多步推理和一种形式的自我修正/优化。 *   **核心范式**: `ProgRAG` 框架本身就是一个 `LLM-based Agent` 的实现。其工作流程（推理->行动->观察）与 `ReAct` 等核心范式高度相似。 3.  **第三步：排除标准——不适用** *   **安全与对齐**: 论文标题和摘要中提到了 \"Hallucination-Resistant\"（抗幻觉）。根据筛选规则，如果论文的**主要贡献**是关于幻觉本身，则应排除。但在这篇论文中，减少幻觉是其提出的 `ProgRAG` 框架所带来的**结果或优势**，而非研究的核心。论文的核心是**如何构建一个更好的规划和推理框架**，这个框架恰好能提高可靠性并减少幻觉。因此，它不应被此规则排除。 *   **多模态与视觉**: 论文聚焦于知识图谱，不涉及多模态内容。 4.  **第四步：处理特殊和模糊情况——符合保留条件** *   **推理/规划**: 这篇论文是“智能体如何进行规划或在复杂任务中进行多步推理”的典型范例。它不是在改进LLM的基础Token预测能力，而是在构建一个外部的、结构化的智能体框架来引导LLM完成复杂推理。因此，它完全符合保留条件。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于构建了一个名为 `ProgRAG` 的LLM智能体框架，该框架集成了规划、工具使用和多步迭代优化等关键能力，旨在解决复杂知识图谱问答问题。其研究焦点完全落在“单智能体”方向，特别是规划和推理机制的构建上。虽然它提到了“抗幻觉”，但这只是其框架的副产品，而非研究主题本身。因此，这篇论文与你的研究目标“构建、改进或演化LLM智能体”高度契合。"
    },
    {
        "index": "#69",
        "title": "Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning",
        "link": "/arxiv/2511.10067",
        "arxiv_id": "2511.10067",
        "authors": "Yuxuan Zhou, Yubin Wang, Bin Wang, Chen Ning, Xien Liu, Ji Wu, Jianye Hao",
        "summary": "Large language models (LLMs) have shown great promise in the medical domain, achieving strong performance on several benchmarks. However, they continue to underperform in real-world medical scenarios, which often demand stronger context-awareness, i.e., the ability to recognize missing or critical details (e.g., user identity, medical history, risk factors) and provide safe, helpful, and contextually appropriate responses. To address this issue, we propose Multifaceted Self-Refinement (MuSeR), a data-driven approach that enhances LLMs' context-awareness along three key facets (decision-making, communication, and safety) through self-evaluation and refinement. Specifically, we first design a attribute-conditioned query generator that simulates diverse real-world user contexts by varying attributes such as role, geographic region, intent, and degree of information ambiguity. An LLM then responds to these queries, self-evaluates its answers along three key facets, and refines its responses to better align with the requirements of each facet. Finally, the queries and refined responses are used for supervised fine-tuning to reinforce the model's context-awareness ability. Evaluation results on the latest HealthBench dataset demonstrate that our method significantly improves LLM performance across multiple aspects, with particularly notable gains in the context-awareness axis. Furthermore, by incorporating knowledge distillation with the proposed method, the performance of a smaller backbone LLM (e.g., Qwen3-32B) surpasses its teacher model, achieving a new SOTA across all open-source LLMs on HealthBench (63.8%) and its hard subset (43.1%). Code and dataset will be released at https://muser-llm.github.io.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-11-13",
        "category": "cs.CL",
        "crawl_time": "2025-11-14T11:00:04.321391",
        "filter_reason": "这篇论文符合我的研究范围，核心依据在于其贡献属于“自我演化”方向。 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将LLM应用于医疗领域，而是提出了一种名为“多方面自我优化”的新方法论。该方法论的核心是让LLM通过“自我评估”和“自我优化”的循环来提升自身能力。这完全符合“自我演化”的定义，即“智能体通过经验、反思或环境反馈进行自我完善和迭代”。因此，它不是“非演化型应用”，而是一种新的演化机制。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点：`Self-Evolving`（自我演化）、`Self-Reflection`（自我评估）、`Self-Refine`（自我优化）、`Self-Improvement`（自我完善）。这些关键词和概念直接命中了我的研究焦点。 3.  **第三步：排除标准** - 论文提到了“安全”作为优化的一个方面，但这并非其主要贡献。论文的核心是提出一个通用的“自我优化”框架，而“安全”只是该框架在医疗应用场景下需要考量的一个维度。因此，这篇论文不是一篇关于“安全与对齐”的研究，不应因此被排除。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**：这是本案例的关键。论文虽然应用在医疗领域，但其核心贡献是提出了一种新的“自我演化”机制。根据筛选规则中的例外条款：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。” 这篇论文完美地符合了这一例外情况。 **最终决策**：综合以上分析，该论文的核心贡献是构建了一个让LLM通过自我反思和迭代进行自我完善的框架。这直接对应了我研究课题中的“自我演化”方向。尽管它以医疗领域为应用背景，但其提出的方法论具有通用性，且是论文的创新点所在。因此，这篇论文应该被保留。"
    },
    {
        "index": "#26",
        "title": "Heuristic Transformer: Belief Augmented In-Context Reinforcement Learning",
        "link": "/arxiv/2511.10251",
        "arxiv_id": "2511.10251",
        "authors": "Oliver Dippel, Alexei Lisitsa, Bei Peng",
        "summary": "Transformers have demonstrated exceptional in-context learning (ICL) capabilities, enabling applications across natural language processing, computer vision, and sequential decision-making. In reinforcement learning, ICL reframes learning as a supervised problem, facilitating task adaptation without parameter updates. Building on prior work leveraging transformers for sequential decision-making, we propose Heuristic Transformer (HT), an in-context reinforcement learning (ICRL) approach that augments the in-context dataset with a belief distribution over rewards to achieve better decision-making. Using a variational auto-encoder (VAE), a low-dimensional stochastic variable is learned to represent the posterior distribution over rewards, which is incorporated alongside an in-context dataset and query states as prompt to the transformer policy. We assess the performance of HT across the Darkroom, Miniworld, and MuJoCo environments, showing that it consistently surpasses comparable baselines in terms of both effectiveness and generalization. Our method presents a promising direction to bridge the gap between belief-based augmentations and transformer-based decision-making.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-11-13",
        "category": "cs.LG",
        "crawl_time": "2025-11-14T11:00:04.728892",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。我的判断依据如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献是提出了一种名为“Heuristic Transformer (HT)”的新方法/框架，用于改进强化学习中的决策制定。它不是一个简单的应用，而是构建了一个新的智能体决策框架。该框架使用Transformer作为策略网络，并通过引入“信念分布”来增强其上下文学习能力，从而提升决策效果。这完全符合“构建、改进LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文虽然未直接使用“LLM”一词，但其核心是“Transformer”和“In-Context Learning (ICL)”。在当前AI研究中，基于Transformer的上下文学习是LLM智能体的基石。该论文将这一范式应用于序列决策，本质上是在构建一个具有LLM核心学习能力的智能体。 - **Agentic AI**: 论文明确提出了一个用于“sequential decision-making”的“transformer policy”，这正是一个智能体的核心。 - **Memory**: 论文中的“in-context dataset”充当了智能体的短期记忆，为其决策提供历史信息。 - **Planning**: “sequential decision-making”本身就隐含了规划和多步推理的过程。论文的目标是让智能体在复杂环境中做出更好的决策序列，这与规划能力直接相关。 3.  **第三步：排除标准** - 论文不涉及安全、对齐、可解释性等排除主题。 - 论文在MuJoCo等环境中进行测试，这些环境可能涉及视觉输入，但论文的核心贡献并非视觉或多模态模型本身，而是决策制定的机制。视觉信息只是作为环境状态的一部分被输入，而非研究焦点，因此不触犯排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是典型的“保留”情况。它研究的是智能体如何在环境中进行“sequential decision-making”，这属于智能体规划和推理的范畴，而不是提升LLM本身的基础数学或逻辑能力。 - **LLM智能体的界定**: 这是本案例的关键。虽然论文没有使用一个预训练的语言模型（如GPT系列），但它使用了Transformer架构和In-Context Learning机制。这使其与LLM智能体在底层原理上高度一致。我的研究焦点是“Agentic AI”，特别是其方法论。这篇论文提出了一种新颖的、基于Transformer上下文学习的智能体构建方法，这完全属于我关注的“单智能体”方向下的“规划”和“记忆”子方向。将其视为LLM智能体研究的一个前沿分支是合理的。 **最终决策**: 综合以上分析，该论文的核心贡献在于提出了一种新的智能体框架（Heuristic Transformer），它通过信念增强机制，显著提升了基于Transformer的智能体在强化学习任务中的规划和决策能力。这直接命中了我研究范围中的“单智能体”方向，特别是其规划和记忆能力。因此，这篇论文高度相关，应被保留。"
    },
    {
        "index": "#54",
        "title": "EEGAgent: A Unified Framework for Automated EEG Analysis Using Large Language Models",
        "link": "/arxiv/2511.09947",
        "arxiv_id": "2511.09947",
        "authors": "Sha Zhao, Mingyi Peng, Haiteng Jiang, Tao Li, Shijian Li, Gang Pan",
        "summary": "Scalable and generalizable analysis of brain activity is essential for advancing both clinical diagnostics and cognitive research. Electroencephalography (EEG), a non-invasive modality with high temporal resolution, has been widely used for brain states analysis. However, most existing EEG models are usually tailored for individual specific tasks, limiting their utility in realistic scenarios where EEG analysis often involves multi-task and continuous reasoning. In this work, we introduce EEGAgent, a general-purpose framework that leverages large language models (LLMs) to schedule and plan multiple tools to automatically complete EEG-related tasks. EEGAgent is capable of performing the key functions: EEG basic information perception, spatiotemporal EEG exploration, EEG event detection, interaction with users, and EEG report generation. To realize these capabilities, we design a toolbox composed of different tools for EEG preprocessing, feature extraction, event detection, etc. These capabilities were evaluated on public datasets, and our EEGAgent can support flexible and interpretable EEG analysis, highlighting its potential for real-world clinical applications.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-11-13",
        "category": "cs.LG",
        "crawl_time": "2025-11-14T11:00:04.752012",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。我的判断依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献是提出了一个名为 **EEGAgent** 的**通用框架**。这个框架的本质是利用LLM来**调度和规划多个工具**，以自动完成复杂的EEG分析任务。这完全符合“构建LLM智能体”的定义。它不是简单地将LLM作为一个黑盒工具应用于EEG领域，而是**设计了一个以LLM为核心决策者的智能体架构**。因此，它不属于“非演化型应用”的排除范畴，而应归类为“构建LLM智能体的方法论或新框架”，应予以**保留**。 2.  **正面指标 (第二步):** 论文明确包含了多个我的核心关注点： *   **核心范式**: 论文构建了一个典型的 `LLM-based Agent` 系统。 *   **智能体能力**: 摘要中明确指出，该框架的核心是让LLM进行 **`schedule and plan` (规划)**，并使用一个由多种工具组成的 **`toolbox` (工具使用)**。这与ReAct等智能体范式高度一致，即通过推理来决定调用哪个工具，并根据工具返回的结果进行下一步行动。 3.  **排除标准 (第三步):** *   **安全与对齐**: 论文虽然提到了“interpretable”（可解释），但这只是其框架带来的一个优点，并非论文的核心研究贡献。论文的主要目标是构建一个高效的自动化分析框架，而不是研究可解释性、安全性或对齐问题。 *   **多模态与视觉**: 论文处理的是EEG时序信号，不涉及视觉或多模态模型作为研究核心。 4.  **特殊和模糊情况 (第四步):** *   **推理/规划**: 这篇论文是关于智能体如何进行规划和多步推理的典型案例。LLM作为“大脑”，分析任务需求，规划出一系列工具调用（如预处理、特征提取、事件检测），最终生成报告。这完全符合“保留”标准，因为它研究的是智能体的规划框架，而非LLM本身的基础推理能力。 **最终决策 (第五步):** 综合来看，尽管这篇论文的应用领域是EEG分析（一个特定领域），但其**核心贡献在于提出了一种新颖的LLM智能体框架**，该框架展示了如何通过规划和工具使用来解决复杂的多步骤任务。这直接命中了我的研究目标——“构建、改进或演化LLM智能体”。它属于“单智能体”方向下的“规划”和“工具使用”子方向。因此，这篇论文与我的研究课题高度相关，应被筛选出来。"
    },
    {
        "index": "#95",
        "title": "Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey",
        "link": "/arxiv/2511.09586",
        "arxiv_id": "2511.09586",
        "authors": "Yuchen Huang, Sijia Li, Minghao Liu, Wei Liu, Shijue Huang, Zhiyuan Fan, Hou Pong Chan, Yi R. Fung",
        "summary": "LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze benchmarks, implementation strategies, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-11-12",
        "category": "cs.LG",
        "crawl_time": "2025-11-14T11:00:04.791491",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献与你的研究目标高度契合。以下是我的详细判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是一篇综述，但它的核心贡献并非简单罗列应用，而是**提出并形式化了一个关于LLM智能体如何通过交互进行学习和演化的核心范式——Generation-Execution-Feedback (GEF) loop（生成-执行-反馈循环）**。这个GEF循环直接对应了你研究目标中的“自我演化”方向。论文的核心论点是：为了让智能体实现自适应行为和长期决策，必须让它们在与环境的交互中学习，而环境是产生这种经验数据的关键。因此，这篇论文是在探讨**如何构建和演化LLM智能体的基础方法论和框架**，而不是将智能体作为工具应用到某个领域。它不属于“非演化型应用”、“非Agentic的推理”或“基础设施”等排除类别。 2.  **第二步：正面指标** - 论文摘要中包含了大量你的核心关注点： - **核心范式**: `LLM-based Agents` (明确提及), `Self-Evolving` (通过 \"learn from experience through reinforcement learning\" 和 \"iterative process\" 体现)。 - **演化机制**: `Self-Improvement` (明确提及 \"learn from experience\"), `Iterative Improvement` (GEF循环本身就是迭代改进的机制)。 - **智能体能力**: 论文讨论的 `task execution` 和 `long-term decision-making` 都与智能体的规划和执行能力紧密相关。 - 论文提出的GEF循环（任务生成、执行、反馈）是一个高度相关的框架，它系统地描述了智能体自我演化的完整流程。 3.  **第三步：排除标准** - 论文的主要贡献**不涉及**安全、对齐、可解释性或多模态等排除焦点。它的核心是智能体的学习和演化机制，因此完全避开了这些排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文讨论的“task execution”和“long-term decision-making”是在智能体与环境交互的框架下进行的，属于Agentic AI的范畴，而非提升LLM本身的基础推理能力。因此，符合保留条件。 - **自我演化的应用**: 这篇论文比“自我演化的应用”更根本，它是在**定义和综述“自我演化”这一机制本身**，并指出环境是实现该机制的关键。这完全符合你的研究核心。 **最终决策**: 这篇论文虽然是一篇综述，但其视角和贡献非常前沿和深刻。它没有停留在描述单个智能体，而是从“环境”这一更高维度，系统性地梳理和形式化了LLM智能体实现“自我演化”的核心范式（GEF loop）。这直接命中了你研究目标的第三个方向“自我演化”，并为理解单智能体和多智能体的能力发展提供了基础理论框架。因此，这篇论文对于你把握“LLM智能体及其演化”领域的前沿动态和未来方向具有极高的价值，**必须保留**。"
    },
    {
        "index": "#15",
        "title": "Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware",
        "link": "/arxiv/2511.10277",
        "arxiv_id": "2511.10277",
        "authors": "Martin Braas, Lukas Esterle",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems.",
        "subjects": "Artificial Intelligence, Information Retrieval",
        "date": "2025-11-13",
        "category": "cs.AI",
        "crawl_time": "2025-11-14T11:00:04.804268",
        "filter_reason": "这篇论文符合你的研究范围，核心判断依据如下： 1.  **第一步：核心判断——保留** - **论文的核心贡献**：这篇论文的本质并非简单地将LLM应用于游戏领域，而是提出了一种**新的智能体架构**，即“带有模块化记忆的NPC对话系统”。其核心创新点在于“运行时可交换的记忆模块”，这是一种用于构建和改进LLM智能体的方法论。 - **为何不是“非演化型应用”**：虽然论文的应用场景是游戏NPC，但它没有使用一个已有的智能体框架去解决领域问题。相反，它**创造了一个新的架构组件（模块化记忆）**来赋予智能体（NPC）长期记忆能力。这属于对智能体本身的构建和改进，符合筛选标准中的“保留”条件。 2.  **第二步：正面指标——高度相关** - **智能体能力**：论文的核心贡献“模块化记忆”直接命中了你研究焦点中的**单智能体**方向下的关键能力——**`Memory`**。它解决了智能体在长期交互中维持上下文和世界知识的问题，这是实现高级智能体行为（如规划、反思）的基础。 - **核心范式**：论文构建的是一个典型的**`LLM-based Agent`**，其设计目标是实现具有特定人设和记忆的对话智能体。 3.  **第三步：排除标准——未触发** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态。虽然提到了在消费级硬件上运行，但这只是对其系统**可扩展性**的评估，而非研究本身的核心贡献（即不是关于基础设施或硬件加速的研究）。 4.  **第四步：处理特殊和模糊情况** - **应用与核心贡献的区分**：这是本案例的关键。根据你的筛选规则，即使论文应用于特定领域（游戏），只要其核心贡献是提出一种新的智能体构建或改进方法，就应该保留。本文的“模块化记忆架构”正是这样一种方法，它具有通用性，作者也提到了其在虚拟助手、客服机器人等领域的潜力。因此，它超越了单纯的应用范畴。 **最终决策**： 综合分析，这篇论文的核心贡献在于提出了一种新颖的、模块化的记忆架构，用以构建和增强LLM智能体的长期记忆能力。这完全符合你“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的核心目标，并且精准地命中了你研究焦点中“单智能体”的“记忆”子方向。因此，应予以保留。"
    },
    {
        "index": "#33",
        "title": "Beyond ReAct: A Planner-Centric Framework for Complex Tool-Augmented LLM Reasoning",
        "link": "/arxiv/2511.10037",
        "arxiv_id": "2511.10037",
        "authors": "Xiaolong Wei, Yuehu Dong, Xingliang Wang, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin",
        "summary": "Existing tool-augmented large language models (LLMs) encounter significant challenges when processing complex queries. Current frameworks such as ReAct are prone to local optimization traps due to their reliance on incremental decision-making processes. To address these limitations, we propose a novel Planner-centric Plan-Execute paradigm that fundamentally resolves local optimization bottlenecks through architectural innovation. Central to our approach is a novel Planner model that performs global Directed Acyclic Graph (DAG) planning for complex queries, enabling optimized execution beyond conventional tool coordination. We also introduce ComplexTool-Plan, a large-scale benchmark dataset featuring complex queries that demand sophisticated multi-tool composition and coordination capabilities. Additionally, we develop a two-stage training methodology that integrates Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), systematically enhancing the Planner's tool selection accuracy and global planning awareness through structured DAG-based planning. When integrated with a capable executor, our framework achieves state-of-the-art performance on the StableToolBench benchmark for complex user queries, demonstrating superior end-to-end execution capabilities and robust handling of intricate multi-tool workflows.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-13",
        "category": "cs.AI",
        "crawl_time": "2025-11-14T11:00:04.813027",
        "filter_reason": "这篇论文完全符合你的研究范围，应被保留。 **判断过程如下:** 1.  **第一步：核心判断——保留** 论文的核心贡献是提出了一种全新的、以规划器为中心的“Plan-Execute”智能体框架，用于解决现有工具增强LLM（如ReAct）在复杂任务中遇到的局部优化问题。这直接命中了你筛选标准的第一步“保留”条件：**论文的核心是关于构建、改进LLM智能体的方法论或新框架**。它不是将现有智能体作为工具去解决某个特定领域的问题，而是对智能体本身的架构和规划能力进行创新。 2.  **第二步：正面指标——高度匹配** 论文包含了大量你关注的核心正面指标： *   **核心范式**: `Agentic AI`, `LLM-based Agents`。 *   **智能体能力**: 论文的主题就是 `Planning`（提出了全局DAG规划）和 `Tool Use / Tool Augmentation`（解决了复杂多工具协调问题）。它还与 `ReAct` 这一经典范式进行对比，表明其研究深度。 这些指标表明，论文的研究内容与你的“单智能体”方向下的“规划”和“工具使用”子方向高度契合。 3.  **第三步：排除标准——未触发** 论文的研究焦点是智能体的架构和规划效率，完全不涉及安全与对齐、多模态与视觉等排除标准。 4.  **第四步：处理特殊和模糊情况——符合保留规则** 这篇论文是“推理/规划”特殊情况的完美范例。它不是在提升LLM的基础数学或逻辑推理能力，而是在**构建一个让智能体能够进行更优规划的框架**。其提出的“Planner模型”和“DAG规划”是典型的智能体层面的创新，旨在解决多步、多工具的复杂任务规划问题，完全符合“保留”标准。 **最终决策:** 综合以上分析，这篇论文的核心贡献在于**构建和改进LLM智能体的规划与工具使用能力**，属于“单智能体”研究范畴的前沿探索。它提出的新框架、新模型和新训练方法，直接推动了LLM智能体在处理复杂任务时能力的演化。因此，这篇论文不仅符合，而且是高度相关的前沿研究，为你的“LLM智能体及其演化”课题提供了关于智能体规划架构创新的重要参考。"
    },
    {
        "index": "#38",
        "title": "Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models",
        "link": "/arxiv/2511.09907",
        "arxiv_id": "2511.09907",
        "authors": "Yongxian Wei, Yilin Zhao, Li Shen, Xinrui Chen, Runxi Cheng, Sinan Du, Hao Yu, Gang Liu, Jiahong Yan, Chun Yuan, Dian Li",
        "summary": "Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-11-13",
        "category": "cs.AI",
        "crawl_time": "2025-11-14T11:00:04.814543",
        "filter_reason": "这篇论文的核心贡献是提出了一种**问题生成器与求解器共同演化的框架**，这完全符合您的研究范围，特别是“自我演化”方向。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质并非简单地将LLM作为工具应用，也不是提升LLM的基础推理能力。它的核心贡献在于构建了一个由“问题生成器”和“求解器”组成的**交互式、自适应的系统**。这个系统通过反馈循环进行迭代和优化，其方法论本身就是关于智能体（或类智能体组件）如何演化的研究。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点： - **自我演化**: 摘要中直接使用了 **\"co-evolution\"（共同演化）** 这一关键术语，并描述了生成器和求解器相互促进、共同提升性能的机制。这直接命中了“自我演化”的核心。 - **演化机制**: 论文描述了 **\"Iterative Improvement\"（迭代改进）** 的过程，即求解器的反馈作为奖励信号来校准生成器，而改进后的生成器又能产生更好的数据来训练求解器。 - **智能体能力**: “问题生成器”展现了类似智能体的能力。它 **\"reasons explicitly to plan problem directions\"（显式推理以规划问题方向）**，这符合“规划”和“推理”的定义。它还 **\"adapts difficulty to the solver's ability\"（根据求解器的能力调整难度）**，这是一种高级的适应和交互行为。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性等排除项。 - 虽然论文提到其方法可以泛化到“视觉语言模型”，但视觉在这里是作为求解器处理的一种模态，并非研究的核心。研究的核心是那个**共同演化的数据合成框架**，而不是视觉技术本身。因此，这符合“除非它们被用作智能体感知环境的工具，而不是研究的核心”的例外情况。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一规则的完美范例。尽管它被应用于数学和推理任务（一个特定领域），但其**核心贡献是提出了一种新的“自我演化”机制**。因此，根据您的规则，它应该被保留。 - **推理/规划**: 论文中的推理是服务于“问题生成器”这一智能体组件的规划和决策过程，而不是孤立地提升LLM的数学能力。因此，它属于“保留”的范畴。 **最终决策**: 综合以上分析，这篇论文的核心是构建一个由两个LLM驱动的组件（生成器和求解器）组成的系统，该系统能够通过反馈和奖励信号进行**共同演化**。这完全符合您对“自我演化”方向的定义，即“智能体通过经验、反思或环境反馈进行自我完善和迭代”。因此，这篇论文与您的研究课题高度相关，应该被保留。"
    },
    {
        "index": "#45",
        "title": "SlideBot: A Multi-Agent Framework for Generating Informative, Reliable, Multi-Modal Presentations",
        "link": "/arxiv/2511.09804",
        "arxiv_id": "2511.09804",
        "authors": "Eric Xie, Danielle Waterfield, Michael Kennedy, Aidong Zhang",
        "summary": "Large Language Models (LLMs) have shown immense potential in education, automating tasks like quiz generation and content summarization. However, generating effective presentation slides introduces unique challenges due to the complexity of multimodal content creation and the need for precise, domain-specific information. Existing LLM-based solutions often fail to produce reliable and informative outputs, limiting their educational value. To address these limitations, we introduce SlideBot - a modular, multi-agent slide generation framework that integrates LLMs with retrieval, structured planning, and code generation. SlideBot is organized around three pillars: informativeness, ensuring deep and contextually grounded content; reliability, achieved by incorporating external sources through retrieval; and practicality, which enables customization and iterative feedback through instructor collaboration. It incorporates evidence-based instructional design principles from Cognitive Load Theory (CLT) and the Cognitive Theory of Multimedia Learning (CTML), using structured planning to manage intrinsic load and consistent visual macros to reduce extraneous load and enhance dual-channel learning. Within the system, specialized agents collaboratively retrieve information, summarize content, generate figures, and format slides using LaTeX, aligning outputs with instructor preferences through interactive refinement. Evaluations from domain experts and students in AI and biomedical education show that SlideBot consistently enhances conceptual accuracy, clarity, and instructional value. These findings demonstrate SlideBot's potential to streamline slide preparation while ensuring accuracy, relevance, and adaptability in higher education.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-12",
        "category": "cs.AI",
        "crawl_time": "2025-11-14T11:00:04.816598",
        "filter_reason": "这篇论文符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献并非简单地将LLM应用于教育领域，而是提出了一个名为 **SlideBot 的“模块化、多智能体幻灯片生成框架”**。摘要明确指出，该框架通过“专业化智能体协作地检索信息、总结内容、生成图表和格式化幻灯片”来完成任务。这表明论文的本质是**构建一个新的多智能体系统（Multi-Agent Systems）**，其方法论和框架设计是核心创新点，而非仅仅是一个应用。因此，它通过了第一步的核心判断，不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标——高度相关** 论文包含了多个你关注的核心正面指标： *   **核心范式**: 论文标题和摘要中明确提到了 `Multi-Agent Framework`，直接命中你的研究焦点。 *   **多智能体**: 摘要中详细描述了 `specialized agents collaboratively`（专业化智能体协作），这直接对应了多智能体方向中的 `Collaboration`（协作）子方向。 *   **智能体能力**: 框架集成了 `retrieval`（检索）、`structured planning`（结构化规划）和 `code generation`（代码生成），这些都是智能体关键能力的体现，分别对应 `Tool Use`（工具使用）和 `Planning`（规划）。 3.  **第三步：排除标准——未触发** *   论文的主要贡献不是关于安全、对齐或可解释性，因此未触发相关的排除标准。 *   虽然标题和摘要提到了 `Multi-Modal`（多模态）和生成图表，但其核心是**如何通过多智能体协作来生成这些内容**，而不是提出一个新的视觉或多模态模型本身。视觉内容的生成是智能体使用工具（代码生成）的结果，符合“除非它们被用作智能体感知环境的工具，而不是研究的核心”这一例外情况。因此，这不构成排除理由。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文明确提到了 `structured planning`（结构化规划）作为其框架的关键组成部分，用于管理任务复杂性。这属于“智能体如何进行规划或在复杂任务中进行多步推理”的范畴，因此应该保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于**构建了一个新颖的多智能体协作框架**来解决复杂的多模态内容生成任务。它详细阐述了智能体之间的分工、协作方式以及所使用的工具（检索、规划、代码生成），完全符合你研究课题中“多智能体”方向的核心目标。尽管其应用场景是教育领域的幻灯片生成，但论文的价值在于其提出的Agentic框架和方法论，而非应用本身。因此，这篇论文与你的研究高度相关，应被筛选入内。"
    },
    {
        "index": "#47",
        "title": "AI Annotation Orchestration: Evaluating LLM verifiers to Improve the Quality of LLM Annotations in Learning Analytics",
        "link": "/arxiv/2511.09785",
        "arxiv_id": "2511.09785",
        "authors": "Bakhtawar Ahtisham, Kirk Vanacore, Jinsook Lee, Zhuqian Zhou, Doug Pietrzak, Rene F. Kizilcec",
        "summary": "Large Language Models (LLMs) are increasingly used to annotate learning interactions, yet concerns about reliability limit their utility. We test whether verification-oriented orchestration-prompting models to check their own labels (self-verification) or audit one another (cross-verification)-improves qualitative coding of tutoring discourse. Using transcripts from 30 one-to-one math sessions, we compare three production LLMs (GPT, Claude, Gemini) under three conditions: unverified annotation, self-verification, and cross-verification across all orchestration configurations. Outputs are benchmarked against a blinded, disagreement-focused human adjudication using Cohen's kappa. Overall, orchestration yields a 58 percent improvement in kappa. Self-verification nearly doubles agreement relative to unverified baselines, with the largest gains for challenging tutor moves. Cross-verification achieves a 37 percent improvement on average, with pair- and construct-dependent effects: some verifier-annotator pairs exceed self-verification, while others reduce alignment, reflecting differences in verifier strictness. We contribute: (1) a flexible orchestration framework instantiating control, self-, and cross-verification; (2) an empirical comparison across frontier LLMs on authentic tutoring data with blinded human \"gold\" labels; and (3) a concise notation, verifier(annotator) (e.g., Gemini(GPT) or Claude(Claude)), to standardize reporting and make directional effects explicit for replication. Results position verification as a principled design lever for reliable, scalable LLM-assisted annotation in Learning Analytics.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-12",
        "category": "cs.AI",
        "crawl_time": "2025-11-14T11:00:04.817145",
        "filter_reason": "这篇论文符合我的研究范围，应该被保留。判断依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献并非简单地将LLM作为工具应用于“学习分析”这一特定领域，而是提出了一种名为“AI Annotation Orchestration”的**新框架和方法论**。这个框架的核心是让LLM进行自我验证或相互验证，以提升其输出质量。这本质上是在构建和改进一个具有特定能力的LLM系统，因此符合“保留”标准，而非“非演化型应用”。 2.  **正面指标 (第二步):** 论文的研究内容与我的核心关注点高度契合，包含了多个关键正面指标： *   **单智能体:** 论文中的 `self-verification`（自我验证）机制，直接对应了智能体的 `Self-Correction`（自我纠正）或 `Self-Reflection`（自我反思）能力，这是单智能体研究的核心方向之一。 *   **多智能体:** 论文中的 `cross-verification`（交叉验证）机制，涉及多个LLM智能体相互审计和协作，这完全属于 `Multi-Agent Systems` 中的 `Collaboration`（协作）和 `Communication`（通信）范畴。 *   **自我演化:** 整个验证框架旨在通过反馈（来自自身或其他智能体）来迭代和改进LLM的输出质量，这是一种 `Self-Improvement`（自我完善）或 `Iterative Improvement`（迭代改进）的机制，是自我演化研究的关键组成部分。 3.  **排除标准 (第三步):** 论文的主要贡献不涉及安全、对齐、可解释性或多模态等排除领域。它的焦点是提升任务性能的机制设计，而非安全性或模型基础能力。 4.  **特殊和模糊情况处理 (第四步):** 这篇论文是“自我演化的应用”这一特殊情况的典型例子。虽然论文的应用场景是“学习分析”，但其**核心贡献是提出了一种新的“自我完善”和“多智能体协作”的机制**。根据筛选规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。” 本文提出的验证框架正是这样一种机制，因此应当被保留。 **最终决策 (第五步):** 综合以上分析，该论文的核心贡献在于构建了一个融合了自我反思、多智能体协作和迭代改进机制的LLM框架。这直接对应了我研究课题中的“单智能体”、“多智能体”和“自我演化”三个核心方向。因此，这篇论文完全符合筛选要求。"
    },
    {
        "index": "#49",
        "title": "Echoing: Identity Failures when LLM Agents Talk to Each Other",
        "link": "/arxiv/2511.09710",
        "arxiv_id": "2511.09710",
        "authors": "Sarath Shekkizhar, Romain Cosentino, Adam Earle, Silvio Savarese",
        "summary": "As large language model (LLM) based agents interact autonomously with one another, a new class of failures emerges that cannot be predicted from single agent performance: behavioral drifts in agent-agent conversations (AxA). Unlike human-agent interactions, where humans ground and steer conversations, AxA lacks such stabilizing signals, making these failures unique. We investigate one such failure, echoing, where agents abandon their assigned roles and instead mirror their conversational partners, undermining their intended objectives. Through experiments across $60$ AxA configurations, $3$ domains, and $2000+$ conversations, we demonstrate that echoing occurs across three major LLM providers, with echoing rates from $5\\%$ to $70\\%$ depending on the model and domain. Moreover, we find that echoing is persistent even in advanced reasoning models with substantial rates ($32.8\\%$) that are not reduced by increased reasoning efforts. We analyze prompt impacts, conversation dynamics, showing that echoing arises as interaction grows longer ($7+$ turns in experiments) and is not merely an artifact of sub-optimal prompting. Finally, we introduce a protocol-level mitigation in which targeted use of structured responses reduces echoing to $9\\%$.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-12",
        "category": "cs.AI",
        "crawl_time": "2025-11-14T11:00:04.817712",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献是**识别、分析并提出了一种针对多智能体系统（Multi-Agent Systems）中特定故障模式的缓解方法**。它研究了当LLM智能体相互对话时出现的“回声”现象，即智能体丧失身份并模仿对方。这并非将智能体作为工具应用到某个领域，而是直接研究智能体交互本身的行为和缺陷。其提出的“协议级缓解措施”是对多智能体交互框架的一种**改进**，旨在提升系统的稳健性。因此，这篇论文的本质是关于**改进LLM多智能体系统**，符合“保留”标准。 2.  **正面指标 (第二步):** 论文与你的核心关注点高度匹配。 *   **核心范式:** 论文明确聚焦于 `LLM-based Agents` 和 `Multi-Agent Systems (MAS)`。 *   **多智能体:** 研究的核心是智能体间的 `Communication`（对话），并探讨了 `Collaboration` 失败的原因（身份丧失导致目标无法实现）。虽然不是积极的社会学习，但“回声”本身可以被视为一种有缺陷的 `Social Learning` 或模仿行为。 3.  **排除标准 (第三步):** 论文没有被排除标准命中。 *   **安全与对齐:** 尽管研究的是一种“失败”，但论文的出发点是**系统性能和稳健性**，而非伦理层面的安全、对齐或可解释性。它关注的是如何让智能体更好地完成其指定任务，而不是如何让它们与人类价值观对齐。因此，它不属于安全与对齐的研究范畴"
    }
]