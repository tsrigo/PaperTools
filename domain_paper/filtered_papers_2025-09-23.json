[
    {
        "index": "#5",
        "title": "Reinforcement Learning on Pre-Training Data",
        "link": "/arxiv/2509.19249",
        "arxiv_id": "2509.19249",
        "authors": "Siheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang, Kun Li, Haoyuan Wu, Jiajia Wu, Zihao Zheng, Chenchen Zhang, Kun Shi, Kyrierl Deng, Qi Yi, Ruibin Xiong, Tingqiang Xu, Yuhao Jiang, Jianfeng Yan, Yuyuan Zeng, Guanghui Xu, Jinbao Xue, Zhijiang Xu, Zheng Fang, Shuai Li, Qibin Liu, Xiaoxue Li, Zhuoyu Li, Yangyu Tao, Fei Gao, Cheng Jiang, Bo Chao Wang, Kai Liu, Jianchen Zhu, Wai Lam, Wayyt Wang, Bo Zhou, Di Wang",
        "summary": "The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$, $6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.454513",
        "filter_reason": "这篇论文完全符合研究目标，核心是提出一种名为RLPT（Reinforcement Learning on Pre-Training Data）的新训练范式，用于提升大语言模型的通用推理能力。从第一步核心判断来看，论文本质上是改进LLM的基础能力，提出新的强化学习训练方法，旨在增强模型的通用推理技能，而非将LLM应用于特定领域。论文明确提到RLPT\"鼓励在更广泛的上下文中探索更丰富的轨迹，从而培养更通用的推理技能\"，并在数学推理等通用能力基准测试中验证了有效性。 从第二步正面指标看，论文包含多个相关主题：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于推理能力(特别是数学推理)；训练方法上提出了基于强化学习的新范式(RLPT)，并与RLHF、RLVR等方法进行了比较。 第三步排除标准方面，论文不涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容，没有任何排除因素。 综上所述，这篇论文的核心贡献是提出一种新的训练范式来增强LLM的通用推理能力，完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#9",
        "title": "Online Process Reward Leanring for Agentic Reinforcement Learning",
        "link": "/arxiv/2509.19199",
        "arxiv_id": "2509.19199",
        "authors": "Xiaoqian Liu, Ke Wang, Yuchuan Wu, Fei Huang, Yongbin Li, Junge Zhang, Jianbin Jiao",
        "summary": "Large language models (LLMs) are increasingly trained with reinforcement learning (RL) as autonomous agents that reason and act over long horizons in interactive environments. However, sparse and sometimes unverifiable rewards make temporal credit assignment extremely challenging. Recent work attempts to integrate process supervision into agent learning but suffers from biased annotation, reward hacking, high-variance from overly fine-grained signals or failtures when state overlap is rare. We therefore introduce Online Process Reward Learning (OPRL), a general credit-assignment strategy for agentic RL that integrates seamlessly with standard on-policy algorithms without relying on additional rollouts or explicit step labels. In OPRL, we optimize an implicit process reward model (PRM) alternately with the agent's policy to transform trajectory preferences into implicit step rewards through a trajectory-based DPO objective. These step rewards are then used to compute step-level advantages, which are combined with episode-level advantages from outcome rewards for policy update, creating a self-reinforcing loop. Theoretical findings guarantee that the learned step rewards are consistent with trajectory preferences and act as potential-based shaping rewards, providing bounded gradients to stabilize training. Empirically, we evaluate OPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as well as open-ended social interactions with unverfiable rewards in SOTOPIA. Crucially, OPRL shows superior performance over frontier LLMs and strong RL baselines across domains, achieving state-of-the-art results with higher sample-efficiency and lower variance during training. Further analysis also demonstrates the efficient exploration by OPRL using fewer actions, underscoring its potential for agentic learning in real-world scenarios.",
        "subjects": "Computation and Language",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.455270",
        "filter_reason": "这篇论文完全符合研究范围，核心贡献是提出一种新的强化学习方法(OPRL)来增强大语言模型作为智能体时的通用推理能力。 从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力的研究。它提出了\"在线过程奖励学习\"(OPRL)这一新的训练范式，专注于解决LLM作为自主智能体在长期任务中的信用分配问题，这直接属于提升LLM通用推理能力的范畴。 从第二步正面指标来看，论文涵盖了所有关键主题：明确以大语言模型(LLMs)为核心研究对象；关注模型的推理能力(\"reason and act over long horizons\")；采用强化学习作为主要训练方法；并聚焦于基于LLM的智能体(\"agentic reinforcement learning\")研究。 从第三步排除标准来看，虽然论文在评估中使用了WebShop、VisualSokoban等特定环境，但这些只是用来验证通用方法的应用场景，论文本身并不专注于任何特定应用领域或多模态研究，其核心贡献是通用的强化学习方法。 从第四步特殊和模糊情况处理来看，论文提出的是一种通用的智能体学习方法，旨在增强LLM的通用问题解决和推理能力，而非针对特定领域的应用。 综上所述，这篇论文通过提出新的强化学习范式来提升LLM的长期推理和决策能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#16",
        "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering",
        "link": "/arxiv/2509.19094",
        "arxiv_id": "2509.19094",
        "authors": "Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Zhuowan Li, Spurthi Amba Hombaiah, Weize Kong, Tao Chen, Hamed Zamani, Michael Bendersky",
        "summary": "Personalization is essential for adapting question answering (QA) systems to user-specific information needs, thereby improving both accuracy and user satisfaction. However, personalized QA remains relatively underexplored due to challenges such as inferring preferences from long, noisy, and implicit contexts, and generating responses that are simultaneously correct, contextually appropriate, and aligned with user expectations and background knowledge. To address these challenges, we propose Pathways of Thoughts (PoT), an inference-stage method that applies to any large language model (LLM) without requiring task-specific fine-tuning. The approach models the reasoning of an LLM as an iterative decision process, where the model dynamically selects among cognitive operations such as reasoning, revision, personalization, and clarification. This enables exploration of multiple reasoning trajectories, producing diverse candidate responses that capture different perspectives. PoT then aggregates and reweights these candidates according to inferred user preferences, yielding a final personalized response that benefits from the complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA benchmark for personalized QA show that PoT consistently outperforms competitive baselines, achieving up to a 13.1% relative improvement. Human evaluation corroborates these results, with annotators preferring outputs from PoT in 66% of cases and reporting ties in only 15% of cases.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.456738",
        "filter_reason": "这篇论文的核心贡献是提出了\"Pathways of Thoughts (PoT)\"方法，一种用于增强大语言模型推理能力的通用框架。根据筛选标准，我判断该论文符合研究范围，原因如下： 首先，从核心判断来看，论文的本质是改进LLM的基础推理能力。PoT方法将LLM的推理过程建模为迭代决策过程，使模型能够动态选择认知操作（推理、修订、个性化、澄清等），这直接涉及增强LLM的多步推理能力和逻辑推理能力，属于提升LLM通用推理能力的研究。 其次，从正面指标分析，论文明确包含多个关键主题：核心概念上聚焦于大语言模型(LLMs)；能力方向上专注于reasoning（推理过程）；方法上提出了一种新的推理范式，使模型能够探索多个推理轨迹并聚合结果。 第三，该论文不涉及任何排除标准领域。它没有关注多模态与视觉，没有针对特定应用领域（如医疗、化学等），也没有专注于模型基础设施或部署优化。 最后，关于特殊情况的考虑，虽然论文应用于个性化问答场景，但其提出的是一种通用的推理框架，可以应用于任何LLM而无需任务特定微调，因此不是将LLM作为工具应用于特定领域，而是致力于提升LLM本身的通用推理能力。 综合来看，这篇论文通过提出多方向思维的推理方法，直接增强了大语言模型的通用推理能力，完全符合研究课题的筛选要求。"
    },
    {
        "index": "#10",
        "title": "Soft Tokens, Hard Truths",
        "link": "/arxiv/2509.19170",
        "arxiv_id": "2509.19170",
        "authors": "Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, Yann Ollivier",
        "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens. This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the \"soft\" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.455533",
        "filter_reason": "这篇论文的核心贡献是提出了一种通过强化学习(RL)来学习连续思维链(CoTs)的新方法，以提高大语言模型的推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是思维链推理这一通用能力，而非将LLM作为工具应用到特定领域。论文使用了强化学习这一训练方法，聚焦于数学推理任务，符合第二步正面指标中的多个关键点：核心概念(LLMs)、能力方向(math reasoning)和训练方法(RL)。论文不涉及第三步排除标准中的任何领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。在第四步特殊和模糊情况处理中，论文提出的是一种通用的推理增强方法，而非针对特定领域的应用。综合来看，这篇论文直接致力于提高LLM的通用推理能力，通过创新的\"soft tokens\"方法增强思维链推理，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#35",
        "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users",
        "link": "/arxiv/2509.18632",
        "arxiv_id": "2509.18632",
        "authors": "Nishant Balepur, Matthew Shu, Yoo Yeon Sung, Seraphina Goldfarb-Tarrant, Shi Feng, Fumeng Yang, Rachel Rudinger, Jordan Lee Boyd-Graber",
        "summary": "To assist users in complex tasks, LLMs generate plans: step-by-step instructions towards a goal. While alignment methods aim to ensure LLM plans are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer, assuming this reflects what helps them. We test this with Planorama: an interface where 126 users answer 300 multi-step questions with LLM plans. We get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA success) and user preferences on plans, and recreate the setup in agents and reward models to see if they simulate or prefer what helps users. We expose: 1) user/model preferences and agent success do not accurately predict which plans help users, so common alignment feedback can misalign with helpfulness; 2) this gap is not due to user-specific preferences, as users are similarly successful when using plans they prefer/disprefer; 3) surface-level cues like brevity and question similarity strongly link to preferences, but such biases fail to predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from real user interactions, not just preferences of what looks helpful, so we discuss the plan NLP researchers can execute to solve this problem.",
        "subjects": "Computation and Language",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.460444",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。 首先，从核心判断来看，这篇论文的本质是研究LLM的规划(planning)能力，属于通用推理能力的重要组成部分。论文探讨了LLM如何生成步骤化计划来帮助用户完成复杂任务，并评估这些计划的实际有效性。这不是将LLM作为工具应用到特定领域，而是关注LLM本身的基础能力和改进方法，特别是当前对齐方法(如RLHF)的局限性。 其次，论文满足多个正面指标： 1) 核心概念：明确研究LLM生成的计划 2) 能力方向：聚焦于规划(planning)能力，这是推理和问题解决的关键部分 3) 训练方法：讨论了RLHF等对齐方法，并指出其局限性 4) 新兴范式：提到了智能体(agents)在模拟用户交互中的作用 第三，论文不符合任何排除标准。它不涉及多模态与视觉，不聚焦于特定应用领域，也不是关于模型可靠性的应用层面研究(如水印、安全等)。 在特殊和模糊情况处理上，论文虽然提到了智能体，但这是作为研究方法的一部分，而非将智能体应用在特定领域。论文讨论的对齐问题是从提升模型基础能力的角度，而不是应用层面的防御技术。 论文的核心贡献在于揭示了当前LLM对齐方法(基于用户偏好)与实际帮助用户之间的差距，并提出需要基于真实用户交互反馈来改进对齐方法。这直接关系到如何提升LLM的通用推理能力和实际效用，完全符合研究目标。"
    },
    {
        "index": "#34",
        "title": "Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering",
        "link": "/arxiv/2509.18655",
        "arxiv_id": "2509.18655",
        "authors": "Lingwen Deng, Yifei Han, Long Zhang, Yue Du, Bin Li",
        "summary": "Parameter-Preserving Knowledge Editing (PPKE) enables updating models with new or corrected information without retraining or parameter adjustment. Recent PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE) capabilities to multi-hop question answering (MHQA). However, these methods often lack consistency, leading to knowledge contamination, unstable updates, and retrieval behaviors that fail to reflect the intended edits. Such inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures KG construction, update, and retrieval are always aligned with the requirements of the MHQA task, maintaining coherent reasoning over both unedited and edited knowledge. Extensive experiments on the MQuAKE benchmark show accuracy improvements in PPKE performance for MHQA, demonstrating the effectiveness of addressing consistency in PPKE.",
        "subjects": "Computation and Language",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.460238",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步：核心判断——这篇论文的本质是关于改进LLM在多跳问答(multi-hop question answering)任务中的推理能力。论文提出了CAPE-KG框架，解决参数保留知识编辑(PPKE)中的一致性问题，确保模型在更新知识后能够保持连贯的多跳推理能力。这属于增强LLM逻辑推理能力的范畴，而非将LLM作为工具应用到特定领域，因此应保留。 第二步：正面指标——论文包含\"reasoning\"这一核心能力方向，特别是\"multi-hop reasoning\"（多跳推理），这是通用推理能力的重要组成部分，涉及模型进行多步逻辑推理来连接不同知识片段以得出答案。 第三步：排除标准——论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域。多跳问答是一种通用的推理任务，不属于特定应用领域。 第四步：特殊和模糊情况——论文提出了一种新方法来解决知识编辑中的不一致性问题，从而提升模型在多跳问答任务中的推理质量和可靠性。这符合\"提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量\"的情况，应保留。 综合以上分析，该论文的核心贡献是提出了一种一致性感知的知识编辑框架，旨在提升LLM在多跳推理任务中的表现，这与研究\"大语言模型通用推理能力\"的目标高度一致，因此最终判断为True。"
    },
    {
        "index": "#52",
        "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs",
        "link": "/arxiv/2509.18314",
        "arxiv_id": "2509.18314",
        "authors": "Hieu Tran, Zonghai Yao, Hong Yu",
        "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over long sequences makes token-level credit assignment the key bottleneck. We study the verifiable-reward setting, where the final answer is checkable and multiple responses can be drawn per prompt. Reasoning tasks in math and medical QA align with this setup, where only a few decision tokens significantly impact the outcome. PPO offers token-level advantages with a learned value model, but it is complex to train both the actor and critic models simultaneously, and it is not easily generalizable, as the token-level values from the critic model can make training prone to overfitting. GRPO is critic-free and supports verifiable rewards, but spreads a single sequence-level return across tokens and ignores branching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that converts a group of responses into a prefix tree and computes \\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes. Built on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated \\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a critic-free algorithm that augments the group-relative outcome signal of GRPO with \\emph{branch-gated} temporal-difference corrections derived from the tree. At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO reduces to GRPO; at branching tokens, it supplies precise token-level credit without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B, TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and reaches higher validation accuracy with roughly the same wall-clock time.",
        "subjects": "Computation and Language",
        "date": "2025-09-22",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.463893",
        "filter_reason": "这篇论文完全符合我的研究目标，核心是关于提高大语言模型通用推理能力的研究。 首先，从本质上看，论文的核心贡献是提出了一种新的强化学习训练方法TEMPO，用于解决LLM在长序列推理任务中的信用分配问题。这直接针对LLM的基础能力改进，特别是提升其在推理任务中的表现，符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。 其次，论文包含多个正面指标：明确以LLMs为核心研究对象；专注于reasoning能力（特别是math reasoning）；提出了新的强化学习训练方法(TEMPO)，改进了现有的PPO和GRPO算法。 第三，论文不涉及任何排除标准领域。虽然论文在实验中使用了医疗问答(MedQA, MMLU-Medical)作为评估基准，但这只是作为验证方法有效性的测试场景，而非论文的核心焦点。论文的核心是改进通用推理能力，而非专注于医疗应用。 最后，论文提出的方法具有通用性，可以应用于各种需要推理能力的任务，不仅限于特定领域。作者在多个数学推理基准(MATH, GSM-HARD, AMC23)和医疗问答基准上进行了验证，证明了其方法的通用性和有效性。 综上所述，这篇论文明确致力于提高LLM的通用推理能力，通过改进强化学习训练方法来解决信用分配问题，完全符合我的研究范围。"
    },
    {
        "index": "#56",
        "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning",
        "link": "/arxiv/2509.18163",
        "arxiv_id": "2509.18163",
        "authors": "Haodong Zhao, Chenyan Zhao, Yansi Li, Zhuosheng Zhang, Gongshen Liu",
        "summary": "The capacity of Large Language Models (LLMs) to reason is fundamental to their application in complex, knowledge-intensive domains. In real-world scenarios, LLMs are often augmented with external information that can be helpful, irrelevant, or even misleading. This paper investigates the causal impact of such auxiliary information on the reasoning process of LLMs with explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset derived from ScienceQA, to systematically test the robustness of the model against these types of information. Our findings reveal a critical vulnerability: the model's deliberative \"thinking mode\" is a double-edged sword. While helpful context improves accuracy, misleading information causes a catastrophic drop in performance, which is amplified by the thinking process. Instead of conferring robustness, thinking reinforces the degree of error when provided with misinformation. This highlights that the challenge is not merely to make models \"think\", but to endow them with the critical faculty to evaluate the information upon which their reasoning is based. The SciAux dataset is available at https://huggingface.co/datasets/billhdzhao/SciAux.",
        "subjects": "Computation and Language",
        "date": "2025-09-17",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.464649",
        "filter_reason": "根据筛选标准，这篇论文符合研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的核心是研究大语言模型(LLM)的推理能力，特别是探讨外部辅助信息对LLM推理过程的影响。论文关注的是LLM的基础推理能力，而不是将LLM作为工具应用到特定领域。论文研究了模型的\"thinking mode\"（思维模式）如何影响其对不同类型信息的处理，这直接关系到LLM的通用推理能力。因此，从核心判断来看，这篇论文应该被保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：直接研究reasoning能力，特别是模型的step-by-step thinking capabilities - 论文探讨了LLM在面对不同类型信息时的推理过程，这与提升模型通用推理能力直接相关 第三步：排除标准 论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉研究 - 不是针对特定应用领域（如医疗、化学等）的研究 - 虽然涉及模型可靠性，但是从提升模型推理能力的根本角度出发，而非仅作为应用层面的防御 第四步：处理特殊和模糊情况 论文研究的是LLM的通用推理能力在面对不同类型信息时的表现，特别是\"thinking mode\"如何影响推理过程。这不是将LLM应用到特定领域，而是研究LLM本身的推理机制和脆弱性。论文提出的挑战是\"not merely to make models 'think', but to endow them with the critical faculty to evaluate the information upon which their reasoning is based\"，这直接指向提升LLM的通用推理能力。 第五步：最终决策 综合以上分析，这篇论文的核心贡献是研究外部辅助信息对LLM推理过程的影响，揭示了模型\"thinking mode\"的双面性，并提出了提升模型批判性评估信息能力的重要性。这直接关系到提升LLM的通用推理能力，符合研究目标。因此，最终判断为True，应该保留这篇论文。"
    },
    {
        "index": "#60",
        "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs",
        "link": "/arxiv/2509.18113",
        "arxiv_id": "2509.18113",
        "authors": "Xin Hu, Yue Kang, Guanzi Yao, Tianze Kang, Mengjie Wang, Heyao Liu",
        "summary": "This study addresses the generalization limitations commonly observed in large language models under multi-task and cross-domain settings. Unlike prior methods such as SPoT, which depends on fixed prompt templates, our study introduces a unified multi-task learning framework with dynamic prompt scheduling mechanism. By introducing a prompt pool and a task-aware scheduling strategy, the method dynamically combines and aligns prompts for different tasks. This enhances the model's ability to capture semantic differences across tasks. During prompt fusion, the model uses task embeddings and a gating mechanism to finely control the prompt signals. This ensures alignment between prompt content and task-specific demands. At the same time, it builds flexible sharing pathways across tasks. In addition, the proposed optimization objective centers on joint multi-task learning. It incorporates an automatic learning strategy for scheduling weights, which effectively mitigates task interference and negative transfer. To evaluate the effectiveness of the method, a series of sensitivity experiments were conducted. These experiments examined the impact of prompt temperature parameters and task number variation. The results confirm the advantages of the proposed mechanism in maintaining model stability and enhancing transferability. Experimental findings show that the prompt scheduling method significantly improves performance on a range of language understanding and knowledge reasoning tasks. These results fully demonstrate its applicability and effectiveness in unified multi-task modeling and cross-domain adaptation.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-09",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.465594",
        "filter_reason": "这篇论文的核心贡献是提出了一种动态提示融合方法，用于改进大语言模型在多任务和跨领域设置下的泛化能力。论文通过引入提示池和任务感知调度策略，动态组合和对齐不同任务的提示，增强了模型捕捉跨任务语义差异的能力。从第一步核心判断来看，这明显属于改进LLM基础能力的研究，提出了新的训练范式来增强模型的通用能力，而非将LLM作为工具应用到特定领域。从第二步正面指标看，论文明确涉及大语言模型(LLMs)核心概念，并特别提到提高了\"知识推理任务\"(knowledge reasoning tasks)的性能，直接符合推理能力方向。论文不涉及第三步中的任何排除标准，如多模态视觉、特定应用领域或模型可靠性等应用层面的研究。综合分析，该论文致力于提升LLM的通用推理能力和泛化能力，完全符合\"大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#57",
        "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization",
        "link": "/arxiv/2509.18158",
        "arxiv_id": "2509.18158",
        "authors": "Seungyoun Yi, Minsoo Khang, Sungrae Park",
        "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM) performance by refining prompts for specific tasks. However, prior APO methods typically focus only on user prompts, rely on unstructured feedback, and require large sample sizes and long iteration cycles-making them costly and brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a novel framework that jointly optimizes both system and user prompts through principled, low-overhead refinement. ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. This enables fast convergence to high-quality prompts using minimal examples and short iteration cycles. We evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning, summarization, and code generation tasks. Experimental results demonstrate consistent improvements over strong baselines. Further ablation studies highlight the contribution of each component to more effective prompt construction. Our implementation including all prompts is publicly available at https://github.com/younatics/zera-agent.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-17",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.464874",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究课题。 首先，从核心判断来看，这篇论文的本质是提出ZERA框架，一种用于自动优化提示的新方法，通过联合优化系统提示和用户提示来提高大语言模型在各种任务上的性能。这不是将LLM作为工具应用到特定领域，而是提出一种通用的方法来增强LLM的基础能力，特别是在推理任务上的表现，因此符合保留条件。 其次，论文包含多个正面指标： - 核心概念：明确关注大语言模型(LLMs)，并在五个不同LLM上进行了评估 - 能力方向：评估数据集包含推理(reasoning)任务，直接符合我们的研究方向 - 训练方法：标题中的\"Instruction Evolving\"和摘要中的\"principled, low-overhead refinement\"表明涉及进化或自我优化的方法 - 新兴范式：论文提出的ZERA是一种基于LLM的智能体(Refinement Agent)，用于优化提示 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域，评估的是通用能力（推理、摘要、代码生成） - 不主要关注模型可靠性的应用层面问题 最后，在特殊和模糊情况处理上，ZERA智能体是一种通用框架，旨在增强LLM的通用问题解决能力，而不是应用于特定领域，因此应该保留。 论文的核心贡献是提出了一种新颖的、基于原则的提示优化框架，能够使用最少的示例和短的迭代周期快速收敛到高质量提示，从而提升LLM在推理等通用任务上的表现。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#61",
        "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World",
        "link": "/arxiv/2509.19265",
        "arxiv_id": "2509.19265",
        "authors": "Saeed Almheiri, Rania Hossam, Mena Attia, Chenxi Wang, Preslav Nakov, Timothy Baldwin, Fajri Koto",
        "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.465825",
        "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围。论文的核心贡献是研究LLMs的常识推理能力在不同文化背景下的迁移效果，并提出了轻量级对齐方法（如上下文学习和基于演示的强化学习DITTO）来提升这种能力。常识推理是通用推理能力的重要组成部分，论文评估的训练方法也属于改进LLM基础能力的范畴。虽然论文涉及文化领域，但其焦点不是将LLM作为工具应用于特定领域，而是研究LLM本身的推理能力如何在不同文化背景下迁移和提升。论文提出的方法（使用12个文化特定示例即可提高其他地区性能10%）可以作为一种通用的方法来增强LLMs的推理能力，而不是仅限于解决特定领域的问题。因此，这篇论文符合我的研究目标，它探索了如何通过文化迁移来提升LLM的通用推理能力，特别是常识推理这一核心能力。"
    },
    {
        "index": "#66",
        "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions",
        "link": "/arxiv/2509.18847",
        "arxiv_id": "2509.18847",
        "authors": "Junhao Su, Yuanliang Wan, Junwei Yang, Hengyu Shi, Tianyang Han, Junfeng Luo, Yurui Qiu",
        "summary": "Tool-augmented large language models (LLMs) are usually trained with supervised imitation or coarse-grained reinforcement learning that optimizes single tool calls. Current self-reflection practices rely on heuristic prompts or one-way reasoning: the model is urged to 'think more' instead of learning error diagnosis and repair. This is fragile in multi-turn interactions; after a failure the model often repeats the same mistake. We propose structured reflection, which turns the path from error to repair into an explicit, controllable, and trainable action. The agent produces a short yet precise reflection: it diagnoses the failure using evidence from the previous step and then proposes a correct, executable follow-up call. For training we combine DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce Tool-Reflection-Bench, a lightweight benchmark that programmatically checks structural validity, executability, parameter correctness, and result consistency. Tasks are built as mini trajectories of erroneous call, reflection, and corrected call, with disjoint train and test splits. Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn tool-call success and error recovery, and a reduction of redundant calls. These results indicate that making reflection explicit and optimizing it directly improves the reliability of tool interaction and offers a reproducible path for agents to learn from failure.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.466964",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础能力，特别是通过提出\"结构化反思\"(structured reflection)这一新训练范式来增强LLM的工具使用和错误恢复能力，这属于提升LLM通用推理能力的范畴。 论文包含多个正面指标：明确提及\"Large language models (LLMs)\"和\"Tool-augmented large language models\"等核心概念；虽然未直接使用\"reasoning\"一词，但\"reflection\"和\"error diagnosis\"本质上是一种推理过程；训练方法涉及强化学习(\"reinforcement learning\"和\"reward scheme\")；同时论文明确讨论了\"llm-based agents\"和\"tool use\"等新兴范式。 论文不符合任何排除标准：它不涉及多模态与视觉内容，不专注于特定应用领域，且讨论的\"reliability\"是从提升模型内在能力的角度而非应用层面的防御。 在特殊和模糊情况处理上，论文提出的是一种通用的智能体反思框架来增强LLM的工具使用能力，而非针对特定领域的应用，因此应该保留。 论文的核心贡献是通过结构化反思机制，使智能体能够从失败中学习并改进其推理过程，这直接提升了LLM的通用推理能力和问题解决能力，符合研究目标。"
    },
    {
        "index": "#78",
        "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning",
        "link": "/arxiv/2509.18169",
        "arxiv_id": "2509.18169",
        "authors": "Hengbo Xiao, Jingyuan Fan, Xin Tong, Jingzhao Zhang, Chao Lu, Guannan He",
        "summary": "Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.",
        "subjects": "Machine Learning, Computational Engineering, Finance, and Science, Computation and Language",
        "date": "2025-09-17",
        "category": "cs.CL",
        "crawl_time": "2025-09-24T16:40:11.469801",
        "filter_reason": "这篇论文的核心贡献是提出PiMoE（物理隔离的专家混合）架构，用于将高精度计算能力内生性地集成到大语言模型中，以增强其推理能力。从筛选标准来看： 第一步（核心判断）：论文本质上是改进LLM的基础推理能力，而非将其作为工具应用到特定领域。PiMoE通过令牌级路由实现计算和推理的迭代交替，属于增强LLM通用推理能力的新范式，类似思维链(CoT)的扩展，因此应保留。 第二步（正面指标）：论文包含多个相关主题，包括核心概念\"large language models (LLMs)\"、能力方向\"reasoning\"，以及新兴范式\"multi-agent systems\"和\"tool use\"的讨论。 第三步（排除标准）：论文未主要聚焦于多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域。虽然提及\"scientific or industrial intelligent systems\"，但这是潜在应用场景而非研究焦点。 第四步（特殊和模糊情况）：论文确实涉及智能体/工具使用，但不是将其应用于特定领域，而是提出一种通用的架构来增强LLM的通用问题解决能力，这与筛选标准中\"提出通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况相符。 综上所述，PiMoE论文直接致力于提高大语言模型本身的通用推理能力，通过新的架构设计实现计算与推理的融合，完全符合研究目标。"
    },
    {
        "index": "#2",
        "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration",
        "link": "/arxiv/2509.19236",
        "arxiv_id": "2509.19236",
        "authors": "Chunhao Tian, Yutong Wang, Xuebo Liu, Zhexuan Wang, Liang Ding, Miao Zhang, Min Zhang",
        "summary": "Proper initialization is crucial for any system, particularly in multi-agent systems (MAS), where it plays a pivotal role in determining both the system's efficiency and effectiveness. However, existing MAS initialization methods do not fully account for the collaborative needs of the generated agents in subsequent stages. Inspired by the principles of effective team composition, we propose AgentInit, which aims to optimize the structure of agent teams. Specifically, in addition to multi-round interactions and reflections between agents during agent generation, AgentInit incorporates a Natural Language to Format mechanism to ensure consistency and standardization. Balanced team selection strategies using Pareto principles are subsequently applied to jointly consider agent team diversity and task relevance to promote effective and efficient collaboration and enhance overall system performance. Experiments show that AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving an overall performance improvement of up to 1.2 and 1.6, respectively, while also significantly reducing token consumption. Further analysis confirms its strong transferability to similar tasks and verifies the effectiveness of its key components, demonstrating its capability and adaptability as a reliable MAS initialization method. Source code and models are available at https://github.com/1737423697/AgentInit.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.160893",
        "filter_reason": "我根据筛选标准对这篇论文进行了全面分析，认为它符合研究范围。以下是详细判断过程： 第一步核心判断：这篇论文的核心是提出AgentInit，一种用于初始化基于大语言模型的多智能体系统的方法。它通过多样性和专业性的协调来优化智能体团队的结构，从而提高系统协作效率和问题解决能力。这属于\"智能体协作框架\"的研究范畴，旨在增强大语言模型的通用能力，而非将其作为工具应用于特定领域。因此，根据第一步标准，应予以保留。 第二步正面指标：论文包含多个相关主题。首先，核心概念方面明确研究\"LLM-based Multi-Agent Systems\"；其次，在新兴范式方面，论文聚焦于多智能体系统的研究；虽然论文没有直接提及推理、规划或强化学习等术语，但其优化多智能体协作的目标本质上是为了提升系统的问题解决能力，这与通用推理能力相关。 第三步排除标准：论文不涉及任何排除领域。它没有关注多模态与视觉内容，不针对医疗、化学、生物等特定应用领域，也不讨论模型可靠性方面如水印、安全等问题。相反，论文强调其方法在\"各种框架和任务\"上的适用性，表明其通用性。 第四步特殊情况处理：论文研究的是智能体协作框架，但提出的是一种通用的初始化方法，而非针对特定领域的应用。它旨在通过优化智能体团队的初始化来增强LLM在多智能体环境下的通用协作能力，因此符合保留条件。 综上所述，这篇论文的核心贡献是提出一种通用的多智能体系统初始化方法，通过优化团队结构和协作机制来增强大语言模型的通用问题解决能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#3",
        "title": "Code Driven Planning with Domain-Adaptive Critic",
        "link": "/arxiv/2509.19077",
        "arxiv_id": "2509.19077",
        "authors": "Zikang Tian, Shaohui Peng, Du Huang, Jiaming Guo, Ruizhi Chen, Rui Zhang, Xishan Zhang, Yuxuan Guo, Zidong Du, Qi Guo, Ling Li, Yewen Pu, Xing Hu, Yunji Chen",
        "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI agents in sequential decision-making problems, leveraging their extensive world knowledge. However, the gap between their general knowledge and environment-specific requirements often leads to inaccurate plans. To address this, existing approaches rely on frequent LLM queries to iteratively refine plans based on immediate environmental feedback, which incurs substantial query costs. However, this refinement is typically guided by short-term environmental feedback, limiting LLMs from developing plans aligned with long-term rewards. We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of relying on frequent queries, CoPiC employs LLMs to generate a diverse set of high-level planning programs, which iteratively produce and refine candidate plans. A trained domain-adaptive critic then evaluates these candidates and selects the one most aligned with long-term rewards for execution. Using high-level planning programs as planner and domain-adaptive critic as estimator, CoPiC improves planning while significantly reducing query costs. Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in query costs.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.161418",
        "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究课题。根据筛选标准，我进行了如下分析： 第一步核心判断：论文的本质是改进LLM的规划能力（planning），这属于通用推理能力的核心组成部分。论文提出CoPiC方法，通过让LLM生成高级规划程序并配合领域自适应批评者来提升规划质量，这是对LLM基础能力的增强，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确关注Large Language Models (LLMs)作为任务规划器 - 能力方向：重点研究planning能力，这是推理能力的重要组成部分 - 新兴范式：涉及LLM-based agents，将LLM用于AI智能体的序列决策问题 第三步排除标准：论文不聚焦于排除的领域： - 虽然在ALFWorld、NetHack和StarCraft II等特定环境中进行实验，但这些仅作为验证方法有效性的测试平台，论文核心是提出通用规划框架，而非针对特定领域的应用 - 不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究 第四步特殊情况处理：论文提出的是一种通用的智能体规划框架，旨在提升LLM的规划能力，减少查询成本并提高与长期奖励的一致性，这属于应保留的情况。 论文的核心贡献是CoPiC方法，它通过减少LLM查询频率并引入领域自适应批评者来评估候选计划，从而提升LLM的规划能力。这直接关系到提升LLM的通用推理能力，特别是规划和决策方面的能力，符合研究目标。"
    },
    {
        "index": "#11",
        "title": "LongCat-Flash-Thinking Technical Report",
        "link": "/arxiv/2509.18883",
        "arxiv_id": "2509.18883",
        "authors": "Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chengcheng Han, Chenhui Yang, Chi Zhang, Chong Peng, Chuyu Zhang, Cong Chen, Fengcun Li, Gang Xu, Guoyuan Lin, Hao Jiang, Hao Liang, Haomin Fu, Haoxiang Ma, Hong Liu, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiahao Liu, Jiahuan Li, Jialin Liu, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiaqi Sun, Jiaqi Zhang, Jiarong Shi, Jiawei Yang, Jingang Wang, Jinrui Ding, Jun Kuang, Jun Xu, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Li Wei, Liang Shi, Lin Qiu, Lingbin Kong, Lingchuan Liu, Linsen Guo, Longfei An, Mai Xia, Meng Zhou, Mengshen Zhu, Peng Pei, Pengcheng Jia, Qi Gu, Qi Guo, Qiong Huang, Quan Chen, Quanchi Weng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shanglin Lei, Shuai Du, Shuaikang Liu, Shuang Zhou, Shuhao Hu, Siyu Xu, Songshan Gong, Tao Liang, Tianhao Hu, Wei He, Wei Shi, Wei Wang, Wei Wu, Wei Zhuo, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Xi Su, Xiangcheng Liu, Xiangyu Xi, Xiangzhou Huang, Xiao Liu, Xiaochen Jiang, Xiaowei Shi, Xiaowen Shi, Xiaoyu Li, Xin Chen, Xinyue Zhao, Xuan Huang, Xuemiao Zhang, Xuezhi Cao, Xunliang Cai, Yajie Zhang, Yang Chen, Yang Liu, Yang Liu, Yang Zheng, Yaoming Wang, Yaqi Huo, Yerui Sun, Yifan Lu, Yiyang Li, Youshao Xiao, Yuanzhe Lei, Yuchen Xie, Yueqing Sun, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunke Zhao, Yuqing Ding, Yuwei Jiang, Zhaohua Yang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhongda Su, Ziran Li, Ziwen Wang, Ziyuan Zhuang, Zongyu Wang, Zunyuan Yang",
        "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities are cultivated through a meticulously crafted training process, beginning with long Chain-of-Thought (CoT) data cold-start and culminating in large-scale Reinforcement Learning (RL). We first employ a well-designed cold-start training strategy, which significantly enhances the reasoning potential and equips the model with specialized skills in both formal and agentic reasoning. Then, a core innovation is our domain-parallel training scheme, which decouples optimization across distinct domains (e.g., STEM, Code, Agentic) and subsequently fuses the resulting expert models into a single, nearly Pareto-optimal model. This entire process is powered by our Dynamic ORchestration for Asynchronous rollout (DORA) system, a large-scale RL framework that delivers a greater than threefold training speedup over synchronous methods on tens of thousands of accelerators. As a result, LongCat-Flash-Thinking achieves state-of-the-art performance among open-source models on a suite of complex reasoning tasks. The model exhibits exceptional efficiency in agentic reasoning, reducing average token consumption by 64.5% (from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We release LongCat-Flash-Thinking to promote further advances in reasoning systems and agentic AI research.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.164317",
        "filter_reason": "这篇论文完全符合研究范围，其核心贡献是提升大语言模型本身的通用推理能力。首先，论文的本质是关于改进LLM的基础推理能力，提出了新的训练范式，包括长思维链(CoT)数据冷启动和大规模强化学习(RL)方法，这直接符合筛选标准中的保留条件。其次，论文包含了多个正面指标：明确研究大语言模型(LLMs)的推理能力，涉及强化学习训练方法，并探讨了智能体推理(agentic reasoning)这一新兴范式。第三，论文没有聚焦于任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。特别值得注意的是，虽然论文提到了STEM、代码和智能体等不同领域，但这是作为其领域并行训练方案的一部分，目的是增强模型的通用推理能力，而非将LLM应用于特定领域解决问题。论文的核心创新——领域并行训练方案和DORA系统——都是为了提升模型的基础推理能力，使其在复杂推理任务上达到最先进性能。因此，这篇论文明确符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#14",
        "title": "MAPO: Mixed Advantage Policy Optimization",
        "link": "/arxiv/2509.18849",
        "arxiv_id": "2509.18849",
        "authors": "Wenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, Huanjin Yao, Guancheng Wan, Ke Liang, Wenwen He, Mingjun Li, Leszek Rutkowski, Mang Ye, Bo Du, Dacheng Tao",
        "summary": "Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.165024",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为MAPO（Mixed Advantage Policy Optimization）的新强化学习策略，用于改进GRPO（Group Relative Policy Optimization）方法，以解决其在推理任务中遇到的advantage reversion和advantage mirror问题。论文通过引入advantage percent deviation和动态重加权advantage function来优化不同查询样本间的advantage分配，从而提升基础模型在推理任务上的性能。 这完全符合研究目标中的\"改进LLM的基础能力\"和\"提出新的训练范式\"，特别是强化学习优化方面的研究。论文明确关注\"reasoning tasks\"，这是筛选标准中的核心能力方向之一。同时，论文属于强化学习（RL）训练方法的研究，这也是正面指标中明确提到的重要内容。 论文不属于任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。它专注于提升基础模型在通用推理任务上的性能，而不是将模型应用到特定领域。因此，这篇论文应该被保留，它对提升大语言模型的通用推理能力有直接贡献。"
    },
    {
        "index": "#12",
        "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution",
        "link": "/arxiv/2509.18868",
        "arxiv_id": "2509.18868",
        "authors": "Dianxing Zhang, Wendong Li, Kani Song, Jiaye Lu, Gang Li, Liuchun Yang, Sheng Li",
        "summary": "Under a unified operational definition, we define LLM memory as a persistent state written during pretraining, finetuning, or inference that can later be addressed and that stably influences outputs. We propose a four-part taxonomy (parametric, contextual, external, procedural/episodic) and a memory quadruple (location, persistence, write/access path, controllability). We link mechanism, evaluation, and governance via the chain write -> read -> inhibit/update. To avoid distorted comparisons across heterogeneous setups, we adopt a three-setting protocol (parametric only, offline retrieval, online retrieval) that decouples capability from information availability on the same data and timeline. On this basis we build a layered evaluation: parametric (closed-book recall, edit differential, memorization/privacy), contextual (position curves and the mid-sequence drop), external (answer correctness vs snippet attribution/faithfulness), and procedural/episodic (cross-session consistency and timeline replay, E MARS+). The framework integrates temporal governance and leakage auditing (freshness hits, outdated answers, refusal slices) and uncertainty reporting via inter-rater agreement plus paired tests with multiple-comparison correction. For updating and forgetting, we present DMM Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC), and RAG to form an auditable loop covering admission thresholds, rollout, monitoring, rollback, and change audits, with specs for timeliness, conflict handling, and long-horizon consistency. Finally, we give four testable propositions: minimum identifiability; a minimal evaluation card; causally constrained editing with verifiable forgetting; and when retrieval with small-window replay outperforms ultra-long-context reading. This yields a reproducible, comparable, and governable coordinate system for research and deployment.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.164543",
        "filter_reason": "这篇论文的核心是研究大语言模型的记忆机制，提出了统一的操作定义、四部分分类法（参数化、上下文、外部、程序性/情景性）和记忆四元组（位置、持久性、写入/访问路径、可控性）。虽然论文没有直接讨论推理能力，但记忆是推理的基础能力之一，模型需要有效记忆信息并在需要时检索使用，才能进行复杂的推理任务。论文链接了机制、评估和治理，提出了三设置协议和分层评估框架，以及更新和遗忘策略（如DMM Gov），这些都是为了增强LLM的基础能力，从而间接提升其通用推理能力。论文不符合排除标准，没有聚焦于多模态、特定应用领域或模型可靠性的应用层面。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。"
    },
    {
        "index": "#18",
        "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models",
        "link": "/arxiv/2509.18771",
        "arxiv_id": "2509.18771",
        "authors": "Xingkun Yin, Kaibin Huang, Dong In Kim, Hongyang Du",
        "summary": "Scaling model size, training data, and compute power have driven advances in large language models (LLMs), but these approaches are reaching saturation as human-generated text is exhausted and further gains diminish. We propose experience scaling, a framework for continuous post-deployment evolution for LLMs through autonomous interaction with the environment and collaborative sharing of accumulated experience. The framework captures raw interactions, distills them into compact, reusable knowledge, and periodically refines stored content to preserve relevance and efficiency. We validate the framework in simulated real-world scenarios involving generalization to previously unseen but related tasks, repetitive queries, and over-saturated knowledge stores. Across all settings, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations. These results demonstrate that structured post-deployment learning can extend LLM capabilities beyond the limits of static human-generated data, offering a scalable path for continued intelligence progress.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.171584",
        "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。在第一步核心判断中，论文本质上是提出一种名为\"经验缩放\"(Experience Scaling)的新框架，用于大语言模型部署后的持续进化，这直接符合\"改进LLM的基础能力、提出新的训练范式\"的保留标准。论文关注LLM通过自主与环境交互和协作共享积累的经验来提升自身能力，这是一种增强LLM通用推理能力的方法论研究。 在第二步正面指标分析中，论文明确包含核心概念\"Large Language Models (LLMs)\"；涉及训练方法中的\"evolution\"和\"self-evolve\"概念（标题中直接提及\"Post-Deployment Evolution\"）；同时提到\"autonomous interaction with the environment\"暗示了agent概念；论文强调\"对以前未见但相关任务的泛化\"，这间接涉及到推理和问题解决能力。 在第三步排除标准检查中，论文完全不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容。 在第四步特殊情况处理中，论文提到的\"autonomous interaction with the environment\"属于通用智能体框架，用于增强LLM的通用问题解决能力，而非应用于特定领域，因此应该保留。 论文的核心贡献是提出了一种让LLM通过部署后的持续学习和经验积累来提升自身通用能力的框架，这种方法超越了静态人类生成数据的限制，为提升LLM的通用推理能力提供了新的路径，完全符合我的研究目标。"
    },
    {
        "index": "#24",
        "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation",
        "link": "/arxiv/2509.18565",
        "arxiv_id": "2509.18565",
        "authors": "Mitchell Piehl, Dillon Wilson, Ananya Kalita, Jugal Kalita",
        "summary": "Large Language Models (LLMs) excel at various tasks, including problem-solving and question-answering. However, LLMs often find Math Word Problems (MWPs) challenging because solving them requires a range of reasoning and mathematical abilities with which LLMs seem to struggle. Recent efforts have helped LLMs solve more complex MWPs with improved prompts. This study proposes a novel method that initially prompts an LLM to create equations from a decomposition of the question, followed by using an external symbolic equation solver to produce an answer. To ensure the accuracy of the obtained answer, inspired by an established recommendation of math teachers, the LLM is instructed to solve the MWP a second time, but this time with the objective of estimating the correct answer instead of solving it exactly. The estimation is then compared to the generated answer to verify. If verification fails, an iterative rectification process is employed to ensure the correct answer is eventually found. This approach achieves new state-of-the-art results on datasets used by prior published research on numeric and algebraic MWPs, improving the previous best results by nearly two percent on average. In addition, the approach obtains satisfactory results on trigonometric MWPs, a task not previously attempted to the authors' best knowledge. This study also introduces two new datasets, SVAMPClean and Trig300, to further advance the testing of LLMs' reasoning abilities.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.172802",
        "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，该论文的本质是提升LLM的数学推理能力，属于改进LLM基础能力和增强其逻辑、数学推理等通用能力的范畴。论文提出了一种结合方程生成和估计验证的新方法，通过外部符号方程求解器和LLM的估计能力相互验证，提高数学问题求解的准确性，这直接针对LLM在推理方面的能力提升。 其次，论文满足多个正面指标：核心概念明确涉及Large language models (LLMs)；能力方向专注于数学推理(math reasoning)，这正是通用推理能力的重要组成部分；同时采用了工具使用(tool use)的新兴范式，即使用外部符号方程求解器来辅助LLM解决问题。 第三，论文不涉及任何排除标准中的领域：没有涉及多模态与视觉内容；虽然专注于数学文字问题，但数学推理被视为基础能力而非特定应用领域；也没有涉及模型可靠性的应用层面问题。 最后，在特殊和模糊情况处理上，论文中使用的外部符号方程求解器作为工具，是为了增强LLM的通用数学问题解决能力，而不是应用在特定领域，因此符合保留条件。 综合来看，这篇论文的核心贡献是提出了一种提升LLM数学推理能力的新方法，通过估计验证和方程生成相结合的方式，显著提高了LLM在解决数学文字问题上的表现，这完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。"
    },
    {
        "index": "#30",
        "title": "Gödel Test: Can Large Language Models Solve Easy Conjectures?",
        "link": "/arxiv/2509.18383",
        "arxiv_id": "2509.18383",
        "authors": "Moran Feldman, Amin Karbasi",
        "summary": "Recent announcements from frontier AI model labs have highlighted strong results on high-school and undergraduate math competitions. Yet it remains unclear whether large language models can solve new, simple conjectures in more advanced areas of mathematics. We propose the Gödel Test: evaluating whether a model can produce correct proofs for very simple, previously unsolved conjectures. To this end, we study the performance of GPT-5 on five conjectures in combinatorial optimization. For each problem, we provided one or two source papers from which the conjecture arose, withheld our own conjecture, and then assessed the model's reasoning in detail. On the three easier problems, GPT-5 produced nearly correct solutions; for Problem 2 it even derived a different approximation guarantee that, upon checking, refuted our conjecture while providing a valid solution. The model failed on Problem 4, which required combining results from two papers. On Problem 5, a harder case without a validated conjecture, GPT-5 proposed the same algorithm we had in mind but failed in the analysis, suggesting the proof is more challenging than expected. Although our sample is small, the results point to meaningful progress on routine reasoning, occasional flashes of originality, and clear limitations when cross-paper synthesis is required. GPT-5 may represent an early step toward frontier models eventually passing the Gödel Test.",
        "subjects": "Artificial Intelligence, Discrete Mathematics, Machine Learning",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.174016",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是评估和探讨大语言模型(GPT-5)的数学推理能力，特别是解决简单数学猜想的能力，这直接涉及LLM的基础推理能力提升，而非将LLM作为工具应用于特定领域。论文提出的\"Gödel Test\"旨在评估模型产生正确证明的能力，这属于通用推理能力的核心范畴。 其次，论文满足多个正面指标：明确以大型语言模型(LLMs)为研究对象；专注于推理能力(reasoning)，特别是数学推理(math reasoning)；涉及问题解决(problem-solving)等关键能力方向。 第三，论文不符合任何排除标准。虽然研究内容涉及数学领域，但数学在这里是作为评估LLM推理能力的测试场，而非作为特定应用领域。论文的核心不是解决数学问题本身，而是研究LLM的推理能力表现和局限。 论文的核心贡献在于提出了一种评估LLM高级推理能力的新方法(Gödel Test)，并通过实验揭示了当前模型在数学推理方面的进展、原创性闪光点以及局限性。这直接服务于提升LLM通用推理能力的研究目标，因此应该被保留。"
    },
    {
        "index": "#31",
        "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints",
        "link": "/arxiv/2509.18382",
        "arxiv_id": "2509.18382",
        "authors": "Adarsha Balaji, Le Chen, Rajeev Thakur, Franck Cappello, Sandeep Madireddy",
        "summary": "Test-time compute scaling has demonstrated the ability to improve the performance of reasoning language models by generating longer chain-of-thought (CoT) sequences. However, this increase in performance comes with a significant increase in computational cost. In this work, we investigate two compute constraint strategies: (1) reasoning length constraint and (2) model quantization, as methods to reduce the compute demand of reasoning models and study their impact on their safety performance. Specifically, we explore two approaches to apply compute constraints to reasoning models: (1) fine-tuning reasoning models using a length controlled policy optimization (LCPO) based reinforcement learning method to satisfy a user-defined CoT reasoning length, and (2) applying quantization to maximize the generation of CoT sequences within a user-defined compute constraint. Furthermore, we study the trade-off between the computational efficiency and the safety of the model.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.174272",
        "filter_reason": "这篇论文的核心是研究如何在计算约束条件下提升大型推理模型的推理能力和安全性，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。论文主要探讨了通过思维链（CoT）和强化学习（LCPO方法）来优化模型的推理能力，这属于改进LLM基础能力和提出新训练范式的范畴。论文明确关注推理能力这一核心指标，并研究了如何通过强化学习方法来控制推理长度，同时保持模型的安全性。论文不是将LLM作为工具应用到特定领域，而是关注LLM本身的通用推理能力提升。虽然论文涉及安全性研究，但这是从提升模型内在推理质量和可靠性的角度出发，而非仅作为应用层面的防御。因此，这篇论文完全符合筛选标准，应该被保留。"
    },
    {
        "index": "#94",
        "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization",
        "link": "/arxiv/2509.18851",
        "arxiv_id": "2509.18851",
        "authors": "Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu, Dexun Wang, Chunmei Xie, Weiqi Xiong, Xianzhou Zeng, Qixuan Zhou, Yadong Li, Xingzhong Xu",
        "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs) across various tasks. However, GRPO, a representative RLVR algorithm, suffers from a critical limitation: when all responses within a group are either entirely correct or entirely incorrect, the model fails to learn from these homogeneous responses. This is particularly problematic for homogeneously incorrect groups, where GRPO's advantage function yields a value of zero, leading to null gradients and the loss of valuable learning signals. To overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy Optimization), an algorithm designed to convert homogeneous errors into robust learning signals. First, NGRPO introduces Advantage Calibration. This mechanism hypothesizes the existence of a virtual maximum-reward sample during advantage calculation, thereby altering the mean and variance of rewards within a group and ensuring that the advantages for homogeneously incorrect samples are no longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the update magnitude for positive samples while imposing stricter constraints on that of negative samples. This serves to stabilize the exploration pressure introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO, DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and AIME2025. These results validate NGRPO's ability to learn from homogeneous errors, leading to stable and substantial improvements in mathematical reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.205817",
        "filter_reason": "这篇论文的核心贡献是提出NGRPO算法，一种改进的强化学习方法，用于增强大语言模型的推理能力。论文针对GRPO算法在处理同质响应组（全部正确或全部错误）时的局限性，提出了两个关键机制：Advantage Calibration和Asymmetric Clipping，使模型能够从同质错误中学习。这直接符合研究目标，因为：(1)论文本质上是改进LLM的基础推理能力，特别是数学推理能力；(2)它提出新的训练范式（强化学习优化）；(3)实验证明该方法能显著提升LLM在多个数学基准测试上的表现。论文不涉及特定应用领域、多模态研究或模型基础设施优化，完全符合\"致力于提高大语言模型本身的通用推理能力\"的核心目标。"
    },
    {
        "index": "#135",
        "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts",
        "link": "/arxiv/2509.18542",
        "arxiv_id": "2509.18542",
        "authors": "Qi Wang, Hanyang Peng, Yue Yu",
        "summary": "Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To circumvent the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a single lightweight stage of router training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.226019",
        "filter_reason": "这篇论文的核心贡献是提出了一种新的Mixture-of-Experts (MoE)模型构建方法Symphony-MoE，该方法通过整合多个不同但架构相同的预训练模型（如Llama2-Chat和Code Llama）来提高整体性能。论文不是将LLM作为工具应用到特定领域，而是关注如何改进LLM的基础架构和能力。它提出了一种两阶段框架来解决不同预训练模型在参数空间中的不协调问题，通过层感知融合策略和基于激活的功能对齐来协调这些模型，然后进行轻量级的路由器训练。论文提到其方法在\"多领域任务\"和\"分布外泛化\"方面取得了显著成果，这表明它关注的是模型的通用能力，而非特定领域应用。虽然论文没有直接提到推理、规划等具体能力，但它提出的架构改进本质上是为了提升LLM的通用能力，符合研究目标中\"改进LLM的基础能力、提出新的训练范式\"的要求。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#193",
        "title": "Self-Evolving LLMs via Continual Instruction Tuning",
        "link": "/arxiv/2509.18133",
        "arxiv_id": "2509.18133",
        "authors": "Le Huang, Jiazheng Kang, Cheng Hou, Zhe Zhao, Zhenxiang Yan, Chuan Shi, Ting Bai",
        "summary": "In real-world industrial settings, large language models (LLMs) must learn continually to keep pace with diverse and evolving tasks, requiring self-evolution to refine knowledge under dynamic data distributions. However, existing continual learning (CL) approaches, such as replay and parameter isolation, often suffer from catastrophic forgetting: training on new tasks degrades performance on earlier ones by overfitting to the new distribution and weakening generalization.We propose MoE-CL, a parameter-efficient adversarial mixture-of-experts framework for industrial-scale, self-evolving continual instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated LoRA expert per task to preserve task-specific knowledge via parameter independence, mitigating forgetting; and (2) a shared LoRA expert to enable cross-task transfer. To prevent transferring task-irrelevant noise through the shared pathway, we integrate a task-aware discriminator within a GAN. The discriminator encourages the shared expert to pass only task-aligned information during sequential training. Through adversarial learning, the shared expert acquires generalized representations that mimic the discriminator, while dedicated experts retain task-specific details, balancing knowledge retention and cross-task generalization and thereby supporting self-evolution.Extensive experiments on the public MTL5 benchmark and an industrial Tencent3 benchmark validate the effectiveness of MoE-CL for continual instruction tuning. In real-world A/B testing for content compliance review on the Tencent Video platform, MoE-CL reduced manual review costs by 15.3%. These results demonstrate that MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-14",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.255992",
        "filter_reason": "这篇论文的核心贡献是提出MoE-CL框架，一种用于大语言模型自进化的参数高效对抗性混合专家框架。论文直接针对LLM的基础能力——持续学习和自进化能力进行研究，提出通过双专家设计（每个任务的专用LoRA专家和共享LoRA专家）以及GAN中的任务感知判别器来解决持续学习中的灾难性遗忘问题。这明显符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求，因为自进化能力是LLM通用推理能力的重要组成部分。论文虽然提到了在Tencent Video平台的应用案例，但这只是验证方法有效性的实验，论文的核心并不是针对特定应用领域的研究，而是提出了一种通用的训练范式来增强LLM的基础能力。论文明确涉及\"self-evolution\"这一正面指标，并且不涉及任何排除标准中的领域。因此，这篇论文完全符合筛选标准，应该被保留。"
    },
    {
        "index": "#201",
        "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization",
        "link": "/arxiv/2509.18116",
        "arxiv_id": "2509.18116",
        "authors": "Nathan Egbuna, Saatvik Gaur, Sunishchal Dev, Ashwinee Panda, Maheep Chaudhary",
        "summary": "Test-time optimization remains impractical at scale due to prohibitive inference costs\\textemdash techniques like iterative refinement and multi-step verification can require $10$--$100\\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101\\% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-10",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:40:12.257607",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础推理能力。论文提出了\"Amortized Latent Steering (ALS)\"方法，这是一种新的训练/推理范式，通过潜在空间引导来增强模型的推理能力，特别是数学推理能力。ALS将测试时优化的迭代过程转化为离线计算的向量，在推理过程中以恒定成本应用，从而在保持或提高推理性能的同时显著降低计算成本。这明确符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、多步推理等通用能力\"的保留标准。 其次，从正面指标来看，论文包含了多个相关主题： - 核心概念：论文虽然未在摘要中明确提到\"Large language models\"，但从上下文（如提到Chain-of-Thought, Self-Consistency等LLM相关技术）可以推断这是针对LLM的研究。 - 能力方向：论文明确在GSM8K和MATH-500这两个数学推理基准测试上评估方法，表明论文关注数学推理能力，并提到\"sophisticated reasoning techniques\"，进一步确认其与推理相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域（如医疗、化学、生物等）或模型可靠性（应用层面）的研究。 最后，论文不涉及特殊或模糊情况，如智能体/工具使用或幻觉/可解释性/安全等问题。 综上所述，这篇论文的核心贡献是提出了一种提高LLM数学推理效率的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#1",
        "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT",
        "link": "/arxiv/2509.19284",
        "arxiv_id": "2509.19284",
        "authors": "Yunzhen Feng, Julia Kempe, Cheng Zhang, Parag Jain, Anthony Hartshorn",
        "summary": "Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the \"longer-is-better\" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy. As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",
        "subjects": "Machine Learning",
        "date": "2025-09-23",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:40:12.092441",
        "filter_reason": "这篇论文的核心贡献是研究什么构成了有效的思维链(CoT)推理，并提出了一种新的方法来评估和改进CoT的质量。论文通过系统评估发现，更长的CoT并不总是更好的，并引入了失败步骤分数(FSF)这一指标来预测CoT的正确性。作者还设计了两种干预措施来测试因果关系，证明有效的CoT是那些\"失败较少\"的，并支持\"结构感知\"的测试时扩展。这直接关系到改进LLM的基础推理能力，特别是数学和科学推理能力，属于通用推理能力的范畴。论文不涉及任何需要排除的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然论文在数学和科学推理上进行了评估，但这些只是作为评估推理能力的基准领域，而非论文的应用目标。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#8",
        "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws",
        "link": "/arxiv/2509.19189",
        "arxiv_id": "2509.19189",
        "authors": "Binghui Li, Fengling Chen, Zixun Huang, Lean Wang, Lei Wu",
        "summary": "Scaling laws have played a cornerstone role in guiding the training of large language models (LLMs). However, most existing works on scaling laws primarily focus on the final-step loss, overlooking the loss dynamics during the training process and, crucially, the impact of learning rate schedule (LRS). In this paper, we aim to bridge this gap by studying a teacher-student kernel regression setup trained via online stochastic gradient descent (SGD). Leveraging a novel intrinsic time viewpoint and stochastic differential equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL), which characterizes the evolution of population risk during the training process for general LRSs. Remarkably, the impact of the LRSs is captured through an explicit convolution-type functional term, making their effects fully tractable. To illustrate the utility of FSL, we analyze three widely used LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under both data-limited and compute-limited regimes. We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as (i) higher-capacity models are more data- and compute-efficient; (ii) learning rate decay can improve training efficiency; (iii) WSD-like schedules can outperform direct-decay schedules. Lastly, we explore the practical relevance of FSL as a surrogate model for fitting, predicting and optimizing the loss curves in LLM pre-training, with experiments conducted across model sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen the understanding of LLM pre-training dynamics and provide insights for improving large-scale model training.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:40:12.094375",
        "filter_reason": "这篇论文的核心贡献是提出\"功能缩放定律\"(FSL)框架，用于理解和优化大语言模型的训练动态，特别是学习率计划(LLM训练中的关键超参数)如何影响训练过程。虽然论文没有直接研究推理能力或解决问题等具体能力，但它关注的是LLM的基础训练机制，属于\"改进LLM的基础能力\"和\"提出新的训练范式\"的范畴。论文通过优化训练过程和学习率计划，为提高LLM的通用能力提供了理论基础和实践指导。论文明确研究LLM训练，不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标，因为它通过改进训练方法来提升模型的基础能力，而非将LLM作为工具应用到特定领域。"
    },
    {
        "index": "#11",
        "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio",
        "link": "/arxiv/2509.19128",
        "arxiv_id": "2509.19128",
        "authors": "Alexandre Piché, Ehsan Kamaloo, Rafael Pardinas, Dzmitry Bahdanau",
        "summary": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs). However, effectively scaling these RL methods presents significant challenges, primarily due to the difficulty in maintaining high AI accelerator utilization without generating stale, off-policy data that harms common RL algorithms. This paper introduces PipelineRL, an approach designed to achieve a superior trade-off between hardware efficiency and data on-policyness for LLM training. PipelineRL employs concurrent asynchronous data generation and model training, distinguished by the novel in-flight weight updates. This mechanism allows the LLM generation engine to receive updated model weights with minimal interruption during the generation of token sequences, thereby maximizing both the accelerator utilization and the freshness of training data. Experiments conducted on long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL achieves approximately $\\sim 2x$ faster learning compared to conventional RL baselines while maintaining highly on-policy training data. A scalable and modular open-source implementation of PipelineRL is also released as a key contribution.",
        "subjects": "Machine Learning",
        "date": "2025-09-23",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:40:12.094940",
        "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的强化学习训练范式(PipelineRL)来增强LLM的推理能力。论文明确提到\"Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs)\"，表明其核心是提升LLM的推理能力这一基础能力。 其次，论文包含多个正面指标：核心概念方面直接涉及Large language models (LLMs)；能力方向上关注reasoning；训练方法上专注于reinforcement learning，这些都是研究目标中明确关注的重点。 第三，论文完全避开了排除标准中的所有领域，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文虽然不直接涉及智能体/工具使用或幻觉/可解释性/安全等问题，但通过提高训练效率和数据新鲜度(on-policyness)来改进RL训练质量，这从根本上提升了LLM的推理能力，而非仅作为应用层面的优化。 综上所述，PipelineRL论文的核心贡献是提出了一种更高效的RL训练方法来提升LLM的推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#15",
        "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment",
        "link": "/arxiv/2509.19104",
        "arxiv_id": "2509.19104",
        "authors": "Sharan Sahu, Martin T. Wells",
        "summary": "Reinforcement learning with human feedback (RLHF) has become crucial for aligning Large Language Models (LLMs) with human intent. However, existing offline RLHF approaches suffer from overoptimization, where models overfit to reward misspecification and drift from preferred behaviors observed during training. We introduce DRO-REBEL, a unified family of robust REBEL updates with type-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality, each update reduces to a simple relative-reward regression, preserving scalability and avoiding PPO-style clipping or auxiliary value networks. Under standard linear-reward and log-linear policy classes with a data-coverage condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$ rate via a localized Rademacher complexity analysis. The same analysis closes the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal parametric rates. We derive practical SGD algorithms for all three divergences: gradient regularization (Wasserstein), importance weighting (KL), and a fast 1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong worst-case robustness across unseen preference mixtures, model sizes, and data scales, with $\\chi^2$-REBEL showing consistently strong empirical performance. A controlled radius--coverage study validates a no-free-lunch trade-off: radii shrinking faster than empirical divergence concentration rates achieve minimax-optimal parametric rates but forfeit coverage, while coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:40:12.095759",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出一种新的强化学习对齐方法(DRO-REBEL)，用于改进LLM与人类意图的对齐效果。论文核心是解决现有RLHF方法中的过优化问题，提出了一种分布鲁棒相对奖励回归的新训练范式。这属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用到特定领域。因此，根据第一步的判断标准，应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念：明确提到了Large Language Models (LLMs) - 训练方法：重点研究了Reinforcement Learning with Human Feedback (RLHF)的改进，属于强化学习优化范畴 虽然论文没有直接提到reasoning、planning等能力方向，但RLHF作为提升LLM与人类意图对齐的方法，本质上可以增强模型的通用推理能力。 第三步：排除标准 论文不涉及任何排除领域： - 没有涉及多模态与视觉相关内容 - 没有针对特定应用领域（如医疗、化学、生物等） - 没有主要关注模型可靠性方面的应用层面研究（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及智能体/工具使用、幻觉/可解释性/安全等特殊或模糊情况。论文提出的DRO-REBEL方法是从根本上提升LLM与人类意图对齐的能力，属于基础能力改进。 最终决策 综合以上分析，这篇论文的核心贡献是提出了一种新的强化学习对齐方法，用于改进LLM的基础能力，使其更好地与人类意图对齐。这符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标，因为更好的对齐可以提升模型在各种推理任务中的表现。因此，最终判断为True。"
    },
    {
        "index": "#49",
        "title": "Reflect before Act: Proactive Error Correction in Language Models",
        "link": "/arxiv/2509.18607",
        "arxiv_id": "2509.18607",
        "authors": "Qiuhai Zeng, Sarvesh Rajkumar, Di Wang, Narendra Gyanchandani, Wenbo Yan",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in interactive decision-making tasks, but existing methods often struggle with error accumulation and lack robust self-correction mechanisms. We introduce \"Reflect before Act\" (REBACT), a novel approach that enhances LLM-based decision-making by introducing a critical reflect step prior to taking the next action. This approach allows for immediate error correction, ensuring smooth action path and adaptibity to environment feedback. We evaluate REBACT on three diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results demonstrate that REBACT significantly outperforms strong baselines, improving success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld (achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's performance improvements are achieved with only a few modification steps, demonstrating its computational efficiency.",
        "subjects": "Machine Learning",
        "date": "2025-09-23",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:40:12.108922",
        "filter_reason": "这篇论文符合我的研究目标，理由如下： 首先，从核心判断来看，论文本质上是提出一种名为\"Reflect before Act\" (REBACT)的新方法，通过在行动前引入反思步骤来增强语言模型的决策能力。这明显属于改进LLM基础能力和增强其通用推理能力的范畴，特别是通过自我纠错机制来提升模型的逻辑推理能力，而不是将LLM作为工具应用到特定领域。 其次，从正面指标分析，论文明确提到了\"Large Language Models (LLMs)\"这一核心概念，并且关注的是\"decision-making\"能力，这与推理和问题解决密切相关。虽然未直接提及reasoning，但反思和纠错机制本质上是在提升模型的推理质量。 第三，从排除标准看，论文不涉及多模态与视觉内容，也没有专注于特定应用领域（如医疗、化学等）。虽然提到了ALFWorld、WebShop和TextCraft三个评估环境，但这些是用于测试通用能力的基准环境，而非特定应用领域。 最后，在特殊和模糊情况处理上，REBACT是一种通用的反思机制，用于增强LLM的通用问题解决能力，而不是针对特定领域的应用。论文关注的是如何通过反思步骤提升模型的内在纠错能力，从而从根本上提高模型的推理质量，这完全符合\"提升LLM通用推理能力\"的研究目标。 综上所述，这篇论文的核心贡献是提出了一种通过前瞻性反思来增强LLM自我纠错能力的新方法，属于提升大语言模型通用推理能力的研究范畴，因此应该被保留。"
    },
    {
        "index": "#82",
        "title": "DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns",
        "link": "/arxiv/2509.18164",
        "arxiv_id": "2509.18164",
        "authors": "Ranfei Chen, Ming Chen",
        "summary": "Diffusion large language models (dLLMs) have emerged as a new architecture following auto regressive models. Their denoising process offers a powerful generative advantage, but they present significant challenges in learning and understanding numerically sensitive mathematical and order-sensitive logical tasks. Current training methods, including pre-training, fine-tuning, and reinforcement learning, focus primarily on improving general knowledge retention and reasoning abilities, but lack a comprehensive understanding of mathematical and logical patterns. We propose DSFT, a simple yet effective Diffusion SFT strategy, by adjusting the masking strategy and loss function, guiding models to understand mathematical and logical patterns. This strategy can be flexibly combined with pre-training, reinforcement learning, and other training methods. Validated on models such as LLaDA and Dream series, we prove that DSFT on small-scale data can achieve improvements of 5-10% and approximately 2% on mathematical and logical problems, respectively. This inspiring masking approach offers insights for future learning of specific patterns, which can be easily and efficiently combined with other training methods and applied to various dLLMs. Our code is publicly available at https://anonymous.4open.science/r/DSFT-0FFB/",
        "subjects": "Machine Learning",
        "date": "2025-09-17",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:40:12.122268",
        "filter_reason": "这篇论文的核心贡献是提出DSFT（Diffusion SFT）策略，通过调整掩码策略和损失函数来增强扩散大语言模型(dLLMs)对数学和逻辑模式的理解能力。论文明确关注LLM的通用推理能力，特别是数学推理和逻辑推理，这完全符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的要求。论文不是将LLM作为工具应用到特定领域，而是直接提升模型本身的推理能力，且不涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性的应用层面研究）。此外，论文提出的方法可以与预训练、强化学习等其他训练方法结合，显示出其作为提升LLM通用推理能力的普适性。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#69",
        "title": "Towards Provable Emergence of In-Context Reinforcement Learning",
        "link": "/arxiv/2509.18389",
        "arxiv_id": "2509.18389",
        "authors": "Jiuqi Wang, Rohan Chandra, Shangtong Zhang",
        "summary": "Typically, a modern reinforcement learning (RL) agent solves a task by updating its neural network parameters to adapt its policy to the task. Recently, it has been observed that some RL agents can solve a wide range of new out-of-distribution tasks without parameter updates after pretraining on some task distribution. When evaluated in a new task, instead of making parameter updates, the pretrained agent conditions its policy on additional input called the context, e.g., the agent's interaction history in the new task. The agent's performance increases as the information in the context increases, with the agent's parameters fixed. This phenomenon is typically called in-context RL (ICRL). The pretrained parameters of the agent network enable the remarkable ICRL phenomenon. However, many ICRL works perform the pretraining with standard RL algorithms. This raises the central question this paper aims to address: Why can the RL pretraining algorithm generate network parameters that enable ICRL? We hypothesize that the parameters capable of ICRL are minimizers of the pretraining loss. This work provides initial support for this hypothesis through a case study. In particular, we prove that when a Transformer is pretrained for policy evaluation, one of the global minimizers of the pretraining loss can enable in-context temporal difference learning.",
        "subjects": "Machine Learning",
        "date": "2025-09-22",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:40:12.118942",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，理由如下： 第一步：核心判断 这篇论文的核心是关于强化学习(RL)预训练如何使大语言模型获得上下文强化学习(ICRL)能力的研究。论文探讨的是为什么RL预训练算法能够产生支持ICRL的网络参数，并提出了假设：能够进行ICRL的参数是预训练损失的最小值。这属于改进LLM基础能力和提出新训练范式的研究，特别是关注模型如何在不更新参数的情况下通过上下文信息进行学习和推理，这与通用推理能力直接相关。 第二步：正面指标 论文包含多个正面指标： - 核心概念：论文研究的是Transformer模型在强化学习中的表现，Transformer是大语言模型的基础架构 - 能力方向：论文关注的是推理能力，特别是上下文学习(in-context learning)和时序差分学习(temporal difference learning)，这些都是通用推理能力的重要组成部分 - 训练方法：论文探讨强化学习预训练方法，这与RLHF/RL相关 第三步：排除标准 论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉 - 不针对特定应用领域（如医疗、化学等） - 不主要关注模型基础设施或部署优化 第四步：处理特殊和模糊情况 论文研究的是一种通用的学习机制（上下文强化学习），而不是将其应用于特定领域。它探讨的是模型如何通过预训练获得在不更新参数的情况下适应新任务的能力，这属于提升模型内在推理能力的研究。 第五步：最终决策 综合分析，这篇论文的核心贡献是研究大语言模型如何通过预训练获得上下文强化学习能力，这是一种重要的通用推理能力。论文探讨的是模型的基础能力和训练范式，而不是将模型作为工具应用于特定领域。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#3",
        "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing",
        "link": "/arxiv/2509.20336",
        "arxiv_id": "2509.20336",
        "authors": "Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang",
        "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-25T09:45:34.877973",
        "filter_reason": "这篇论文符合研究范围，应该被保留。我的判断过程如下： 第一步：核心判断 这篇论文的本质是研究Transformer-based LLMs在图推理任务中的内部机制和工作原理。论文使用了circuit-tracer框架来解释decoder-only transformers的推理过程，识别了图推理中的两个核心机制（token merging和structural memorization），并提供了一个统一的可解释性框架。这属于研究LLM基础能力和推理机制的范畴，而不是将LLM作为工具应用到特定领域，因此应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念：明确提到了\"Transformer-based LLMs\" - 能力方向：聚焦于\"graph reasoning\"，属于推理能力的研究范畴 虽然论文没有提到强化学习等训练方法或智能体等新兴范式，但已经包含了两个重要的正面指标。 第三步：排除标准 论文没有主要聚焦于任何排除标准中提到的领域： - 不涉及多模态与视觉研究 - 不针对特定应用领域（如医疗、化学等），虽然研究图推理，但这是作为理解LLM通用推理能力的一个窗口 - 不关注模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况 论文涉及可解释性研究，但这是为了理解LLM的推理机制，提供\"统一的可解释性框架来理解结构推理\"，从而提升对模型内部工作原理的理解，符合研究目标，应该保留。 综上所述，这篇论文的核心贡献是揭示和解释LLM在图推理任务中的内部机制，这直接关系到提高LLM的通用推理能力，符合研究目标。"
    },
    {
        "index": "#13",
        "title": "Failure Modes of Maximum Entropy RLHF",
        "link": "/arxiv/2509.20265",
        "arxiv_id": "2509.20265",
        "authors": "Ömer Veysel Çağatan, Barış Akgün",
        "summary": "In this paper, we show that Simple Preference Optimization (SimPO) can be derived as Maximum Entropy Reinforcement Learning with length-normalized temperature, providing a theoretical foundation for this reference-free method. Motivated by SimPO's strong performance in offline preference optimization, we investigate whether Maximum Entropy RL can achieve similar results in online RLHF settings. Our experiments find that Maximum Entropy RL consistently exhibits overoptimization and unstable KL dynamics, even at very low learning rates. Unlike KL-constrained methods that maintain stable training, entropy regularization fails to prevent reward hacking and appears to correlate with overoptimization. Lastly, we discuss possible explanations for why SimPO succeeds in offline settings while Maximum Entropy RL struggles in online scenarios. Our findings suggest that reference-free approaches may face distinct challenges when applied to online or offline preference learning.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-25T09:45:34.879946",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，论文的核心是关于RLHF（Reinforcement Learning from Human Feedback）的研究，特别是最大熵RLHF的失败模式分析。RLHF是改进大语言模型基础能力的关键训练范式，属于\"强化学习优化\"这一类别，符合第一步的保留标准。论文虽然没有直接提出新方法，而是分析现有方法的问题，但这种分析有助于理解如何更好地训练LLM，从而提升其通用能力。其次，论文明确涉及强化学习（RLHF）这一训练方法，符合第二步中的正面指标。同时，论文不聚焦于多模态、特定应用领域或模型可靠性的应用层面，因此不触犯第三步的排除标准。总体而言，这篇论文对改进LLM的训练方法有贡献，与提升大语言模型通用推理能力的研究目标相符。"
    },
    {
        "index": "#45",
        "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning",
        "link": "/arxiv/2509.19894",
        "arxiv_id": "2509.19894",
        "authors": "Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong",
        "summary": "Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at https://github.com/inclusionAI/PromptCoT.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-25T09:45:34.892384",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提出PromptCoT 2.0框架，这是一种新的训练范式，旨在通过合成高质量提示词来增强LLM的基础推理能力。论文明确关注提升LLM在数学推理和编程推理等通用能力上的表现，而非将LLM作为工具应用于特定领域。 从正面指标看，论文包含多个相关主题：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于reasoning(特别是数学推理)、planning和problem-solving；训练方法上涉及self-play(类似强化学习的思想)和self-evolve(模型通过自我博弈自主改进)。 论文不涉及任何排除标准中的领域：没有关注多模态与视觉，没有将LLM应用于特定领域(虽然使用数学和编程作为评估任务，但这些是用于评估通用推理能力的基准)，也没有主要关注模型可靠性的应用层面。 在特殊情况下，论文提到的self-play可以视为一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非应用于特定领域的智能体。 论文的核心贡献是提出了一种可扩展的框架，通过迭代改进提示词构建来生成更难且更多样化的问题，从而提升LLM的推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#54",
        "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models",
        "link": "/arxiv/2509.19803",
        "arxiv_id": "2509.19803",
        "authors": "Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang",
        "summary": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-25T09:45:34.899918",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步：核心判断 这篇论文的本质是提出一种名为VCRL的课程强化学习框架，用于改进大语言模型的数学推理能力。论文核心关注的是改进LLM的基础推理能力，提出了一种新的训练范式（基于方差的课程强化学习），这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学推理等通用能力\"的保留标准。论文不是将LLM作为工具应用到特定领域，而是专注于提升模型本身的推理能力。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：专注于reasoning，特别是mathematical reasoning - 训练方法：提出了reinforcement learning的新方法(VCRL) 第三步：排除标准 论文不涉及任何需要排除的领域： - 不涉及多模态与视觉相关内容 - 不针对特定应用领域（如医疗、化学等），虽然实验在数学推理任务上进行，但数学推理被视为通用推理能力的基础组成部分 - 不关注模型基础设施、部署优化或硬件加速 第四步：特殊和模糊情况处理 虽然论文在数学推理任务上进行了实验评估，但这并不使其成为特定应用领域的研究。数学推理通常被视为评估和提升LLM通用推理能力的关键基准。论文提出的VCRL是一种通用的课程强化学习框架，其原理可以推广到其他需要逐步学习的推理任务上，因此应被视为对LLM通用推理能力的提升。 综上所述，这篇论文的核心贡献是提出了一种基于方差的课程强化学习方法，通过动态控制训练样本难度来提升LLM的推理能力，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#62",
        "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms",
        "link": "/arxiv/2509.19702",
        "arxiv_id": "2509.19702",
        "authors": "Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama",
        "summary": "We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nyström extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nyström extrapolation, highlighting a powerful capability of in-context learning.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-25T09:45:34.901690",
        "filter_reason": "根据筛选标准，我认为这篇论文符合研究范围。以下是我的判断过程： 第一步：核心判断上，论文本质是研究线性注意力transformer如何通过训练隐式发现数值算法的能力。这属于改进LLM基础能力的研究，特别是增强其数学推理和问题解决能力。论文探索了模型如何通过矩阵补全任务自动学习数学计算规则，而非被明确编程这些规则，这符合\"提出新的训练范式、增强其逻辑、数学推理等通用能力\"的保留标准。 第二步：正面指标方面，论文包含以下相关主题： - 核心概念：提及\"Linear Transformers\"，与LLMs直接相关 - 能力方向：涉及数值算法的发现，与数学推理和问题解决能力密切相关 - 论文展示了transformer的上下文学习能力，这是提升通用推理能力的关键 第三步：排除标准方面，论文不涉及任何排除领域： - 没有多模态与视觉内容 - 没有将LLM应用于特定领域（如医疗、化学等） - 没有聚焦于模型可靠性的应用层面（如水印、安全等） 第四步：论文不涉及特殊或模糊情况（如智能体/工具使用、幻觉/可解释性等），所以无需额外判断。 综合来看，这篇论文的核心贡献在于展示了transformer如何通过训练隐式发现统一的数值算法，这直接提升了LLM的数学推理和问题解决能力，属于\"大语言模型通用推理能力\"的研究范围。论文关注的是模型的基础能力提升，而非特定领域应用，因此符合研究目标。"
    },
    {
        "index": "#70",
        "title": "Mamba Modulation: On the Length Generalization of Mamba",
        "link": "/arxiv/2509.19633",
        "arxiv_id": "2509.19633",
        "authors": "Peng Lu, Jerry Huang, Qiuhao Zeng, Xinyu Wang, Boxing Wang, Philippe Langlais, Yufei Cui",
        "summary": "The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\\exp(-\\sum_{t=1}^N\\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.LG",
        "crawl_time": "2025-09-25T09:45:34.909127",
        "filter_reason": "这篇论文的核心是研究Mamba架构（一种大语言模型架构）在长上下文泛化方面的局限性，并提出了一种改进方法。论文通过分析Mamba在处理比预训练时更长的上下文时性能下降的问题，将其归因于状态空间动态的参数化，特别是状态转移矩阵A的特性。作者提出了一种频谱缩放方法来改进预训练Mamba模型，以实现稳健的长上下文泛化。这符合我的研究目标，因为论文关注的是改进大语言模型的基础能力（特别是长上下文推理能力），而不是将LLM应用于特定领域。论文的贡献在于增强模型本身的通用推理能力，使其能够更好地处理长序列，这对于复杂的多步推理任务至关重要。因此，这篇论文应该被保留。"
    },
    {
        "index": "#109",
        "title": "Thinking Augmented Pre-training",
        "link": "/arxiv/2509.20186",
        "arxiv_id": "2509.20186",
        "authors": "Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei",
        "summary": "This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\\%$ on several challenging reasoning benchmarks.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-25T09:45:34.927019",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文本质是改进LLM的基础能力和通用推理能力。论文提出\"Thinking augmented Pre-Training (TPT)\"方法，通过在文本数据中增加思维轨迹来增强LLM的预训练过程。这是一种新的训练范式，旨在通过逐步推理和分解来提高模型的学习效率和推理能力，而不是将LLM应用于特定领域。 第二步正面指标：论文包含多个正面指标。它明确关注\"Large language models (LLMs)\"这一核心概念，并专注于\"reasoning\"能力方向（论文提到\"step-by-step reasoning\"和\"reasoning benchmarks\"）。实验结果也证明该方法在多个具有挑战性的推理基准上提升了模型性能。 第三步排除标准：论文不涉及任何排除标准中的领域。它没有关注多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性的应用层面问题。 第四步特殊和模糊情况：论文没有涉及智能体/工具使用或幻觉/可解释性/安全等特殊情况，它直接关注通过思维轨迹增强来提升LLM的通用推理能力。 最终决策：论文的核心贡献是提出了一种通用的训练方法(TPT)来增强LLM的推理能力，提高数据效率，这与研究目标完全一致。实验表明该方法能显著提升模型在推理任务上的表现，因此应该被保留。"
    },
    {
        "index": "#126",
        "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
        "link": "/arxiv/2509.19736",
        "arxiv_id": "2509.19736",
        "authors": "Cheng Qian, Zuxin Liu, Akshara Prabhakar, Jielin Qiu, Zhiwei Liu, Haolin Chen, Shirley Kokane, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang",
        "summary": "Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-25T09:45:34.936074",
        "filter_reason": "这篇论文提出了UserRL框架，通过强化学习训练交互式以用户为中心的智能体，核心是关于提升大语言模型的交互能力和多轮对话能力。从第一步核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式（UserRL框架），并使用强化学习（GRPO算法）来优化模型，这符合\"改进LLM基础能力和提出新训练范式\"的保留标准。 从第二步正面指标看，论文包含多个相关主题：1)核心概念方面，论文明确基于Qwen3等大语言模型；2)训练方法方面，论文使用了强化学习(RL)方法；3)新兴范式方面，论文关注的是基于LLM的智能体(llm-based agents)的训练。 从第三步排除标准看，论文没有涉及多模态与视觉内容，没有聚焦于特定应用领域（如医疗、化学等），也没有主要关注模型可靠性的应用层面问题（如水印、安全性），因此不符合任何排除标准。 从第四步特殊和模糊情况处理看，论文提出的是通用的智能体训练框架，旨在增强LLM的通用交互能力，而不是将智能体应用在特定领域，因此应该保留。 综合分析，论文的核心贡献是提供了一种新的强化学习框架来提升LLM的交互式推理和多轮对话能力，这属于大语言模型通用推理能力的重要组成部分，与研究目标高度一致。"
    },
    {
        "index": "#143",
        "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
        "link": "/arxiv/2509.19517",
        "arxiv_id": "2509.19517",
        "authors": "Sai Teja Reddy Adapala",
        "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.LG",
        "crawl_time": "2025-09-25T09:45:34.945215",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，该论文的本质是研究大语言模型在认知负荷下的多跳推理能力限制。论文提出了计算认知负荷的形式理论，设计了新的评估方法(ICE基准测试)，旨在理解和提升LLMs在复杂推理任务中的表现。这明显属于改进LLM基础推理能力的研究，特别是关注多跳推理这一通用推理能力的核心方面，而非将LLM作为工具应用于特定领域。 其次，论文符合多个正面指标：它明确关注\"Large Language Models (LLMs)\"这一核心概念，并深入研究\"multi-hop reasoning\"(多跳推理)这一关键推理能力方向。虽然论文不涉及训练方法和新兴范式，但对推理能力的深入研究已足够表明其与研究目标的高度相关性。 第三，论文不符合任何排除标准：它不涉及多模态与视觉研究，不聚焦于任何特定应用领域(如医疗、化学等)，也不主要关注模型可靠性的应用层面问题(如水印、安全性等)。虽然论文结尾提到了\"安全性\"评估，但这是作为评估模型推理能力的一个方面，而非主要焦点。 综上所述，这篇论文的核心贡献是提出了新的理论框架和评估方法来理解和提升LLMs的通用推理能力，特别是在认知负荷条件下的多跳推理表现，完全符合研究目标。"
    },
    {
        "index": "#167",
        "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution",
        "link": "/arxiv/2509.19349",
        "arxiv_id": "2509.19349",
        "authors": "Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin",
        "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large language models (LLMs) to advance scientific discovery with state-of-the-art performance and unprecedented efficiency. Recent advances in scaling inference time compute of LLMs have enabled significant progress in generalized scientific discovery. These approaches rely on evolutionary agentic harnesses that leverage LLMs as mutation operators to generate candidate solutions. However, current code evolution methods suffer from critical limitations: they are sample inefficient, requiring thousands of samples to identify effective solutions, and remain closed-source, hindering broad adoption and extension. ShinkaEvolve addresses these limitations, introducing three key innovations: a parent sampling technique balancing exploration and exploitation, code novelty rejection-sampling for efficient search space exploration, and a bandit-based LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks, demonstrating consistent improvements in sample efficiency and solution quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution using only 150 samples, designs high-performing agentic harnesses for AIME mathematical reasoning tasks, identifies improvements to ALE-Bench competitive programming solutions, and discovers novel mixture-of-expert load balancing loss functions that illuminate the space of optimization strategies. Our results demonstrate that ShinkaEvolve achieves broad applicability with exceptional sample efficiency. By providing open-source accessibility and cost-efficiency, this work democratizes open-ended discovery across diverse computational problems.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-17",
        "category": "cs.LG",
        "crawl_time": "2025-09-25T09:45:34.956410",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础能力和提出新的训练范式。论文提出的ShinkaEvolve框架利用LLMs作为变异算子，通过演化的方式增强其问题解决和推理能力。论文的核心贡献是提高LLM在程序演化方面的样本效率和解决方案质量，而不是将LLM作为工具应用于特定领域，因此符合保留标准。 其次，论文包含多个正面指标：明确涉及大语言模型(LLMs)作为核心组件；关注推理能力，特别是在数学推理任务中的应用；采用演化(evolution)作为训练方法；并使用了基于LLM的智能体框架(evolutionary agentic harnesses)。 第三，论文不符合排除标准。虽然论文提到了一些应用案例(如圆包装、数学推理、竞争编程等)，但这些是作为评估框架性能的示例，而非论文的主要焦点。论文的核心是提出一种通用的程序演化框架，而非专注于多模态、特定应用领域或模型可靠性等排除领域。 最后，在特殊和模糊情况的处理上，论文提出的是一种通用的演化智能体框架，旨在增强LLM的通用问题解决能力，而不是针对特定领域的应用，因此应该保留。 综合来看，ShinkaEvolve论文致力于通过演化的方法增强LLM的通用推理和问题解决能力，与研究目标高度一致。"
    }
]