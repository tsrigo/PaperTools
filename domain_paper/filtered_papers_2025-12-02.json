[
    {
        "index": "#34",
        "title": "A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation",
        "link": "/arxiv/2512.02195",
        "arxiv_id": "2512.02195",
        "authors": "David Ph. Shakouri, Crit Cremers, Niels O. Schiller",
        "summary": "This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-12-01",
        "category": "cs.CL",
        "crawl_time": "2025-12-03T11:00:04.329426",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为MODOMA的**计算多智能体实验室环境**。其核心贡献并非将LLM作为工具去解决某个外部领域（如生物、金融）的问题，而是**构建了一个全新的多智能体框架**，用于研究语言习得这一特定过程。该框架包含一个“成人智能体”和一个“儿童智能体”，它们之间的交互是整个研究的核心。这完全符合“构建LLM智能体”和“多智能体系统”的定义。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **`Multi-Agent Systems (MAS)`**: 论文标题和摘要都明确指出这是一个“多智能体语言习得模拟”。 - **`Collaboration` / `Communication`**: 研究的核心机制就是两个智能体之间的交互和通信，儿童智能体通过这个过程进行学习。 - **`Self-Evolving` / `Self-Improvement`**: 儿童智能体通过与成人智能体的交互，从零开始“习得”语法知识，并最终形成一个“基于知识的语言模型”。这个过程本身就是一种通过环境反馈（与成人智能体的交互）进行的**自我完善和迭代**，属于自我演化的范畴。 3.  **第三步：排除标准** - 论文的主要贡献是关于多智能体的学习和演化机制，**不涉及**安全、对齐、可解释性或视觉等多模态内容。因此，没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的“推理”是关于语法规则的习得，这是在多智能体交互框架中发生的，而不是为了提升单个LLM的基础Token预测能力。因此，它符合“保留关于智能体如何进行规划或在复杂任务中进行多步推理”的规则。 - **自我演化的应用**: 这篇论文的核心就是提出一种新的“自我演化”机制（通过多智能体交互进行语言习得），因此即使其应用场景是“语言习得”这一特定领域，也应被保留。 **最终决策**: 这篇论文的核心贡献是构建了一个多智能体系统（MODOMA），并展示了该系统中的智能体如何通过交互和通信实现知识的自我演化（语言习得）。这精准地命中了您研究范围中的“多智能体”和“自我演化”两个核心方向。它不是简单的应用，而是关于智能体框架和演化机制的方法论创新。因此，应予以保留。"
    },
    {
        "index": "#45",
        "title": "OptPO: Optimal Rollout Allocation for Test-time Policy Optimization",
        "link": "/arxiv/2512.02882",
        "arxiv_id": "2512.02882",
        "authors": "Youkang Wang, Jian Wang, Rubing Chen, Tianyi Zeng, Xiao-Yong Wei, Qing Li",
        "summary": "Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-12-02",
        "category": "cs.CL",
        "crawl_time": "2025-12-03T11:00:04.340291",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献与“自我演化”方向高度契合。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM作为工具应用，而是提出了一种名为OptPO的新**方法论/框架**。该框架的核心是让LLM在测试时通过“自我生成的rollouts”和“反馈”来“适应”和“优化其策略”。这完全符合“自我演化”的定义，即智能体通过经验或环境反馈进行自我完善和迭代。它不是简单的应用，也不是关于基础设施或非Agentic的基础推理。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **自我演化**: 论文的核心是“Test-time policy optimization”（测试时策略优化），这是一种明确的自我演化机制。它通过“on-policy updates”（在策略更新）来迭代改进模型，直接对应了`Self-Improvement`、`Self-Refine`和`Iterative Improvement`。 - **智能体能力**: 虽然没有明确使用“Agent”一词，但“policy optimization”、“rollouts”和“adaptation”都是智能体研究中的核心概念。LLM在这里被视作一个能够根据自身行为（rollouts）调整策略的智能体。其优化过程可以看作是一种高级的`Self-Correction`。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态。它的焦点是计算效率和性能优化，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文在“reasoning benchmarks”上进行了测试，但其贡献并非提出一种新的推理方法（如CoT变体）。相反，它提出了一种**优化推理过程本身**的元框架。它研究的是智能体（LLM）如何更高效地利用其推理能力进行自我学习和策略调整，这属于智能体层面的规划与优化，符合保留条件。 **核心依据总结**: 该论文的核心贡献是提出了一种**新的、高效的LLM自我演化框架**。它通过一种统计学上最优的停止策略，动态地分配计算资源，并利用自我生成的数据来更新模型策略。这直接命中了您研究目标中的“自我演化”方向，是一种关于智能体如何进行自我完善和迭代的创新方法论。因此，这篇论文是您课题下的高度相关的前沿研究。"
    },
    {
        "index": "#48",
        "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
        "link": "/arxiv/2512.02472",
        "arxiv_id": "2512.02472",
        "authors": "Wenhao Yu, Zhenwen Liang, Chengsong Huang, Kishan Panaganti, Tianqing Fang, Haitao Mi, Dong Yu",
        "summary": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-12-02",
        "category": "cs.CL",
        "crawl_time": "2025-12-03T11:00:04.341726",
        "filter_reason": "这篇论文完全符合你的研究范围。 1.  **核心判断 (第一步)**: 论文的核心贡献是提出了一种名为 R-Few 的、用于引导LLM自我演化的新框架。这直接命中了你研究目标中的“自我演化”方向。它不是将LLM作为工具应用到某个领域，而是研究LLM如何通过一个新颖的框架进行自我完善和迭代，其本质是构建和改进一种演化方法论。因此，根据第一步的判断标准，应该**保留**。 2.  **正面指标 (第二步)**: 论文包含了大量你的核心关注点，如 `Self-Evolving` (自我演化), `Self-Improvement` (自我完善), `Iterative Improvement` (迭代改进), 以及 `Co-evolutionary dynamics` (协同演化动态)。其提出的 Challenger-Solver 框架本身就是一个多组件的智能体系统，与 Agentic AI 的理念高度契合。 3.  **排除标准 (第三步)**: 论文的研究焦点是演化机制的效率和稳定性，而非安全、对齐或多模态，因此没有触犯你的排除标准。 4.  **特殊和模糊情况 (第四步)**: 这篇论文是“自我演化应用”的完美范例。根据你的规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域（如‘用于化学实验的自我演化智能体’），也应该保留。” 本文的核心正是提出 R-Few 这一新的自我演化机制，虽然它在数学和推理任务上进行了验证，但其核心价值在于机制本身，而非应用领域，因此应该被**保留**。 **最终决策**: 综合以上分析，该论文为你关于“LLM智能体及其演化”的研究课题提供了直接相关的前沿方法论，是理想的筛选对象。"
    },
    {
        "index": "#51",
        "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
        "link": "/arxiv/2512.02425",
        "arxiv_id": "2512.02425",
        "authors": "Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang",
        "summary": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language, Information Retrieval, Machine Learning",
        "date": "2025-12-02",
        "category": "cs.CL",
        "crawl_time": "2025-12-03T11:00:04.343119",
        "filter_reason": "这篇论文完全符合您的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出了一种名为 \"WorldMM\" 的**新型多模态记忆智能体**。它不是简单地将现有LLM或智能体框架应用于视频领域，而是**构建了一个全新的智能体架构**。该架构包含三种互补的记忆模块（情景记忆、语义记忆、视觉记忆）和一个关键的**自适应检索智能体**。这个检索智能体能够自主地、迭代地选择和检索信息，直到满足任务需求。这完全符合“构建、改进LLM智能体”的核心目标，因此应予以保留。 2.  **第二步：正面指标——高度匹配** 论文包含了多个核心关注点： *   **核心范式**: 论文标题和摘要明确使用了 \"Agent\"，其核心是 `LLM-based Agents`。 *   **智能体能力**: 论文的绝对核心是 `Memory`，它提出了一种复杂的多模态记忆系统。同时，其“自适应检索智能体”的迭代决策过程，本质上是一种在复杂任务（长视频问答）中进行信息收集的 `Planning` 和多步推理能力。 3.  **第三步：排除标准——未触犯** *   **安全与对齐**: 论文未涉及安全、对齐或可解释性问题。 *   **多模态与视觉**: 这是本篇论文需要仔细辨析的关键点。虽然论文处理的是视频（视觉）数据，但其研究**核心并非新的视觉模型或多模态融合技术**。相反，视觉信息是作为智能体感知环境的一种输入，而论文的真正贡献在于**如何设计一个智能体架构来高效地存储、检索和利用这些多模态信息**。根据您的规则“除非它们被用作智能体感知环境的工具，而不是研究的核心”，这篇论文中的视觉正是作为智能体工具存在的，研究的核心是智能体本身，因此不应被排除。 4.  **第四步：处理特殊和模糊情况——符合保留条件** *   **推理/规划**: 论文中的“自适应检索智能体”通过迭代循环来决定何时停止检索，这是一个典型的智能体自主决策和规划过程。它不是在改进LLM的基础推理能力，而是在构建一个用于解决复杂任务的智能体框架，因此符合保留条件。 **最终决策**: 这篇论文的核心贡献在于**构建了一个具有先进多模态记忆和自主检索规划能力的单智能体系统**。它直接命中了您研究焦点中的“单智能体”方向，特别是在“记忆”和“规划”这两个子方向上做出了创新。尽管它应用于视频领域，但其方法论和框架本身具有普适性，是典型的Agentic AI研究。因此，这篇论文与您的研究目标高度相关，应被筛选为 **True**。"
    },
    {
        "index": "#41",
        "title": "Deep Research: A Systematic Survey",
        "link": "/arxiv/2512.02038",
        "arxiv_id": "2512.02038",
        "authors": "Zhengliang Shi, Yiqun Chen, Haitao Li, Weiwei Sun, Shiyu Ni, Yougang Lyu, Run-Ze Fan, Bowen Jin, Yixuan Weng, Minjun Zhu, Qiujie Xie, Xinyu Guo, Qu Yang, Jiayi Wu, Jujia Zhao, Xiaqiang Tang, Xinbei Ma, Cunxiang Wang, Jiaxin Mao, Qingyao Ai, Jen-Tse Huang, Wenxuan Wang, Yue Zhang, Yiming Yang, Zhaopeng Tu, Zhaochun Ren",
        "summary": "Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-11-24",
        "category": "cs.CL",
        "crawl_time": "2025-12-03T11:00:04.333036",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献不是提出一个全新的智能体，而是对“深度研究”这一新兴的LLM智能体范式进行系统性综述。它本质上是在**构建和定义**一个关于“研究智能体”的知识框架和蓝图。它详细拆解了这类智能体的构成部分（规划、信息获取、记忆、生成），这直接服务于“构建、改进或演化LLM智能体”的核心目标。它不是将智能体作为工具去解决一个外部领域问题，而是将智能体本身作为研究对象。 2.  **第二步：正面指标** - 论文摘要中包含了大量与你核心关注点高度匹配的正面指标： - **核心范式**: 明确提到了 `LLM-based Agents`，并将其定义为 `research agents`。 - **智能体能力**: 详细列出了 `query planning` (规划), `information acquisition` (工具使用), `memory management` (记忆)。这些都是单智能体研究的核心能力。 - **优化技术**: 提到了 `agentic reinforcement learning`，这是一种高级的智能体训练和演化方法。 3.  **第三步：排除标准** - 论文的主要焦点是智能体的架构和能力，不涉及 `Safety`, `Alignment`, `Interpretability` 等安全与对齐问题。 - 论文也未涉及 `Vision`, `MLLMs` 等多模态内容，其焦点是基于文本的智能体。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文明确将 `query planning` 作为智能体的一个核心组件进行讨论，这完全符合“保留”标准，因为它关注的是智能体如何进行规划和多步推理，而不是提升LLM本身的基础推理能力。 **最终决策**: 尽管这是一篇综述性论文而非提出新方法的研究论文，但它的核心贡献在于系统性地梳理和定义了“研究智能体”这一Agentic AI的重要分支。它为你提供了该领域的**路线图、核心组件和关键技术**，这对于你深入理解“LLM智能体及其演化”这一课题具有极高的参考价值。它精准地聚焦于**单智能体**的规划、工具使用和记忆等核心能力，完全符合你的研究焦点。因此，这篇论文是必须保留的前沿文献。"
    },
    {
        "index": "#22",
        "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
        "link": "/arxiv/2512.02631",
        "arxiv_id": "2512.02631",
        "authors": "Zhengcheng Wang, Zichuan Lin, Yijun Yang, Haobo Fu, Deheng Ye",
        "summary": "Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.",
        "subjects": "Machine Learning",
        "date": "2025-12-02",
        "category": "cs.LG",
        "crawl_time": "2025-12-03T11:00:04.707613",
        "filter_reason": "这篇论文的核心贡献是构建了一个名为 SeeNav-Agent 的新型视觉语言导航（VLN）智能体框架，并提出了两个关键创新：一个用于减少感知错误的视觉提示（VP）模块，以及一个用于提升规划能力的步级强化微调方法（SRGPO）。根据您的筛选标准，我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是**构建和改进一个LLM智能体**。它不是简单地将现有框架应用于导航任务，而是提出了新的方法论（VP和SRGPO）来增强智能体的感知和规划能力。特别是SRGPO，它通过强化学习让智能体根据环境反馈（奖励信号）进行自我优化，这完全符合“自我演化”的定义。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: 论文明确提出了一个 `LLM-based Agent` 框架。 - **智能体能力**: 论文的核心目标之一是解决智能体的 `Planning` 错误，并通过SRGPO `enhances its planning capability`。 - **演化机制**: SRGPO是一种强化学习算法，它通过奖励信号驱动智能体进行迭代优化，这属于 `Self-Improvement` 和 `Iterative Improvement` 的范畴。 3.  **第三步：排除标准** - **安全与对齐**: 论文不涉及安全、对齐或可解释性问题。 - **多模态与视觉**: 这是需要仔细判断的一点。虽然论文大量涉及视觉（`Vision-Language Navigation`, `LVLMs`），但它符合您设定的例外情况：“**除非它们被用作智能体感知环境的工具，而不是研究的核心**”。在这篇论文中，视觉模块是智能体感知物理环境的工具，而研究的**核心**是智能体整体的框架、规划策略和自我演化机制（SRGPO），而不是视觉模型本身的创新。因此，不应因此排除。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文明确聚焦于智能体的规划能力，并提出了新的方法来优化其策略，这属于“保留”的范畴。 - **自我演化的应用**: 论文的核心贡献之一是提出了一种新的“自我演化”机制（SRGPO），即使它被应用在导航这个特定领域，根据您的规则也应该保留。 **最终决策**: 这篇论文的核心贡献在于**构建了一个新的智能体框架，并为其设计了一种自我演化的学习机制**。它直接对齐了您研究目标中的“单智能体”（规划、感知）和“自我演化”（通过环境反馈进行迭代改进）两个方向。尽管其应用场景是视觉导航，但论文的焦点在于智能体本身的方法论创新，而非应用领域或视觉技术本身。因此，这篇论文完全符合您的研究范围。"
    },
    {
        "index": "#28",
        "title": "In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs",
        "link": "/arxiv/2512.02543",
        "arxiv_id": "2512.02543",
        "authors": "Vishnu Sarukkai, Asanshay Gupta, James Hong, Michaël Gharbi, Kayvon Fatahalian",
        "summary": "The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. We propose a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly, we introduce $\\textit{in-context distillation}$, which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. Our approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. We combine in-context distillation with the established idea of $\\textit{self-consistency cascades}$ to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, our method matches teacher-level accuracy at $\\textbf{2.5$\\times$ lower cost}$, reducing per-episode costs from \\$0.059 to \\$0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding \\$34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\\textbf{2$\\times$ cost reduction}$ at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, our approach makes advanced agentic systems economically viable for a broader range of applications.",
        "subjects": "Machine Learning",
        "date": "2025-12-02",
        "category": "cs.LG",
        "crawl_time": "2025-12-03T11:00:04.710444",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为“上下文蒸馏”的新方法，旨在显著降低LLM智能体的推理成本。根据您的筛选标准，这篇论文应该被保留，具体判断过程如下： 1.  **第一步：核心判断——保留** - 论文的本质不是将现有智能体作为工具去解决某个特定领域（如生物、金融）的问题，而是聚焦于**改进LLM智能体本身**。它提出了一种新的方法论，让一个低成本的“学生”模型能够动态模仿一个高成本的“教师”智能体的行为。这直接属于“改进LLM智能体”的范畴，符合您的核心目标。 - 它不是关于提高LLM的基础推理能力（如数学），而是关于**优化智能体在执行多步任务时的成本效益**，这属于Agentic框架的改进。 - 虽然涉及推理成本，但其方法是在智能体的执行循环中实现的（“在每个智能体步骤”），是一种算法层面的创新，而非底层的基础设施或硬件加速。因此，不应被归为“基础设施”类研究。 2.  **第二步：正面指标——高度相关** - 论文明确围绕 `LLM-based Agents` 展开，并讨论了 `agentic designs` 和 `agentic systems`。 - 其方法在 `ALFWorld`（一个多步具身推理基准）和 `AppWorld`（一个需要多步API工作流的基准）上进行验证。这两个基准都深度依赖智能体的 `Planning`（规划）和 `Tool Use`（工具使用）能力。论文的核心贡献正是让这些复杂的智能体行为变得更经济高效。 3.  **第三步：排除标准——未触发** - 论文的主要贡献是关于成本和效率，不涉及 `Safety`、`Alignment`、`Interpretability` 或多模态等排除主题。 4.  **第四步：处理特殊和模糊情况——符合保留规则** - **推理/规划**: 论文的研究对象是智能体在复杂任务中的多步推理过程（如ALFWorld中的任务执行），其方法是为了让这个过程更便宜，而不是改进LLM本身的基础逻辑或数学能力。因此，它符合“保留”关于智能体推理/规划的论文的规则。 **最终决策**: 这篇论文的核心贡献在于提出了一种**改进LLM智能体运行效率和可扩展性**的新方法。虽然它没有提出全新的智能体架构、多智能体协作机制或自我演化范式，但它解决了当前LLM智能体研究和应用中的一个关键瓶颈：高昂的推理成本。通过让更强大的智能体行为能够以更低成本复现，该方法极大地促进了智能体系统的快速迭代和大规模部署。这完全符合您“构建、改进或演化LLM智能体”的核心研究目标，特别是“改进”这一维度。因此，这篇论文与您的研究范围高度相关，应该被保留。"
    },
    {
        "index": "#82",
        "title": "Self-Improving AI Agents through Self-Play",
        "link": "/arxiv/2512.02731",
        "arxiv_id": "2512.02731",
        "authors": "Przemyslaw Chojecki",
        "summary": "We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $ν_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $Θ$, and we identify the coefficient of self-improvement $κ$ as the Lie derivative of the capability functional along this flow. The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $κ> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough. We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-12-02",
        "category": "cs.LG",
        "crawl_time": "2025-12-03T11:00:04.762354",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献直接命中了“自我演化”这一核心方向。 **判断过程分析:** 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是应用，而是提出一个全新的、高度理论化的框架来理解和统一智能体的自我改进机制。其核心贡献是形式化了一个“Generator-Verifier-Updater (GVU)”算子，并推导出保证自我改进稳定性的“Variance Inequality”。这是一个关于智能体如何进行“自我演化”的基础性方法论和理论框架，完全符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文摘要中包含了大量与你研究焦点高度相关的核心范式和能力关键词： - **核心范式**: `Self-Evolving` (体现在标题和摘要的 \"Self-Improving\"), `Multi-Agent Systems` (体现在 \"Self-Play\")。 - **智能体能力**: `Self-Correction` (被明确提及为该框架统一的概念之一)。 - **演化机制**: `Self-Improvement`, `Iterative Improvement` (GVU算子是递归的，体现了迭代改进)。 - 论文通过统一STaR, SPIN, Reflexion, AlphaZero等知名自我演化架构，进一步证明了其在“自我演化”领域的核心地位。 3.  **第三步：排除标准** - 论文的主要贡献是关于智能体自我演化的数学理论，不涉及安全、对齐、可解释性或水印等问题。 - 论文也未涉及多模态或视觉，其焦点纯粹在智能体的演化机制上。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 该论文提出的GVU框架是一个高级的智能体框架，它统一了包含自我反思在内的多种自我演化路径，远超于对LLM基础推理能力的改进，因此符合保留条件。 - **自我演化的应用**: 这篇论文本身就是提出“自我演化”机制的典范，而非应用。它为其他应用提供了理论基础，因此是必须保留的核心论文。 **最终决策:** 这篇论文的核心贡献在于构建了一个名为GVU的数学框架，用以描述、分析和统一AI智能体的自我演化过程。它不仅提出了新的理论（如Variance Inequality），还成功地将现有多种前沿的自我演化方法（如Self-Play, Self-Correction）纳入其理论体系之下。这完全符合你“筛选出那些核心贡献在于构建、改进或演化 LLM智能体的论文”的核心目标，特别是精准地命中了“自我演化”这一研究方向。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#79",
        "title": "Phase-Adaptive LLM Framework with Multi-Stage Validation for Construction Robot Task Allocation: A Systematic Benchmark Against Traditional Optimization Algorithms",
        "link": "/arxiv/2512.02810",
        "arxiv_id": "2512.02810",
        "authors": "Shyam prasad reddy Kaitha, Hongrui Yu",
        "summary": "Multi-robot task allocation in construction automation has traditionally relied on optimization methods such as Dynamic Programming and Reinforcement Learning. This research introduces the LangGraph-based Task Allocation Agent (LTAA), an LLM-driven framework that integrates phase-adaptive allocation strategies, multi-stage validation with hierarchical retries, and dynamic prompting for efficient robot coordination. Although recent LLM approaches show potential for construction robotics, they largely lack rigorous validation and benchmarking against established algorithms. This paper presents the first systematic comparison of LLM-based task allocation with traditional methods in construction scenarios.The study validates LLM feasibility through SMART-LLM replication and addresses implementation challenges using a Self-Corrective Agent Architecture. LTAA leverages natural-language reasoning combined with structured validation mechanisms, achieving major computational gains reducing token usage by 94.6% and allocation time by 86% through dynamic prompting. The framework adjusts its strategy across phases: emphasizing execution feasibility early and workload balance in later allocations.The authors evaluate LTAA against Dynamic Programming, Q-learning, and Deep Q-Network (DQN) baselines using construction operations from the TEACh human-robot collaboration dataset. In the Heavy Excels setting, where robots have strong task specializations, LTAA achieves 77% task completion with superior workload balance, outperforming all traditional methods. These findings show that LLM-based reasoning with structured validation can match established optimization algorithms while offering additional advantages such as interpretability, adaptability, and the ability to update task logic without retraining.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2025-12-02",
        "category": "cs.LG",
        "crawl_time": "2025-12-03T11:00:04.760906",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接命中了您设定的多个关键方向。以下是我的详细判断过程： 1.  **第一步：核心判断 (保留)** - **论文本质**: 这篇论文的本质是**构建和改进一个LLM智能体框架**。它提出了一个全新的框架——LTAA（LangGraph-based Task Allocation Agent），而不是简单地将一个已有的智能体框架应用到新领域。 - **核心贡献**: 论文的核心贡献在于LTAA框架本身的设计，包括其“phase-adaptive allocation strategies”（自适应规划）、“multi-stage validation with hierarchical retries”（多阶段验证与重试机制）以及“Self-Corrective Agent Architecture”（自我纠正架构）。这些都是对LLM智能体能力的**构建、改进和演化**，完全符合您的核心目标。 - **排除非演化型应用**: 虽然论文的应用领域是“Construction Robot Task Allocation”，但其重点在于**如何设计一个更优的智能体**来完成这项任务，并系统地与传统方法进行基准测试。这属于方法论层面的贡献，而非单纯的应用。 2.  **第二步：正面指标 (高度匹配)** - **核心范式**: 论文明确涉及 `LLM-based Agents` 和 `Multi-Agent Systems (MAS)`。 - **智能体能力**: 论文的核心创新点直接对应了多个关键能力： - `Planning`: “phase-adaptive allocation strategies” 是一种高级的、分阶段的规划策略。 - `Self-Correction`: 明确提出了 “Self-Corrective Agent Architecture” 和 “hierarchical retries”，这是自我纠正和反思能力的直接体现。 - **多智能体**: 论文研究的是 “Multi-robot task allocation”，核心是 “robot coordination”，这完全属于多智能体间的协作与规划范畴。 - **演化机制**: “Self-Corrective” 和 “hierarchical retries” 机制体现了智能体在执行过程中的迭代改进和自我完善。 3.  **第三步：排除标准 (未触发)** - **安全与对齐**: 论文的主要贡献是关于智能体的效率、性能和框架设计，而非安全、对齐或可解释性（虽然它提到了可解释性是一个优势，但这并非其核心研究贡献）。 - **多模态与视觉**: 论文虽然涉及机器人，但其核心是任务分配的**推理与决策过程**，而非视觉感知。它强调的是 “natural-language reasoning combined with structured validation”，视觉（如果存在）只是作为环境输入的一部分，而非研究焦点。 4.  **第四步：处理特殊和模糊情况 (符合保留规则)** - **推理/规划**: 该论文是典型的关于智能体如何进行复杂规划和多步推理的研究。它提出的“phase-adaptive”策略超越了简单的单步推理，属于Agentic框架的规划范畴，因此应**保留**。 - **自我演化的应用**: 论文的核心贡献之一是提出了一种新的“自我纠正”机制。根据您的规则，即使它被应用在特定领域（建筑机器人），只要核心是提出新的自我演化机制，就应该**保留**。这篇论文完美符合这一例外情况。 **最终决策**: 综合以上分析，该论文的核心贡献在于**构建了一个新颖的多智能体框架（LTAA），并为其引入了自适应规划和自我纠正等关键的演化能力**。它直接对齐了您研究课题中的“多智能体”和“自我演化”两个核心方向。因此，这篇论文是您研究范围内的前沿高质量文献，应予以保留。"
    },
    {
        "index": "#108",
        "title": "Synthetic Error Injection Fails to Elicit Self-Correction In Language Models",
        "link": "/arxiv/2512.02389",
        "arxiv_id": "2512.02389",
        "authors": "David X. Wu, Shreyas Kapur, Anant Sahai, Stuart Russell",
        "summary": "Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-12-02",
        "category": "cs.LG",
        "crawl_time": "2025-12-03T11:00:04.795957",
        "filter_reason": "这篇论文完全符合您的研究范围，应被保留。我的判断依据如下： 1.  **核心判断 (第一步):** 这篇论文的本质是关于**改进LLM智能体核心能力的方法论研究**。它的核心贡献并非应用LLM解决某个外部领域问题，而是深入探究了一种旨在**诱导“自我修正”能力**的训练方法（合成错误注入）的有效性。自我修正 是您定义的“单智能体”和“自我演化”方向中的关键子能力。因此，这篇论文直接触及了您研究课题的核心——如何构建和改进LLM智能体。 2.  **正面指标 (第二步):** 论文明确包含了您关注的核心范式和能力。 *   **核心范式:** 论文的研究对象是 `Agentic AI` 和 `Self-Evolving` 的一个关键方面。 *   **智能体能力:** 论文的标题和摘要都紧紧围绕 `Self-Correction` (自我修正) 展开，这与 `Self-Reflection` (自我反思) 紧密相关，是智能体在复杂任务中迭代优化的核心机制。 3.  **排除标准 (第三步):** 该论文不涉及任何排除标准。它的主要贡献不是关于安全、对齐、可解释性，也没有涉及多模态或视觉内容。其焦点纯粹在于智能体的能力机制。 4.  **特殊和模糊情况处理 (第四步):** *   **推理/规划:** 这篇论文的研究内容——自我修正，是智能体在执行复杂规划和多步推理过程中的一个关键环节。它不是在提升LLM的基础数学或逻辑推理能力，而是在研究如何让智能体在推理链条中**识别并修复自身的错误**，这完全符合“保留”标准。 *   **核心贡献的界定:** 尽管论文的结论是该方法“失败”了，但这正是其核心价值所在。它通过严谨的实验，揭示了为什么一种看似合理的方法（合成错误注入）无效，并从反面论证了为什么强化学习等方法在激发自我修正能力上更为有效。这种对“方法论边界”的探索，对于指导未来如何更有效地构建和演化LLM智能体具有极高的学术价值。它回答了“如何改进智能体”这一核心问题，即使答案是否定的。 **总结:** 该论文是一篇典型的、高质量的Agentic AI研究。它没有提出一个全新的、成功的智能体框架，而是对一种**改进智能体核心能力（自我修正）的特定技术路径进行了深入的实证分析**。这种“证伪”或“解释失败原因”的研究，对于推动领域发展、避免研究者走弯路至关重要，完全符合您筛选“构建、改进或演化LLM智能体”论文的核心目标。因此，应判定为符合要求。"
    },
    {
        "index": "#107",
        "title": "WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate",
        "link": "/arxiv/2512.02405",
        "arxiv_id": "2512.02405",
        "authors": "Anoop Cherian, River Doyle, Eyal Ben-Dov, Suhas Lohit, Kuan-Chuan Peng",
        "summary": "Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents' solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-12-02",
        "category": "cs.LG",
        "crawl_time": "2025-12-03T11:00:04.795472",
        "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心判断 (第一步):** - **保留:** 论文的核心贡献是提出一个名为 \"WISE\" (Weighted Iterative Society-of-Experts) 的新框架。这个框架是一个“广义和模块化的多智能体辩论框架”，其本质是关于如何**构建和改进一个多智能体系统**。这直接命中了您研究目标中的“构建、改进或演化 LLM智能体”以及“多智能体”方向。 - 论文并非将已有框架简单应用于特定领域，而是**提出了一种新的多智能体协作范式**（将智能体划分为 Solvers 和 Reflectors），并设计了新的聚合算法。因此，它不属于“非演化型应用”的排除范畴。 2.  **正面指标 (第二步):** - 论文包含了大量您关注的核心关键词和概念： - **核心范式:** `Multi-Agent Systems (MAS)` 是论文的绝对核心。 - **多智能体:** 论文详细描述了智能体间的 `Collaboration` (协作)、`Communication` (通过自然语言反馈进行通信) 和 `Social Learning` (通过辩论迭代学习)。 - **智能体能力:** 框架中的 \"Reflectors\" 角色执行了 `Self-Correction` 和 `Self-Reflection` 的功能，它们验证解决方案并提供反馈，这是智能体能力的重要组成部分。 3.  **排除标准 (第三步):** - **安全与对齐:** 论文未涉及安全、对齐或可解释性等问题。 - **多模态与视觉:** 这是本篇论文最需要仔细辨析的一点。虽然论文在多模态任务上进行评估，但其**核心贡献并非多模态技术本身**，而是一个**能够处理多模态智能体的多智能体框架**。视觉在这里是智能体感知和推理的“环境”或“任务领域”，而不是研究的核心。论文的焦点在于“如何让不同能力的专家（包括单模态和多模态）进行有效辩论”，这完全符合“除非它们被用作智能体感知环境的工具，而不是研究的核心”这一例外条款。 4.  **特殊和模糊情况 (第四步):** - **推理/规划:** 论文研究的 MAD (Multi-Agent Debate) 是一种典型的 Agentic 推理框架，它超越了单个LLM的内部推理，转向多个智能体间的交互式、迭代式推理。这完全符合“保留”的条件。 **总结:** 该论文的核心是提出了一种创新的**多智能体系统架构 (WISE)**，旨在通过结构化的辩论和反思机制来提升集体智能。它深入探讨了智能体间的角色划分、通信协议和迭代优化，这与您研究课题中的“多智能体”方向高度契合。尽管其应用场景是多模态推理，但这恰恰证明了该框架的通用性和前沿性，而非其研究的核心。因此，这篇论文是您应该保留的高质量前沿研究。"
    },
    {
        "index": "#119",
        "title": "Orchestration Framework for Financial Agents: From Algorithmic Trading to Agentic Trading",
        "link": "/arxiv/2512.02227",
        "arxiv_id": "2512.02227",
        "authors": "Jifeng Li, Arnav Grover, Abraham Alpuerto, Yupeng Cao, Xiao-Yang Liu",
        "summary": "The financial market is a mission-critical playground for AI agents due to its temporal dynamics and low signal-to-noise ratio. Building an effective algorithmic trading system may require a professional team to develop and test over the years. In this paper, we propose an orchestration framework for financial agents, which aims to democratize financial intelligence to the general public. We map each component of the traditional algorithmic trading system to agents, including planner, orchestrator, alpha agents, risk agents, portfolio agents, backtest agents, execution agents, audit agents, and memory agent. We present two in-house trading examples. For the stock trading task (hourly data from 04/2024 to 12/2024), our approach achieved a return of $20.42\\%$, a Sharpe ratio of 2.63, and a maximum drawdown of $-3.59\\%$, while the S&P 500 index yielded a return of $15.97\\%$. For the BTC trading task (minute data from 27/07/2025 to 13/08/2025), our approach achieved a return of $8.39\\%$, a Sharpe ratio of $0.38$, and a maximum drawdown of $-2.80\\%$, whereas the BTC price increased by $3.80\\%$. Our code is available on \\href{https://github.com/Open-Finance-Lab/AgenticTrading}{GitHub}.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Computational Engineering, Finance, and Science, Machine Learning",
        "date": "2025-12-01",
        "category": "cs.LG",
        "crawl_time": "2025-12-03T11:00:04.801653",
        "filter_reason": "这篇论文的核心贡献是提出一个用于金融交易的**编排框架**，将传统的算法交易系统组件映射为多个专门的智能体（如规划器、风险智能体、投资组合智能体、记忆智能体等）。我的判断过程如下： 1.  **第一步：核心判断——保留。** 这篇论文的本质不是简单地将一个已有的LLM智能体应用到金融领域去赚钱，而是**提出了一种构建多智能体系统的新方法论和框架**。它详细阐述了如何设计、组织和协调多个具有不同职责的智能体来协同完成一个复杂任务（算法交易）。这完全符合“构建、改进LLM智能体”和“多智能体系统”的核心要求。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标——高度相关。** 论文中包含了大量我的核心关注点： *   **核心范式**: `Multi-Agent Systems (MAS)` 是论文的绝对核心。`Agentic AI` 和 `LLM-based Agents` 也是其基础。 *   **智能体能力**: 明确提到了 `Planning` (planner agent) 和 `Memory` (memory agent)。 *   **多智能体**: 整个框架就是关于 `Collaboration` 和 `Communication` 的，通过“编排”来实现智能体间的协同工作。 3.  **第三步：排除标准——未触发。** 论文的主要贡献不在于安全、对齐、可解释性，也未涉及多模态或视觉模型。因此，没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况——不适用。** 论文的核心是构建一个多智能体协作框架，而不是关于自我演化机制，因此第四步的特殊情况规则在此处不直接适用，但其核心贡献已在第一步得到确认。 **最终决策**： 综合分析，这篇论文的核心贡献在于**构建一个新颖的多智能体协作框架**来解决复杂问题。它属于我研究焦点中的“多智能体”方向，详细探讨了智能体的分工、规划、记忆与协作机制。尽管其应用场景是金融领域，但其研究价值在于智能体架构本身，而非应用结果。因此，这篇论文完全符合我的筛选要求。"
    },
    {
        "index": "#127",
        "title": "The 4/$δ$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee",
        "link": "/arxiv/2512.02080",
        "arxiv_id": "2512.02080",
        "authors": "PIerre Dantas, Lucas Cordeiro, Youcheng Sun, Waldir Junior",
        "summary": "The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($δ$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $δ> 0$, with an expected iteration count bounded by $\\mathbb{E}[n] \\leq 4/δ$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.",
        "subjects": "Artificial Intelligence, Formal Languages and Automata Theory, Machine Learning, Software Engineering",
        "date": "2025-11-30",
        "category": "cs.LG",
        "crawl_time": "2025-12-03T11:00:04.805657",
        "filter_reason": "这篇论文符合我的研究范围，核心判断依据如下： 1.  **第一步：核心判断——保留** 论文的核心贡献并非将LLM作为工具应用于形式化验证领域，而是**构建并分析了一个LLM智能体系统（LLM-Verifier System）**。它提出了一个“LLM-Verifier收敛定理”，为LLM与外部工具（验证器）的迭代交互过程提供了形式化的理论框架和可证明的保证（终止性和收敛性）。这本质上是对**LLM智能体方法论**的改进和深化，使其从启发式调优走向了可预测、可设计的工程化阶段。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标——高度相关** 论文的研究内容与我的核心关注点高度契合： *   **核心范式**: 论文研究的“LLM-Verifier System”是典型的 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**: 该系统明确涉及 `Tool Use / Tool Augmentation`（使用形式化验证工具），并且其核心研究内容是 `Self-Correction` / `Iterative Improvement`（LLM根据验证器反馈进行迭代修正）。论文的收敛定理正是对这种自我修正能力的理论保证。 *   **演化机制**: 论文虽然没有提出全新的演化算法，但它为智能体的“迭代改进”过程提供了坚实的理论基础，使其演化路径变得可预测，这属于对**自我演化机制**的深刻理解和优化。 3.  **第三步：排除标准——未触发** *   **安全与对齐**: 尽管论文提到了“safety-critical software environments”，但这只是其应用场景。论文的**核心贡献是关于系统性能的收敛性保证（termination, convergence, iteration count）**，而非研究如何防止AI产生有害行为、如何对齐人类价值观或如何解释模型内部决策。因此，它不属于安全与对齐的研究范畴。 *   **多模态与视觉**: 论文不涉及多模态内容。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文研究的不是LLM内部的推理能力（如数学或逻辑），而是**智能体在复杂任务（形式化验证）中的多步、迭代式问题解决过程**。它通过建模LLM与工具的交互，为这个过程提供了理论保证，这完全符合“保留”标准。 *   **自我演化的应用**: 这篇论文是“自我演化应用”的一个绝佳范例。虽然它应用于形式化验证领域，但其核心是提出了一种**保证自我修正过程收敛的理论框架**。根据筛选规则，这种提出新“自我演化”机制（此处为理论保证机制）的论文，即使应用于特定领域，也应保留。 **最终决策**: 这篇论文的核心贡献在于为LLM智能体的一个关键能力——**基于工具反馈的自我修正**——提供了首个形式化的收敛性理论框架。它直接回答了“如何构建一个可靠、可预测的LLM智能体”这一根本问题，属于对LLM智能体基础架构和演化机制的深刻研究。因此，它完全符合我关于“LLM智能体及其演化”的研究目标，应予以保留。"
    },
    {
        "index": "#3",
        "title": "Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control",
        "link": "/arxiv/2512.02814",
        "arxiv_id": "2512.02814",
        "authors": "Yongrui Yu, Zhongzhen Huang, Linjie Mu, Shaoting Zhang, Xiaofan Zhang",
        "summary": "Radiology reporting is an essential yet time-consuming and error-prone task for radiologists in clinical examinations, especially for volumetric medical images. Rigorous quality control is also critical but tedious, ensuring that the final report meets clinical standards. Existing automated approaches, including radiology report generation methods and medical vision-language models, focus mainly on the report generation phase and neglect the crucial quality control procedure, limiting their capability to provide comprehensive support to radiologists. We propose Radiologist Copilot, an agentic AI assistant equipped with orchestrated tools designed for automated radiology reporting with quality control. Leveraging large language models as the reasoning backbone, the agentic system autonomously selects tools, plans, and executes actions, emulating the behavior of radiologists throughout the holistic radiology reporting process. The orchestrated tools include region localization, think with image paradigm directed region analysis planning, strategic template selection for report generation, quality assessment and feedback-driven adaptive refinement for quality control. Therefore, Radiologist Copilot facilitates accurate, complete, and efficient radiology reporting, assisting radiologists and improving clinical efficiency. Experimental results demonstrate that Radiologist Copilot significantly surpasses other state-of-the-art methods in radiology reporting. The source code will be released upon acceptance.",
        "subjects": "Artificial Intelligence",
        "date": "2025-12-02",
        "category": "cs.AI",
        "crawl_time": "2025-12-03T11:00:04.811806",
        "filter_reason": "这篇论文符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将LLM作为工具应用于放射学领域，而是**构建了一个全新的、名为“Radiologist Copilot”的Agentic框架**。其核心贡献在于这个智能体系统的设计，包括它如何“自主选择工具、规划和执行行动”，以及如何通过“编排的工具”来完成一个复杂的多步骤流程（报告生成+质量控制）。这完全符合“构建LLM智能体”的核心目标，而不是一个“非演化型应用”。 2.  **第二步：正面指标** - 论文包含了大量您关注的核心指标： - **核心范式**: `Agentic AI` (标题和摘要中明确提及)。 - **智能体能力**: `Planning` (“plans and executes actions”), `Tool Use / Tool Augmentation` (“equipped with orchestrated tools”, “autonomously selects tools”), `Self-Correction` (“feedback-driven adaptive refinement”)。 - 这些指标表明，论文的研究内容与您在“单智能体”方向上的关注点高度重合。 3.  **第三步：排除标准** - **安全与对齐**: 论文的主要贡献是关于智能体框架和任务执行，不涉及安全、对齐或可解释性。 - **多模态与视觉**: 论文提到了“think with image paradigm”和“region localization”，这确实涉及视觉。但根据您的规则，视觉在这里是作为智能体**感知和分析环境的工具**（即工具箱中的一个工具），而不是研究的核心。研究的核心是这个智能体如何使用这些工具进行规划和决策，因此不触发排除条件。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文明确描述了智能体如何进行规划和多步执行（“autonomously selects tools, plans, and executes actions”），这属于您希望保留的“关于智能体如何进行规划”的范畴，而非提升LLM基础推理能力。 - **自我演化的应用**: 论文中的“quality assessment and feedback-driven adaptive refinement”是一种**自我修正和迭代改进**的机制，属于“自我演化”的子方向。虽然它应用在特定领域，但该机制是所提出的智能体框架的核心组成部分之一，因此符合保留条件。 **最终决策**: 综合以上分析，尽管这篇论文的应用领域是放射学，但其**核心贡献在于提出并实现了一个具有规划、工具使用和自我修正能力的LLM智能体框架**。它详细阐述了智能体的架构、工作流程和关键能力，这正是您“构建、改进或演化LLM智能体”研究目标所需要的前沿论文。因此，这篇论文高度相关，应被筛选出来。"
    },
    {
        "index": "#4",
        "title": "Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents",
        "link": "/arxiv/2512.02812",
        "arxiv_id": "2512.02812",
        "authors": "Zijie Lin, Qilin Cai, Liang Shen, Mingjun Xiao",
        "summary": "Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\\% and 13\\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.",
        "subjects": "Artificial Intelligence",
        "date": "2025-12-02",
        "category": "cs.AI",
        "crawl_time": "2025-12-03T11:00:04.812357",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接对应了您设定的“多智能体”和“自我演化”两个方向。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将现有LLM智能体应用到“论文复现”这个领域，而是**提出了一种新的智能体框架**来解决现有框架的缺陷。其核心贡献是“免提示的协作智能体框架”，这是一种关于如何构建和改进LLM智能体的方法论。它不属于“非演化型应用”，因为它引入了自动化的验证和修正机制，使智能体系统能够自我完善。 2.  **第二步：正面指标** - 论文命中了多个核心关注点： - **多智能体**: 论文明确提出了一个由“验证智能体”和“修正智能体”组成的**协作智能体**框架。这直接对应了您研究焦点中的“多智能体”方向，特别是“协作”子方向。 - **自我演化**: 论文的核心机制是智能体通过**自我修正**和**自我反思**来迭代改进输出质量。验证智能体检查问题，修正智能体解决问题，形成了一个自动化的反馈循环。这完全符合“自我演化”中的“自我完善”和“迭代改进”机制。 - **智能体能力**: 论文涉及了`Self-Correction`（自我修正）和`Self-Reflection`（自我反思）等关键能力。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或视觉多模态等问题。它的焦点是提升智能体系统的性能和自动化程度，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化应用”的一个绝佳范例。虽然它的应用场景是“论文复现”，但其核心创新点在于提出了一种**新的“自我演化”机制**（免提示的协作修正）。根据您的规则，即使应用在特定领域，只要核心是提出新的演化机制，就应该保留。 - **推理/规划**: 论文中的协作智能体在一个多步骤工作流中进行操作，这属于智能体在复杂任务中的规划和执行，而非提升LLM本身的基础推理能力。 **最终决策**: 综合以上分析，这篇论文的核心贡献是构建了一个新颖的**多智能体协作框架**，该框架通过**自动化的验证与修正机制**实现了智能体的**自我完善和迭代**。这精准地命中了您研究课题中的“多智能体”和“自我演化”两大核心方向。因此，这篇论文是高度相关且应该被保留的前沿研究。"
    },
    {
        "index": "#16",
        "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
        "link": "/arxiv/2512.02589",
        "arxiv_id": "2512.02589",
        "authors": "Junyi Hou, Andre Lin Huikai, Nuo Chen, Yiwei Gong, Bingsheng He",
        "summary": "Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.",
        "subjects": "Artificial Intelligence, Software Engineering",
        "date": "2025-12-02",
        "category": "cs.AI",
        "crawl_time": "2025-12-03T11:00:04.817618",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是**构建一个多智能体系统**。其标题和摘要明确指出，它提出了一个“Plugin-Based Multi-Agent System”（基于插件的多智能体系统）。论文的核心贡献在于解决构建这样一个系统的技术挑战，如“多智能体调度”、“与外部工具的可扩展通信”以及“可靠的双向同步”。这完全符合您筛选标准中“构建LLM智能体（Agentic LLM）、多智能体系统的方法论或新框架”的要求。 - 它不是“非演化型应用”。虽然应用场景是学术写作，但论文的焦点是**如何构建这个智能体系统**，而不是将一个已有的智能体作为工具去解决写作问题。其贡献在于系统架构和智能体间的协作机制，而非应用领域本身的结果。 2.  **第二步：正面指标** - 论文包含了大量您关注的核心关键词和概念： - **核心范式**: `Multi-Agent Systems (MAS)` 在标题和摘要中反复出现。`Agentic AI` 体现在 \"agentic, context-aware operations\" 的描述中。 - **智能体能力**: `Tool Use / Tool Augmentation` 是一个关键特性，摘要明确提到 \"extensible communication with external tools\" 并列举了文献搜索、参考文献查找等具体工具。 - **多智能体**: `Collaboration` 和 `Communication` 体现在 \"multi-agent scheduling\" 和 \"parallel agent execution\" 中，这些都是多智能体系统研究的核心。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全与对齐，也未提及多模态或视觉技术。因此，没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文描述的智能体执行 \"revision pipelines\"（修订流程）和 \"structured reviews\"（结构化评审），这属于智能体在复杂任务中进行多步规划和推理的范畴，符合保留条件。 - **自我演化的应用**: 此处不适用，因为论文的核心是构建多智能体系统，而非自我演化机制。 **最终决策**: 综合以上分析，这篇论文的核心贡献是**提出并实现了一个新颖的多智能体系统框架**，用于解决在特定环境（LaTeX编辑器）中进行复杂任务（学术写作与编辑）的挑战。它直接对齐了您研究焦点中的“多智能体”方向，并深入探讨了智能体的工具使用、调度和协作等关键能力。因此，这篇论文是您研究课题“LLM智能体及其演化”的前沿相关文献。"
    },
    {
        "index": "#15",
        "title": "IACT: A Self-Organizing Recursive Model for General AI Agents: A Technical White Paper on the Architecture Behind kragent.ai",
        "link": "/arxiv/2512.02605",
        "arxiv_id": "2512.02605",
        "authors": "Pengju Lu",
        "summary": "This technical white paper introduces the Interactive Agents Call Tree (IACT), a computational model designed to address the limitations of static, hard-coded agent workflows. Unlike traditional systems that require pre-defined graphs or specialized programming, IACT operates as a general-purpose autonomous system driven purely by user dialogue. Given a high-level objective, the system autonomously grows a dynamic, recursive agent topology incrementally tailored to the problem's structure. This allows it to scale its organizational complexity to match open-ended tasks. To mitigate the error propagation inherent in unidirectional function calls, IACT introduces interactional redundancy by replacing rigid invocations with bidirectional, stateful dialogues. This mechanism enables runtime error correction and ambiguity resolution. We describe the architecture, design principles, and practical lessons behind the production deployment of this model in the kragent.ai system, presenting qualitative evidence from real-world workflows rather than exhaustive benchmark results.",
        "subjects": "Artificial Intelligence, Multiagent Systems, Software Engineering",
        "date": "2025-12-02",
        "category": "cs.AI",
        "crawl_time": "2025-12-03T11:00:04.817167",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出了一种名为“交互式智能体调用树（IACT）”的**全新计算模型和架构**。其目标是解决现有静态智能体工作流的局限性，构建一个能够**自主增长和动态调整其内部拓扑结构**的通用智能体系统。这完全符合“构建、改进或演化 LLM智能体”的核心目标。它不是将现有智能体作为工具应用到某个领域，而是提出了智能体本身的一种新构建范式。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了大量与你研究焦点直接相关的核心关键词和概念： *   **自我演化**: 论文标题和摘要中明确提到了“Self-Organizing”（自组织）和“autonomously grows a dynamic, recursive agent topology”（自主地增长一个动态的、递归的智能体拓扑结构）。这正是“自我演化”方向的典型体现，即智能体根据任务需求动态地改变和进化自身的组织结构。 *   **单智能体**: 论文提出的IACT模型旨在提升单个智能体系统的能力。其中，“runtime error correction”（运行时错误纠正）直接对应了“自我修正”这一关键能力。而“autonomously grows a dynamic... topology”可以看作是一种高级的、动态的“规划”能力。 *   **核心范式**: 论文围绕“General AI Agents”（通用AI智能体）展开，提出了一个新的架构，这完全属于`Agentic AI`和`LLM-based Agents`的范畴。 3.  **第三步：排除标准——未触发** 论文的主要贡献是关于智能体的架构和演化机制，而非安全、对齐或可解释性。同时，论文也未涉及多模态或视觉内容，其交互方式是“user dialogue”（用户对话）。因此，所有排除标准均不适用。 4.  **第四步：处理特殊和模糊情况——符合保留规则** *   **推理/规划**: 论文提出的IACT模型是一个典型的Agentic框架，它通过动态生成和调整智能体拓扑来处理开放式任务，这属于“智能体如何进行规划或在复杂任务中进行多步推理”的范畴，因此应该保留。 *   **自我演化的应用**: 这篇论文的核心就是提出一种新的“自我演化/自组织”机制，即使它被部署在`kragent.ai`这个具体的生产系统中，其贡献依然是方法论层面的，完全符合“保留（例外）”的规则。 **总结**: 该论文的核心贡献是提出了一种能够让智能体**自组织、动态演化其内部结构**的新架构（IACT），以解决复杂开放式任务。这精准地命中了你研究目标中的“自我演化”和“单智能体”方向，并且不涉及任何排除标准。因此，这是一篇高度相关且应该保留的前沿论文。"
    },
    {
        "index": "#23",
        "title": "Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games",
        "link": "/arxiv/2512.02358",
        "arxiv_id": "2512.02358",
        "authors": "Ran Zhang, Kun Ouyang, Tiancheng Ma, Yida Yang, Dong Fang",
        "summary": "Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.",
        "subjects": "Artificial Intelligence",
        "date": "2025-12-02",
        "category": "cs.AI",
        "crawl_time": "2025-12-03T11:00:04.820889",
        "filter_reason": "这篇论文符合我的研究范围，核心判断依据如下： 1.  **第一步：核心判断——保留** - 论文的核心贡献并非简单地将LLM作为工具应用于游戏领域，而是**构建了一个全新的“生成式多智能体模拟系统”**。其本质是提出一种方法论和框架，用于创建能够模拟真实玩家行为的LLM智能体，并让这些智能体在一个模拟的MMO环境中互动。这完全符合“构建、改进或演化LLM智能体”的核心目标，特别是“多智能体”方向。 - 虽然论文的最终应用目标是游戏优化，但这属于其构建的智能体系统的应用场景，而非其核心方法论贡献。它不是在“使用”一个已有的智能体框架，而是在“创造”一个新颖的智能体模拟框架。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标——高度相关** - 论文明确包含了我的核心关注点： - **核心范式**: `Multi-Agent Systems (MAS)` 和 `LLM-based Agents` 在标题和摘要中被直接提出。 - **多智能体**: 论文构建的系统本质上是一个`Agent Society`，智能体在共享环境中互动，其行为会相互影响，这隐含了`Collaboration`与`Communication`等复杂的多智能体交互模式。 - **智能体能力**: 摘要中提到的“realistic and interpretable player decision-making”表明，这些智能体具备复杂的决策和推理能力，这是高级智能体的核心特征。 3.  **第三步：排除标准——未触发** - 论文虽然提到了“interpretable”（可解释性），但这是作为其系统的一个优点，而非研究的核心贡献。论文的主要焦点是构建模拟系统本身，而不是研究可解释性、安全性或对齐问题。 - 论文不涉及多模态或视觉内容。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的智能体需要进行“player decision-making”，这属于智能体在复杂环境中的推理和规划范畴，符合保留标准。 **最终决策**: 这篇论文的核心贡献在于提出了一种构建**多智能体系统**的新方法，通过微调LLM来创建能够模拟真实人类行为的智能体，并将它们置于一个动态环境中进行大规模模拟。这直接命中了我的研究焦点中的“多智能体”方向。尽管其应用背景是游戏设计，但其方法论本身是对LLM智能体构建和模拟技术的前沿探索，因此完全符合我的筛选要求。"
    },
    {
        "index": "#29",
        "title": "Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence",
        "link": "/arxiv/2512.02280",
        "arxiv_id": "2512.02280",
        "authors": "Noorbakhsh Amiri Golilarz, Sindhuja Penchala, Shahram Rahimi",
        "summary": "Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fun- damentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self- monitoring, lack of meta-cognitive awareness, fixed and non- adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust general- ization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-12-01",
        "category": "cs.AI",
        "crawl_time": "2025-12-03T11:00:04.822851",
        "filter_reason": "这篇论文符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - 论文的核心贡献并非提出一个具体的、可执行的LLM智能体框架或算法，而是**对现有AI系统（包括LLM）的根本性缺陷进行系统性分析，并提出了一个名为“认知自主”的未来发展方向**。 - 虽然它不是一篇“构建”型的论文，但它直接触及了您研究课题的核心——“LLM智能体及其演化”的**根本性挑战和演化目标**。它定义了“演化”需要解决什么问题，以及“演化”的终极形态（认知自主）应该具备哪些特征。因此，这篇论文的本质是关于**如何改进和演化LLM智能体的理论性、前瞻性研究**，而非简单的应用或基础设施研究。它属于“保留”范畴。 2.  **第二步：正面指标** - 论文摘要中包含了大量与您核心关注点高度相关的关键词和概念： - **自我演化**: 明确指出现有AI的“固定且非自适应的学习机制”，并主张实现“自我导向的适应”。 - **单智能体**: 详细分析了“自我监控”、“自我纠正”、“元认知意识”、“内在的能动性”等核心能力的缺失，这些都是构建高级智能体的关键。 - **智能体能力**: “自我监控”、“自我纠正”、“目标导向的行为”等直接对应了您关注的`Self-Correction`, `Self-Reflection`等能力。 3.  **第三步：排除标准** - **安全与对齐**: 论文在结尾处提到了“可解释、可治理、与人类价值观对齐”，但这并非论文的**主要贡献**。它的主要贡献是分析缺陷并提出“认知自主”的范式，而对齐等机制是作为实现这一范式后需要配套的监督机制被提及的。因此，这不构成排除的理由。 - **多模态与视觉**: 论文未涉及此内容。 4.  **第四步：处理特殊和模糊情况** - 这篇论文恰好处于一个特殊但极其重要的位置：它不是关于具体的“推理/规划”技术，而是关于**为什么当前的智能体无法实现真正的自主推理和规划**。它从更高层面剖析了阻碍`Self-Evolving`和`Agentic`能力实现的根本原因。 5.  **第五步：最终决策** - **核心依据**: 这篇论文的核心贡献在于**系统性地定义了LLM智能体向“认知自主”演化所必须克服的七大核心缺陷**。它为您的研究课题“LLM智能体及其演化”提供了至关重要的理论框架和问题定义。对于一位顶尖研究员而言，理解当前技术的根本性局限和未来的演化方向，与学习一个新的具体框架同等重要，甚至更为基础。这篇论文为后续所有关于“构建、改进或演化LLM智能体”的研究指明了方向和目标，因此它完全符合您筛选前沿论文的核心目标。"
    },
    {
        "index": "#69",
        "title": "Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System",
        "link": "/arxiv/2512.02567",
        "arxiv_id": "2512.02567",
        "authors": "Martin Weiss, Jesko Hecking-Harbusch, Jochen Quante, Matthias Woehrle",
        "summary": "The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes. We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables. Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-12-02",
        "category": "cs.AI",
        "crawl_time": "2025-12-03T11:00:04.835068",
        "filter_reason": "这篇论文符合您的研究范围，核心依据在于其贡献虽然以一个具体应用（C-to-Rust翻译）为案例，但其研究本质是分析和验证一种**自我演化机制**。 1.  **第一步：核心判断** - 论文的标题和摘要初看像一个典型的“非演化型应用”，因为它聚焦于软件工程领域的特定任务。 - 然而，深入分析其核心贡献，可以发现论文的重点并非“我们用LLM成功翻译了C代码”，而是**研究“反馈循环”和“代码扰动”这两个变量如何影响系统的性能**。特别是，论文详细描述了一个“generate-and-check”模式，其中LLM的输出被自动检查，对于错误结果，系统会通过“反馈循环”重新提示LLM进行修复。这个“反馈循环”和“自我修复”机制，正是智能体“自我演化”能力的体现。因此，它超越了单纯的应用，进入了方法论和机制研究的范畴，应予以**保留**。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点： - **自我演化机制**: `Feedback Loops` (反馈循环) 是论文的核心研究对象之一。 - **智能体能力**: `Self-Correction` (自我修正) 通过“re-prompted in a feedback loop to repair its output”直接体现。 - **迭代改进**: 整个反馈循环过程就是一种 `Iterative Improvement`。 - **核心范式**: “generate-and-check”模式是一种基础的 `Agentic AI` 工作流。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态视觉，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - 这篇论文完美地符合**“自我演化的应用”**这一例外规则。规则明确指出：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。” 本论文的核心贡献正是对一种“自我演化”机制（反馈循环）的实证研究和效果分析，尽管其应用场景是C-to-Rust翻译。因此，根据此核心规则，该论文应被保留。 **最终决策**: 综合以上分析，尽管论文的案例研究性质使其看起来像一个应用型论文，但其真正的科学贡献在于对LLM智能体**自我修正和迭代改进**这一核心演化机制的深入探究。它提供了关于如何通过反馈循环来提升智能体系统可靠性和鲁棒性的实证见解，这与您“LLM智能体及其演化”的研究课题，特别是“自我演化”方向，高度契合。因此，最终判断为 **True**。"
    },
    {
        "index": "#71",
        "title": "EZYer: A simulacrum of high school with generative agent",
        "link": "/arxiv/2512.02561",
        "arxiv_id": "2512.02561",
        "authors": "Jinming Yang, Zimu Ji, Weiqi Luo, Gaoxi Wang, Bin Ma, Yueling Deng",
        "summary": "With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in terms of courseware generation, interactive notes and quality assurance of content. In particular, the proposed generative agent EZYer : 1) Teacher Module: Integrating the Text Corpus retrieval and in-depth generation technologies, it automatically generates structured teaching materials and LaTeX Beamer courseware in line with the high school mathematics syllabus and supports user-defined image insertion. 2) Student Module: Throughout the collaborative interaction of the four roles of Teacher, Assistant, Top Student and Struggling Student, Note Taker summarizes and generates academic notes to enhance the depth and interest of learning. 3) Controller: set up keyword filtering system, content scoring system, role co-validation system, and dynamic content correction system. This ensure academic strictness and pedagogical propriety of EZYer inputs and outputs. In order to evaluate EZYer, this paper designs five-dimensional evaluation indexes of content accuracy, knowledge coverage, usability, formatting correctness and visual design and appeal, and scores 100 Beamer and Notes generated by EZYer by five large language models, separately, and the results show that the quality of EZYer-generated content is excellent and has a good application prospect.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-12-02",
        "category": "cs.AI",
        "crawl_time": "2025-12-03T11:00:04.835708",
        "filter_reason": "这篇论文符合我的研究范围，核心依据在于其贡献是构建了一个新颖的多智能体系统，而非简单地应用现有技术。 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM作为一个黑盒工具去解决教育问题，而是**构建了一个名为EZYer的多智能体框架**。其核心贡献在于设计并实现了一个由多个角色（教师、助理、优等生、学困生、笔记记录者）组成的智能体社会，并通过它们之间的协作交互来完成复杂任务。这完全符合“构建LLM智能体”和“多智能体系统”的核心目标。它不是“非演化型应用”，因为智能体系统的架构和交互机制本身就是论文的创新点。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `LLM-based Agents`, `Multi-Agent Systems (MAS)`。摘要明确提出了“generative agent”和“collaborative interaction of the four roles”。 - **多智能体**: `Collaboration`, `Communication`。学生模块的设计就是基于智能体间的协作与通信。 - **智能体能力**: `Tool Use`。教师模块集成了文本语料库检索和生成技术，这可以看作是智能体的工具使用能力。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全与对齐。虽然提到了“keyword filtering system”和“content scoring system”，但这是为了确保教育内容的“学术严谨性”和“教学适当性”，属于特定领域的质量控制，而非通用的AI安全、对齐或可解释性研究。 - 论文的核心不涉及多模态。虽然支持“用户自定义图像插入”，但这只是一个功能点，研究的核心是基于文本的智能体交互框架，而非视觉或多模态模型本身。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的多智能体协作过程本身就是一种复杂的规划和推理。不同角色的智能体为了共同的目标（生成高质量笔记）进行交互，这属于智能体在复杂任务中进行多步推理和协作规划的范畴，因此应该保留。 - **自我演化的应用**: 此条不直接适用，因为论文的核心机制不是“自我演化”，而是“多智能体协作”。 **最终决策**: 综合分析，这篇论文的核心贡献是提出并实现了一个由多个角色化LLM智能体构成的协作系统（EZYer），用于模拟高中教学环境。这完全符合我研究课题中的“多智能体”方向。尽管其应用场景是教育领域，但论文的创新点在于智能体系统的构建方法论本身，而非应用结果。因此，这篇论文是高度相关且应该保留的前沿研究。"
    },
    {
        "index": "#77",
        "title": "UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making",
        "link": "/arxiv/2512.02485",
        "arxiv_id": "2512.02485",
        "authors": "Qianhan Feng, Zhongzhen Huang, Yakun Zhu, Xiaofan Zhang, Qi Dou",
        "summary": "Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-12-02",
        "category": "cs.AI",
        "crawl_time": "2025-12-03T11:00:04.839921",
        "filter_reason": "这篇论文符合你的研究范围，应被保留。判断依据如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出一个名为 **UCAgents** 的**分层多智能体框架**。它并非简单地将现有智能体框架应用于医疗领域，而是为了解决现有多智能体框架在特定任务（医疗决策）中暴露出的“推理脱节”和“文本噪声放大”等问题，**创新性地设计了一套新的智能体交互机制**（单向收敛、结构化证据审计）。这完全符合“构建、改进LLM智能体”的核心目标，属于对多智能体系统（Multi-Agent Systems）方法论的创新。 2.  **第二步：正面指标——高度相关** 论文包含了多个核心关注点： *   **核心范式**: `Multi-Agent Systems (MAS)` 是论文的绝对核心。 *   **多智能体**: 论文详细描述了智能体间的 `Collaboration`（协作）和 `Communication`（通信）方式，并提出了新的交互协议来优化这一过程。 *   **智能体能力**: 论文的核心机制——结构化证据审计，本质上是一种高级的 `Planning` 和 `Reasoning` 过程，旨在确保智能体的推理过程严谨、有据可查。 3.  **第三步：排除标准——未触发** *   **安全与对齐**: 尽管论文提到了“临床信任”和“诊断可靠性”，但其主要贡献是**通过改进智能体框架的交互机制来提升性能和效率**，从而间接增强可靠性。论文的核心并非提出一种新的安全、对齐或可解释性理论或方法，因此不属于排除范畴。 *   **多模态与视觉**: 论文确实大量涉及 `Vision-Language Models (VLMs)` 和视觉证据。但根据筛选标准，这属于“被用作智能体感知环境的工具，而不是研究的核心”的情况。论文的研究核心是**智能体如何处理和利用这些视觉证据进行辩论和决策**，即智能体的框架和交互协议，而不是VLM模型本身。因此，不应被排除。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文提出的UCAgents框架，其“单向收敛”和“结构化证据审计”机制，正是关于**智能体如何在复杂任务中进行多步推理和规划**的典型范例。它超越了简单的ReAct，提出了一种更结构化、更高效的智能体协作推理范式，完全符合保留条件。 **总结**: 该论文的核心贡献是**构建和改进一个多智能体系统（UCAgents）**，通过设计新颖的交互协议来解决现有框架的缺陷。虽然其应用场景是医疗领域，并使用了视觉模型，但其研究焦点始终在**智能体本身的架构和协作机制**上，这与你的研究课题“LLM智能体及其演化”中的“多智能体”方向高度契合。因此，应判定为符合要求。"
    }
]