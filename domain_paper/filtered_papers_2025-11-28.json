[
    {
        "index": "#9",
        "title": "Automated Design Optimization via Strategic Search with Large Language Models",
        "link": "/arxiv/2511.22651",
        "arxiv_id": "2511.22651",
        "authors": "Anthony Carreon, Vansh Sharma, Venkat Raman",
        "summary": "Traditional optimization methods excel in well-defined search spaces but struggle with design problems where transformations and design parameters are difficult to define. Large language models (LLMs) offer a promising alternative by dynamically interpreting design spaces and leveraging encoded domain knowledge. To this end, we introduce AUTO, an LLM agent framework that treats design optimization as a gradient-free search problem guided by strategic LLM reasoning. The framework employs two collaborative agents: a Strategist that selects between exploration and exploitation strategies, and an Implementor that executes detailed designs. Applied to GPU code optimization -- a domain critical to fields from machine learning to scientific computing -- AUTO generates solutions competitive with expert implementations for chemical kinetics integration and dense matrix multiplication. The framework achieves 50-70% search efficiency relative to Bayesian optimization methodologies. It completes optimizations in approximately 8 hours at an estimated cost of up to \\$159 per run, compared to an estimated cost of up to \\$480 with median-wage software developers. These findings open the door to automating design optimization in ill-defined search spaces with limited prior information.",
        "subjects": "Machine Learning, Artificial Intelligence, Computational Engineering, Finance, and Science, Multiagent Systems",
        "date": "2025-11-27",
        "category": "cs.MA",
        "crawl_time": "2025-12-02T11:00:04.649700",
        "filter_reason": "这篇论文符合研究范围，应被保留。判断依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献是构建了一个名为 **AUTO** 的 **LLM智能体框架**，而不是简单地将LLM应用于GPU优化。摘要明确指出 \"we introduce AUTO, an LLM agent framework\"，这直接命中了“构建LLM智能体”的核心目标。虽然它被应用在GPU代码优化这个特定领域，但其核心创新在于提出了一种新的智能体架构和工作流来解决一类通用问题（“ill-defined search spaces”），因此不属于“非演化型应用”的排除范畴。 2.  **正面指标 (第二步):** 论文包含了多个核心关注点： *   **多智能体:** 论文明确提出了一个包含两个角色的多智能体系统：“一个Strategist（战略家）智能体和一个Implementor（执行者）智能体”，它们是“collaborative agents（协作智能体）”。这完全符合“多智能体”方向中的“协作”子方向。 *   **单智能体:** Strategist智能体的功能是“selects between exploration and exploitation strategies（在探索和利用策略之间进行选择）”，这本质上是一种高级的**规划**和**推理**能力。整个框架被描述为“guided by strategic LLM reasoning（由战略LLM推理引导）”，这符合“单智能体”方向中的“规划”子方向。 3.  **排除标准 (第三步):** 论文内容不涉及安全、对齐、多模态等排除领域，因此没有触发排除标准。 4.  **特殊情况处理 (第四步):** *   **推理/规划:** 该论文是关于智能体如何进行规划和战略决策的典型案例，符合保留条件。它不是在提升LLM的基础数学能力，而是在构建一个利用LLM进行复杂搜索规划的智能体框架。 *   **自我演化的应用:** 虽然论文没有明确提出“自我演化”机制，但其多智能体协作和迭代搜索的过程具有演化的雏形。但即便不考虑这一点，其在“多智能体”和“单智能体规划”上的贡献已经足够突出。 **最终决策 (第五步):** 综合来看，这篇论文的核心贡献是提出了一种新颖的、基于多智能体协作的LLM智能体框架（AUTO），用于解决复杂的自动化设计优化问题。它直接贡献于“多智能体”和“单智能体（规划）”这两个核心研究方向，完全符合“构建、改进或演化LLM智能体”的研究目标。因此，应判定为 **True**。"
    },
    {
        "index": "#7",
        "title": "Agentic AI Framework for Cloudburst Prediction and Coordinated Response",
        "link": "/arxiv/2511.22767",
        "arxiv_id": "2511.22767",
        "authors": "Toqeer Ali Syed, Sohail Khan, Salman Jan, Gohar Ali, Muhammad Nauman, Ali Akarma, Ahmad Ali",
        "summary": "The challenge is growing towards extreme and short-duration rainfall events like a cloudburst that are peculiar to the traditional forecasting systems, in which the predictions and the response are taken as two distinct processes. The paper outlines an agentic artificial intelligence system to study atmospheric water-cycle intelligence, which combines sensing, forecasting, downscaling, hydrological modeling and coordinated response into a single, interconnected, priceless, closed-loop system. The framework uses autonomous but cooperative agents that reason, sense, and act throughout the entire event lifecycle, and use the intelligence of weather prediction to become real-time decision intelligence. Comparison of multi-year radar, satellite, and ground-based evaluation of the northern part of Pakistan demonstrates that the multi-agent configuration enhances forecast reliability, critical success index and warning lead time compared to the baseline models. Population reach was maximised, and errors during evacuation were minimised through communication and routing agents, and adaptive recalibration and transparent auditability were provided by the embedded layer of learning. Collectively, this leads to the conclusion that collaborative AI agents are capable of transforming atmospheric data streams into practicable foresight and provide a platform of scalable adaptive and learning-based climate resilience.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-11-27",
        "category": "cs.MA",
        "crawl_time": "2025-12-02T11:00:04.649151",
        "filter_reason": "这篇论文完全符合你的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将一个现有工具应用于气象领域，而是**提出并构建了一个全新的“Agentic AI框架”**。摘要明确指出，其核心贡献是“outlines an agentic artificial intelligence system”，并详细描述了该框架如何通过“autonomous but cooperative agents”来整合感知、预测、建模和响应。这完全符合“构建、改进或演化LLM智能体”的核心目标。它不是非演化型应用，因为框架包含了“adaptive recalibration”和“embedded layer of learning”，表明智能体具备自我完善的能力。 2.  **第二步：正面指标** - 论文包含了大量你的核心关注点，相关性极高： - **核心范式**: `Agentic AI` (标题和摘要中多次出现), `Multi-Agent Systems (MAS)` (明确提到 \"cooperative agents\", \"multi-agent configuration\")。 - **智能体能力**: `Planning` (隐含在 \"coordinated response\" 和 \"routing agents\" 中), `Tool Use` (智能体使用 \"sensing, forecasting, downscaling, hydrological modeling\" 等作为工具)。 - **多智能体**: `Collaboration` (\"cooperative agents\"), `Communication` (\"communication and routing agents\")。 - **演化机制**: `Self-Improvement` / `Iterative Improvement` (通过 \"adaptive recalibration\" 和 \"embedded layer of learning\" 体现)。 3.  **第三步：排除标准** - 论文未涉及任何排除标准。其焦点是智能体系统的构建与效能，而非安全、对齐或多模态技术本身。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一例外情况的完美范例。虽然它被应用在“cloudburst prediction”这一特定领域，但其**核心贡献是提出了一种具备自适应学习和重新校准能力的多智能体协作框架**。这种“adaptive recalibration”机制本身就是一种自我演化的体现，因此完全符合保留条件。 **最终决策**: 这篇论文的核心贡献在于构建了一个新颖的多智能体协作框架，该框架中的智能体能够自主推理、协作通信，并通过一个嵌入式学习层进行自适应的重新校准（自我演化）。这直接命中了你研究范围中的“多智能体”和“自我演化”两个核心方向。尽管其应用场景是气象预测，但论文的焦点是智能体架构和机制的创新，而非领域应用本身。因此，这篇论文是高度相关且应被筛选入的前沿研究。"
    },
    {
        "index": "#6",
        "title": "CRAwDAD: Causal Reasoning Augmentation with Dual-Agent Debate",
        "link": "/arxiv/2511.22854",
        "arxiv_id": "2511.22854",
        "authors": "Finn G. Vamosi, Nils D. Forkert",
        "summary": "When people reason about cause and effect, they often consider many competing \"what if\" scenarios before deciding which explanation fits best. Analogously, advanced language models capable of causal inference can consider multiple interventions and counterfactuals to judge the validity of causal claims. Crucially, this type of reasoning is less like a single calculation and more like an internal dialogue between alternative hypotheses. In this paper, we make this dialogue explicit through a dual-agent debate framework where one model provides a structured causal inference, and the other critically examines this reasoning for logical flaws. When disagreements arise, agents attempt to persuade each other, challenging each other's logic and revising their conclusions until they converge on a mutually agreed answer. To take advantage of this deliberative process, we specifically use reasoning language models, whose strengths in both causal inference and adversarial debate remain under-explored relative to standard large language models. We evaluate our approach on the CLadder dataset, a benchmark linking natural language questions to formally defined causal graphs across all three rungs of Pearl's ladder of causation. With Qwen3 and DeepSeek-R1 as debater agents, we demonstrate that multi-agent debate improves DeepSeek-R1's overall accuracy in causal inference from 78.03% to 87.45%, with the counterfactual category specifically improving from 67.94% to 80.04% accuracy. Similarly, Qwen3's overall accuracy improves from 84.16% to 89.41%, and counterfactual questions from 71.53% to 80.35%, showing that strong models can still benefit greatly from debate with weaker agents. Our results highlight the potential of reasoning models as building blocks for multi-agent systems in causal inference, and demonstrate the importance of diverse perspectives in causal problem-solving.",
        "subjects": "Machine Learning, Multiagent Systems",
        "date": "2025-11-28",
        "category": "cs.MA",
        "crawl_time": "2025-12-02T11:00:04.648857",
        "filter_reason": "这篇论文完全符合你的研究范围，核心判断依据如下： 1.  **第一步：核心判断 (保留)** 论文的核心贡献是提出了一种名为 **CRAwDAD 的双智能体辩论框架**。这并非将现有智能体框架简单应用于某个领域，而是**构建了一个全新的多智能体协作方法论**。该框架通过让两个智能体进行结构化的辩论、批判和说服，来共同完成复杂的因果推理任务。这直接命中了你筛选标准中的“构建、改进或演化 LLM智能体”以及“多智能体系统”的核心。 2.  **第二步：正面指标 (高度匹配)** 论文包含了大量你的核心关注点： *   **核心范式**: 明确提出了一个 `Multi-Agent Systems (MAS)` 框架。 *   **多智能体**: 论文的核心机制就是智能体间的 `Communication`（通信）、`Collaboration`（协作）和一种形式的 `Negotiation`（博弈/说服）。 *   **智能体能力**: 智能体在辩论中相互“critically examines this reasoning for logical flaws”（批判性审视逻辑缺陷）并“revising their conclusions”（修正结论），这体现了 `Self-Correction`（自我修正）和 `Self-Reflection`（自我反思）的能力。 *   **演化机制**: 整个辩论过程直到“converge on a mutually agreed answer”（收敛于共同答案）为止，这是一个典型的 `Iterative Improvement`（迭代改进）过程，属于自我演化的范畴。 3.  **第三步：排除标准 (未触发)** 论文的主要贡献是关于提升智能体在因果推理任务上的性能框架，而非研究安全、对齐、可解释性或视觉多模态问题。因此，所有排除标准均不适用。 4.  **第四步：处理特殊和模糊情况 (符合保留规则)** 论文虽然聚焦于“因果推理”，但它完全符合“保留”规则。它不是在研究如何提升LLM本身的基础数学或逻辑能力，而是在研究**智能体如何通过一个多智能体框架来进行复杂的推理**。这与 ReAct、ToT 等Agentic框架一脉相承，都是关于智能体的推理范式，属于你的研究焦点。论文将推理过程建模为“internal dialogue between alternative hypotheses”，并将其显式化为一个多智能体辩论系统，这正是Agentic AI研究的核心。 **总结**: 该论文的本质是提出了一种新颖的**多智能体协作框架**，通过辩论机制来增强LLM智能体的复杂推理能力。它完美契合你研究课题中的“多智能体”方向，并触及了“自我演化”中的迭代改进和自我修正。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#2",
        "title": "MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation",
        "link": "/arxiv/2511.23397",
        "arxiv_id": "2511.23397",
        "authors": "Mahdi Rahmani, AmirHossein Saffari, Reyhane Rahmani",
        "summary": "Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet",
        "subjects": "Computation and Language, Artificial Intelligence, Multiagent Systems",
        "date": "2025-11-28",
        "category": "cs.MA",
        "crawl_time": "2025-12-02T11:00:04.647783",
        "filter_reason": "这篇论文符合您的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断** - **保留**。这篇论文的本质并非简单地应用一个已有的智能体框架，而是**提出了一种“新颖的、自动化的多智能体架构”**。虽然这个架构的最终产物是一个特定领域（波斯语销售聊天机器人）的数据集，但论文的核心贡献在于**构建这个多智能体系统的方法论本身**。它详细描述了如何通过专门的智能体（问题生成、验证、优化）进行协作来完成复杂任务。这完全符合“构建、改进LLM智能体”的核心目标，因此不属于“非演化型应用”的排除范畴。 **第二步：正面指标** - 论文包含了多个核心关注点，相关性极高： - **核心范式**: 明确提到了 `Multi-Agent Systems (MAS)`，并描述了其架构。 - **多智能体**: 详细阐述了智能体间的 `Collaboration`（协作），即问题生成、验证和优化智能体如何协同工作。 - **智能体能力**: “验证和优化”智能体的功能体现了 `Self-Correction` 或 `Self-Refine` 的思想，即一个智能体对另一个智能体的输出进行评估和改进。 **第三步：排除标准** - 论文不涉及任何排除标准中的内容。其焦点是数据生成的方法论和效率，而非安全、对齐或多模态技术。 **第四步：处理特殊和模糊情况** - **核心模糊情况处理**: 这篇论文是“应用驱动的方法论创新”的典型案例。虽然其应用场景（波斯语销售数据集）非常具体，但它的核心贡献是提出了一种新的多智能体协作框架。这与筛选标准中提到的“用于化学实验的自我演化智能体”的例外情况逻辑一致：**只要论文的核心是提出一种新的智能体机制（这里是多智能体协作机制），即使它被应用在特定领域，也应该保留。** 论文的评估部分也直接对比了其“agentic architecture”与传统方法，进一步证明了其核心贡献在于架构本身。 **第五步：最终决策** 综合以上分析，尽管论文标题和部分摘要内容聚焦于数据集这一“产品”，但其核心创新点和贡献在于**提出并验证了一种新颖的多智能体协作架构**。这直接命中了您研究范围中的“多智能体”方向。因此，这篇论文高度相关，应该被筛选出来。"
    },
    {
        "index": "#8",
        "title": "Solving Context Window Overflow in AI Agents",
        "link": "/arxiv/2511.22729",
        "arxiv_id": "2511.22729",
        "authors": "Anton Bulle Labate, Valesca Moura de Sousa, Sandro Rama Fiorini, Leonardo Guerreiro Azevedo, Raphael Melo Thiago, Viviane Torres da Silva",
        "summary": "Large Language Models (LLMs) have become increasingly capable of interacting with external tools, granting access to specialized knowledge beyond their training data - critical in dynamic, knowledge-intensive domains such as Chemistry and Materials Science. However, large tool outputs can overflow the LLMs' context window, preventing task completion. Existing solutions such as truncation or summarization fail to preserve complete outputs, making them unsuitable for workflows requiring the full data. This work introduces a method that enables LLMs to process and utilize tool responses of arbitrary length without loss of information. By shifting the model's interaction from raw data to memory pointers, the method preserves tool functionality, allows seamless integration into agentic workflows, and reduces token usage and execution time. The proposed method is validated on a real-world Materials Science application that cannot be executed with conventional workflows, and its effectiveness is demonstrated via a comparative analysis where both methods succeed. In this experiment, the proposed approach consumed approximately seven times fewer tokens than the traditional workflow.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-11-27",
        "category": "cs.MA",
        "crawl_time": "2025-12-02T11:00:04.649431",
        "filter_reason": "这篇论文完全符合你的研究范围，核心判断依据如下： 1.  **核心贡献判断 (第一步):** *   **保留:** 这篇论文的核心贡献是提出了一种**新的方法论**，用于解决LLM智能体在工具使用过程中遇到的一个根本性瓶颈——上下文窗口溢出。它不是简单地将现有智能体框架应用到某个领域，而是**改进了智能体本身的核心工作机制**。论文提出的“从原始数据转向内存指针”的交互方式，是对智能体**记忆**和**工具使用**能力的直接增强和优化。因此，它属于“构建、改进或演化LLM智能体”的范畴。 2.  **正面指标匹配 (第二步):** *   论文明确提到了 `Agentic workflows`，这直接命中了你的核心范式。 *   论文的核心问题是 `Tool Use` 过程中产生的，并且其解决方案也旨在优化这一过程。 *   解决方案的核心是引入 `memory pointers`，这直接对应了你的关注点 `Memory`。 *   这些指标都强烈表明该论文与你的研究焦点高度相关。 3.  **排除标准检查 (第三步):** *   论文的主要贡献不涉及安全、对齐、可解释性或视觉多模态问题，因此没有触犯任何排除标准。 4.  **特殊情况处理 (第四步):** *   **推理/规划:** 该论文不涉及提升LLM的基础推理能力，而是聚焦于智能体在执行任务（使用工具）时的数据处理流程，这完全符合保留条件。 *   **自我演化的应用:** 虽然论文在材料科学领域进行了验证，但这属于“核心是提出一种新机制，即使它被应用在特定领域”的例外情况。其核心贡献是通用的智能体交互机制，而非材料科学的解决方案。这个验证案例恰恰证明了该机制能够解决传统工作流无法完成的任务，突显了其作为智能体基础组件的价值。 **总结:** 这篇论文的本质是**对LLM智能体核心组件（工具使用与记忆）的一次关键性改进**。它提出了一种创新的交互范式，使智能体能够处理任意长度的工具输出，从而极大地扩展了智能体在知识密集型任务中的能力边界。这完全符合你“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的核心目标，特别是聚焦于**单智能体**方向下的**工具使用**和**记忆**子方向。因此，应该保留。"
    },
    {
        "index": "#8",
        "title": "MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)",
        "link": "/arxiv/2511.23281",
        "arxiv_id": "2511.23281",
        "authors": "Aaron Steiner, Ralph Peeters, Christian Bizer",
        "summary": "Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks. To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.",
        "subjects": "Computation and Language",
        "date": "2025-11-28",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:05.879150",
        "filter_reason": "这篇论文符合筛选要求，应被保留。判断依据如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献并非将LLM智能体作为一个工具去解决某个特定领域（如电商）的业务问题，而是对LLM智能体本身的设计进行了一次系统性的比较研究。它探究了“智能体如何与外部环境（网页）交互”这一根本性问题，通过对比四种不同的接口（HTML, RAG, MCP, NLWeb），来评估哪种设计能让智能体更有效、更高效。这直接属于“**改进LLM智能体**”的范畴，符合我的核心研究目标。它关注的是智能体的架构和交互机制，而非其应用。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点： - **核心范式**: `LLM-based Agents` 是全文的研究对象。 - **智能体能力**: 论文的核心是比较不同的 `Tool Use / Tool Augmentation` 方式。HTML、RAG、MCP等都是智能体用来感知和操作网页环境的工具。论文通过评估智能体完成复杂任务（如结账）的表现，间接衡量了其规划和执行能力。 - 这些正面指标强烈表明该论文与我的研究高度相关。 3.  **第三步：排除标准** - 论文的主要贡献是关于智能体的 `Effectiveness` 和 `Efficiency`，不涉及 `Safety`, `Alignment`, `Interpretability` 等安全与对齐议题。 - 论文研究的是基于文本的网页交互，不涉及 `Vision` 或多模态模型。 - 因此，该论文未触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 该论文不属于“提高LLM本身基础Token预测能力”的范畴。它评估的是智能体在真实任务流程中的表现，这必然包含了多步推理和规划。其研究焦点在于“接口设计如何影响智能体的整体任务执行能力”，这完全符合保留条件。 5.  **第五步：最终决策** - 综合来看，这篇论文虽然是一篇技术报告，但其研究内容非常前沿且切中要害。它没有提出一个全新的智能体框架，但它通过严谨的对比实验，为“**如何构建更好的LLM智能体**”提供了宝贵的实证依据和设计指导。这种对智能体核心交互机制的深入剖析，正是我研究课题中“单智能体”方向所关注的核心问题之一。因此，这篇论文完全符合筛选标准，应被保留。"
    },
    {
        "index": "#14",
        "title": "Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models",
        "link": "/arxiv/2511.23136",
        "arxiv_id": "2511.23136",
        "authors": "Yujiao Yang, Jing Lian, Linhui Li",
        "summary": "The complex reasoning ability of Large Language Models (LLMs) poses a critical bottleneck for their practical applications. Test-time expansion methods such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT) enhance reasoning by introducing intermediate reasoning structures, tree search, or graph-based exploration mechanisms. However, their reasoning strategies suffer from limited diversity, redundant search branches, and inadequate integration and error correction across heterogeneous reasoning paths. To address these limitations, we propose a novel reasoning framework called Multi-chain Graph Refinement & Selection (MGRS), which first generates multiple diverse reasoning trajectories for a given problem, refines candidate responses using a composite self- and cross-verification strategy, then constructs a reasoning relation graph and estimates the success rate of intermediate nodes, and finally computes cumulative success rates to select the most reliable answer and corresponding reasoning trajectory. Experimental results demonstrate that MGRS significantly advances both the reasoning capability and computational efficiency of reasoning enhancement methods. Across six benchmark datasets spanning four distinct tasks, MGRS achieves an average accuracy of 82.9%, outperforming state-of-the-art baselines by a clear margin of 2.1%. Remarkably, on the 24-point game, MGRS attains 100% accuracy for the first time, while delivering a 13.6x speed-up compared to the leading Forest of Thoughts framework.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-11-28",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:05.887357",
        "filter_reason": "这篇论文完全符合您的研究范围，核心判断依据如下： 1.  **第一步：核心判断——保留** - 论文的核心贡献是提出一个名为 **MGRS (Multi-chain Graph Refinement & Selection)** 的**全新推理框架**。这并非将现有LLM或智能体框架简单应用于某个领域，而是对LLM智能体如何进行复杂推理这一核心能力的**方法论创新和改进**。 - 它直接对标并改进了现有的Agentic推理范式，如Tree-of-Thought (ToT) 和 Graph-of-Thought (GoT)。因此，其本质是关于**构建和改进LLM智能体**的，应予以保留。 2.  **第二步：正面指标——高度匹配** - 论文的研究内容与您的核心关注点高度契合： - **核心范式**: 论文明确建立在 `Graph-of-Thought (GoT)` 之上，属于 `Agentic AI` 和 `LLM-based Agents` 的范畴。 - **智能体能力**: 论文的核心机制涉及 `Planning`（规划）。它通过生成多条推理链、构建关系图并选择最优路径，本质上是一种复杂的规划和搜索策略。同时，其“composite self- and cross-verification strategy”（复合自我和交叉验证策略）直接对应了 `Self-Correction`（自我纠错）和 `Self-Reflection`（自我反思）的能力。 3.  **第四步：处理特殊和模糊情况——符合保留条件** - **推理/规划**: 这是判断的关键。虽然论文标题和摘要聚焦于“推理”，但它并非研究如何提升LLM底层的数学或逻辑能力（例如通过新的微调数据集）。相反，它研究的是**智能体在测试时如何组织其思考过程**以解决复杂问题。这完全符合“保留”条件：“如果论文是关于智能体如何进行规划或在复杂任务中进行多步推理（如 ReAct、ToT 或新的Agentic框架）”。MGRS正是一个新的Agentic框架。 4.  **第三步：排除标准——未触发** - 论文的主要贡献是提升推理的准确性和效率，不涉及 `Safety`、`Alignment`、`Interpretability` 等安全与对齐问题。 - 论文是纯文本推理任务，不涉及 `Vision`、`MLLMs` 等多模态内容。 **总结**: 该论文的核心贡献是提出了一种新的LLM智能体推理框架MGRS，它通过引入图优化、节点评估和路径选择机制，显著增强了智能体的规划和自我纠错能力。这完全符合您研究课题中“单智能体”方向下的“规划”和“自我反思”子方向。因此，这篇论文是高度相关且应被筛选保留的前沿研究。"
    },
    {
        "index": "#25",
        "title": "Visual Puns from Idioms: An Iterative LLM-T2IM-MLLM Framework",
        "link": "/arxiv/2511.22943",
        "arxiv_id": "2511.22943",
        "authors": "Kelaiti Xiao, Liang Yang, Dongyu Zhang, Paerhati Tulajiang, Hongfei Lin",
        "summary": "We study idiom-based visual puns--images that align an idiom's literal and figurative meanings--and present an iterative framework that coordinates a large language model (LLM), a text-to-image model (T2IM), and a multimodal LLM (MLLM) for automatic generation and evaluation. Given an idiom, the system iteratively (i) generates detailed visual prompts, (ii) synthesizes an image, (iii) infers the idiom from the image, and (iv) refines the prompt until recognition succeeds or a step limit is reached. Using 1,000 idioms as inputs, we synthesize a corresponding dataset of visual pun images with paired prompts, enabling benchmarking of both generation and understanding. Experiments across 10 LLMs, 10 MLLMs, and one T2IM (Qwen-Image) show that MLLM choice is the primary performance driver: GPT achieves the highest accuracies, Gemini follows, and the best open-source MLLM (Gemma) is competitive with some closed models. On the LLM side, Claude attains the strongest average performance for prompt generation.",
        "subjects": "Computation and Language, Computer Vision and Pattern Recognition",
        "date": "2025-11-28",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:05.904592",
        "filter_reason": "这篇论文的核心贡献是提出一个**迭代的LLM-T2IM-MLLM框架**，用于自动生成和评估基于成语的视觉双关图。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心并非简单地将LLM应用于“视觉双关”这个特定领域，而是构建了一个具有**自主规划、工具使用、评估和自我修正**能力的完整框架。该框架的迭代循环（生成提示 -> 合成图像 -> 评估 -> 优化提示）本质上是一个智能体的工作流。因此，它不属于“非演化型应用”，而是关于“构建LLM智能体”的方法论，符合保留标准。 2.  **第二步：正面指标——高度相关** 论文包含了多个我关注的核心指标： *   **自我演化**: 论文的标题和摘要都强调了其核心是“Iterative”（迭代）框架。其工作流程中的“refines the prompt until recognition succeeds”（优化提示直至识别成功）是典型的**自我修正**和**迭代完善**机制，完全符合“自我演化”的研究方向。 *   **单智能体能力**: 该框架展现了智能体的多个关键能力： *   **规划**: 生成详细的视觉提示是一种规划行为。 *   **工具使用**: 将文本到图像模型（T2IM）作为工具来执行任务。 *   **自我反思/评估**: 使用多模态LLM（MLLM）来推断图像中的成语，这相当于对上一步行动结果的评估和反思。 *   **自我修正**: 基于评估结果来优化提示，是明确的自我修正行为。 3.  **第三步：排除标准——未触发** *   **安全与对齐**: 论文不涉及安全、对齐等问题。 *   **多模态与视觉**: 虽然论文使用了T2IM和MLLM，但它们是作为智能体框架的**组件和工具**存在的。研究的核心是**如何协调这些工具以实现目标的迭代框架**，而不是视觉模型本身。这完全符合“除非它们被用作智能体感知环境的工具，而不是研究的核心”这一例外条款。 4.  **第四步：处理特殊和模糊情况——符合保留条件** *   **自我演化的应用**: 这篇论文是“自我演化的应用”的绝佳范例。它的核心贡献是提出了一种新的“自我演化/自我修正”机制（迭代优化框架），并将其应用在“生成视觉双关”这个特定领域。根据筛选规则，这种情况应该**保留**。 **最终决策**: 该论文的核心贡献在于构建了一个具备规划、工具使用、评估和自我修正能力的迭代式智能体框架。其研究焦点是智能体的工作机制和自我完善能力，而非其在特定领域的应用结果。这完全符合我关于“LLM智能体及其演化”的研究课题，特别是“单智能体”和“自我演化”两个方向。因此，这篇论文应该被保留。"
    },
    {
        "index": "#26",
        "title": "Language-conditioned world model improves policy generalization by reading environmental descriptions",
        "link": "/arxiv/2511.22904",
        "arxiv_id": "2511.22904",
        "authors": "Anh Nguyen, Stefan Lee",
        "summary": "To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying \"what to do\". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-11-28",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:05.905225",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种名为 **LED-WM** 的新框架。该框架是一个**语言条件的世界模型**，旨在通过让智能体理解描述环境动态的语言，来提升其策略的泛化能力。 - **判断**: 这完全符合“**构建、改进或演化 LLM智能体**”的核心要求。它不是将现有智能体简单应用于某个领域，而是提出了一种新的方法论来**改进**智能体的核心组件——世界模型。因此，根据第一步，应**保留**。 2.  **第二步：正面指标分析** - 论文的核心是构建一个 `LLM-based Agent`，其架构属于 `Agentic AI` 范畴。 - 论文研究的 `world model`（世界模型）是智能体进行有效 `Planning`（规划）和决策的基础。虽然论文明确指出其方法“无需规划”，但它构建的正是支撑规划能力的底层模型，这与智能体的规划能力密切相关。 - 世界模型本质上是一种关于环境动态的结构化 `Memory`（记忆），因此也触及了智能体的记忆能力。 - 综上，该论文包含了多个核心关注点，相关性很高。 3.  **第三步：排除标准分析** - 论文的主要贡献不是关于 `Safety`、`Alignment` 或 `Interpretability`。 - 论文可能涉及视觉输入，但视觉信息是作为智能体感知环境的一部分，研究的核心是**如何将语言描述与这些观察（包括视觉）进行关联**，而不是视觉模型本身。这符合“除非它们被用作智能体感知环境的工具”的例外情况。 - 因此，该论文未触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文处于一个关键位置。它不是提出一个新的规划算法（如ToT），而是提出了一个**支撑规划的底层架构——世界模型**。根据筛选标准，“如果论文是关于智能体如何进行规划或在复杂任务中进行多步推理...的新的Agentic框架”，就应该保留。LED-WM正是这样一个新的Agentic框架，它通过改进世界模型来增强智能体的推理基础，使其策略能更好地泛化到新环境。这属于“保留”的范畴，因为它直接关系到智能体的自主决策能力，而非仅仅提升LLM本身的数学或逻辑推理。 **最终决策**: 该论文的核心贡献在于提出了一种**改进LLM智能体基础能力（世界建模）的新框架LED-WM**。它通过让智能体理解描述环境动态的语言，显著提升了策略的泛化能力。这直接命中了研究课题中“**构建、改进或演化 LLM智能体**”的核心目标，特别是“**单智能体**”方向下的感知与规划基础能力。它不是简单的应用，也不是关于安全或多模态本身的研究。因此，这篇论文与你的研究范围高度相关，应被筛选出来。"
    },
    {
        "index": "#83",
        "title": "Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks",
        "link": "/arxiv/2511.21726",
        "arxiv_id": "2511.21726",
        "authors": "Yicong Zheng, Kevin L. McKee, Thomas Miconi, Zacharie Bugaud, Mick van Gelderen, Jed McCaleb",
        "summary": "How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-11-20",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:05.964650",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于构建和改进一个具有特定能力的LLM智能体。我的判断过程如下： **第一步：核心判断——保留** 这篇论文的本质是提出一个名为SUMER的新框架，其核心是一个“端到端强化学习智能体”。这个智能体的目标不是简单地将LLM应用于某个领域，而是提出了一种全新的、更通用的方法来解决LLM的长上下文记忆问题。它挑战了主流的“记忆压缩”范式，转而构建一个能够主动“搜索”原始信息的智能体。这完全符合“构建、改进LLM智能体的方法论或新框架”的保留标准。 **第二步：正面指标——高度相关** 论文包含了多个你的核心关注点： - **核心范式**: 论文明确提出了一个`LLM-based Agent`（SUMER），并且使用了`Reinforcement Learning`来训练它。 - **智能体能力**: - **Tool Use / Tool Augmentation**: 论文的核心贡献之一就是让智能体“学会使用搜索工具”，这是智能体能力的关键一环。 - **Memory**: 整篇论文都围绕LLM的`长上下文记忆`问题展开，并提出了一种新的记忆处理范式（搜索 vs. 压缩）。 - **Planning**: “Goal-Directed Search”（目标导向搜索）本身就是一种规划能力的体现。智能体需要根据目标问题规划其搜索策略。 - **演化机制**: 智能体通过“强化学习”和“经验回放”来“学会”如何更有效地使用工具，这本质上是一种通过经验和反馈进行的`Self-Improvement`（自我完善）或`Iterative Improvement`（迭代改进）。 **第三步：排除标准——未触及** 论文的研究焦点是智能体的架构和能力提升，完全没有涉及安全与对齐、多模态与视觉等排除领域。 **第四步：处理特殊和模糊情况** - **推理/规划**: 论文的研究内容完美符合“保留”条件。它不是在改进LLM的基础推理能力，而是在构建一个智能体框架，让智能体通过规划（目标导向搜索）和行动（使用工具）来完成复杂的长上下文任务。这与ReAct等Agentic框架的思想一脉相承。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是构建了一个名为SUMER的新型LLM智能体。该智能体通过强化学习学会了如何使用搜索工具来处理长上下文记忆任务，这直接对应了你研究范围内的“单智能体”方向，特别是“工具使用”和“记忆”这两个子方向，并且其学习机制也触及了“自我演化”的范畴。因此，这篇论文是高度相关且应该被保留的前沿研究。"
    },
    {
        "index": "#80",
        "title": "A Benchmark for Procedural Memory Retrieval in Language Agents",
        "link": "/arxiv/2511.21730",
        "arxiv_id": "2511.21730",
        "authors": "Ishant Kohar, Aswanth Krishnan",
        "summary": "Current AI agents excel in familiar settings, but fail sharply when faced with novel tasks with unseen vocabularies -- a core limitation of procedural memory systems. We present the first benchmark that isolates procedural memory retrieval from task execution, evaluating whether agents can recognize functionally equivalent procedures that span different object instantiations. Using ALFWorld, we construct dual corpora of expert and LLM-generated trajectories and evaluate six retrieval methods using systematically stratified queries. Our results expose a clear generalization cliff: embedding-based methods perform strongly on familiar contexts, yet degrade considerably on novel ones, while LLM-generated procedural abstractions demonstrate reliable cross-context transfer. Controlled ablations show that although embeddings capture some lexical-level abstraction, they fundamentally treat procedures as unordered bags of words, discarding temporal structure necessary for cross-context transfer. Corpus scale delivers far larger gains than representation enrichment, revealing an architectural ceiling in current encoders. Our benchmark offers the first diagnostic framework separating genuine procedural understanding from surface-level memorization and gives tools for developing retrieval systems capable of dependable generalization. Resources available at our GitHub repository (https://github.com/qpiai/Proced_mem_bench).",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-11-21",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:05.963094",
        "filter_reason": "这篇论文符合筛选标准，应该保留。我的判断过程如下： 1.  **第一步：核心判断** - 论文的本质是构建一个**基准**，用于评估LLM智能体的**程序性记忆检索**能力。虽然它没有提出一个全新的智能体架构，但它提供了一个**方法论工具**，其核心目标是诊断和衡量现有智能体在记忆能力上的缺陷。 - 这不属于“非演化型应用”，因为它没有将智能体应用于生物、金融等特定领域，而是研究智能体本身的核心能力。 - 这不属于“非Agentic的推理”，因为它关注的是智能体如何“检索”和“泛化”执行任务的程序，这是智能体自主行为的基础，而非提升LLM本身的数学或逻辑推理能力。 - 因此，这篇论文的核心贡献在于为**改进LLM智能体**提供了关键的评估框架和诊断工具，符合“保留”标准。 2.  **第二步：正面指标** - 论文高度符合我的核心关注点。标题和摘要中明确包含了 `Language Agents` 和 `Procedural Memory Retrieval`，这直接命中了“单智能体”方向下的**`Memory`（记忆）**这一核心能力。 - 论文探讨了智能体如何“recognize functionally equivalent procedures”，这涉及到智能体的泛化和理解能力，是构建更强大智能体的关键。 3.  **第三步：排除标准** - 论文的研究焦点是智能体的记忆机制，不涉及 `Safety`, `Alignment`, `Interpretability` 等安全与对齐主题。 - 论文基于文本环境ALFWorld，不涉及 `Vision`, `MLLMs` 等多模态内容。 - 因此，没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - 论文研究的“程序性记忆检索”是智能体进行有效规划和执行多步任务的前提。它分析了现有方法（如embedding-based）的局限性，并指明了未来改进的方向（如保留时间结构），这完全符合“保留”关于智能体推理/规划类论文的标准。 5.  **第五步：最终决策** - 综合来看，这篇论文的核心贡献是创建了一个专门用于评估和诊断LLM智能体“记忆”能力的基准。它通过严谨的实验揭示了当前方法的不足，并为“开发能够实现可靠泛化的检索系统”提供了工具和方向。这项工作直接服务于“构建、改进或演化LLM智能体”这一核心目标，特别是深化了对智能体关键组件——记忆系统的理解。因此，这篇论文与我的研究课题高度相关，应该被保留。"
    },
    {
        "index": "#81",
        "title": "Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems",
        "link": "/arxiv/2511.21729",
        "arxiv_id": "2511.21729",
        "authors": "Jithin Krishnan",
        "summary": "Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, \"abstained\" versus \"unsupported\"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-11-21",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:05.963538",
        "filter_reason": "这篇论文符合您的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——保留** 论文的核心贡献并非简单地将RAG（检索增强生成）技术应用于某个特定领域，而是提出了一种**构建和改进多智能体系统的方法论**。其核心思想是“协同整合”和“自适应校准”，探讨了如何让系统内的多个组件（如混合检索、集成验证）有效协作，从而实现远超单个组件性能的系统级提升。这直接对应了您研究目标中的“构建、改进或演化 LLM智能体”，特别是“多智能体”方向。它不是在“使用”一个已有的智能体框架，而是在“研究如何构建”一个更高效的智能体系统架构，因此不属于“非演化型应用”的排除范畴。 **第二步：正面指标——高度相关** 论文包含了多个核心关注点： 1.  **核心范式**: 论文标题和摘要明确提到了 **`Multi-Agent RAG Systems`**，直接命中“多智能体”方向。 2.  **多智能体**: 论文的核心发现“协同整合 matters more than the strength of any single component”本质上是在研究智能体（或组件）间的**`Collaboration`**（协作）机制。 3.  **演化机制**: 论文提出的“`Adaptive Calibration`”（自适应校准）是一种根据环境反馈（检索质量）进行动态调整的机制，这可以被视为一种系统层面的**`Self-Correction`**（自我纠正）或**`Iterative Improvement`**（迭代改进），与“自我演化”的理念相符。 **第三步：排除标准——未触发** 虽然论文提到了“hallucinations”（幻觉）和“safely”（安全地），但这并非其研究主旨。论文是将“不增加幻觉”和“行为安全”作为评估系统性能的**指标**，而不是提出新的安全或对齐理论。其核心贡献在于系统架构设计，而非安全与对齐研究，因此不触发排除标准。 **第四步：处理特殊和模糊情况** 本论文不涉及“推理/规划”或“自我演化的应用”的特殊情况，其核心贡献非常明确地落在多智能体系统的构建方法论上。 **第五步：最终决策** 综合分析，该论文的核心贡献在于提出了一种通过“协同整合”与“自适应校准”来构建更高效、更可靠的多智能体RAG系统的方法论。它深入探讨了多智能体间的协作与系统层面的自我纠正机制，完全符合您研究课题中的“多智能体”和“自我演化”方向。因此，这篇论文是高度相关的前沿研究，应被保留。"
    },
    {
        "index": "#82",
        "title": "Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue",
        "link": "/arxiv/2511.21728",
        "arxiv_id": "2511.21728",
        "authors": "Lin Yu, Xiaofei Han, Yifei Kang, Chiung-Yi Tseng, Danyang Zhang, Ziqian Bi, Zhimo Han",
        "summary": "Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\\%), persuasive success rate (+19\\%), and long-term user engagement (+23\\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-11-21",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:05.964123",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为 `AffectMind` 的新颖智能体框架。其核心贡献并非简单地将现有LLM应用于营销领域，而是构建了一个具有特定架构（PKGN, EIAM, RDL）的智能体，以实现“proactive reasoning”（主动推理）和“dynamic knowledge grounding”（动态知识获取）。这完全符合“构建、改进LLM智能体的方法论或新框架”的保留标准。它不是简单的非演化型应用，因为它提出了新的智能体组件和交互机制。 **第二步：正面指标** - 论文包含了多个我的核心关注点： - **智能体能力**: - `Planning`: 论文明确提出了 `proactive reasoning`（主动推理）和 `persuasion strategies`（说服策略），这属于智能体的规划能力。 - `Memory`: `Proactive Knowledge Grounding Network (PKGN)` “continuously updates factual and affective context”（持续更新事实和情感上下文），这是一种明确的记忆机制。 - `Self-Correction / Self-Improvement`: `Reinforced Discourse Loop (RDL)` 通过用户反馈的强化信号来“optimizes emotional coherence and engagement”（优化情感连贯性和参与度），这是一个典型的基于环境反馈进行自我修正和迭代的机制，与“自我演化”方向高度相关。 **第三步：排除标准** - **安全与对齐**: 论文中的 \"emotionally aligned\" 指的是在营销对话中与用户的情感保持一致以实现说服目标，这属于任务层面的对齐，而非AI安全、伦理或价值观层面的对齐研究。因此，不触发排除标准。 - **多模态与视觉**: 论文确实是多模态的，使用了文本、视觉和韵律。但根据筛选规则，这些多模态输入是作为智能体感知环境的工具（通过PKGN组件），服务于智能体的核心功能（知识获取和情感理解），而不是论文研究的核心本身。论文的核心是智能体架构，而非新的视觉或多模态模型。因此，不触发排除标准。 **第四步：处理特殊和模糊情况** - **推理/规划**: 论文的 `proactive reasoning` 和策略调整属于智能体在复杂任务中的多步推理和规划，符合保留条件。 - **自我演化的应用**: 论文中的 `Reinforced Discourse Loop (RDL)` 是一个明确的自我演化/改进机制。即使它被应用在营销这个特定领域，根据“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域，也应该保留”的规则，这篇论文也应该被保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是构建了一个集成了规划、记忆和自我演化机制的新型LLM智能体（`AffectMind`）。虽然它以营销对话为应用场景，并使用了多模态输入，但这些元素都是为了支撑其智能体架构的创新性。该研究直接推动了“单智能体”和“自我演化”两个核心方向的前沿，完全符合我关于“LLM智能体及其演化”的研究目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#96",
        "title": "A General Highly Accurate Online Planning Method Integrating Large Language Models into Nested Rollout Policy Adaptation for Dialogue Tasks",
        "link": "/arxiv/2511.21706",
        "arxiv_id": "2511.21706",
        "authors": "Hui Wang, Fafa Zhang, Xiaoyu Zhang, Chaoxu Mu",
        "summary": "In goal-oriented dialogue tasks, the main challenge is to steer the interaction towards a given goal within a limited number of turns. Existing approaches either rely on elaborate prompt engineering, whose effectiveness is heavily dependent on human experience, or integrate policy networks and pre-trained policy models, which are usually difficult to adapt to new dialogue scenarios and costly to train. Therefore, in this paper, we present Nested Rollout Policy Adaptation for Goal-oriented Dialogue (NRPA-GD), a novel dialogue policy planning method that completely avoids specific model training by utilizing a Large Language Model (LLM) to simulate behaviors of user and system at the same time. Specifically, NRPA-GD constructs a complete evaluation mechanism for dialogue trajectories and employs an optimization framework of nested Monte Carlo simulation and policy self-adaptation to dynamically adjust policies during the dialogue process. The experimental results on four typical goal-oriented dialogue datasets show that NRPA-GD outperforms both existing prompt engineering and specifically pre-trained model-based methods. Impressively, NRPA-GD surpasses ChatGPT and pre-trained policy models with only a 0.6-billion-parameter LLM. The proposed approach further demonstrates the advantages and novelty of employing planning methods on LLMs to solve practical planning tasks.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-11-17",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:06.002784",
        "filter_reason": "这篇论文完全符合你的研究范围，核心判断依据如下： 1.  **第一步：核心判断 (保留)** 论文的核心贡献并非将LLM简单应用于对话领域，而是提出了一种名为 **NRPA-GD** 的**全新对话策略规划方法**。这个方法是一个完整的框架，它利用LLM来模拟用户和系统行为，并通过嵌套蒙特卡洛模拟和策略自适应机制来动态调整策略。这本质上是在**构建和改进一个LLM智能体的规划与决策能力**，完全符合“构建、改进或演化LLM智能体”的核心目标。它不是简单的应用，而是一个关于智能体“如何做规划”的方法论创新。 2.  **第二步：正面指标 (高度匹配)** 论文包含了多个核心关注点： *   **智能体能力**: 论文的核心是 **`Planning`**。标题和摘要中反复强调“Online Planning Method”、“dialogue policy planning”，这直接命中了单智能体方向的核心。 *   **演化机制**: 论文中的“policy self-adaptation”和“dynamically adjust policies during the dialogue process”体现了 **`Self-Correction`** 和 **`Iterative Improvement`** 的思想。智能体不是静态执行策略，而是在交互中根据模拟结果动态优化自身策略，这是一种在任务执行过程中的自我完善和迭代。 3.  **第三步：排除标准 (未触发)** 论文的研究焦点是智能体的规划算法，不涉及安全、对齐、可解释性等问题，也未使用多模态技术。因此，它没有被任何排除标准所排除。 4.  **第四步：特殊和模糊情况 (明确符合保留条件)** 论文完美地符合“推理/规划”的保留规则。它不是在提升LLM本身的基础推理能力（如数学计算），而是在构建一个**让智能体在复杂任务（目标导向对话）中进行多步规划和决策的框架**。这与ReAct、ToT等Agentic框架的研究范式一脉相承，都是关于如何让LLM作为智能体的大脑，更好地进行规划和行动。 **总结**: 该论文的核心贡献是提出了一种新颖的、基于LLM的在线规划框架（NRPA-GD），用于提升智能体在目标导向对话任务中的表现。它直接解决了智能体如何进行**自主规划**和**动态策略调整**的关键问题，属于“单智能体”研究范畴下的“规划”与“自我反思/修正”子方向。因此，这篇论文与你的研究课题“LLM智能体及其演化”高度相关，应当保留。"
    },
    {
        "index": "#105",
        "title": "ThetaEvolve: Test-time Learning on Open Problems",
        "link": "/arxiv/2511.23473",
        "arxiv_id": "2511.23473",
        "authors": "Yiping Wang, Shao-Rong Su, Zhiyuan Zeng, Eva Xu, Liliang Ren, Xinyu Yang, Zeyi Huang, Xuehai He, Luyao Ma, Baolin Peng, Hao Cheng, Pengcheng He, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen",
        "summary": "Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-11-28",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:06.018800",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于提出了一种新的LLM智能体自我演化框架。以下是根据你的筛选标准进行的详细判断： 1.  **第一步：核心判断——保留** - 论文的本质是构建一个名为ThetaEvolve的**新框架**，该框架的核心目标是让LLM智能体能够**“自我演化”**。摘要中明确指出，该框架允许模型“continually learn from their experiences”（从经验中持续学习），并且“the model indeed learns evolving capabilities”（模型确实学习到了演化能力）。这完全符合“构建、改进或演化LLM智能体”的核心目标。 - 它不是将已有框架简单应用于数学领域，而是提出了一种新的演化机制（在测试时进行强化学习），使得模型能够内化演化策略，这属于方法论层面的创新，而非单纯的应用。 2.  **第二步：正面指标——高度匹配** - **核心范式**: 论文标题和摘要反复强调“Evolve”，核心贡献是“evolving framework”，完全命中`Self-Evolving`。 - **智能体能力**: 框架通过生成程序来与环境（开放性问题）交互，这属于`Tool Use`。整个测试时学习的过程，包括奖励塑造和懒惰惩罚，都是为了实现`Self-Correction`和`Self-Improvement`。 - **演化机制**: 论文的核心机制是“Reinforcement Learning (RL) at test time”，这是一种明确的`Self-Improvement`和`Iterative Improvement`机制。摘要中“learns evolving capabilities”的结论，直接证明了其演化机制的有效性。 3.  **第三步：排除标准——未触发** - 论文的主要贡献是关于智能体的能力和演化机制，没有涉及`Safety`、`Alignment`、`Interpretability`等安全与对齐议题。 - 论文不涉及`Vision`、`MLLMs`等多模态内容，其智能体通过生成和执行代码/程序来感知和改变环境，这属于工具使用，而非研究的核心。 4.  **第四步：处理特殊和模糊情况——符合保留规则** - **自我演化的应用**: 这篇论文是“自我演化应用”的完美范例。虽然它应用在“数学开放问题”这一特定领域，但其**核心贡献是提出了一种新的“自我演化”机制（Test-time RL）**，并证明了该机制能让模型获得演化能力。根据你的规则“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”，这篇论文必须保留。 - **推理/规划**: 论文中的智能体需要规划生成什么样的程序来优化问题，并通过RL学习更好的规划策略。这属于智能体在复杂任务中的多步推理和规划，符合保留条件，而非简单的LLM基础能力提升。 **最终决策**: 这篇论文的核心贡献是ThetaEvolve框架，它通过在测试时进行强化学习，使LLM智能体能够从经验中学习并实现自我演化。这直接命中了你研究课题中的“自我演化”方向，并且是一个方法论层面的创新，而非简单的领域应用。因此，这篇论文高度相关，应该被保留。"
    },
    {
        "index": "#119",
        "title": "SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning",
        "link": "/arxiv/2511.22367",
        "arxiv_id": "2511.22367",
        "authors": "Hugo Hazard, Zafeirios Fountas, Martin A. Benfeghoul, Adnan Oomerjee, Jun Wang, Haitham Bou-Ammar",
        "summary": "Continual learning, one's ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-11-27",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:06.036712",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接属于“自我演化”方向。以下是我的详细判断过程： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心是提出一种名为 `SuRe` 的新方法，用于解决LLM的持续学习问题。持续学习的目标是让模型在适应新任务的同时不忘记旧知识，这本质上是一个通过经验进行**自我完善和迭代**的过程。 - **判断**: 论文的核心贡献是构建一种**新的自我演化机制**，而不是将LLM作为工具应用到某个特定领域。因此，根据第一步的筛选标准，应该**保留**。 2.  **第二步：正面指标** - 论文与您的核心关注点高度契合： - **核心范式**: 论文的研究内容是 `Self-Evolving` 的一个关键实现路径。 - **演化机制**: 论文明确提出了 `Self-Improvement` 和 `Iterative Improvement` 的机制。`SuRe` 方法通过“惊喜”来选择需要重点学习的样本，而“双学习器”设计则负责整合新知识，这两者共同构成了一个完整的自我演化闭环。 - **智能体能力**: 论文深入探讨了 `Memory` 机制。它解决了智能体记忆的两个核心问题：**选择**（通过“惊喜”优先级回放决定记忆什么）和**整合**（通过快慢学习器决定如何巩固记忆）。这是对智能体长期记忆和知识管理能力的直接贡献。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或视觉多模态。其焦点纯粹在于提升模型的学习和演化能力，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是提出“自我演化”机制本身的典型范例，而不是应用。它提出的 `SuRe` 方法和双学习器设计是通用的，可以应用于任何需要持续学习的LLM场景，完全符合您对“提出新的自我演化机制”的要求。 **最终决策**: 这篇论文的核心贡献在于提出了一种新颖的、由“惊喜”驱动的优先级回放机制和双学习器整合策略，以实现LLM的持续学习。这直接解决了LLM智能体如何通过经验进行**自我演化**和**知识巩固**的根本性问题。虽然论文标题和摘要中没有频繁使用“Agent”一词，但其研究内容——即让模型具备持续学习和适应新任务的能力——是构建高级、自主演化的LLM智能体的基石。因此，这篇论文是您研究课题“LLM智能体及其演化”中“自我演化”方向的**高质量前沿文献**，应被保留。"
    },
    {
        "index": "#117",
        "title": "DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning",
        "link": "/arxiv/2511.22570",
        "arxiv_id": "2511.22570",
        "authors": "Zhihong Shao, Yuxiang Luo, Chengda Lu, Z. Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, Xiaokang Zhang",
        "summary": "Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-11-27",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:06.030583",
        "filter_reason": "这篇论文的核心贡献完全符合您的研究范围，特别是“自我演化”方向。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断——保留** - **论文本质**: 这篇论文的本质并非简单地应用LLM解决数学问题，而是提出了一种**新的方法论和框架**，用于实现LLM智能体的自我验证和自我完善。其核心是构建一个由“生成器”和“验证器”组成的系统，让智能体能够识别并修正自身推理过程中的错误。这完全符合“构建、改进或演化LLM智能体”的核心目标。 - **排除项分析**: - **非演化型应用**: 论文虽然应用于数学领域，但其核心贡献是“自我验证”这一机制本身，而非数学问题的解决方案。这符合第四步中“自我演化应用”的例外保留规则。 - **非Agentic的推理**: 论文超越了单纯提升LLM基础数学推理能力的范畴。它引入了一个结构化的、包含自我反思和修正循环的Agentic框架（生成器-验证器循环），这正是智能体自主性的体现。 2.  **第二步：正面指标——高度匹配** - **核心范式**: 论文的核心是 `Self-Evolving`。它描述了一个系统如何通过内部反馈（验证器）来迭代提升自身（生成器）。 - **智能体能力**: 论文明确体现了 `Self-Correction`（“identify and resolve as many issues as possible in their own proofs”）和 `Self-Reflection`（通过验证器进行自我验证）。 - **演化机制**: 论文的核心机制就是 `Self-Improvement` 和 `Iterative Improvement`。它提出了一个动态演化的循环：更强的生成器产生更难的证明，这些证明被用来训练更强的验证器，而更强的验证器又能训练出更强的生成器。这是一个典型的自我演化闭环。 3.  **第三步：排除标准——不适用** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态，因此没有被这些标准排除。 4.  **第四步：处理特殊和模糊情况——符合保留条件** - **推理/规划**: 这篇论文是关于智能体如何进行规划和推理的绝佳范例。它不是提出一个新的CoT技巧，而是构建了一个完整的Agentic框架来确保推理过程的正确性。生成器负责规划和生成证明步骤，验证器则作为反思和纠错工具，这完全符合“保留”标准。 - **自我演化的应用**: 如第一步所述，这篇论文是“自我演化应用”例外情况的典型代表。其核心创新点在于“自我演化”的机制，即使应用场景是数学，也应被保留。 **最终决策**: 这篇论文的核心贡献在于提出了一种新颖的、用于实现LLM智能体自我验证和自我演化的框架。它通过构建生成器-验证器的协同演化循环，使智能体能够主动发现并修正自身推理链中的错误，从而实现能力的迭代提升。这直接命中了您研究目标中的“自我演化”方向，并深刻触及了“自我反思”和“自我修正”等核心智能体能力。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#121",
        "title": "Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation",
        "link": "/arxiv/2511.22311",
        "arxiv_id": "2511.22311",
        "authors": "Fiona Y. Wang, Di Sheng Lee, David L. Kaplan, Markus J. Buehler",
        "summary": "Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.",
        "subjects": "Artificial Intelligence, Mesoscale and Nanoscale Physics, Soft Condensed Matter, Computation and Language, Machine Learning",
        "date": "2025-11-27",
        "category": "cs.CL",
        "crawl_time": "2025-12-02T11:00:06.037811",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建和演化一个新颖的LLM多智能体系统。以下是详细的判断过程： 1.  **第一步：核心判断——保留** - **论文本质**: 这篇论文的本质不是简单地将LLM作为工具应用于蛋白质设计，而是**提出了一种全新的、去中心化的、基于群体智能的LLM智能体框架**。摘要中明确指出 \"we present a decentralized, agent-based framework inspired by swarm intelligence\"。这完全符合“构建LLM智能体（Agentic LLM）、多智能体系统”的核心要求。 - **排除规则应用**: 论文虽然应用于蛋白质设计领域，但它不属于“非演化型应用”。因为其核心贡献是方法论本身——即如何通过多个智能体的协作与演化来完成复杂设计任务，而不是仅仅报告应用结果。论文最后一句 \"Beyond proteins, the approach lays the groundwork for collective LLM-driven design...\" 更是强调了其方法的通用性，进一步证明其核心是框架而非应用。 2.  **第二步：正面指标——高度相关** - **核心范式**: 论文明确包含了 `LLM-based Agents` 和 `Multi-Agent Systems (MAS)` 的核心范式，并提到了 `swarm intelligence`，这是多智能体研究的一个重要分支。 - **智能体能力**: 论文描述了智能体具备 `Memory` (\"memory and feedback from previous iterations\") 和迭代改进的能力。 - **多智能体**: 论文的核心就是多智能体系统，包含了 `Collaboration` 和 `Communication` (\"local neighborhood interactions\")，并实现了 `emergent behaviors`（涌现行为），这是多智能体系统的关键特征。 - **演化机制**: 论文描述了一个清晰的迭代演化过程：\"agents iteratively propose... mutations by integrating... feedback from previous iterations\"。这完全符合 `Self-Improvement` 和 `Iterative Improvement` 的定义，属于自我演化的范畴。 3.  **第三步：排除标准——不适用** - 论文的主要贡献不是关于安全、对齐或多模态，因此不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这正是“自我演化的应用”这一例外情况的完美体现。论文的核心是提出一种**新的“自我演化”机制**（通过多智能体迭代、记忆和反馈实现的集体演化），并将其应用于蛋白质设计。根据您的规则，这种情况应该**保留**。 **最终决策**: 这篇论文的核心贡献是构建了一个新颖的、去中心化的LLM多智能体框架，该框架通过智能体间的协作、记忆和迭代反馈，实现了复杂任务的涌现式设计和自我演化。它精准地命中了您研究焦点中的“多智能体”和“自我演化”两个方向，并且其方法论贡献超越了具体的应用领域。因此，这篇论文是您研究课题的理想候选。"
    },
    {
        "index": "#36",
        "title": "Experts are all you need: A Composable Framework for Large Language Model Inference",
        "link": "/arxiv/2511.22955",
        "arxiv_id": "2511.22955",
        "authors": "Shrihari Sridharan, Sourjya Roy, Anand Raghunathan, Kaushik Roy",
        "summary": "Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or \"experts\". However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential \"plan--act--observe\" loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.",
        "subjects": "Machine Learning",
        "date": "2025-11-28",
        "category": "cs.LG",
        "crawl_time": "2025-12-02T11:00:06.519084",
        "filter_reason": "这篇论文完全符合您的研究范围，核心判断依据如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将现有智能体框架应用于某个领域，也不是关于模型基础设施。它的核心贡献是提出一个名为 **Comp-LLM 的新颖推理框架**。这个框架通过分解查询、分配专家、构建依赖图和并行执行来改进LLM的推理过程。这本质上是在**构建和改进一个LLM智能体的规划和执行架构**，属于Agentic AI的核心方法论研究。 2.  **第二步：正面指标** - 论文命中了多个核心正面指标： - **核心范式**: 论文明确与 **Multi-Agent Systems (MAS)** 框架进行对比，并旨在改进其性能。其提出的“专家”协作模式可以被视为一种多智能体协作的变体。 - **智能体能力**: 论文的核心是 **Planning**。其“Sub-query Generator”负责分解复杂问题并构建“sub-query dependency graph”，这是一个明确的规划过程。同时，将子查询分配给不同的“专家”，这与智能体的 **Tool Use / Tool Augmentation** 机制高度相似，每个专家就像一个专门化的工具或子智能体。 - **多智能体**: 论文直接对比了“sequential multi-agent frameworks”，并提出了一个通过依赖图实现并行协作的改进方案，这直接触及了多智能体间的**协作**与**通信**机制。 3.  **第三步：排除标准** - 论文的主要贡献是关于推理框架的效率和准确性，不涉及安全、对齐、可解释性或多模态等排除领域。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是典型的“保留”案例。它不是在提升LLM的基础数学或逻辑能力，而是在构建一个**新的Agentic框架**来处理复杂的多步推理任务。它提出的依赖图和并行执行机制是对现有ReAct等顺序推理框架的演进和优化，完全符合您对“智能体如何进行规划或在复杂任务中进行多步推理”的研究焦点。 **总结**: 该论文的核心贡献是 **Comp-LLM**，一个旨在提升LLM推理效率和效果的可组合框架。它通过引入规划（依赖图）、专业化（专家分配）和并行执行等机制，构建了一个更高级的智能体架构。这直接对齐了您研究目标中的“构建、改进或演化LLM智能体”，特别是在**单智能体**的**规划**和**工具使用**，以及**多智能体**的**协作**方向上。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#96",
        "title": "TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices",
        "link": "/arxiv/2511.22138",
        "arxiv_id": "2511.22138",
        "authors": "Mohd Ariful Haque, Fahad Rahman, Kishor Datta Gupta, Khalil Shujaee, Roy George",
        "summary": "This paper investigates the effectiveness of small language models (SLMs) for agentic tasks (function/tool/API calling) with a focus on running agents on edge devices without reliance on cloud infrastructure. We evaluate SLMs using the Berkeley Function Calling Leaderboard (BFCL) framework and describe parameter-driven optimization strategies that include supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), reinforcement learning (RL)-based optimization, preference alignment via Direct Preference Optimization (DPO), and hybrid methods. We report results for models including TinyAgent, TinyLlama, Qwen, and xLAM across BFCL categories (simple, multiple, parallel, parallel-multiple, and relevance detection), both in live and non-live settings, and in multi-turn evaluations. We additionally detail a DPO training pipeline constructed from AgentBank data (e.g., ALFRED), including our conversion of SFT data to chosen-rejected pairs using TinyLlama responses as rejected outputs and manual validation. Our results demonstrate clear accuracy differences across model scales where medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters), achieving up to 65.74% overall accuracy, and 55.62% multi-turn accuracy with hybrid optimization. This study highlights the importance of hybrid optimization strategies that enable small language models to deliver accurate, efficient, and stable agentic AI on edge devices, making privacy-preserving, low-latency autonomous agents practical beyond the cloud.",
        "subjects": "Machine Learning",
        "date": "2025-11-27",
        "category": "cs.LG",
        "crawl_time": "2025-12-02T11:00:06.580660",
        "filter_reason": "这篇论文符合筛选标准，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是**改进LLM智能体**。其核心贡献并非简单地将现有智能体框架应用到边缘设备这一特定场景，而是提出并验证了一套**参数驱动的优化策略**（包括SFT、PEFT、RL、DPO及混合方法），旨在**提升小型语言模型（SLMs）执行智能体任务（特别是工具/函数调用）的能力**。虽然论文的最终目标是在边缘设备上部署，但其研究核心是“如何让模型在智能体能力上变得更强”，而非“如何部署模型”。因此，它不属于“非演化型应用”或“基础设施”的排除范畴。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点： - **核心范式**: `Agentic AI`, `LLM-based Agents`。 - **智能体能力**: `Tool Use / Tool Augmentation` 是论文评估和优化的核心能力。 - **演化机制**: 论文提出的`Supervised Fine-Tuning (SFT)`, `Reinforcement Learning (RL)`, `Direct Preference Optimization (DPO)` 等方法，本质上都是一种**迭代改进** 和 **自我完善** 的机制，通过数据和反馈来提升模型性能，这与“自我演化”的思路高度相关。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全与对齐。虽然使用了DPO（Direct Preference Optimization），但其目的是为了提升工具调用的**准确性**，而非为了实现伦理、安全或防止幻觉等对齐目标。因此，不属于排除范围。 - 论文不涉及多模态与视觉。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文聚焦于`Tool Use`，这是智能体的一项核心能力，与规划、推理并列。它研究的是智能体如何更准确地与外部工具交互，完全符合“保留”标准。 - **自我演化的应用**: 这篇论文不属于“自我演化的应用”，但其核心贡献是提出一种**改进智能体的方法论**，这正是我研究范围的核心。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于**提出并系统评估了一套用于提升LLM智能体（特别是工具使用能力）的优化策略**。它直接回应了研究课题中“构建、改进或演化LLM智能体”的核心目标，尤其是在“单智能体”方向下的“工具使用”和“自我完善”子方向上做出了明确的贡献。因此，这篇论文高度相关，应被保留。"
    },
    {
        "index": "#125",
        "title": "Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs",
        "link": "/arxiv/2511.21928",
        "arxiv_id": "2511.21928",
        "authors": "Yifan Zhou, Sachin Grover, Mohamed El Mistiri, Kamalesh Kalirathnam, Pratyush Kerhalkar, Swaroop Mishra, Neelesh Kumar, Sanket Gaurav, Oya Aran, Heni Ben Amor",
        "summary": "Reinforcement Learning (RL) traditionally relies on scalar reward signals, limiting its ability to leverage the rich semantic knowledge often available in real-world tasks. In contrast, humans learn efficiently by combining numerical feedback with language, prior knowledge, and common sense. We introduce Prompted Policy Search (ProPS), a novel RL method that unifies numerical and linguistic reasoning within a single framework. Unlike prior work that augment existing RL components with language, ProPS places a large language model (LLM) at the center of the policy optimization loop-directly proposing policy updates based on both reward feedback and natural language input. We show that LLMs can perform numerical optimization in-context, and that incorporating semantic signals, such as goals, domain knowledge, and strategy hints can lead to more informed exploration and sample-efficient learning. ProPS is evaluated across fifteen Gymnasium tasks, spanning classic control, Atari games, and MuJoCo environments, and compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO). It outperforms all baselines on eight out of fifteen tasks and demonstrates substantial gains when provided with domain knowledge. These results highlight the potential of unifying semantics and numerics for transparent, generalizable, and human-aligned RL.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-11-26",
        "category": "cs.LG",
        "crawl_time": "2025-12-02T11:00:06.622335",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为“Prompted Policy Search (ProPS)”的新型强化学习方法，该方法将大型语言模型（LLM）置于策略优化循环的中心，使其能够根据数值奖励和自然语言提示直接提出策略更新。 根据筛选标准，我的判断过程如下： 1.  **第一步：核心判断——保留。** 论文的核心并非将LLM作为工具应用于某个特定领域（如金融、医疗），而是提出了一种全新的、让LLM智能体进行自我学习和优化的方法论。LLM在这里不是被动的工具，而是主动的学习者和策略制定者，它通过接收环境反馈（奖励）来迭代改进自身的行为策略。这完全符合“构建、改进或演化LLM智能体”的核心目标，特别是与“自我演化”方向高度契合。 2.  **第二步：正面指标——高度相关。** 论文明确体现了多个核心关注点： *   **自我演化:** 整个ProPS框架就是一个自我演化的闭环。智能体（由LLM驱动的策略）根据环境反馈进行“自我完善”和“迭代改进”，这正是自我演化的核心机制。 *   **LLM-based Agents:** 论文将LLM作为智能体的决策核心，直接参与策略优化，是典型的LLM智能体研究。 *   **规划:** 智能体通过“提出策略更新”来规划如何提升自身性能，这是一种高级的元规划或元认知能力。 3.  **第三步：排除标准——未触发。** 论文虽然提到了“human-aligned RL”，但这只是其方法带来的一个潜在好处，并非论文的核心贡献。论文的核心是算法框架ProPS本身，而不是一个关于对齐或安全性的研究。因此，不触发“安全与对齐”的排除标准。论文也未涉及多模态或视觉内容。 4.  **第四步：处理特殊和模糊情况——符合保留条件。** 这篇论文完美地落在了“推理/规划”的保留范畴。它不是在提升LLM的基础数学或逻辑推理能力，而是在构建一个**Agentic框架**，让LLM的推理能力（包括语言和数值推理）被用于一个复杂的、多步的自主任务——即优化自身的策略以适应环境。这超越了简单的Chain-of-Thought，进入了智能体自主演化的层面。 **最终决策：** 这篇论文的核心贡献在于提出了一种让LLM智能体通过结合语言和数值反馈进行自我策略优化的新框架。这直接命中了研究课题中的“自我演化”和“单智能体”方向，是关于如何构建和演化LLM智能体的前沿方法论研究，而非简单的应用或基础能力提升。因此，这篇论文完全符合筛选要求，应予以保留。"
    },
    {
        "index": "#179",
        "title": "GEO-Detective: Unveiling Location Privacy Risks in Images with LLM Agents",
        "link": "/arxiv/2511.22441",
        "arxiv_id": "2511.22441",
        "authors": "Xinyu Zhang, Yixin Wu, Boyang Zhang, Chenhao Lin, Chao Shen, Michael Backes, Yang Zhang",
        "summary": "Images shared on social media often expose geographic cues. While early geolocation methods required expert effort and lacked generalization, the rise of Large Vision Language Models (LVLMs) now enables accurate geolocation even for ordinary users. However, existing approaches are not optimized for this task. To explore the full potential and associated privacy risks, we present Geo-Detective, an agent that mimics human reasoning and tool use for image geolocation inference. It follows a procedure with four steps that adaptively selects strategies based on image difficulty and is equipped with specialized tools such as visual reverse search, which emulates how humans gather external geographic clues. Experimental results show that GEO-Detective outperforms baseline large vision language models (LVLMs) overall, particularly on images lacking visible geographic features. In country level geolocation tasks, it achieves an improvement of over 11.1% compared to baseline LLMs, and even at finer grained levels, it still provides around a 5.2% performance gain. Meanwhile, when equipped with external clues, GEO-Detective becomes more likely to produce accurate predictions, reducing the \"unknown\" prediction rate by more than 50.6%. We further explore multiple defense strategies and find that Geo-Detective exhibits stronger robustness, highlighting the need for more effective privacy safeguards.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-11-27",
        "category": "cs.LG",
        "crawl_time": "2025-12-02T11:00:06.683522",
        "filter_reason": "这篇论文符合我的研究范围，应当保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是**构建一个新的LLM智能体框架**。其核心贡献是提出了名为 \"GEO-Detective\" 的智能体，并详细描述了其工作机制（模仿人类推理的四步流程、自适应策略选择、工具使用）。虽然它被应用于“图像地理定位”这一特定领域，但论文的焦点在于**如何构建这个智能体**来解决该问题，而不是简单地应用一个已有的智能体框架。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标** - 论文明确包含了多个我的核心关注点： - **核心范式**: 论文标题和摘要中多次提到 `LLM Agents`。 - **智能体能力**: 论文的核心贡献在于智能体的能力构建。摘要明确指出该智能体 `mimics human reasoning and tool use`，并详细描述了其 `Planning` 能力（`a procedure with four steps that adaptively selects strategies`）和 `Tool Use` 能力（`equipped with specialized tools such as visual reverse search`）。这些都是单智能体研究的核心要素。 3.  **第三步：排除标准** - **安全与对齐**: 论文虽然提到了 `privacy risks` 和 `defense strategies`，但这并非其核心贡献。论文的主要目的是展示其构建的智能体的强大能力，并探讨这种能力所带来的隐私影响。它没有提出新的安全、对齐或可解释性方法，因此不属于被排除的类别。 - **多模态与视觉**: 论文使用了 `Large Vision Language Models (LVLMs)` 和 `Images`。但根据筛选标准，视觉在这里是作为**智能体感知环境的工具**而存在的。研究的核心不是改进LVLM本身，而是如何设计一个智能体架构来有效地利用LVLM进行推理和工具调用。这完全符合“除非它们被用作智能体感知环境的工具”的例外情况。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文明确是关于智能体如何进行规划和多步推理的。它提出的“四步流程”和“自适应策略选择”是一个典型的Agentic框架，用于解决复杂任务，而非提升LLM本身的基础推理能力。因此，符合“保留”条件。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于**构建了一个具备规划和工具使用能力的单智能体（GEO-Detective）**，以解决图像地理定位这一复杂任务。它完全符合我研究目标中的“单智能体”方向，特别是关于“规划”和“工具使用”的子方向。尽管其应用场景和讨论的隐私影响可能触及边缘领域，但其本质和方法论贡献是纯粹的Agentic AI研究。因此，应予以保留。"
    },
    {
        "index": "#184",
        "title": "Enhanced Conditional Generation of Double Perovskite by Knowledge-Guided Language Model Feedback",
        "link": "/arxiv/2511.22307",
        "arxiv_id": "2511.22307",
        "authors": "Inhyo Lee, Junhyeong Lee, Jongwon Park, KyungTae Lim, Seunghwa Ryu",
        "summary": "Double perovskites (DPs) are promising candidates for sustainable energy technologies due to their compositional tunability and compatibility with low-energy fabrication, yet their vast design space poses a major challenge for conditional materials discovery. This work introduces a multi-agent, text gradient-driven framework that performs DP composition generation under natural-language conditions by integrating three complementary feedback sources: LLM-based self-evaluation, DP-specific domain knowledge-informed feedback, and ML surrogate-based feedback. Analogous to how knowledge-informed machine learning improves the reliability of conventional data-driven models, our framework incorporates domain-informed text gradients to guide the generative process toward physically meaningful regions of the DP composition space. Systematic comparison of three incremental configurations, (i) pure LLM generation, (ii) LLM generation with LLM reasoning-based feedback, and (iii) LLM generation with domain knowledge-guided feedback, shows that iterative guidance from knowledge-informed gradients improves stability-condition satisfaction without additional training data, achieving over 98% compositional validity and up to 54% stable or metastable candidates, surpassing both the LLM-only baseline (43%) and prior GAN-based results (27%). Analyses of ML-based gradients further reveal that they enhance performance in in-distribution (ID) regions but become unreliable in out-of-distribution (OOD) regimes. Overall, this work provides the first systematic analysis of multi-agent, knowledge-guided text gradients for DP discovery and establishes a generalizable blueprint for MAS-driven generative materials design aimed at advancing sustainable technologies.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-11-27",
        "category": "cs.LG",
        "crawl_time": "2025-12-02T11:00:06.691287",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将一个已有的LLM智能体应用到材料科学领域。它的核心贡献是**构建了一个新颖的“多智能体、文本梯度驱动的框架”**。这个框架本身是方法论上的创新，它定义了多个智能体（或反馈源）如何协同工作，通过迭代反馈来引导生成过程。这完全符合“构建、改进或演化LLM智能体”的核心目标。它不是非演化型应用，因为其框架包含了迭代和自我完善的机制。 2.  **第二步：正面指标** - 论文摘要中包含了多个核心关注点的关键词： - **核心范式**: `Multi-Agent Systems (MAS)` 被明确提及。 - **智能体能力**: `Self-Correction` / `Self-Reflection` 体现在 \"LLM-based self-evaluation\" 中。 - **多智能体**: `Collaboration` 体现在三个互补的反馈源（LLM自评估、领域知识、ML代理）的集成上。 - **演化机制**: `Self-Improvement` / `Iterative Improvement` 体现在 \"iterative guidance from knowledge-informed gradients\" 和整个框架的迭代优化过程中。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或视觉多模态等问题，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这是本案例的关键。虽然论文的应用领域是“双钙钛矿材料发现”，但这完全符合“自我演化的应用”的例外规则。论文的核心是提出了一种**新的“自我演化”机制**——即通过多智能体反馈和知识引导的文本梯度来迭代优化生成结果。作者自己也强调，其贡献是“建立了MAS驱动生成式材料设计的通用蓝图”。这表明，其研究价值在于**智能体框架本身**，而不仅仅是其在特定领域的应用成果。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一种创新的**多智能体协作框架**，该框架通过**迭代反馈机制**实现了生成过程的**自我优化和演化**。尽管其应用场景是材料科学，但其方法论具有通用性，直接命中了研究课题中的“多智能体”和“自我演化”两个核心方向。因此，这篇论文高度相关，应该被保留。"
    },
    {
        "index": "#2",
        "title": "Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent",
        "link": "/arxiv/2511.23436",
        "arxiv_id": "2511.23436",
        "authors": "Jianzhe Lin, Zeyu Pan, Yun Zhu, Ruiqi Song, Jining Yang",
        "summary": "We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-28",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.766217",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接命中了“自我演化”这一核心方向。以下是详细的判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为 `SuperIntelliAgent` 的新框架。其核心目标是实现“持续智能增长”，这是一个典型的自我演化过程。论文明确指出，该框架通过“自我监督交互”和“终身优化过程”来工作，这完全符合“构建、改进或演化 LLM智能体”的核心要求。它不是将现有智能体应用于某个领域，而是提出了一种让智能体自身不断演化的新方法论。 2.  **第二步：正面指标** - 论文摘要中包含了大量与您研究焦点高度相关的核心范式和能力关键词： - **核心范式**: `Agentic AI` (agentic learning framework), `Self-Evolving` (continuous intelligence growth, continual learning, lifelong optimization process)。 - **智能体能力**: `Memory` (dual-scale memory), `Self-Correction` / `Self-Reflection` (verifier evaluates, interaction produces chosen/rejected pairs)。 - **演化机制**: `Self-Improvement` (continual improvement), `Self-Refine` (refinement cycles), `Iterative Improvement` (continual learning)。 - 这些指标密集出现，强有力地证明了论文与您研究课题的高度相关性。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或水印等问题。 - 论文虽然提到了一个扩散模型，但它是作为“学习者”的一部分，与LLM“验证者”协同工作，共同构成自我演化的机制，而不是研究的核心是视觉或多模态本身。因此，它不触犯排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的推理是作为智能体自我演化循环的一部分存在的。LLM验证器通过“step-by-step reasoning”来评估学习者生成的结果，这个评估过程是驱动自我完善和DPO训练的关键。这完全符合“保留”的条件，即推理是智能体框架内用于自我反思和改进的机制。 - **自我演化的应用**: 这篇论文是“自我演化”方向的一个绝佳范例。它的核心贡献就是那个“自我演化机制”本身（学习者-验证者对、双尺度记忆、回放缓冲区等），而不是某个具体应用。因此，它完全符合保留规则。 **最终决策**: 该论文的核心贡献是构建了一个能够让LLM智能体通过自主交互和记忆机制实现持续学习和自我演化的新框架。这精准地契合了您研究课题中的“自我演化”方向，并涉及了“单智能体”中的“记忆”和“自我反思”能力。它不是应用型研究，而是关于智能体如何演化的方法论创新。因此，这篇论文应该被保留。"
    },
    {
        "index": "#1",
        "title": "Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction",
        "link": "/arxiv/2511.23476",
        "arxiv_id": "2511.23476",
        "authors": "Bao Shu, Yan Cai, Jianjian Sun, Chunrui Han, En Yu, Liang Zhao, Jingcheng Hu, Yinmin Zhang, Haoran Lv, Yuang Peng, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Xiangyu Yue",
        "summary": "Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-28",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.765900",
        "filter_reason": "这篇论文完全符合您的研究范围，应被保留。以下是我的详细判断过程： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出了一种名为WMAct的新框架，旨在通过多轮交互来构建和改进LLM智能体的世界模型推理能力。这直接命中了您筛选标准的第一步——论文的本质是关于**构建和改进LLM智能体的方法论**。它并非将智能体作为工具应用于特定领域（如生物、金融），而是专注于智能体本身的核心能力（世界模型、规划、推理）的提升，因此不属于“非演化型应用”或“非Agentic的推理”的排除范畴。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了大量您关注的核心正面指标： *   **核心范式**: 明确提到了 `LLM agents`。 *   **智能体能力**: 核心讨论了 `Planning`（规划）、`Reasoning`（推理），并通过与环境交互来学习，这隐含了工具使用和自我反思的元素。 *   **演化机制**: 论文的核心机制，如“奖励缩放”和“交互频率退火策略”，其目的是让模型“内化环境动态”、“减少冗余”，这本质上是一种**自我完善**和**迭代改进**的过程，属于“自我演化”的范畴。 3.  **第三步：排除标准——不涉及** 该论文的研究焦点是智能体的推理框架，完全不涉及安全对齐、可解释性、多模态视觉等排除标准中的内容。 4.  **第四步：处理特殊和模糊情况——符合保留规则** *   **推理/规划**: 这篇论文是典型的“保留”案例。它不是在提升LLM本身的基础Token预测能力（如数学计算），而是在构建一个让智能体在复杂环境中进行多步规划和交互的Agentic框架。其提出的“Thinking by Doing”范式，是对现有ReAct、ToT等Agentic规划框架的一种创新和改进，完全符合您对智能体规划能力的研究兴趣。 **核心依据总结**: 该论文的核心贡献在于提出了一种新的Agentic框架（WMAct），用于**改进LLM智能体的规划与推理能力**。其方法通过让智能体在与环境的交互中主动学习并内化世界模型，实现了一种**自我完善**的演化机制。这精准地契合了您研究课题中的“单智能体”和“自我演化”两个核心方向。因此，这篇论文是您研究范围内的前沿高质量文献。"
    },
    {
        "index": "#7",
        "title": "Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning",
        "link": "/arxiv/2511.23262",
        "arxiv_id": "2511.23262",
        "authors": "Yang Li, Zhiyuan He, Yuxuan Huang, Zhuhanling Xiao, Chao Yu, Meng Fang, Kun Shao, Jun Wang",
        "summary": "Recent Vision-Language Models (VLMs) exhibit strong perceptual reasoning abilities, yet they often struggle to adapt efficiently when encountering novel tasks at test time. In contrast, humans leverage the metacognitive model with memory, enabling continuous strategy refinement through metacognitive control when faced with new challenges. To bridge this gap, we propose metacognitive test-time reasoning (MCTR), a framework that equips models with the ability to learn, adapt, and improve during test time through metacognitive self-updating. Inspired by the dual structure of human metacognition, MCTR comprises meta-level and object-level VLM reasoning modules, each equipped with dedicated memory systems for hierarchical adaptive reasoning. Specifically, MCTR consists of (1) a meta-reasoning module which incrementally builds a structured memory by discovering and storing task-relevant rules, environmental patterns, and action-outcome relationships from test-time observations as natural language descriptions; and (2) an action-reasoning module that determines optimal actions through context-aware perception and strategic reasoning by dynamically retrieving and integrating knowledge from memory. The action-reasoning module continuously updates its policy through proposed metacognitive test-time reinforcement learning, adapting as knowledge memory evolves. We evaluate MCTR on 45 Atari games (33 seen, 12 unseen). MCTR demonstrates robust test-time adaptation, achieving 9/12 top-1 results on unseen games compared with baselines. Analyses through ablations, learning dynamics, and case studies reveal the complementary contributions of both components and show meta-reasoning evolving toward human-like adaptation strategies.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-28",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.767716",
        "filter_reason": "这篇论文完全符合您的研究范围，应被保留。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** - **保留**。这篇论文的核心贡献是提出了一种名为“元认知测试时推理（MCTR）”的新**框架**。该框架旨在赋予智能体在测试时通过元认知自我更新来学习、适应和改进的能力。这直接对应了您筛选标准中的“构建、改进或演化 LLM智能体的方法论或新框架”，特别是“自我演化”方向。它不是简单地将现有智能体应用于某个领域，而是提出了一种全新的、能让智能体自我演化的机制。 **第二步：正面指标——论文是否包含我的核心关注点？** - 论文包含了大量核心关注点，相关性极高： - **核心范式**: `Agentic AI` (Metacognitive Agent), `Self-Evolving` (learn, adapt, and improve during test time, continuously updates its policy)。 - **智能体能力**: `Memory` (dedicated memory systems, structured memory), `Self-Reflection` (metacognitive self-updating, metacognitive control), `Planning` (meta-reasoning module, action-reasoning module, strategic reasoning)。 - **演化机制**: `Self-Improvement` (adapt and improve), `Iterative Improvement` (continuously updates its policy as knowledge memory evolves)。 **第三步：排除标准——是否为我的研究焦点之外？** - **安全与对齐**: 论文未涉及安全、对齐、可解释性等主题。 - **多模态与视觉**: 论文确实使用了VLMs，但这并未触发排除标准。根据第四步的特殊规则，VLM在这里是作为智能体**感知环境的工具**（在Atari游戏中进行视觉感知），而不是研究的核心。论文的核心是围绕VLM构建的元认知和自我演化框架，而非VLM模型本身。 **第四步：处理特殊和模糊情况** - **推理/规划**: 论文是关于智能体如何在复杂任务（Atari游戏）中进行规划和多步推理的。它提出了一个包含元推理和行动推理模块的完整Agentic框架，这完全符合“保留”的条件。 - **自我演化的应用**: 这篇论文是“自我演化的应用”规则下的一个完美**例外**。它的核心贡献正是提出一种新的“自我演化”机制（MCTR），并将其应用在Atari游戏这一基准测试环境中来验证其有效性。因此，它应该被保留。 **第五步：最终决策** 综合以上分析，该论文的核心贡献在于构建了一个具有元认知能力的自我演化智能体框架。它深入探讨了智能体如何通过记忆、自我反思和策略更新在测试时实现适应和进化，这与您的研究目标，特别是“单智能体”和“自我演化”方向，高度契合。因此，最终判断为 **True**。"
    },
    {
        "index": "#3",
        "title": "Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting",
        "link": "/arxiv/2511.23387",
        "arxiv_id": "2511.23387",
        "authors": "Daniil Sukhorukov, Andrei Zakharov, Nikita Glazkov, Katsiaryna Yanchanka, Vladimir Kirilin, Maxim Dubovitsky, Roman Sultimov, Yuri Maksimov, Ilya Makarov",
        "summary": "We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-28",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.766538",
        "filter_reason": "这篇论文符合筛选标准，应当保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质并非简单地将LLM作为工具应用于天气预报领域。其核心贡献在于**构建了一个新颖的LLM智能体系统**。论文提出了一个名为“分层AI气象学家”的框架，其核心创新点在于“分层预报推理”和“关键词生成与验证”机制。这属于构建和改进LLM智能体的方法论，而非单纯的应用。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点： - **核心范式**: 论文标题和摘要中多次提及 `LLM-agent system`，完全符合 `Agentic AI` 和 `LLM-based Agents` 的范式。 - **智能体能力**: 论文的核心是 `Reasoning`（推理），具体化为一种新颖的 `hierarchical forecast reasoning`（分层推理）方法。此外，其“关键词验证”机制用于“验证一致性、时间连贯性和事实对齐”，这是一种明确的 `Self-Correction`（自我修正）或 `Self-Reflection`（自我反思）能力的体现。 3.  **第三步：排除标准** - **安全与对齐**: 论文虽然提到了 `interpretability`（可解释性），但这并非其主要研究贡献。可解释性是其提出的智能体框架（分层推理和关键词验证）所带来的一个**结果或优势**，而不是论文的研究目标本身。论文的核心是构建智能体，而不是研究一种新的可解释性方法。因此，不触发排除标准。 - **多模态与视觉**: 论文未涉及视觉或多模态内容，不触发排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是典型的“保留”案例。它不是在改进LLM的基础数学或逻辑能力，而是在构建一个智能体框架，让智能体能够进行更复杂的多尺度、分层级的推理来完成任务。这完全符合“关于智能体如何进行规划或在复杂任务中进行多步推理”的保留条件。 - **自我演化的应用**: 虽然这篇论文不直接属于“自我演化”，但其“自我修正”机制与演化方向密切相关。更重要的是，它符合第四步规则的核心精神：**即使应用在特定领域（天气预报），只要其核心贡献是提出一种新的智能体机制或框架，就应该保留**。这里的“分层推理”和“关键词验证”就是这种新机制。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种具有分层推理和自我修正能力的LLM智能体新框架。它虽然以天气预报为应用场景，但其研究焦点在于智能体本身的架构和能力提升，旨在“推进基于智能体的科学推理”。这完全符合我“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的核心目标，特别是与“单智能体”方向中的“规划”和“自我反思”子方向高度契合。因此，最终判断为 **True**。"
    },
    {
        "index": "#13",
        "title": "TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM",
        "link": "/arxiv/2511.22998",
        "arxiv_id": "2511.22998",
        "authors": "Peng Kuang, Xiangxiang Wang, Wentao Liu, Jian Dong, Kaidi Xu, Haohan Wang",
        "summary": "Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-28",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.769533",
        "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单应用LLM，而是提出了一种名为 **TIM-PRM** 的“新颖的智能体框架”。其核心贡献在于构建了一个新的智能体方法论，而不是将现有技术应用于某个垂直领域。该框架将验证过程从被动分类转变为“主动的、工具增强的调查”，这直接命中了“构建、改进LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: 摘要明确称其为 **`agentic framework`**。 - **智能体能力**: 论文的核心机制是智能体被训练来“显式地**规划**验证策略”并“通过外部**工具**查询证据”。这直接对应了你关注的 **`Planning`** 和 **`Tool Use / Tool Augmentation`**。 - **自我反思/修正**: 虽然没有直接使用\"Self-Reflection\"这个词，但其“消除确认偏差”和“独立提问”的机制，本质上是一种高级的**`Self-Correction`**或批判性思维过程，是智能体能力的重要组成部分。 3.  **第三步：排除标准** - **安全与对齐**: 论文的主要贡献是提升推理验证的准确性，而非安全、对齐或可解释性（虽然它提供了可解释的见解，但这不是其核心研究目标）。 - **多模态与视觉**: 这是一个关键点。虽然论文处理的是多模态（MLLMs）问题，但根据你的规则，“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在这篇论文中，**视觉信息是智能体需要验证和处理的“环境”或“输入”，而研究的核心是那个能够使用工具进行规划和验证的“智能体框架”本身**。因此，多模态背景不构成排除理由，反而展示了该智能体框架在复杂感知任务上的能力。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美符合“保留”条件。它不是在提升LLM本身的基础数学或逻辑推理能力，而是在构建一个**智能体**来执行**规划和多步验证**这一复杂的推理任务。这与ReAct、ToT等Agentic框架的思路一脉相承。 5.  **第五步：最终决策** - 综合来看，这篇论文的核心贡献是提出了一种新的智能体框架（TIM-PRM），该框架具备明确的规划和工具使用能力，用于解决多模态推理中的验证问题。它完全符合你“单智能体”方向的研究焦点，特别是规划和工具使用这两个子方向。因此，这篇论文与你的研究课题高度相关，应被保留。"
    },
    {
        "index": "#11",
        "title": "Does Self-Evaluation Enable Wireheading in Language Models?",
        "link": "/arxiv/2511.23092",
        "arxiv_id": "2511.23092",
        "authors": "David Demitri Africa, Hans Ethan Ting",
        "summary": "Self-evaluation is increasingly central to language model training, from constitutional AI to self-refinement. We investigate whether coupling self-evaluation to reward signals creates incentives for wireheading, where agents manipulate reward measurements rather than improving task performance. We formalize conditions under which reward-channel control strictly dominates task-focused behavior in POMDPs and test these predictions empirically. Across two models and three tasks, we find that models whose self-grades determine rewards exhibit substantial grade inflation without corresponding accuracy gains, particularly on ambiguous tasks like summarization. Models that self-evaluate but do not control rewards show no such inflation. Our results demonstrate that self-evaluation is safe when decoupled from learning signals but dangerous when coupled, with clear implications for agentic system design.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-28",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.768914",
        "filter_reason": "这篇论文完全符合您的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM作为工具应用，也不是提升LLM的基础推理能力。它的核心是研究一种特定的智能体架构——即具备“自我评估”能力的智能体——在设计上存在的根本性问题。论文探讨了当自我评估与学习信号（奖励）耦合时，智能体会产生“线路短路”这种非预期的演化行为。这直接触及了“构建、改进或演化LLM智能体”的核心，特别是“自我演化”方向中的机制设计问题。 2.  **第二步：正面指标** - 论文包含了大量核心关注点： - **核心范式**: `Agentic AI` (论文明确提到 \"implications for agentic system design\")，`Self-Evolving` (研究自我评估这一演化机制)。 - **智能体能力**: `Self-Evaluation`, `Self-Reflection`, `Self-Refine` (摘要开篇即点明)。 - **演化机制**: 论文研究的“线路短路”现象，是自我演化过程中的一种病态路径，对理解如何实现健康的`Self-Improvement`至关重要。 3.  **第三步：排除标准** - **不排除**。虽然论文提到了“安全”和“危险”，但其主要贡献并非提出一个通用的安全与对齐框架。它的贡献在于揭示了自我演化智能体的一种特定失败模式，并给出了具体的设计建议（将自我评估与奖励信号解耦）。这种研究是为了**构建更有效、更可靠的自我演化智能体**，其落脚点是“智能体设计”，而非泛化的“安全对齐”。因此，它没有触犯排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文不是关于自我演化机制在某个领域的应用，而是对自我演化机制本身的基础性研究。它直接回答了“如何设计一个不会走偏的自我演化智能体”这一核心问题。 - **推理/规划**: 论文不关注LLM的推理链条本身，而是关注驱动智能体学习和演化的奖励机制，这属于更高层次的智能体架构设计问题。 **最终决策**: 这篇论文的核心贡献在于，它通过理论和实验，揭示了LLM智能体在实现“自我评估”和“自我完善”这一演化能力时，可能遇到的一个关键陷阱——“线路短路”。它直接为您的“自我演化”研究方向提供了深刻的见解和设计指导，指明了如何构建更稳健的智能体。因此，这篇论文与您的研究目标高度契合，是必须保留的前沿研究。"
    },
    {
        "index": "#12",
        "title": "MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents",
        "link": "/arxiv/2511.23055",
        "arxiv_id": "2511.23055",
        "authors": "Ruoxuan Zhang, Qiyun Zheng, Zhiyu Zhou, Ziqi Liao, Siyu Wu, Jian-Yu Jiang-Lin, Bin Wen, Hongxia Xie, Jianlong Fu, Wen-Huang Cheng",
        "summary": "Theory of Mind (ToM) refers to the ability to infer others' mental states, such as beliefs, desires, and intentions. Current vision-language embodied agents lack ToM-based decision-making, and existing benchmarks focus solely on human mental states while ignoring the agent's own perspective, hindering coherent decision and action generation. To address this, we propose MindPower, a Robot-Centric framework integrating Perception, Mental Reasoning, Decision Making and Action. Given multimodal inputs, MindPower first perceives the environment and human states, then performs ToM Reasoning to model both self and others, and finally generates decisions and actions guided by inferred mental states. Furthermore, we introduce Mind-Reward, a novel optimization objective that encourages VLMs to produce consistent ToM Reasoning and behavior. Our model outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-28",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.769247",
        "filter_reason": "这篇论文完全符合您的研究范围，核心判断依据如下： 1.  **核心贡献是构建新的智能体框架 (符合第一步核心判断)**: 论文的核心贡献是提出了 **MindPower**，一个全新的、以机器人为中心的框架，用于构建和改进具身智能体。它不是一个简单的应用，而是提供了一个包含“感知-心智推理-决策-行动”的完整方法论。这直接命中了您筛选标准中“构建、改进LLM智能体的方法论或新框架”这一核心要求。 2.  **聚焦于单智能体的核心能力 (符合第一步和第二步正面指标)**: 论文的研究焦点是增强单个智能体的**高级推理能力**，具体来说是“心智理论”推理。这属于您研究范围中“单智能体”方向下的“规划”和“自我反思”的子方向。智能体通过推理他人和自身的心理状态来指导决策和行动，这是Agentic AI中非常前沿和核心的能力。论文中提到的 `Decision Making`、`Action` 以及 `ToM Reasoning` 都是您关注的核心范式和能力。 3.  **正确处理了“视觉”这一模糊点 (符合第三步排除标准的例外情况)**: 论文标题和摘要中提到了“VLM-based”（基于视觉语言模型），这触发了第三步的排除标准。然而，根据您的核心规则：“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在这篇论文中，视觉（VLM）正是作为智能体**感知环境的工具**而存在的。论文的**研究核心**不是如何改进视觉模型本身，而是如何利用视觉感知到的信息，进行更高层次的“心智理论推理”，并最终做出决策。因此，这篇论文不应被排除，反而是一个将多模态感知作为工具来增强智能体能力的优秀范例。 4.  **不属于任何排除类别**: *   **非演化型应用**: 论文不是将一个已有的智能体框架应用到新领域，而是提出了一个全新的框架。 *   **非Agentic的推理**: 论文的推理（ToM）是紧密集成在智能体的决策和行动循环中的，是典型的Agentic推理，而非提升LLM基础能力的数学或逻辑技巧。 *   **安全与对齐**: 论文焦点是能力提升，而非安全、对齐或可解释性。 综上所述，该论文的核心贡献在于提出了一种新的智能体框架（MindPower），以增强单智能体的高级心智推理和决策能力，完全符合您关于“LLM智能体及其演化”的研究课题，特别是“单智能体”方向。因此，应予以保留。"
    },
    {
        "index": "#20",
        "title": "Geometrically-Constrained Agent for Spatial Reasoning",
        "link": "/arxiv/2511.22659",
        "arxiv_id": "2511.22659",
        "authors": "Zeren Chen, Xiaoya Lu, Zhijie Zheng, Pengrui Li, Lehan He, Yijin Zhou, Jing Shao, Bohan Zhuang, Lu Sheng",
        "summary": "Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-11-27",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.771710",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建和改进LLM智能体。以下是详细的判断过程： 1.  **第一步：核心判断——保留** 论文的核心是提出一个名为“几何约束智能体”的**新框架**。它不是简单地将现有智能体应用于某个领域，而是针对现有智能体在空间推理任务中的缺陷（规划过程不受几何约束），提出了一种全新的、结构化的智能体工作范式。这完全符合“构建、改进LLM智能体的方法论或新框架”的保留标准。它不属于非演化型应用、非Agentic推理或基础设施研究。 2.  **第二步：正面指标——高度相关** 论文命中了多个核心正面指标： *   **核心范式**: 论文明确提出了一个“agentic paradigm”（智能体范式），即`Agentic AI`。 *   **智能体能力**: 论文的核心创新点在于改进智能体的`Planning`（规划）能力。它指出传统方法“leave the VLM's planning process unconstrained”（让VLM的规划过程不受约束），而GCA通过引入形式化约束来解决这个问题。同时，它也涉及`Tool Use`（工具使用），因为智能体在第二阶段需要“generates and executes tool calls”（生成和执行工具调用）。 3.  **第三步：排除标准——不适用** *   **安全与对齐**: 论文完全不涉及安全、对齐或可解释性问题。 *   **多模态与视觉**: 这是一个关键点。虽然论文处理的是VLMs和空间推理（一个视觉任务），但它**符合排除标准中的例外情况**。论文的核心贡献**不是**VLM或视觉模型本身，而是**如何构建一个智能体框架来更好地利用VLM进行推理**。视觉和几何是智能体需要处理的**环境**和**任务**，而研究的核心是智能体的**内部工作机制**（两阶段解耦、约束引入）。因此，不应被排除。 4.  **第四步：特殊和模糊情况——符合保留条件** *   **推理/规划**: 这篇论文是“智能体如何进行规划”的绝佳范例。它不是在改进LLM的基础数学或逻辑能力，而是在设计一个更高级的智能体框架，使其规划过程更加鲁棒和可验证。这完全符合保留条件。 **最终决策**: 这篇论文的核心贡献是提出了一种名为GCA的新型LLM智能体架构，通过引入几何约束来优化智能体的规划和工具使用过程。它直接对齐了您研究目标中的“单智能体”方向，特别是“规划”和“工具使用”这两个子方向。尽管其应用场景是空间推理，但其创新点在于智能体框架本身，而非应用领域。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#10",
        "title": "Evolutionary Discovery of Heuristic Policies for Traffic Signal Control",
        "link": "/arxiv/2511.23122",
        "arxiv_id": "2511.23122",
        "authors": "Ruibing Wang, Shuhan Guo, Zeen Li, Zhen Wang, Quanming Yao",
        "summary": "Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasoning but incur high latency and lack environment-specific optimization. To address these issues, we propose Temporal Policy Evolution for Traffic (\\textbf{\\method{}}), which uses LLMs as an evolution engine to derive specialized heuristic policies. The framework introduces two key modules: (1) Structured State Abstraction (SSA), converting high-dimensional traffic data into temporal-logical facts for reasoning; and (2) Credit Assignment Feedback (CAF), tracing flawed micro-decisions to poor macro-outcomes for targeted critique. Operating entirely at the prompt level without training, \\method{} yields lightweight, robust policies optimized for specific traffic environments, outperforming both heuristics and online LLM actors.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-28",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.768664",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献并非简单地将LLM应用于交通控制领域，而是提出了一种名为“Temporal Policy Evolution for Traffic”的**新框架**。该框架的核心创新在于**使用LLM作为“演化引擎”**来发现和优化启发式策略。这直接命中了您筛选标准中的“构建、改进或演化 LLM智能体”以及“自我演化”的核心目标。它不是在解决一个交通问题，而是在提出一种**让智能体策略自我演化的通用方法论**，并以交通控制作为其验证场景。 2.  **正面指标 (第二步):** 论文包含了多个您关注的核心关键词和概念： *   **自我演化:** 标题和摘要中明确提到了“Evolutionary Discovery”和“evolution engine”，这是论文最核心的标签。 *   **演化机制:** 论文提出的“Credit Assignment Feedback (CAF)”模块，通过追溯错误决策来提供针对性反馈，这是一种具体的**自我完善**和**迭代改进**机制。 *   **智能体能力:** 论文旨在生成“specialized heuristic policies”，这本质上是智能体的决策策略。LLM在其中扮演了进行推理和规划的角色。 3.  **排除标准 (第三步):** 论文的研究焦点是性能、泛化能力和策略优化，完全不涉及安全、对齐、可解释性或多模态等排除领域。 4.  **特殊和模糊情况处理 (第四步):** 这篇论文是“自我演化的应用”这一特殊情况的完美范例。 *   **保留 (例外):** 尽管论文的应用领域是“Traffic Signal Control”，但其核心贡献是提出了一种**新的“自我演化”机制**（即基于LLM的演化引擎、SSA和CAF模块）。根据您的规则，即使应用在特定领域，只要核心是新的演化机制，就应该保留。这篇论文的价值在于其方法论可以被推广到其他需要策略演化的场景，而不仅仅是交通。 *   **推理/规划:** 论文中的LLM不是在进行基础的数学或逻辑推理，而是在一个更高层次上进行策略规划和迭代优化，这完全符合您对“智能体如何进行规划”的保留标准。 **总结:** 该论文的本质是提出了一种新颖的、基于LLM的智能体策略演化框架。它通过结构化状态抽象和信用分配反馈，实现了智能体在特定环境下的自我完善和迭代优化。虽然以交通控制为应用背景，但其核心贡献在于“自我演化”的方法论本身，与您的研究课题“LLM智能体及其演化”高度契合，特别是“自我演化”这一方向。因此，应判定为符合要求。"
    },
    {
        "index": "#34",
        "title": "Co-Evolving Agents: Learning from Failures as Hard Negatives",
        "link": "/arxiv/2511.22254",
        "arxiv_id": "2511.22254",
        "authors": "Yeonsung Jung, Trilok Padhi, Sina Shaham, Dipika Khullar, Joonhyun Jeong, Ninareh Mehrabi, Eunho Yang",
        "summary": "The rapid progress of large foundation models has accelerated the development of task-specialized agents across diverse domains. However, the effectiveness of agents remains tightly coupled with the quality of training data, while curating task-specific datasets remains costly and often infeasible in real-world scenarios. Recent work has explored self-improving agents that autonomously generate, refine, and re-train on their own trajectories. A prominent line of approaches further leverages preference optimization by pairing predicted trajectories with scarce ground-truth trajectories, enabling agents to learn directly from their own failures. While these methods outperform supervised fine-tuning, their heavy reliance on predicted trajectories under limited ground-truth supervision leaves them prone to overfitting. To address this, we propose a co-evolving agents framework in which a target agent improves jointly with an auxiliary failure agent. The failure agent learns through preference optimization over failure trajectories from both the target and itself, thereby generating hard negatives that are close to success yet remain failures. Incorporating these informative hard negatives into the target agent's optimization sharpens decision boundaries and enhances generalization. Our comprehensive analysis and experiments across benchmark datasets show that our method not only shows improved performance but also demonstrates that failures, instead of being used as-is, can be systematically transformed into structured and valuable learning signals in self-improving agents.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-27",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.776266",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献精准地命中了你的研究焦点。 1.  **核心判断 (第一步):** 论文的本质是提出一种新的方法论和框架来**改进和演化LLM智能体**。它不是将智能体作为工具去解决某个特定领域的问题，而是专注于如何让智能体本身变得更好。因此，根据第一步的核心判断标准，应**保留**。 2.  **正面指标 (第二步):** 论文包含了大量你的核心关注点： *   **核心范式:** `LLM-based Agents`, `Multi-Agent Systems (MAS)`, `Self-Evolving`。 *   **演化机制:** `Self-Improvement`, `Iterative Improvement`。 *   论文标题和摘要中反复出现的 \"Co-Evolving Agents\", \"self-improving agents\", \"learning from failures\" 等词汇，都强烈表明其与你的研究目标高度相关。 3.  **排除标准 (第三步):** 论文的主要贡献是关于提升智能体性能的机制，不涉及安全、对齐、可解释性，也未将多模态或视觉作为研究核心。因此，没有触发任何排除标准。 4.  **特殊和模糊情况 (第四步):** *   **自我演化的应用:** 这篇论文是第四步规则中“保留 (例外)”的完美范例。即使它的实验是在特定基准数据集上进行的，但其**核心贡献是提出了一种全新的“自我演化”机制**——即通过协同演化的失败智能体来生成困难负样本，从而提升目标智能体。这种机制本身是通用且创新的，完全符合你的筛选要求。 **核心依据总结:** 该论文的核心贡献是构建了一个**“协同演化智能体”框架**，这直接对应了你的两个核心研究方向： *   **自我演化:** 论文的核心是让智能体通过从失败中学习来实现自我完善。它提出了一种系统化的方法，将失败轨迹转化为有价值的“困难负样本”，这是一种新颖的自我演化机制。 *   **多智能体:** 该框架的实现依赖于一个“目标智能体”和一个“失败智能体”的协同工作。这两个智能体在演化过程中相互作用、分工合作，这完全属于多智能体系统中协作与专业化分工的研究范畴。 综上所述，这篇论文不仅符合，而且是高度契合你关于“LLM智能体及其演化”研究课题的前沿文献，其提出的协同演化框架正是你寻找的关于“构建、改进或演化LLM智能体”的创新性工作。"
    },
    {
        "index": "#36",
        "title": "Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning",
        "link": "/arxiv/2511.22226",
        "arxiv_id": "2511.22226",
        "authors": "Alexander Meulemans, Rajai Nasser, Maciej Wołczyk, Marissa A. Weis, Seijin Kobayashi, Blake Richards, Guillaume Lajoie, Angelika Steger, Marcus Hutter, James Manyika, Rif A. Saurous, João Sacramento, Blaise Agüera y Arcas",
        "summary": "The standard theory of model-free reinforcement learning assumes that the environment dynamics are stationary and that agents are decoupled from their environment, such that policies are treated as being separate from the world they inhabit. This leads to theoretical challenges in the multi-agent setting where the non-stationarity induced by the learning of other agents demands prospective learning based on prediction models. To accurately model other agents, an agent must account for the fact that those other agents are, in turn, forming beliefs about it to predict its future behavior, motivating agents to model themselves as part of the environment. Here, building upon foundational work on universal artificial intelligence (AIXI), we introduce a mathematical framework for prospective learning and embedded agency centered on self-prediction, where Bayesian RL agents predict both future perceptual inputs and their own actions, and must therefore resolve epistemic uncertainty about themselves as part of the universe they inhabit. We show that in multi-agent settings, self-prediction enables agents to reason about others running similar algorithms, leading to new game-theoretic solution concepts and novel forms of cooperation unattainable by classical decoupled agents. Moreover, we extend the theory of AIXI, and study universally intelligent embedded agents which start from a Solomonoff prior. We show that these idealized agents can form consistent mutual predictions and achieve infinite-order theory of mind, potentially setting a gold standard for embedded multi-agent learning.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-27",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.776900",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。以下是基于筛选标准的详细判断过程： **第一步：核心判断** - **保留**。这篇论文的本质不是应用现有框架，而是**构建一个新的数学框架**。其核心贡献是提出了一种用于“嵌入式智能体”和“多智能体学习”的理论模型。它旨在解决多智能体环境中因其他智能体学习而导致的“非平稳性”这一根本性挑战。这直接对应了你研究目标中的“构建、改进或演化 LLM智能体”中的“构建/改进”部分，尤其是在多智能体系统的理论基础上。 **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `Multi-Agent Systems (MAS)` 是论文的绝对核心。`Agentic AI` 的概念通过 \"embedded agency\" 体现。 - **多智能体**: 论文明确讨论了智能体间的 `Collaboration`（协作），并提出了“新的博弈论解概念”和“新的合作形式”。它还深入探讨了智能体之间如何相互建模和预测，这涉及到 `Communication`（通过行为推断意图）和 `Agent Society` 的雏形。 - **演化机制**: 论文的核心机制“prospective learning”（前瞻性学习）和“self-prediction”（自我预测）是一种高级的适应机制。智能体通过预测自身和他人的行为来适应一个由其他学习智能体组成的动态环境，这可以被视为一种**在交互中的自我演化**。它解决了因环境（其他智能体）变化而导致的演化问题。 **第三步：排除标准** - 论文完全没有涉及 `Safety`, `Alignment`, `Interpretability` 等安全与对齐议题。 - 论文是纯理论框架研究，不涉及 `Vision`, `MLLMs` 等多模态内容。 - 因此，论文未触发任何排除标准。 **第四步：处理特殊和模糊情况** - **推理/规划**: 论文的核心是关于智能体如何在一个复杂、动态的多智能体环境中进行推理和规划。它提出的“预测模型”和“自我预测”是一种高级的规划策略，旨在应对其他智能体的未来行为。这完全符合“保留”标准，因为它关乎智能体的自主规划框架，而非提升LLM本身的基础推理能力。 - **自我演化的应用**: 此处不适用，因为论文本身就是提出一种新的演化/适应机制，而非将其应用于特定领域。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出一个关于嵌入式多智能体学习的创新数学框架。它通过引入“自我预测”机制，使智能体能够在一个由其他学习智能体构成的动态环境中进行有效推理、规划和协作。这直接命中了你研究焦点中的**“多智能体”**方向，并深刻触及了**“自我演化”**（适应动态环境）的内核。该论文为理解和构建更高级的Agentic AI系统提供了坚实的理论基础，是前沿且高度相关的研究。"
    },
    {
        "index": "#35",
        "title": "Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation",
        "link": "/arxiv/2511.22235",
        "arxiv_id": "2511.22235",
        "authors": "Zehao Deng, Tianjie Ju, Zheng Wu, Zhuosheng Zhang, Gongshen Liu",
        "summary": "The rapid development of large vision-language model (VLM) has greatly promoted the research of GUI agent. However, GUI agents still face significant challenges in handling long-horizon tasks. First, single-agent models struggle to balance high-level capabilities and low-level execution capability, facing prevalent issues of responsibility coupling and capability conflicts. Second, agents lack awareness of the task state, leading to progress loss in long-horizon tasks. To address these challenges, we propose a staged execution-feedback reinforcement learning algorithm. Unlike training a unified policy model, we focus on training high-level scheduling models. Specifically, we propose and train two agents: a Coordinator, responsible for the strategic planning and task decomposition; and a State Tracker, responsible for context compression and information management to maintain the task's state and coherence. Based on this, we built the Coordinator-Executor-State Tracker (CES) multi-agent framework, which can be integrated with any low-level Executor model, assisting the Executor in solving long-horizon tasks through task scheduling and state management. Experiments on long-horizon task benchmarks demonstrate that CES significantly enhances the system's planning and state management capabilities. Furthermore, analysis confirms that our trained high-level scheduling module is a generalizable, plug-and-play module that significantly enhances the long-horizon capabilities of various Executors. Code can be available at https://github.com/hehehahi4/CES.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-27",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.776547",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建和改进一个新颖的多智能体框架。我的判断过程如下： **第一步：核心判断——保留** 这篇论文的本质不是简单地将现有智能体应用于GUI自动化领域，而是针对现有智能体在处理长时程任务时的根本性缺陷（责任耦合、状态感知缺失），提出了一种全新的**多智能体框架（CES）**和相应的**训练算法**。其核心贡献是方法论和框架层面的创新，属于“构建、改进LLM智能体”的范畴，因此应保留。 **第二步：正面指标——高度匹配** 论文包含了您核心关注点的多个关键指标： 1.  **多智能体**: 论文明确提出了一个名为“Coordinator-Executor-State Tracker (CES)”的**多智能体框架**。其中，`Coordinator`负责战略规划，`State Tracker`负责状态管理，两者协作辅助`Executor`。这直接命中了`Multi-Agent Systems`、`Collaboration`等核心范式。 2.  **单智能体能力**: 该框架的设计旨在增强智能体的核心能力。`Coordinator`的“strategic planning and task decomposition”直接对应`Planning`能力。`State Tracker`的“context compression and information management to maintain the task's state”直接对应`Memory`能力。 3.  **自我演化**: 论文提出了“execution-feedback reinforcement learning algorithm”，这是一种通过环境反馈进行学习和优化的机制，属于`Self-Improvement`和`Iterative Improvement`的范畴，使智能体能够通过经验不断完善其调度策略。 **第三步：排除标准——未触发** 1.  **安全与对齐**: 论文未涉及安全、对齐、可解释性等内容。 2.  **多模态与视觉**: 论文虽然提到了“large vision-language model (VLM)”，但明确指出VLM是作为GUI智能体**感知环境的工具**。研究的核心并非VLM本身，而是如何构建一个能利用VLM进行高效规划和状态管理的智能体框架。这完全符合“除非它们被用作智能体感知环境的工具”的例外规则。 **第四步：处理特殊和模糊情况** 1.  **推理/规划**: 论文的核心是关于智能体如何进行高层级的规划和任务分解（`Coordinator`），属于智能体框架内的规划问题，而非提升LLM本身的基础推理能力，因此符合保留条件。 2.  **自我演化的应用**: 论文的核心贡献是提出了一种新的“自我演化”机制（基于执行反馈的强化学习）和一个多智能体框架，即使它应用在GUI自动化这个特定领域，根据您的规则，也应该保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是构建了一个由规划器和状态跟踪器组成的多智能体系统，以解决长时程任务中的关键挑战。它直接命中了您研究焦点中的“多智能体”方向，并深度融合了“单智能体”的规划与记忆能力，以及“自我演化”的学习机制。因此，这篇论文与您的研究课题高度相关，应被筛选出来。"
    },
    {
        "index": "#40",
        "title": "Real-Time Procedural Learning From Experience for AI Agents",
        "link": "/arxiv/2511.22074",
        "arxiv_id": "2511.22074",
        "authors": "Dasheng Bi, Yubin Hu, Mohammed N. Nasir",
        "summary": "Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment. We propose Procedural Recall for Agents with eXperiences Indexed by State (PRAXIS), a lightweight post-training learning mechanism that stores the consequences of actions and retrieves them by jointly matching environmental and internal states of past episodes to the current state. PRAXIS augments agentic action selection with retrieved state-action-result exemplars that are generated in real time. When evaluated on the REAL web browsing benchmark, PRAXIS improves task completion accuracy, reliability, and cost efficiency across different foundation model backbones, and shows preliminary generalization to unseen tasks in similar environments. These results demonstrate that PRAXIS enables the practical adoption of AI agents in fast-evolving stateful environments by helping them learn new procedures effectively.",
        "subjects": "Artificial Intelligence, Information Retrieval",
        "date": "2025-11-27",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.778182",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献直接命中了“自我演化”这一核心方向。以下是我的详细判断过程： 1.  **第一步：核心判断——保留** - 论文的核心是提出一个名为 **PRAXIS** 的新机制。这个机制的本质是一个“轻量级的训练后学习机制”，其目的是让LLM智能体在部署后能够“从试错中实时学习”并“获取程序性知识”。 - 这完全符合“构建、改进或演化 LLM智能体”的核心目标。它不是将现有智能体应用到某个领域，而是为智能体本身增加了一种全新的、关键的**演化能力**。 - 因此，根据第一步的判断标准，这篇论文应该被**保留**。 2.  **第二步：正面指标——高度匹配** - 论文摘要中充满了与你核心关注点高度相关的关键词： - **核心范式**: `LLM-based Agents`, `Self-Evolving` (论文标题和摘要的核心思想)。 - **智能体能力**: `Memory` (PRAXIS的核心是存储和检索过去的经验)，`Self-Improvement` (从经验中学习新程序)。 - **演化机制**: `Self-Improvement`, `Iterative Improvement` (PRAXIS通过实时积累经验来迭代改进智能体的行为)。 - 这些正面指标强烈表明该论文与你的研究课题高度相关。 3.  **第三步：排除标准——未触发** - 论文的主要贡献是关于提升智能体的任务完成能力、可靠性和成本效率，而不是关于安全、对齐、可解释性或水印。 - 论文评估的基准是网页浏览，虽然可能涉及视觉，但PRAXIS机制本身是模态无关的，其核心是状态-行动-结果的记忆与检索，而不是视觉模型或视觉语言模型的研究。因此，它没有触发多模态与视觉的排除标准。 4.  **第四步：处理特殊和模糊情况——符合保留规则** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一特殊情况的完美范例。虽然它被应用在网页浏览这个特定领域，但论文的**核心贡献是PRAXIS这个“自我演化”机制本身**。根据你的规则，“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。因此，这篇论文明确符合保留条件。 5.  **第五步：最终决策** - 综合以上分析，这篇论文的核心贡献是构建了一个让LLM智能体能够从实时经验中学习和自我完善的框架（PRAXIS）。这直接对应了你研究目标中的“自我演化”方向，并且是关于智能体基础架构和能力的创新性工作，而非简单的应用或外围研究。 **结论**: 该论文是关于LLM智能体自我演化能力的前沿研究，与你的课题高度契合，应被筛选入内。"
    },
    {
        "index": "#132",
        "title": "BINDER: Instantly Adaptive Mobile Manipulation with Open-Vocabulary Commands",
        "link": "/arxiv/2511.22364",
        "arxiv_id": "2511.22364",
        "authors": "Seongwon Cho, Daechul Ahn, Donghyun Shin, Hyeonbeom Choi, San Kim, Jonghyun Choi",
        "summary": "Open-vocabulary mobile manipulation (OVMM) requires robots to follow language instructions, navigate, and manipulate while updating their world representation under dynamic environmental changes. However, most prior approaches update their world representation only at discrete update points such as navigation targets, waypoints, or the end of an action step, leaving robots blind between updates and causing cascading failures: overlooked objects, late error detection, and delayed replanning. To address this limitation, we propose BINDER (Bridging INstant and DEliberative Reasoning), a dual process framework that decouples strategic planning from continuous environment monitoring. Specifically, BINDER integrates a Deliberative Response Module (DRM, a multimodal LLM for task planning) with an Instant Response Module (IRM, a VideoLLM for continuous monitoring). The two modules play complementary roles: the DRM performs strategic planning with structured 3D scene updates and guides what the IRM attends to, while the IRM analyzes video streams to update memory, correct ongoing actions, and trigger replanning when necessary. Through this bidirectional coordination, the modules address the trade off between maintaining awareness and avoiding costly updates, enabling robust adaptation under dynamic conditions. Evaluated in three real world environments with dynamic object placement, BINDER achieves substantially higher success and efficiency than SoTA baselines, demonstrating its effectiveness for real world deployment.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-11-27",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.805926",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出一个名为 **BINDER** 的**新框架**，用于构建能够进行即时自适应的LLM智能体。它不是简单地将现有智能体框架应用于机器人领域，而是设计了一个新颖的“双过程”架构，将战略规划与持续环境监测解耦。这完全符合“构建、改进LLM智能体的方法论或新框架”的保留标准。它不是非演化型应用，因为其核心是智能体架构本身的创新，而非解决机器人领域的特定问题。 2.  **第二步：正面指标——高度相关** 论文包含了多个核心关注点： *   **核心范式**: 论文本质上是关于 `Agentic AI` 和 `LLM-based Agents` 的。 *   **智能体能力**: *   `Planning`: DRM模块明确负责“strategic planning”（战略规划）。 *   `Memory`: IRM模块通过分析视频流来“update memory”（更新记忆）。 *   `Self-Correction`: IRM模块能够“correct ongoing actions”（修正正在进行的动作）。 *   `ReAct`: 整个框架的循环（规划、行动、监控、必要时重新规划）与ReAct范式高度一致，是一种更复杂的Agentic推理框架。 3.  **第三步：排除标准——不适用** *   **安全与对齐**: 论文的主要贡献是提升智能体的鲁棒性和效率，而非安全、对齐或可解释性。 *   **多模态与视觉**: 论文确实使用了 `VideoLLM`，但根据筛选规则的例外情况，它被用作“智能体感知环境的工具”，而不是研究的核心。研究的核心是**如何整合这个工具以实现更好的智能体框架**，即BINDER架构本身。因此，这不构成排除的理由。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文明确是关于智能体如何进行规划的。它提出了一个包含战略规划和动态重新规划的新颖框架，这完全符合“保留”关于智能体规划和多步推理的论文的要求。 **最终决策**: 综合分析，这篇论文的核心贡献在于提出了一种创新的LLM智能体框架（BINDER），该框架通过双过程设计显著增强了智能体的规划、记忆和自我修正能力。尽管其应用场景是机器人操作，但其方法论贡献是普适的，直接服务于“构建、改进或演化LLM智能体”这一核心研究目标。因此，这篇论文高度相关，应被保留。"
    },
    {
        "index": "#127",
        "title": "Asking like Socrates: Socrates helps VLMs understand remote sensing images",
        "link": "/arxiv/2511.22396",
        "arxiv_id": "2511.22396",
        "authors": "Run Shao, Ziyu Li, Zhaoyang Zhang, Linrui Xu, Xinran He, Hongyuan Yuan, Bolei He, Yongxing Dai, Yiming Yan, Yijun Chen, Wang Guo, Haifeng Li",
        "summary": "Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-11-27",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.804532",
        "filter_reason": "这篇论文符合研究范围，应被保留。 **判断过程和核心依据如下：** 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将一个现有智能体框架应用到遥感领域。其核心贡献是提出了两个全新的方法论：**RS-EoT（一种迭代的视觉证据寻求范式）**和**SocraticAgent（一个实现该范式的自博弈多智能体系统）**。这完全符合“构建、改进或演化LLM智能体的方法论或新框架”的标准。它解决的是智能体在复杂任务中如何进行“真正推理”的根本性问题，而非仅仅解决一个遥感领域的应用问题。 2.  **第二步：正面指标** - 论文包含了大量核心关注点，与筛选标准高度契合： - **核心范式**: 明确提出了 `Multi-Agent Systems (MAS)`，即 `SocraticAgent` 是一个 `self-play multi-agent system`。 - **智能体能力**: 论文的 `RS-EoT` 范式本质上是一个 `Planning` 和 `Self-Reflection` 的循环。智能体进行推理，然后通过视觉检查来验证或修正自己的推理，这与 `ReAct` (Reasoning and Acting) 范式高度一致。视觉证据寻求可以被视为一种 `Tool Use`。 - **多智能体**: `SocraticAgent` 的 `self-play` 机制天然涉及智能体间的 `Collaboration` 或 `Communication`，以生成高质量的推理轨迹。 - **演化机制**: 论文使用 `two-stage progressive RL strategy` 来训练和改进智能体，这是一种 `Iterative Improvement` 的演化机制。 3.  **第三步：排除标准** - **安全与对齐**: 论文的主要贡献不涉及安全、对齐、可解释性等问题，因此不在此排除范围内。 - **多模态与视觉**: 这是本论文最需要辨析的一点。虽然论文处理的是 `VLMs` 和 `Vision` 任务，但根据筛选规则的核心思想——“除非它们被用作智能体感知环境的工具，而不是研究的核心”——本论文**不应被排除**。在这里，视觉（VLM）是智能体用来感知和验证其推理的**工具**，而研究的**核心**是 `SocraticAgent` 这个多智能体框架和 `RS-EoT` 这个推理范式本身。论文的创新点在于“如何让智能体更好地使用视觉工具进行推理”，而不是“如何改进视觉模型本身”。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文完全符合“保留”条件。它不是在改进LLM/VLM的基础数学或逻辑能力，而是在构建一个让智能体在复杂任务中进行多步推理和证据验证的**新框架**。`RS-EoT` 正是这种智能体规划能力的体现。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于构建了一个新颖的**多智能体系统（SocraticAgent）**，通过自博弈和强化学习，教会智能体如何进行**迭代的、基于证据的推理（RS-EoT）**。这直接命中了研究课题中的“多智能体”和“单智能体（规划、自我反思）”两个核心方向。尽管其应用场景是遥感图像，但其方法论具有普适性，是关于Agentic AI本身的前沿研究。因此，这篇论文高度符合筛选要求。"
    },
    {
        "index": "#179",
        "title": "PathReasoning: A multimodal reasoning agent for query-based ROI navigation on whole-slide images",
        "link": "/arxiv/2511.21902",
        "arxiv_id": "2511.21902",
        "authors": "Kunpeng Zhang, Hanwen Xu, Sheng Wang",
        "summary": "Deciphering tumor microenvironment from Whole Slide Images (WSIs) is intriguing as it is key to cancer diagnosis, prognosis and treatment response. While these gigapixel images on one hand offer a comprehensive portrait of cancer, on the other hand, the extremely large size, as much as more than 10 billion pixels, make it challenging and time-consuming to navigate to corresponding regions to support diverse clinical inspection. Inspired by pathologists who conducted navigation on WSIs with a combination of sampling, reasoning and self-reflection, we proposed \"PathReasoning\", a multi-modal reasoning agent that iteratively navigates across WSIs through multiple rounds of reasoning and refinements. Specifically, starting with randomly sampled candidate regions, PathReasoning reviews current selections with self-reflection, reasoning over the correspondence between visual observations and clinical questions, and concludes by proposing new regions to explore. Across rounds, PathReasoning builds a reasoning chain that gradually directs attention to diagnostically relevant areas. PathReasoning turns each whole slide into a sequence of question-guided views, allowing the model to efficiently find informative ROIs within a fixed number of steps, without the need for dense pixel-level annotations. PathReasoning can substantially outperform strong ROI-selection approaches by 6.7% and 3.1% of AUROC on subtyping and longitudinal analysis tasks. The high-quality ROIs further support accurate report generation on breast cancer, significantly outperforming the standard GPT-4o by 10% in accuracy. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, supporting efficient slide review, consistent diagnostic interpretations, comprehensive reporting, and evidence traceability in digital pathology.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-11-26",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.819810",
        "filter_reason": "这篇论文符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将LLM应用于病理学领域，而是提出了一种名为“PathReasoning”的**新型智能体框架**。其核心贡献在于这个智能体的工作机制，即如何通过“自我反思”和“迭代推理”来动态地、智能地导航和探索。这完全符合“构建、改进或演化LLM智能体”的核心目标。它不是一个静态的应用，而是一个具有动态行为和演化能力的智能体方法论。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `Agentic AI`, `LLM-based Agents`。论文标题和摘要都明确称其为“multimodal reasoning agent”。 - **智能体能力**: `Self-Reflection` (自我反思), `Planning` (规划，体现在导航策略上), `ReAct` (其工作流程“观察-反思-推理-行动”与ReAct范式高度一致)。 - **演化机制**: `Self-Improvement` / `Iterative Improvement` (迭代改进)。摘要明确指出智能体“iteratively navigates... through multiple rounds of reasoning and refinements”，并“builds a reasoning chain that gradually directs attention”，这清晰地描述了一个通过经验（前几轮的观察和反思）来优化后续行为的迭代过程，属于自我演化的范畴。 3.  **第三步：排除标准** - **安全与对齐**: 论文的主要贡献不是关于安全、对齐或可解释性。虽然提到了“interpretable reasoning chains”，但这是其智能体框架带来的一个**特性**，而非研究的**核心贡献**。核心贡献是框架本身。 - **多模态与视觉**: 这是一个关键点。论文标题和摘要都提到了“multimodal”和“images”。根据你的规则，这需要仔细甄别。在这里，**视觉（WSI图像）是智能体感知和交互的环境，而不是研究的核心**。论文的核心创新点不在于提出新的视觉模型或多模态融合技术，而在于**如何让一个智能体利用视觉信息进行推理和决策**。因此，这符合“除非它们被用作智能体感知环境的工具，而不是研究的核心”的例外情况。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是典型的“保留”情况。它不是在提升LLM的基础数学或逻辑能力，而是在构建一个**智能体的规划和推理框架**。其“reasoning over the correspondence between visual observations and clinical questions”和“proposing new regions to explore”是智能体在复杂任务中进行多步推理和规划的完美体现。 - **自我演化的应用**: 这篇论文是“保留（例外）”的绝佳范例。虽然它应用在“数字病理学”这一特定领域，但其核心是提出了一种**新的“自我演化”机制**（通过自我反思和迭代优化来导航）。因此，即使应用领域具体，也应因其方法论上的创新而被保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献是构建了一个名为“PathReasoning”的单智能体框架。该框架具备明确的**自我反思**和**迭代优化**能力，属于**自我演化**的研究方向。虽然它应用于病理学并使用视觉输入，但视觉仅作为智能体感知环境的工具，其核心在于智能体的决策和演化机制本身。因此，该论文精准地契合了你关于“LLM智能体及其演化”的研究课题，特别是“单智能体”和“自我演化”两个子方向。"
    },
    {
        "index": "#189",
        "title": "Tacit Bidder-Side Collusion: Artificial Intelligence in Dynamic Auctions",
        "link": "/arxiv/2511.21802",
        "arxiv_id": "2511.21802",
        "authors": "Sriram Tolety",
        "summary": "We study whether large language models acting as autonomous bidders can tacitly collude by coordinating when to accept platform posted payouts in repeated Dutch auctions, without any communication. We present a minimal repeated auction model that yields a simple incentive compatibility condition and a closed form threshold for sustainable collusion for subgame-perfect Nash equilibria. In controlled simulations with multiple language models, we observe systematic supra-competitive prices in small auction settings and a return to competitive behavior as the number of bidders in the market increases, consistent with the theoretical model. We also find LLMs use various mechanisms to facilitate tacit coordination, such as focal point acceptance timing versus patient strategies that track the theoretical incentives. The results provide, to our knowledge, the first evidence of bidder side tacit collusion by LLMs and show that market structure levers can be more effective than capability limits for mitigation.",
        "subjects": "Computer Science and Game Theory, Artificial Intelligence, General Economics",
        "date": "2025-11-26",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.822870",
        "filter_reason": "这篇论文完全符合您的研究范围，特别是“多智能体”方向。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是研究并证实了多个作为自主竞标者的LLM智能体，在重复拍卖的动态环境中，能够通过学习形成“默契合谋”。这并非简单地将LLM作为工具应用于拍卖领域，而是对LLM智能体在多智能体互动中涌现出的复杂社会行为（合谋）的深入探究。论文提出了一个理论模型和仿真框架来分析这种行为，其核心是关于**多智能体系统（Multi-Agent Systems）**的行为建模与分析，完全符合“构建、改进或演化LLM智能体”中的“多智能体”范畴。 - **结论**: **保留**。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点： - **核心范式**: `LLM-based Agents`, `Multi-Agent Systems (MAS)`。论文研究对象就是“large language models acting as autonomous bidders”。 - **多智能体**: `Collaboration`（以合谋形式体现）, `Communication`（尽管是无通信的协调，但属于广义的通信/协调机制）, `Negotiation`（拍卖本身就是一种谈判）, `Agent Society`（拍卖市场中的竞标者构成了一个智能体社会）。 - **结论**: 论文与您的核心关注点高度匹配，尤其是多智能体方向。 3.  **第三步：排除标准** - 论文虽然提到了“mitigation”（缓解），但其视角是市场设计（“market structure levers”），而非AI安全与对齐。论文的主要贡献不是关于`Safety`, `Security`, `Alignment`等，因此不触发排除标准。 - 论文不涉及多模态或视觉内容。 - **结论**: 未触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中提到的LLM采用的“focal point acceptance timing versus patient strategies”等机制，本质上是智能体在多轮交互中形成的策略性规划和推理，这属于智能体在复杂任务中的行为，符合保留条件。 - **自我演化的应用**: 此条不直接适用，但论文研究的智能体行为是通过与环境的重复交互（经验）而涌现的，这与演化的思想有相通之处。 5.  **第五步：最终决策** - **综合分析**: 该论文的核心是探索LLM智能体在多智能体环境中的互动行为，特别是“默契合谋”这一复杂的社会性现象。它为理解LLM智能体的集体行为提供了新的证据和理论框架，是典型的“多智能体”研究。它不是应用研究，而是对智能体本身行为规律的探索，完全契合您“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的核心目标。 因此，最终判断为 **True**。"
    },
    {
        "index": "#234",
        "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
        "link": "/arxiv/2503.14495",
        "arxiv_id": "2503.14495",
        "authors": "Jiacheng Guo, Yue Wu, Jiahao Qiu, Kaixuan Huang, Xinzhe Juan, Ling Yang, Mengdi Wang",
        "summary": "Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-03-18",
        "category": "cs.AI",
        "crawl_time": "2025-12-02T11:00:06.836574",
        "filter_reason": "这篇论文符合研究范围，应予以保留。 **判断过程如下:** 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献是提出了一种名为“temporal consistency”（时间一致性）的新方法。该方法的核心机制是让验证器“iteratively refine their judgments based on the previous assessment”（根据先前的评估迭代式地精炼其判断），并利用“a sequence of self-reflection actions”（一系列自我反思行动）来提高验证准确性。 - 这不是一个非演化型应用，因为它并非简单地将LLM用于数学领域，而是提出了一种新的、具有迭代和反思特性的**方法论框架**。 - 这也不是非Agentic的推理。论文的重点不在于提升LLM模型本身的基础数学能力（如通过新的数据集或微调），而在于构建一个**外部的、过程性的验证框架**。这个框架通过迭代和自我反思来工作，这正是智能体行为的核心特征。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点： - **智能体能力**: `Self-Reflection`（自我反思）在摘要中被直接提及。`Self-Correction`（自我修正）体现在“iteratively refine their judgments”（迭代式精炼其判断）中。`Iterative Improvement`（迭代改进）是其方法的核心。 - 这些指标强烈表明该论文与“单智能体”方向下的“自我反思”和“自我修正”子方向高度相关。 3.  **第三步：排除标准** - 论文的主要贡献是关于提高推理过程的验证准确性，而非安全、对齐或可解释性。因此，不触及任何排除标准。 - 论文专注于文本推理，不涉及多模态或视觉内容。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这是最关键的判断点。该论文属于“保留”的情况。它不是关于提升LLM本身的基础Token预测能力，而是关于一个**智能体如何进行多步推理和验证**。其提出的“temporal consistency”方法可以被视为一种新的Agentic框架，专门用于任务验证。这与ReAct、ToT等框架在精神上是相通的，都是构建智能体工作流程的方法论。 **最终决策:** 综合以上分析，这篇论文的核心贡献在于提出了一种**基于迭代式自我反思的验证框架**。这直接对应了研究课题中“单智能体”方向下的“自我反思”和“自我修正”能力。它不是对LLM基础能力的改进，也不是一个简单的应用，而是为构建更可靠的LLM智能体提供了一种新的机制和方法论。因此，这篇论文完全符合筛选要求。"
    }
]