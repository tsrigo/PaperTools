[
    {
        "index": "#4",
        "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective",
        "link": "/arxiv/2509.21134",
        "arxiv_id": "2509.21134",
        "authors": "Yiwen Zhang, Ziang Chen, Fanqi Kong, Yizhe Huang, Xue Feng",
        "summary": "Large Language Models (LLMs) have been used to make decisions in complex scenarios, where they need models to think deeply, reason logically, and decide wisely. Many existing studies focus solely on multi-round conversations in social tasks or simulated environments, neglecting the various types of decisions and their interdependence. Current reinforcement learning methods struggle to consider the strategies of others during training. To address these issues, we first define a strategic decision-making problem that includes two types of decisions and their temporal dependencies. Furthermore, we propose **T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to optimize the perception of other individual strategies and the game situation trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm, ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating rollouts based on reasoning the strategies of other individuals, 2) estimating advantages at both the graph-level and sample-level, and 3) balancing global and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in terms of model output compliance and cooperative outcomes. Additionally, when compared to models with parameter sizes 100 times larger, it shows an 18% improvement. This demonstrates the effectiveness of the ToMPO algorithm in enhancing the model's strategic decision-making capabilities.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-09-25",
        "category": "cs.MA",
        "crawl_time": "2025-09-26T21:01:54.501280",
        "filter_reason": "这篇论文完全符合我的研究目标，因为它专注于提升大语言模型本身的通用推理能力。从核心判断来看，论文的本质是提出ToMPO算法来增强LLM的战略决策能力，这是一种新的训练范式，属于改进LLM基础能力的研究。论文明确关注LLM在复杂场景中的推理和决策能力，需要\"深入思考、逻辑推理和明智决策\"，这正是通用推理能力的核心要素。 从正面指标看，论文包含了多个相关主题：核心概念是LLMs；能力方向涉及reasoning和strategic decision-making；训练方法采用了reinforcement learning（ToMPO算法）；新兴范式方面则从multi-agent perspective研究问题。 论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文从多智能体角度研究，但它是提出一种通用的框架来增强LLM的战略决策能力，而不是将智能体应用在特定领域。 论文的核心贡献是ToMPO算法，通过基于推理其他个体策略生成rollouts、在图级和样本级估计优势、平衡全局和部分奖励来增强LLM的战略决策能力，这直接提升了LLM的通用推理和决策能力，完全符合我的研究目标。"
    },
    {
        "index": "#13",
        "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning",
        "link": "/arxiv/2509.21193",
        "arxiv_id": "2509.21193",
        "authors": "Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin",
        "summary": "Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: https://github.com/tangxiangru/Eigen-1.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.160443",
        "filter_reason": "这篇论文的核心贡献是提出了一种统一框架，通过结合隐式检索和结构化协作来增强大型语言模型的推理能力。从筛选标准来看，该论文完全符合我的研究目标。首先，论文本质上是关于改进LLM的基础推理能力，提出了基于Monitor的检索模块、分层解决方案改进(HSR)和质量感知迭代推理(QAIR)等新方法，这些都属于提升LLM通用推理能力的范畴。其次，论文包含多个正面指标，如明确研究大型语言模型(LLMs)、关注推理能力(reasoning)、采用多智能体系统(multi-agent systems)和工具使用(tool use)等新兴范式。虽然论文标题提到\"Scientific Reasoning\"，但其框架是通用的，并非局限于特定应用领域，而是以科学推理作为测试场景来验证其通用推理框架的有效性。论文解决的是LLM推理中的普遍问题（如检索导致的推理碎片化和多智能体流水线中的解决方案稀释问题），提出的方法具有通用性，可以应用于各种推理任务。因此，该论文符合研究范围，应被保留。"
    },
    {
        "index": "#6",
        "title": "Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond",
        "link": "/arxiv/2509.21284",
        "arxiv_id": "2509.21284",
        "authors": "Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng",
        "summary": "Existing research indicates that the output of Chain-of-Thought (CoT) is significantly affected by input perturbations. Although many methods aim to mitigate such impact by optimizing prompts, a theoretical explanation of how these perturbations influence CoT outputs remains an open area of research. This gap limits our in-depth understanding of how input perturbations propagate during the reasoning process and hinders further improvements in prompt optimization methods. Therefore, in this paper, we theoretically analyze the effect of input perturbations on the fluctuation of CoT outputs. We first derive an upper bound for input perturbations under the condition that the output fluctuation is within an acceptable range, based on which we prove that: (i) This upper bound is positively correlated with the number of reasoning steps in the CoT; (ii) Even an infinitely long reasoning process cannot eliminate the impact of input perturbations. We then apply these conclusions to the Linear Self-Attention (LSA) model, which can be viewed as a simplified version of the Transformer. For the LSA model, we prove that the upper bound for input perturbation is negatively correlated with the norms of the input embedding and hidden state vectors. To validate this theoretical analysis, we conduct experiments on three mainstream datasets and four mainstream models. The experimental results align with our theoretical analysis, empirically demonstrating the correctness of our findings.",
        "subjects": "Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.158803",
        "filter_reason": "这篇论文的核心贡献是对思维链(CoT)的鲁棒性进行了理论分析，研究了输入扰动如何影响CoT的推理过程和输出。论文推导了在输出波动可接受范围内输入扰动的上界，并证明了该上界与CoT中的推理步骤数量正相关，以及即使无限长的推理过程也无法消除输入扰动的影响。这些发现对于理解和改进LLM的推理能力具有重要意义。思维链(CoT)是提高大语言模型推理能力的关键方法，论文从理论角度分析其鲁棒性，属于改进LLM基础能力和推理能力的研究范畴，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。论文不涉及多模态、特定应用领域或模型可靠性的应用层面研究，而是关注通用推理能力的理论分析，因此应该被保留。"
    },
    {
        "index": "#10",
        "title": "Query-Centric Graph Retrieval Augmented Generation",
        "link": "/arxiv/2509.21237",
        "arxiv_id": "2509.21237",
        "authors": "Yaxiong Wu, Jianyuan Bo, Yongyue Zhang, Sheng Liang, Yong Liu",
        "summary": "Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs) with external knowledge for long-context understanding and multi-hop reasoning, but existing methods face a granularity dilemma: fine-grained entity-level graphs incur high token costs and lose context, while coarse document-level graphs fail to capture nuanced relations. We introduce QCG-RAG, a query-centric graph RAG framework that enables query-granular indexing and multi-hop chunk retrieval. Our query-centric approach leverages Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with controllable granularity, improving graph quality and interpretability. A tailored multi-hop retrieval mechanism then selects relevant chunks via the generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG consistently outperforms prior chunk-based and graph-based RAG methods in question answering accuracy, establishing a new paradigm for multi-hop reasoning.",
        "subjects": "Computation and Language, Information Retrieval",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.159650",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出QCG-RAG，一种以查询为中心的图检索增强生成框架，旨在改进LLM的多跳推理能力。论文明确指出其目标是解决现有图RAG方法中的粒度困境，通过查询粒度索引和多跳块检索机制来增强LLM的推理能力。这属于改进LLM基础能力和增强其多步推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs)\" - 能力方向：专注于\"multi-hop reasoning\"（多跳推理），这是通用推理能力的重要组成部分 - 新兴范式：RAG本身可视为一种工具使用形式，论文提出的新框架增强了LLM利用外部知识的能力 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不是针对医疗、化学、生物等特定应用领域 - 不主要关注水印、安全性等模型可靠性问题 第四步：特殊和模糊情况处理 论文提出的RAG框架可以视为一种通用的工具使用方法，目的是增强LLM的通用问题解决能力（特别是多跳推理），而非应用于特定领域。虽然论文提到了\"improving graph quality and interpretability\"，但这是作为提高推理质量的手段，而非主要焦点。 综上所述，这篇论文的核心贡献是提出了一种新的通用框架来增强LLM的多跳推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#11",
        "title": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents",
        "link": "/arxiv/2509.21212",
        "arxiv_id": "2509.21212",
        "authors": "Yaxiong Wu, Yongyue Zhang, Sheng Liang, Yong Liu",
        "summary": "Long-term conversational agents require effective memory management to handle dialogue histories that exceed the context window of large language models (LLMs). Existing methods based on fact extraction or summarization reduce redundancy but struggle to organize and retrieve relevant information across different granularities of dialogue and generated memory. We introduce SGMem (Sentence Graph Memory), which represents dialogue as sentence-level graphs within chunked units, capturing associations across turn-, round-, and session-level contexts. By combining retrieved raw dialogue with generated memory such as summaries, facts and insights, SGMem supplies LLMs with coherent and relevant context for response generation. Experiments on LongMemEval and LoCoMo show that SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.",
        "subjects": "Computation and Language, Information Retrieval",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.159851",
        "filter_reason": "这篇论文的核心贡献是提出SGMem（句子图记忆）方法，用于增强大语言模型在长期对话中的记忆管理能力。从筛选标准来看，首先，论文的本质是改进LLM的基础能力，特别是在处理超出上下文窗口的长期对话时的信息组织和检索能力，这属于增强LLM通用推理能力的范畴。有效的记忆管理是进行连贯推理和多步对话的基础，因此这项工作直接提升了LLM的通用能力。其次，论文包含了正面指标中的\"Large language models, LLMs\"和\"llm-based agents\"概念。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。最后，论文提出的SGMem是一种通用的记忆管理框架，用于增强对话代理的通用能力，而非应用于特定领域。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#2",
        "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards",
        "link": "/arxiv/2509.21319",
        "arxiv_id": "2509.21319",
        "authors": "Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev",
        "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.157988",
        "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 首先，从核心判断来看，该论文的本质是提出一种新的强化学习范式RLBFF（Reinforcement Learning with Binary Flexible Feedback），用于改进LLM的后训练阶段。这属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，而非将LLM作为工具应用到特定领域。论文通过结合人类反馈(RLHF)和可验证奖励(RLVR)的优势，提出了一种新的训练方法来增强LLM的响应质量和对齐能力。 其次，论文包含多个正面指标： 1. 核心概念：明确关注LLMs，并使用Qwen3-32B作为示例模型 2. 训练方法：核心就是提出一种新的强化学习方法(RLBFF)，这是RLHF和RLVR的结合 3. 能力方向：虽然未直接强调推理，但提高LLM的响应质量隐含了推理能力的提升，且论文在MT-Bench等包含推理任务的基准上进行了评估 第三，论文不涉及任何排除标准中的领域： 1. 未涉及多模态与视觉内容 2. 未针对特定应用领域（如医疗、化学等） 3. 虽然涉及模型对齐，但核心是提出新方法提高整体响应质量，而非专注于水印、安全等应用层面 最后，在特殊和模糊情况处理上，论文明确解决了RLHF的可解释性问题，属于\"提出新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量\"的情况，符合保留条件。 综上所述，该论文的核心贡献是提出了一种新的强化学习训练方法来提升LLM的通用能力和对齐质量，这与研究目标\"提高大语言模型的通用推理能力\"高度一致，因此应该被保留。"
    },
    {
        "index": "#34",
        "title": "WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs",
        "link": "/arxiv/2509.20863",
        "arxiv_id": "2509.20863",
        "authors": "Guowei Xu, Wenxin Xu, Jiawang Zhao, Kaisheng Ma",
        "summary": "Diffusion models have recently shown strong potential in language modeling, offering faster generation compared to traditional autoregressive approaches. However, applying supervised fine-tuning (SFT) to diffusion models remains challenging, as they lack precise probability estimates at each denoising step. While the diffusion mechanism enables the model to reason over entire sequences, it also makes the generation process less predictable and often inconsistent. This highlights the importance of controlling key tokens that guide the direction of generation. To address this issue, we propose WeFT, a weighted SFT method for diffusion language models, where tokens are assigned different weights based on their entropy. Derived from diffusion theory, WeFT delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and MATH-500). The code and models will be made publicly available.",
        "subjects": "Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.169830",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出WeFT（加权熵驱动微调方法），这是一种针对扩散语言模型(dLLMs)的新训练范式。论文的核心贡献在于解决扩散语言模型在监督微调过程中的挑战，通过基于熵的标记加权来增强模型的推理能力。这明确属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴，因此应保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究扩散语言模型(dLLMs)，这是大语言模型的一种变体 - 能力方向：论文在四个推理基准(Sudoku、Countdown、GSM8K和MATH-500)上评估方法，这些基准涉及数学推理和逻辑推理能力 - 论文虽然不涉及强化学习或智能体等新兴范式，但已包含足够的核心正面指标 第三步：排除标准 论文不符合任何排除标准： - 虽然提到扩散模型，但这是应用于语言建模而非视觉或多模态领域 - 没有专注于任何特定应用领域（如医疗、化学等） - 没有主要关注水印、安全等模型可靠性问题 第四步：特殊和模糊情况 论文不涉及需要特殊判断的智能体/工具使用或幻觉/可解释性/安全等模糊情况。 综合分析，这篇论文通过提出新的微调方法来增强扩散语言模型的通用推理能力，并在多个推理基准上验证了其有效性，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#37",
        "title": "Distilling Many-Shot In-Context Learning into a Cheat Sheet",
        "link": "/arxiv/2509.20820",
        "arxiv_id": "2509.20820",
        "authors": "Ukyo Honda, Soichiro Murakami, Peinan Zhang",
        "summary": "Recent advances in large language models (LLMs) enable effective in-context learning (ICL) with many-shot examples, but at the cost of high computational demand due to longer input tokens. To address this, we propose cheat-sheet ICL, which distills the information from many-shot ICL into a concise textual summary (cheat sheet) used as the context at inference time. Experiments on challenging reasoning tasks show that cheat-sheet ICL achieves comparable or better performance than many-shot ICL with far fewer tokens, and matches retrieval-based ICL without requiring test-time retrieval. These findings demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs in downstream tasks.",
        "subjects": "Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.170381",
        "filter_reason": "这篇论文的核心贡献是提出了一种称为\"cheat-sheet ICL\"的新方法，旨在提高大语言模型在上下文学习中的推理效率和性能。论文将多样本上下文学习的信息提炼成简洁的文本摘要，在保持或提高推理性能的同时大幅减少了计算需求。这直接关系到提升LLM的通用推理能力，特别是在处理需要多步推理的任务时。论文不是将LLM作为工具应用到特定领域，而是关注如何改进LLM本身的推理机制，因此完全符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。此外，论文明确涉及\"Large language models\"和\"reasoning\"等正面指标，且不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文提出的是一种通用方法论，可以应用于各种推理任务，而非针对特定领域的应用。"
    },
    {
        "index": "#42",
        "title": "SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs",
        "link": "/arxiv/2509.20758",
        "arxiv_id": "2509.20758",
        "authors": "Jiacheng Lin, Zhongruo Wang, Kun Qian, Tian Wang, Arvind Srinivasan, Hansi Zeng, Ruochen Jiao, Xie Zhou, Jiri Gesi, Dakuo Wang, Yufan Guo, Kai Zhong, Weiqi Zhang, Sujay Sanghavi, Changyou Chen, Hyokun Yun, Lihong Li",
        "summary": "Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach to adapt Large Language Models (LLMs) to specialized tasks but is often believed to degrade their general capabilities. In this work, we revisit this trade-off and present both empirical and theoretical insights. First, we show that SFT does not always hurt: using a smaller learning rate can substantially mitigate general performance degradation while preserving comparable target-domain performance. We then provide a theoretical analysis that explains these phenomena and further motivates a new method, Token-Adaptive Loss Reweighting (TALR). Building on this, and recognizing that smaller learning rates alone do not fully eliminate general-performance degradation in all cases, we evaluate a range of strategies for reducing general capability loss, including L2 regularization, LoRA, model averaging, FLOW, and our proposed TALR. Experimental results demonstrate that while no method completely eliminates the trade-off, TALR consistently outperforms these baselines in balancing domain-specific gains and general capabilities. Finally, we distill our findings into practical guidelines for adapting LLMs to new domains: (i) using a small learning rate to achieve a favorable trade-off, and (ii) when a stronger balance is further desired, adopt TALR as an effective strategy.",
        "subjects": "Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.171412",
        "filter_reason": "这篇论文的核心贡献是研究如何在大语言模型进行特定领域微调时保持其通用能力，提出了一种新的训练方法Token-Adaptive Loss Reweighting (TALR)。从本质上看，论文关注的是改进LLM的基础能力，提出新的训练范式来增强模型的通用推理和问题解决能力，而不是将LLM作为工具应用到特定领域。论文通过理论和实验分析，探索了如何减轻监督微调对模型通用能力的损害，这直接关系到提升LLM的通用推理能力。虽然论文讨论了领域特定的微调，但其焦点不是特定应用领域，而是如何在领域微调时保持通用能力。论文符合\"改进LLM的基础能力、提出新的训练范式\"的标准，不涉及多模态、特定应用领域或模型可靠性等排除标准。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#43",
        "title": "Confidence-guided Refinement Reasoning for Zero-shot Question Answering",
        "link": "/arxiv/2509.20750",
        "arxiv_id": "2509.20750",
        "authors": "Youwon Jang, Woo Suk Choi, Minjoon Jung, Minsu Lee, Byoung-Tak Zhang",
        "summary": "We propose Confidence-guided Refinement Reasoning (C2R), a novel training-free framework applicable to question-answering (QA) tasks across text, image, and video domains. C2R strategically constructs and refines sub-questions and their answers (sub-QAs), deriving a better confidence score for the target answer. C2R first curates a subset of sub-QAs to explore diverse reasoning paths, then compares the confidence scores of the resulting answer candidates to select the most reliable final answer. Since C2R relies solely on confidence scores derived from the model itself, it can be seamlessly integrated with various existing QA models, demonstrating consistent performance improvements across diverse models and benchmarks. Furthermore, we provide essential yet underexplored insights into how leveraging sub-QAs affects model behavior, specifically analyzing the impact of both the quantity and quality of sub-QAs on achieving robust and reliable reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.171615",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Confidence-guided Refinement Reasoning (C2R)\"的新型推理框架，专注于改进大语言模型的通用推理能力。从筛选标准来看： 首先，在核心判断层面，论文本质上是关于改进LLM的推理能力的，提出了一种新的训练免费推理框架，通过构建和细化子问题及其答案来增强模型的推理过程。这直接符合\"改进LLM基础能力\"和\"增强其逻辑推理能力\"的保留标准。 其次，从正面指标看，论文明确聚焦于\"reasoning\"这一核心能力方向，讨论了\"探索多样化推理路径\"和\"实现稳健可靠的推理\"，这些都是通用推理能力的关键组成部分。 关于多模态方面，虽然论文提到其框架适用于文本、图像和视频领域，但这只是表明其方法的通用性，而非主要焦点。论文的核心是推理框架本身，而不是解决多模态或视觉特定问题，因此不应被排除。 最后，C2R作为一种可以与各种现有QA模型无缝集成的通用方法，代表了提升LLM推理能力的新方法论，完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#53",
        "title": "Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures",
        "link": "/arxiv/2509.20577",
        "arxiv_id": "2509.20577",
        "authors": "Sampurna Roy, Ayan Sar, Anurag Kaushish, Kanav Gupta, Tanupriya Choudhury, Abhijit Kumar",
        "summary": "Contemporary transformer architectures apply identical processing depth to all inputs, creating inefficiencies and limiting reasoning quality. Simple factual queries are subjected to the same multilayered computation as complex logical problems, wasting resources while constraining deep inference. To overcome this, we came up with a concept of Dynamic Reasoning Chains through Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends the Mixture of Experts paradigm from width-based to depth specialised computation. DS-MoE introduces expert modules optimised for distinct reasoning depths, shallow pattern recognition, compositional reasoning, logical inference, memory integration, and meta-cognitive supervision. A learned routing network dynamically assembles custom reasoning chains, activating only the necessary experts to match input complexity. The dataset on which we trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse domains such as scientific papers, legal texts, programming code, and web content, enabling systematic assessment across reasoning depths. Experimental results demonstrate that DS-MoE achieves up to 16 per cent computational savings and 35 per cent faster inference compared to uniform-depth transformers, while delivering 2.8 per cent higher accuracy on complex multi-step reasoning benchmarks. Furthermore, routing decisions yield interpretable reasoning chains, enhancing transparency and scalability. These findings establish DS-MoE as a significant advancement in adaptive neural architectures, demonstrating that depth-specialised modular processing can simultaneously improve efficiency, reasoning quality, and interpretability in large-scale language models.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.179137",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。根据筛选标准，我的判断过程如下： 第一步：核心判断 论文的核心是提出DS-MoE（Depth Specialised Mixture of Experts）框架，这是一种改进Transformer架构的新方法，通过深度专业化的专家混合模型来动态构建推理链。论文明确关注增强LLM的逻辑推理和多步推理能力，而不是将LLM作为工具应用于特定领域。这完全符合改进LLM基础能力和增强其通用推理能力的研究目标。 第二步：正面指标 论文包含多个正面指标： - 核心概念：虽然摘要未直接提及\"LLMs\"，但DS-MoE明显是针对大语言模型的基础架构改进 - 能力方向：明确涉及\"reasoning chains\"、\"reasoning depths\"、\"logical inference\"和\"complex multi-step reasoning benchmarks\"，直接针对推理能力，特别是逻辑推理和多步推理 - 论文强调通过动态组装定制的推理链来匹配输入复杂性，这正是提升通用推理能力的核心 第三步：排除标准 论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉相关内容 - 虽然训练数据集包含多个领域，但论文本身不是针对特定应用领域的研究 - 未涉及模型可靠性方面的水印、安全性等内容 第四步：特殊和模糊情况 论文提到\"routing decisions yield interpretable reasoning chains, enhancing transparency\"，这表明其方法增强了模型的可解释性，从而可能提升模型的通用可靠性和推理质量，符合研究目标。 综上所述，这篇论文的核心贡献是通过改进Transformer架构来增强LLM的通用推理能力，特别是逻辑推理和多步推理能力，同时提高计算效率和可解释性，完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#56",
        "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning",
        "link": "/arxiv/2509.20502",
        "arxiv_id": "2509.20502",
        "authors": "Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang",
        "summary": "Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\\%. Code is available at https://github.com/xwang97/MARS.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.179790",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是提出MARS（Multi-Agent Review System），一种基于角色的多智能体协作框架，旨在提高LLM的推理能力。论文的核心贡献是改进LLM的基础推理能力，提出了一种新的多智能体协作范式，而不是将LLM作为工具应用于特定领域。因此，这篇论文符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Large language models (LLMs)\" - 能力方向：直接关注\"LLM reasoning\"，旨在提高模型的推理质量 - 新兴范式：提出了多智能体系统(MARS)，这是一种基于LLM的智能体协作框架 论文符合3个正面指标，表明其与研究主题高度相关。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域（如医疗、化学等） - 不关注模型可靠性层面的水印、安全等问题 第四步：特殊和模糊情况 论文提出的MARS框架是一种通用的智能体协作方法，用于增强LLM的通用推理能力，而不是将智能体应用于特定领域。根据筛选标准，这种通用的智能体协作框架应该保留。 综合分析，这篇论文的核心贡献是提出了一种更高效的多智能体协作框架(MARS)来增强LLM的通用推理能力，与研究目标\"提高大语言模型的通用推理能力\"直接一致。论文通过改进多智能体协作方式，在保持推理质量的同时显著提高了效率，这正属于对LLM基础推理能力的改进研究。"
    },
    {
        "index": "#61",
        "title": "SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation",
        "link": "/arxiv/2509.20377",
        "arxiv_id": "2509.20377",
        "authors": "Tomoaki Isoda",
        "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive tasks in recent years. However, since retrieval systems may return irrelevant content, incorporating such information into the model often leads to hallucinations. Thus, identifying and filtering out unhelpful retrieved content is a key challenge for improving RAG performance.To better integrate the internal knowledge of the model with external knowledge from retrieval, it is essential to understand what the model \"knows\" and \"does not know\" (which is also called \"self-knowledge\"). Based on this insight, we propose SKILL-RAG (Self-Knowledge Induced Learning and Filtering for RAG), a novel method that leverages the model's self-knowledge to determine which retrieved documents are beneficial for answering a given query. We design a reinforcement learning-based training framework to explicitly elicit self-knowledge from the model and employs sentence-level granularity to filter out irrelevant content while preserving useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several question answering benchmarks. Experimental results demonstrate that SKILL-RAG not only improves generation quality but also significantly reduces the number of input documents, validating the importance of self-knowledge in guiding the selection of high-quality retrievals.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-20",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.181043",
        "filter_reason": "这篇论文符合我的研究目标，因为它核心是关于改进大语言模型本身的通用推理能力。具体分析如下： 从第一步核心判断来看，论文的本质是提出SKILL-RAG方法，这是一种新的训练范式，通过强化学习框架来增强模型的自我知识认知能力。该方法不是将LLM作为工具应用于特定领域，而是致力于提升LLM在知识处理和推理方面的基础能力，特别是通过识别模型\"知道\"和\"不知道\"的内容来优化其推理过程，这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的标准。 从第二步正面指标看，论文包含多个相关主题： - 核心概念：明确研究LLMs（Llama2-7B和Qwen3-8B） - 能力方向：涉及推理能力，特别是在知识密集型任务中的问题解决 - 训练方法：使用强化学习（reinforcement learning-based training framework）来训练模型 - 新兴范式：RAG本身可视为一种工具使用范式，论文改进了这一通用框架 从第三步排除标准看，论文不涉及任何应排除的领域，没有专注于多模态、特定应用领域或模型基础设施等。 从第四步特殊和模糊情况看，虽然论文涉及减少幻觉的问题，但它是通过提出新方法（利用模型自我知识）来提升模型的内在推理质量，而不是应用层面的讨论，因此应该保留。 综上所述，SKILL-RAG论文的核心贡献是提出了一种增强LLM自我知识认知和推理能力的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#73",
        "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns",
        "link": "/arxiv/2509.21124",
        "arxiv_id": "2509.21124",
        "authors": "Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai",
        "summary": "Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model's reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.189451",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础推理能力。论文提出了一种新的训练范式，通过学习多样化的思维链模式来扩展基础模型的推理潜力，这直接针对提升LLM的通用推理能力，而非将LLM作为工具应用于特定领域。 其次，论文包含了多个正面指标： 1. 核心概念：论文明确研究基础模型(Foundation Model)的推理能力 2. 能力方向：聚焦于推理能力(reasoning)，特别是数学推理(mathematical reasoning) 3. 训练方法：涉及强化学习(RL)作为关键方法，并讨论如何提高下游RL性能 4. 新兴范式：关注思维链(CoT)的优化，这是提高LLM推理能力的重要范式 第三，论文不符合任何排除标准： 1. 不涉及多模态与视觉内容 2. 虽然在数学推理上评估，但这是作为通用推理能力的测试案例，而非针对特定应用领域 3. 不涉及模型可靠性方面的水印、安全等问题 论文的核心贡献是首次定义了基础模型的\"推理潜力\"，并提出了一种通过选择高价值思维链数据来扩展这种潜力的方法。这种方法通过抽象原子推理模式，构建核心参考集，并使用双粒度算法选择高价值的CoT数据，从而训练模型更有效地掌握推理能力。这些贡献直接针对提升LLM的通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#76",
        "title": "ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning",
        "link": "/arxiv/2509.21070",
        "arxiv_id": "2509.21070",
        "authors": "Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu",
        "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between \"Thinking\" and \"NoThinking\" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.190261",
        "filter_reason": "根据筛选标准，这篇论文符合研究范围。核心判断上，论文本质是提升大语言模型的数学推理能力，属于增强LLM通用推理能力的范畴。论文提出ScaleDiff流程，通过高效生成困难数学问题来训练模型，从而提升LLM的数学推理能力，这是对LLM基础能力的改进，而非将LLM作为工具应用于特定领域。 从正面指标看，论文明确涉及Large Reasoning Models (LRMs)这一核心概念，并专注于Advanced Mathematical Reasoning这一推理能力方向。论文通过生成困难问题数据集并微调模型，实质上提出了一种新的训练范式来增强LLM的推理能力。 在排除标准方面，虽然论文专注于数学推理，但数学推理通常被视为评估和提升LLM通用推理能力的重要方面，而非特定应用领域。论文也不涉及多模态、视觉内容或特定应用领域如医疗、化学等。 论文的核心贡献是通过增加困难问题的数量来提升LLM的数学推理能力，并观察到\"随着困难问题数量增加，模型在困难基准测试上的性能呈现明显的扩展现象\"，这直接与研究目标\"提高大语言模型（LLM）本身的『通用推理能力』\"相符。因此，这篇论文应被保留。"
    },
    {
        "index": "#80",
        "title": "DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?",
        "link": "/arxiv/2509.21016",
        "arxiv_id": "2509.21016",
        "authors": "Yiyou Sun, Yuhan Cao, Pohao Huang, Haoyue Bai, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song",
        "summary": "It remains an open question whether LLMs can acquire or generalize genuinely new reasoning strategies, beyond the sharpened skills encoded in their parameters during pre-training or post-training. To attempt to answer this debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding, a controlled benchmark of synthetic coding problem families designed to probe two fundamental aspects: learnability -- can LLMs, through reinforcement learning (RL), solve problem families where pretrained models exhibit failure with large enough attempts (pass@K=0)? --and transferrability -- if learnability happens, can such skills transfer systematically to out-of-distribution (OOD) test sets? Unlike prior public coding datasets, DELTA isolates reasoning skills through templated problem generators and introduces fully OOD problem families that demand novel strategies rather than tool invocation or memorized patterns. Our experiments reveal a striking grokking phase transition: after an extended period with near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To enable learnability on previously unsolvable problem families, we explore key training ingredients such as staged warm-up with dense rewards, experience replay, curriculum training, and verification-in-the-loop. Beyond learnability, we use DELTA to evaluate transferability or generalization along exploratory, compositional, and transformative axes, as well as cross-family transfer. Results show solid gains within families and for recomposed skills, but persistent weaknesses in transformative cases. DELTA thus offers a clean testbed for probing the limits of RL-driven reasoning and for understanding how models can move beyond existing priors to acquire new algorithmic skills.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.191214",
        "filter_reason": "这篇论文的核心贡献是提出了DELTA-Code基准测试，用于探索大语言模型(LLMs)是否能通过强化学习(RL)获取和迁移全新的推理策略。论文研究的是LLMs如何通过强化学习解决预训练时无法解决的问题，以及这些学到的技能如何迁移到分布外测试集。这完全符合研究\"大语言模型通用推理能力\"的目标，因为：(1)论文本质是改进LLM的基础推理能力，特别是算法推理能力，而不是将LLM作为工具应用到特定领域；(2)论文使用了强化学习这一训练范式来增强模型能力，探索了staged warm-up、experience replay、curriculum training等关键训练成分；(3)论文聚焦于推理能力的获取和迁移，这是通用推理能力的核心方面。论文符合多个正面指标，包括核心概念(LLMs)、能力方向(reasoning)和训练方法(RL)，同时不符合任何排除标准。因此，这篇论文应该被纳入研究范围。"
    },
    {
        "index": "#81",
        "title": "Mechanism of Task-oriented Information Removal in In-context Learning",
        "link": "/arxiv/2509.21012",
        "arxiv_id": "2509.21012",
        "authors": "Hakaze Cho, Haolin Yang, Gouki Minegishi, Naoya Inoue",
        "summary": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.191430",
        "filter_reason": "我按照筛选标准对这篇论文进行了全面分析： 第一步：核心判断——这篇论文的本质是研究In-context Learning (ICL)的内在工作机制，特别是从\"信息移除\"角度解析ICL如何帮助语言模型专注于目标任务。论文不是将LLM作为工具应用到特定领域，而是深入研究LLM本身的基础工作机制，属于\"改进LLM的基础能力\"的研究方向，符合核心判断标准。 第二步：正面指标——论文符合\"核心概念\"指标，因为它明确研究现代语言模型(LMs)的ICL机制。虽然论文没有直接讨论reasoning、planning等能力方向，也没有涉及reinforcement learning等训练方法或llm-based agents等新兴范式，但ICL本身是LLM的一种重要通用能力，与推理能力密切相关。 第三步：排除标准——论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。 第四步：特殊和模糊情况——论文在可解释性方面有显著相关性，因为它揭示了ICL的\"面向任务的信息移除\"机制，这种对模型内在机制的深入理解有助于提升模型的通用可靠性和推理质量。 综合判断：这篇论文的核心贡献是揭示了ICL的工作机制，特别是\"面向任务的信息移除\"过程，这属于研究LLM基础能力的重要工作。理解ICL如何帮助模型从纠缠的信息中选择性移除冗余信息，对于提升LLM的通用推理能力具有重要意义。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#78",
        "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems",
        "link": "/arxiv/2509.21054",
        "arxiv_id": "2509.21054",
        "authors": "Haodong Zhao, Jidong Li, Zhaomin Wu, Tianjie Ju, Zhuosheng Zhang, Bingsheng He, Gongshen Liu",
        "summary": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to solve complex problems, necessitates a deep understanding of the persuasion dynamics that govern their interactions. This paper challenges the prevailing hypothesis that persuasive efficacy is primarily a function of model scale. We propose instead that these dynamics are fundamentally dictated by a model's underlying cognitive process, especially its capacity for explicit reasoning. Through a series of multi-agent persuasion experiments, we uncover a fundamental trade-off we term the Persuasion Duality. Our findings reveal that the reasoning process in LRMs exhibits significantly greater resistance to persuasion, maintaining their initial beliefs more robustly. Conversely, making this reasoning process transparent by sharing the \"thinking content\" dramatically increases their ability to persuade others. We further consider more complex transmission persuasion situations and reveal complex dynamics of influence propagation and decay within multi-hop persuasion between multiple agent networks. This research provides systematic evidence linking a model's internal processing architecture to its external persuasive behavior, offering a novel explanation for the susceptibility of advanced models and highlighting critical implications for the safety, robustness, and design of future MAS.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.190683",
        "filter_reason": "这篇论文的核心贡献是研究大型语言模型(LLMs)和大型推理模型(LRMs)的推理过程如何影响其在多智能体系统中的说服动态。论文提出了\"说服二元性\"(Persuasion Duality)的概念，揭示了模型的底层认知过程，特别是其显式推理能力，如何决定其在多智能体交互中的说服行为。 根据筛选标准，这篇论文符合研究目标，原因如下： 1. 核心判断：论文的本质是研究LLM的基础能力——推理能力，而不是将LLM作为工具应用到特定领域。论文探索的是模型的底层认知过程和推理架构如何影响其外部行为，这属于对LLM通用能力的深入研究。 2. 正面指标：论文包含多个关键正面指标： - 核心概念：明确研究Large Language Models (LLMs)和Large Reasoning Models (LRMs) - 能力方向：核心关注reasoning能力，特别是模型的\"thinking process\"和\"explicit reasoning\" - 新兴范式：研究Multi-Agent Systems (MAS)中LLMs的交互和协作 3. 排除标准：论文不主要聚焦于任何排除领域。虽然研究\"说服\"这一社会心理学概念，但论文不是将LLM应用到社会学领域，而是研究LLM本身的推理能力如何影响其在多智能体系统中的行为。 4. 特殊情况处理：论文研究的是通用多智能体系统框架中的推理问题，属于\"提出一种通用的智能体协作框架来增强LLM的通用问题解决能力\"的情况，应该保留。 因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，它通过研究模型推理过程与说服动态的关系，增进了我们对LLM推理能力的理解，有助于未来设计具有更强推理能力的LLM和多智能体系统。"
    },
    {
        "index": "#84",
        "title": "On Theoretical Interpretations of Concept-Based In-Context Learning",
        "link": "/arxiv/2509.20882",
        "arxiv_id": "2509.20882",
        "authors": "Huaze Tang, Tianren Peng, Shao-lun Huang",
        "summary": "In-Context Learning (ICL) has emerged as an important new paradigm in natural language processing and large language model (LLM) applications. However, the theoretical understanding of the ICL mechanism remains limited. This paper aims to investigate this issue by studying a particular ICL approach, called concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs well for predicting query labels in prompts with only a few demonstrations. In addition, the proposed theory quantifies the knowledge that can be leveraged by the LLMs to the prompt tasks, and leads to a similarity measure between the prompt demonstrations and the query input, which provides important insights and guidance for model pre-training and prompt engineering in ICL. Moreover, the impact of the prompt demonstration size and the dimension of the LLM embeddings in ICL are also explored based on the proposed theory. Finally, several real-data experiments are conducted to validate the practical usefulness of CB-ICL and the corresponding theory.",
        "subjects": "Information Theory, Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.192126",
        "filter_reason": "这篇论文的核心是对大语言模型的上下文学习(ICL)能力进行理论解释，特别是研究基于概念的上下文学习(CB-ICL)方法。上下文学习是大语言模型的一种重要通用推理能力，它允许模型从少量示例中学习并应用到新问题上，这本质上是一种推理和问题解决过程。论文提出了理论分析，解释了CB-ICL在少量示例情况下表现良好的原因，量化了LLMs可以利用的知识，并提出了相似性度量方法，这些都有助于深入理解LLM的推理机制。虽然论文没有涉及强化学习、智能体协作等新兴范式，但它聚焦于LLM的基础能力研究，特别是对通用推理能力的理论解释，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文不涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。因此，这篇论文应该被保留。"
    },
    {
        "index": "#89",
        "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning",
        "link": "/arxiv/2509.20712",
        "arxiv_id": "2509.20712",
        "authors": "Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou",
        "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose \\textbf{C}ontrolling \\textbf{E}ntropy via \\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.193213",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种新的强化学习算法CE-GPPO，用于改进大语言模型的基础推理能力。论文明确指出其目标是优化LLM处理\"复杂推理任务\"的能力，并通过改进PPO算法的梯度处理机制来提升模型性能，这直接属于改进LLM基础能力和通用推理能力的范畴。 其次，从正面指标看，论文包含了多个相关主题：核心概念上明确研究\"large language models (LLMs)\"；能力方向上聚焦于\"complex reasoning tasks\"和\"mathematical reasoning\"；训练方法上提出了新的强化学习优化算法(CE-GPPO)。这些都是高度相关的指标。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉内容，没有将LLM应用于特定领域（数学推理仅作为评估通用推理能力的基准），也没有涉及模型基础设施或应用层面的可靠性问题。 论文的核心贡献是提出了一种新的强化学习训练范式，通过改进梯度处理机制来优化LLM的推理能力，这直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。因此，这篇论文应该被保留。"
    },
    {
        "index": "#85",
        "title": "StyleBench: Evaluating thinking styles in Large Language Models",
        "link": "/arxiv/2509.20868",
        "arxiv_id": "2509.20868",
        "authors": "Junyu Guo, Shangding Gu, Ming Jin, Costas Spanos, Javad Lavaei",
        "summary": "The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-09-26T21:01:55.192332",
        "filter_reason": "这篇论文的核心贡献是提出了StyleBench基准测试，用于系统评估不同推理风格（如CoT、ToT、AoT等）对大语言模型性能的影响。论文直接关注LLM的通用推理能力，研究不同思考风格如何影响模型在各种推理任务上的表现。这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文没有将LLM作为工具应用到特定领域，也没有关注模型基础设施或部署优化。相反，它通过大规模分析揭示了推理风格、模型规模和任务类型之间的复杂关系，为选择最优推理策略提供了指导，这对提升LLM的基础推理能力具有重要价值。论文涉及的核心概念（LLMs）和能力方向（reasoning）进一步确认了它与我的研究目标高度相关。因此，这篇论文应该被保留。"
    },
    {
        "index": "#5",
        "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA",
        "link": "/arxiv/2509.21199",
        "arxiv_id": "2509.21199",
        "authors": "Kaiyang Wan, Lang Gao, Honglin Mu, Preslav Nakov, Yuxia Wang, Xiuying Chen",
        "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.256868",
        "filter_reason": "这篇论文的核心是研究大语言模型在多跳问答(MHQA)任务中的单遍推理能力，属于提升LLM通用推理能力的研究。论文建立了Fano风格的精度上界理论，揭示了LLM在单遍推理中面临的容量瓶颈问题，并提出了InfoQA框架作为解决方案。该框架通过容量感知的任务分解和主动修剪推理轨迹来增强LLM的多步推理能力，确保信息处理不超过单遍限制。论文符合核心判断标准，因为它关注的是改进LLM的基础推理能力，特别是多步推理这一通用能力。论文也符合正面指标中的核心概念(LLMs)和能力方向(reasoning, multi-step reasoning)。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。InfoQA框架虽然不是典型的智能体系统，但它提出了一种通用的方法来增强LLM在推理任务中的能力，属于提升LLM内在推理能力的研究。因此，这篇论文完全符合\"提高大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#4",
        "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns",
        "link": "/arxiv/2509.21224",
        "arxiv_id": "2509.21224",
        "authors": "Stefan Szeider",
        "summary": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.256679",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是研究大语言模型(LLM)智能体在没有外部任务时的自发行为和元认知模式。论文提出的\"持续推理与行动\"框架，使用持久记忆和自我反馈来增强LLM的自主运行能力，这属于改进LLM基础能力和推理能力的研究，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：涉及reasoning（特别是元认知和自发推理模式）、planning和problem-solving - 新兴范式：研究llm-based agents和deep research（对LLM认知过程的深入研究） 第三步排除标准：论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 第四步特殊和模糊情况：论文提出的智能体框架是一种通用的框架，旨在增强LLM的自主推理和元认知能力，而不是应用于特定领域，因此应该保留。 论文的核心贡献是首次系统性地记录了无提示LLM智能体的自发行为模式，为理解LLM的元认知能力和自主推理能力提供了重要见解，这直接符合研究\"大语言模型通用推理能力\"的目标。"
    },
    {
        "index": "#9",
        "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
        "link": "/arxiv/2509.21128",
        "arxiv_id": "2509.21128",
        "authors": "Kohsei Matsutani, Shota Takashiro, Gouki Minegishi, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo",
        "summary": "Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.257610",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究如何通过强化学习(RL)和监督微调(SFT)两种训练方法来提升大语言模型的推理能力，而不是将LLM作为工具应用到特定领域。论文提出了新的分析框架来量化推理路径，这属于改进LLM基础能力的研究。 其次，论文包含了多个正面指标：明确以大语言模型(LLMs)为研究对象；聚焦于推理能力(reasoning abilities)，特别是在数学领域；比较了强化学习(RL)和监督微调(SFT)这两种训练方法对推理能力的影响。 第三，论文不涉及任何排除标准中的领域。虽然论文在数学领域进行了实验，但数学只是作为推理能力的测试案例，而非论文的核心应用焦点。 最后，在特殊和模糊情况处理上，论文从可解释性角度提出了新的分析框架来理解训练方法如何影响推理过程，这有助于提升模型的内在可解释性和推理质量，符合保留标准。 论文的核心贡献是揭示了RL和SFT对推理过程的不同影响：RL压缩不正确的推理轨迹，而SFT扩展正确的推理轨迹，这解释了为什么当前最佳实践是两阶段训练(SFT后跟RL)。这项研究对理解和提升LLM的通用推理能力具有重要意义。"
    },
    {
        "index": "#14",
        "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
        "link": "/arxiv/2509.21043",
        "arxiv_id": "2509.21043",
        "authors": "Samuel Schapiro, Sumuk Shashidhar, Alexi Gladstone, Jonah Black, Royce Moon, Dilek Hakkani-Tur, Lav R. Varshney",
        "summary": "Artificial intelligence (AI) systems, and large language models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Though in many ways similar to forms of compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, marking a new frontier in generalization abilities.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.258861",
        "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)的组合创造力(Combinatorial Creativity)，将其视为泛化能力的新前沿。论文提出了评估LLM创造力的理论框架和算法任务，研究了LLM创造力的扩展行为，发现了模型架构对创造能力的影响，并探讨了\"构想-执行差距\"和\"新颖性-实用性权衡\"等根本性问题。虽然论文提到了科学想法生成作为创造力的应用例子，但其重点是研究创造力这一通用能力本身，而不是专注于特定领域的应用。创造力可以被视为一种高级的问题解决和推理能力，与通用推理能力密切相关。论文明确关注LLMs的核心能力提升，试图为理解和改进现代AI模型中的创造力提供基础，这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容，因此应该被保留。"
    },
    {
        "index": "#24",
        "title": "Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning",
        "link": "/arxiv/2509.20744",
        "arxiv_id": "2509.20744",
        "authors": "Qihang Ai, Haiyun Jiang",
        "summary": "We study reasoning tasks through a framework that integrates auto-regressive (AR) and non-autoregressive (NAR) language models. AR models, which generate text sequentially, excel at producing coherent outputs but often suffer from slow inference, particularly in reasoning-intensive domains such as mathematics and code, where lengthy chains of thought are required. In contrast, NAR models, such as discrete diffusion models, allow parallel generation and offer substantial speedups, though typically at the cost of reduced output quality. To address these limitations, we introduce a new paradigm in which an NAR model efficiently produces intermediate reasoning traces, which subsequently guide an AR model to deliver precise final answers. Experiments demonstrate that our approach yields significant 26% improvements over strong baselines while substantially reducing inference cost.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.266155",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础推理能力，提出了一种结合自回归(AR)和非自回归(NAR)模型的新范式来增强推理效率。论文明确关注推理密集型任务，特别是数学和代码领域，这属于提升LLM通用推理能力的核心研究。 其次，论文包含多个重要的正面指标：核心概念上涉及语言模型(LLMs)；能力方向上明确关注reasoning和math reasoning；论文提出的方法论是通过NAR模型生成中间推理轨迹，然后由AR模型生成最终答案，这是一种增强模型推理能力的新方法。 第三，论文不符合任何排除标准。虽然提到了\"discrete diffusion models\"，但这是在语言模型推理的上下文中讨论的，而非视觉或多模态应用。论文虽然以数学和代码为例，但其方法是一种通用推理框架，并非针对特定应用领域的研究。 论文的核心贡献是提出了一种新的推理范式，通过并行生成中间推理步骤来提高推理效率，同时保持输出质量，这直接服务于提升大语言模型的通用推理能力的研究目标。"
    },
    {
        "index": "#29",
        "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection",
        "link": "/arxiv/2509.20562",
        "arxiv_id": "2509.20562",
        "authors": "Yubin Ge, Salvatore Romeo, Jason Cai, Monica Sunkara, Yi Zhang",
        "summary": "Despite the rapid advancements in LLM agents, they still face the challenge of generating meaningful reflections due to inadequate error analysis and a reliance on rare successful trajectories, especially in complex tasks. In this work, we propose SAMULE, a new framework for self-learning agents powered by a retrospective language model that is trained based on Multi-Level Reflection Synthesis. It first synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights based on same typed errors from diverse task failures. Then we fine-tune a language model serving as the retrospective model to generate reflections during inference. We further extend our framework to interactive settings through a foresight-based reflection mechanism, enabling agents to proactively reflect and adapt during user interactions by comparing predicted and actual responses. Extensive experiments on three challenging benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our approach significantly outperforms reflection-based baselines. Our results highlight the critical role of well-designed reflection synthesis and failure-centric learning in building self-improving LLM agents.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.267251",
        "filter_reason": "这篇论文的核心贡献是提出SAMULE框架，一种通过多层次反思增强的自学习智能体方法。论文本质上是关于改进LLM的基础能力，特别是通过反思机制增强其通用推理和问题解决能力。该方法在三个层次（单轨迹、任务内、任务间）合成高质量反思，并微调语言模型作为回顾性模型，从而提升LLM智能体的自我学习和适应能力。这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文涉及LLM智能体、问题解决和自我学习等正面指标，同时没有被排除标准所涵盖。虽然使用了TravelPlanner等基准测试，但这些是用于评估通用推理能力的标准测试集，而非特定应用领域的研究。因此，该论文符合筛选条件。"
    },
    {
        "index": "#35",
        "title": "LATTS: Locally Adaptive Test-Time Scaling",
        "link": "/arxiv/2509.20368",
        "arxiv_id": "2509.20368",
        "authors": "Theo Uscidda, Matthew Trager, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto",
        "summary": "One common strategy for improving the performance of Large Language Models (LLMs) on downstream tasks involves using a \\emph{verifier model} to either select the best answer from a pool of candidates or to steer the auto-regressive generation process towards better outputs. This class of methods typically results in improved accuracy at the cost of increased computation at test-time, a paradigm known as \\emph{test-time scaling}. However, most existing approaches increase computation uniformly across all samples and generation steps, without considering the complexity of individual instances, leading to inefficient resource use. We address this limitation by proposing an approach, called \\emph{Locally Adaptive Test-Time Scaling (LATTS)}, that allocates variable compute across generation steps. Specifically, at each generation step, LATTS employs a verifier-based acceptance criterion to decide whether to resample, backtrack, restart, or stop the generation process. This criterion effectively adjusts the per-step computational effort based on a precise notion of \\emph{local difficulty} derived from the verifier model. Empirical results show that LATTS achieves significantly superior accuracy--compute tradeoffs compared to standard verifier-based methods.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.268515",
        "filter_reason": "这篇论文符合我的研究目标，因为它专注于提高大语言模型本身的通用推理能力。论文的核心贡献是提出了一种名为\"Locally Adaptive Test-Time Scaling (LATTS)\"的新方法，该方法通过在每个生成步骤中动态调整计算资源分配来提高LLM的推理性能。具体来说，LATTS使用验证模型来评估局部难度，并据此决定是否重新采样、回溯、重启或停止生成过程，从而更有效地利用计算资源。 从筛选标准来看： 1. 核心判断：论文的本质是改进LLM的基础能力，特别是其推理效率，而不是将LLM作为工具应用到特定领域。它提出了一种新的测试时计算资源分配范式，这与提高LLM的通用推理能力直接相关。 2. 正面指标：论文明确涉及\"Large language models (LLMs)\"这一核心概念，并关注提高LLM在下游任务上的性能，这通常涉及推理和问题解决能力。 3. 排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容。 4. 特殊情况：论文不涉及需要特殊处理的情况。 虽然论文没有明确提到思维链、强化学习等具体方法，但它提出了一种新的测试时计算资源分配方法，通过验证模型动态调整计算资源来提高LLM的推理性能，这与提高LLM的通用推理能力直接相关。因此，这篇论文符合我的研究目标。"
    },
    {
        "index": "#41",
        "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL",
        "link": "/arxiv/2509.21282",
        "arxiv_id": "2509.21282",
        "authors": "Madeleine Dwyer, Adam Sobey, Adriane Chapman",
        "summary": "Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information and introduces gradient discontinuities. We propose Probability Smoothing Policy Optimisation (PSPO), which smooths the current policy's probabilities toward the old (behaviour) policy before computing the importance ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient signal, while interpolation toward the old policy creates a soft trust region that discourages large, destabilising updates, with formal guarantees. We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO (single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar performance but improves the reasoning leading to clearer and more concise responses which are more logical. Compared to clipped GRPO, GR-PSPO substantially improves performance both the 0.5B and 1.5B models, with a boost of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.269842",
        "filter_reason": "这篇论文的核心贡献是提出一种名为Probability Smoothing Policy Optimisation (PSPO)的新方法，用于改进LLM的强化学习训练过程。从本质上看，这篇论文直接关注改进LLM的基础训练方法，而不是将LLM作为工具应用到特定领域。论文提出的方法通过平滑策略概率来创建软信任区域，解决了传统裁剪方法带来的信息丢失和梯度不连续问题，从而提升了模型的推理能力。论文在多个数学推理数据集(GSM8K、SVAMP、ASDiv和MATH-500)上进行了评估，结果显示PSPO能显著提升模型的推理性能，这符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文涉及的强化学习训练方法和推理能力提升等主题也符合正面指标，同时不涉及任何排除标准中的领域(如多模态、特定应用领域或模型可靠性的应用层面)。因此，这篇论文完全符合研究范围。"
    },
    {
        "index": "#52",
        "title": "Tree Search for LLM Agent Reinforcement Learning",
        "link": "/arxiv/2509.21240",
        "arxiv_id": "2509.21240",
        "authors": "Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu",
        "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.278269",
        "filter_reason": "根据筛选标准，这篇论文完全符合研究目标。首先，从核心判断来看，论文本质上是提出Tree-based Group Relative Policy Optimization (Tree-GRPO)这一新的强化学习方法，旨在增强LLM智能体的通用推理和规划能力，特别是在长期和多轮任务中解决稀疏监督问题。这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理、规划等通用能力\"的范畴。 其次，论文符合所有正面指标：核心概念涉及LLM；能力方向关注推理和问题解决；训练方法采用强化学习；新兴范式涉及LLM-based agents。论文没有涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。 特别地，论文提出的是一种通用的智能体强化学习方法，而不是将智能体应用于特定领域，这符合第四步中关于智能体/工具使用的保留标准。论文的核心贡献是通过树搜索技术增强LLM的通用推理能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。"
    },
    {
        "index": "#60",
        "title": "GRPO is Secretly a Process Reward Model",
        "link": "/arxiv/2509.21154",
        "arxiv_id": "2509.21154",
        "authors": "Michael Sullivan",
        "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and show that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.279973",
        "filter_reason": "这篇论文完全符合研究\"大语言模型通用推理能力\"的目标。从核心判断来看，论文本质上是关于改进LLM的训练方法，具体研究了GRPO强化学习算法的内在机制，并提出改进版本λ-GRPO来提升LLM的推理能力。这属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。 从正面指标看，论文包含了多个相关主题： 1. 核心概念：明确研究LLMs（大语言模型） 2. 能力方向：关注\"downstream reasoning tasks\"（下游推理任务） 3. 训练方法：深入研究\"GRPO RL algorithm\"（GRPO强化学习算法） 从排除标准看，论文不涉及任何应排除的领域： - 不涉及多模态与视觉 - 不是将LLM应用于特定领域（如医疗、化学等） - 不主要关注模型可靠性方面的应用问题 论文的核心贡献是发现了GRPO算法隐含地构建了一个过程奖励模型(PRM)，并基于这一发现提出改进算法λ-GRPO，显著提升了LLM在推理任务上的表现。这直接服务于提升大语言模型本身的通用推理能力，而非将LLM作为工具应用于特定领域。因此，该论文完全符合研究目标。"
    },
    {
        "index": "#68",
        "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
        "link": "/arxiv/2509.21091",
        "arxiv_id": "2509.21091",
        "authors": "Junpei Komiyama, Daisuke Oba, Masafumi Oyamada",
        "summary": "We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.287125",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断——这篇论文的本质是关于改进LLM的基础能力。论文研究的是通过测试时计算(test-time compute)优化来提高大语言模型性能的方法，具体聚焦于best-of-N采样和多数投票机制，以及当N趋近于无穷大时的渐近性能分析。这明显属于提升LLM本身通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确提到\"large language models (LLMs)\" - 能力方向：虽然未直接使用\"reasoning\"等词汇，但best-of-N方法和多数投票本质上是为了提高模型的问题解决能力和推理质量 - 新兴范式：论文探讨了多个LLMs的加权集成方法，这与模型协作和优化相关 第三步：排除标准——论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉内容 - 未针对医疗、化学、生物等特定应用领域 - 未关注水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况——论文不涉及特殊或模糊情况。虽然提到了多个LLMs的加权集成，但主要焦点是测试时计算的优化，而非智能体协作框架或工具使用方法。 核心贡献：论文提出了一种自适应生成方案和多个LLMs的加权集成方法，通过优化测试时计算来提高LLM的性能。这种方法本质上是增强模型通用推理能力的有效途径，因为它不依赖于特定领域知识，而是通过更有效的计算资源分配和模型集成来提升LLM的问题解决能力，完全符合\"提高大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#76",
        "title": "Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs",
        "link": "/arxiv/2509.21044",
        "arxiv_id": "2509.21044",
        "authors": "Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li",
        "summary": "Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.288960",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究强化学习(RL)微调如何增强大语言模型的内部机制，特别是激活强度和多样性。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，论文探索了RL微调相比监督微调(SFT)能够更有效提升LLM能力的原因，属于对LLM基础能力的深入研究。 其次，从正面指标来看，论文包含多个相关主题： - 核心概念：明确研究大语言模型(LLMs) - 训练方法：核心研究强化学习(RL)微调对LLM的影响，包括比较PPO、GRPO和DPO等不同RL方法 - 能力方向：虽然未直接研究具体推理任务，但探讨了RL微调如何提升LLM的泛化能力，这与通用推理能力密切相关 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学、生物等） - 不关注模型可靠性的应用层面（如水印、安全等） 论文的核心贡献在于揭示了RL微调如何系统性地改变LLM的内部电路机制，发现RL微调导致激活强度增加和激活模式多样化，这些内部变化可能是RL提升LLM泛化能力的原因。这种对LLM内部机制的基础性研究，直接有助于理解如何提升LLM的通用推理能力，因此完全符合研究目标。"
    },
    {
        "index": "#176",
        "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments",
        "link": "/arxiv/2509.20386",
        "arxiv_id": "2509.20386",
        "authors": "Nishant Gaurav, Adit Akarsh, Ankit Ranjan, Manoj Bajaj",
        "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef- ficiently operate with extensive Model Control Protocol (MCP) tool sets that exceed the contextual memory limitations of large language models. Our approach addresses the fundamental challenge of tool selection in environments containing hundreds or thousands of available tools, where loading all tools simultaneously is computationally infeasible. We propose and evaluate five distinct architectures that progressively refine the tool selection process, culminating in a search-and-load mechanism that achieves intelligent tool selection with minimal computational overhead. Our experimental results demonstrate that the proposed approach reduces tool loading by up to 50% while maintaining task completion accuracy, advancing the path towards truly general-purpose AI agents capable of dynamically adapting to diverse task environments.",
        "subjects": "Software Engineering, Artificial Intelligence, Information Retrieval",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.331925",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出Dynamic ReAct方法，用于改进ReAct智能体在大型工具环境中的工具选择能力。ReAct是一种结合推理和行动的智能体框架，使大语言模型能够进行推理并使用工具解决问题。论文的核心贡献是解决当工具数量超过LLM上下文记忆限制时的工具选择挑战，这属于增强LLM通用推理能力的范畴，特别是工具使用和问题解决方面。论文不是将LLM作为工具应用到特定领域，而是改进LLM本身通过智能体框架使用工具的能力，因此符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：涉及ReAct agents，这是基于LLM的智能体框架 - 能力方向：与problem-solving相关，因为工具选择是问题解决的关键环节 - 新兴范式：明确涉及llm-based agents和tool use，这是论文的核心主题 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不是针对特定应用领域（如医疗、化学等），而是关注通用工具选择机制 - 不涉及模型可靠性方面的应用层面问题 第四步：特殊和模糊情况处理 论文提出的Dynamic ReAct方法是一种通用的工具选择机制，旨在增强LLM通过智能体框架使用工具的通用问题解决能力，而不是将工具应用在特定领域。这符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况，因此应该保留。 最终决策 这篇论文的核心贡献是提出了一种改进LLM通过智能体框架使用工具的通用能力的方法，从而增强了LLM的通用推理和问题解决能力。论文专注于工具选择这一关键环节，使LLM能够更有效地在大型工具环境中运作，这直接符合提高大语言模型通用推理能力的研究目标。"
    },
    {
        "index": "#18",
        "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say",
        "link": "/arxiv/2509.21164",
        "arxiv_id": "2509.21164",
        "authors": "Jacob Fein-Ashley, Dhruv Parikh, Rajgopal Kannan, Viktor Prasanna",
        "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain (e.g., math, code, general reasoning), motivating systems that leverage complementary strengths across models. Prior multi-LLM approaches either (i) route a query to one or a few experts and generate independently, (ii) aggregate outputs from each model via costly multi-turn exchanges, or (iii) fuse weights into a single model-typically requiring architectural homogeneity. We introduce Mixture of Thoughts (MoT), a simple method for latent-level collaboration among heterogeneous experts under a global routing scheme. For each query, a lightweight router selects top-$K$ experts and designates a primary expert; uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active (selected) peers. Pre-trained experts remain frozen; only the router and the lightweight interaction layers are trained with a novel joint training objective that improves both the expert selection and inter-expert collaboration. Across five in-distribution (ID) and three out-of-distribution (OOD) benchmarks, MoT surpasses the current routing and aggregation-based state-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further, MoT significantly outperforms the best-performing single model. It achieves this with single-pass inference, runtime comparable to routing baselines, and none of the overheads of iterative aggregation. MoT offers a simple latent-space mechanism for combining heterogeneous LLMs, a practical step toward broader multi-LLM collaboration. Our code is publicly available at https://github.com/jacobfa/mot.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.405083",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Mixture of Thoughts (MoT)\"的新方法，用于在不同专业领域（如数学、代码、通用推理）的开源大语言模型之间进行潜在层面的协作。该方法通过一个轻量级路由器选择顶级专家，并在共享潜在空间中通过交叉注意力机制进行协作，从而提升整体推理性能。这符合研究目标，因为：(1) 论文的核心是改进LLM的基础能力，特别是通用推理能力，而不是将LLM作为工具应用到特定领域；(2) 论文涉及多个正面指标，包括大语言模型、推理能力（数学和通用推理）以及多智能体系统；(3) 论文提出了一种通用的多LLM协作框架，用于增强LLM的通用问题解决能力，而非针对特定应用领域；(4) 实验结果表明，MoT在多个基准测试上超过了当前最先进的方法，有效提升了LLM的推理能力。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#83",
        "title": "Theoretical Bounds for Stable In-Context Learning",
        "link": "/arxiv/2509.20677",
        "arxiv_id": "2509.20677",
        "authors": "Tongxi Wang, Zhuoyang Xia",
        "summary": "In-context learning (ICL) is flexible but its reliability is highly sensitive to prompt length. This paper establishes a non-asymptotic lower bound that links the minimal number of demonstrations to ICL stability under fixed high-dimensional sub-Gaussian representations. The bound gives explicit sufficient conditions in terms of spectral properties of the covariance, providing a computable criterion for practice. Building on this analysis, we propose a two-stage observable estimator with a one-shot calibration that produces practitioner-ready prompt-length estimates without distributional priors. Experiments across diverse datasets, encoders, and generators show close alignment between the predicted thresholds and empirical knee-points, with the theory acting as a conservative but reliable upper bound; the calibrated variant further tightens this gap. These results connect spectral coverage to stable ICL, bridge theory and deployment, and improve the interpretability and reliability of large-scale prompting in realistic finite-sample regimes.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.418330",
        "filter_reason": "这篇论文的核心贡献是研究大语言模型的上下文学习(ICL)能力的稳定性问题，建立了理论界限并提出了估计提示长度的方法。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，这篇论文研究的是LLM的一个基础能力——上下文学习(ICL)的稳定性，而非将LLM作为工具应用到特定领域。ICL是LLM的核心能力之一，论文通过建立非渐近下界和提出两阶段可观测估计器，旨在提高这一基础能力的可靠性和稳定性，这属于改进LLM基础能力的研究范畴。 其次，论文涉及LLM的核心概念(ICL)，虽然未直接提及reasoning、planning等能力方向，但ICL本身与这些通用能力密切相关，因为它是模型适应新任务并进行推理的基础机制。 第三，论文不涉及任何需要排除的领域：没有关注多模态与视觉问题，没有将LLM应用到医疗、化学等特定领域，也没有从应用层面研究模型可靠性问题（如水印、安全等）。 最后，虽然论文关注了ICL的可靠性，但这是从理论角度研究LLM基础能力的稳定性，而非应用层面的可靠性问题，因此不应被排除。 综上所述，这篇论文致力于提高LLM本身的通用推理能力（通过增强ICL稳定性），符合研究目标。"
    },
    {
        "index": "#88",
        "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning",
        "link": "/arxiv/2509.20616",
        "arxiv_id": "2509.20616",
        "authors": "Hanjiang Hu, Changliu Liu, Na Li, Yebin Wang",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in higher multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks with over 30 steps. We also theoretically and empirically validate the strong cross-task generalizability that the models trained on complex tasks can lead to the successful completion of all simpler subtasks.",
        "subjects": "Machine Learning, Systems and Control",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.419365",
        "filter_reason": "这篇论文完全符合研究目标。从核心判断来看，论文的本质是关于改进LLM的基础能力，特别是提出了一种新的训练范式（单轮强化学习）来增强LLM的任务规划和多步推理能力。论文的核心贡献是将复杂的多轮任务规划转化为单轮任务推理问题，并通过Group Relative Policy Optimization (GRPO)进行高效策略优化，这直接提升了LLM的通用推理能力。 从正面指标看，论文明确包含了多个关键主题：Large Language Models (LLMs)、reasoning、task planning、reinforcement learning以及LLM agents，这些都与研究目标高度一致。 从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容，因此不应被排除。 在特殊和模糊情况处理方面，论文提出的是一种通用的智能体框架来增强LLM的任务规划能力，而不是将智能体应用于特定领域，因此符合保留标准。 综上所述，这篇论文直接致力于提高大语言模型本身的通用推理能力，特别是在任务规划和多步推理方面，与研究目标完全一致。"
    },
    {
        "index": "#135",
        "title": "RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training",
        "link": "/arxiv/2509.21009",
        "arxiv_id": "2509.21009",
        "authors": "Wei Gao, Yuheng Zhao, Dakai An, Tianyuan Wu, Lunxi Cao, Shaopan Xiong, Ju Huang, Weixun Wang, Siran Yang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang",
        "summary": "Reinforcement Learning (RL) is a pivotal post-training technique for enhancing the reasoning capabilities of Large Language Models (LLMs). However, synchronous RL post-training often suffers from significant GPU underutilization, referred to as bubbles, caused by imbalanced response lengths within rollout steps. Many RL systems attempt to alleviate this problem by relaxing synchronization, but this can compromise training accuracy. In this paper, we introduce tail batching, a novel rollout scheduling strategy for synchronous RL that systematically consolidates prompts leading to long-tail responses into a small subset of rollout steps (long rounds), while ensuring that the majority of steps (short rounds) involve only balanced, short rollouts. By excluding long responses from short rounds and rescheduling them into a few designated long rounds, tail batching effectively reduces GPU idle time during rollouts and significantly accelerates RL training without sacrificing accuracy. We present RollPacker, a system that fully harnesses the benefits of tail batching through holistic optimizations across all three RL stages: elastic parallelism adaptation for rollout, dynamic resource allocation and scheduling for reward, and stream-based training. Empirical results show that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5 family of LLMs on up to 128 H800 GPUs.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.428818",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"tail batching\"的新型rollout调度策略和RollPacker系统，用于优化强化学习(RL)作为大语言模型(LLM)后训练技术的效率和性能。论文明确指出RL是\"enhancing the reasoning capabilities of Large Language Models (LLMs)\"的关键技术，这与研究目标\"提高大语言模型（LLM）本身的『通用推理能力』\"直接相关。论文不是将LLM作为工具应用到特定领域，而是专注于改进LLM的基础训练方法，特别是强化学习这一提升LLM推理能力的关键技术。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。因此，这篇论文完全符合研究范围。"
    }
]