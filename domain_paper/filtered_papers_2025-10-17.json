[
    {
        "index": "#1",
        "title": "PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction",
        "link": "/arxiv/2510.15863",
        "arxiv_id": "2510.15863",
        "authors": "Simon Yu, Gang Li, Weiyan Shi, Peng Qi",
        "summary": "Large language models (LLMs) are moving beyond static uses and are now powering agents that learn continually during their interaction with external environments. For example, agents can learn reusable skills while navigating web pages or toggling new tools. However, existing methods for skill learning often create skills that are over-specialized to a single website and fail to generalize. We introduce PolySkill, a new framework that enables agents to learn generalizable and compositional skills. The core idea, inspired by polymorphism in software engineering, is to decouple a skill's abstract goal (what it accomplishes) and its concrete implementation (how it is executed). Experiments show that our method (1) improves skill reuse by 1.7x on seen websites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on unseen websites, while reducing steps by over 20%. (3) In self-exploration settings without specified tasks, our framework improves the quality of proposed tasks and enables agents to learn generalizable skills that work across different sites. By enabling the agent to identify and refine its own goals, the PolySkill enhances the agent's ability to learn a better curriculum, leading to the acquisition of more generalizable skills compared to baseline methods. This work provides a practical path toward building agents capable of continual learning in adaptive environments. Our findings show that separating a skill's goal from its execution is a crucial step toward developing autonomous agents that can learn and generalize across the open web continuously.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.CL",
        "crawl_time": "2025-10-20T11:00:04.930152",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献与您的筛选标准高度契合。以下是详细的判断过程： **第一步：核心判断——论文的本质是什么？** - **保留 (Keep)**。这篇论文的本质是构建一个新的框架 **PolySkill**，其核心目标是改进LLM智能体的学习能力。它不是将现有智能体作为一个黑盒工具去解决网页浏览等应用问题，而是深入到智能体内部，提出了一种新的技能学习和抽象机制。这直接命中了“构建、改进或演化LLM智能体的方法论或新框架”这一核心保留标准。 **第二步：正面指标——论文是否包含我的核心关注点？** - 该论文命中了多个核心指标： - **核心范式**: 论文明确以 `LLM-based Agents` 为研究对象。 - **智能体能力**: 论文探讨了 `Tool Use`（提及“toggling new tools”），并通过学习可复用的技能来增强智能体的规划和执行能力。 - **自我演化**: 这是最突出的亮点。论文明确提到了 `self-exploration`（自我探索）、`identify and refine its own goals`（识别和完善自身目标）、`learn a better curriculum`（学习更好的课程）以及 `continual learning`（持续学习）。这些都是“自我演化”方向下的核心概念。 **第三步：排除标准——是否为我的研究焦点之外？** - 论文内容与排除标准无关。它没有讨论安全对齐问题，也没有将视觉或多模态作为研究核心（虽然应用场景是网页，但核心贡献是抽象的技能学习框架，而非视觉理解技术）。 **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化”方向的典型范例。它提出了一种通用的自我演化机制（通过多态抽象和自我探索来学习泛化技能），并应用在网页浏览这一具体环境中。根据您的规则“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域，也应该保留”，这篇论文应该被保留。 **综合判断** 论文的核心贡献是 **PolySkill** 框架，旨在解决LLM智能体在学习技能时面临的过拟合和泛化能力差的问题。其核心思想——将技能的抽象目标与具体实现解耦——是一种对智能体内部学习机制的深刻改进。更重要的是，该框架赋予了智能体在无任务指定环境下的自我探索、自我完善和持续学习的能力。 因此，这篇论文不仅属于您关注的 **“单智能体”** 范畴（改进其技能学习和工具使用能力），更深度触及了 **“自我演化”** 这一前沿方向。它是一篇关于如何构建更智能、更具适应性和演化能力的LLM智能体的高质量研究，完全符合您的筛选要求。"
    },
    {
        "index": "#5",
        "title": "Paper2Web: Let's Make Your Paper Alive!",
        "link": "/arxiv/2510.15842",
        "arxiv_id": "2510.15842",
        "authors": "Yuhang Chen, Tianpeng Lv, Siyi Zhang, Yixiang Yin, Yao Wan, Philip S. Yu, Dongping Chen",
        "summary": "Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation.",
        "subjects": "Computation and Language, Computer Vision and Pattern Recognition",
        "date": "2025-10-17",
        "category": "cs.CL",
        "crawl_time": "2025-10-20T11:00:04.932591",
        "filter_reason": "这篇论文符合我的研究范围，核心判断依据如下： 1.  **第一步：核心判断——保留。** 论文的核心贡献并非简单地应用LLM生成网页，而是提出了一个名为 **PWAgent** 的**自主智能体框架**。这个框架的核心机制是**“迭代地优化”**内容和布局。这表明论文的本质是构建一个具备自我完善能力的智能体，而不是一个一次性的、非演化的应用工具。这直接命中了我研究目标中的“自我演化”方向。 2.  **第二步：正面指标——高度相关。** 论文明确包含了我的核心关注点： *   **核心范式**: `Agentic AI`, `LLM-based Agents`。论文标题和摘要中直接使用了 \"Agent\"。 *   **智能体能力**: `Tool Use / Tool Augmentation`，摘要明确提到PWAgent通过 \"MCP tools\" 进行工作。 *   **演化机制**: `Self-Evolving`, `Self-Refine`, `Iterative Improvement`。摘要中的关键描述 \"iteratively refines both content and layout\" 完美契合这些关键词，说明该智能体具备通过反馈进行自我迭代和改进的能力。 3.  **第三步：排除标准——未触发。** 论文的主要贡献不是关于安全、对齐、可解释性或多模态模型本身。虽然它生成的网站是多媒体的，但研究的核心是生成过程的智能体框架，而不是新的视觉或多模态技术。因此，没有触犯任何排除标准。 4.  **第四步：处理特殊和模糊情况——符合核心规则。** 这篇论文是“自我演化的应用”这一特殊情况的典型范例。虽然其应用场景是“学术网页生成”（一个特定领域），但论文的**核心贡献是提出了一种新的“自我演化”机制**。根据我设定的核心规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。” 因此，这篇论文应该被保留。 **最终决策**: 综合来看，这篇论文的标题虽然是应用导向的，但其核心内容是关于构建一个能够使用工具并进行**迭代式自我优化**的LLM智能体。这完全符合我“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的核心目标，特别是聚焦于“自我演化”这一关键方向。因此，这篇论文高度相关，应被保留。"
    },
    {
        "index": "#3",
        "title": "Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation",
        "link": "/arxiv/2510.15624",
        "arxiv_id": "2510.15624",
        "authors": "Ed Li, Junyu Ren, Xintian Pan, Cat Yan, Chuanhao Li, Dirk Bergemann, Zhuoran Yang",
        "summary": "The automation of scientific discovery represents a critical milestone in Artificial Intelligence (AI) research. However, existing agentic systems for science suffer from two fundamental limitations: rigid, pre-programmed workflows that cannot adapt to intermediate findings, and inadequate context management that hinders long-horizon research. We present \\texttt{freephdlabor}, an open-source multiagent framework featuring \\textit{fully dynamic workflows} determined by real-time agent reasoning and a \\coloremph{\\textit{modular architecture}} enabling seamless customization -- users can modify, add, or remove agents to address domain-specific requirements. The framework provides comprehensive infrastructure including \\textit{automatic context compaction}, \\textit{workspace-based communication} to prevent information degradation, \\textit{memory persistence} across sessions, and \\textit{non-blocking human intervention} mechanisms. These features collectively transform automated research from isolated, single-run attempts into \\textit{continual research programs} that build systematically on prior explorations and incorporate human feedback. By providing both the architectural principles and practical implementation for building customizable co-scientist systems, this work aims to facilitate broader adoption of automated research across scientific domains, enabling practitioners to deploy interactive multiagent systems that autonomously conduct end-to-end research -- from ideation through experimentation to publication-ready manuscripts.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning, Multiagent Systems",
        "date": "2025-10-17",
        "category": "cs.MA",
        "crawl_time": "2025-10-20T11:00:04.141851",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出一个名为 `freephdlabor` 的**开源多智能体框架**。它并非简单地将现有智能体作为工具应用于科学领域，而是针对现有科学智能体的局限性（如工作流僵化、上下文管理不足），提出了一套全新的**架构和方法论**。这完全符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了大量与你研究焦点直接相关的正面指标： *   **核心范式**: 明确提出了 `Multiagent Framework` (多智能体框架)。 *   **智能体能力**: 框架的核心特性包括由“实时智能体推理”决定的“动态工作流”，这直接关联到 `Planning` (规划) 和 `ReAct` 范式。同时，它还强调了 `Memory` (记忆持久性) 和 `Self-Correction` (通过人机交互实现)。 *   **多智能体**: 论文的核心就是 `Multi-Agent Systems`，并提到了 `Communication` (基于工作空间的通信) 和 `Collaboration` (协作)。 *   **演化机制**: 论文最突出的亮点之一是实现了“持续的研究程序”，能够“在先前探索的基础上系统地构建”，这完美契合了 `Self-Evolving` (自我演化) 和 `Iterative Improvement` (迭代改进) 的概念。它将自动化研究从单次尝试转变为可积累、可迭代的长期项目。 3.  **第三步：排除标准——未触发** 论文的主要贡献是关于智能体的架构和演化机制，而非安全、对齐或可解释性。同时，它也未涉及多模态或视觉作为研究核心。 4.  **第四步：处理特殊和模糊情况——符合例外规则** 这篇论文是“自我演化的应用”这一例外情况的绝佳范例。虽然它的应用领域是“科学自动化”，但其**核心贡献是提出一种新的“自我演化”机制**（即持续、迭代、可积累的研究框架），而不是仅仅展示一个应用结果。因此，根据你的规则，这种提出新机制的论文应该被保留。 **总结**: 该论文的核心是构建一个新颖的、支持持续演化的多智能体框架。它直接解决了多智能体系统中的协作、通信、记忆和规划问题，并引入了强大的自我演化机制。这与你的研究课题“LLM智能体及其演化”中的“多智能体”和“自我演化”两个方向高度契合，是一篇非常前沿且相关的论文。"
    },
    {
        "index": "#26",
        "title": "CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs",
        "link": "/arxiv/2510.15455",
        "arxiv_id": "2510.15455",
        "authors": "Gucongcong Fan, Chaoyue Niu, Chengfei Lyu, Fan Wu, Guihai Chen",
        "summary": "Mobile agents rely on Large Language Models (LLMs) to plan and execute tasks on smartphone user interfaces (UIs). While cloud-based LLMs achieve high task accuracy, they require uploading the full UI state at every step, exposing unnecessary and often irrelevant information. In contrast, local LLMs avoid UI uploads but suffer from limited capacity, resulting in lower task success rates. We propose $\\textbf{CORE}$, a $\\textbf{CO}$llaborative framework that combines the strengths of cloud and local LLMs to $\\textbf{R}$educe UI $\\textbf{E}$xposure, while maintaining task accuracy for mobile agents. CORE comprises three key components: (1) $\\textbf{Layout-aware block partitioning}$, which groups semantically related UI elements based on the XML screen hierarchy; (2) $\\textbf{Co-planning}$, where local and cloud LLMs collaboratively identify the current sub-task; and (3) $\\textbf{Co-decision-making}$, where the local LLM ranks relevant UI blocks, and the cloud LLM selects specific UI elements within the top-ranked block. CORE further introduces a multi-round accumulation mechanism to mitigate local misjudgment or limited context. Experiments across diverse mobile apps and tasks show that CORE reduces UI exposure by up to 55.6% while maintaining task success rates slightly below cloud-only agents, effectively mitigating unnecessary privacy exposure to the cloud. The code is available at https://github.com/Entropy-Fighter/CORE.",
        "subjects": "Computation and Language",
        "date": "2025-10-17",
        "category": "cs.CL",
        "crawl_time": "2025-10-20T11:00:04.964953",
        "filter_reason": "这篇论文符合你的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献是提出一个名为 **CORE** 的新框架。它不是简单地将现有智能体应用到一个新领域，而是针对移动智能体在执行任务时遇到的核心问题（隐私暴露与性能的权衡），提出了一种全新的**架构和方法论**。该框架通过“协同规划”和“协同决策”等机制来改进智能体的内部工作流程，这完全符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `LLM-based Agents` (论文明确指出是为 \"mobile agents\" 设计的框架)。 - **智能体能力**: `Planning` (论文详细描述了 `Co-planning` 机制，这是对智能体规划能力的改进)。同时，智能体与UI交互本身就是一种 `Tool Use`，而该框架优化了这一过程。 - **多智能体**: 虽然这不是典型的多个独立智能体间的协作，但论文的核心创新点在于 **`Collaboration`**（云端与本地LLM的协作）。这种内部协作机制是提升单一智能体性能的一种新颖架构，与你的“多智能体”研究方向中的协作思想在技术上有共通之处，值得作为前沿研究关注。 3.  **第三步：排除标准** - **安全与对齐**: 论文的目标是“减少UI暴露”和“保护隐私”，这触及了安全领域。但是，根据筛选标准，只有当论文的**主要贡献**是关于安全机制本身时才需排除。本文的贡献在于**提出一种新的智能体协作框架**来**实现**隐私保护，其核心是智能体的架构设计，而非隐私保护理论或技术分析。因此，它不在此排除项内。 - **多模态与视觉**: 论文处理的是UI（用户界面），这属于视觉信息。但根据筛选标准，只要它们是作为“智能体感知环境的工具”而不是研究核心，就不排除。在本文中，UI是智能体操作的环境和对象，论文的核心是智能体如何处理和决策关于这些UI信息，而不是提出新的视觉模型。因此，这不构成排除理由。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文完全符合“保留”条件。它提出的 `Co-planning` 和 `Co-decision-making` 正是关于“智能体如何进行规划或在复杂任务中进行多步推理”的新方法，是对现有Agentic框架（如ReAct）的一种改进和扩展。 5.  **第五步：最终决策** - 综合来看，这篇论文的核心贡献是构建了一个创新的LLM智能体框架（CORE），通过引入云端与本地模型的协作机制，显著改进了移动智能体的规划和决策能力，同时解决了隐私这一关键瓶颈。这直接契合了你“构建、改进或演化 LLM智能体”的核心目标，特别是在“单智能体”的规划和能力增强方面。因此，应判定为 **True**。"
    },
    {
        "index": "#43",
        "title": "Planner and Executor: Collaboration between Discrete Diffusion And Autoregressive Models in Reasoning",
        "link": "/arxiv/2510.15244",
        "arxiv_id": "2510.15244",
        "authors": "Lina Berrayana, Ahmed Heakl, Muhammad Abdullah Sohail, Thomas Hofmann, Salman Khan, Wei Chen",
        "summary": "Current autoregressive language models (ARMs) achieve high accuracy but require long token sequences, making them costly. Discrete diffusion language models (DDLMs) enable parallel and flexible generation within a fixed number of steps and have recently emerged for their strong performance in complex reasoning and long-term planning tasks. We present a study exploring hybrid architectures that couple DDLMs with ARMs to assess whether their collaboration can yield complementary benefits. We first examine collaboration in text space, where one model plans the reasoning process and another executes the final answer based on that plan. We then extend this setup to latent-space communication, introducing a learned projector that maps DDLM latents into the ARM's embedding space, potentially bypassing some of the text-generation limitations of diffusion models. We find that shifting DDLM --> ARM communication from text space to latent space yields significant accuracy gains, for example increasing from 27.0% to 54.0% on DART-5 and from 0.0% to 14.0% on AIME24. We also find that combining a DDLM planner with an ARM executor can provide substantial computational savings with little to no impact on accuracy. For example, the latent-space pipeline, using 64 tokens for planning and roughly 5 for execution, surpasses Qwen3.1-7B on DART-5 and AIME, despite Qwen using 44 times more tokens. Overall, our study offers new insights into reasoning with DDLMs and highlights their potential in hybrid architectures.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-17",
        "category": "cs.CL",
        "crawl_time": "2025-10-20T11:00:05.000355",
        "filter_reason": "这篇论文完全符合你的研究范围。 **第一步：核心判断** 这篇论文的本质是构建一种新的混合架构（Hybrid Architecture），用于改进复杂推理任务。其核心机制是“规划者-执行者”范式，其中DDLM负责生成推理计划，ARM根据计划生成最终答案。这并非将已有智能体作为工具去解决某个特定领域的问题，而是对智能体的核心架构和工作流程本身的创新。因此，它通过了第一步的核心判断，属于“构建、改进LLM智能体的方法论或新框架”，应予以保留。 **第二步：正面指标** 论文明确包含了你的核心关注点： *   **核心范式**: 论文虽然未直接使用“Agentic AI”一词，但其“Planner and Executor”架构是Agentic AI的典型范式。 *   **智能体能力**: 论文的标题和摘要反复强调了 **`Planning`**（规划）。整个研究就是围绕如何通过分工协作来优化智能体的规划与执行能力展开的。 *   **多智能体概念**: 论文提到了两个模型间的 **`Collaboration`**（协作），虽然这更像是单个智能体内部模块的协作，而非多个独立智能体的社会性互动，但其理念与多智能体系统的协作思想相通，显示了其先进性。 **第三步：排除标准** 论文内容不涉及安全、对齐、可解释性或视觉多模态等排除方向。因此，第三步的排除标准不适用。 **第四步：处理特殊和模糊情况** 这篇论文的关键在于 **“推理/规划”** 的界定。 *   **保留**: 该论文的研究属于“智能体如何进行规划或在复杂任务中进行多步推理”。它提出了一种结构化的方法（规划-执行），并探索了不同模块间的通信机制（文本空间 vs. 潜在空间），这是一种典型的Agentic框架创新。它超越了单纯提升LLM基础数学或逻辑推理能力的范畴，而是构建了一个具备规划和执行能力的系统。因此，根据规则，应当保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一种新颖的“规划者-执行者”混合架构，通过协同DDLM和ARM两种模型，显著提升了智能体在复杂推理任务中的表现和效率。这完全契合你研究课题中“单智能体”方向下的“规划”子方向，属于对LLM智能体构建与改进的前沿探索。因此，最终判断为 **True**。"
    },
    {
        "index": "#35",
        "title": "AutoGraph-R1: End-to-End Reinforcement Learning for Knowledge Graph Construction",
        "link": "/arxiv/2510.15339",
        "arxiv_id": "2510.15339",
        "authors": "Hong Ting Tsang, Jiaxin Bai, Haoyu Huang, Qiao Xiao, Tianshi Zheng, Baixuan Xu, Shujie Liu, Yangqiu Song",
        "summary": "Building effective knowledge graphs (KGs) for Retrieval-Augmented Generation (RAG) is pivotal for advancing question answering (QA) systems. However, its effectiveness is hindered by a fundamental disconnect: the knowledge graph (KG) construction process is decoupled from its downstream application, yielding suboptimal graph structures. To bridge this gap, we introduce AutoGraph-R1, the first framework to directly optimize KG construction for task performance using Reinforcement Learning (RL). AutoGraph-R1 trains an LLM constructor by framing graph generation as a policy learning problem, where the reward is derived from the graph's functional utility in a RAG pipeline. We design two novel, task-aware reward functions, one for graphs as knowledge carriers and another as knowledge indices. Across multiple QA benchmarks, AutoGraph-R1 consistently enables graph RAG methods to achieve significant performance gains over using task-agnostic baseline graphs. Our work shows it is possible to close the loop between construction and application, shifting the paradigm from building intrinsically ``good'' graphs to building demonstrably ``useful'' ones.",
        "subjects": "Computation and Language",
        "date": "2025-10-17",
        "category": "cs.CL",
        "crawl_time": "2025-10-20T11:00:04.979856",
        "filter_reason": "这篇论文符合我的研究范围，核心依据在于其贡献本质上属于 **自我演化** 的范畴。 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献并非简单地将LLM应用于知识图谱构建这一特定领域，而是提出了一个名为 **AutoGraph-R1 的新框架**。该框架的核心机制是使用强化学习（RL）来训练一个LLM，使其构建的知识图谱能够根据下游任务（RAG）的性能进行**端到端的优化和迭代**。这完全符合“构建、改进或演化 LLM智能体”的目标，特别是“自我演化”方向。它不是一次性的应用，而是一个持续改进的闭环系统。 2.  **第二步：正面指标** - 论文包含了多个核心关注点。最突出的是 **`Self-Evolving`**、**`Self-Improvement`** 和 **`Iterative Improvement`**。整个强化学习循环就是一个典型的自我演化机制：LLM constructor（智能体）执行动作（构建图谱），环境（RAG系统）给予反馈（QA性能），智能体根据奖励更新策略（优化自身）。此外，**`Reinforcement Learning`** 作为实现演化的核心算法，也是一个强烈的正面指标。 3.  **第三步：排除标准** - 论文的研究焦点不在于安全、对齐或多模态，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - 这篇论文是 **“自我演化的应用”** 这一特殊情况的完美例证。根据规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。” - **核心贡献是机制**：AutoGraph-R1的核心是其**通过RL进行闭环优化的方法论**，而不是知识图谱构建本身。 - **应用领域是载体**：知识图谱构建和RAG问答是验证该演化机制有效性的载体和应用场景。 - 因此，尽管它看起来像是一个“应用”论文，但其本质是提出了一种能让LLM在特定任务上自我完善的新范式，这与我的研究目标高度一致。 **最终决策**：综合分析，这篇论文虽然不涉及典型的单智能体规划或多智能体协作，但它提出了一种新颖的、基于强化学习的自我演化框架，使LLM能够根据任务反馈进行迭代式自我完善。这完全符合我研究课题中的“自我演化”方向，因此应该被保留。"
    },
    {
        "index": "#49",
        "title": "Continual Learning via Sparse Memory Finetuning",
        "link": "/arxiv/2510.15103",
        "arxiv_id": "2510.15103",
        "authors": "Jessy Lin, Luke Zettlemoyer, Gargi Ghosh, Wen-Tau Yih, Aram Markosyan, Vincent-Pierre Berges, Barlas Oğuz",
        "summary": "Modern language models are powerful, but typically static after deployment. A major obstacle to building models that continually learn over time is catastrophic forgetting, where updating on new data erases previously acquired capabilities. Motivated by the intuition that mitigating forgetting is challenging because trainable parameters are shared across all tasks, we investigate whether sparse parameter updates can enable learning without catastrophic forgetting. We introduce sparse memory finetuning, leveraging memory layer models (Berges et al., 2024), which are sparsely updated by design. By updating only the memory slots that are highly activated by a new piece of knowledge relative to usage on pretraining data, we reduce interference between new knowledge and the model's existing capabilities. We evaluate learning and forgetting compared to full finetuning and parameter-efficient finetuning with LoRA on two question answering tasks. We find that sparse memory finetuning learns new knowledge while exhibiting substantially less forgetting: while NaturalQuestions F1 drops by 89% after full finetuning on new facts and 71% with LoRA, sparse memory finetuning yields only an 11% drop with the same level of new knowledge acquisition. Our results suggest sparsity in memory layers offers a promising path toward continual learning in large language models.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-20T11:00:05.008877",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为“稀疏记忆微调”的新方法，旨在解决大型语言模型在持续学习过程中的“灾难性遗忘”问题。 根据筛选标准，我的判断过程如下： 1.  **第一步：核心判断——保留。** 论文的本质不是将LLM作为工具应用于某个特定领域，也不是提升LLM的基础推理能力。它的核心是提出一种**让LLM能够持续学习新知识而不遗忘旧知识的方法论**。这直接关联到我的研究焦点 **“自我演化”**。一个能够持续学习、迭代更新的模型，正是自我演化的基础和关键体现。因此，该论文的核心贡献是构建和改进LLM的自我演化机制，符合保留标准。 2.  **第二步：正面指标——高度相关。** 论文摘要中明确提到了多个核心关注点： *   **演化机制**: 论文的核心是解决“Continual Learning”（持续学习）问题，这与 `Self-Improvement`（自我完善）和 `Iterative Improvement`（迭代改进）的概念高度一致。 *   **智能体能力**: 论文的方法基于 `Memory`（记忆）机制，具体是通过“记忆层”和“记忆槽”来实现。记忆是智能体（无论是单智能体还是多智能体）的核心能力之一，用于积累经验和知识。 3.  **第三步：排除标准——未触犯。** 论文的主要贡献是关于持续学习的算法机制，而非安全、对齐、可解释性或多模态。因此，它不在排除的范围内。 4.  **第四步：特殊和模糊情况——不适用。** 该论文本身就是关于“自我演化”机制的提出，而不是其应用，因此无需进行例外情况的判断。 **最终决策**：综合以上分析，这篇论文的核心贡献在于提出了一种新颖的、基于稀疏记忆的持续学习方法，这直接解决了LLM自我演化的一个核心障碍——灾难性遗忘。其研究内容完全落在“自我演化”这一核心研究方向上，并涉及了“记忆”这一关键的智能体能力。因此，这篇论文高度符合我的研究范围，应被保留。"
    },
    {
        "index": "#56",
        "title": "SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation",
        "link": "/arxiv/2510.15682",
        "arxiv_id": "2510.15682",
        "authors": "Ines Besrour, Jingbo He, Tobias Schreieder, Michael Färber",
        "summary": "We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy multi-agent retrieval-augmented generation (RAG) framework for scientific question answering (QA) with large language models (LLMs). SQuAI addresses key limitations of existing RAG systems in the scholarly domain, where complex, open-domain questions demand accurate answers, explicit claims with citations, and retrieval across millions of scientific documents. Built on over 2.3 million full-text papers from arXiv.org, SQuAI employs four collaborative agents to decompose complex questions into sub-questions, retrieve targeted evidence via hybrid sparse-dense retrieval, and adaptively filter documents to improve contextual relevance. To ensure faithfulness and traceability, SQuAI integrates in-line citations for each generated claim and provides supporting sentences from the source documents. Our system improves faithfulness, answer relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG baseline. We further release a benchmark of 1,000 scientific question-answer-evidence triplets to support reproducibility. With transparent reasoning, verifiable citations, and domain-wide scalability, SQuAI demonstrates how multi-agent RAG enables more trustworthy scientific QA with LLMs.",
        "subjects": "Information Retrieval, Computation and Language",
        "date": "2025-10-17",
        "category": "cs.CL",
        "crawl_time": "2025-10-20T11:00:05.028157",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。 详细判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出了一个名为 SQuAI 的**多智能体检索增强生成（RAG）框架**。它不是简单地将一个已有的智能体框架应用到科学问答领域，而是**设计和构建了一个由四个协作智能体组成的新系统**，以解决现有RAG系统在学术领域的局限性。论文的核心是关于**构建和改进多智能体系统（Multi-Agent Systems）**的方法论，因此通过了第一步的核心判断。 2.  **第二步：正面指标——高度相关** 论文包含了多个核心关注点： *   **核心范式**: 明确提到了 `Multi-Agent Systems (MAS)`。 *   **多智能体**: 论文的精髓在于 `Collaboration`（协作），标题和摘要中多次强调“four collaborative agents”。 *   **智能体能力**: 智能体执行了 `Planning`（将复杂问题分解为子问题）和 `Tool Use`（使用检索工具获取证据）。 3.  **第三步：排除标准——未触犯** 论文提到了 `faithfulness`（忠实度）和 `trustworthy`（可信度），这可能让人联想到对齐或幻觉问题。但是，根据筛选标准，论文的**主要贡献并非研究如何实现安全或对齐**，而是提出一个多智能体架构来**实现**这些特性。忠实度和可追溯性（通过引用）是其多智能体框架设计带来的**结果**，而非研究本身的核心焦点。因此，这不触犯排除标准。 4.  **第四步：处理特殊和模糊情况** 这篇论文位于“应用”和“方法论”的交叉点，但更偏向后者。虽然应用领域是“科学问答”，但其核心价值在于提出的**多智能体协作框架**。这符合第一步中“保留：如果论文的核心是关于构建……多智能体系统（Multi-Agent Systems）的方法论或新框架”的规则。它不是“非演化型应用”，因为它创新了智能体系统本身的结构和协作方式，而不是仅仅调用一个现有框架。 **最终决策**: 论文的核心贡献在于**构建了一个新颖的多智能体协作框架（SQuAI）**来执行复杂的科学问答任务。该框架通过智能体间的分工与协作（问题分解、检索、过滤、生成），实现了对现有方法的改进。这完全符合研究课题中“**多智能体**”方向的核心目标，即研究智能体间的协作、通信等机制。因此，这篇论文与你的研究范围高度相关，应予以保留。"
    },
    {
        "index": "#58",
        "title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism",
        "link": "/arxiv/2510.15600",
        "arxiv_id": "2510.15600",
        "authors": "Haoran Sun, Yankai Jiang, Zhenyu Tang, Yaning Pan, Shuang Gu, Zekai Lin, Lilong Wang, Wenjie Lou, Lei Liu, Lei Bai, Xiaosong Wang",
        "summary": "The foundation of reproducible science lies in protocols that are precise, logically ordered, and executable. The autonomous generation of these protocols through natural language queries could greatly improve the efficiency of the reproduction process. However, current leading large language models (LLMs) often generate incomplete or inconsistent protocols, limiting their utility. To address this limitation, we first introduce SciRecipe, a large-scale dataset of over 12K structured protocols spanning 27 biological subfields and encompassing both comprehension and problem-solving tasks. To further improve protocol generation, we propose the \"Sketch-and-Fill\" paradigm, which separates analysis, structuring, and expression to ensure each step is explicit and verifiable. Complementing this, the structured component-based reward mechanism evaluates step granularity, action order, and semantic fidelity, aligning model optimization with experimental reliability. Building on these components, we develop Thoth, trained through a staged Knowledge-to-Action process that progresses from knowledge acquisition to operational reasoning and ultimately to robust, executable protocol generation. Across multiple benchmarks, Thoth consistently surpasses both proprietary and open-source LLMs, achieving significant improvements in step alignment, logical sequencing, and semantic accuracy. Our approach paves the way for reliable scientific assistants that bridge knowledge with experimental execution. All data, code, and models will be released publicly.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-17",
        "category": "cs.CL",
        "crawl_time": "2025-10-20T11:00:05.029279",
        "filter_reason": "这篇论文的核心贡献是构建了一个名为 Thoth 的智能体系统，用于解决生物实验方案生成的复杂任务，完全符合您的研究目标。我的判断过程如下： **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将现有LLM应用于生物学领域。它的核心贡献在于提出了一套全新的方法论来**构建和改进一个LLM智能体（Thoth）**，使其能够执行复杂的多步任务。具体而言，其贡献包括： 1.  **\"Sketch-and-Fill\" 范式**：这是一个新的智能体规划和推理框架，将复杂的任务分解为分析、结构和表达三个明确的步骤。 2.  **结构化组件奖励机制**：这是一个用于优化和评估智能体输出质量的机制，确保其生成的方案具有逻辑性和可执行性。 3.  **一个完整的智能体系统**：论文开发的 Thoth 不是一个简单的微调模型，而是一个通过 \"Knowledge-to-Action\" 流程训练的、旨在连接知识与行动的“科学助手”。 因此，它不属于“非演化型应用”，因为其核心在于提出新**框架**和**机制**来构建智能体，而非简单使用。 **第二步：正面指标** 论文包含了多个核心关注点： - **Agentic AI / LLM-based Agents**: 论文目标明确是构建一个“自主生成”方案的“可靠科学助手”，这完全符合Agentic AI的定义。 - **Planning**: “Sketch-and-Fill” 范式就是一种高级的规划和推理方法，它将生成任务结构化，确保步骤的逻辑顺序，这与智能体的规划能力直接相关。 - **Self-Improvement / Iterative Improvement**: 结构化奖励机制和分阶段的训练过程（Knowledge-to-Action）本质上是一种通过反馈进行迭代改进和优化的机制，是智能体自我完善的一种体现。 **第三步：排除标准** 论文的主要焦点不在于安全、对齐或可解释性，也不涉及多模态或视觉内容。因此，没有触犯排除标准。 **第四步：特殊和模糊情况处理** - **推理/规划**: 该论文属于“保留”情况。它研究的不是LLM本身的基础逻辑或数学能力，而是智能体如何进行结构化的、面向行动的推理和规划，以生成可执行的方案。这完全符合Agentic框架下的规划研究。 - **自我演化的应用**: 这是一个典型的“保留”例外情况。尽管论文应用于生物学领域，但其核心是提出一种构建智能体的新机制（“Sketch-and-Fill”和奖励机制），而不是将现有框架直接应用。因此，它符合您设定的“核心是新机制，即使应用在特定领域也应保留”的规则。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一种构建专门化LLM智能体的新框架，该框架包含创新的规划范式和优化机制。它虽然应用在生物领域，但其方法论本身是对Agentic AI的推进，尤其是在**单智能体的规划和行动生成**方面。因此，这篇论文与您的研究范围高度相关，应该被保留。"
    },
    {
        "index": "#70",
        "title": "Internalizing World Models via Self-Play Finetuning for Agentic RL",
        "link": "/arxiv/2510.15047",
        "arxiv_id": "2510.15047",
        "authors": "Shiqi Chen, Tongyao Zhu, Zian Wang, Jinghan Zhang, Kangrui Wang, Siyang Gao, Teng Xiao, Yee Whye Teh, Junxian He, Manling Li",
        "summary": "Large Language Models (LLMs) as agents often struggle in out-of-distribution (OOD) scenarios. Real-world environments are complex and dynamic, governed by task-specific rules and stochasticity, which makes it difficult for LLMs to ground their internal knowledge in those dynamics. Under such OOD conditions, vanilla RL training often fails to scale; we observe Pass@k--the probability that at least one of (k) sampled trajectories succeeds--drops markedly across training steps, indicating brittle exploration and limited generalization. Inspired by model-based reinforcement learning, we hypothesize that equipping LLM agents with an internal world model can better align reasoning with environmental dynamics and improve decision-making. We show how to encode this world model by decomposing it into two components: state representation and transition modeling. Building on this, we introduce SPA, a simple reinforcement learning framework that cold-starts the policy via a Self-Play supervised finetuning (SFT) stage to learn the world model by interacting with the environment, then uses it to simulate future states prior to policy optimization. This simple initialization outperforms the online world-modeling baseline and greatly boosts the RL-based agent training performance. Experiments across diverse environments like Sokoban, FrozenLake, and Sudoku show that our approach significantly improves performance. For example, SPA boosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake score from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-20T11:00:05.068126",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是详细的判断过程： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出一个名为**SPA**的新框架，用于**构建和改进LLM智能体**。它不是简单地将现有智能体应用于某个领域，而是专注于解决智能体在动态环境中的一个根本性难题——泛化能力差，并提供了一套新的方法论（通过Self-Play学习内部世界模型）。这完全符合“核心贡献在于构建、改进或演化LLM智能体”的要求。 2.  **第二步：正面指标——高度相关** 论文命中了您几乎所有的核心关注点： *   **单智能体:** *   `Planning`: 论文的核心是让智能体通过内部世界模型来“**模拟未来状态**”，这是一种高级的规划和决策能力。 *   `Memory`: “**内部世界模型**”本身就可以看作是一种关于环境动态的结构化记忆，它超越了简单的对话历史记忆。 *   **自我演化:** *   `Self-Improvement` / `Self-Evolving`: 论文的核心机制“**Self-Play Finetuning**”是一种典型的自我演化过程。智能体通过与环境的自我交互来学习和完善其世界模型，这正是“通过经验进行自我完善和迭代”的体现。 *   **核心范式:** 论文明确聚焦于 `Agentic RL` 和 `LLM-based Agents`。 3.  **第三步：排除标准——未触发** 论文的研究重点在于提升智能体的性能和泛化能力，完全不涉及安全、对齐、可解释性或多模态等排除项。它的实验环境是Sokoban、FrozenLake等标准决策环境，而非特定应用领域。 4.  **第四步：处理特殊和模糊情况——完全适用** *   **推理/规划:** 该论文正是“关于智能体如何进行规划或在复杂任务中进行多步推理”的典型范例。它的贡献是智能体框架层面的，而不是改进LLM本身的数学或逻辑推理。 *   **自我演化的应用:** 论文的核心就是提出一种新的“自我演化”机制（Self-Play Finetuning），并验证了其有效性。因此，即使它应用在特定领域，也应该保留，何况它是在通用环境中验证的。 **最终决策:** 这篇论文的核心贡献在于提出了一种名为**SPA**的新框架，它通过**Self-Play（自我演化）**机制为LLM智能体构建**内部世界模型（一种高级记忆）**，并利用该模型进行**规划**。这精准地命中了您研究目标的三大方向中的两个：**单智能体**（规划、记忆）和**自我演化**。它是一项关于智能体如何学习和适应环境的基础性方法论研究，而非简单的应用。因此，这篇论文是您研究课题的**高度相关**的前沿文献，应予保留。"
    },
    {
        "index": "#14",
        "title": "ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations",
        "link": "/arxiv/2510.15700",
        "arxiv_id": "2510.15700",
        "authors": "Alex Gu, Bartosz Piotrowski, Fabian Gloeckle, Kaiyu Yang, Aram H. Markosyan",
        "summary": "Neural theorem proving has advanced rapidly in the past year, reaching IMO gold-medalist capabilities and producing formal proofs that span thousands of lines. Although such proofs are mechanically verified by formal systems like Lean, their excessive length renders them difficult for humans to comprehend and limits their usefulness for mathematical insight. Proof simplification is therefore a critical bottleneck. Yet, training data for this task is scarce, and existing methods -- mainly agentic scaffolding with off-the-shelf LLMs -- struggle with the extremely long proofs generated by RL-trained provers. We introduce ProofOptimizer, the first language model trained to simplify Lean proofs without requiring additional human supervision. ProofOptimizer is trained via expert iteration and reinforcement learning, using Lean to verify simplifications and provide training signal. At inference time, it operates within an iterative proof-shortening workflow, progressively reducing proof length. Experiments show that ProofOptimizer substantially compresses proofs generated by state-of-the-art RL-trained provers on standard benchmarks, reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check faster in Lean and further improve downstream prover performance when reused as training data for supervised finetuning.",
        "subjects": "Machine Learning, Artificial Intelligence, Programming Languages",
        "date": "2025-10-17",
        "category": "cs.LG",
        "crawl_time": "2025-10-20T11:00:05.325043",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献与“自我演化”和“单智能体”方向高度契合。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质并非简单地将LLM应用于数学领域，而是提出了一种**新的方法论和框架**来构建和训练一个能够自我完善的智能体。其核心贡献是 `ProofOptimizer` 这个模型以及通过“专家迭代和强化学习”来训练它的机制。这个机制利用环境（Lean验证器）的反馈来迭代式地改进模型自身，这本质上是一个**自我演化**的过程。它不是在解决一个固定的数学问题，而是在学习一项**“如何简化证明”**的动态技能，这完全符合“构建、改进或演化LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **自我演化**: 论文的核心训练方法是“专家迭代和强化学习”，模型通过与环境的交互和反馈进行自我完善，这是典型的自我演化机制。 - **工具使用**: `Lean` 形式化验证器在这里扮演了关键的工具和环境角色。智能体（ProofOptimizer）执行动作（简化证明），然后使用这个工具来验证结果并获得奖励信号，这是一个清晰的工具使用闭环。 - **迭代改进**: 论文明确提到了“iterative proof-shortening workflow”，这体现了智能体在执行任务时的规划和多步执行能力。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐或多模态等问题，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一例外情况的完美范例。虽然它应用于数学定理证明这一特定领域，但其**核心贡献是提出了一种新颖的“自我演化”机制**（通过RL和专家迭代进行训练，利用环境反馈）。根据你的规则，这种情况下应该保留。 - **推理/规划**: 论文的研究内容属于“保留”范畴。它不是在提升LLM的基础数学推理能力，而是在构建一个智能体框架，让这个智能体能够在一个复杂的任务（简化长证明）中进行多步、迭代的规划和执行。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一种通过与环境交互进行自我演化的LLM智能体训练框架。它不仅涉及智能体的工具使用（Lean验证器），更核心的是其自我改进和迭代优化的能力。因此，它精准地命中了你研究课题中的“自我演化”和“单智能体”方向，是一篇高度相关的前沿论文，应当保留。"
    },
    {
        "index": "#32",
        "title": "The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling",
        "link": "/arxiv/2510.15502",
        "arxiv_id": "2510.15502",
        "authors": "Shijia Kang, Muhan Zhang",
        "summary": "Reinforcement learning (RL) has been pivotal in enhancing the reasoning capabilities of large language models (LLMs), but it often suffers from limited exploration and entropy collapse, where models exploit a narrow set of solutions, leading to a loss of sampling diversity and subsequently preventing RL from further improving performance. This issue is exacerbated in parallel sampling methods, where multiple outputs are drawn from the same distribution, potentially causing the model to converge to similar solutions. We propose SESA, a novel SEquential SAmpling framework that mitigates this challenge by generating diverse solution sketches sequentially before expanding them into full reasoning paths. This approach ensures broader exploration by conditioning each new output on previous ones, promoting diversity throughout the process and preventing policy collapse. Our experiments on a synthetic task show that sequential sampling consistently outperforms traditional RL methods in terms of path diversity and recovery from collapse. Further evaluations on real-world tasks demonstrate that SESA improves both the exploration of valid strategies and the overall performance of LLMs. On three agent benchmarks, SESA lifts success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up to an additional $211\\%$ relative improvement over baseline RL), underscoring its exploration advantage. This work introduces a structured approach to exploration, paving the way for more effective and diverse reasoning in RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.LG",
        "crawl_time": "2025-10-20T11:00:05.342072",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断** - **保留 (Keep)**。这篇论文的核心贡献是提出了一种名为SESA（SEquential SAmpling）的新框架。该框架旨在解决强化学习（RL）训练LLM时遇到的“探索不足”和“熵崩溃”问题。其核心机制是通过顺序采样生成多样化的解决方案草图，再将其扩展为完整的推理路径。这本质上是一种**改进LLM在复杂任务中推理和探索能力的通用方法论**，而非针对特定领域的应用。论文明确提到在三个“智能体基准”（agent benchmarks）上进行了测试，并显著提升了成功率，这表明其贡献直接作用于LLM智能体的性能提升，符合“构建、改进或演化LLM智能体”的核心目标。 **第二步：正面指标** - 论文包含了多个核心关注点： - **智能体能力**: 论文的核心是提升LLM的**推理（Reasoning）**和**规划（Planning）**能力。SESA框架通过确保更广泛的探索，直接促进了智能体在复杂任务中寻找有效策略的能力，这与`ReAct`、`ToT`等Agentic框架的目标一致。 - **演化机制**: 论文解决了RL训练中的“策略崩溃”（policy collapse）问题，并帮助模型从崩溃中“恢复”（recovery）。这可以被视为一种**迭代改进（Iterative Improvement）**机制，它通过优化训练过程，使得智能体的策略（policy）能够更有效地演化和提升，而不是陷入局部最优。虽然不是完全自主的自我演化，但它属于“通过经验进行自我完善和迭代”这一大范畴下的关键技术改进。 **第三步：排除标准** - **安全与对齐**: 论文未涉及`Safety`, `Alignment`, `Hallucination`等主题，其焦点是性能和探索，因此不在此排除范围内。 - **多模态与视觉**: 论文是纯文本语言模型研究，不涉及视觉或多模态内容，因此不在此排除范围内。 **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美地符合“保留”条件。它不是在研究如何提升LLM的基础数学或逻辑能力（如通过特定数据集微调），而是在研究**智能体如何进行规划或在复杂任务中进行多步推理**。SESA框架本身就是一个新的、旨在提升推理过程多样性和有效性的Agentic方法论。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种新的框架（SESA），用于**改进LLM智能体的核心推理与规划能力**。它通过解决强化学习中的探索瓶颈问题，显著提升了智能体在多个基准测试中的表现。这完全符合我研究课题中“单智能体 (Agentic)”方向下的“规划”和“自我反思/改进”子方向。因此，最终判断为 **True**。"
    },
    {
        "index": "#157",
        "title": "FIRE: Fact-checking with Iterative Retrieval and Verification",
        "link": "/arxiv/2411.00784",
        "arxiv_id": "2411.00784",
        "authors": "Zhuohan Xie, Rui Xing, Yuxia Wang, Jiahui Geng, Hasan Iqbal, Dhruv Sahnan, Iryna Gurevych, Preslav Nakov",
        "summary": "Fact-checking long-form text is challenging, and it is therefore common practice to break it down into multiple atomic claims. The typical approach to fact-checking these atomic claims involves retrieving a fixed number of pieces of evidence, followed by a verification step. However, this method is usually not cost-effective, as it underutilizes the verification model's internal knowledge of the claim and fails to replicate the iterative reasoning process in human search strategies. To address these limitations, we propose FIRE, a novel agent-based framework that integrates evidence retrieval and claim verification in an iterative manner. Specifically, FIRE employs a unified mechanism to decide whether to provide a final answer or generate a subsequent search query, based on its confidence in the current judgment. We compare FIRE with other strong fact-checking frameworks and find that it achieves slightly better performance while reducing large language model (LLM) costs by an average of 7.6 times and search costs by 16.5 times. These results indicate that FIRE holds promise for application in large-scale fact-checking operations. Our code is available at https://github.com/mbzuai-nlp/fire.git.",
        "subjects": "Information Retrieval, Artificial Intelligence, Computation and Language",
        "date": "2024-10-17",
        "category": "cs.LG",
        "crawl_time": "2025-10-20T11:00:05.440558",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **第一步：核心判断——本质是构建智能体框架** 论文的核心贡献是提出了一个名为FIRE的“新颖的智能体框架”。这并非简单地将LLM作为工具应用于事实核查领域，而是设计了一种新的方法论来让LLM作为一个智能体去执行任务。该框架通过迭代的方式整合证据检索和声明验证，这本身就是对智能体工作流程的构建和改进。因此，它不属于“非演化型应用”或“非Agentic的推理”，应予以保留。 2.  **第二步：正面指标——高度吻合核心关注点** 论文明确包含了多个你关注的核心范式和能力： *   **核心范式**: 摘要中直接使用了 `agent-based framework`，完全命中 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**: *   **规划**: FIRE的核心机制是“决定是提供最终答案还是生成后续的搜索查询”，这是一个典型的自主决策和规划过程。 *   **工具使用**: “生成后续的搜索查询”表明智能体能够主动使用“搜索”这一外部工具来获取信息。 *   **自我反思/修正**: 智能体基于“对当前判断的置信度”来做决策。当置信度不足时，它不会草率给出答案，而是选择继续搜索，这体现了一种基于内部状态的自我反思和修正机制，是迭代改进的基础。 3.  **第四步：处理特殊和模糊情况——属于保留的推理/规划类型** 论文描述的迭代过程（评估置信度 -> 决定行动 -> 执行搜索 -> 重新评估）正是智能体在复杂任务中进行多步推理的典型范例。它不是在提升LLM底层的数学或逻辑能力，而是在构建一个让LLM能够像人一样进行迭代搜索和验证的Agentic框架。这与ReAct、ToT等范式一脉相承，完全符合你保留的标准。 4.  **第三步：排除标准——未触及排除项** 论文的研究焦点是提升事实核查的效率和效果，其核心贡献在于框架设计，而非安全、对齐、可解释性或多模态技术。因此，它没有被任何排除标准所排除。 **总结**: 尽管论文的应用场景是事实核查，但其核心创新点在于提出了一种新的、具备规划、工具使用和自我反思能力的LLM智能体框架。这直接对应了你研究课题中的“单智能体”方向，特别是关于智能体规划和自我反思的子方向。因此，这篇论文是高度相关且应该被保留的前沿研究。"
    },
    {
        "index": "#3",
        "title": "Self-evolving expertise in complex non-verifiable subject domains: dialogue as implicit meta-RL",
        "link": "/arxiv/2510.15772",
        "arxiv_id": "2510.15772",
        "authors": "Richard M. Bailey",
        "summary": "So-called `wicked problems', those involving complex multi-dimensional settings, non-verifiable outcomes, heterogeneous impacts and a lack of single objectively correct answers, have plagued humans throughout history. Modern examples include decisions over justice frameworks, solving environmental pollution, planning for pandemic resilience and food security. The use of state-of-the-art artificial intelligence systems (notably Large Language Model-based agents) collaborating with humans on solving such problems is being actively explored. While the abilities of LLMs can be improved by, for example, fine-tuning, hand-crafted system prompts and scaffolding with external tools, LLMs lack endogenous mechanisms to develop expertise through experience in such settings. This work address this gap with Dialectica, a framework where agents engage in structured dialogue on defined topics, augmented by memory, self-reflection, and policy-constrained context editing. Formally, discussion is viewed as an implicit meta-reinforcement learning process. The `dialogue-trained' agents are evaluated post-hoc using judged pairwise comparisons of elicited responses. Across two model architectures (locally run Qwen3:30b and OpenAI's o4-mini) results show that enabling reflection-based context editing during discussion produces agents which dominate their baseline counterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and AlphaRank mass. The predicted signatures of learning are observed qualitatively in statement and reflection logs, where reflections identify weaknesses and reliably shape subsequent statements. Agreement between quantitative and qualitative evidence supports dialogue-driven context evolution as a practical path to targeted expertise amplification in open non-verifiable domains.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.AI",
        "crawl_time": "2025-10-20T11:00:05.442694",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献与你的研究目标高度契合。我的判断过程如下： 1.  **第一步：核心判断——保留** - 论文的核心贡献是提出一个名为 **Dialectica** 的新框架。这个框架的目的不是将现有智能体应用于某个领域，而是为了解决LLM智能体缺乏“通过经验发展专业知识的内生机制”这一根本问题。 - 论文的核心是构建一种让智能体能够**自我演化**的方法论，即通过“结构化对话、记忆、自我反思和策略约束的上下文编辑”来实现。这完全符合“构建、改进或演化LLM智能体”的核心要求。 - 它不属于“非演化型应用”，因为其焦点是演化机制本身，而非应用结果。它也不属于“非Agentic的推理”或“基础设施”范畴。 2.  **第二步：正面指标——高度匹配** - 论文摘要中明确包含了你的核心关注点： - **核心范式**: `Self-Evolving` (标题和摘要中多次出现), `LLM-based Agents`。 - **智能体能力**: `Self-Reflection` (核心机制), `Memory` (核心机制)。 - **演化机制**: `Self-Improvement` (论文主旨), `Iterative Improvement` (通过对话迭代)。 - 这些关键词的出现频率和核心地位表明，该论文与你的研究焦点（特别是“自我演化”方向）高度相关。 3.  **第三步：排除标准——未触发** - 论文的主要贡献不是关于安全、对齐、可解释性或幻觉，因此不触发排除标准。 - 论文研究的是基于文本对话的智能体，不涉及视觉或多模态内容，因此也未触发相关排除标准。 4.  **第四步：处理特殊和模糊情况——符合核心规则** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一特殊情况的完美范例。虽然它被用来解决“棘手问题”（如环境、政策），但其**核心贡献是提出了一种新的“自我演化”机制**——“对话驱动的上下文演化”。根据你的规则，这种论文应该被保留。 - **推理/规划**: 论文中的“结构化对话”和“元强化学习”过程，可以被看作是一种高级的、在复杂环境中的多步推理和规划框架，属于智能体层面的能力，而非基础模型推理能力的提升。 **最终决策**: 这篇论文的核心贡献是提出了一种名为 **Dialectica** 的创新框架，旨在让LLM智能体通过**结构化对话、记忆和自我反思**来实现在复杂、不可验证领域中的**自我演化**和专业知识增长。这精准地命中了你研究范围中的“自我演化”核心方向，并深度融合了“单智能体”研究中的关键能力（记忆、自我反思）。它不是简单的应用，而是对智能体演化机制的深刻探索。因此，这篇论文是必须保留的前沿研究。"
    },
    {
        "index": "#15",
        "title": "Adaptive Minds: Empowering Agents with LoRA-as-Tools",
        "link": "/arxiv/2510.15416",
        "arxiv_id": "2510.15416",
        "authors": "Pavan C Shekar, Ashwanth Krishnan",
        "summary": "We present Adaptive Minds, an agentic system that treats LoRA adapters as domain-specific tools. Instead of relying on a single fine-tuned model or rigid rule-based routing, our approach empowers the base LLM itself to act as a semantic router analyzing each query and dynamically selecting the most relevant LoRA tool. This enables the agent to seamlessly switch between different domain experts on demand. By combining the flexibility of multi-agent orchestration with the efficiency of parameter-efficient fine-tuning, Adaptive Minds delivers accurate, specialized responses while preserving conversational ability. The system is built with LangGraph for workflow management, supports both API and web interfaces, and is fully open source, providing a scalable and extensible foundation for domain-adaptive AI assistance.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.AI",
        "crawl_time": "2025-10-20T11:00:05.449041",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建和改进LLM智能体。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出了一种名为“Adaptive Minds”的**新型智能体框架**。它的核心贡献不是将LLM应用到一个特定领域，而是**构建了一个能够让LLM自主选择和使用工具（LoRA适配器）的智能体系统**。这完全符合“构建、改进或演化LLM智能体”的核心目标，因此应**保留**。它不属于“非演化型应用”，因为其核心是方法论而非应用本身；也不属于“非Agentic的推理”，因为它涉及智能体的工具使用和动态决策。 **第二步：正面指标——论文是否包含我的核心关注点？** 该论文命中了多个核心正面指标，证明了其高度相关性： 1.  **核心范式**: 论文明确提出了一个“agentic system”，直接对应`Agentic AI`和`LLM-based Agents`。 2.  **智能体能力**: 论文的核心机制是让LLM“act as a semantic router analyzing each query and dynamically selecting the most relevant LoRA tool”。这完美体现了`Tool Use / Tool Augmentation`能力。同时，动态选择不同领域的专家模型也隐含了某种形式的`Planning`（规划如何响应查询）。 3.  **多智能体**: 摘要中提到“combining the flexibility of multi-agent orchestration”，表明其设计思想借鉴了多智能体系统的编排理念，虽然实现方式是单智能体调用多个工具，但其架构思想与`Multi-Agent Systems`相关。 **第三步：排除标准——是否为我的研究焦点之外？** 该论文没有触及任何明确的排除标准。它的主要贡献是关于智能体的架构和能力提升，而不是安全、对齐或可解释性。虽然它使用了LoRA（一种模型微调技术），但研究的焦点不是LoRA本身，而是如何将LoRA**作为工具**整合进智能体框架中，这与基础设施或部署优化有本质区别。 **第四步：处理特殊和模糊情况** 本论文的情况非常清晰，不涉及模糊地带。它提出的“动态选择LoRA工具”是一种典型的**智能体工具使用范式**，属于`Agentic AI`的核心研究范畴。它不是在提升LLM的基础推理能力，而是在构建一个能够利用外部能力（LoRA专家模型）来完成复杂任务的智能体。 **第五步：最终决策** 综合以上分析，论文“Adaptive Minds: Empowering Agents with LoRA-as-Tools”的核心贡献是提出了一种创新的LLM智能体框架，该框架通过将LoRA适配器视为工具，并让LLM自主进行语义路由和动态选择，从而显著增强了智能体的领域适应性和工具使用能力。这完全契合您关于“构建、改进LLM智能体”的研究目标，特别是“单智能体”方向下的“工具使用”子方向。因此，最终判断为**True**。"
    },
    {
        "index": "#16",
        "title": "MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in Strategic Games",
        "link": "/arxiv/2510.15414",
        "arxiv_id": "2510.15414",
        "authors": "Huining Yuan, Zelai Xu, Zheyue Tan, Xiangmin Yi, Mo Guang, Kaiwen Long, Haojia Hui, Boxun Li, Xinlei Chen, Bo Zhao, Xiao-Ping Zhang, Chao Yu, Yu Wang",
        "summary": "Developing Large Language Models (LLMs) to cooperate and compete effectively within multi-agent systems is a critical step towards more advanced intelligence. While reinforcement learning (RL) has proven effective for enhancing reasoning in single-agent tasks, its extension to multi-turn, multi-agent scenarios remains underexplored due to the challenges of long-horizon credit assignment and agent-specific advantage estimation. To address these challenges, we introduce MARS, an end-to-end RL framework that incentivizes Multi-Agent Reasoning of LLMs through Self-play in both cooperative and competitive games. MARS features a turn-level advantage estimator that aligns learning signals with each interaction for credit assignment, and an agent-specific advantage normalization to stabilize multi-agent training. By learning with self-play across cooperative and competitive games, the MARS agent trained from Qwen3-4B develops strong strategic abilities that generalize to held-out games with up to 28.7% performance improvements. More importantly, the capability acquired through self-play generalizes beyond games, yielding consistent performance gains of multi-agent systems in reasoning benchmarks. When integrated into leading multi-agent systems, our MARS agent achieves significant performance gains of 10.0% on AIME and 12.5% on GPQA-Diamond. These results establish end-to-end RL training with self-play in strategic games as a powerful approach for developing generalizable multi-agent reasoning capabilities in LLMs. Our code and models are publicly available at https://github.com/thu-nics/MARS.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.AI",
        "crawl_time": "2025-10-20T11:00:05.449617",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。以下是基于您筛选标准的详细判断过程： **第一步：核心判断——保留 (Keep)** 论文的核心贡献是构建了一个名为 **MARS** 的全新方法论框架。该框架是一个端到端的强化学习（RL）系统，专门用于**提升LLM在多智能体系统中的推理能力**。它并非简单地将现有智能体框架应用于某个领域，而是提出了一种新的训练范式来解决多智能体强化学习中的核心挑战（如长期信用分配和智能体特定的优势估计）。因此，它直接命中了您“构建、改进或演化LLM智能体”的核心目标，属于 **Multi-Agent** 方向的构建性工作。 **第二步：正面指标——高度匹配** 论文包含了大量您关注的核心指标： - **核心范式**: `Multi-Agent Systems (MAS)` 是论文的绝对核心。`Self-Play`（自我博弈）作为一种特殊的演化机制，也与 `Self-Evolving` 和 `Evolutionary Algorithms` 的思想高度相关。 - **多智能体**: 论文明确研究了智能体在合作（`Collaboration`）和竞争（`Competition`）环境下的行为，这涉及到智能体间的交互和策略形成。 - **推理/规划**: 论文的核心是提升 `Multi-Agent Reasoning`（多智能体推理）能力，这与您关注的智能体在复杂任务中的多步推理能力完全一致。 - **演化机制**: `Self-Play` 本身就是一种强大的迭代改进（`Iterative Improvement`）机制，智能体通过与自身或其他智能体的不断对战来演化出更强的策略。 **第三步：排除标准——未触及** 论文的主要贡献并非安全、对齐或多模态。虽然它使用了“游戏”作为训练和评估环境，但游戏在这里是作为研究多智能体推理和策略的抽象载体，而非一个具体的应用领域。论文的最终目标是获得**可泛化的多智能体推理能力**，并成功将其迁移到推理基准（AIME, GPQA-Diamond）上，证明了其通用性，这完全避开了“非演化型应用”的排除规则。 **第四步：处理特殊和模糊情况——符合保留规则** - **推理/规划**: 论文是关于智能体如何在多轮、多智能体的复杂环境中进行战略推理的典型案例，属于“保留”范畴。它不是在提升LLM的基础数学或逻辑能力，而是在构建一个能让LLM在多智能体情境下进行有效推理的框架。 - **自我演化的应用**: 论文提出的 `Self-Play` 机制本身就是一种新颖的“自我演化”方法。即使它以“战略游戏”为应用场景，根据您的规则（“如果论文的核心是提出一种新的‘自我演化’机制……也应该保留”），这篇论文也应该被保留。 **第五步：最终决策** 综合以上分析，论文 **MARS** 的核心贡献是提出了一种通过自我博弈的强化学习来**构建和改进多智能体LLM推理能力**的新框架。它直接解决了多智能体系统中的关键挑战，并展示了其能力的泛化性。这篇论文是典型的、高质量的 **Multi-Agent** 方向的前沿研究，与您“LLM智能体及其演化”的研究课题高度契合。因此，最终判断为 **True**。"
    },
    {
        "index": "#1",
        "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold",
        "link": "/arxiv/2510.15862",
        "arxiv_id": "2510.15862",
        "authors": "Yi Wan, Jiuqi Wang, Liam Li, Jinsong Liu, Ruihao Zhu, Zheqing Zhu",
        "summary": "Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under MIT license at https://github.com/Pokee-AI/PokeeResearchOSS.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.AI",
        "crawl_time": "2025-10-20T11:00:05.441746",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建和改进LLM智能体。以下是我的详细判断过程： 1.  **第一步：核心判断——这篇论文的本质是什么？** *   **保留 (Keep)**。论文的本质是提出一种构建更优LLM智能体的新方法。它没有简单地将现有智能体框架应用到一个新领域，而是聚焦于智能体本身的构建和训练过程。其核心贡献是统一的强化学习框架（RLAIF）和一个鲁棒的推理框架，用于训练和增强一个名为“PokeeResearch-7B”的深度研究智能体。这完全符合“构建、改进或演化LLM智能体”的核心目标。 2.  **第二步：正面指标——论文是否包含我的核心关注点？** *   这篇论文包含了多个核心关注点，相关性极高： *   **核心范式**: 论文明确围绕 `LLM-based Agents` 展开。 *   **智能体能力**: 论文的核心是改进智能体的能力。`Tool Use / Tool Augmentation` 是智能体的基础。`Self-Correction / Self-Reflection` 体现在 \"self-verification and adaptive recovery from tool failures\"（自我验证和从工具失败中自适应恢复）中。`Planning` 和复杂推理则由其 \"chain-of-thought-driven multi-call reasoning scaffold\"（思维链驱动的多调用推理框架）支持。 *   **演化机制**: `Reinforcement Learning from AI Feedback (RLAIF)` 是一种明确的训练和改进机制，旨在迭代优化智能体的策略，属于 `Self-Improvement` 和 `Iterative Improvement` 的范畴。 3.  **第三步：排除标准——是否为我的研究焦点之外？** *   论文未被排除。虽然摘要中提到了 \"alignment\"（对齐），但它的角色是作为RLAIF框架优化目标的一部分（与事实准确性、引用忠实性并列），用于提升智能体的整体性能，而不是论文的主要研究贡献。论文的核心是提出这个训练框架本身，而不是研究对齐理论或技术。因此，它不属于“安全与对齐”的排除范畴。论文也未涉及多模态或视觉内容。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文中的推理部分（\"reasoning scaffold\"）是作为智能体框架的一个组成部分，用于增强其鲁棒性和处理工具失败的能力，属于智能体自主规划和多步推理的范畴，因此应该**保留**。它不是关于提升LLM本身的基础数学或逻辑能力。 5.  **第五步：最终决策** *   综合以上分析，这篇论文的核心贡献是提出了一种新的训练范式（RLAIF）和推理框架，用于构建一个更鲁棒、更高效的LLM研究智能体。它直接触及了单智能体方向中的工具使用、自我反思/修正和规划能力，并引入了自我改进的训练机制。这与您“构建、改进或演化LLM智能体”的核心目标高度一致，是一篇非常相关的论文。"
    },
    {
        "index": "#22",
        "title": "AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory",
        "link": "/arxiv/2510.15261",
        "arxiv_id": "2510.15261",
        "authors": "Jitesh Jain, Shubham Maheshwari, Ning Yu, Wen-mei Hwu, Humphrey Shi",
        "summary": "Riding on the success of LLMs with retrieval-augmented generation (RAG), there has been a growing interest in augmenting agent systems with external memory databases. However, the existing systems focus on storing text information in their memory, ignoring the importance of multimodal signals. Motivated by the multimodal nature of human memory, we present AUGUSTUS, a multimodal agent system aligned with the ideas of human memory in cognitive science. Technically, our system consists of 4 stages connected in a loop: (i) encode: understanding the inputs; (ii) store in memory: saving important information; (iii) retrieve: searching for relevant context from memory; and (iv) act: perform the task. Unlike existing systems that use vector databases, we propose conceptualizing information into semantic tags and associating the tags with their context to store them in a graph-structured multimodal contextual memory for efficient concept-driven retrieval. Our system outperforms the traditional multimodal RAG approach while being 3.5 times faster for ImageNet classification and outperforming MemGPT on the MSC benchmark.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.AI",
        "crawl_time": "2025-10-20T11:00:05.452503",
        "filter_reason": "这篇论文的核心贡献是构建了一个名为AUGUSTUS的新型多模态智能体系统，其核心创新点在于为智能体设计了一种全新的、受人类认知科学启发的记忆机制。这完全符合您的研究范围。 以下是详细的判断过程： 1.  **第一步：核心判断** - **保留 (Keep)**。这篇论文的本质不是将现有智能体框架应用于某个领域，而是**构建和改进LLM智能体本身**。其核心贡献是提出了一种新的智能体方法论和框架，具体体现在其独特的记忆系统上。论文详细描述了智能体的四个循环阶段（编码、存储、检索、行动），这构成了一个完整的智能体工作流。因此，它不属于“非演化型应用”或“非Agentic的推理”。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `LLM-based Agents`。标题和摘要都明确指出这是一个LLM驱动的智能体系统。 - **智能体能力**: `Memory`。这是论文最核心的贡献，它提出了一种“图结构的多模态情境记忆”（graph-structured multimodal contextual memory），这是对现有智能体记忆能力的重大改进。 - **智能体能力**: `Planning`。虽然未直接使用“规划”一词，但其“编码-存储-检索-行动”的循环框架本质上是一个智能体在任务中持续进行决策和行动的流程，属于Agentic AI的核心范畴。 3.  **第三步：排除标准** - 论文的主要贡献**不是**关于安全、对齐、可解释性或水印。它也**不是**一篇关于多模态模型（如VLMs）本身的基础研究。虽然论文提到了“多模态”（multimodal），但其目的是将这些多模态信息作为智能体感知和记忆的**内容**，而不是研究多模态模型本身。这符合筛选标准中“除非它们被用作智能体感知环境的工具，而不是研究的核心”这一例外情况。因此，该论文不应被排除。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文提出的四阶段循环框架（特别是“行动”阶段）是智能体执行任务的核心，属于Agentic框架的范畴，而非单纯的LLM基础推理能力提升。 - **自我演化的应用**: 论文虽然不直接属于“自我演化”方向，但其提出的记忆系统是智能体通过与环境交互不断积累经验、改进未来决策的基础，这与自我演化的理念高度相关。更重要的是，它完全符合“单智能体 (Agentic)”方向中关于“记忆”能力的研究。 **核心依据总结**: 该论文的核心贡献是**构建了一个具有新型记忆机制的LLM智能体系统**。它不是在应用智能体，而是在**改进智能体的核心组件（记忆）**。这种对智能体内部架构和能力的创新性研究，正是您“LLM智能体及其演化”课题所关注的“单智能体 (Agentic)”方向下的“记忆”子方向。因此，这篇论文高度相关，应该被保留。"
    },
    {
        "index": "#23",
        "title": "Experience-Driven Exploration for Efficient API-Free AI Agents",
        "link": "/arxiv/2510.15259",
        "arxiv_id": "2510.15259",
        "authors": "Chenwei Tang, Jingyu Xing, Xinyu Liu, Zizhou Wang, Jiawei Du, Liangli Zhen, Jiancheng Lv",
        "summary": "Most existing software lacks accessible Application Programming Interfaces (APIs), requiring agents to operate solely through pixel-based Graphical User Interfaces (GUIs). In this API-free setting, large language model (LLM)-based agents face severe efficiency bottlenecks: limited to local visual experiences, they make myopic decisions and rely on inefficient trial-and-error, hindering both skill acquisition and long-term planning. To address these challenges, we propose KG-Agent, an experience-driven learning framework that structures an agent's raw pixel-level interactions into a persistent State-Action Knowledge Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking functionally similar but visually distinct GUI states, forming a rich neighborhood of experience that enables the agent to generalize from a diverse set of historical strategies. To support long-horizon reasoning, we design a hybrid intrinsic reward mechanism based on the graph topology, combining a state value reward for exploiting known high-value pathways with a novelty reward that encourages targeted exploration. This approach decouples strategic planning from pure discovery, allowing the agent to effectively value setup actions with delayed gratification. We evaluate KG-Agent in two complex, open-ended GUI-based decision-making environments (Civilization V and Slay the Spire), demonstrating significant improvements in exploration efficiency and strategic depth over the state-of-the-art methods.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.AI",
        "crawl_time": "2025-10-20T11:00:05.452888",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建和改进LLM智能体。以下是根据您的筛选标准进行的详细判断过程： **第一步：核心判断——这篇论文的本质是什么？** - **保留 (Keep)**。这篇论文的本质是提出一个名为 **KG-Agent** 的新框架，用于解决LLM智能体在无API环境下面临的效率瓶颈。它不是简单地将现有智能体应用于某个领域，而是**构建了一个全新的、具有学习和规划能力的智能体方法论**。论文的核心是关于智能体如何通过结构化经验（知识图谱）来改进其探索和长期规划能力，这完全属于“构建、改进或演化LLM智能体”的范畴。 **第二步：正面指标——论文是否包含我的核心关注点？** 这篇论文包含了多个核心关注点，相关性极高： - **核心范式**: 论文明确研究 `LLM-based Agents`，并提出了一个改进其能力的框架。 - **智能体能力**: - **规划 (Planning)**: 论文的核心贡献之一是支持“长期规划”（long-horizon reasoning）。它通过设计基于图拓扑的混合内在奖励机制，将战略规划与纯粹的探索分离开来，使智能体能够进行有延迟满足的规划。这完全符合您对智能体规划能力的关注。 - **记忆 (Memory)**: 论文提出的 **State-Action Knowledge Graph (SA-KG)** 是一种持久化的、结构化的记忆机制。它将原始的像素级交互经验组织起来，使智能体能够利用历史策略进行泛化。这是一种高级的、结构化的记忆形式。 - **自我反思/修正 (Self-Correction/Reflection)**: 虽然没有直接使用“自我反思”这个词，但其“经验驱动的学习”（experience-driven learning）和“从历史策略中泛化”的机制，本质上是一种基于经验的自我改进过程。智能体通过构建和利用知识图谱来修正其短视的决策行为。 **第三步：排除标准——是否为我的研究焦点之外？** - **安全与对齐**: 论文完全不涉及安全、对齐、可解释性等内容。其焦点是智能体的效率和规划能力。 - **多模态与视觉**: 论文虽然处理像素级的GUI输入（视觉信息），但**视觉是作为智能体感知环境的工具，而不是研究的核心**。研究的核心是如何利用这些视觉输入来构建知识图谱并改进决策，而不是改进视觉模型本身。这完全符合您设定的例外情况。 **第四步：处理特殊和模糊情况** - **推理/规划 (Reasoning/Planning)**: 这篇论文是典型的“保留”案例。它不是在改进LLM的基础数学或逻辑推理，而是在研究**智能体如何在复杂、开放式的环境中进行多步、长周期的战略规划**。其提出的混合奖励机制和知识图谱都是为了服务于这一目标，这与ReAct、ToT等Agentic框架的精神内核一致。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是**提出了一种名为KG-Agent的新框架，通过构建知识图谱来赋予LLM智能体结构化记忆和高效的长期规划能力**。它直接解决了智能体在复杂环境中的探索效率和战略深度问题，完全属于“单智能体 (Agentic)”研究方向下的“规划”和“记忆”子方向。因此，这篇论文与您的研究课题“LLM智能体及其演化”高度相关，应当被保留。"
    },
    {
        "index": "#60",
        "title": "The Spark Effect: On Engineering Creative Diversity in Multi-Agent AI Systems",
        "link": "/arxiv/2510.15568",
        "arxiv_id": "2510.15568",
        "authors": "Alexander Doudkin, Anton Voelker, Friedrich von Borries",
        "summary": "Creative services teams increasingly rely on large language models (LLMs) to accelerate ideation, yet production systems often converge on homogeneous outputs that fail to meet brand or artistic expectations. Art of X developed persona-conditioned LLM agents -- internally branded as \"Sparks\" and instantiated through a library of role-inspired system prompts -- to intentionally diversify agent behaviour within a multi-agent workflow. This white paper documents the problem framing, experimental design, and quantitative evidence behind the Spark agent programme. Using an LLM-as-a-judge protocol calibrated against human gold standards, we observe a mean diversity gain of +4.1 points (on a 1-10 scale) when persona-conditioned Spark agents replace a uniform system prompt, narrowing the gap to human experts to 1.0 point. We also surface evaluator bias and procedural considerations for future deployments.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.AI",
        "crawl_time": "2025-10-20T11:00:05.463801",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断** 这篇论文的核心贡献是提出了一种名为“Sparks”的、通过人格化系统提示（persona-conditioned system prompts）来工程化多智能体系统中创意多样性的方法论。这并非简单地将LLM作为工具应用于创意领域，而是构建了一个**多智能体系统（Multi-Agent Systems）**，并研究了如何通过设计智能体间的差异性来影响整个系统的输出。因此，它属于“构建或改进LLM智能体”的范畴，符合保留标准。 **第二步：正面指标** 论文明确包含了我的多个核心关注点： - **核心范式**: 论文直接研究`Multi-Agent Systems (MAS)`，探讨了多智能体工作流（multi-agent workflow）。 - **多智能体**: 论文的核心是关于如何通过赋予不同智能体不同“人格”（persona）来**多样化智能体行为**，这可以被视为一种协作形式，旨在提升整个智能体“社会”的集体产出质量。虽然未直接使用`Collaboration`或`Communication`等词，但其本质是研究多智能体协同工作的机制。 **第三步：排除标准** 论文不涉及任何排除标准中的内容。其研究焦点是提升多智能体系统的创意多样性，而非安全、对齐或多模态技术。 **第四步：处理特殊和模糊情况** 本论文的情况不模糊。它清晰地属于**多智能体（Multi-Agent）**研究方向。论文的核心不是LLM的基础推理能力，也不是一个非演化型的应用。它提出了一种新的框架（通过人格化提示来构建多样化的智能体），并验证了该框架在多智能体环境下的有效性，这正是我研究课题所关注的前沿方向。 **第五步：最终决策** 综合以上分析，该论文的核心贡献在于提出并验证了一种改进多智能体系统性能（具体指创意多样性）的新方法。它直接对应我研究焦点的第二个方向“多智能体”，探讨了智能体间的行为差异如何影响系统整体表现。因此，这篇论文与我的研究目标高度相关，最终判断为 **True**。"
    }
]