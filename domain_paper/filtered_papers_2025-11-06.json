[
    {
        "index": "#4",
        "title": "DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration",
        "link": "/arxiv/2511.04646",
        "arxiv_id": "2511.04646",
        "authors": "Narjes Nourzad, Hanqing Yang, Shiyu Chen, Carlee Joe-Wong",
        "summary": "Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning, Multiagent Systems",
        "date": "2025-11-06",
        "category": "cs.MA",
        "crawl_time": "2025-11-07T11:00:03.705467",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献直接命中了“多智能体”和“自我演化”两个关键方向。 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为 DR. WELL 的**新框架**，用于解决协作式多智能体规划问题。它不是简单地将现有框架应用于某个领域，而是构建了一个包含协商、规划和学习机制的全新方法论。这完全符合“构建、改进LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文包含了大量你的核心关注点： - **多智能体**: 标题和摘要明确指出这是关于 \"LLM-Based Multi-Agent Collaboration\" 的研究。摘要中详细描述了 \"two-phase negotiation protocol\"（两阶段协商协议）、\"joint allocation\"（联合分配）和 \"cooperative multi-agent planning\"（协作式多智能体规划），这些都是多智能体研究的核心。 - **自我演化**: 摘要中明确提到 \"agents adapt across episodes\"（智能体跨回合适应）、\"dynamic world model capturing reusable patterns\"（动态世界模型捕获可复用模式）以及 \"evolving, more efficient collaboration strategies\"（演化出更高效的协作策略）。这直接对应了“自我演化”中的通过经验和环境反馈进行自我完善和迭代。 - **智能体能力**: 论文涉及了 `Planning`（符号规划）、`Communication`（协商协议）和 `Memory`（共享世界模型）。 3.  **第三步：排除标准** - 论文的主要贡献**不是**关于安全、对齐或多模态。虽然提到了 \"interpretable\"（可解释性），但这只是其符号规划方法带来的一个优点，而非论文的核心研究目标。因此，不触及排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文是关于智能体如何进行**协作规划**的，提出了一个新的神经符号框架，这属于“保留”的范畴，而不是单纯提升LLM的基础推理能力。 - **自我演化的应用**: 论文虽然应用在 \"cooperative block-push tasks\"（合作推积木任务）这个具身智能领域，但其核心贡献是提出了一种**新的自我演化机制**（通过动态世界模型和协商实现策略演化）。根据你的规则，这种情况应该保留。 **结论**: 该论文的核心贡献是构建了一个用于多智能体协作与自我演化的新框架 DR. WELL。它不仅研究了智能体间的协商与规划，还展示了智能体如何通过经验演化出更高效的协作策略。这精准地契合了你关于“LLM智能体及其演化”的研究课题，特别是“多智能体”和“自我演化”两个方向。因此，应予以保留。"
    },
    {
        "index": "#1",
        "title": "Multi-Agent Collaborative Framework For Math Problem Generation",
        "link": "/arxiv/2511.03958",
        "arxiv_id": "2511.03958",
        "authors": "Kia Karbasi, Kevin Hong, Mohammad Amin Samadi, Gregory Pottie",
        "summary": "Automatic question generation (AQG) for mathematics education remains an elusive goal for Intelligent Tutoring Systems and educators. While pre-trained transformer-based language models have significantly advanced natural language generation, they often struggle to precisely control problem complexity and cognitive demands. In this paper, we introduce a collaborative multi-agent framework as a novel method of incorporating inference-time computation into AQG. This approach leverages multiple agents that iteratively refine generated question-answer pairs to better balance complexity and cognitive demand. We evaluate the generated questions on five meta-evaluation criteria: relevance, importance, clarity, difficulty matching, answerability, to assess the system's ability to control the required complexity and quality of the questions. Preliminary evaluations show that this collaborative multi-agent framework elevates the quality of generated educational content by fostering a more nuanced balance between cognitive challenge and clarity. These promising outcomes suggest that integrating collaborative multi-agent workflows can yield more controlled, pedagogically valuable content that can help advance automated educational content generation and adaptive learning environments.",
        "subjects": "Multiagent Systems, Computation and Language, Human-Computer Interaction",
        "date": "2025-11-06",
        "category": "cs.MA",
        "crawl_time": "2025-11-07T11:00:03.703568",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是“如何用LLM生成数学题”，而是“提出了一种**协作式多智能体框架**”来完成这项任务。其核心贡献在于**方法论和新框架**的设计，即多个智能体如何通过协作和迭代优化来提升生成内容的质量。这直接命中了您“构建、改进LLM智能体”的核心目标。它不属于“非演化型应用”，因为论文的重点是智能体系统本身，而非其在教育领域的应用细节。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `Multi-Agent Systems (MAS)` 是论文标题和摘要的核心。 - **多智能体**: 明确提到了 `Collaboration`（协作），这是多智能体研究的关键方向。 - **演化机制**: 摘要中描述的“iteratively refine generated question-answer pairs”（迭代地优化生成的问题-答案对）是一种典型的 `Self-Refine` 或 `Iterative Improvement` 机制，属于“自我演化”的范畴。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或视觉多模态等问题，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一特殊情况的完美例证。虽然它的应用场景是数学教育（特定领域），但其核心贡献是提出了一种新的“自我演化”机制（通过多智能体协作进行迭代优化）。根据您的规则，这种情况应该被**保留**。 - **推理/规划**: 论文中的多智能体协作与迭代优化过程，本身就是一种在复杂任务（平衡问题复杂度和认知需求）中进行多步推理和规划的Agentic行为，而非单纯提升LLM的基础数学能力。 **最终决策**: 综合以上分析，该论文的核心贡献是提出了一种新颖的**多智能体协作框架**，该框架通过**迭代优化**（一种自我演化形式）来完成任务。这完全符合您研究课题中的“多智能体”和“自我演化”两个核心方向。因此，这篇论文是高度相关且应该被保留的前沿研究。"
    },
    {
        "index": "#5",
        "title": "IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection",
        "link": "/arxiv/2511.04528",
        "arxiv_id": "2511.04528",
        "authors": "Kaveh Eskandari Miandoab, Katharine Kowalyshyn, Kabir Pamnani, Anesu Gavhera, Vasanth Sarathy, Matthias Scheutz",
        "summary": "We present IntelliProof, an interactive system for analyzing argumentative essays through LLMs. IntelliProof structures an essay as an argumentation graph, where claims are represented as nodes, supporting evidence is attached as node properties, and edges encode supporting or attacking relations. Unlike existing automated essay scoring systems, IntelliProof emphasizes the user experience: each relation is initially classified and scored by an LLM, then visualized for enhanced understanding. The system provides justifications for classifications and produces quantitative measures for essay coherence. It enables rapid exploration of argumentative quality while retaining human oversight. In addition, IntelliProof provides a set of tools for a better understanding of an argumentative essay and its corresponding graph in natural language, bridging the gap between the structural semantics of argumentative essays and the user's understanding of a given text. A live demo and the system are available here to try: \\textbf{https://intelliproof.vercel.app}",
        "subjects": "Computation and Language",
        "date": "2025-11-06",
        "category": "cs.CL",
        "crawl_time": "2025-11-07T11:00:04.318321",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。判断依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献是构建一个名为IntelliProof的**交互式系统**。这个系统并非简单地将LLM应用于教育领域，而是提出了一个完整的**方法论和框架**，该框架利用LLM作为核心引擎，通过构建论证图、提供工具集、与用户交互来完成复杂的分析任务。这完全符合“构建、改进LLM智能体”的核心目标。它不是一个非演化型的应用，因为其贡献在于系统本身的设计和实现，而非其在特定领域的应用效果。 2.  **正面指标 (第二步):** 论文包含了多个核心关注点： *   **Agentic AI / LLM-based Agents:** 论文明确将其描述为一个“interactive system”和“Conversational Helper”，这表明它是一个具备自主交互能力的智能体。 *   **Tool Use / Tool Augmentation:** 摘要中明确提到“IntelliProof provides a set of tools for a better understanding...”，这是智能体能力的关键指标。 *   **Planning:** 将一篇非结构化的论文转化为结构化的“argumentation graph”的过程，本身就是一种复杂的规划和结构化分析能力，远超简单的线性推理。 *   **Self-Reflection:** 标题和摘要中都强调了“Organized Reflection”。虽然这个反思主要是为了辅助用户，但该系统通过结构化分析和可视化，**实现并促进了**这一反思过程，这是智能体高级认知能力的体现。 3.  **排除标准 (第三步):** 论文的主要贡献不是关于安全、对齐或可解释性。尽管它提供了“justifications”（理由），但这只是系统功能的一部分，是为了增强用户体验和信任，而不是论文的核心研究问题。论文的核心是智能体的架构和行为。论文也不涉及多模态或视觉内容。 4.  **特殊和模糊情况 (第四步):** 这篇论文的推理/规划能力符合保留标准。它不是在提升LLM的基础数学或逻辑能力，而是在构建一个能让LLM进行多步、结构化分析（构建论证图）的智能体框架。 **总结:** 该论文的核心是提出一个具备工具使用、结构化规划和交互反思能力的LLM智能体框架。它完全符合“单智能体”研究方向下的“规划、工具使用、自我反思”等子方向，因此应被保留。"
    },
    {
        "index": "#8",
        "title": "RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG",
        "link": "/arxiv/2511.04502",
        "arxiv_id": "2511.04502",
        "authors": "Joshua Gao, Quoc Huy Pham, Subin Varghese, Silwal Saurav, Vedhus Hoskere",
        "summary": "Retrieval-Augmented Generation (RAG) is a critical technique for grounding Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in specialized, safety-critical domains remains a significant challenge. Existing evaluation frameworks often rely on heuristic-based metrics that fail to capture domain-specific nuances and other works utilize LLM-as-a-Judge approaches that lack validated alignment with human judgment. This paper introduces RAGalyst, an automated, human-aligned agentic framework designed for the rigorous evaluation of domain-specific RAG systems. RAGalyst features an agentic pipeline that generates high-quality, synthetic question-answering (QA) datasets from source documents, incorporating an agentic filtering step to ensure data fidelity. The framework refines two key LLM-as-a-Judge metrics-Answer Correctness and Answerability-using prompt optimization to achieve a strong correlation with human annotations. Applying this framework to evaluate various RAG components across three distinct domains (military operations, cybersecurity, and bridge engineering), we find that performance is highly context-dependent. No single embedding model, LLM, or hyperparameter configuration proves universally optimal. Additionally, we provide an analysis on the most common low Answer Correctness reasons in RAG. These findings highlight the necessity of a systematic evaluation framework like RAGalyst, which empowers practitioners to uncover domain-specific trade-offs and make informed design choices for building reliable and effective RAG systems. RAGalyst is available on our Github.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.CL",
        "crawl_time": "2025-11-07T11:00:04.320003",
        "filter_reason": "这篇论文符合筛选标准，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献并非将LLM或智能体作为工具应用于特定领域，而是**构建了一个名为RAGalyst的新颖Agentic框架**。该框架本身就是一个LLM智能体（或智能体工作流），其设计目标是自动化地执行一个复杂的、多步骤的任务：生成评估数据、过滤数据、并优化评估指标。因此，论文的本质是关于“构建LLM智能体的方法论”，完全符合“保留”标准。 2.  **第二步：正面指标** - 论文摘要中明确包含了多个核心关注点： - **核心范式**: `Agentic AI`, `LLM-based Agents`。摘要直接称RAGalyst为“agentic framework”和“agentic pipeline”。 - **智能体能力**: `Planning`（隐含在agentic pipeline中，智能体需要规划生成数据、过滤、评估的完整流程）、`Tool Use`（隐含，智能体需要使用源文档、LLM-as-a-Judge等工具）。 - 这些正面指标强烈表明该论文与我的研究焦点高度相关。 3.  **第三步：排除标准** - **安全与对齐**: 论文提到了“human-aligned”，这是一个需要仔细甄别的点。然而，这里的“对齐”指的是**评估指标与人类判断的对齐**，是确保评估框架有效性的一个技术手段，而非论文的核心贡献。论文的核心是那个**Agentic评估框架本身**，而不是提出一种新的对齐理论或方法。因此，这不触发排除规则。 - **多模态与视觉**: 论文未涉及视觉或多模态内容，不在此排除范围内。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文描述的RAGalyst框架通过一个“agentic pipeline”来执行任务，这完全符合“保留”条件，即“关于智能体如何进行规划或在复杂任务中进行多步推理”。它不是在提升LLM的基础推理能力，而是在构建一个能自主执行复杂工作流的智能体。 **最终决策**: 综合以上分析，这篇论文的核心贡献是**构建了一个用于自动化评估的LLM智能体框架（RAGalyst）**。它详细描述了该智能体的工作流程（agentic pipeline），这属于“单智能体”研究范畴下的智能体构建与改进。虽然其应用场景是评估RAG系统，但其核心创新点在于**如何构建一个能自主完成评估任务的智能体**，而非评估结果本身或其在特定领域的应用。因此，这篇论文精准地符合我关于“构建、改进LLM智能体”的研究目标。"
    },
    {
        "index": "#34",
        "title": "Direct Semantic Communication Between Large Language Models via Vector Translation",
        "link": "/arxiv/2511.03945",
        "arxiv_id": "2511.03945",
        "authors": "Fu-Chun Yang, Jason Eshraghian",
        "summary": "In multi-agent settings, such as debate, reflection, or tool-calling, large language models (LLMs) pass messages as plain tokens, discarding most latent semantics. This constrains information transfer and adds unnecessary computational overhead. We form a latent bridge via vector translations, which use learned mappings that enable direct semantic exchange between representation spaces. A dual-encoder translator trained between Llama-2-7B and Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the translated vectors at 30 percent blending strength steers the target model's generation without destabilizing logits. Bidirectional evaluation shows a 2.01:1 transfer asymmetry, indicating that general-purpose models yield more transferable representations than instruction-tuned variants. This conservative injection preserves computational stability while demonstrating that cross-model latent communication is feasible, enabling collaborative AI systems that share meaning rather than tokens.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.CL",
        "crawl_time": "2025-11-07T11:00:04.350525",
        "filter_reason": "这篇论文符合研究范围，应予以保留。 **判断过程如下:** 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种名为“向量翻译”的新方法，旨在解决多智能体系统中LLM之间通信效率低下的问题。它不是将LLM智能体作为工具去解决某个外部领域（如生物、金融）的问题，而是直接针对多智能体系统内部的**通信机制**进行创新和改进。 - **结论**: 论文的核心是关于**改进多智能体系统**的方法论，符合“保留”标准。 2.  **第二步：正面指标** - 论文摘要中明确包含了多个核心关注点： - **核心范式**: `Multi-Agent Systems (MAS)` (多智能体系统) - **智能体能力**: 提到了 `Tool-calling` (工具使用) 和 `reflection` (自我反思) 作为其方法的应用场景。 - **多智能体**: 明确提到了 `Communication` (通信)，并指出其目标是实现 `collaborative AI systems` (协作AI系统)。 - **结论**: 论文命中了多个关键正面指标，特别是“多智能体”方向下的“通信”子方向，与研究焦点高度相关。 3.  **第三步：排除标准** - 论文的主要贡献是关于通信效率和语义传递，不涉及 `Safety`、`Alignment`、`Interpretability` 或 `Vision` 等排除领域。 - **结论**: 论文未触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - 本论文不涉及“推理/规划”或“自我演化的应用”等特殊情况，其核心贡献非常清晰，即多智能体通信。 5.  **第五步：最终决策** - **综合分析**: 该论文直接针对多智能体系统中的一个基础且关键的问题——如何让智能体之间更高效、更深刻地交流。它提出的“向量翻译”方法，旨在让智能体共享“意义”而非“token”，这是对多智能体协作机制的根本性改进。这项工作为构建更强大的协作AI系统铺平了道路，完全符合“构建、改进或演化LLM智能体”的核心目标，尤其是在“多智能体”这一研究方向上。 因此，这篇论文是高度相关的前沿研究，应被筛选出来。"
    },
    {
        "index": "#30",
        "title": "Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering",
        "link": "/arxiv/2511.04072",
        "arxiv_id": "2511.04072",
        "authors": "Xinying Qian, Ying Zhang, Yu Zhao, Baohang Zhou, Xuhui Sui, Xiaojie Yuan",
        "summary": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer time-sensitive questions by leveraging factual information from Temporal Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG embeddings or graph neural networks to inject temporal knowledge, they fail to fully understand the complex semantic information of time constraints. Recently, Large Language Models (LLMs) have shown remarkable progress, benefiting from their strong semantic understanding and reasoning generalization capabilities. However, their temporal reasoning ability remains limited. LLMs frequently suffer from hallucination and a lack of knowledge. To address these limitations, we propose the Plan of Knowledge framework with a contrastive temporal retriever, which is named PoK. Specifically, the proposed Plan of Knowledge module decomposes a complex temporal question into a sequence of sub-objectives from the pre-defined tools, serving as intermediate guidance for reasoning exploration. In parallel, we construct a Temporal Knowledge Store (TKS) with a contrastive retrieval framework, enabling the model to selectively retrieve semantically and temporally aligned facts from TKGs. By combining structured planning with temporal knowledge retrieval, PoK effectively enhances the interpretability and factual consistency of temporal reasoning. Extensive experiments on four benchmark TKGQA datasets demonstrate that PoK significantly improves the retrieval precision and reasoning accuracy of LLMs, surpassing the performance of the state-of-the-art TKGQA methods by 56.0% at most.",
        "subjects": "Computation and Language",
        "date": "2025-11-06",
        "category": "cs.CL",
        "crawl_time": "2025-11-07T11:00:04.348345",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。判断依据如下： 1.  **第一步：核心判断** - **论文的本质**: 这篇论文的核心贡献并非简单地应用LLM去解决时序知识图谱问答（TKGQA）这一特定领域问题，而是提出了一个名为“Plan of Knowledge”（PoK）的**新框架**。该框架的核心机制是**将复杂问题分解为一系列子目标，并使用预定义工具来执行**。这本质上是一种**构建和改进LLM智能体规划能力**的方法论。 - **排除项分析**: - **非演化型应用**: 虽然论文在TKGQA任务上进行了验证，但其核心贡献是PoK框架本身，这是一个通用的、可迁移的智能体规划与工具使用范式，而非针对TKGQA的特定应用。因此，它不属于“非演化型应用”。 - **非Agentic的推理**: 论文明确提出了一个外部的“规划”模块，这与仅仅提升LLM内部推理链（如CoT变体）有本质区别。它构建了一个包含“规划-行动”循环的智能体框架，因此属于Agentic AI的范畴。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: 论文构建了一个典型的 `LLM-based Agent` 框架。 - **智能体能力**: 论文的核心贡献直接对应 `Planning`（将问题分解为子目标序列）和 `Tool Use / Tool Augmentation`（从预定义工具中选择子目标）。其工作流程与 `ReAct`（Reasoning and Acting）范式高度相似，即通过推理来指导行动（工具使用）。 - **记忆**: 论文中的“Temporal Knowledge Store”（TKS）和检索机制，可以被视为一种外部记忆或知识库，供智能体在规划时调用。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态等排除领域。其目标是提升智能体的推理准确性和事实一致性，属于性能优化，而非安全或对齐研究。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是“保留”的典型案例。它不是在改进LLM的基础Token预测能力，而是在构建一个**外部的、结构化的规划框架**来指导LLM进行多步推理和工具使用。这完全符合“关于智能体如何进行规划或在复杂任务中进行多步推理”的保留标准。 **结论**: 该论文的核心贡献在于提出了一种名为PoK的新颖框架，该框架通过**结构化规划**和**工具使用**来增强LLM在复杂任务中的推理能力。这直接命中了我的研究焦点“单智能体”方向下的“规划”和“工具使用”子方向。因此，尽管它在一个特定应用领域（TKGQA）上进行评估，但其方法论本质是关于LLM智能体的构建与改进，完全符合筛选要求。"
    },
    {
        "index": "#43",
        "title": "TextualVerifier: Verify TextGrad Step-by-Step",
        "link": "/arxiv/2511.03739",
        "arxiv_id": "2511.03739",
        "authors": "Eugenius Mario Situmorang, Adila Alfa Krisnadhi, Ari Wibisono",
        "summary": "TextGrad is a novel approach to text-based automatic differentiation that enables composite AI systems to perform optimization without explicit numerical equations. However, it currently lacks self-verification mechanisms that ensure reasoning validity in text-based decision making. This research introduces TextualVerifier, a verification framework that leverages chain-of-thought reasoning and majority voting with large language models to address this verification gap. TextualVerifier implements a four-stage workflow: chain-of-thought decomposition, variant generation, majority voting, and consensus aggregation. It integrates non-invasively with TextGrad at both the loss function and optimization result verification stages. Experimental evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1) standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically significant improvements (p < 0.001). In phase one, TextualVerifier improves the validity of reasoning steps by 29 percent. In phase two, integration into TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4 percent with a moderate overhead of 5.9 LLM calls on average. Further evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92 percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively. TextualVerifier thus presents the first self-verification framework for TextGrad through LLM-based techniques without requiring numerical gradients, enabling more reliable reasoning and opening new directions for verification in text-based optimization.",
        "subjects": "Computation and Language",
        "date": "2025-10-29",
        "category": "cs.CL",
        "crawl_time": "2025-11-07T11:00:04.377895",
        "filter_reason": "这篇论文符合我的研究范围，其核心贡献在于为LLM智能体系统增加了一种关键的自我完善机制。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM作为工具应用，也不是提升LLM的基础推理能力。它的核心贡献是构建了一个名为 `TextualVerifier` 的**验证框架**。这个框架被设计用来增强 `TextGrad`（一个用于复合AI系统进行文本优化的框架）的能力。`TextualVerifier` 通过引入自我验证机制，确保了AI系统在决策过程中的推理有效性。这直接属于“构建、改进或演化 LLM智能体”的范畴，特别是“改进”和“演化”这两个方面。 2.  **第二步：正面指标** - 论文明确包含了多个我的核心关注点： - **自我演化**: 论文的核心是 `self-verification`（自我验证），这是 `Self-Correction`（自我修正）和 `Self-Reflection`（自我反思）的具体实现，是自我演化机制的关键组成部分。 - **智能体能力**: 论文关注 `reasoning`（推理）步骤的有效性，并使用 `chain-of-thought`（思维链）进行分解，这与智能体的规划和推理能力紧密相关。 - **核心范式**: 论文研究的是一个“复合AI系统”的优化，这属于 `Agentic AI` 的范畴。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或水印等排除主题。它关注的是推理过程的**有效性**，而非伦理安全。同时，论文完全基于文本，不涉及视觉或多模态内容，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美地符合“保留”条件。它不是在提升LLM本身的基础数学或逻辑能力，而是在为智能体系统（`TextGrad`）构建一个**元层面的验证机制**，用于检查和验证其多步推理过程。这正是智能体框架中关于规划和推理的高级研究，而非基础模型能力的改进。 5.  **第五步：最终决策** - 综合以上分析，这篇论文的核心贡献是提出了一种名为 `TextualVerifier` 的自我验证框架，用于增强复合AI系统（可视为一种LLM智能体）的推理可靠性。这种自我验证能力是智能体实现自我反思和自我演化的关键一步。因此，该论文精准地契合了我对“自我演化”方向的研究目标，应当被保留。"
    },
    {
        "index": "#45",
        "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks",
        "link": "/arxiv/2511.04662",
        "arxiv_id": "2511.04662",
        "authors": "Yu Feng, Nathaniel Weir, Kaj Bostrom, Sam Bayless, Darion Cassel, Sapana Chaudhary, Benjamin Kiesl-Reiter, Huzefa Rangwala",
        "summary": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT's verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-11-06",
        "category": "cs.CL",
        "crawl_time": "2025-11-07T11:00:04.379311",
        "filter_reason": "这篇论文符合筛选标准，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - 论文的本质是提出一种名为 VeriCoT 的新方法，其核心功能是验证并利用 LLM 的思维链推理过程。虽然它看起来像是对 CoT 推理能力的改进，但其关键贡献在于将验证信号用于**“推理时的自我反思”**和**“通过微调进行自我完善”**。 - 这不属于“非演化型应用”，因为其目标不是解决法律或生物领域的具体问题，而是提出一种通用的、能让模型自我完善的机制。 - 这也不属于“非Agentic的推理”。虽然它处理的是 CoT，但它超越了单纯提升模型基础推理能力的范畴。它构建了一个包含“推理-验证-反思/改进”的闭环，这正是智能体自主性和演化能力的体现。因此，这篇论文的核心贡献在于为 LLM 智能体提供了一种**自我演化的机制**。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点： - **自我演化**: 论文的核心贡献就是提出一种能让模型自我完善的机制。 - **自我反思**: 明确提到利用验证信号进行 \"inference-time self-reflection\"。 - **自我完善**: 通过 \"supervised fine-tuning (SFT)\" 和 \"preference fine-tuning (PFT)\" 来迭代改进模型，这直接对应了 `Self-Improvement` 和 `Iterative Improvement`。 3.  **第三步：排除标准** - **安全与对齐**: 尽管论文涉及“识别有缺陷的推理”，这与可解释性和幻觉检测相关，但其**主要贡献**并非安全或对齐本身，而是利用这种验证来驱动模型的自我演化和能力提升。因此，它不应被归为此类排除项。 - **多模态与视觉**: 论文不涉及多模态内容。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美地符合“保留”条件。它不是简单地提出一个新的 CoT 变体来提升数学或逻辑准确率，而是构建了一个**验证和反思的框架**，这是智能体在复杂任务中进行可靠多步推理的关键。它将 CoT 从一个单向的推理过程，变成了一个可验证、可反思、可迭代的智能体核心能力。 - **自我演化的应用**: 这篇论文是“自我演化应用”例外情况的典型范例。尽管它在 LegalBench 和 BioASQ 等特定领域数据集上进行了实验，但其核心是提出一种**新的“自我演化”机制**。因此，即使有应用背景，也应被保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一种通过逻辑验证来驱动 LLM 进行自我反思和自我完善的机制。这直接命中了研究课题中的“自我演化”方向，并为智能体的“自我反思”能力提供了具体的方法论。它不是简单的应用或基础推理能力提升，而是构建了一个能让智能体自主演化的关键组件。因此，这篇论文高度符合研究范围，应被**保留**。"
    },
    {
        "index": "#48",
        "title": "Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper",
        "link": "/arxiv/2511.04583",
        "arxiv_id": "2511.04583",
        "authors": "Atsuyuki Miyai, Mashiro Toyooka, Takashi Otonari, Zaiying Zhao, Kiyoharu Aizawa",
        "summary": "Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-11-06",
        "category": "cs.CL",
        "crawl_time": "2025-11-07T11:00:04.381498",
        "filter_reason": "这篇论文完全符合你的研究范围。 1.  **核心判断 (第一步):** 论文的核心贡献是构建一个名为 \"Jr. AI Scientist\" 的自主AI智能体系统。它不是将现有智能体作为工具应用到一个新领域，而是**构建了一个新的智能体框架**来执行复杂的科研任务。这完全符合第一步的“保留”标准。 2.  **正面指标 (第二步):** 论文明确包含了你的多个核心关注点： *   **核心范式:** 论文描述了一个 `LLM-based Agent` (\"autonomous AI scientist system\")。 *   **智能体能力:** 摘要中详细描述了智能体的工作流程，这直接对应了你的研究焦点： *   `Planning`: \"formulates novel hypotheses for improvement\" (规划改进方案)。 *   `Self-Reflection`: \"analyzes its limitations\" (分析局限性)。 *   `Tool Use`: \"leverages modern coding agents to handle complex, multi-file implementations\" (使用编码智能体作为工具)。 *   **演化机制:** 整个流程——从一篇基线论文出发，通过分析、假设、实验，最终生成一篇改进的新论文——体现了 `Self-Improvement` 和 `Iterative Improvement` 的迭代演化思想。 3.  **排除标准 (第三步):** 论文虽然包含 \"Risk Report\"，但其主要贡献是构建和评估智能体系统本身，而不是研究安全、对齐或可解释性。风险报告是对所构建系统的评估和讨论，而非研究的核心方法论。因此，它不触犯第三步的排除标准。 4.  **特殊与模糊情况 (第四步):** *   **推理/规划:** 该论文是关于智能体如何进行复杂的多步规划和推理（整个科研工作流），而不是提升LLM本身的基础推理能力。这符合“保留”的条件。 *   **自我演化的应用:** 这不是一个简单的应用，其核心就是提出一个能够自主进行科研探索的智能体，这本身就是一种智能体构建和演化的研究。 **最终决策:** 该论文的核心是提出并评估一个具备规划、工具使用、自我反思和迭代改进能力的LLM智能体，旨在实现自主的科学探索。这与你的研究目标——“构建、改进或演化LLM智能体”——高度契合，属于单智能体和自我演化方向的交叉研究。因此，应予以保留。"
    },
    {
        "index": "#10",
        "title": "The Peril of Preference: Why GRPO fails on Ordinal Rewards",
        "link": "/arxiv/2511.04439",
        "arxiv_id": "2511.04439",
        "authors": "Anisha Garg, Ganesh Venkatesh",
        "summary": "Group-relative Policy Optimization's (GRPO) simplicity makes it highly desirable for adapting LLMs to become experts at specific tasks. But this simplicity also makes it ill-specified as we seek to enhance RL training with richer, non-binary feedback. When using ordinal rewards to give partial credit, GRPO's simplicity starts to hurt, as its group-average baseline often assigns a positive advantage to failed trajectories and reinforces incorrect behavior. We introduce Correctness Relative Policy Optimization (CoRPO), a new formulation that solves this flaw. CoRPO uses an adaptive baseline that enforces a minimum quality threshold, ensuring failed solutions are never positively reinforced. Once the policy consistently meets this threshold, the baseline automatically transitions to a relative preference mode, pushing the model to find optimal solutions rather than just \"acceptable\" ones. We empirically validate CoRPO on a code verification task, where it demonstrates more stable convergence and better out-of-domain generalization. This work represents a critical step in our broader research program to enable LLMs to learn genuinely new capabilities through reinforcement learning. We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback - progressing from binary to ordinal rewards in this work, and onward to denser, per-step supervision.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.884857",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种新的强化学习算法 `CoRPO` (Correctness Relative Policy Optimization)，用于解决现有 `GRPO` 算法在处理序数奖励时的缺陷。这并非将LLM或已有智能体框架作为工具去解决一个特定领域（如代码验证）的问题，而是**提出了一种改进LLM学习机制的基础方法**。这种通过环境反馈（奖励）进行学习和优化的过程，是LLM智能体实现“自我演化”的核心技术路径之一。因此，论文的本质是关于**改进LLM的自我演化机制**，应予以保留。 2.  **第二步：正面指标** 论文明确涉及了多个核心关注点： *   **自我演化**: 论文的核心是改进强化学习（RL）这一自我演化的关键范式。摘要中提到“enable LLMs to learn genuinely new capabilities through reinforcement learning”和“progressing from binary to ordinal rewards”，这直接指向了智能体通过反馈进行迭代和自我完善的过程。 *   **自我改进**: `CoRPO` 算法的设计目标就是让模型能够更有效地进行自我改进，避免强化错误行为，并从“可接受”的解向“最优”解推进。 *   **迭代改进**: 强化学习本身就是一种迭代改进的框架，论文提出的算法旨在使这一过程更稳定、更高效。 3.  **第三步：排除标准** 论文的主要贡献不涉及安全、对齐、可解释性或多模态。虽然它在代码任务上进行验证，但其核心是算法创新，而非代码应用本身，因此不触犯任何排除标准。 4.  **第四步：处理特殊和模糊情况** 此处最相关的规则是“自我演化的应用”的例外情况。规则明确指出：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域（如‘用于化学实验的自我演化智能体’），也应该保留。” 本论文正是这种情况：其核心贡献是提出了一种新的**自我演化机制**（`CoRPO`算法），并应用在“代码验证”这一特定领域来验证其有效性。这完全符合保留的例外规则。 **最终决策**: 综合以上分析，尽管这篇论文没有提出一个完整的、包含规划、记忆、工具使用等模块的智能体架构，但它触及了“自我演化”这一研究方向的更底层、更根本的问题：**智能体如何更有效地从反馈中学习**。提出一种新的、更强大的学习算法，是推动LLM智能体能力演化的关键一步。因此，这篇论文的核心贡献与“LLM智能体及其演化”的研究课题高度相关，特别是与“自我演化”方向紧密契合。应判定为 **True**。"
    },
    {
        "index": "#11",
        "title": "Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach",
        "link": "/arxiv/2511.04393",
        "arxiv_id": "2511.04393",
        "authors": "Chanwoo Park, Ziyang Chen, Asuman Ozdaglar, Kaiqing Zhang",
        "summary": "Large language models (LLMs) are increasingly deployed as \"agents\" for decision-making (DM) in interactive and dynamic environments. Yet, since they were not originally designed for DM, recent studies show that LLMs can struggle even in basic online DM problems, failing to achieve low regret or an effective exploration-exploitation tradeoff. To address this, we introduce Iterative Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure that repeatedly distills low-regret decision trajectories back into the base model. At each iteration, the model rolls out multiple decision trajectories, selects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior methods that (a) distill action sequences from known DM algorithms or (b) rely on manually crafted chain-of-thought templates, our approach leverages the regret metric to elicit the model's own DM ability and reasoning rationales. This reliance on model-generated reasoning avoids rigid output engineering and provides more flexible, natural-language training signals. Empirical results show that Iterative RMFT improves LLMs' DM performance across diverse models - from Transformers with numerical input/output, to open-weight LLMs, and advanced closed-weight models like GPT-4o mini. Its flexibility in output and reasoning formats enables generalization across tasks with varying horizons, action spaces, reward processes, and natural-language contexts. Finally, we provide theoretical insight showing that a single-layer Transformer under this paradigm can act as a no-regret learner in a simplified setting. Overall, Iterative RMFT offers a principled and general post-training framework for enhancing LLMs' decision-making capabilities.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.885312",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献精准地命中了你定义的“自我演化”和“单智能体”方向。 1.  **第一步：核心判断——保留** - 论文的本质是**构建一个改进LLM智能体的新方法论**。它提出了一种名为“迭代遗憾最小化微调”的后训练框架，其唯一目的就是让LLM成为一个更好的决策智能体。这完全符合“构建、改进或演化LLM智能体”的核心目标。 - 它不是将现有智能体框架应用到某个领域，而是**提出了一种新的、通用的智能体能力增强机制**。 - 它不是关于提升LLM的基础推理能力（如解数学题），而是关于提升智能体在**动态、交互式环境中的决策能力**，这是一个典型的Agentic问题。 2.  **第二步：正面指标——高度匹配** - **核心范式**: 论文明确研究 `LLM-based Agents`。 - **自我演化**: 这是最关键的匹配点。论文提出的 `Iterative RMFT` 是一个典型的**自我演化机制**。其过程是：模型生成多个决策轨迹 -> 根据regret指标筛选出最优轨迹 -> 基于这些最优轨迹对自身进行微调。这个“生成-评估-学习”的闭环迭代过程，完美诠释了智能体通过经验反馈进行自我完善和迭代。 - **智能体能力**: 论文的核心是提升智能体的 `Decision-Making` (决策) 能力，这是 `Planning` 和 `Reasoning` 在动态环境中的具体体现。 3.  **第三步：排除标准——未触发** - 论文的主要贡献是关于提升智能体的决策性能，而非 `Safety`、`Alignment` 或 `Interpretability`。 - 论文不涉及 `Vision` 或多模态内容，专注于文本LLM。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 该论文研究的是智能体在交互环境中的决策，这属于Agentic框架下的推理与规划，而非静态的、非Agentic的基础推理能力提升。因此，符合保留条件。 - **自我演化的应用**: 该论文本身就是提出一种新的“自我演化”机制，因此完全符合保留规则。 **总结**: 该论文的核心贡献是提出了一种名为Iterative RMFT的**后训练框架**，通过一个迭代式的自我微调过程，让LLM智能体能够从自身的决策经验中学习，从而**演化**出更强的决策能力。这精准地对应了你研究课题中的“自我演化”方向，同时也属于“单智能体”能力提升的范畴。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#12",
        "title": "Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning",
        "link": "/arxiv/2511.04341",
        "arxiv_id": "2511.04341",
        "authors": "Nick Oh, Fernand Gobet",
        "summary": "Test-time reasoning architectures such as those following the Generate-Verify paradigm -- where a model iteratively refines or verifies its own generated outputs -- prioritise generation and verification but exclude the monitoring processes that determine when and how reasoning should begin. This omission may contribute to the prefix dominance trap, in which models commit early to suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy loss. We address this architectural gap by formalising Flavell's and Nelson and Narens' metacognitive theories into computational specifications, proposing the Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify paradigm by adding explicit monitoring that captures metacognitive experiences (from difficulty assessments to confidence judgements) before generation begins and refines future monitoring through verification feedback. Though we present no empirical validation, this work provides the first systematic computational translation of foundational metacognitive theories, offering a principled vocabulary for understanding reasoning system failures and suggesting specific architectural interventions for future test-time reasoning designs.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.885734",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **第一步核心判断：论文的核心贡献是构建和改进LLM智能体框架。** 论文的核心是提出一个名为“Monitor-Generate-Verify (MGV)”的新框架。这个框架并非将LLM作为工具应用于某个特定领域，而是直接针对LLM智能体在推理过程中的架构缺陷进行改进。它通过形式化元认知理论，为LLM智能体增加了一个关键的“监控”模块，这属于构建和改进智能体方法论的范畴，因此应**保留**。 2.  **第二步正面指标：论文高度契合你的核心关注点。** -   **自我演化:** 论文的核心机制之一是“通过验证反馈来优化未来的监控”。这构成了一个明确的反馈循环，使智能体能够根据经验（验证结果）来调整和改进自身的行为（监控策略），这正是“自我完善和迭代”的体现。 -   **自我反思/自我修正:** 论文引入的“监控”过程，其理论基础是“元认知理论”，这直接对应了智能体的自我反思能力。它让智能体在行动（生成）之前先评估任务难度和自身信心，并在行动后进行反思（验证），从而避免陷入错误的推理路径，这是一种高级的自我修正机制。 -   **规划/推理:** 论文聚焦于“test-time reasoning architectures”，即智能体在执行任务时的推理架构。它改进了“Generate-Verify”这一已有的推理范式，使其更加鲁棒，这完全属于智能体规划和多步推理的研究范畴。 3.  **第四步特殊情况的精准匹配：** -   **推理/规划:** 这篇论文是关于智能体如何进行规划和推理的典型案例。它不是在提升LLM的基础数学或逻辑能力，而是在设计一个更优的**智能体推理框架**。它扩展了ReAct、Generate-Verify等Agentic框架，因此完全符合“保留”的条件。 4.  **第三步排除标准：论文未触及任何排除领域。** 论文的研究焦点是智能体的推理架构和元认知机制，不涉及安全、对齐、可解释性，也未涉及多模态或视觉内容。 **总结:** 该论文的核心贡献是提出了一种名为MGV的新颖智能体框架，该框架通过引入基于元认知理论的显式“监控”机制，显著增强了LLM智能体的自我反思和自我修正能力。其“通过验证反馈优化未来监控”的设计，构成了一个清晰的自我演化循环。因此，这篇论文精准地命中了你研究目标中的“单智能体”和“自我演化”两个核心方向，是一篇高度相关的前沿研究。"
    },
    {
        "index": "#17",
        "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization",
        "link": "/arxiv/2511.04285",
        "arxiv_id": "2511.04285",
        "authors": "Zeng Zhiyuan, Jiashuo Liu, Zhangyue Yin, Ge Zhang, Wenhao Huang, Xipeng Qiu",
        "summary": "While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for training large reasoning models, its training dynamics harbor a critical challenge: RL overfitting, where models gain training rewards but lose generalization. Our analysis reveals this is driven by policy over-specialization and catastrophic forgetting of diverse solutions generated during training. Standard optimization discards this valuable inter-step policy diversity. To address this, we introduce RLoop, a self-improving framework built on iterative policy initialization. RLoop transforms the standard training process into a virtuous cycle: it first uses RL to explore the solution space from a given policy, then filters the successful trajectories to create an expert dataset. This dataset is used via Rejection-sampling Fine-Tuning (RFT) to refine the initial policy, creating a superior starting point for the next iteration. This loop of exploration and exploitation via iterative re-initialization effectively converts transient policy variations into robust performance gains. Our experiments show RLoop mitigates forgetting and substantially improves generalization, boosting average accuracy by 9% and pass@32 by over 15% compared to vanilla RL.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.888311",
        "filter_reason": "这篇论文完全符合您的研究范围，核心贡献在于提出了一种新颖的“自我演化”机制。 以下是根据您的筛选标准进行的详细判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是应用现有技术，而是构建了一个名为RLoop的**新框架**。其核心目标是解决强化学习训练中的过拟合和泛化问题，但它实现这一目标的方式是提出一个**自我改进的循环机制**。这个机制通过“探索（RL）-利用（RFT）-再初始化”的迭代过程，让智能体的策略不断自我完善。这直接对应了您研究目标中的“自我演化”方向，其核心贡献是方法论和框架本身，而非特定领域的应用。 2.  **第二步：正面指标** - 论文包含了多个核心关注点的关键词： - **核心范式**: `Self-Evolving` (摘要中明确提到 \"self-improving framework\")。 - **演化机制**: `Self-Improvement` (标题和摘要中直接使用), `Iterative Improvement` (摘要中描述为 \"iterative policy initialization\" 和 \"loop of exploration and exploitation\")。 - 这些正面指标强烈表明该论文与您的研究焦点高度相关。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全与对齐（Safety, Alignment等），也没有涉及多模态或视觉（Vision, MLLMs等）。因此，它没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文虽然涉及“大型推理模型”，但其核心贡献并非提出一种新的推理技巧（如CoT变体），而是提出一个**训练框架**来提升模型（作为智能体策略）的整体性能和泛化能力。这属于“关于智能体如何进行规划或在复杂任务中进行多步推理”的范畴，因为它改进的是智能体底层的决策策略，因此应该**保留**。 - **自我演化的应用**: 该论文提出的RLoop框架本身就是一种新的“自我演化”机制。即使它被应用于数学或代码推理等具体任务上，根据您的规则，只要核心是提出新的演化机制，就应该保留。本论文正是这种情况。 **最终决策**: 综合以上分析，该论文的核心贡献是提出了一种名为RLoop的**自我改进框架**，通过迭代式的策略初始化，让智能体在强化学习训练中能够不断自我完善，有效解决了过拟合问题并提升了泛化能力。这完全符合您对“自我演化”方向的定义，即“智能体通过经验、反思或环境反馈进行自我完善和迭代”。因此，这篇论文是您研究课题下的高相关性前沿论文，应予以保留。"
    },
    {
        "index": "#9",
        "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context",
        "link": "/arxiv/2511.04464",
        "arxiv_id": "2511.04464",
        "authors": "Carnot Braun, Rafael O. Jarczewski, Gabriel U. Talasso, Leandro A. Villas, Allan M. de Souza",
        "summary": "Traditional vehicle routing systems efficiently optimize singular metrics like time or distance, and when considering multiple metrics, they need more processes to optimize . However, they lack the capability to interpret and integrate the complex, semantic, and dynamic contexts of human drivers, such as multi-step tasks, situational constraints, or urgent needs. This paper introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a hybrid agentic assistant designed to augment classical pathfinding algorithms with contextual reasoning. Our approach employs a Large Language Model (LLM) agent that operates on a candidate set of routes generated by a multi-objective (time, CO2) Dijkstra algorithm. The agent evaluates these options against user-provided tasks, preferences, and avoidance rules by leveraging a pre-processed geospatial cache of urban Points of Interest (POIs). In a benchmark of realistic urban scenarios, PAVe successfully used complex user intent into appropriate route modifications, achieving over 88% accuracy in its initial route selections with a local model. We conclude that combining classical routing algorithms with an LLM-based semantic reasoning layer is a robust and effective approach for creating personalized, adaptive, and scalable solutions for urban mobility optimization.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.884399",
        "filter_reason": "这篇论文符合你的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是构建了一个名为PAVe（Personalized Agentic Vehicular Routing）的**新型LLM智能体框架**。它并非简单地将一个现成的LLM或智能体框架应用到车辆路径规划领域，而是提出了一种**混合智能体架构**：将经典算法（多目标Dijkstra）作为工具生成候选方案，再由LLM智能体进行语义层面的推理和决策。这种“算法+智能体”的设计本身就是对LLM智能体构建方法的一种探索和改进，因此其本质是关于“构建LLM智能体”，符合保留标准。它不属于“非演化型应用”，因为其创新点在于智能体的架构和工作流，而非应用领域本身。 2.  **第二步：正面指标——高度相关** 论文包含了多个核心关注点： *   **核心范式**: 明确提出了 `Agentic Vehicular Routing` 和 `LLM agent`。 *   **智能体能力**: 论文的核心是智能体的**规划**能力。LLM智能体负责评估和选择由传统算法生成的路径，这属于复杂任务中的多步推理和决策过程。同时，它利用了**工具使用**能力，其工具是“预处理的地理空间POI缓存”，通过查询这个知识库来辅助决策。 3.  **第三步：排除标准——未触发** 论文的主要贡献不在于安全、对齐、可解释性，也未涉及多模态或视觉模型作为研究核心。POI缓存是作为智能体查询的结构化数据工具，而非研究的核心视觉模型。 4.  **第四步：处理特殊和模糊情况——符合保留规则** 论文的研究内容完美契合“推理/规划”的保留规则。它不是在提升LLM本身的基础数学或逻辑推理能力，而是在构建一个**智能体框架**来解决一个需要复杂规划和推理的现实世界问题（车辆路径规划）。智能体的规划过程（评估候选路径）是其核心贡献。 **最终决策**: 这篇论文的核心贡献在于提出了一种新的LLM智能体架构（PAVe），该架构通过结合经典算法和LLM的语义推理能力，并利用外部工具（POI缓存），来解决复杂的个性化路径规划问题。这完全符合你研究目标中“构建、改进LLM智能体”以及“单智能体”方向下的“规划”和“工具使用”子方向。因此，应判定为 **True**。"
    },
    {
        "index": "#18",
        "title": "Shared Spatial Memory Through Predictive Coding",
        "link": "/arxiv/2511.04235",
        "arxiv_id": "2511.04235",
        "authors": "Zhengru Fang, Yu Guo, Jingjing Wang, Yuang Zhang, Haonan An, Yinhai Wang, Yuguang Fang",
        "summary": "Sharing and reconstructing a consistent spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulate coordination as the minimization of mutual uncertainty among agents. Instantiated as an information bottleneck objective, it prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners' locations: an artificial analogue of hippocampal social place cells (SPCs). These social representations are further enacted by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to social collective intelligence.",
        "subjects": "Artificial Intelligence, Computational Engineering, Finance, and Science",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.888812",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出了一种**多智能体预测编码框架**，用于解决多智能体系统中的空间记忆共享和协调问题。这直接对应了你研究焦点中的“**多智能体**”方向。它不是将现有框架作为工具去解决某个特定领域的问题，而是**构建了一个新的多智能体协作与通信的框架和方法论**，因此符合保留标准。 2.  **第二步：正面指标——高度匹配** 论文包含了大量你的核心关注点： *   **核心范式**: `Multi-Agent Systems (MAS)` 是论文的绝对核心。 *   **多智能体**: 论文深入探讨了 `Collaboration` (协调)、`Communication` (通信机制)，并提出了一个让智能体学习“何时、与谁、沟通什么”的机制。 *   **智能体能力**: 论文的基石是 `Memory` (空间记忆)，具体是一种“网格单元样度量”的内部空间编码和“社会位置细胞”的伙伴位置编码。 *   **演化机制**: 论文强调了能力的“涌现”和“发展”。例如，内部空间编码是“自发出现”的，通信机制是“逐渐发展”的。这完全符合“**自我演化**”的精髓，即智能体通过与环境的交互和内部的学习机制，自主地发展出复杂的能力。 3.  **第三步：排除标准——未触发** 论文的主要贡献不涉及安全、对齐、可解释性，也未涉及多模态或视觉模型。因此，没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文中的智能体使用“分层强化学习策略”来主动探索和决策，这是一种典型的智能体规划和行动方式，而非提升LLM本身的基础推理能力，因此符合保留条件。 *   **自我演化的应用**: 论文的核心就是提出一种新的“自我演化/涌现”机制（通过预测编码驱动社会表征的出现），即使它是在Memory-Maze这个特定基准上测试的，根据你的规则，也应该保留。 **核心依据总结**: 这篇论文的本质是**探索多智能体如何通过一个统一的、自监督的预测驱动框架，自发地演化出高效的通信机制和共享的社会空间记忆**。它直接贡献于“多智能体”和“自我演化”这两个核心研究方向。虽然论文没有明确使用LLM作为智能体的大脑，但它所研究的**智能体架构、记忆机制、通信协议和演化原理**，是构建更高级LLM智能体和多智能体系统的前沿基础。作为顶尖AI研究员，这类关于智能体底层机制和原理的突破性研究，正是你课题所需要的。它回答了“智能体如何协同演化”这一根本问题，而非仅仅应用现有技术。"
    },
    {
        "index": "#26",
        "title": "ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering",
        "link": "/arxiv/2511.03985",
        "arxiv_id": "2511.03985",
        "authors": "Zhuowen Yuan, Tao Liu, Yang Yang, Yang Wang, Feng Qi, Kaushik Rangadurai, Bo Li, Shuang Yang",
        "summary": "Recent LLM-based agents have demonstrated strong capabilities in automated ML engineering. However, they heavily rely on repeated full training runs to evaluate candidate solutions, resulting in significant computational overhead, limited scalability to large search spaces, and slow iteration cycles. To address these challenges, we introduce ArchPilot, a multi-agent system that integrates architecture generation, proxy-based evaluation, and adaptive search into a unified framework. ArchPilot consists of three specialized agents: an orchestration agent that coordinates the search process using a Monte Carlo Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and manages memory of previous candidates; a generation agent that iteratively generates, improves, and debugs candidate architectures; and an evaluation agent that executes proxy training runs, generates and optimizes proxy functions, and aggregates the proxy scores into a fidelity-aware performance metric. This multi-agent collaboration allows ArchPilot to prioritize high-potential candidates with minimal reliance on expensive full training runs, facilitating efficient ML engineering under limited budgets. Experiments on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE and ML-Master, validating the effectiveness of our multi-agent system.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.898038",
        "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出一个名为 **ArchPilot** 的 **多智能体系统**。它并非简单地将现有智能体框架应用于机器学习工程领域，而是为了解决现有智能体方法（依赖重复的完整训练）的固有缺陷（计算开销大、迭代慢），而**构建了一个全新的、由多个专门化智能体协作组成的框架**。这完全符合“构建、改进或演化 LLM智能体”的核心目标，属于方法论和新框架的贡献，而非简单的领域应用。 2.  **第二步：正面指标——高度相关** 论文摘要中包含了大量您关注的核心关键词和概念： *   **核心范式**: 明确提出了 `Multi-Agent System`。 *   **智能体能力**: *   `Planning`: 编排智能体使用“受蒙特卡洛树搜索（MCTS）启发的算法”来协调搜索过程，这是一种复杂的规划机制。 *   `Memory`: 编排智能体“管理先前候选者的记忆”。 *   `Self-Correction` / `Self-Improvement`: 生成智能体“迭代地生成、改进和调试候选架构”，这体现了智能体的自我完善能力。 *   **多智能体**: 论文的核心就是 `Multi-Agent`，并明确提到了“多智能体协作”和三个专门化智能体之间的分工。 3.  **第三步：排除标准——未触发** 论文的主要贡献不涉及安全、对齐、可解释性，也未涉及多模态或视觉模型。因此，没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文中的规划是智能体层面的规划（MCTS-inspired search for coordination），而非提升LLM基础推理能力，因此符合保留条件。 *   **自我演化的应用**: 虽然论文应用于机器学习工程领域，但其核心是提出一种新的多智能体协作框架，该框架内含了迭代改进和调试的机制，这可以看作是一种在任务执行过程中的自我演化。因此，它不属于“非演化型应用”的排除范畴。 **最终决策**: 综合分析，这篇论文的核心贡献在于**设计并实现了一个新颖的多智能体协作框架**，以解决自动化机器学习中的效率问题。该框架包含了规划、记忆、自我修正等多种智能体核心能力，并重点研究了智能体间的协作机制。这与您研究课题中的“多智能体”和“自我演化”方向高度契合，是一篇非常相关的前沿论文，应当保留。"
    },
    {
        "index": "#32",
        "title": "Scaling Agent Learning via Experience Synthesis",
        "link": "/arxiv/2511.03773",
        "arxiv_id": "2511.03773",
        "authors": "Zhaorun Chen, Zhuokai Zhao, Kai Zhang, Bo Liu, Qi Qi, Yifan Wu, Tarun Kalluri, Sara Cao, Yuanhao Xiong, Haibo Tong, Huaxiu Yao, Hengduo Li, Jiacheng Zhu, Xian Li, Dawn Song, Bo Li, Jason Weston, Dat Huynh",
        "summary": "While reinforcement learning (RL) can empower large language model (LLM) agents by enabling self-improvement through interaction, its practical adoption remains challenging due to costly rollouts, limited task diversity, unreliable reward signals, and infrastructure complexity, all of which obstruct the collection of scalable experience data. To address these challenges, we introduce DreamGym, the first unified framework designed to synthesize diverse experiences with scalability in mind to enable effective online RL training for autonomous agents. Rather than relying on expensive real-environment rollouts, DreamGym distills environment dynamics into a reasoning-based experience model that derives consistent state transitions and feedback signals through step-by-step reasoning, enabling scalable agent rollout collection for RL. To improve the stability and quality of transitions, DreamGym leverages an experience replay buffer initialized with offline real-world data and continuously enriched with fresh interactions to actively support agent training. To improve knowledge acquisition, DreamGym adaptively generates new tasks that challenge the current agent policy, enabling more effective online curriculum learning. Experiments across diverse environments and agent backbones demonstrate that DreamGym substantially improves RL training, both in fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in RL-ready but costly settings, it matches GRPO and PPO performance using only synthetic interactions. When transferring a policy trained purely on synthetic experiences to real-environment RL, DreamGym yields significant additional performance gains while requiring far fewer real-world interactions, providing a scalable warm-start strategy for general-purpose RL.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-05",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.906249",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献精准地落在“自我演化”方向上。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM智能体作为工具去解决某个特定领域的问题，而是提出了一种全新的、旨在**改进LLM智能体学习与演化能力**的方法论框架。论文的核心是解决“如何让智能体更高效、更规模化地通过经验进行自我完善”这一根本性问题。其提出的DreamGym框架，通过合成经验来赋能智能体的强化学习训练，这直接对应了您研究目标中的“构建、改进或演化 LLM智能体”。 2.  **第二步：正面指标** - 论文包含了大量您的核心关注点： - **核心范式**: `LLM-based Agents`, `Self-Evolving`。摘要开篇就点明“empower large language model (LLM) agents by enabling self-improvement”。 - **演化机制**: `Self-Improvement`, `Iterative Improvement`。整个框架都是为了实现智能体的自我改进和迭代优化。论文还提到了“online curriculum learning”，这是一种高级的自我演化策略。 - **智能体能力**: 论文通过强化学习（RL）来训练智能体，这必然涉及智能体在环境中的交互、规划和决策，这些都是Agentic AI的核心能力。 3.  **第三步：排除标准** - 论文的主要贡献**不涉及**安全与对齐、多模态与视觉等排除领域。虽然它在WebArena等环境中进行测试，但这些环境是作为验证其智能体演化能力的试验场，而非研究焦点。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中提到的“reasoning-based experience model”是用于合成经验的工具，是服务于智能体演化框架的一个组件，而不是论文的核心贡献。这符合“保留”的条件，因为它是在智能体框架的背景下使用推理。 - **自我演化的应用**: 论文虽然将DreamGym应用在了WebArena等具体任务上，但其核心是提出“DreamGym”这一**通用的自我演化机制**。这完全符合您设定的“例外”情况，即“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域，也应该保留”。 **最终决策**: 该论文的核心贡献是构建了一个名为DreamGym的创新框架，旨在通过合成经验来克服传统强化学习在训练LLM智能体时面临的成本和可扩展性瓶颈，从而实现智能体更高效的自我完善和演化。这精准地命中了您研究课题中的“自我演化”方向，并且是关于构建和改进智能体方法论的前沿研究。因此，该论文应被保留。"
    },
    {
        "index": "#23",
        "title": "Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents",
        "link": "/arxiv/2511.04076",
        "arxiv_id": "2511.04076",
        "authors": "Hao Li, Haotian Chen, Ruoyuan Gong, Juanjuan Wang, Hao Jiang",
        "summary": "Redistricting plays a central role in shaping how votes are translated into political power. While existing computational methods primarily aim to generate large ensembles of legally valid districting plans, they often neglect the strategic dynamics involved in the selection process. This oversight creates opportunities for partisan actors to cherry-pick maps that, while technically compliant, are politically advantageous. Simply satisfying formal constraints does not ensure fairness when the selection process itself can be manipulated. We propose \\textbf{Agentmandering}, a framework that reimagines redistricting as a turn-based negotiation between two agents representing opposing political interests. Drawing inspiration from game-theoretic ideas, particularly the \\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction into the redistricting process via large language model (LLM) agents. Agents alternate between selecting and freezing districts from a small set of candidate maps, gradually partitioning the state through constrained and interpretable choices. Evaluation on post-2020 U.S. Census data across all states shows that Agentmandering significantly reduces partisan bias and unfairness, while achieving 2 to 3 orders of magnitude lower variance than standard baselines. These results demonstrate both fairness and stability, especially in swing-state scenarios. Our code is available at https://github.com/Lihaogx/AgentMandering.",
        "subjects": "Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.896479",
        "filter_reason": "这篇论文符合研究范围，应予以保留。 **核心判断与分析过程:** 1.  **第一步：核心判断——论文的本质是什么？** - **保留**。这篇论文的本质是构建一个新颖的**多智能体系统（MAS）**。虽然它的应用场景是“选区划分”，但其核心贡献并非“用LLM解决选区划分问题”，而是提出了一个名为“Agentmandering”的**框架**。这个框架的核心是设计了一种基于博弈论的、让两个LLM智能体进行“轮流谈判”的交互协议。这完全符合“构建、改进或演化LLM智能体”的核心目标，特别是“多智能体”方向。它不是简单地将已有智能体作为工具应用，而是创新了智能体之间的协作与博弈模式，因此不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标——论文是否包含我的核心关注点？** - 论文包含了多个高度相关的核心关注点： - **核心范式**: `Multi-Agent Systems (MAS)` 是论文的核心。 - **多智能体**: `Negotiation`（谈判）是论文描述的智能体交互的核心机制。虽然不是`Collaboration`（协作），但`Negotiation`是多智能体研究中的一个重要子领域，涉及智能体间的策略互动。 - **智能体能力**: `Planning`（规划）是智能体在每一步决策时必须进行的核心活动，即规划下一步应该“选择”还是“冻结”哪个选区以最大化自身利益。 3.  **第三步：排除标准——是否为我的研究焦点之外？** - 论文不涉及任何排除标准。其主题是关于智能体的博弈和公平性，而非`Safety`、`Alignment`、`Interpretability`或`Vision`等。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 该论文完美符合“保留”条件。它研究的是智能体在一个复杂的、动态的博弈环境中如何进行多步推理和规划（`ReAct`风格的交互），而不是提升LLM本身的基础数学或逻辑推理能力。智能体的每一步行动（选择/冻结）都是基于当前状态的战略规划。 **最终决策:** 综合以上分析，这篇论文的核心贡献在于提出了一种新颖的**多智能体谈判框架**，以解决一个复杂的战略问题。它直接命中了研究课题中的“多智能体”方向，探讨了智能体间的`Negotiation`、`Communication`（通过行动）和`Planning`。尽管它有一个具体的应用领域，但其方法论本身是对LLM智能体构建和交互模式的创新，因此完全符合筛选要求。"
    },
    {
        "index": "#72",
        "title": "Learning from Online Videos at Inference Time for Computer-Use Agents",
        "link": "/arxiv/2511.04137",
        "arxiv_id": "2511.04137",
        "authors": "Yujian Liu, Ze Wang, Hao Chen, Ximeng Sun, Xiaodong Yu, Jialian Wu, Jiang Liu, Emad Barsoum, Zicheng Liu, Shiyu Chang",
        "summary": "Computer-use agents can operate computers and automate laborious tasks, but despite recent rapid progress, they still lag behind human users, especially when tasks require domain-specific procedural knowledge about particular applications, platforms, and multi-step workflows. Humans can bridge this gap by watching video tutorials: we search, skim, and selectively imitate short segments that match our current subgoal. In this paper, we study how to enable computer-use agents to learn from online videos at inference time effectively. We propose a framework that retrieves and filters tutorial videos, converts them into structured demonstration trajectories, and dynamically selects trajectories as in-context guidance during execution. Particularly, using a VLM, we infer UI actions, segment videos into short subsequences of actions, and assign each subsequence a textual objective. At inference time, a two-stage selection mechanism dynamically chooses a single trajectory to add in context at each step, focusing the agent on the most helpful local guidance for its next decision. Experiments on two widely used benchmarks show that our framework consistently outperforms strong base agents and variants that use only textual tutorials or transcripts. Analyses highlight the importance of trajectory segmentation and selection, action filtering, and visual information, suggesting that abundant online videos can be systematically distilled into actionable guidance that improves computer-use agents at inference time. Our code is available at https://github.com/UCSB-NLP-Chang/video_demo.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.947381",
        "filter_reason": "这篇论文完全符合你的研究范围，核心判断依据如下： 1.  **第一步：核心判断 (保留)** - 论文的核心贡献是提出一个**新框架**，旨在**改进** \"computer-use agents\"（一种LLM智能体）。该框架使智能体能够在推理时从在线视频中学习，以弥补其在特定领域流程知识上的不足。这直接命中了“构建、改进或演化LLM智能体”的核心目标，属于对单智能体能力的增强，因此应保留。 2.  **第二步：正面指标 (高度相关)** - **核心范式**: 论文明确研究 \"Computer-use agents\"，属于 `LLM-based Agents` 范畴。 - **智能体能力**: - **Tool Use / Tool Augmentation**: 论文的核心机制是将在线视频作为一种新的、动态的知识来源和工具。智能体通过检索、解析和利用视频中的演示轨迹来指导自己的行动，这是一种高级的工具使用形式。 - **Planning**: 论文中的 \"two-stage selection mechanism\" 在每一步动态选择最相关的轨迹作为上下文指导，这本质上是一种增强的、基于外部知识的规划与决策过程。 - **Memory**: 将视频转换为结构化的演示轨迹，并在需要时动态加载到上下文中，这可以看作是一种扩展的、按需调用的外部记忆机制。 3.  **第三步：排除标准 (不适用)** - **安全与对齐**: 论文未涉及安全、对齐或幻觉等问题。 - **多模态与视觉**: 论文确实使用了VLM（视觉语言模型），但它的角色是作为智能体**感知环境（UI界面）和解析工具（视频）**的组件，而不是研究的核心。研究的核心是**如何利用VLM的输出来构建一个更强大的智能体框架**，这完全符合“除非它们被用作智能体感知环境的工具，而不是研究的核心”这一例外规则。 4.  **第四步：特殊和模糊情况 (符合保留条件)** - **推理/规划**: 该论文的研究内容是关于智能体如何进行规划和决策的，它通过引入外部视频轨迹来增强这一过程，因此属于“保留”范畴，而不是单纯提升LLM的基础推理能力。 **总结**: 这篇论文的本质不是将现有智能体应用于某个领域，而是提出了一种创新的方法论来**增强智能体本身的能力**。它通过引入“在推理时从视频中学习”这一新机制，显著提升了智能体在复杂任务中的表现。这项工作直接贡献于“单智能体”方向下的“工具使用”和“规划”能力，是典型的Agentic AI前沿研究，因此与你的研究课题高度契合。"
    },
    {
        "index": "#82",
        "title": "An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue",
        "link": "/arxiv/2511.04042",
        "arxiv_id": "2511.04042",
        "authors": "Kailun Ji, Xiaoyu Hu, Xinyu Zhang, Jun Chen",
        "summary": "Large-scale disaster Search And Rescue (SAR) operations are persistently challenged by complex terrain and disrupted communications. While Unmanned Aerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area search and supply delivery, yet their effective coordination places a significant cognitive burden on human operators. The core human-machine collaboration bottleneck lies in the ``intention-to-action gap'', which is an error-prone process of translating a high-level rescue objective into a low-level swarm command under high intensity and pressure. To bridge this gap, this study proposes a novel LLM-CRF system that leverages Large Language Models (LLMs) to model and augment human-swarm teaming cognition. The proposed framework initially captures the operator's intention through natural and multi-modal interactions with the device via voice or graphical annotations. It then employs the LLM as a cognitive engine to perform intention comprehension, hierarchical task decomposition, and mission planning for the UAV swarm. This closed-loop framework enables the swarm to act as a proactive partner, providing active feedback in real-time while reducing the need for manual monitoring and control, which considerably advances the efficacy of the SAR task. We evaluate the proposed framework in a simulated SAR scenario. Experimental results demonstrate that, compared to traditional order and command-based interfaces, the proposed LLM-driven approach reduced task completion time by approximately $64.2\\%$ and improved task success rate by $7\\%$. It also leads to a considerable reduction in subjective cognitive workload, with NASA-TLX scores dropping by $42.9\\%$. This work establishes the potential of LLMs to create more intuitive and effective human-swarm collaborations in high-stakes scenarios.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.952822",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。判断依据如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出一个名为“LLM-CRF”的**新框架**，该框架利用LLM作为“认知引擎”来桥接人类意图与无人机集群行动之间的鸿沟。这完全符合“构建、改进或演化LLM智能体的方法论或新框架”的保留标准。虽然论文应用在灾难搜救这一特定领域，但其核心价值在于**如何构建**这样一个智能体系统，而不是简单地应用一个已有框架去解决领域问题。论文详细描述了LLM如何被用于意图理解、分层任务分解和任务规划，这些都是构建智能体的核心方法论。 2.  **第二步：正面指标——高度相关** 论文内容与我的核心关注点高度契合： *   **核心范式**: 论文明确提出了一个基于LLM的智能体框架，属于`Agentic AI`和`LLM-based Agents`。同时，它协调的是一个无人机集群，因此也直接涉及`Multi-Agent Systems (MAS)`。 *   **智能体能力**: 论文的核心功能包括`Planning`（任务分解、任务规划），并且LLM作为认知引擎，其工作方式与`ReAct`等范式有相似之处，即理解目标并规划行动步骤。 *   **多智能体**: 论文的核心是解决人-集群协作问题，其中`Collaboration`（协作）和`Communication`（通信，通过主动反馈实现）是关键要素。 3.  **第三步：排除标准——未触发** 论文的主要贡献是关于提升人机协作的效率和降低认知负荷，而非`Safety`、`Alignment`或`Interpretability`。虽然提到了多模态输入（图形注释），但这只是作为智能体感知人类意图的工具，并非研究的核心，研究的核心是LLM的认知处理和规划框架，因此不触发多模态排除标准。 4.  **第四步：处理特殊和模糊情况——符合保留规则** 论文的核心是关于智能体如何进行**规划和多步推理**（将高层救援目标分解为可执行的集群命令），这完全符合“保留”规则。它不是在提升LLM的基础数学或逻辑能力，而是在构建一个能够完成复杂任务的自主规划框架。 **结论**: 该论文的核心贡献在于构建了一个新颖的LLM智能体框架，该框架具备意图理解、任务规划和多智能体协调能力。这直接对应了我研究目标中的“构建LLM智能体”和“多智能体”方向。因此，尽管它有明确的应用场景，但其方法论贡献使其成为一篇高度相关的前沿论文，应被保留。"
    },
    {
        "index": "#93",
        "title": "PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI",
        "link": "/arxiv/2511.03934",
        "arxiv_id": "2511.03934",
        "authors": "Athma Narayanan, Mahesh Subedar, Omesh Tickoo",
        "summary": "We present an agentic flow consisting of multiple agents that combine specialized LLMs and hardware simulation tools to collaboratively complete the complex task of Register Transfer Level (RTL) generation without human intervention. A key feature of the proposed flow is the progressive error feedback system of agents (PEFA), a self-correcting mechanism that leverages iterative error feedback to progressively increase the complexity of the approach. The generated RTL includes checks for compilation, functional correctness, and synthesizable constructs. To validate this adaptive approach to code generation, benchmarking is performed using two opensource natural language-to-RTL datasets. We demonstrate the benefits of the proposed approach implemented on an open source agentic framework, using both open- and closed-source LLMs, effectively bridging the performance gap between them. Compared to previously published methods, our approach sets a new benchmark, providing state-of-the-art pass rates while being efficient in token counts.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.958570",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于构建和改进LLM智能体，而非简单的应用。我的判断依据如下： 1.  **核心判断 (第一步):** - **保留**。这篇论文的本质是提出了一种新的智能体框架。摘要明确指出，其核心贡献是“一个由多个智能体组成的智能体流程”和“智能体的渐进式错误反馈系统（PEFA）”。这并非简单地将现有智能体框架应用于RTL领域，而是**构建了一个具有自我修正能力的多智能体协作新框架**。因此，它不属于“非演化型应用”的排除范畴。 2.  **正面指标 (第二步):** - 论文命中了多个核心关注点： - **多智能体:** 明确提到“multiple agents”和“collaboratively complete”。 - **自我演化:** 这是论文最突出的亮点。它提出了“self-correcting mechanism”（自我修正机制）、“iterative error feedback”（迭代错误反馈）和“progressive error feedback”（渐进式错误反馈），这些都直接对应“自我演化”和“自我完善”的研究方向。 - **工具使用:** 提到智能体“combine specialized LLMs and hardware simulation tools”，这是典型的工具使用能力。 3.  **排除标准 (第三步):** - 论文的主要贡献不涉及安全、对齐或多模态，因此没有触发任何排除标准。 4.  **特殊和模糊情况处理 (第四步):** - **自我演化的应用:** 这篇论文是“自我演化的应用”这一例外情况的完美范例。虽然论文的应用领域是特定的RTL生成，但其**核心贡献是提出了一种名为PEFA的、全新的“自我演化”机制**。根据你的规则，“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。因此，这篇论文应该被保留。 **最终决策 (第五步):** 综合分析，该论文的核心是构建一个多智能体协作系统，并为其设计了一种创新的、基于迭代反馈的自我演化/自我修正机制（PEFA）。这直接命中了你研究课题中的“多智能体”和“自我演化”两个核心方向。尽管它以RTL生成为验证场景，但其方法论贡献是通用且前沿的，完全符合你筛选“构建、改进或演化LLM智能体”论文的核心目标。因此，最终判断为保留。"
    },
    {
        "index": "#95",
        "title": "Collaborative Agents for Automated Program Repair in Ruby",
        "link": "/arxiv/2511.03925",
        "arxiv_id": "2511.03925",
        "authors": "Nikta Akbarpour, Mahdieh Sadat Benis, Fatemeh Hendijani Fard, Ali Ouni, Mohamed Aymen Saied",
        "summary": "Automated Program Repair (APR) has advanced rapidly with Large Language Models (LLMs), but most existing methods remain computationally expensive, and focused on a small set of languages. Ruby, despite its widespread use in web development and the persistent challenges faced by its developers, has received little attention in APR research. In this paper, we introduce RAMP, a novel lightweight framework that formulates program repair as a feedback-driven, iterative process for Ruby. RAMP employs a team of collaborative agents that generate targeted tests, reflect on errors, and refine candidate fixes until a correct solution is found. Unlike prior approaches, RAMP is designed to avoid reliance on large multilingual repair databases or costly fine-tuning, instead operating directly on Ruby through lightweight prompting and test-driven feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly within five iterations, and ablation studies confirm that test generation and self-reflection are key drivers of its performance. Further analysis shows that RAMP is particularly effective at repairing wrong answers, compilation errors, and runtime errors. Our approach provides new insights into multi-agent repair strategies, and establishes a foundation for extending LLM-based debugging tools to under-studied languages.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-11-06",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.961107",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于构建了一个新颖的多智能体协作与自我演化框架。我的判断依据如下： 1.  **核心判断 (第一步):** - **保留**: 论文的核心贡献是提出一个名为 RAMP 的**新颖的轻量级框架**，而不是简单地将现有LLM或智能体框架应用于程序修复。该框架的核心是**方法论创新**，即如何组织智能体进行协作和演化。因此，它不属于“非演化型应用”的排除范畴。 2.  **正面指标 (第二步):** - 论文高度符合你的核心关注点： - **多智能体**: 明确提出 employs a team of **collaborative agents**，这直接命中了 `Multi-Agent Systems (MAS)` 和 `Collaboration` 等关键词。 - **自我演化**: 将程序修复定义为一个 **feedback-driven, iterative process**，智能体能够 **reflect on errors, and refine candidate fixes**。这完美契合了 `Self-Evolving`、`Self-Reflection` 和 `Iterative Improvement` 的核心概念。论文的消融研究也证实了 `self-reflection` 是其性能的关键驱动力。 - **智能体能力**: 智能体团队分工明确，包括生成测试、反思错误和修复代码，这体现了智能体的**规划**和**工具使用**（将测试生成和代码执行作为工具）能力。 3.  **排除标准 (第三步):** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态。它专注于智能体的架构和协作机制，因此没有触发任何排除标准。 4.  **特殊和模糊情况 (第四步):** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一例外情况的绝佳范例。虽然它的应用领域是特定的（Ruby程序修复），但其**核心贡献是提出了一种新的“自我演化”机制**——即通过多智能体协作、测试驱动反馈和自我反思来迭代完善解决方案。这种机制具有通用性，可以被借鉴到其他需要迭代式问题求解的领域。 **总结:** 这篇论文的本质是提出了一种**多智能体协作与自我演化的新范式（RAMP框架）**，并将其成功应用于自动化程序修复任务。它的核心贡献在于**“如何构建和演化智能体”**，而不是**“智能体在某个领域的应用结果”**。因此，它精准地命中了你研究课题中的“多智能体”和“自我演化”两个核心方向，是一篇非常相关且有价值的前沿论文。"
    },
    {
        "index": "#99",
        "title": "Secure Code Generation at Scale with Reflexion",
        "link": "/arxiv/2511.03898",
        "arxiv_id": "2511.03898",
        "authors": "Arup Datta, Ahmed Aljohani, Hyunsook Do",
        "summary": "Large language models (LLMs) are now widely used to draft and refactor code, but code that works is not necessarily secure. We evaluate secure code generation using the Instruct Prime, which eliminated compliance-required prompts and cue contamination, and evaluate five instruction-tuned code LLMs using a zero-shot baseline and a three-round reflexion prompting approach. Security is measured using the Insecure Code Detector (ICD), and results are reported by measuring Repair, Regression, and NetGain metrics, considering the programming language and CWE family. Our findings show that insecurity remains common at the first round: roughly 25-33% of programs are insecure at a zero-shot baseline (t0 ). Weak cryptography/config-dependent bugs are the hardest to avoid while templated ones like XSS, code injection, and hard-coded secrets are handled more reliably. Python yields the highest secure rates; C and C# are the lowest, with Java, JS, PHP, and C++ in the middle. Reflexion prompting improves security for all models, improving average accuracy from 70.74% at t0 to 79.43% at t3 , with the largest gains in the first round followed by diminishing returns. The trends with Repair, Regression, and NetGain metrics show that applying one to two rounds produces most of the benefits. A replication package is available at https://doi.org/10.5281/zenodo.17065846.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Computational Engineering, Finance, and Science, Software Engineering",
        "date": "2025-11-05",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.962278",
        "filter_reason": "这篇论文符合我的研究范围，核心依据在于其贡献聚焦于一种**自我演化**的智能体机制，而非仅仅是安全领域的应用。 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是提出一种新的安全技术或检测方法，而是**评估和应用一种名为“Reflexion”的智能体框架**来解决安全代码生成问题。Reflexion是一种典型的智能体自我反思和迭代改进机制。论文的核心贡献在于验证了这种Agentic方法（多轮反思提示）能够有效提升LLM在特定任务上的表现，这属于“改进或演化LLM智能体”的范畴。它不是简单地将LLM作为工具，而是研究智能体如何通过一个结构化的过程（生成-反思-修正）来演化其输出。 2.  **第二步：正面指标** - 论文明确包含了我的核心关注点：`Self-Evolving`（通过多轮迭代实现演化）、`Self-Reflection`（Reflexion的核心）、`Self-Correction`（根据反思结果修正代码）和`Iterative Improvement`（从t0到t3的迭代过程）。这些都是Agentic AI，特别是自我演化方向的关键范式和能力。 3.  **第三步：排除标准** - **安全与对齐**：这是最需要辨析的一点。虽然论文的**应用领域**是“安全”，但其**主要贡献**并非一种新的安全算法、对齐方法或可解释性技术。论文的核心是**方法论**——即“Reflexion prompting”这种Agentic框架的有效性。安全代码生成只是一个用来验证该框架效果的实验场景。根据筛选规则，只要论文的主要贡献不是关于安全本身，而是关于智能体的构建和演化，就不应排除。这篇论文可以被理解为“一项关于自我演化智能体在安全任务上的实证研究”，其焦点在于智能体，而非安全。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**：这篇论文完美符合“自我演化的应用”这一例外情况。论文的核心是提出并分析一种“自我演化”机制（Reflexion），即使它被应用在“安全代码生成”这个特定领域，也应该被保留。这正符合筛选标准中“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域...也应该保留”的精神。 **最终决策**：综合以上分析，该论文的核心贡献在于对一种LLM智能体的自我反思和迭代演化机制（Reflexion）的评估和应用。它直接服务于我研究目标中的“自我演化”方向，探讨了智能体如何通过经验（前几轮的输出和反思）来完善自身。因此，尽管其应用场景是安全，但其研究本质和方法论完全符合我的筛选要求。"
    },
    {
        "index": "#117",
        "title": "Conversational Collective Intelligence (CCI) using Hyperchat AI in an Authentic Forecasting Task",
        "link": "/arxiv/2511.03732",
        "arxiv_id": "2511.03732",
        "authors": "Hans Schumann, Louis Rosenberg, Ganesh Mani, Gregg Willcox",
        "summary": "Hyperchat AI is a novel agentic technology that enables thoughtful conversations among networked human groups of potentially unlimited size. It allows large teams to discuss complex issues, brainstorm ideas, surface risks, assess alternatives and efficiently converge on optimized solutions that amplify the group's Collective Intelligence (CI). A formal study was conducted to quantify the forecasting accuracy of human groups using Hyperchat AI to conversationally predict the outcome of Major League Baseball (MLB) games. During an 8-week period, networked groups of approximately 24 sports fans were tasked with collaboratively forecasting the winners of 59 baseball games through real-time conversation facilitated by AI agents. The results showed that when debating the games using Hyperchat AI technology, the groups converged on High Confidence predictions that significantly outperformed Vegas betting markets. Specifically, groups were 78% accurate in their High Confidence picks, a statistically strong result vs the Vegas odds of 57% (p=0.020). Had the groups bet against the spread (ATS) on these games, they would have achieved a 46% ROI against Vegas betting markets. In addition, High Confidence forecasts that were generated through above-average conversation rates were 88% accurate, suggesting that real-time interactive deliberation is central to amplified accuracy.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.975894",
        "filter_reason": "这篇论文符合你的研究范围，应被保留。我的判断过程如下： **第一步：核心判断——保留** 这篇论文的核心贡献并非简单地将一个已有的智能体框架应用于体育预测领域。相反，它提出并验证了一种名为“Hyperchat AI”的**新颖的智能体技术**。论文的本质是**构建**一个能够促进大规模人类群体进行“深思熟虑的对话”的系统，以放大集体智能。这完全符合“构建LLM智能体”或“多智能体系统”的核心要求。它不是在解决一个生物或金融问题，而是在探索一种新的智能体交互范式。 **第二步：正面指标——高度匹配** 论文包含了多个与你研究焦点直接相关的核心指标： *   **核心范式**: 论文明确将Hyperchat AI定义为一种“agentic technology”，其核心是“Conversational Collective Intelligence (CCI)”，这直接指向了`Agentic AI`和`Multi-Agent Systems (MAS)`。 *   **多智能体**: 研究的核心是“networked human groups”通过“AI agents”进行协作。这完美契合了`Collaboration`（协作）、`Communication`（通信）和`Agent Society`（智能体社会）等子方向。论文探讨的是智能体（包括人类和AI）如何通过交互来提升整体性能。 **第三步：排除标准——未触发** 论文的主要贡献不涉及安全、对齐、可解释性或水印等问题。同时，它也不以多模态或视觉为核心研究内容。因此，没有触发任何排除标准。 **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文中的“推理”和“规划”体现在群体层面。通过AI促进的实时对话、辩论和协商，群体能够“评估备选方案”并“收敛到优化解决方案”。这是一种分布式的、社会性的智能体推理过程，属于保留范畴。 *   **自我演化的应用**: 虽然论文不直接涉及“自我演化”，但它属于一个更广泛的“Agentic AI”范畴。关键在于，它的核心是提出一种新的智能体框架，而不是应用一个旧框架。 **第五步：最终决策** 综合来看，这篇论文的核心贡献是**构建并验证了一个新颖的多智能体协作框架**。它探索了AI智能体如何作为媒介，组织和增强人类群体的集体智能。这完全符合你研究课题中的“多智能体”方向，特别是关于智能体间的协作与通信。尽管其验证任务（预测棒球比赛）是一个具体的应用，但论文的焦点和价值在于其提出的“Hyperchat AI”这一智能体技术本身，而非预测结果。因此，这篇论文是高度相关的前沿研究，应当保留。"
    },
    {
        "index": "#121",
        "title": "Efficient On-Device Agents via Adaptive Context Management",
        "link": "/arxiv/2511.03728",
        "arxiv_id": "2511.03728",
        "authors": "Sanidhya Vijayvargiya, Rahul Lokesh",
        "summary": "On-device AI agents offer the potential for personalized, low-latency assistance, but their deployment is fundamentally constrained by limited memory capacity, which restricts usable context. This reduced practical context window creates a trade-off between supporting rich, stateful interactions with complex tool capabilities and maintaining on-device feasibility. We break this trade-off with a framework for context-efficient on-device agents, driven by three synergistic optimizations (1) a dynamic memory system using specialized LoRA adapters to distill conversational history into a compressed, and structured Context State Object; (2) a minimalist serialization format for tool schemas to minimize token overhead per tool; and (3) a just-in-time schema-passing mechanism that loads full tool definitions only upon tool selection. We instantiate this framework by adapting a 3B parameter SLM to context-efficient trajectories and rigorously evaluate it against a conventional baseline on complex user tasks. Our agent matches, or exceeds, the performance of a conventional baseline while dramatically compressing context, achieving more than a 6-fold reduction in initial system prompt context and a 10- to 25-fold reduction in context growth rate based on the interaction verbosity, demonstrating that strategic context management is key to unlocking capable and persistent on-device AI.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-11-07T11:00:04.977684",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于**构建和改进LLM智能体**。我的判断过程如下： 1.  **第一步：核心判断——保留** - 论文的本质是提出一个**新的框架**，用于解决设备端LLM智能体面临的核心挑战（上下文限制）。这并非将现有智能体作为工具应用到某个领域，而是直接对智能体本身的架构进行优化和创新。 - 论文的核心贡献是三个协同优化技术：动态记忆系统、极简工具序列化和即时模式传递。这些都是关于**如何构建一个更高效、更强大的智能体**的方法论，完全符合“构建、改进或演化LLM智能体”的核心目标。 - 虽然论文涉及“On-Device”（设备端）和“memory capacity”（内存容量），这些看似基础设施的术语，但它们是作为**研究背景和约束条件**出现的。论文的解决方案并非通用的部署优化或硬件加速，而是**专门针对智能体的“记忆”和“工具使用”能力**设计的架构改进。因此，它不属于基础设施研究，而是Agentic AI研究。 2.  **第二步：正面指标——高度相关** - 论文明确命中了多个核心关注点： - **智能体能力**: 论文的核心贡献直接对应了 `Memory`（动态记忆系统）和 `Tool Use / Tool Augmentation`（极简序列化、即时传递）。这两个是单智能体研究的核心子方向。 - **核心范式**: 论文的研究对象是 `LLM-based Agents`，其目标是提升智能体的能力，属于 `Agentic AI` 范畴。 3.  **第三步：排除标准——未触发** - 论文的主要贡献不是关于安全、对齐、可解释性或幻觉。 - 论文也未涉及多模态或视觉，其焦点完全在文本上下文的管理上。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文虽然未提出新的推理算法（如ToT），但其框架通过提供更高效的上下文管理，**直接赋能了智能体进行更复杂的多步推理和规划**。它解决了智能体在执行复杂任务时的实际瓶颈，因此属于对智能体规划能力的支撑性改进，应予以保留。 - **基础设施 vs. Agentic**: 这是最关键的判断点。本文的巧妙之处在于，它利用一个基础设施问题（设备端内存限制）作为切入点，但其解决方案是纯粹的**智能体架构创新**。它回答的不是“如何让模型在设备上跑得快”，而是“**如何在资源受限的设备上，构建一个拥有持久记忆和丰富工具使用能力的智能体**”。这个问题的答案，对于所有LLM智能体的设计都具有普适性的启发意义，而不仅限于设备端场景。 **结论**: 该论文的核心贡献是提出了一种创新的LLM智能体框架，通过优化上下文管理，显著增强了智能体的**记忆**和**工具使用**能力。这完全符合您研究课题中“单智能体”方向下的“构建、改进”目标。因此，这篇论文应被**保留**。"
    }
]