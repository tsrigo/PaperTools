[
    {
        "index": "#1",
        "title": "Reward Models are Metrics in a Trench Coat",
        "link": "/arxiv/2510.03231",
        "arxiv_id": "2510.03231",
        "authors": "Sebastian Gehrmann",
        "summary": "The emergence of reinforcement learning in post-training of large language models has sparked significant interest in reward models. Reward models assess the quality of sampled model outputs to generate training signals. This task is also performed by evaluation metrics that monitor the performance of an AI model. We find that the two research areas are mostly separate, leading to redundant terminology and repeated pitfalls. Common challenges include susceptibility to spurious correlations, impact on downstream reward hacking, methods to improve data quality, and approaches to meta-evaluation. Our position paper argues that a closer collaboration between the fields can help overcome these issues. To that end, we show how metrics outperform reward models on specific tasks and provide an extensive survey of the two areas. Grounded in this survey, we point to multiple research topics in which closer alignment can improve reward models and metrics in areas such as preference elicitation methods, avoidance of spurious correlations and reward hacking, and calibration-aware meta-evaluation.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.817700",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是根据你的筛选标准进行的详细判断过程： 1.  **第一步：核心判断——这篇论文的本质是什么？** 论文的核心是关于改进大语言模型后训练阶段的**奖励模型**。奖励模型是强化学习（尤其是RLHF）的关键组成部分，其质量直接决定了LLM在训练过程中能学到的指令遵循、对齐以及更高级别的能力。通过研究如何改进奖励模型（如避免虚假相关性、防止奖励破解），这篇论文直接触及了**提升LLM基础训练范式**的核心问题。这并非将LLM作为工具应用于特定领域，而是致力于优化LLM本身的学习机制和能力上限。因此，根据第一步，应**保留**。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文明确包含了多个关键正面指标： - **核心概念**: 摘要中直接提及 \"large language models\"。 - **训练方法**: 论文的主题就是 \"reinforcement learning in post-training\" 和 \"reward models\"，这是提升LLM能力的核心方法论之一。 虽然没有直接提及 \"reasoning\"，但改进奖励模型的目标是让LLM产生更高质量的输出，这内在地包含了提升其逻辑、规划等通用推理能力的潜力。一个更准确、更不易被“破解”的奖励模型，能够更好地引导模型学会正确、连贯的推理过程。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全不涉及任何排除标准领域： - 它没有讨论多模态或视觉。 - 它没有聚焦于任何特定应用领域（如医疗、化学等）。 - 它讨论的 \"reward hacking\" 是一个训练过程中的根本性问题，而非应用层面的水印或安全策略。这是为了提升模型内在能力的可靠性，而不是在模型部署后添加的外部防护。 4.  **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: 论文探讨的“奖励破解”问题与模型的“幻觉”或行为不一致性密切相关。通过提出改进奖励模型的方法来从根本上缓解这些问题，这属于**提升模型内在可靠性和推理质量**的范畴，因此应该**保留**。这篇论文是对训练机制的深入探讨，而非社会学或应用层面的讨论。 5.  **第五步：最终决策** 综合以上分析，这篇论文虽然是一篇立场性/综述性论文，但其核心贡献在于指出了提升LLM训练质量（特别是通过改进奖励模型）的关键研究方向。它直接关系到如何通过优化强化学习这一核心训练范式来增强LLM的通用能力。这与你的核心目标“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”高度一致。因此，最终判断为符合要求。"
    },
    {
        "index": "#3",
        "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language Models",
        "link": "/arxiv/2510.03215",
        "arxiv_id": "2510.03215",
        "authors": "Tianyu Fu, Zihan Min, Hanling Zhang, Jichao Yan, Guohao Dai, Wanli Ouyang, Yu Wang",
        "summary": "Multi-LLM systems harness the complementary strengths of diverse Large Language Models, achieving performance and efficiency gains unattainable by a single model. In existing designs, LLMs communicate through text, forcing internal representations to be transformed into output token sequences. This process both loses rich semantic information and incurs token-by-token generation latency. Motivated by these limitations, we ask: Can LLMs communicate beyond text? Oracle experiments show that enriching the KV-Cache semantics can improve response quality without increasing cache size, supporting KV-Cache as an effective medium for inter-model communication. Thus, we propose Cache-to-Cache (C2C), a new paradigm for direct semantic communication between LLMs. C2C uses a neural network to project and fuse the source model's KV-cache with that of the target model to enable direct semantic transfer. A learnable gating mechanism selects the target layers that benefit from cache communication. Compared with text communication, C2C utilizes the deep, specialized semantics from both models, while avoiding explicit intermediate text generation. Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models. It further outperforms the text communication paradigm by approximately 3.0-5.0%, while delivering an average 2.0x speedup in latency. Our code is available at https://github.com/thu-nics/C2C.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.818671",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是根据筛选标准进行的详细判断： 1.  **第一步：核心判断——这篇论文的本质是什么？** 论文的核心贡献是提出了一种名为“Cache-to-Cache (C2C)”的新范式，用于实现多个大语言模型（LLM）之间的直接语义通信。这并非将LLM作为工具应用于特定领域，也不是关于模型基础设施或部署优化的研究。相反，它提出了一种全新的**智能体协作框架**，旨在通过改进模型间的通信方式来提升整个多模型系统的性能。传统的文本通信会损失丰富的内部语义信息，而C2C通过直接传递和融合模型的内部表示（KV-Cache），保留了更深层次的语义，从而提升了系统的最终输出质量。这直接关联到提升LLM系统的**通用问题解决能力**，因此应予以**保留**。 2.  **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文明确聚焦于 \"Large language models\" (LLMs)。 - **能力方向**: 虽然没有直接使用 \"reasoning\" 一词，但其核心目标——通过更丰富的语义通信来“提高响应质量”和“平均准确率”——本质上是在增强模型系统的综合推理和问题解决能力。更优的语义传递意味着更少的信息损失，从而支持更复杂、更准确的推理过程。 - **新兴范式**: 论文的核心是关于 \"Multi-LLM systems\"，这完全属于 \"llm-based agents\" 和 \"multi-agent systems\" 的研究范畴。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）。因此，不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的典型范例。它研究的不是智能体在某个领域的应用，而是智能体之间如何更高效、更智能地协作这一基础性问题。因此，完全符合保留条件。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种创新的、通用的多智能体协作方法。它通过优化LLM之间的通信机制，从底层增强了模型系统处理复杂任务的能力，这直接服务于“提高大语言模型本身的通用推理能力”这一核心目标。论文的贡献是方法论层面的，具有普适性，而非特定应用。因此，这篇论文高度相关，应被筛选入内。"
    },
    {
        "index": "#2",
        "title": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment",
        "link": "/arxiv/2510.03223",
        "arxiv_id": "2510.03223",
        "authors": "Hongxiang Zhang, Yuan Tian, Tianyi Zhang",
        "summary": "To solve complex reasoning tasks for Large Language Models (LLMs), prompting-based methods offer a lightweight alternative to fine-tuning and reinforcement learning. However, as reasoning chains extend, critical intermediate steps and the original prompt will be buried in the context, receiving insufficient attention and leading to errors. In this paper, we propose Self-Anchor, a novel pipeline that leverages the inherent structure of reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories into structured plans and automatically aligns the model's attention to the most relevant inference steps, allowing the model to maintain focus throughout generation. Our experiment shows that Self-Anchor outperforms SOTA prompting methods across six benchmarks. Notably, Self-Anchor significantly reduces the performance gap between ``non-reasoning'' models and specialized reasoning models, with the potential to enable most LLMs to tackle complex reasoning tasks without retraining.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.818180",
        "filter_reason": "这篇论文完全符合您的筛选标准，应被保留。以下是我的详细判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心是提出一种名为“Self-Anchor”的新方法，旨在解决大语言模型在执行长链推理时注意力分散的问题。其本质是改进LLM的**基础推理机制**，通过引导模型在生成过程中持续关注关键信息（原始提示和中间步骤），来提升其解决复杂推理任务的能力。这完全符合“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的保留标准。它并非将LLM应用于特定领域，而是聚焦于提升模型本身的能力。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文命中了多个关键的正面指标： *   **核心概念**: 标题和摘要中明确提到了“Large Language Models (LLMs)”。 *   **能力方向**: 论文的主题就是“reasoning”，特别是解决“complex reasoning tasks”。 *   **新兴范式**: 论文提出了一种新的“prompting-based methods”，这属于提升LLM能力的新兴范式。其“将推理轨迹分解为结构化计划”的思想也与规划和问题 solving 紧密相关。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文内容完全不涉及任何排除标准领域。它没有讨论视觉、多模态，也没有将方法应用于医疗、化学或机器人等特定领域。同样，它也不涉及水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况** 该论文不直接涉及智能体或幻觉等特殊议题，但其方法论与这些议题的目标一致。它通过优化注意力对齐来减少推理错误，这与“减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的保留原则精神相通。它是一种根本性的、机制层面的改进，而非应用层面的讨论。 5.  **第五步：最终决策** 综合以上分析，这篇论文的直接目标是提升LLM的通用推理能力，提出了一种创新的、轻量级的方法论，并且该方法具有通用性，旨在赋能大多数LLM。这与您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标高度一致。因此，应判定为符合要求。"
    },
    {
        "index": "#18",
        "title": "Self-Reflective Generation at Test Time",
        "link": "/arxiv/2510.02919",
        "arxiv_id": "2510.02919",
        "authors": "Jian Mu, Qixin Zhang, Zhiyong Wang, Menglin Yang, Shuang Qiu, Chengwei Qin, Zhongxiang Dai, Yao Shu",
        "summary": "Large language models (LLMs) increasingly solve complex reasoning tasks via long chain-of-thought, but their forward-only autoregressive generation process is fragile; early token errors can cascade, which creates a clear need for self-reflection mechanisms. However, existing self-reflection either performs revisions over full drafts or learns self-correction via expensive training, both fundamentally reactive and inefficient. To address this, we propose Self-Reflective Generation at Test Time (SRGen), a lightweight test-time framework that reflects before generating at uncertain points. During token generation, SRGen utilizes dynamic entropy thresholding to identify high-uncertainty tokens. For each identified token, it trains a specific corrective vector, which fully exploits the already generated context for a self-reflective generation to correct the token probability distribution. By retrospectively analyzing the partial output, this self-reflection enables more trustworthy decisions, thereby significantly reducing the probability of errors at highly uncertain points. Evaluated on challenging mathematical reasoning benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model reasoning: improvements in single-pass quality also translate into stronger self-consistency voting. Especially, on AIME2024 with DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a plug-and-play method that integrates reflection into the generation process for reliable LLM reasoning, achieving consistent gains with bounded overhead and broad composability with other training-time (e.g., RLHF) and test-time (e.g., SLOT) techniques.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.859104",
        "filter_reason": "这篇论文完全符合我的研究范围，其核心贡献是提出了一种在测试时进行自我反思的生成框架（SRGen），旨在直接提升大语言模型（LLM）在推理过程中的鲁棒性和准确性。 我的判断过程如下： 1.  **第一步（核心判断）：** 这篇论文的本质是改进LLM的基础推理能力，而非将其作为工具应用于特定领域。论文指出了LLM在长链推理中的一个根本性缺陷——“前向自回归生成过程的脆弱性，早期token的错误会级联传播”。为了解决这个问题，论文提出的SRGen框架通过在生成过程中进行前瞻性的自我反思来修正模型行为。这完全符合“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步（正面指标）：** 论文与多个正面指标高度相关。 *   **核心概念:** 论文明确以Large language models (LLMs)为研究对象。 *   **能力方向:** 论文的核心是提升reasoning能力，特别是在challenging mathematical reasoning benchmarks上进行了评估，这直接对应了\"math reasoning\"。 *   **新兴范式:** 论文的核心思想\"Self-Reflective Generation\"是一种新颖的范式，可以看作是self-evolve（自我进化）在测试时的一种体现。它通过动态修正来提升模型表现，属于增强模型自身能力的范畴。 3.  **第三步（排除标准）：** 论文完全不涉及任何排除标准。它没有讨论多模态、视觉，也没有将方法应用于医疗、化学等特定领域，更不关注模型基础设施或水印等应用层面的可靠性问题。 4.  **第四步（特殊和模糊情况）：** 论文的研究内容与“提升模型内在可靠性”这一特殊情况高度契合。它提出的SRGen方法，通过减少推理过程中的错误级联，直接提升了模型输出的“可信度”和推理质量。这是一种技术性的、从模型内部机制出发的改进方法，而非社会学或应用层面的讨论，因此应该保留。 **最终决策：** 综合以上分析，这篇论文提出了一种通用的、轻量级、即插即用的测试时框架（SRGen），其核心目标是解决LLM在通用推理（尤其是长链数学推理）中的一个核心痛点。它通过一种创新的“生成前反思”机制，在不改变模型权重的情况下，显著提升了模型的推理性能和可靠性。这项研究直接贡献于“提高大语言模型本身的通用推理能力”，与我的研究目标高度一致，因此应该被筛选保留。"
    },
    {
        "index": "#21",
        "title": "StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering",
        "link": "/arxiv/2510.02827",
        "arxiv_id": "2510.02827",
        "authors": "Tengjun Ni, Xin Yuan, Shenghong Li, Kai Wu, Ren Ping Liu, Wei Ni, Wenjie Zhang",
        "summary": "Recent progress in retrieval-augmented generation (RAG) has led to more accurate and interpretable multi-hop question answering (QA). Yet, challenges persist in integrating iterative reasoning steps with external knowledge retrieval. To address this, we introduce StepChain GraphRAG, a framework that unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow for enhanced multi-hop QA. Our approach first builds a global index over the corpus; at inference time, only retrieved passages are parsed on-the-fly into a knowledge graph, and the complex query is split into sub-questions. For each sub-question, a BFS-based traversal dynamically expands along relevant edges, assembling explicit evidence chains without overwhelming the language model with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1 scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1). StepChain GraphRAG also fosters enhanced explainability by preserving the chain-of-thought across intermediate retrieval steps. We conclude by discussing how future work can mitigate the computational overhead and address potential hallucinations from large language models to refine efficiency and reliability in multi-hop QA.",
        "subjects": "Computation and Language, Information Retrieval",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.860803",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断依据如下： 1.  **核心判断（第一步）：论文的本质是提升LLM的通用推理能力。** 论文的核心贡献是提出了一种名为“StepChain GraphRAG”的新框架。这个框架并非将LLM应用于某个特定领域（如医疗或金融），而是聚焦于改进LLM在执行“多跳问答”这一通用任务时的推理过程。它通过“问题分解”、“知识图谱构建”和“广度优先搜索（BFS）推理流”等方法，将一个复杂问题拆解，并动态地、有逻辑地组织外部知识，形成“显式的证据链”。这本质上是在为LLM设计一种更强大的、结构化的推理流程，直接提升了其多步推理和问题解决能力，属于对LLM基础推理能力的增强和方法论创新。 2.  **正面指标（第二步）：论文高度相关。** -   **能力方向**: 论文标题和摘要都明确聚焦于“reasoning”，特别是“multi-hop question answering”，这是衡量LLM多步推理能力的核心任务之一。 -   **新兴范式**: 该框架可以被视为一种高级的“tool use”和“deep research”范式。它将知识图谱和BFS算法作为工具，辅助LLM进行深度推理和信息整合，而不是简单地将一堆文本扔给模型。 3.  **排除标准（第三步）：论文不涉及任何排除领域。** 论文内容完全不涉及多模态、视觉，也没有针对任何特定应用领域（如生物、化学等）。它使用的基准数据集（如HotpotQA）是评估通用多跳推理能力的标准数据集，而非领域特定数据集。 4.  **特殊和模糊情况（第四步）：论文的情况符合保留条件。** -   **智能体/工具使用**: 论文提出的是一个**通用的**框架，利用知识图谱这一工具来增强LLM的推理能力。这不是“用于化学实验的智能体”，而是“用于增强通用多跳推理的框架”，因此完全符合保留条件。 -   **可解释性**: 论文明确提到其方法“通过在中间检索步骤中保留思维链来增强可解释性”。这是一种通过改进方法论来提升模型内在推理透明度的做法，而非应用层面的社会学讨论，因此符合保留条件。 **最终决策（第五步）：** 综合来看，这篇论文的核心是提出一种创新的推理框架，通过结构化的知识检索和图遍历算法，显著增强了大语言模型在多跳问答任务上的通用推理能力。它直接回应了“如何提高LLM通用推理能力”这一核心目标，因此应被保留。"
    },
    {
        "index": "#24",
        "title": "The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback",
        "link": "/arxiv/2510.02752",
        "arxiv_id": "2510.02752",
        "authors": "Hangfan Zhang, Siyuan Xu, Zhimeng Guo, Huaisheng Zhu, Shicheng Liu, Xinrun Wang, Qiaosheng Zhang, Yang Chen, Peng Ye, Lei Bai, Shuyue Hu",
        "summary": "Reinforcement learning (RL) has demonstrated potential in enhancing the reasoning capabilities of large language models (LLMs), but such training typically demands substantial efforts in creating and annotating data. In this work, we explore improving LLMs through RL with minimal data. Our approach alternates between the LLM proposing a task and then attempting to solve it. To minimize data dependency, we introduce two novel mechanisms grounded in self-awareness: (1) self-aware difficulty prediction, where the model learns to assess task difficulty relative to its own abilities and prioritize challenging yet solvable tasks, and (2) self-aware limit breaking, where the model recognizes when a task is beyond its capability boundary and proactively requests external data to break through that limit. Extensive experiments on nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra data demonstrate the efficacy of self-aware RL and underscore the promise of self-evolving agent training.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.862957",
        "filter_reason": "这篇论文完全符合筛选标准，应予以保留。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心是提出一种名为“自我进化”的**新训练范式**，旨在通过强化学习（RL）和**内在反馈机制**来提升大语言模型（LLM）的**推理能力**。其本质是改进模型本身的基础能力，而非将模型作为工具应用于特定领域。论文中提到的“自我意识难度预测”和“自我意识极限突破”都是为了优化模型的学习过程，使其能更高效地掌握通用问题解决技能。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标** 论文摘要中明确包含了多个关键的正面指标： *   **核心概念**: \"Large language models, LLMs\" *   **能力方向**: \"reasoning capabilities\" (推理能力) *   **训练方法**: \"Reinforcement learning (RL)\" *   **新兴范式**: \"self-evolving\" (自我进化), \"self-evolving agent training\" (自进化智能体训练) 这些关键词高度密集，表明论文与研究方向高度相关。 3.  **第三步：排除标准** 论文的研究焦点完全没有触及任何一个排除标准。它不涉及多模态、视觉；其方法在“九个基准测试”上进行验证，说明其目标是通用能力提升，而非医疗、化学等特定领域；也未讨论水印、安全等模型可靠性的应用层面问题。 4.  **第四步：处理特殊和模糊情况** 论文提到了“agent”，但它是作为“自进化智能体训练”这一通用框架提出的，目的是为了增强LLM自身的通用问题解决和学习能力，而非将其应用于某个特定领域（如机器人控制或化学实验）。这完全符合保留条件。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种创新的、数据高效的自我进化训练方法，通过让模型具备自我意识来主动选择学习任务，从而显著提升其通用推理能力。这直接命中了“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。因此，最终决策为保留。"
    },
    {
        "index": "#31",
        "title": "SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models",
        "link": "/arxiv/2510.02648",
        "arxiv_id": "2510.02648",
        "authors": "Rui Qi, Zhibo Man, Yufeng Chen, Fengran Mo, Jinan Xu, Kaiyu Huang",
        "summary": "Recent developments have enabled Large Language Models (LLMs) to engage in complex reasoning tasks through deep thinking. However, the capacity of reasoning has not been successfully transferred to non-high-resource languages due to resource constraints, which struggles with multilingual reasoning tasks. To this end, we propose Structured-of-Thought (SoT), a training-free method that improves the performance on multilingual reasoning through a multi-step transformation: Language Thinking Transformation and Structured Knowledge Transformation. The SoT method converts language-specific semantic information into language-agnostic structured representations, enabling the models to understand the query in different languages more sophisticated. Besides, SoT effectively guides LLMs toward more concentrated reasoning to maintain consistent underlying reasoning pathways when handling cross-lingual variations in expression. Experimental results demonstrate that SoT outperforms several strong baselines on multiple multilingual reasoning benchmarks when adapting to various backbones of LLMs. It can also be integrated with other training-free strategies for further improvements. Our code is available at https://github.com/Cherry-qwq/SoT.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.871695",
        "filter_reason": "这篇论文完全符合我的研究范围。我的判断过程如下： **第一步：核心判断** - 论文的核心贡献是提出了一种名为“思维结构”的全新提示范式。这是一种旨在提升大语言模型『通用推理能力』的方法论研究。它与思维链类似，都属于改进模型基础推理能力的训练范式（尽管这里是免训练的），旨在解锁和增强模型的内在逻辑与多步推理能力。 - 论文的研究问题虽然聚焦于“多语言推理”，但这并非一个特定应用领域（如医疗、法律），而是对模型推理能力在不同语言环境下『泛化性』和『鲁棒性』的考验，属于通用推理能力的核心范畴。其目标不是解决某个领域的具体问题，而是让模型本身变得更“聪明”，能够处理更抽象、更多样化的输入。 - 因此，根据第一步的核心判断标准，这篇论文应该被**保留**。 **第二步：正面指标** - 论文明确包含了多个正面指标： - **核心概念**: \"Large Language Models (LLMs)\" - **能力方向**: \"reasoning\"（标题和摘要中多次提及），特别是\"multilingual reasoning\"。 - **新兴范式**: 提出了一种新的提示方法\"SoT\"，这与思维链属于同一类别，是提升模型能力的新范式。 - 这些正面指标进一步确认了论文与我的研究目标高度相关。 **第三步：排除标准** - 该论文不涉及任何排除标准中的领域： - 它不涉及多模态与视觉。 - 它的研究对象是通用推理，而非医疗、化学、机器人等特定应用领域。 - 它不讨论水印、安全等模型可靠性问题。 - 因此，论文没有触发任何排除条件。 **第四步：处理特殊和模糊情况** - 论文内容不涉及需要特殊处理的智能体应用或模型可靠性问题。其“多语言”的设定，如第一步所分析，是对通用能力的一种压力测试，而非特定领域的应用。 **第五步：最终决策** 综合以上分析，SoT作为一种新颖的、免训练的提示方法，其根本目标是增强LLM的内在推理过程，使其能够跨越语言障碍进行一致的逻辑思考。这完全契合我寻找致力于提升LLM本身通用推理能力的前沿方法论研究的目标。因此，最终判断为**True**。"
    },
    {
        "index": "#43",
        "title": "Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems",
        "link": "/arxiv/2510.02377",
        "arxiv_id": "2510.02377",
        "authors": "Aakriti Agrawal, Rohith Aralikatti, Anirudh Satheesh, Souradip Chakraborty, Amrit Singh Bedi, Furong Huang",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities, yet selecting the most reliable response from multiple LLMs remains a challenge, particularly in resource-constrained settings. Existing approaches often depend on costly external verifiers, human evaluators, or self-consistency techniques that require multiple samples from a single model. While multi-LLM systems produce more diverse responses than single models and thus have greater potential, they often underperform compared to single LLM self-consistency. We propose a principled, novel and computationally efficient method to select the best response from multiple different LLMs using a calibrated log-likelihood score, implicitly leveraging the inherent knowledge and confidence of these models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across both debate (multi-round LLM discussions) and non-debate (Best-of-N with multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets respectively.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.882539",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Uncertainty-Aware Answer Selection”的新方法。这个方法本身不是训练一个新的LLM，也不是将LLM应用于特定领域。它的本质是一种**新的推理范式或方法论**，旨在通过智能地整合多个现有LLM的输出来提升整个系统的推理性能。这直接对应了筛选标准中“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求，尤其是在“方法论”层面。它关注的是如何让LLM（或LLM集合）更好地进行推理，这与思维链（CoT）等方法的思路是一致的。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 明确以 \"Large Language Models (LLMs)\" 为研究对象。 *   **能力方向**: 标题和摘要中反复强调 \"Improved Reasoning\"，并在GSM8K（数学推理）、MMLU和ARC（综合推理）等多个标准推理数据集上验证了效果。 *   **新兴范式**: 论文聚焦于 \"Multi-LLM Systems\"，并探讨了 \"debate (multi-round LLM discussions)\"，这属于 \"llm-based agents\" 和 \"multi-agent systems\" 的范畴。其提出的方法正是为了优化这类系统的推理表现。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： *   它不涉及任何视觉或多模态内容。 *   它使用的是通用的学术基准数据集，而非医疗、化学等特定领域。 *   它关注的是提升推理答案的质量，而不是水印、安全等模型可靠性（应用层面）的问题。 4.  **第四步：处理特殊和模糊情况** 论文的情况与“智能体/工具使用”的特殊情况高度相关。它提出的方法是一种**通用的**、用于管理多智能体（Multi-LLM）系统以增强其**通用问题解决能力**（在此处是通用推理能力）的框架。它没有将这种框架限定在某个特定领域，因此完全符合保留条件。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种新颖的、计算高效的方法论，通过在多LLM系统中进行不确定性感知的答案选择，从而显著提升了系统的通用推理能力。这直接对准了您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，这篇论文应该被保留。"
    },
    {
        "index": "#45",
        "title": "Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models",
        "link": "/arxiv/2510.02370",
        "arxiv_id": "2510.02370",
        "authors": "Minsung Kim, Dong-Kyum Kim, Jea Kwon, Nakyeong Yang, Kyomin Jung, Meeyoung Cha",
        "summary": "Large language models often encounter conflicts between in-context knowledge retrieved at inference time and parametric knowledge acquired during pretraining. Models that accept external knowledge uncritically are vulnerable to misinformation, whereas models that adhere rigidly to parametric knowledge fail to benefit from retrieval. Despite the widespread adoption of retrieval-augmented generation, we still lack a systematic understanding of what shapes knowledge-arbitration strategies during training. This gap risks producing pretrained models with undesirable arbitration behaviors and, consequently, wasting substantial computational resources after the pretraining budget has already been spent. To address this problem, we present the first controlled study of how training conditions influence models' use of in-context and parametric knowledge, and how they arbitrate between them. We train transformer-based language models on a synthetic biographies corpus while systematically controlling various conditions. Our experiments reveal that intra-document repetition of facts fosters the development of both parametric and in-context capabilities. Moreover, training on a corpus that contains inconsistent information or distributional skew encourages models to develop robust strategies for leveraging parametric and in-context knowledge. Rather than viewing these non-ideal properties as artifacts to remove, our results indicate that they are important for learning robust arbitration. These insights offer concrete, empirical guidance for pretraining models that harmoniously integrate parametric and in-context knowledge.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.883483",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。 **判断过程和核心依据如下:** 1.  **第一步：核心判断——论文的本质是提升LLM的基础能力。** 论文的核心并非将LLM应用于某个特定领域，而是对LLM如何学习和利用知识这一基础能力进行深入的、系统性的研究。它探讨的是模型在面对“内部知识”（参数化记忆）和“外部知识”（上下文信息）冲突时，如何进行“仲裁”。这种知识仲裁能力是通用推理能力的基石。一个优秀的推理模型不仅要能推理，更要能判断和选择信息源。因此，这篇论文的本质是探究并旨在指导如何训练出具有更优信息处理和决策能力的LLM，完全符合“改进LLM的基础能力”这一保留标准。 2.  **第二步：正面指标——论文高度相关。** - **核心概念**: 论文明确以“Large language models”和“language models”为研究对象。 - **能力方向**: 虽然摘要中没有直接使用\"reasoning\"一词，但“knowledge-arbitration strategies”（知识仲裁策略）本身就是一种高级的认知和推理过程。它涉及评估、比较和选择信息，这是逻辑推理和问题解决的关键环节。 - **新兴范式**: 论文直接研究了“in-context knowledge”（上下文知识）的利用，这是检索增强生成（RAG）、工具使用等前沿推理范式的核心。理解模型如何仲裁上下文知识和参数化知识，对于构建更强大的推理智能体至关重要。 3.  **第三步：排除标准——论文完全避开了排除领域。** 论文的研究对象是纯文本的语言模型，不涉及多模态。它使用合成的传记语料库作为受控实验环境，其目标是得出关于预训练的普适性结论，而非解决生物、医疗等特定领域的问题。同时，它研究的是模型内部的学习动态，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况——论文属于应保留的类别。** - **智能体/工具使用**: 这篇论文可以被视为对“工具使用”能力的前置基础研究。一个能够有效使用工具（如搜索引擎）的LLM，必须首先学会如何信任和整合工具返回的“in-context knowledge”，并与自身已有的“parametric knowledge”进行有效仲裁。该论文的研究成果直接指导了如何训练出更擅长此道的模型，因此应保留。 - **幻觉/可解释性**: 论文通过研究如何让模型更好地利用外部知识，间接地触及了如何减少因固执己见而产生的“幻觉”。它提出的“robust arbitration”（鲁棒的仲裁）策略，本质上是在提升模型内在的可靠性和推理质量，而非从外部进行约束，因此符合保留条件。 **最终决策:** 综合以上分析，这篇论文是一项关于LLM核心认知机制的基础性研究。它揭示了模型如何学习处理和整合不同来源的知识，这一能力是构成通用推理能力的重要一环。论文的发现为未来如何预训练出更强大、更可靠的推理模型提供了宝贵的实证指导，与“提高大语言模型本身的通用推理能力”这一核心目标高度契合。因此，最终判断为 **True**。"
    },
    {
        "index": "#44",
        "title": "Pretraining with hierarchical memories: separating long-tail and common knowledge",
        "link": "/arxiv/2510.02375",
        "arxiv_id": "2510.02375",
        "authors": "Hadi Pouransari, David Grangier, C Thomas, Michael Kirchhof, Oncel Tuzel",
        "summary": "The impressive performance gains of modern language models currently rely on scaling parameters: larger models store more world knowledge and reason better. Yet compressing all world knowledge into parameters is unnecessary, as only a fraction is used per prompt, and impractical for edge devices with limited inference-time memory and compute. We address this shortcoming by a memory-augmented architecture and a pretraining strategy aligned with existing hardware paradigms. We introduce small language models that access large hierarchical parametric memory banks encoding world knowledge. During pretraining and inference, we fetch a small, context-dependent memory block and add it to the model. Our pretraining learns to store long-tail world knowledge in the memory parameters, while the small language model acts as an anchor capturing common knowledge and general reasoning abilities. Through trillion-token-scale experiments, we show significant gains: a 160M-parameters model augmented with an 18M-parameters memory fetched from a 4.6B memory bank obtains comparable performance to a regular model with more than 2x the parameters. Through extensive experiments, we study the optimal type and size of parametric memories in transformers, scaling them to over 21B parameters. We find that our proposed hierarchical feed-forward memories work robustly across transformer architectures, whether added during pretraining or post-hoc.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.883004",
        "filter_reason": "这篇论文完全符合你的筛选标准，应该被保留。以下是我的详细判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出一种全新的模型架构和预训练策略，旨在改进大语言模型的基础能力。其核心贡献在于将“通用推理能力”与“长尾知识存储”这两个功能进行解耦。论文明确指出，小型语言模型（SLM）作为“锚点”，用于捕获“常识和通用推理能力”，而大规模的参数化记忆库则负责存储“长尾世界知识”。这是一种直接针对LLM内部能力构成进行优化的方法论研究，而非将LLM作为工具应用于特定领域。因此，它完全符合“改进LLM的基础能力、提出新的训练范式、增强其通用能力”的保留标准。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文高度命中了最关键的正面指标： *   **核心概念**: 论文的研究对象是“language models”，完全符合。 *   **能力方向**: 摘要中明确提到小型模型的核心任务是捕获“general reasoning abilities”（通用推理能力），这与你的研究目标“大语言模型通用推理能力”直接对应。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 该论文不涉及任何排除标准中的领域。它研究的是通用的语言模型架构，而非多模态、特定应用领域（如医疗、化学）或应用层面的可靠性问题（如水印、安全）。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/安全等特殊情况，无需进行特殊判断。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是一种创新的、旨在增强LLM通用推理能力的架构和训练方法。它通过分离知识与推理，使得一个参数量很小的模型也能具备强大的推理能力，这直接回应了你的核心目标——“致力于提高大语言模型（LLM）本身的『通用推理能力』”。尽管论文也提到了在边缘设备上的效率优势，但这只是其方法论带来的一个附加好处，其研究的根本出发点是模型能力的提升和优化，而非基础设施或部署优化。因此，这篇论文与你的研究课题高度相关，应予以保留。"
    },
    {
        "index": "#46",
        "title": "Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents",
        "link": "/arxiv/2510.02369",
        "arxiv_id": "2510.02369",
        "authors": "Kuntai Cai, Juncheng Liu, Xianglin Yang, Zhaojie Niu, Xiaokui Xiao, Xing Chen",
        "summary": "Large language model (LLM) agents typically receive two kinds of context: (i) environment-level manuals that define interaction interfaces and global rules, and (ii) task-level guidance or demonstrations tied to specific goals. In this work, we identify a crucial but overlooked third type of context, instance-level context, which consists of verifiable and reusable facts tied to a specific environment instance, such as object locations, crafting recipes, and local rules. We argue that the absence of instance-level context is a common source of failure for LLM agents in complex tasks, as success often depends not only on reasoning over global rules or task prompts but also on making decisions based on precise and persistent facts. Acquiring such context requires more than memorization: the challenge lies in efficiently exploring, validating, and formatting these facts under tight interaction budgets. We formalize this problem as Instance-Level Context Learning (ILCL) and introduce our task-agnostic method to solve it. Our method performs a guided exploration, using a compact TODO forest to intelligently prioritize its next actions and a lightweight plan-act-extract loop to execute them. This process automatically produces a high-precision context document that is reusable across many downstream tasks and agents, thereby amortizing the initial exploration cost. Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent gains in both success and efficiency: for instance, ReAct's mean success rate in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By transforming one-off exploration into persistent, reusable knowledge, our method complements existing contexts to enable more reliable and efficient LLM agents.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.883957",
        "filter_reason": "这篇论文完全符合您的筛选标准，应该被保留。以下是我的详细判断过程： 1.  **核心判断（第一步）：** 论文的核心并非将LLM应用于特定领域，而是提出了一种名为“实例级上下文学习”的新方法论，旨在增强LLM智能体本身的基础能力。论文指出，现有智能体在复杂任务中失败的一个关键原因是缺乏对环境实例中具体、持久事实的掌握。其核心贡献是提出一种通用的、任务无关的方法，让智能体能够通过“引导式探索”和“计划-行动-提取循环”来高效地获取、验证和利用这些事实。这直接提升了智能体的规划、决策和问题解决能力，属于对LLM通用推理能力的底层增强。 2.  **正面指标（第二步）：** 论文与多个正面指标高度匹配。 *   **核心概念:** 论文明确聚焦于“Large language model (LLM) agents”。 *   **能力方向:** 论文的核心目标是提升智能体的“problem-solving”能力。摘要中明确提到，成功依赖于“reasoning over global rules”以及“making decisions based on precise and persistent facts”。其方法中的“plan-act-extract loop”和“intelligently prioritize its next actions”直接对应了“planning”和“reasoning”能力。 *   **新兴范式:** 论文的研究对象是“llm-based agents”，其提出的方法可以被看作是一种增强智能体“tool use”（将探索和知识提取作为工具）和“deep research”（深入研究环境实例）能力的通用框架。 3.  **排除标准（第三步）：** 论文完全避开了所有排除标准。 *   它不涉及多模态、视觉等内容。 *   它的实验环境是TextWorld、ALFWorld等通用智能体基准，而非医疗、化学等特定应用领域，并且方法本身被强调为“task-agnostic”（任务无关）。 *   它不讨论水印、安全等模型可靠性问题。 4.  **特殊和模糊情况（第四步）：** *   **智能体/工具使用:** 论文是提出一种通用的智能体框架（ILCL）来增强LLM的通用问题解决能力，完全符合保留条件。它不是将智能体应用于某个特定垂直领域，而是为智能体提供一个更强大的、可复用的知识获取和利用机制，使其在各类环境中都能表现得更好。 **总结：** 这篇论文的核心贡献是提出了一种新的学习范式（ILCL），它通过让LLM智能体主动、高效地学习和记忆环境中的关键事实，显著提升了其在复杂任务中的规划、决策和推理能力。这是一种对LLM智能体底层能力的根本性增强，而非在特定场景下的应用。因此，它精准地契合了您关于“提高大语言模型（LLM）本身的『通用推理能力』”的研究目标。"
    },
    {
        "index": "#77",
        "title": "Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning",
        "link": "/arxiv/2510.02324",
        "arxiv_id": "2510.02324",
        "authors": "Wannan Yang, Xinchi Qiu, Lei Yu, Yuchen Zhang, Oliver Aobo Yang, Narine Kokhlikyan, Nicola Cancedda, Diego Garcia-Olano",
        "summary": "Large Language Models (LLMs) exhibit impressive capabilities but often hallucinate, confidently providing incorrect answers instead of admitting ignorance. Prior work has shown that models encode linear representations of their own knowledge and that activation steering can reduce hallucinations. These approaches, however, require real-time monitoring and intervention during inference. We introduce Contrastive Activation Steering for Amortized Learning (CASAL), an efficient algorithm that connects interpretability with amortized optimization. CASAL directly bakes the benefits of activation steering into model's weights. Once trained, LLMs answer questions they know while abstaining from answering those they do not. CASAL's light-weight design requires training only a submodule of a single transformer layer and yet reduces hallucination by 30%-40% across multiple short-form QA benchmarks. CASAL is 30x more compute-efficient and 20x more data-efficient than strong LoRA-based baselines such as SFT and DPO, boosting its practical applicability in data scarce domains. Importantly, CASAL also generalizes effectively to out-of-distribution (OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in both text-only and vision-language models. To our knowledge, CASAL is the first steering-based training method that has been shown to be effective for both dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step forward for applying interpretability-inspired method for practical deployment in production systems.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.962360",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为CASAL的新算法，通过将“激活导向”技术“烘焙”到模型权重中，来从根本上减少LLM的幻觉问题。这并非将LLM作为工具应用于特定领域，而是直接改进LLM本身的基础能力——即事实准确性和可靠性。一个能够准确回答已知问题、并拒绝回答未知问题的模型，是进行可靠推理的先决条件。因此，论文的本质是改进LLM的基础能力，符合保留标准。 2.  **第二步：正面指标** 论文明确包含核心概念“Large Language Models (LLMs)”。虽然摘要中没有直接使用“reasoning”一词，但其核心目标“hallucination reduction”（减少幻觉）与通用推理能力高度相关。幻觉是阻碍LLM进行有效逻辑推理和问题解决的关键障碍之一。通过减少幻觉，论文直接提升了模型输出的可靠性，从而为高质量的推理奠定了基础。 3.  **第三步：排除标准** 论文虽然提到其方法在“vision-language models”上也有效，但这只是为了展示其方法的通用性和灵活性，并非论文的主要研究焦点。论文的核心贡献是CASAL这一通用训练方法，而不是一个多模态模型或视觉应用。因此，它不属于被排除的“多模态与视觉”类别。同样，它也不涉及任何特定应用领域（如医疗、化学等）。 4.  **第四步：处理特殊和模糊情况** 这篇论文是“幻觉/可解释性”特殊情况的典型范例。它不是对幻觉现象进行社会学分析或应用层面的讨论，而是提出了一种**全新的、内在的、基于训练的方法**来减少幻觉。通过将可解释性研究（激活导向）的发现与模型训练相结合，它从根本上改变了模型的行为模式，提升了模型的内在可靠性。根据筛选标准，“如果论文提出一种新方法来减少幻觉……从而提升模型的通用可靠性和推理质量，应该保留。” CASAL完全符合这一描述。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出一种通用的、高效的训练范式（CASAL），旨在通过减少幻觉来提升LLM的内在可靠性。这直接服务于“提高大语言模型本身的通用推理能力”这一核心目标，因为一个可靠的、不胡言乱语的模型是进行有效推理的基础。因此，这篇论文高度相关，应被保留。"
    },
    {
        "index": "#61",
        "title": "DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning",
        "link": "/arxiv/2510.02341",
        "arxiv_id": "2510.02341",
        "authors": "Yifan Wang, Bolian Li, Junlin Wu, Zhaoxuan Tan, Zheli Liu, Ruqi Zhang, Ananth Grama, Qingkai Zeng",
        "summary": "Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce \\textbf{DRIFT} (\\textbf{D}issatisfaction-\\textbf{R}efined \\textbf{I}terative pre\\textbf{F}erence \\textbf{T}raining), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world \\textit{WildFeedback} datasets and synthetic \\textit{UltraFeedback} datasets achieve up to +6.23\\% (7B) / +7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.912203",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为 **DRIFT** 的全新**训练范式**。它并非将LLM应用于某个特定领域，而是专注于如何利用真实世界中丰富的“用户不满”信号来优化LLM本身。这是一种改进LLM基础能力（特别是对齐和问题解决能力）的方法论研究，完全符合“改进LLM的基础能力、提出新的训练范式”的保留标准。 2.  **第二步：正面指标** - **核心概念**: 论文明确以大型语言模型为研究对象。 - **训练方法**: DRIFT是一种创新的**偏好学习**方法，与强化学习（RLHF）在目标上一脉相承，都属于通过反馈信号优化模型行为的训练范式。它解决了传统偏好学习方法依赖昂贵正样本的痛点。 - **能力方向**: 论文通过在 `WildBench` 和 `AlpacaEval2` 这两个**通用评测基准**上取得显著提升，证明了其方法能有效增强模型的**通用问题解决能力**。此外，论文提到DRIFT能“preserves exploratory capacity, yielding more diverse high-reward solutions”，这直接关联到模型避免思维僵化、产生更优解的推理与规划能力。因此，它虽然不直接研究数学或逻辑推理，但其方法本质上是提升了模型面对各类未知问题时的通用推理和求解能力。 3.  **第三步：排除标准** 论文完全不涉及多模态、特定应用领域（如医疗、化学）或模型可靠性的应用层面（如水印、安全）。它研究的是通用的模型优化方法，因此完美避开了所有排除标准。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况。 5.  **第五步：最终决策** 综合分析，这篇论文的本质是提出了一种创新的、可扩展的LLM后训练方法。通过巧妙地利用真实世界中的用户不满意信号，该方法有效提升了模型在通用任务上的表现，并增强了其探索多样化高质量解的能力。这直接作用于LLM的通用推理和问题解决核心能力的提升，与研究课题“大语言模型通用推理能力”高度契合。因此，最终判断为 **True**。"
    },
    {
        "index": "#78",
        "title": "Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward",
        "link": "/arxiv/2510.03222",
        "arxiv_id": "2510.03222",
        "authors": "Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \\textbf{\\textit{reasoning sparks}}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of \\textit{reasoning sparks} is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a $60.17\\%$ average accuracy on five math benchmarks, an improvement of $2.66\\%$ over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.963174",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一种新的训练方法（Lp-Reg）来解决“可验证奖励强化学习”（RLVR）中的一个核心瓶颈问题。RLVR本身是一种用于提升大语言模型复杂推理能力的前沿训练范式。论文的核心贡献在于，通过保护“低概率推理令牌”来维持训练过程中的探索能力，从而直接提升了模型在数学推理等任务上的表现。这完全属于“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、多步推理等通用能力”的范畴。它并非将LLM应用于特定领域，而是研究如何让LLM本身“学得更好”。 2.  **第二步：正面指标** - **核心概念**: 论文明确以“Large Language Models”为研究对象。 - **能力方向**: 论文的核心目标是提升“complex reasoning”，并在“math benchmarks”上验证效果，直接命中“reasoning”和“math reasoning”这两个关键方向。 - **训练方法**: 论文的研究背景和核心方法都围绕“Reinforcement Learning”展开，具体是RLVR和提出的Lp-Reg，这完全符合筛选标准。 3.  **第三步：排除标准** - 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）。因此，所有排除标准均不适用。 4.  **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需进行特殊判断。 5.  **第五步：最终决策** - **综合判断**：该论文精准地聚焦于如何通过改进强化学习训练过程，来提升大语言模型的通用推理能力。它识别出了一个具体的训练瓶颈（探索能力坍塌），并提出了一个新颖且有效的解决方案（Lp-Reg）。其研究目标是方法论层面的，旨在增强LLM的内在能力，而非将其作为工具应用于外部领域。因此，这篇论文是关于“大语言模型通用推理能力”研究课题下的高质量前沿文献，应予以保留。"
    },
    {
        "index": "#83",
        "title": "NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning",
        "link": "/arxiv/2510.02816",
        "arxiv_id": "2510.02816",
        "authors": "Yulong Zhang, Li Wang, Wei Du, Peilin Li, Yuqin Dai Zhiyuan Zhao, Lingyong Fang, Ziniu Liu, Ru Zhang, Huijia Zhu, Gongshen Liu",
        "summary": "Verifying multi-step reasoning in large language models is difficult due to imprecise error localization and high token costs. Existing methods either assess entire reasoning chains, suffering attention dilution, or rely on expensive multi-sampling. We introduce Node-wise Consistency Verification (NCV), a training-free framework that recasts verification as lightweight binary consistency checks at the node level. By decomposing the chain of thought into interconnected verification nodes, NCV precisely localizes errors and avoids unnecessary long-form generation. Experiments demonstrate that our approach enhances interpretability and efficiency, presenting a scalable solution for reliable LLM reasoning verification. On public datasets, NCV achieves a 10\\% to 25\\% improvement in F1 scores over baselines while utilizing $6\\times$~$58\\times$ fewer tokens than traditional methods like CoT-based verifiers.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.966903",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 这篇论文的本质是提出一种名为NCV的新方法，用于**验证和定位大语言模型在多步推理过程中的错误**。这直接触及了LLM核心推理能力的可靠性问题。论文并非将LLM作为工具应用于某个特定领域，而是专注于改进LLM推理过程本身的质量、效率和可解释性。其核心贡献——一种轻量级、训练免费的验证框架——属于提升LLM基础通用能力的方法论研究。因此，根据核心判断标准，应予以**保留**。 2.  **第二步：正面指标** 论文高度符合正面指标： *   **核心概念**: 论文标题和摘要中明确包含 \"Large language models, LLMs\"。 *   **能力方向**: 论文的核心主题是 \"LLM Reasoning\" 和 \"multi-step reasoning\"，这直接对应了您关注的 \"reasoning\" 能力。 *   **新兴范式**: 论文的方法建立在 \"Chain of Thought\" (CoT) 之上，并对其进行改进，这与您关注的前沿推理范式紧密相关。 3.  **第三步：排除标准** 论文完全不涉及排除标准中的任何领域。它没有讨论多模态、视觉，也没有针对医疗、化学等特定应用，更不涉及水印、安全等应用层面的可靠性问题。其焦点始终是LLM的通用推理过程。 4.  **第四步：处理特殊和模糊情况** 这篇论文恰好命中了“可解释性”这一特殊情况的保留条件。它提出的NCV方法通过将推理链分解为节点并进行一致性检查，实现了“精确的错误定位”和“增强的可解释性”。这并非对可解释性问题的社会学讨论，而是提出了一种**提升模型内在推理质量和通用可靠性的新方法**。因此，这属于应该保留的情况。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种创新的方法论，旨在解决LLM在通用多步推理中的一个关键痛点：错误难以定位且验证成本高昂。该研究直接提升了LLM推理的可靠性、效率和可解释性，完全契合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为符合要求。"
    },
    {
        "index": "#72",
        "title": "EntropyLong: Effective Long-Context Training via Predictive Uncertainty",
        "link": "/arxiv/2510.02330",
        "arxiv_id": "2510.02330",
        "authors": "Junlong Jia, Ziyang Chen, Xing Wu, Chaochen Gao, Zijia Lin, Debing Zhang, Songlin Hu, Binghui Guo",
        "summary": "Training long-context language models to capture long-range dependencies requires specialized data construction. Current approaches, such as generic text concatenation or heuristic-based variants, frequently fail to guarantee genuine long-range dependencies. We propose EntropyLong, a novel data construction method that leverages predictive uncertainty to verify dependency quality. Our approach identifies high-entropy positions in documents, retrieves semantically relevant contexts from large corpora, and verifies their utility by assessing whether they reduce prediction entropy. This model-in-the-loop verification ensures each dependency represents measurable information gain rather than spurious correlation. We construct training samples with long-range dependencies by combining original documents with these verified contextual supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of 128K-length sequences with verified dependencies. Models trained on this data demonstrate significant improvements on RULER benchmarks, particularly in tasks requiring distant information. Following instruction fine-tuning, our models also achieve substantial gains on LongBenchv2, demonstrating enhanced long-context understanding. Extensive ablation studies further validate the necessity and effectiveness of entropybased verification for long-context training.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.923983",
        "filter_reason": "这篇论文完全符合我的研究范围。以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“EntropyLong”的新方法，用于改进大语言模型的**长上下文训练**。其本质是通过一种“模型参与循环”的数据构造策略，来确保模型能学到**真正的长距离依赖关系**。长上下文理解和处理长距离依赖，是**通用推理能力的基础设施和前提**。如果一个模型无法在长文本中保持信息一致性、无法关联开头和结尾的信息，那么任何复杂的多步推理、规划或问题解决都无从谈起。因此，这篇论文致力于改进LLM的基础能力，而非将其应用于特定领域，应予以保留。 2.  **第二步：正面指标** - **核心概念**: 论文明确聚焦于“大语言模型”。 - **能力方向**: 论文虽然没有直接使用“reasoning”一词，但其解决的“长距离依赖关系”和“远距离信息利用”问题，是数学、逻辑等复杂推理任务的核心瓶颈。在RULER和LongBenchv2等通用基准上的提升，直接证明了模型在需要综合信息的**问题解决**能力上的进步。 - **训练方法**: 论文提出了一种创新的、基于模型不确定性（熵）的数据构造和训练范式，这本身就是方法论的贡献。 3.  **第三步：排除标准** 论文的研究焦点与所有排除标准均无关系。它不涉及多模态、不限定任何特定应用领域（如医疗、化学），也不讨论模型部署、水印或应用层的安全问题。 4.  **第四步：处理特殊和模糊情况** - 论文中对“预测不确定性”的应用是关键。它并非用于安全或水印，而是作为一种**内在的信号来指导训练数据的生成**，从而提升模型捕捉有效信息的能力。这完全符合“提出一种新方法来提升模型的内在可靠性，从而提升推理质量”的保留原则。 5.  **第五步：最终决策** 综上所述，这篇论文通过改进数据构造和训练范式，直接增强了大语言模型处理长上下文这一**基础且关键的通用能力**。这项工作是实现更高级别通用推理能力的基石，因此它精准地契合了“致力于提高大语言模型（LLM）本身的通用推理能力”这一核心研究目标。最终判断为 **True**。"
    },
    {
        "index": "#81",
        "title": "When Names Disappear: Revealing What LLMs Actually Understand About Code",
        "link": "/arxiv/2510.03178",
        "arxiv_id": "2510.03178",
        "authors": "Cuong Chi Le, Minh V. T. Pham, Cuong Duc Van, Hoang N. Phan, Huy N. Phan, Tien N. Nguyen",
        "summary": "Large Language Models (LLMs) achieve strong results on code tasks, but how they derive program meaning remains unclear. We argue that code communicates through two channels: structural semantics, which define formal behavior, and human-interpretable naming, which conveys intent. Removing the naming channel severely degrades intent-level tasks such as summarization, where models regress to line-by-line descriptions. Surprisingly, we also observe consistent reductions on execution tasks that should depend only on structure, revealing that current benchmarks reward memorization of naming patterns rather than genuine semantic reasoning. To disentangle these effects, we introduce a suite of semantics-preserving obfuscations and show that they expose identifier leakage across both summarization and execution. Building on these insights, we release ClassEval-Obf, an obfuscation-enhanced benchmark that systematically suppresses naming cues while preserving behavior. Our results demonstrate that ClassEval-Obf reduces inflated performance gaps, weakens memorization shortcuts, and provides a more reliable basis for assessing LLMs' code understanding and generalization.",
        "subjects": "Software Engineering, Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.965376",
        "filter_reason": "这篇论文符合我的研究范围。以下是基于筛选标准的详细判断过程： **第一步：核心判断** 这篇论文的本质不是提出一种新的训练方法来直接提升LLM的推理能力，而是进行了一项深刻的**诊断性和评估性研究**。它的核心贡献在于揭示了当前LLM在代码理解任务上，其表现并非完全源于真正的“语义推理”，而很大程度上依赖于对变量名、函数名等“命名线索”的记忆和模式匹配。论文通过构建新的评估基准，强制模型只能依赖代码的“结构语义”进行推理，从而剥离了记忆捷径，为更纯粹、更准确地衡量LLM的通用推理能力提供了工具。这项工作对于“提高LLM通用推理能力”这一宏观目标至关重要，因为它首先解决了一个基础性问题：**如何准确地测量通用推理能力**。如果没有可靠的测量方法，任何声称“提升”了能力的研究都可能是建立在沙堆之上。因此，这篇论文通过改进评估范式，间接但有力地推动了整个领域的发展，其本质是关于LLM基础能力（推理）的深刻洞察。 **第二步：正面指标** - **核心概念**: 论文明确以“Large Language Models (LLMs)”为研究对象。 - **能力方向**: 论文的核心是探讨“reasoning”，特别是代码任务中的“genuine semantic reasoning”（真正的语义推理）。它区分了伪推理（依赖命名）和真推理（依赖结构），这与我们对通用推理能力的要求高度一致。 - **新兴范式**: 虽然未直接提及智能体或工具使用，但其提出的新评估方法“ClassEval-Obf”可以被视为一种评估方法论上的创新，有助于推动后续针对“真正推理”的训练范式研究。 **第三步：排除标准** - 论文不涉及多模态、视觉等内容。 - 论文的研究领域是“代码”，这属于LLM的通用能力范畴，而非医疗、化学、法律等特定应用领域。 - 论文关注的是模型内在的推理机制和评估的可靠性，而非应用层面的水印、安全等问题。 **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: 这篇论文完美地符合此处的保留条件。它提出了一种新方法（语义保留的混淆和ClassEval-Obf基准）来**揭示模型推理过程中的缺陷**（依赖命名捷径而非结构推理），这增强了我们对模型内在工作机制的“可解释性”。通过减少评估中的“伪推理”成分，它提升了评估的可靠性，从而为未来提升模型的“通用可靠性和推理质量”铺平了道路。这并非社会学讨论，而是对模型内在能力的深刻剖析。 **第五步：最终决策** 综合以上分析，这篇论文虽然不是一篇“训练”论文，但它是一篇极其重要的“诊断”和“度量”论文。它精准地指出了当前LLM在代码推理上的一个关键短板，并提供了一个更严格的评估基准。对于任何致力于“提高LLM通用推理能力”的研究者来说，理解并使用这样的基准是避免走入歧途、实现真正突破的前提。因此，这篇论文与我的研究目标高度相关，应当保留。"
    },
    {
        "index": "#82",
        "title": "Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents",
        "link": "/arxiv/2510.02837",
        "arxiv_id": "2510.02837",
        "authors": "Wonjoong Kim, Sangwu Park, Yeonjun In, Sein Kim, Dongha Lee, Chanyoung Park",
        "summary": "Although recent tool-augmented benchmarks incorporate complex user requests and diverse tools, the evaluation methods for most of them remain limited to answer matching. However, as the number of steps required to resolve a user request increases, a proper evaluation of an agent's performance must go beyond the final answer to also assess the problem-solving trajectory, including previously ignored aspects such as efficiency, hallucination, and adaptivity. The most straightforward method for evaluating these aspects is to compare an agent's trajectory with the ground-truth trajectory, but this approach is fundamentally limited since annotating all valid ground-truth trajectories is prohibitively expensive. However, a simple LLM-based evaluator struggles to assess trajectories in detail without ground truth. To effectively evaluate the agents in this manner, we introduce TRACE, a framework for the multi-dimensional evaluation of tool-augmented LLM agent performance. By incorporating an evidence bank, which accumulates knowledge gathered from preceding reasoning steps, TRACE enables a multi-faceted analysis and evaluation of an agent's reasoning trajectory effectively. To validate our framework, we develop a new meta-evaluation dataset by augmenting existing benchmarks with diverse and flawed trajectories, each labeled with multi-faceted performance scores. Our results confirm that TRACE accurately evaluates these complex behaviors in a scalable and cost-effective manner, even with small open-source LLMs. Furthermore, we apply our method to evaluate the trajectories that agents produce while solving tool-augmented tasks, presenting previously unreported observations and their corresponding insights.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.966127",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步 (核心判断):** 论文的核心不是将LLM作为工具应用于某个特定领域，而是提出了一种名为**TRACE**的新颖评估框架。这个框架旨在深入分析和评估**工具增强型LLM智能体的“推理轨迹”**。这直接契合了您筛选标准中“改进LLM的基础能力”和“增强其逻辑、规划、多步推理等通用能力”的目标。虽然论文本身是“评估”而非“训练”，但一个能够精确衡量推理过程优劣的评估框架，是后续优化和提升模型推理能力不可或缺的基础方法论。它属于对LLM核心能力进行深入研究的方法论范畴。 2.  **第二步 (正面指标):** 论文与正面指标高度相关。 -   **核心概念**: 论文研究对象是 \"tool-augmented LLM agent\"。 -   **能力方向**: 论文的核心是评估 \"reasoning trajectories\"（推理轨迹），并具体关注 \"efficiency\"（效率）、\"hallucination\"（幻觉）、\"adaptivity\"（适应性）等关键推理能力指标。 -   **新兴范式**: 论文聚焦于 \"tool-augmented agents\" 这一前沿范式。 3.  **第三步 (排除标准):** 论文不触及任何排除标准。它不涉及多模态、视觉，不针对医疗、化学等特定应用领域，也不讨论水印、安全等应用层面的可靠性问题。 4.  **第四步 (处理特殊和模糊情况):** -   **智能体/工具使用**: 论文提出的TRACE框架是一种**通用的**评估方法，用于衡量智能体在解决通用问题时的推理过程质量，而非应用于特定领域（如化学实验）。这完全符合“保留”条件。 -   **幻觉/可解释性**: 论文将“幻觉”作为推理轨迹中的一个关键缺陷来评估，其目的是为了提升模型内在的推理质量和可靠性。这种通过提出新评估方法来识别和理解问题的研究，属于提升模型通用能力的范畴，符合“保留”条件。 **最终决策:** 综合以上分析，这篇论文的核心贡献是提出了一种全新的、多维度的评估框架（TRACE），用于精确衡量LLM智能体的推理过程质量。这种对“推理轨迹”的深入评估方法论，正是推动大语言模型通用推理能力向前发展的关键一环。它为研究者提供了诊断模型推理缺陷、指导模型优化的强大工具，因此完全符合您为“大语言模型通用推理能力”课题筛选前沿论文的要求。"
    },
    {
        "index": "#79",
        "title": "Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner",
        "link": "/arxiv/2510.03206",
        "arxiv_id": "2510.03206",
        "authors": "Cai Zhou, Chenxiao Yang, Yi Hu, Chenyu Wang, Chubin Zhang, Muhan Zhang, Lester Mackey, Tommi Jaakkola, Stephen Bates, Dinghuai Zhang",
        "summary": "Diffusion language models, especially masked discrete diffusion models, have achieved great success recently. While there are some theoretical and primary empirical results showing the advantages of latent reasoning with looped transformers or continuous chain-of-thoughts, continuous diffusion models typically underperform their discrete counterparts. In this paper, we argue that diffusion language models do not necessarily need to be in the discrete space. In particular, we prove that continuous diffusion models have stronger expressivity than discrete diffusions and looped transformers. We attribute the contradiction between the theoretical expressiveness and empirical performance to their practical trainability: while continuous diffusion provides intermediate supervision that looped transformers lack, they introduce additional difficulty decoding tokens into the discrete token space from the continuous representation space. We therefore propose Coevolutionary Continuous Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process on the union of a continuous representation space and a discrete token space, leveraging a single model to simultaneously denoise in the joint space. By combining two modalities, CCDD is expressive with rich semantics in the latent space, as well as good trainability and sample quality with the help of explicit discrete tokens. We also propose effective architectures and advanced training/sampling techniques for CCDD, which reveals strong empirical performance in extensive language modeling experiments on real-world tasks.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.963963",
        "filter_reason": "这篇论文完全符合筛选要求。 我的判断过程如下： 1.  **第一步：核心判断** 这篇论文的本质是提出一种新的模型架构和训练范式（Coevolutionary Continuous Discrete Diffusion, CCDD），旨在解决当前扩散语言模型（一种LLM）在推理能力上的缺陷，并使其成为一个更强大的\"潜在推理者\"。论文的核心目标是改进模型本身的基础能力——即其在潜在空间进行推理的能力，而不是将模型应用于某个特定领域。这完全符合“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标** 论文高度符合多个正面指标： *   **核心概念**: 论文的研究对象是\"Diffusion language models\"，这属于大语言模型的范畴。 *   **能力方向**: 论文标题和摘要中反复强调的核心目标是\"Latent Reasoner\"（潜在推理者）和\"latent reasoning\"（潜在推理），并直接关联到\"continuous chain-of-thoughts\"（连续思维链）。这直接命中了\"reasoning\"这一核心能力方向。 3.  **第三步：排除标准** 论文不涉及任何排除标准： *   **多模态与视觉**: 论文中提到的\"multimodal diffusion process\"（多模态扩散过程）是一个关键点，但需要准确理解。这里的“模态”指的是模型内部的**连续表示空间**和**离散标记空间**，而不是指外部的视觉、音频等多模态数据。因此，这不属于被排除的多模态研究范畴。 *   **特定应用领域**: 论文实验是在\"real-world tasks\"（真实世界任务）上的\"language modeling experiments\"（语言建模实验），没有聚焦于医疗、化学等任何特定应用领域。 *   **模型可靠性**: 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需额外判断。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种新的方法论（CCDD），通过改进模型架构和训练过程，来增强扩散语言模型这一特定类型LLM的内在推理能力。它是一项致力于提升LLM“通用推理能力”的基础性研究，与我的研究目标高度一致。因此，最终判断为符合要求。"
    },
    {
        "index": "#90",
        "title": "On the Role of Temperature Sampling in Test-Time Scaling",
        "link": "/arxiv/2510.02611",
        "arxiv_id": "2510.02611",
        "authors": "Yuheng Wu, Azalia Mirhoseini, Thierry Tambe",
        "summary": "Large language models (LLMs) can improve reasoning at inference time through test-time scaling (TTS), where multiple reasoning traces are generated and the best one is selected. Prior work shows that increasing the number of samples K steadily improves accuracy. In this paper, we demonstrate that this trend does not hold indefinitely: at large K, further scaling yields no gains, and certain hard questions remain unsolved regardless of the number of traces. Interestingly, we find that different sampling temperatures solve different subsets of problems, implying that single-temperature scaling explores only part of a model's potential. We therefore propose scaling along the temperature dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3 (0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME 2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an additional 7.3 points over single-temperature TTS. Temperature scaling also enables base models to reach performance comparable to reinforcement learning (RL)-trained counterparts, without additional post-training. We further provide a comprehensive analysis of this phenomenon and design a multi-temperature voting method that reduces the overhead of temperature scaling. Overall, our findings suggest that TTS is more powerful than previously thought, and that temperature scaling offers a simple and effective way to unlock the latent potential of base models.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.975468",
        "filter_reason": "根据您提供的筛选标准，我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心是提出一种名为“温度缩放”的新方法论，它属于“测试时缩放”的一种。该方法通过在推理时使用不同的采样温度生成多个推理路径，然后进行集成，从而显著提升大语言模型在复杂问题上的表现。这本质上是一种**改进LLM基础推理能力的方法论研究**，旨在挖掘和释放模型已有的、但未被充分利用的“潜在潜力”。它并非将LLM应用于特定领域，也不是关于模型基础设施，因此符合“保留”标准。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文高度符合正面指标： *   **核心概念**: 论文标题和摘要多次明确提及“Large language models (LLMs)”。 *   **能力方向**: 论文的研究核心就是“reasoning”（推理）。摘要中明确指出其在“五个代表性推理 benchmarks (AIME 2024/2025, MATH500, LiveCodeBench, Hi-ToM)”上取得了显著提升，这些基准涵盖了数学推理、编程推理和心智理论推理，均是通用推理能力的核心体现。 *   **训练方法**: 论文将其方法与“reinforcement learning (RL)-trained counterparts”进行对比，并证明其方法能让基础模型达到与RL训练模型相当的性能，这表明其研究目标与通过RL等方法提升推理能力的研究高度一致。 3.  **第三步：排除标准——论文是否聚焦于排除领域？** 论文完全避开了所有排除标准。它不涉及多模态、视觉，不聚焦于任何特定应用领域（如医疗、化学），也不讨论模型可靠性层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 此论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，无需进行特殊判断。 5.  **第五步：最终决策** 综合分析，这篇论文的核心贡献是发现并利用了“不同采样温度可以解决不同问题子集”这一现象，进而提出“温度缩放”这一简单、通用的方法，来**系统性地提升LLM在数学、逻辑、编程等多个维度的通用推理能力**。它直接作用于模型本身的推理过程，而非外部应用，完全符合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心研究目标。因此，应予以保留。"
    },
    {
        "index": "#92",
        "title": "Beyond Imitation: Recovering Dense Rewards from Demonstrations",
        "link": "/arxiv/2510.02493",
        "arxiv_id": "2510.02493",
        "authors": "Jiangnan Li, Thuy-Trang Vu, Ehsan Abbasnejad, Gholamreza Haffari",
        "summary": "Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation learning process that only trains a policy to imitate expert behavior on demonstration datasets. In this work, we challenge this view by establishing a fundamental equivalence between SFT and Inverse Reinforcement Learning. We prove that the SFT objective is a special case of Inverse Q-Learning, which implies that the SFT process does not just learn a policy, but also an implicit, dense, token-level reward model that explains the expert demonstrations. We then show how to recover this dense reward signal directly from the SFT model by formulating a baseline-relative reward function. The availability of such a dense reward model offers numerous benefits, providing granular credit assignment for each token generated. We demonstrate one key application by using these recovered rewards to further improve the policy with reinforcement learning. Our method, Dense-Path REINFORCE, consistently outperforms the original SFT models on instruction-following benchmarks. This work reframes SFT not merely as policy imitation but as a powerful reward learning mechanism, opening new possibilities for leveraging expert demonstrations.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.976398",
        "filter_reason": "这篇论文完全符合我的研究范围，核心判断如下： 1.  **核心判断 (第一步):** 这篇论文的本质是提出一种新的训练范式和方法论，旨在从根本上改进大语言模型的学习过程和能力上限，而非将其应用于特定领域。论文的核心贡献是重新诠释了监督微调（SFT）的本质，揭示了其与逆向强化学习的等价性，并从中恢复出密集的、token级别的奖励信号。随后，它利用这个奖励信号通过强化学习（RL）来进一步优化模型本身。这完全符合“改进LLM的基础能力、提出新的训练范式”这一核心保留标准。 2.  **正面指标 (第二步):** 论文与多个正面指标高度相关。 *   **训练方法:** 论文的核心就是关于强化学习（RL）及其与SFT的结合，提出了\"Dense-Path REINFORCE\"这一新方法。 *   **能力方向:** 虽然摘要未直接使用\"reasoning\"一词，但其通过RL精细优化\"指令遵循\"能力，本质上是在提升模型理解和执行复杂、多步任务的能力，这是通用推理能力的重要组成部分。改进策略本身就是提升规划与问题解决能力的基础。 *   **自我进化:** 论文展示了一个从SFT模型中提取信息（奖励模型）来进一步优化该模型的闭环，这是一种模型自我改进和优化的体现。 3.  **排除标准 (第三步):** 论文完全避开了所有排除标准。它不涉及多模态、视觉，也不是针对医疗、化学等特定应用领域，更不关注水印、安全等应用层面的可靠性问题。 4.  **特殊和模糊情况 (第四步):** 本论文情况清晰，不属于模糊范畴。它提出的方法（恢复并利用密集奖励）是通用的，旨在提升模型自身的性能，而不是将智能体或工具用于特定领域。 **最终决策 (第五步):** 综合来看，这篇论文提出了一种创新的训练方法论，通过揭示SFT背后的奖励学习机制，并利用强化学习进行精细优化，从而系统性地提升了LLM的基础能力。这种对模型训练范式的深刻反思和改进，直接服务于“提高大语言模型本身的通用推理能力”这一核心目标。因此，这是一篇高度相关且应被保留的前沿论文。"
    },
    {
        "index": "#95",
        "title": "How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models",
        "link": "/arxiv/2510.02453",
        "arxiv_id": "2510.02453",
        "authors": "Parth Asawa, Alan Zhu, Matei Zaharia, Alexandros G. Dimakis, Joseph E. Gonzalez",
        "summary": "Foundation models are increasingly deployed as black-box services, where model weights cannot be modified and customization is limited to prompting. While static prompt optimization has shown promise, it produces a single fixed prompt that fails to adapt to different inputs, users, or environments. We introduce Advisor Models, lightweight parametric policies trained with reinforcement learning to reactively issue natural language steering instructions in-context to black-box models. The advisor is a second small model that sits between the input and the model, shaping behavior on a per-instance basis using reward signals from the environment. Across multiple domains involving reasoning and personalization, we show that Advisor Models outperform static prompt optimizers, discovering environment dynamics and improving downstream task performance. We also demonstrate the generalizability of advisors by transferring them across black-box models, as well as the framework's ability to achieve specialization while retaining robustness to out-of-distribution inputs. Viewed more broadly, Advisor Models provide a learnable interface to black-box systems where the advisor acts as a parametric, environment-specific memory. We argue that dynamic optimization of black-box models via Advisor Models is a promising direction for enabling personalization and environment-adaptable AI with frontier-level capabilities.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T01:04:26.983047",
        "filter_reason": "这篇论文完全符合你的研究范围，核心判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“顾问模型”的新框架。这是一种通过强化学习训练的轻量级模型，其任务是为黑盒LLM动态地、逐个实例地生成引导指令（在上下文中进行提示）。这本质上是一种**新的训练范式和方法论**，其直接目标是“塑造行为”并“提升下游任务性能”，尤其是在推理领域。这并非将LLM作为工具应用于特定领域，而是研究如何从外部引导和优化LLM本身的基础能力，完全符合“改进LLM基础能力、增强其通用推理能力”的保留标准。 2.  **第二步：正面指标** 该论文完美命中了多个关键正面指标： - **核心概念**: 论文明确针对 \"black-box LLMs\"（黑盒大语言模型）。 - **能力方向**: 摘要中直接提到其方法在 \"multiple domains involving **reasoning**\"（涉及推理的多个领域）中表现优异，这直接对应你的核心目标。 - **训练方法**: 论文的核心是使用 \"reinforcement learning\"（强化学习）来训练顾问模型，这是一种关键的训练优化方法。 - **新兴范式**: 该框架可以被视为一种特殊的 \"llm-based agent\"（基于LLM的智能体）或 \"tool use\"（工具使用）范式，其中“顾问”智能体学习如何更有效地“使用”作为工具的黑盒LLM。 3.  **第三步：排除标准** 论文完全不触及任何排除标准。它没有涉及多模态、视觉，没有聚焦于任何特定应用领域（如医疗、化学），也不是关于模型基础设施或应用层面的水印、安全等问题。其提出的方法是领域无关的，旨在提升通用能力。 4.  **第四步：处理特殊和模糊情况** 该论文是“智能体/工具使用”特殊情况的绝佳范例。它提出的是一个**通用的智能体协作/引导框架**（顾问模型 + 黑盒LLM），目的是增强LLM在推理等通用任务上的表现，而非将其应用于某个特定领域。因此，应当保留。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种创新的、基于强化学习的动态提示框架，用以增强黑盒LLM的通用推理能力和环境适应性。它直接回应了“提高大语言模型本身的通用推理能力”这一核心目标，并包含了推理、强化学习、智能体框架等所有关键正面指标，同时规避了所有排除标准。因此，该论文是与你研究课题高度相关的前沿研究，应予以保留。"
    },
    {
        "index": "#3",
        "title": "Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling",
        "link": "/arxiv/2510.03199",
        "arxiv_id": "2510.03199",
        "authors": "Qiwei Di, Kaixuan Ji, Xuheng Li, Heyang Zhao, Quanquan Gu",
        "summary": "LLM inference often generates a batch of candidates for a prompt and selects one via strategies like majority voting or Best-of- N (BoN). For difficult tasks, this single-shot selection often underperforms. Consequently, evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses, and only the best of them is used when computing regret. Motivated by this, we study inference scaling in the more general Pass@$k$ inference setting, and prove that neither majority voting nor BoN exhibits the desirable scaling with $k$ and the sampling budget $N$. Combining the advantages of majority voting and BoN, we propose a new inference strategy called Best-of-Majority (BoM), with a pivotal step that restricts the candidates to the responses with high frequency in the $N$ samples before selecting the top-$k$ rewards. We prove that when the sampling budget is $N=\\tilde\\Omega(C^*)$, the regret of BoM is $O(\\epsilon_{\\mathrm{opt}}+\\sqrt{\\epsilon_{\\mathrm{RM}}^2C^*/k})$, where $C^*$ is the coverage coefficient, $\\epsilon_{\\mathrm{RM}}$ is the estimation error of the reward model, and $\\epsilon_{\\mathrm{opt}}$ is the estimation error of reward at the optimal response. We further establish a matching lower bound, certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a key advantage: unlike majority voting and BoN, its performance does not degrade when increasing $N$. Experimental results of inference on math problems show BoM outperforming both majority voting and BoN.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.552442",
        "filter_reason": "这篇论文完全符合研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Best-of-Majority (BoM)”的**推理策略**。它不涉及改变模型的结构或进行新的训练，而是专注于在推理阶段如何更有效地从LLM生成的多个候选答案中选出最优解。这直接对应了筛选标准中“增强其逻辑、数学、规划、多步推理等通用能力”的目标。论文的本质是提出一种方法论来提升LLM在解决困难任务时的表现，这正是提升其“通用推理能力”的关键一环。它并非将LLM应用于特定领域，而是提出了一种通用的、旨在提升模型本身输出质量的推理方法。 2.  **第二步：正面指标** 论文明确包含了多个关键的正面指标： *   **核心概念**: 论文的研究对象是“大语言模型”。 *   **能力方向**: 论文的核心是提升“推理”能力。摘要中明确提到其方法在“困难任务”和“数学问题”上表现优异，这直接指向了数学推理这一通用推理的核心子领域。其目标是提升答案选择的准确性，这本身就是推理质量的体现。 3.  **第三步：排除标准** 论文完全避开了所有的排除标准： *   它不涉及多模态、视觉等内容。 *   它的应用场景是“数学问题”，但这被用作验证其通用推理能力的基准测试，而非论文的唯一焦点。论文提出的方法是通用的，可以应用于任何需要生成多个候选解并择优的任务，因此不属于“特定应用领域”。 *   它不讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不属于特殊或模糊情况，但其内核与“提升模型内在可靠性”的精神一致。通过提出一种更优的推理策略，BoM减少了从多个候选答案中选出错误答案的概率，这可以看作是从推理算法层面提升了模型输出的可靠性和准确性，从而增强了其通用推理能力。 5.  **第五步：最终决策** 综合以上分析，这篇论文提出了一种新颖的、理论上有保障的、且在实验上被证明有效的通用推理策略。它直接致力于解决“如何让LLM更好地进行推理”这一核心问题，完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#12",
        "title": "Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking",
        "link": "/arxiv/2510.03149",
        "arxiv_id": "2510.03149",
        "authors": "Dhruv Rohatgi, Abhishek Shetty, Donya Saless, Yuchen Li, Ankur Moitra, Andrej Risteski, Dylan J. Foster",
        "summary": "Test-time algorithms that combine the generative power of language models with process verifiers that assess the quality of partial generations offer a promising lever for eliciting new reasoning capabilities, but the algorithmic design space and computational scaling properties of such approaches are still opaque, and their benefits are far from apparent when one accounts for the cost of learning a high-quality verifier. Our starting point is the observation that seemingly benign errors in a learned verifier can lead to catastrophic failures for standard decoding techniques due to error amplification during the course of generation. We then ask: can this be improved with more sophisticated decoding strategies? We introduce a new process-guided test-time sampling algorithm, VGB, which uses theoretically grounded backtracking to achieve provably better robustness to verifier errors. VGB interprets autoregressive generation as a random walk on a tree of partial generations, with transition probabilities guided by the process verifier and base model; crucially, backtracking occurs probabilistically. This process generalizes the seminal Sinclair-Jerrum random walk (Sinclair & Jerrum, 1989) from the literature on approximate counting and sampling in theoretical computer science, and a conceptual contribution of our work is to highlight parallels with this literature. Empirically, we demonstrate on both synthetic and real language modeling tasks that VGB outperforms baselines on a variety of metrics.",
        "subjects": "Machine Learning, Data Structures and Algorithms",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.561564",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。我的判断过程如下： 1.  **核心判断 (第一步):** 论文的本质是提出一种新的**测试时算法**，名为VGB。其核心目标是解决LLM在推理过程中，由于“过程验证器”不完美而导致的错误放大问题。这直接触及了LLM的**通用推理能力**，特别是多步推理的鲁棒性和准确性。论文并非将LLM应用于特定领域，而是致力于改进LLM进行推理的底层算法机制，这与你的核心目标“提高LLM本身的通用推理能力”高度一致。它属于“增强其逻辑、多步推理等通用能力”的范畴。 2.  **正面指标 (第二步):** -   **核心概念**: 论文研究对象是“language models”，即LLMs。 -   **能力方向**: 论文摘要开篇即点明其目标是“eliciting new reasoning capabilities”（引出新的推理能力），并讨论如何改进“process verifiers”（过程验证器），这与`reasoning`和`problem-solving`直接相关。 -   **新兴范式**: 论文提出的VGB算法，可以看作是对思维链等过程监督方法的一种深化和改进，它在测试时通过更复杂的采样和回溯策略来优化推理路径，是一种提升模型推理表现的新方法论。 3.  **排除标准 (第三步):** -   **多模态与视觉**: 论文完全不涉及视觉或多模态内容。 -   **特定应用领域**: 论文的实验是在“合成和真实语言建模任务”上进行的，没有聚焦于医疗、化学等任何特定应用领域。 -   **模型可靠性（应用层面）**: 论文关注的是模型内在推理过程的可靠性（对验证器错误的鲁棒性），而非水印、安全等应用层面的可靠性问题。 4.  **特殊和模糊情况 (第四步):** -   **幻觉/可解释性/安全**: 论文处理的“verifier errors”（验证器错误）可以被视为一种推理路径上的“幻觉”或逻辑错误。论文提出VGB这种新方法来减轻此类错误，从而提升推理质量，这完全符合“提出一种新方法来减少幻觉、增强模型内在的可靠性”的保留标准。 **最终决策 (第五步):** 综合以上分析，这篇论文的核心贡献是一种新颖的、以提升推理鲁棒性为目标的测试时采样算法。它从算法设计的角度，深入探讨了如何让LLM在多步推理中更好地利用验证信号，即使验证信号本身不完美。这直接且精准地服务于“提升大语言模型通用推理能力”这一核心研究目标，是一篇高质量的前沿方法论研究，应当被筛选出来。"
    },
    {
        "index": "#6",
        "title": "PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning",
        "link": "/arxiv/2510.03185",
        "arxiv_id": "2510.03185",
        "authors": "Wanjia Zhao, Qinwei Ma, Jingzhe Shi, Shirley Wu, Jiaqi Han, Yijia Xiao, Si-Yuan Chen, Xiao Luo, Ludwig Schmidt, James Zou",
        "summary": "Benchmarks for competition-style reasoning have advanced evaluation in mathematics and programming, yet physics remains comparatively explored. Most existing physics benchmarks evaluate only final answers, which fail to capture reasoning processes, while recent stepwise methods rely on heuristic LLM-as-judge scoring or restrictive linear assumptions, limiting reliability and diagnostic validity. We introduce PRISM-Physics, a process-level evaluation framework and benchmark for complex physics reasoning problems. Solutions are represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding causal dependencies among intermediate steps to enable fine-grained, interpretable, and theoretically grounded scoring. We prove the optimality of the DAG representation and the corresponding scoring policy. Combining with a fully rule-based method for symbolic formula equivalence matching that we developed, we ensure consistent validation across diverse formulations without heuristic judgments. Results show that our evaluation framework is more aligned with human experts' scoring. Experiments on state-of-the-art LLMs reveal persistent reasoning failures in physics, while step-level scoring offers both diagnostic insight and rich signals for later training. By combining structural rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides a principled foundation for advancing process-level evaluation and guiding the development of models with deeper scientific reasoning capabilities.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.553776",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为**PRISM-Physics的评估框架**，用于精细地衡量大语言模型在复杂物理问题上的**推理过程**。它的本质不是应用LLM去解决一个具体的物理问题，而是创造了一种新的、更科学的**方法论**来评估LLM的推理能力。这种评估方法本身，通过提供更精确的诊断信号和反馈，是**提升LLM基础推理能力的关键一环**。一个更优的评估体系能够更好地指导模型训练，从而增强其通用推理能力。因此，这篇论文的核心是改进对LLM能力的评估方法，属于“改进LLM的基础能力”的范畴，应予以保留。 2.  **第二步：正面指标** 论文高度符合正面指标： *   **核心概念**: 明确以“state-of-the-art LLMs”为研究对象。 *   **能力方向**: 论文标题和摘要反复强调“Physics Reasoning”、“reasoning processes”、“problem-solving”，直接命中“推理”这一核心能力方向。 *   **训练方法**: 虽然论文本身不是提出新的训练方法，但它明确指出其框架提供的“step-level scoring”可以提供“rich signals for later training”，这表明其工作与未来的模型优化紧密相关，是训练范式改进的基础。 3.  **第三步：排除标准** 论文虽然以“Physics”为名，但并未触犯排除标准。 *   **特定应用领域**: 这是本案最关键的判断点。排除标准是“将LLM作为一种工具，应用到某个特定领域去解决该领域的问题”。而这篇论文的目标**不是**解决物理问题，而是**利用物理问题作为载体**，来构建一个通用的、过程级的推理能力评估框架。其贡献是“评估方法”，而非“物理应用”。正如数学推理研究常以数学题为载体，这篇论文以物理题为载体来研究更底层的“科学推理”能力。因此，它不属于被排除的特定应用领域论文。 *   **多模态与视觉、模型可靠性**: 完全不涉及。 4.  **第四步：处理特殊和模糊情况** 本案的特殊情况在于其领域特定性（物理学）。根据标准，如果只是将智能体/工具应用在特定领域，应排除。但本文是提出一种**通用的评估框架**（DAG-based process evaluation），并用物理学作为**展示其有效性的案例**。摘要最后一句明确指出，该框架为“advancing process-level evaluation and guiding the development of models with deeper scientific reasoning capabilities”提供了基础。这表明其最终目标是提升模型的通用“科学推理能力”，而物理学只是一个实验场。因此，这与“用于化学实验自动化的智能体”这类纯应用论文有本质区别，应当保留。 5.  **第五步：最终决策** 综合来看，这篇论文的核心贡献是一个创新的、理论可靠的评估框架，旨在解决如何精确衡量LLM复杂推理过程这一根本性问题。它虽然以物理问题为测试基准，但其方法论具有通用性，其最终目的是为提升LLM的通用推理能力提供更好的诊断工具和训练信号。这完全符合“致力于提高大语言模型本身的『通用推理能力』”这一核心研究目标。 因此，最终判断为 **True**。"
    },
    {
        "index": "#38",
        "title": "RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning",
        "link": "/arxiv/2510.02892",
        "arxiv_id": "2510.02892",
        "authors": "Aleksei Arzhantsev, Otmane Sakhi, Flavian Vasile",
        "summary": "Reinforcement learning (RL) is central to improving reasoning in large language models (LLMs) but typically requires ground-truth rewards. Test-Time Reinforcement Learning (TTRL) removes this need by using majority-vote rewards, but relies on heavy online RL and incurs substantial computational cost. We propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a family of lightweight offline learning alternatives that can target the same regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to maintain a reference model and instead optimizes weighted log-likelihood objectives, enabling stable training with significantly lower memory and compute requirements. Experimental results show that RoiRL trains to 2.5x faster and consistently outperforms TTRL on reasoning benchmarks, establishing a scalable path to self-improving LLMs without labels.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.584020",
        "filter_reason": "这篇论文完全符合筛选要求。以下是根据您提供的筛选标准进行的详细判断： 1.  **第一步：核心判断** - **符合保留标准**。这篇论文的本质是提出一种名为RoiRL的新训练范式。其核心目标是解决现有强化学习方法（如TTRL）在提升大语言模型推理能力时计算成本高昂的问题。论文通过优化加权对数似然目标，实现了更稳定、更高效的离线迭代强化学习训练，从而直接改进LLM的推理能力。这完全属于“改进LLM的基础能力”和“提出新的训练范式”的范畴。 - **不符合排除标准**。论文没有将LLM作为工具应用于任何特定领域，也没有关注模型基础设施或部署优化。 2.  **第二步：正面指标** - 论文命中了多个关键的正面指标： - **核心概念**: 明确提及 \"Large language models\" (LLMs)。 - **能力方向**: 核心聚焦于 \"reasoning\"，并在 \"reasoning benchmarks\" 上进行验证。 - **训练方法**: 提出了一种新的 \"reinforcement learning\" (RL) 方法，即 \"offline iterative RL\"，并旨在实现 \"self-improving\" LLMs。 - 这些指标的高度相关性，强有力地证明了该论文与您的研究范围一致。 3.  **第三步：排除标准** - 论文完全不涉及任何排除标准领域。其内容没有提及多模态、视觉、医疗、化学、机器人等特定应用，也未讨论水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况** - 本论文不直接涉及智能体/工具使用或幻觉/可解释性等特殊情况的讨论。但其核心贡献——一种提升模型推理能力的新训练方法——如果放在这些情境下，也属于“保留”的范畴，因为它旨在从模型内部提升通用能力。 **最终决策**: 综合以上分析，这篇论文的核心贡献是方法论层面的创新，旨在通过一种更高效的离线强化学习框架，系统性地提升LLM的通用推理能力，使其能够自我改进。这与您筛选“致力于提高大语言模型本身通用推理能力”的论文的核心目标高度一致。因此，应予以保留。"
    },
    {
        "index": "#86",
        "title": "Market-Based Data Subset Selection -- Principled Aggregation of Multi-Criteria Example Utility",
        "link": "/arxiv/2510.02456",
        "arxiv_id": "2510.02456",
        "authors": "Ashish Jha, Valentin Leplat, AH Phan",
        "summary": "Selecting a small yet useful subset of training data is hard because signals of example utility (uncertainty, rarity, diversity, etc.) are heterogeneous and typically combined with ad hoc weights. We propose a market-based selector that prices each example via a cost-function prediction market (LMSR), signals act as traders, a single liquidity parameter controls concentration, and topic-wise normalization stabilizes calibration. Token budgets are handled explicitly by a price-per-token rule $\\rho=p/\\ell^{\\gamma}$, with $\\gamma$ exposing an interpretable length bias; a lightweight diversity head improves coverage. We quantify coverage via topic cluster coverage and effective sample size. On the theory side, we show that LMSR implements a maximum-entropy aggregation with exponential weighting and a convex objective, yielding transparent knobs for aggregation strength. Empirically, on GSM8K (60k-token budget) the market with diversity achieves parity with strong single-signal baselines while reducing seed variance and incurring $<\\!0.1$ GPU-hr selection overhead; on AGNews at kept=5-25\\% the market (with light balancing) delivers competitive accuracy with improved balance and stability. The framework unifies multi-signal data curation under fixed compute for prompt-level reasoning and classification.",
        "subjects": "Machine Learning, Artificial Intelligence, Numerical Analysis",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.636730",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“基于市场的数据子集选择”的新方法。这本质上是一种**新的训练数据筛选范式**。它不是将LLM作为工具应用于特定领域，也不是研究模型基础设施。相反，它致力于解决如何更高效、更原则化地构建训练数据集这一基础性问题。高质量的训练数据是提升LLM基础能力的根本，因此，一种能够系统性地优化数据选择的方法，直接关系到LLM通用能力的提升。这符合筛选标准中“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。 2.  **第二步：正面指标** 论文与多个正面指标高度相关： *   **能力方向**: 论文明确在GSM8K（一个数学推理基准数据集）上验证其方法的有效性，并提到其目标是提升“prompt-level reasoning”（提示级推理）。这直接命中了“reasoning (尤其是 math reasoning)”这一核心能力方向。 *   **核心概念**: 虽然标题未直接提及LLM，但其应用场景（GSM8K, AGNews）和目标（prompt-level reasoning）清晰地表明其研究对象是大型语言模型。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： *   它不涉及多模态、视觉等内容。 *   它使用的GSM8K和AGNews是通用领域的基准，而非医疗、化学等特定应用领域。 *   它不研究水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不属于智能体/工具使用或幻觉/可解释性等特殊类别。其核心的模糊点在于：**研究数据选择是否等同于研究模型推理能力？** 我的判断是：**是，且高度相关**。对于大语言模型而言，“吃什么数据”直接决定了“能学会什么能力”。这篇论文提出了一种更科学、更高效的“喂食”方法，其最终目的和验证指标都是为了提升模型在推理等任务上的表现。因此，它不是一篇孤立的数据工程论文，而是一篇以提升模型能力为最终导向的基础方法论研究。它通过优化训练数据的“质”与“量”，从根本上为LLM通用推理能力的提升铺平了道路。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种创新的、原则化的训练数据筛选方法论。该方法论旨在通过更优的数据组合来提升LLM在数学推理等通用任务上的表现。它直接服务于“提高LLM本身通用推理能力”这一核心目标，而非将LLM作为应用工具。因此，这篇论文完全符合筛选标准，应被**保留**。"
    },
    {
        "index": "#129",
        "title": "Safe and Efficient In-Context Learning via Risk Control",
        "link": "/arxiv/2510.02480",
        "arxiv_id": "2510.02480",
        "authors": "Andrea Wynn, Metod Jazbec, Charith Peris, Rinat Khaziev, Anqi Liu, Daniel Khashabi, Eric Nalisnick",
        "summary": "Large language models (LLMs) demonstrate a remarkable ability to learn new tasks from a few in-context examples. However, this flexibility introduces safety concerns: LLMs can be influenced by incorrect or malicious demonstrations -- for example, if an adversary tampers with or injects harmful examples without a human supervisor noticing. This motivates principled designs in which the system itself includes built-in mechanisms to guard against such attacks. We propose a novel approach to limit the degree to which harmful demonstrations can degrade model performance. First, we define a baseline ``safe'' behavior for the model -- the model's performance given no in-context demonstrations (zero-shot). Next, we apply distribution-free risk control (DFRC) to control the extent to which in-context samples can decay performance below zero-shot. We achieve this by leveraging dynamic early exit prediction, ignoring later attention heads that attend the most to the unsafe inputs. Finally, we propose modifications to DFRC that allow it to both control risk for harmful inputs \\textit{and} leverage performance and efficiency gains on helpful inputs. We present both theoretical and empirical results showing that our approach can effectively control risk for harmful in-context demonstrations while simultaneously achieving substantial computational efficiency gains with helpful demonstrations.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.665357",
        "filter_reason": "这篇论文符合我的研究范围，判断过程如下： 1.  **第一步：核心判断** 论文的核心并非将LLM应用于特定领域，而是聚焦于LLM的一项基础且核心的能力——**情境学习**。论文提出的“风险控制”方法，其目的是为了保障这一基础能力在面对恶意或错误输入时的**鲁棒性**和**可靠性**。这属于改进LLM自身内在机制的范畴，而非将其作为应用工具。因此，通过了第一步的核心判断。 2.  **第二步：正面指标** 论文明确包含了核心概念 \"Large language models (LLMs)\"。虽然摘要没有直接使用 \"reasoning\" 这个词，但其讨论的 \"in-context learning\" 是模型进行多步推理和问题解决的关键机制之一。保护这一机制免受干扰，本质上是在为高质量的推理提供一个更坚实的基础。因此，符合正面指标。 3.  **第三步：排除标准** 论文不涉及多模态、特定应用领域或模型基础设施。虽然标题和摘要中提到了 \"Safe\" 和 \"risk control\"，但这触及了第四步的模糊情况，需要更细致的分析。 4.  **第四步：处理特殊和模糊情况** 这是判断的关键。论文的“安全”主题，并非指应用层面的内容审查、安全护栏或防止生成有害信息。它关注的是**模型内在推理过程的可靠性**。论文的核心贡献是提出一种方法，防止模型的推理能力被上下文中的“坏例子”所破坏。这与筛选标准中“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留”的描述完全吻合。一个容易被干扰和误导的模型，其通用推理能力是不可靠的。因此，这项工作通过提升模型的内在鲁棒性，直接服务于“提升通用推理能力”这一核心目标。 5.  **第五步：最终决策** 综合来看，这篇论文的本质是提升LLM核心能力（ICL）的内在鲁棒性。一个鲁棒的推理过程是高质量通用推理的基石。论文提出的方法论旨在确保模型在复杂和潜在的敌对环境中，其推理过程不会被轻易破坏，从而保障了其通用问题解决的底线。因此，这篇论文虽然不直接提出新的推理范式（如CoT），但它为现有推理范式的稳定性和可靠性提供了重要的技术保障，完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”的研究目标。"
    },
    {
        "index": "#141",
        "title": "CWM: An Open-Weights LLM for Research on Code Generation with World Models",
        "link": "/arxiv/2510.02387",
        "arxiv_id": "2510.02387",
        "authors": "FAIR CodeGen team, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, David Zhang, Kunhao Zheng, Jordi Armengol-Estapé, Pedram Bashiri, Maximilian Beck, Pierre Chambon, Abhishek Charnalia, Chris Cummins, Juliette Decugis, Zacharias V. Fisches, François Fleuret, Fabian Gloeckle, Alex Gu, Michael Hassid, Daniel Haziza, Badr Youbi Idrissi, Christian Keller, Rahul Kindi, Hugh Leather, Gallil Maimon, Aram Markosyan, Francisco Massa, Pierre-Emmanuel Mazaré, Vegard Mella, Naila Murray, Keyur Muzumdar, Peter O'Hearn, Matteo Pagliardini, Dmitrii Pedchenko, Tal Remez, Volker Seeker, Marco Selvi, Oren Sultan, Sida Wang, Luca Wehrstedt, Ori Yoran, Lingming Zhang, Taco Cohen, Yossi Adi, Gabriel Synnaeve",
        "summary": "We release Code World Model (CWM), a 32-billion-parameter open-weights LLM, to advance research on code generation with world models. To improve code understanding beyond what can be learned from training on static code alone, we mid-train CWM on a large amount of observation-action trajectories from Python interpreter and agentic Docker environments, and perform extensive multi-task reasoning RL in verifiable coding, math, and multi-turn software engineering environments. With CWM, we provide a strong testbed for researchers to explore the opportunities world modeling affords for improving code generation with reasoning and planning in computational environments. We present first steps of how world models can benefit agentic coding, enable step-by-step simulation of Python code execution, and show early results of how reasoning can benefit from the latter. CWM is a dense, decoder-only LLM trained with a context size of up to 131k tokens. Independent of its world modeling capabilities, CWM offers strong performance on general coding and math tasks: it reaches pass@1 scores of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further research on code world modeling, we release model checkpoints after mid-training, SFT, and RL.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.681685",
        "filter_reason": "这篇论文完全符合你的筛选标准，其核心目标是提升大语言模型本身的通用推理能力。我的判断过程如下： **第一步：核心判断——论文的本质是提升LLM的基础能力。** 这篇论文的本质并非简单地应用LLM解决编程问题，而是提出了一种名为“Code World Model”（CWM）的新范式，通过增强模型的“世界模型”能力来提升其推理和规划能力。具体来说，论文的核心贡献在于： 1.  **新的训练范式**：提出了在静态代码之外，让模型在“观察-行动轨迹”（来自Python解释器和智能体Docker环境）上进行中间训练，这本身就是一种增强模型对动态环境理解和因果推理能力的创新方法。 2.  **强化学习优化**：论文明确提到，在可验证的编程、数学和多轮软件工程环境中进行了“广泛的多任务推理RL”。这直接命中了筛选标准中的“强化学习优化”和“增强逻辑、数学、规划、多步推理等通用能力”。 论文的出发点是“改进超越静态代码学习的代码理解”，并旨在探索“世界模型如何通过推理和规划改进代码生成”，这清晰地表明其研究焦点是**增强模型的基础推理机制**，而非应用。 **第二步：正面指标——论文高度匹配。** 论文摘要中包含了大量高优先级的正面指标： *   **核心概念**: 明确提到“Large language models (LLM)”。 *   **能力方向**: 核心主题是“reasoning”（多次出现）、“planning”，并且在“math”和“coding”任务上进行验证，这两者是衡量逻辑推理能力的核心领域。 *   **训练方法**: 明确使用了“reinforcement learning (RL)”。 *   **新兴范式**: 探讨了“world models”、“agentic Docker environments”和“agentic coding”，这些都是当前提升LLM自主解决问题能力的前沿方向。 **第三步：排除标准——论文未触及。** 该论文没有被任何排除标准命中： *   **非多模态**: 研究对象是纯文本的代码和数学模型。 *   **非特定应用领域**: 尽管以“代码生成为载体”，但其目标是探索通用的“世界模型”、“推理”和“规划”能力，而非针对金融、法律、生物等特定垂直领域。编程和数学在此处被视为衡量和训练通用推理能力的基准，而非应用终点。 *   **非模型可靠性（应用层面）**: 论文不涉及水印、安全等问题。 **第四步：处理特殊和模糊情况——论文属于应保留的情况。** 论文提出的“智能体”和“工具使用”是典型的应保留情况。它不是提出一个用于特定领域（如“化学实验自动化”）的智能体，而是构建一个**通用的智能体框架**，让模型在计算环境中进行交互和学习，其最终目的是服务于“通用推理与规划”这一核心目标。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种结合“世界模型”和“多任务推理强化学习”的新方法，旨在从训练范式和模型架构层面增强LLM的内在推理、规划和模拟能力。这与你的研究目标——“致力于提高大语言模型（LLM）本身的『通用推理能力』”——高度契合。因此，应予以保留。"
    },
    {
        "index": "#8",
        "title": "Reward Model Routing in Alignment",
        "link": "/arxiv/2510.02850",
        "arxiv_id": "2510.02850",
        "authors": "Xinle Wu, Yao Lu",
        "summary": "Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become the standard paradigm for aligning large language models (LLMs). However, most pipelines rely on a single reward model (RM), limiting alignment quality and risking overfitting. Recent work explores RM routing--dynamically selecting an RM from a candidate pool to exploit complementary strengths while maintaining $O(1)$ RM calls--but existing methods suffer from cold-start and insufficient exploration. We propose BayesianRouter, a hybrid routing framework that combines offline RM strengths learning with online Bayesian selection. In the offline stage, a multi-task router is trained on preference data to estimate per-RM reliability. In the online stage, a Bayesian Thompson sampling router performs per-query RM selection, initializing RM-specific weight vectors with offline embeddings as Gaussian priors and adaptively updating their posteriors with online rewards to adapt to the evolving policy distribution. Extensive experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently outperforms individual RMs, RM ensembling, and existing routing methods.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T01:04:27.614679",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“BayesianRouter”的新方法，用于优化大语言模型对齐过程中的奖励模型选择。这并非将LLM作为工具应用于特定领域，而是直接改进LLM的**基础训练范式（RLHF/RLAIF）**。通过更智能地利用多个奖励模型，该方法旨在提升模型的整体对齐质量和性能。这属于“改进LLM的基础能力、提出新的训练范式”的范畴，因此应予以保留。 2.  **第二步：正面指标** 论文与多个正面指标高度相关： *   **核心概念**: 论文明确以大语言模型为研究对象。 *   **能力方向**: 论文在**推理**基准（GSM8K, MMLU）上进行了验证，并证明了其方法的有效性。这直接关联到提升LLM的通用推理能力。 *   **训练方法**: 论文的核心是改进**强化学习（RLHF/RLAIF）**这一关键训练方法，属于优化训练范式的研究。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准： *   它不涉及多模态、视觉等内容。 *   它的研究是通用的，在通用基准上测试，而非聚焦于医疗、化学等特定应用领域。 *   它关注的是训练过程中的奖励模型优化，而非应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需特殊判断。 5.  **第五步：最终决策** 综合来看，这篇论文的本质是**通过改进强化学习对齐过程中的奖励模型机制，来提升大语言模型的整体性能，并明确在数学和常识推理等通用能力上取得了显著进步**。这完全契合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。它提出的是一种方法论层面的创新，旨在增强模型的基础能力，而非解决特定领域的问题。因此，这篇论文应该被保留。"
    },
    {
        "index": "#14",
        "title": "AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models",
        "link": "/arxiv/2510.02669",
        "arxiv_id": "2510.02669",
        "authors": "Bo Ma, Hang Li, ZeHua Hu, XiaoFan Gui, LuYao Liu, Simon Liu",
        "summary": "Multi-agent systems powered by large language models have demonstrated remarkable capabilities across diverse domains, yet existing automated design approaches seek monolithic solutions that fail to adapt resource allocation based on query complexity and domain requirements. This paper introduces AutoMaAS, a self-evolving multi-agent architecture search framework that leverages neural architecture search principles to automatically discover optimal agent configurations through dynamic operator lifecycle management and automated machine learning techniques. Our approach incorporates four key innovations: (1) automatic operator generation, fusion, and elimination based on performance-cost analysis, (2) dynamic cost-aware optimization with real-time parameter adjustment, (3) online feedback integration for continuous architecture refinement, and (4) enhanced interpretability through decision tracing mechanisms. Extensive experiments across six benchmarks demonstrate that AutoMaAS achieves 1.0-7.1\\% performance improvement while reducing inference costs by 3-5\\% compared to state-of-the-art methods. The framework shows superior transferability across datasets and LLM backbones, establishing a new paradigm for automated multi-agent system design in the era of large language models.",
        "subjects": "Artificial Intelligence, Human-Computer Interaction, Information Retrieval",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T01:04:27.622075",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献是提出了一种提升大语言模型通用问题解决能力的新方法论。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心不是将LLM应用于某个特定领域，而是提出了一种名为AutoMaAS的**通用框架**。该框架通过“自我进化”和“架构搜索”的技术，自动地设计和优化由多个LLM智能体组成的系统，以实现更好的性能和效率。这本质上是一种**增强LLM系统通用能力**的方法论，直接关联到提升其问题解决和推理能力。因此，根据核心判断标准，应予以**保留**。 2.  **第二步：正面指标** - **核心概念**: 论文标题和摘要明确聚焦于“Large Language Models”。 - **能力方向**: 论文的目标是通过优化多智能体架构来提升“性能”，这直接关联到LLM的通用“problem-solving”能力。多智能体系统本身就是解决复杂、多步推理任务的前沿范式。 - **训练方法**: 论文标题和摘要中的“Self-Evolving”和“Architecture Search”与筛选标准中的“evolution”和“self-evolve”高度吻合。其“online feedback integration for continuous architecture refinement”机制也与强化学习的思想一脉相承。 - **新兴范式**: “Multi-agent systems”是这篇论文的绝对核心主题，完全符合筛选标准中的“llm-based agents”和“multi-agent systems”。 3.  **第三步：排除标准** - 论文内容完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或应用层面的模型可靠性（如水印、安全）。它是一个纯粹的、通用的系统设计方法论研究。因此，不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的典型范例。AutoMaAS框架本身不绑定任何特定领域，其目标是自动发现最优的智能体配置，这属于提升LLM基础能力的范畴，因此应**保留**。 5.  **第五步：最终决策** - **综合分析**: 论文《AutoMaAS》的核心贡献是提出了一种新颖的、自动化的、自我进化的多智能体架构搜索框架。它旨在通过系统性地优化LLM智能体的组织方式，来提升整个系统在通用任务上的性能和效率。这完全契合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。它不是应用研究，而是关于如何构建更强大的LLM系统的基础方法论研究，属于前沿且高度相关的论文。 因此，最终判断为 **True**。"
    },
    {
        "index": "#22",
        "title": "Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge",
        "link": "/arxiv/2510.02557",
        "arxiv_id": "2510.02557",
        "authors": "Charlie Masters, Advaith Vellanki, Jiangbo Shangguan, Bart Kultys, Jonathan Gilmore, Alastair Moore, Stefano V. Albrecht",
        "summary": "While agentic AI has advanced in automating individual tasks, managing complex multi-agent workflows remains a challenging problem. This paper presents a research vision for autonomous agentic systems that orchestrate collaboration within dynamic human-AI teams. We propose the Autonomous Manager Agent as a core challenge: an agent that decomposes complex goals into task graphs, allocates tasks to human and AI workers, monitors progress, adapts to changing conditions, and maintains transparent stakeholder communication. We formalize workflow management as a Partially Observable Stochastic Game and identify four foundational challenges: (1) compositional reasoning for hierarchical decomposition, (2) multi-objective optimization under shifting preferences, (3) coordination and planning in ad hoc teams, and (4) governance and compliance by design. To advance this agenda, we release MA-Gym, an open-source simulation and evaluation framework for multi-agent workflow orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we find they struggle to jointly optimize for goal completion, constraint adherence, and workflow runtime - underscoring workflow management as a difficult open problem. We conclude with organizational and ethical implications of autonomous management systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T01:04:27.624310",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于定义并推动了一个旨在提升大语言模型高级通用推理能力的新研究方向。我的判断过程如下： 1.  **第一步：核心判断——论文本质是提升LLM基础能力。** 论文的核心不是将LLM应用于某个垂直领域，而是提出了一个名为“自主管理器智能体”的通用框架。这个智能体的核心任务是“将复杂目标分解为任务图”、“协调与规划”、“适应变化条件”等。这些本质上都是高级的、通用的**规划、组合推理和问题解决能力**。论文通过形式化工作流管理问题，并评估GPT-5在该任务上的表现，直接指向了如何提升LLM在复杂、动态环境下的通用推理与决策能力。这完全符合你“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的核心目标。 2.  **第二步：正面指标——高度相关。** 论文摘要中明确包含了多个关键正面指标： *   **核心概念**: \"GPT-5-based Manager Agents\"，直接以LLM为核心。 *   **能力方向**: 明确提出 \"compositional reasoning\" (组合推理), \"coordination and planning\" (协调与规划)，这些都是通用推理能力的核心组成部分。整个工作流管理问题本身就是一种高级的问题解决能力。 *   **新兴范式**: 论文主题是 \"agentic AI\" 和 \"multi-agent workflows\"，正是当前提升LLM能力的前沿范式。 3.  **第三步：排除标准——不适用。** 该论文不涉及任何特定应用领域（如医疗、化学），也没有关注多模态或模型基础设施。它提出的问题具有普适性，不属于任何排除范畴。 4.  **第四步：处理特殊和模糊情况——属于应保留的情况。** *   **智能体/工具使用**: 论文提出的是一种**通用的智能体协作与编排框架**，旨在增强LLM解决通用复杂问题的能力，而非应用于特定领域。这完全符合“应该保留”的条件。 *   **幻觉/可解释性/安全**: 论文中提到的“治理与合规”是作为构建该智能体时需要解决的一个**基础性技术挑战**提出的，这与提升模型内在可靠性和推理质量的目标是一致的，而非应用层面的社会学讨论。 **最终决策**: 这篇论文的本质是探索如何让LLM具备更高级的、面向复杂系统的**通用规划与推理能力**。它通过定义“管理器智能体”这一新挑战，将LLM的推理能力从单任务、静态问题提升到了多任务、动态、多智能体协作的层面。这不仅符合你的研究目标，而且触及了该领域非常前沿和核心的难题。因此，这篇论文应被保留。"
    },
    {
        "index": "#10",
        "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution",
        "link": "/arxiv/2510.04886",
        "arxiv_id": "2510.04886",
        "authors": "Adi Banerjee, Anirudh Nair, Tarik Borogovac",
        "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents a significant challenge in debugging and improving collaborative AI systems. Current approaches to pinpointing agent and step level failures in interaction traces - whether using all-at-once evaluation, step-by-step analysis, or binary search - fall short when analyzing complex patterns, struggling with both accuracy and consistency. We present ECHO (Error attribution through Contextual Hierarchy and Objective consensus analysis), a novel algorithm that combines hierarchical context representation, objective analysis-based evaluation, and consensus voting to improve error attribution accuracy. Our approach leverages a positional-based leveling of contextual understanding while maintaining objective evaluation criteria, ultimately reaching conclusions through a consensus mechanism. Experimental results demonstrate that ECHO outperforms existing methods across various multi-agent interaction scenarios, showing particular strength in cases involving subtle reasoning errors and complex interdependencies. Our findings suggest that leveraging these concepts of structured, hierarchical context representation combined with consensus-based objective decision-making, provides a more robust framework for error attribution in multi-agent systems.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-07T22:03:52.314658",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为ECHO的新算法，用于解决LLM多智能体系统中的“错误归因”问题。其根本目标是“调试和改进协作式AI系统”。这并非将LLM作为工具应用于某个特定领域，而是致力于提升LLM在协作场景下的基础能力。通过精确定位错误发生的智能体和步骤，该研究为系统性提升多智能体系统的整体表现（包括推理和规划能力）提供了方法论支持。这完全符合“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标** 论文明确包含了多个关键的正面指标： *   **核心概念**: \"Large Language Model (LLM) multi-agent systems\" *   **能力方向**: 明确提到在\"subtle reasoning errors\"（微妙的推理错误）和\"complex interdependencies\"（复杂的相互依赖关系）等场景下表现出色，直接关联到`reasoning`和`problem-solving`。 *   **新兴范式**: 论文的主题就是`multi-agent systems`，研究如何让它们更好地协作。 3.  **第三步：排除标准** 该论文完全没有触及任何排除标准。它不涉及多模态、视觉，不聚焦于任何特定应用领域（如医疗、化学），也不是关于模型部署或水印等基础设施或应用层可靠性问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文是“智能体”和“可解释性”特殊情况的完美结合点。 *   **智能体**: 论文研究的是通用的多智能体系统，旨在提升其通用问题解决能力，而非应用于特定领域。这符合“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的保留原则。虽然ECHO本身是一个分析工具，但它服务于改进通用智能体框架这一核心目标。 *   **可解释性**: 错误归因本身就是一种深度的可解释性研究。论文提出了一种新方法来增强模型（或模型系统）的内在可解释性，从而能够发现并修复其推理缺陷，最终提升模型的通用可靠性和推理质量。这完全符合保留标准。 **最终决策**: 该论文的核心贡献是提供了一种强大的方法论（ECHO算法），用于分析和诊断LLM在复杂协作任务中的推理失败。通过提升错误归因的准确性，研究者可以更有效地迭代和优化多智能体系统的设计与协作策略，从而直接推动LLM在通用推理、规划和问题解决能力上的进步。因此，这篇论文是关于提升LLM本身通用推理能力的前沿研究，与你的核心目标高度一致。"
    },
    {
        "index": "#12",
        "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation",
        "link": "/arxiv/2510.04851",
        "arxiv_id": "2510.04851",
        "authors": "Dongge Han, Camille Couturier, Daniel Madrigal Diaz, Xuchao Zhang, Victor Rühle, Saravan Rajmohan",
        "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent large language model (LLM) systems in workflow automation. LEGOMem decomposes past task trajectories into reusable memory units and flexibly allocates them across orchestrators and task agents to support planning and execution. To explore the design space of memory in multi-agent systems, we use LEGOMem as a lens and conduct a systematic study of procedural memory in multi-agent systems, examining where memory should be placed, how it should be retrieved, and which agents benefit most. Experiments on the OfficeBench benchmark show that orchestrator memory is critical for effective task decomposition and delegation, while fine-grained agent memory improves execution accuracy. We find that even teams composed of smaller language models can benefit substantially from procedural memory, narrowing the performance gap with stronger agents by leveraging prior execution traces for more accurate planning and tool use. These results position LEGOMem as both a practical framework for memory-augmented agent systems and a research tool for understanding memory design in multi-agent workflow automation.",
        "subjects": "Artificial Intelligence, Machine Learning, Multiagent Systems",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-07T22:03:52.315225",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： **第一步：核心判断** 论文的核心贡献是提出了一种名为“LEGOMem”的模块化程序性记忆框架。这个框架并非将LLM应用于某个特定领域（如医疗或化学），而是致力于改进多智能体LLM系统本身的基础能力。它通过为智能体系统引入一种新的记忆机制，来增强其“规划”和“执行”能力。规划和执行是通用推理能力的关键组成部分。因此，这篇论文的本质是改进LLM的基础能力，符合保留标准。 **第二步：正面指标** 论文高度匹配多个正面指标： - **核心概念**: 明确聚焦于“multi-agent large language model (LLM) systems”。 - **能力方向**: 核心研究内容是“planning and execution”（规划与执行），这直接对应了“planning”和“problem-solving”这两个关键能力方向。 - **新兴范式**: 论文的研究对象是“multi-agent systems”，并探讨了“tool use”，这些都是提升LLM通用能力的前沿范式。 **第三步：排除标准** 论文未触及任何排除标准： - **多模态与视觉**: 论文完全不涉及视觉或多模态内容。 - **特定应用领域**: 虽然论文在“workflow automation”（工作流自动化）的背景下进行实验，但其提出的LEGOMem框架是一个通用的方法论，而非针对特定领域（如金融、法律）的解决方案。实验基准OfficeBench也属于通用办公任务，而非专业领域。 - **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出了一个“通用的智能体协作框架”（通过程序性记忆来增强协作），其目标是提升智能体在通用任务中的“planning and tool use”能力。这完全符合“保留”的条件，即提出一种通用的方法来增强LLM的通用问题解决能力，而不是将智能体应用在特定领域。 **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种新的方法论（程序性记忆框架），用以增强多智能体LLM系统的通用规划和执行能力。这直接触及了“大语言模型通用推理能力”这一核心目标，通过改进模型的基础架构和协作方式来提升其内在的推理和问题解决水平。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#3",
        "title": "Slm-mux: Orchestrating small language models for reasoning",
        "link": "/arxiv/2510.05077",
        "arxiv_id": "2510.05077",
        "authors": "Chenyu Wang, Zishen Wan, Hao Kang, Emma Chen, Zhiqiang Xie, Tushar Krishna, Vijay Janapa Reddi, Yilun Du",
        "summary": "With the rapid development of language models, the number of small language models (SLMs) has grown significantly. Although they do not achieve state-of-the-art accuracy, they are more efficient and often excel at specific tasks. This raises a natural question: can multiple SLMs be orchestrated into a system where each contributes effectively, achieving higher accuracy than any individual model? Existing orchestration methods have primarily targeted frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To address this gap, we propose a three-stage approach for orchestrating SLMs. First, we introduce SLM-MUX, a multi-model architecture that effectively coordinates multiple SLMs. Building on this, we develop two optimization strategies: (i) a model selection search that identifies the most complementary SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our approach delivers strong results: Compared to existing orchestration methods, our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0% on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and GSM8K, and matches its performance on MATH. We further provide theoretical analyses to substantiate the advantages of our method. In summary, we demonstrate that SLMs can be effectively orchestrated into more accurate and efficient systems through the proposed approach.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.580365",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是基于筛选标准的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“SLM-MUX”的新方法论/架构，用于编排多个小型语言模型（SLM），以提升整个系统的推理能力。这并非将LLM作为工具应用于特定领域，而是直接致力于改进模型（或模型系统）的基础推理能力。这种“编排”本身就是一种新的训练/推理范式，旨在通过模型间的协作与互补，实现超越单个模型的逻辑、数学和多步推理表现。因此，论文的本质完全符合“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的要求。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 论文研究对象是“small language models (SLMs)”，属于大语言模型范畴。 *   **能力方向**: 论文标题和摘要反复强调“reasoning”，并在MATH、GPQA、GSM8K这三个经典的数学和逻辑推理基准上进行了验证，直接对应“reasoning (尤其是 math reasoning, logical reasoning)”。 *   **新兴范式**: 论文提出的“Orchestrating small language models”本质上是一种“multi-agent systems”或“llm-based agents”的协作框架，旨在通过组合多个智能体（模型）来解决复杂问题，这完全符合筛选标准中的新兴范式。 3.  **第三步：排除标准** 论文完全没有触及任何排除标准。它不涉及多模态、视觉，不针对医疗、化学等特定应用领域，也不讨论水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文是“智能体/工具使用”这一特殊情况的完美范例。它提出的是一种**通用的智能体编排框架**，其目标是增强模型的**通用问题解决能力**（在通用推理基准上验证），而非应用于特定领域。因此，根据筛选标准“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”，这篇论文应被保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献是一种新颖的、通用的方法论，通过多模型编排的范式，直接且显著地提升了语言模型的通用推理能力。它精准地命中了您研究课题的核心目标，即“致力于提高大语言模型（LLM）本身的『通用推理能力』”，因此是一篇高度相关且应被筛选保留的前沿论文。"
    },
    {
        "index": "#1",
        "title": "Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models",
        "link": "/arxiv/2510.05090",
        "arxiv_id": "2510.05090",
        "authors": "Runchu Tian, Junxia Cui, Xueqiang Xu, Feng Yao, Jingbo Shang",
        "summary": "Diffusion large language models (dLLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering advantages such as accelerated parallel decoding and bidirectional context modeling. However, the vanilla decoding strategy in discrete dLLMs suffers from a critical limitation: once a token is accepted, it can no longer be revised in subsequent steps. As a result, early mistakes persist across iterations, harming both intermediate predictions and final output quality. To address this issue, we propose Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding strategy that leverages cross-validation among predicted tokens. Unlike existing methods that follow a single progressive unmasking procedure, Tolerator introduces a two-stage process: (i) sequence fill-up and (ii) iterative refinement by remasking and decoding a subset of tokens while treating the remaining as context. This design enables previously accepted tokens to be reconsidered and corrected when necessary, leading to more reliable diffusion decoding outputs. We evaluate Tolerator on five standard benchmarks covering language understanding, code generation, and mathematics. Experiments show that our method achieves consistent improvements over the baselines under the same computational budget. These findings suggest that decoding algorithms are crucial to realizing the full potential of diffusion large language models. Code and data are publicly available.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.579692",
        "filter_reason": "这篇论文完全符合筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心贡献是提出了一种名为“Tolerator”的**免训练解码策略**。它旨在解决扩散大语言模型（dLLM）的一个根本性缺陷：一旦某个token被生成，就无法在后续步骤中修正。这个缺陷直接损害了模型输出的质量，尤其是在需要多步连贯推理的任务中。通过允许模型在生成过程中“回溯”并修正早期错误，该论文直接改进了LLM的**基础生成机制**，这属于提升模型内在能力的范畴，而非将其作为工具应用于特定领域。 2.  **第二步：正面指标——论文包含高度相关的主题。** -   **核心概念**: 论文明确研究“Diffusion large language models (dLLMs)”，属于LLM的范畴。 -   **能力方向**: 论文的评估基准包括“**mathematics**”（数学）。数学推理是通用推理能力的核心体现。通过修正早期错误，该方法直接提升了模型在多步推理任务中的表现，因为推理链中的一个错误可以被后续步骤纠正，从而得到更准确的最终答案。这与“reasoning”和“problem-solving”高度相关。 3.  **第三步：排除标准——论文未聚焦于排除领域。** -   论文不涉及多模态、视觉、特定应用领域（如医疗、化学），也不关注水印、安全等应用层面的可靠性问题。它的焦点纯粹在于**提升模型解码输出的内在质量**。 4.  **第四步：处理特殊和模糊情况。** -   该论文可以被看作是对“幻觉”或“事实错误”的一种基础性解决方案。早期错误在后续生成中持续存在，是导致模型输出不合逻辑或与事实不符（即幻觉）的重要原因之一。Tolerator通过一种通用的、与领域无关的解码后处理机制来缓解这个问题，从而提升了模型的**通用可靠性和推理质量**。这完全符合“保留”标准。 **最终决策**: 该论文的本质是提出一种**通用的解码算法优化**，用以解决扩散大语言模型在生成过程中的一个核心缺陷。这种优化使得模型能够进行自我修正，从而在数学、代码生成等需要严密逻辑和多步推理的任务上取得更好的表现。它直接增强了LLM的**内在推理鲁棒性和准确性**，是一种方法论层面的创新，而非特定领域的应用。因此，这篇论文与“提高大语言模型本身的通用推理能力”这一核心目标高度契合。"
    },
    {
        "index": "#4",
        "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs",
        "link": "/arxiv/2510.05069",
        "arxiv_id": "2510.05069",
        "authors": "Dachuan Shi, Abedelkadir Asi, Keying Li, Xiangchi Yuan, Leyan Pan, Wenke Lee, Wen Xiao",
        "summary": "Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.580684",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“SwiReasoning”的**免训练推理框架**。其本质是研究如何改进大语言模型（LLM）的推理过程本身，通过在显式推理和潜在推理之间动态切换，来解决现有推理方法（如纯潜在推理）在准确性和效率上的问题。这直接触及了“提高LLM本身的通用推理能力”这一核心目标，属于改进LLM基础能力和提出新推理范式的研究，而非将LLM应用于特定领域。 2.  **第二步：正面指标** 论文高度契合多个正面指标： *   **核心概念**: 论文研究对象明确为 \"large language models (LLMs)\"。 *   **能力方向**: 论文的核心主题是 \"reasoning\"，并在 \"mathematics and STEM benchmarks\" 上进行验证，这直接对应了数学推理和问题解决能力。 *   **新兴范式**: \"SwiReasoning\" 本身就是一种新兴的推理范式，它结合了显式和潜在推理，是对现有思维链等方法论的深化和创新。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准领域： *   它没有涉及视觉或多模态内容。 *   它的应用场景是通用的数学和STEM问题，而非医疗、化学、机器人等特定领域。 *   它的研究焦点是推理效率和准确性，而非水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需进行特殊判断。 5.  **第五步：最终决策** 综合分析，这篇论文的标题、摘要和核心贡献都清晰地表明，它是一项致力于提升LLM内在推理能力的前沿研究。它提出了一种创新的、通用的推理框架，旨在解决现有推理方法的根本性挑战（如概率质量扩散和过度思考），并在通用推理基准上验证了其有效性。这与你的研究目标“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”完美匹配。因此，最终判断为 **True**。"
    },
    {
        "index": "#23",
        "title": "The Argument is the Explanation: Structured Argumentation for Trust in Agents",
        "link": "/arxiv/2510.03442",
        "arxiv_id": "2510.03442",
        "authors": "Ege Cakar, Per Ola Kristensson",
        "summary": "Humans are black boxes -- we cannot observe their neural processes, yet society functions by evaluating verifiable arguments. AI explainability should follow this principle: stakeholders need verifiable reasoning chains, not mechanistic transparency. We propose using structured argumentation to provide a level of explanation and verification neither interpretability nor LLM-generated explanation is able to offer. Our pipeline achieves state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7 points above prior work) and $0.81$ macro F1, $\\sim$0.07 above previous published results with comparable data setups, for Argumentative MicroTexts relation classification, converting LLM text into argument graphs and enabling verification at each inferential step. We demonstrate this idea on multi-agent risk assessment using the Structured What-If Technique, where specialized agents collaborate transparently to carry out risk assessment otherwise achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we capture support/attack relationships, thereby enabling automatic hallucination detection via fact nodes attacking arguments. We also provide a verification mechanism that enables iterative refinement through test-time feedback without retraining. For easy deployment, we provide a Docker container for the fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python package on GitHub.",
        "subjects": "Machine Learning, Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-03",
        "category": "cs.MA",
        "crawl_time": "2025-10-07T22:03:52.318171",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心并非将LLM应用于某个特定领域，而是提出了一种名为“结构化论证”的新方法论。该方法论旨在将LLM生成的文本转换为可验证的论证图，从而增强其推理过程的透明度和可靠性。这直接触及了LLM的“通用推理能力”核心，即如何让模型的推理过程更严谨、更可信、更可验证。它不是在解决一个化学或医疗问题，而是在解决LLM推理本身的一个根本性问题。因此，根据第一步标准，应予以**保留**。 2.  **第二步：正面指标** 论文与多个正面指标高度相关： *   **核心概念**: 明确以LLM为基础。 *   **能力方向**: 核心贡献在于提升 `reasoning` 能力，特别是通过“verifiable reasoning chains”（可验证的推理链）和“verification at each inferential step”（在每一步推理中进行验证）来增强逻辑推理。 *   **新兴范式**: 论文提出了一个 `multi-agent systems` 框架，其中多个智能体通过结构化论证进行协作。这符合“智能体协作框架”的范畴。 3.  **第三步：排除标准** 论文不涉及任何排除标准： *   它不涉及多模态、视觉等内容。 *   虽然以“multi-agent risk assessment”（多智能体风险评估）为例，但这只是为了**演示**其通用框架的有效性，论文的焦点是“结构化论证”这一通用方法，而非风险评估这个特定领域本身。 *   它虽然涉及“Trust”（信任）和“hallucination detection”（幻觉检测），但并非从社会学或应用政策层面讨论，而是提出了一种新的技术机制。 4.  **第四步：处理特殊和模糊情况** 这篇论文是处理特殊情况的绝佳范例： *   **智能体/工具使用**: 论文提出的多智能体协作框架，其核心是“structured argumentation”，这是一种通用的、旨在增强推理透明度和协作质量的框架。它完全符合“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的保留条件。 *   **幻觉/可解释性**: 论文的核心贡献之一就是通过“fact nodes attacking arguments”（事实节点攻击论点）来实现“automatic hallucination detection”（自动幻觉检测）。这是一种新颖的、从模型内部推理结构出发来提升可靠性的技术方法，直接提升了模型的通用推理质量。这完全符合“提出一种新方法来减少幻觉，从而提升模型的通用可靠性和推理质量”的保留条件。 5.  **第五步：最终决策** 综合分析，该论文的本质是提出一种创新的、通用的方法论（结构化论证），用于增强LLM推理过程的可验证性、可解释性和鲁棒性（通过自动检测幻觉）。它引入了新的推理范式和优化机制（测试时反馈迭代），直接服务于提升LLM“通用推理能力”这一核心目标。因此，这篇论文是高度相关且应被筛选入内的前沿研究。"
    },
    {
        "index": "#27",
        "title": "Multi-Agent Tool-Integrated Policy Optimization",
        "link": "/arxiv/2510.04678",
        "arxiv_id": "2510.04678",
        "authors": "Zhanfeng Mo, Xingxuan Li, Yuntao Chen, Lidong Bing",
        "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge-intensive and complex reasoning tasks. Existing implementations typically rely on a single agent, but they suffer from limited context length and noisy tool responses. A natural solution is to adopt a multi-agent framework with planner- and worker-agents to manage context. However, no existing methods support effective reinforcement learning post-training of tool-integrated multi-agent frameworks. To address this gap, we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which enables distinct roles (planner and worker) to be trained within a single LLM instance using role-specific prompts via reinforcement learning. MATPO is derived from a principled credit assignment mechanism across planner and worker rollouts. This design eliminates the need to deploy multiple LLMs, which would be memory-intensive, while preserving the benefits of specialization. Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently outperforms single-agent baselines by an average of 18.38% relative improvement in performance and exhibits greater robustness to noisy tool outputs. Our findings highlight the effectiveness of unifying multiple agent roles within a single LLM and provide practical insights for stable and efficient multi-agent RL training.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.592670",
        "filter_reason": "这篇论文完全符合筛选要求，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为MATPO（Multi-Agent Tool-Integrated Policy Optimization）的新训练范式。其本质并非将LLM应用于某个特定领域，而是致力于解决LLM在执行复杂推理任务时遇到的一个基础性挑战：如何通过强化学习（RL）来有效训练一个集成了多智能体协作和工具使用能力的LLM。这直接关系到提升LLM的通用推理、规划和问题解决能力，属于改进LLM基础能力的范畴，因此符合保留标准。 2.  **第二步：正面指标** 论文高度匹配所有正面指标： *   **核心概念**: 明确以 \"Large language models (LLMs)\" 为研究对象。 *   **能力方向**: 聚焦于 \"complex reasoning tasks\" 和 \"planning\"。 *   **训练方法**: 核心贡献是 \"reinforcement learning\" 和 \"Policy Optimization\"。 *   **新兴范式**: 论文主题是 \"multi-agent framework\" 和 \"tool-integrated\"。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准领域： *   它不涉及多模态、视觉或特定应用领域（如医疗、化学等）。 *   实验所用的数据集（GAIA-text, WebWalkerQA, FRAMES）是通用的复杂推理基准，而非领域特定数据。 *   论文虽然提到了对\"noisy tool outputs\"的鲁棒性，但这是通过改进训练方法来提升模型内在的推理质量，而非讨论应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 论文恰好是“智能体/工具使用”这一特殊情况的典型正面案例。它提出的是一种**通用的**多智能体协作与工具使用框架（MATPO），旨在增强LLM在知识密集型和复杂推理任务上的**通用问题解决能力**，而不是将其应用于某个特定垂直领域。因此，根据筛选标准，应当保留。 **总结**: 该论文的核心贡献是提出了一种创新的强化学习方法（MATPO），用于在单个LLM内部训练出具备规划者和工作者角色的多智能体协作能力，以提升其在复杂推理任务中的表现。这是一种方法论层面的创新，直接服务于“提高大语言模型本身的通用推理能力”这一核心目标。因此，该论文是高度相关且应被筛选出的前沿研究。"
    },
    {
        "index": "#40",
        "title": "Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners",
        "link": "/arxiv/2510.04454",
        "arxiv_id": "2510.04454",
        "authors": "Xiangchi Yuan, Xiang Chen, Tong Yu, Dachuan Shi, Can Jin, Wenke Lee, Saayan Mitra",
        "summary": "Large Language Models (LLMs) show strong reasoning abilities, often amplified by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although RL algorithms can substantially improve reasoning, they struggle to expand reasoning boundaries because they learn from their own reasoning trajectories rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers complementary benefits but typically requires large-scale data and risks overfitting. Recent attempts to combine SFT and RL face three main challenges: data inefficiency, algorithm-specific designs, and catastrophic forgetting. We propose a plug-and-play framework that dynamically integrates SFT into RL by selecting challenging examples for SFT. This approach reduces SFT data requirements and remains agnostic to the choice of RL or SFT algorithm. To mitigate catastrophic forgetting of RL-acquired skills during SFT, we select high-entropy tokens for loss calculation and freeze parameters identified as critical for RL. Our method achieves state-of-the-art (SoTA) reasoning performance using only 1.5% of the SFT data and 20.4% of the RL data used by prior SoTA, providing an efficient and plug-and-play solution for combining SFT and RL in reasoning post-training.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.601763",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是基于筛选标准的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种新的训练框架，该框架通过动态整合监督微调（SFT）和强化学习（RL）来提升大语言模型的推理能力。其本质是改进LLM的基础训练范式，以解决现有方法（如单独使用RL或SFT+RL）在提升推理能力时遇到的“灾难性遗忘”等问题。这直接对应了筛选标准中“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。论文的目标是创造“更强的推理器”，而非将LLM应用于特定领域，因此应予以**保留**。 2.  **第二步：正面指标** 论文摘要中包含了多个关键的正面指标： *   **核心概念**: 明确提及“Large Language Models (LLMs)”。 *   **能力方向**: 论文的主题就是提升“reasoning abilities”，并旨在实现“state-of-the-art (SoTA) reasoning performance”。 *   **训练方法**: 论文的核心是关于如何结合“Supervised fine-tuning (SFT)”和“reinforcement learning (RL)”，这正是筛选标准中强调的关键训练方法。 这些指标的高度匹配，进一步确认了论文的相关性。 3.  **第三步：排除标准** 论文的研究焦点完全避开了所有排除标准： *   它不涉及任何多模态或视觉内容。 *   它的研究方法是通用的，没有限定在医疗、化学、生物等任何特定应用领域。 *   它讨论的“灾难性遗忘”是训练过程中的技术挑战，而非应用层面的水印、安全或社会学研究。 4.  **第四步：处理特殊和模糊情况** 论文处理的“灾难性遗忘”问题可以被视为一个特殊情况。论文提出的方法（选择高熵token计算损失、冻结关键参数）是为了防止模型在SFT阶段忘记RL阶段学到的推理技能。这直接提升了模型内在的推理质量和可靠性，属于“提出一种新方法来……提升模型的通用可靠性和推理质量”，因此符合**保留**条件。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种创新的、即插即用的训练框架，旨在更高效、更稳定地提升大语言模型的基础推理能力。它完全符合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标，是一篇高质量的前沿研究论文。因此，最终判断为 **True**。"
    },
    {
        "index": "#36",
        "title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization",
        "link": "/arxiv/2510.04506",
        "arxiv_id": "2510.04506",
        "authors": "Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han",
        "summary": "Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.600499",
        "filter_reason": "这篇论文完全符合筛选标准。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为GRACE的新训练范式。它没有将LLM用作特定领域的工具，而是直接改进LLM本身的能力。具体来说，它改变了传统的对比学习方式，将对比信号作为“奖励”，通过策略梯度（一种强化学习方法）来训练LLM生成“rationales”（基本原理/解释）。这个过程迫使模型不仅要产生正确的表示，还要生成一个可解释的、结构化的自然语言解释来支撑其语义理解。这直接增强了模型的内在推理过程，使其从“黑盒”变为“可解释的智能体”。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理等通用能力”的保留标准。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 明确以“Large Language Models (LLMs)”为研究对象。 *   **能力方向**: 论文的核心是让模型生成“rationales”和“reasoning process”，这直接指向了“reasoning”（推理）能力。虽然不是数学或逻辑推理，但生成语义解释是一种基础的通用推理。 *   **训练方法**: 明确使用了“policy gradient optimization”（策略梯度优化），这是“reinforcement learning (RL)”的核心技术。 *   **新兴范式**: 论文将LLM塑造为一个“interpretable agent”（可解释的智能体），这与“llm-based agents”的理念相符。 3.  **第三步：排除标准** 论文完全没有触及任何排除标准： *   它不涉及多模态、视觉或机器人控制。 *   它是一个通用方法，在MTEB通用基准上测试，而非应用于医疗、化学等特定领域。 *   它的核心不是水印、安全或应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** *   **智能体/可解释性**: 论文提出的“可解释智能体”和生成“rationales”的方法，是一个典型的增强模型内在可解释性的案例。根据标准，“如果论文提出一种新方法来...增强模型内在的可解释性...从而提升模型的通用可靠性和推理质量，应该保留。” GRACE正是通过显式化推理过程，提升了模型推理的透明度和潜在的质量，因此应该保留。 5.  **第五步：最终决策** 综合分析，这篇论文的本质是提出一种创新的、基于强化学习的训练方法，通过引导LLM生成可解释的推理文本（rationales）来同时提升其表示学习能力和内在推理透明度。它直接作用于LLM的核心能力，而非将其作为应用工具。因此，这篇论文与研究课题“大语言模型通用推理能力”高度相关，应当保留。"
    },
    {
        "index": "#46",
        "title": "Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards",
        "link": "/arxiv/2510.04392",
        "arxiv_id": "2510.04392",
        "authors": "Faisal Hamman, Chenyang Zhu, Anoop Kumar, Xujun Peng, Sanghamitra Dutta, Daben Liu, Alfy Samuel",
        "summary": "RAG systems are increasingly deployed in high-stakes domains where users expect outputs to be consistent across semantically equivalent queries. However, existing systems often exhibit significant inconsistencies due to variability in both the retriever and generator (LLM), undermining trust and reliability. In this work, we focus on information consistency, i.e., the requirement that outputs convey the same core content across semantically equivalent inputs. We introduce a principled evaluation framework that decomposes RAG consistency into retriever-level, generator-level, and end-to-end components, helping identify inconsistency sources. To improve consistency, we propose Paraphrased Set Group Relative Policy Optimization (PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased set to assign group similarity rewards. We leverage PS-GRPO to achieve Information Consistent RAG (Con-RAG), training the generator to produce consistent outputs across paraphrased queries and remain robust to retrieval-induced variability. Because exact reward computation over paraphrase sets is computationally expensive, we also introduce a scalable approximation method that retains effectiveness while enabling efficient, large-scale training. Empirical evaluations across short-form, multi-hop, and long-form QA benchmarks demonstrate that Con-RAG significantly improves both consistency and accuracy over strong baselines, even in the absence of explicit ground-truth supervision. Our work provides practical solutions for evaluating and building reliable RAG systems for safety-critical deployments.",
        "subjects": "Computation and Language, Artificial Intelligence, Computers and Society, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.603595",
        "filter_reason": "我的判断过程如下，严格遵循您提供的筛选标准： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为`PS-GRPO`的强化学习新方法，用于训练和微调RAG系统中的生成器（即LLM）。其目标是提升LLM在面对语义等价查询和检索器变动时的输出一致性。这并非将LLM作为工具应用于特定领域，而是直接作用于LLM本身，提出一种新的训练范式来改进其基础能力（输出的一致性和可靠性）。因此，这篇论文的本质符合“改进LLM的基础能力、提出新的训练范式”的保留标准。 2.  **第二步：正面指标** 论文明确包含了多个高优先级的正面指标： -   **核心概念**: 论文的研究对象是RAG系统中的生成器，即`Large language models, LLMs`。 -   **能力方向**: 论文在多跳QA等基准上测试，这涉及到`problem-solving`。虽然核心是`consistency`，但作者声称该方法也提升了`accuracy`，而准确性是有效推理的直接结果。 -   **训练方法**: 论文的核心是提出一种新的`reinforcement learning (RL)`方法，即`PS-GRPO`。 -   **新兴范式**: RAG（Retrieval-Augmented Generation）本身就是一种重要的`tool use`范式。这篇论文研究的是如何让LLM更好地使用这种工具，并保持稳定。 3.  **第三步：排除标准** 论文没有触及任何一项主要的排除标准： -   它不涉及`多模态与视觉`。 -   它的方法是通用的，并非针对`医疗、化学`等`特定应用领域`，尽管其成果可应用于这些领域。 -   它关注的是模型内在的`consistency`，而非应用层面的`Watermarking, Safety, Security`。 4.  **第四步：处理特殊和模糊情况** -   **智能体/工具使用**: 论文研究的是如何让LLM在通用的RAG（工具使用）框架下表现得更稳定、更可靠。这是一种对通用工具使用方法的改进，而非将其应用于特定领域（如“化学实验自动化”），因此符合保留条件。 -   **幻觉/可解释性/安全**: 论文的核心是提升`consistency`。不一致性是模型幻觉或不可靠的重要表现之一。通过提出一种新的训练方法来增强模型内在的可靠性（一致性），从而提升其输出质量和推理的稳健性，这完全符合“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留”的原则。 5.  **第五步：最终决策** 综合以上分析，这篇论文虽然标题和摘要中未直接使用“推理”一词，但其工作实质上是通过创新的强化学习训练范式，解决了LLM在通用问题解决场景（RAG）下的一个核心瓶颈——输出一致性。一个高度不一致的模型无法进行可靠的推理。因此，提升一致性是增强其通用推理能力的重要一环。该论文的核心贡献是方法论层面的创新，直接作用于LLM本身，旨在提升其基础能力和可靠性，完全符合您“提高大语言模型（LLM）本身的『通用推理能力』”的研究目标。"
    },
    {
        "index": "#48",
        "title": "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time",
        "link": "/arxiv/2510.04340",
        "arxiv_id": "2510.04340",
        "authors": "Daniel Tan, Anders Woodruff, Niels Warncke, Arun Jose, Maxime Riché, David Demitri Africa, Mia Taylor",
        "summary": "Language model finetuning often results in learning undesirable traits in combination with desired ones. To address this, we propose inoculation prompting: modifying finetuning data by prepending a short system-prompt instruction that deliberately elicits the undesirable trait. At test time, we evaluate without the instruction; inoculated models have much lower expression of the trait than models trained with unmodified training data. Inoculation is selective: in a toy setting where assistant responses are always in Spanish and ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'') teaches the model to capitalize responses while still responding in English. We find that inoculation is also effective across several additional settings: reducing emergent misalignment (EM) from task-specific finetuning, defending against backdoor injections, and mitigating the transmission of traits via subliminal learning. Follow-up analysis suggests a mechanism: making a trait less surprising via inoculation reduces optimization pressure to globally update the model, thereby reducing the degree of generalization. Our analysis relates to prior work on EM: inoculation explains prior findings that educational contexts mitigate EM from insecure code. Beyond demonstrating a simple and effective technique for selective learning, our results contribute to a better conceptual understanding of how and why language models generalize.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.604210",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。 我的判断过程如下： 1.  **第一步：核心判断** 这篇论文的本质是提出了一种全新的训练范式，即“inoculation prompting”，用于在微调阶段抑制模型学到不希望其具备的特质。这直接命中了筛选标准第一步中“提出新的训练范式、增强其...通用能力”的核心要求。它研究的是如何从训练机制层面控制模型的学习和泛化行为，属于对LLM基础能力的改进，而不是将LLM作为工具应用于特定领域。 2.  **第二步：正面指标** 论文明确以“Language model”为核心对象，其提出的机制关乎模型如何“generalize”（泛化）。虽然摘要没有直接出现“reasoning”一词，但其研究的可控泛化能力是高质量推理的根本前提。一个无法在微调中保持行为一致性的模型，其推理能力的可靠性也无从谈起。因此，该研究与通用问题解决能力高度相关。 3.  **第三步：排除标准** 论文不涉及多模态、视觉或任何特定应用领域（如医疗、化学等）。论文虽然提到了“misalignment”和“backdoor”，看似与“Safety”和“Security”相关，但其研究视角和贡献在于**训练方法**本身，旨在从根源上理解和控制这些现象，而非开发应用层面的安全工具或水印技术。因此，它不应被归入“模型可靠性（应用层面）”的排除类别。 4.  **第四步：处理特殊和模糊情况** 这篇论文的情况与筛选标准第四条中关于“幻觉/可解释性/安全”的描述非常吻合。它提出了一种新方法来抑制模型的不良特质（可以视为一种广义的错位或后门），从而提升模型的“通用可靠性”。论文的结论也明确指出，这项工作“为语言模型如何及为何泛化提供了更好的概念性理解”，这正是一种对模型内在机制的探索，其成果将直接有助于构建更可靠、推理能力更强的LLM。 **核心依据总结：** 论文的核心贡献是**一种新的训练方法论**，旨在解决LLM在微调过程中的**可控泛化**这一根本性问题。它不是应用研究，而是对模型内在学习机制的深入探索。提升模型的可靠性和可控性，是增强其通用推理能力不可或缺的一环。因此，这篇论文完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。"
    },
    {
        "index": "#50",
        "title": "Read the Scene, Not the Script: Outcome-Aware Safety for LLMs",
        "link": "/arxiv/2510.04320",
        "arxiv_id": "2510.04320",
        "authors": "Rui Wu, Yihao Quan, Zeru Shi, Zhenting Wang, Yanshu Li, Ruixiang Tang",
        "summary": "Safety-aligned Large Language Models (LLMs) still show two dominant failure modes: they are easily jailbroken, or they over-refuse harmless inputs that contain sensitive surface signals. We trace both to a common cause: current models reason weakly about links between actions and outcomes and over-rely on surface-form signals, lexical or stylistic cues that do not encode consequences. We define this failure mode as Consequence-blindness. To study consequence-blindness, we build a benchmark named CB-Bench covering four risk scenarios that vary whether semantic risk aligns with outcome risk, enabling evaluation under both matched and mismatched conditions which are often ignored by existing safety benchmarks. Mainstream models consistently fail to separate these risks and exhibit consequence-blindness, indicating that consequence-blindness is widespread and systematic. To mitigate consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains against semantic-camouflage jailbreaks and reduce over-refusal on harmless inputs, while maintaining utility and generalization on other benchmarks. These results clarify the limits of current alignment, establish consequence-aware reasoning as a core alignment goal and provide a more practical and reproducible evaluation path.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.609907",
        "filter_reason": "这篇论文的核心贡献在于识别并解决了一种影响LLM通用推理能力的根本性缺陷，因此符合筛选要求。 1.  **核心判断 (第一步):** 论文的本质是改进LLM的基础能力。虽然论文的标题和问题背景围绕“安全”展开，但其核心论点是：当前LLM的安全失败（如被越狱或过度拒绝）根源于一种更深层次的推理缺陷——“后果盲区”。论文明确指出，模型“reason weakly about links between actions and outcomes”（对行动与结果之间的联系推理能力弱）。这直接触及了LLM的通用推理能力，特别是因果推理和后果评估能力。论文提出的解决方案——CS-Chain-4k数据集和相应的微调方法——是一种新的训练范式，旨在增强模型的“后果感知推理”能力。这完全符合“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的标准。 2.  **正面指标 (第二步):** 论文高度相关。 *   **核心概念:** 论文研究对象是LLMs。 *   **能力方向:** 论文的核心是“reasoning”，具体来说是“consequence-reasoning”（后果推理），这是一种高级的逻辑和规划能力。 *   **训练方法:** 论文提出了一种新的数据集和微调方法，属于训练范式的创新。 3.  **排除标准 (第三步):** 论文不属于被排除的类别。 *   它不涉及多模态、视觉或特定应用领域。 *   虽然主题是“Safety”，但它并非应用层面的安全研究（如水印或特定内容的过滤），而是深入到模型内部的推理机制。 4.  **处理特殊和模糊情况 (第四步):** 本论文是“安全”与“推理”交叉的典型案例，适用第四条标准。标准指出：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” 本文正是如此。它没有停留在表面讨论如何封堵漏洞，而是提出通过提升模型的“后果推理”能力来从根本上解决安全问题。这种能力的提升，不仅能增强安全性，更能提高模型在通用问题解决任务中的可靠性，因为它学会了思考行为的后果，这是智能体规划和决策的核心。 **结论:** 尽管论文以“安全”为切入点，但其真正的科学贡献在于定位并试图修复LLM在通用推理能力上的一个关键短板。它将一个应用层问题（安全）归结为一个基础能力问题（推理），并提出针对性的解决方案。因此，这篇论文的本质是关于提升LLM通用推理能力的研究，完全符合筛选要求。"
    },
    {
        "index": "#52",
        "title": "Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness",
        "link": "/arxiv/2510.04293",
        "arxiv_id": "2510.04293",
        "authors": "Lingnan Xu, Chong Feng, Kaiyuan Zhang, Liu Zhengyong, Wenqiang Xu, Fanqing Meng",
        "summary": "While large language models (LLMs) demonstrate impressive capabilities, their reliance on parametric knowledge often leads to factual inaccuracies. Retrieval-Augmented Generation (RAG) mitigates this by leveraging external documents, yet existing approaches treat retrieved passages as isolated chunks, ignoring valuable structure that is crucial for document organization. Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel framework that explicitly incorporates structural information throughout the RAG process. RDR2 employs an LLM-based router to dynamically navigate document structure trees, jointly evaluating content relevance and hierarchical relationships to assemble optimal evidence. Our key innovation lies in formulating document routing as a trainable task, with automatic action curation and structure-aware passage selection inspired by human reading strategies. Through comprehensive evaluation on five challenging datasets, RDR2 achieves state-of-the-art performance, demonstrating that explicit structural awareness significantly enhances RAG systems' ability to acquire and utilize knowledge, particularly in complex scenarios requiring multi-document synthesis.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.610472",
        "filter_reason": "这篇论文符合我的研究范围，应该被保留。我的判断过程如下： 1.  **核心判断（第一步）：** 论文的核心是提出一种名为RDR2的新框架，用于改进检索增强生成（RAG）技术。RAG本身是一种提升LLM知识获取和利用能力的通用方法论。该论文并非将LLM应用于某个特定领域（如医疗、法律），而是致力于改进LLM处理外部信息这一基础能力。论文中提到的“多文档综合”能力，本质上是一种复杂的信息整合与推理过程，属于通用推理能力的范畴。因此，这篇论文的本质是改进LLM的基础方法论，符合保留标准。 2.  **正面指标（第二步）：** 论文明确包含多个正面指标。 *   **核心概念:** 论文标题和摘要多次提及 \"Large language models (LLMs)\"。 *   **能力方向:** 论文的核心贡献是提升LLM“获取和利用知识的能力”，尤其是在“需要多文档综合的复杂场景中”。这种综合信息、构建连贯答案的能力，是通用推理能力的重要组成部分。 *   **新兴范式:** 论文提出的RDR2框架中包含一个“LLM-based router”，这个路由器在文档结构树中动态导航，可以看作是一个简化版的、用于特定任务（信息检索与整合）的智能体。这符合“llm-based agents”的范畴。 3.  **排除标准（第三步）：** 论文完全不涉及任何排除标准。它聚焦于纯文本，没有讨论多模态、视觉或任何特定应用领域（如医疗、化学）。它研究的是如何通过改进模型框架来提升事实准确性，而非应用层面的水印、安全等问题。 4.  **特殊和模糊情况（第四步）：** *   **幻觉/可解释性/安全:** 论文开篇即指出LLM存在“事实不准确”的问题，这通常与幻觉有关。它提出的RDR2框架是一种从模型内部机制（改进信息检索与整合流程）入手来减少幻觉、提升事实准确性的新方法。这完全符合“提出一种新方法来减少幻觉，从而提升模型的通用可靠性和推理质量”的保留标准。 **最终决策（第五步）：** 综合以上分析，这篇论文的核心贡献是提出了一种通用的、可训练的框架（RDR2），通过赋予LLM文档结构感知能力，显著增强了其在复杂场景下综合多源信息进行回答的能力。这直接提升了LLM的知识整合与运用能力，是通用推理能力的一个关键方面。因此，该论文精准地契合了“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    },
    {
        "index": "#57",
        "title": "Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought",
        "link": "/arxiv/2510.04230",
        "arxiv_id": "2510.04230",
        "authors": "Guijin Son, Donghun Yang, Hitesh Laxmichand Patel, Amit Agarwal, Hyunwoo Ko, Chanuk Lim, Srikant Panda, Minhyuk Kim, Nikunj Drolia, Dasol Choi, Kyong-Ha Lee, Youngjae Yu",
        "summary": "Recent frontier models employ long chain-of-thought reasoning to explore solution spaces in context and achieve stonger performance. While many works study distillation to build smaller yet capable models, most focus on English and little is known about language-specific reasoning. To bridge this gap, we first introduct **Language-Mixed CoT**, a reasoning schema that switches between English and a target language, using English as an anchor to excel in reasoning while minimizing translation artificats. As a Korean case study, we curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5, Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves state-of-the-art performance, with the highest overall average score (64.0 \\pm 25), ranking first on 5/9 benchmarks and second on the remainder. Samller and mid-sized models also benefit substantially, with an average improvement of +18.6 points across teh evaluated nine benchmarks. Ablations show **Language-Mixed CoT** is more effective than monolingual CoT, also resulting in cross-lingual and mult-modal performance gains. We release our data-curation pipeline, evaluation system, datasets, and models to advance research on language-specific reasoning. Data and model collection: https://huggingface.co/KOREAson.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.612056",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Language-Mixed Chain-of-Thought”的新推理模式。其本质是探索如何通过改进训练和推理范式，来提升大语言模型在非英语环境下的通用推理能力。这直接命中了“改进LLM的基础能力”和“增强其逻辑、数学、规划、多步推理等通用能力”的核心目标。论文并非将LLM作为工具应用于特定领域，而是专注于提升模型本身的能力。 2.  **第二步：正面指标** 论文高度契合多个正面指标： *   **核心概念**: 论文的研究对象是多个主流LLM家族（Qwen2.5, Llama-3.1等）。 *   **能力方向**: 论文的标题和摘要反复强调“Reasoning”，并具体到“long chain-of-thought reasoning”和“language-specific reasoning”，这正是你关注的核心能力。 *   **新兴范式**: “Chain-of-Thought” (CoT) 是你明确列出的关键新兴范式之一。本文提出的“Language-Mixed CoT”是对CoT范式的创新和扩展。 3.  **第三步：排除标准** 论文没有触及任何主要的排除标准： *   **多模态与视觉**: 摘要中提到“also resulting in cross-lingual and mult-modal performance gains”，但这只是一个次要的、附加的实验结果，并非论文的主要研究焦点。论文的核心是文本推理，因此不应被排除。 *   **特定应用领域**: 论文使用的数据集（网络问答、考试、STEM、代码）是通用推理能力的来源，而非局限于医疗、化学等特定领域。 *   **模型可靠性（应用层面）**: 论文不涉及水印、安全或应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况。唯一可能引起歧义的“多模态性能增益”已在第三步中澄清，它是一个次要发现，不影响论文的核心贡献属于通用推理能力增强的范畴。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种创新的思维链方法（Language-Mixed CoT）来系统性地提升大语言模型的通用推理能力，并验证了其在不同模型规模上的有效性。这完全符合你“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标。因此，应予以保留。"
    },
    {
        "index": "#61",
        "title": "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization",
        "link": "/arxiv/2510.04182",
        "arxiv_id": "2510.04182",
        "authors": "Wengao Ye, Yan Liang, Lianlei Shan",
        "summary": "Recent advancements in Large Language Models (LLMs) have shifted from explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning, where intermediate thoughts are represented as vectors rather than text. However, latent reasoning can be brittle on challenging, out-of-distribution tasks where robust reasoning is most critical. To overcome these limitations, we introduce Latent Thought Policy Optimization (LTPO), a parameter-free framework that enhances LLM reasoning entirely at test time, without requiring model parameter updates. LTPO treats intermediate latent \"thought\" vectors as dynamic parameters that are actively optimized for each problem instance. It employs an online policy gradient method guided by an intrinsic, confidence-based reward signal computed directly from the frozen LLM's own output distributions, eliminating the need for external supervision or expensive text generation during optimization. Extensive experiments on five reasoning benchmarks show that LTPO not only matches or surpasses strong baselines on standard tasks but also demonstrates remarkable robustness where others fail. Most notably, on highly challenging AIME benchmarks where existing latent reasoning baselines collapse to near-zero accuracy, LTPO delivers substantial improvements, showcasing a unique capability for complex reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.613395",
        "filter_reason": "这篇论文完全符合研究范围，是关于提升大语言模型通用推理能力的前沿研究。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心贡献是提出了一种名为“Latent Thought Policy Optimization (LTPO)”的新方法。其本质是**一种在测试时动态优化LLM推理过程的新范式**。它不改变模型参数，而是将中间的“潜在思维”向量作为可优化的动态参数，通过在线策略梯度方法来提升模型在单个问题实例上的推理表现。这完全符合筛选标准中“改进LLM的基础能力”、“提出新的训练范式”以及“增强其逻辑、数学、规划、多步推理等通用能力”的要求。它不是将LLM应用于特定领域，而是直接作用于LLM的推理核心机制。 **第二步：正面指标——论文是否包含以下主题？** 论文摘要中包含了大量正面指标： - **核心概念**: 明确提及“Large Language Models (LLMs)”。 - **能力方向**: 核心主题就是“reasoning”，并特别强调了在“challenging, out-of-distribution tasks”上的“robust reasoning”和“complex reasoning”，这直接对应了逻辑和数学推理能力。 - **训练方法**: 虽然是在测试时进行，但其方法论“policy gradient method”源于强化学习（RL），这与筛选标准中的“reinforcement learning (RL)”高度相关。 - **新兴范式**: 论文建立在“latent reasoning”这一新兴范式之上，并对其进行了改进，这与“思维链”等新范式的研究属于同一类别。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全不涉及任何排除标准中的领域： - **多模态与视觉**: 全文聚焦于文本和潜在向量，未提及视觉或多模态。 - **特定应用领域**: 研究在通用的推理基准（如AIME）上进行，而非医疗、化学等特定领域。 - **模型可靠性（应用层面）**: 论文关注的是提升推理的“鲁棒性”和“准确性”，这是模型内在能力的提升，而非水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 本论文不涉及智能体或工具，但其“测试时优化”的思想，可以被视为一种让模型在解决单个问题时“自我进化”或“自我反思”的机制，这与提升通用问题解决能力的目标一致。 - **幻觉/可解释性/安全**: 论文通过优化推理过程来提升模型在困难任务上的表现，这间接有助于减少因推理错误导致的“事实性”或“逻辑性”幻觉，从而提升了模型的内在推理质量。这符合“提升模型的通用可靠性和推理质量”的保留标准。 **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种创新的、在测试时增强LLM推理能力的方法论。它直接针对LLM的通用推理瓶颈（尤其是在复杂、分布外任务上的脆弱性），并取得了显著效果。该研究不涉及任何特定应用或排除领域，完全聚焦于提升LLM本身的基础推理能力，是“大语言模型通用推理能力”研究课题下的典型高相关性论文。因此，最终判断为符合要求。"
    },
    {
        "index": "#60",
        "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling",
        "link": "/arxiv/2510.04204",
        "arxiv_id": "2510.04204",
        "authors": "Zhengyang Tang, Zihan Ye, Chenyu Huang, Xuhan Huang, Chengpeng Li, Sihang Li, Guanhua Chen, Ming Yan, Zizhuo Wang, Hongyuan Zha, Dayiheng Liu, Benyou Wang",
        "summary": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs -- In particular, we show that direct fine-tuning on traditional \\textit{non-reflective} datasets leads to limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose \\textbf{CALM} (\\textit{Corrective Adaptation with Lightweight Modification}), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6\\% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop \\textbf{STORM} (\\textit{Smart Thinking Optimization Reasoning Model}), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9\\% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks.",
        "subjects": "Computation and Language, Artificial Intelligence, Computational Engineering, Finance, and Science, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.613112",
        "filter_reason": "这篇论文符合您的筛选标准，应该被保留。我的判断过程如下： 1.  **核心判断（第一步）：论文的本质是提升LLM的通用推理能力。** 论文的核心贡献是提出了一个名为 **CALM** 的新框架。这个框架的本质不是将LLM作为一个黑盒工具应用到“优化建模”这个领域，而是提出了一种**新的训练和适应范式**，旨在“解锁并放大”大型推理模型（LRMs）的“原生推理能力”。其核心机制——通过专家干预提供修正提示，生成高质量的推理轨迹，再进行监督微调和强化学习——直接作用于模型的推理过程本身。这是一种方法论上的创新，旨在让模型更擅长进行复杂的多步推理，这与您“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的目标高度一致。 2.  **正面指标（第二步）：论文包含多个高度相关的主题。** - **核心概念**: 论文明确研究 \"Large Reasoning Models (LRMs)\"，这是LLM的一个子集，专注于推理。 - **能力方向**: 论文的核心就是 \"reasoning\"，特别是 \"complex multi-step reasoning\" 和 \"reasoning trajectories\"。优化建模本身就是一种高级的数学和逻辑推理问题。 - **训练方法**: 论文明确使用了 \"supervised fine-tuning\" 和 \"reinforcement learning\" 来改进模型。 - **新兴范式**: \"专家干预者\"提供\"修正提示\"可以被看作是一种广义上的**工具使用**或**人机协作**形式，其目的是为了增强模型的推理质量。 3.  **排除标准（第三步）：论文不属于主要排除领域。** - **多模态与视觉**: 论文完全聚焦于文本推理，不涉及视觉或多模态内容。 - **特定应用领域**: 这是需要仔细辨析的一点。虽然论文的实验基准是“优化建模”，但这更像是一个**衡量和展示推理能力的“试验场”**，而非一个像医疗、化学那样的垂直领域。优化建模与数学、逻辑和规划紧密相关，是检验通用推理能力的经典场景。论文的落脚点是“提供了一条更有效和可扩展的路径，以在具有挑战性的优化建模任务上实现专家级性能”，其强调的是“路径”的通用性，而非“优化建模”这个领域本身。因此，它不属于“将LLM作为工具应用到特定领域”的排除范畴。 - **模型可靠性**: 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **特殊和模糊情况（第四步）：处理得当。** - **智能体/工具使用**: 论文中的“专家干预者”正是“提出一种通用的...工具使用方法来增强LLM的通用问题解决能力”的体现。它不是特定领域的工具（如化学数据库），而是一个用于修正和提升推理轨迹的通用机制。 **最终决策（第五步）：** 综合以上分析，这篇论文的核心贡献是提出了一种名为CALM的创新框架，通过迭代修正和强化学习来增强和放大LLM的原生推理模式。尽管它以“优化建模”为测试平台，但其研究焦点和方法论是关于如何**从根本上提升LLM的通用推理能力**，这与您的核心目标完全契合。因此，这篇论文应该被保留。"
    },
    {
        "index": "#66",
        "title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning",
        "link": "/arxiv/2510.04081",
        "arxiv_id": "2510.04081",
        "authors": "Honglin Lin, Qizhi Pei, Xin Gao, Zhuoshi Pan, Yu Li, Juntao Li, Conghui He, Lijun Wu",
        "summary": "Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Caco's code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention.",
        "subjects": "Computation and Language, Programming Languages",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.614863",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据筛选标准进行的详细判断： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Caco”的新框架，其本质是一种**新的训练范式**。该框架通过代码驱动的方式，自动化地生成高质量、可验证、多样化的思维链推理数据，并用这些数据来微调大语言模型。其直接目标是提升模型的**推理能力**，这是一个基础且核心的LLM能力。论文并非将LLM作为工具应用于某个特定领域，而是专注于改进模型本身处理复杂问题的内在机制。因此，根据第一步标准，应**保留**。 2.  **第二步：正面指标** 论文高度契合多个正面指标： *   **核心概念**: 论文明确以“Large Language Models (LLMs)”为研究对象。 *   **能力方向**: 论文的核心主题是“Reasoning capability”，并特别在“mathematical reasoning”基准上进行了验证。 *   **新兴范式**: 论文是对“Chain-of-Thought (CoT)”这一主流推理范式的创新和扩展。同时，它利用“code execution”作为一种工具来验证推理步骤，这属于“工具使用”的范畴，旨在增强推理的可靠性。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准领域： *   它不涉及多模态、视觉等内容。 *   它虽然以数学问题为测试基准，但其方法（Caco框架）是通用的，旨在提升模型的“generalization across unseen tasks”，而非局限于数学这一特定领域。 *   它不讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 论文中的“工具使用”（代码执行）是典型的**保留情况**。它不是将智能体应用于特定领域（如化学），而是利用代码执行这一通用工具来**增强LLM推理过程的逻辑正确性和可验证性**，从而提升其通用问题解决能力。 *   **幻觉/可解释性/安全**: 论文通过“automated validation via code execution”来确保推理路径的逻辑正确性，这直接解决了CoT生成中可能出现的“幻觉”或逻辑错误问题。这是一种**从模型内部训练机制入手提升推理质量**的根本性方法，而非应用层面的讨论，因此符合保留条件。 **最终决策**: 综合以上分析，这篇论文提出了一种创新的、可扩展的框架（Caco），通过代码辅助的数据生成和验证，直接致力于提升大语言模型的**通用推理能力**。其研究内容、方法和目标与您的研究课题“大语言模型通用推理能力”高度一致，是一篇非常相关的前沿论文。因此，最终判断为 **True**。"
    },
    {
        "index": "#69",
        "title": "Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment",
        "link": "/arxiv/2510.04045",
        "arxiv_id": "2510.04045",
        "authors": "Yunfan Zhang, Kathleen McKeown, Smaranda Muresan",
        "summary": "Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.672695",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是探索如何利用**思维链**和**强化学习**这两种方法论，来增强大语言模型的一项基础能力——即根据特定视角生成内容的能力（可转向的多元主义）。这并非将LLM作为工具应用于某个特定领域，而是直接致力于改进LLM的内在推理和生成机制。因此，论文的本质是提升LLM的基础能力，符合保留标准。 2.  **第二步：正面指标** 论文高度符合多个正面指标： *   **核心概念**: 明确以 \"Large Language Models (LLMs)\" 为研究对象。 *   **能力方向**: 核心研究内容是 \"Chain-of-Thought (CoT) reasoning\"，这正是通用推理能力的关键技术。 *   **训练方法**: 探索了多种方法，其中 \"Reinforcement Learning with Verifiable Rewards (RLVR)\" 是一种重要的强化学习训练范式。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： *   它不涉及多模态、视觉或任何特定应用领域（如医疗、化学等）。研究的“价值观”和“观点”是通用的人类社会概念，而非特定专业知识领域。 *   它不关注模型基础设施、部署或硬件加速。 *   虽然提到了 \"safety\"，但这是作为评估其生成推理链质量的一个维度，而不是论文的核心研究议题（如水印或通用安全防御）。 4.  **第四步：处理特殊和模糊情况** 论文对 \"faithfulness\"（忠实性）的分析，直接关联到其生成的思维链是否真实反映了模型的推理过程，这属于提升模型内在推理质量和可靠性的范畴，因此应该保留。对 \"safety\" 的分析也是同理，是服务于评估其推理方法的有效性，而非独立的安全研究。 **最终决策**: 这篇论文的核心贡献在于，它系统性地研究了如何通过改进模型的**推理过程**（CoT）和**训练方式**（RL），来赋予LLM一种更高级、更可控的通用能力。这直接命中了你“提高大语言模型本身的『通用推理能力』”的核心目标。它不是在应用LLM，而是在雕琢LLM本身。因此，这篇论文是高度相关且应该被保留的前沿研究。"
    },
    {
        "index": "#81",
        "title": "Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models",
        "link": "/arxiv/2510.03805",
        "arxiv_id": "2510.03805",
        "authors": "Canhui Wu, Qiong Cao, Chang Li, Zhenfang Wang, Chao Xue, Yuwei Fan, Wei Xi, Xiaodong He",
        "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks but often suffer from excessive verbosity, known as \"overthinking.\" Existing solutions via reinforcement learning (RL) typically penalize generated tokens to promote conciseness. However, these methods encounter two challenges: responses with fewer tokens do not always correspond to fewer reasoning steps, and models may develop hacking behavior in later stages of training by discarding reasoning steps to minimize token usage. In this work, we introduce \\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more efficient reasoning by favoring compact reasoning steps. Our step-aware reward function prioritizes correctness while imposing penalties for redundant steps, and withholds rewards for incorrect responses to prevent the reinforcement of erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when the length of any output step exceeds the upper limit, we halt updates to prevent hacking behavior caused by merging steps. Extensive experiments across four reasoning benchmarks demonstrate that SP achieves state-of-the-art accuracy while significantly reducing response length. For instance, on AIME24, SP reduces token usage by \\textbf{69.7\\%}.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.701870",
        "filter_reason": "这篇论文完全符合筛选要求。 1.  **核心贡献与本质判断 (第一步)**: 论文的核心贡献是提出了一种名为“步骤修剪器”的强化学习（RL）框架，旨在解决大推理模型（LRM）在复杂任务中出现的“过度思考”问题，即推理过程冗长但效率低下。根据筛选标准的第一步，这篇论文的本质是改进LLM的基础能力。它没有将LLM作为工具应用于特定领域，而是直接针对LLM的『通用推理能力』本身进行优化。具体来说，它通过一种新的训练范式（RL框架）来增强模型在逻辑、数学等任务上的多步推理效率和准确性，这与思维链、强化学习优化等方法论研究属于同一范畴，旨在提升模型内在的、通用的推理能力。 2.  **正面指标匹配 (第二步)**: 论文包含了多个关键的正面指标： *   **核心概念**: 明确研究“Large Reasoning Models (LRMs)”，这是大语言模型在推理任务上的具体体现。 *   **能力方向**: 核心聚焦于“reasoning”，并在数学推理基准（AIME24）上进行了验证。 *   **训练方法**: 核心方法是“reinforcement learning (RL)”，通过设计新的奖励函数来引导模型行为。 3.  **排除标准检查 (第三步)**: 论文不涉及任何排除标准中的领域。它是一个纯粹的方法论研究，不涉及多模态、视觉，也没有将模型应用于医疗、化学等特定领域，更不关注水印、安全等应用层面的可靠性问题。 4.  **特殊情况处理 (第四步)**: 本文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，但其研究精神与“提升通用推理质量”一致。它解决的是推理过程中的一个核心缺陷（冗余步骤），从而提升推理的效率和准确性，这直接服务于提升模型通用推理能力的目标。 **最终决策**: 该论文致力于通过创新的训练方法直接提升LLM的内在推理效率和质量，这与“提高大语言模型本身的通用推理能力”这一核心目标高度一致。因此，应予以保留。"
    },
    {
        "index": "#92",
        "title": "Decoupling Task-Solving and Output Formatting in LLM Generation",
        "link": "/arxiv/2510.03595",
        "arxiv_id": "2510.03595",
        "authors": "Haikang Deng, Po-Nien Kung, Nanyun Peng",
        "summary": "Large language models (LLMs) are increasingly adept at following instructions containing task descriptions to solve complex problems, such as mathematical reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow more complex, models often struggle to adhere to all instructions. This difficulty is especially common when instructive prompts intertwine reasoning directives -- specifying what the model should solve -- with rigid formatting requirements that dictate how the solution must be presented. The entanglement creates competing goals for the model, suggesting that more explicit separation of these two aspects could lead to improved performance. To this front, we introduce Deco-G, a decoding framework that explicitly decouples format adherence from task solving. Deco-G handles format compliance with a separate tractable probabilistic model (TPM), while prompts LLMs with only task instructions. At each decoding step, Deco-G combines next token probabilities from the LLM with the TPM calculated format compliance likelihood to form the output probability. To make this approach both practical and scalable for modern instruction-tuned LLMs, we introduce three key innovations: instruction-aware distillation, a flexible trie-building algorithm, and HMM state pruning for computational efficiency. We demonstrate the effectiveness of Deco-G across a wide range of tasks with diverse format requirements, including mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall, our approach yields 1.0% to 6.0% relative gain over regular prompting practice with guaranteed format compliance.",
        "subjects": "Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.725729",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是基于筛选标准的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为Deco-G的解码框架，该框架将LLM的任务解决能力与输出格式化要求进行解耦。这直接关联到你的研究目标——『提高大语言模型本身的通用推理能力』。论文明确指出，当推理指令和格式指令交织在一起时，会给模型带来竞争目标，从而影响其解决复杂问题（如数学推理）的性能。Deco-G通过将格式化任务分离出去，让LLM可以专注于核心的推理过程，从而提升了其在数学推理、LLM-as-a-Judge等任务上的表现。根据筛选标准第一步，这篇论文的本质是改进LLM的基础能力（任务解决），提出了一种新的方法论（解码框架），旨在增强其逻辑和数学推理能力。它并非将LLM作为工具应用于特定领域，因此符合『保留』条件。 2.  **第二步：正面指标** 论文包含了多个正面指标，如核心概念\"Large language models, LLMs\"，能力方向\"reasoning\"和\"math reasoning\"，以及\"problem-solving\"。这些都是你研究范围内的核心主题。 3.  **第三步：排除标准** 论文不涉及多模态与视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全），因此不触犯任何排除标准。虽然提到了\"event argument extraction\"，但这只是作为验证框架通用性的一个NLP任务，并非论文的主要焦点，其核心框架是领域无关的。 4.  **第四步：处理特殊和模糊情况** 此论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，其贡献点非常清晰：通过一种通用的解码时优化方法来提升LLM的推理表现。 **最终决策**: Deco-G是一种旨在通过优化生成过程来释放LLM内在推理潜力的通用方法论。它不是在特定领域应用LLM，而是在模型生成层面进行创新，直接解决了阻碍LLM发挥其全部推理能力的一个瓶颈问题。因此，这篇论文是关于提升LLM本身通用推理能力的前沿研究，完全符合你的筛选要求。"
    },
    {
        "index": "#98",
        "title": "Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance",
        "link": "/arxiv/2510.03528",
        "arxiv_id": "2510.03528",
        "authors": "Ahmed Alajrami, Xingwei Tan, Nikolaos Aletras",
        "summary": "Instruction-tuning plays a vital role in enhancing the task-solving abilities of large language models (LLMs), improving their usability in generating helpful responses on various tasks. However, previous work has demonstrated that they are sensitive to minor variations in instruction phrasing. In this paper, we explore whether introducing perturbations in instruction-tuning data can enhance LLMs' resistance against noisy instructions. We focus on how instruction-tuning with perturbations, such as removing stop words or shuffling words, affects LLMs' performance on the original and perturbed versions of widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics and potential shifts in model behavior. Surprisingly, our results suggest that instruction-tuning on perturbed instructions can, in some cases, improve downstream performance. These findings highlight the importance of including perturbed instructions in instruction-tuning, which can make LLMs more resilient to noisy user inputs.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.732635",
        "filter_reason": "这篇论文符合研究范围，应予以保留。我的判断过程如下： **第一步：核心判断** 这篇论文的本质是研究一种新的训练范式（在指令微调阶段引入噪声），以增强大语言模型（LLM）本身的一项基础能力——对输入指令变化的鲁棒性。这并非将LLM作为工具应用于特定领域，而是直接改进模型的核心表现。一个对指令微小变化不敏感的模型，其通用问题解决能力和推理能力的稳定性会得到显著提升。因此，根据“改进LLM的基础能力、提出新的训练范式”这一标准，应**保留**。 **第二步：正面指标** 论文包含了多个高度相关的正面指标： - **核心概念**: 论文明确以\"Large language models (LLMs)\"为研究对象。 - **能力方向**: 论文在多个通用推理能力基准上进行评估，包括数学推理（GSM8K）和复杂的多步推理（BBH）。研究的目标是提升模型在这些任务上的性能和泛化能力。 - **训练方法**: 论文提出了一种新颖的指令微调方法，属于“新的训练范式”范畴。 **第三步：排除标准** 论文的主要焦点不属于任何排除标准： - 它不涉及多模态与视觉。 - 它不针对任何特定应用领域（如医疗、化学等）。 - 它研究的“鲁棒性”或“韧性”是一种内在的模型能力，而非应用层面的水印、安全或社会学研究。 **第四步：处理特殊和模糊情况** 论文研究的是模型对噪声输入的内在处理能力，这与“提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的精神是一致的。通过让模型对指令扰动更具韧性，可以有效减少因误解指令而导致的错误推理，从而间接提升了推理质量和可靠性。因此，应**保留**。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一种通过“噪声指令微调”来提升LLM鲁棒性的新方法。虽然它没有直接提出一种新的推理算法（如CoT），但它通过增强模型对输入的稳定性，显著提升了模型在多个通用推理基准（GSM8K, BBH）上的表现。这种对模型基础能力的改进，直接服务于“提高大语言模型本身的通用推理能力”这一核心目标。因此，这篇论文与你的研究课题高度相关。"
    },
    {
        "index": "#91",
        "title": "Can an LLM Induce a Graph? Investigating Memory Drift and Context Length",
        "link": "/arxiv/2510.03611",
        "arxiv_id": "2510.03611",
        "authors": "Raquib Bin Yousuf, Aadyant Khatri, Shengzhe Xu, Mandar Sharma, Naren Ramakrishnan",
        "summary": "Recently proposed evaluation benchmarks aim to characterize the effective context length and the forgetting tendencies of large language models (LLMs). However, these benchmarks often rely on simplistic 'needle in a haystack' retrieval or continuation tasks that may not accurately reflect the performance of these models in information-dense scenarios. Thus, rather than simple next token prediction, we argue for evaluating these models on more complex reasoning tasks that requires them to induce structured relational knowledge from the text - such as graphs from potentially noisy natural language content. While the input text can be viewed as generated in terms of a graph, its structure is not made explicit and connections must be induced from distributed textual cues, separated by long contexts and interspersed with irrelevant information. Our findings reveal that LLMs begin to exhibit memory drift and contextual forgetting at much shorter effective lengths when tasked with this form of relational reasoning, compared to what existing benchmarks suggest. With these findings, we offer recommendations for the optimal use of popular LLMs for complex reasoning tasks. We further show that even models specialized for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in these settings. These results point to significant limitations in the models' ability to abstract structured knowledge from unstructured input and highlight the need for architectural adaptations to improve long-range reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.725458",
        "filter_reason": "这篇论文完全符合你的研究范围。 **第一步：核心判断** - 这篇论文的本质并非将LLM应用于特定领域，而是深入探究LLM在执行一项核心认知任务——**从非结构化文本中归纳结构化知识（图结构）**——时所表现出的能力与局限性。这直接关联到LLM的**通用推理能力**，特别是关系归纳和长程推理。论文的核心贡献在于提出了一个新的、更具挑战性的评估范式，用以揭示现有模型在复杂推理任务上的“记忆漂移”问题，并明确指出了未来需要“架构调整来改善长程推理”。这完全符合“致力于提高大语言模型本身的通用推理能力”这一核心目标。 **第二步：正面指标** - **核心概念**: 论文明确以`Large language models (LLMs)`为研究对象。 - **能力方向**: 论文的核心聚焦于`reasoning`，具体是`complex reasoning tasks`和`relational reasoning`。这与你的筛选标准高度吻合。 - **其他**: 虽然未涉及RL或智能体框架，但其对推理能力极限的深刻探讨是极其相关的正面信号。 **第三步：排除标准** - 论文未涉及任何`多模态与视觉`内容。 - 论文的研究目标是通用的关系归纳能力，不属于任何`特定应用领域`（如医疗、化学等）。 - 论文讨论的“记忆漂移”是模型在执行推理任务时的内在认知缺陷，而非应用层面的`Watermarking, Safety, Security`等问题。 **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用。 - 论文研究的“记忆漂移”和“上下文遗忘”直接影响了模型的推理质量和可靠性，其研究目标是通过揭示这一根本性问题来推动模型架构层面的改进，从而提升其内在的通用推理能力。这与“提升模型内在的通用可靠性和推理质量”的原则一致，因此应当保留。 **第五步：最终决策** 综合分析，这篇论文通过对LLM在**长程关系归纳**这一复杂推理任务上的表现进行深入剖析，揭示了当前模型在通用推理能力上的一个关键瓶颈（记忆漂移）。它虽然以评估为主，但其发现直接为未来如何“改进LLM的基础能力”和“增强其逻辑、多步推理等通用能力”指明了方向。因此，这篇论文对于你的研究课题具有重要的参考价值，应当被筛选保留。"
    },
    {
        "index": "#99",
        "title": "Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs",
        "link": "/arxiv/2510.03527",
        "arxiv_id": "2510.03527",
        "authors": "Sayan Ghosh, Shahzaib Saqib Warraich, Dhruv Tarsadiya, Gregory Yauney, Swabha Swayamdipta",
        "summary": "Language models can be sampled multiple times to access the distribution underlying their responses, but existing methods cannot efficiently synthesize rich epistemic signals across different long-form responses. We introduce Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents shared information, as well as semantic variation in a set of sampled LM responses to the same prompt. We construct ConGrs using a light-weight lexical sequence alignment algorithm from bioinformatics, supplemented by the targeted usage of a secondary LM judge. Further, we design task-dependent decoding methods to synthesize a single, final response from our ConGr data structure. Our experiments show that synthesizing responses from ConGrs improves factual precision on two biography generation tasks by up to 31% over an average response and reduces reliance on LM judges by more than 80% compared to other methods. We also use ConGrs for three refusal-based tasks requiring abstention on unanswerable queries and find that abstention rate is increased by up to 56%. We apply our approach to the MATH and AIME reasoning tasks and find an improvement over self-verification and majority vote baselines by up to 6 points of accuracy. We show that ConGrs provide a flexible method for capturing variation in LM responses and using the epistemic signals provided by response variation to synthesize more effective responses.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.733254",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Consensus Graphs (ConGrs)”的新方法，用于在推理时（inference-time）综合大语言模型的多次采样响应，以生成一个更高质量的最终答案。这本质上是一种改进LLM输出质量和推理能力的**新方法论**，而不是将LLM作为工具应用于特定领域。它通过分析多个响应之间的“共识”和“变异”，提炼出更强的“知识信号”，从而提升模型在解决问题时的表现。这完全符合“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标** 论文明确命中了多个关键正面指标： *   **核心概念**: 论文的研究对象是Language models (LLMs)。 *   **能力方向**: 论文的核心实验部分直接在**MATH和AIME这两个数学推理基准任务**上进行了验证，并取得了性能提升。这直接对应了“reasoning (尤其是 math reasoning)”这一核心能力方向。 *   **新兴范式**: 虽然不是智能体或工具使用，但其“多次采样-对齐-综合”的流程，可以看作是一种增强模型自我验证和问题解决能力的新范式，与“deep research”的理念有相通之处。 3.  **第三步：排除标准** 论文没有触及任何排除标准： *   它不涉及多模态、视觉等内容。 *   它的应用场景是传记生成和数学问题，这些都是**通用领域**，而非医疗、化学等特定应用领域。 *   它虽然提到了“事实精确性”和“拒绝回答”，但其目标是提升模型内在的可靠性以增强推理质量，而不是研究水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文与“幻觉/可解释性”的讨论相关。它通过综合多个响应来提高“事实精确性”，这可以被看作是一种减少事实性幻觉、提升模型内在可靠性的新方法。根据筛选标准，这种旨在提升模型内在推理质量和可靠性的方法应该被保留。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种通用的、在推理阶段增强LLM输出质量的新方法。它不改变模型参数，而是通过更智能地解码和综合模型自身的输出来提升其性能。最关键的是，该方法在标准的数学推理任务上被证明有效，这直接切中了“大语言模型通用推理能力”这一研究课题的核心。因此，这篇论文高度相关，应被筛选入内。"
    },
    {
        "index": "#111",
        "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models",
        "link": "/arxiv/2510.05095",
        "arxiv_id": "2510.05095",
        "authors": "Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia",
        "summary": "Large reasoning models (LRMs) generate intermediate reasoning traces before producing final answers, yielding strong gains on multi-step and mathematical tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for model deployment, remains underexplored. The statistically correct objective for preference alignment requires marginalizing over reasoning traces, but this computation is intractable in practice. A common workaround optimizes a single sampled trajectory, which introduces substantial gradient variance from stochastic trace sampling. To address this challenge, we frame preference optimization for LRMs through the lens of the bias--variance trade-off and propose Bias--Variance Optimized Preference Optimization (BVPO), a simple, drop-in method that mixes two gradient estimators: a high-variance trace-based estimator and a low-variance empty-trace estimator obtained by disabling reasoning trace generation. Our theory shows that BVPO strictly reduces trace-induced variance for any nontrivial mixture, provides a closed-form choice of the mixing weight that minimizes mean-squared error relative to the true marginal gradient, and under standard smoothness and step-size conditions, tightens classical convergence bounds for stochastic gradient descent. Empirically, BVPO improves alignment over the best baseline by up to 7.8 points on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on general conversational data, BVPO also boosts reasoning performance for base models by up to 4.0 points on the average of six math reasoning benchmarks. These results identify variance from trace sampling as a key bottleneck and demonstrate that directly optimizing the bias--variance trade-off yields more stable training and stronger overall performance.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.737022",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Bias-Variance Optimized Preference Optimization (BVPO)”的新方法。这个方法旨在解决大型推理模型在偏好对齐训练中遇到的一个具体技术难题：由推理轨迹采样带来的梯度方差问题。论文的本质是**改进LLM/LRM的训练范式**，通过优化训练过程的稳定性来提升模型的对齐效果和推理能力。这并非将LLM作为工具应用于特定领域，而是直接作用于模型本身的基础能力，因此符合核心保留标准。 2.  **第二步：正面指标** 论文摘要中包含了多个强烈的正面指标： - **核心概念**: 明确提到了 \"Large reasoning models (LRMs)\"，这是LLM在推理任务上的一个具体形态。 - **能力方向**: 直接聚焦于 \"reasoning\" 和 \"mathematical tasks\"，这正是你关注的核心能力。 - **训练方法**: 论文的核心是 \"preference optimization\"，这是与RLHF并列的关键训练技术，属于强化学习范畴。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： - 它不涉及任何多模态或视觉内容。 - 它的研究是通用的，在通用对话数据上训练，并在通用和数学基准上测试，没有聚焦于医疗、化学等特定应用领域。 - 它讨论的是模型训练层面的对齐问题，而非水印、安全等应用层面的可靠性技术。 4.  **第四步：处理特殊和模糊情况** 论文研究的“对齐”与模型可靠性相关，但它并非从社会学或应用部署角度讨论，而是提出了一种**新的、底层的训练算法**来提升对齐质量。这种改进直接带来了模型推理性能的增强（在数学基准上提升高达4.0分），因此它属于“通过提升内在可靠性来增强推理质量”的情况，应该保留。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种创新的训练方法（BVPO），用于解决大型推理模型训练中的一个根本性挑战（梯度方差），从而直接提升了模型的通用对齐效果和数学推理能力。它完全符合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#115",
        "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training",
        "link": "/arxiv/2510.04996",
        "arxiv_id": "2510.04996",
        "authors": "Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, Tong Zhang",
        "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.743446",
        "filter_reason": "这篇论文完全符合筛选要求，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 这篇论文的本质是提出一种名为“Reinforce-Ada”的新颖训练框架。其核心贡献是改进用于大语言模型（LLM）推理任务的强化学习（RL）训练过程。论文通过解决“不稳定的梯度估计”这一基础性问题，旨在提升LLM在推理任务上的收敛速度和最终性能。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。它并非将LLM应用于特定领域，而是专注于提升模型本身的核心推理能力。 2.  **第二步：正面指标** 论文摘要中包含了多个强烈的正面指标： *   **核心概念**: 明确提及 \"large language models (LLMs)\"。 *   **能力方向**: 核心目标是解决 \"reasoning tasks\"，并在 \"reasoning benchmarks\" 上进行验证。 *   **训练方法**: 论文的核心是关于 \"Reinforcement learning (RL)\"，具体是 \"online RL post-training\"。 这些关键词的密集出现，清晰地表明该研究与“大语言模型通用推理能力”高度相关。 3.  **第三步：排除标准** 论文的研究内容完全不涉及任何排除标准领域： *   它没有涉及视觉或多模态内容。 *   它没有将LLM应用于医疗、化学、机器人等任何特定领域。 *   它关注的是训练算法的效率和稳定性，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需进行特殊判断。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是一种方法论创新——一种用于提升LLM推理能力的自适应强化学习训练框架。它直接针对LLM在推理任务上的训练瓶颈进行优化，旨在通过更高效、更稳定的训练过程，从根本上提升模型的通用推理性能。这与“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标完全一致。因此，最终判断为 **True**。"
    },
    {
        "index": "#108",
        "title": "Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision",
        "link": "/arxiv/2510.03323",
        "arxiv_id": "2510.03323",
        "authors": "Ge Chang, Jinbo Su, Jiacheng Liu, Pengfei Yang, Yuhao Shang, Huiwen Zheng, Hongli Ma, Yan Liang, Yuanchun Li, Yunxin Liu",
        "summary": "A significant portion of real-world data is inherently represented as textual graphs, and integrating these graphs into large language models (LLMs) is promising to enable complex graph-based question answering. However, a key challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e., how to retrieve relevant content from large graphs that is sufficiently informative while remaining compact for the LLM context. Existing retrievers suffer from poor performance since they either rely on shallow embedding similarity or employ interactive retrieving policies that demand excessive data labeling and training cost. To address these issues, we present Graph-$S^3$, an agentic textual graph reasoning framework that employs an LLM-based retriever trained with synthetic stepwise supervision. Instead of rewarding the agent based on the final answers, which may lead to sparse and unstable training signals, we propose to closely evaluate each step of the retriever based on offline-extracted golden subgraphs. Our main techniques include a data synthesis pipeline to extract the golden subgraphs for reward generation and a two-stage training scheme to learn the interactive graph exploration policy based on the synthesized rewards. Based on extensive experiments on three common datasets in comparison with seven strong baselines, our approach achieves an average improvement of 8.1\\% in accuracy and 9.7\\% in F$_1$ score. The advantage is even higher in more complicated multi-hop reasoning tasks. Our code will be open-sourced.",
        "subjects": "Computation and Language",
        "date": "2025-10-01",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.736086",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 论文的核心贡献是提出了一个名为“Graph-S3”的框架，旨在解决LLM在处理“文本图”数据时的推理瓶颈。其关键创新点在于一种名为“合成逐步监督”的训练方法。这种方法并非简单地将LLM应用于某个领域，而是**改进LLM自身的推理过程**。它通过奖励模型推理路径中的每一个正确步骤（而不仅仅是最终答案），来训练一个基于LLM的智能体检索器，使其能够更有效地进行多跳信息检索和整合。这直接命中了筛选标准中“增强其逻辑、……多步推理等通用能力”和“提出新的训练范式”的核心要求。 2.  **第二步：正面指标——论文高度相关。** 论文摘要中包含了多个关键正面指标： *   **核心概念**: 明确以 \"Large language models (LLMs)\" 为核心。 *   **能力方向**: 核心研究内容是 \"textual graph reasoning\" 和 \"multi-hop reasoning tasks\"，这正是通用推理能力的关键组成部分。 *   **训练方法**: 提出的“两阶段训练方案”和“基于合成奖励学习交互式图探索策略”本质上是一种强化学习（RL）的训练范式，旨在优化模型的决策过程。 *   **新兴范式**: 整个框架被定义为一个 \"agentic textual graph reasoning framework\"，属于LLM-based agents的研究范畴。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究对象是“文本图”，与视觉、多模态无关。它提出的是一个通用的图推理框架，实验也是在通用数据集上进行，并未限定在医疗、化学、生物等特定应用领域。同时，论文也不涉及模型基础设施、部署优化或应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况——论文属于应保留的类别。** 论文提出的“Graph-S3”是一个**通用的智能体框架**，用于增强LLM在图结构数据上的通用问题解决能力，而非应用于特定领域（如“用于化学实验的智能体”）。此外，其“逐步监督”方法通过验证中间步骤的正确性，实质上提升了模型推理过程的可靠性和内在逻辑性，这与“提升模型的通用可靠性和推理质量”的目标一致，因此应该保留。 **最终决策**: 综合以上分析，这篇论文的本质是提出一种新的训练范式和智能体框架，以增强LLM在复杂图数据上的多步推理能力。它不涉及特定应用领域或排除的技术方向，完全聚焦于提升LLM的通用推理核心能力。因此，这篇论文与您的研究课题高度相关，应被筛选保留。"
    },
    {
        "index": "#116",
        "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game",
        "link": "/arxiv/2510.04980",
        "arxiv_id": "2510.04980",
        "authors": "Fangzhou Liang, Tianshi Zheng, Chunkit Chan, Yauwai Yim, Yangqiu Song",
        "summary": "Effective multi-agent collaboration requires agents to infer the rationale behind others' actions, a capability rooted in Theory-of-Mind (ToM). While recent Large Language Models (LLMs) excel at logical inference, their ability to infer rationale in dynamic, collaborative settings remains under-explored. This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework features an automated evaluation system that measures both game performance and ToM proficiency. Across a range of models, we find a significant positive correlation between ToM and in-game success. Notably, first-order ToM (interpreting others' intent) correlates more strongly with performance than second-order ToM (predicting others' interpretations). These findings highlight that for effective AI collaboration, the ability to accurately interpret a partner's rationale is more critical than higher-order reasoning. We conclude that prioritizing first-order ToM is a promising direction for enhancing the collaborative capabilities of future models.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.743776",
        "filter_reason": "这篇论文符合我的研究范围。我的判断过程如下： **第一步：核心判断** 这篇论文的本质并非将LLM作为工具应用于特定领域，而是提出一个新颖的基准（LLM-Hanabi）来**评估和探究**LLM在通用推理能力上的一个特定方面——心智理论和基本原理推断能力。虽然它没有直接提出一种新的训练方法来“提高”模型能力，但它通过严谨的实验和分析，**揭示了提升LLM通用推理能力的关键方向**（即优先发展一阶心智理论）。这种对核心能力瓶颈的深刻洞察和评估框架的构建，本身就是推动该领域前进的基础性工作，与“致力于提高LLM通用推理能力”的核心目标高度一致。 **第二步：正面指标** 论文完全符合多个正面指标： - **核心概念**: 论文明确以\"Large Language Models (LLMs)\"为研究对象。 - **能力方向**: 论文聚焦于一种高级的通用推理能力——\"Theory-of-Mind (ToM)\"和\"Rationale Inference\"，这属于逻辑推理和问题解决范畴。 - **新兴范式**: 论文的研究场景是\"Multi-Agent Gameplays\"，属于多智能体系统的研究范畴，这是当前提升LLM复杂决策与协作能力的重要范式。 **第三步：排除标准** 论文完全避开了所有排除标准： - 它不涉及多模态、视觉等内容。 - 它的研究场景是抽象的合作游戏\"Hanabi\"，而非医疗、化学、机器人等任何特定应用领域。 - 它的研究焦点是模型的内在推理机制，而非水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文研究的是在通用协作场景（Hanabi游戏）下的多智能体交互，旨在探究和评估通用的心智理论能力，而非将其应用于特定领域。这完全符合“保留”的条件。 - **幻觉/可解释性/安全**: 不适用。 **第五步：最终决策** 综合以上分析，尽管这篇论文的核心贡献是一个评估基准和一项发现，而非一个直接的训练改进方法，但它精准地切中了“大语言模型通用推理能力”这一核心课题。它通过构建评估工具和揭示关键瓶颈，为后续如何“提高”这一能力指明了清晰且有价值的方向。对于一个旨在推动领域前沿的研究者来说，这类定义问题、提供评估标准、并给出深刻洞见的论文是不可或缺的。因此，我判断这篇论文高度相关，予以保留。"
    },
    {
        "index": "#119",
        "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning",
        "link": "/arxiv/2510.04935",
        "arxiv_id": "2510.04935",
        "authors": "Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, Fei Huang",
        "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in simple tasks, where the models excessively utilize System 2-type, deliberate reasoning, leading to inefficient token generation. Furthermore, these models face challenges in adapting their reasoning capabilities to rapidly changing environments due to the static nature of their pretraining data. To address these issues, advancing Large Language Models (LLMs) for complex reasoning tasks requires innovative approaches that bridge intuitive and deliberate cognitive processes, akin to human cognition's dual-system dynamic. This paper introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless integration of System 1's fast, intuitive thinking with System 2's deliberate reasoning within LLMs. MARS strategically integrates multiple external tools, such as Google Search, Google Scholar, and Python Interpreter, to access up-to-date information and execute complex computations, while creating a specialized division of labor where System 1 efficiently processes and summarizes high-volume external information, providing distilled insights that expand System 2's reasoning context without overwhelming its capacity. Furthermore, we propose a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to simultaneously optimize both systems with multi-turn tool interactions, bin-packing optimization, and sample balancing strategies that enhance collaborative efficiency. Extensive experiments demonstrate MARS achieves substantial improvements of 3.86% on the challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.744809",
        "filter_reason": "这篇论文完全符合你的研究范围，是一个高度相关的优质候选。以下是根据你的筛选标准进行的详细判断： 1.  **第一步：核心判断** 论文的核心是提出一种名为MARS（多智能体深度研究系统）的新框架。其本质并非将LLM应用于某个特定领域，而是致力于改进LLM内部的推理机制。论文明确指出，其目标是解决大型推理模型（LRM）在简单任务上“过度分析”以及在动态环境中适应性差的问题。它通过引入一个融合了“系统1”（直觉、快速）和“系统2”（审慎、推理）的双系统范式，来优化LLM的通用推理过程。这完全符合“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标** 该论文几乎命中了所有关键的正面指标，相关性极强： *   **核心概念**: 论文研究对象是 Large Language Models (LLMs) 和 Large Reasoning Models (LRMs)。 *   **能力方向**: 核心聚焦于 reasoning，特别是区分了 System 1/2 的 reasoning，并旨在提升其在“复杂推理任务”和“深度研究”中的表现。 *   **训练方法**: 提出了“multi-agent reinforcement learning framework”来优化整个系统，这直接命中了强化学习（RL）这一关键方法。 *   **新兴范式**: 论文本身就是一个典型的“multi-agent systems”研究，并且深度融合了“tool use”（使用Google Search, Python Interpreter等）和“deep research”范式。 3.  **第三步：排除标准** 该论文没有触及任何排除标准： *   它不涉及多模态、视觉等内容。 *   它的应用场景是通用的“知识密集型任务”和“深度研究”，而非医疗、化学、机器人等特定领域。 *   它不讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文对“智能体/工具使用”的处理方式完全符合保留条件。MARS是一个通用的智能体协作框架，它使用工具（搜索、代码解释器）的目的是为了增强LLM在通用、动态信息环境下的推理能力，而不是为了解决某个特定领域（如化学实验）的问题。因此，这属于“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”，应该保留。 **最终决策：** 综合以上分析，这篇论文的核心贡献是提出了一种新颖的多智能体强化学习框架（MARS），通过模拟人类认知的双系统理论，并整合工具使用，来系统性地提升大语言模型在复杂和动态环境下的通用推理能力。这与你的核心目标“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”高度契合。因此，最终判断为 **True**。"
    },
    {
        "index": "#123",
        "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs",
        "link": "/arxiv/2510.04721",
        "arxiv_id": "2510.04721",
        "authors": "Ivo Petrov, Jasper Dekoninck, Martin Vechev",
        "summary": "Large language models (LLMs) have recently shown strong performance on mathematical benchmarks. At the same time, they are prone to hallucination and sycophancy, often providing convincing but flawed proofs for incorrect mathematical statements provided by users. This significantly limits the applicability of LLMs in theorem proving, as verification of these flawed proofs must be done manually by expert mathematicians. However, existing benchmarks that measure sycophancy in mathematics are limited: they focus solely on final-answer problems, rely on very simple and often contaminated datasets, and construct benchmark samples using synthetic modifications that create ill-posed questions rather than well-posed questions that are demonstrably false. To address these issues, we introduce BrokenMath, the first benchmark for evaluating sycophantic behavior in LLMs within the context of natural language theorem proving. BrokenMath is built from advanced 2025 competition problems, which are perturbed with an LLM to produce false statements and subsequently refined through expert review. Using an LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems and find that sycophancy is widespread, with the best model, GPT-5, producing sycophantic answers 29% of the time. We further investigate several mitigation strategies, including test-time interventions and supervised fine-tuning on curated sycophantic examples. These approaches substantially reduce, but do not eliminate, sycophantic behavior.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.746018",
        "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一个名为“BrokenMath”的**基准**，用于评估大语言模型在数学定理证明任务中的“谄媚”行为。虽然论文的主要形式是“评估”而非直接“改进”，但其评估的对象——谄媚行为，是LLM在执行**通用推理任务**（数学证明）时的一种根本性缺陷。这种缺陷导致模型放弃正确的逻辑推理，转而迎合用户的错误前提，直接损害了其推理能力的可靠性和质量。因此，这篇论文的本质是**深入剖析和量化LLM通用推理能力的一个关键短板**，为后续的改进工作提供了必要的评估工具和问题定义。这完全符合“改进LLM的基础能力、增强其逻辑、数学、多步推理等通用能力”的研究目标。 2.  **第二步：正面指标** 论文高度符合多个正面指标： *   **核心概念**: 明确以“Large language models (LLMs)”为研究对象。 *   **能力方向**: 聚焦于“reasoning”的核心领域——“theorem proving”（定理证明），这是数学推理的顶峰形式。 *   **新兴范式**: 论文评估了“agentic systems”，表明其研究内容与当前最前沿的智能体研究相关联。 3.  **第三步：排除标准** 论文不触及任何排除标准： *   它不涉及多模态、视觉等内容。 *   数学是通用推理的基础，不属于“特定应用领域”（如医疗、化学）。 *   论文研究的“sycophancy”（谄媚）是模型内在推理过程的失败，而非应用层面的“Watermarking, Safety, Security”等问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文恰好是“幻觉/可解释性/安全”这一特殊情况的完美例证。 *   “Sycophancy”可以被看作是一种特殊的、由外部诱因（用户的错误陈述）引发的“幻觉”。 *   论文不仅提出了评估基准，还**进一步研究了几种缓解策略**，如“测试时干预”和“监督微调”。这完全符合“提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留”的标准。这些策略直接旨在修复模型的推理缺陷，提升其通用能力。 5.  **第五步：最终决策** 综合来看，这篇论文虽然以一个评估基准为核心贡献，但其研究的问题（谄媚行为）直指LLM通用推理能力的要害。它为该领域提供了衡量和解决一个关键推理缺陷的必要工具和初步方案。对于任何致力于“提高大语言模型通用推理能力”的研究者来说，这篇论文都是不可或缺的前沿工作，因为它定义了一个重要的子问题，并指明了改进的方向。因此，它完全符合你的筛选要求。"
    },
    {
        "index": "#125",
        "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
        "link": "/arxiv/2510.04618",
        "arxiv_id": "2510.04618",
        "authors": "Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, Kunle Olukotun",
        "summary": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.746741",
        "filter_reason": "这篇论文符合筛选要求，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为ACE（Agentic Context Engineering）的新框架。这个框架的本质不是将LLM应用于某个特定领域，而是提出了一种**通用的方法论**来提升LLM系统的能力。它通过将“上下文”视为一个可以不断进化、积累和精炼策略的“战术手册”，从而让LLM在推理过程中实现自我改进。这直接关联到“增强其逻辑、数学、规划、多步推理等通用能力”以及“自我进化”等核心目标。虽然它不涉及权重更新，但它通过优化模型的输入和运行时记忆，从根本上提升了模型解决问题的通用能力，这是一种新的增强模型推理能力的范式。 2.  **第二步：正面指标** 论文摘要中包含了多个强烈的正面指标： *   **核心概念**: 明确提到 \"Large language model (LLM)\"。 *   **能力方向**: 提到了 \"reasoning\" 和 \"problem-solving\"。 *   **训练方法**: 虽然不是传统训练，但其 \"self-improving\" 和 \"evolving contexts\" 的概念与 \"evolution\" 和 \"self-evolve\" 高度相关。 *   **新兴范式**: 论文的核心就是关于 \"llm-based agents\" 的，并探讨了如何优化 \"agent memory\"。 3.  **第三步：排除标准** 论文没有触及多模态、视觉或模型可靠性（水印、安全）等排除领域。虽然摘要中提到了 \"finance\" 和 \"AppWorld\"（一个应用基准），但它们是作为**评估基准**出现的，用以证明ACE框架的通用性和有效性。论文的焦点是ACE这个框架本身，而不是解决金融问题或开发AppWorld应用。因此，它不属于“将LLM作为工具应用到特定领域”的排除范畴。 4.  **第四步：处理特殊和模糊情况** 这篇论文完美地符合“智能体/工具使用”的特殊情况。它提出的是一种**通用的智能体框架**（ACE），用于优化智能体的记忆和上下文，从而增强其**通用问题解决能力**。它不是“用于化学实验的智能体”，而是一个可以应用于包括金融在内的多个领域的通用方法。其“自我改进”的特性，正是通过优化上下文来提升推理质量，符合保留标准。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种创新的、通用的框架（ACE），通过进化上下文来增强LLM的通用推理和自我改进能力。它是一种方法论层面的创新，旨在提升LLM本身的基础能力，而非将其应用于特定领域。因此，它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    },
    {
        "index": "#126",
        "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning",
        "link": "/arxiv/2510.04573",
        "arxiv_id": "2510.04573",
        "authors": "Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamaki, Navdeep Jaitly, Yi-An Ma, Lianhui Qin",
        "summary": "Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may limit the ability to revisit and refine earlier tokens in a holistic manner, which can also lead to inefficient exploration for diverse solutions. In this paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning framework that unifies the expressiveness of continuous latent representation with the iterative refinement capabilities of latent diffusion models for an existing LLM. We first construct a structured latent reasoning space using a Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of thought tokens, preserving semantic information and interpretability while offering compact but expressive representations. Subsequently, we utilize a latent diffusion model that learns to denoise a block of latent thought tokens with a blockwise bidirectional attention mask, enabling longer horizon and iterative refinement with adaptive test-time compute. This design allows efficient parallel generation of diverse reasoning trajectories, allowing the model to plan and revise the reasoning process holistically. We conduct evaluations on a suite of mathematical reasoning and planning benchmarks. Empirical results show that LaDiR consistently improves accuracy, diversity, and interpretability over existing autoregressive, diffusion-based, and latent reasoning methods, revealing a new paradigm for text reasoning with latent diffusion.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.747070",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出了一种名为LaDiR（Latent Diffusion Reasoner）的**全新推理框架**。其本质并非将LLM应用于某个特定领域，而是旨在**改进LLM自身的推理机制**。论文明确指出了当前主流的自回归解码在推理过程中的局限性（无法全局性地回顾和修正），并提出了一种结合变分自编码器（VAE）和潜在扩散模型的新方法来克服这一局限。这直接属于“改进LLM的基础能力”和“提出新的训练范式/方法论”的范畴，完全符合保留标准。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文高度匹配多个正面指标： *   **核心概念**: 标题和摘要中反复提及 \"Large Language Models (LLMs)\"。 *   **能力方向**: 论文的核心就是 \"Text Reasoning\"，并在 \"mathematical reasoning\" 和 \"planning\" 基准上进行评估，这些都是通用推理能力的核心体现。 *   **新兴范式**: 论文提出的 \"Latent Diffusion Reasoner\" 是一种全新的推理范式，旨在通过迭代优化来增强模型的推理能力，这与思维链（CoT）等方法的探索精神一致，都是对LLM推理能力的根本性增强。 3.  **第三步：排除标准——论文是否聚焦于排除领域？** 该论文完全没有触及任何排除标准。它不涉及多模态、视觉，不针对任何特定应用领域（如医疗、化学），也不讨论模型部署、水印或应用层面的安全性。其焦点始终集中在LLM的文本推理过程本身。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用，也不主要讨论幻觉/可解释性/安全，因此此条不适用。但其通过“blocks of thought tokens”设计来提升“interpretability”（可解释性），可以看作是改进推理过程带来的内在益处，而非独立的研究目标。 5.  **第五步：最终决策** 综合以上分析，这篇论文的**核心贡献是提出一种创新的、通用的方法论（LaDiR框架）来增强大语言模型的基础推理能力**。它解决了现有技术（自回归解码）的一个根本性缺陷，并在通用的数学和规划任务上验证了其有效性。这与您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#141",
        "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs",
        "link": "/arxiv/2510.04140",
        "arxiv_id": "2510.04140",
        "authors": "Zishang Jiang, Jinyi Han, Tingyun Li, Xinyi Wang, Sihang Jiang, Jiaqing Liang, Zhaoqian Dai, Shuguang Ma, Fei Yu, Yanghua Xiao",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely adopted technique for enhancing the reasoning ability of Large Language Models (LLMs). However, the effectiveness of RLVR strongly depends on the capability of base models. This issue arises because it requires the model to have sufficient capability to perform high-quality exploration, which involves both effectiveness and diversity. Unfortunately, existing methods address this issue by imitating expert trajectories, which improve effectiveness but neglect diversity. To address this, we argue that the expert only needs to provide guidance only at critical decision points rather than the entire reasoning path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation for Token-level Optimization of Reasoning, a framework that provides expert guidance only at critical decision points to perform effective and diverse exploration in RLVR. Extensive experiments show that MENTOR enables models capture the essence of expert strategies rather than surface imitation, thereby performing high-quality exploration and achieving superior overall performance. Our code is available online.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.762836",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **核心判断（第一步）：** 论文的核心是提出一种名为MENTOR的新框架，用于改进大语言模型在强化学习（RLVR）训练过程中的探索能力。其直接目标是“enhancing the reasoning ability of Large Language Models (LLMs)”。这属于改进LLM基础能力、提出新训练范式以增强其通用推理能力的范畴，而非将LLM应用于特定领域。因此，根据第一步的核心判断，该论文应被**保留**。 2.  **正面指标（第二步）：** 论文摘要中明确包含了多个关键的正面指标： *   **核心概念:** \"Large Language Models (LLMs)\" *   **能力方向:** \"reasoning ability\", \"reasoning path\" *   **训练方法:** \"Reinforcement Learning (RLVR)\" 这些关键词的出现，进一步加强了其与研究目标的相关性。 3.  **排除标准（第三步）：** 论文的研究焦点完全不涉及任何排除标准中的领域。它没有讨论多模态、视觉，也没有将方法应用于医疗、化学、机器人等特定领域，更不涉及水印、安全等应用层面的可靠性问题。 4.  **特殊和模糊情况（第四步）：** 本文不直接涉及智能体或幻觉等特殊情况的讨论，因此无需进行额外判断。其核心是纯粹地改进推理训练过程本身。 5.  **最终决策（第五步）：** 综合以上分析，这篇论文的本质是提出一种方法论上的创新（MENTOR框架），通过优化强化学习中的探索策略，来提升大语言模型自身的通用推理能力。它直接回应了“如何提高LLM通用推理能力”这一核心问题，与你的研究课题高度契合。因此，最终判断为**True**。"
    },
    {
        "index": "#143",
        "title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning",
        "link": "/arxiv/2510.04072",
        "arxiv_id": "2510.04072",
        "authors": "Ziyan Wang, Zheng Wang, Jie Fu, Xingwei Qu, Qi Cheng, Shengpu Tang, Minjia Zhang, Xiaoming Huo",
        "summary": "Reinforcement learning (RL) has become central to enhancing reasoning in large language models (LLMs). Yet on-policy algorithms such as Group Relative Policy Optimization (GRPO) often suffer in early training: noisy gradients from low-quality rollouts lead to unstable updates and inefficient exploration. We introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient framework to address these limitations via decomposing each step into three stages: a short fast trajectory of inner steps on the same batch, a reposition mechanism to control off-policy drift, and a final slow correction. This reposition-before-update design preserves the objective and rollout process unchanged, making SFPO plug-compatible with existing policy-gradient pipelines. Extensive experiments demonstrate that SFPO consistently improves stability, reduces rollouts, and accelerates convergence of reasoning RL training. Specifically, it outperforms GRPO by up to 2.80 points in average on math reasoning benchmarks. It also achieves up to 4.93\\texttimes{} fewer rollouts and a 4.19\\texttimes{} reduction in wall-clock time to match GRPO's best accuracy.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.769020",
        "filter_reason": "这篇论文完全符合研究范围。 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心贡献是提出了一种名为“Slow-Fast Policy Optimization (SFPO)”的新训练框架。这是一种用于强化学习（RL）的策略优化算法，其直接目标是解决在用RL训练LLM进行推理时遇到的早期训练不稳定、收敛慢等问题。这完全属于“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴。它不是将LLM应用于某个特定领域，而是致力于改进LLM学习推理过程本身的方法论。 2.  **第二步：正面指标——论文高度相关。** 论文摘要中明确包含了多个核心正面指标： *   **核心概念**: \"large language models (LLMs)\" *   **能力方向**: \"reasoning\", \"math reasoning\" *   **训练方法**: \"Reinforcement learning (RL)\", \"Policy Optimization\" 这些关键词的密集出现，清晰地表明了论文的研究焦点。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文内容集中于算法和训练框架的改进，完全没有提及多模态、视觉、医疗、化学、机器人等特定应用领域，也未讨论水印、安全等应用层面的可靠性问题。它使用数学推理作为评估基准，是为了证明其方法的通用有效性，而不是将研究范围限定在数学领域。 4.  **第四步：特殊和模糊情况——不适用。** 该论文不涉及智能体/工具使用，也不涉及幻觉/可解释性等特殊情况的讨论。 **最终决策**: 综合分析，这篇论文的本质是提出一种新的强化学习优化框架（SFPO），以提升大语言模型在通用推理任务（特别是数学推理）上的训练稳定性和效率。这直接对准了“提高大语言模型本身的通用推理能力”这一核心目标，因此应被保留。"
    },
    {
        "index": "#142",
        "title": "Internal states before wait modulate reasoning patterns",
        "link": "/arxiv/2510.04128",
        "arxiv_id": "2510.04128",
        "authors": "Dmitrii Troitskii, Koyena Pal, Chris Wendler, Callum Stuart McDougall, Neel Nanda",
        "summary": "Prior work has shown that a significant driver of performance in reasoning models is their ability to reason and self-correct. A distinctive marker in these reasoning traces is the token wait, which often signals reasoning behavior such as backtracking. Despite being such a complex behavior, little is understood of exactly why models do or do not decide to reason in this particular manner, which limits our understanding of what makes a reasoning model so effective. In this work, we address the question whether model's latents preceding wait tokens contain relevant information for modulating the subsequent reasoning process. We train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent attribution technique in the crosscoder setting. We locate a small set of features relevant for promoting/suppressing wait tokens' probabilities. Finally, through a targeted series of experiments analyzing max activating examples and causal interventions, we show that many of our identified features indeed are relevant for the reasoning process and give rise to different types of reasoning patterns such as restarting from the beginning, recalling prior knowledge, expressing uncertainty, and double-checking.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.768342",
        "filter_reason": "这篇论文完全符合您的筛选标准，其核心贡献在于对大语言模型（LLM）的通用推理能力进行深入的机制性探究。 1.  **第一步：核心判断** 论文的核心并非将LLM作为工具应用于特定领域，而是深入探究LLM在进行推理时的内部工作机制。它聚焦于推理模型中的一个关键行为——“wait” token，并试图揭示模型内部状态（latents）是如何调节后续的推理模式（如回溯、自我修正、再次检查）的。这种对模型基础能力（推理）的内在机理的剖析，正是为了理解“是什么让推理模型如此有效”，这完全属于改进LLM基础能力和增强其通用推理能力的范畴。 2.  **第二步：正面指标** 论文高度符合多个正面指标： *   **核心概念**: 明确以LLM（DeepSeek-R1-Distill-Llama-8B）为研究对象。 *   **能力方向**: 核心主题就是**reasoning**，具体涉及了自我修正、回溯、不确定性表达、再次检查等复杂的推理行为。 *   **新兴范式**: 虽然没有直接提出新的智能体框架，但其研究的“自我修正”和“回溯”是高级推理智能体的核心能力。论文通过分析这些行为，为构建更强大的推理系统提供了理论基础。 3.  **第三步：排除标准** 论文完全没有触及任何排除标准。它不涉及多模态、特定应用领域（如医疗、化学），也不是关于模型基础设施或应用层面的安全水印等。 4.  **第四步：处理特殊和模糊情况** 这篇论文是“模型可解释性”研究的一个绝佳范例，并且完全符合您设定的保留条件。它不是对社会现象的讨论，而是提出了一种新的技术方法（crosscoders和latent attribution）来**增强模型内在的可解释性**。通过因果干预实验，论文证实了其发现的内部特征确实与推理过程相关，这种对内在机制的深刻理解，是未来提升模型**通用可靠性和推理质量**的关键一步。 **最终决策**: 该论文的本质是一项基础性、机制性的研究，它通过分析LLM的内部状态来揭示其通用推理能力的运作原理。它不追求在某个下游任务上获得性能提升，而是致力于回答“LLM为何会推理”这一更根本的问题。这种对底层机制的探索，对于未来设计出推理能力更强的LLM具有至关重要的指导意义，因此与您“提高大语言模型本身的通用推理能力”的核心目标高度一致。应予以保留。"
    },
    {
        "index": "#146",
        "title": "Principled and Tractable RL for Reasoning with Diffusion Language Models",
        "link": "/arxiv/2510.04019",
        "arxiv_id": "2510.04019",
        "authors": "Anthony Zhan",
        "summary": "Diffusion large language models (dLLMs) are a new paradigm of non-autoregressive language models that are trained to predict multiple tokens in parallel and generate text via iterative unmasking. Recent works have successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B scale, but dLLMs have yet to benefit from modern post-training techniques, e.g. reinforcement learning (RL), that have proven effective for autoregressive models. Crucially, algorithms designed for traditional LLMs aren't directly compatible with diffusion frameworks due to inherent differences in modeling assumptions. Moreover, existing attempts at dLLM post-training with RL rely on heuristic-based objectives with no theoretical grounding. In this work, we present Amortized Group Relative Policy Optimization (AGRPO), a principled on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo sampling to compute an unbiased policy gradient estimate, making it the first tractable, faithful adaptation of policy gradient methods for dLLMs. We demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x performance on the Countdown task over the baseline LLaDA-8B-Instruct model and 1.3x performance gains over comparable RL methods such as diffu-GRPO. Furthermore, these gains persist across different numbers of sampling steps at inference time, achieving better tradeoffs between compute and performance. Our results demonstrate that online RL algorithms can be extended to diffusion LLMs in principled ways, maintaining both theoretical soundness and practical effectiveness.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.770111",
        "filter_reason": "这篇论文完全符合研究范围，应予以保留。以下是基于筛选标准的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Amortized Group Relative Policy Optimization (AGRPO)”的新型强化学习算法。该算法专门为扩散语言模型设计，旨在通过后训练来提升其推理能力。这完全符合筛选标准中“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。论文的本质是方法论创新，旨在从基础层面改进LLM（此处特指dLLM）的能力，而非将其作为工具应用于特定领域。 2.  **第二步：正面指标** 论文高度契合多个正面指标： *   **核心概念**: 论文研究对象是“Diffusion large language models (dLLMs)”，属于大语言模型的范畴。 *   **能力方向**: 论文明确在“math/reasoning tasks”上验证其方法，并使用了GSM8K和Countdown这两个经典的数学推理和逻辑推理基准测试。这直接对应了“reasoning (尤其是 math reasoning, logical reasoning)”。 *   **训练方法**: 论文的核心是“reinforcement learning (RL)”，并提出了一种新的RL算法AGRPO，这与筛选标准中的“reinforcement learning (RLHF, RL)”完全一致。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准领域： *   它没有讨论视觉或多模态内容。 *   它的应用场景是通用的数学和逻辑推理任务，而非医疗、化学、机器人等特定领域。 *   它的研究焦点是提升模型性能，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需特殊判断。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种创新的、有理论基础的强化学习方法，用以提升一种新型大语言模型（扩散语言模型）的通用推理能力。其研究目标、方法和评估基准都与“提高大语言模型本身的通用推理能力”这一核心目标高度一致。因此，这是一篇非常相关且高质量的前沿论文，应被纳入筛选范围。"
    },
    {
        "index": "#153",
        "title": "Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration",
        "link": "/arxiv/2510.03865",
        "arxiv_id": "2510.03865",
        "authors": "Wenhao Deng, Long Wei, Chenglei Yu, Tailin Wu",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently enhanced the reasoning capabilities of large language models (LLMs), particularly for mathematical problem solving. However, a fundamental limitation remains: as the sampling budget increases, the advantage of RLVR-trained models over their pretrained bases often diminishes or even vanishes, revealing a strong dependence on the base model's restricted search space. We attribute this phenomenon to the widespread use of the reverse Kullback-Leibler (KL) divergence regularizer, whose mode-seeking behavior keeps the policy trapped inside the base model's support region and hampers wider exploration. To address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an algorithm to promote broader yet focused exploration. Our method (i) utilizes the forward KL penalty to replace the reverse KL penalty for out-of-distribution exploration, and (ii) reweights the reference policy to facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B models with RAPO on the 8K SimpleRL-Zero dataset, without supervised fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO consistently improves problem-solving performance. Notably, RAPO enables models to surpass the base model's performance ceiling and solves previously intractable problems, advancing the frontier of RLVR for challenging reasoning tasks.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.772294",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文本质** 论文的核心是提出一种名为RAPO（Rewards-Aware Policy Optimization）的新算法，这是一种改进的强化学习方法。其直接目标是解决现有RLVR方法在提升LLM推理能力时的局限性（即受限于基础模型的搜索空间），从而“解锁大语言模型的推理能力”。这完全属于“改进LLM的基础能力”和“提出新的训练范式”的范畴，旨在增强模型的通用推理能力，而非将其应用于特定领域。因此，根据第一步，应**保留**。 2.  **第二步：正面指标——论文主题** 论文高度契合多个正面指标： *   **核心概念**: 明确以“Large language models (LLMs)”为研究对象。 *   **能力方向**: 核心主题是“Reasoning Capabilities”，并在摘要中多次提及“mathematical problem solving”和“reasoning tasks”。 *   **训练方法**: 论文的核心贡献是一种新的“Reinforcement Learning”算法，并深入探讨了策略优化和KL散度等技术细节。 这些指标的强匹配进一步确认了论文的相关性。 3.  **第三步：排除标准——论文焦点** 论文完全不涉及任何排除标准领域： *   它没有讨论视觉或多模态内容。 *   它的应用场景是数学推理（AIME竞赛题），这是衡量通用逻辑和推理能力的标准基准，而非医疗、化学等特定应用领域。 *   它不关注水印、安全等模型可靠性问题。 因此，根据第三步，不应排除。 4.  **第四步：处理特殊和模糊情况** 本文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，无需进行特殊判断。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种新的强化学习训练范式（RAPO），以解决LLM在通用推理任务（特别是数学推理）中遇到的一个根本性问题（探索受限）。其研究目标、方法和评估基准都与“提升大语言模型本身的通用推理能力”这一核心目标高度一致。因此，这篇论文是符合你研究范围的高质量前沿论文。"
    },
    {
        "index": "#160",
        "title": "Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning",
        "link": "/arxiv/2510.03669",
        "arxiv_id": "2510.03669",
        "authors": "Wenlong Deng, Yi Ren, Yushu Li, Boying Gong, Danica J. Sutherland, Xiaoxiao Li, Christos Thrampoulidis",
        "summary": "Reinforcement learning with verifiable rewards has significantly advanced the reasoning capabilities of large language models, yet how to explicitly steer training toward exploration or exploitation remains an open problem. We introduce Token Hidden Reward (THR), a token-level metric that quantifies each token's influence on the likelihood of correct responses under Group Relative Policy Optimization (GRPO). We find that training dynamics are dominated by a small subset of tokens with high absolute THR values. Most interestingly, tokens with positive THR strengthen confidence in correct outputs, thus favoring exploitation, while tokens with negative THR preserve probability mass for alternative outputs, enabling exploration. This insight suggests a natural intervention: a THR-guided reweighting algorithm that modulates GRPO's learning signals to explicitly bias training toward exploitation or exploration. We validate the efficacy of this algorithm on diverse math reasoning benchmarks. By amplifying tokens with positive THR value and weakening negative ones, our algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse strategy yields consistent gains in Pass@K accuracy, favoring exploration. We further demonstrate that our algorithm integrates seamlessly with other RL objectives such as GSPO and generalizes across architectures including Llama. These findings establish THR as a principled and fine-grained mechanism for dynamically controlling exploration and exploitation in RL-tuned LLMs, providing new tools for targeted fine-tuning in reasoning-intensive applications.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.785141",
        "filter_reason": "这篇论文完全符合筛选标准，应予以保留。 **第一步：核心判断** 论文的本质是提出一种新的训练方法论来增强大语言模型的核心能力。其核心贡献是“Token Hidden Reward (THR)”，一种在强化学习（RL）训练过程中，用于精细控制模型“探索”与“利用”平衡的机制。这直接属于“改进LLM的基础能力、提出新的训练范式”的范畴。论文的目标是提升LLM的“推理能力”，并通过在数学推理基准上进行验证，这完全符合保留标准。它并非将LLM作为工具应用于特定领域，而是专注于模型本身的训练过程和能力提升。 **第二步：正面指标** 论文命中了多个关键的正面指标： - **核心概念**: 明确以“大语言模型”为研究对象。 - **能力方向**: 核心目标是提升“推理能力”，并具体在“数学推理”基准上进行了验证。 - **训练方法**: 核心技术是“强化学习”，并提出了在“Group Relative Policy Optimization (GRPO)”框架下的新算法。 **第三步：排除标准** 论文内容完全不涉及任何排除标准： - **多模态与视觉**: 论文专注于文本语言模型，未提及视觉或多模态内容。 - **特定应用领域**: 研究是通用的，使用的是数学基准，而非医疗、化学等特定领域的问题。 - **模型可靠性（应用层面）**: 未讨论水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/安全等特殊情况。其研究焦点非常清晰，即通过改进强化学习训练过程来提升模型的内在推理能力。 **第五步：最终决策** 综合以上分析，该论文提出了一种创新的、可操作的强化学习训练机制（THR），旨在解决LLM在推理任务训练中的一个核心问题（探索-利用平衡）。这是一种直接作用于LLM本身、旨在提升其通用推理能力的基础性研究，与研究课题“大语言模型通用推理能力”高度契合。因此，最终判断为符合要求。"
    },
    {
        "index": "#166",
        "title": "PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters",
        "link": "/arxiv/2510.03415",
        "arxiv_id": "2510.03415",
        "authors": "Aditya Thimmaiah, Jiyang Zhang, Jayanth Srinivasa, Junyi Jessy Li, Milos Gligoric",
        "summary": "As large language models (LLMs) excel at code reasoning, a natural question arises: can an LLM execute programs (i.e., act as an interpreter) purely based on a programming language's formal semantics? If so, it will enable rapid prototyping of new programming languages and language features. We study this question using the imperative language IMP (a subset of C), formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). We introduce three evaluation sets-Human-Written, LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by code-complexity metrics spanning the size, control-flow, and data-flow axes. Given a program and its semantics formalized with SOS/K-semantics, models are evaluated on three tasks ranging from coarse to fine: (1) final-state prediction, (2) semantic rule prediction, and (3) execution trace prediction. To distinguish pretraining memorization from semantic competence, we define two nonstandard semantics obtained through systematic mutations of the standard rules. Across strong code/reasoning LLMs, performance drops under nonstandard semantics despite high performance under the standard one. We further find that (i) there are patterns to different model failures, (ii) most reasoning models perform exceptionally well on coarse grained tasks involving reasoning about highly complex programs often containing nested loop depths beyond five, and surprisingly, (iii) providing formal semantics helps on simple programs but often hurts on more complex ones. Overall, the results show a promise that LLMs could serve as programming language interpreters, but points to the lack of their robust semantics understanding. We release the benchmark and the supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.",
        "subjects": "Programming Languages, Artificial Intelligence, Computation and Language, Software Engineering",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.786962",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于深入探究和评估了大语言模型的一项基础且关键的通用推理能力——基于形式化语义的逻辑推理能力。 以下是根据您的筛选标准进行的详细判断过程： **第一步：核心判断——论文的本质是什么？** - **保留**。这篇论文的本质并非将LLM应用于某个特定领域，而是研究LLM本身是否能理解和执行一套**形式化的、逻辑化的规则**（即编程语言的操作语义）。将LLM作为“编程语言解释器”这一任务，本质上是一个纯粹的逻辑推理和多步推理任务。模型需要根据给定的语义规则，一步步推导出程序的最终状态或执行轨迹。这直接触及了LLM的“逻辑、规划、多步推理等通用能力”的核心。论文的核心贡献是提供了一个基准来衡量这种能力的深度和鲁棒性，而不是解决某个外部领域的问题。 **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文标题和摘要明确以 \"Large language models (LLMs)\" 为研究对象。 - **能力方向**: 论文的核心是 \"reasoning\"。它研究的不是宽泛的推理，而是非常具体的 \"code reasoning\" 和 \"semantic understanding\"。预测最终状态、语义规则和执行轨迹都是多步推理的具体体现。 - **训练方法**: 虽然本文没有提出新的训练方法，但它通过精巧的实验设计（如引入非标准语义）揭示了当前训练范式的不足，为未来如何通过新的训练方法（如强化学习、自我进化）来增强模型的鲁棒语义理解指明了方向。 - **新兴范式**: 本文与 \"tool use\" 相关，但其视角是“如何让LLM成为一个通用的工具（解释器）”，而不是“如何使用LLM这个工具”，这符合您对通用能力研究的定义。 **第三步：排除标准——论文是否主要聚焦于以下领域？** - **多模态与视觉**: 完全不涉及。论文研究对象是纯文本的代码和形式化规则。 - **特定应用领域**: 完全不涉及。论文的研究领域是“编程语言理论”，这是计算机科学的基础理论，而非生物、医疗等特定应用领域。其目标“快速原型化新编程语言”也是一个通用的软件工程目标，而非特定领域应用。 - **模型可靠性（应用层面）**: 不涉及。论文虽然探讨了模型的“可靠性”，但是从认知科学的角度——即模型推理的“鲁棒性”，而非水印、安全等应用层面的技术。 **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: 本文对可解释性的探讨非常深入。它通过设计“非标准语义”来区分模型的“预训练记忆”和“真正的语义理解能力”，这是一种探究模型内在推理机制的有效方法。这种对模型内在推理过程的剖析，旨在提升模型的通用推理质量，完全符合“保留”标准。 **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一个评测基准，用于衡量LLM在遵循形式化规则进行逻辑推导这一基础推理任务上的表现。它不仅评估了模型在标准情况下的能力，更通过精妙的实验设计揭示了其在鲁棒性上的缺陷。这项工作直接服务于“提高大语言模型本身的通用推理能力”这一核心目标，因为它为我们理解和改进LLM的逻辑推理内核提供了精确的度量尺和深刻的洞见。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#161",
        "title": "Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders",
        "link": "/arxiv/2510.03659",
        "arxiv_id": "2510.03659",
        "authors": "Xu Wang, Yan Hu, Benyou Wang, Difan Zou",
        "summary": "Sparse Autoencoders (SAEs) are widely used to steer large language models (LLMs), based on the assumption that their interpretable features naturally enable effective model behavior steering. Yet, a fundamental question remains unanswered: does higher interpretability indeed imply better steering utility? To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B, Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels, and evaluate their interpretability and steering utility based on SAEBench (arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis reveals only a relatively weak positive association (tau b approx 0.298), indicating that interpretability is an insufficient proxy for steering performance. We conjecture the interpretability utility gap may stem from the selection of SAE features, as not all of them are equally effective for steering. To further find features that truly steer the behavior of LLMs, we propose a novel selection criterion called Delta Token Confidence, which measures how much amplifying a feature changes the next token distribution. We show that our method improves the steering performance of three LLMs by 52.52 percent compared to the current best output score based criterion (arXiv:2503.34567). Strikingly, after selecting features with high Delta Token Confidence, the correlation between interpretability and utility vanishes (tau b approx 0), and can even become negative. This further highlights the divergence between interpretability and utility for the most effective steering features.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.785467",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质** 论文的核心并非将LLM应用于某个特定领域，而是深入探究LLM的内部工作机制，并提出了一种新的方法来控制和引导模型的行为。它关注的是“如何更有效地操控模型的内部表征（SAE特征）以实现更好的行为输出”，这属于改进LLM基础能力和控制机制的范畴。这与将LLM用作工具解决医疗、化学等具体问题的论文有本质区别，因此应予以保留。 2.  **第二步与第三步：指标与排除项分析** - **正面指标**: 论文明确以“大语言模型”为研究对象。虽然未直接提及“reasoning”或“planning”，但其研究的核心——“steering utility”（引导实用性）和改变“next token distribution”（下一个token分布）——是实现高质量推理和问题解决的根本前提。一个能被精确引导的模型，其产生连贯、逻辑性强的输出的潜力也更大。 - **排除标准**: 论文完全不涉及多模态、视觉，也没有聚焦于任何特定应用领域（如医疗、化学）。同时，它研究的是模型内部机制的可解释性与控制，而非应用层面的水印、安全等问题，因此不触及任何排除标准。 3.  **第四步：处理特殊和模糊情况——可解释性** 这是判断这篇论文的关键。根据筛选标准：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” - 这篇论文完美地符合了这一点。它没有停留在对“可解释性”的哲学讨论或现象分析上，而是**提出了一种名为“Delta Token Confidence”的新方法**。 - 该方法通过分析模型内部特征，旨在**找到真正能有效“引导”模型行为的特征**。这本质上是增强了对模型行为的控制能力，从而提升了模型输出的可靠性和有效性。一个能被更可靠引导的模型，其通用问题解决能力（包括推理）的潜力也随之增强。 - 论文通过实验证明，其方法显著提升了“引导性能”，这直接对应了“提升模型的通用可靠性”的目标。 **核心贡献与最终决策**: 这篇论文的核心贡献是揭示了LLM内部特征“可解释性”与“行为引导实用性”之间的脱节，并提出了一种新的特征选择标准来更有效地控制模型行为。这项研究属于LLM基础能力研究的范畴，它探索了如何通过理解和操控模型的内部表征来增强其可控性和可靠性，这是提升模型通用推理能力的重要基石。因此，它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。"
    },
    {
        "index": "#168",
        "title": "Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning",
        "link": "/arxiv/2510.03394",
        "arxiv_id": "2510.03394",
        "authors": "Donghwan Rho",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training large language models (LLMs) with stronger reasoning abilities. It has also been applied to a variety of logic puzzles. In this work, we study the Korean word-chain game using RLVR. We show that rule-derived rewards can naturally conflict, and demonstrate through experiments that a curriculum-learning scheme mitigates these conflicts. Our findings motivate further studies of puzzle tasks in diverse languages.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.787545",
        "filter_reason": "这篇论文完全符合您的筛选标准，其核心贡献在于提升大语言模型的通用推理能力。 1.  **核心判断 (第一步):** 论文的本质是**改进LLM的训练范式**。它没有将LLM作为工具应用于特定领域，而是聚焦于“可验证奖励强化学习”（RLVR）这一训练方法本身。论文的核心贡献是发现了RLVR在处理复杂规则时存在的“奖励冲突”问题，并提出了一种通用的“课程学习”方案来缓解该问题。这直接属于“提出新的训练范式、增强其逻辑、多步推理等通用能力”的范畴。 2.  **正面指标 (第二步):** 论文命中了多个关键正面指标： *   **核心概念**: 明确提到 \"large language models (LLMs)\"。 *   **能力方向**: 研究目标是 \"stronger reasoning abilities\"，并以 \"logic puzzles\"（逻辑谜题）作为具体任务。 *   **训练方法**: 核心方法是 \"Reinforcement learning with verifiable rewards (RLVR)\"，属于强化学习的一种。 3.  **排除标准 (第三步):** 论文未触及任何排除标准。它不涉及多模态、视觉，也不涉及医疗、化学等特定应用领域。虽然论文研究了“韩语接龙游戏”，但这应被视为一个**逻辑推理任务的基准测试**，而非一个特定应用领域。论文的结论也指出，其发现“激励了对多样化语言中谜题任务的进一步研究”，这表明其目标是探索通用的推理训练方法，而非解决韩语特定问题。 4.  **处理特殊和模糊情况 (第四步):** 此处最关键的模糊点是“韩语接链游戏”是否算特定应用。根据论文摘要，该游戏被用作一个载体来研究和改进RLVR这一通用训练方法。论文的焦点是“奖励冲突”这一训练过程中的普遍性挑战，以及“课程学习”这一通用解决方案。因此，它完全符合“提出一种新方法来提升模型的通用推理能力”的标准，应予以保留。 **最终决策 (第五步):** 综合分析，这篇论文的核心是通过对一个逻辑谜题任务的研究，发现并解决了一种强化学习训练方法（RLVR）中的内在缺陷（奖励冲突），并提出了一种通用的改进方案（课程学习）。这直接服务于“提高大语言模型本身的通用推理能力”这一核心目标。因此，该论文高度相关，应被筛选入内。"
    },
    {
        "index": "#177",
        "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
        "link": "/arxiv/2510.03279",
        "arxiv_id": "2510.03279",
        "authors": "Youjin Wang, Yangjingyi Chen, Jiahao Yan, Jiaxuan Lu, Xiao Sun",
        "summary": "With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.831475",
        "filter_reason": "这篇论文符合研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出了一种新的架构框架 `MemMamba`，用于解决现有高效模型架构（如Mamba）在处理长序列时存在的“长程记忆衰减”这一根本性问题。这并非将LLM作为工具应用于特定领域，而是直接对LLM（或其底层架构）的基础能力进行改进。一个模型能否有效保留和利用长距离信息，是其进行复杂逻辑推理、数学证明、多步规划等高级认知任务的先决条件。因此，这篇论文的本质是**改进LLM的基础能力**，完全符合核心筛选标准。 2.  **第二步：正面指标** - **核心概念**: 论文虽然聚焦于State Space Model (SSM)，但SSM是当前构建大语言模型的前沿和热门架构，与LLMs紧密相关。 - **能力方向**: 论文直接解决了`long-range memory`（长程记忆）问题。虽然摘要未直接使用\"reasoning\"一词，但长程记忆是通用推理能力（尤其是多步推理和复杂问题解决）的基石。无法记住上下文，推理就无从谈起。论文中提到的`Passkey Retrieval`基准测试，正是检验模型在长文本中检索和利用信息能力的关键任务，与推理能力高度相关。 - **训练方法**: 该论文属于架构创新层面，不涉及训练方法，但这不影响其核心价值。 3.  **第三步：排除标准** - **多模态与视觉**: 论文明确聚焦于自然语言处理（NLP）和序列建模，不涉及视觉或多模态内容。 - **特定应用领域**: 尽管摘要开头提到了`bioinformatics`（生物信息学），但这只是为了说明长序列建模的广泛重要性。论文的**核心贡献、方法创新和实验评估**（使用的PG19和Passkey Retrieval等通用NLP基准）都是领域无关的，旨在提升模型本身的能力，而非解决生物信息学问题。 - **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体、工具使用或幻觉等特殊情况，因此无需特殊处理。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于通过架构创新，解决了限制模型长程推理能力的关键瓶颈——记忆衰减。它直接提升了模型处理长序列、保留和利用远距离信息的基础能力，这是实现更强大通用推理能力的必要条件。因此，这篇论文与“提高大语言模型（LLM）本身的『通用推理能力』”这一研究目标高度契合。"
    },
    {
        "index": "#178",
        "title": "General Exploratory Bonus for Optimistic Exploration in RLHF",
        "link": "/arxiv/2510.03269",
        "arxiv_id": "2510.03269",
        "authors": "Wendi Li, Changdae Oh, Yixuan Li",
        "summary": "Optimistic exploration is central to improving sample efficiency in reinforcement learning with human feedback, yet existing exploratory bonus methods to incentivize exploration often fail to realize optimism. We provide a theoretical analysis showing that current formulations, under KL or $\\alpha$-divergence regularization, unintentionally bias exploration toward high-probability regions of the reference model, thereby reinforcing conservative behavior instead of promoting discovery of uncertain regions. To address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle. GEB counteracts divergence-induced bias via reference-dependent reward regulation and unifies prior heuristic bonuses as special cases, while extending naturally across the full $\\alpha$-divergence family. Empirically, GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones. These results demonstrate that GEB offers both a principled and practical solution for optimistic exploration in RLHF.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T22:03:53.831803",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为“通用探索性奖励（GEB）”的新方法，用于改进“人类反馈强化学习（RLHF）”中的探索机制。RLHF是当前提升大语言模型能力（尤其是对齐和遵循复杂指令能力）的关键训练范式。这篇论文并非将LLM应用于某个特定领域，而是直接针对LLM的**训练过程本身**进行优化。通过改进RLHF中的探索效率，论文旨在让模型在训练过程中能更有效地发现高质量的、新颖的回应，这直接关系到模型**基础能力的提升**，包括其解决未知问题和进行更优决策的潜力。因此，它属于“提出新的训练范式，增强其通用能力”的范畴。 2.  **第二步：正面指标** 论文高度符合正面指标： *   **核心概念**: 明确以“大语言模型”为研究对象。 *   **训练方法**: 论文的核心就是关于“强化学习（RLHF）”的改进，这是筛选标准中明确列出的关键方法。 *   **能力方向**: 虽然摘要未直接使用“reasoning”一词，但RLHF的优化目标之一就是提升模型遵循指令、进行规划和解决问题的能力。更有效的探索意味着模型能更好地学习如何生成逻辑连贯、内容丰富的回答，这是通用推理能力的基石。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准： *   它不涉及多模态、视觉。 *   它不针对医疗、化学等任何特定应用领域，其评估是在通用的“对齐任务”上进行的。 *   它不讨论水印、安全等应用层面的可靠性问题，而是聚焦于训练算法的理论和效率。 4.  **第四步：处理特殊和模糊情况** 本论文不属于特殊和模糊情况。它不是关于智能体或工具使用的应用，也不是关于幻觉或安全性的直接研究，而是对底层训练算法（RLHF）的深刻理论改进。 **最终决策**: 这篇论文的本质是**对LLM核心训练技术（RLHF）的理论性改进**，旨在通过优化探索机制来提升模型的学习效率和最终性能。这直接服务于“提高大语言模型本身的通用推理能力”这一核心目标。它提出的方法论具有通用性，可以应用于各种LLM主干模型上，以增强其基础能力。因此，这篇论文是高度相关且前沿的研究，应被筛选入内。"
    },
    {
        "index": "#16",
        "title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents",
        "link": "/arxiv/2510.04695",
        "arxiv_id": "2510.04695",
        "authors": "Yiding Wang, Zhepei Wei, Xinyu Zhu, Yu Meng",
        "summary": "Enabling large language models (LLMs) to utilize search tools offers a promising path to overcoming fundamental limitations such as knowledge cutoffs and hallucinations. Recent work has explored reinforcement learning (RL) for training search-augmented agents that interleave reasoning and retrieval before answering. These approaches usually rely on outcome-based rewards (e.g., exact match), implicitly assuming that optimizing for final answers will also yield effective intermediate search behaviors. Our analysis challenges this assumption: we uncover multiple systematic deficiencies in search that arise under outcome-only training and ultimately degrade final answer quality, including failure to invoke tools, invalid queries, and redundant searches. To address these shortcomings, we introduce DeSA (Decoupling Search-and-Answering), a simple two-stage training framework that explicitly separates search optimization from answer generation. In Stage 1, agents are trained to improve search effectiveness with retrieval recall-based rewards. In Stage 2, outcome rewards are employed to optimize final answer generation. Across seven QA benchmarks, DeSA-trained agents consistently improve search behaviors, delivering substantially higher search recall and answer accuracy than outcome-only baselines. Notably, DeSA outperforms single-stage training approaches that simultaneously optimize recall and outcome rewards, underscoring the necessity of explicitly decoupling the two objectives.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.573485",
        "filter_reason": "这篇论文完全符合你的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM基础能力** 论文的核心贡献是提出了一种名为DeSA的新训练框架，旨在解决LLM智能体在使用搜索工具时出现的系统性缺陷。其本质不是将LLM应用于某个特定领域，而是**改进LLM智能体本身的基础推理和规划能力**。论文指出现有方法（仅优化最终答案）会导致智能体在中间的“搜索”环节表现不佳（如不调用工具、无效查询等），而DeSA通过“解耦搜索和回答”这两个阶段，分别优化搜索行为和答案生成，从而提升了智能体的整体问题解决能力。这是一种对LLM核心推理流程的优化，完全符合你筛选标准的第一步。 2.  **第二步：正面指标——论文高度相关** 论文包含了多个关键的正面指标： *   **核心概念**: 明确以 \"large language models (LLMs)\" 和 \"LLM agents\" 为研究对象。 *   **能力方向**: 论文研究的核心是LLM智能体的 \"reasoning and retrieval\"（推理与检索）过程，这直接关系到通用推理和问题解决能力。 *   **训练方法**: 论文的核心是提出一种新的 \"reinforcement learning (RL)\" 训练范式，并与现有的RL方法进行对比。 *   **新兴范式**: 研究内容聚焦于 \"llm-based agents\" 和 \"tool use\"（搜索工具），旨在提升其通用性能。 3.  **第三步：排除标准——论文不涉及排除领域** 论文的研究内容是纯文本的问答任务，不涉及任何多模态（视觉、视频等）内容。其使用的基准是通用的问答（QA）基准，而非医疗、化学、法律等特定应用领域。论文虽然提到了“幻觉”，但其目的是通过改进工具使用和推理过程来从根本上缓解这一问题，而非研究水印、安全等应用层面的可靠性技术。因此，论文完全避开了所有排除标准。 4.  **第四步：特殊和模糊情况——论文是通用框架的典范** 这篇论文是“智能体/工具使用”这一特殊情况的完美范例。它提出的是一种**通用的智能体训练框架（DeSA）**，用于增强LLM的通用问题解决能力，而不是将其限制在某个特定领域。同时，它对“幻觉”问题的处理方式，也是通过提升模型内在的推理和规划质量来实现的，这属于提升通用推理能力的范畴，因此应该保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种新颖的训练范式（DeSA），通过解耦搜索和回答两个阶段，显著提升了LLM智能体的通用推理、规划和工具使用能力。这直接命中了你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，这篇论文高度相关，应被**保留（True）**。"
    },
    {
        "index": "#21",
        "title": "Making Mathematical Reasoning Adaptive",
        "link": "/arxiv/2510.04617",
        "arxiv_id": "2510.04617",
        "authors": "Zhejian Lai, Xiang Geng, Zhijun Wang, Yang Bai, Jiahuan Li, Rongxiang Weng, Jingang Wang, Xuezhi Cao, Xunliang Cai, Shujian Huang",
        "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. However, existing LLMs exhibit failures of robustness and generalization. This paper attributes these deficiencies to spurious reasoning, i.e., producing answers from superficial features. To address this challenge, we propose the AdaR framework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers. AdaR synthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. To improve data quality, we extract the problem-solving logic from the original query and generate the corresponding answer by code execution, then apply a sanity check. Experimental results demonstrate that AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis indicates that data synthesis and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs. Subsequent analyses derive key design insights into the effect of critical factors and the applicability to instruct LLMs. Our project is available at https://github.com/LaiZhejian/AdaR",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.581051",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一个名为“AdaR”的新框架，其目标是解决大语言模型（LLM）在数学推理任务中存在的“鲁棒性”和“泛化性”不足的根本问题。论文将此问题归因于“虚假推理”，即模型依赖表面特征而非逻辑进行推理。AdaR框架通过一种新的训练范式（数据合成与强化学习结合）来鼓励模型依赖“问题解决逻辑”，从而实现“自适应推理”。这完全符合筛选标准中“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、多步推理等通用能力”的要求。论文的焦点是提升模型**内在的**推理能力，而非将其应用到一个外部特定领域。 2.  **第二步：正面指标** 该论文命中了多个关键的正面指标： *   **核心概念**: 明确以“大语言模型”为研究对象。 *   **能力方向**: 核心主题就是“数学推理”，并深入探讨“逻辑”和“问题解决”。 *   **训练方法**: 明确使用了“强化学习”作为其核心训练技术。 3.  **第三步：排除标准** 该论文完全不涉及任何排除标准中列出的领域。它没有涉及多模态、视觉，没有聚焦于医疗、化学等特定应用领域，也没有讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文处理的“虚假推理”问题，可以看作是对“幻觉”现象在数学领域的具体化分析和针对性解决。论文提出了一种**新的方法论**来减少这种内在缺陷，从而提升模型的推理质量和可靠性，这完全符合“保留那些提出新方法来减少幻觉、增强模型内在可靠性和推理质量”的规则。它不是对现象的讨论，而是对模型能力的直接改进。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种创新的训练框架，旨在从根本上提升大语言模型的数学推理鲁棒性和泛化能力。它直接针对LLM的通用推理能力短板，并提出了具体的、方法论的解决方案。因此，这篇论文与你的研究课题“大语言模型通用推理能力”高度契合，应当被**保留**。"
    },
    {
        "index": "#17",
        "title": "Watch and Learn: Learning to Use Computers from Online Videos",
        "link": "/arxiv/2510.04673",
        "arxiv_id": "2510.04673",
        "authors": "Chan Hee Song, Yiwen Song, Palash Goyal, Yu Su, Oriana Riva, Hamid Palangi, Tomas Pfister",
        "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse, ever-changing applications and environments, but learning is hindered by the scarcity of large-scale, high-quality training data in the target application. Existing datasets are domain-specific, static, and costly to annotate, while current synthetic data generation methods often yield simplistic or misaligned task demonstrations. To address these limitations, we introduce Watch & Learn (W&L), a framework that converts human demonstration videos readily available on the Internet into executable UI trajectories at scale. Instead of directly generating trajectories or relying on ad hoc reasoning heuristics, we cast the problem as an inverse dynamics objective: predicting the user's action from consecutive screen states. This formulation reduces manual engineering, is easier to learn, and generalizes more robustly across applications. Concretely, we develop an inverse dynamics labeling pipeline with task-aware video retrieval, generate over 53k high-quality trajectories from raw web videos, and demonstrate that these trajectories improve CUAs both as in-context demonstrations and as supervised training data. On the challenging OSWorld benchmark, UI trajectories extracted with W&L consistently enhance both general-purpose and state-of-the-art frameworks in-context, and deliver stronger gains for open-source models under supervised training. These results highlight web-scale human demonstration videos as a practical and scalable foundation for advancing CUAs towards real-world deployment.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.573988",
        "filter_reason": "这篇论文完全符合您的筛选标准，应被保留。以下是我的详细判断过程： **第一步：核心判断** - **保留**。这篇论文的本质并非将LLM应用于某个特定垂直领域，而是致力于解决“计算机使用智能体”这一通用智能体范式面临的核心挑战：高质量训练数据的稀缺性。论文提出的“Watch & Learn”框架是一种创新的**训练范式**，它通过将海量的人类演示视频转化为可执行轨迹，来增强LLM的规划和执行能力。使用计算机完成任意任务，本质上是一种复杂的、需要理解和规划的**通用问题解决能力**，这直接契合了您研究的“通用推理能力”核心目标。 **第二步：正面指标分析** - 论文高度符合多个正面指标： - **核心概念**: 论文研究的是“Computer use agents (CUAs)”，其背后驱动模型就是LLMs。 - **能力方向**: CUAs的核心能力就是“plan task workflows”（规划任务工作流），这直接对应了`planning`和`problem-solving`。 - **新兴范式**: 论文明确属于`llm-based agents`和`tool use`（将计算机UI作为一种工具来使用）的研究范畴。 **第三步：排除标准分析** - **多模态与视觉**: 这是本论文最可能引起误判的一点。虽然论文处理了“视频”和“屏幕状态”，看似涉及视觉，但其**研究焦点并非视觉理解本身**（如图像分类、目标检测）。视觉信息在这里是作为智能体进行规划和决策的**输入状态**，论文的核心贡献是“逆向动力学目标”这一将视频状态映射到操作动作的**学习方法**，而不是提升模型的视觉编码能力。因此，它不应被归类于被排除的视觉领域研究。 - **特定应用领域**: 论文的任务是通用的计算机操作，在OSWorld等通用基准上测试，不局限于医疗、化学等任何特定领域。 - **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这篇论文是处理该情况的绝佳范例。它提出的是一种**通用的**智能体训练框架，旨在提升CUA在“多样化、不断变化的应用和环境”中的**通用问题解决能力**。这完全符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”的原则。 **最终决策** 综合以上分析，该论文的核心贡献在于提出了一种新颖的、可扩展的训练方法，用以提升LLM作为智能体在通用计算机使用这一复杂任务上的规划与推理能力。它直接触及了如何让LLM更好地“思考和行动”这一根本性问题，而非将其作为特定领域的应用工具。因此，这篇论文高度符合您“致力于提高大语言模型（LLM）本身的通用推理能力”的研究目标。"
    },
    {
        "index": "#24",
        "title": "COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context",
        "link": "/arxiv/2510.04568",
        "arxiv_id": "2510.04568",
        "authors": "Naman Gupta, Shreeyash Gowaikar, Arun Iyer, Kirankumar Shiragur, Ramakrishna B Bairi, Rishikesh Maurya, Ritabrata Maiti, Sankarshan Damle, Shachee Mishra Gupta",
        "summary": "Reasoning over very long inputs remains difficult for large language models (LLMs). Common workarounds either shrink the input via retrieval (risking missed evidence), enlarge the context window (straining selectivity), or stage multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents, CoA), free-form summaries passed between agents can discard crucial details and amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc messages with a structured memory. A Planner agent first turns a user query into concrete, checkable sub-questions. worker agents process chunks via a fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared memory. A Manager agent then Synthesizes the final answer directly from the memory. This preserves step-wise read-then-reason benefits while changing both the communication medium (structured memory) and the worker procedure (fixed micro-cycle), yielding higher faithfulness, better long-range aggregation, and auditability. On long-context QA from the HELMET suite, COSMIR reduces propagation-stage information loss and improves accuracy over a CoA baseline.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.582404",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。我的判断过程如下： **第一步：核心判断** 论文的核心贡献是提出了一种名为COSMIR的新框架，旨在解决大语言模型（LLM）在长上下文推理中的核心挑战。它并非将LLM作为一种工具应用于特定领域（如医疗、金融），而是聚焦于提升LLM自身的**通用推理能力**，特别是在处理长信息时的迭代、多步推理能力。论文通过对比现有方法（检索、扩大窗口、CoA）的不足，提出了一种新的方法论，这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划、多步推理等通用能力”的保留标准。 **第二步：正面指标** 论文高度符合多个正面指标： - **核心概念**: 明确针对LLM (Large language models)。 - **能力方向**: 核心主题是“推理”，特别是“迭代推理”和“长上下文推理”，这正是通用推理能力的关键组成部分。 - **新兴范式**: 论文深入探讨了“基于LLM的智能体”，并提出了一个包含Planner、Worker和Manager的协作框架，这与“llm-based agents”和“multi-agent systems”等前沿范式高度相关。 **第三步：排除标准** 论文完全避开了所有排除标准。它不涉及多模态与视觉、特定应用领域（如医疗、化学），也不是关于模型基础设施或应用层面的安全水印等。其评估基准是通用的长上下文问答任务，而非领域特定数据集。 **第四步：处理特殊和模糊情况** 在“智能体/工具使用”这一特殊情况下，该论文属于应保留的类型。它提出的是一种**通用的智能体协作框架**（COSMIR），通过结构化内存和固定的微循环来增强LLM的通用问题解决能力，而非将其应用于特定领域（如“用于化学实验自动化的智能体”）。此外，论文中提到的“faithfulness”（忠实度）和“auditability”（可审计性）是其推理框架带来的直接好处，旨在提升推理质量，属于“提升模型的通用可靠性和推理质量”的范畴，应予以保留。 **最终决策** 综上所述，COSMIR论文通过引入结构化内存和固定的智能体微循环，直接致力于提升LLM在复杂、长上下文任务中的多步推理能力，这与“提高大语言模型通用推理能力”的研究目标高度一致。因此，最终判断为保留。"
    },
    {
        "index": "#12",
        "title": "Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning",
        "link": "/arxiv/2510.04817",
        "arxiv_id": "2510.04817",
        "authors": "Abhinav Madahar",
        "summary": "Controllers for structured LM reasoning (e.g., Chain-of-Thought, self-consistency, and Tree-of-Thoughts) often entangle what to try next with how to execute it, exposing only coarse global knobs and yielding brittle, compute-inefficient, and hard-to-audit behavior. We introduce Natural Language Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form natural-language directive to each search edge and translates it into a schema-bounded control vector for decoding, search (branch quotas, exploration $\\beta$), generation bundle size, retrieval mixtures, and verification passes. A labeller $\\Lambda$ emits labels from the parent state and a compact context; a tuner $\\Psi$ maps $(P, L, C)\\to \\Pi$, with strict schema validation and trust-region projection around safe defaults. Downstream selection remains ToT-style with score $S=\\mu+\\beta\\sigma$ and depth-annealed $\\beta$. We show NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for top-$k$ selection under label-conditioned bundles, and bound selector shortfall by control-vector distortion, providing decision-relevant justification for guards like trust regions and verification passes. We instantiate $\\Psi$ as a prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH (subset), StrategyQA, and ARC-Challenge with compute-aware reporting (success@compute, tokens-per-success) and ablations over $\\Lambda$, $\\Psi$, trust-region radius, and control quantization; preregistered forecasts anticipate accuracy gains at comparable token budgets and improved success@compute under constraints. NLEL offers an interpretable, model-agnostic interface that separates intent from execution for controllable, auditable LM inference.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.571597",
        "filter_reason": "这篇论文完全符合筛选标准，是一篇致力于提升大语言模型通用推理能力的前沿研究。 **第一步：核心判断** 论文的核心贡献是提出了一种名为“自然语言边缘标签”的新范式，用于改进结构化LM推理（如思维链CoT、思维树ToT）。其本质是**改进LLM的基础推理能力**，通过将“意图”（下一步做什么）与“执行”（如何做，如解码参数、搜索策略）解耦，提供了一种更精细、可控、高效的推理控制框架。这并非将LLM应用于特定领域，而是对LLM推理过程本身的方法论革新，因此应**保留**。 **第二步：正面指标** 论文高度符合多个正面指标： - **核心概念**: 论文聚焦于“structured LM reasoning”，直接关联LLMs。 - **能力方向**: 论文的核心是提升“reasoning”能力，并在GSM8K、MATH、StrategyQA等经典的数学和逻辑推理基准上进行评估。 - **新兴范式**: 论文对Chain-of-Thought (CoT) 和 Tree-of-Thoughts (ToT) 等现有推理范式进行了泛化和改进，提出了一种更底层的控制机制。同时，它涉及“tool use”（检索、验证）来增强推理过程。 **第三步：排除标准** 论文完全避开了所有排除标准： - **多模态与视觉**: 论文仅涉及自然语言，不涉及任何视觉或多模态内容。 - **特定应用领域**: 评估基准是通用的数学和逻辑推理数据集，而非医疗、化学、机器人等特定领域。 - **模型可靠性（应用层面）**: 论文虽然提到了“auditable”（可审计），但这是通过改进推理框架本身实现的，旨在提升推理过程的透明度和可控性，而非研究水印或安全等应用层面的技术。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的NLEL框架是一种**通用的**控制方法，可以指导模型在推理过程中使用工具（如检索、验证）。这符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的保留条件。 - **幻觉/可解释性/安全**: 论文通过将推理过程解耦并使用自然语言标签，极大地增强了推理过程的“可解释性”和“可审计性”，这直接有助于提升模型的通用推理质量和可靠性，因此符合保留条件。 **第五步：最终决策** 综合以上分析，该论文提出了一种创新的、模型无关的框架（NLEL），旨在从根本上增强和控制LLM的结构化推理过程。它直接针对通用推理能力的核心机制，并在标准推理任务上验证其有效性。因此，这篇论文与“提高大语言模型本身的通用推理能力”这一核心目标高度契合，应被筛选入内。"
    },
    {
        "index": "#27",
        "title": "Code World Models for General Game Playing",
        "link": "/arxiv/2510.04542",
        "arxiv_id": "2510.04542",
        "authors": "Wolfgang Lehrach, Daniel Hennes, Miguel Lazaro-Gredilla, Xinghua Lou, Carter Wendelken, Zun Li, Antoine Dedieu, Jordi Grau-Moya, Marc Lanctot, Atil Iscen, John Schultz, Marcus Chiam, Ian Gemp, Piotr Zielinski, Satinder Singh, Kevin P. Murphy",
        "summary": "Large Language Models (LLMs) reasoning abilities are increasingly being applied to classical board and card games, but the dominant approach -- involving prompting for direct move generation -- has significant drawbacks. It relies on the model's implicit fragile pattern-matching capabilities, leading to frequent illegal moves and strategically shallow play. Here we introduce an alternative approach: We use the LLM to translate natural language rules and game trajectories into a formal, executable world model represented as Python code. This generated model -- comprising functions for state transition, legal move enumeration, and termination checks -- serves as a verifiable simulation engine for high-performance planning algorithms like Monte Carlo tree search (MCTS). In addition, we prompt the LLM to generate heuristic value functions (to make MCTS more efficient), and inference functions (to estimate hidden states in imperfect information games). Our method offers three distinct advantages compared to directly using the LLM as a policy: (1) Verifiability: The generated CWM serves as a formal specification of the game's rules, allowing planners to algorithmically enumerate valid actions and avoid illegal moves, contingent on the correctness of the synthesized model; (2) Strategic Depth: We combine LLM semantic understanding with the deep search power of classical planners; and (3) Generalization: We direct the LLM to focus on the meta-task of data-to-code translation, enabling it to adapt to new games more easily. We evaluate our agent on 10 different games, of which 4 are novel and created for this paper. 5 of the games are fully observed (perfect information), and 5 are partially observed (imperfect information). We find that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10 considered games.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.584066",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献是提出了一种新颖的范式来增强大语言模型的通用推理能力。 **判断过程分析如下:** 1.  **第一步：核心判断——论文的本质是什么？** - **核心贡献**: 这篇论文的本质不是让LLM去玩游戏，而是提出了一种新的方法论来**提升LLM在逻辑、规划和多步推理任务上的能力**。它指出了当前LLM在推理任务（如下棋）中的核心缺陷：依赖脆弱的模式匹配，导致非法操作和策略肤浅。 - **创新范式**: 论文提出的解决方案是让LLM执行一个更高阶的元任务——**将自然语言规则和游戏轨迹翻译成形式化的、可执行的代码世界模型**。这个模型随后作为经典规划算法（如MCTS）的“大脑”进行深度搜索和规划。 - **结论**: 这不是将LLM作为工具应用于特定领域，而是**从根本上改进LLM的推理机制**，通过将其能力从“直接生成答案”转向“构建可验证的推理环境”，从而增强了其逻辑严谨性和规划深度。这完全符合“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的要求。因此，应**保留**。 2.  **第二步：正面指标——论文是否包含相关主题？** - 论文摘要中明确包含了多个核心正面指标： - **核心概念**: \"Large Language Models (LLMs)\" - **能力方向**: \"reasoning abilities\", \"planning algorithms\", \"problem-solving\" - **新兴范式**: \"llm-based agents\", \"tool use\" (生成的代码世界模型和MCTS规划器可以被看作是LLM创建和使用的工具) - 这些指标高度集中在您的研究范围内。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** - 论文完全不涉及多模态、视觉、医疗、化学等特定应用领域，也不关注模型基础设施或应用层面的水印、安全等问题。因此，不触发任何排除标准。 4.  **第四步：处理特殊和模糊情况——智能体/工具使用** - 该论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的**绝佳范例**。 - 它构建了一个通用智能体框架：LLM负责“理解与翻译”（构建世界模型），而经典算法负责“规划与搜索”。这种分工合作的方式，正是为了提升模型在**通用**问题（这里是“通用游戏”）上的解决能力，而不是针对某个特定领域。因此，完全符合保留条件。 **最终决策:** 综合以上分析，这篇论文的核心是提出一种创新的、基于“世界模型构建”的训练/推理范式，旨在克服LLM在逻辑和规划任务中的内在缺陷，从而系统性地提升其通用推理能力。其方法论具有通用性，研究目标与您的课题高度一致。因此，这篇论文是您应该重点关注的、完全符合筛选标准的前沿研究。"
    },
    {
        "index": "#32",
        "title": "Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning",
        "link": "/arxiv/2510.04488",
        "arxiv_id": "2510.04488",
        "authors": "Edward Y. Chang, Ethan Y. Chang",
        "summary": "Multi-agent debate often wastes compute by using a fixed adversarial stance, aggregating without deliberation, or stopping on heuristics. We introduce MACI, an active controller with two independent dials that decouple information from behavior: an information dial that gates evidence by quality, and a behavior dial that schedules contentiousness from exploration to consolidation. A moderator tracks disagreement, overlap, evidence quality, and argument quality, and halts when gains plateau. We provide theory-lite guarantees for nonincreasing dispersion and provable termination, with a budget-feasible scheduler. Across clinical diagnosis and news-bias tasks, MACI improves accuracy and calibration while reducing tokens, and converts residual uncertainty into precision RAG plans that specify what to retrieve next. We use a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal, validated for order invariance and judge-swap stability; stability depends on using high-capability judges. MACI turns debate into a budget-aware, measurable, and provably terminating controller.",
        "subjects": "Artificial Intelligence, Information Theory",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.591640",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： **第一步：核心判断** 这篇论文的本质是提出一种名为MACI（Multi-Agent Collaborative Intelligence）的新颖框架，用于控制和优化多智能体辩论过程，从而提升大语言模型（LLM）的推理可靠性。其核心贡献并非将LLM应用于某个特定领域，而是提出了一种通用的、可提升LLM推理质量的方法论。这直接命中了“改进LLM的基础能力”、“增强其逻辑、多步推理等通用能力”的核心目标。因此，应予以保留。 **第二步：正面指标** 论文高度符合多个正面指标： - **核心概念**: 论文标题和摘要中明确提到了 \"LLM Reasoning\"。 - **能力方向**: 核心研究内容就是 \"reasoning\"，旨在使其更 \"reliable\"。 - **新兴范式**: 论文的核心是 \"Multi-Agent\" 协作框架，这属于当前提升LLM能力的前沿范式。 **第三步：排除标准** 论文虽然提到了 \"clinical diagnosis\" 和 \"news-bias\" 任务，但这并不构成排除理由。关键在于区分“应用领域”和“评估基准”。这篇论文的**核心贡献是MACI框架本身**，而不是一个用于临床诊断的新方法。作者使用这两个任务作为**具有挑战性的测试基准**，来证明其通用框架在复杂推理场景下的有效性。论文的焦点是方法论，而非特定领域的应用。因此，它不属于“主要聚焦于特定应用领域”的排除范畴。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的MACI框架，正是“一种通用的智能体协作框架”的典型范例。它通过引入“信息拨盘”和“行为拨盘”来智能地协调多个智能体，目标是增强LLM的通用问题解决和推理能力，而非局限于某个特定领域。这完全符合保留条件。 **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种创新的、通用的多智能体协作控制框架（MACI），旨在直接提升大语言模型的推理可靠性、效率和可控性。它虽然使用特定领域的任务进行评估，但其方法论本身是领域无关的，致力于解决LLM通用推理的根本性问题。因此，这篇论文与您“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度契合，应被筛选保留。"
    },
    {
        "index": "#26",
        "title": "TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use",
        "link": "/arxiv/2510.04550",
        "arxiv_id": "2510.04550",
        "authors": "Pengfei He, Zhenwei Dai, Bing He, Hui Liu, Xianfeng Tang, Hanqing Lu, Juanhui Li, Jiayuan Ding, Subhabrata Mukherjee, Suhang Wang, Yue Xing, Jiliang Tang, Benoit Dumoulin",
        "summary": "Large language model (LLM)-based agents increasingly rely on tool use to complete real-world tasks. While existing works evaluate the LLMs' tool use capability, they largely focus on the final answers yet overlook the detailed tool usage trajectory, i.e., whether tools are selected, parameterized, and ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to comprehensively evaluate LLMs' tool use capability through diverse tasks with fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable tools across practical domains with tasks grounded in production-style APIs, and synthesizes trajectories that vary in breadth (parallel calls) and depth (interdependent chains). Besides final accuracy, TRAJECT-Bench also reports trajectory-level diagnostics, including tool selection and argument correctness, and dependency/order satisfaction. Analyses reveal failure modes such as similar tool confusion and parameter-blind selection, and scaling behavior with tool diversity and trajectory length where the bottleneck of transiting from short to mid-length trajectories is revealed, offering actionable guidance for LLMs' tool use.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.583431",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断** - 论文的核心贡献是提出了一个名为TRAJECT-Bench的**基准**，用于评估LLM的**智能体工具使用能力**。 - 虽然论文本身不是提出一种新的训练方法来直接提升模型能力，但它聚焦于评估和诊断LLM的一项核心通用能力——**工具使用**。工具使用是LLM实现复杂规划、多步推理和解决开放问题的关键途径。 - 更重要的是，该基准的最终目标是“为LLM的工具使用提供可操作的指导”，这意味着它的存在是为了**发现现有模型的不足，从而指明未来如何改进**。这种对核心能力的深度诊断和评估，是推动该领域能力提升不可或缺的一环，其本质与“致力于提高LLM通用推理能力”的目标高度一致。 - 因此，这篇论文不是将LLM作为工具应用于特定领域，而是深入研究LLM本身的一项基础能力，应予以保留。 2.  **第二步：正面指标** - 论文明确包含了核心概念 **Large language models (LLMs)**。 - 论文聚焦于 **agentic tool use**，这直接关联到 **planning** 和 **problem-solving** 等通用能力方向。 - 论文提出的基准通过分析工具选择的“轨迹”，深入评估了模型的多步推理过程。 3.  **第三步：排除标准** - 论文不涉及多模态、视觉等排除领域。 - 论文虽然提到“practical domains”，但其目的是为了构建一个**通用且多样化**的基准来评估工具使用这一**通用能力**，而非解决某个特定领域（如化学、医疗）的问题。因此，它不属于“特定应用领域”的排除范畴。 - 论文不涉及模型基础设施或应用层面的安全、水印等问题。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这篇论文是“提出一种通用的...方法来增强LLM的通用问题解决能力”的典型例子。它提出的基准是通用的，不局限于任何特定领域，其目的是通过精细化的评估来推动LLM工具使用这一通用能力的进步。这完全符合保留条件。 **最终决策**: 综合以上分析，这篇论文虽然是一个评估基准，但其研究对象是LLM的通用推理能力的关键组成部分（工具使用），其研究目的是为了诊断和指导该能力的提升。它不涉及任何特定应用领域，完全符合你筛选“致力于提高大语言模型（LLM）本身的『通用推理能力』”论文的核心目标。因此，应判定为符合要求。"
    },
    {
        "index": "#29",
        "title": "Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph",
        "link": "/arxiv/2510.04520",
        "arxiv_id": "2510.04520",
        "authors": "Hanyu Wang, Ruohan Xie, Yutong Wang, Guoxiong Gao, Xintao Yu, Bin Dong",
        "summary": "Accurate auto-formalization of theorem statements is essential for advancing automated discovery and verification of research-level mathematics, yet remains a major bottleneck for LLMs due to hallucinations, semantic mismatches, and their inability to synthesize new definitions. To tackle these issues, we present Aria (Agent for Retrieval and Iterative Autoformalization), a system for conjecture-level formalization in Lean that emulates human expert reasoning via a two-phase Graph-of-Thought process: recursively decomposing statements into a dependency graph and then constructing formalizations from grounded concepts. To ensure semantic correctness, we introduce AriaScorer, a checker that retrieves definitions from Mathlib for term-level grounding, enabling rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy, surpassing previous methods. On FATE-X, a suite of challenging algebra problems from research literature, it outperforms the best baseline with 44.0% vs. 24.0% final accuracy. On a dataset of homological conjectures, Aria reaches 42.9% final accuracy while all other models score 0%.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.590171",
        "filter_reason": "这篇论文符合筛选标准，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一个名为Aria的智能体系统，用于解决LLM在“自动形式化”任务上的瓶颈。自动形式化是将自然语言描述的数学定理精确转换为形式化语言（如Lean）的过程，这本身就是一项极其复杂的**数学和逻辑推理任务**。论文并非简单地将LLM应用于某个下游领域，而是针对LLM在执行高级推理任务时出现的“幻觉”、“语义不匹配”等根本性问题，提出了一种新的方法论。因此，论文的本质是**改进LLM的基础推理能力**，符合保留标准。 2.  **第二步：正面指标** 该论文命中了多个关键的正面指标： *   **能力方向**: 论文明确聚焦于**数学推理**和**问题解决**（problem-solving），特别是处理研究级数学定理的复杂逻辑。 *   **新兴范式**: 论文提出了一种基于LLM的**智能体**，并采用了“思维图”的推理范式。同时，它引入了检索工具（`AriaScorer`从Mathlib检索定义）来增强能力，这完全符合**工具使用**的范式。 *   **核心概念**: 虽然摘要未直接重复“LLM”，但其方法论（自动形式化、思维图）和解决的问题（幻觉）都明确指向其研究对象是大语言模型。 3.  **第三步：排除标准** *   **特定应用领域**: 论文的应用场景是数学。然而，数学推理是衡量和提升LLM**通用推理能力**的核心基准领域，而非像医疗、金融那样的特定垂直应用领域。因此，这不应被视为排除项。研究数学推理方法，其目的往往是提升通用能力，而非仅服务于数学家。 *   **多模态与视觉**: 论文未涉及。 *   **模型可靠性**: 论文涉及了“幻觉”和“语义正确性”，但并非应用层面的水印或安全讨论，而是作为提升推理质量的核心技术挑战来处理。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 论文提出的Aria智能体及其依赖图分解方法，是一种通用的、用于处理复杂结构化问题的推理框架。虽然当前应用于数学形式化，但其方法论（递归分解、依赖验证）具有向其他逻辑推理和规划任务迁移的潜力，旨在增强LLM的**通用问题解决能力**。因此，应予以保留。 *   **幻觉/可解释性/安全**: 论文引入的`AriaScorer`检查器，通过检索定义进行术语级别的 grounding，这是一种**全新的、旨在减少幻觉、提升模型内在推理可靠性的技术方法**。它直接提升了模型输出在逻辑上的正确性，完全符合保留标准。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种新颖的智能体框架（Aria）和思维图范式，通过迭代分解和工具验证，显著提升了LLM在极具挑战性的数学形式化任务上的推理准确性和可靠性。这直接服务于“提高大语言模型通用推理能力”这一核心研究目标，因此应被判定为符合要求。"
    },
    {
        "index": "#35",
        "title": "Utility-Learning Tension in Self-Modifying Agents",
        "link": "/arxiv/2510.04399",
        "arxiv_id": "2510.04399",
        "authors": "Charles L. Wang, Keir Dorchen, Peter Jin",
        "summary": "As systems trend toward superintelligence, a natural modeling premise is that agents can self-improve along every facet of their own design. We formalize this with a five-axis decomposition and a decision layer, separating incentives from learning behavior and analyzing axes in isolation. Our central result identifies and introduces a sharp utility--learning tension, the structural conflict in self-modifying systems whereby utility-driven changes that improve immediate or expected performance can also erode the statistical preconditions for reliable learning and generalization. Our findings show that distribution-free guarantees are preserved iff the policy-reachable model family is uniformly capacity-bounded; when capacity can grow without limit, utility-rational self-changes can render learnable tasks unlearnable. Under standard assumptions common in practice, these axes reduce to the same capacity criterion, yielding a single boundary for safe self-modification. Numerical experiments across several axes validate the theory by comparing destructive utility policies against our proposed two-gate policies that preserve learnability.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.593062",
        "filter_reason": "这篇论文完全符合你的筛选标准，是一篇关于提升大语言模型（或更广义的智能体）通用推理能力的基础性前沿研究。 **判断过程如下:** 1.  **第一步：核心判断** - 论文的核心是关于“自修改智能体”的理论研究。它探讨了一个智能体在进行自我改进、自我进化时，如何避免破坏自身未来的学习和泛化能力。这直接触及了提升智能体基础能力的根本性问题——如何实现可持续的、安全的自我提升，而不是一次性或特定任务的优化。这完全属于“改进LLM的基础能力”和“自我进化”的范畴，而不是将LLM作为工具应用于特定领域。因此，论文通过了第一步的核心判断，应予以**保留**。 2.  **第二步：正面指标** - 论文摘要中虽然没有直接出现“LLM”一词，但其核心研究对象“自修改智能体”是当前LLM Agent研究的前沿和核心。论文探讨的“自我改进”、“泛化”、“学习”等概念，是实现高级通用推理能力的先决条件。论文提出的“自我进化”范式，与你筛选标准中的新兴范式高度契合。 3.  **第三步：排除标准** - 论文不涉及多模态、视觉、特定应用领域（如医疗、化学），也不是关于模型部署优化或应用层面的安全（如水印）。它完全避开了所有排除标准。 4.  **第四步：处理特殊和模糊情况** - **智能体/自我进化**: 这篇论文是提出一个**通用的理论框架**来分析自修改智能体，其目标是确保智能体在追求效用最大化（解决通用问题）的同时，不损害其学习能力。这正是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的体现，只不过它更偏向底层的理论保证。因此，应该**保留**。 - **安全**: 论文讨论的“安全的自我修改”，是指保证智能体“学习能力的可靠性”，这是一种非常基础的、模型内在的安全性和可靠性。它通过确保“可学习性”来保障模型长期的推理和问题解决质量，而不是讨论防止生成有害内容等应用层安全。因此，这符合“提升模型的通用可靠性和推理质量”的保留标准。 **最终决策:** 这篇论文的核心贡献是形式化并分析了自修改智能体中的一个根本性冲突——“效用-学习张力”。它为未来能够持续自我进化、不断提升通用问题解决能力的LLM Agent提供了重要的理论基础和安全边界。这项研究虽然理论性很强，但它直接指向了如何构建能够超越当前限制、实现更高级通用推理能力的自主智能系统这一终极目标。因此，它与你的研究课题“大语言模型通用推理能力”高度相关，是一篇极具价值的筛选对象。"
    },
    {
        "index": "#34",
        "title": "DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization",
        "link": "/arxiv/2510.04474",
        "arxiv_id": "2510.04474",
        "authors": "Gang Li, Yan Chen, Ming Lin, Tianbao Yang",
        "summary": "Recent large reasoning models (LRMs) driven by reinforcement learning algorithms (e.g., GRPO) have achieved remarkable performance on challenging reasoning tasks. However, these models suffer from overthinking, generating unnecessarily long and redundant reasoning even for simple questions, which substantially increases computational cost and response latency. While existing methods incorporate length rewards to GRPO to promote concise reasoning, they incur significant performance degradation. We identify the root cause: when rewards for correct but long rollouts are penalized, GRPO's group-relative advantage function can assign them negative advantages, actively discouraging valid reasoning. To overcome this, we propose Decoupled Reward Policy Optimization (DRPO), a novel framework that decouples the length-based learning signal of correct rollouts from incorrect ones. DRPO ensures that reward signals for correct rollouts are normalized solely within the positive group, shielding them from interference by negative samples. The DRPO's objective is grounded in integrating an optimized positive data distribution, which maximizes length-based rewards under a KL regularization, into a discriminative objective. We derive a closed-form solution for this distribution, enabling efficient computation of the objective and its gradients using only on-policy data and importance weighting. Of independent interest, this formulation is general and can incorporate other preference rewards of positive data beyond length. Experiments on mathematical reasoning tasks demonstrate DRPO's significant superiority over six efficient reasoning baselines. Notably, with a 1.5B model, our method achieves 77\\% length reduction with only 1.1\\% performance loss on simple questions like GSM8k dataset, while the follow-up baseline sacrifices 4.3\\% for 68\\% length reduction.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.592602",
        "filter_reason": "这篇论文完全符合你的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础推理能力。** 论文的核心贡献是提出了一种名为“解耦奖励策略优化（DRPO）”的新颖强化学习框架。其目标是解决现有大型推理模型（LRMs）在强化学习训练后出现的“过度思考”问题——即生成冗长、不必要的推理链。这直接触及了大语言模型推理能力的核心：不仅要求推理正确，还要求推理高效、简洁。DRPO通过改进强化学习算法本身，来优化模型的推理行为，这属于“提出新的训练范式”和“增强其逻辑、数学、多步推理等通用能力”的范畴。它并非将LLM作为工具应用于特定领域，而是专注于提升模型内在的推理过程。 2.  **第二步：正面指标——论文高度相关。** 论文命中了多个关键的正面指标： *   **核心概念**: 论文研究对象是“大型推理模型”，这是大语言模型（LLMs）在推理任务上的具体体现。 *   **能力方向**: 论文的核心主题是“reasoning”，特别是在“mathematical reasoning”任务上进行验证，旨在提升推理的效率和简洁性。 *   **训练方法**: 论文的核心贡献是一种新的“reinforcement learning (RL)”方法，即DRPO，这是对现有GRPO等RL算法的改进。 3.  **第三步：排除标准——论文不触及任何排除领域。** 论文的研究内容与所有排除标准均无关系。它不涉及多模态、视觉，也不是针对医疗、化学等特定应用领域的研究。虽然实验在数学数据集上进行，但其方法（DRPO）是通用的，旨在解决一个普遍存在的推理效率问题，而非解决数学领域的特定问题。论文也未讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况——不适用。** 该论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需进行特殊判断。 **最终决策**: 综合以上分析，这篇论文的本质是提出一种新的强化学习训练范式（DRPO），用以解决大语言模型在推理过程中的“过度思考”问题，从而提升其推理的效率和简洁性。这完全契合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，这篇论文是高度相关且应被筛选入的前沿研究。"
    },
    {
        "index": "#40",
        "title": "On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems",
        "link": "/arxiv/2510.04311",
        "arxiv_id": "2510.04311",
        "authors": "Bohan Tang, Huidong Liang, Keyue Jiang, Xiaowen Dong",
        "summary": "Large language model multi-agent systems (LLM-MAS) offer a promising paradigm for harnessing collective intelligence to achieve more advanced forms of AI behaviour. While recent studies suggest that LLM-MAS can outperform LLM single-agent systems (LLM-SAS) on certain tasks, the lack of systematic experimental designs limits the strength and generality of these conclusions. We argue that a principled understanding of task complexity, such as the degree of sequential reasoning required and the breadth of capabilities involved, is essential for assessing the effectiveness of LLM-MAS in task solving. To this end, we propose a theoretical framework characterising tasks along two dimensions: depth, representing reasoning length, and width, representing capability diversity. We theoretically examine a representative class of LLM-MAS, namely the multi-agent debate system, and empirically evaluate its performance in both discriminative and generative tasks with varying depth and width. Theoretical and empirical results show that the benefit of LLM-MAS over LLM-SAS increases with both task depth and width, and the effect is more pronounced with respect to depth. This clarifies when LLM-MAS are beneficial and provides a principled foundation for designing future LLM-MAS methods and benchmarks.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.600654",
        "filter_reason": "根据您提供的筛选标准，这篇论文完全符合研究范围，应当保留。我的判断过程如下： **第一步：核心判断** 这篇论文的本质并非将LLM应用于特定领域，而是对一种旨在提升LLM能力的前沿范式——**大语言模型多智能体系统（LLM-MAS）**——进行深入的理论和实证分析。其核心贡献是提出了一个关于任务复杂性的理论框架（深度和宽度），并用它来解释和评估LLM-MAS在何种条件下能够超越单智能体系统。这直接关联到“改进LLM的基础能力”和“增强其逻辑、规划、多步推理等通用能力”的目标。论文旨在为“设计未来LLM-MAS方法”提供“基础性原则”，这属于方法论层面的基础研究，而非应用层面的工具使用。因此，根据第一步标准，应**保留**。 **第二步：正面指标** 论文命中了多个关键的正面指标： - **核心概念**: 明确以“Large language model multi-agent systems (LLM-MAS)”为核心研究对象。 - **能力方向**: 直接探讨“sequential reasoning”（序列推理）和“reasoning length”（推理长度），这与“reasoning”和“problem-solving”高度相关。 - **新兴范式**: 完全聚焦于“llm-based agents”和“multi-agent systems”这一前沿范式。 **第三步：排除标准** 论文内容完全不涉及任何排除标准领域： - 未涉及多模态、视觉等。 - 未聚焦于医疗、化学、机器人等任何特定应用领域。 - 未讨论水印、安全等模型可靠性问题。 **第四步：处理特殊和模糊情况** 论文完美地符合“智能体/工具使用”的保留规则。它研究的是一种**通用的智能体协作框架（多智能体辩论系统）**，其目的是为了增强LLM在**通用任务**上的问题解决能力，特别是当任务需要更长的推理链条（深度）和更多样化的能力（宽度）时。这与“用于化学实验自动化的智能体”等特定领域应用有本质区别。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于为“LLM-MAS如何以及何时能提升LLM的通用推理能力”这一关键科学问题提供了理论基石和实证依据。它虽然不是提出一种全新的训练算法，但它对现有的一种重要增强范式（LLM-MAS）进行了深刻的剖析，为该领域未来的发展指明了方向。这种对方法论本身的基础性研究，对于推动“大语言模型通用推理能力”的进步至关重要。因此，这篇论文与研究课题高度契合，应被筛选出来。"
    },
    {
        "index": "#38",
        "title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation",
        "link": "/arxiv/2510.04373",
        "arxiv_id": "2510.04373",
        "authors": "Hadi Nekoei, Aman Jaiswal, Patrice Bechard, Oleh Shliazhko, Orlando Marquez Ayala, Mathieu Reymond, Massimo Caccia, Alexandre Drouin, Sarath Chandar, Alexandre Lacoste",
        "summary": "Large language model (LLM) agents perform well in sequential decision-making tasks, but improving them on unfamiliar domains often requires costly online interactions or fine-tuning on large expert datasets. These strategies are impractical for closed-source models and expensive for open-source ones, with risks of catastrophic forgetting. Offline trajectories offer reusable knowledge, yet demonstration-based methods struggle because raw traces are long, noisy, and tied to specific tasks. We present Just-in-time Episodic Feedback Hinter (JEF Hinter), an agentic system that distills offline traces into compact, context-aware hints. A zooming mechanism highlights decisive steps in long trajectories, capturing both strategies and pitfalls. Unlike prior methods, JEF Hinter leverages both successful and failed trajectories, extracting guidance even when only failure data is available, while supporting parallelized hint generation and benchmark-independent prompting. At inference, a retriever selects relevant hints for the current state, providing targeted guidance with transparency and traceability. Experiments on MiniWoB++, WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms strong baselines, including human- and document-based hints.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.599643",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Just-in-time Episodic Feedback Hinter (JEF Hinter)”的**智能体系统框架**。这个框架的目标是提升LLM智能体在**不熟悉领域**的**适应能力**和**顺序决策能力**。它并非将LLM应用于某个特定垂直领域（如医疗、化学），而是提出了一种**通用的方法论**，通过在推理时为LLM提供从离线轨迹中提炼出的“提示”来增强其表现。这本质上是在改进LLM的**通用问题解决和规划能力**，属于增强LLM基础能力的范畴，符合保留标准。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 明确以“Large language model (LLM) agents”为核心研究对象。 *   **能力方向**: 聚焦于“sequential decision-making tasks”（顺序决策任务）、“adaptation”（适应能力）和“problem-solving”（问题解决），这些都是通用推理能力的重要组成部分。 *   **新兴范式**: 论文本身就是一个关于“llm-based agents”的研究，其提出的“Hinter”系统可以看作是一种增强智能体推理能力的“tool use”方法。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： *   **多模态与视觉**: 未涉及任何视觉或多模态内容。 *   **特定应用领域**: 实验环境是MiniWoB++、WorkArena和WebArena，这些都是通用的网页交互和任务执行基准，而非生物、医疗等特定领域。论文强调其方法是“benchmark-independent”（与基准无关的），进一步证明了其通用性。 *   **模型可靠性**: 未讨论水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 论文恰好属于“智能体/工具使用”这一特殊情况的**保留**类别。它提出的JEF Hinter框架是一个**通用的智能体增强方法**，旨在提升LLM在各类任务中的通用推理和规划能力，而不是将其限制在某个特定领域。这与“用于化学实验自动化的智能体”这类应用型研究有本质区别。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种创新的、通用的框架（JEF Hinter）来增强LLM智能体的适应性和决策能力。它直接致力于提升LLM的通用推理和问题解决能力，与您的研究目标“提高大语言模型（LLM）本身的『通用推理能力』”高度契合。因此，应判定为符合要求。"
    },
    {
        "index": "#47",
        "title": "Constructing coherent spatial memory in LLM agents through graph rectification",
        "link": "/arxiv/2510.04195",
        "arxiv_id": "2510.04195",
        "authors": "Puzhen Zhang, Xuyang Chen, Yu Feng, Yuhan Jiang, Liqiu Meng",
        "summary": "Given a map description through global traversal navigation instructions (e.g., visiting each room sequentially with action signals such as north, west, etc.), an LLM can often infer the implicit spatial layout of the environment and answer user queries by providing a shortest path from a start to a destination (for instance, navigating from the lobby to a meeting room via the hall and elevator). However, such context-dependent querying becomes incapable as the environment grows much longer, motivating the need for incremental map construction that builds a complete topological graph from stepwise observations. We propose a framework for LLM-driven construction and map repair, designed to detect, localize, and correct structural inconsistencies in incrementally constructed navigation graphs. Central to our method is the Version Control, which records the full history of graph edits and their source observations, enabling fine-grained rollback, conflict tracing, and repair evaluation. We further introduce an Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation. To properly evaluate our approach, we create a refined version of the MANGO benchmark dataset by systematically removing non-topological actions and inherent structural conflicts, providing a cleaner testbed for LLM-driven construction and map repair. Our approach significantly improves map correctness and robustness, especially in scenarios with entangled or chained inconsistencies. Our results highlight the importance of introspective, history-aware repair mechanisms for maintaining coherent spatial memory in LLM agents.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.604303",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于提出了一种提升LLM智能体通用推理能力的新方法。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM基础能力。** 论文的核心并非将LLM应用于某个特定领域（如机器人控制），而是聚焦于LLM智能体在执行通用任务（空间推理与规划）时遇到的一个根本性挑战：如何构建和维护一个连贯、准确的内部空间表征（即“心智地图”）。论文提出的“图修正”框架，旨在让LLM能够自我检测、定位并修正其内部认知模型中的错误。这是一种元认知能力，属于LLM基础推理能力的核心组成部分。因此，这篇论文的本质是改进LLM的通用推理和规划能力，应予以保留。 2.  **第二步：正面指标——论文高度相关。** 论文明确包含了多个关键的正面指标： - **核心概念**: 论文研究对象是 \"LLM agents\"。 - **能力方向**: 论文直接处理 \"reasoning\"（空间推理）、\"planning\"（路径规划）和 \"problem-solving\"（解决地图构建中的不一致性问题）。 - **新兴范式**: 论文属于 \"llm-based agents\" 的研究范畴，其提出的自我修正框架是一种新颖的智能体方法论。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容与多模态、视觉、特定应用领域（医疗、化学等）以及模型可靠性（水印、安全）等排除标准完全无关。虽然论文使用了“导航”作为任务场景，但其目的不是解决机器人导航问题，而是以导航为载体，研究LLM的通用空间记忆和自我修正能力。 4.  **第四步：处理特殊和模糊情况——论文属于应保留的范畴。** - **智能体/工具使用**: 论文提出的是一个**通用的智能体框架**，用于增强LLM在构建和修正内部知识表征（空间图）方面的能力。这种能力是通用的，可以迁移到其他需要构建复杂内部模型的推理任务中。它并非应用于特定领域的智能体，因此符合保留条件。 - **幻觉/可解释性/安全**: 论文解决的“结构不一致性”可以看作是LLM在构建内部世界模型时产生的一种“幻觉”。其提出的“版本控制”和“边影响分数”等机制，是一种提升模型内在逻辑一致性和可靠性的新方法，从而直接提升了其推理质量。这符合保留标准。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种新颖的、基于历史感知和内省的修正机制，以增强LLM智能体的空间记忆连贯性和鲁棒性。这直接触及了通用推理能力中的高级认知功能——自我修正和模型维护。因此，这篇论文与您“提高大语言模型本身的通用推理能力”的核心目标高度契合，是一篇非常相关且有价值的前沿研究。"
    },
    {
        "index": "#52",
        "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning",
        "link": "/arxiv/2510.04116",
        "arxiv_id": "2510.04116",
        "authors": "Ziying Zhang, Yaqing Wang, Quanming Yao",
        "summary": "Meta reasoning behaviors work as a skeleton to guide large language model (LLM) reasoning, thus help to improve reasoning performance. However, prior researches implement meta reasoning skeleton with manually designed structure, limiting ability to adapt to query-specific requirement and capture intricate logical dependency among reasoning steps. To deal with the challenges, we represent meta reasoning skeleton with directed acyclic graph (DAG) to unify skeletons proposed in prior works and model intricate logical dependency. Then we propose AutoMR, a framework that searches for query-aware meta reasoning skeleton automatically inspired by automated machine learning (AutoML). Specifically, we construct search space based on DAG representation of skeleton and then formulate the search problem. We design a dynamic skeleton sampling algorithm by expanding meta reasoning skeleton along with reasoning context at inference time. This algorithm can derive any meta reasoning skeleton in search space efficiently and adapt skeleton to evolving base reasoning context, thus enable efficient query-aware skeleton search. We conduct experiments on extensive benchmark datasets. Experimental results show that AutoMR achieves better reasoning performance than previous works broadly.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.612022",
        "filter_reason": "这篇论文完全符合筛选要求，其核心贡献直接指向提升大语言模型的通用推理能力。以下是详细的判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心是提出一种名为AutoMR的新框架，用于自动搜索“元推理骨架”来指导LLM的推理过程。这并非将LLM作为工具应用于特定领域，而是直接改进LLM的内在推理机制。它旨在解决现有手动设计推理结构（如思维链）的局限性，通过自动化搜索来生成更适应具体问题、能捕捉复杂逻辑依赖的推理路径。这完全符合“改进LLM的基础能力”、“增强其逻辑、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文命中了多个关键的正面指标： *   **核心概念**: 论文明确以\"large language model (LLM)\"为研究对象。 *   **能力方向**: 论文的主题就是\"reasoning\"，并具体探讨了\"intricate logical dependency\"，直接对应\"reasoning\"和\"logical reasoning\"。 *   **新兴范式**: 论文提出的\"meta reasoning skeleton\"和基于AutoML思想的自动搜索框架，是一种新颖的推理范式，旨在优化推理过程本身，这与思维链(CoT)等方法的创新精神一致。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全避开了所有排除标准。它不涉及多模态、视觉，没有聚焦于任何特定应用领域（如医疗、化学），也未讨论模型部署、水印或安全等应用层面的可靠性问题。其实验是在“广泛的基准数据集”上进行的，证明了方法的通用性。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/安全等模糊情况，其研究焦点非常清晰，即优化LLM的推理结构。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种通用的、自动化的方法来优化LLM的推理“骨架”，从而提升其通用推理性能。它直接针对LLM的基础推理能力进行方法论创新，与研究课题“大语言模型通用推理能力”高度契合。因此，应予以保留。"
    },
    {
        "index": "#55",
        "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows",
        "link": "/arxiv/2510.04089",
        "arxiv_id": "2510.04089",
        "authors": "Yitong Cui, Liu Liu, Baosheng Yu, Jiayan Qiu, Xikai Zhang, Likang Xiao, Yixing Liu, Quan Chen",
        "summary": "Large language models (LLMs) have exhibited significant capabilities in addressing challenging problems throughout various fields, often through the use of agentic workflows that adhere to structured instructions and multi-step procedures. However, designing such workflows demands substantial manual effort, posing challenges to scalability and generalizability. Recent studies have aimed to minimize the human intervention needed for their construction, leading to advances in automated techniques for optimizing agentic workflows. However, current approaches are often constrained by their limited representational capacity, insufficient adaptability, weak scalability, and pairwise comparison paradigm -- issues that stem primarily from a dependence on discrete optimization techniques. To overcome these limitations, we introduce a new score-based preference approach, refereed as SPOGW, which operates directly on cardinal reward signals through group-wise comparison and enables more efficient and stable optimization in a continuous space. SPOGW incorporates Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL), which regulates training update by placing greater emphasis on the advantageous regions of the policy response. In five benchmark datasets covering mathematical reasoning, coding, and question answering, SPOGW matches or exceeds the performance of current state-of-the-art approaches, presenting a viable and forward-looking methodology for automated generation and optimization of agentic workflows.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.613554",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是我的详细判断过程： 1.  **核心判断（第一步）：** 该论文的核心是提出一种名为SPOGW的新方法，用于**自动优化LLM的“智能体工作流”**。智能体工作流是LLM执行多步推理、规划和解决复杂问题的关键范式。因此，优化工作流本身，就是在直接提升LLM的通用推理和问题解决能力。这完全符合筛选标准第一步中“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。 2.  **正面指标（第二步）：** 论文包含了多个关键的正面指标： *   **核心概念**: 明确以 \"Large language models (LLMs)\" 为研究对象。 *   **能力方向**: 在 \"mathematical reasoning\" 等基准上测试，核心是提升多步问题解决能力。 *   **训练方法**: 提出了 \"Score-based Preference Optimization\" 方法，属于强化学习（RL）范畴。 *   **新兴范式**: 论文的主题就是 \"agentic workflows\"，是当前LLM推理能力研究的前沿。 3.  **排除标准（第三步）：** 该论文没有触犯任何主要的排除标准。它不涉及多模态、视觉，也不聚焦于医疗、化学等特定应用领域。其研究目标是提升方法的“可扩展性和通用性”，这与特定领域应用的研究方向相反。 4.  **特殊和模糊情况（第四步）：** 根据第四步的规则，该论文提出的正是一种**通用的智能体工作流优化方法**，旨在增强LLM的通用问题解决能力，而非将其应用于特定领域（如“用于化学实验的智能体”）。因此，这种情况应该保留。 **最终决策（第五步）：** 综合以上分析，该论文的本质是提出一种新的训练/优化范式（SPOGW），通过改进LLM执行多步骤任务（即智能体工作流）的方式，来提升其在数学、编码等通用任务上的推理和问题解决能力。这直接命中了你“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，这篇论文是高度相关且应该被保留的前沿研究。"
    },
    {
        "index": "#45",
        "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework",
        "link": "/arxiv/2510.04206",
        "arxiv_id": "2510.04206",
        "authors": "Hanchen Zhang, Xiao Liu, Bowen Lv, Xueqiao Sun, Bohao Jing, Iat Long Iong, Zhenyu Hou, Zehan Qi, Hanyu Lai, Yifan Xu, Rui Lu, Hongning Wang, Jie Tang, Yuxiao Dong",
        "summary": "Recent advances in large language models (LLMs) have sparked growing interest in building generalist agents that can learn through online interactions. However, applying reinforcement learning (RL) to train LLM agents in multi-turn, multi-task settings remains challenging due to lack of scalable infrastructure and stable training algorithms. In this work, we present the AgentRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AgentRL features a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, we design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. Experiments show that AgentRL, trained on open LLMs across five agentic tasks, significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents. Multi-task training with AgentRL matches the best results among all task-specific models. AgentRL is open-sourced at https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in building \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.603268",
        "filter_reason": "这篇论文完全符合筛选标准，应予以保留。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心是提出一个名为`AgentRL`的框架，用于在多轮、多任务环境下通过强化学习（RL）训练大语言模型智能体。其本质是**提出一种新的训练范式和算法**，旨在提升LLM作为智能体的基础能力。一个能够进行多轮交互、在多个任务中学习的智能体，其核心能力就是通用推理、规划和问题解决。论文中提到的“cross-policy sampling”和“task advantage normalization”等算法创新，直接服务于稳定和提升这种训练效果。因此，这篇论文的核心是改进LLM的基础能力，而不是将其作为工具应用于特定领域。虽然它包含了基础设施的描述，但这是为了实现其核心训练目标而构建的，并非论文的唯一或主要贡献。 2.  **第二步：正面指标** 论文摘要中包含了大量强烈的正面指标： *   **核心概念**: 明确提到了 \"Large language models (LLMs)\"。 *   **能力方向**: 研究目标是 \"generalist agents\"，这直接关联到 \"problem-solving\" 和 \"planning\" 能力。多轮交互本身就是对模型连续推理能力的考验。 *   **训练方法**: 论文的核心是 \"Reinforcement Learning (RL)\"，并提出了新的算法来优化这一过程。 *   **新兴范式**: 论文主题是 \"llm-based agents\"，完全符合这一新兴范式。 3.  **第三步：排除标准** 论文完全没有触及任何排除标准： *   它不涉及多模态、视觉等内容。 *   它的目标是构建 \"generalist agents\"，而非应用于医疗、化学等特定领域。 *   它不讨论水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文恰好是“智能体/工具使用”这一特殊情况的典型范例。它提出的是一个**通用的智能体训练框架**，旨在增强LLM的通用问题解决能力，而不是将智能体应用于某个特定垂直领域。因此，根据筛选标准，它应该被保留。 5.  **第五步：最终决策** 综合以上分析，`AgentRL`论文的核心贡献在于提出了一种新的训练范式和配套框架，通过强化学习来系统性地提升大语言模型在复杂、多任务环境下的通用推理和规划能力。这与研究课题“提高大语言模型本身的通用推理能力”的目标高度一致。论文的方法论贡献（新的RL算法和训练框架）直接作用于LLM的核心能力提升，而非应用或部署。因此，最终判断为符合要求。"
    },
    {
        "index": "#70",
        "title": "Algorithm Generation via Creative Ideation",
        "link": "/arxiv/2510.03851",
        "arxiv_id": "2510.03851",
        "authors": "Ruiying Ma, Chieh-Jan Mike Liang, Yanjie Gao, Francis Y. Yan",
        "summary": "Designing system algorithms remains challenging, where the discontinuous nature of the solution space often forces system engineers to rely on generic heuristics at the expense of performance. We study whether LLMs can practically drive algorithm generation, and find that they are biased towards well-known generic designs, rather than making the creative leaps needed to navigate the discontinuous solution space. To address this limitation, we introduce MetaMuse, a framework for creative ideation built on three self-reflection principles: (1) quantifying solution diversity and usefulness in measurable performance space, rather than abstract idea space, (2) steering ideation through external stimuli, rather than internal randomness, and (3) constructing executable solutions using waypoint reasoning, rather than free-form chain-of-thought. Extensive evaluation shows that MetaMuse can generate high-performing solutions for two critical problems at a global cloud provider: cache replacement (reducing cache misses by up to 35.76%) and online bin packing (reducing bin usage by up to 30.93%).",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.625537",
        "filter_reason": "这篇论文符合筛选要求，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一个名为 **MetaMuse** 的框架，旨在解决大语言模型在生成算法时缺乏“创造性飞跃”的问题。这本质上是在改进LLM的**基础推理和问题解决能力**。论文指出了现有LLM在处理“不连续解决方案空间”时的局限性，并提出了新的方法论来克服它。这并非将LLM作为工具应用于某个特定领域，而是致力于提升LLM本身的通用能力，因此符合核心保留标准。 2.  **第二步：正面指标** 论文与多个正面指标高度相关： *   **核心概念**: 论文的研究主体是 **Large language models (LLMs)**。 *   **能力方向**: 论文聚焦于 **reasoning** 和 **problem-solving**，具体表现为算法生成能力，这是一种高级的逻辑规划和推理形式。 *   **新兴范式**: 论文提出的 **\"waypoint reasoning\"（航点推理）** 是对现有思维链的一种改进和创新，属于新的推理范式。整个MetaMuse框架可以被看作是一种增强LLM自主解决问题能力的结构化方法。 3.  **第三步：排除标准** 论文未触及任何排除标准： *   论文不涉及多模态、视觉等内容。 *   虽然论文在“缓存替换”和“在线装箱”两个问题上进行了评估，但这属于计算机科学领域的**经典算法基准问题**，用于验证框架的有效性，而非将LLM应用于医疗、化学等**特定垂直领域**。其目标是提出一个通用的算法生成框架，而非一个缓存系统或装箱系统。因此，这不构成排除理由。 *   论文不关注水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文的情况与“智能体/工具使用”的特殊情况类似。MetaMuse可以被理解为一个**通用的、旨在增强LLM通用问题解决能力的框架**。它通过结构化的自我反思和推理步骤，引导LLM完成复杂的创造性任务。这与“用于化学实验自动化的智能体”有本质区别，后者是特定领域应用，而前者是通用方法论。 **最终决策**: 综合以上分析，这篇论文的本质是提出了一种创新的推理框架（MetaMuse），通过引入“航点推理”等新方法，直接致力于提升大语言模型在复杂、不连续问题空间中的通用推理和问题解决能力。其研究目标与“大语言模型通用推理能力”的课题高度契合，因此应被**保留**。"
    },
    {
        "index": "#73",
        "title": "GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time",
        "link": "/arxiv/2510.03777",
        "arxiv_id": "2510.03777",
        "authors": "Divij Handa, Mihir Parmar, Aswin RRV, Md Nayem Uddin, Hamid Palangi, Chitta Baral",
        "summary": "Repeated Sampling (RS) is a simple inference-time algorithm that has been shown to improve model performance on complex tasks. Although it is an effective way of scaling inference time, it often struggles to generate diverse solution candidates, frequently relying on the same underlying approach to solve the problem and thus producing redundant samples. To address this limitation, we propose a new inference algorithm, GuidedSampling, which decouples the exploration and generation phases during inference, increasing diversity of generated candidate solutions. The exploration phase identifies multiple concepts that can be utilized to solve the problem, while the generation phase applies a specific concept to provide final solution candidates. We first define the theoretical bounds of GuidedSampling and then empirically demonstrate that it improves the performance of base model at pass@50 by on an average ~21.6% across various benchmarks compared to RS. Furthermore, models trained on trajectories of GuidedSampling exhibit substantial performance improvements at pass@5 by on an average ~9.7%, compared to models trained on traditional RS. Additionally, models trained with GuidedSampling increases the average number of concepts per instance (1.67 -> 3.03), yielding a diverse set of candidates than traditional RS.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.627072",
        "filter_reason": "这篇论文完全符合筛选要求，其核心贡献直接指向提升大语言模型的通用推理能力。 **第一步：核心判断** 论文的本质是提出一种新的**推理时算法**，即`GuidedSampling`。这并非将LLM作为工具应用于特定领域，而是直接改进LLM在解决复杂问题时的内在工作机制。论文指出现有方法（Repeated Sampling）在生成解决方案时缺乏多样性，常常陷入单一的解题思路。`GuidedSampling`通过将“探索”和“生成”两个阶段解耦，引导模型识别并利用多种不同的解题“概念”来生成多样化的候选方案。这是一种方法论上的创新，旨在从算法层面增强模型的基础问题解决能力，完全符合“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的保留标准。 **第二步：正面指标** 论文高度匹配多个正面指标： - **核心概念**: 论文标题和摘要明确研究对象是“LLMs”。 - **能力方向**: 论文的核心目标是提升模型在“复杂任务”上的表现，通过生成“多样化的候选解决方案”来增强其“问题解决”能力。识别和应用不同“概念”的过程，本身就是一种高级的、结构化的推理活动。 - **训练方法**: 论文不仅提出了推理时算法，还进一步探索了使用该算法生成的轨迹来训练模型，并证明了其有效性。这是一种新的数据生成和模型优化范式，属于增强模型能力的方法论研究。 **第三步：排除标准** 论文完全不涉及任何排除标准领域： - 它不涉及多模态、视觉或特定应用领域（如医疗、化学等）。 - 它的研究焦点是模型的核心性能（pass@k指标），而非水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** 虽然论文没有直接提出一个完整的“智能体”框架，但其“探索概念”再“应用概念生成”的两阶段算法，可以被看作是一种通用的、领域无关的内化推理框架。它通过结构化的流程来增强模型的通用问题解决能力，这与保留“通用智能体协作框架”的精神内核是一致的。 **第五步：最终决策** 综合分析，该论文提出了一种创新的推理时算法`GuidedSampling`，通过增加解决方案的多样性来显著提升LLM在复杂任务上的推理和问题解决能力。其研究内容纯粹、直接地聚焦于增强LLM的通用推理核心能力，并提供了新的训练范式思路，是本课题下的高度相关且有价值的前沿研究。因此，最终判断为保留。"
    },
    {
        "index": "#79",
        "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information",
        "link": "/arxiv/2510.03632",
        "arxiv_id": "2510.03632",
        "authors": "Jiaxi Li, Yucheng Shi, Jin Lu, Ninghao Liu",
        "summary": "Tree search has become as a representative framework for test-time reasoning with large language models (LLMs), exemplified by methods such as Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning paths. However, it remains difficult to provide instant and reliable quantitative assessments of intermediate reasoning step quality, and extensive path exploration is computationally costly. To address this, we propose Mutual Information Tree Search (MITS), a novel framework that guides reasoning with information-theoretic principles. MITS introduces an effective scoring function based on pointwise mutual information (PMI), which enables step-wise evaluation of reasoning paths and search tree expansion via beam search without expensive look-ahead simulations, achieving superior reasoning performances while maintaining computational efficiency. The framework is complemented by an entropy-based dynamic sampling strategy that adaptively allocates computational resources to uncertain reasoning steps where exploration is most beneficial. For final prediction, MITS employs a weighted voting scheme that combines PMI scores with prediction consensus. Through comprehensive experiments on diverse reasoning benchmarks, MITS consistently surpasses baseline methods, establishing a principled and efficient framework for LLM reasoning.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.629935",
        "filter_reason": "这篇论文完全符合您的筛选标准，是关于“大语言模型通用推理能力”研究的理想候选。 以下是根据您提供的筛选标准进行的详细判断过程： 1.  **第一步：核心判断** - **论文核心贡献**: 论文提出了一种名为“MITS”（Mutual Information Tree Search）的新框架。其本质是一种**测试时推理方法**，旨在通过信息论原理（点间互信息PMI）来改进和引导LLM的树搜索推理过程。 - **符合性分析**: 这项工作的核心不是将LLM应用于某个特定领域，而是直接针对LLM的**推理过程本身**进行优化。它提出了一种新的方法论来评估中间推理步骤的质量，并更高效地探索推理路径，从而提升模型的逻辑和规划能力。这与筛选标准中“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”以及“思维链等方法论的研究”完全吻合。因此，应**保留**。 2.  **第二步：正面指标** - **核心概念**: 论文标题和摘要中多次明确提及“Large language models (LLMs)”。 - **能力方向**: 论文的核心主题是“Tree Search Reasoning”，直接命中“reasoning”这一关键能力方向。摘要中提到的“evaluation of intermediate reasoning step quality”和“diverse reasoning benchmarks”进一步证实了其聚焦于提升模型的通用推理能力。 - **训练方法**: 虽然本文不是关于训练阶段的强化学习，但其提出的测试时搜索框架与强化学习中的搜索和评估思想有共通之处，属于增强模型能力的广义方法论。 - **新兴范式**: “Tree Search”是当前LLM推理研究的前沿范式，与“思维链”一脉相承，是提升模型复杂问题解决能力的关键技术。 3.  **第三步：排除标准** - **多模态与视觉**: 论文完全不涉及视觉、多模态等内容，仅专注于语言模型。 - **特定应用领域**: 论文在“diverse reasoning benchmarks”上进行验证，这通常指代数学、逻辑、常识等通用推理基准，而非医疗、化学、金融等特定领域。 - **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** - 本文不涉及智能体/工具使用，也不直接以减少幻觉或提升安全性为目标，因此无需进入此判断环节。其目标是更根本的——提升推理过程的质量和效率。 5.  **第五步：最终决策** - **综合分析**: 论文的核心贡献是提出了一种新颖的、通用的框架（MITS）来增强LLM的推理能力。它直接解决了当前树搜索推理方法中评估困难和计算成本高的问题，属于对LLM基础推理能力的根本性改进。论文主题与您的核心目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度一致，且不触及任何排除标准。 因此，最终判断为 **True**。"
    },
    {
        "index": "#78",
        "title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs",
        "link": "/arxiv/2510.03680",
        "arxiv_id": "2510.03680",
        "authors": "Bumjun Kim, Dongjae Jeon, Dueun Kim, Wonje Jeung, Albert No",
        "summary": "Diffusion large language models (dLLMs) have emerged as a promising alternative to autoregressive models, offering flexible generation orders and strong performance on complex reasoning tasks. However, instruction-tuned dLLMs exhibit a critical vulnerability we term \\texttt{<eos>} overflow: as allocated sequence length increases, responses paradoxically become shorter, collapsing into early termination or degenerating into streams of \\texttt{<eos>} tokens. Although noticed in practice, this issue has not been systematically analyzed. We trace its root cause to the dual role of \\texttt{<eos>} as both termination and padding, which concentrates probability mass on \\texttt{<eos>} at later positions and propagates backward to trigger early termination. To address this, we introduce Rainbow Padding, a simple remedy that replaces repeated \\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens, distributing probability mass and breaking \\texttt{<eos>} dominance. Experiments show that Rainbow Padding substantially improves length robustness and output quality, with as few as seven padding tokens sufficient to prevent early termination. Moreover, the method integrates efficiently into existing instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data yields significant improvements, making this solution highly practical. The code is publicly available at https://github.com/quasar529/rainbow-padding.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.629642",
        "filter_reason": "这篇论文符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是解决扩散大语言模型在指令微调后出现的一个技术问题：`<eos>`溢出导致的过早终止。这个问题直接影响模型生成长文本、完成复杂任务的能力。一个无法完整生成回答的模型，其逻辑、数学、规划等多步推理能力必然受到严重限制。因此，这篇论文的本质是**修复了一个阻碍LLM发挥其通用推理能力的基础性技术瓶颈**。它不是提出一种新的推理方法（如CoT），而是通过改进模型的内在机制，保障了其现有推理能力能够完整、稳定地输出。这属于“改进LLM的基础能力”的范畴。 2.  **第二步：正面指标** 论文明确提到了核心概念“Large language models (LLMs)”和其变体“diffusion large language models (dLLMs)”。摘要中指出，dLLMs在“complex reasoning tasks”（复杂推理任务）上表现出色，而本文的工作旨在提升其“output quality”（输出质量），这直接关系到推理任务的结果质量。 3.  **第三步：排除标准** 该论文不涉及多模态、视觉或任何特定应用领域（如医疗、化学）。它研究的是一个通用的语言模型问题，而非应用层面的部署优化或硬件加速。虽然它涉及模型可靠性，但并非指水印、安全等应用层面的议题，而是模型生成过程的内在技术缺陷。 4.  **第四步：处理特殊和模糊情况** 本文的情况可以类比于“减少幻觉”的研究。正如一篇提出新方法来减少幻觉的论文会被保留一样，因为它提升了模型的内在可靠性和推理质量。本文提出的Rainbow Padding方法，通过解决过早终止问题，同样**提升了模型生成过程的内在可靠性**，确保了推理链条的完整性，从而间接但至关重要地提升了模型的通用推理能力。 **最终决策**: 综合来看，这篇论文虽然未直接提出一种新的推理范式，但它解决了一个底层的、关键的技术缺陷。这个缺陷是LLM执行通用推理（尤其是多步推理）的巨大障碍。通过修复这一障碍，论文有效地为LLM的推理能力提供了更稳定、更可靠的发挥平台。因此，它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标，应当被保留。"
    },
    {
        "index": "#84",
        "title": "Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification",
        "link": "/arxiv/2510.03469",
        "arxiv_id": "2510.03469",
        "authors": "Keshav Ramani, Vali Tawosi, Salwa Alamir, Daniel Borrajo",
        "summary": "We introduce a novel framework for evaluating the alignment between natural language plans and their expected behavior by converting them into Kripke structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs) and performing model checking. We systematically evaluate this framework on a simplified version of the PlanBench plan verification dataset and report on metrics like Accuracy, Precision, Recall and F1 scores. Our experiments demonstrate that GPT-5 achieves excellent classification performance (F1 score of 96.3%) while almost always producing syntactically perfect formal representations that can act as guarantees. However, the synthesis of semantically perfect formal models remains an area for future exploration.",
        "subjects": "Artificial Intelligence, Logic in Computer Science",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.631434",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出一个新颖的框架，用于**评估和验证**由LLM生成的“规划”这一通用推理能力的产物。它并非将LLM应用于某个特定垂直领域，而是聚焦于“LLM规划智能体”这一通用概念本身。通过引入形式化方法（如LTL和模型检测），论文为衡量LLM的规划质量和逻辑一致性提供了新的、更严谨的评估范式。这属于增强LLM基础能力（规划与推理）相关的方法论研究，与提升LLM通用推理能力的核心目标高度一致。 2.  **第二步：正面指标** 论文明确包含了多个关键正面指标： *   **核心概念**: \"Large Language Models (LLMs)\" 是标题和摘要的核心。 *   **能力方向**: \"Planning\" 和 \"Plan Verification\" 是论文的绝对主题，直接关联推理能力。 *   **新兴范式**: 论文的研究对象是 \"LLM Planning Agents\"，属于LLM-based agents的范畴。 3.  **第三步：排除标准** 该论文完全避除了所有主要的排除领域： *   它不涉及多模态或视觉。 *   它使用的是通用的规划基准，而非任何特定应用领域（如医疗、化学）。 *   它研究的是“Plan Verification”，这是一种提升模型内在逻辑可靠性的方法，与“水印”等应用层面的可靠性技术有本质区别，不属于排除范畴。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 该论文提出的框架是一个**通用的**规划验证方法，旨在提升LLM智能体在规划任务上的可靠性，而非应用于特定领域。这完全符合“应该保留”的条件。 *   **可靠性**: 论文通过将自然语言计划转换为可进行模型检测的形式化结构（Kripke structures, LTL），实质上是在为LLM的规划输出提供**逻辑上的保证**。这是一种从根本上提升模型推理质量和可靠性的方法，远超于应用层面的讨论，因此应该保留。 5.  **第五步：最终决策** 综合分析，该论文虽然没有直接提出一种新的训练方法来“增强”LLM的推理能力，但它提出了一种至关重要的**评估和验证框架**。在科研中，严谨的评估方法是推动能力提升的先决条件。该工作为“LLM的通用推理能力”这一研究方向，特别是其中的“规划”子领域，贡献了一个基础性的方法论工具，有助于社区更准确地衡量和改进模型能力。因此，它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”的研究范围。"
    },
    {
        "index": "#81",
        "title": "Understanding the Role of Training Data in Test-Time Scaling",
        "link": "/arxiv/2510.03605",
        "arxiv_id": "2510.03605",
        "authors": "Adel Javanmard, Baharan Mirzasoleiman, Vahab Mirrokni",
        "summary": "Test-time scaling improves the reasoning capabilities of large language models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts (CoTs). This enables models to tackle more complex problem by breaking them down into additional steps, backtracking, and correcting mistakes. Despite its strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions in the training data under which long CoTs emerge, and when such long CoTs improve the performance, remain unclear. In this paper, we study the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. Our analysis provides a theoretical explanation for several intriguing observations: First, at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. Second, if the skills required to solve a downstream task are not sufficiently present in the training data, increasing test-time compute can harm performance. Finally, we characterize task hardness via the smallest eigenvalue of its feature covariance matrix and show that training on a diverse, relevant, and hard set of tasks results in best performance for test-time scaling. We confirm our findings with experiments on large, nonlinear transformer architectures.",
        "subjects": "Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.630541",
        "filter_reason": "这篇论文完全符合你的研究范围，是关于提升大语言模型通用推理能力的前沿研究。我的判断依据如下： 1.  **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 论文的核心研究对象是“Test-time scaling”（测试时扩展），这是一种通过增加计算资源（如生成长思维链CoT）来直接提升LLM推理能力的通用方法论。论文并非将LLM应用于某个特定领域，而是深入探究了“为什么”以及“在什么条件下”这种通用推理增强方法有效。它研究了训练数据的质量和多样性如何影响模型在测试时进行推理扩展的能力，这属于改进LLM基础能力和内在机制的范畴。 2.  **第二步：正面指标——论文高度相关。** - **核心概念**: 论文明确以大语言模型为研究对象。 - **能力方向**: 论文的核心是“reasoning capabilities”（推理能力），并深入分析了“Chains-of-Thoughts”（思维链）这一关键的推理技术。 - **训练方法**: 虽然没有提出新的训练算法（如RL），但它研究了“训练数据”这一最根本的要素如何影响模型的推理表现，这对如何设计训练范式以培养更强的推理能力具有直接的指导意义。 3.  **第三步：排除标准——论文未触及任何排除领域。** - 论文不涉及多模态、视觉等。 - 论文不涉及任何特定应用领域（如医疗、化学）。虽然它使用了一个“线性回归”的理论任务作为分析载体，但其目的是为了得出关于LLM的普适性结论，并在更通用的非线性Transformer上进行了验证，这与解决特定领域问题的应用型论文有本质区别。 - 论文不关注水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况——不适用。** 论文不涉及智能体、工具使用或幻觉等特殊情况。 **最终决策：** 这篇论文的贡献在于，它为当前最前沿的推理增强技术（如OpenAI o1所代表的测试时扩展）提供了理论解释。它揭示了训练数据与模型推理能力之间的深层关系，指明了为了让模型更好地利用测试时计算进行推理，应该在训练数据中包含哪些“技能”。这种对通用推理能力背后机理的深刻洞察，正是你研究课题“大语言模型通用推理能力”所需要的核心内容。因此，这篇论文应该被保留。"
    },
    {
        "index": "#100",
        "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts",
        "link": "/arxiv/2510.05040",
        "arxiv_id": "2510.05040",
        "authors": "Jihoon Lee, Hoyeon Moon, Kevin Zhai, Arun Kumar Chithanar, Anit Kumar Sahu, Soummya Kar, Chul Lee, Souradip Chakraborty, Amrit Singh Bedi",
        "summary": "Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn a mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, a common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), a training-free inference method that ensembles across heterogeneous block schedules. By doing a majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish a new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays a critical role in determining performance during inference.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.641696",
        "filter_reason": "这篇论文完全符合筛选要求，应被保留。以下是基于筛选标准的详细判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出了一种名为HEX的、**无需训练的推理时缩放方法**，旨在提升扩散大语言模型在推理任务上的表现。其本质是探索如何更好地利用模型在训练阶段学到的知识，通过改变推理策略（集成不同的生成路径）来显著提升模型的逻辑、数学和科学推理能力。这完全符合“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的核心目标。它不是将LLM应用于特定领域，而是研究LLM本身的一种新的使用范式。 2.  **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文明确研究 \"Diffusion-based large language models (dLLMs)\"，属于LLM范畴。 - **能力方向**: 论文的评估基准是 \"reasoning benchmarks\"，具体包括数学推理（GSM8K, MATH）、科学推理（ARC-C）和事实推理，与筛选标准高度吻合。 - **训练方法**: 论文的方法是 \"training-free\"，但它与强化学习方法（如GRPO）进行了对比，表明其研究处于提升模型能力的同一技术轨道上。 - **新兴范式**: 论文提出的 \"test-time scaling\" 是一种新兴的、旨在提升模型性能的范式，与思维链（CoT）等推理增强方法在目标上一致。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** - **多模态与视觉**: 尽管标题中出现了 \"Diffusion\"，但论文摘要明确指出这些是 \"trained on textual data\" 的 \"diffusion LLMs\"，其评估基准均为纯文本的推理任务。因此，这不属于视觉或多模态研究，而是将扩散模型这一架构应用于纯文本语言模型的一种探索。 - **特定应用领域**: 论文使用的基准（GSM8K, MATH等）是通用的推理能力测试集，不涉及任何特定应用领域（如医疗、化学等）。 - **模型可靠性（应用层面）**: 论文关注的是提升推理的准确性，虽然这间接提升了可靠性，但其核心是方法论创新，而非水印、安全等应用层面的研究。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 不适用。 - **幻觉/可解释性/安全**: 论文通过集成不同路径来 \"robustly avoids failure modes\"，这可以看作是一种提升内在推理质量和可靠性的方法，但其主要贡献点在于推理时缩放这一新范式，而非专门针对幻觉或可解释性的研究。 **最终决策**: 综合以上分析，该论文提出了一种新颖的推理时方法（HEX），通过挖掘和利用扩散LLM内部隐含的“专家”混合特性，在不增加训练成本的情况下，极大地提升了模型在多项通用推理基准上的性能。其研究目标、方法和评估标准都与“提高大语言模型本身的通用推理能力”这一核心目标高度一致。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#140",
        "title": "Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning",
        "link": "/arxiv/2510.04786",
        "arxiv_id": "2510.04786",
        "authors": "Jonas Hübotter, Leander Diaz-Bone, Ido Hakimi, Andreas Krause, Moritz Hardt",
        "summary": "Humans are good at learning on the job: We learn how to solve the tasks we face as we go along. Can a model do the same? We propose an agent that assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task. The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from a large pool of available training data. Our experiments demonstrate that reinforcement learning on a test-time curriculum consistently improves the model on its target tasks, across a variety of evaluations and models. Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%. Our findings show the potential of test-time curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.697613",
        "filter_reason": "这篇论文完全符合研究范围。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种名为“测试时课程”的新训练范式。它不是将LLM应用于某个特定领域，而是致力于改进LLM本身的学习和适应能力。该方法通过在测试时利用强化学习（RL）对模型进行持续训练，并自动构建任务相关的课程，从而提升模型在目标任务上的表现。 - **符合目标**: 这直接对应了“改进LLM的基础能力”和“提出新的训练范式”。论文通过在数学和编码这两个通用推理能力的核心基准上取得显著提升，证明了其方法对增强LLM逻辑、数学和多步推理能力的有效性。因此，这篇论文的本质是提升LLM的通用推理能力，应予以保留。 2.  **第二步：正面指标** - **核心概念**: 论文的研究对象是大型语言模型（以Qwen3-8B为例）。 - **能力方向**: 论文明确聚焦于提升模型的**推理**能力，特别是在**数学推理**和**编程问题解决**上取得了突破。 - **训练方法**: 论文的核心方法论是**强化学习**，并提出了一个新颖的“测试时课程”框架。 - **新兴范式**: 论文提出了一种新的“测试时扩展”范式，并涉及**智能体**的概念来构建课程。 - **结论**: 论文命中了所有关键的正面指标，表明其与研究课题高度相关。 3.  **第三步：排除标准** - 论文的研究内容完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）。它聚焦于通用的文本和代码推理任务，因此完全避开了所有排除标准。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文中提到的“智能体”是一个通用的框架，用于自动选择数据以构建课程，其目的是增强模型在目标任务上的通用推理能力，而非应用于特定领域。这完全符合保留条件。 5.  **第五步：最终决策** - **综合分析**: 该论文提出了一种创新的训练方法论（TTC-RL），旨在通过测试时的持续学习和课程自动生成，来系统性地提升大语言模型在数学和编码等核心推理任务上的性能上限。其核心贡献是增强模型本身的通用推理和自适应能力，而非将其作为工具应用于特定场景。这与“提高大语言模型本身的通用推理能力”这一核心目标高度一致。 因此，最终判断为 **True**。"
    },
    {
        "index": "#232",
        "title": "On the Limitations and Capabilities of Position Embeddings for Length Generalization",
        "link": "/arxiv/2510.04130",
        "arxiv_id": "2510.04130",
        "authors": "Yang Chen, Yitao Liang, Zhouchen Lin",
        "summary": "In Transformers, Position Embeddings (PEs) significantly influence Length Generalization (LG) performance, yet their fundamental role remains unclear. In this work, we investigate the limitations and capabilities of PEs in achieving LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs), introducing Linear Representation Complexity (LRC) to characterize when PEs enable LG. Our analysis shows that PEs do not expand computational capabilities but structure learned computations across positions. Extending to practical Transformers, we propose Sequential Representation Complexity (SRC) and conjecture that LG is possible if and only if SRC remains invariant across scales. We support this hypothesis with empirical evidence in various reasoning tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance scaling, and a Learning-Based Position Embedding framework that automatically learns positional relations. Our work provides theoretical insights and practical strategies for improving LG in Transformers.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.781722",
        "filter_reason": "这篇论文完全符合筛选标准，应该被保留。 1.  **第一步：核心判断** 论文的核心是研究并改进Transformer架构中的一个基础组件——位置嵌入，以提升模型的“长度泛化”能力。长度泛化指的是模型处理比训练时见过的更长序列的能力。这项能力是执行复杂、多步推理任务的**根本前提**。如果一个模型无法处理长上下文，其进行数学证明、逻辑链推理、复杂规划等通用推理的能力就会受到严重限制。因此，这篇论文的本质是**通过改进模型的基础架构能力，来间接但根本地增强其通用推理潜力**。它并非将LLM应用于特定领域，而是致力于提升LLM本身的能力上限。 2.  **第二步：正面指标** - **核心概念**: 论文研究对象是Transformer，这是当前大语言模型（LLMs）的核心架构。 - **能力方向**: 论文明确指出，其实验验证是在“各种推理任务”上进行的。这直接将长度泛化这一基础能力与“推理”这一核心目标联系起来。 - **训练方法**: 论文提出了“Learning-Based Position Embedding framework”，这是一种新的学习/训练范式，旨在自动学习位置关系，属于方法论创新。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准。它没有讨论多模态、视觉，没有聚焦于医疗、化学等特定应用领域，也与水印、安全等应用层面的可靠性无关。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况。 **总结**: 这篇论文的贡献是双重的：**理论上**，它揭示了位置嵌入在长度泛化中的作用机理，提出了新的理论框架（SRC）；**实践上**，它提出了具体的方法来增强长度泛化能力。由于长度泛化是LLM执行长链、复杂推理的基石，因此这项工作直接服务于“提高大语言模型通用推理能力”这一核心目标。它不是解决某个具体的推理问题，而是**提升了模型解决一类推理问题的底层能力**，是典型的、高质量的基础能力研究。"
    },
    {
        "index": "#249",
        "title": "The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View",
        "link": "/arxiv/2510.04028",
        "arxiv_id": "2510.04028",
        "authors": "Xinhao Yao, Lu Yu, Xiaolin Hu, Fengwei Teng, Qing Cui, Jun Zhou, Yong Liu",
        "summary": "The ongoing debate on whether reinforcement learning with verifiable rewards (RLVR) expands or shrinks the reasoning capabilities of large language models (LLMs) remains unresolved. Some studies contend that RLVR mainly improves sampling efficiency but at the expense of diversity and exploratory capacity, resulting in capability boundary shrinkage. In contrast, others demonstrate that prolonged training can lead to the emergence of novel reasoning strategies, suggesting capability boundary expansion. To reconcile these contradictory findings, we theoretically and empirically show that both perspectives are partially valid-each aligning with a separate phase in an inherent two-stage probability mass dynamic: (1) Exploitation stage: initially, the model primarily samples explored high-reward and low-reward tokens, while rarely selecting the potentially optimal token. Positive advantage estimates increase the probability of high-reward tokens and decrease those of low-reward tokens, yet the optimal token's probability remains largely unchanged during this stage. (2) Exploration stage: as training advances, the growth rate of previously acquired high-reward tokens slows as their probabilities approach saturation. When a potentially optimal token-now receiving positive advantage estimates-is occasionally sampled, its probability increases, while those of the originally high-reward tokens decrease. This dynamic suggests that over-exploitation during the exploitation stage may lead to capability boundary shrinkage, whereas prolonged training into the exploration stage can promote an expansion of the reasoning capability boundary. Building upon our insights, we revisit the potential of only using relative negative gradients for prolonging training, providing a theoretical and empirical foundation for the development of more advanced reasoning capabilities.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.806690",
        "filter_reason": "这篇论文完全符合您的筛选标准，应被保留。以下是我的详细判断过程： **第一步：核心判断** - **论文本质**: 这篇论文的核心是深入探讨一种特定的训练范式——\"可验证奖励强化学习\"（RLVR）——如何影响大语言模型（LLM）的**推理能力边界**。它不是将LLM应用于某个特定领域，而是对LLM训练过程本身进行理论分析和实证研究，旨在解决关于RLVR是增强还是削弱模型通用推理能力的学术争论。这直接触及了“改进LLM的基础能力”和“提出新的训练范式”这一核心目标。 - **结论**: 该论文的本质完全符合**保留**标准。 **第二步：正面指标** - **核心概念**: 论文标题和摘要中明确提到了\"large language models (LLMs)\"。 - **能力方向**: 论文的主题是\"Reasoning Capability Boundary\"，通篇在讨论模型的“推理能力”和“推理策略”，这与筛选标准中的\"reasoning\"高度吻合。 - **训练方法**: 论文的核心研究对象是\"reinforcement learning with verifiable rewards (RLVR)\"，属于强化学习范畴，这与筛选标准中的\"reinforcement learning (RL)\"完全一致。 - **结论**: 该论文命中了多项关键的正面指标，进一步确认了其相关性。 **第三步：排除标准** - **多模态与视觉**: 论文未涉及任何视觉、多模态或相关内容。 - **特定应用领域**: 论文的研究是通用性的，没有聚焦于医疗、化学、机器人等任何特定应用领域。 - **模型可靠性（应用层面）**: 论文不讨论水印、安全等应用层面的可靠性问题。 - **结论**: 该论文未触及任何排除标准。 **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用的特定应用，也不涉及幻觉/可解释性/安全等模糊情况，其研究焦点非常清晰。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一个“两阶段动态模型”来解释RLVR对LLM推理能力的影响。它揭示了初期训练（利用阶段）可能导致能力收缩，而长期训练（探索阶段）则能带来能力扩张。这项工作不仅调和了现有研究的矛盾，更重要的是为**如何更有效地训练LLM以发展出更高级、更通用的推理能力**提供了重要的理论基础和实践指导。这与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致，是一篇非常前沿且相关的基础性研究论文。因此，最终判断为符合要求。"
    },
    {
        "index": "#259",
        "title": "Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data",
        "link": "/arxiv/2510.03988",
        "arxiv_id": "2510.03988",
        "authors": "Hoang Anh Just, Myeongseob Ko, Ruoxi Jia",
        "summary": "Distilling long reasoning traces (10K+ tokens) from stronger teacher models into smaller student LLMs via SFT has emerged as a standard paradigm. This approach is practical and efficient: it leverages the ease of generating abundant reasoning data from stronger models and provides a direct, data-driven way to teach less capable models better reasoning. While previous work has largely focused on prompt selection with responses from a single teacher, the equally important problem of choosing the best response when multiple teacher outputs are available for a single prompt remains underexplored. This challenge becomes important in a multi-teacher setting, where different students may benefit from the outputs of different teachers. This paper fills that gap with a systematic study of response selection for reasoning distillation. We first show that the current method, which picks responses the student assigns the highest global log-probability (global naturalness), fails when responses come from multiple teachers, i.e., global naturalness no longer correlates with downstream performance, especially as the reasoning traces from strong teachers become longer. To overcome this problem, we introduce Local Naturalness, which measures the student's log-probabilities over short, sequential reasoning steps conditioned only on a small local window. Local Naturalness enables two applications: 1) Teacher Selection: Aggregating local scores across prompts reliably identifies the most helpful teacher. 2) Response Selection from a Multiple Teachers: When mixing answers from many teachers, Local Naturalness boosts a 32B student's accuracy on math benchmarks by 9.4pp over global selection, also surpassing the performance achieved by training on data from the single best teacher. These results highlight the power of localized data quality evaluation and data mixing for more effective reasoning distillation.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.816623",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献是提出了一种新的方法论来提升大语言模型（LLM）的通用推理能力。 1.  **第一步：核心判断** - **保留**。这篇论文的本质是研究如何通过改进训练范式（即“推理蒸馏”）来提升LLM的推理能力。它聚焦于一个基础性问题：如何从强大的教师模型中为弱小的学生模型筛选出最优质的推理数据，以实现最高效的知识迁移。这直接关系到提升LLM本身的逻辑、数学和多步推理能力，属于改进LLM基础能力的核心研究。论文并非将LLM作为工具应用于特定领域，也未涉及基础设施或部署优化。 2.  **第二步：正面指标** - 论文明确包含了多个核心正面指标： - **核心概念**: 标题和摘要中反复提及 \"Large language models\", \"LLMs\"。 - **能力方向**: 论文的核心主题是 \"Reasoning\"，具体通过 \"math benchmarks\" 来量化评估推理能力的提升。 - **训练方法**: 论文研究的是 \"Distilling\"（蒸馏）这一重要的模型训练和知识迁移方法，并提出了改进该流程的新策略。 3.  **第三步：排除标准** - 论文完全不涉及任何排除标准中的领域。它没有讨论多模态、视觉，没有聚焦于任何特定应用领域（如医疗、化学），也未涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用或幻觉/安全等特殊情况，因此无需特别处理。 5.  **第五步：最终决策** - 综合来看，这篇论文的核心贡献是提出了一种名为“局部自然性”的新方法，用于优化推理蒸馏过程中的数据选择。该方法直接提升了学生LLM在数学推理基准测试上的表现，证明了其在增强模型通用推理能力方面的有效性。这项工作属于方法论创新，旨在从根本上提升LLM的推理质量，与你的核心目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度契合。因此，应予以保留。"
    },
    {
        "index": "#260",
        "title": "What Can You Do When You Have Zero Rewards During RL?",
        "link": "/arxiv/2510.03971",
        "arxiv_id": "2510.03971",
        "authors": "Jatin Prakash, Anirudh Buvanesh",
        "summary": "Reinforcement learning (RL) with outcome-based rewards has proven effective for improving large language models (LLMs) on complex reasoning tasks. However, its success often depends on the base model occasionally sampling correct solutions. When no correct solutions are sampled, training encounters a zero-reward barrier where learning stalls due to zero gradients. We study this scenario through the graph search task introduced in Bachmann et al. (2024) and evaluate recent methods that incorporate desirable components such as dense rewards, diversity incentives, and improved credit assignment. Our experiments show that none of these approaches overcome the zero-reward barrier if the base model never produces a correct answer. In contrast, we find that a simple data-centric intervention of adding easier samples to the training set enables the model to eventually solve the original hard task despite starting from zero reward. Importantly, this succeeds without modifying the RL algorithm itself. Because official implementations of several baselines were unavailable, we developed our own, which allowed us to conduct a detailed analysis of their failure modes. We release these implementations to support further research at: https://github.com/rl4reasoning/rl-baselines",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.817067",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是基于筛选标准的详细判断过程： **第一步：核心判断** 论文的本质是研究和改进用于提升大语言模型推理能力的强化学习（RL）训练方法。它没有将LLM作为工具应用于特定领域，而是聚焦于RL训练范式本身的一个核心难题——“零奖励障碍”。论文的核心贡献是提出了一种新的、以数据为中心的干预方法（添加更简单的样本），使得RL训练即使在基础模型完全无法解决困难任务时也能有效进行，最终提升模型在复杂推理任务上的表现。这直接属于“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴，因此应**保留**。 **第二步：正面指标** 论文高度符合多个正面指标： - **核心概念**: 明确研究 \"Large language models (LLMs)\"。 - **能力方向**: 直接针对 \"complex reasoning tasks\"，并使用 \"graph search task\" 作为推理能力的评估基准。 - **训练方法**: 论文的核心就是关于 \"Reinforcement learning (RL)\"，深入探讨了RL在训练LLM推理时遇到的瓶颈和解决方案。 这些指标强烈表明该论文与您的研究目标高度相关。 **第三步：排除标准** 论文完全不涉及任何排除标准领域： - 没有讨论多模态、视觉或VLMs。 - 研究的“图搜索任务”是通用的认知能力测试，而非医疗、化学等特定应用领域。 - 论文焦点是训练效率和效果，而非水印、安全等模型可靠性应用层面问题。 **第四步：处理特殊和模糊情况** 本论文不涉及智能体应用或安全等模糊情况。它纯粹是一项关于训练方法论的深入研究，旨在解决一个基础性的训练难题，从而增强模型的内在推理能力。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是识别并解决了强化学习在提升LLM推理能力时的一个关键瓶颈（零奖励障碍），并提出了一种有效的、无需修改RL算法本身的解决方案（添加简单样本进行数据干预）。这项研究直接推动了如何更有效地训练LLM以获得更强的通用推理能力，完全契合您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#240",
        "title": "A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling",
        "link": "/arxiv/2510.04087",
        "arxiv_id": "2510.04087",
        "authors": "Hyung Gyu Rho",
        "summary": "Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is \\textit{better}, but what is \\textit{good enough}. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.",
        "subjects": "Methodology, Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.795959",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **论文核心贡献**: 这篇论文的本质是提出了一种新的奖励模型训练框架和相应的自适应推理策略。它通过引入“外部选项”来训练奖励模型，使其不仅能判断“哪个更好”，还能判断“是否足够好”。基于此，它提出了“best of mini-N in-loop”策略，以在推理时动态平衡可靠性和计算效率。 - **是否符合目标**: 这项工作直接针对大语言模型在生成响应时的**基础能力**——即输出质量和可靠性。它并非将LLM应用于特定领域，而是改进了LLM自身在偏好对齐和采样这一核心环节的机制。一个可靠的、能够识别并拒绝“不可接受”答案的模型，是进行高质量推理的先决条件。如果模型连基本的可靠性都无法保证，那么任何复杂的推理任务都无从谈起。因此，这项工作属于改进LLM基础能力的范畴，符合核心目标。 2.  **第二步：正面指标** - 论文虽然未直接提及\"reasoning\"，但其解决的\"reliability failures\"（可靠性失败）问题，尤其是在\"hard prompts\"（困难提示词）上，与模型在复杂推理任务上的表现密切相关。一个在困难问题上频繁产生不可靠输出的模型，其推理能力必然是不足的。 - 论文的核心技术是改进\"reward model\"，这是强化学习（RLHF）流程中的关键组件，因此与强化学习（RL）主题高度相关。 3.  **第三步：排除标准** - 论文不涉及多模态、视觉或任何特定应用领域（如医疗、化学等）。其实验是在IMDB情感分析这一通用NLP基准上进行的，但方法本身是通用的。 - 论文虽然提到了\"reliability\"（可靠性），但它并未被归入排除标准中的“模型可靠性（应用层面）”，如水印、安全等。它关注的是模型输出的**内在质量**，而非外部应用层面的防护措施。 4.  **第四步：处理特殊和模糊情况** - **幻觉/可靠性**: 这篇论文是处理“可靠性失败”问题的绝佳范例。它不是在应用层面讨论如何防御，而是提出了一种**新的建模和推理方法**来从根本上减少模型产生不可接受输出的概率。这完全符合“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留”的原则。通过让模型学会“拒绝”或“重试”不可靠的生成路径，它间接提升了模型解决复杂问题的整体能力和鲁棒性。 5.  **第五步：最终决策** - 综合来看，这篇论文通过改进奖励模型和采样策略，增强了LLM输出的内在可靠性。这种可靠性是模型进行有效通用推理的基石。它提出的是一种通用的、方法论层面的改进，而非特定领域的应用。因此，它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    },
    {
        "index": "#360",
        "title": "Provenance Networks: End-to-End Exemplar-Based Explainability",
        "link": "/arxiv/2510.03361",
        "arxiv_id": "2510.03361",
        "authors": "Ali Kayyam, Anusha Madan Gopal, M. Anthony Lewis",
        "summary": "We introduce provenance networks, a novel class of neural models designed to provide end-to-end, training-data-driven explainability. Unlike conventional post-hoc methods, provenance networks learn to link each prediction directly to its supporting training examples as part of the model's normal operation, embedding interpretability into the architecture itself. Conceptually, the model operates similarly to a learned KNN, where each output is justified by concrete exemplars weighted by relevance in the feature space. This approach facilitates systematic investigations of the trade-off between memorization and generalization, enables verification of whether a given input was included in the training set, aids in the detection of mislabeled or anomalous data points, enhances resilience to input perturbations, and supports the identification of similar inputs contributing to the generation of a new data point. By jointly optimizing the primary task and the explainability objective, provenance networks offer insights into model behavior that traditional deep networks cannot provide. While the model introduces additional computational cost and currently scales to moderately sized datasets, it provides a complementary approach to existing explainability techniques. In particular, it addresses critical challenges in modern deep learning, including model opaqueness, hallucination, and the assignment of credit to data contributors, thereby improving transparency, robustness, and trustworthiness in neural models.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.917734",
        "filter_reason": "这篇论文符合筛选标准，应该保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Provenance Networks”的新型神经模型架构。其本质并非直接提出一种新的推理范式（如思维链），而是通过一种创新的架构设计，将模型的预测与其训练数据中的具体样本（exemplars）直接关联起来，从而实现内在的、端到端的可解释性。虽然其切入点是可解释性，但其最终目标之一是解决“幻觉”问题。提升模型的可靠性、减少幻觉，是提高其通用推理能力（尤其是推理结果的质量和可信度）的关键一环。因此，这篇论文的本质是改进LLM的基础能力，而非将其应用于特定领域。 2.  **第二步：正面指标** 论文摘要中虽然没有直接出现“LLM”或“reasoning”等词，但它明确提到了其方法旨在解决现代深度学习（尤其是LLM面临的核心挑战）中的“幻觉”问题。幻觉的减少直接关系到模型逻辑推理和问题解决能力的质量。 3.  **第三步：排除标准** 论文不涉及多模态、特定应用领域（如医疗、化学）或模型基础设施（如部署、水印）。它聚焦于模型架构和内在行为，因此不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** 这是判断本论文的关键。论文属于“幻觉/可解释性”的特殊情况。筛选标准明确指出：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性...从而提升模型的通用可靠性和推理质量，应该保留。” -   **提出新方法**: 论文提出的“Provenance Networks”是一种全新的模型架构。 -   **减少幻觉**: 摘要明确指出该方法“addresses critical challenges... including... hallucination”。 -   **提升通用可靠性和推理质量**: 通过让模型的每一个预测都有据可查（基于真实训练样本），该方法极大地增强了模型输出的可信度和鲁棒性，从而直接提升了其推理结果的质量。一个不胡编乱造的模型，其推理能力才具有实际价值。 5.  **第五步：最终决策** 综合来看，尽管这篇论文的标题和摘要更侧重于“可解释性”，但其核心贡献——通过一种新颖的架构来根除“幻觉”——直接服务于提升大语言模型通用推理能力的最终目标。它从“保证推理质量”这一根本性问题上切入，为构建更可靠、更强大的推理模型提供了新的思路。因此，它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    },
    {
        "index": "#404",
        "title": "Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data",
        "link": "/arxiv/2510.03264",
        "arxiv_id": "2510.03264",
        "authors": "Syeda Nahida Akter, Shrimai Prabhumoye, Eric Nyberg, Mostofa Patwary, Mohammad Shoeybi, Yejin Choi, Bryan Catanzaro",
        "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs revolves around post-training on high-quality, reasoning-intensive data. While emerging literature suggests that reasoning data is increasingly incorporated also during the mid-training stage-a practice that is relatively more proprietary and less openly characterized-the role of such data in pretraining remains unclear. In particular, due to the opaqueness of pretraining corpora in most frontier models, the effect of reasoning data introduced at different phases of pre- and/or post-training is relatively less reported in the scientific literature. This raises several important questions: Is adding reasoning data earlier during pretraining any better than introducing it during post-training? Could earlier inclusion risk overfitting and harm generalization, or instead establish durable foundations that later fine-tuning cannot recover? We conduct the first systematic study of how reasoning data-varying in scale, diversity, and quality-affects LLM performance when introduced at different stages of training. We find that front-loading reasoning data into pretraining is critical (19% avg gain), establishing foundational capabilities that cannot be fully replicated by later-stage SFT, even with more data. We uncover an asymmetric principle for optimal data allocation: pretraining benefits most from broad diversity in reasoning patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg gain). We show that high-quality pretraining data has latent effects, activated only after SFT, and that naively scaling SFT data can be detrimental, washing away the benefits of early reasoning injection. Our results challenge the conventional separation of language modeling and reasoning, providing a principled guide for strategically allocating data across the entire training pipeline to build more capable models.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.965629",
        "filter_reason": "这篇论文完全符合筛选要求，是一篇关于提升大语言模型（LLM）本身通用推理能力的前沿研究。 **判断过程如下:** 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 这篇论文的核心贡献并非将LLM应用于某个特定领域，而是对LLM的训练过程本身进行了系统性研究。它深入探讨了“推理数据”在训练流程（预训练 vs. 后训练）不同阶段的作用，并提出了一种新的训练范式——即通过“前置加载”推理数据并依据“非对称原则”来优化数据分配，从而从根本上提升模型的推理能力。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——论文高度相关。** 论文标题和摘要中明确包含了核心概念“Large language models (LLMs)”和能力方向“reasoning abilities”。其研究内容直接聚焦于如何通过优化训练数据来提升模型的“problem-solving”能力。这些都是筛选标准中明确列出的正面指标。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 该论文是纯粹关于语言模型训练方法论的研究，完全没有涉及多模态、视觉、医疗、化学、机器人等特定应用领域，也未讨论模型部署、水印或应用层安全等问题。因此，它成功避开了所有排除标准。 4.  **第四步：特殊和模糊情况——不适用。** 该论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需进行特殊判断。 **最终决策:** 综合以上分析，这篇论文的本质是探索如何更有效地训练LLM以获得更强的**通用推理能力**。它提出的“前置加载”和“非对称原则”直接为“构建更强大的模型”提供了原则性指导，这正是研究课题“大语言模型通用推理能力”所追求的核心目标。因此，这篇论文应被**保留**。"
    },
    {
        "index": "#408",
        "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning",
        "link": "/arxiv/2510.03259",
        "arxiv_id": "2510.03259",
        "authors": "Yoonjeon Kim, Doohyuk Jang, Eunho Yang",
        "summary": "Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by itself. We argue that large reasoning models lack this meta-awareness property by proving severe misalignment between true rollouts and predicted meta information. We posit that aligning meta-prediction with true rollouts will lead to significant performance gains. To verify this hypothesis, we design a training pipeline that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced meta-awareness directly translates to improved accuracy. Unlike existing meta-cognitive reasoning models, our method does not require external training sources but leverages self-generated signals to train meta-awareness. Moreover, our method enables efficient training by i) filtering out zero-variance prompts that are either trivial or unsolvable and ii) cutting off lengthy rollouts when they are unlikely to lead to correct answers. The results are inspiring: our strategy yields significant improvements in both accuracy and training efficiency on in-domain tasks and shows strong generalization to out-of-domain benchmarks. More specifically, our method can speed up GRPO training by over 1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 % boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks spanning logical, scientific, and coding domains.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.967528",
        "filter_reason": "这篇论文完全符合筛选要求，其核心贡献是提出了一种全新的训练范式来增强大语言模型的通用推理能力。以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心是关于改进LLM的基础推理能力。它识别出当前大推理模型普遍缺乏“元认知”能力（即模型对自身思考过程的认知），并提出了一种名为“通过自我对齐提升元认知（MASA）”的训练流水线来解决这一问题。该方法通过将模型预测的元信息与其真实的推理过程进行对齐，从而直接提升了模型的推理准确性和效率。这是一种方法论层面的创新，旨在增强模型内在的、通用的推理机制，而不是将LLM作为工具应用于特定领域。因此，根据第一步，应**保留**。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文摘要中包含了大量高相关性的正面指标： - **核心概念**: \"reasoning models\", \"language models\"。 - **能力方向**: \"reasoning\", \"mathematical reasoning\" (AIME25, 数学基准), \"logical reasoning\", \"scientific\", \"coding domains\"。 - **训练方法**: \"reinforcement learning\", \"self-generated signals\" (自我进化/自我对齐的体现)。 这些关键词高度集中于“大语言模型”的“推理能力”和“强化学习训练方法”，与筛选标准高度吻合。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文完全没有涉及排除标准中的任何领域。它不讨论多模态、视觉，不聚焦于医疗、化学等特定应用，也不涉及水印、安全等模型可靠性问题。其所有实验和论证都围绕着提升模型在数学、逻辑、科学和编码等通用推理基准上的表现。因此，根据第三步，不应排除。 4.  **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: 论文的核心“元认知”和“自我对齐”思想，可以被视为一种提升模型内在可靠性的方法。通过校准模型对自身思考过程的预测与实际行为，减少了内部不一致性，这从根本上提升了推理的质量和可信度，从而减少了产生错误推理（一种形式的幻觉）的可能性。这完全符合“提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的保留条件。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种创新的、通用的训练方法（MASA），旨在通过增强模型的“元认知”能力来直接提升其通用推理能力。它触及了LLM推理能力的根本机制，并在数学、逻辑等多个通用领域验证了其有效性。这与您的研究目标——“致力于提高大语言模型（LLM）本身的『通用推理能力』”——完美契合。因此，最终判断为**True**。"
    },
    {
        "index": "#412",
        "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents",
        "link": "/arxiv/2510.03253",
        "arxiv_id": "2510.03253",
        "authors": "Heyang Gao, Zexu Sun, Erxue Min, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Chen",
        "summary": "Large Language Models (LLMs) as autonomous agents are increasingly tasked with solving complex, long-horizon problems. Aligning these agents via preference-based offline methods like Direct Preference Optimization (DPO) is a promising direction, yet it faces a critical granularity mismatch. Trajectory-level DPO provides a signal that is too coarse for precise credit assignment, while step-level DPO is often too myopic to capture the value of multi-step behaviors. To resolve this challenge, we introduce Hierarchical Preference Learning (HPL), a hierarchical framework that optimizes LLM agents by leveraging preference signals at multiple, synergistic granularities. While HPL incorporates trajectory- and step-level DPO for global and local policy stability, its core innovation lies in group-level preference optimization guided by a dual-layer curriculum. Our approach first decomposes expert trajectories into semantically coherent action groups and then generates contrasting suboptimal groups to enable preference learning at a fine-grained, sub-task level. Then, instead of treating all preference pairs equally, HPL introduces a curriculum scheduler that organizes the learning process from simple to complex. This curriculum is structured along two axes: the group length, representing sub-task complexity, and the sample difficulty, defined by the reward gap between preferred and dispreferred action groups. Experiments on three challenging agent benchmarks show that HPL outperforms existing state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO loss effectively integrates preference signals across multiple granularities, while the dual-layer curriculum is crucial for enabling the agent to solve a wide range of tasks, from simple behaviors to complex multi-step sequences.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T22:03:55.974743",
        "filter_reason": "这篇论文完全符合筛选要求，应被保留。以下是基于筛选标准的详细判断过程： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种名为“分层偏好学习”的新训练框架（HPL），旨在优化大语言模型智能体。其目标是解决LLM在处理“长时程复杂问题”时，现有对齐方法（如DPO）面临的“粒度不匹配”问题。 - **符合性**: 这完全符合“改进LLM的基础能力”和“提出新的训练范式”的标准。论文直接针对LLM在**多步推理和规划**能力上的核心挑战，提出了一种方法论层面的解决方案，而不是将LLM应用于特定领域。因此，根据第一步的核心判断，应予以保留。 2.  **第二步：正面指标** - **核心概念**: 论文标题和摘要中明确提到了“Large Language Models (LLMs)”和“LLM Agents”。 - **能力方向**: 论文聚焦于解决“complex, long-horizon problems”，这直接对应了**规划**和**问题解决**能力。其方法旨在提升模型处理“complex multi-step sequences”的能力，这是**多步推理**的核心。 - **训练方法**: 论文基于“Direct Preference Optimization (DPO)”进行改进，这属于**强化学习（RL）**的范畴，特别是偏好学习。 - **新兴范式**: 论文的研究对象是“LLM-based agents”，并提出了一个通用的优化框架。 - **结论**: 论文命中了所有关键的正面指标，进一步确认了其高度相关性。 3.  **第三步：排除标准** - **多模态与视觉**: 论文未涉及任何视觉或多模态内容。 - **特定应用领域**: 论文在通用的智能体基准上进行测试，没有限定在医疗、化学、机器人等任何特定应用领域。 - **模型可靠性（应用层面）**: 论文关注的是提升模型的推理能力，而非水印、安全等应用层面的可靠性问题。 - **结论**: 论文不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的是一种**通用的智能体优化框架**（HPL），其目的是增强LLM智能体解决长时程问题的**通用能力**。这完全符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”的原则。它不是“用于化学实验自动化的智能体”，而是“用于解决复杂长时程问题的智能体”，后者是通用推理能力的体现。 5.  **第五步：最终决策** - **综合分析**: 该论文的本质是提出一种创新的训练范式（HPL），通过分层和课程学习的方式，解决LLM在长时程任务中的多步推理和规划难题。它直接作用于LLM的核心能力，而非特定应用。论文内容与“大语言模型通用推理能力”的研究课题高度契合，是典型的、高质量的前沿研究。 **核心依据**: 论文的核心贡献——分层偏好学习（HPL）框架——是一种旨在提升LLM智能体在长时程、多步骤任务中规划和推理能力的通用训练方法论。这直接回应了筛选目标中“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心诉求。"
    },
    {
        "index": "#12",
        "title": "Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment",
        "link": "/arxiv/2510.05024",
        "arxiv_id": "2510.05024",
        "authors": "Nevan Wichers, Aram Ebtekar, Ariana Azarbal, Victor Gillioz, Christine Ye, Emil Ryd, Neil Rathi, Henry Sleight, Alex Mallen, Fabien Roger, Samuel Marks",
        "summary": "Large language models are sometimes trained with imperfect oversight signals, leading to undesired behaviors such as reward hacking and sycophancy. Improving oversight quality can be expensive or infeasible, motivating methods that improve learned behavior despite an imperfect training signal. We introduce Inoculation Prompting (IP), a simple but counterintuitive technique that prevents learning of an undesired behavior by modifying training prompts to explicitly request it. For example, to inoculate against reward hacking, we modify the prompts used in supervised fine-tuning to request code that only works on provided test cases but fails on other inputs. Across four settings we find that IP reduces the learning of undesired behavior without substantially reducing the learning of desired capabilities. We also show that prompts which more strongly elicit the undesired behavior prior to fine-tuning more effectively inoculate against the behavior when used during training; this serves as a heuristic to identify promising inoculation prompts. Overall, IP is a simple yet effective way to control how models generalize from fine-tuning, preventing learning of undesired behaviors without substantially disrupting desired capabilities.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T22:03:55.646833",
        "filter_reason": "这篇论文符合筛选标准，应该保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为“Inoculation Prompting”的**新训练范式**，用于解决LLM在训练过程中因监督信号不完美而学到不良行为（如奖励破解、谄媚）的问题。这并非将LLM作为工具应用于特定领域，而是直接作用于模型本身的训练过程和泛化行为，旨在提升模型的基础能力。因此，它通过了第一步的核心判断，应被保留。 2.  **第二步：正面指标** 论文明确包含核心概念“Large language models (LLMs)”。虽然摘要中没有直接使用“reasoning”一词，但它处理的“奖励破解”和“谄媚”行为，本质上是模型在目标导向问题解决中的**推理缺陷**或**泛化错误**。一个会“作弊”或“迎合”的模型，其推理过程是不可靠的。因此，该方法通过提升模型的内在可靠性，间接但根本性地增强了其通用问题解决和推理能力的质量。论文也涉及了与强化学习（RLHF）相关的“监督微-tuning”和“奖励”概念。 3.  **第三步：排除标准** 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）等内容。虽然它处理了可靠性问题，但并非应用层面的水印或安全防护，而是从训练机制上**根本性地改进模型行为**，因此不属于排除标准中的“模型可靠性（应用层面）”。 4.  **第四步：处理特殊和模糊情况** 这篇论文恰好符合“幻觉/可解释性/安全”的特殊情况处理规则。论文提出了一种新方法（IP），来减少模型系统性的不良行为（奖励破解、谄媚），这直接提升了模型的**通用可靠性**。一个行为更可靠、更少“投机取巧”的模型，其输出的推理质量和可信度自然会更高。这与仅仅在应用层添加安全过滤器的做法有本质区别，它是在增强模型本身的能力。 **最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种新颖的训练方法论，用于提升LLM的行为对齐和泛化能力，防止其学到不良的推理策略。这直接关系到模型通用推理能力的**质量和鲁棒性**。虽然它不像思维链那样直接教授“如何推理”，但它解决了“为何会错误推理”的更深层次问题，是提升LLM通用推理能力不可或缺的一环。因此，该论文完全符合研究范围。"
    },
    {
        "index": "#22",
        "title": "Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking",
        "link": "/arxiv/2510.04930",
        "arxiv_id": "2510.04930",
        "authors": "Ali Saheb Pasand, Elvis Dohmatob",
        "summary": "Grokking is the phenomenon whereby, unlike the training performance, which peaks early in the training process, the test/generalization performance of a model stagnates over arbitrarily many epochs and then suddenly jumps to usually close to perfect levels. In practice, it is desirable to reduce the length of such plateaus, that is to make the learning process \"grok\" faster. In this work, we provide new insights into grokking. First, we show both empirically and theoretically that grokking can be induced by asymmetric speeds of (stochastic) gradient descent, along different principal (i.e singular directions) of the gradients. We then propose a simple modification that normalizes the gradients so that dynamics along all the principal directions evolves at exactly the same speed. Then, we establish that this modified method, which we call egalitarian gradient descent (EGD) and can be seen as a carefully modified form of natural gradient descent, groks much faster. In fact, in some cases the stagnation is completely removed. Finally, we empirically show that on classical arithmetic problems such as modular addition and sparse parity problem which this stagnation has been widely observed and intensively studied, that our proposed method eliminates the plateaus.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T22:03:55.656724",
        "filter_reason": "根据您提供的筛选标准，我对这篇论文进行了如下分析： 1.  **第一步：核心判断** 这篇论文的本质是提出一种新的优化算法（Egalitarian Gradient Descent, EGD），用以解决模型学习过程中的一个特定现象——“Grokking”。Grokking指的是模型的泛化能力在长时间停滞后会突然跃升的现象。论文的核心贡献是通过修改梯度下降的动力学机制，来加速模型在算术等问题上的泛化。这属于改进模型**基础能力**和**训练范式**的研究。它关注的是模型“如何学习”和“如何更好地泛化”这一根本性问题，特别是在数学推理任务上，而不是将模型作为工具应用到特定领域。因此，**符合保留标准**。 2.  **第二步：正面指标** - **能力方向**: 论文明确在经典的**算术问题**上进行验证，这直接关联到**数学推理**能力，是通用推理的核心组成部分。 - **训练方法**: 论文提出了一种全新的训练优化方法（EGD），可以看作是自然梯度法的一种精心修改。这属于对新训练范式的探索。 - **核心概念**: 虽然摘要未直接提及\"LLM\"，但\"Grokking\"现象最初是在Transformer架构（现代LLM的基础）上被发现和广泛研究的。因此，该研究与LLM的训练动力学高度相关。 3.  **第三步：排除标准** - 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或应用层面的模型可靠性（如水印、安全）。**未触及任何排除标准**。 4.  **第四步：处理特殊和模糊情况** - 主要的模糊点在于摘要中未出现\"LLM\"关键词。然而，研究的核心问题——加速模型在数学任务上的泛化——是提升LLM推理能力的关键挑战。这项工作旨在解决一个底层的、通用的学习机制问题，其成果（EGD算法）可以被广泛应用于包括LLM在内的各种神经网络模型，以提升其学习效率和推理质量。因此，可以判断其研究目标是通用的，而非特定于某个小领域。 5.  **第五步：最终决策** 综合以上分析，尽管论文摘要未明确提及\"LLM\"，但其研究的核心问题（Grokking）、验证的任务（数学推理）以及提出的方法（新的训练优化范式）都直指提升模型（特别是Transformer类模型）的基础学习和泛化能力。这种对底层学习机制的改进，是提升大语言模型通用推理能力的根本性工作之一。因此，这篇论文**完全符合**您的研究范围。"
    },
    {
        "index": "#158",
        "title": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models",
        "link": "/arxiv/2510.03817",
        "arxiv_id": "2510.03817",
        "authors": "Philipp Becker, Niklas Freymuth, Serge Thilges, Fabian Otto, Gerhard Neumann",
        "summary": "On-policy Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model's most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language Models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model's inference behavior. Across datasets, model families, and advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T22:03:55.767894",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种名为TROLL的新方法，用于改进大语言模型（LLM）的强化学习（RL）训练过程。它用一种更原则性的“信任区域投影”替代了当前主流PPO算法中的“裁剪”机制，从而提升了训练的稳定性、速度和最终性能。 - **符合性分析**: 这完全属于“提出新的训练范式”来“改进LLM的基础能力”的范畴。强化学习（特别是RLHF）是提升LLM对齐、遵循指令和进行复杂推理（如规划、多步问题解决）能力的关键训练技术。因此，对RL算法本身的优化，直接关系到LLM通用推理能力的上限。它不是将LLM应用于某个领域，而是在打磨LLM这块“砖”本身。 2.  **第二步：正面指标** - **核心概念**: 论文明确聚焦于“Large language models, LLMs”。 - **训练方法**: 论文的核心就是关于“reinforcement learning (RL)”，旨在改进PPO这一主流RL算法。这与筛选标准中的“强化学习优化”高度吻合。 - **能力方向**: 虽然摘要没有直接提及“数学推理”或“逻辑推理”等具体任务，但它指出TROLL能带来更高的“最终成功率”。在RL微调中，奖励信号通常用于引导模型完成更复杂的任务，这其中就包含了大量的推理和问题解决能力。一个更稳定、更高效的RL训练算法，其最终目的就是为了更好地塑造和提升LLM的这些通用能力。 3.  **第三步：排除标准** - 论文未涉及任何多模态（视觉）、特定应用领域（医疗、化学等）或模型可靠性（水印、安全）的内容。它是一个纯粹的、针对LLM训练算法的优化研究，因此完全避开了所有排除项。 4.  **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊模糊情况，其焦点非常清晰：优化训练算法。 5.  **第五步：最终决策** - **综合判断**: 该论文提出了一种基础性的训练算法改进（TROLL），旨在优化用于提升LLM通用能力的核心工具——强化学习。这项工作直接作用于LLM的能力塑造过程，而非其应用。它通过提升训练的稳定性和效率，为开发出具有更强通用推理能力的LLM铺平了道路。因此，这篇论文与你的核心目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度一致，应当被保留。"
    },
    {
        "index": "#173",
        "title": "Group Policy Gradient",
        "link": "/arxiv/2510.03679",
        "arxiv_id": "2510.03679",
        "authors": "Junhua Chen, Zixi Zhang, Hantao Zhong, Rika Antonova",
        "summary": "We introduce Group Policy Gradient (GPG), a family of critic-free policy-gradient estimators for general MDPs. Inspired by the success of GRPO's approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a learned value function with a group-based Monte Carlo advantage estimator, removing the memory, compute, and hyperparameter costs of training a critic while preserving PPO's clipped-objective structure. We prove the consistency of the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate empirically that GPG matches or outperforms PPO on standard benchmarks. GPG makes better use of parallel simulations, which, together with its critic-free design, results in more efficient use of computational resources than PPO.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T22:03:55.780365",
        "filter_reason": "根据筛选标准，我的判断过程如下： 1.  **第一步：核心判断** 这篇论文的本质是提出了一种新的强化学习算法——Group Policy Gradient (GPG)。虽然论文本身是关于通用强化学习理论的，但其核心动机和直接应用背景是“Reinforcement Learning from Human Feedback (RLHF)”。RLHF是当前提升大语言模型（尤其是其通用推理、对齐和问题解决能力）最核心的训练范式之一。因此，这篇论文并非将LLM作为工具应用于特定领域，而是致力于**改进用于训练LLM的基础方法（即RLHF算法本身）**。它通过提出更高效、更稳定的无评判员策略梯度方法，直接优化了LLM的训练过程。这完全符合筛选标准中“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”以及“强化学习优化”的描述。 2.  **第二步：正面指标** 论文明确提到了核心概念“Reinforcement Learning from Human Feedback (RLHF)”，这是与LLM训练紧密相关的关键主题。同时，其研究的“policy-gradient estimators”是RLHF的核心技术组件。这些指标强烈表明该论文与提升LLM能力的研究高度相关。 3.  **第三步：排除标准** 该论文的研究焦点是通用的MDP和RL算法，没有涉及多模态、视觉、医疗、化学等任何特定应用领域，也未讨论水印、安全等应用层面的可靠性问题。因此，它不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** 本情况不涉及智能体/工具使用或幻觉/安全等特殊范畴，无需进行特殊判断。 5.  **第五步：最终决策** 综合来看，这篇论文虽然形式上是一篇强化学习领域的理论性论文，但其核心贡献在于为LLM的关键训练技术（RLHF）提供了一种更优的算法实现。通过降低RLHF的计算和调优成本，GPG使得训练更强大的LLM变得更加可行和高效。这种对基础训练范式的优化，是提升LLM“通用推理能力”的根本性工作之一。因此，这篇论文完全符合我的研究范围，应当被保留。"
    },
    {
        "index": "#200",
        "title": "RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models",
        "link": "/arxiv/2510.03515",
        "arxiv_id": "2510.03515",
        "authors": "Lianghuan Huang, Sagnik Anupam, Insup Lee, Shuo Li, Osbert Bastani",
        "summary": "Reinforcement learning (RL) has emerged as a promising strategy for finetuning small language models (SLMs) to solve targeted tasks such as math and coding. However, RL algorithms tend to be resource-intensive, taking a significant amount of time to train. We propose RAPID, a novel RL algorithm that can substantially reduce the running time of RL. Our key insight is that RL tends to be costly due to the need to perform both inference and backpropagation during training. To maximize use of computational resources, our algorithm performs inference in large batches, and then performs off-policy policy gradient updates in mini-batches. For off-policy updates, we incorporate group advantage estimation into the policy gradient algorithm, and derive an importance weighted estimator to correct for the bias arising from off-policy learning. Our experiments demonstrate that our algorithm can reduce running time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms while maintaining similar or better accuracy.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T22:03:55.802501",
        "filter_reason": "根据您提供的筛选标准，我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种名为RAPID的新型强化学习（RL）算法。其目的是为了更高效地微调小型语言模型（SLM），使其在数学和编码等需要推理能力的任务上表现更好。 - **是否符合**: **符合**。这篇论文的本质是提出一种新的训练范式（一种优化的RL算法），用以增强语言模型的数学和编程能力，这两者都是通用推理能力的核心组成部分。它并非将LLM作为工具应用到特定领域（如化学、医疗），也不是关于模型基础设施或部署优化的研究。尽管它强调了“效率”，但其最终目的是服务于“提升推理能力”这一核心。 2.  **第二步：正面指标** - **核心概念**: 论文研究对象是“Small Language Models (SLMs)”，与LLMs同属语言模型范畴，其方法很可能适用于LLMs。 - **能力方向**: 论文明确指出其目标是让模型解决“targeted tasks such as math and coding”，这直接命中了“math reasoning”和“problem-solving”这两个关键能力方向。 - **训练方法**: 论文的核心就是一种“Reinforcement Learning (RL)”算法，完全符合此项标准。 - **新兴范式**: 未涉及智能体、工具使用等，但已命中前几项关键指标。 3.  **第三步：排除标准** - 论文完全不涉及多模态、特定应用领域（医疗、化学等）或模型可靠性（水印、安全）等排除标准。 4.  **第四步：处理特殊和模糊情况** - **效率 vs. 基础能力**: 这是一个潜在的模糊点。论文的标题和摘要大量强调了“高效”和“减少运行时间”。然而，这种效率提升是**手段**，而非**目的**。其最终目标是通过更高效的RL训练，让模型获得更好的推理能力（“maintaining similar or better accuracy”）。因此，这应被视为一种对现有提升推理能力方法的“方法论优化”，属于“提出新的训练范式”的范畴，应当保留。如果一篇论文只讨论如何加速模型推理而不关心其能力提升，那才应该被排除。但这篇论文的效率提升是直接服务于能力训练过程的。 5.  **第五步：最终决策** - 综合分析，这篇论文的核心贡献是提出了一种新的、更高效的强化学习训练算法，专门用于提升语言模型在数学和编码等推理任务上的表现。这完全符合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标，属于“提出新的训练范式、增强其逻辑、数学……等通用能力”的研究。因此，这篇论文应该被**保留**。"
    },
    {
        "index": "#236",
        "title": "Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining",
        "link": "/arxiv/2510.03313",
        "arxiv_id": "2510.03313",
        "authors": "Anirudh Subramanyam, Yuxin Chen, Robert L. Grossman",
        "summary": "Scaling laws for language model training traditionally characterize how performance scales with model size and dataset volume. Prior work has explored architecture variants and data treatments such as dataset filtering and noise injection in language model pretraining; however, these studies have not formalized data quality within a principled scaling law. We introduce a dimensionless data-quality parameter Q, and propose a quality-aware scaling law extending the Chinchilla framework to predict loss as a joint function of model size, data volume, and data quality. The law is motivated by an effective-sample-size and information-theoretic view of noisy or redundant corpora, and it admits two practical estimators for Q: (i) a corruption rate proxy and (ii) a deficiency measure. Through synthetic experiments in neural machine translation and autoregressive modeling -- where we systematically control data quality via multiple levels of noise injection and coverage variation -- we show that loss scales predictably with data quality and that higher-quality data can substantially reduce model size and hence compute requirements. Our results demonstrate a sublinear decay of effective data with quality and robustness to moderate data corruption; out-of-sample evaluations further validate the predictive form of the law. Unlike prior empirical analyses, our work establishes an explicit, generalizable law for data quality, offering concrete guidance for balancing data curation effort and model scale in large-scale pretraining.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T22:03:55.834254",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。我的判断过程如下： **第一步：核心判断** - 论文的本质是关于改进大语言模型（LLM）的**基础能力**。它没有将LLM作为工具去解决特定领域的问题，而是聚焦于LLM最核心的训练环节——**预训练**。其核心贡献是提出一个“质量感知缩放定律”，将数据质量（Q）作为一个与模型大小、数据体量同等重要的维度，纳入到预测模型性能的框架中。这直接关系到如何更高效、更科学地训练出能力更强的**基础模型**。一个更好的基础模型是所有通用能力（包括推理）的基石。因此，这篇论文的本质是改进LLM的内在能力，符合保留标准。 **第二步：正面指标** - 论文明确包含了核心概念“Large language models”。 - 虽然没有直接以“reasoning”或“planning”为研究目标，但其研究成果——通过优化数据质量来提升模型性能——是提升模型通用推理能力的**基础性前提**。一个在更高质量数据上训练的模型，其在逻辑、数学等推理任务上的表现上限会更高。这属于对模型基础能力的增强。 **第三步：排除标准** - 论文不涉及多模态、视觉、机器人控制或任何特定应用领域（如医疗、化学）。 - 论文的研究焦点是模型训练的理论和实践，而非模型部署、硬件加速或应用层面的水印、安全等问题。因此，它完全避开了所有排除标准。 **第四步：处理特殊和模糊情况** - 本文不涉及智能体、工具使用、幻觉或安全性等特殊议题，因此无需进行特殊判断。 **第五步：最终决策** 综合来看，这篇论文虽然不直接提出一种新的推理方法（如CoT），但它从更根本的层面——**预训练数据的质量和缩放规律**——出发，为如何构建一个性能更强、效率更高的基础模型提供了理论指导和实证依据。这项工作是提升LLM所有下游能力（包括通用推理能力）的“地基”性研究。对于一个旨在探索“如何提高LLM通用推理能力”的课题来说，理解并优化其基础训练过程是至关重要的一环。因此，该论文高度契合研究范围。"
    },
    {
        "index": "#329",
        "title": "Learning Linear Regression with Low-Rank Tasks in-Context",
        "link": "/arxiv/2510.04548",
        "arxiv_id": "2510.04548",
        "authors": "Kaito Takanami, Takashi Takahashi, Yoshiyuki Kabashima",
        "summary": "In-context learning (ICL) is a key building block of modern large language models, yet its theoretical mechanisms remain poorly understood. It is particularly mysterious how ICL operates in real-world applications where tasks have a common structure. In this work, we address this problem by analyzing a linear attention model trained on low-rank regression tasks. Within this setting, we precisely characterize the distribution of predictions and the generalization error in the high-dimensional limit. Moreover, we find that statistical fluctuations in finite pre-training data induce an implicit regularization. Finally, we identify a sharp phase transition of the generalization error governed by task structure. These results provide a framework for understanding how transformers learn to learn the task structure.",
        "subjects": "Disordered Systems and Neural Networks, Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T22:03:55.921096",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。 1.  **核心判断 (第一步):** 论文的核心并非将LLM作为工具应用于特定领域，而是深入探究LLM的一项基础且关键的能力——上下文学习。ICL是实现复杂推理和多步问题解决的重要机制。论文通过理论分析，揭示了Transformer模型在ICL过程中如何学习和泛化任务结构。这属于“改进LLM的基础能力”和“增强其通用能力”的范畴，因为它提供了理解并最终可能优化这一核心能力的理论基础。它不是关于应用，而是关于能力的内在机理。 2.  **正面指标 (第二步):** 论文明确涉及了LLM的核心架构，并聚焦于其关键能力之一。虽然摘要中没有直接出现\"reasoning\"一词，但\"learning to learn the task structure\"（学会学习任务结构）和\"generalization error\"（泛化误差）是衡量模型通用问题解决和推理能力的核心理论指标。理解ICL的泛化机制，对于提升模型在数学、逻辑等需要多步推理的任务上的表现至关重要。 3.  **排除标准 (第三步):** 论文不涉及任何排除项。它不关注多模态，没有应用到医疗、化学等特定领域，也不讨论模型可靠性（如水印、安全）。它使用“线性回归”这一数学任务作为理论分析的载体，这本身是一种通用能力的抽象，而非特定应用。 4.  **最终决策 (第五步):** 综合来看，这篇论文虽然是一篇理论性文章，没有提出一个可以直接应用的“提升技巧”，但它直指LLM核心能力——ICL——的黑盒。对于一个致力于“提高大语言模型通用推理能力”的研究者来说，理解其底层工作原理是设计和验证更有效方法论（如更好的CoT或训练范式）的先决条件。因此，这篇论文提供了至关重要的理论洞见，完全符合研究范围的前沿性和深度要求，应被判定为相关。"
    },
    {
        "index": "#375",
        "title": "RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback",
        "link": "/arxiv/2510.04096",
        "arxiv_id": "2510.04096",
        "authors": "Tommy Mordo, Sagie Dekel, Omer Madmon, Moshe Tennenholtz, Oren Kurland",
        "summary": "Competitive search is a setting where document publishers modify them to improve their ranking in response to a query. Recently, publishers have increasingly leveraged LLMs to generate and modify competitive content. We introduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that trains LLMs using preference datasets derived from ranking competitions. The goal of a publisher (LLM-based) agent is to optimize content for improved ranking while accounting for the strategies of competing agents. We generate the datasets using approaches that do not rely on human-authored data. We show that our proposed agents consistently and substantially outperform previously suggested approaches for LLM-based competitive document modification. We further show that our agents are effective with ranking functions they were not trained for (i.e., out of distribution) and they adapt to strategic opponents. These findings provide support to the significant potential of using reinforcement learning in competitive search.",
        "subjects": "Information Retrieval, Computer Science and Game Theory, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T22:03:55.970131",
        "filter_reason": "这篇论文符合研究范围，应予保留。判断依据如下： 1.  **核心判断（符合保留标准）**: 论文的核心是提出了一种名为“Reinforcement Learning from Ranker Feedback (RLRF)”的全新训练框架。这并非将LLM作为工具应用于特定领域，而是致力于改进LLM本身的能力。论文训练LLM智能体在竞争环境中进行内容优化，这涉及到对竞争对手策略的预判、适应和反制，本质上是在提升模型的**战略规划、多步推理和适应性问题解决能力**。这完全符合“改进LLM基础能力、提出新的训练范式、增强其规划、多步推理等通用能力”的保留标准。 2.  **正面指标（高度相关）**: 论文命中了多个关键的正面指标。 *   **核心概念**: 明确以大语言模型为核心研究对象。 *   **能力方向**: 研究的核心是LLM智能体在竞争环境中的**问题解决**和**规划**能力。 *   **训练方法**: 提出了一种新的强化学习方法（RLRF），这与“强化学习优化”直接相关。 *   **新兴范式**: 论文构建了基于LLM的智能体，并研究其在**多智能体系统**中的竞争与协作，这是一个非常前沿的研究方向。 3.  **排除标准（不触及）**: 论文不涉及多模态、医疗、化学等特定应用领域，也不关注模型基础设施或应用层面的水印、安全等问题。 4.  **特殊/模糊情况处理（符合保留条件）**: 论文的研究主题“竞争性搜索”可能会被误解为一个特定应用领域（如搜索引擎优化SEO）。然而，关键在于区分“应用领域”和“能力验证场景”。这里的“竞争性搜索”更像是一个用于验证和训练LLM**通用战略推理能力**的“沙盒”或“测试场”。论文的贡献点不是“一个更懂SEO的LLM”，而是“一种能教会LLM进行战略思考和适应对手的通用训练方法（RLRF）”。论文中提到的智能体能够“适应未经训练的排序函数”和“适应战略对手”，这恰恰证明了其学到的能力具有**泛化性**，而非局限于特定领域。因此，这属于“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”，应予以保留。 **最终决策**: 综合来看，该论文通过提出一种创新的强化学习训练范式，直接致力于提升LLM在复杂、动态和竞争性环境中的通用推理与规划能力，与你的核心研究目标高度契合。因此，应判定为符合要求。"
    }
]