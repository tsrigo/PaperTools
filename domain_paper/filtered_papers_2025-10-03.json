[
    {
        "index": "#8",
        "title": "FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol",
        "link": "/arxiv/2510.01674",
        "arxiv_id": "2510.01674",
        "authors": "He Zhang, Anzhou Zhang, Jian Dai",
        "summary": "Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT) organize internal deliberation but lack an explicit mechanism for external questioning that elicits self-revision. We present FOR-Prompting (From Objection to Revision Prompting), an asymmetric protocol where a Defender proposes an answer, an Objectioner raises question-style objections with no direct fixes, and a Host enforces consistency and closure. On GSM8K we observe about a 22% point gain over single-prompt and accuracy on par with CoT, with more than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1 judge. FOR-Prompting also corrects mistakes without tools or human supervision on tricky queries, and improves performance for small-scale model (approx. 19% accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for small models and on personal device use. Beyond factual QA, qualitative analyses on open-ended tasks show enhanced exploration and refinement, with dialogue traces that make assumptions and trade-offs explicit. The protocol is model agnostic and operates purely at the prompt level through role-structured turns, so it works with hosted and local models of different sizes without retraining, and it supports large-scale study of objection-guided reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-02",
        "category": "cs.MA",
        "crawl_time": "2025-10-07T00:13:07.817685",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **核心判断 (第一步):** 论文的核心贡献是提出了一种名为\"FOR-Prompting\"的新型提示协议。其本质不是将LLM应用于特定领域，而是致力于改进LLM自身的推理过程。它通过引入\"反对者\"角色，在模型内部模拟一种自我批判和修订的机制，从而弥补了现有思维链等方法在自我修正能力上的不足。这是一种直接提升LLM**通用推理能力**（特别是多步推理、自我修正和问题解决能力）的基础方法论研究，完全符合你的核心目标。 2.  **正面指标 (第二步):** 论文与多个正面指标高度相关。 *   **核心概念:** 论文的研究对象是大语言模型。 *   **能力方向:** 论文的核心是**推理**，并在数学推理基准GSM8K上进行了验证，同时探讨了其在开放性问题解决中的表现。 *   **新兴范式:** 论文提出了一种新的**推理协议**，与思维链、思维树等范式一脉相承，但其创新点在于引入了非对称的、基于反对的交互机制。这可以被看作是一种结构化的内部\"智能体\"协作，旨在增强模型的通用问题解决能力。 3.  **排除标准 (第三步):** 论文完全没有触及任何排除标准。它不涉及多模态、特定应用领域（如医疗、化学），也不讨论水印、安全等应用层面的可靠性问题。 4.  **特殊和模糊情况 (第四步):** *   **智能体/工具使用:** 论文中的Defender、Objectioner、Host角色是一种**通用的、模型内部的推理框架**，而非应用于特定领域的外部智能体。其目的是增强模型的内在推理质量，因此应该保留。 *   **幻觉/可解释性/安全:** 论文通过\"反对者\"提出质疑，迫使模型重新审视其假设和逻辑链，这直接有助于**减少推理过程中的错误（一种减少幻觉的形式）**，并通过对话追踪使推理过程更加透明，**增强了内在可解释性**。这完全符合保留条件。 **最终决策 (第五步):** 综合分析，这篇论文提出了一种创新的提示协议，旨在通过结构化的自我批判机制来显著提升大语言模型的通用推理和自我修正能力。它的研究内容、方法目标和实验验证都与\"提高大语言模型本身通用推理能力\"这一核心课题高度契合。因此，这篇论文是应该被**保留**的。"
    },
    {
        "index": "#4",
        "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective",
        "link": "/arxiv/2510.02272",
        "arxiv_id": "2510.02272",
        "authors": "Wen Yang, Junhong Wu, Chong Li, Chengqing Zong, Jiajun Zhang",
        "summary": "Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes a novel cross-linguistic perspective to investigate reasoning generalization. This raises a crucial question: $\\textit{Does the reasoning capability achieved from English RPT effectively transfer to other languages?}$ We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing a metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct a thorough parallel training study. Experimental results yield three key findings: $\\textbf{First-Parallel Leap}$, a substantial leap in performance when transitioning from monolingual to just a single parallel language, and a predictable $\\textbf{Parallel Scaling Law}$, revealing that cross-lingual reasoning transfer follows a power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as $\\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.236000",
        "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： **第一步：核心判断** 这篇论文的本质是研究如何提升大语言模型（LLM）的推理能力，特别是其泛化能力。论文的核心并非将LLM作为工具应用于某个特定领域，而是聚焦于LLM自身的基础能力——**推理的泛化性**。它通过“跨语言”这一新颖视角，深入探讨了强化后训练（RPT）带来的推理能力是否能从一个语言迁移到另一个语言。为了解决这个问题，论文提出了“并行训练”这一新的训练范式，并发现了“并行扩展定律”。这直接触及了“改进LLM基础能力、提出新的训练范式、增强其通用能力”的核心，因此符合保留标准。 **第二步：正面指标** 该论文命中了多个关键的正面指标： - **核心概念**: 论文研究对象是“Large Reasoning Models (LRMs)”，这是大语言模型（LLMs）的核心子集。 - **能力方向**: 论文通篇围绕“reasoning”展开，具体研究了“multilingual reasoning”和“reasoning generalization”，这正是你关注的核心能力方向。 - **训练方法**: 论文的核心方法论是“Reinforcement Post-Training (RPT)”，这属于“reinforcement learning”的范畴，是提升LLM能力的关键技术之一。 **第三步：排除标准** 该论文完全不涉及任何排除标准： - **多模态与视觉**: 论文的研究范围限定在文本和语言，不涉及视觉或多模态内容。 - **特定应用领域**: 论文的研究对象是通用语言和推理能力，而非生物、医疗、化学等特定应用领域。 - **模型可靠性**: 论文关注的是模型能力的“泛化性”，而非水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用、幻觉/可解释性等特殊情况的讨论。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于： 1.  提出了一个研究LLM推理能力泛化的新视角（跨语言）。 2.  揭示了当前以英语为中心的训练范式在推理泛化上的局限性（Monolingual Generalization Gap）。 3.  提出了一种新的训练方法（并行训练）并发现了一个新的规律（并行扩展定律），旨在构建具有更强、更通用推理能力的“语言无关”模型。 这些研究内容和发现，直接服务于“提高大语言模型本身的通用推理能力”这一核心目标，属于前沿且深刻的方法论研究。因此，这篇论文高度相关，应被筛选为 **True**。"
    },
    {
        "index": "#6",
        "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation",
        "link": "/arxiv/2510.02249",
        "arxiv_id": "2510.02249",
        "authors": "Tianyi Jiang, Yi Bin, Yujuan Ding, Kainian Zhu, Fei Ma, Jingkuan Song, Heng Tao Shen",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities on complex problems using long Chain-of-Thought (CoT) reasoning. However, they often suffer from overthinking, meaning generating unnecessarily lengthy reasoning steps for simpler problems. This issue may degrade the efficiency of the models and make them difficult to adapt the reasoning depth to the complexity of problems. To address this, we introduce a novel metric Token Entropy Cumulative Average (TECA), which measures the extent of exploration throughout the reasoning process. We further propose a novel reasoning paradigm -- Explore Briefly, Then Decide -- with an associated Cumulative Entropy Regulation (CER) mechanism. This paradigm leverages TECA to help the model dynamically determine the optimal point to conclude its thought process and provide a final answer, thus achieving efficient reasoning. Experimental results across diverse mathematical benchmarks show that our approach substantially mitigates overthinking without sacrificing problem-solving ability. With our thinking paradigm, the average response length decreases by up to 71% on simpler datasets, demonstrating the effectiveness of our method in creating a more efficient and adaptive reasoning process.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.237632",
        "filter_reason": "这篇论文完全符合筛选标准，应予以保留。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心是解决大语言模型在推理过程中的一个内在缺陷：“过度思考”。它并非将LLM应用于某个特定领域，而是聚焦于优化LLM的推理过程本身。论文提出了一种新的推理范式和一种名为“累积熵调节（CER）”的机制，旨在让模型能够动态地、自适应地调整其推理链的长度，从而实现更高效的推理。这完全符合“改进LLM的基础能力”、“增强其逻辑、数学、规划、多步推理等通用能力”的核心要求。 **第二步：正面指标——论文是否包含相关主题？** 论文高度匹配多个正面指标： - **核心概念**: 明确以\"Large Language Models (LLMs)\"为研究对象。 - **能力方向**: 直接针对\"reasoning\"能力，特别是在\"mathematical benchmarks\"上验证其\"problem-solving ability\"，目标是优化\"reasoning process\"。 - **新兴范式**: 提出了一种全新的\"reasoning paradigm\"（推理范式），这与思维链（CoT）属于同一类别，是对LLM推理方法论的创新。 **第三步：排除标准——论文是否聚焦于排除领域？** 论文完全不涉及任何排除标准领域： - **多模态与视觉**: 未提及。 - **特定应用领域**: 虽然在数学基准上测试，但数学是评估通用推理能力的标准测试集，而非论文要解决的应用领域。其提出的方法（CER）是通用的，不局限于数学。 - **模型可靠性（应用层面）**: 未讨论水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** 本论文不直接涉及智能体或幻觉等特殊情况。但其核心贡献——优化推理过程的效率和适应性——可以被视为提升模型内在“可靠性”和“推理质量”的一种方式，因为它避免了资源浪费和潜在的路径混乱，使推理过程更加稳健。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种通用的、方法论层面的创新（CER机制和“先简短探索，后决策”的范式），用于直接提升LLM的通用推理能力，使其推理过程更高效、更智能。这完全契合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#13",
        "title": "RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization",
        "link": "/arxiv/2510.02172",
        "arxiv_id": "2510.02172",
        "authors": "Zhaoning Yu, Will Su, Leitian Tao, Haozhu Wang, Aashu Singh, Hanchao Yu, Jianyu Wang, Hongyang Gao, Weizhe Yuan, Jason Weston, Ping Yu, Jing Xu",
        "summary": "Reinforcement learning with human-annotated data has boosted chain-of-thought reasoning in large reasoning models, but these gains come at high costs in labeled data while faltering on harder tasks. A natural next step is experience-driven learning, where models improve without curated labels by adapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with Self-restraint), a self-penalizing RL framework that converts the absence of gold labels into a useful learning signal. Instead of overcommitting to spurious majority votes, RESTRAIN exploits signals from the model's entire answer distribution: penalizing overconfident rollouts and low-consistency examples while preserving promising reasoning chains. The self-penalization mechanism integrates seamlessly into policy optimization methods such as GRPO, enabling continual self-improvement without supervision. On challenging reasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data. With Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to +140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on GPQA-Diamond, nearly matching gold-label training while using no gold labels. These results demonstrate that RESTRAIN establishes a scalable path toward stronger reasoning without gold labels.",
        "subjects": "Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.246453",
        "filter_reason": "这篇论文完全符合您的筛选标准。我的判断过程如下： 1.  **第一步：核心判断——论文本质是改进LLM基础能力。** 论文的核心贡献是提出了一种名为RESTRAIN的新训练框架。这是一种“自我驱动的、自惩罚的强化学习”方法，其目标是让大语言模型在没有人工标注数据的情况下，通过自我反思和惩罚机制来提升自身的推理能力。这直接属于“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴。它并非将LLM应用于特定领域，而是专注于改进模型本身的基础推理能力，因此应**保留**。 2.  **第二步：正面指标——论文高度匹配核心主题。** 论文摘要中明确包含了多个关键的正面指标： *   **核心概念**: \"large reasoning models\" (大语言模型的一种)。 *   **能力方向**: \"chain-of-thought reasoning\", \"challenging reasoning benchmarks\" (如数学推理AIME, 通用知识推理GPQA)。 *   **训练方法**: \"Reinforcement learning\" (强化学习), \"experience-driven learning\" (经验驱动学习), \"self-improvement\" (自我提升)。 这些主题与您的研究范围高度重合。 3.  **第三步：排除标准——论文未触及排除领域。** 论文的研究内容是纯粹的语言模型推理训练方法。它完全没有涉及多模态与视觉、任何特定的应用领域（如医疗、化学、机器人），也未讨论水印、安全等应用层面的可靠性问题。因此，它完全避开了所有排除标准。 4.  **第四步：处理特殊和模糊情况——不适用。** 这篇论文的情况非常清晰，不属于需要特殊处理的模糊案例。它提出的是一种通用的、能够提升模型内在推理质量的训练方法论。 5.  **第五步：最终决策——符合核心目标。** 综合以上分析，这篇论文的本质是提出一种创新的、可扩展的训练范式（RESTRAIN），旨在解决提升LLM通用推理能力过程中的一个核心痛点（对昂贵标注数据的依赖）。其研究成果直接体现在多个通用推理基准上的显著性能提升。这与您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标完全一致。因此，最终判断为**True**。"
    },
    {
        "index": "#12",
        "title": "Learning to Reason for Hallucination Span Detection",
        "link": "/arxiv/2510.02173",
        "arxiv_id": "2510.02173",
        "authors": "Hsuan Su, Ting-Yao Hu, Hema Swetha Koppula, Kundan Krishna, Hadi Pouransari, Cheng-Yu Hsieh, Cem Koc, Joseph Yitan Cheng, Oncel Tuzel, Raviteja Vemulapalli",
        "summary": "Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While most prior works frame hallucination detection as a binary task, many real-world applications require identifying hallucinated spans, which is a multi-step decision making process. This naturally raises the question of whether explicit reasoning can help the complex task of detecting hallucination spans. To answer this question, we first evaluate pretrained models with and without Chain-of-Thought (CoT) reasoning, and show that CoT reasoning has the potential to generate at least one correct answer when sampled multiple times. Motivated by this, we propose RL4HS, a reinforcement learning framework that incentivizes reasoning with a span-level reward function. RL4HS builds on Group Relative Policy Optimization and introduces Class-Aware Policy Optimization to mitigate reward imbalance issue. Experiments on the RAGTruth benchmark (summarization, question answering, data-to-text) show that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, demonstrating the necessity of reinforcement learning with span-level rewards for detecting hallucination spans.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.245909",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出一个名为RL4HS的强化学习框架，其目标是**提升LLM在特定任务上的推理能力**。论文将“幻觉片段检测”这一复杂任务明确地定义为一个“多步决策过程”，并探讨如何通过“显式推理”来解决。这并非将LLM作为工具应用于特定领域，而是致力于改进LLM本身的基础能力——即通过推理来识别自身输出的不一致性。这直接命中了“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的核心目标。 2.  **正面指标（第二步）：** 论文高度匹配多个关键正面指标。 *   **核心概念:** 论文的研究对象是 \"Large language models (LLMs)\"。 *   **能力方向:** 论文的核心是 \"reasoning\"，并明确使用了 \"Chain-of-Thought (CoT) reasoning\" 这一关键技术。 *   **训练方法:** 论文提出的方法是基于 \"reinforcement learning (RL)\" 的，具体是 \"RL4HS, a reinforcement learning framework\"。 3.  **排除标准（第三步）：** 论文未触及任何排除标准。它不涉及多模态、视觉，也不聚焦于医疗、化学等特定应用领域。虽然主题是“幻觉”，与可靠性相关，但其处理方式并非应用层面的水印或安全策略。 4.  **特殊/模糊情况处理（第四步）：** 这篇论文是“通过改进模型内在能力来减少幻觉”的绝佳范例。根据筛选标准，“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” 本文正是如此：它没有停留在表面检测，而是深入到模型内部，通过强化学习**激励模型进行更高质量的推理**，从而从根本上提升其识别自身错误的能力。这种方法论上的创新，旨在提升模型的内在推理质量，完全符合你的筛选意图。 **总结：** 论文的核心贡献是提出一种新的训练范式（RL4HS），通过强化学习来增强LLM的推理能力，以解决一个复杂的认知问题（幻觉片段检测）。这直接关系到提升LLM的通用推理质量和内在可靠性，与你的研究课题“提高大语言模型（LLM）本身的『通用推理能力』”高度契合。因此，应予以保留。"
    },
    {
        "index": "#9",
        "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration",
        "link": "/arxiv/2510.02227",
        "arxiv_id": "2510.02227",
        "authors": "Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Hengtao Shen",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This \"guidance-on-demand\" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at https://github.com/SII-Enigma/AMPO.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.244311",
        "filter_reason": "这篇论文完全符合你的研究范围，应被保留。判断依据如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“自适应多指导策略优化”（AMPO）的**新训练范式**。其本质目标是**增强大语言模型（LLM）的推理能力**，特别是通过改进强化学习（RLVR）方法来激发更长的思维链推理。这直接命中了你筛选标准中的“改进LLM的基础能力”、“提出新的训练范式”和“增强其逻辑、数学、多步推理等通用能力”。论文并非将LLM作为工具应用于特定领域，而是专注于提升模型本身的核心能力。 2.  **第二步：正面指标** 论文摘要中包含了多个关键的正面指标： *   **核心概念**: 明确提到了 \"Large Language Models (LLMs)\"。 *   **能力方向**: 核心主题是 \"reasoning ability\"，并具体在 \"mathematical reasoning\" 任务上进行了验证，同时提到了 \"LongCoT reasoning\" 和 \"out-of-distribution tasks\" 的泛化能力。 *   **训练方法**: 核心方法是 \"Reinforcement Learning\" 的一种变体，即 \"Policy Optimization\"。 *   **新兴范式**: 其“多教师指导”的框架可以被视为一种广义上的“智能体协作”，旨在通过协作来提升单一模型的通用问题解决能力。 3.  **第三步：排除标准** 该论文完全没有触及任何一个排除标准： *   它不涉及多模态、视觉或机器人控制。 *   它的研究焦点是通用的数学推理，而非生物、医疗、化学等特定应用领域。数学推理在这里是作为衡量“通用推理能力”的基准，而不是应用本身。 *   它不讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文提出的“多教师指导”框架，是一种**通用的方法论**，旨在通过自适应地整合多个模型的指导来提升学生模型的探索能力和推理质量。这完全符合“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的保留条件，而非将其应用于特定领域。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种创新的、基于强化学习的训练框架（AMPO），旨在系统性地提升LLM的通用推理能力、探索多样性和泛化性。它与你的研究目标——“致力于提高大语言模型（LLM）本身的『通用推理能力』”——高度契合，是一篇非常相关的前沿研究。"
    },
    {
        "index": "#24",
        "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration",
        "link": "/arxiv/2510.01879",
        "arxiv_id": "2510.01879",
        "authors": "Yisu Wang, Ming Wang, Haoyuan Song, Wenjie Huang, Chaozheng Wang, Yi Xie, Xuming Ran",
        "summary": "Post-training for large language models (LLMs) is constrained by the high cost of acquiring new knowledge or correcting errors and by the unintended side effects that frequently arise from retraining. To address these issues, we introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and Reintegration), a lifelong editing framework designed to support precise and low-cost model updates while preserving non-target knowledge. REPAIR mitigates the instability and conflicts of large-scale sequential edits through a closed-loop feedback mechanism coupled with dynamic memory management. Furthermore, by incorporating frequent knowledge fusion and enforcing strong locality guards, REPAIR effectively addresses the shortcomings of traditional distribution-agnostic approaches that often overlook unintended ripple effects. Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30% across multiple model families and significantly reduces knowledge forgetting. This work introduces a robust framework for developing reliable, scalable, and continually evolving LLMs.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.256886",
        "filter_reason": "这篇论文符合筛选标准，应当保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 这篇论文的核心不是将LLM应用于特定领域，而是提出了一种名为REPAIR的**新的模型编辑框架**。这个框架旨在解决LLM在后训练阶段更新知识和修正错误时面临的两大挑战：高成本和副作用（如知识遗忘）。这属于改进LLM**基础能力**的范畴，特别是其知识的**准确性、稳定性和可进化性**。这些能力是模型进行可靠推理的基石。因此，从本质上讲，这篇论文致力于改进LLM本身，而非将其作为工具应用。 2.  **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文明确以\"large language models (LLMs)\"为研究对象。 - **能力方向**: 虽然摘要没有直接使用\"reasoning\"一词，但其核心工作——修正错误、保持知识一致性、防止知识遗忘——是**高质量推理的必要前提**。一个充满错误和矛盾知识的模型无法进行有效的逻辑或数学推理。论文旨在构建一个\"可靠\"的模型，这直接服务于推理能力的提升。 - **训练方法**: 论文提出了一种\"终身编辑框架\"，这可以被视为一种新颖的后训练或持续学习范式。 - **新兴范式**: 摘要结尾明确提到，这项工作旨在开发\"持续进化的LLMs\"，这与\"self-evolve\"高度相关。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文的研究内容完全不涉及多模态、特定应用领域（如医疗、化学）或应用层面的模型可靠性（如水印、安全）。它聚焦于LLM内部的知识管理机制，因此不触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** 这篇论文的情况与\"幻觉/可解释性/安全\"的保留规则高度吻合。它提出了一种新技术来\"修正错误\"和\"显著减少知识遗忘\"。这本质上是一种减少事实性幻觉、提升模型内在可靠性的方法。通过确保模型知识的准确性，该方法能够从根源上提升模型的**推理质量和可信度**。这并非对现象的社会学讨论，而是提出了一种具体的、模型内在的技术解决方案，完全符合保留条件。 5.  **第五步：最终决策** 综合以上分析，尽管论文标题和摘要未直接突出“推理”，但其核心贡献——提出一种让LLM能够低成本、高精度、持续进化的知识编辑框架——是提升LLM通用推理能力的**基础性工作**。一个无法可靠更新和修正自身知识的模型，其推理能力必然会受到限制。因此，这篇论文所研究的“如何让模型更可靠、更可进化”的问题，与“提升模型通用推理能力”的最终目标高度一致且至关重要。它为构建更强大的推理模型提供了坚实的技术基础。"
    },
    {
        "index": "#23",
        "title": "Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey",
        "link": "/arxiv/2510.01925",
        "arxiv_id": "2510.01925",
        "authors": "Qiyuan Liu, Hao Xu, Xuhong Chen, Wei Chen, Yee Whye Teh, Ning Miao",
        "summary": "Reward models (RMs) play a critical role in enhancing the reasoning performance of LLMs. For example, they can provide training signals to finetune LLMs during reinforcement learning (RL) and help select the best answer from multiple candidates during inference. In this paper, we provide a systematic introduction to RMs, along with a comprehensive survey of their applications in LLM reasoning. We first review fundamental concepts of RMs, including their architectures, training methodologies, and evaluation techniques. Then, we explore their key applications: (1) guiding generation and selecting optimal outputs during LLM inference, (2) facilitating data synthesis and iterative self-improvement for LLMs, and (3) providing training signals in RL-based finetuning. Finally, we address critical open questions regarding the selection, generalization, evaluation, and enhancement of RMs, based on existing research and our own empirical findings. Our analysis aims to provide actionable insights for the effective deployment and advancement of RMs for LLM reasoning.",
        "subjects": "Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.256404",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** - 论文的本质是关于如何通过“奖励模型”这一核心技术来增强大语言模型的“推理性能”。它不是将LLM应用于某个特定领域，而是聚焦于改进LLM本身的一种基础能力——推理。 - 摘要中明确提到了论文探讨的核心方法，包括“在强化学习（RL）中提供训练信号来微调LLM”、“帮助从多个候选答案中选择最佳答案”、“促进数据合成和迭代自我改进”。这些都是直接作用于LLM内部，提升其通用推理能力的训练范式和推理优化方法，完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标** - 论文高度匹配所有正面指标： - **核心概念**: 标题和摘要中反复出现 \"Large Language Models\"。 - **能力方向**: 论文的核心主题就是 \"reasoning\"。 - **训练方法**: 明确提到了 \"reinforcement learning (RL)\" 和 \"iterative self-improvement\"。 - **新兴范式**: \"self-improvement\" 是自我进化的一种体现，而整个奖励模型框架是构建高级智能体和优化工具使用效果的关键技术。 3.  **第三步：排除标准** - 论文完全没有涉及任何排除标准中的领域。它不讨论多模态、视觉，不涉及任何特定应用领域（如医疗、化学），也不关注水印、安全等应用层面的可靠性议题。 4.  **第四步：处理特殊和模糊情况** - 本文不直接涉及智能体或幻觉等特殊情况的讨论，但它所研究的“奖励模型”是解决这些问题的底层技术之一。通过奖励模型优化推理过程，可以直接减少因逻辑错误导致的幻觉，提升模型内在的可靠性。因此，其研究方向与保留这些情况的初衷是一致的。 **最终决策**: 这篇论文是一篇关于“奖励模型”如何提升“LLM推理能力”的分析性综述。其核心贡献在于系统性地梳理和分析了用于增强LLM通用推理能力的关键技术（奖励模型）的原理、应用和挑战。这直接服务于你“提高大语言模型本身的通用推理能力”的核心目标，因此应该被保留。"
    },
    {
        "index": "#21",
        "title": "Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning",
        "link": "/arxiv/2510.01932",
        "arxiv_id": "2510.01932",
        "authors": "Qi He, Cheng Qian, Xiusi Chen, Bingxiang He, Yi R., Fung, Heng Ji",
        "summary": "Claim verification with large language models (LLMs) has recently attracted considerable attention, owing to their superior reasoning capabilities and transparent verification pathways compared to traditional answer-only judgments. Online claim verification requires iterative evidence retrieval and reasoning, yet existing approaches mainly rely on prompt engineering or predesigned reasoning workflows without offering a unified training paradigm to improve necessary skills. Therefore, we introduce Veri-R1, an online reinforcement learning (RL) framework that enables an LLM to interact with a search engine and to receive reward signals that explicitly shape its planning, retrieval, and reasoning behaviors. The dynamic interaction between models and retrieval systems more accurately reflects real-world verification scenarios and fosters comprehensive verification skills. Empirical results show that Veri-R1 improves joint accuracy by up to 30% and doubles evidence score, often surpassing larger-scale counterparts. Ablation studies further reveal the impact of reward components and the link between output logits and label accuracy. Our results highlight the effectiveness of online RL for precise and faithful claim verification and provide a foundation for future research. We release our code to support community progress in LLM empowered claim verification.",
        "subjects": "Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.255477",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为Veri-R1的在线强化学习（RL）框架。该框架并非简单地将LLM应用于特定任务，而是通过一种新的训练范式，直接提升LLM本身的基础能力。 具体分析如下： 1.  **第一步（核心判断）**: 论文的核心是提出一种新的训练范式（在线强化学习），让LLM通过与搜索引擎（工具）交互，并利用奖励信号来优化其『规划、检索和推理』行为。这完全符合筛选标准中关于“改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划、多步推理等通用能力”的描述。它不是将LLM作为工具应用于某个领域，而是在训练LLM本身。 2.  **第二步（正面指标）**: 论文命中了多个关键正面指标： *   **核心概念**: 明确以Large language models (LLMs)为研究对象。 *   **能力方向**: 聚焦于reasoning, planning, problem-solving（声明验证是一个复杂的问题解决过程）。 *   **训练方法**: 核心方法是online reinforcement learning (RL)。 *   **新兴范式**: 涉及llm-based agents（与搜索引擎交互）和tool use。 3.  **第三步（排除标准）**: 论文不涉及多模态、视觉或医疗、化学等特定应用领域。虽然提到了“faithful”，但其目标是通过改进推理过程来提升模型的内在可靠性，而非研究应用层面的水印、安全或安保技术，因此不属于排除范围。 4.  **第四步（特殊和模糊情况）**: *   **智能体/工具使用**: 论文提出的框架是一种通用的智能体与工具使用方法，旨在增强LLM的通用问题解决能力（规划、检索、推理），而不是将其限制在某个特定领域。因此，这符合保留条件。 *   **幻觉/可解释性/安全**: 论文追求的“faithful claim verification”直接关系到减少模型在推理过程中的“幻觉”。它通过一种新的训练方法（在线RL）来提升推理的忠实性，这属于提升模型内在推理质量的方法，因此符合保留条件。 **最终决策**: 综合来看，这篇论文的本质是探索如何通过在线强化学习来系统性地训练和提升LLM的通用推理、规划和工具使用能力。虽然其验证任务是“声明验证”，但这只是一个衡量通用推理能力的试验场，其核心贡献和方法论具有普适性，与“提高大语言模型本身的通用推理能力”这一研究目标高度一致。因此，应该保留。"
    },
    {
        "index": "#27",
        "title": "Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors",
        "link": "/arxiv/2510.01831",
        "arxiv_id": "2510.01831",
        "authors": "Dane Williamson, Yangfeng Ji, Matthew Dwyer",
        "summary": "Large Language Models (LLMs) demonstrate strong mathematical problem-solving abilities but frequently fail on problems that deviate syntactically from their training distribution. We identify a systematic failure mode, syntactic blind spots, in which models misapply familiar reasoning strategies to problems that are semantically straightforward but phrased in unfamiliar ways. These errors are not due to gaps in mathematical competence, but rather reflect a brittle coupling between surface form and internal representation. To test this, we rephrase incorrectly answered questions using syntactic templates drawn from correct examples. These rephrasings, which preserve semantics while reducing structural complexity, often lead to correct answers. We quantify syntactic complexity using a metric based on Dependency Locality Theory (DLT), and show that higher DLT scores are associated with increased failure rates across multiple datasets. Our findings suggest that many reasoning errors stem from structural misalignment rather than conceptual difficulty, and that syntax-aware interventions can reveal and mitigate these inductive failures.",
        "subjects": "Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.258270",
        "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的通用推理能力。** -   **核心贡献**: 这篇论文的核心是识别并分析了一种导致LLM在数学推理中失败的系统性模式——“句法盲点”。它发现，许多错误并非源于模型缺乏数学知识，而是源于问题表面的“句法结构”与模型内部知识表示的“错位”。 -   **为何符合**: 这篇论文的研究目标不是将LLM应用于某个特定领域，而是深入剖析LLM推理能力的内在缺陷。它通过揭示“表面形式”与“内在逻辑”之间的脆弱联系，为我们理解并提升LLM的通用推理能力提供了全新的视角和诊断工具。这属于改进LLM基础能力的核心研究范畴。 2.  **第二步：正面指标——论文高度相关。** -   **核心概念**: 论文明确以“Large Language Models (LLMs)”为研究对象。 -   **能力方向**: 论文直接聚焦于“mathematical problem-solving abilities”和“reasoning strategies”，这正是你关注的核心能力方向之一。 -   **潜在影响**: 虽然论文没有提出全新的训练范式（如RLHF），但它提出的“句法感知的干预措施”为未来的数据增强、微调或模型架构改进指明了方向，是提升推理能力的重要基础性工作。 3.  **第三步：排除标准——论文不涉及任何排除领域。** -   论文完全没有提及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施。它是一个纯粹关注模型内在认知机制的文本模型研究。 4.  **第四步：处理特殊和模糊情况——论文属于提升模型内在可靠性的研究。** -   这篇论文可以被归类为对“推理错误”的深入分析。它不是简单地记录错误，而是提出了一种技术性的解释（句法错位），并给出了量化指标（DLT）和缓解方法。这完全符合“提出一种新方法来……增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的保留标准。它不是社会学讨论，而是对模型内在缺陷的技术性诊断。 **总结**: 该论文精准地定位了阻碍LLM实现更可靠、更通用推理能力的一个根本性问题。它通过严谨的分析，论证了LLM的推理失败很多时候是“表达方式”的问题，而非“理解能力”的问题。这种对模型内在机制的深刻洞察，是推动LLM通用推理能力进步的关键一步，因此完全符合你的筛选要求。"
    },
    {
        "index": "#34",
        "title": "How Do Language Models Compose Functions?",
        "link": "/arxiv/2510.01685",
        "arxiv_id": "2510.01685",
        "authors": "Apoorv Khandelwal, Ellie Pavlick",
        "summary": "While large language models (LLMs) appear to be increasingly capable of solving compositional tasks, it is an open question whether they do so using compositional mechanisms. In this work, we investigate how feedforward LLMs solve two-hop factual recall tasks, which can be expressed compositionally as $g(f(x))$. We first confirm that modern LLMs continue to suffer from the \"compositionality gap\": i.e. their ability to compute both $z = f(x)$ and $y = g(z)$ does not entail their ability to compute the composition $y = g(f(x))$. Then, using logit lens on their residual stream activations, we identify two processing mechanisms, one which solves tasks $\\textit{compositionally}$, computing $f(x)$ along the way to computing $g(f(x))$, and one which solves them $\\textit{directly}$, without any detectable signature of the intermediate variable $f(x)$. Finally, we find that which mechanism is employed appears to be related to the embedding space geometry, with the idiomatic mechanism being dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in the embedding spaces. We fully release our data and code at: https://github.com/apoorvkh/composing-functions .",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.282408",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 这篇论文的核心是**基础性分析研究**，而非应用性研究。它没有将LLM作为工具应用于某个特定领域，而是深入探究了LLM在执行一种核心推理任务——**组合性推理**——时的内部工作机制。论文的核心贡献在于： *   **识别并验证了“组合性鸿沟”**：这是一个LLM通用推理能力上的根本缺陷。 *   **揭示了两种内部处理机制**：通过“logit lens”等技术，论文区分了模型是“组合性地”还是“直接地”解决问题，这为我们理解LLM的推理路径提供了前所未有的洞见。 *   **连接了机制与模型内部表征**：论文发现机制的选择与嵌入空间的几何特性有关，这为从模型结构层面理解和改进推理能力指明了方向。 虽然这篇论文没有提出一个能直接“提高”模型性能的新训练方法，但它**精准地诊断了通用推理能力中的一个核心问题，并揭示了其内在机理**。这种基础性的、机理层面的研究是“致力于提高LLM本身的通用推理能力”的必要前提和关键组成部分。不了解问题为何产生，就无法有效地解决问题。因此，它完全符合“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的核心目标。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文高度相关： *   **核心概念**: 明确以 \"Large language models (LLMs)\" 为研究对象。 *   **能力方向**: 聚焦于 \"compositional tasks\"（组合性任务），这是逻辑推理和问题解决能力的核心。论文标题和摘要反复强调“compose functions”，这正是多步推理的数学本质。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文完全避开了所有排除标准。它不涉及多模态、特定应用领域（如医疗、化学），也不讨论模型部署或应用层面的安全水印等问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/安全等特殊情况。但其研究精神与“增强模型内在的可解释性”高度一致。通过揭示模型内部的推理机制，它极大地增强了我们对模型推理过程的理解，这本身就是一种深层次的可解释性研究，其最终目的也是为了提升推理的可靠性和质量。 5.  **第五步：最终决策** 综合以上分析，这篇论文是一篇高质量的、聚焦于LLM核心推理机制的前沿研究。它没有停留在“模型能否做”的表面，而是深入到“模型如何做”的内部机理层面。对于你“提高LLM通用推理能力”这一核心目标而言，这类揭示根本性问题和机制的研究是不可或缺的基石。因此，**这篇论文应该被保留**。"
    },
    {
        "index": "#40",
        "title": "AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System",
        "link": "/arxiv/2510.01617",
        "arxiv_id": "2510.01617",
        "authors": "Hui Yi Leong, Yuheng Li, Yuqing Wu, Wenwen Ouyang, Wei Zhu, Jiechao Gao",
        "summary": "Although large language models (LLMs) have revolutionized natural language processing capabilities, their practical implementation as autonomous multi-agent systems (MAS) for industrial problem-solving encounters persistent barriers. Conventional MAS architectures are fundamentally restricted by inflexible, hand-crafted graph topologies that lack contextual responsiveness, resulting in diminished efficacy across varied academic and commercial workloads. To surmount these constraints, we introduce AMAS, a paradigm-shifting framework that redefines LLM-based MAS through a novel dynamic graph designer. This component autonomously identifies task-specific optimal graph configurations via lightweight LLM adaptation, eliminating the reliance on monolithic, universally applied structural templates. Instead, AMAS exploits the intrinsic properties of individual inputs to intelligently direct query trajectories through task-optimized agent pathways. Rigorous validation across question answering, mathematical deduction, and code generation benchmarks confirms that AMAS systematically exceeds state-of-the-art single-agent and multi-agent approaches across diverse LLM architectures. Our investigation establishes that context-sensitive structural adaptability constitutes a foundational requirement for high-performance LLM MAS deployments.",
        "subjects": "Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.290428",
        "filter_reason": "这篇论文完全符合您的研究范围。 **第一步：核心判断——论文的本质是什么？** 这篇论文的核心贡献是提出了一种名为AMAS的新颖框架，用于改进基于LLM的多智能体系统（MAS）。它不是将LLM应用于某个特定领域（如化学或医学），而是专注于提升LLM系统架构本身的通用能力。其核心创新点——“动态图设计器”——旨在根据不同任务自适应地调整智能体之间的通信结构，从而优化整个系统的协同推理和问题解决能力。这直接归属于“增强其逻辑、数学、规划、多步推理等通用能力”以及“智能体协作框架”的研究范畴。 **第二步：正面指标——论文是否包含以下主题？** 论文高度契合多个正面指标： - **核心概念**: 摘要中明确提到 \"large language models (LLMs)\"。 - **能力方向**: 论文的验证基准包括 \"mathematical deduction\"（数学演绎），这是通用推理能力的核心体现。其目标是提升 \"problem-solving\"（问题解决）能力。 - **新兴范式**: 论文的主题正是 \"LLM-based multi-agent system\"，这是当前提升LLM复杂推理能力的前沿范式。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文明确排除了所有排除标准： - **多模态与视觉**: 未涉及。 - **特定应用领域**: 虽然提到了 \"industrial problem-solving\"，但其核心方法AMAS是通用的，并且验证是在问答、数学、代码等通用基准上完成的，而非特定领域数据。因此，它的焦点是通用方法论，而非领域应用。 - **模型可靠性（应用层面）**: 未涉及。 **第四步：处理特殊和模糊情况** 这篇论文是“智能体/工具使用”特殊情况的完美例证。 - **保留情况**: AMAS提出的是一种“通用的智能体协作框架”，其目的是通过优化系统结构来“增强LLM的通用问题解决能力”，而不是将其限定在某个垂直领域。这完全符合保留标准。 **第五步：最终决策** 综合来看，AMAS论文的本质是通过提出一种新的多智能体协作架构（动态通信拓扑），来系统性地提升LLM在数学、编程等通用任务上的推理表现。它直接触及了如何通过组织和优化LLM间的交互来增强其整体的通用推理能力这一核心问题，与您的研究目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度一致。因此，应该保留。"
    },
    {
        "index": "#45",
        "title": "ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning",
        "link": "/arxiv/2510.01585",
        "arxiv_id": "2510.01585",
        "authors": "Haochen You, Baojing Liu",
        "summary": "While Transformer architectures have demonstrated impressive scalability across domains, they continue to face challenges in long-context reasoning, computational efficiency, and structural generalization - largely due to rigid layer stacking, dense attention, and reliance on positional encodings. We present ReSSFormer, a Recursive Sparse Structured Transformer that integrates three complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM) for efficient and focused context selection, and Self-Organizing Encoder Structure (SOES) for position-free structure induction. ReSSFormer replaces conventional depth stacking with recurrent inference, substitutes full attention with token- and expert-level sparsity, and models latent token topology directly from content. Across language modeling, multi-hop QA, and structure-sensitive tasks, ReSSFormer consistently outperforms strong baselines under comparable FLOPs and parameter budgets, highlighting its scalability, efficiency, and structural flexibility.",
        "subjects": "Computation and Language, Networking and Internet Architecture",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.292852",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断** - 论文的核心贡献是提出了一种名为 `ReSSFormer` 的新型 Transformer 架构。它的目标是解决现有 Transformer 在“长上下文推理”和“结构泛化”方面的根本性挑战。 - 论文通过引入“递归推理与记忆单元 (R2MU)”来实现“迭代推理”，用“自适应稀疏注意力模块 (ASAM)”来高效选择上下文，并用“自组织编码器结构 (SOES)”来摆脱对位置编码的依赖。 - 这些都属于对 LLM **基础架构和内在推理机制** 的改进，旨在提升其通用的、多步的推理能力，而不是将其应用于特定领域。因此，该论文通过了第一步的核心判断，应予以保留。 2.  **第二步：正面指标** - 论文的研究对象是基于 Transformer 的大语言模型架构。 - 论文的核心主题是 **推理**，明确聚焦于 **长上下文推理** 和 **迭代推理**，并在 **多跳问答** 等任务上进行了验证。这些都是“通用推理能力”的关键组成部分。 - 该论文命中了最关键的正面指标：**reasoning**。 3.  **第三步：排除标准** - 论文的研究内容完全基于文本，不涉及任何视觉、多模态或特定应用领域（如医疗、化学、机器人等）。 - 论文也未讨论模型部署、水印、安全等应用层面的可靠性问题。 - 因此，该论文不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - 本论文情况清晰，不涉及智能体/工具使用的特定领域应用，也不涉及幻觉/可解释性的社会学讨论，因此无需进行特殊情况的模糊判断。 5.  **第五步：最终决策** - 综合来看，这篇论文的本质是一项方法论和架构层面的研究，其核心目标是通过对模型结构的创新，直接增强大语言模型在长上下文下的通用推理能力和效率。这与您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标高度一致。论文的贡献是基础性的、通用的，而非应用性的。因此，最终判断为符合要求。"
    },
    {
        "index": "#48",
        "title": "TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies",
        "link": "/arxiv/2510.01391",
        "arxiv_id": "2510.01391",
        "authors": "Maithili Kadam, Francis Ferraro",
        "summary": "Large language models (LLMs) excel at general language tasks but often struggle with event-based questions-especially those requiring causal or temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question Answering), a prompting framework that injects causal event graphs into LLM inputs by converting structured relations into natural-language statements. TAG-EQA spans nine prompting configurations, combining three strategies (zero-shot, few-shot, chain-of-thought) with three input modalities (text-only, graph-only, text+graph), enabling a systematic analysis of when and how structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA improves accuracy by 5% on average over text-only baselines, with gains up to 12% in zero-shot settings and 18% when graph-augmented CoT prompting is effective. While performance varies by model and configuration, our findings show that causal graphs can enhance event reasoning in LLMs without fine-tuning, offering a flexible way to encode structure in prompt-based QA.",
        "subjects": "Computation and Language",
        "date": "2025-10-01",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.299443",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是详细的判断过程： 1.  **第一步：核心判断** 论文的核心是提出一种名为TAG-EQA的**提示框架**，其目的是通过将因果事件图转化为自然语言并注入提示中，来增强大语言模型在**事件问答**任务上的**因果与时序推理能力**。这本质上是探索一种新的方法论（结构化提示），以解锁和提升LLM在特定推理维度上的表现。这与思维链（CoT）的思路一脉相承，都属于改进LLM基础推理能力的范畴，而不是将LLM作为工具应用在某个外部领域。因此，根据第一步标准，应**保留**。 2.  **第二步：正面指标** 论文高度符合正面指标： *   **核心概念**: 摘要开篇即点明研究对象是“Large language models (LLMs)”。 *   **能力方向**: 论文的核心贡献是解决LLM在“event-based questions”上的困难，特别是“causal or temporal reasoning”（因果或时序推理）。这直接命中了“reasoning”这一核心能力方向。 *   **新兴范式**: 论文提出的“Structured Prompting Strategies”是对现有提示工程范式的创新，与思维链等方法并列讨论，属于提升LLM能力的前沿探索。 3.  **第三步：排除标准** 论文完全不符合排除标准： *   **多模态与视觉**: 论文中的“Graph”指的是抽象的**因果事件图**，是一种结构化知识表示，而非视觉图像。摘要明确指出其方法是“converting structured relations into natural-language statements”，处理过程仍在语言模态内，因此不属于多模态研究。 *   **特定应用领域**: 论文的实验基准是TORQUESTRA，这是一个事件推理的通用基准，而非医疗、化学等特定领域的应用场景。其研究目标是通用的推理能力，而非解决特定领域问题。 *   **模型可靠性（应用层面）**: 论文关注的是推理的准确性，这是模型能力的体现，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不直接涉及智能体/工具使用或幻觉/可解释性，但其研究精神与这些领域的前沿探索一致。它通过提供更结构化的信息，引导模型做出更准确的推理，这本质上是在提升模型的内在推理质量，从而可以间接减少因信息不足导致的“幻觉”。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种通用的、方法论层面的提示策略，旨在直接增强大语言模型在因果和时序推理这一通用能力上的表现。它不涉及特定应用领域，也不属于多模态或基础设施研究。因此，这篇论文与你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度契合。"
    },
    {
        "index": "#51",
        "title": "TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture",
        "link": "/arxiv/2510.01279",
        "arxiv_id": "2510.01279",
        "authors": "Yongchao Chen, Jiefeng Chen, Rui Meng, Ji Yin, Na Li, Chuchu Fan, Chi Wang, Tomas Pfister, Jinsung Yoon",
        "summary": "While integrating tools like Code Interpreter and Search has significantly enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and Gemini-Pro, practical guidance on optimal tool use is lacking. The core challenge is effectively combining textual reasoning, coding, and search for diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an ensemble framework that runs multiple agents in parallel, each employing distinct tool-use strategies and answer paths. Agents in TUMIX iteratively share and refine responses based on the question and previous answers. In experiments, TUMIX achieves significant gains over state-of-the-art tool-augmented and test-time scaling methods, delivering an average accuracy improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference costs. We find that agent diversity and quality are crucial and can be enhanced by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt refinement upon reaching sufficient confidence, preserving performance at only 49% of the inference cost. Further scaling can achieve higher performance, albeit at a greater cost.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.300994",
        "filter_reason": "这篇论文完全符合您的筛选标准，应该被保留。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Tool-Use Mixture (TUMIX)”的集成框架。这个框架通过并行运行多个采用不同工具使用策略的智能体，并让它们迭代地分享和优化答案，从而提升大语言模型在测试时的推理性能。这本质上是一种**新的方法论/范式**，旨在增强LLM的**通用问题解决能力**，而不是将LLM应用于某个特定领域。因此，根据第一步的核心判断标准，该论文应被**保留**。 2.  **第二步：正面指标** 论文摘要中包含了大量与您研究目标高度相关的正面指标： *   **核心概念**: 明确提到了 \"Large Language Model (LLM)\"。 *   **能力方向**: 核心关注点是 \"LLM reasoning\"，并在 \"key reasoning benchmarks\" 上进行验证。 *   **新兴范式**: 论文的主题是 \"Multi-Agent\" 和 \"Tool-Use\"，这完全符合您关注的 \"llm-based agents\" 和 \"tool use\" 范式。其提出的 \"Test-Time Scaling\" 也是一种提升模型性能的前沿方法。 这些指标的密集出现，强烈表明该论文与您的研究范围高度相关。 3.  **第三步：排除标准** 论文的研究焦点完全避开了所有的排除标准： *   它不涉及任何多模态或视觉内容。 *   它的应用场景是通用的“diverse questions”，而非医疗、化学、机器人等特定领域。 *   它不讨论水印、安全等应用层面的可靠性问题。 因此，根据第三步，该论文不应被排除。 4.  **第四步：处理特殊和模糊情况** 论文恰好处于“智能体/工具使用”这一特殊情况的讨论范围。TUMIX框架是一个**通用的智能体协作框架**，其目标是增强LLM的通用推理和问题解决能力，而不是针对某个特定领域（如化学实验自动化）。因此，完全符合“应该保留”的条件。论文中提到的“auto-optimize agent designs”也带有“自我进化”的意味，进一步增强了其相关性。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种新颖的、基于多智能体和工具使用的通用框架（TUMIX），以提升大语言模型在多样化问题上的推理能力。它精准地命中了您“提高LLM本身通用推理能力”的核心目标，同时满足所有正面指标，并规避了所有排除标准。 **结论：该论文是您研究课题“大语言模型通用推理能力”下的高质量前沿研究，应予以采纳。**"
    },
    {
        "index": "#68",
        "title": "A Comparative Analysis of Sparse Autoencoder and Activation Difference in Language Model Steering",
        "link": "/arxiv/2510.01246",
        "arxiv_id": "2510.01246",
        "authors": "Jiaqing Xie",
        "summary": "Sparse autoencoders (SAEs) have recently emerged as a powerful tool for language model steering. Prior work has explored top-k SAE latents for steering, but we observe that many dimensions among the top-k latents capture non-semantic features such as punctuation rather than semantic attributes like instructions. To address this, we propose focusing on a single, most relevant SAE latent (top-1), eliminating redundant features. We further identify a limitation in constant SAE steering, which often produces degenerate outputs such as repetitive single words. To mitigate this, we introduce a token-wise decaying steering strategy, enabling more faithful comparisons with mean activation difference baselines. Empirically, we show that steering an SAE latent associated with reasoning reliably elicits step-by-step mathematical reasoning and enhances inference quality, functionally resembling the effect of appending a guiding token. Our results demonstrate that SAEs outperform mean activation difference methods on mathematical reasoning benchmarks and match their performance on IF-Eval.",
        "subjects": "Computation and Language",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.314218",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。以下是根据你的筛选标准进行的详细判断： 1.  **第一步：核心判断——这篇论文的本质是什么？** - **符合**。这篇论文的核心贡献是提出一种新的方法来\"引导\"（steering）语言模型，具体是使用稀疏自编码器（SAE）来操纵模型的内部激活状态。其直接目标是增强模型的『推理能力』。摘要明确指出，通过引导一个与\"推理\"相关的SAE潜在特征，可以\"可靠地引出分步的数学推理并增强推理质量\"。这并非将LLM作为工具应用于特定领域，而是直接探索和改进LLM内部的通用推理机制。它提出的方法（top-1 SAE latent + token-wise decaying steering）是一种旨在提升模型基础能力的新方法论。 2.  **第二步：正面指标——论文是否包含以下主题？** - **高度符合**。论文包含了多个关键正面指标： - **核心概念**: 论文研究对象是\"Language Model\"，即LLM。 - **能力方向**: 论文的核心是\"reasoning\"，特别是\"mathematical reasoning\"和\"inference quality\"。 - **新兴范式**: \"Language Model Steering\"（模型引导）是一种理解并控制LLM行为的前沿探索，与增强模型通用问题解决能力的目标高度相关。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** - **不符合排除标准**。论文完全没有涉及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施。虽然它在数学推理基准上测试，但数学推理被视为衡量LLM通用能力的核心基准，而非一个狭窄的特定领域。 4.  **第四步：处理特殊和模糊情况** - **符合保留条件**。这篇论文可以被视为对\"可解释性\"的深入探索。通过使用SAE分离出与\"推理\"相关的特征，论文不仅提升了推理性能，还增进了我们对模型内部工作机制的理解。这符合\"如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留\"的原则。它不是泛泛而谈，而是提出了一种具体的技术手段来提升推理质量。 5.  **第五步：最终决策** - **综合判断**：该论文提出了一种新颖的、基础性的方法，通过直接干预和操纵LLM的内部表征来增强其通用推理能力。它不涉及特定应用，也不属于基础设施研究，其核心贡献完全聚焦于提升LLM本身的推理质量和过程。这与你的核心目标\"筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文\"高度契合。因此，最终判断为保留。"
    },
    {
        "index": "#76",
        "title": "Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation",
        "link": "/arxiv/2510.01237",
        "arxiv_id": "2510.01237",
        "authors": "Nandakishor M",
        "summary": "Large Language Models suffer from hallucination, generating plausible yet factually incorrect content. Current mitigation strategies focus on post-generation correction, which is computationally expensive and fails to prevent unreliable content generation. We propose a confidence-aware routing system that proactively assesses model uncertainty before generation and redirects queries based on estimated reliability. Our approach combines three complementary signals: semantic alignment between internal representations and reference embeddings, internal convergence analysis across model layers, and learned confidence estimation. The unified confidence score determines routing to four pathways: local generation for high confidence, retrieval-augmented generation for medium confidence, larger models for low confidence, and human review for very low confidence. Evaluation on knowledge-intensive QA benchmarks demonstrates significant improvements in hallucination detection (0.74 vs. 0.42 baseline) while reducing computational costs by 40% compared to post-hoc methods. The F1 score improves from 0.61 to 0.82 with low false positive rates (0.09). This paradigm shift from reactive correction to proactive assessment offers a computationally efficient approach to LLM reliability enhancement.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.323244",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种“感知信心的路由系统”，这是一种在生成内容**之前**主动评估并缓解LLM幻觉的新方法论。它并非将LLM作为工具应用于某个特定领域，也不是关于模型基础设施或部署优化。相反，它致力于解决LLM生成内容时的根本性问题——事实可靠性。一个可靠的、不产生幻觉的模型是进行有效推理的**基础前提**。因此，这篇论文的本质是改进LLM的基础能力，符合“保留”标准。 2.  **第二步：正面指标** 论文明确包含核心概念“Large Language Models”。虽然摘要中没有直接使用“reasoning”一词，但其核心目标“hallucination mitigation”（幻觉缓解）和“reliability enhancement”（可靠性增强）与通用推理能力紧密相关。一个频繁产生事实错误的模型，其逻辑推理链条的价值也会大打折扣。此外，论文提出的路由系统本身可以被视为一种通用的元智能体框架，它根据内部状态决定使用何种工具（本地生成、RAG、调用更大模型），这与“llm-based agents”和“tool use”等新兴范式高度契合。 3.  **第三步：排除标准** 该论文不涉及多模态、视觉、特定应用领域（如医疗、化学），也不关注水印、安全等应用层面的可靠性问题。因此，它不触犯任何排除标准。 4.  **第四步：处理特殊和模糊情况** 这是判断的关键。论文主题是“幻觉缓解”，属于“模型可靠性”范畴。根据筛选标准：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” - **提出新方法**：是的，论文提出了“confidence-aware routing”这一全新的、主动的范式，区别于传统的生成后修正方法。 - **减少幻觉**：是的，这是论文的直接目标，并在实验中取得了显著效果。 - **提升通用可靠性和推理质量**：是的。通过在生成前就过滤掉可能导致幻觉的查询，或为其提供更可靠的生成路径（如RAG），该方法从根本上提升了模型输出的整体可靠性。这种可靠性是后续进行任何复杂推理任务（如多步逻辑推理、数学问题求解）的基石。因此，它直接服务于提升通用推理质量这一核心目标。 5.  **第五步：最终决策** 综合以上分析，这篇论文虽然聚焦于“幻觉缓解”，但其提出的方法论是一种旨在提升LLM内在输出质量的通用框架。它通过增强模型的基础可靠性，为更高级的通用推理能力铺平了道路。这完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”的研究目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#84",
        "title": "Who is In Charge? Dissecting Role Conflicts in Instruction Following",
        "link": "/arxiv/2510.01228",
        "arxiv_id": "2510.01228",
        "authors": "Siqi Zeng",
        "summary": "Large language models should follow hierarchical instructions where system prompts override user inputs, yet recent work shows they often ignore this rule while strongly obeying social cues such as authority or consensus. We extend these behavioral findings with mechanistic interpretations on a large-scale dataset. Linear probing shows conflict-decision signals are encoded early, with system-user and social conflicts forming distinct subspaces. Direct Logit Attribution reveals stronger internal conflict detection in system-user cases but consistent resolution only for social cues. Steering experiments show that, despite using social cues, the vectors surprisingly amplify instruction following in a role-agnostic way. Together, these results explain fragile system obedience and underscore the need for lightweight hierarchy-sensitive alignment methods.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.332401",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。 **核心判断与依据:** 1.  **第一步：核心判断** 论文的核心并非将LLM作为工具应用于特定领域，而是深入**探究LLM内部的机制性问题**。它研究的是LLM在“指令遵循”这一基础能力上的缺陷——即在面对层级冲突（系统提示 vs. 用户输入）和社会线索冲突时，模型如何决策。这直接关联到LLM的**通用推理与问题解决能力**，因为正确理解和处理指令的优先级是高级推理的基石。论文的目标是解释“脆弱的系统服从性”，并指向“层级敏感的对齐方法”，这本质上是在寻求提升LLM基础能力的新路径。因此，它完全符合“改进LLM的基础能力”这一核心要求。 2.  **第二步：正面指标** 论文明确包含核心概念“Large language models”。虽然未直接使用“reasoning”一词，但其研究的“instruction following”和“conflict resolution”是推理能力的具体体现和前置条件。模型需要推理出哪个指令应该被优先执行。论文最终指向的“alignment methods”也与强化学习等训练范式紧密相关。 3.  **第三步：排除标准** 论文完全不涉及多模态、特定应用领域（如医疗、化学）或应用层面的可靠性（如水印）。它聚焦的是一个纯粹的、通用的语言模型行为问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文是**“可解释性”研究的一个绝佳范例**，并且这种研究服务于提升模型通用能力的最终目标。它没有停留在表面现象的描述，而是通过“线性探测”、“直接Logit归因”和“引导实验”等机制性分析方法，深入模型内部，探究决策信号的形成和解决路径。这种对内在机理的深刻理解，正是未来开发出更强大、更可靠的通用推理能力（例如，能正确处理复杂指令层次）的理论基础。因此，它属于“增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量”的情况，应该保留。 **最终决策:** 综合来看，该论文通过机制性分析方法，深入剖析了LLM在指令遵循（一种核心的通用能力）中的内在缺陷。其研究成果不仅解释了现有模型的不足，更重要的是为未来开发能够更好地进行复杂推理和问题解决的LLM（即能正确处理指令冲突的模型）指明了方向。这与“提高大语言模型本身的通用推理能力”的研究目标高度契合。因此，**结果为 True**。"
    },
    {
        "index": "#93",
        "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems",
        "link": "/arxiv/2510.02263",
        "arxiv_id": "2510.02263",
        "authors": "Yuxiao Qu, Anikait Singh, Yoonho Lee, Amrith Setlur, Ruslan Salakhutdinov, Chelsea Finn, Aviral Kumar",
        "summary": "Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement \"algorithmic procedures\" that can be used to deduce answers to hard problems. Doing so requires realizing the most relevant primitives, intermediate results, or shared procedures, and building upon them. While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration. To address more effective reasoning, we introduce reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward learning successful reasoning. We train models to be capable of proposing multiple abstractions given a problem, followed by RL that incentivizes building a solution while using the information provided by these abstractions. This results in a two-player RL training paradigm, abbreviated as RLAD, that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation, and improves generalization to harder problems. We also show that allocating more test-time compute to generating abstractions is more beneficial for performance than generating more solutions at large test budgets, illustrating the role of abstractions in guiding meaningful exploration.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.336864",
        "filter_reason": "这篇论文完全符合筛选标准，应当被保留。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出一种名为RLAD的新训练范式，用于提升大语言模型（LLM）的通用推理能力。其核心贡献在于引入了“推理抽象”这一概念，并设计了一个双玩家强化学习框架，让模型学会自主发现和利用这些抽象来解决更复杂、更难的推理问题。这直接触及了“改进LLM的基础能力、提出新的训练范式、增强其逻辑、多步推理等通用能力”的核心目标。论文并未将LLM应用于特定领域，而是聚焦于模型内在推理机制的优化，因此符合核心判断的保留标准。 **第二步：正面指标——论文是否包含相关主题？** 论文高度符合多项正面指标： - **核心概念**: 论文标题和摘要中多次明确提到 \"Large language models (LLMs)\"。 - **能力方向**: 论文的主题就是 \"Solving Reasoning Problems\"，并深入探讨了如何实现超越模式匹配的 \"algorithmic procedures\"，这正是对 \"reasoning\" 能力的核心研究。 - **训练方法**: 论文的核心方法论是 \"RL post-training\" 和一个 \"two-player RL training paradigm\"，完全命中 \"reinforcement learning (RL)\" 这一关键指标。 **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文完全不涉及任何排除标准中列出的领域。它没有讨论视觉、多模态，也没有应用在医疗、化学、机器人等特定领域，更不关心模型部署、水印或安全等问题。因此，可以安全地通过此步筛选。 **第四步：处理特殊和模糊情况** 这篇论文的情况不属于模糊范畴。它不是简单的应用研究，而是方法论层面的创新。虽然可以将其双玩家框架（抽象生成器与解决方案生成器）看作一种简化的智能体协作，但其目的是为了增强模型的通用推理过程，而非解决特定领域任务。因此，它属于“提出一种通用的智能体协作框架...来增强LLM的通用问题解决能力”的情况，应当保留。 **第五步：最终决策** 综合以上分析，这篇论文精准地定位在“提升大语言模型通用推理能力”这一研究课题的核心。它通过提出创新的训练范式（RLAD）和关键概念（推理抽象），直接致力于解决现有方法（如思维链）在泛化和复用推理步骤上的不足。该研究对于推动LLM基础能力的发展具有重要意义，与研究目标高度契合。因此，最终决策为 **True**。"
    },
    {
        "index": "#95",
        "title": "ExGRPO: Learning to Reason from Experience",
        "link": "/arxiv/2510.02245",
        "arxiv_id": "2510.02245",
        "authors": "Runzhe Zhan, Yafu Li, Zhi Wang, Xiaoye Qu, Dongrui Liu, Jing Shao, Derek F. Wong, Yu Cheng",
        "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after a single update, leading to computational inefficiency and instability. While prior work on RL has highlighted the benefits of reusing past experience, the role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored. In this paper, we are the first to investigate what makes a reasoning experience valuable and identify rollout correctness and entropy as effective indicators of experience value. Based on these insights, we propose ExGRPO (Experiential Group Relative Policy Optimization), a framework that organizes and prioritizes valuable experiences, and employs a mixed-policy objective to balance exploration with experience exploitation. Experiments on five backbone models (1.5B-8B parameters) show that ExGRPO consistently improves reasoning performance on mathematical/general benchmarks, with an average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and weaker models where on-policy methods fail. These results highlight principled experience management as a key ingredient for efficient and scalable RLVR.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.337894",
        "filter_reason": "这篇论文完全符合研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的本质是提出一种新的训练范式（ExGRPO）来提升大语言模型的基础推理能力。摘要开宗明义，指出其目标是“improving the reasoning ability of large language models”（提升大语言模型的推理能力）。它并非将LLM作为工具应用于特定领域，而是深入研究如何通过改进强化学习过程中的经验利用效率，来从本质上增强模型的推理性能。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 论文明确聚焦于 \"large language models\"。 *   **能力方向**: 核心主题是 \"reasoning ability\"，并在 \"mathematical/general benchmarks\" 上进行验证，这正是研究目标所关注的数学和通用推理。 *   **训练方法**: 论文基于 \"Reinforcement learning from verifiable rewards (RLVR)\"，并对 \"on-policy training\" 进行改进，这属于强化学习优化的范畴。 3.  **第三步：排除标准** 论文完全没有触及任何排除标准。它不涉及多模态、视觉、任何特定应用领域（如医疗、化学），也不讨论水印、安全等模型可靠性（应用层面）的问题。其研究焦点纯粹在于提升模型本身的通用推理核心能力。 4.  **第四步：处理特殊和模糊情况** 本论文情况清晰，不属于需要特殊处理的模糊范畴。它提出的ExGRPO框架是一种通用的训练方法论，旨在提升模型的内在推理能力，而非针对特定应用的智能体或工具。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种名为“ExGRPO”的新颖训练框架。该框架通过智能地管理和重用强化学习过程中的经验，显著提升了LLM在数学和通用推理任务上的表现。这是一种致力于从学习机制层面增强LLM通用推理能力的基础性研究，与你的研究课题“大语言模型通用推理能力”精准契合。因此，最终判定为符合要求。"
    },
    {
        "index": "#94",
        "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
        "link": "/arxiv/2510.02250",
        "arxiv_id": "2510.02250",
        "authors": "Gonzalo Gonzalez-Pumariega, Vincent Tu, Chih-Lun Lee, Jiachen Yang, Ang Li, Xin Eric Wang",
        "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital tasks, but their unreliability and high variance hinder their application to long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method that scales over agents by generating multiple rollouts and selecting among them using behavior narratives that describe the agents' rollouts. It enables both wide exploration and principled trajectory selection, substantially improving robustness and success rates. On OSWorld, our bBoN scaling method establishes a new state of the art (SoTA) at 69.9%, significantly outperforming prior methods and approaching human-level performance at 72%, with comprehensive ablations validating key design choices. We further demonstrate strong generalization results to different operating systems on WindowsAgentArena and AndroidWorld. Crucially, our results highlight the unreasonable effectiveness of scaling CUAs, when you do it right: effective scaling requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve this.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.337379",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为 \"Behavior Best-of-N (bBoN)\" 的新方法。这种方法并非将LLM应用于某个特定领域，而是致力于解决LLM作为智能体在执行复杂、长时程任务时的核心挑战：可靠性和规划能力。bBoN通过生成多个行为轨迹并进行智能选择，本质上是一种增强LLM**规划和多步推理能力**的新范式。这与思维链（CoT）的思路一脉相承，都属于提升模型基础推理能力的方法论研究，因此应予以**保留**。 2.  **第二步：正面指标** 论文高度符合多个正面指标： *   **能力方向**: 论文明确聚焦于解决 \"long-horizon, complex tasks\"，这直接对应了 **planning** 和 **problem-solving** 能力。 *   **新兴范式**: 论文的核心是关于 **llm-based agents** 的规模化方法，并且其操作计算机的行为本身就是一种高级的 **tool use**。 *   **核心概念**: 虽然 \"LLM\" 一词未在摘要中显式出现，但 \"Computer-use agents (CUAs)\" 在当前AI研究中默认是由LLM驱动的，这是该领域的基本设定。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： *   它不涉及多模态与视觉（尽管智能体可能“看到”屏幕，但论文核心是行为规划而非视觉模型本身）。 *   它不属于任何特定应用领域（如医疗、化学）。“计算机使用”是一个极其通用的任务域，旨在自动化日常数字任务，这与“通用推理”的目标高度一致。 *   它不关注模型可靠性层面的水印、安全等问题，而是关注任务执行的成功率和鲁棒性，这属于核心能力范畴。 4.  **第四步：处理特殊和模糊情况** 论文完美地契合了“智能体/工具使用”的保留条件。它提出的是一种**通用的智能体规模化框架**，旨在提升智能体在通用环境（操作系统）中的问题解决能力。这与“用于化学实验自动化的智能体”等特定领域应用有本质区别。bBoN方法本身具有通用性，其核心思想——通过结构化的轨迹理解和选择来提升推理质量——可以迁移到其他需要复杂规划的智能体任务中。 **最终决策**: 这篇论文的本质是提出一种新的推理和规划范式（bBoN），用以增强LLM智能体在复杂、通用任务中的表现。它直接触及了“大语言模型通用推理能力”的核心，即如何让模型更好地进行规划、决策和多步问题解决。因此，这篇论文不仅符合，而且是高度相关的前沿研究，应被**保留**。"
    },
    {
        "index": "#102",
        "title": "Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning",
        "link": "/arxiv/2510.01833",
        "arxiv_id": "2510.01833",
        "authors": "Zhihao Dou, Qinjian Zhao, Zhongwei Wan, Dinggen Zhang, Weida Wang, Towsif Raiyan, Benteng Chen, Qingtao Pan, Yang Ouyang, Zhiqiang Gao, Shufei Zhang, Sumon Biswas",
        "summary": "Large language models (LLMs) have demonstrated remarkable reasoning abilities in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However, due to their autoregressive token-level generation, the reasoning process is largely constrained to local decision-making and lacks global planning. This limitation frequently results in redundant, incoherent, or inaccurate reasoning, which significantly degrades overall performance. Existing approaches, such as tree-based algorithms and reinforcement learning (RL), attempt to address this issue but suffer from high computational costs and often fail to produce optimal reasoning trajectories. To tackle this challenge, we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy Optimization PTA-GRPO, a two-stage framework designed to improve both high-level planning and fine-grained CoT reasoning. In the first stage, we leverage advanced LLMs to distill CoT into compact high-level guidance, which is then used for supervised fine-tuning (SFT). In the second stage, we introduce a guidance-aware RL method that jointly optimizes the final output and the quality of high-level guidance, thereby enhancing reasoning effectiveness. We conduct extensive experiments on multiple mathematical reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently achieves stable and significant improvements across different models and tasks, validating its effectiveness and generalization.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.352741",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断** - **保留**。该论文的本质是提出一种新的训练和推理框架（PTA-GRPO），用以解决大语言模型在推理过程中的一个核心缺陷：缺乏全局规划能力。论文并未将LLM作为工具应用于特定领域，而是聚焦于改进模型自身的推理范式。它通过“先规划后行动”的两阶段方法，结合监督微调（SFT）和一种新颖的强化学习（GRPO）方法，直接增强了LLM的通用规划和多步推理能力。这完全符合您定义的“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的目标。 2.  **第二步：正面指标** - 论文高度匹配所有正面指标： - **核心概念**: 标题和摘要中明确提及 \"Large language models (LLMs)\"。 - **能力方向**: 核心主题是 \"reasoning\"、\"planning\" 和 \"problem-solving\"，特别是在 \"mathematical reasoning\" 上进行验证。 - **训练方法**: 提出了结合 \"supervised fine-tuning (SFT)\" 和一种新的 \"reinforcement learning (RL)\" 方法，即 \"Group Relative Policy Optimization (GRPO)\"。 - **新兴范式**: 论文建立在 \"Chain-of-Thought (CoT)\" 范式之上，并提出了一个结构化的改进框架 \"Plan-Then-Action\"，这与智能体框架中的规划思想高度一致。 3.  **第三步：排除标准** - 论文完全规避了所有排除标准： - **多模态与视觉**: 全文聚焦于文本语言的推理，不涉及任何视觉或多模态内容。 - **特定应用领域**: 论文在数学推理基准（MATH, AIME等）上进行实验，但这里的数学是作为评估**通用推理能力**的“试金石”，而非研究目标是解决数学问题本身。其提出的方法（PTA-GRPO）是领域无关的通用框架，可以迁移到其他需要规划和推理的任务中。 - **模型可靠性（应用层面）**: 论文不讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的“Plan-Then-Action”框架本质上是一种通用的推理增强方法论，旨在提升模型解决复杂问题的通用能力，而非应用于特定领域的智能体。因此，它符合保留条件。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种新颖的、通用的训练与推理框架，通过强化全局规划来显著提升大语言模型的基础推理能力。其研究内容、方法和目标与您“提高大语言模型本身通用推理能力”的核心目标高度一致，是一篇非常相关的前沿研究论文。因此，应予以保留。"
    },
    {
        "index": "#97",
        "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models",
        "link": "/arxiv/2510.02230",
        "arxiv_id": "2510.02230",
        "authors": "Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, Khoa D. Doan",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Models' reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at https://github.com/mail-research/SELF-llm-interference.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.349069",
        "filter_reason": "这篇论文完全符合您的研究范围，是一篇高度相关的前沿研究。以下是我的详细判断过程： 1.  **第一步：核心判断——论文本质** 论文的核心并非将LLM应用于某个特定领域，而是深入分析并改进一种旨在提升LLM**基础推理能力**的训练方法。它研究了“强化学习与可验证奖励（RLVR）”这一训练范式如何影响LLM的推理边界，发现其反而会**约束**而非扩展模型的能力。论文的核心贡献在于揭示了这一现象背后的学习动态（负干扰、赢者通吃），并提出了针对性的改进算法。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——主题匹配度** 论文与多个正面指标高度契合： *   **核心概念**: 明确以“Large Language Models”为研究对象。 *   **能力方向**: 核心议题是“reasoning capabilities”，并在“multiple mathematical reasoning benchmarks”上进行验证，这直接对应了“reasoning (尤其是 math reasoning)”。 *   **训练方法**: 论文的整个分析都围绕“Reinforcement Learning with Verifiable Rewards (RLVR)”展开，完全命中“reinforcement learning (RLHF, RL)”。 3.  **第三步：排除标准——领域聚焦** 论文的研究焦点非常清晰，与所有排除标准均无关系。它不涉及视觉、多模态，不面向医疗、化学等任何特定应用领域，也非讨论模型基础设施、部署或水印等应用层面的可靠性问题。 4.  **第四步：处理特殊情况** 本论文不属于特殊情况的范畴，但其内容与“提升模型内在可靠性”的理念相通。它通过改进训练方法，直接提升了模型在数学推理任务上的Pass@$k$性能，这增强了模型在通用问题解决上的鲁棒性和有效性，属于提升模型内在推理质量的研究。 **综合论断：** 该论文精准地聚焦于“如何通过改进训练方法来提升LLM的通用推理能力”这一核心问题。它不仅指出了当前主流方法（RLVR）的一个关键缺陷，还从理论和实证上分析了其原因，并提出了有效的解决方案。这种对LLM内在能力提升机制的深入探究，正是您研究课题“大语言模型通用推理能力”所需要的前沿成果。因此，这篇论文应被**保留**。"
    },
    {
        "index": "#105",
        "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
        "link": "/arxiv/2510.01670",
        "arxiv_id": "2510.01670",
        "authors": "Erfan Shayegani, Keegan Hines, Yue Dong, Nael Abu-Ghazaleh, Roman Lutz, Spencer Whitehead, Vidhisha Balachandran, Besmira Nushi, Vibhav Vineet",
        "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that take actions on GUIs to accomplish user goals. In this paper, we show that CUAs consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals regardless of feasibility, safety, reliability, or context. We characterize three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii) assumptions and decisions under ambiguity, and (iii) contradictory or infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement with human annotations. We use BLIND-ACT to evaluate nine frontier models, including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing high average BGD rates (80.8%) across them. We show that BGD exposes subtle risks that arise even when inputs are not directly harmful. While prompting-based interventions lower BGD levels, substantial risk persists, highlighting the need for stronger training- or inference-time interventions. Qualitative analysis reveals observed failure modes: execution-first bias (focusing on how to act over whether to act), thought-action disconnect (execution diverging from reasoning), and request-primacy (justifying actions due to user request). Identifying BGD and introducing BLIND-ACT establishes a foundation for future research on studying and mitigating this fundamental risk and ensuring safe CUA deployment.",
        "subjects": "Artificial Intelligence, Computation and Language, Cryptography and Security, Computers and Society, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.359694",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心并非将LLM应用于某个特定领域，而是深入剖析了LLM作为“计算机使用智能体”时，在通用推理和规划能力上存在的一个根本性缺陷——“盲目目标导向性”。它识别并定义了这种缺陷的具体表现（如缺乏上下文推理、在模糊性下做决策等），这直接触及了LLM通用推理能力的核心问题。虽然论文本身没有提出一种新的训练范式来“解决”这个问题，但它通过创建评测基准（BLIND-ACT）来“量化”这个问题，为未来研究如何“提升”这一能力奠定了至关重要的基础。因此，其本质是关于改进LLM基础能力的研究，应予以保留。 2.  **第二步：正面指标** 论文与多个正面指标高度相关： *   **核心概念**: 论文的研究对象是前沿的LLM模型。 *   **能力方向**: 论文的核心是`reasoning`和`planning`。它详细分析了LLM在目标导向任务中的推理失败模式，如`lack of contextual reasoning`（缺乏上下文推理）和`contradictory or infeasible goals`（处理矛盾或不可行目标）。 *   **新兴范式**: 论文聚焦于`llm-based agents`和`tool use`（将GUI操作视为一种工具使用），这正是当前提升LLM通用问题解决能力的热点范式。 3.  **第三步：排除标准** 论文未触及任何排除标准： *   它不涉及多模态或视觉模型的核心研究，GUI只是智能体推理和行动的环境。 *   它的研究领域是通用的“计算机使用”，而非医疗、化学等特定领域。 *   它讨论的“安全”和“可靠性”是源于模型内在推理过程的缺陷，而非应用层面的水印或对抗攻击等。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 这篇论文是分析通用智能体（CUA）推理能力的典范。它不是将智能体应用于特定领域，而是研究智能体范式本身暴露出的通用推理短板，完全符合保留标准。 *   **模型可靠性**: 论文对“安全”和“可靠性”的探讨，是从提升模型内在推理质量的角度出发的。它指出“Blind Goal-Directedness”是一种根本性风险，并呼吁更强的“训练时或推理时干预”来缓解，这直接关联到如何从根源上优化模型的推理过程，因此应保留。 **最终决策**: 该论文的核心贡献在于**识别、定义并量化了LLM在通用智能体任务中的一种关键推理缺陷**，并为此建立了评测基准。这项工作为未来研究如何提升LLM的上下文理解、规划可行性判断等通用推理能力提供了明确的目标和衡量标准。它不是应用研究，而是对LLM核心能力的深刻洞察和基础性建设，与你的研究目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度一致。因此，应判定为符合。"
    },
    {
        "index": "#112",
        "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression",
        "link": "/arxiv/2510.01581",
        "arxiv_id": "2510.01581",
        "authors": "Joykirat Singh, Justin Chih-Yao Chen, Archiki Prasad, Elias Stengel-Eskin, Akshay Nambi, Mohit Bansal",
        "summary": "Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.363174",
        "filter_reason": "这篇论文完全符合您关于“大语言模型通用推理能力”的研究范围。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断——论文的本质是改进LLM基础能力。** 论文的核心贡献并非将LLM应用于特定领域，而是提出了一种名为TRAAC的新方法来解决“思维模型”中一个普遍存在的问题：**思考长度的欠适应性**。具体来说，它旨在优化LLM在进行复杂推理时的内部计算过程，使其能够根据问题难度自适应地分配“思考预算”，从而在“欠思考”和“过度思考”之间取得平衡。这是一种直接针对LLM通用推理过程进行改进的基础性研究，完全属于改进LLM基础能力的范畴。 2.  **第二步：正面指标——论文包含多个高度相关的主题。** -   **核心概念与能力方向**: 论文聚焦于\"thinking models\"（思维模型）和\"complex reasoning tasks\"，明确以提升模型的通用推理能力为目标。 -   **训练方法**: 论文的核心是一种\"online post-training RL method\"（在线后训练强化学习方法），这与筛选标准中的`reinforcement learning (RLHF, RL)`完全吻合。 -   **新兴范式**: 该研究建立在思维链/思维模型这类新兴范式之上，并试图对其进行优化。论文提出的\"adaptive thinking\"（自适应思考）正是该领域追求的更高层次的能力。 3.  **第三步：排除标准——论文不涉及排除领域。** -   论文研究的是纯文本语言模型，没有涉及任何多模态内容。 -   论文使用的是数学和常识推理的通用基准数据集（如AIME, AMC, GPQA-D, BBEH），并未聚焦于生物、化学、法律等任何特定应用领域。特别值得注意的是，论文强调了其方法的**泛化能力**——即使在数学数据集上训练，模型也在非数学的分布外数据集上表现出色，这进一步证明了其方法的通用性，而非特定应用。 -   论文不涉及水印、安全等应用层面的可靠性研究。 4.  **第四步：处理特殊情况——不适用模糊情况。** 论文的情况非常清晰，不属于智能体/工具使用在特定领域的应用，也不属于对幻觉或安全性的应用层面讨论。它提出的是一个纯粹的、旨在提升模型内在推理效率和质量的训练方法论。 5.  **第五步：最终决策。** 综合以上分析，这篇论文的核心贡献是提出了一种通过强化学习来优化LLM推理过程、实现自适应思考的新范式。它直面当前思维模型的关键缺陷，并提出了解决方案，其方法被证明在多种通用推理任务上有效且具有泛化性。这与您“致力于提高大语言模型本身的『通用推理能力』”的核心目标高度一致。因此，应判定为符合要求。"
    },
    {
        "index": "#115",
        "title": "Information Seeking for Robust Decision Making under Partial Observability",
        "link": "/arxiv/2510.01531",
        "arxiv_id": "2510.01531",
        "authors": "Djengo Cyun-Jyun Fang, Tsung-Wei Ke",
        "summary": "Explicit information seeking is essential to human problem-solving in practical environments characterized by incomplete information and noisy dynamics. When the true environmental state is not directly observable, humans seek information to update their internal dynamics and inform future decision-making. Although existing Large Language Model (LLM) planning agents have addressed observational uncertainty, they often overlook discrepancies between their internal dynamics and the actual environment. We introduce Information Seeking Decision Planner (InfoSeeker), an LLM decision-making framework that integrates task-oriented planning with information seeking to align internal dynamics and make optimal decisions under uncertainty in both agent observations and environmental dynamics. InfoSeeker prompts an LLM to actively gather information by planning actions to validate its understanding, detect environmental changes, or test hypotheses before generating or revising task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark suite featuring partially observable environments with incomplete observations and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74% absolute performance gain over prior methods without sacrificing sample efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms baselines on established benchmarks such as robotic manipulation and web navigation. These findings underscore the importance of tightly integrating planning and information seeking for robust behavior in partially observable environments. The project page is available at https://infoseekerllm.github.io",
        "subjects": "Artificial Intelligence, Computation and Language, Robotics",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.369792",
        "filter_reason": "这篇论文完全符合您的筛选标准，应予以保留。 **第一步：核心判断** 论文的核心是提出一个名为\"InfoSeeker\"的LLM决策框架，其本质是增强LLM在部分可观测环境下的通用规划与决策能力。它并非将LLM应用于某个特定领域，而是致力于解决LLM智能体在通用场景下面临的一个根本性挑战：内部认知与外部环境动态不一致的问题。这直接属于改进LLM基础能力和通用推理能力的范畴，因此符合保留条件。 **第二步：正面指标** 论文中包含了大量正面指标： - **核心概念**: 明确以\"Large Language Model (LLM)\"为基础。 - **能力方向**: 核心关注\"planning\"（规划）、\"decision making\"（决策制定）、\"problem-solving\"（问题解决），这些都是通用推理能力的关键组成部分。 - **新兴范式**: 论文聚焦于\"LLM planning agents\"（LLM规划智能体），提出了一个新的框架来增强其能力。 **第三步：排除标准** 论文的主要焦点完全避开了排除标准中的领域： - 它不是关于多模态或视觉的。 - 它不是关于医疗、化学、机器人等特定应用领域的研究。 - 它不涉及水印、安全等模型可靠性（应用层面）的问题。 **第四步：处理特殊和模糊情况** 论文中提到了在\"robotic manipulation and web navigation\"等基准上进行评估。这里需要精确判断：论文的主旨是提出一个**通用的决策框架**，而机器人操控和网页导航是用来**验证该框架通用性和有效性**的测试平台（benchmark）。论文的贡献是方法论本身，而非其在特定领域的应用。这完全符合筛选标准中“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”的原则。 **第五步：最终决策** 综合分析，这篇论文的核心贡献是提出了一种新的方法论（InfoSeeker框架），通过将信息寻求与任务规划紧密结合，显著提升了LLM在不确定和部分可观测环境下的通用决策与推理能力。其研究目标是增强LLM的基础能力，评估方式也旨在证明其通用性。因此，这篇论文与您关于“大语言模型通用推理能力”的研究课题高度相关，应被筛选通过。"
    },
    {
        "index": "#114",
        "title": "InvThink: Towards AI Safety via Inverse Reasoning",
        "link": "/arxiv/2510.01569",
        "arxiv_id": "2510.01569",
        "authors": "Yubin Kim, Taehan Kim, Eugene Park, Chunjong Park, Cynthia Breazeal, Daniel McDuff, Hae Won Park",
        "summary": "We present InvThink, a simple yet powerful approach that gives large language models (LLMs) the capability of inverse thinking: reasoning through failure modes before generating responses. Unlike existing safety alignment methods that optimize directly for safe response, InvThink instructs models to 1) enumerate potential harms, 2) analyze their consequences, and 3) generate safe outputs that proactively avoid these risks. Our method reveals three key findings: (i) safety improvements show stronger scaling with model size compared to existing safety methods. (ii) InvThink mitigates safety tax; by training models to systematically consider failure modes, it preserves general reasoning capabilities on standard benchmarks. (iii) beyond general safety tasks, InvThink excels in high-stakes domains including external-facing (medicine, finance, law) and agentic (blackmail, murder) risk scenarios, achieving up to 15.7% reduction in harmful responses compared to baseline methods like SafetyPrompt. We further implement InvThink via supervised fine-tuning, and reinforcement learning across three LLM families. These results suggest that inverse reasoning provides a scalable and generalizable path toward safer, more capable language models.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.369344",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心贡献是提出了一种名为“逆向推理”的全新推理范式。它并非简单地将LLM应用于安全领域，而是从根本上改变了模型的思考过程：在生成回答前，先系统性地推演潜在的失败模式和危害。这是一种对LLM**基础推理机制**的增强，属于提出新的训练/推理范式，旨在提升其逻辑和规划能力。因此，它符合“保留”标准。 2.  **第二步：正面指标——论文高度相关。** 论文摘要中明确包含了多个关键正面指标： - **核心概念**: \"large language models (LLMs)\" - **能力方向**: \"inverse reasoning\", \"reasoning through failure modes\", \"preserves general reasoning capabilities\" - **训练方法**: \"reinforcement learning\" 这些关键词直接指向了LLM的推理能力优化，与你的研究目标高度一致。 3.  **第三步：排除标准——论文并未主要聚焦于排除领域。** 虽然论文标题和摘要多次提及“Safety”，并且评估时涉及了“medicine, finance, law”等特定领域，但这并非论文的**主要焦点**。论文的焦点是“逆向推理”这一**通用方法论**。这些特定领域只是用来验证该方法有效性的“高风险”测试场景，而非研究本身局限于该领域。因此，不应因此排除。 4.  **第四步：处理特殊和模糊情况——安全与推理的结合。** 这是判断的关键。论文完美地符合了关于“安全”的特殊情况处理规则： - 论文提出了一种**新方法**来提升安全性。 - 更重要的是，该方法通过**增强模型的内在推理过程**（系统性地考虑失败模式）来实现安全。 - 摘要明确指出，这种方法“mitigates safety tax; by training models to systematically consider failure modes, it **preserves general reasoning capabilities** on standard benchmarks.” 这直接表明，该方法不仅提升了安全性，还**保留甚至提升了通用推理能力**，最终目标是构建“safer, **more capable** language models”。 **最终决策**: 综合以上分析，这篇论文的本质是提出一种新的通用推理范式（逆向推理），通过增强模型的内在逻辑和规划能力，来同时提升安全性和通用推理质量。这完全契合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，这篇论文应该被**保留**。"
    },
    {
        "index": "#118",
        "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning",
        "link": "/arxiv/2510.01459",
        "arxiv_id": "2510.01459",
        "authors": "Weizhe Chen, Sven Koenig, Bistra Dilkina",
        "summary": "Since the release of Deepseek-R1, reinforcement learning with verifiable rewards (RLVR) has become a central approach for training large language models (LLMs) on reasoning tasks. Recent work has largely focused on modifying loss functions to make RLVR more efficient and effective. In this paper, motivated by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects training data at each step based on the average response length. We evaluate LSPO across multiple base models and datasets, demonstrating that it consistently improves learning effectiveness. In addition, we conduct a detailed ablation study to examine alternative ways of incorporating length signals into dynamic sampling, offering further insights and highlighting promising directions for future research.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-01",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.371149",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **核心判断（第一步）：论文的核心是提升LLM的基础推理能力。** 论文的核心贡献是提出了一种名为“LSPO”的新算法，这是一种新颖的“元RLVR算法”（meta-RLVR algorithm）。RLVR（可验证奖励强化学习）是当前用于训练LLM推理能力的前沿范式。论文的目标是通过改进训练过程中的数据采样策略，来“提升学习效果”。这直接属于“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴。它不是将LLM应用于特定领域，而是聚焦于模型本身的训练方法论。 2.  **正面指标（第二步）：论文命中了多个关键正面指标。** - **核心概念**: 论文标题和摘要明确提到了“Large language models (LLMs)”。 - **能力方向**: 论文标题直接点明了“LLM Reasoning”，摘要也提到其应用于“reasoning tasks”。 - **训练方法**: 论文的核心是“reinforcement learning (RL)”和“policy optimization”，这与你列出的“reinforcement learning (RLHF, RL)”高度吻合。 3.  **排除标准（第三步）：论文不涉及任何排除领域。** 论文的研究内容是纯文本的LLM训练算法，完全没有涉及多模态/视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）。 4.  **特殊和模糊情况（第四步）：不适用。** 论文的研究内容清晰，不涉及需要特殊处理的智能体应用或安全研究等模糊情况。 **最终决策（第五步）：** 综合来看，这篇论文的研究目标是探索如何通过一种更高效的强化学习算法（LSPO）来提升大语言模型在通用推理任务上的表现。其核心贡献是方法论层面的创新，旨在直接增强模型的基础能力，这与你的核心目标“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”完美契合。因此，这篇论文应该被保留。"
    },
    {
        "index": "#108",
        "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead",
        "link": "/arxiv/2510.01624",
        "arxiv_id": "2510.01624",
        "authors": "Feiyang Kang, Michael Kuchnik, Karthik Padthe, Marin Vlastelica, Ruoxi Jia, Carole-Jean Wu, Newsha Ardalani",
        "summary": "In post-training for reasoning Large Language Models (LLMs), the current state of practice trains LLMs in two independent stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as ``RL'' below). In this work, we challenge whether high SFT scores translate to improved performance after RL. We provide extensive counter-examples where this is not true. We find high SFT scores can be biased toward simpler or more homogeneous data and are not reliably predictive of subsequent RL gains or scaled-up post-training effectiveness. In some cases, RL training on models with improved SFT performance could lead to substantially worse outcome compared to RL on the base model without SFT. We study alternative metrics and identify generalization loss on held-out reasoning examples and Pass@large k performance to provide strong proxies for the RL outcome. We trained hundreds of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL performance, prediction based on generalization loss and Pass@large k achieves substantial higher precision, improving $R^2$ coefficient and Spearman's rank correlation coefficient by up to 0.5 (2x). This provides strong utility for broad use cases. For example, in most experiments, we find SFT training on unique examples for a one epoch underperforms training on half examples for two epochs, either after SFT or SFT-then-RL; With the same SFT budget, training only on short examples may lead to better SFT performance, though, it often leads to worse outcome after RL compared to training on examples with varying lengths. Evaluation tool will be open-sourced.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.361243",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。 **核心判断分析 (第一步):** 这篇论文的本质并非将LLM应用于特定领域，而是深入研究了提升LLM推理能力的核心后训练范式——即“监督微调(SFT) + 强化学习(RL)”流程。论文的核心贡献在于，它挑战了“高SFT分数必然带来更好RL效果”这一普遍认知，揭示了当前训练方法中的潜在问题，并提出了更有效的评估指标（如泛化损失、Pass@large k）来预测和指导RL训练。这直接关系到如何更有效地训练和提升LLM的**基础推理能力**，属于对训练范式的改进和优化，完全符合“改进LLM的基础能力、提出新的训练范式”的保留标准。 **正面指标匹配 (第二步):** 论文明确包含了多个强正面指标： - **核心概念**: 论文标题和摘要多次提及 \"Large Language Models (LLMs)\"。 - **能力方向**: 研究的核心目标是提升 \"reasoning\" 能力，并在7个 \"math benchmarks\" 上进行了验证。 - **训练方法**: 论文深入探讨了 \"Supervised Fine-Tuning (SFT)\" 和 \"Reinforcement Learning (RLVR, RL)\" 这两种关键的训练方法。 **排除标准检查 (第三步):** 论文完全不涉及任何排除标准中的领域： - 它不涉及多模态、视觉或机器人控制。 - 它的研究对象是通用的推理能力，而非医疗、化学等特定应用领域。 - 它关注的是训练效果和模型能力，而非水印、安全等应用层面的可靠性问题。 **特殊情况和最终决策 (第四、五步):** 这篇论文不属于需要特殊处理的模糊情况。它清晰地聚焦于LLM的通用推理能力训练方法论。通过揭示SFT阶段的潜在误区并提供更科学的评估手段，该研究为整个社区如何更高效地“提升大语言模型通用推理能力”提供了重要的指导。其结论（如训练数据的选择策略）直接作用于模型训练过程，旨在产出推理能力更强的LLM，这与研究课题的核心目标高度一致。 因此，综合判断，这篇论文是关于LLM推理能力训练方法论的深度研究，对于提升LLM的通用推理能力具有重要的理论和实践价值，应当被保留。"
    },
    {
        "index": "#122",
        "title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort",
        "link": "/arxiv/2510.01367",
        "arxiv_id": "2510.01367",
        "authors": "Xinpeng Wang, Nitish Joshi, Barbara Plank, Rico Angell, He He",
        "summary": "Reward hacking, where a reasoning model exploits loopholes in a reward function to achieve high rewards without solving the intended task, poses a significant threat. This behavior may be explicit, i.e. verbalized in the model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE (Truncated Reasoning AUC Evaluation). Our key observation is that hacking occurs when exploiting the loophole is easier than solving the actual task. This means that the model is using less `effort' than required to achieve high reward. TRACE quantifies effort by measuring how early a model's reasoning becomes sufficient to pass a verifier. We progressively truncate a model's CoT at various lengths, force the model to answer, and measure the verifier-passing rate at each cutoff. A hacking model, which takes a shortcut, will achieve a high passing rate with only a small fraction of its CoT, yielding a large area under the accuracy-vs-length curve. TRACE achieves over 65% gains over our strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B monitor in coding. We further show that TRACE can discover unknown loopholes during training. Overall, TRACE offers a scalable unsupervised approach for oversight where current monitoring methods prove ineffective.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-01",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.373073",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为TRACE的方法，用于检测大语言模型在推理过程中出现的“隐式奖励破解”行为。这种行为指的是模型并没有真正进行推理，而是找到了奖励函数的漏洞来“作弊”。论文的本质是提供一种评估和监督工具，以确保在训练（尤其是强化学习训练）过程中，模型真正学到了我们期望的**通用推理能力**，而不是学会了投机取巧。这直接关系到如何**改进LLM的基础能力**和**优化其训练范式**，因此应该保留。 2.  **第二步：正面指标** 论文明确包含了多个强相关的正面指标： *   **核心概念**: 论文的研究对象是“reasoning model”，属于大语言模型（LLMs）的范畴。 *   **能力方向**: 论文的核心是“reasoning”，并在“math reasoning”和“coding”这两个典型的推理任务上进行了验证。 *   **训练方法**: 论文讨论的“Reward hacking”是强化学习训练中的核心问题之一。提出的方法旨在改进RL训练的监督机制，从而提升训练质量。 3.  **第三步：排除标准** 论文不涉及任何排除标准中的领域： *   它不涉及多模态、视觉。 *   它的应用场景是数学和编码，这两个被视为衡量**通用推理能力**的基准，而非医疗、化学等特定应用领域。 *   虽然涉及“可靠性”，但它不属于应用层面的水印或安全策略。 4.  **第四步：处理特殊和模糊情况** 这篇论文恰好是“模型可靠性”这一特殊情况的完美例证。它不是从社会学角度讨论奖励破解，而是提出了一种**新的技术方法（TRACE）**来检测和缓解一种会严重损害模型推理质量的内在行为（奖励破解）。通过确保模型真正“思考”而非“作弊”，该方法直接**提升了模型的内在可靠性和推理质量**。根据筛选标准，这种情况应该保留。 5.  **第五步：最终决策** 综合来看，这篇论文虽然没有提出一种新的训练算法来直接提升模型性能，但它提出了一种至关重要的**评估和诊断工具**。这个工具能够确保我们在训练LLM的通用推理能力时，模型走在正确的道路上。这对于整个“提升大语言模型通用推理能力”的研究领域来说，是一个基础性的、不可或缺的贡献。它解决了“如何衡量和保证模型真正学会了推理”这一根本性问题，因此完全符合你的筛选要求。"
    },
    {
        "index": "#133",
        "title": "Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs",
        "link": "/arxiv/2510.01218",
        "arxiv_id": "2510.01218",
        "authors": "Sergey Troshin, Wafaa Mohammed, Yan Meng, Christof Monz, Antske Fokkens, Vlad Niculae",
        "summary": "Diversity is an essential metric for evaluating the creativity of outputs generated by language models. Temperature-based sampling is a common strategy to increase diversity. However, for tasks that require high precision, e.g., mathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$ or top-$p$, degrades reasoning quality. We demonstrate that the loss of accuracy is caused by sampling incorrect continuations in sensitive decoding positions. To address this, in this paper, we propose \\textbf{selective sampling}, a method that dynamically switches between greedy and high-temperature sampling based on a sampling risk metric. This risk metric estimates the likelihood of output errors when applying high-temperature sampling on the current token position. To predict sampling risk, we train a lightweight classifier on a small subset of verifiable problems. The trained classifier can be integrated with the base language model with minimal latency overhead. Experiments on mathematical reasoning tasks demonstrate that selective sampling enhances the quality-diversity trade-off, even in high-temperature settings.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-20",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.383906",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心贡献是提出了一种名为“选择性采样”的**解码策略**。它不是将LLM应用于某个特定领域，而是直接作用于LLM的**推理过程本身**。该方法通过动态调整采样温度，旨在解决LLM在执行需要高精度的推理任务（如数学推理）时，因采样策略不当而导致输出质量下降的问题。这是一种直接提升LLM内在推理质量和鲁棒性的方法论研究，属于改进LLM基础能力的范畴，因此应**保留**。 2.  **第二步：正面指标——论文高度相关。** 论文明确包含了多个关键正面指标： *   **核心概念**: 论文的研究对象是“language models”。 *   **能力方向**: 论文的核心任务是提升“mathematical reasoning”的质量，这直接隶属于“reasoning”和“problem-solving”的关键能力方向。 *   这些指标的存在，进一步确认了论文与您研究目标的高度相关性。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容纯粹聚焦于文本语言模型的解码过程，不涉及任何多模态、视觉内容。虽然它使用数学推理作为验证任务，但其提出的方法（选择性采样）是一种**通用的解码框架**，并非为数学领域量身定制的应用，因此不属于“特定应用领域”的排除范畴。同时，论文也未讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况——论文提升了内在推理质量。** 该论文提出的“选择性采样”方法，其本质是通过预测“采样风险”来避免在推理的关键步骤中出现错误。这可以被视为一种减少推理过程错误（类似于一种特定类型的幻觉）的技术，从而提升了模型内在的**推理质量和可靠性**。根据筛选标准，这种通过新方法提升模型内在可靠性和推理质量的研究，应该**保留**。 **最终决策**： 综合以上分析，这篇论文的本质是提出一种通用的解码策略，以增强大语言模型在执行推理任务时的输出质量和稳定性。它直接针对LLM的通用推理能力进行优化，而非将其作为工具应用于特定领域。因此，这篇论文与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度契合，应被筛选入选。"
    },
    {
        "index": "#128",
        "title": "RLP: Reinforcement as a Pretraining Objective",
        "link": "/arxiv/2510.01265",
        "arxiv_id": "2510.01265",
        "authors": "Ali Hatamizadeh, Syeda Nahida Akter, Shrimai Prabhumoye, Jan Kautz, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Yejin Choi",
        "summary": "The dominant paradigm for training large reasoning models starts with pre-training using next-token prediction loss on vast amounts of data. Reinforcement learning, while powerful in scaling reasoning, is introduced only as the very last phase of post-training, preceded by supervised fine-tuning. While dominant, is this an optimal way of training? In this paper, we present RLP, an information-driven reinforcement pretraining objective, that brings the core spirit of reinforcement learning -- exploration -- to the last phase of pretraining. The key idea is to treat chain-of-thought as an exploratory action, with rewards computed based on the information gain it provides for predicting future tokens. This training objective essentially encourages the model to think for itself before predicting what comes next, thus teaching an independent thinking behavior earlier in the pretraining. More concretely, the reward signal measures the increase in log-likelihood of the next token when conditioning on both context and a sampled reasoning chain, compared to conditioning on context alone. This approach yields a verifier-free dense reward signal, allowing for efficient training for the full document stream during pretraining. Specifically, RLP reframes reinforcement learning for reasoning as a pretraining objective on ordinary text, bridging the gap between next-token prediction and the emergence of useful chain-of-thought reasoning. Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an eight-benchmark math-and-science suite by 19%. With identical post-training, the gains compound, with the largest improvements on reasoning-heavy tasks such as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2 increases the overall average from 42.81% to 61.32% and raises the average on scientific reasoning by 23%, demonstrating scalability across architectures and model sizes.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.381524",
        "filter_reason": "该论文完全符合您的研究范围，其核心贡献直接指向提升大语言模型的基础推理能力。以下是详细的判断过程： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心是提出一种全新的训练范式 **RLP (Reinforcement as a Pretraining Objective)**。它没有将强化学习局限于传统的后训练阶段，而是创造性地将其思想融入到预训练阶段。其关键机制是奖励模型生成能够提供“信息增益”的思维链，从而鼓励模型在预训练阶段就学会“独立思考”。 - **符合保留标准**: 这完全符合“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的标准。论文的目标是改进LLM的基础能力本身，而不是将其应用于特定领域。 2.  **第二步：正面指标** - **核心概念**: 论文明确研究 \"large reasoning models\"，即LLM的一种。 - **能力方向**: 摘要中反复强调 \"reasoning\", \"math-and-science suite\", \"reasoning-heavy tasks\"，直接命中了推理、数学推理等核心能力方向。 - **训练方法**: 论文的标题和核心就是 \"Reinforcement as a Pretraining Objective\"，并且明确讨论了 \"chain-of-thought\" 的训练，完全符合强化学习和思维链这两个关键方法。 3.  **第三步：排除标准** - 论文未涉及任何多模态、视觉内容。 - 论文虽然使用了数学和科学基准进行评估，但其方法是通用的，并非为医疗、化学等特定领域设计，因此不属于“特定应用领域”的排除范畴。 - 论文也未讨论模型部署、水印或安全等应用层可靠性问题。 4.  **第四步：处理特殊和模糊情况** - 此处不涉及智能体/工具使用的特殊情况，也不涉及幻觉/安全的特殊情况。论文的研究焦点非常清晰，即通过改进预训练目标来内生地提升模型的推理能力。 5.  **第五步：最终决策** - **综合判断**: 这篇论文的立意非常前沿和深刻。它挑战了当前“预训练+微调+对齐”的主流范式，提出了一种在预训练阶段就注入推理能力的创新方法。其核心贡献 **RLP** 是一种旨在提升模型通用推理能力的**方法论创新**，而非应用创新。论文通过在数学和科学推理任务上的显著提升，有力地证明了其方法的有效性。因此，这篇论文是您研究“大语言模型通用推理能力”的必读文献，完全符合筛选要求。"
    },
    {
        "index": "#1",
        "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
        "link": "/arxiv/2510.02312",
        "arxiv_id": "2510.02312",
        "authors": "Anna Kuzina, Maciej Pioro, Paul N. Whatmough, Babak Ehteshami Bejnordi",
        "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference.",
        "subjects": "Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.798121",
        "filter_reason": "这篇论文完全符合你的筛选标准，应被保留。以下是详细的判断过程： 1.  **第一步：核心判断——本质是提升LLM基础能力** 论文的核心贡献是提出了一种名为“KaVa”的新训练框架。其本质并非将LLM应用于特定领域，而是直接针对LLM的推理能力进行优化。具体来说，它旨在解决“潜在推理”这一新兴推理范式中的监督不足问题，通过一种创新的“压缩KV缓存蒸馏”方法，提升了模型进行多步推理的准确性和效率。这是一种典型的、致力于改进LLM基础通用能力（推理）的方法论研究，因此完全符合核心保留标准。 2.  **第二步：正面指标——高度相关的主题** 论文命中了多个关键的正面指标： *   **核心概念**: 论文标题和摘要中明确提到了“Large Language Models (LLMs)”。 *   **能力方向**: 研究的核心是“reasoning”，特别是“multi-step reasoning”和“natural-language reasoning traces”，这正是你关注的核心。 *   **训练方法**: 提出了“distillation”（蒸馏）作为一种新的训练范式，这与强化学习（RL）等一样，是提升模型能力的重要方法论。 3.  **第三步：排除标准——无任何排除项** 论文的研究内容纯粹聚焦于语言模型内部的推理机制和训练方法，完全避开了所有排除标准： *   不涉及任何多模态（如视觉）内容。 *   不局限于任何特定应用领域（如医疗、化学、金融等）。 *   不讨论模型部署、硬件加速或应用层面的安全水印等问题。 4.  **第四步：处理特殊和模糊情况** 该论文不涉及智能体/工具使用，也不属于幻觉/可解释性/安全的范畴，因此无需进行特殊情况的判断。其焦点非常清晰：通过改进训练方法来增强模型内在的推理能力。 **最终决策**: 综合来看，KaVa这篇论文的核心是提出一种创新的训练框架，通过压缩KV缓存蒸馏来增强大语言模型的“潜在推理”能力。这直接回应了你“提高大语言模型本身的『通用推理能力』”的核心目标。它不是应用研究，而是基础方法论的创新，旨在让LLM在保持推理效率的同时，达到接近显式思维链的准确性。因此，这篇论文是高度相关且前沿的，应被**保留**。"
    },
    {
        "index": "#121",
        "title": "Fine-tuning with RAG for Improving LLM Learning of New Skills",
        "link": "/arxiv/2510.01375",
        "arxiv_id": "2510.01375",
        "authors": "Humaid Ibrahim, Nikolai Rozanov, Marek Rei",
        "summary": "Large language model (LLM) agents deployed for multi-step tasks frequently fail in predictable ways: attempting actions with unmet preconditions, issuing redundant commands, or mishandling environment constraints. While retrieval-augmented generation (RAG) can improve performance by providing runtime guidance, it requires maintaining external knowledge databases and adds computational overhead at every deployment. We propose a simple pipeline that converts inference-time retrieval into learned competence through distillation. Our approach: (1) extracts compact, reusable hints from agent failures, (2) uses these hints to generate improved teacher trajectories via one-shot retrieval at episode start, and (3) trains student models on these trajectories with hint strings removed, forcing internalization rather than memorization. Across two interactive benchmarks, ALFWorld (household tasks) and WebShop (online shopping), distilled students consistently outperform baseline agents, achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens than retrieval-augmented teachers depending on the environment. The approach generalizes across model scales (7B/14B parameters) and agent architectures (ReAct/StateAct), demonstrating that retrieval benefits can be effectively internalized through targeted fine-tuning without permanent runtime dependencies.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-01",
        "category": "cs.CL",
        "crawl_time": "2025-10-07T00:13:09.372613",
        "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种新的训练范式（微调流水线），旨在将推理时检索（RAG）的优势“蒸馏”并“内化”到大语言模型自身的参数中。其本质是改进LLM的基础能力，特别是通过将外部知识转化为内部能力，来提升模型在多步任务中的表现。这直接对应了筛选标准中“改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划、多步推理等通用能力”的要求。它并非将LLM作为工具应用于特定领域，而是专注于提升模型本身的能力。 2.  **第二步：正面指标** 论文高度匹配多个正面指标： *   **核心概念**: 明确以 \"Large language model (LLM) agents\" 为研究对象。 *   **能力方向**: 论文解决的核心问题是LLM在\"multi-step tasks\"（多步任务）中的失败，这直接关联到 **planning**（规划）和 **problem-solving**（问题解决）能力，这些都是通用推理的关键组成部分。 *   **新兴范式**: 论文聚焦于 **llm-based agents**，并研究了如何通过改进其训练方式来提升其能力。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： *   它不涉及多模态、视觉等内容。 *   它使用的基准（ALFWorld和WebShop）是通用的交互式任务环境，用于评估通用规划和推理能力，而非医疗、化学等特定应用领域。 *   它不关注水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文涉及 **智能体**，但其处理方式完全符合“保留”标准。它没有提出一个用于特定领域的智能体，而是提出了一种通用的方法来增强智能体本身的规划和推理能力。其核心思想——将外部检索的“拐杖”通过蒸馏转化为模型内在的“能力”——是一种提升模型通用推理质量的根本性方法，这与“减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的精神内核是一致的。 5.  **第五步：最终决策** 综合来看，这篇论文的核心是提出一种创新的训练方法，通过知识蒸馏将RAG的优势内化到LLM中，从而系统性地提升了模型在多步规划和推理任务上的表现。这完全契合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#22",
        "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning",
        "link": "/arxiv/2510.02212",
        "arxiv_id": "2510.02212",
        "authors": "Hanyang Zhao, Dawen Liang, Wenpin Tang, David Yao, Nathan Kallus",
        "summary": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified framework for training masked diffusion large language models (dLLMs) to reason not only better (furious), but also faster via reinforcement learning (RL). We first unify the existing baseline approach such as d1 by proposing to train surrogate policies via off-policy RL, whose likelihood is much more tractable as an approximation to the true dLLM policy. This naturally motivates a more accurate and informative two-stage likelihood approximation combined with importance sampling correction, which leads to generalized RL algorithms with better sample efficiency and superior task performance. Second, we propose a new direction of joint training efficient samplers/controllers of dLLMs policy. Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by letting the model learn to adaptively allocate an inference threshold for each prompt. By jointly training the sampler, we yield better accuracies with lower number of function evaluations (NFEs) compared to training the model only, obtaining the best performance in improving the Pareto frontier of the inference-time compute of dLLMs. We showcase the effectiveness of our pipeline by training open source large diffusion language models over benchmark math and planning tasks.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.806913",
        "filter_reason": "这篇论文完全符合我的研究范围。以下是基于筛选标准的详细判断过程： 1.  **第一步：核心判断** 论文的核心是提出一种名为 DiFFPO 的全新训练框架。其本质目标是通过强化学习（RL）来直接改进和优化大语言模型（特指扩散LLM）的推理能力，使其在推理质量上“更好”，在推理速度上“更快”。这完全属于改进LLM基础能力和通用推理能力的范畴，而不是将其作为工具应用于特定领域。论文的核心贡献是方法论层面的创新，直接服务于提升模型本身的能力，因此符合核心判断的**保留**标准。 2.  **第二步：正面指标** 论文包含了大量与筛选目标高度相关的正面指标： - **核心概念**: 明确研究 \"diffusion large language models (dLLMs)\"，即大语言模型。 - **能力方向**: 核心关注 \"reasoning\"，并明确指出在 \"math and planning tasks\" 上进行验证，这直接命中了数学推理和规划能力。 - **训练方法**: 整个方法的核心是 \"reinforcement learning (RL)\"，提出了 \"off-policy RL\" 和新的优化算法，完全匹配强化学习这一关键方法。 论文在所有关键正面指标上都有明确体现，相关性极高。 3.  **第三步：排除标准** 论文的研究焦点与所有排除标准均无关联： - 它不涉及任何多模态、视觉或扩散模型在图像生成等领域的应用。此处的\"Diffusion\"指的是语言模型的解码/生成过程，而非图像生成模型。 - 它的应用场景是 \"benchmark math and planning tasks\"，这些是评估通用推理能力的标准基准，而非医疗、化学、机器人等特定应用领域。 - 它不关注水印、安全、安保等模型可靠性问题，而是聚焦于提升模型的内在推理效率和准确性。 4.  **第四步：处理特殊和模糊情况** 本论文的情况不涉及特殊或模糊的类别，其研究方向非常清晰和纯粹：通过创新的训练框架（RL）来提升LLM的通用推理能力。 **最终决策**: 综合以上分析，这篇论文提出了一种创新的强化学习框架（DiFFPO），旨在从训练和推理两个层面直接提升大语言模型的通用推理能力，包括推理的准确性和效率。其工作核心是方法论的创新，并以经典的通用推理任务（数学、规划）作为验证基准，与“提高大语言模型本身的通用推理能力”这一研究目标完全契合。因此，这是一篇高度相关且应保留的论文。"
    },
    {
        "index": "#26",
        "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning",
        "link": "/arxiv/2510.02180",
        "arxiv_id": "2510.02180",
        "authors": "Silvia Sapora, Devon Hjelm, Alexander Toshev, Omar Attia, Bogdan Mazoure",
        "summary": "Inverse Reinforcement Learning aims to recover reward models from expert demonstrations, but traditional methods yield \"black-box\" models that are difficult to interpret and debug. In this work, we introduce GRACE (Generating Rewards As CodE), a method for using Large Language Models within an evolutionary search to reverse-engineer an interpretable, code-based reward function directly from expert trajectories. The resulting reward function is executable code that can be inspected and verified. We empirically validate GRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns highly accurate rewards, even in complex, multi-task settings. Further, we demonstrate that the resulting reward leads to strong policies, compared to both competitive Imitation Learning and online RL approaches with ground-truth rewards. Finally, we show that GRACE is able to build complex reward APIs in multi-task setups.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.813562",
        "filter_reason": "这篇论文完全符合你的筛选标准，应该被保留。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为GRACE的新框架。这个框架的本质并非将LLM应用于某个特定垂直领域，而是利用LLM的代码生成和模式识别能力，结合进化搜索，来解决一个基础的人工智能问题——逆强化学习（IRL）。IRL的目标是从专家行为中推断出其背后的奖励函数，这是智能体进行规划和决策的核心。因此，该论文致力于提出一种新的方法论来增强智能体（由LLM驱动或与LLM相关）的规划和问题解决能力，这直接属于“改进LLM的基础能力、增强其规划、多步推理等通用能力”的范畴。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 明确以“Large Language Models”为核心组件。 *   **能力方向**: 虽然没有直接使用\"reasoning\"一词，但其解决的逆强化学习问题是实现高级规划和问题解决的关键一步。学习到的奖励函数是智能体进行有效推理和规划的基石。 *   **训练方法**: 明确使用了“evolutionary search”（进化搜索），这是一种重要的优化和学习范式。 *   **新兴范式**: 该框架可以被看作是一种高级的“tool use”形式，其中LLM被用作生成“奖励函数代码”的工具，以增强整个智能体系统的能力。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： *   它不涉及任何多模态或视觉内容。 *   它使用的基准测试是通用的AI环境，而非医疗、化学等特定应用领域。 *   它不关注水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文是“特殊和模糊情况”的绝佳范例，并且完全符合“保留”条件： *   **智能体/工具使用**: GRACE框架提出了一种**通用的**方法，即利用LLM生成可解释的奖励函数。这并非“用于化学实验的智能体”，而是一个可以应用于多种任务（如论文中的BabyAI和AndroidWorld）的通用框架，旨在提升智能体本身的学习和规划能力。 *   **可解释性**: 论文的核心亮点之一就是生成“interpretable, code-based reward function”。这不仅仅是讨论可解释性的重要性，而是提出了一种**新方法来增强模型内在的可解释性**。通过将黑盒的奖励模型变为可检查、可验证的代码，该方法直接提升了智能体决策过程的透明度和可靠性，从而间接提升了其推理质量。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种创新的、结合了LLM和进化搜索的通用框架（GRACE），旨在解决逆强化学习这一基础AI问题，从而增强智能体的规划和问题解决能力。它通过生成可解释的代码来提升模型的内在可靠性。这完全契合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#78",
        "title": "Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning",
        "link": "/arxiv/2510.01656",
        "arxiv_id": "2510.01656",
        "authors": "Jiashun Liu, Johan Obando-Ceron, Han Lu, Yancheng He, Weixun Wang, Wenbo Su, Bo Zheng, Pablo Samuel Castro, Aaron Courville, Ling Pan",
        "summary": "Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing them with average advantage baselines. This shift is largely pragmatic: conventional value functions are computationally expensive to train at LLM scale and often fail under sparse rewards and long reasoning horizons. We revisit this bottleneck from an architectural perspective and introduce Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable framework that restores the critics role while remaining efficient in large-model settings. AsyPPO employs a set of lightweight mini-critics, each trained on disjoint prompt shards. This design encourages diversity while preserving calibration, reducing value-estimation bias. Beyond robust estimation, AsyPPO leverages inter-critic uncertainty to refine the policy update: (i) masking advantages in states where critics agree and gradients add little learning signal, and (ii) filtering high-divergence states from entropy regularization, suppressing spurious exploration. After training on open-source data with only 5,000 samples, AsyPPO consistently improves learning stability and performance across multiple benchmarks over strong baselines, such as GRPO, achieving performance gains of more than six percent on Qwen3-4b-Base and about three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without additional tricks. These results highlight the importance of architectural innovations for scalable, efficient algorithms.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.864184",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“Asymmetric Proximal Policy Optimization (AsyPPO)”的**新训练框架/算法**。其本质是改进强化学习（RL）在大语言模型（LLM）上的应用，以解决现有方法（如经典PPO）在稀疏奖励和长推理轨迹上的瓶颈。这是一种**基础性的方法论创新**，旨在通过优化训练过程来提升模型的能力，完全符合“改进LLM的基础能力”和“提出新的训练范式”的保留标准。论文标题和摘要都明确指出其目标是“boost LLM reasoning”（提升LLM推理能力），这与您的研究目标高度一致。 2.  **第二步：正面指标** 该论文命中了多个关键正面指标： *   **核心概念**: 论文的研究对象是“LLMs”（如Qwen3-4b-Base）。 *   **能力方向**: 论文的直接目标就是提升“LLM reasoning”（LLM推理能力）。 *   **训练方法**: 论文的核心内容是关于“Reinforcement Learning (RL)”，具体是对PPO算法的改进。 *   **新兴范式**: 虽然没有直接提及Agent，但其研究的强化学习训练方法是构建高性能LLM Agent的基石，属于更上游的基础研究。 3.  **第三步：排除标准** 该论文完全没有触及任何排除标准： *   它不涉及多模态、视觉等内容。 *   它的研究是通用的，在“多个基准测试”上验证，而非聚焦于医疗、化学等特定应用领域。 *   它关注的是训练算法的效率和性能，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文的情况非常清晰，不涉及模糊地带。它不是将智能体或工具应用于特定领域，而是研究如何更好地训练模型本身。它也不是在讨论幻觉的社会学影响，而是通过改进算法来从根本上提升模型的推理稳定性和质量。 5.  **第五步：最终决策** 综合以上分析，这篇论文是一篇典型的、致力于提升大语言模型**通用推理能力**的前沿方法论研究。它通过提出一种新颖的、可扩展的强化学习架构（AsyPPO），直接解决了LLM在复杂推理任务中训练的难题，并展示了在多个通用基准上的性能提升。因此，这篇论文是您研究课题的理想候选。"
    },
    {
        "index": "#94",
        "title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization",
        "link": "/arxiv/2510.01555",
        "arxiv_id": "2510.01555",
        "authors": "Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) leverages a Kullback-Leibler (KL) divergence loss to stabilize training and prevent overfitting. However, in methods such as GRPO, its implementation may be guided by principles from numerical value estimation-a practice that overlooks the term's functional role as an optimization loss. To analyze this issue, we establish a unified framework that connects two seemingly distinct implementation styles: using the mathematical term $k_n$ as a detached coefficient for the policy's score function ('$k_n$ in reward') or as a direct loss function through which gradients are propagated ('$k_n$ as loss'). We show that the latter can always be analyzed via an equivalent gradient coefficient in the former, unifying the two perspectives. Through this framework, we prove that the conventional '$k_1$ in reward' (like in PPO) is the principled loss for Reverse KL (RKL) regularization. We further establish a key finding: under on-policy conditions, the '$k_2$ as loss' formulation is, in fact, gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our work, identifies both as the theoretically sound implementations of the RKL objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like in GRPO) is merely a first-order, biased approximation of the principled loss. Furthermore, we argue that common off-policy implementations of '$k_n$ as loss' methods are biased due to neglected importance sampling, and we propose a principled correction. Our findings provide a comprehensive, gradient-based rationale for choosing and correctly implementing KL regularization, paving the way for more robust and effective RLHF systems.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.877275",
        "filter_reason": "这篇论文完全符合我的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是**改进RLHF（Reinforcement Learning from Human Feedback）这一训练范式本身**。RLHF是提升大语言模型（尤其是其通用能力、对齐和推理能力）的关键技术。这篇论文深入剖析了RLHF中的一个核心组件——KL正则化，通过理论分析统一了不同的实现方式，并指出了更优、更无偏的实现方法。这直接属于“提出新的训练范式、增强其基础能力”的范畴，其目标是让LLM的训练过程更“robust and effective”，从而提升模型本身的能力。它并非将LLM应用于特定领域，而是专注于改进LLM的“内功”。 2.  **第二步：正面指标** 论文高度符合正面指标。其核心主题是**Reinforcement Learning (RLHF, RL)**，这正是筛选标准中明确列出的关键训练方法。虽然摘要未直接提及“reasoning”，但RLHF是提升模型包括推理在内的各项通用能力的基石。优化RLHF，就是在为提升LLM的通用推理能力打下更坚实的基础。 3.  **第三步：排除标准** 论文完全避开了所有排除标准。它不涉及多模态、视觉、任何特定应用领域（如医疗、化学），也不讨论模型部署、硬件加速或应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需特殊判断。 **最终决策**: 这篇论文的本质是对LLM核心训练技术RLHF的深入优化。它通过理论分析和数学证明，为如何更正确、更有效地进行KL正则化提供了指导，从而有助于构建更鲁棒、能力更强的LLM。这种对基础训练范式的改进，是提升LLM通用推理能力的根本性工作之一。因此，这篇论文与我的研究目标高度契合，应予以保留。"
    },
    {
        "index": "#97",
        "title": "Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code",
        "link": "/arxiv/2510.01539",
        "arxiv_id": "2510.01539",
        "authors": "Aniket Vashishtha, Qirun Dai, Hongyuan Mei, Amit Sharma, Chenhao Tan, Hao Peng",
        "summary": "Counterfactual reasoning, a hallmark of intelligence, consists of three steps: inferring latent variables from observations (abduction), constructing alternatives (interventions), and predicting their outcomes (prediction). This skill is essential for advancing LLMs' causal understanding and expanding their applications in high-stakes domains such as scientific research. However, existing efforts in assessing LLM's counterfactual reasoning capabilities tend to skip the abduction step, effectively reducing to interventional reasoning and leading to overestimation of LLM performance. To address this, we introduce executable counterfactuals, a novel framework that operationalizes causal reasoning through code and math problems. Our framework explicitly requires all three steps of counterfactual reasoning and enables scalable synthetic data creation with varying difficulty, creating a frontier for evaluating and improving LLM's reasoning. Our results reveal substantial drop in accuracy (25-40%) from interventional to counterfactual reasoning for SOTA models like o4-mini and Claude-4-Sonnet. To address this gap, we construct a training set comprising counterfactual code problems having if-else condition and test on out-of-domain code structures (e.g. having while-loop); we also test whether a model trained on code would generalize to counterfactual math word problems. While supervised finetuning on stronger models' reasoning traces improves in-domain performance of Qwen models, it leads to a decrease in accuracy on OOD tasks such as counterfactual math problems. In contrast, reinforcement learning induces the core cognitive behaviors and generalizes to new domains, yielding gains over the base model on both code (improvement of 1.5x-2x) and math problems. Analysis of the reasoning traces reinforces these findings and highlights the promise of RL for improving LLMs' counterfactual reasoning.",
        "subjects": "Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.878733",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心是致力于提升大语言模型（LLM）的通用推理能力。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM基础能力。** 论文的核心贡献并非将LLM应用于某个特定领域，而是聚焦于LLM在**反事实推理**这一高级认知能力上的不足。反事实推理是逻辑推理和因果推理的核心组成部分，属于LLM的通用基础能力。论文提出了一个新框架来评估和训练这种能力，并发现强化学习（RL）是比监督微调更有效的训练范式。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——论文高度相关。** 该论文命中了多个关键的正面指标： *   **核心概念**: 论文研究对象明确为 `Large language models, LLMs`。 *   **能力方向**: 论文的核心是 `reasoning`，特别是更深层次的 `causal reasoning` 和 `counterfactual reasoning`，并通过 `math reasoning` 和代码问题进行验证。 *   **训练方法**: 论文对比了监督微调和 `reinforcement learning (RL)`，并得出RL在提升该能力上具有更好泛化性的结论，这与你的筛选标准高度一致。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文内容是纯粹的关于LLM推理能力的方法论研究，不涉及多模态、视觉、医疗、化学、机器人等任何特定应用领域，也不关注模型基础设施或水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况——论文属于应保留的范畴。** 论文将推理过程“可执行化”，通过代码来构建和验证反事实问题。这可以被理解为一种**通用的工具使用方法**（将代码作为形式化推理的工具），其目的是为了增强LLM的通用问题解决能力，而非应用于特定领域，因此应该保留。同时，论文通过提出新方法来提升模型的推理质量，这与“提升模型的通用可靠性和推理质量”的保留方向一致。 **最终决策**: 综合以上分析，这篇论文的本质是一项方法论研究，它精准地定位了当前LLM在通用推理能力（反事实推理）上的一个具体缺陷，并提出了创新的评估框架和有效的训练方法（强化学习）来弥补这一缺陷。其研究目标与你的“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标完全吻合。因此，应判定为符合要求。"
    },
    {
        "index": "#107",
        "title": "Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information",
        "link": "/arxiv/2510.01499",
        "arxiv_id": "2510.01499",
        "authors": "Rui Ai, Yuqi Pan, David Simchi-Levi, Milind Tambe, Haifeng Xu",
        "summary": "With the rapid progress of multi-agent large language model (LLM) reasoning, how to effectively aggregate answers from multiple LLMs has emerged as a fundamental challenge. Standard majority voting treats all answers equally, failing to consider latent heterogeneity and correlation across models. In this work, we design two new aggregation algorithms called Optimal Weight (OW) and Inverse Surprising Popularity (ISP), leveraging both first-order and second-order information. Our theoretical analysis shows these methods provably mitigate inherent limitations of majority voting under mild assumptions, leading to more reliable collective decisions. We empirically validate our algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all cases, our methods consistently outperform majority voting, offering both practical performance gains and conceptual insights for the design of robust multi-agent LLM pipelines.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Science and Game Theory",
        "date": "2025-10-01",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.888596",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文本质** 论文的核心贡献是提出了两种新的聚合算法（Optimal Weight 和 Inverse Surprising Popularity），旨在提升多智能体LLM系统在推理任务中的集体决策质量。这并非将LLM作为工具应用于特定领域，而是直接作用于LLM系统的核心推理过程。通过改进多智能体间的信息聚合方式，论文致力于提升LLM系统整体的通用推理能力。因此，它符合“改进LLM的基础能力”和“智能体协作框架”的保留标准。 2.  **第二步：正面指标——论文主题** 论文摘要中明确包含了多个核心正面指标： *   **核心概念**: \"multi-agent large language model (LLM)\" *   **能力方向**: \"LLM reasoning\", \"collective decisions\" *   **新兴范式**: \"multi-agent LLM pipelines\" 这些关键词表明论文直接聚焦于LLM的推理能力和多智能体系统，与你的研究目标高度相关。 3.  **第三步：排除标准——主要焦点** 论文虽然提到了在\"real-world healthcare setting ARMMAN\"上进行验证，但这只是为了证明其方法的普适性和有效性，其**主要焦点和核心贡献**是通用聚合算法的设计与理论分析，而非医疗领域的应用本身。因此，它没有触犯“特定应用领域”的排除标准。论文也不涉及多模态、视觉或模型基础设施等排除领域。 4.  **第四步：处理特殊和模糊情况** 这篇论文是“智能体”相关研究的一个典型范例。它提出了一种**通用的智能体协作方法**（更优的聚合策略）来增强LLM的通用问题解决能力（即集体推理）。这与“用于化学实验自动化的智能体”这类特定领域应用有本质区别。因此，根据筛选标准，应该保留。 5.  **第五步：最终决策** 综合以上分析，该论文的本质是提出一种新的方法论来增强多智能体LLM系统的通用推理能力。它通过改进信息聚合这一基础环节，为构建更强大、更可靠的LLM推理框架提供了理论和实践支持。这完全契合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为符合要求。"
    },
    {
        "index": "#112",
        "title": "The Three Regimes of Offline-to-Online Reinforcement Learning",
        "link": "/arxiv/2510.01460",
        "arxiv_id": "2510.01460",
        "authors": "Lu Li, Tianwei Ni, Yihao Sun, Pierre-Luc Bacon",
        "summary": "Offline-to-online reinforcement learning (RL) has emerged as a practical paradigm that leverages offline datasets for pretraining and online interactions for fine-tuning. However, its empirical behavior is highly inconsistent: design choices of online-fine tuning that work well in one setting can fail completely in another. We propose a stability--plasticity principle that can explain this inconsistency: we should preserve the knowledge of pretrained policy or offline dataset during online fine-tuning, whichever is better, while maintaining sufficient plasticity. This perspective identifies three regimes of online fine-tuning, each requiring distinct stability properties. We validate this framework through a large-scale empirical study, finding that the results strongly align with its predictions in 45 of 63 cases. This work provides a principled framework for guiding design choices in offline-to-online RL based on the relative performance of the offline dataset and the pretrained policy.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-01",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.896051",
        "filter_reason": "这篇论文符合筛选标准，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种关于“离线到在线强化学习”的理论框架和原则。这本质上是一种关于模型如何学习的新训练范式。它探讨的是模型从静态数据（离线）过渡到动态交互（在线）学习时，如何平衡“稳定性”（保留已有知识）和“可塑性”（学习新知识）这一根本性问题。这完全符合筛选标准中“改进LLM的基础能力、提出新的训练范式”的要求，而非将模型作为工具应用于特定领域。 2.  **第二步：正面指标** - **训练方法**: 论文的核心主题是“Reinforcement Learning (RL)”，特别是“Offline-to-Online RL”，这是当前提升大语言模型（特别是通过RLHF、智能体交互等方式）的关键技术方向。 - **能力方向**: 虽然摘要没有直接提及“reasoning”或“planning”，但“online fine-tuning”（在线交互微调）的最终目的就是提升模型在动态环境中的问题解决能力。一个能够有效进行在线学习的模型，其逻辑推理和规划能力必然会得到增强。论文所解决的稳定性与可塑性难题，是实现这种能力提升的底层技术保障。 3.  **第三步：排除标准** 论文完全不涉及多模态、特定应用领域（如医疗、化学），也未讨论模型可靠性（如水印、安全）。因此，它没有被任何排除标准命中。 4.  **第四步：处理特殊和模糊情况** 主要的模糊点在于摘要中未明确提及“Large Language Models (LLMs)”。然而，考虑到当前AI研究的前沿背景，“离线到在线强化学习”这一范式与LLM的训练和优化（例如，从预训练模型到通过与人类或环境交互进行微调）高度相关。论文提出的“稳定性-可塑性原则”是LLM社区在进行在线学习时面临的核心挑战之一。这项工作提供了一个通用的、原则性的框架，可以直接指导如何更有效地对LLM进行在线微调，从而提升其通用能力。因此，尽管没有明说，其贡献对LLM领域是直接且基础性的。 5.  **第五步：最终决策** 综合来看，这篇论文的核心贡献是提出了一种新的、更稳健的强化学习训练范式。这种范式是提升大语言模型通用能力（尤其是在需要与环境交互的复杂任务中）的关键方法论。它关注的是学习过程本身的优化，而非特定应用，完全契合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，应判定为符合要求。"
    },
    {
        "index": "#114",
        "title": "How Well Can Preference Optimization Generalize Under Noisy Feedback?",
        "link": "/arxiv/2510.01458",
        "arxiv_id": "2510.01458",
        "authors": "Shawn Im, Yixuan Li",
        "summary": "As large language models (LLMs) advance their capabilities, aligning these models with human preferences has become crucial. Preference optimization, which trains models to distinguish between preferred and non-preferred responses based on human feedback, has become a crucial component for aligning LLMs. However, most existing works assume noise-free feedback, which is unrealistic due to the inherent errors and inconsistencies in human judgments. This paper addresses the impact of noisy feedback on preference optimization, providing generalization guarantees under these conditions. In particular, we consider noise models that correspond to common real-world sources of noise, such as mislabeling and uncertainty. Unlike traditional analyses that assume convergence, our work focuses on finite-step preference optimization, offering new insights that are more aligned with practical LLM training. We describe how generalization decays with different types of noise across levels of noise rates based on the preference data distribution and number of samples. Our analysis for noisy preference learning applies to a broad family of preference optimization losses such as DPO, IPO, SLiC, etc. Empirical validation on contemporary LLMs confirms the practical relevance of our findings, offering valuable insights for developing AI systems that align with human preferences.",
        "subjects": "Machine Learning",
        "date": "2025-10-01",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.896927",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是研究“偏好优化”这一训练范式。偏好优化（如DPO、IPO）是继RLHF之后，对齐大语言模型与人类偏好的关键技术，属于改进LLM基础训练方法的范畴。论文并非将LLM作为工具应用于特定领域，而是深入分析并改进这一基础训练过程本身，旨在提升模型输出的整体质量和可靠性。这完全符合“改进LLM的基础能力、提出新的训练范式”的保留标准。 2.  **第二步：正面指标** - **核心概念**: 论文明确以大语言模型为研究对象。 - **训练方法**: 论文聚焦于“偏好优化”，这是RLHF（基于人类反馈的强化学习）家族的核心方法，直接关系到模型的基础能力训练。 - **能力方向**: 虽然摘要未直接使用“reasoning”一词，但“与人类偏好对齐”是模型能够进行高质量推理和问题解决的前提。一个无法区分好坏回答、无法遵循复杂指令的模型，不可能具备强大的通用推理能力。因此，优化对齐过程，就是在为提升模型的通用能力（包括推理）打下更坚实的基础。 3.  **第三步：排除标准** 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或应用层面的模型可靠性（如水印）。因此，它不触犯任何排除标准。 4.  **第四步：处理特殊和模糊情况** 这篇论文的情况与“幻觉/可解释性/安全”的保留条款高度相关。人类反馈中的“噪声”是导致模型学习到错误偏好、产生幻觉或不可靠输出的重要原因之一。该论文通过理论分析，揭示了噪声如何影响偏好优化的泛化能力，这为开发更鲁棒、更可靠的训练方法提供了关键见解。这本质上是在提出一种新方法（通过理论分析指导实践）来增强模型的内在可靠性，从而提升其输出质量（包括推理质量），而非进行社会学层面的讨论。因此，应该保留。 **最终决策**: 该论文的核心贡献在于对“偏好优化”这一基础训练范式进行了深入的理论分析，探讨了在更真实的“有噪声反馈”条件下，模型的泛化表现。这项工作直接服务于如何更好地训练和优化LLM，使其更稳定、更可靠地与人类偏好对齐。一个对齐得更好的模型，其内在的逻辑性、连贯性和遵循指令的能力都会更强，这正是通用推理能力的基石。因此，该论文与“提高大语言模型本身的通用推理能力”这一核心目标高度相关，应予以保留。"
    },
    {
        "index": "#209",
        "title": "Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation",
        "link": "/arxiv/2510.01528",
        "arxiv_id": "2510.01528",
        "authors": "Daniel Zhao, Abhilash Shankarampeta, Lanxiang Hu, Tajana Rosing, Hao Zhang",
        "summary": "We propose a novel method that leverages sparse autoencoders (SAEs) and clustering techniques to analyze the internal token representations of large language models (LLMs) and guide generations in mathematical reasoning tasks. Our approach first trains an SAE to generate sparse vector representations for training tokens, then applies k-means clustering to construct a graph where vertices represent token clusters and weighted edges capture sequential token transitions. Using this graph, we define an edge-weight based reward function to quantify adherence to established reasoning traces, thereby identifying exploitative reasoning trajectories. Additionally, we measure generation diversity from clustering to assess the extent of exploration. Our findings indicate that balancing both exploitation and exploration is crucial for achieving high accuracy in mathematical reasoning tasks. During generation, the SAE can serve as a scalable reward model to guide generations, ensuring a balanced trade-off between exploitation and exploration. This prevents extreme behaviors in either direction, ultimately fostering a higher-quality reasoning process in LLMs.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.968909",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。以下是我的详细判断过程： 1.  **第一步：核心判断——这篇论文的本质是什么？** 论文的核心是提出一种**全新的方法论**，旨在**提升大语言模型在推理任务中的表现**。它通过稀疏自编码器（SAE）分析模型的内部表示，并构建一个基于图的奖励模型来引导生成过程，从而优化思维链（CoT）推理。这完全符合“改进LLM的基础能力”、“提出新的训练范式”和“增强其逻辑、数学、多步推理等通用能力”的保留标准。论文的本质是探索如何让LLM“更好地思考”，而不是将LLM用作解决特定领域问题的工具。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文摘要中包含了大量高度相关的正面指标： *   **核心概念**: 明确提及 \"large language models (LLMs)\"。 *   **能力方向**: 核心主题是 \"reasoning\"，特别是 \"mathematical reasoning\" 和 \"reasoning process\"。 *   **训练方法**: 提出了 \"reward function\" 来引导生成，这与强化学习（RL）的优化思想一致，属于新的训练/引导范式。 *   **新兴范式**: 研究内容与 \"思维链\" 紧密相关，并试图对其进行优化。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全避开了所有排除标准： *   **多模态与视觉**: 未涉及任何视觉或多模态内容。 *   **特定应用领域**: 虽然在 \"mathematical reasoning tasks\" 上进行验证，但数学推理被视为衡量LLM通用推理能力的基准，而不是一个像医疗、化学那样的特定应用领域。论文提出的方法是通用的，旨在提升模型底层的推理过程。 *   **模型可靠性（应用层面）**: 未提及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文标题中提到了 \"Interpretable\"（可解释的），这属于“可解释性”的特殊情况。根据筛选标准，如果提出新方法来增强模型内在的可解释性，从而提升推理质量，则应保留。本文正是如此：它利用SAE来分析内部表示，其目的不是为了社会学讨论，而是为了量化推理轨迹、构建奖励模型，最终“fostering a higher-quality reasoning process in LLMs”（在LLM中培养更高质量的推理过程）。因此，这里的可解释性是服务于提升推理能力这一核心目标的工具，应予以保留。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种创新的、通用的方法（SAE引导的生成）来直接优化LLM的推理过程。它聚焦于提升模型的基础能力（推理），而非特定领域应用。论文内容与你的核心目标——“致力于提高大语言模型（LLM）本身的『通用推理能力』”——高度契合。因此，最终判断为 **True**。"
    },
    {
        "index": "#210",
        "title": "WALT: Web Agents that Learn Tools",
        "link": "/arxiv/2510.01524",
        "arxiv_id": "2510.01524",
        "authors": "Viraj Prabhu, Yutong Dai, Matthew Fernandez, Jing Gu, Krithika Ramakrishnan, Yanqi Luo, Silvio Savarese, Caiming Xiong, Junnan Li, Zeyuan Chen, Ran Xu",
        "summary": "Web agents promise to automate complex browser tasks, but current methods remain brittle -- relying on step-by-step UI interactions and heavy LLM reasoning that break under dynamic layouts and long horizons. Humans, by contrast, exploit website-provided functionality through high-level operations like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools), a framework that reverse-engineers latent website functionality into reusable invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust implementations of automations already designed into websites -- spanning discovery (search, filter, sort), communication (post, comment, upvote), and content management (create, edit, delete). Tools abstract away low-level execution: instead of reasoning about how to click and type, agents simply call search(query) or create(listing). This shifts the computational burden from fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena and WebArena, WALT achieves higher success with fewer steps and less LLM-dependent reasoning, establishing a robust and generalizable paradigm for browser automation.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-01",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.969475",
        "filter_reason": "这篇论文符合我的研究目标，应予以保留。 我的判断过程如下： 1.  **核心判断（第一步）**: 论文的核心贡献是提出了一种名为WALT的框架，它改变了LLM智能体的推理模式。传统方法依赖LLM进行脆弱的、低级的、逐步的UI操作推理（“我应该点击哪里”），而WALT将网站的潜在功能逆向工程为可重用的高级工具（如`search(query)`）。这使得智能体的推理从“如何执行”转变为“调用哪个工具”，本质上是一种更高级、更鲁棒的抽象推理。这直接触及并改进了LLM在长任务、复杂环境下的『通用推理能力』和『规划能力』，属于提出新的方法论来增强LLM基础能力的范畴。因此，它通过了第一步的核心判断。 2.  **正面指标（第二步）**: 论文包含了多个关键的正面指标。 *   **核心概念**: 论文的研究对象是基于LLM的Web Agents。 *   **能力方向**: 论文的核心是解决`reasoning`问题，特别是长视野下的`planning`和`problem-solving`。 *   **新兴范式**: 论文提出了一个关于`llm-based agents`和`tool use`的全新框架。 3.  **排除标准（第三步）**: 论文的主要焦点不涉及任何排除标准。虽然它应用于Web浏览器环境，但这并非生物、医疗等特定领域。Web是一个通用的信息交互平台，论文的目标是建立一个“robust and generalizable paradigm”，而非针对某个特定网站的解决方案。论文也未涉及多模态视觉、模型基础设施或应用层面的水印与安全等问题。 4.  **处理特殊和模糊情况（第四步）**: 这篇论文是“智能体/工具使用”特殊情况的完美例证。WALT不是将智能体应用于特定领域（如“用于化学实验的智能体”），而是提出了一种**通用的智能体协作/工具使用框架**。它通过让LLM学习和使用工具，来增强其在复杂、动态环境下的通用问题解决能力，从而提升了推理的鲁棒性和效率。这完全符合“应该保留”的条件。 **最终决策（第五步）**: 综合以上分析，WALT论文的核心贡献在于提出了一种新的方法论，通过工具抽象来优化和增强LLM的通用推理与规划能力。它不是简单地将LLM作为工具应用，而是从根本上改进了LLM的思考方式和解决问题的范式。因此，这篇论文与“大语言模型通用推理能力”的研究课题高度相关，应当被筛选出来。"
    },
    {
        "index": "#235",
        "title": "OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models",
        "link": "/arxiv/2510.01253",
        "arxiv_id": "2510.01253",
        "authors": "Jianzhang Zhang, Jialong Zhou, Chuang Liu",
        "summary": "Large language models (LLMs) demonstrate strong mathematical reasoning, but reliance on closed-source APIs for OR tasks raises privacy concerns, and training open-source models from scratch incurs high compute costs. We introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a semi-automatic data synthesis pipeline that generates diverse OR problem-answer pairs and augments the model with external solvers to produce API calls. On three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot evaluation on two unseen OR problem types, it attains 54% average accuracy, a 21 percentage-point improvement over the strongest baseline. These findings validate the efficacy of tool-augmented fine-tuning LLMs for accurate and generalizable OR problem modeling and solving.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T00:13:09.997766",
        "filter_reason": "根据您提供的筛选标准，我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为\"OR-Toolformer\"的新方法，通过工具增强的微调范式来提升大语言模型解决运筹学问题的能力。运筹学是数学、逻辑和规划推理的典型应用领域。因此，这篇论文的本质是**提出一种新的训练范式（工具增强的微调）来增强LLM的通用推理能力（特别是数学和规划推理）**，而不是将LLM作为一个黑箱工具应用到一个垂直领域。这完全符合\"保留\"标准。 2.  **第二步：正面指标** 该论文命中了多个关键的正面指标： *   **核心概念**: 论文明确研究\"Large language models (LLMs)\"。 *   **能力方向**: 论文聚焦于\"mathematical reasoning\"（摘要中明确提及），而运筹学问题本身就是高级的规划 和问题-solving 的体现。 *   **训练方法**: 论文提出了一种新的微调范式，并包含半自动数据合成管道，这是一种创新的训练方法。 *   **新兴范式**: 论文的核心是\"Tool Augmented Large Language Models\"，属于\"tool use\"这一新兴范式。 3.  **第三步：排除标准** 该论文没有被排除标准覆盖： *   它不涉及多模态与视觉。 *   虽然标题和摘要提到了\"Operations Research (OR)\"，但OR本身不是一个像医疗、化学那样的特定领域知识。它更偏向于一种通用的、抽象的**方法论**，用于解决优化、规划和决策问题，这些都是通用推理能力的核心组成部分。因此，它不应被视为应被排除的\"特定应用领域\"。 *   它不关注水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 论文提出了一个工具使用框架来增强LLM。虽然它应用于OR问题，但如前所述，OR是通用推理的一个基础领域，而非垂直应用领域。该论文的贡献在于验证了\"工具增强的微调\"这一通用方法论对于提升模型在某一类推理任务上的有效性，这与\"用于化学实验自动化的智能体\"有本质区别。前者是增强通用能力，后者是应用通用能力到特定领域。因此，应该保留。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是**通过一种新颖的工具增强微调方法，显著提升了LLM在数学、规划和逻辑推理这一核心通用能力上的表现**。它虽然以运筹学为测试床，但其方法论和目标与提升LLM的通用推理能力高度一致。因此，该论文完全符合您的研究范围。"
    },
    {
        "index": "#9",
        "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning",
        "link": "/arxiv/2510.02091",
        "arxiv_id": "2510.02091",
        "authors": "Xinyuan Song, Keyu Wang, PengXiang Li, Lu Yin, Shiwei Liu",
        "summary": "Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T00:13:09.818102",
        "filter_reason": "这篇论文完全符合你的筛选要求。以下是我的详细判断过程： 1.  **第一步：核心判断** 这篇论文的本质是一项关于LLM内部工作机制的**基础性、分析性研究**。它并非将LLM作为工具应用于特定领域，而是直接探究LLM本身的能力。论文的核心贡献在于**揭示了LLM的“通用推理能力”是如何在网络的不同层级中分布和实现的**。它发现，推理能力，尤其是在生成任务中，严重依赖于模型的中深层。这种对模型内在能力的“解剖”和“理解”，是未来提出新方法来“提升”这种能力的理论基础和前提。因此，它属于“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”这一核心范畴。 2.  **第二步：正面指标** 论文高度符合多项正面指标： - **核心概念**: 论文标题和摘要明确聚焦于“Large Language Models (LLMs)”。 - **能力方向**: 标题和摘要反复提及并深入分析了“reasoning”这一核心能力，特别是将其与“retrieval”和“knowledge”进行对比，突出了其独特性。 - 论文虽然没有提出新的训练方法，但其发现对未来的模型压缩和训练（如摘要末尾提到的“distillation”）具有直接的指导意义。 3.  **第三步：排除标准** 该论文完全避开了所有排除标准： - 它不涉及多模态、视觉或任何特定应用领域（如医疗、化学、机器人）。 - 它关注的是模型内部的认知功能分布，而非应用层面的水印、安全或部署优化。 4.  **第四步：处理特殊和模糊情况** 这篇论文可以看作是**可解释性研究**的一种形式。它通过分析模型结构来解释“为什么模型能进行推理”，这属于“增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的范畴。这种基础性的理解工作，对于解决幻觉等问题、提升推理质量至关重要。 5.  **第五步：最终决策** 综合以上分析，这篇论文虽然不是一篇直接提出“新方法”来提升推理能力的论文，但它通过严谨的实验和分析，**深刻揭示了LLM通用推理能力的内在机制和关键组成部分**。这项工作是任何试图系统性地提升LLM推理能力的研究都无法绕过的基础。它回答了“推理能力在哪里”这一根本问题，为后续“如何提升”提供了明确的靶点和方向。因此，它与你“致力于提高大语言模型本身的『通用推理能力』”的核心目标高度一致，应当被保留。"
    },
    {
        "index": "#14",
        "title": "Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning",
        "link": "/arxiv/2510.01857",
        "arxiv_id": "2510.01857",
        "authors": "Claudio Fanconi, Nicolás Astorga, Mihaela van der Schaar",
        "summary": "We reframe and operationalise adversarial inverse reinforcement learning (IRL) to large language model reasoning, learning a dense, token-level reward model for process supervision directly from expert demonstrations rather than imitating style via supervised fine-tuning. The learned reasoning reward serves two complementary roles: (i) it provides step-level feedback to optimise a reasoning policy during training; and (ii) it functions at inference as a critic to rerank sampled traces under fixed compute budgets. We demonstrate that our approach prioritises correctness over surface form, yielding scores that correlate with eventual answer validity and enabling interpretable localisation of errors within a trace. Empirically, on GSM8K with Llama3 and Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a learning signal to elicit reasoning, and (ii) predictive performance is improved from reward-guided reranking (notably for Llama-based policies). By unifying training signals, inference-time selection, and token-level diagnostics into a single reasoning reward, this work suggests reusable process-level rewards with broad potential to enhance multi-step reasoning in language models.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T00:13:09.820589",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心是提出一种新的训练范式，即利用逆向强化学习（IRL）从专家演示中学习一个稠密的、token级别的推理奖励模型。这个奖励模型直接用于提升大语言模型的多步推理能力，通过提供步骤级的反馈来优化推理策略，并在推理时对结果进行重排序。这完全属于“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴。它并非将LLM应用于特定领域，而是聚焦于提升模型内在的推理过程本身。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文高度契合多个正面指标： *   **核心概念**: 论文研究对象明确为“大语言模型”。 *   **能力方向**: 论文标题和摘要反复强调“推理”，并具体到“多步推理”，在GSM8K（数学推理）数据集上进行验证。 *   **训练方法**: 论文的核心方法论是“逆向强化学习”，这是一种强化学习的高级形式，旨在学习奖励函数，直接命中“reinforcement learning (RL)”指标。 *   **新兴范式**: 论文提出的“过程监督”和“步骤级反馈”是当前提升LLM推理能力的前沿研究方向，与思维链等范式一脉相承，旨在增强模型的内在问题解决能力。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全避开了所有排除标准。它不涉及多模态、视觉，不针对任何特定应用领域（如医疗、化学），也不讨论模型部署、水印或安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文在特殊情况下也表现出相关性。摘要中提到，该方法能够实现“可解释的局部化错误”。这属于“提出一种新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量”的情况，因此应该保留。它不是对现象的讨论，而是通过技术手段直接服务于提升推理质量的目标。 **最终决策**: 综上所述，该论文的核心贡献是提出一种方法论（基于IRL的奖励模型）来系统性地提升LLM的通用推理能力，这与您筛选“致力于提高大语言模型本身通用推理能力”论文的核心目标高度一致。它不仅符合核心判断，还命中了多个关键的正面指标，并成功避开了所有排除项。因此，这篇论文是您研究课题的理想候选。"
    },
    {
        "index": "#33",
        "title": "LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning",
        "link": "/arxiv/2510.01530",
        "arxiv_id": "2510.01530",
        "authors": "Navapat Nananukul, Yue Zhang, Ryan Lee, Eric Boxer, Jonathan May, Vibhav Giridhar Gogate, Jay Pujara, Mayank Kejriwal",
        "summary": "High-assurance reasoning, particularly in critical domains such as law and medicine, requires conclusions that are accurate, verifiable, and explicitly grounded in evidence. This reasoning relies on premises codified from rules, statutes, and contracts, inherently involving defeasible or non-monotonic logic due to numerous exceptions, where the introduction of a single fact can invalidate general rules, posing significant challenges. While large language models (LLMs) excel at processing natural language, their capabilities in standard inference tasks do not translate to the rigorous reasoning required over high-assurance text guidelines. Core reasoning challenges within such texts often manifest specific logical structures involving negation, implication, and, most critically, defeasible rules and exceptions. In this paper, we propose a novel neurosymbolically-grounded architecture called LOGicalThought (LogT) that uses an advanced logical language and reasoner in conjunction with an LLM to construct a dual symbolic graph context and logic-based context. These two context representations transform the problem from inference over long-form guidelines into a compact grounded evaluation. Evaluated on four multi-domain benchmarks against four baselines, LogT improves overall performance by 11.84% across all LLMs. Performance improves significantly across all three modes of reasoning: by up to +10.2% on negation, +13.2% on implication, and +5.5% on defeasible reasoning compared to the strongest baseline.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T00:13:09.840731",
        "filter_reason": "根据您提供的筛选标准，我对这篇论文进行了严格的、分步的评估，最终判断其完全符合您的研究范围。 **第一步：核心判断——论文的本质是什么？** 这篇论文的核心是提出一种名为 LOGicalThought (LogT) 的新颖**神经符号架构**，旨在解决大语言模型在处理复杂逻辑（特别是可废止或非单调逻辑）时的根本性缺陷。论文的本质并非将LLM应用于法律或医学领域，而是将法律和医学中遇到的“高保证推理”难题作为一个典型的挑战，来驱动和验证其提出的通用方法论。其核心贡献是**增强LLM本身的基础推理能力**，通过引入符号逻辑推理器，将LLM从自然语言推理提升到更严谨的逻辑层面。这完全符合“改进LLM的基础能力、增强其逻辑推理等通用能力”的保留标准。 **第二步：正面指标——论文是否包含相关主题？** 论文高度契合多项正面指标： - **核心概念**: 论文明确以 Large language models (LLMs) 为研究对象。 - **能力方向**: 论文的主题就是“reasoning”，并深入探讨了“logical reasoning”和“defeasible reasoning”，这是通用推理能力的核心组成部分。 - **新兴范式**: 论文提出的“neurosymbolically-grounded architecture”是一种新兴的、旨在提升模型能力的技术范式，与您关注的思维链、智能体框架等在目标上是一致的，都是为了突破LLM的现有能力瓶颈。 **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文没有触及任何排除标准： - **多模态与视觉**: 论文完全专注于文本推理，不涉及视觉或多模态内容。 - **特定应用领域**: 这是最需要辨析的一点。虽然论文以法律和医学为例，但这仅仅是作为“高保证推理”这一通用问题的具体场景。论文的解决方案LogT是一个通用架构，并且在“四个多领域基准”上进行了评估，证明了其通用性。因此，它不属于“将LLM作为工具应用到特定领域”的排除范畴。 - **模型可靠性（应用层面）**: 论文关注的是提升推理的内在质量和准确性，而非水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** 本案例中的模糊点在于“法律和医学”的提及。根据筛选标准的指引，这属于“提出一种通用的...方法来增强LLM的通用问题解决能力”，而不是“将智能体/工具应用在特定领域”。论文的动机源于特定领域的挑战，但其贡献和目标是通用的，因此应该保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种新的、通用的神经符号架构（LOGicalThought），用以系统性提升大语言模型在处理复杂逻辑结构（如否定、蕴含和可废止规则）时的推理能力。这直接命中了您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，这篇论文是高度相关且应该被保留的前沿研究。"
    },
    {
        "index": "#31",
        "title": "Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models",
        "link": "/arxiv/2510.01544",
        "arxiv_id": "2510.01544",
        "authors": "Shaoan Xie, Lingjing Kong, Xiangchen Song, Xinshuai Dong, Guangyi Chen, Eric P. Xing, Kun Zhang",
        "summary": "Diffusion language models (dLLMs) offer a promising, non-autoregressive paradigm for text generation, yet training them for complex reasoning remains a key challenge. Current reinforcement learning approaches often rely on sparse, outcome-based rewards, which can reinforce flawed reasoning paths that lead to coincidentally correct answers. We argue that this stems from a fundamental mismatch with the natural structure of reasoning. We first propose a theoretical framework that formalizes complex problem solving as a hierarchical selection process, where an intractable global constraint is decomposed into a series of simpler, localized logical steps. This framework provides a principled foundation for algorithm design, including theoretical insights into the identifiability of this latent reasoning structure. Motivated by this theory, we identify unstructured refinement -- a failure mode where a model's iterative steps do not contribute meaningfully to the solution -- as a core deficiency in existing methods. We then introduce Step-Aware Policy Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising process with the latent reasoning hierarchy. By using a process-based reward function that encourages incremental progress, SAPO guides the model to learn structured, coherent reasoning paths. Our empirical results show that this principled approach significantly improves performance on challenging reasoning benchmarks and enhances the interpretability of the generation process.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T00:13:09.839689",
        "filter_reason": "这篇论文完全符合你的研究范围，是一个高度相关的筛选结果。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 论文的核心贡献是提出了一种名为“Step-Aware Policy Optimization (SAPO)”的新型强化学习算法。其根本目标是解决扩散大语言模型在复杂推理任务上的训练难题。论文明确指出，当前基于稀疏、结果导向奖励的RL方法会强化错误的推理路径，而SAPO通过引入过程导向的奖励，引导模型学习结构化、连贯的推理路径。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、多步推理等通用能力”的保留标准。它不是将LLM应用于特定领域，而是致力于提升模型本身的推理“内功”。 2.  **第二步：正面指标——论文高度匹配多个关键主题。** - **核心概念**: 论文研究对象是“Diffusion Large Language Models”，属于LLMs范畴。 - **能力方向**: 论文标题和摘要反复强调“Reasoning”、“complex reasoning”、“logical steps”，直指核心目标。 - **训练方法**: 论文的核心贡献SAPO是一种“novel RL algorithm”，直接命中“reinforcement learning (RL)”这一关键训练方法。 - **新兴范式**: 虽然不直接涉及智能体或工具使用，但它提出了一种新的训练范式来优化推理过程，这与提升通用问题解决能力的方向一致。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容纯粹聚焦于文本生成和推理过程，完全没有涉及视觉、多模态、医疗、化学、机器人等特定应用领域，也未讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况——不适用。** 论文不涉及智能体/工具使用的特定应用，也不以幻觉或安全为主要议题。它提到的“enhances the interpretability”是其方法（学习结构化推理路径）带来的一个积极副作用，进一步证明了其方法对提升推理内在质量的有效性，这属于应保留的情况。 5.  **第五步：最终决策。** 综合以上分析，这篇论文是一项典型的基础性、方法论研究。它针对当前LLM推理训练中的一个核心缺陷（稀疏奖励导致路径错误），提出了一个理论框架和一种创新的RL算法（SAPO）来系统性解决该问题。其本质是增强LLM的通用、多步、逻辑推理能力，与你的研究目标“提高大语言模型（LLM）本身的『通用推理能力』”完美契合。因此，应果断保留。"
    },
    {
        "index": "#29",
        "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning",
        "link": "/arxiv/2510.01586",
        "arxiv_id": "2510.01586",
        "authors": "Zhenyu Pan, Yiting Zhang, Zhuo Liu, Yolo Yunlong Tang, Zeliang Zhang, Haozheng Luo, Yuwei Han, Jianshu Zhang, Dennis Wu, Hong-Yu Chen, Haoran Lu, Haoyang Fang, Manling Li, Chenliang Xu, Philip S. Yu, Han Liu",
        "summary": "LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T00:13:09.838677",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出了一种名为 **AdvEvo-MARL** 的**对抗性共同进化多智能体强化学习框架**。这并非将LLM作为工具应用于特定领域，而是提出了一种**全新的训练范式**。其本质是通过对抗训练，让任务智能体（LLM agents）在与攻击者的共同进化中，**将安全能力内化**为其自身能力的一部分，而不是依赖外部防御模块。这直接属于“改进LLM的基础能力”和“提出新的训练范式”的范畴，因此应**保留**。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文命中了多个关键的正面指标： *   **核心概念**: 明确研究 \"LLM-based multi-agent systems\"。 *   **能力方向**: 摘要中提到智能体擅长 \"planning\"，并且实验结果明确指出该方法在 \"reasoning tasks\" 上保持了甚至提升了任务准确率。 *   **训练方法**: 核心方法是 \"Multi-Agent Reinforcement Learning\" 和 \"co-evolutionary\"（共同进化），这与强化学习、自我进化等主题高度相关。 *   **新兴范式**: 研究对象是 \"llm-based agents\" 和 \"multi-agent systems\"。 3.  **第三步与第四步：排除标准与特殊情况处理** 这篇论文的焦点是“安全”，这通常属于排除标准。但根据第四步的特殊情况处理规则，需要深入分析其方法本质： *   **安全**: 论文并非讨论应用层面的水印、安全策略或社会学影响。它提出了一种**基础性的训练方法**来提升模型的内在鲁棒性。通过对抗训练，模型学会了在执行任务的同时识别并抵抗恶意指令，这直接提升了模型在复杂、开放环境下的**推理质量和可靠性**。一个容易被越狱的模型，其推理能力是不可靠和有缺陷的。因此，这种提升内在安全性的方法，本质上是在提升模型的通用推理和问题解决能力。 *   **智能体**: 论文提出的是一个**通用的智能体训练框架**，旨在解决所有开放LLM智能体系统普遍面临的安全挑战，而非针对化学、医疗等特定领域。 4.  **第五步：最终决策** 综合来看，这篇论文虽然以“安全”为切入点，但其核心贡献是一种创新的训练范式（对抗性共同进化强化学习），旨在从根本上提升LLM智能体的内在能力。它通过将安全防御内化到模型本身，不仅解决了安全问题，还**直接证明了对通用推理任务（reasoning tasks）的性能有积极影响**。这完全契合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，这篇论文是高度相关且应该保留的前沿研究。"
    },
    {
        "index": "#35",
        "title": "Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates",
        "link": "/arxiv/2510.01500",
        "arxiv_id": "2510.01500",
        "authors": "Abhinav Madahar",
        "summary": "Modern deployments increasingly allocate large test-time compute (thousands of tokens or many node expansions) to boost reliability. Under such budgets, standard Tree-of-Thoughts-style search exhibits two pathologies: breadth saturation (additional samples mostly produce near-duplicates, so width stops growing) and depth myopia (noisy short-horizon utilities prune branches whose payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts (LToT), a drop-in controller that separates utility from logical consistency and treats low-utility but consistent candidates as assets rather than waste. The frontier is split into mainlines (high-utility candidates used for exploitation) and laterals (consistent, initially low-utility candidates that receive short, cheap probes before judgment). LToT explores laterals via Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race that spreads tiny probes across a very wide lateral set, uses width-aware thresholds with repeat-to-confirm, and immediately promotes a branch once its envelope clears the mainline bar; mainlines are kept intentionally narrow so surplus compute is invested where width is cheap. We prove a pseudolinear lateral cost $\\Theta(N_0 \\log_{\\eta} N_0)$ with logarithmically many rungs (initial lateral width $N_0$; culling factor $\\eta>1$), in contrast to the exponential growth of uncapped mainlines. Empirical evaluations on benchmark tasks are in preparation and will be added in a future revision. In short, LToT turns large test-time budgets into principled diversity while preserving promotion discipline, mitigating saturation and myopia without inflating compute.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-01",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T00:13:09.841700",
        "filter_reason": "该论文完全符合筛选标准，应被保留。 **第一步：核心判断** 这篇论文的本质是提出一种新的推理框架\"Lateral Tree-of-Thoughts\" (LToT)，旨在对现有的\"Tree-of-Thoughts\" (ToT) 进行改进。ToT本身就是一种公认的、用于增强大语言模型多步推理和问题解决能力的核心方法论。该论文通过解决ToT在复杂推理任务中遇到的\"广度饱和\"和\"深度短视\"两大瓶颈，直接致力于提升LLM的通用推理能力。它的核心贡献不是将LLM应用于某个具体领域，而是优化LLM进行思考和规划的内在过程，这完全符合\"改进LLM本身的基础能力\"的要求。 **第二步：正面指标** 论文主题与筛选标准中的多个正面指标高度契合： - **能力方向**: 论文明确聚焦于reasoning（推理）、planning（规划）和problem-solving（问题解决），这正是核心研究目标的中心。 - **新兴范式**: ToT（思维树）是当前大语言模型推理研究中的一个关键范式，与llm-based agents和deep research等方法紧密相关。对ToT的改进，就是在推动"
    },
    {
        "index": "#39",
        "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining",
        "link": "/arxiv/2510.01427",
        "arxiv_id": "2510.01427",
        "authors": "Sipeng Zhang, Longfei Yun, Zilong Wang, Jingbo Shang, Letian Peng",
        "summary": "At the core of Deep Research is knowledge mining, the task of extracting structured information from massive unstructured text in response to user instructions. Large language models (LLMs) excel at interpreting such instructions but are prohibitively expensive to deploy at scale, while traditional pipelines of classifiers and extractors remain efficient yet brittle and unable to generalize to new tasks. We introduce Falconer, a collaborative framework that combines the agentic reasoning of LLMs with lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act as planners, decomposing user instructions into executable pipelines, and as annotators, generating supervision to train small proxies. The framework unifies classification and extraction into two atomic operations, get label and get span, enabling a single instruction-following model to replace multiple task-specific components. To evaluate the consistency between proxy models incubated by Falconer and annotations provided by humans and large models, we construct new benchmarks covering both planning and end-to-end execution. Experiments show that Falconer closely matches state-of-the-art LLMs in instruction-following accuracy while reducing inference cost by up to 90% and accelerating large-scale knowledge mining by more than 20x, offering an efficient and scalable foundation for Deep Research.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-01",
        "category": "cs.AI",
        "crawl_time": "2025-10-07T00:13:09.848982",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。 **第一步：核心判断** 这篇论文的本质是提出一种名为“Falconer”的协作框架，其核心贡献并非将LLM作为工具解决特定领域问题，而是提出了一种新的方法论来增强和扩展LLM的推理与问题解决能力。在该框架中，LLM的核心角色是“规划器”和“标注器”，这直接利用并构建了LLM的**规划能力**——一种关键的通用推理能力。论文通过让LLM分解复杂指令并生成训练数据来“孵化”小型代理模型，这是一种新颖的、旨在提升LLM驱动系统整体效率和可扩展性的训练范式。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划、多步推理等通用能力”的保留标准。 **第二步：正面指标** 论文高度匹配多个正面指标： - **核心概念**: 明确以“Large language models (LLMs)”为核心。 - **能力方向**: 直接涉及“planning”和“problem-solving”，并提到LLM的“agentic reasoning”。 - **新兴范式**: 论文的核心就是一个“llm-based agents”框架，涉及“tool use”（LLM使用小型代理作为执行工具），并且其目标是为“Deep Research”提供基础，这是一个前沿的通用问题解决范式。 **第三步：排除标准** 论文完全不涉及任何排除标准领域： - 它不涉及多模态、视觉。 - 它的应用场景“知识挖掘”是一个通用的NLP任务，而非医疗、化学、生物等特定领域。 - 它不讨论水印、安全等模型可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的典型范例。虽然论文以“知识挖掘”为评估任务，但其提出的“LLM规划器 + 小型代理执行器”的框架具有通用性，旨在为“Deep Research”等更广泛的通用问题解决场景提供一个高效、可扩展的基础。它不是“用于化学实验自动化的智能体”，而是探索如何让LLM的推理能力本身变得更高效、更具扩展性。 **最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一种创新的智能体框架，通过利用LLM的规划推理能力来指导和训练更小的代理模型，从而在保持高精度的同时极大地提升了推理系统的效率和可扩展性。这直接服务于“提高大语言模型本身的通用推理能力”这一核心目标，因为它探索了一种新的、让LLM的推理能力能够大规模、低成本落地的方法论。因此，该论文应被保留。"
    }
]