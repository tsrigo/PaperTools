[
    {
        "index": "#1",
        "title": "Language Models that Think, Chat Better",
        "link": "/arxiv/2509.20357",
        "arxiv_id": "2509.20357",
        "authors": "Adithya Bhaskar, Xi Ye, Danqi Chen",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) improves language model reasoning by using rule-based rewards in verifiable domains such as mathematics and code. However, RLVR leads to limited generalization for open-ended tasks -- such as writing outline essays or making meal plans -- where humans reason routinely. This paper shows that the RLVR paradigm is effective beyond verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking (**RLMT**) for general-purpose chat capabilities. Using diverse real-world prompts, RLMT requires LMs to generate long CoT reasoning before response, and optimizes them with online RL against a preference-based reward model used in RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT consistently outperforms standard RLHF pipelines. This includes substantial gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and ArenaHardV2), along with 1-3 point improvements on other tasks like creative writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be applied directly to base models without an SFT stage, akin to R1-Zero training. Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex multi-staged pipeline with 25M+ examples. We close with qualitative and quantitative analyses of how trained models plan their responses. Our results rethink the post-training pipeline and call upon future work to understand and employ thinking more broadly.",
        "subjects": "Computation and Language",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.406700",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。论文的核心贡献是提出了一种名为\"RL with Model-rewarded Thinking (RLMT)\"的新训练范式，旨在增强大语言模型的通用推理能力。该方法要求模型在回答前生成长链思维(CoT)推理，并使用基于偏好的奖励模型进行在线强化学习优化，从而提升模型的规划和问题解决能力。 从筛选标准来看： 1. 第一步核心判断：论文本质是改进LLM的基础能力，提出新的训练范式(RLMT)，增强其推理能力，完全符合保留标准。 2. 第二步正面指标：论文包含多个正面指标，如大语言模型(Llama-3.1-8B和Qwen-2.5-7B)、推理能力(reasoning)、强化学习(RL)等。 3. 第三步排除标准：论文不涉及多模态、特定应用领域或模型可靠性等排除领域。 4. 第四步特殊情况：论文专注于通过思维链推理和强化学习来提升模型本身的推理能力，而非将LLM作为工具应用到特定领域。 论文在多个聊天基准测试和任务上取得了显著改进，包括创意写作和一般知识，这表明其方法有效提升了模型的通用推理能力，而非仅限于特定领域。因此，这篇论文完全符合研究目标。"
    },
    {
        "index": "#13",
        "title": "Thinking Augmented Pre-training",
        "link": "/arxiv/2509.20186",
        "arxiv_id": "2509.20186",
        "authors": "Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei",
        "summary": "This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\\%$ on several challenging reasoning benchmarks.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.410478",
        "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式\"Thinking augmented Pre-Training (TPT)\"，通过在预训练阶段增加思维轨迹来增强模型的推理能力。这直接关注提升LLM的通用推理能力，而非将其作为工具应用于特定领域。 其次，论文包含了多个正面指标：核心概念明确关注大型语言模型(LLMs)，能力方向聚焦于推理能力(reasoning)，特别是通过\"step-by-step reasoning and decomposition\"来提升模型性能。实验结果也显示该方法在多个具有挑战性的推理基准上提高了模型性能。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 论文的核心贡献是提出了一种通用的预训练方法，通过自动生成的思维轨迹增强文本数据，使高质量token更易学习，从而提高LLM的数据效率和推理能力。这种方法不是针对特定领域，而是旨在从根本上提升LLM的通用推理能力，完全符合我筛选\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"论文的目标。"
    },
    {
        "index": "#18",
        "title": "Causal Understanding by LLMs: The Role of Uncertainty",
        "link": "/arxiv/2509.20088",
        "arxiv_id": "2509.20088",
        "authors": "Oscar Lithgow-Serrano, Vani Kanjirangat, Alessandro Antonucci",
        "summary": "Recent papers show LLMs achieve near-random accuracy in causal relation classification, raising questions about whether such failures arise from limited pretraining exposure or deeper representational gaps. We investigate this under uncertainty-based evaluation, testing whether pretraining exposure to causal examples improves causal understanding >18K PubMed sentences -- half from The Pile corpus, half post-2024 -- across seven models (Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model behavior through: (i) causal classification, where the model identifies causal relationships in text, and (ii) verbatim memorization probing, where we assess whether the model prefers previously seen causal statements over their paraphrases. Models perform four-way classification (direct/conditional/correlational/no-relationship) and select between originals and their generated paraphrases. Results show almost identical accuracy on seen/unseen sentences (p > 0.05), no memorization bias (24.8% original selection), and output distribution over the possible options is almost flat, with entropic values near the maximum (1.35/1.39), confirming random guessing. Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence, 32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11% vs. direct). These findings suggest that failures in causal understanding arise from the lack of structured causal representation, rather than insufficient exposure to causal examples during pretraining.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.411669",
        "filter_reason": "这篇论文的核心是研究LLMs在因果关系理解方面的能力，属于LLM的基础推理能力研究，特别是逻辑推理能力的重要组成部分。论文通过多种模型测试，分析了LLMs在因果分类和记忆探测方面的表现，发现LLMs在因果理解上的失败源于缺乏结构化的因果表示，而非预训练中因果例子暴露不足。虽然论文使用了PubMed句子作为测试数据，但这只是为了评估LLM的通用因果理解能力，而不是将LLM应用于医疗领域。论文关注的是LLMs本身的推理能力缺陷，属于对LLM基础能力的探索，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。因此，这篇论文应该被保留。"
    },
    {
        "index": "#6",
        "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
        "link": "/arxiv/2509.20317",
        "arxiv_id": "2509.20317",
        "authors": "Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin",
        "summary": "Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited the application of implicit CoT. We identify a core latent instability issue by scaling the computational budget of implicit CoT approaches: as we increase the number of implicit reasoning tokens to enhance performance, the training process often becomes unstable and collapses. Our analysis reveals that this instability arises from the latent representations becoming homogeneous and losing their semantic diversity, a failure caused by insufficient step-level supervision in existing implicit CoT approaches. To address this issue, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring that latent states capture distinct and meaningful information. The proposed auxiliary decoder is removed during inference, preserving the computational efficiency of implicit CoT methods with no added overhead. In addition, the auxiliary decoder affords interpretability of implicit reasoning by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization of semantic roles and diagnosis. SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain stability of various implicit CoT methods, boosting baselines like Coconut by +8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1% with 2.3\\times greater token efficiency, while substantially closing the performance gap on larger models like LLaMA-3.1 8B.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.408890",
        "filter_reason": "根据筛选标准，这篇论文完全符合研究目标。首先，从核心判断来看，论文的本质是关于改进大语言模型的推理能力，特别是针对Implicit Chain-of-Thought (CoT)方法提出了一种新的训练范式SIM-CoT，这直接属于改进LLM基础能力和通用推理能力的范畴，符合保留标准。 其次，论文包含多项正面指标：核心概念明确涉及Large Language Models (LLMs)；能力方向聚焦于reasoning，特别是Chain-of-Thought推理；训练方法方面提出了创新的step-level supervision机制来增强模型训练过程。 第三，论文不涉及任何排除标准领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文提出的辅助解码器增强了模型推理的可解释性，通过\"projecting each latent token onto an explicit reasoning vocabulary\"来提升模型的推理质量，这符合保留条件。 论文的核心贡献是解决了implicit CoT方法中的潜在不稳定性问题，通过step-level supervision稳定和丰富潜在推理空间，从而提升LLM的推理能力，这与研究目标\"致力于提高大语言模型的通用推理能力\"高度一致。"
    },
    {
        "index": "#20",
        "title": "Can Constructions \"SCAN\" Compositionality ?",
        "link": "/arxiv/2509.20074",
        "arxiv_id": "2509.20074",
        "authors": "Ganesh Katrapati, Manish Shrivastava",
        "summary": "Sequence to Sequence models struggle at compositionality and systematic generalisation even while they excel at many other tasks. We attribute this limitation to their failure to internalise constructions conventionalised form meaning pairings that license productive recombination. Building on these insights, we introduce an unsupervised procedure for mining pseudo-constructions: variable-slot templates automatically extracted from training data. When applied to the SCAN dataset, our method yields large gains out-of-distribution splits: accuracy rises to 47.8 %on ADD JUMP and to 20.3% on AROUND RIGHT without any architectural changes or additional supervision. The model also attains competitive performance with? 40% of the original training data, demonstrating strong data efAciency. Our findings highlight the promise of construction-aware preprocessing as an alternative to heavy architectural or training-regime interventions.",
        "subjects": "Computation and Language",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.412471",
        "filter_reason": "这篇论文的核心是关于改进序列到序列模型(Seq2Seq)的组合性和系统泛化能力，这些能力是大语言模型通用推理能力的重要组成部分。论文提出了一种无监督方法来挖掘\"伪构造\"(pseudo-constructions)，即从训练数据中自动提取的可变槽模板，这种方法在SCAN数据集上显著提高了模型在分布外分割上的准确性。这属于改进模型基础能力的方法论研究，而非将LLM应用于特定领域。组合性是逻辑推理和语言理解的基础，系统泛化则涉及到模型如何处理新的、未见过的组合，这些都是通用推理能力的关键方面。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#29",
        "title": "Future Policy Aware Preference Learning for Mathematical Reasoning",
        "link": "/arxiv/2509.19893",
        "arxiv_id": "2509.19893",
        "authors": "Minjae Oh, Yunho Choi, Dongmin Choi, Yohan Jo",
        "summary": "Preference learning methods such as Direct Preference Optimization (DPO) have become standard for Large Language Model (LLM) post-training, yet they are often ineffective for mathematical reasoning. A key challenge is the large token overlap between preferred and dispreferred trajectories; lowering the probability of dispreferred trajectories also reduces the probability of shared useful tokens, leading to over-penalization and overall performance collapse. As a mitigation, existing algorithms include the probability of a trajectory under the current policy as a regularization term, which decreases the effect of the gradient when the probability is low. However, by the time this effect takes hold, useful tokens may have already been over-penalized as the model has begun to degrade. To address this, we propose Future Policy Aware (FPA) preference learning, which replaces the current policy with a future policy in the regularization term. This future policy is estimated via lightweight, logit-space extrapolation from a reference model toward the current model. FPA enables safer training by preemptively regularizing potentially problematic gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH and GSM8K benchmarks. FPA yields consistent performance gains, with the largest improvements observed with SimPER, achieving gains of up to 5.75%. We demonstrate that FPA provides proactive regularization while preserving the probability of shared, useful mathematical tokens, and enables longer, degradation-free training with negligible computational overhead. We will release our code publicly upon publication.",
        "subjects": "Computation and Language",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.414766",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是改进LLM的数学推理能力，属于\"增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。论文提出了FPA（Future Policy Aware）方法，用于解决偏好学习在数学推理中的问题，这是直接提升LLM基础能力的研究，而非将LLM作为工具应用到特定领域。 其次，论文满足多个正面指标：核心概念上明确研究Large Language Models (LLMs)；能力方向上专注于mathematical reasoning（数学推理）；训练方法上涉及Direct Preference Optimization (DPO)等偏好学习方法，这些通常与强化学习相关。 第三，论文不符合任何排除标准：它不涉及多模态与视觉内容；虽然聚焦于数学推理，但数学推理被视为评估和提升LLM通用能力的重要方面，而非特定应用领域；也没有主要关注模型可靠性方面的应用问题。 论文的核心贡献是提出了一种新的偏好学习方法FPA，通过在正则化项中使用未来策略而非当前策略，解决了数学推理中偏好学习的过度惩罚问题，从而提升了LLM的数学推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#48",
        "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models",
        "link": "/arxiv/2509.19593",
        "arxiv_id": "2509.19593",
        "authors": "Dylan Hutson, Daniel Vennemeyer, Aneesh Deshmukh, Justin Zhan, Tianyu Jiang",
        "summary": "We introduce GuessingGame, a protocol for evaluating large language models (LLMs) as strategic question-askers in open-ended, open-domain settings. A Guesser LLM identifies a hidden object by posing free-form questions to an Oracle without predefined choices or candidate lists. To measure question quality, we propose two information gain (IG) metrics: a Bayesian method that tracks belief updates over semantic concepts using LLM-scored relevance, and an entropy-based method that filters candidates via ConceptNet. Both metrics are model-agnostic and support post hoc analysis. Across 858 games with multiple models and prompting strategies, higher IG strongly predicts efficiency: a one-standard-deviation IG increase reduces expected game length by 43\\%. Prompting constraints guided by IG, such as enforcing question diversity, enable weaker models to significantly improve performance. These results show that question-asking in LLMs is both measurable and improvable, and crucial for interactive reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.418907",
        "filter_reason": "这篇论文符合我的研究目标，因为它核心关注的是大语言模型的通用推理能力。论文提出了GuessingGame协议，用于评估LLMs作为战略提问者的能力，本质上是在研究模型如何通过提问和获取信息来进行有效推理。这种开放式提问能力是一种通用推理能力，类似于思维链(CoT)等多步推理能力，而非将LLM应用于特定领域。论文提出的两种信息增益指标旨在衡量和提升LLMs的推理效率，结果显示信息增益与推理效率显著相关，这种研究直接针对提升LLM的基础推理能力。论文不涉及任何排除标准中的领域（如多模态、特定应用或模型基础设施），而是聚焦于提高LLM本身的通用推理能力，因此完全符合我的研究范围。"
    },
    {
        "index": "#50",
        "title": "ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities",
        "link": "/arxiv/2509.19569",
        "arxiv_id": "2509.19569",
        "authors": "Aleksis Datseris, Sylvia Vassileva, Ivan Koychev, Svetla Boytcheva",
        "summary": "This paper introduces a novel approach to position embeddings in transformer models, named \"Exact Positional Embeddings\" (ExPE). An absolute positional embedding method that can extrapolate to sequences of lengths longer than the ones it was trained on. Traditional transformer models rely on absolute or relative position embeddings to incorporate positional information into token embeddings, which often struggle with extrapolation to sequences longer than those seen during training. Our proposed method utilizes a novel embedding strategy that encodes exact positional information by overriding specific dimensions of the embedding vectors, thereby enabling a more precise representation of token positions. The proposed approach not only maintains the integrity of the original embeddings but also enhances the model's ability to generalize to more extended sequences. In causal language modeling, our ExPE embeddings significantly reduce perplexity compared to rotary and sinusoidal embeddings, when tested on sequences longer than those used in training.",
        "subjects": "Computation and Language",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.419452",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步核心判断：这篇论文的本质是改进Transformer模型的位置编码方法，提出了\"精确位置嵌入\"(ExPE)，使模型能够处理比训练时更长的序列。这属于改进LLM基础架构的研究，旨在增强模型处理长序列的基础能力，而非将LLM作为工具应用到特定领域。因此，论文符合保留标准。 第二步正面指标：论文涉及\"Generative Transformer Models\"，属于LLM范畴。虽然未直接讨论推理、规划等能力，但处理长序列的能力是支持复杂推理任务的基础。例如，数学推理、逻辑推理和多步规划通常需要处理长序列的能力，因此这项工作间接支持了通用推理能力的提升。 第三步排除标准：论文不涉及任何排除标准中的领域，包括多模态与视觉、特定应用领域以及模型可靠性的应用层面问题。它关注的是基础模型架构的改进。 第四步特殊和模糊情况：论文情况清晰，不涉及特殊或模糊情况。它明确关注的是Transformer模型的位置编码方法，属于基础模型架构的改进。 最终决策：虽然论文没有直接讨论推理、规划或问题解决能力，但改进模型处理长序列的能力是支持复杂推理任务的基础。因此，这篇致力于改进LLM基础架构能力的研究符合\"提高大语言模型（LLM）本身的『通用推理能力』\"的核心研究目标。"
    },
    {
        "index": "#56",
        "title": "How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models",
        "link": "/arxiv/2509.19371",
        "arxiv_id": "2509.19371",
        "authors": "Kangtao Lv, Haibin Chen, Yujin Yuan, Langming Liu, Shilei Liu, Yongwei Wang, Wenbo Su, Bo Zheng",
        "summary": "Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-19",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.420726",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，主要基于以下分析： 第一步核心判断：这篇论文的本质是关于改进LLM基础能力的研究，具体聚焦于预训练阶段的知识注入方法。论文提出了\"知识注入缩放定律\"，这是一种新的训练范式，旨在解决LLM在知识获取与保留方面的核心挑战。虽然论文提到了\"领域知识\"，但其核心贡献是通用的方法论，用于平衡知识注入与避免灾难性遗忘，这直接关系到提升LLM的基础能力，符合\"改进LLM的基础能力、提出新的训练范式\"的保留标准。 第二步正面指标：论文明确包含\"Large language models, LLMs\"这一核心概念。虽然论文没有直接讨论reasoning、planning等具体能力方向，但知识获取和保留是推理能力的基础，论文研究的是如何更有效地让模型获取并保留知识，这间接支持了通用推理能力的提升。 第三步排除标准：论文不涉及多模态与视觉研究。虽然提到\"domain-specific data\"和\"specialized knowledge\"，但论文的核心是提出一种通用的知识注入缩放定律，而非专注于某个特定应用领域（如医疗、化学等）。论文提到\"hallucination\"问题，但是从知识注入角度研究如何减少幻觉，而非仅作为应用层面的防御。 第四步特殊和模糊情况处理：论文虽然涉及\"领域知识\"，但其核心贡献是通用的方法论，可以应用于各种领域知识的注入，而不是针对特定领域的应用研究。因此，它更符合\"改进LLM基础能力\"而非\"特定应用领域\"的特征。 综上所述，这篇论文的核心贡献是提出了一种通用的知识注入缩放定律，用于优化LLM预训练过程中的知识获取和保留，这属于提升LLM基础能力的研究范畴，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#66",
        "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution",
        "link": "/arxiv/2509.19349",
        "arxiv_id": "2509.19349",
        "authors": "Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin",
        "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large language models (LLMs) to advance scientific discovery with state-of-the-art performance and unprecedented efficiency. Recent advances in scaling inference time compute of LLMs have enabled significant progress in generalized scientific discovery. These approaches rely on evolutionary agentic harnesses that leverage LLMs as mutation operators to generate candidate solutions. However, current code evolution methods suffer from critical limitations: they are sample inefficient, requiring thousands of samples to identify effective solutions, and remain closed-source, hindering broad adoption and extension. ShinkaEvolve addresses these limitations, introducing three key innovations: a parent sampling technique balancing exploration and exploitation, code novelty rejection-sampling for efficient search space exploration, and a bandit-based LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks, demonstrating consistent improvements in sample efficiency and solution quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution using only 150 samples, designs high-performing agentic harnesses for AIME mathematical reasoning tasks, identifies improvements to ALE-Bench competitive programming solutions, and discovers novel mixture-of-expert load balancing loss functions that illuminate the space of optimization strategies. Our results demonstrate that ShinkaEvolve achieves broad applicability with exceptional sample efficiency. By providing open-source accessibility and cost-efficiency, this work democratizes open-ended discovery across diverse computational problems.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-17",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.422777",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文本质上是关于改进LLM的基础能力和提出新的训练范式。ShinkaEvolve框架利用LLMs作为变异操作符，通过进化机制增强模型生成解决方案的能力，这直接关注提升LLM的通用推理和问题解决能力，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确提到\"large language models (LLMs)\" - 能力方向：涉及\"mathematical reasoning\"和\"competitive programming solutions\"，属于通用推理能力范畴 - 训练方法：提出\"evolutionary agentic harnesses\"和\"bandit-based LLM ensemble selection strategy\"，属于进化学习方法 - 新兴范式：包含\"agentic harnesses\"，与LLM-based agents相关 第三步排除标准：论文不符合任何排除标准。虽然提到了圆打包、数学推理等应用场景，但这些是作为评估框架通用性的示例，而非论文的主要焦点。论文核心是提出通用的程序进化框架，而非针对特定领域应用。 第四步特殊情况处理：论文提出的\"evolutionary agentic harnesses\"是一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非针对特定领域的应用，因此符合保留条件。 综合分析，ShinkaEvolve的核心贡献是提出了一种新的进化框架，通过创新的采样和集成选择策略，提高LLM在程序进化方面的样本效率和解决方案质量，这直接服务于提升大语言模型的通用推理能力，符合研究目标。"
    },
    {
        "index": "#73",
        "title": "Pluralistic Off-policy Evaluation and Alignment",
        "link": "/arxiv/2509.19333",
        "arxiv_id": "2509.19333",
        "authors": "Chengkai Huang, Junda Wu, Zhouhang Xie, Yu Xia, Rui Wang, Tong Yu, Subrata Mitra, Julian McAuley, Lina Yao",
        "summary": "Personalized preference alignment for LLMs with diverse human preferences requires evaluation and alignment methods that capture pluralism. Most existing preference alignment datasets are logged under policies that differ substantially from the evaluated LLMs, and existing off-policy estimators focus solely on overall utility while ignoring preference pluralism. Extending Off-Policy Evaluation (OPE) to pluralistic preference alignment, therefore, remains an open question. Thus, we propose the Pluralistic Off-Policy Evaluation (POPE), the first framework for offline pluralistic preference evaluation and alignment in LLMs. POPE includes a unified reward function that combines (1) a collaborative utility component derived from human preference signals (e.g., upvotes or relevance scores) and (2) a diversity component inspired by entropy-based coverage measures, together reflecting pluralistic alignment. Furthermore, to estimate this reward from logged interactions, we derive decomposable inverse propensity scoring (IPS) estimators that separately evaluate relevance and diversity. Theoretically, we prove that our decomposed IPS estimators establish a lower bound on their variance. With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance pluralistic alignment. Empirical results demonstrate that POPE efficiently enhances pluralistic response generation and maintains the models' general capabilities on downstream tasks",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-15",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.424248",
        "filter_reason": "这篇论文的核心贡献是提出了POPE（Pluralistic Off-Policy Evaluation）框架，用于解决LLM在多元人类偏好下的评估和对齐问题。从第一步判断来看，论文本质上是关于改进LLM的基础能力（偏好对齐），提出新的评估和优化框架，这符合保留标准。论文明确针对LLM的偏好对齐问题，并涉及到强化学习中的离线策略评估和优化概念，这与第二步中的正面指标部分吻合。论文不符合第三步中的排除标准，它不主要聚焦于多模态、特定应用领域或模型可靠性的应用层面问题。虽然论文没有直接讨论推理、规划或问题解决能力，但它关注的是偏好对齐，这是LLM的一个重要基础能力，良好的偏好对齐是模型展现高质量推理能力的前提。论文提出的框架通过结合人类偏好信号和多样性组件来改进模型的基础能力，这种改进可能间接提升模型在推理和其他任务上的表现。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#84",
        "title": "Failure Modes of Maximum Entropy RLHF",
        "link": "/arxiv/2509.20265",
        "arxiv_id": "2509.20265",
        "authors": "Ömer Veysel Çağatan, Barış Akgün",
        "summary": "In this paper, we show that Simple Preference Optimization (SimPO) can be derived as Maximum Entropy Reinforcement Learning with length-normalized temperature, providing a theoretical foundation for this reference-free method. Motivated by SimPO's strong performance in offline preference optimization, we investigate whether Maximum Entropy RL can achieve similar results in online RLHF settings. Our experiments find that Maximum Entropy RL consistently exhibits overoptimization and unstable KL dynamics, even at very low learning rates. Unlike KL-constrained methods that maintain stable training, entropy regularization fails to prevent reward hacking and appears to correlate with overoptimization. Lastly, we discuss possible explanations for why SimPO succeeds in offline settings while Maximum Entropy RL struggles in online scenarios. Our findings suggest that reference-free approaches may face distinct challenges when applied to online or offline preference learning.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.426438",
        "filter_reason": "这篇论文的核心是研究RLHF（Reinforcement Learning from Human Feedback）的优化问题，特别是分析了最大熵强化学习在在线RLHF设置中的失败模式，并探讨了SimPO在离线设置中成功的原因。RLHF是提升大语言模型通用能力的关键训练技术，论文研究的是如何改进这一训练方法，属于\"改进LLM的基础能力、提出新的训练范式\"的范畴。论文直接关注强化学习（RLHF）这一训练方法，符合正面指标。同时，论文不涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性的应用层面）。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。"
    },
    {
        "index": "#90",
        "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning",
        "link": "/arxiv/2509.19894",
        "arxiv_id": "2509.19894",
        "authors": "Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong",
        "summary": "Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at https://github.com/inclusionAI/PromptCoT.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.427978",
        "filter_reason": "这篇论文完全符合我的研究范围。首先，从核心判断来看，论文的本质是提升大语言模型的通用推理能力，特别是数学和编程推理能力。论文提出了PromptCoT 2.0框架，通过改进提示合成方法来增强LLM的推理能力，这属于\"改进LLM的基础能力和提出新的训练范式\"的范畴。 其次，论文包含多个正面指标：明确关注\"Large language models (LLMs)\"；核心能力方向是\"reasoning\"，特别是\"math reasoning\"和\"logical reasoning\"；提出了新的训练方法，包括\"Self-Play\"和\"Supervised Fine-Tuning (SFT)\"。 第三，论文不涉及任何排除标准中的领域：没有关注多模态与视觉问题；虽然涉及数学和编程，但这些是通用推理的基础领域而非特定应用领域；也没有主要关注模型可靠性的应用层面问题。 论文的核心贡献是提出了一种可扩展的提示合成框架，通过迭代改进推理过程来生成更难、更多样化的问题，从而提升LLM的推理能力。这种方法从根本上增强了模型的基础推理能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#91",
        "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models",
        "link": "/arxiv/2509.19803",
        "arxiv_id": "2509.19803",
        "authors": "Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang",
        "summary": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.428228",
        "filter_reason": "这篇论文的核心是提出VCRL，一种基于课程学习的强化学习框架，用于提高大语言模型的推理能力。论文本质上是关于改进LLM的基础能力，特别是数学推理能力，这符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。论文包含多个正面指标，如关注LLMs核心概念、数学推理能力方向以及强化学习训练方法。同时，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文主要在数学推理任务上进行实验，但提出的方法是通用的课程学习强化学习框架，通过动态控制训练样本的难度来提高LLM对不同难度样本的学习能力，这与人类从易到难的认知过程一致，可以推广到其他需要推理能力的任务。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#86",
        "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI",
        "link": "/arxiv/2509.20175",
        "arxiv_id": "2509.20175",
        "authors": "Lorenzo Giusti, Ole Anton Werner, Riccardo Taiello, Matilde Carvalho Costa, Emre Tosun, Andrea Protani, Marc Molina, Rodrigo Lopes de Almeida, Paolo Cacace, Diogo Reis Santos, Luigi Serio",
        "summary": "We present Federation of Agents (FoA), a distributed orchestration framework that transforms static multi-agent coordination into dynamic, capability-driven collaboration. FoA introduces Versioned Capability Vectors (VCVs): machine-readable profiles that make agent capabilities searchable through semantic embeddings, enabling agents to advertise their capabilities, cost, and limitations. Our aarchitecturecombines three key innovations: (1) semantic routing that matches tasks to agents over sharded HNSW indices while enforcing operational constraints through cost-biased optimization, (2) dynamic task decomposition where compatible agents collaboratively break down complex tasks into DAGs of subtasks through consensus-based merging, and (3) smart clustering that groups agents working on similar subtasks into collaborative channels for k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe semantics for scalable message passing, FoA achieves sub-linear complexity through hierarchical capability matching and efficient index maintenance. Evaluation on HealthBench shows 13x improvements over single-model baselines, with clustering-enhanced laboration particularly effective for complex reasoning tasks requiring multiple perspectives. The system scales horizontally while maintaining consistent performance, demonstrating that semantic orchestration with structured collaboration can unlock the collective intelligence of heterogeneous federations of AI agents.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.427158",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Federation of Agents (FoA)\"的分布式编排框架，用于实现大规模智能体AI的动态协作。从本质上看，论文属于\"智能体协作框架\"的研究范畴，符合筛选标准中的保留条件。论文提出的版本化能力向量(VCVs)、语义路由、动态任务分解和智能聚类等创新方法，都是为了提升智能体系统的通用协作和推理能力，而非将LLM作为工具应用到特定领域。 论文在正面指标上表现良好，涉及了\"multi-agent systems\"这一新兴范式，并明确提到该系统在\"complex reasoning tasks\"上表现出色，这与\"通用推理能力\"的研究目标直接相关。虽然论文没有直接提及\"Large language models\"，但智能体系统通常基于LLM构建，且论文关注的是通用能力的提升。 在排除标准方面，论文没有主要关注多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然评估中使用了HealthBench数据集，但这仅用于验证系统性能，论文本身并非针对医疗等特定领域的研究。 综合分析，这篇论文提出的是一种通用的智能体协作框架，旨在通过语义感知的通信机制增强智能体系统的协作和推理能力，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。"
    },
    {
        "index": "#92",
        "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
        "link": "/arxiv/2509.19736",
        "arxiv_id": "2509.19736",
        "authors": "Cheng Qian, Zuxin Liu, Akshara Prabhakar, Jielin Qiu, Zhiwei Liu, Haolin Chen, Shirley Kokane, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang",
        "summary": "Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.428533",
        "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是提出UserRL框架，通过强化学习训练用户中心的智能体，这属于\"智能体协作框架\"的范畴，是改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用于特定领域。其次，论文包含多个正面指标：明确使用了大语言模型(Qwen3)，采用了强化学习(RL)方法训练模型，研究了基于LLM的智能体(agentic models)，并关注动态多轮交互能力，这些都符合研究目标。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。最后，虽然论文涉及智能体研究，但它提出的是通用的智能体训练框架，而非针对特定领域的应用，因此应予以保留。论文的核心贡献在于探索如何通过奖励塑造和用户模拟选择来提升智能体的通用交互能力，这与提高大语言模型通用推理能力的研究目标高度一致。"
    },
    {
        "index": "#97",
        "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
        "link": "/arxiv/2509.19517",
        "arxiv_id": "2509.19517",
        "authors": "Sai Teja Reddy Adapala",
        "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.CL",
        "crawl_time": "2025-09-25T09:53:05.429575",
        "filter_reason": "这篇论文完全符合研究目标，核心贡献是研究大语言模型在认知负荷下的多跳推理能力限制。根据筛选标准，我的判断过程如下： 第一步：核心判断——论文本质是研究LLM本身的推理能力限制。论文提出了计算认知负荷的形式理论，设计了新的评估基准(ICE)来测试LLM在多跳推理任务上的表现，特别是在认知负荷条件下的性能变化。这不是将LLM应用于特定领域，而是直接研究LLM的基础推理能力，属于改进LLM通用能力的研究。 第二步：正面指标——论文包含关键正面指标：(1)核心概念：明确研究Large Language Models (LLMs)；(2)能力方向：聚焦于multi-hop reasoning（多跳推理），属于逻辑推理范畴。虽然未涉及训练方法和新兴范式，但这两个核心正面指标已足够表明论文与研究方向高度相关。 第三步：排除标准——论文不涉及任何需要排除的领域。它没有研究多模态与视觉问题，没有聚焦于特定应用领域（如医疗、化学等），也没有从应用层面研究模型可靠性。 第四步：特殊和模糊情况——论文提到\"hallucination-as-guessing under uncertainty\"，这是从认知机制角度解释幻觉现象，探讨其与推理能力的关系，而非仅进行社会学研究或应用层面讨论，这有助于理解LLM推理能力的本质限制。 综合来看，这篇论文通过研究认知负荷对LLM推理能力的影响，提出了新的评估方法和理论框架，直接服务于提升LLM通用推理能力的研究目标，完全符合筛选要求。"
    },
    {
        "index": "#5",
        "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs",
        "link": "/arxiv/2509.20105",
        "arxiv_id": "2509.20105",
        "authors": "Venkat Margapuri, Garik Kazanjian, Naren Kosaraju",
        "summary": "Large Language Models (LLMs) often struggle with maintaining coherent multi-step reasoning traces, particularly in tasks that require a structured logical flow. This work introduces a quantum-inspired approach to address the challenge by incorporating a fidelity-based reward derived from Projected Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior approaches that use direct supervision or contrastive objectives, the proposed method guides learning through structural consistency, offering a novel approach to enforce global coherence in generated reasoning traces. The proposed framework is evaluated using multiple coherence-determining metrics on diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning arithmetic, intuitive, and entailment-based reasoning. Results show that the proposed quantum-inspired approach offers significant improvements over supervised, contrastive, and pretrained baseline approaches, highlighting the effectiveness of quantum-inspired fidelity as a foundation to improve reasoning trace coherence in LLMs.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:53:05.896620",
        "filter_reason": "这篇论文完全符合研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础推理能力，提出了一种量子启发的强化学习方法来增强LLM在连贯多步推理方面的表现。这直接对应了\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的核心标准。 从正面指标分析，论文明确包含以下关键要素： - 核心概念：直接关注\"Large Language Models (LLMs)\" - 能力方向：专注于\"coherent multi-step reasoning traces\"和\"structured logical flow\"，属于推理能力范畴 - 训练方法：采用强化学习方法（Proximal Policy Optimization），结合量子物理中的Projected Entangled Pair States (PEPS)概念 论文不涉及任何排除标准中的领域。它不是关于多模态与视觉研究，不是将LLM应用到特定领域，也不是关于模型可靠性在应用层面的研究。虽然论文在GSM8K、StrategyQA和EntailmentBank等数据集上进行了评估，但这些是评估通用推理能力的标准数据集，而非特定领域应用。 论文的核心贡献是提出了一种基于量子物理概念的强化学习方法，通过保真度奖励机制来提高LLM生成连贯推理痕迹的能力，这是一种从根本上提升模型推理能力的方法论创新，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#14",
        "title": "The Conductor and the Engine: A Path Towards Co-Designed Reasoning",
        "link": "/arxiv/2509.19762",
        "arxiv_id": "2509.19762",
        "authors": "Yuanxin Wang, Pawel Filipczuk, Anisha Garg, Amaan Dhada, Mohammad Hassanpour, David Bick, Ganesh Venkatesh",
        "summary": "Modern LLM reasoning relies on extensive test-time computation, driven by internal model training and external agentic orchestration. However, this synergy is often inefficient, as model verbosity and poor instruction following lead to wasted compute. We analyze this capability-cost trade-off and introduce an optimized reasoning workflow (\\cepo) that empowers smaller open-source models to outperform models multiple times their size. We will open-source this workflow to enable further research. Our work demonstrates a clear path toward co-designing orchestration frameworks with the underlying model capabilities to unlock powerful reasoning in small-to-medium sized models.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:53:05.898361",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，该论文的本质是关于改进LLM的推理能力，提出了一种优化的推理工作流程(\\cepo)，通过协调内部模型训练和外部智能体编排来提高推理效率，使较小的开源模型能够超越比它们大得多的模型。这直接符合\"改进LLM的基础能力、增强其逻辑、多步推理等通用能力\"的标准。 其次，从正面指标看，论文明确涉及\"LLM reasoning\"这一核心概念和\"reasoning\"这一能力方向，同时提到了\"external agentic orchestration\"，与智能体(llm-based agents)这一新兴范式相关。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，在特殊情况下，论文提出的智能体编排框架是通用性的，旨在增强LLM的通用推理能力，而非应用于特定领域，因此应该保留。 综上所述，该论文的核心贡献是提出了一种协同设计编排框架与底层模型能力的方法，以释放中小型模型的强大推理能力，这与研究目标高度一致。"
    },
    {
        "index": "#16",
        "title": "Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving",
        "link": "/arxiv/2509.19681",
        "arxiv_id": "2509.19681",
        "authors": "Anisha Garg, Engin Tekin, Yash More, David Bick, Nishit Neema, Ganesh Venkatesh",
        "summary": "Advanced test-time computing strategies are essential for scaling reasoning models, but their effectiveness is capped by the models' poor self-evaluation. We propose a pairwise Explanatory Verifier, trained via reinforcement learning (GRPO), that produces calibrated confidence scores and associated natural language reasoning for generated solutions. Our verifier improves the accuracy and efficiency of test-time strategies like best-of-n and self-reflection. Crucially, it excels at identifying challenging failure modes, such as when both candidate solutions are identically incorrect, succeeding where standard methods like majority voting fail.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:53:05.898800",
        "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出一种\"解释性验证器\"(Explanatory Verifier)来增强大语言模型的通用推理能力。具体分析如下： 从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是通过强化学习(GRPO)训练的验证器来提升模型的自我评估能力，这属于增强LLM通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 从第二步正面指标看，论文涉及多个相关主题： 1. 能力方向：明确聚焦于\"reasoning\"和\"problem-solving\"，这正是研究目标的核心 2. 训练方法：使用\"reinforcement learning (GRPO)\"进行训练，符合强化学习优化LLM能力的方向 3. 提到的\"self-reflection\"也与提升模型自主推理能力相关 从第三步排除标准看，论文不涉及任何多模态、视觉内容，也不针对医疗、化学、生物等特定应用领域，同时虽然涉及到模型可靠性，但目的是从根本上提升模型的推理能力而非仅作为应用层面的防御。 论文特别关注提高LLM的\"自我评估\"能力，这是通用推理能力的重要组成部分，通过校准的置信度分数和自然语言解释来增强模型的推理质量，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#28",
        "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing",
        "link": "/arxiv/2509.20336",
        "arxiv_id": "2509.20336",
        "authors": "Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang",
        "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:53:05.902024",
        "filter_reason": "这篇论文的核心是研究基于Transformer的LLMs在图推理任务中的内部机制，通过circuit-tracer框架来解释decoder-only transformers的推理过程。论文识别了图推理中的两个核心机制：token merging和structural memorization，并提供了统一的可解释性框架来理解结构推理。这符合研究目标中\"改进LLM的基础能力\"和\"增强其逻辑、多步推理等通用能力\"的要求。论文关注的是LLM本身的推理能力机制，而不是将LLM作为工具应用到特定领域。虽然论文聚焦于图推理这一特定类型的推理，但其目标是提供\"统一的可解释性框架\"来理解结构推理，这属于通用推理能力的研究范畴。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。因此，这篇论文符合研究范围。"
    },
    {
        "index": "#107",
        "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms",
        "link": "/arxiv/2509.19702",
        "arxiv_id": "2509.19702",
        "authors": "Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama",
        "summary": "We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nyström extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nyström extrapolation, highlighting a powerful capability of in-context learning.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:53:05.925481",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究线性注意力transformer（一种LLM架构）的基础能力。论文展示了模型通过训练能够隐式地发现统一的数值算法，这直接涉及LLM的内在能力提升，而非将LLM作为工具应用于特定领域。论文关注的是上下文学习(in-context learning)能力，这是一种基础能力的研究，与提高LLM的通用推理能力密切相关。 其次，从正面指标分析，论文符合以下关键点： - 核心概念：研究的是线性注意力transformer，属于LLM架构变体 - 能力方向：涉及数学推理(math reasoning)和问题解决(problem-solving)能力，模型通过学习解决矩阵补全问题，隐式发现了数值算法 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于特定应用领域 - 不涉及模型可靠性的应用层面研究 最后，论文的核心贡献是揭示了LLM能够通过训练隐式发现统一的、资源自适应的迭代求解器，这展示了LLM在算法发现和数学推理方面的强大能力，直接关系到通用推理能力的提升。论文研究的是LLM内在的能力机制，而非特定应用，因此完全符合研究目标。"
    }
]