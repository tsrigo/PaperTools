[
    {
        "index": "#9",
        "title": "GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation via Large Language Model Agents",
        "link": "/arxiv/2510.13896",
        "arxiv_id": "2510.13896",
        "authors": "Xi Yu, Yang Yang, Qun Liu, Yonghua Du, Sean McSweeney, Yuewei Lin",
        "summary": "Cellular image segmentation is essential for quantitative biology yet remains difficult due to heterogeneous modalities, morphological variability, and limited annotations. We present GenCellAgent, a training-free multi-agent framework that orchestrates specialist segmenters and generalist vision-language models via a planner-executor-evaluator loop (choose tool $\\rightarrow$ run $\\rightarrow$ quality-check) with long-term memory. The system (i) automatically routes images to the best tool, (ii) adapts on the fly using a few reference images when imaging conditions differ from what a tool expects, (iii) supports text-guided segmentation of organelles not covered by existing models, and (iv) commits expert edits to memory, enabling self-evolution and personalized workflows. Across four cell-segmentation benchmarks, this routing yields a 15.7\\% mean accuracy gain over state-of-the-art baselines. On endoplasmic reticulum and mitochondria from new datasets, GenCellAgent improves average IoU by 37.6\\% over specialist models. It also segments novel objects such as the Golgi apparatus via iterative text-guided refinement, with light human correction further boosting performance. Together, these capabilities provide a practical path to robust, adaptable cellular image segmentation without retraining, while reducing annotation burden and matching user preferences.",
        "subjects": "Quantitative Methods, Artificial Intelligence, Computer Vision and Pattern Recognition, Multiagent Systems",
        "date": "2025-10-14",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.522103",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质并非简单地将一个现有LLM或智能体框架应用于生物学领域，而是**构建了一个新的、具有特定架构的多智能体框架**。其核心贡献是 `GenCellAgent` 这个“免训练的多智能体框架”本身，以及其内部的“规划器-执行器-评估器”工作流。这完全符合“构建、改进或演化LLM智能体的方法论或新框架”的保留标准。 2.  **第二步：正面指标** - 论文命中了多个核心正面指标，与您的研究焦点高度契合： - **多智能体**: 明确提出了 `multi-agent framework`。 - **规划**: 核心循环是 `planner-executor-evaluator loop`，这是一种典型的智能体规划与执行机制。 - **工具使用**: 智能体框架“编排”了“专业分割器和通用视觉语言模型”，并执行“选择工具”的操作，这是典型的工具使用能力。 - **记忆**: 论文明确提到了 `long-term memory` 和将专家编辑“提交到记忆中”。 - **自我演化**: 摘要直接指出该框架“实现了自我演化和个性化工作流”。这是您最关心的方向之一。 3.  **第三步：排除标准** - 该论文成功避开了所有排除标准： - **安全与对齐**: 全文未提及安全、对齐、可解释性等内容。 - **多模态与视觉**: 虽然论文处理的是图像分割任务，并使用了视觉语言模型（VLMs），但根据您的规则，这些VLMs是作为智能体感知和解决问题的**工具**，而不是研究的核心。研究的核心是**如何组织和控制这些工具的智能体框架**。因此，这不构成排除理由。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化应用”这一例外情况的完美范例。它的应用领域是高度特定的（细胞图像分割），但其**核心贡献是提出了一种新的“自我演化”机制**（通过将专家编辑存入记忆来迭代改进）。根据您的指示，这种情况应该保留。 - **推理/规划**: 论文的“规划器-执行器-评估器循环”属于智能体在复杂任务中进行多步推理和规划的范畴，符合保留条件。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于**构建了一个集规划、工具使用、记忆和自我演化能力于一体的多智能体框架**。尽管其应用场景是细胞生物学，但其方法论是通用的，并且直接命中了您“多智能体”和“自我演化”这两个核心研究方向。因此，这篇论文是您研究课题下的高质量前沿文献，应被筛选出来。"
    },
    {
        "index": "#3",
        "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
        "link": "/arxiv/2510.13982",
        "arxiv_id": "2510.13982",
        "authors": "Jinkun Chen, Sher Badshah, Xuemin Yu, Sijia Han, Jiechao Gao",
        "summary": "What if artificial agents could not just communicate, but also evolve, adapt, and reshape their worlds in ways we cannot fully predict? With llm now powering multi-agent systems and social simulations, we are witnessing new possibilities for modeling open-ended, ever-changing environments. Yet, most current simulations remain constrained within static sandboxes, characterized by predefined tasks, limited dynamics, and rigid evaluation criteria. These limitations prevent them from capturing the complexity of real-world societies. In this paper, we argue that static, task-specific benchmarks are fundamentally inadequate and must be rethought. We critically review emerging architectures that blend llm with multi-agent dynamics, highlight key hurdles such as balancing stability and diversity, evaluating unexpected behaviors, and scaling to greater complexity, and introduce a fresh taxonomy for this rapidly evolving field. Finally, we present a research roadmap centered on open-endedness, continuous co-evolution, and the development of resilient, socially aligned AI ecosystems. \\textbf{We call on the community to move beyond static paradigms and help shape the next generation of adaptive, socially-aware multi-agent simulations.}",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.520456",
        "filter_reason": "这篇论文完全符合您的研究范围，是一个高度相关的筛选目标。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断——保留** - 论文的核心贡献不是将现有智能体应用于某个特定领域，而是对**LLM多智能体模拟这一研究范式本身进行批判、重构和展望**。 - 它明确指出了当前主流的“静态沙箱”方法的局限性，并提出了一个全新的研究方向：**具有开放式协同演化能力的LLM多智能体模拟**。 - 这直接对应了您研究目标中的“**构建、改进或演化 LLM智能体**”，特别是“多智能体”和“自我演化”两个方向的交叉点。因此，这篇论文的本质是方法论和框架层面的探讨，符合保留标准。 2.  **第二步：正面指标——高度匹配** - 论文摘要中充斥着您核心关注点的关键词和概念： - **核心范式**: `LLM-Based Multi-Agent Simulations` (直接命中), `Co-Evolution` (自我演化的核心机制)。 - **多智能体**: `Multi-Agent Dynamics`, `Socially-aware Multi-Agent Simulations`。 - **演化机制**: `Evolves`, `Adapt`, `Open-Ended Co-Evolution`, `Continuous Co-Evolution`。 - 论文不仅涉及这些概念，更是将其作为核心论点来组织，提出了新的分类法（`taxonomy`）和研究路线图（`research roadmap`），贡献度很高。 3.  **第三步：排除标准——未触发** - 论文虽然提到了 \"socially aligned AI ecosystems\"，但其主要贡献是**提出实现这种对齐的架构和演化路径**，而不是研究对齐（Alignment）本身的具体技术。因此，它不属于以安全与对齐为核心贡献的论文。 - 论文不涉及视觉或多模态内容，未触发相关排除标准。 4.  **第四步：处理特殊和模糊情况** - 这篇论文可以被视为一篇**领域综述与前瞻性研究**。这类论文对于把握领域前沿、确定未来研究方向至关重要。它提出的“开放式协同演化”框架，正是您“自我演化”研究方向的最高阶形态之一，是构建真正复杂、自适应智能体系统的关键。 **总结**: 该论文的核心是推动LLM多智能体系统从“静态”走向“动态”和“演化”，提出了“开放式协同演化”这一前沿范式。它完美契合您研究目标中的“**多智能体**”和“**自我演化**”两大方向，并且贡献点在于提出新的研究框架和路线图，而非简单的应用。因此，这是一篇必须保留的高质量前沿论文。"
    },
    {
        "index": "#4",
        "title": "Benefits and Limitations of Communication in Multi-Agent Reasoning",
        "link": "/arxiv/2510.13903",
        "arxiv_id": "2510.13903",
        "authors": "Michael Rizvi-Martel, Satwik Bhattamishra, Neil Rathi, Guillaume Rabusseau, Michael Hahn",
        "summary": "Chain-of-thought prompting has popularized step-by-step reasoning in large language models, yet model performance still degrades as problem complexity and context length grow. By decomposing difficult tasks with long contexts into shorter, manageable ones, recent multi-agent paradigms offer a promising near-term solution to this problem. However, the fundamental capacities of such systems are poorly understood. In this work, we propose a theoretical framework to analyze the expressivity of multi-agent systems. We apply our framework to three algorithmic families: state tracking, recall, and $k$-hop reasoning. We derive bounds on (i) the number of agents required to solve the task exactly, (ii) the quantity and structure of inter-agent communication, and (iii) the achievable speedups as problem size and context scale. Our results identify regimes where communication is provably beneficial, delineate tradeoffs between agent count and bandwidth, and expose intrinsic limitations when either resource is constrained. We complement our theoretical analysis with a set of experiments on pretrained LLMs using controlled synthetic benchmarks. Empirical outcomes confirm the tradeoffs between key quantities predicted by our theory. Collectively, our analysis offers principled guidance for designing scalable multi-agent reasoning systems.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.520733",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于对多智能体系统进行基础性的理论分析和设计指导，属于“多智能体”方向的前沿研究。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将多智能体框架应用到一个新领域，而是对“多智能体推理”这一范式本身进行深入的理论分析。它提出了一个“理论框架”来分析多智能体系统的表达能力，并推导出关于智能体数量、通信结构和性能权衡的界限。这种对系统基础能力和设计原则的探索，直接服务于“构建、改进或演化LLM智能体”的核心目标，因为它为如何设计更高效、更可扩展的多智能体系统提供了“原则性指导”。 2.  **第二步：正面指标** - 论文高度契合您的核心关注点： - **核心范式**: 明确聚焦于 `Multi-Agent Systems (MAS)`。 - **多智能体**: 核心主题是 `Communication`（通信），并探讨了其在多智能体 `Reasoning`（推理）中的作用和局限性。这直接关联到智能体间的协作与信息交换。 - **智能体能力**: 论文研究了多智能体如何通过协作完成复杂的 `Reasoning` 任务，这是智能体能力的关键体现。 3.  **第三步：排除标准** - 论文完全不涉及安全与对齐、多模态与视觉等排除领域。其焦点纯粹在于多智能体系统的计算理论和性能分析，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文研究的“推理”是典型的“保留”情况。它不是在提升单个LLM的数学或逻辑能力，而是在分析多个智能体如何通过分解任务和相互通信来协同完成复杂推理。这正是Agentic AI中多智能体推理的核心议题。 **总结**: 该论文的核心贡献是提供了一个理论框架，用于理解和指导多智能体系统的设计，特别是通信机制在其中的作用。它直接回答了“如何构建更好的多智能体系统”这一根本问题，为该领域的发展提供了理论基础和设计原则。因此，它精准地落在您“多智能体”研究焦点之内，是一篇高质量、高相关性的前沿论文。"
    },
    {
        "index": "#1",
        "title": "The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems",
        "link": "/arxiv/2510.14401",
        "arxiv_id": "2510.14401",
        "authors": "Prateek Gupta, Qiankun Zhong, Hiromu Yakura, Thomas Eisenmann, Iyad Rahwan",
        "summary": "A growing body of multi-agent studies with Large Language Models (LLMs) explores how norms and cooperation emerge in mixed-motive scenarios, where pursuing individual gain can undermine the collective good. While prior work has explored these dynamics in both richly contextualized simulations and simplified game-theoretic environments, most LLM systems featuring common-pool resource (CPR) games provide agents with explicit reward functions directly tied to their actions. In contrast, human cooperation often emerges without full visibility into payoffs and population, relying instead on heuristics, communication, and punishment. We introduce a CPR simulation framework that removes explicit reward signals and embeds cultural-evolutionary mechanisms: social learning (adopting strategies and beliefs from successful peers) and norm-based punishment, grounded in Ostrom's principles of resource governance. Agents also individually learn from the consequences of harvesting, monitoring, and punishing via environmental feedback, enabling norms to emerge endogenously. We establish the validity of our simulation by reproducing key findings from existing studies on human behavior. Building on this, we examine norm evolution across a $2\\times2$ grid of environmental and social initialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and benchmark how agentic societies comprised of different LLMs perform under these conditions. Our results reveal systematic model differences in sustaining cooperation and norm formation, positioning the framework as a rigorous testbed for studying emergent norms in mixed-motive LLM societies. Such analysis can inform the design of AI systems deployed in social and organizational contexts, where alignment with cooperative norms is critical for stability, fairness, and effective governance of AI-mediated environments.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.519792",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接命中了“多智能体”和“自我演化”两个关键方向。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心本质是**构建一个新的方法论框架**。摘要明确指出：“We introduce a CPR simulation framework that removes explicit reward signals and embeds cultural-evolutionary mechanisms...”。这并非将现有智能体框架简单应用于某个领域，而是提出了一种全新的、用于研究LLM多智能体社会行为的模拟环境。该框架的核心是引入了“社会学习”和“基于规范的惩罚”等演化机制，这完全符合“构建、改进或演化LLM智能体”的核心目标。 2.  **第二步：正面指标——高度匹配** 论文包含了大量您关注的核心范式和能力指标： *   **核心范式**: `Multi-Agent Systems (MAS)` (标题和摘要多次提及), `Self-Evolving` (通过 \"cultural-evolutionary mechanisms\", \"norm evolution\", \"emerge endogenously\" 体现)。 *   **多智能体**: `Collaboration` / `Cooperation` (论文核心主题), `Communication` (隐含在社会学习和惩罚中), `Social Learning` (明确作为核心机制), `Agent Society` (摘要中提及 \"agentic societies\")。 *   **演化机制**: `Self-Improvement` (通过环境反馈学习), `Iterative Improvement` (规范的内生演化过程)。 3.  **第三步：排除标准——未触发** *   **安全与对齐**: 论文虽然提到了 \"governance\" 和 \"fairness\"，但其主要贡献是研究规范如何“涌现”的框架，而不是提出一种新的安全或对齐技术。这些是研究的结果和应用场景，而非论文的核心方法论。 *   **多模态与视觉**: 论文完全不涉及视觉或多模态内容。 4.  **第四步：处理特殊和模糊情况——适用例外规则** 论文完美地符合“自我演化的应用”这一例外情况。它的核心贡献就是提出一种新的“自我演化”机制（社会学习和规范形成），并将其应用在“公共池资源”这个特定领域。根据您的规则，即使有特定应用背景，只要核心是新的演化机制，就应该保留。 **核心依据总结**: 该论文的核心贡献是**提出一个嵌入文化演化机制（社会学习、规范形成）的LLM多智能体模拟框架**。它直接研究了智能体社会如何在没有明确奖励信号的情况下，通过互动和演化自发形成合作规范。这精准地落在您“多智能体”和“自我演化”的研究焦点上，是一项关于Agentic AI方法论和框架的前沿研究，而非简单的应用或安全性分析。因此，这篇论文是高度相关的，应该被保留。"
    },
    {
        "index": "#13",
        "title": "To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models",
        "link": "/arxiv/2510.14826",
        "arxiv_id": "2510.14826",
        "authors": "Eran Malach, Omid Saremi, Sinead Williamson, Arwen Bradley, Aryo Lotfi, Emmanuel Abbe, Josh Susskind, Etai Littwin",
        "summary": "State Space Models (SSMs) have become the leading alternative to Transformers for sequence modeling. Their primary advantage is efficiency in long-context and long-form generation, enabled by fixed-size memory and linear scaling of computational complexity. We begin this work by showing a simple theoretical result stating that SSMs cannot accurately solve any ``truly long-form'' generation problem (in a sense we formally define), undermining their main competitive advantage. However, we show that this limitation can be mitigated by allowing SSMs interactive access to external tools. In fact, we show that given the right choice of tool access and problem-dependent training data, SSMs can learn to solve any tractable problem and generalize to arbitrary problem length/complexity (i.e., achieve length generalization). Following our theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable length generalization on a variety of arithmetic, reasoning, and coding tasks. These findings highlight SSMs as a potential efficient alternative to Transformers in interactive tool-based and agentic settings.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.958591",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接命中了“单智能体”方向下的“工具使用”子方向。我的判断过程如下： **第一步：核心判断** - **保留**。这篇论文的本质并非应用已有技术解决特定领域问题，也不是单纯提升模型的基础推理能力。其核心贡献是提出一种**全新的方法论**：通过赋予状态空间模型（SSM）“交互式工具使用”的能力，从根本上克服了模型在“长度泛化”上的理论局限。这是一种构建和改进LLM智能体的框架性工作，而非简单的应用。 **第二步：正面指标** - 论文高度符合您的核心关注点。 - **核心范式**: 摘要中明确提到了 `agentic settings` (智能体设置)。 - **智能体能力**: 论文的标题和核心就是 `Tool Use` (工具使用)。它探讨了如何通过工具解锁模型的新能力，这正是智能体研究的关键。虽然未直接提及`Planning`或`ReAct`，但其“交互式工具使用”和解决长链推理任务的描述，内在地包含了推理和行动的循环。 **第三步：排除标准** - 论文不涉及任何排除标准。它的焦点是提升模型的能力和效率，而非安全、对齐或可解释性，也不涉及多模态。 **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是**保留**的典型案例。它不是在研究如何让LLM更好地做数学题（非Agentic的推理），而是在研究如何构建一个能够使用工具（如计算器）来解决任意长度数学问题的智能体框架。这里的推理是嵌入在“工具使用”这个智能体行为之中的。 - **自我演化的应用**: 此处不适用，但该论文的思路（通过外部机制突破模型内在限制）与“演化”精神有相通之处。 **最终决策** 综合来看，这篇论文的核心贡献是提出了一种**工具增强的智能体新框架**。它不仅理论上证明了该方法的有效性，还在推理和编码等任务上进行了实证。研究目标是让一个基础模型（SSM）通过工具使用变得更强，这完全符合您“构建、改进或演化LLM智能体”的核心目标。论文的最后一句话——“These findings highlight SSMs as a potential efficient alternative to Transformers in interactive tool-based and agentic settings”——更是明确地将其工作定位在“智能体设置”中。因此，这是一篇高度相关且应保留的前沿论文。"
    },
    {
        "index": "#36",
        "title": "Agentic Entropy-Balanced Policy Optimization",
        "link": "/arxiv/2510.14545",
        "arxiv_id": "2510.14545",
        "authors": "Guanting Dong, Licheng Bao, Zhongyuan Wang, Kangzhi Zhao, Xiaoxi Li, Jiajie Jin, Jinghan Yang, Hangyu Mao, Fuzheng Zhang, Kun Gai, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou",
        "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.981244",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心不是将现有智能体应用于某个特定领域，而是提出了一种名为“Agentic Entropy-Balanced Policy Optimization (AEPO)”的**新算法**。这个算法旨在解决当前Agentic强化学习在训练具有多轮、长时程工具使用能力的Web智能体时遇到的关键挑战（训练崩溃）。因此，这篇论文的本质是**改进LLM智能体的训练和优化方法**，属于典型的“构建、改进或演化LLM智能体”的范畴，应予以**保留**。它不属于“非演化型应用”或“非Agentic的推理”。 2.  **第二步：正面指标** 论文摘要中明确包含了您的多个核心关注点： *   **核心范式**: `Agentic RL` (Agentic Reinforcement Learning), `Agentic AI` (标题中)。 *   **智能体能力**: `Tool Use` (反复提及“tool-use capabilities”和“tool-call steps”)。论文中提到的“multi-turn, long-horizon”任务也暗示了复杂的**规划**能力。 *   **演化机制**: 论文提出的AEPO算法通过平衡熵来改善训练过程，这是一种**迭代改进**智能体性能的机制，可以归类为训练层面的自我演化。 3.  **第三步：排除标准** 论文的主要贡献不涉及安全、对齐、多模态等排除项。它的焦点纯粹在于如何提升智能体在执行任务时的性能和训练稳定性，与您的排除标准无冲突。 4.  **第四步：处理特殊和模糊情况** 该论文与“推理/规划”相关，但完全符合保留条件。它不是在研究如何提升LLM模型本身的基础逻辑或数学推理能力，而是在研究**智能体（Agent）在复杂、多步骤任务中的决策和行动过程**（即rollout和policy update）。这是一种典型的Agentic框架下的规划与执行研究，属于您研究焦点的核心。 5.  **第五步：最终决策** 综合来看，这篇论文的核心贡献是提出了一种创新的Agentic RL算法，用于解决LLM智能体在长时程工具使用任务中的训练难题。这直接切中了您研究课题中的“单智能体”方向，特别是其“工具使用”和“规划”能力的改进。因此，这篇论文高度相关，应该被筛选出来。"
    },
    {
        "index": "#52",
        "title": "Stop-RAG: Value-Based Retrieval Control for Iterative RAG",
        "link": "/arxiv/2510.14337",
        "arxiv_id": "2510.14337",
        "authors": "Jaewan Park, Solbee Cho, Jay-Yoon Lee",
        "summary": "Iterative retrieval-augmented generation (RAG) enables large language models to answer complex multi-hop questions, but each additional loop increases latency, costs, and the risk of introducing distracting evidence, motivating the need for an efficient stopping strategy. Existing methods either use a predetermined number of iterations or rely on confidence proxies that poorly reflect whether more retrieval will actually help. We cast iterative RAG as a finite-horizon Markov decision process and introduce Stop-RAG, a value-based controller that adaptively decides when to stop retrieving. Trained with full-width forward-view Q($\\lambda$) targets from complete trajectories, Stop-RAG learns effective stopping policies while remaining compatible with black-box APIs and existing pipelines. On multi-hop question-answering benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines and prompting-based stopping with LLMs. These results highlight adaptive stopping as a key missing component in current agentic systems, and demonstrate that value-based control can improve the accuracy of RAG systems.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.991351",
        "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **第一步：核心判断 (保留)** 论文的核心贡献并非将RAG应用于某个新领域，而是提出了一种名为 **Stop-RAG** 的**新方法论和新框架**，用于**改进**LLM智能体的核心行为。它将迭代式RAG过程建模为马尔可夫决策过程（MDP），并引入一个基于价值的控制器来决定何时停止检索。这直接属于对LLM智能体**构建和改进**的范畴，而非简单的应用。 2.  **第二步：正面指标 (高度相关)** 论文触及了您研究焦点的多个核心关键词： *   **核心范式**: 论文摘要明确指出其工作是为 \"agentic systems\" 做贡献，这与您的 `Agentic AI` 焦点完全一致。 *   **智能体能力**: 迭代式RAG本身就是一个“思考-行动-观察”的循环，而本文提出的自适应停止机制，是对这个循环的**控制**和**优化**。这可以看作是一种高级的**规划**或**自我反思**能力——智能体需要评估自己当前的状态和信息是否足够，并决定是继续行动（检索）还是结束流程。这是一种元级别的决策，是智能体自主性的重要体现。 *   **工具使用**: 论文研究的迭代式RAG，其核心就是 `Tool Use`（检索工具）。本文的工作本质上是**优化工具使用的策略**，决定何时停止使用工具。 3.  **第三步：排除标准 (不涉及)** 论文的主要贡献是提升智能体决策的效率和准确性，不涉及安全、对齐、可解释性或多模态等排除领域。 4.  **第四步：处理特殊和模糊情况 (符合保留条件)** *   **推理/规划**: 本文完美符合“保留”条件。它不是在提升LLM的基础推理能力（如数学计算），而是在研究**智能体如何进行多步推理的规划与控制**。将RAG循环视为MDP并使用强化学习（Q(λ)）来学习最优停止策略，这正是对智能体规划/执行过程的深刻改进。 **总结:** 这篇论文的核心贡献是**为LLM智能体设计了一个更智能的“控制器”**，使其在执行多步、工具增强的任务时能更高效、更自主地决策。这直接对应了您研究范围中的**“单智能体”**方向，特别是在**规划、自我反思和工具使用**的子方向上做出了创新性的改进。因此，这篇论文是您课题筛选中应优先保留的高质量前沿研究。"
    },
    {
        "index": "#64",
        "title": "Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models",
        "link": "/arxiv/2510.14232",
        "arxiv_id": "2510.14232",
        "authors": "Mehrzad Samadi, Aleksander Ficek, Sean Narenthiran, Siddhartha Jain, Wasi Uddin Ahmad, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg",
        "summary": "Competitive programming has become a rigorous benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). The International Olympiad in Informatics (IOI) stands out as one of the most prestigious annual competitions in competitive programming and has become a key benchmark for comparing human and AI-level programming ability. While several proprietary models have been claimed to achieve gold medal-level performance at the IOI, often with undisclosed methods, achieving comparable results with open-weight models remains a significant challenge. In this paper, we present \\gencluster, a scalable and reproducible test-time compute framework that attains IOI gold-level performance using open-weight models. It combines large-scale generation, behavioral clustering, ranking, and a round-robin submission strategy to efficiently explore diverse solution spaces under limited validation budgets. Our experiments show that the performance of our proposed approach scales consistently with available compute, narrowing the gap between open and closed systems. Notably, we will show that GenCluster can achieve a gold medal at IOI 2025 for the first time with an open-weight model gpt-oss-120b, setting a new benchmark for transparent and reproducible evaluation of reasoning in LLMs.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.995118",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将LLM作为工具应用于竞赛编程领域。其核心贡献是提出了一个名为 `\\gencluster` 的**新框架**。这个框架是一种方法论，它通过“大规模生成、行为聚类、排序和循环提交策略”来系统性地提升LLM在复杂任务上的表现。这构建了一个围绕LLM的、具有多步骤处理能力的系统，而非简单的应用。它不属于“非演化型应用”或“非Agentic的推理”的排除范畴。 2.  **第二步：正面指标** - 论文包含了多个核心关注点。`\\gencluster` 框架的“探索多样化的解决方案空间”体现了**规划**能力。其“循环提交策略”与 `ReAct` 范式高度相似，即智能体生成一个解决方案（行动），提交后获得反馈（观察），然后进行下一步操作。这是一种典型的**工具使用**或与环境交互的形式。虽然论文没有明确使用 \"Agent\" 一词，但其描述的框架本质上是一个Agentic系统。 3.  **第三步：排除标准** - 论文的主要贡献是关于性能提升和框架设计，不涉及 `Safety`、`Alignment`、`Hallucination` 等安全与对齐问题。同时，它是一个纯文本/代码任务，不涉及 `Vision` 或多模态内容。因此，该论文完全避开了排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**：这是判断的关键。这篇论文完美符合“保留”条件——“如果论文是关于智能体如何进行规划或在复杂任务中进行多步推理（如 ReAct、ToT 或新的Agentic框架）”。`\\gencluster` 正是一个新的、用于多步推理和规划的Agentic框架。它不是在改进LLM本身的基础数学或逻辑能力（即非Agentic的推理），而是在LLM之上构建一个更高级的、能进行系统性探索和迭代的推理架构。 5.  **第五步：最终决策** - 综合以上分析，尽管论文的评测基准是特定领域（IOI竞赛编程），但其**核心贡献是提出了一种新颖的、可扩展的、用于增强LLM复杂任务解决能力的Agentic框架**。这个框架的核心机制（规划、行动-观察循环）与我的研究焦点“单智能体”方向中的“规划”和“工具使用”高度吻合。因此，这篇论文完全符合我的研究目标，应被保留。"
    },
    {
        "index": "#69",
        "title": "MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation",
        "link": "/arxiv/2510.14184",
        "arxiv_id": "2510.14184",
        "authors": "Mahmood Hegazy, Aaron Rodrigues, Azzam Naeem",
        "summary": "We present MAFA (Multi-Agent Framework for Annotation), a production-deployed system that transforms enterprise-scale annotation workflows through configurable multi-agent collaboration. Addressing the critical challenge of annotation backlogs in financial services, where millions of customer utterances require accurate categorization, MAFA combines specialized agents with structured reasoning and a judge-based consensus mechanism. Our framework uniquely supports dynamic task adaptation, allowing organizations to define custom annotation types (FAQs, intents, entities, or domain-specific categories) through configuration rather than code changes. Deployed at JP Morgan Chase, MAFA has eliminated a 1 million utterance backlog while achieving, on average, 86% agreement with human annotators, annually saving over 5,000 hours of manual annotation work. The system processes utterances with annotation confidence classifications, which are typically 85% high, 10% medium, and 5% low across all datasets we tested. This enables human annotators to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's effectiveness across multiple datasets and languages, showing consistent improvements over traditional and single-agent annotation baselines: 13.8% higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1 in our internal intent classification dataset and similar gains on public benchmarks. This work bridges the gap between theoretical multi-agent systems and practical enterprise deployment, providing a blueprint for organizations facing similar annotation challenges.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.001822",
        "filter_reason": "这篇论文符合我的研究范围，应该被保留。判断依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献是构建一个**多智能体框架**，而不是将已有智能体作为工具进行应用。摘要中明确指出这是一个“Multi-Agent Framework”，并且其核心创新点在于“configurable multi-agent collaboration”（可配置的多智能体协作）和“dynamic task adaptation”（动态任务适配）。这表明论文的重点在于提出一种新的、可复用的智能体系统设计方法论，而非仅仅解决金融领域的标注问题。虽然它部署在金融领域，但这被用作验证框架有效性的案例，其贡献在于框架本身，符合“构建、改进LLM智能体”的核心目标。因此，它不属于“非演化型应用”的排除范畴。 2.  **正面指标 (第二步):** 论文包含了多个核心关注点。 *   **核心范式:** `Multi-Agent Systems (MAS)` 是论文的标题和核心。 *   **多智能体:** 论文详细描述了智能体间的 `Collaboration`（协作）和 `Communication`（通信，通过“judge-based consensus mechanism”体现）。 *   **智能体能力:** 提到了“structured reasoning”（结构化推理），这是智能体能力的一部分。 3.  **排除标准 (第三步):** 论文的主要贡献不涉及安全与对齐（Safety, Alignment）、可解释性（Interpretability）或多模态（Vision）等排除领域。它的焦点是系统架构和协作效率。 4.  **特殊和模糊情况 (第四步):** 论文不涉及自我演化，但其对多智能体协作框架的构建和改进完全符合我的研究焦点。它提出的“可配置任务适配”机制，虽然不是严格意义上的自我演化，但展示了智能体框架的灵活性和适应性，这是构建高级智能体系统的重要一环。 **最终决策 (第五步):** 综合来看，这篇论文的核心贡献在于提出并验证了一个新颖的、可配置的多智能体协作框架（MAFA），以解决企业级实际问题。它直接贡献于“多智能体”这一研究方向，提供了关于如何设计、构建和部署多智能体系统的宝贵见解。因此，它精准地符合我筛选“构建、改进或演化LLM智能体”论文的核心目标。"
    },
    {
        "index": "#102",
        "title": "Agentic Design of Compositional Machines",
        "link": "/arxiv/2510.14980",
        "arxiv_id": "2510.14980",
        "authors": "Wenqian Zhang, Weiyang Liu, Zhen Liu",
        "summary": "The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Graphics, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.032254",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断依据如下： 1.  **核心判断 (第一步):** *   论文的核心并非将LLM简单地应用于“机器设计”这一特定领域，而是**研究LLM作为智能体如何执行复杂的、多步骤的创造性任务**。标题中的“Agentic Design”和摘要中的“agentic workflows”明确指出了其研究重点是智能体的工作流程和能力。 *   论文的贡献是双重的：首先，构建了一个新的测试平台来**评估LLM智能体的能力**；其次，探索了如何通过强化学习（RL）来**改进和提升**该智能体在这项任务上的表现。这完全符合“构建、改进或演化 LLM智能体”的核心目标。 *   因此，它不是“非演化型应用”，而是对智能体能力本身的探索和增强。 2.  **正面指标 (第二步):** *   论文包含了大量核心关注点。摘要明确提及了 **`Agentic AI`**（通过 \"agentic workflows\" 体现）。 *   任务“Compositional Machine Design”要求智能体进行**`Planning`**（策略性组装）和复杂的**推理**（空间推理、物理推理）。 *   使用RL进行微调，其目的是为了**`Iterative Improvement`**（迭代改进），这与“改进智能体”的方向一致。 3.  **排除标准 (第三步):** *   论文的主要焦点并非安全、对齐或多模态技术。虽然任务在模拟物理环境中进行，可能隐含了空间感知，但论文的核心是语言模型作为智能体的决策和规划能力，而不是视觉模型本身。因此，不触及任何排除红线。 4.  **处理特殊和模糊情况 (第四步):** *   **推理/规划:** 这篇论文是典型的关于**智能体如何进行规划**的例子。它不是在提升LLM的基础数学或逻辑能力，而是在研究智能体如何在一个具有明确目标和约束的环境中（模拟物理环境），通过一系列步骤（组装部件）来达成目标。这种“策略性组装”是高级规划和任务执行的体现，完全符合保留条件。 **核心依据总结:** 该论文的核心贡献在于**提出并研究了一个LLM智能体框架，用于解决复杂的组合式设计问题**。它不仅定义了任务和评测基准，还进一步探索了通过RL来优化该智能体性能的方法。研究的焦点始终是**智能体的能力（规划、推理、任务执行）及其改进方法**，而非其应用领域的具体成果。这与您的研究课题“LLM智能体及其演化”中的“单智能体”和“自我演化（改进）”方向高度契合。因此，这篇论文应该被保留。"
    },
    {
        "index": "#107",
        "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training",
        "link": "/arxiv/2510.14969",
        "arxiv_id": "2510.14969",
        "authors": "Yiming Wang, Da Yin, Yuedong Cui, Ruichen Zheng, Zhiqian Li, Zongyu Lin, Di Wu, Xueqing Wu, Chenchen Ye, Yu Zhou, Kai-Wei Chang",
        "summary": "Digital agents require diverse, large-scale UI trajectories to generalize across real-world tasks, yet collecting such data is prohibitively expensive in both human annotation, infra and engineering perspectives. To this end, we introduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates structured UI states and transitions to synthesize training trajectories at scale. Our paradigm integrates a digital world simulator for diverse UI states, a guided rollout process for coherent exploration, and a trajectory wrapper that produces high-quality and diverse trajectories for agent training. We further propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that enables more rapid and data-efficient scaling by prioritizing high-impact tasks and synthesizes informative trajectory variants. Experiments on WebArena and AndroidWorld show that UI-Simulator rivals or surpasses open-source agents trained on real UIs with significantly better robustness, despite using weaker teacher models. Moreover, UI-Simulator-Grow matches the performance of Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model, highlighting the potential of targeted synthesis scaling paradigm to continuously and efficiently enhance the digital agents.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.033919",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接命中了“LLM智能体及其演化”中的“自我演化”方向。 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM智能体应用到一个新领域，而是提出了一种新的方法论来**训练和演化**数字智能体。其核心贡献是`UI-Simulator`框架和`UI-Simulator-Grow`策略，它们的目标是解决智能体训练中的数据瓶颈，从而让智能体本身变得更强大、更高效。这完全符合“构建、改进或演化LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文内容高度契合您的核心关注点。 - **核心范式**: 论文聚焦于`LLM-based Agents`的训练与演化。 - **演化机制**: 论文的亮点`UI-Simulator-Grow`被明确描述为一种“targeted scaling strategy”（有针对性的扩展策略），其目的是“continuously and efficiently enhance the digital agents”（持续且高效地增强数字智能体）。这直接对应了`Self-Improvement`、`Iterative Improvement`和`Generational Evolution`等演化机制。实验结果（8B模型通过该策略达到70B模型性能）是其演化能力的有力证明。 3.  **第三步：排除标准** - 论文不涉及任何关于安全、对齐、可解释性或水印的研究。 - 论文虽然涉及UI（用户界面），但这属于智能体与数字世界交互的感知部分。论文的核心贡献**不是**一个新的视觉或多模态模型，而是**如何生成高质量的交互轨迹来训练智能体**。因此，这符合“视觉被用作智能体感知环境的工具，而不是研究的核心”这一例外情况，不应被排除。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化”方向的典型范例。它的核心贡献就是提出一种新的“自我演化”机制（通过`UI-Simulator-Grow`策略进行数据高效的迭代训练），即使其应用场景是数字UI领域，也完全符合您设定的“保留”规则。这恰恰是您所寻找的，提出新机制以实现智能体自我完善的研究。 **结论**: 该论文的核心贡献是提出了一种可扩展的、用于合成训练数据的模拟器范式（`UI-Simulator`）和一种高效的迭代扩展策略（`UI-Simulator-Grow`）。这套方法论旨在通过改善训练过程来**持续、高效地演化（增强）数字智能体**的能力，使较小的模型能够达到更大模型的性能。这完全契合您研究课题中“自我演化”的核心焦点，因此应被保留。"
    },
    {
        "index": "#109",
        "title": "Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents",
        "link": "/arxiv/2510.14967",
        "arxiv_id": "2510.14967",
        "authors": "Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying",
        "summary": "Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.034590",
        "filter_reason": "这篇论文完全符合你的研究范围，应予保留。我的判断过程如下： 1.  **第一步：核心判断（保留）** 论文的核心贡献是提出了一种名为 **IGPO（Information Gain-based Policy Optimization）** 的强化学习框架，其目标是**改进和训练多轮LLM智能体**。它不是将智能体作为工具去解决某个特定领域的问题，而是专注于智能体本身的学习机制和性能优化。这直接命中了你筛选标准中的“保留”条件：“论文的核心是关于构建、改进或演化LLM智能体的方法论或新框架”。 2.  **第二步：正面指标（高度相关）** 论文包含了大量你的核心关注点： *   **核心范式**：`LLM-based Agents` 是论文的绝对核心。 *   **智能体能力**：论文明确提到了`Tool Use`（工具使用）、`Multi-Turn Reasoning`（多轮推理，属于`Planning`范畴）。其提出的IGPO框架通过提供密集的、逐轮的奖励，本质上是在促进智能体的`Self-Improvement`（自我改进）和更有效的学习。 *   **演化机制**：整个IGPO框架就是一种**自我演化/自我优化**的机制。它通过内在的、基于信息增益的奖励信号，引导智能体的策略在多轮交互中不断迭代和优化，这完全符合你“自我演化”的研究方向。 3.  **第三步：排除标准（未触发）** 论文的研究焦点是提升智能体的决策效率和准确性，完全没有涉及安全、对齐、可解释性或幻觉等问题。同时，它也不涉及任何多模态内容。因此，它没有触发任何排除标准。 4.  **第四步：特殊和模糊情况（符合保留规则）** 论文的研究内容属于典型的“推理/规划”范畴，并且完全符合保留规则。它不是在研究如何提升LLM本身的基础数学或逻辑能力，而是在研究**一个智能体如何在与环境的多轮交互中进行有效的规划和学习**。这与ReAct、ToT等Agentic框架一脉相承，其贡献在于解决了这类框架在训练中遇到的“奖励稀疏”和“信用分配”等核心难题。 **核心依据总结**： 这篇论文的本质是**提出一种新的训练算法（IGPO）来让LLM智能体在与环境的多轮交互中学得更好、迭代得更快**。这精准地命中了你“构建、改进或演化LLM智能体”的核心目标，特别是“单智能体”的规划/工具使用能力和“自我演化”的迭代优化能力。它不是应用，而是对智能体底层学习机制的深刻改进，是典型的Agentic AI前沿研究。因此，这篇论文高度相关，应当入选。"
    },
    {
        "index": "#108",
        "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
        "link": "/arxiv/2510.14968",
        "arxiv_id": "2510.14968",
        "authors": "Mingxuan Yan, Yuping Wang, Zechun Liu, Jiachen Li",
        "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at rdd-neurips.github.io.",
        "subjects": "Robotics, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning, Systems and Control",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.034250",
        "filter_reason": "这篇论文符合筛选要求。我的判断过程如下： 1.  **第一步：核心判断** - 这篇论文的本质是**改进LLM智能体的核心能力**。它没有将一个已有的智能体框架简单应用到机器人领域，而是针对现有分层智能体框架中的一个关键瓶颈——规划器（VLM Planner）与底层策略的对齐问题——提出了一种新的解决方案（RDD）。 - 该方案的核心是**自动化的任务分解**，这是智能体在处理长时程复杂任务时进行**规划**的关键环节。因此，这篇论文的核心贡献是关于如何构建一个更优的智能体规划模块，属于“构建、改进LLM智能体”的范畴，应予以**保留**。 - 它不属于“非演化型应用”，因为其贡献是方法论层面的改进，而非应用落地。 2.  **第二步：正面指标分析** - 论文高度符合核心关注点。其核心范式是**Agentic AI**，具体实现为一个基于VLM的分层智能体。 - 论文的核心能力是**Planning**（规划），并深入到了规划的子问题——**Decomposition**（分解）。这与您关注点中的“规划”完全匹配。 3.  **第三步：排除标准分析** - **安全与对齐**：论文不涉及此方面。 - **多模态与视觉**：这是本案例的关键点。论文确实使用了VLM和视觉信息，但它符合排除标准中的例外情况：“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在这篇论文中，视觉信息是规划器用来对齐底层策略的**信号和工具**，论文的**研究核心**是“如何利用这个工具进行更好的任务分解算法设计”，而不是如何改进视觉理解能力本身。因此，不应被排除。 4.  **第四步：特殊和模糊情况处理** - **推理/规划**：论文完全符合“保留”条件。它研究的是智能体如何进行多步规划和推理（将长时程任务分解为子任务），并提出了一种新的Agentic框架（RDD）来优化这一过程。 5.  **第五步：最终决策** - 综合来看，这篇论文的核心贡献是提出了一种新颖的、用于改进智能体规划能力的方法论（RDD）。尽管它的应用场景和验证平台是机器人视觉，但其研究焦点和本质是Agentic AI中的规划问题，与您“筛选核心贡献在于构建、改进或演化LLM智能体”的目标高度契合。因此，最终判断为符合要求。"
    },
    {
        "index": "#113",
        "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
        "link": "/arxiv/2510.14943",
        "arxiv_id": "2510.14943",
        "authors": "Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, Yankai Lin",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.035875",
        "filter_reason": "这篇论文符合您的研究范围，核心判断依据如下： 1.  **核心贡献分析 (第一步)**: 论文的核心贡献是提出了一种名为LaSeR的新算法。该算法的本质不是简单地将LLM应用于某个领域，也不是提升LLM的基础数学或逻辑能力，而是**构建了一种让LLM进行自我评估和自我奖励的机制**。这种机制使LLM在生成解决方案后，能够立即通过自身最后一个token的概率分布来评估自己答案的质量。这属于构建和改进LLM智能体核心能力的方法论，因此应予以保留。 2.  **与研究焦点的匹配度 (第二步)**: *   **单智能体**: 论文的核心是`Self-Rewarding`（自我奖励）和`Self-Verification`（自我验证）。这与您研究焦点中的“自我反思”和“自我修正”高度契合。一个能够自主评估自身输出质量的模型，是构建更复杂智能体（如ReAct循环中的“观察”和“反思”环节）的关键基础。 *   **自我演化**: 论文通过强化学习训练模型获得“自我奖励能力”，并将其用于提升推理时的性能。这可以被视为一种**自我完善**的形式。模型学会了如何评判自己，这是实现迭代改进和自我演化的前提条件。它符合“自我完善”和“迭代改进”的演化机制。 3.  **排除标准的适用性 (第三步)**: *   论文的主要贡献不是关于安全、对齐、可解释性或多模态，因此不触及排除标准。 4.  **特殊情况的判断 (第四步)**: *   **推理/规划**: 这篇论文恰好处于“保留”的范畴。它不是在研究如何让LLM更好地解数学题（非Agentic的推理），而是在研究**如何让LLM在推理后进行自我验证**。这种自我验证是智能体在复杂任务中进行多步推理和决策时不可或缺的一环。它为智能体的“反思”步骤提供了一种新颖、高效的实现方案。 **总结**: LaSeR论文的核心是提出了一种创新的训练算法，用于赋予LLM自我评估和奖励的能力。这直接对应了您研究目标中的“构建、改进LLM智能体”，特别是聚焦于“单智能体”的“自我反思/修正”能力和“自我演化”的“自我完善”机制。它不是简单的应用或基础能力提升，而是一个关键的智能体组件和能力的创新，因此完全符合您的筛选要求。"
    },
    {
        "index": "#129",
        "title": "Agentic NL2SQL to Reduce Computational Costs",
        "link": "/arxiv/2510.14808",
        "arxiv_id": "2510.14808",
        "authors": "Dominik Jehle, Lennart Purucker, Frank Hutter",
        "summary": "Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL) has recently been empowered by large language models (LLMs). Using LLMs to perform NL2SQL methods on a large collection of SQL databases necessitates processing large quantities of meta-information about the databases, which in turn results in lengthy prompts with many tokens and high processing costs. To address this challenge, we introduce Datalake Agent, an agentic system designed to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing direct solvers for NL2SQL that call the LLM once with all meta-information in the prompt, the Datalake Agent employs an interactive loop to reduce the utilized meta-information. Within the loop, the LLM is used in a reasoning framework that selectively requests only the necessary information to solve a table question answering task. We evaluate the Datalake Agent on a collection of 23 databases with 100 table question answering tasks. The Datalake Agent reduces the tokens used by the LLM by up to 87\\% and thus allows for substantial cost reductions while maintaining competitive performance.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.051922",
        "filter_reason": "这篇论文符合筛选要求，应被保留。 以下是根据筛选标准进行的详细判断过程： 1.  **第一步：核心判断** - **保留**。论文的核心贡献并非简单地将LLM应用于NL2SQL领域，而是提出了一种名为“Datalake Agent”的**新型智能体系统**。其本质是构建了一个方法论框架，通过一个交互式循环和推理机制来优化任务执行过程。这完全符合“构建、改进LLM智能体”的核心目标。它不属于“非演化型应用”，因为论文的重点是这个“Agent”本身的设计和工作原理，而不是它在NL2SQL任务上的最终表现数据。 2.  **第二步：正面指标** - 论文标题和摘要中明确包含了核心范式关键词：`Agentic`。 - 论文描述了一个`reasoning framework`（推理框架）和`interactive loop`（交互式循环），这与`ReAct`范式高度相似，属于智能体的核心能力。 - 智能体“selectively requests only the necessary information”（选择性地请求必要信息），这明确体现了`Tool Use / Tool Augmentation`（工具使用）能力，即智能体使用一个工具来查询数据库元信息。 - 整个过程涉及智能体根据当前状态决定下一步行动，这隐含了`Planning`（规划）能力。 - 这些指标都强烈指向“单智能体”研究方向。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态技术。其核心是智能体的架构和效率优化，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**：这篇论文是关于智能体如何进行规划和推理的典型案例。它没有停留在改进LLM的基础推理能力，而是构建了一个让LLM在循环中进行多步推理和行动的框架。这完全符合“保留”的条件，即“关于智能体如何进行规划或在复杂任务中进行多步推理”。 **最终决策**： 综合以上分析，这篇论文的核心贡献是提出了一种新的LLM智能体框架（Datalake Agent），该框架通过交互式循环和工具使用来降低计算成本。它直接对“单智能体”研究方向的“规划”和“工具使用”子方向做出了贡献。因此，这篇论文与“LLM智能体及其演化”的研究课题高度相关，应被**保留**。"
    },
    {
        "index": "#158",
        "title": "CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization",
        "link": "/arxiv/2510.14150",
        "arxiv_id": "2510.14150",
        "authors": "Henrique Assumpção, Diego Ferreira, Leandro Campos, Fabricio Murai",
        "summary": "In this work, we introduce CodeEvolve, an open-source evolutionary coding agent that unites Large Language Models (LLMs) with genetic algorithms to solve complex computational problems. Our framework adapts powerful evolutionary concepts to the LLM domain, building upon recent methods for generalized scientific discovery. CodeEvolve employs an island-based genetic algorithm to maintain population diversity and increase throughput, introduces a novel inspiration-based crossover mechanism that leverages the LLMs context window to combine features from successful solutions, and implements meta-prompting strategies for dynamic exploration of the solution space. We conduct a rigorous evaluation of CodeEvolve on a subset of the mathematical benchmarks used to evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that our method surpasses AlphaEvolve's performance on several challenging problems. To foster collaboration and accelerate progress, we release our complete framework as an open-source repository.",
        "subjects": "Artificial Intelligence, Machine Learning, Neural and Evolutionary Computing",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.066126",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献直指“LLM智能体及其演化”中的“自我演化”方向。我的判断过程如下： **第一步：核心判断——论文的本质是构建和演化LLM智能体。** - **保留**: 论文的核心是提出一个名为 \"CodeEvolve\" 的**演化型编码智能体**。它不是一个简单的应用，而是构建了一个全新的**方法论框架**，该框架将LLM与遗传算法相结合，以实现智能体的自我演化。其核心贡献在于机制的创新，而非应用本身。 - **排除项分析**: 1.  **非演化型应用**: 虽然论文应用于算法发现和优化领域，但其重点在于描述智能体**如何**通过演化来发现和优化算法，而不是简单地用LLM解决一个领域问题。 2.  **非Agentic的推理**: 论文的焦点不是提升LLM的基础推理能力，而是构建一个更高层级的智能体系统，通过演化迭代来生成和改进解决方案。 3.  **基础设施**: 论文不涉及模型部署或硬件加速。 **第二步：正面指标——论文高度契合核心关注点。** - **核心范式**: 论文明确包含了 `LLM-based Agents`, `Self-Evolving`, `Evolutionary Algorithms` (遗传算法)。 - **演化机制**: 论文的全部内容都围绕 `Self-Improvement` 和 `Generational Evolution` 展开。它引入了**“新颖的基于灵感的交叉机制”**和**“元提示策略”**，这些都是具体的、创新的自我演化实现方式。 **第三步：排除标准——论文不在排除范围内。** - **安全与对齐**: 论文未涉及安全、对齐、可解释性等内容。 - **多模态与视觉**: 论文专注于代码和算法生成，不涉及视觉或多模态模型。 **第四步：处理特殊和模糊情况——完美符合“自我演化的应用”例外规则。** - **自我演化的应用**: 这篇论文是第四步规则中提到的**例外情况的典型范例**。规则指出：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域（如‘用于化学实验的自我演化智能体’），也应该保留。” - **核心机制**: \"CodeEvolve\" 的核心贡献正是其**新颖的自我演化机制**（island-based genetic algorithm, inspiration-based crossover, meta-prompting）。论文将这些机制应用于“算法发现和优化”这一特定领域，但其价值在于这些机制本身的可推广性和创新性，而非在特定领域的应用效果。 **第五步：最终决策** 综合以上分析，这篇论文的核心是构建一个能够通过遗传算法和自身生成能力进行迭代优化、自我演化的LLM智能体框架。它直接命中了你研究方向的第三个核心——“自我演化”，并且其贡献在于提出新的演化机制，而非简单应用。因此，这篇论文是高度相关且必须保留的前沿研究。"
    },
    {
        "index": "#24",
        "title": "Code-driven Number Sequence Calculation: Enhancing the inductive Reasoning Abilities of Large Language Models",
        "link": "/arxiv/2510.14620",
        "arxiv_id": "2510.14620",
        "authors": "Kedi Chen, Zhikai Lei, Xu Guo, Xuecheng Wu, Siyuan Zeng, Jianghao Yin, Yinqi Zhang, Qin Chen, Jie Zhou, Liang He, Qipeng Guo, Kai Chen, Wei Zhang",
        "summary": "Large language models (LLMs) make remarkable progress in reasoning tasks. Among different reasoning modes, inductive reasoning, due to its better alignment with human learning, attracts increasing interest. However, research on inductive reasoning faces certain challenges. First, existing inductive data mostly focuses on superficial regularities while lacking more complex internal patterns. Second, current works merely prompt LLMs or finetune on simple prompt-response pairs, but do not provide precise thinking processes nor implement difficulty control. Unlike previous work, we address these challenges by introducing \\textit{CodeSeq}, a synthetic post-training dataset built from number sequences. We package number sequences into algorithmic problems to discover their general terms, defining a general term generation (GTG) task correspondingly. Our pipeline generates supervised finetuning data by reflecting on failed test cases and incorporating iterative corrections, thereby teaching LLMs to learn autonomous case generation and self-checking. Additionally, it leverages reinforcement learning with a novel Case-Synergy Solvability Scaling Reward based on both solvability, estimated from the problem pass rate, and the success rate of self-directed case generation, enabling models to learn more effectively from both successes and failures. Experimental results show that the models trained with \\textit{CodeSeq} improve on various reasoning tasks and can preserve the models' OOD performance.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.151608",
        "filter_reason": "这篇论文的核心贡献符合我的研究范围，具体判断依据如下： 1.  **第一步：核心判断**： -   论文的核心不是简单地将LLM应用到一个特定领域，也不是在优化基础设施。其核心是提出了一套新的**训练方法论**（`CodeSeq`数据集及其生成流程），旨在让LLM通过特定机制提升自身能力。 -   该方法论的关键在于它**不是**简单的监督微调（SFT），而是引入了一个包含“反思失败测试用例”和“迭代修正”的循环。这已经超出了单纯提升基础推理能力的范畴，触及了模型如何从自身错误中学习和改进的机制。 -   因此，这篇论文的本质是关于**改进LLM能力的方法论**，且该方法论包含了自我演化的元素，应初步判断为**保留**。 2.  **第二步：正面指标**： -   论文摘要中明确包含了多个与“自我演化”高度相关的关键词： -   `Self-Reflection`：“by **reflecting on failed test cases**” -   `Self-Refine` / `Iterative Improvement`：“and incorporating **iterative corrections**” -   `Self-Improvement`：“teaching LLMs to learn **autonomous case generation and self-checking**” -   “enabling models to learn more effectively from both successes and failures”这直接描述了通过经验进行自我完善的演化过程。 -   这些正面指标强烈表明，论文的核心贡献与“自我演化”方向高度契合。 3.  **第三步：排除标准**： -   论文的主要贡献不涉及安全、对齐、可解释性，也不涉及多模态或视觉。因此，没有触发任何明确的排除标准。 4.  **第四步：处理特殊和模糊情况**： -   **推理/规划**：这篇论文的情况介于两者之间，但更偏向于“保留”的一方。它虽然目标是为了提升“归纳推理”，但其实现方式并非设计一个新的CoT变体或提供一个更高质量的数据集那么简单。它构建了一个**训练过程中的自我演化循环**（反思-修正-再尝试），这是一种元能力的提升，属于智能体自我演化的范畴。它不是直接教模型“如何推理”，而是教模型“**如何通过反思和迭代来学会更好地推理**”。 -   **自我演化的应用**：这正是本论文的关键所在。根据筛选规则“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域（如‘用于化学实验的自我演化智能体’），也应该保留”。本文的核心贡献正是提出了一套新的自我演化机制（基于代码生成、测试失败反思和迭代强化的训练流程），并将其应用在“归纳推理”这个任务上。因此，它完全符合这一保留规则。 5.  **第五步：最终决策**： -   综合分析，这篇论文的核心贡献是提出了一种新颖的、能够让LLM通过反思自身错误和迭代修正来提升特定能力的**自我演化训练框架**。虽然它没有构建一个完整意义上的、与环境交互的自主智能体，但其方法论的核心是“自我演化”，这直接命中了我研究目标的第三个方向。因此，这篇论文高度相关，应该保留。"
    },
    {
        "index": "#29",
        "title": "Natural Language Tools: A Natural Language Approach to Tool Calling In Large Language Agents",
        "link": "/arxiv/2510.14453",
        "arxiv_id": "2510.14453",
        "authors": "Reid T. Johnson, Michelle D. Pain, Jordan D. West",
        "summary": "We present Natural Language Tools (NLT), a framework that replaces programmatic JSON tool calling in large language models (LLMs) with natural language outputs. By decoupling tool selection from response generation, NLT eliminates task interference and format constraints that degrade tool call performance. When evaluated across 10 models and 6,400 trials spanning customer service and mental health domains, NLT improves tool calling accuracy by 18.4 percentage points while reducing output variance by 70%. Open-weight models see the largest gains, surpassing flagship closed-weight alternatives, with implications for model training in both reinforcement learning and supervised fine-tuning stages. These improvements persist under prompt perturbations and extend tool-calling capabilities to models lacking native support.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.159796",
        "filter_reason": "这篇论文完全符合研究范围，应被保留。 **判断过程如下:** 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为“自然语言工具”的新**框架**，其核心目标是**改进**大型语言智能体的**工具调用能力**。这完全符合“构建、改进或演化LLM智能体”的核心目标。它不是将智能体作为工具去解决某个特定领域的问题，而是聚焦于优化智能体本身的一个核心机制（工具调用），因此不属于“非演化型应用”。同时，它也不是关于提升LLM基础推理能力或基础设施的研究。 2.  **第二步：正面指标** - 论文高度匹配核心关注点。其标题和摘要明确提到了 `Large Language Agents` 和 `Tool Calling`，直接命中了研究焦点中的**单智能体**方向，特别是**工具使用**这一核心能力。论文提出的NLT框架是对现有工具使用方法（如JSON格式调用）的改进，属于对智能体能力的实质性增强。 3.  **第三步：排除标准** - 论文不涉及任何排除标准。其研究焦点是提升工具调用的准确性和效率，而非安全、对齐、可解释性或多模态视觉问题。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 该论文属于应**保留**的情况。工具使用是智能体进行复杂规划和多步推理（如ReAct框架）的关键环节。通过改进工具调用机制，该论文直接增强了智能体在复杂任务中执行计划的能力，因此属于“关于智能体如何进行规划或在复杂任务中进行多步推理”的范畴。 **最终决策:** 该论文的核心贡献是提出了一种改进LLM智能体“工具使用”能力的新框架。这直接对应了研究课题中“单智能体”方向下的一个关键子方向。论文的工作是方法论层面的创新，旨在提升智能体本身的能力，而非将其作为应用工具。因此，这篇论文与研究范围高度相关，应被筛选为**True**。"
    },
    {
        "index": "#30",
        "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents",
        "link": "/arxiv/2510.14438",
        "arxiv_id": "2510.14438",
        "authors": "Rui Wang, Ce Zhang, Jun-Yu Ma, Jianshu Zhang, Hongru Wang, Yi Chen, Boyang Xue, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu, Kam-Fai Wong",
        "summary": "Deep research web agents not only retrieve information from diverse sources such as web environments, files, and multimodal inputs, but more importantly, they need to rigorously analyze and aggregate knowledge for insightful research. However, existing open-source deep research agents predominantly focus on enhancing information-seeking capabilities of web agents to locate specific information, while overlooking the essential need for information aggregation, which would limit their ability to support in-depth research. We propose an Explore to Evolve paradigm to scalably construct verifiable training data for web agents. Begins with proactive online exploration, an agent sources grounded information by exploring the real web. Using the collected evidence, the agent then self-evolves an aggregation program by selecting, composing, and refining operations from 12 high-level logical types to synthesize a verifiable QA pair. This evolution from high-level guidance to concrete operations allowed us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K websites and 11 domains. Based on an open-source agent framework, SmolAgents, we collect supervised fine-tuning trajectories to develop a series of foundation models, WebAggregator. WebAggregator-8B matches the performance of GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text and closely approaches Claude-3.7-sonnet. Moreover, given the limited availability of benchmarks that evaluate web agents' information aggregation abilities, we construct a human-annotated evaluation split of WebAggregatorQA as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves 28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all references, they still struggle on WebAggregatorQA, highlighting the need to strengthen the information aggregation capabilities of web agent foundations.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.160452",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献直接命中了“LLM智能体及其演化”中的“自我演化”和“单智能体”方向。 **判断过程分析如下:** 1.  **第一步：核心判断 (保留)** *   论文的核心贡献是提出了一种名为 **“Explore to Evolve”** 的新范式。这个范式的本质不是简单应用现有智能体，而是构建一个能让智能体**自我演化**其内部逻辑（聚合程序）的方法论。智能体通过“主动在线探索”获取信息，然后通过“选择、组合和提炼操作”来**自我演化**一个聚合程序。这完全符合“构建、改进或演化 LLM智能体”的核心要求。 *   它不属于“非演化型应用”，因为其核心创新点在于演化机制本身，而非将智能体应用于某个领域。 *   它不属于“非Agentic的推理”，因为它研究的是智能体如何通过规划和工具使用（探索网络）来完成复杂任务，并在此过程中进行自我完善。 2.  **第二步：正面指标 (高度匹配)** *   **核心范式**: 论文标题和摘要中明确提到了 `Self-Evolving` 和 `Agentic AI` (`Deep Research Agents`)。 *   **智能体能力**: 论文涉及 `Planning` (通过 proactive online exploration 规划信息获取路径)、`Tool Use` (将网络环境作为工具)、以及 `Self-Improvement` / `Self-Refine` (通过 refining operations 来演化聚合程序)。 *   **演化机制**: 论文的标题和核心思想就是 `Self-Evolving`，具体实现为 `Iterative Improvement` (从高层指导到具体操作的演化)。 3.  **第三步：排除标准 (未触发)** *   论文的主要贡献是关于提升智能体的能力（信息聚合），而非安全、对齐或可解释性。 *   虽然提到了 `multimodal inputs`，但研究的核心是文本和网页信息的聚合逻辑，视觉等多模态只是作为信息来源之一，并非研究的核心。因此，不触发多模态排除标准。 4.  **第四步：处理特殊和模糊情况 (符合保留规则)** *   **推理/规划**: 论文提出的“Explore to Evolve”范式是一个完整的智能体框架，指导智能体如何进行多步推理（探索 -> 聚合 -> 生成答案），这属于应保留的“智能体如何进行规划”的范畴。 *   **自我演化的应用**: 这篇论文是“自我演化应用”的完美范例，但其核心是提出了一种**新的自我演化机制**（“Explore to Evolve”范式）。根据你的规则，即使它被应用在“深度研究”这个特定领域，也应该被保留。 **最终决策:** 这篇论文的核心贡献是提出了一种让LLM智能体通过与环境交互（探索网络）来**自我演化**其核心能力（信息聚合逻辑）的创新框架。这直接对应了你研究目标中的“自我演化”方向，并涉及“单智能体”的规划和工具使用能力。它不是简单的应用，而是对智能体本身构建和演化方法的根本性探索。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#46",
        "title": "PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering",
        "link": "/arxiv/2510.14278",
        "arxiv_id": "2510.14278",
        "authors": "Md Mahadi Hasan Nahid, Davood Rafiei",
        "summary": "Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG -- demonstrates that our approach consistently outperforms strong baselines.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.171438",
        "filter_reason": "根据您提供的筛选标准，这篇论文完全符合您的研究范围。我的判断过程如下： **第一步：核心判断——这篇论文的本质是什么？** - **保留**。这篇论文的本质不是简单地将LLM作为工具应用于问答领域，而是**构建了一种全新的Agentic框架（PRISM）**来解决多跳问答中的检索问题。其核心贡献是这个由多个专门化智能体组成的系统架构和它们之间的协作流程，而不是问答任务本身的结果。这完全符合“构建、改进或演化LLM智能体”的核心目标。 **第二步：正面指标——论文是否包含我的核心关注点？** 该论文高度契合您的核心关注点，包含了多个关键正面指标： - **核心范式**: 论文明确提出了一个`Agentic Retrieval System`，并且其由三个智能体构成，这直接命中了`Agentic AI`和`LLM-based Agents`，并且触及了`Multi-Agent Systems (MAS)`的范畴。 - **智能体能力**: - `Planning`: 论文中的 `Question Analyzer` 智能体负责将复杂的多跳问题**分解 (decomposes)** 为子问题，这是典型的规划能力。 - `Tool Use`: `Selector` 和 `Adder` 智能体执行的是信息检索任务，可以看作是使用检索工具来获取上下文信息。 - **多智能体**: - `Collaboration`: `Selector` 和 `Adder` 之间通过**迭代交互** 来共同构建最终的证据集合，这是一种明确的协作形式。 - `Communication`: 智能体之间通过传递检索结果和状态信息进行隐式通信，以指导下一步的行动。 **第三步：排除标准——是否为我的研究焦点之外？** - 该论文完全不涉及安全与对齐（Safety, Alignment等）相关的主题。 - 该论文是纯文本任务，没有涉及多模态或视觉（Vision, MLLMs等）内容。 因此，论文没有触及任何排除标准。 **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的规划（问题分解）是由`Question Analyzer`这个智能体在一个更大的Agentic框架内执行的，其目的是为了驱动后续智能体的行动。这完全符合“保留”的条件，即“关于智能体如何进行规划或在复杂任务中进行多步推理”，而不是单纯提升LLM本身的基础推理能力。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种新颖的**多智能体协作框架**，通过**规划**（问题分解）和**智能体间的迭代协作**来提升复杂任务的性能。它直接推动了LLM智能体在设计和协作模式上的边界，与您研究的“单智能体（规划）”和“多智能体（协作）”两个方向高度相关。因此，这篇论文是您应该保留的前沿研究。"
    },
    {
        "index": "#54",
        "title": "DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans",
        "link": "/arxiv/2510.14205",
        "arxiv_id": "2510.14205",
        "authors": "Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang",
        "summary": "The emerging large language model role-playing agents (LLM RPAs) aim to simulate individual human behaviors, but the persona fidelity is often undermined by manually-created profiles (e.g., cherry-picked information and personality characteristics) without validating the alignment with the target individuals. To address this limitation, our work introduces the Dynamic Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM RPAs' behaviors with those of target individuals by iteratively identifying the cognitive divergence, either through free-form or theory-grounded, structured analysis, between generated behaviors and human ground truth, and refining the persona profile to mitigate these divergences.We evaluate DPRF with five LLMs on four diverse behavior-prediction scenarios: formal debates, social media posts with mental health issues, public interviews, and movie reviews.DPRF can consistently improve behavioral alignment considerably over baseline personas and generalizes across models and scenarios.Our work provides a robust methodology for creating high-fidelity persona profiles and enhancing the validity of downstream applications, such as user simulation, social studies, and personalized AI.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.179603",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献精准地落在“自我演化”方向上。 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM智能体作为工具应用，而是提出了一种全新的方法论框架（DPRF）来**改进和演化**LLM智能体。它的核心是解决“如何让智能体变得更好”的问题，而不是“如何用智能体解决某个领域问题”。因此，它通过了第一步的核心判断。 2.  **第二步：正面指标** - 论文高度匹配你的核心关注点。其标题和摘要中明确包含了： - **核心范式**: `LLM-based Agents` (LLM角色扮演智能体)。 - **演化机制**: `Dynamic Persona Refinement` (动态人设精炼) 本质上就是一种 `Self-Refine` (自我精炼) 和 `Self-Improvement` (自我完善) 的机制。其工作方式是“iteratively identifying... divergences... and refining the persona profile” (迭代地识别分歧并精炼人设档案)，这正是 `Iterative Improvement` (迭代改进) 的体现。 - **智能体能力**: 虽然没有直接出现 `Self-Reflection`，但“识别认知分歧”的过程可以被理解为一种结构化的自我反思，即智能体（在框架的辅助下）反思其输出与目标之间的差距。 3.  **第三步：排除标准** - 论文虽然提到了 \"Optimizing Behavior Alignment\" (优化行为对齐)，但这里的“对齐”是指智能体的行为与**特定目标个体**的行为对齐（\"alignment with the target individuals\"），目的是为了实现高保真度的角色扮演（\"high-fidelity persona\"）。这不同于你筛选标准中要排除的、关于宏观伦理和安全的 `Alignment` (对齐)。因此，它不属于排除范围。 - 论文不涉及安全、多模态等排除领域。 4.  **第四步：处理特殊和模糊情况** - 论文是“自我演化的应用”的完美范例。它提出了一种新的“自我演化”机制（DPRF框架），并将其应用在辩论、社交媒体等场景中进行评估。根据你的规则，**“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域，也应该保留。”** DPRF的核心贡献正是这个动态精炼框架，而不是它在某个领域的应用效果。 **最终决策**: 这篇论文的核心贡献是构建了一个名为DPRF的框架，通过迭代地识别和修正行为偏差，使LLM角色扮演智能体能够动态地自我完善其人设，从而更逼真地模拟目标个体。这完全符合你研究目标中的“自我演化”方向，即“智能体通过经验、反思或环境反馈进行自我完善和迭代”。因此，这篇论文应当被保留。"
    },
    {
        "index": "#73",
        "title": "FACTS: Table Summarization via Offline Template Generation with Agentic Workflows",
        "link": "/arxiv/2510.13920",
        "arxiv_id": "2510.13920",
        "authors": "Ye Yuan, Mohammad Amin Shabani, Siqi Liu",
        "summary": "Query-focused table summarization requires generating natural language summaries of tabular data conditioned on a user query, enabling users to access insights beyond fact retrieval. Existing approaches face key limitations: table-to-text models require costly fine-tuning and struggle with complex reasoning, prompt-based LLM methods suffer from token-limit and efficiency issues while exposing sensitive data, and prior agentic pipelines often rely on decomposition, planning, or manual templates that lack robustness and scalability. To mitigate these issues, we introduce an agentic workflow, FACTS, a Fast, Accurate, and Privacy-Compliant Table Summarization approach via Offline Template Generation. FACTS produces offline templates, consisting of SQL queries and Jinja2 templates, which can be rendered into natural language summaries and are reusable across multiple tables sharing the same schema. It enables fast summarization through reusable offline templates, accurate outputs with executable SQL queries, and privacy compliance by sending only table schemas to LLMs. Evaluations on widely-used benchmarks show that FACTS consistently outperforms baseline methods, establishing it as a practical solution for real-world query-focused table summarization.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.199477",
        "filter_reason": "这篇论文符合您的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。尽管论文的应用领域是“表格摘要”，这似乎是一个特定领域的应用，但其**核心贡献并非简单地应用已有智能体框架**。论文明确指出现有智能体管道的局限性（“缺乏鲁棒性和可扩展性”），并提出了一个全新的“智能体工作流”来解决这个问题。因此，论文的本质是**构建和改进一个LLM智能体的方法论**，而不是将智能体作为工具应用于新领域。这完全符合您“核心贡献在于构建、改进LLM智能体”的目标。 2.  **第二步：正面指标** - 论文摘要中明确包含了核心范式关键词 `Agentic Workflows`。 - 其实现方式（生成SQL查询和Jinja2模板）是典型的**工具使用**能力的体现。智能体通过调用外部工具（SQL执行器、模板渲染器）来完成任务，这属于单智能体能力范畴。 3.  **第三步：排除标准** - 论文主要关注点不在于安全、对齐或多模态，因此不触及排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文提出的FACTS工作流，通过生成离线模板来处理复杂的查询式摘要任务，这本身就是一种高级的规划和执行策略。它不是在提升LLM的基础Token预测能力，而是在构建一个能让LLM有效完成多步任务的智能体框架。因此，这符合保留条件。 **综合判断**: 该论文的核心是提出一种名为FACTS的**新型智能体工作流**，旨在解决现有智能体在特定任务（表格摘要）中的不足。它的贡献在于**改进了智能体的架构和执行流程**，特别是通过引入离线模板生成和工具使用（SQL、Jinja2）来提升效率、准确性和隐私合规性。这完全符合您研究课题中“单智能体”方向下的“构建、改进”以及“工具使用”等子方向。虽然它以一个具体应用为验证场景，但其核心价值在于方法论创新，而非应用本身。因此，这篇论文高度相关，应被保留。"
    },
    {
        "index": "#59",
        "title": "ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models",
        "link": "/arxiv/2510.14077",
        "arxiv_id": "2510.14077",
        "authors": "Haziq Mohammad Khalid, Athikash Jeyaganthan, Timothy Do, Yicheng Fu, Sean O'Brien, Vasu Sharma, Kevin Zhu",
        "summary": "Large Language Models (LLMs) suffer significant performance degradation in multi-turn conversations when information is presented incrementally. Given that multi-turn conversations characterize everyday interactions with LLMs, this degradation poses a severe challenge to real world usability. We hypothesize that abrupt increases in model uncertainty signal misalignment in multi-turn LLM interactions, and we exploit this insight to dynamically realign conversational context. We introduce ERGO (Entropy-guided Resetting for Generation Optimization), which continuously quantifies internal uncertainty via Shannon entropy over next token distributions and triggers adaptive prompt consolidation when a sharp spike in entropy is detected. By treating uncertainty as a first class signal rather than a nuisance to eliminate, ERGO embraces variability in language and modeling, representing and responding to uncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO yields a 56.6% average performance gain over standard baselines, increases aptitude (peak performance capability) by 24.7%, and decreases unreliability (variability in performance) by 35.3%, demonstrating that uncertainty aware interventions can improve both accuracy and reliability in conversational AI.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.182031",
        "filter_reason": "这篇论文的核心贡献在于提出了一种名为ERGO的新机制，用于提升LLM在多轮对话中的性能和可靠性。以下是根据您提供的筛选标准进行的详细判断过程： 1.  **第一步：核心判断** - **保留 (Keep)**。论文的本质并非将LLM作为工具应用于某个特定领域，也不是单纯提升LLM的基础推理能力（如数学、逻辑）。其核心是构建了一个**方法论（ERGO）**，用于解决LLM在多轮交互中一个普遍存在的性能衰减问题。这个方法论通过监测模型内部的“不确定性”（熵）并动态调整其“上下文”（Prompt Consolidation），本质上是在**改进LLM在复杂交互环境中的行为能力**。这可以被视为对LLM智能体核心组件（特别是记忆和状态管理）的一种改进和优化，符合“构建、改进LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文虽然没有直接使用`Agent`或`Planning`等高频词，但其内容与多个核心关注点高度相关： - **`Memory` (记忆)**：ERGO的核心功能是动态管理对话上下文，这直接关系到智能体的长期记忆能力。当上下文过长或信息矛盾导致不确定性升高时，ERGO通过“重置”或“整合”来优化记忆状态，防止智能体“遗忘”或“混乱”。 - **`Self-Correction` (自我纠正)**：ERGO可以被看作是一种内生的、基础的自我纠正机制。智能体通过感知内部状态（熵的飙升）判断自身性能即将下降，并主动采取措施（整合上下文）来纠正这种趋势，从而维持对话的连贯性和准确性。 - **`Self-Reflection` (自我反思)**：虽然ERGO不是高层次的目标导向反思，但它实现了对自身内部不确定性的“表征和响应”，是自我反思的初级形态。它让模型能够“意识到”自己可能出错了，并进行调整。 3.  **第三步：排除标准** - 论文不涉及安全、对齐、可解释性等排除主题。 - 论文是纯文本模型研究，不涉及多模态，因此不触及相关排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**：ERGO不属于“排除”类别中的“非Agentic的推理”。它不是解决一个静态的数学或逻辑问题，而是优化**动态的、多轮的交互过程**。这种对交互过程的监控和调整，是智能体在环境中持续行动和适应的关键能力，因此符合保留条件。 - **自我演化的应用**：ERGO的机制是在单次对话中进行的动态适应，而非跨代际的自我演化。但它的核心贡献是一种**改进机制**，因此即使它被应用于某个领域（如客服机器人），也应因其方法论创新而被保留。 **最终决策**：综合分析，这篇论文的核心贡献是提出了一种通过内部不确定性信号来动态优化多轮对话上下文的新方法。该方法直接解决了LLM作为智能体在长期交互中面临的核心挑战——记忆管理和状态维护。它通过一种内生的、自动化的方式提升了智能体的鲁棒性和可靠性，可以被视为对智能体“记忆”和“自我纠正”能力的有效增强。因此，这篇论文完全符合您关于“LLM智能体及其演化”的研究范围。"
    },
    {
        "index": "#77",
        "title": "Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms",
        "link": "/arxiv/2510.13913",
        "arxiv_id": "2510.13913",
        "authors": "Shrey Pandit, Xuan-Phi Nguyen, Yifei Ming, Austin Xu, Jiayu Wang, Caiming Xiong, Shafiq Joty",
        "summary": "Web-based 'deep research' agents aim to solve complex question - answering tasks through long-horizon interactions with online tools. These tasks remain challenging, as the underlying language models are often not optimized for long-horizon reasoning and exploration. Prior work has proposed workflows for constructing instruction-tuning datasets, often leveraging knowledge graphs. However, such methods typically lack fine-grained control over difficulty and quality, yielding synthetic data that falls short of capturing the complexity required for long-horizon reasoning. Furthermore, many studies conflate data and training effects by comparing models trained under different optimization recipes, making it difficult to isolate and evaluate the effectiveness of the data itself. We introduce a two-pronged data synthesis pipeline that generates question - answer pairs by progressively increasing task complexity until a frontier baseline web agent fails. The baseline agent plays multiple roles in this process: attempting the questions, validating factuality, checking for alternative answers, and enforcing filtering. To evaluate the effectiveness of our synthesis methods, we adopt a controlled training setup based on distillation from strong web agents. Experiments across multiple web-based benchmarks show that our dataset - despite being smaller - enables the training of more effective web agents than existing datasets. In particular, our data exhibits twice the diversity in tool-use actions, allowing models trained on it to achieve stronger performance while avoiding repetitive tool-calling behaviors.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.201522",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接聚焦于“构建、改进或演化 LLM智能体”。 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将现有智能体作为工具去解决某个特定领域（如金融、医疗）的问题，而是提出了一种全新的**方法论**来**构建和改进**LLM智能体本身。其核心贡献是一个“数据合成管道”，专门用于生成能够训练出更强大Web智能体的数据。这直接命中了“构建、改进或演化 LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `Agentic AI`, `LLM-based Agents`。论文标题和摘要明确指出研究对象是“Web Agents”。 - **智能体能力**: `Tool Use / Tool Augmentation`。摘要多次提到智能体与“online tools”的交互，并强调其数据能带来“twice the diversity in tool-use actions”。 - **演化机制**: `Iterative Improvement`。论文的核心创新点“Progressive Difficulty Enhancement Mechanisms”（渐进式难度增强机制）是一种典型的迭代改进方法。它通过逐步增加任务复杂度，直到一个基线智能体失败，来生成高质量、高难度的训练数据。这是一种在**数据层面**驱动智能体演化的机制。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态等排除领域。它完全聚焦于提升智能体的任务执行能力。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文关注的是智能体在“long-horizon reasoning and exploration”（长时程推理和探索）任务中的表现，这属于智能体规划能力的范畴。其提出的数据合成方法正是为了优化这种Agentic推理能力，而非提升LLM的基础数学或逻辑能力，因此符合保留条件。 - **自我演化的应用**: 论文提出的“渐进式难度增强”本身就是一种新颖的演化机制，即使它被应用在Web Agent这个特定领域，根据筛选规则也应保留。实际上，这篇论文的贡献是普适性的方法论，而非特定应用。 **总结**: 该论文的核心贡献是提出了一种通过**迭代式地增加数据难度**来**合成高质量智能体训练数据**的新方法。这种方法论直接服务于**构建和改进LLM智能体**（特别是单智能体）的目标，其“渐进式难度增强”机制可以被视为一种在数据层面的**自我演化**策略。因此，这篇论文与您的研究课题高度相关，应被保留。"
    },
    {
        "index": "#82",
        "title": "LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization",
        "link": "/arxiv/2510.13907",
        "arxiv_id": "2510.13907",
        "authors": "Yuanchen Wu, Saurabh Verma, Justin Lee, Fangzhou Xiong, Poppy Zhang, Amel Awadelkarim, Xu Chen, Yubai Yuan, Shawndra Hill",
        "summary": "Large language models (LLMs) are highly sensitive to their input prompts, making prompt design a central challenge. While automatic prompt optimization (APO) reduces manual engineering, most approaches assume access to ground-truth references such as labeled validation data. In practice, however, collecting high-quality labels is costly and slow. We propose the Prompt Duel Optimizer (PDO), a sample-efficient framework for label-free prompt optimization. PDO formulates the problem as a dueling-bandit setting, where supervision signal comes from pairwise preference feedback provided by an LLM judge. The framework combines Double Thompson Sampling (D-TS), which prioritizes informative prompt comparisons, with Top-Performer Guided Mutation, which expands the candidate pool by mutating high-performing prompts. PDO naturally operates in label-free settings and can also incorporate partial labels to mitigate judge noise. Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently outperforms baseline methods. Ablation studies further demonstrate the effectiveness of both D-TS and prompt mutation.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.209076",
        "filter_reason": "这篇论文符合您的研究范围，其核心贡献属于“自我演化”方向。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 该论文的核心是提出一种名为“Prompt Duel Optimizer (PDO)”的框架，用于自动、高效地优化LLM的提示。其方法论包含两个关键部分：1) 使用LLM裁判进行两两比较以获得反馈信号；2) 对表现优异的提示进行“变异”以生成新的候选提示。 - **判断**: 这篇论文的本质不是将LLM作为工具去解决某个特定领域的问题（非演化型应用），也不是提升LLM的基础推理能力（非Agentic的推理）。相反，它提出了一种**迭代改进机制**。虽然它没有直接构建一个执行任务的智能体（如规划或使用工具），但它构建了一个**用于优化智能体核心“指令”（即Prompt）的元框架**。这个框架通过反馈和变异进行迭代，本质上是**一种自我演化的方法论**。因此，我初步判断为**保留 (Keep)**。 2.  **第二步：正面指标** - 论文摘要中明确包含了多个与我核心关注点高度相关的正面指标： - **核心范式**: `Self-Evolving` (自我演化)，`Evolutionary Algorithms` (其“变异”机制是演化算法的核心思想)。 - **演化机制**: `Self-Improvement` (自我改进)，`Self-Refine` (自我精炼)，`Iterative Improvement` (迭代改进)。 - 这些指标强烈表明，论文的研究焦点与您的“自我演化”方向高度契合。 3.  **第三步：排除标准** - 论文的研究内容不涉及安全、对齐、可解释性 或多模态视觉。因此，没有触发任何硬性排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这是判断的关键。根据筛选标准第四步的特殊情况处理规则2：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域，也应该保留。” 本论文的核心贡献正是PDO这一**新的自我演化机制**（通过LLM裁判的反馈和提示变异来迭代优化）。它被应用在BIG-bench Hard等通用基准上，而非特定领域，因此完全符合这一保留规则。 - **推理/规划**: 该论文不涉及此特殊情况，因为它既不是关于智能体在任务中的规划，也不是提升LLM的数学逻辑能力。 5.  **第五步：最终决策** - 综合以上分析，尽管这篇论文没有构建一个传统意义上具备规划、工具使用能力的单智能体或多智能体系统，但它的核心贡献是提出了一种**新颖的、迭代的、基于反馈的自我演化框架**。这个框架的目标是优化LLM智能体的核心组件——Prompt，从而提升智能体的整体能力。这与您研究目标中的“自我演化”方向完全一致，属于构建和改进LLM智能体的基础性方法论研究。因此，最终判断为**符合要求**。"
    },
    {
        "index": "#95",
        "title": "Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production",
        "link": "/arxiv/2510.13879",
        "arxiv_id": "2510.13879",
        "authors": "Alexandre Galashov, Matt Jones, Rosemary Ke, Yuan Cao, Vaishnavh Nagarajan, Michael C. Mozer",
        "summary": "We explore a class of supervised training objectives that allow a language model to dynamically and autonomously scale the number of compute steps used for each input token. For any token, the model can request additional compute steps by emitting a <don't know> output. If the model is granted a delay, a specialized <pause> token is inserted at the next input step, providing the model with additional compute resources to generate an output. The model can request multiple pauses. To train the model to use <don't know> outputs judiciously and to calibrate its uncertainty, we frame the selection of each output token as a sequential-decision problem with a time cost. We refer to the class of methods as $\\textit{Catch Your Breath}$ losses and we study three methods in this class: CYB-AP frames the model's task as anytime prediction, where an output may be required at any step and accuracy is discounted over time; CYB-VA is a variational approach that aims to maximize prediction accuracy subject to a specified distribution over stopping times; and CYB-DP imposes a penalty based on a computational budget. Through fine-tuning experiments, we identify the best performing loss variant. The CYB model needs only one third as much training data as the baseline (no pause) model needs to achieve the same performance, and half as much data as a model with pauses and a cross-entropy loss. We find that the CYB model requests additional steps when doing so improves accuracy, and the model adapts its processing time to token-level complexity and context. For example, it often pauses after plural nouns like $\\textit{patients}$ and $\\textit{challenges}$ but never pauses after the first token of contracted words like $\\textit{wasn}$ and $\\textit{didn}$, and it shows high variability for ambiguous tokens like $\\textit{won}$, which could function as either a verb or part of a contraction.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.213012",
        "filter_reason": "这篇论文符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种名为 \"Catch Your Breath\" (CYB) 的新型训练目标。该方法允许语言模型在生成序列时，**自主地**决定何时需要更多的计算资源来处理一个困难的token。它通过让模型发出`<don't know>`信号来请求“暂停”，并将这个过程构建为一个**序列决策问题**。 - **判断**: 这不是将LLM作为工具应用到特定领域，也不是关于模型基础设施。虽然它提升了LLM的基础生成能力，但其方法的核心是引入了一个**自主决策机制**。模型不再是被动地、匀速地处理每个token，而是学会了根据自身的不确定性和任务的复杂性来**自我调节**其计算过程。这种自主性和自我调节能力是Agentic AI的一个基本特征。因此，这篇论文的本质是关于**构建和改进LLM智能体的内部机制**，应**保留**。 2.  **第二步：正面指标** - 论文的核心思想与多个正面指标高度相关： - **自我演化**: 论文的核心是让模型通过训练学会一种自我完善和自我调节的策略。它学会了在何时“停下来思考”，这是一种内在的演化过程。 - **自我反思/自我修正**: 模型发出`<don't know>`信号，本质上是对自身当前状态（“我无法确定下一个token”）的一种**反思**，并主动请求资源进行**修正**或深化思考。 - **规划**: 虽然不是复杂任务的外部规划，但模型在token级别上进行了一种**元规划**——规划自己的计算资源分配。这种“序列决策”的框架与智能体的规划思想一脉相承。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态等领域。因此，没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这是本案例的关键。论文确实在提高LLM的基础推理能力，但其方式并非简单的数据增强或非Agentic的微调。它提出的是一个**新的Agentic框架**，让模型学习一个关于“如何进行推理”的**策略**。这符合“保留”条件：“如果论文是关于智能体如何进行规划或在复杂任务中进行多步推理（如 ReAct、ToT 或新的Agentic框架）”。CYB框架可以被看作是一种非常底层的、嵌入在生成过程中的Agentic推理框架。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一种让LLM实现**自适应计算**和**自我调节**的新方法。它通过将token生成建模为序列决策问题，赋予了模型一种基础的**自主性**和**元认知能力**（即知道自己何时“不知道”）。虽然它不涉及工具使用或多智能体交互，但它直接触及了智能体“自我演化”和“自我反思”的核心机制，是构建更高级智能体的重要基础。因此，这篇论文精准地符合您关于“LLM智能体及其演化”的研究目标，特别是“自我演化”和“单智能体”方向。"
    },
    {
        "index": "#137",
        "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence",
        "link": "/arxiv/2510.14670",
        "arxiv_id": "2510.14670",
        "authors": "Marco Simoni, Aleksandar Fontana, Andrea Saracino, Paolo Mori",
        "summary": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph. It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence. Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses. To support training and evaluation, we introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test: 13951) pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations. Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.",
        "subjects": "Artificial Intelligence, Computation and Language, Cryptography and Security, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.251792",
        "filter_reason": "这篇论文符合筛选标准，应当保留。判断过程和核心依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一个名为 **TITAN 的新框架**，而非简单地将现有模型应用于特定领域。该框架包含一个“路径规划器模型”和一个“图执行器”。这构成了一个具备规划和行动能力的系统，其本质是构建了一个LLM智能体来解决特定类型的复杂推理任务。因此，它不属于“非演化型应用”或“非Agentic的推理”的排除范畴。它提出了关于“如何构建一个能进行图推理的智能体”的方法论。 2.  **正面指标匹配（第二步）：** 论文与我的核心关注点高度匹配，特别是“单智能体”方向。 *   **核心范式**: 论文描述的 TITAN 框架是一个典型的 `LLM-based Agent`。 *   **智能体能力**: 论文明确并直接涉及了两个核心的智能体能力： *   **`Planning` (规划)**: “路径规划器模型”的核心功能就是根据自然语言查询预测一个逻辑关系链，这正是智能体的规划能力。 *   **`Tool Use` (工具使用)**: “图执行器”作为一个外部工具，被智能体用来在结构化的知识图谱上执行操作并获取信息。整个“规划器-执行器”的结构非常符合 ReAct (Reason+Act) 的范式。 3.  **排除标准检查（第三步）：** 论文的研究领域是“网络威胁情报”，虽然涉及“安全”，但其**主要贡献是构建一个新的推理框架，而不是提出一种新的安全防护、对齐或可解释性方法**。因此，它不属于安全与对齐的排除范围。论文也未涉及多模态或视觉内容。 4.  **特殊情况分析（第四步）：** *   **推理/规划**: 这正是论文的核心。它不是在提升LLM基础的单步逻辑推理能力，而是在构建一个**外部系统框架**，让智能体能够进行多步、可执行的、有工具辅助的复杂规划与推理。这完全符合“保留”的条件。 *   **自我演化的应用**: 论文不涉及自我演化机制。 **结论:** 尽管论文的应用领域是“网络威胁情报”，但其**核心贡献是提出了一种具备规划和工具使用能力的LLM智能体新框架（TITAN）**。该框架通过“路径规划器”实现了智能体的规划能力，通过“图执行器”实现了工具使用能力，并整合成类似ReAct的多步推理循环。这直接回应了研究课题中关于“构建”和“改进”LLM智能体，特别是其规划与工具使用能力的核心目标。因此，这篇论文是高度相关的，应当保留。"
    },
    {
        "index": "#139",
        "title": "Just-In-Time Objectives: A General Approach for Specialized AI Interactions",
        "link": "/arxiv/2510.14591",
        "arxiv_id": "2510.14591",
        "authors": "Michelle S. Lam, Omar Shaikh, Hallie Xu, Alice Guo, Diyi Yang, Jeffrey Heer, James A. Landay, Michael S. Bernstein",
        "summary": "Large language models promise a broad set of functions, but when not given a specific objective, they default to milquetoast results such as drafting emails littered with cliches. We demonstrate that inferring the user's in-the-moment objective, then rapidly optimizing for that singular objective, enables LLMs to produce tools, interfaces, and responses that are more responsive and desired. We contribute an architecture for automatically inducing just-in-time objectives by passively observing user behavior, then steering downstream AI systems through generation and evaluation against this objective. Inducing just-in-time objectives (e.g., \"Clarify the abstract's research contribution\") enables automatic generation of tools, e.g., those that critique a draft based on relevant HCI methodologies, anticipate related researchers' reactions, or surface ambiguous terminology. In a series of experiments (N=14, N=205) on participants' own tasks, JIT objectives enable LLM outputs that achieve 66-86% win rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT objectives produce specialized tools unique to each participant.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.252445",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM作为工具应用到一个特定领域，而是提出了一种全新的**架构**来改进LLM与用户的交互方式。其核心贡献是“通过被动观察用户行为来自动诱导即时目标，然后引导下游AI系统”。这本质上是在构建一个更智能、更具适应性的LLM智能体框架，使其能够自主地理解和执行动态目标。这完全符合“构建、改进LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: 论文提出的架构是一种典型的 `Agentic AI` 框架。 - **智能体能力**: - **`Planning`**: 论文的核心机制“推断用户的即时目标”是一种高级的、动态的规划能力。智能体不再是执行预设指令，而是根据环境（用户行为）实时生成目标。 - **`Tool Use / Tool Augmentation`**: 摘要中明确提到，该架构能够“自动生成工具”，例如“根据相关HCI方法论批判草稿的工具”。这不仅是使用工具，更是**创造工具**，是工具使用能力的重大演进。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态。它的焦点是提升智能体的能力和交互的专门性，因此没有触碰到任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 该论文的研究属于“保留”范畴。它不是在改进LLM的基础数学或逻辑推理，而是在构建一个**智能体框架**，让智能体能够进行更复杂的、面向任务的规划和行动（即设定即时目标并生成相应工具）。 5.  **第五步：最终决策** - 综合分析，该论文的核心贡献是提出了一种创新的LLM智能体架构。该架构通过动态目标设定和自动工具生成，显著增强了智能体的自主性和适应性。这直接命中了您研究范围中的“单智能体”方向，特别是在“规划”和“工具使用”这两个子方向上做出了明确的、方法论的贡献。因此，这篇论文是高度相关且应该被保留的前沿研究。"
    },
    {
        "index": "#136",
        "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
        "link": "/arxiv/2510.14846",
        "arxiv_id": "2510.14846",
        "authors": "Zhuo-Yang Song",
        "summary": "The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via a majority-vote instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.",
        "subjects": "Artificial Intelligence, Computation and Language, Logic in Computer Science",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.251493",
        "filter_reason": "这篇论文完全符合您的研究范围，应该被保留。以下是根据您的筛选标准进行的详细判断过程： **第一步：核心判断——这篇论文的本质是什么？** - **判断结果**: 保留 (Keep)。 - **核心依据**: 这篇论文的核心贡献并非将LLM智能体应用于某个特定领域，而是提出了一个**“紧凑的形式化理论”**来**“描述和衡量”**由LLM驱动的迭代搜索过程。它的本质是构建一个用于理解和分析LLM智能体（特别是其搜索和规划能力）的理论框架和度量工具。这直接关系到“**构建、改进或演化 LLM智能体的方法论**”，完全符合您的核心目标。 **第二步：正面指标——论文是否包含我的核心关注点？** - **判断结果**: 高度符合。 - **核心依据**: 论文中包含了大量您关注的核心范式和能力： - **核心范式**: 论文围绕 `LLM-based Agents` 展开，并深入探讨了 `generate-filter-refine` 这一**迭代范式**，这与 `Self-Evolving` 和 `Iterative Improvement` 的思想高度一致。 - **智能体能力**: 论文的核心是研究智能体的**多步推理和搜索**，并提出了 `coverage generating function` 和 `measure of reachability difficulty` 等概念来量化这一过程。这直接触及了智能体的 `Planning` 和 `Self-Correction`（在filter-refine阶段体现）能力。 - **演化机制**: `generate-filter-refine` 本身就是一种迭代改进和自我完善的机制。论文旨在为这种机制提供理论基础，使其更可衡量和优化，属于 `Self-Improvement` 和 `Iterative Improvement` 的范畴。 **第三步：排除标准——是否为我的研究焦点之外？** - **判断结果**: 未触发排除标准。 - **核心依据**: - **安全与对齐**: 论文中提到了 \"safety envelope\"，但从摘要上下文（\"capture feasible transitions\"）来看，这是一个数学上的**约束边界或可行域**，用来定义智能体操作的合法范围，而非人工智能伦理层面的 `Safety` 或 `Alignment` 研究。论文的主要贡献是理论框架，而非安全机制。 - **多模态与视觉**: 论文未涉及任何视觉或多模态内容。 **第四步：处理特殊和模糊情况 (核心规则)** - **判断结果**: 符合保留规则。 - **核心依据**: - **推理/规划**: 这篇论文是“**保留**”的典型案例。它不是在研究如何提升LLM本身的基础数学或逻辑能力，而是在研究**“智能体如何进行规划和在复杂任务中进行多步推理”**。其提出的“形式化理论”正是对智能体规划/搜索过程的一种新描述和度量框架。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是为LLM智能体的迭代搜索行为提供了一个全新的、形式化的理论框架和度量方法。它深入探讨了智能体的规划、多步推理和迭代改进（自我演化的一种形式）等核心能力，与您的研究课题“LLM智能体及其演化”高度契合，属于该领域内非常前沿和基础性的研究工作。因此，最终判断为 **True**。"
    },
    {
        "index": "#127",
        "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning",
        "link": "/arxiv/2510.14958",
        "arxiv_id": "2510.14958",
        "authors": "Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li",
        "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.243611",
        "filter_reason": "这篇论文符合筛选标准，应予保留。以下是基于五大步骤的详细判断过程： **第一步：核心判断——论文的本质是什么？** - **核心贡献**: 论文的核心贡献是提出了 `MathCanvas` 这个**综合框架**，用于训练大型多模态模型（LMM）使其具备**内在的、策略性的视觉辅助推理能力**。 - **判断依据**: 这篇论文并非简单地将一个已有的LLM或智能体框架应用于数学领域。它的核心创新在于**方法论本身**——一个包含预训练和微调两阶段的完整框架，旨在**教会模型一项新技能**：自主决定何时生成/编辑视觉图表来辅助其数学推理过程。这种“学习何时及如何使用工具（内在的图表生成能力）”的能力，是构建高级智能体的关键一环。因此，它不属于“非演化型应用”的排除范围。 - **与“非Agentic的推理”的区别**: 论文研究的不是通过微调数据集来提升LLM的纯粹数学计算或文本推导能力。它关注的是一个更高级的、包含**行动**的推理循环：模型需要**规划**其解题步骤，在某个节点**决策**（“何时”）使用视觉辅助，然后**执行**（“如何”）图表的生成与编辑，最后基于这个视觉输出继续推理。这完全符合智能体规划与工具使用的定义。 **第二步：正面指标——论文是否包含我的核心关注点？** - **核心范式**: 论文虽未直接使用 `Agentic AI` 或 `Multi-Agent Systems` 等词，但其构建的 `MathCanvas` 框架本质上是在增强模型的Agentic能力。 - **智能体能力**: - **`Planning`**: 论文明确提到“Strategic Visual-Aided Reasoning”（战略性视觉辅助推理），并训练模型“teaching it when and how to leverage visual aids”（教导它何时以及如何利用视觉辅助）。这正是智能体规划和策略决策能力的体现。 - **`Tool Use / Tool Augmentation`**: 这是本文最相关的亮点。图表生成和编辑被模型内化为一种**工具**。研究重点就是让智能体掌握这个工具并策略性地使用它，这与`Tool Use`的定义高度契合，且是一种更高级的、内生的工具使用形式。 - **演化机制**: 虽然没有直接使用 `Self-Evolving` 等词，但模型通过在两个阶段的训练中迭代地掌握新技能（从生成到策略性使用），可以被看作是一种能力上的**迭代提升**，符合演化的广义精神。 **第三步：排除标准——是否为我的研究焦点之外？** - **安全与对齐**: 论文内容完全不涉及 `Safety`, `Alignment`, `Hallucination` 等议题。 - **多模态与视觉**: 这是本文最需要辨析的一点。虽然论文核心是多模态（`LMMs`, `Vision`），但它并未触犯排除规则。规则指出：“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在本论文中，视觉能力（图表生成）**正是智能体用来解决问题和进行推理的工具**，而论文研究的**核心**就是智能体如何掌握和策略性地使用这个工具。研究的焦点是**Agentic Process**，而非多模态模型本身。因此，它属于排除规则的例外情况。 **第四步：处理特殊和模糊情况** - **推理/规划**: 如第一步所述，本文的研究内容完美符合“保留”条件：“如果论文是关于智能体如何进行规划或在复杂任务中进行多步推理”。MathCanvas框架教给模型的正是一种包含行动（生成图表）的多步、策略性推理方法。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是构建了一个名为 `MathCanvas` 的新框架，用于赋予LLM智能体一种内生化的、策略性的工具使用能力（生成/编辑图表）以辅助复杂推理。它虽然以数学为应用场景，但其贡献在于**智能体构建的方法论**，而非应用本身。论文重点探讨了智能体的**规划**和**工具使用**能力，完全契合“单智能体”研究方向的核心。因此，这篇论文与您关于“LLM智能体及其演化”的研究课题高度相关，应当被**保留 (Keep)**。"
    },
    {
        "index": "#7",
        "title": "The Gatekeeper Knows Enough",
        "link": "/arxiv/2510.14881",
        "arxiv_id": "2510.14881",
        "authors": "Fikresilase Wondmeneh Abebayew",
        "summary": "Large Language Models (LLMs) are increasingly deployed as autonomous agents, yet their practical utility is fundamentally constrained by a limited context window and state desynchronization resulting from the LLMs' stateless nature and inefficient context management. These limitations lead to unreliable output, unpredictable behavior, and inefficient resource usage, particularly when interacting with large, structured, and sensitive knowledge systems such as codebases and documents. To address these challenges, we introduce the Gatekeeper Protocol, a novel, domain-agnostic framework that governs agent-system interactions. Our protocol mandates that the agent first operate and reason on a minimalist, low-fidelity \"latent state\" representation of the system to strategically request high-fidelity context on demand. All interactions are mediated through a unified JSON format that serves as a declarative, state-synchronized protocol, ensuring the agent's model of the system remains verifiably grounded in the system's reality. We demonstrate the efficacy of this protocol with Sage, a reference implementation of the Gatekeeper Protocol for software development. Our results show that this approach significantly increases agent reliability, improves computational efficiency by minimizing token consumption, and enables scalable interaction with complex systems, creating a foundational methodology for building more robust, predictable, and grounded AI agents for any structured knowledge domain.",
        "subjects": "Artificial Intelligence, Information Theory",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.281758",
        "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **第一步：核心判断 (Keep)** 论文的核心贡献是提出了一种名为“守门人协议”的**新框架**，旨在解决LLM智能体在交互时面临的根本性挑战（上下文窗口限制和状态失同步）。这完全符合“核心贡献在于构建、改进LLM智能体的方法论或新框架”这一保留标准。它不是将现有智能体应用在某个领域，而是提出了一种让智能体本身变得更可靠、更高效的基础性方法论。 2.  **第二步：正面指标 (高度匹配)** 论文包含了多个你的核心关注点： *   **核心范式**: 论文明确讨论了“自主智能体”，并提出了一种构建更稳健AI智能体的“基础方法论”，这与 `Agentic AI` 和 `LLM-based Agents` 范式高度一致。 *   **智能体能力**: *   **记忆 (Memory)**: 论文的核心创新点之一就是解决状态失同步问题。它通过“统一的JSON格式”作为“状态同步的协议”，并使用“潜在状态”表示，本质上是在设计一种更高效、更可靠的外部记忆和工作记忆管理机制。这是智能体研究中的关键问题。 *   **规划 (Planning)**: 摘要中提到，智能体在“潜在状态”上进行“操作和推理”，以“策略性地请求高保真度上下文”。这是一种典型的规划和信息检索策略，是智能体在复杂环境中有效行动的关键。 3.  **第三步：排除标准 (未命中)** 论文的主要焦点不是`安全与对齐`，也不是`多模态与视觉`。它的目标是提升智能体的`可靠性`、`效率`和`可预测性`，这些都属于智能体核心能力的范畴，而非安全或对齐研究。 4.  **第四步：特殊/模糊情况 (符合保留规则)** 论文讨论的推理（在“潜在状态”上推理以决定下一步操作）是紧密集成在智能体行动循环中的，是关于智能体如何与环境交互、如何做规划的，这正是需要保留的“智能体如何进行规划或在复杂任务中进行多步推理”的情况。它不是孤立地提升LLM的数学或逻辑能力。 **综合结论**: 这篇论文的核心是提出一种创新的框架（Gatekeeper Protocol）来解决LLM智能体的一个核心瓶颈：状态和记忆管理。它通过一种新颖的交互协议，直接改进了单智能体的可靠性和效率。这项工作完全属于“单智能体”研究范畴，尤其在**记忆**和**规划**两个子方向上做出了明确的、方法论的贡献。因此，它是一篇与你研究目标高度相关且应保留的前沿论文。"
    },
    {
        "index": "#2",
        "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning",
        "link": "/arxiv/2510.14942",
        "arxiv_id": "2510.14942",
        "authors": "Yao Zhang, Yu Wu, Haowei Zhang, Weiguo Li, Haokun Chen, Jingpei Wu, Guohao Li, Zhen Han, Volker Tresp",
        "summary": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.280168",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为 GroundedPRM 的新框架，用于自动构建高质量的过程奖励模型（PRM），以提升LLM的多步推理能力。根据筛选标准，这篇论文完全符合您的研究范围，具体判断过程如下： 1.  **第一步：核心判断——保留** 论文的本质是**构建和改进LLM智能体的方法论**，而非简单的应用。它没有将LLM作为黑盒工具去解决某个特定领域（如医疗、金融）的问题，而是聚焦于如何让LLM本身在推理过程中表现得更“智能”。GroundedPRM框架通过整合蒙特卡洛树搜索（MCTS）和外部工具验证，构建了一个增强LLM推理能力的Agentic框架。这完全符合“核心贡献在于构建、改进LLM智能体”的保留标准。 2.  **第二步：正面指标——高度相关** 论文包含了多个核心关注点： *   **智能体能力**: 论文的核心是解决复杂任务中的**多步推理**问题。其方法涉及两个关键的Agentic能力： *   **规划**: 使用**蒙特卡洛树搜索 (MCTS)** 来构建和探索结构化的推理路径，这是一种经典的智能体规划与搜索策略。 *   **工具使用**: 使用**外部工具**来验证中间步骤的正确性，提供了基于执行的真实反馈，这是智能体与外部环境交互并校准自身行为的核心能力。 *   **自我修正**: 通过外部工具验证，智能体可以发现并纠正推理路径中的错误步骤，这是一种隐式的自我修正机制。 3.  **第三步：排除标准——未触发** 论文的主要贡献是提升推理性能和自动化监督信号生成，而非安全、对齐或多模态。虽然摘要中提到“promote interpretability”，但这只是其设计带来的一个次要优点，并非论文的核心研究目标。因此，它不在排除标准之列。 4.  **第四步：处理特殊和模糊情况——符合保留条件** 这篇论文恰好处于“推理/规划”这一关键模糊情况的中心，但明确符合保留条件。 *   **保留**: 论文是关于**智能体如何进行规划和多步推理**的。它不是提出一个新的CoT提示词或者一个简单的微调方法，而是构建了一个包含规划（MCTS）和行动（工具验证）的完整**Agentic框架**。这种通过外部搜索和验证来引导LLM生成过程的方式，与ReAct、ToT等Agentic范式一脉相承，是对智能体推理能力的实质性改进。 *   **排除**: 论文并非仅仅关注提升LLM本身的基础Token预测能力（如数学逻辑）。它的创新点在于**LLM之外的机制**（MCTS和工具），这些机制协同LLM完成复杂任务，这正是Agentic AI研究的精髓。 **最终决策**: 综合以上分析，这篇论文提出了一种新颖的、结合了规划（MCTS）和工具使用的框架来增强LLM的推理能力。其核心贡献直接对应您研究焦点中的“单智能体”方向，特别是规划和工具使用这两个子方向。因此，这篇论文是高度相关且应该被**保留**的前沿研究。"
    },
    {
        "index": "#145",
        "title": "AI for Service: Proactive Assistance with AI Glasses",
        "link": "/arxiv/2510.14359",
        "arxiv_id": "2510.14359",
        "authors": "Zichen Wen, Yiyu Wang, Chenfei Liao, Boxue Yang, Junxian Li, Weifeng Liu, Haocong He, Bolong Feng, Xuyang Liu, Yuanhuiyi Lyu, Xu Zheng, Xuming Hu, Linfeng Zhang",
        "summary": "In an era where AI is evolving from a passive tool into an active and adaptive companion, we introduce AI for Service (AI4Service), a new paradigm that enables proactive and real-time assistance in daily life. Existing AI services remain largely reactive, responding only to explicit user commands. We argue that a truly intelligent and helpful assistant should be capable of anticipating user needs and taking actions proactively when appropriate. To realize this vision, we propose Alpha-Service, a unified framework that addresses two fundamental challenges: Know When to intervene by detecting service opportunities from egocentric video streams, and Know How to provide both generalized and personalized services. Inspired by the von Neumann computer architecture and based on AI glasses, Alpha-Service consists of five key components: an Input Unit for perception, a Central Processing Unit for task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit for long-term personalization, and an Output Unit for natural human interaction. As an initial exploration, we implement Alpha-Service through a multi-agent system deployed on AI glasses. Case studies, including a real-time Blackjack advisor, a museum tour guide, and a shopping fit assistant, demonstrate its ability to seamlessly perceive the environment, infer user intent, and provide timely and useful assistance without explicit prompts.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.254371",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献与你的研究目标高度一致。判断依据如下： 1.  **第一步：核心判断 (保留)** - 论文的核心贡献并非将现有智能体框架应用于某个领域，而是**提出了一种全新的、用于构建主动式AI助手的统一框架“Alpha-Service”**。这个框架本身是关于“如何构建”一个智能体，而不是“如何使用”一个智能体去解决特定领域问题。因此，它不属于“非演化型应用”的排除范畴。 - 论文明确提出了一个包含感知、规划、工具使用、记忆和交互的智能体架构，其本质是方法论和框架的创新，完全符合“构建、改进LLM智能体”的核心目标。 2.  **第二步：正面指标 (高度匹配)** - 论文包含了多个你的核心关注点： - **核心范式**: 论文通篇围绕构建一个主动的 `Agentic AI`，并且明确指出其实现方式是“通过一个**多智能体系统 (Multi-Agent system)**”，直接命中了你的两个核心研究方向。 - **智能体能力**: 摘要中描述的框架组件完美对应了你的研究焦点： - `Central Processing Unit for task scheduling` 对应 **`Planning`**。 - `Arithmetic Logic Unit for tool utilization` 对应 **`Tool Use / Tool Augmentation`**。 - `Memory Unit for long-term personalization` 对应 **`Memory`**。 - `anticicipating user needs and taking actions proactively` 体现了智能体的高级自主能力。 3.  **第三步：排除标准 (未触发)** - **安全与对齐**: 论文焦点是智能体的能力构建，不涉及安全、对齐等问题。 - **多模态与视觉**: 论文虽然提到了“AI glasses”和“egocentric video streams”，但视觉（`Input Unit`）在这里被明确定义为智能体**感知环境的工具**，其研究的核心是利用这个感知信息进行规划、决策和行动的整个智能体框架，而不是视觉模型本身。这完全符合“除非它们被用作智能体感知环境的工具，而不是研究的核心”的例外规则。 4.  **第四步：处理特殊和模糊情况 (符合保留规则)** - **推理/规划**: 论文中的“task scheduling”和“infer user intent”是典型的智能体规划和推理，属于应保留的范畴，而不是提升LLM基础Token预测能力的研究。 **综合结论**: 这篇论文的核心是提出一个名为“Alpha-Service”的智能体构建框架，该框架系统地集成了**规划、工具使用、记忆**等单智能体能力，并最终以一个**多智能体系统**的形式实现。虽然它以AI眼镜这一具体硬件为载体，并应用于购物、游戏等场景，但其根本贡献在于智能体架构的设计方法学。这精准地命中了你关于“单智能体”和“多智能体”的研究焦点，是一篇高质量的相关论文，应当保留。"
    },
    {
        "index": "#6",
        "title": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates",
        "link": "/arxiv/2510.14900",
        "arxiv_id": "2510.14900",
        "authors": "Wen-Kwang Tsao, Yao-Ching Yu, Chien-Ming Huang",
        "summary": "The Enterprise Intelligence Platform must integrate logs from numerous third-party vendors in order to perform various downstream tasks. However, vendor documentation is often unavailable at test time. It is either misplaced, mismatched, poorly formatted, or incomplete, which makes schema mapping challenging. We introduce a reinforcement learning agent that can self-improve without labeled examples or model weight updates. During inference, the agent: 1) Identifies ambiguous field-mapping attempts. 2) Generates targeted web-search queries to gather external evidence. 3) Applies a confidence-based reward to iteratively refine its mappings. To demonstrate this concept, we converted Microsoft Defender for Endpoint logs into a common schema. Our method increased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\% over 100 iterations using GPT-4o. At the same time, it reduced the number of low-confidence mappings requiring expert review by 85\\%. This new approach provides an evidence-driven, transparent method for solving future industry problems, paving the way for more robust, accountable, scalable, efficient, flexible, adaptable, and collaborative solutions.",
        "subjects": "Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.281466",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献精准地命中了“自我演化”这一研究方向。 以下是根据你的筛选标准进行的详细判断过程： 1.  **第一步：核心判断——保留** 论文的核心并非简单地将LLM应用于日志映射这一具体任务。它的本质贡献是提出了一种**新颖的、在测试时让智能体自我完善的强化学习框架**。这个框架使智能体能够在**不依赖标签数据、不更新模型权重**的情况下，通过与环境的交互（生成网络搜索查询）和内部反馈（基于置信度的奖励）来迭代式地提升性能。这完全符合“构建、改进或演化LLM智能体”的核心目标，特别是“自我演化”方向。因此，它不是“非演化型应用”，而是关于演化机制本身的研究。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了多个与你核心关注点高度相关的关键词和概念： *   **核心范式**: `LLM-based Agents`, `Self-Evolving`。 *   **智能体能力**: `Tool Use`（生成网络搜索查询收集外部证据）、`Self-Reflection`（识别模糊的字段映射尝试）、`Self-Improvement`（标题和摘要中反复强调的“improves”、“iteratively refine”）。 *   **演化机制**: `Iterative Improvement`（通过100次迭代提升准确率）。 3.  **第三步：排除标准——不适用** 论文的主要贡献是关于智能体的性能提升机制，而非安全、对齐或多模态。虽然提到了“transparent”（透明），但这是指其决策过程有外部证据支撑，而非以可解释性（XAI）为研究核心。 4.  **第四步：处理特殊和模糊情况——适用“自我演化的应用”例外规则** 这篇论文是“自我演化的应用”这一特殊情况的完美范例。虽然它的应用场景是具体的企业日志映射，但其核心贡献是提出了一种**通用的自我演化方法论**：一个通过测试时强化学习，利用工具和置信度奖励进行自我迭代的智能体框架。根据你的规则，“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域...也应该保留”，这篇论文显然应该被保留。 **最终决策**： 综合以上分析，这篇论文的核心贡献是构建了一个能够在测试时通过工具使用和自我反思进行迭代优化的自我演化LLM智能体。它提出的强化学习循环机制具有通用性，直接服务于“自我演化”这一研究焦点。因此，这篇论文与你的研究目标高度相关，应当被保留。"
    },
    {
        "index": "#8",
        "title": "LabOS: The AI-XR Co-Scientist That Sees and Works With Humans",
        "link": "/arxiv/2510.14861",
        "arxiv_id": "2510.14861",
        "authors": "Le Cong, Zaixi Zhang, Xiaotong Wang, Yin Di, Ruofan Jin, Michal Gerasimiuk, Yinkai Wang, Ravi K. Dinesh, David Smerkous, Alex Smerkous, Xuekun Wu, Shilong Liu, Peishan Li, Yi Zhu, Simran Serrao, Ning Zhao, Imran A. Mohammad, John B. Sunwoo, Joseph C. Wu, Mengdi Wang",
        "summary": "Modern science advances fastest when thought meets action. LabOS represents the first AI co-scientist that unites computational reasoning with physical experimentation through multimodal perception, self-evolving agents, and Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see what scientists see, understand experimental context, and assist in real-time execution. Across applications--from cancer immunotherapy target discovery to stem-cell engineering -- LabOS shows that AI can move beyond computational design to participation, turning the laboratory into an intelligent, collaborative environment where human and machine discovery evolve together.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.282215",
        "filter_reason": "这篇论文完全符合您的筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 论文的核心贡献是提出了一个名为 LabOS 的**新框架/系统**。这个系统并非简单地将现有LLM或Agent框架应用于科学领域，而是构建了一个集成了“多模态感知”、“自我演化智能体”和“人机协作”的AI-XR联合科学家。其核心在于**构建和改进智能体**，特别是强调了“自我演化智能体”这一关键组件。因此，它不属于“非演化型应用”的排除范畴。 - **判断结果**: **保留**。 2.  **第二步：正面指标** - 论文摘要中包含了多个与您研究焦点高度相关的核心范式和能力关键词： - **自我演化**: 明确提到了 `self-evolving agents`，直接命中您的第三个核心研究方向。 - **多智能体**: 提到了 `multi-model AI agents`，指向您的第二个核心研究方向。 - **智能体能力**: 系统通过 `multimodal perception` 来 `understand experimental context` 并 `assist in real-time execution`，这涉及到智能体的感知、规划和工具使用能力。 - **判断结果**: 包含多个强正面指标，相关性极高。 3.  **第三步：排除标准** - **安全与对齐**: 论文摘要未提及安全、对齐、可解释性等内容，不涉及排除标准。 - **多模态与视觉**: 摘要中提到了 `multimodal perception` 和 \"sees what scientists see\"。这是一个潜在的排除点，但根据筛选规则的补充说明“除非它们被用作智能体感知环境的工具，而不是研究的核心”，这里的视觉能力显然是作为智能体感知物理实验环境的**工具**，服务于整个Agentic框架的构建，而非论文的核心研究贡献。核心贡献是整个LabOS系统及其中的自我演化机制。 - **判断结果**: 未触发排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一特殊情况的完美范例。论文将一个核心为“自我演化智能体”的系统应用到了“癌症免疫疗法”和“干细胞工程”等特定科学领域。根据您的规则，“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。LabOS的核心正是其自我演化智能体架构，因此必须保留。 - **判断结果**: 符合保留的例外情况。 5.  **第五步：最终决策** - **综合分析**: 论文的核心贡献是构建一个名为LabOS的**新颖的、集成了自我演化机制和多智能体协作的AI智能体系统**。它直接命中了您研究范围的“自我演化”和“多智能体”两大方向。虽然它被应用于具体的科学实验场景，但这恰恰是验证其智能体框架有效性的方式，其核心贡献在于智能体本身的构建与演化方法论，而非科学应用本身。因此，这篇论文与您的研究目标高度契合。"
    },
    {
        "index": "#11",
        "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning",
        "link": "/arxiv/2510.14828",
        "arxiv_id": "2510.14828",
        "authors": "Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li",
        "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots to complete complex human instructions in long-view manipulation tasks successfully. Despite the success of large language models and vision language models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue facing challenges in performing long-horizon manipulation tasks in complex real-world environments, owing to their restricted common sense and reasoning capabilities. Considering that aligning general-purpose vision language models to robotic planning tasks via supervised fine-tuning suffers from poor generalization and insufficient physical understanding, we propose RoboGPT-R1, a two-stage fine-tuning framework for embodied planning. In this framework, supervised training acquires foundational knowledge through expert sequences, followed by RL to address the model's shortcomings in visual-spatial understanding and reasoning. To achieve physical understanding and action sequence consistency in multi-step reasoning tasks, we design a rule-based reward function that simultaneously considers long-horizon performance and action constraint in the environment. The reasoning model, trained on Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini, by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the EmbodiedBench benchmark.",
        "subjects": "Artificial Intelligence, Robotics",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.283160",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献是提出一个名为 `RoboGPT-R1` 的新框架，其本质是**构建和改进一个LLM智能体**。虽然应用领域是机器人技术，但论文的焦点并非“用LLM解决机器人问题”，而是“如何通过一个新颖的两阶段微调框架（SFT+RL）来**增强LLM智能体的规划能力**”。这完全符合“构建、改进LLM智能体的方法论或新框架”的保留标准。它不属于“非演化型应用”，因为其核心是提出一种改进智能体本身能力的方法。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点： - **核心范式**: `Agentic AI`, `LLM-based Agents` (论文研究的是具身智能体)。 - **智能体能力**: `Planning` (标题和摘要的核心)，其RL阶段可以看作是一种通过环境反馈实现的 `Self-Improvement` 或 `Self-Correction` 机制。 3.  **第三步：排除标准** - 论文不涉及安全、对齐等排除主题。 - 论文虽然使用了视觉语言模型 (`Qwen2.5-VL-3B`)，但符合排除标准中的例外情况。在这里，视觉是智能体**感知环境的工具**，是具身智能体不可或缺的一部分，但论文的研究核心并非VLM模型本身，而是如何训练这个模型成为一个更好的规划者。因此，不应被排除。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是关于智能体规划的典型案例。它不是在提升LLM的基础数学或逻辑能力，而是在研究一个**具身智能体如何在复杂、长视野的任务中进行规划和多步推理**。这完全符合“保留”的条件。 **最终决策**: 该论文的核心贡献在于提出了一种新的训练框架来**提升LLM智能体的规划能力**，这直接命中了研究范围中的“单智能体”方向，特别是“规划”这一子方向。尽管其应用场景是机器人，但其方法论贡献是普适于Agentic AI的。因此，这篇论文与你的研究目标高度相关，应被保留。"
    },
    {
        "index": "#143",
        "title": "IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning",
        "link": "/arxiv/2510.14406",
        "arxiv_id": "2510.14406",
        "authors": "Xikai Zhang, Bo Wang, Likang Xiao, Yongzhi Li, Quan Chen, Wenju Wu, Liu Liu",
        "summary": "Although large language models (LLMs) have made significant strides across various tasks, they still face significant challenges in complex reasoning and planning. For example, even with carefully designed prompts and prior information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on the TravelPlanner dataset in the sole-planning mode. Similarly, even in the thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent Systems (MAS) can offer improved collective reasoning, they often suffer from high reasoning costs due to multi-round internal interactions, long per-response latency, and difficulties in end-to-end training. To address these challenges, we propose a general and scalable framework called IMAGINE, short for Integrating Multi-Agent System into One Model. This framework not only integrates the reasoning and planning capabilities of MAS into a single, compact model, but also significantly surpass the capabilities of the MAS through a simple end-to-end training. Through this pipeline, a single small-scale model is not only able to acquire the structured reasoning and planning capabilities of a well-organized MAS but can also significantly outperform it. Experimental results demonstrate that, when using Qwen3-8B-Instruct as the base model and training it with our method, the model achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.253686",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接对应“构建、改进或演化LLM智能体”的目标。以下是详细的判断依据： 1.  **第一步：核心判断——论文的本质是构建和改进LLM智能体。** - **保留理由**: 论文的核心贡献是提出一个名为 **IMAGINE** 的新框架。这个框架的本质并非简单应用现有技术，而是提出了一种全新的方法论：**将多智能体系统（MAS）的协作推理能力“集成”或“蒸馏”到一个单一、高效的模型中**。这直接属于“构建、改进LLM智能体”的范畴。它解决了现有MAS系统（一种智能体形式）的固有缺陷（高成本、高延迟），从而创造出一种更优越的智能体架构。 - **排除项不适用**: 论文不是将LLM或MAS作为工具去解决一个特定领域问题（如旅行规划本身只是验证框架的基准），也不是关于非Agentic的基础推理能力提升，更不是基础设施研究。 2.  **第二步：正面指标——论文高度契合您的核心关注点。** - 论文摘要中明确包含了多个核心关键词：`Multi-Agent Systems (MAS)`、`Complex Reasoning`、`Planning`。 - 它直接涉及您的三个研究方向中的两个： - **多智能体**: 论文的起点和被改造的对象就是MAS，研究其协作推理能力的本质。 - **单智能体**: 论文的成果是一个单一、紧凑的模型，其核心能力是`Planning`（规划），这是单智能体的核心能力之一。 - 论文提出的方法本身就是一种对智能体能力的**改进**，通过端到端训练，使得单一模型超越其“灵感来源”——原始MAS系统。 3.  **第三步：排除标准——论文不属于排除焦点。** - 论文全文围绕**提升智能体的推理规划性能和效率**展开，没有涉及`Safety`、`Alignment`、`Interpretability`等安全与对齐议题。 - 论文是纯文本模型，不涉及`Vision`、`MLLMs`等多模态内容。 4.  **第四步：特殊和模糊情况处理——明确属于保留范围。** - **推理/规划**: 论文的研究内容是“智能体如何进行规划或在复杂任务中进行多步推理”。它提出了一个名为IMAGINE的**新Agentic框架**来解决规划问题，而不是仅仅提出一个新的CoT变体或微调数据集来提升LLM的基础数学逻辑能力。因此，完全符合“保留”条件。 **结论**: 论文《IMAGINE》的核心贡献是提出了一种创新的、旨在提升LLM智能体规划与推理能力的框架。它通过将多智能体系统的优势整合到单一模型中，不仅解决了现有技术的瓶颈，还显著提升了性能。这完全符合您“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的核心目标，是关于Agentic AI方法论的前沿研究，应被保留。"
    },
    {
        "index": "#14",
        "title": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling",
        "link": "/arxiv/2510.14703",
        "arxiv_id": "2510.14703",
        "authors": "Jianghao Lin, Yuanyuan Shi, Xin Peng, Renjie Ding, Hairui Wang, Yuxuan Peng, Bizhe Bai, Weixi Song, Fengshuo Bai, Huacan Chai, Weinan Zhang, Fei Huang, Ying Wen",
        "summary": "Large language models (LLMs) are increasingly demonstrating strong capabilities as autonomous agents, with function calling serving as a core mechanism for interaction with the environment. Meanwhile, inference scaling has become a cutting-edge technique to enhance LLM performance by allocating more computational resources during the inference process. However, current research on inference scaling primarily focuses on unstructured output generation tasks, leaving its application in structured outputs, like function calling, largely underexplored. To bridge this gap, we propose an inference scaling framework that combines fine-grained beam search with a process reward model, ToolPRM, which scores the internal steps of each single function call. To train ToolPRM, we construct the first fine-grained intra-call process supervision dataset, automatically annotated with function-masking techniques to provide step-level rewards for structured tool-use reasoning. Extensive experiments demonstrate that ToolPRM beats the coarse-grained and outcome reward models in terms of predictive accuracy, indicating its stronger capability in supervising the function calling inference process. Inference scaling technique equipped with ToolPRM also significantly improves the backbone model performance across various function calling tasks and benchmarks. More importantly, we reveal a key principle for applying inference scaling techniques to structured outputs: \"explore more but retain less\" due to the unrecoverability characteristics of structured function calling generation.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.284181",
        "filter_reason": "这篇论文完全符合您的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM或智能体作为工具去解决某个外部领域的问题，而是聚焦于**改进LLM智能体的一项核心能力**。其核心贡献是提出了一个名为 `ToolPRM` 的新框架，专门用于提升和优化 `Function Calling`（函数调用）这一智能体与环境交互的关键机制。这直接属于“构建、改进或演化LLM智能体”的范畴。 2.  **第二步：正面指标** - 论文高度匹配您的核心关注点： - **核心范式**: 论文开篇即点明 `LLMs as autonomous agents`，其研究内容 `Function Calling` 是 `Agentic AI` 的核心。 - **智能体能力**: 论文的核心是 `Tool Use / Tool Augmentation`。更进一步，它提出的 `Process Reward Model` (PRM) 通过对函数调用的内部步骤进行评分和引导，本质上是一种精细化的 `Self-Correction` 或 `Self-Refinement` 机制，帮助智能体在工具使用过程中纠正错误路径。 3.  **第三步：排除标准** - 论文的主要贡献是提升智能体的性能和能力，而非 `Safety`、`Alignment` 或 `Interpretability`。同时，它不涉及 `Vision` 或多模态内容。因此，没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美地符合“保留”条件。它研究的不是LLM基础的数学或逻辑推理能力，而是**智能体在执行工具使用这一复杂任务时的多步推理过程**。`ToolPRM` 监督和优化的正是函数调用内部的推理链条，这与 `ReAct` 等框架中强调的“推理-行动”循环一脉相承，是对智能体推理过程的直接改进。 **总结**: 该论文的核心贡献是提出了一种新的方法论（`ToolPRM`框架）来**改进LLM智能体的工具使用能力**。它通过引入过程监督和推理缩放技术，让智能体在调用函数时能进行更有效的自我纠正，从而提升了其作为自主代理的可靠性。这完全契合您研究目标中的“单智能体”方向，特别是“工具使用”和“自我反思/纠正”子方向。因此，这是一篇高度相关的前沿论文，应被筛选入内。"
    },
    {
        "index": "#25",
        "title": "JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol",
        "link": "/arxiv/2510.14537",
        "arxiv_id": "2510.14537",
        "authors": "Emanuele Antonioni, Stefan Markovic, Anirudha Shankar, Jaime Bernardo, Lovro Markovic, Silvia Pareti, Benedetto Proietti",
        "summary": "AI systems are continually evolving and advancing, and user expectations are concurrently increasing, with a growing demand for interactions that go beyond simple text-based interaction with Large Language Models (LLMs). Today's applications often require LLMs to interact with external tools, marking a shift toward more complex agentic systems. To support this, standards such as the Model Context Protocol (MCP) have emerged, enabling agents to access tools by including a specification of the capabilities of each tool within the prompt. Although this approach expands what agents can do, it also introduces a growing problem: prompt bloating. As the number of tools increases, the prompts become longer, leading to high prompt token costs, increased latency, and reduced task success resulting from the selection of tools irrelevant to the prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework designed to help agents manage prompt size more effectively when using large sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and uses the user's prompt to identify and include only the most relevant tools, based on both the query and the taxonomy structure. In this paper, we describe the design of the taxonomy, the tool selection algorithm, and the dataset used to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt size without significantly compromising the agent's ability to respond effectively. As the number of available tools for the agent grows substantially, JSPLIT even improves the tool selection accuracy of the agent, effectively reducing costs while simultaneously improving task success in high-complexity agent environments.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.287669",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。以下是我的详细判断过程： 1.  **第一步：核心判断** - **论文的核心贡献**: 论文提出了一个名为JSPLIT的框架，用于解决LLM智能体在使用大量工具时遇到的“提示膨胀”问题。 - **判断依据**: 这篇论文的本质不是将LLM智能体作为一个现成工具去应用，而是**直接针对LLM智能体本身的一个核心能力——工具使用——进行改进和优化**。它提出了一种新的方法论（基于分类法的动态工具选择框架），使得智能体能够更高效、更准确地使用工具。这完全符合“构建、改进或演化LLM智能体”的核心目标。因此，根据第一步的规则，应**保留**。 2.  **第二步：正面指标** - 论文摘要中明确包含了多个核心关注点： - **核心范式**: `Agentic AI`, `LLM-based Agents`。摘要开篇就点明了研究背景是“更复杂的智能体系统”。 - **智能体能力**: `Tool Use / Tool Augmentation`。这是论文的绝对核心，JSPLIT框架就是为了优化这一能力而设计的。通过改进工具选择，它间接支持了智能体的`Planning`（规划）能力，因为正确的工具是成功规划的前提。 - 论文在“单智能体”方向上具有非常强的相关性。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或水印等问题。 - 论文也未涉及多模态或视觉模型，其焦点完全在于基于文本的工具和提示。 - 因此，论文没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文虽然不直接提出新的规划算法，但它通过优化工具选择这一前置环节，极大地提升了智能体在复杂任务中的执行效率和成功率。一个智能体即使有完美的规划能力，如果无法从海量工具中快速、准确地选出所需工具，规划也无法落地。因此，JSPLIT是对智能体规划-执行链条中关键一环的实质性增强，属于Agentic AI研究的范畴。 **总结**: 这篇论文的核心是提出了一种新的框架（JSPLIT）来**改进**LLM智能体的**工具使用**能力。它解决了智能体在工具数量增多时面临的可扩展性和效率问题，这是构建更强大、更实用的LLM智能体的关键技术挑战之一。该研究完全聚焦于Agentic AI的内部机制优化，而非应用或安全性问题，因此精准地符合您“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的核心目标。"
    },
    {
        "index": "#23",
        "title": "LLM Agents Beyond Utility: An Open-Ended Perspective",
        "link": "/arxiv/2510.14548",
        "arxiv_id": "2510.14548",
        "authors": "Asen Nachkov, Xi Wang, Luc Van Gool",
        "summary": "Recent LLM agents have made great use of chain of thought reasoning and function calling. As their capabilities grow, an important question arises: can this software represent not only a smart problem-solving tool, but an entity in its own right, that can plan, design immediate tasks, and reason toward broader, more ambiguous goals? To study this question, we adopt an open-ended experimental setting where we augment a pretrained LLM agent with the ability to generate its own tasks, accumulate knowledge, and interact extensively with its environment. We study the resulting open-ended agent qualitatively. It can reliably follow complex multi-step instructions, store and reuse information across runs, and propose and solve its own tasks, though it remains sensitive to prompt design, prone to repetitive task generation, and unable to form self-representations. These findings illustrate both the promise and current limits of adapting pretrained LLMs toward open-endedness, and point to future directions for training agents to manage memory, explore productively, and pursue abstract long-term goals.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.286989",
        "filter_reason": "这篇论文完全符合你的研究范围，是一个高质量的筛选目标。 **判断过程和核心依据如下:** 1.  **第一步：核心判断——保留** 论文的核心贡献是构建并研究一个具有“开放性”的LLM智能体框架。它不是将现有智能体作为工具去解决某个特定领域的问题，而是探索如何让智能体本身变得更加自主和演化。具体来说，论文“增强了一个预训练的LLM智能体，使其具备生成自己的任务、积累知识并与环境广泛互动的能力”。这直接命中了你筛选标准的第一步“保留”条件：论文的核心是关于**构建和改进LLM智能体的方法论和新框架**。 2.  **第二步：正面指标——高度匹配** 论文包含了大量你的核心关注点，并且与你的三个研究方向紧密相关： *   **自我演化:** 这是论文最核心的亮点。赋予智能体“生成自己的任务”和“积累知识”的能力，这正是智能体通过经验和环境反馈进行自我完善和迭代的关键体现，完全符合`Self-Evolving`、`Self-Improvement`和`Iterative Improvement`的定义。 *   **单智能体:** 论文明确研究了智能体的`Planning`（规划、设计即时任务）和`Memory`（积累知识、跨运行存储和重用信息）能力，这些都是单智能体研究的核心子方向。 3.  **第三步：排除标准——未触发** 论文的研究焦点是智能体的架构和能力演化，不涉及`Safety`、`Alignment`、`Hallucination`等安全与对齐问题，也未涉及`Vision`、`MLLMs`等多模态内容。因此，它没有触发任何排除标准。 4.  **第四步：特殊和模糊情况——适用保留规则** 论文的研究内容完美地适用了“推理/规划”和“自我演化”的保留规则。 *   **推理/规划:** 论文探讨的是智能体如何“规划”和“推理”，以实现“更广泛、更模糊的目标”，这属于智能体层面的规划，而非LLM底层Token预测能力的提升，因此应该保留。 *   **自我演化的应用:** 论文的核心就是提出一种新的“自我演化”机制（开放性任务生成与知识积累），因此即使它被应用在某个实验环境中，也应该被保留。 **最终决策:** 该论文的核心贡献在于提出并验证了一种让LLM智能体走向“开放性”和“自我演化”的框架。它直接回应了“LLM智能体及其演化”这一课题，尤其是在“自我演化”和“单智能体”的规划与记忆能力上做出了前沿探索。因此，这篇论文与你的研究目标高度一致，应被**保留**。"
    },
    {
        "index": "#26",
        "title": "Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration",
        "link": "/arxiv/2510.14512",
        "arxiv_id": "2510.14512",
        "authors": "Haoyuan Li, Mathias Funk, Aaqib Saeed",
        "summary": "Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.287967",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建和演化一个多智能体系统。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心不是将LLM或智能体作为工具去解决一个联邦学习（FL）问题，而是**提出一个名为Helmsman的、全新的多智能体系统框架**，用于自动化地“合成”联邦学习系统。论文的本质是关于“如何构建一个能够自主完成复杂工程任务的智能体系统”，这直接命中了“构建LLM智能体”和“多智能体系统”的核心目标。它不属于“非演化型应用”，因为其贡献在于方法论本身，而非应用结果。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了大量您关注的核心关键词和概念： *   **核心范式**: `Multi-Agent Systems (MAS)` 被明确提及。 *   **智能体能力**: `Planning`（“interactive human-in-the-loop planning”）、`Tool Use`（“modular code generation”可以视为一种工具使用）、`Self-Correction`/`Self-Refine`（“autonomous evaluation and refinement”）。 *   **多智能体**: `Collaboration`（“Multi-Agent Collaboration”、“collaborative phases”、“supervised agent teams”）。 *   **演化机制**: `Self-Improvement`/`Iterative Improvement`（“closed-loop of autonomous evaluation and refinement”是一个典型的自我演化、迭代优化的闭环机制）。 3.  **第三步：排除标准——未触发** 论文的主要贡献是关于提升智能体的自动化构建和优化能力，不涉及安全、对齐、可解释性或水印等问题。同时，它也不以多模态或视觉为核心研究内容。 4.  **第四步：处理特殊和模糊情况——符合保留规则** *   **推理/规划**: 论文中的“planning”是智能体层面的、为了完成“合成FL系统”这一复杂任务而进行的多步骤规划，完全符合保留条件。 *   **自我演化的应用**: 这是一个典型的“例外”情况。虽然论文应用在联邦学习这个特定领域，但其核心贡献是提出了一种**新的“自我演化”机制**（即“评估-优化的闭环”）。因此，根据您的规则，这种提出新机制的应用论文应该被保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献是构建了一个新颖的多智能体协作框架，该框架具备规划、工具使用和通过闭环反馈进行自我演化的能力。它完美地契合了您研究课题中的“多智能体”和“自我演化”两个核心方向，是关于Agentic AI方法论的前沿研究，而非简单的应用。因此，应予以保留。"
    },
    {
        "index": "#29",
        "title": "Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control",
        "link": "/arxiv/2510.14388",
        "arxiv_id": "2510.14388",
        "authors": "Zhe Wu, Hongjin Lu, Junliang Xing, Changhao Zhang, Yin Zhu, Yuhao Yang, Yuheng Jing, Kai Li, Kun Shao, Jianye Hao, Jun Wang, Yuanchun Shi",
        "summary": "Building agents that autonomously operate mobile devices has attracted increasing attention. While Vision-Language Models (VLMs) show promise, most existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks or unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical vision-language agent for mobile control, featuring a high-level reasoning model and a low-level action model that are jointly optimized. For efficient training, we reformulate multi-step decision-making as a sequence of single-step subgoals and propose a foresight advantage function, which leverages execution feedback from the low-level model to guide high-level optimization. This design alleviates the path explosion issue encountered by Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art (SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods across three paradigms: prompt-based (AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark. On the more challenging AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones, showing strong adaptability in high-complexity mobile control scenarios.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.288938",
        "filter_reason": "这篇论文符合研究范围，应被保留。判断依据如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出了一种名为 Hi-Agent 的**新型分层视觉语言智能体框架**，用于移动设备控制。它不是一个简单的应用，而是构建了一个具有“高级推理模型”和“低级动作模型”的全新智能体架构，并设计了“预见性优势函数”等创新方法来优化这个智能体。这完全符合“核心贡献在于构建、改进LLM智能体的方法论或新框架”的保留标准。 2.  **第二步：正面指标——高度相关** 论文内容与研究焦点高度契合： *   **核心范式**: 论文明确提出了 \"vision-language agents\"，并构建了一个分层智能体系统。 *   **智能体能力**: 论文的出发点正是解决现有方法 “lack structured reasoning and planning” 的问题。其分层设计（高级推理 + 低级动作）本身就是一种复杂的**规划**和执行框架。此外，“leverages execution feedback ... to guide high-level optimization” 这部分内容，体现了智能体基于环境反馈进行**自我修正**和**自我反思**的能力。 3.  **第三步：排除标准——不适用** *   **安全与对齐**: 论文未涉及安全、对齐、可解释性等内容。 *   **多模态与视觉**: 这是本篇论文最需要辨析的一点。虽然标题和摘要都提到了 \"Vision-Language\"，但根据筛选规则的例外情况：“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在这篇论文中，视觉（VLM）是智能体**感知手机屏幕（环境）的工具**，而论文的**核心贡献是智能体的分层架构、规划与训练方法**，而非视觉模型本身。因此，它不应被排除。 4.  **第四步：处理特殊和模糊情况——符合保留条件** *   **推理/规划**: 论文明确是关于智能体如何在长时程任务中进行结构化推理和规划，完全符合“保留”条件。它不是在提升LLM的基础数学或逻辑能力，而是在构建一个用于复杂任务决策的Agentic框架。 **最终决策**: 这篇论文的核心是提出一种创新的**单智能体**架构，重点解决了智能体在复杂环境中的**规划**和**基于反馈的自我修正**问题。尽管它应用于移动设备控制这一特定领域，但其贡献在于智能体本身的构建和演化方法，而非应用本身。因此，它完全符合“LLM智能体及其演化”的研究课题，特别是“单智能体”方向。应判定为 **True**。"
    },
    {
        "index": "#32",
        "title": "Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction",
        "link": "/arxiv/2510.14319",
        "arxiv_id": "2510.14319",
        "authors": "Xu Shen, Qi Zhang, Song Wang, Zhen Tan, Xinyu Zhao, Laura Yao, Vaishnav Tadiparthi, Hossein Nourkhiz Mahjoub, Ehsan Moradi Pari, Kwonjoon Lee, Tianlong Chen",
        "summary": "Large Language Model based multi-agent systems (MAS) excel at collaborative problem solving but remain brittle to cascading errors: a single faulty step can propagate across agents and disrupt the trajectory. In this paper, we present MASC, a metacognitive framework that endows MAS with real-time, unsupervised, step-level error detection and self-correction. MASC rethinks detection as history-conditioned anomaly scoring via two complementary designs: (1) Next-Execution Reconstruction, which predicts the embedding of the next step from the query and interaction history to capture causal consistency, and (2) Prototype-Guided Enhancement, which learns a prototype prior over normal-step embeddings and uses it to stabilize reconstruction and anomaly scoring under sparse context (e.g., early steps). When an anomaly step is flagged, MASC triggers a correction agent to revise the acting agent's output before information flows downstream. On the Who&When benchmark, MASC consistently outperforms all baselines, improving step-level error detection by up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers consistent end-to-end gains across architectures, confirming that our metacognitive monitoring and targeted correction can mitigate error propagation with minimal overhead.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.290050",
        "filter_reason": "这篇论文完全符合筛选标准，其核心贡献是构建和改进多智能体系统，并赋予其自我演化的能力。以下是详细的判断依据： 1.  **第一步：核心判断** - **核心贡献**：论文的核心是提出了一个名为**MASC**的**元认知框架**。这个框架的目的是**赋予多智能体系统实时、无监督、步级行动层面的错误检测和自我修正能力**。 - **符合目标**：这不是一篇应用型论文，没有将现有智能体简单地应用到某个特定领域（如生物、金融）。相反，它直接针对LLM多智能体系统（MAS）的一个根本性弱点（错误级联传播），并提出了一种新的、通用的方法论来**改进（improve）**智能体系统的鲁棒性。这完全命中了“构建、改进或演化LLM智能体”的核心目标。 2.  **第二步：正面指标** - **多智能体**: 论文明确聚焦于`Multi-Agent System (MAS)`，并致力于解决多智能体协作中的问题。 - **自我演化**: 论文的标题和摘要反复强调`Self-Correction`（自我修正）和`Metacognitive`（元认知）。自我修正属于自我演化（Self-Evolving）的关键子方向，而元认知是实现高级自我反思和改进的重要机制。 - **智能体能力**: 论文直接解决了智能体在协作中缺乏`Self-Correction`和`Self-Reflection`能力的问题。它引入一个`correction agent`来修正`acting agent`的输出，这是一种具体的、可操作的自我修正实现方式。 3.  **第三步：排除标准** - 论文的主要贡献并非关于`Safety`、`Alignment`或`Interpretability`。虽然它处理的是“错误”，但其目标是提升系统性能和鲁棒性，而不是从伦理、安全或可解释性的角度对齐模型。 - 论文是纯语言模型研究，不涉及`Vision`或其他多模态内容，因此不触及相关的排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文研究的不是提升LLM基础逻辑或数学推理能力，而是**多智能体在执行复杂任务过程中的错误处理**。它通过预测下一步（`Next-Execution Reconstruction`）来检测因果不一致性，这属于智能体规划和执行层面的高级推理，完全符合保留条件。 - **自我演化的应用**: 论文的核心就是提出一种新的“自我演化”（具体为自我修正）机制。虽然它在特定基准上测试，但该机制是通用性的，可以“plugged into diverse MAS frameworks”，这进一步证明了它的方法论价值，而非仅仅是应用价值。 **最终决策**：该论文提出了一个创新的框架MASC，旨在通过元认知和自我修正机制来增强多智能体系统的鲁棒性。这直接对应了研究课题中的“多智能体”和“自我演化”两个核心方向。其贡献在于**改进智能体本身的能力**，而非将其作为工具应用。因此，这篇论文高度相关，应被保留。"
    },
    {
        "index": "#36",
        "title": "Towards Agentic Self-Learning LLMs in Search Environment",
        "link": "/arxiv/2510.14253",
        "arxiv_id": "2510.14253",
        "authors": "Wangtao Sun, Xiang Cheng, Jialin Fan, Yao Xu, Xing Yu, Shizhu He, Jun Zhao, Kang Liu",
        "summary": "We study whether self-learning can scale LLM-based agents without relying on human-curated datasets or predefined rule-based rewards. Through controlled experiments in a search-agent setting, we identify two key determinants of scalable agent training: the source of reward signals and the scale of agent task data. We find that rewards from a Generative Reward Model (GRM) outperform rigid rule-based signals for open-domain learning, and that co-evolving the GRM with the policy further boosts performance. Increasing the volume of agent task data-even when synthetically generated-substantially enhances agentic capabilities. Building on these insights, we propose \\textbf{Agentic Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning framework that unifies task generation, policy execution, and evaluation within a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator, a Policy Model, and a Generative Reward Model to form a virtuous cycle of harder task setting, sharper verification, and stronger solving. Empirically, ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines (e.g., Search-R1) that plateau or degrade, and continues improving under zero-labeled-data conditions, indicating superior sample efficiency and robustness. We further show that GRM verification capacity is the main bottleneck: if frozen, it induces reward hacking and stalls progress; continual GRM training on the evolving data distribution mitigates this, and a small late-stage injection of real verification data raises the performance ceiling. This work establishes reward source and data scale as critical levers for open-domain agent learning and demonstrates the efficacy of multi-role co-evolution for scalable, self-improving agents. The data and code of this paper are released at https://github.com/forangel2014/Towards-Agentic-Self-Learning",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.291496",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。其核心贡献与你设定的筛选标准高度契合。 以下是详细的判断过程： 1.  **第一步：核心判断** - 论文的核心是提出一个名为 **\"Agentic Self-Learning\" (ASL)** 的全新框架。这个框架旨在让LLM智能体在没有人类标注数据或预设规则奖励的情况下进行自我学习和演化。 - 这完全符合**保留**标准：论文的核心贡献在于**构建、改进或演化LLM智能体的方法论和新框架**。它不是将已有智能体当作工具应用，而是研究如何让智能体本身“进化”。 2.  **第二步：正面指标** - 论文命中了多个核心正面指标： - **核心范式**: 标题和摘要中明确出现了 `Agentic AI` 和 `LLM-based Agents`。整个ASL框架描述了一个 `Self-Evolving` 的过程。 - **演化机制**: 摘要中反复强调了 `Self-Learning`、`self-improving agents`、`steady, round-over-round gains`，这直接对应了你的 `Self-Improvement` 和 `Generational Evolution` 关注点。框架通过多角色（任务生成器、策略模型、奖励模型）的`co-evolving`（协同演化）形成一个良性循环，这正是演化的核心。 - **智能体能力**: 论文在 `search-agent setting` 中进行研究，这涉及到智能体的 `Policy execution`（策略执行）能力。 3.  **第三步：排除标准** - 论文的研究焦点是提升智能体的能力和可扩展性，完全没有涉及 `Safety`, `Alignment`, `Interpretability` 等安全与对齐议题。 - 论文的研究环境是搜索环境，没有提及 `Vision`, `MLLMs` 等多模态内容，因此不触及相关排除标准。 4.  **第四步：处理特殊和模糊情况** - 论文完美地符合了“自我演化的应用”这一例外规则。虽然论文是在一个特定的“搜索环境”中进行实验，但其**核心贡献是ASL这个通用的自我演化机制本身**，而不是这个搜索任务的结果。正如你所说，即使应用在特定领域，只要核心是提出新的“自我演化”机制，就应该保留。这篇论文正是如此。 **最终决策**: 这篇论文的核心贡献是提出了一个名为**Agentic Self-Learning (ASL)**的闭环、多角色强化学习框架，旨在实现LLM智能体的**自我演化和持续改进**。它直接命中了你研究课题中的“自我演化”方向，并深入探讨了奖励信号来源、数据规模等关键因素对智能体演化的影响。因此，这篇论文是与你研究目标高度相关的前沿文献，必须保留。"
    },
    {
        "index": "#209",
        "title": "From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering",
        "link": "/arxiv/2510.13857",
        "arxiv_id": "2510.13857",
        "authors": "Qiang Xu, Xiangyu Wen, Changran Xu, Zeju Li, Jianyuan Zhong",
        "summary": "The advent of powerful Large Language Models (LLMs) has ushered in an ``Age of the Agent,'' enabling autonomous systems to tackle complex goals. However, the transition from prototype to production is hindered by a pervasive ``crisis of craft,'' resulting in agents that are brittle, unpredictable, and ultimately untrustworthy in mission-critical applications. This paper argues this crisis stems from a fundamental paradigm mismatch -- attempting to command inherently probabilistic processors with the deterministic mental models of traditional software engineering. To solve this crisis, we introduce a governance-first paradigm for principled agent engineering, embodied in a formal architecture we call ArbiterOS.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-12",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.361540",
        "filter_reason": "这篇论文符合我的研究范围，核心判断依据如下： 1.  **第一步：核心判断——论文的本质** - **结果: 保留**。 - **依据**: 论文的核心贡献并非将现有智能体应用于某个特定领域，也不是提升LLM的基础推理能力。它的核心是提出一个全新的“构建和改进LLM智能体”的**方法论和架构范式**（governance-first paradigm, ArbiterOS）。这直接命中了我筛选标准中“构建、改进或演化 LLM智能体的论文”的核心目标。论文旨在解决当前智能体工程中的根本性“危机”，提出了一种更原则化的工程方法，这属于对智能体构建框架的**根本性改进**。 2.  **第二步：正面指标——论文包含我的核心关注点** - **结果: 高度相关**。 - **依据**: 论文摘要明确提到了 `Agent Engineering` 和 `principled agent`，直接关联到我的核心范式 `Agentic AI` 和 `LLM-based Agents`。虽然摘要没有直接列出 `Planning`、`Memory` 等具体能力的词汇，但它所解决的核心问题——智能体的“brittle, unpredictable, and untrustworthy”——往往是由于规划、记忆、工具使用等模块设计不当或缺乏整体协调导致的。因此，一个“principled”的工程架构必然涉及对这些核心能力的更好组织和构建。 3.  **第三步：排除标准——不在我研究焦点之外** - **结果: 未触犯排除规则**。 - **依据**: 论文提到了 `trustworthy` 和 `principled`，这可能与 Safety/Alignment 产生联想。但需要明确的是，**论文的主要贡献不是一种安全对齐技术，而是一种工程化架构**。它的目标是解决从原型到生产过程中的“craft crisis”，这是一个关于如何更可靠、更系统地构建智能体的方法论问题，而非仅仅如何约束智能体的行为。因此，它属于Agentic AI的范畴，而不是被排除的安全与对齐子领域。论文也不涉及多模态内容。 4.  **第四步：处理特殊和模糊情况** - **结果: 不适用，但可进一步分析其价值**。 - **依据**: 这篇论文不属于“推理/规划”或“自我演化应用”的特殊情况。它处于一个更基础的层面：**如何构建一个能让规划、记忆等能力稳定发挥作用的智能体框架**。这项工作为我课题中的“单智能体”和“多智能体”方向提供了更坚实、更可靠的底层架构基础。一个“脆弱、不可预测”的智能体是无法有效进行协作或自我演化的。因此，这篇论文提出的治理范式，是后续所有智能体能力演化的**前提和基石**。 **最终决策综合分析:** 该论文的核心贡献是提出了一种“治理优先”的LLM智能体工程新范式和架构（ArbiterOS），旨在解决当前智能体构建中普遍存在的脆弱性和不可预测性问题。这完全符合我“筛选核心贡献在于构建、改进LLM智能体”的研究目标。它不属于任何排除类别，并且与我的“Agentic AI”研究方向高度契合，为构建更高级、更可靠的智能体（包括多智能体和自我演化智能体）提供了基础理论与工程框架。因此，应**保留**这篇论文。"
    }
]