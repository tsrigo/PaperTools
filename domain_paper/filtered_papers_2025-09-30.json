[
    {
        "index": "#10",
        "title": "AIPOM: Agent-aware Interactive Planning for Multi-Agent Systems",
        "link": "/arxiv/2509.24826",
        "arxiv_id": "2509.24826",
        "authors": "Hannah Kim, Kushan Mitra, Chen Shen, Dan Zhang, Estevam Hruschka",
        "summary": "Large language models (LLMs) are being increasingly used for planning in orchestrated multi-agent systems. However, existing LLM-based approaches often fall short of human expectations and, critically, lack effective mechanisms for users to inspect, understand, and control their behaviors. These limitations call for enhanced transparency, controllability, and human oversight. To address this, we introduce AIPOM, a system supporting human-in-the-loop planning through conversational and graph-based interfaces. AIPOM enables users to transparently inspect, refine, and collaboratively guide LLM-generated plans, significantly enhancing user control and trust in multi-agent workflows. Our code and demo video are available at https://github.com/megagonlabs/aipom.",
        "subjects": "Human-Computer Interaction, Multiagent Systems",
        "date": "2025-09-29",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T20:55:00.689959",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断——这篇论文的本质是提出AIPOM系统，用于增强多智能体系统中LLM的规划能力。这属于\"智能体协作框架\"的范畴，论文核心是改进LLM在规划方面的通用能力，而非将LLM作为工具应用到特定领域。因此符合保留标准。 第二步：正面指标——论文包含多个相关主题： - 核心概念: 明确提到\"Large language models (LLMs)\" - 能力方向: 明确提到\"planning\"，这是通用推理能力的重要组成部分 - 新兴范式: 明确提到\"multi-agent systems\"，这是当前LLM研究的重要方向 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学等） - 虽然提到透明度和可控性，但这是作为增强LLM规划能力的手段，而非主要研究模型可靠性的应用层面问题 第四步：特殊和模糊情况处理—— 论文涉及智能体/工具使用，但AIPOM是一个通用的多智能体系统框架，旨在增强LLM的通用规划能力和用户控制，而非针对特定领域的应用。因此符合保留条件。 综合判断：这篇论文的核心贡献是提出一个通用框架来增强LLM在多智能体系统中的规划能力，通过人机交互提高透明度和可控性，这直接关系到提升LLM的通用推理能力，特别是规划和问题解决方面，因此符合研究目标。"
    },
    {
        "index": "#2",
        "title": "MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems",
        "link": "/arxiv/2509.24323",
        "arxiv_id": "2509.24323",
        "authors": "Kun Wang, Guibin Zhang, ManKit Ye, Xinyu Deng, Dongxia Wang, Xiaobin Hu, Jinyang Guo, Yang Liu, Yufei Guo",
        "summary": "The past two years have witnessed the meteoric rise of Large Language Model (LLM)-powered multi-agent systems (MAS), which harness collective intelligence and exhibit a remarkable trajectory toward self-evolution. This paradigm has rapidly progressed from manually engineered systems that require bespoke configuration of prompts, tools, roles, and communication protocols toward frameworks capable of automated orchestration. Yet, dominant automatic multi-agent systems, whether generated by external modules or a single LLM agent, largely adhere to a rigid ``\\textit{generate-once-and-deploy}'' paradigm, rendering the resulting systems brittle and ill-prepared for the dynamism and uncertainty of real-world environments. To transcend this limitation, we introduce MAS$^2$, a paradigm predicated on the principle of recursive self-generation: a multi-agent system that autonomously architects bespoke multi-agent systems for diverse problems. Technically, we devise a ``\\textit{generator-implementer-rectifier}'' tri-agent team capable of dynamically composing and adaptively rectifying a target agent system in response to real-time task demands. Collaborative Tree Optimization is proposed to train and specialize these meta-agents. Extensive evaluation across seven benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\\%$ over state-of-the-art MAS in complex scenarios such as deep research and code generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization, effectively leveraging previously unseen LLMs to yield improvements of up to $15.1\\%$. Crucially, these gains are attained without incurring excessive token costs, as MAS$^2$ consistently resides on the Pareto frontier of cost-performance trade-offs. The source codes are available at https://github.com/yeyeyeah2/MAS2.",
        "subjects": "Multiagent Systems, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T20:55:00.687824",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步核心判断：这篇论文的本质是提出一种名为MAS$^2$的新型多智能体系统范式，该系统基于递归自我生成原则，能够自主构建定制化的多智能体系统。论文核心是改进LLM的基础能力，提出新的训练范式（Collaborative Tree Optimization），增强其自我进化、自适应配置和纠错能力，这些都是提升LLM通用推理能力的重要方面。论文不是将LLM作为工具应用于特定领域，而是专注于提升LLM系统本身的能力，因此应被保留。 第二步正面指标：论文包含多个正面指标主题： - 核心概念：明确提到\"Large Language Model (LLM)-powered multi-agent systems\" - 能力方向：涉及problem-solving，在复杂场景如\"deep research and code generation\"中表现优异 - 训练方法：提到系统的\"self-evolution\"轨迹，并提出\"Collaborative Tree Optimization\"来训练元智能体 - 新兴范式：完全围绕\"llm-based agents\"和\"multi-agent systems\"展开，并涉及\"deep research\" 第三步排除标准：论文不涉及任何需要排除的领域： - 不涉及多模态与视觉相关内容 - 不专注于特定应用领域（如医疗、化学等），而是在通用场景测试 - 不关注模型可靠性方面的水印、安全等问题 第四步特殊情况处理：论文提出的智能体框架是通用的，旨在增强LLM的通用问题解决能力，而不是应用于特定领域，符合保留标准。 综上所述，这篇论文的核心贡献是提出了一种自我生成、自我配置和自我纠错的多智能体系统框架，通过递归自我生成和动态组合来增强LLM的通用推理能力和自我进化能力，完全符合研究目标中关于\"智能体协作框架\"和\"自我进化\"等提升LLM通用推理能力的方法论研究。"
    },
    {
        "index": "#1",
        "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents",
        "link": "/arxiv/2509.25189",
        "arxiv_id": "2509.25189",
        "authors": "Gongrui Zhang, Jialiang Zhu, Ruiqi Yang, Kai Qiu, Miaosen Zhang, Zhirong Wu, Qi Dai, Bei Liu, Chong Luo, Zhengyuan Yang, Linjie Li, Lijuan Wang, Weizhu Chen, Yuan Zhang, Xin Li, Zhaoyi Liu, Xin Geng, Baining Guo",
        "summary": "Building Large Language Model agents that expand their capabilities by interacting with external tools represents a new frontier in AI research and applications. In this paper, we introduce InfoAgent, a deep research agent powered by an innovative data synthesis pipeline and orchestrated web search tools. To construct challenging, hard-to-find queries,we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty. Unlike prior work that relies heavily on commercial search tools, we develop a dedicated self-hosted search infrastructure, enhancing transparency of agent environments and facilitating further advancement of agent capacity. We evaluate the effectiveness of our data pipeline by measuring the average number of tool calls required to correctly answer a question, and also show that our agent yields better performance when equipped with our tools. Our \\mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy on BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.384453",
        "filter_reason": "这篇论文的核心贡献是提出InfoAgent，一个通过创新的数据合成管道和协调的网络搜索工具驱动的深度研究智能体，旨在增强大语言模型的通用信息寻求和推理能力。论文符合我的研究目标，原因如下：1）论文本质上是关于改进LLM的基础能力，特别是通过工具使用和信息寻求来增强其推理能力；2）论文包含多个正面指标，如LLM、推理、规划、问题解决、强化学习、智能体、工具使用和深度研究等主题；3）论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面；4）论文提出的智能体/工具使用方法是通用的，旨在增强LLM的通用问题解决能力，而不是应用在特定领域。论文通过两阶段训练方法（监督微调和强化学习）显著提高了模型推理驱动的工具使用能力，这正是我所关注的提高LLM通用推理能力的研究方向。"
    },
    {
        "index": "#14",
        "title": "An empirical study on the limitation of Transformers in program trace generation",
        "link": "/arxiv/2509.25073",
        "arxiv_id": "2509.25073",
        "authors": "Simeng Sun",
        "summary": "We study Transformers on the task \\emph{program trace generation} (PTG), where models produce step-by-step execution traces for synthetic programs. Unlike existing algorithmic problems, PTG externalizes reasoning through long traces where each step is trivial. We train small Transformers with diverse modifications, including alternative position encodings, softmax replacements, hybrid model, and short convolutions. While these models achieve strong in-distribution accuracy, they exhibit systematic failures when generalizing to various factors (e.g., program length, trace steps), though some designs significantly improve generalization.",
        "subjects": "Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.418559",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究课题。具体分析如下： 第一步：核心判断 这篇论文的本质是研究Transformer架构在程序跟踪生成(PTG)任务上的局限性及其改进方法。程序跟踪生成要求模型为合成程序生成逐步执行跟踪，这本质上是一种逻辑推理任务，需要模型理解程序逻辑并按照步骤执行推理。论文探讨了通过修改模型架构（如替代位置编码、softmax替换、混合模型和短卷积）来提高模型的泛化能力，这属于\"改进LLM的基础能力\"和\"增强其逻辑推理能力\"的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标 论文涉及多个正面指标： - 能力方向：论文核心研究的是reasoning（推理），特别是logical reasoning（逻辑推理），因为程序跟踪生成本质上是一种逻辑推理过程。 - 虽然论文没有明确提到\"Large language models\"或\"LLMs\"，但研究的Transformer架构是LLM的基础，且程序跟踪生成是评估模型推理能力的重要任务。 第三步：排除标准 论文不涉及任何排除标准中的领域： - 没有涉及多模态与视觉内容 - 虽然研究的是程序跟踪生成，但这更准确地说是一个评估模型推理能力的基准任务，而非针对特定领域（如医疗、化学等）的应用研究 - 没有涉及模型可靠性方面的水印、安全性等问题 第四步：特殊和模糊情况 论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊内容。 综上所述，这篇论文通过研究Transformer在程序跟踪生成任务上的表现和改进方法，探索了如何增强模型的逻辑推理和泛化能力，符合提高大语言模型通用推理能力的研究目标。因此，我认为这篇论文应该被保留。"
    },
    {
        "index": "#13",
        "title": "Scaling Generalist Data-Analytic Agents",
        "link": "/arxiv/2509.25084",
        "arxiv_id": "2509.25084",
        "authors": "Shuofei Qiao, Yanqiu Zhao, Zhisong Qiu, Xiaobin Wang, Jintian Zhang, Zhao Bin, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen",
        "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.418132",
        "filter_reason": "这篇论文的核心贡献是提出DataMind，一种可扩展的数据合成和智能体训练方法，用于构建通用的数据分析智能体。论文主要关注增强LLM在数据分析领域的通用推理能力，特别是处理多格式、大规模数据文件和长时程、多步推理的能力。根据筛选标准，该论文符合研究目标，原因如下： 1. 核心判断：论文的本质是改进LLM的基础能力和提出新的训练范式，特别是增强其多步推理能力。DataMind方法结合了SFT和RL的训练目标，旨在提升模型的通用推理能力，而不是将LLM作为工具应用到特定领域。 2. 正面指标：论文包含了所有关键正面指标： - 核心概念：训练了DataMind-14B和DataMind-7B等LLM模型 - 能力方向：明确关注\"long-horizon, multi-step reasoning\"（长时程、多步推理） - 训练方法：使用了结合SFT和RL损失的动态调整训练目标 - 新兴范式：核心是构建\"generalist data-analytic agents\"（通用数据分析智能体） 3. 排除标准：论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。 4. 特殊情况处理：虽然论文关注数据分析，但它提出的是一种通用的智能体训练框架，而不是针对特定领域的应用。论文的核心是增强LLM的通用推理能力，这与研究目标高度一致。 综上所述，这篇论文致力于提高大语言模型的通用推理能力，特别是在数据分析领域的多步推理能力，完全符合研究范围。"
    },
    {
        "index": "#23",
        "title": "MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes",
        "link": "/arxiv/2509.24945",
        "arxiv_id": "2509.24945",
        "authors": "Changsheng Zhao, Ernie Chang, Zechun Liu, Chia-Jung Chang, Wei Wen, Chen Lai, Rick Cao, Yuandong Tian, Raghuraman Krishnamoorthi, Yangyang Shi, Vikas Chandra",
        "summary": "The paradigm shift in large language models (LLMs) from instinctive responses to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1) reasoning capabilities only emerge in sufficiently large models, and (2) such capabilities require training on massive datasets. While the first assumption has already been challenged by recent sub-billion-parameter reasoning models such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely unquestioned. In this work, we revisit the necessity of scaling to extremely large corpora (>10T tokens) for reasoning emergence. By carefully curating and resampling open-source datasets that we identify as beneficial under our designed metrics, we demonstrate that strong reasoning abilities can emerge with far less data. Specifically, we show that only ~2T tokens of high-quality data are sufficient, and pre-training with 4.2T tokens on the dataset resampled from these ~2T tokens, followed by a established post-training procedure, enables the development of MobileLLM-R1, a series of sub-billion-parameter reasoning models that substantially outperform prior models trained on fully open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of 15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B. Remarkably, despite being trained on only 11.7% of the tokens compared to Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate further research in this direction, we have released the complete training recipe, data sources, data mixing ratio, and model checkpoints, together with the key insights obtained throughout this study.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.428393",
        "filter_reason": "这篇论文的核心贡献是探索并挑战了关于语言模型推理能力的两个假设：(1)推理能力只在足够大的模型中出现，(2)这种能力需要在大规模数据集上训练。论文通过精心策划和重采样数据集，证明了可以用更少的数据（约2T tokens）在小型语言模型（sub-billion-parameter）上实现强大的推理能力。这完全符合研究目标，因为论文的本质是改进LLM的基础推理能力，提出新的训练范式（数据策划和重采样），增强其通用推理能力。论文在多个推理基准上测试了模型性能，如AIME数学推理测试，证明其方法的有效性。论文没有将LLM作为工具应用于特定领域，也没有关注多模态、特定应用或模型可靠性等排除标准中的内容。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#19",
        "title": "Circuit Distillation",
        "link": "/arxiv/2509.25002",
        "arxiv_id": "2509.25002",
        "authors": "Somin Wadhwa, Silvio Amir, Byron C. Wallace",
        "summary": "Model distillation typically focuses on behavioral mimicry, where a student model is trained to replicate a teacher's output while treating its internal computations as a black box. In this work we propose an alternative approach: Distilling the underlying computational mechanisms implemented by a teacher model. Specifically, we propose circuit distillation, which introduces an objective to align internal representations between analogous circuit components in teacher and student models. We propose a method to match ``functionally correspondent'' circuit components and introduce a loss reflecting similarities between the representations that these induce. We evaluate circuit distillation on entity tracking and theory of mind (ToM) tasks using models from the Llama3 family. Our results demonstrate that circuit distillation outperforms standard distillation, successfully transferring algorithmic capabilities by adjusting only a small, targeted subset of student model parameters. This work establishes the feasibility of transferring mechanisms, which may in turn allow for efficient distillation of targeted teacher capabilities via interpretable and controllable internal student mechanisms.",
        "subjects": "Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.426307",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的模型蒸馏方法——\"电路蒸馏\"(Circuit Distillation)，其核心目标是转移模型内部的计算机制而非仅仅模仿输出行为。这属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，因为它关注如何通过理解和转移模型内部的计算机制来提升模型能力，特别是算法能力的转移。 其次，从正面指标分析，论文明确使用了Llama3模型家族进行评估，包含了\"Large language models, LLMs\"这一核心概念。同时，论文在实体跟踪(entity tracking)和心理理论(ToM)任务上进行了评估，这两个任务都与推理能力密切相关，尤其是心理理论(ToM)涉及高级认知推理能力，属于\"reasoning\"能力方向。 第三，论文不涉及任何排除标准中的领域。它没有关注多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。 最后，在特殊和模糊情况处理上，论文提到了\"interpretable and controllable internal student mechanisms\"，这涉及到模型的可解释性，属于通过增强模型内在可解释性来提升模型通用推理质量的情况，符合保留标准。 论文的核心贡献是提出了一种新的蒸馏方法，通过匹配和转移模型内部的\"电路\"组件，成功实现了算法能力的转移，这种方法有望通过可解释和可控的内部机制来高效蒸馏目标能力，这与提升大语言模型的通用推理能力直接相关。"
    },
    {
        "index": "#26",
        "title": "Expanding Computation Spaces of LLMs at Inference Time",
        "link": "/arxiv/2509.24884",
        "arxiv_id": "2509.24884",
        "authors": "Yoonna Jang, Kisu Yang, Isabelle Augenstein",
        "summary": "Chain-of-thought (CoT) rationale enables language models to use additional task-related text for problem-solving, benefiting not only from detailed reasoning steps but also from the expanded computational space of longer inputs. Prior work has trained filler or special tokens to serve as additional computation spaces. In this study, we investigate whether language models can leverage artificially inserted sequences of filler tokens solely at inference. We first identify effective token types, numbers, and insertion locations, then examine at what stage of training models begin to exploit the expanded computation space, and finally analyze dynamics within these spaces via attention maps. Experiments on models ranging from 1.7B to 32B across open-domain QA and math tasks show that appropriate token types and counts vary, but placing filler tokens directly before the final 'Answer:' token is most effective. Smaller models benefit most, up to 12.372 percentage points in SmolLM2-1.7B-Instruct, indicating that these spaces act as additional computational capacity rather than redundant input. Attention maps reveal that expanded spaces often continue the original attention mechanism and sometimes focus on questions or answer options, suggesting meaningful computation for problem-solving.",
        "subjects": "Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.435162",
        "filter_reason": "这篇论文完全符合研究目标，其核心是改进大语言模型的基础推理能力。论文提出在推理时通过插入填充令牌来扩展LLM的计算空间，这是一种增强模型通用问题解决能力的新方法。从核心判断来看，论文本质上是研究如何通过扩展计算空间来提升LLM的推理能力，类似于思维链(CoT)的扩展，属于改进LLM基础能力的研究。从正面指标看，论文明确涉及LLMs核心概念，并在开放域问答和数学任务上验证了推理能力提升。论文不符合任何排除标准，它不关注多模态、特定应用领域或模型可靠性等排除领域。虽然论文没有涉及智能体、工具使用等新兴范式，但其核心贡献——通过扩展推理时的计算空间来增强LLM的通用推理能力——直接与研究目标\"提高大语言模型的通用推理能力\"高度一致。实验表明这种方法特别有利于提升小模型的推理能力，进一步证明其通用价值。"
    },
    {
        "index": "#35",
        "title": "LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space",
        "link": "/arxiv/2509.24771",
        "arxiv_id": "2509.24771",
        "authors": "Guibin Zhang, Fanci Meng, Guancheng Wan, Zherui Li, Kun Wang, Zhenfei Yin, Lei Bai, Shuicheng Yan",
        "summary": "Test-time Scaling (TTS) has been demonstrated to significantly enhance the reasoning capabilities of Large Language Models (LLMs) during the inference phase without altering model parameters. However, existing TTS methods are largely independent, implying that LLMs have not yet evolved to progressively learn how to scale more effectively. With the objective of evolving LLMs to learn ``how to scale test-time computation,'' we propose LatentEvolve, a self-evolving latent TTS framework inspired by the complementary learning system (CLS) theory. Analogous to the human brain's dual system of a fast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve comprises two evolutionary components: \\textit{daytime scaling}, which rapidly retrieves historical latent representations to better guide current LLM reasoning; and \\textit{nighttime scaling}, which integrates past latent optimizations in a manner akin to the human brain's consolidation of experiences during sleep. The alternation of daytime and nighttime processes facilitates a fast and slow evolution of LLM TTS, mirroring human cognitive dynamics in a fully unsupervised manner. Extensive experiments across eight benchmarks and five model backbones demonstrate that our LatentEvolve surpasses state-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\\%$ and exhibits exceptional cross-domain and cross-backbone generalization.",
        "subjects": "Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.445233",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提出LatentEvolve，一种自我进化的潜在测试时扩展(TTS)框架，旨在增强大语言模型的推理能力，这直接属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的范畴。论文不是将LLM作为工具应用到特定领域，而是专注于提升LLM本身的通用推理能力。 从正面指标看，论文明确包含多个相关主题：核心概念方面直接研究Large Language Models (LLMs)；能力方向专注于reasoning能力的提升；训练方法涉及self-evolve（自我进化）这一创新范式。论文在八个基准测试上验证了其方法的有效性，表明其关注的是通用推理能力而非特定领域应用。 论文没有涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。相反，它提出了一种受人类互补学习系统理论启发的通用框架，通过\"daytime scaling\"和\"nighttime scaling\"两个组件的交替，实现了LLM推理能力的自我进化，这种创新方法直接服务于提升LLM的通用推理能力这一核心目标。"
    },
    {
        "index": "#34",
        "title": "SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models",
        "link": "/arxiv/2509.24781",
        "arxiv_id": "2509.24781",
        "authors": "Jun Rao, Yunjie Liao, Xuebo Liu, Zepeng Lin, Lian Lian, Dong Jin, Shengjun Cheng, Jun Yu, Min Zhang",
        "summary": "Existing alignment methods for preference optimization of large language models (LLMs) aim to enhance model performance by utilizing pairs of positive and negative samples. However, due to the limited capacity of models in scoring or generating responses, the quality of positive and negative samples may become similar during training, which complicates optimization for preference learning. To address this issue, we introduce SeaPO, a Strategic Error Amplification method that leverages three error types commonly occurring in LLMs to introduce specific error patterns into the model Preference Optimization. This strategy ensures that negative samples are more erroneous than positive samples and preference-based training is employed to mitigate the occurrence of these errors, thereby enhancing model performance. Evaluations across five capability dimensions and different model scales (1.5B to 14B) demonstrate that the generated data significantly improved overall model performance, particularly in terms of truthfulness, with improvements of 5-10 percentage points observed. Further analysis reveals that task performance varies depending on the error types introduced. Injecting the most common error types improves performance in related tasks, while a mix of error types leads to a broader performance enhancement: most tasks show stable improvements, while a few tasks exhibit significant gains.",
        "subjects": "Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.444654",
        "filter_reason": "这篇论文的核心贡献是提出SeaPO（Strategic Error Amplification）方法，用于大语言模型的偏好优化。根据筛选标准，我判断它符合研究范围，原因如下： 首先，从本质上看，这篇论文专注于改进LLM的基础能力，提出了一种新的训练范式来增强模型的偏好学习。它通过策略性地放大错误来确保负样本比正样本更有错误性，从而提高模型的整体性能和真实性，这属于改进LLM通用推理能力的研究。 其次，论文包含多个正面指标：核心概念明确聚焦于大语言模型(LLMs)；训练方法涉及偏好优化，这与强化学习人类反馈(RLHF)相关；虽然未直接提及reasoning等术语，但提高模型真实性是通用推理能力的重要组成部分。 第三，论文不符合任何排除标准：它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，虽然论文没有涉及智能体/工具使用，但它关注于通过新方法提高模型的真实性，这与减少幻觉相关，属于提升模型内在可靠性和推理质量的方法，符合研究目标。 综上所述，SeaPO论文致力于通过改进训练范式来提高LLM的通用推理能力，特别是真实性方面，因此符合研究范围。"
    },
    {
        "index": "#38",
        "title": "MemGen: Weaving Generative Latent Memory for Self-Evolving Agents",
        "link": "/arxiv/2509.24704",
        "arxiv_id": "2509.24704",
        "authors": "Guibin Zhang, Muxin Fu, Shuicheng Yan",
        "summary": "Agent memory shapes how Large Language Model (LLM)-powered agents, akin to the human brain, progressively refine themselves through environment interactions. Existing paradigms remain constrained: parametric memory forcibly adjusts model parameters, and retrieval-based memory externalizes experience into structured databases, yet neither captures the fluid interweaving of reasoning and memory that underlies human cognition. To address this gap, we propose MemGen, a dynamic generative memory framework that equips agents with a human-esque cognitive faculty. It consists of a \\textit{memory trigger}, which monitors the agent's reasoning state to decide explicit memory invocation, and a \\textit{memory weaver}, which takes the agent's current state as stimulus to construct a latent token sequence as machine-native memory to enrich its reasoning. In this way, MemGen enables agents to recall and augment latent memory throughout reasoning, producing a tightly interwoven cycle of memory and cognition. Extensive experiments across eight benchmarks show that MemGen surpasses leading external memory systems such as ExpeL and AWM by up to $38.22\\%$, exceeds GRPO by up to $13.44\\%$, and exhibits strong cross-domain generalization ability. More importantly, we find that without explicit supervision, MemGen spontaneously evolves distinct human-like memory faculties, including planning memory, procedural memory, and working memory, suggesting an emergent trajectory toward more naturalistic forms of machine cognition.",
        "subjects": "Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.446844",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础能力，提出了一种新的记忆框架来增强LLM驱动智能体的推理能力。论文提出的MemGen框架通过\"记忆触发器\"和\"记忆编织器\"实现了记忆与推理的紧密交织，这属于增强LLM通用推理能力的方法论研究，特别是关于智能体框架和自我进化的研究方向。 其次，论文包含多个正面指标：明确涉及\"Large Language Model (LLM)-powered agents\"这一核心概念；关注推理能力的提升(\"enrich its reasoning\")；涉及自我进化(\"Self-Evolving Agents\")；并且研究LLM-based agents这一新兴范式。 第三，论文不符合任何排除标准：没有涉及多模态与视觉内容；没有专注于特定应用领域；也没有关注模型可靠性方面的应用问题。 在特殊和模糊情况处理上，论文提出的是一种通用的智能体记忆框架，而非针对特定领域的应用。实验在\"八个基准测试\"上进行，展示了其跨领域泛化能力，进一步证明其通用性。 论文的核心贡献是提出了一种动态生成记忆框架，使LLM驱动的智能体能够通过记忆与推理的交织循环实现自我进化，自发演化出类似人类的记忆能力，从而提升通用推理能力。这完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#37",
        "title": "Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution",
        "link": "/arxiv/2509.24726",
        "arxiv_id": "2509.24726",
        "authors": "Shaobo Wang, Zhengbo Jiao, Zifan Zhang, Yilang Peng, Xu Ze, Boyu Yang, Wei Wang, Hu Wei, Linfeng Zhang",
        "summary": "Recent breakthroughs in large language models (LLMs) on reasoning tasks rely heavily on massive, high-quality datasets-typically human-annotated and thus difficult to scale. While data synthesis or distillation offers a promising alternative, existing methods struggle with inconsistent data quality and an inability to dynamically adapt to the evolving capabilities of the model, leading to suboptimal training signals. To address these limitations, we introduce Socratic-Zero, a fully autonomous framework that generates high-quality training data from minimal seed examples through the co-evolution of three agents: the Teacher, the Solver, and the Generator. The Solver continuously refines its reasoning by learning from preference feedback on both successful and failed trajectories; the Teacher adaptively crafts increasingly challenging questions based on the Solver's weaknesses; and the Generator distills the Teacher's question-design strategy to enable scalable, high-fidelity curriculum generation. This closed-loop system produces a self-improving curriculum-requiring no pre-existing tasks or labels. Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25, Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3 and GLM4 series models. Even more surprisingly, synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks, including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4, and Claude-4.1-Opus.",
        "subjects": "Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.446354",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：这篇论文的本质是提出Socratic-Zero框架，通过三个智能体（Teacher、Solver和Generator）的共同进化来生成高质量训练数据，从而提高大语言模型的推理能力。这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划和多步推理等通用能力\"的范畴，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确研究large language models (LLMs) - 能力方向：专注于reasoning，特别是math reasoning，在多个数学推理基准测试上评估 - 训练方法：涉及evolution和self-evolve概念，通过智能体共同进化实现自我改进 - 新兴范式：提出了llm-based agents和multi-agent systems框架 第三步排除标准：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于特定应用领域（虽然使用数学推理作为评估基准，但方法是通用的） - 不讨论模型可靠性的应用层面问题 第四步特殊和模糊情况：论文提出的是通用的智能体协作框架来增强LLM的通用推理能力，而非针对特定领域的应用，因此应该保留。 综上所述，这篇论文的核心贡献是提出了一种通过智能体共同进化来增强LLM通用推理能力的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#48",
        "title": "GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training",
        "link": "/arxiv/2509.24494",
        "arxiv_id": "2509.24494",
        "authors": "Hongcheng Wang, Yinuo Huang, Sukai Wang, Guanghui Ren, Hao Dong",
        "summary": "Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling between thoughts and answers, sparse reward signals caused by limited parallel sampling, and unstable advantage estimation. To mitigate these challenges, we propose GRPO-MA, a simple yet theoretically grounded method that leverages multi-answer generation from each thought process, enabling more robust and efficient optimization. Theoretically, we show that the variance of thought advantage decreases as the number of answers per thought increases. Empirically, our gradient analysis confirms this effect, showing that GRPO-MA reduces gradient spikes compared to GRPO. Experiments on math, code, and diverse multimodal tasks demonstrate that GRPO-MA substantially improves performance and training efficiency. Our ablation studies further reveal that increasing the number of answers per thought consistently enhances model performance.",
        "subjects": "Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.457520",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文本质上是关于改进LLM推理能力的训练方法。具体来说，它提出了GRPO-MA方法来优化GRPO算法，以更稳定和高效地训练大语言模型的链式思维(CoT)推理能力。这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的标准。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：专注于reasoning，特别是Chain-of-Thought推理，并在数学和代码任务上验证 - 训练方法：使用强化学习(GRPO)进行训练优化 第三步排除标准：虽然论文提到了VLMs和\"多样化的多模态任务\"，但多模态并非论文的主要焦点，而是作为评估方法的一种应用场景。论文核心是提出一种通用的训练方法改进，而非专注于多模态技术研究。论文也不涉及医疗、化学等特定应用领域，或水印、安全等模型可靠性问题。 第四步特殊情况处理：论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊内容。关于多模态方面，论文只是将多模态任务作为评估场景之一，而非研究核心。 综合判断：论文的核心贡献是提出GRPO-MA方法来改进强化学习训练过程，以增强大语言模型的链式思维推理能力。这是一种通用推理能力的提升方法，符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。因此，这篇论文应该被保留。"
    },
    {
        "index": "#52",
        "title": "Alternatives To Next Token Prediction In Text Generation - A Survey",
        "link": "/arxiv/2509.24435",
        "arxiv_id": "2509.24435",
        "authors": "Charlie Wyatt, Aditya Joshi, Flora Salim",
        "summary": "The paradigm of Next Token Prediction (NTP) has driven the unprecedented success of Large Language Models (LLMs), but is also the source of their most persistent weaknesses such as poor long-term planning, error accumulation, and computational inefficiency. Acknowledging the growing interest in exploring alternatives to NTP, the survey describes the emerging ecosystem of alternatives to NTP. We categorise these approaches into five main families: (1) Multi-Token Prediction, which targets a block of future tokens instead of a single one; (2) Plan-then-Generate, where a global, high-level plan is created upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the autoregressive process itself into a continuous latent space; (4) Continuous Generation Approaches, which replace sequential generation with iterative, parallel refinement through diffusion, flow matching, or energy-based methods; and (5) Non-Transformer Architectures, which sidestep NTP through their inherent model structure. By synthesizing insights across these methods, this survey offers a taxonomy to guide research into models that address the known limitations of token-level generation to develop new transformative models for natural language processing.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.459466",
        "filter_reason": "这篇论文的核心是探讨替代传统下一个词元预测(NTP)范式的方法，以解决大语言模型在长期规划、错误累积和计算效率等方面的弱点。论文明确聚焦于改进LLM的基础能力，特别是通过\"Plan-then-Generate\"和\"Latent Reasoning\"等方法来增强模型的规划和推理能力，这与研究目标中\"提高大语言模型本身的通用推理能力\"高度一致。论文没有涉及任何特定应用领域（如医疗、化学等），也不是将LLM作为工具应用于特定问题，而是专注于改进LLM的核心生成机制和推理能力。作为一篇综述，它系统地分类了五种改进LLM推理能力的方法家族，包括多词元预测、先规划后生成、潜在推理等，这些都是直接针对提升LLM通用推理能力的研究方向。因此，这篇论文完全符合筛选标准，应该被纳入研究范围。"
    },
    {
        "index": "#56",
        "title": "LLaDA-MoE: A Sparse MoE Diffusion Language Model",
        "link": "/arxiv/2509.24389",
        "arxiv_id": "2509.24389",
        "authors": "Fengqi Zhu, Zebin You, Yipeng Xing, Zenan Huang, Lin Liu, Yihong Zhuang, Guoshan Lu, Kangyu Wang, Xudong Wang, Lanning Wei, Hongrui Guo, Jiaqi Hu, Wentao Ye, Tieyuan Chen, Chenchen Li, Chengfu Tang, Haibo Feng, Jun Hu, Jun Zhou, Xiaolu Zhang, Zhenzhong Lan, Junbo Zhao, Da Zheng, Chongxuan Li, Jianguo Li, Ji-Rong Wen",
        "summary": "We introduce LLaDA-MoE, a large language diffusion model with the Mixture-of-Experts (MoE) architecture, trained from scratch on approximately 20T tokens. LLaDA-MoE achieves competitive performance with significantly reduced computational overhead by maintaining a 7B-parameter capacity while activating only 1.4B parameters during inference. Our empirical evaluation reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion language models with larger parameters, surpassing previous diffusion language models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation, mathematical reasoning, agent and alignment tasks, despite using fewer active parameters. Our results show that integrating a sparse MoE architecture into the training objective of masked diffusion language models still brings out MoE's strengths under efficient inference with few active parameters, and opens ample room for further exploration of diffusion language models. LLaDA-MoE models are available at Huggingface.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.467375",
        "filter_reason": "根据筛选标准，我对这篇论文进行了详细分析： 第一步：核心判断——这篇论文的本质是提出了一种新的模型架构LLaDA-MoE，结合了Mixture-of-Experts (MoE)架构和扩散模型技术。论文的核心贡献在于提高大语言模型的计算效率（保持7B参数容量但只激活1.4B参数），同时保持或提升其在多种通用任务上的性能。这属于改进LLM的基础能力和架构的研究，符合\"改进LLM的基础能力\"的研究目标。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确提到\"large language diffusion model\"，属于LLMs范畴 - 能力方向：特别提到了模型在\"数学推理\"(mathematical reasoning)方面的表现 - 新兴范式：提到了模型在\"智能体\"(agent)任务上的表现 第三步：排除标准——论文不主要聚焦于任何排除领域： - 虽然提到了\"Diffusion Language Model\"，但这是应用于语言模型的扩散技术，而非视觉或多模态领域的扩散模型 - 没有聚焦于任何特定应用领域（如医疗、化学等） - 没有主要关注模型可靠性的应用层面（如水印、安全等） 第四步：特殊和模糊情况——论文提到模型在智能体任务上的表现，但主要是评估而非提出新的智能体框架；没有明确讨论幻觉、可解释性或安全性问题。 综合判断：这篇论文的核心是改进LLM的基础架构和效率，同时评估了其在数学推理等通用推理能力上的表现。虽然主要关注点是计算效率，但其目的是提升模型在多种通用任务上的性能，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。因此，这篇论文应该被保留。"
    },
    {
        "index": "#59",
        "title": "Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining",
        "link": "/arxiv/2509.24356",
        "arxiv_id": "2509.24356",
        "authors": "Matthew Theodore Roque, Dan John Velasco",
        "summary": "Most studies on language model pretraining focus on large datasets, leaving open questions about optimization in data-constrained settings. In such settings, the effects of training data order and of including alternative versions of the same text remain underexplored. We address this by studying curriculum learning in pretraining, focusing on text-complexity ordering and data augmentation via simplification. We ask: (1) Does simplifying texts enhance representation quality more than reusing the original data? and (2) Does ordering data by text complexity yield better representations? To answer, we build on a pair of parallel corpora where human-written paragraphs are aligned with LLM-simplified variants, and test four data schedules: repeated exposure, low-to-high complexity, high-to-low, and interleaved. We analyze models' representation quality from a sample efficiency perspective via fine-tuning, as well as its zero-shot performance on linguistic knowledge, entity tracking, world knowledge, and commonsense reasoning. Our findings show that adding simplified data improves fine-tuning and zero-shot performance over a repeated-exposure baseline: smaller models benefit from low-to-high complexity, while larger models perform better with interleaved ordering.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.468996",
        "filter_reason": "这篇论文的核心是研究在数据受限情况下如何通过课程学习和文本简化来优化语言模型的预训练过程，提高模型的表示质量和性能。根据筛选标准的第一步，这属于改进LLM基础能力的研究，提出了新的训练范式（课程学习结合文本简化），旨在增强模型的语言理解、常识推理等通用能力。论文评估了模型在\"常识推理\"等能力上的表现，虽然不是直接针对推理能力的研究，但其方法可以增强模型的通用推理能力。论文没有涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。"
    },
    {
        "index": "#58",
        "title": "Reinforcement Mid-Training",
        "link": "/arxiv/2509.24375",
        "arxiv_id": "2509.24375",
        "authors": "Yijun Tian, Shaoyu Chen, Zhichao Xu, Yawei Wang, Jinhe Bi, Peng Han, Wei Wang",
        "summary": "The development of state-of-the-art large language models is commonly understood as a two-stage process involving pre-training and post-training. We point out the need for an additional intermediate stage called reinforcement mid-training with potential for strong performance gains. In this paper, we formally define the problem and identify three key challenges: (1) inefficient training due to excessive reasoning steps, (2) disregard of the imbalanced token entropy distribution, and (3) underutilization of token information. To address these challenges, we propose RMT, a framework for efficient, adaptive, and unified reinforcement mid-training with various innovative components. In particular, we first introduce a dynamic token budget mechanism that constrains unnecessary reasoning steps and mitigates model overthinking. Next, we design a curriculum-based adaptive sampling method that fosters a progressive learning trajectory from easy to hard tokens. Finally, we present a dual training strategy that combines reinforcement learning with next-token prediction, ensuring targeted learning on key tokens and full exploitation of all token information. Extensive experiments demonstrate the superiority of RMT over state-of-the-art methods, achieving up to +64.91% performance improvement with only 21% of the reasoning length in language modeling. We also show that checkpoints obtained after reinforcement mid-training can benefit the subsequent post-training, yielding up to +18.76% improvement in the mathematical domain.",
        "subjects": "Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.468506",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"强化中训练\"(RMT)的新训练范式，用于提高大语言模型的推理能力和训练效率。从核心判断来看，论文本质上是改进LLM的基础能力，特别是通过动态token预算机制减少不必要的推理步骤，通过基于课程的自适应采样方法促进从易到难的学习轨迹，以及通过结合强化学习和下一个token预测的双重训练策略来充分利用token信息。这些方法都是直接提升LLM的通用推理能力，而不是将LLM作为工具应用到特定领域。论文在数学领域展示了改进，但这只是作为通用推理能力的一个示例，而不是论文的主要焦点。从正面指标来看，论文包含了LLM核心概念、推理能力（特别是数学推理）和强化学习方法。论文不符合任何排除标准，没有涉及多模态、特定应用领域或模型可靠性（应用层面）的内容。因此，这篇论文完全符合研究目标，即筛选出那些致力于提高大语言模型本身的通用推理能力的论文。"
    },
    {
        "index": "#70",
        "title": "Prompt and Parameter Co-Optimization for Large Language Models",
        "link": "/arxiv/2509.24245",
        "arxiv_id": "2509.24245",
        "authors": "Xiaohe Bo, Rui Li, Zexu Sun, Quanyu Dai, Zeyu Zhang, Zihang Tian, Xu Chen, Zhenhua Dong",
        "summary": "Prompt optimization and fine-tuning are two major approaches to improve the performance of Large Language Models (LLMs). They enhance the capabilities of LLMs from complementary perspectives: the former through explicit natural language, and the latter through implicit parameter updates. However, prior work has typically studied them in isolation, leaving their synergistic potential largely underexplored. To bridge this gap, in this paper, we introduce MetaTuner, a novel framework that jointly integrates prompt optimization and fine-tuning for LLM training. Specifically, we introduce two neural networks to generate prompts and parameters, respectively, while allowing them to share a common bottom encoding layer to enable knowledge sharing. By the guidance of the final supervised signals, our framework is optimized to discover the optimal combinations between the prompts and parameters. Given that prompt learning involves discrete optimization while fine-tuning operates in a continuous parameter space, we design a supervised regularization loss to train our framework effectively. Extensive experiments across diverse benchmarks show that our method consistently outperforms the baselines.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.479979",
        "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，论文本质上是关于改进LLM基础能力的研究，提出了一种名为MetaTuner的新框架，联合优化提示和参数来提升LLM性能。这是一种新的训练范式，属于增强LLM通用能力的方法论研究，而非将LLM作为工具应用于特定领域。 从正面指标看，论文明确关注\"Large Language Models (LLMs)\"这一核心概念；虽然未直接提及reasoning等词，但提示优化和参数微调都是提高LLM通用推理能力的方法；同时，论文提出了一种创新的训练方法，符合训练方法这一正面指标。 从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性等应排除的内容。 论文的核心贡献是提出了一种联合优化提示和参数的新框架，通过两个神经网络分别生成提示和参数，并允许它们共享底层编码层以实现知识共享。这种方法通过结合提示优化（显式自然语言）和微调（隐式参数更新）两种互补的方法，提升了LLM的通用能力，因此完全符合我筛选\"致力于提高大语言模型通用推理能力\"论文的研究目标。"
    },
    {
        "index": "#80",
        "title": "Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight",
        "link": "/arxiv/2509.24169",
        "arxiv_id": "2509.24169",
        "authors": "Haolin Yang, Hakaze Cho, Kaize Ding, Naoya Inoue",
        "summary": "Large Language Models (LLMs) can perform new tasks from in-context demonstrations, a phenomenon known as in-context learning (ICL). Recent work suggests that these demonstrations are compressed into task vectors (TVs), compact task representations that LLMs exploit for predictions. However, prior studies typically extract TVs from model outputs or hidden states using cumbersome and opaque methods, and they rarely elucidate the mechanisms by which TVs influence computation. In this work, we address both limitations. First, we propose directly training Learned Task Vectors (LTVs), which surpass extracted TVs in accuracy and exhibit superior flexibility-acting effectively at arbitrary layers, positions, and even with ICL prompts. Second, through systematic analysis, we investigate the mechanistic role of TVs, showing that at the low level they steer predictions primarily through attention-head OV circuits, with a small subset of \"key heads\" most decisive. At a higher level, we find that despite Transformer nonlinearities, TV propagation is largely linear: early TVs are rotated toward task-relevant subspaces to improve logits of relevant labels, while later TVs are predominantly scaled in magnitude. Taken together, LTVs not only provide a practical approach for obtaining effective TVs but also offer a principled lens into the mechanistic foundations of ICL.",
        "subjects": "Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.506873",
        "filter_reason": "这篇论文的核心是研究大型语言模型(LLMs)的上下文学习(ICL)机制，并提出了一种新的方法——学习任务向量(LTVs)来增强这一能力。从本质上看，论文关注的是改进LLM的基础能力而非将其作为工具应用到特定领域，这符合第一步的核心判断标准。论文明确研究LLMs的内部工作机制，提出了直接训练任务向量的新范式，而不是从模型中提取，这可以被视为一种新的训练方法来增强LLM的能力。虽然论文没有直接针对推理或规划，但上下文学习是LLM的一项基础能力，与通用问题解决相关。论文深入分析了任务向量如何通过注意力头的OV电路引导预测，以及它们在Transformer中的传播方式，这种机制层面的理解可能对改进LLM的通用推理能力有重要启示。论文不符合任何排除标准，没有涉及多模态、特定应用领域或应用层面的模型可靠性。因此，这篇论文符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究范围。"
    },
    {
        "index": "#76",
        "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play",
        "link": "/arxiv/2509.24193",
        "arxiv_id": "2509.24193",
        "authors": "Ran Xu, Yuchen Zhuang, Zihan Dong, Jonathan Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang",
        "summary": "Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at https://github.com/ritaranx/AceSearcher and https://huggingface.co/AceSearcher.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.488866",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，论文的本质是改进LLM的基础推理能力。AceSearcher提出了一个合作式自我对弈框架，训练单个大语言模型在分解器和求解器两个角色间切换，这是一种新的训练范式，旨在增强LLM的复杂推理能力。论文结合了监督微调和强化微调，优化最终答案准确性，这些都是提升LLM通用推理能力的方法论研究。 其次，论文包含多个正面指标主题： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：专注于推理能力(reasoning)，特别是复杂推理任务和多步推理 - 训练方法：使用强化学习(reinforcement learning)和自我对弈(self-play)框架 - 新兴范式：提出了类似智能体的框架，让同一模型在不同角色间切换 第三，论文不主要聚焦于任何排除标准中的领域。虽然提到了在金融推理任务上的实验，但这只是作为评估模型性能的一个应用场景，而非论文核心焦点。论文的核心是提出一种通用的推理和搜索增强框架。 最后，在特殊和模糊情况处理上，论文提出的自我对弈框架可以视为一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非应用在特定领域，因此符合保留标准。 综上所述，AceSearcher论文的核心贡献是提出了一种通过强化自我对弈来增强LLM推理和搜索能力的通用框架，完全符合\"提高大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#75",
        "title": "Can Large Language Models Express Uncertainty Like Human?",
        "link": "/arxiv/2509.24202",
        "arxiv_id": "2509.24202",
        "authors": "Linwei Tao, Yi-Fan Yeh, Bo Kai, Minjing Dong, Tao Huang, Tom A. Lamb, Jialin Yu, Philip H. S. Torr, Chang Xu",
        "summary": "Large language models (LLMs) are increasingly used in high-stakes settings, where overconfident responses can mislead users. Reliable confidence estimation has been shown to enhance trust and task accuracy. Yet existing methods face practical barriers: logits are often hidden, multi-sampling is computationally expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score) deviates from natural communication. We revisit linguistic confidence (LC), where models express uncertainty through hedging language (e.g., probably, might), offering a lightweight and human-centered alternative. To advance this direction, we (1) release the first diverse, large-scale dataset of hedging expressions with human-annotated confidence scores, and (2) propose a lightweight mapper that converts hedges into confidence scores at near-zero cost. Building on these resources, we (3) conduct the first systematic study of LC across modern LLMs and QA benchmarks, revealing that while most LLMs underperform in expressing reliable LC, carefully designed prompting achieves competitive calibration and discriminability. Finally, we (4) introduce a fine-tuning framework that further improves LC reliability. Taken together, our work positions linguistic confidence as a scalable, efficient, and human-aligned approach to LLM uncertainty estimation, and calls for deeper exploration of this promising yet underexplored direction.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.488259",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究大语言模型如何像人类一样表达不确定性，通过语言置信度(Linguistic Confidence, LC)的方式提升模型表达可靠不确定性的能力。这不是将LLM作为工具应用到特定领域的研究，而是专注于提升LLM本身的一种基础能力——准确评估和表达自己的知识边界和置信度。这种能力对于模型的推理质量和可靠性至关重要，因此符合保留标准。 第二步：正面指标 论文明确涉及\"Large language models, LLMs\"这一核心概念。虽然未直接提及数学推理、逻辑推理或规划，但不确定性表达与推理质量密切相关，可靠的置信度估计能增强推理任务的准确性和可信度，因此与能力方向间接相关。 第三步：排除标准 论文不符合任何排除标准。它不涉及多模态与视觉研究，不是针对特定应用领域（如医疗、化学等）的研究，也不主要关注模型可靠性的应用层面（如水印、安全等）。 第四步：特殊和模糊情况 论文提出了一种新方法（语言置信度LC和相关的微调框架）来增强模型表达不确定性的能力，这可以视为提升模型内在可靠性和推理质量的方法。通过更准确地表达不确定性，模型可以减少过度自信导致的误导性回答，从而间接减少幻觉和提高推理质量，符合保留标准。 综上所述，这篇论文的核心贡献是提出了一种提升大语言模型表达不确定性能力的方法，这属于改进LLM基础能力的研究，与提高模型通用推理能力的研究目标一致，因此应该被保留。"
    },
    {
        "index": "#81",
        "title": "Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis",
        "link": "/arxiv/2509.24164",
        "arxiv_id": "2509.24164",
        "authors": "Haolin Yang, Hakaze Cho, Naoya Inoue",
        "summary": "We investigate the mechanistic underpinnings of in-context learning (ICL) in large language models by reconciling two dominant perspectives: the component-level analysis of attention heads and the holistic decomposition of ICL into Task Recognition (TR) and Task Learning (TL). We propose a novel framework based on Task Subspace Logit Attribution (TSLA) to identify attention heads specialized in TR and TL, and demonstrate their distinct yet complementary roles. Through correlation analysis, ablation studies, and input perturbations, we show that the identified TR and TL heads independently and effectively capture the TR and TL components of ICL. Using steering experiments with geometric analysis of hidden states, we reveal that TR heads promote task recognition by aligning hidden states with the task subspace, while TL heads rotate hidden states within the subspace toward the correct label to facilitate prediction. We further show how previous findings on ICL mechanisms, including induction heads and task vectors, can be reconciled with our attention-head-level analysis of the TR-TL decomposition. Our framework thus provides a unified and interpretable account of how large language models execute ICL across diverse tasks and settings.",
        "subjects": "Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.507405",
        "filter_reason": "这篇论文符合我的研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的本质是研究大语言模型(LLM)的上下文学习(ICL)机制，通过分析注意力头来理解任务识别(TR)和任务学习(TL)的内部工作机制。论文提出了TSLA框架来识别专门用于TR和TL的注意力头，并揭示了它们在ICL过程中的不同作用。这属于对LLM基础能力和内部工作机制的研究，特别是关注上下文学习这种通用推理能力的机制分析，而不是将LLM作为工具应用到特定领域。因此，根据第一步的标准，应该保留。 第二步：正面指标 - 核心概念：论文明确研究大语言模型(LLMs)的上下文学习机制，符合这一指标。 - 能力方向：上下文学习(ICL)本身就是一种通用推理能力的体现，论文研究的是ICL的机制，涉及到任务识别和任务学习，这与推理能力直接相关，符合这一指标。 - 训练方法：论文没有明确提到强化学习、进化或自我进化等训练方法，不符合这一指标。 - 新兴范式：论文没有直接讨论基于LLM的智能体、多智能体系统、工具使用或深度研究等新兴范式，不符合这一指标。 第三步：排除标准 论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 第四步：特殊和模糊情况 论文关注的是LLM内部工作机制的可解释性，通过分析注意力头来理解上下文学习的机制。这属于\"增强模型内在的可解释性\"的情况，从而提升我们对模型推理能力的理解。根据标准，应该保留。 最终决策： 这篇论文的核心贡献是提供了一个统一且可解释的框架，用于理解大语言模型如何执行上下文学习。这种研究有助于我们深入理解LLM的通用推理机制，从而可能指导未来改进LLM推理能力的方法开发。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。"
    },
    {
        "index": "#83",
        "title": "Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE",
        "link": "/arxiv/2509.24130",
        "arxiv_id": "2509.24130",
        "authors": "Guancheng Wan, Lucheng Fu, Haoxin Liu, Yiqiao Jin, Hui Yi Leong, Eric Hanchen Jiang, Hejia Geng, Jinhe Bi, Yunpu Ma, Xiangru Tang, B. Aditya Prakash, Yizhou Sun, Wei Wang",
        "summary": "The performance of Large Language Models (LLMs) hinges on carefully engineered prompts. However, prevailing prompt optimization methods, ranging from heuristic edits and reinforcement learning to evolutionary search, primarily target point-wise accuracy. They seldom enforce paraphrase invariance or searching stability, and therefore cannot remedy this brittleness in practice. Automated prompt search remains brittle: small, semantically preserving paraphrases often cause large performance swings. We identify this brittleness as the textual sharpness of the prompt landscape. In this work, we provide the first formal treatment of textual sharpness in the discrete, semantic space of prompts, together with an operational robustness criterion over a semantic neighborhood; the design is black-box or API-only, requiring no gradients to update the model's parameters. Then we introduce TARE (Textual Sharpness-Aware Evolving), a derivative-free framework that alternates between an inner, sampling-based adversarial search that stresses a prompt with hard paraphrases and an outer, robust selection that prefers candidates whose neighborhoods remain strong. We further propose ATARE, which learns anisotropic weights to shape the semantic neighborhood and adapts its radius over time to balance exploration and fidelity. Diverse tasks evaluate our methods, whose design for minimizing textual sharpness gap leads to prompts that preserve accuracy under paraphrasing, outperforming accuracy-only prompt search while remaining computationally practical.",
        "subjects": "Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.508715",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是我的详细分析： 第一步：核心判断 这篇论文的本质是改进LLM的基础能力，具体关注提示词工程的鲁棒性问题。论文提出的TARE框架通过减少提示词在语义相同但表达不同情况下的性能波动，增强了LLM的稳定性和可靠性。提示词工程是直接影响LLM推理能力和输出质量的关键因素，因此这属于改进LLM基础能力的范畴，应当保留。 第二步：正面指标 - 核心概念：论文明确聚焦于Large Language Models (LLMs)，符合这一指标。 - 能力方向：虽然论文没有直接讨论推理能力，但提示词的鲁棒性直接影响LLM在推理任务中的表现。稳定的提示词可以确保LLM在面对不同表述的相同问题时保持一致的推理能力。 - 训练方法：论文提到了\"evolutionary search\"作为现有方法之一，并提出了\"Textual Sharpness-Aware Evolving\"(TARE)框架，涉及进化方法，部分符合这一指标。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域（如医疗、化学等） - 不主要关注模型可靠性的应用层面（如水印、安全等） 第四步：特殊和模糊情况 论文关注提示词鲁棒性，通过减少提示词微小变化导致的性能波动，间接提高了模型输出的稳定性和可靠性。这可以视为提升模型内在可靠性的方法，有助于提高LLM的通用推理质量，因此应当保留。 综合判断：该论文提出的TARE框架通过优化提示词的鲁棒性，提高了LLM在面对语义相同但表达不同的问题时的稳定性，这直接关系到LLM的通用推理能力和问题解决能力，符合研究范围的核心目标。"
    },
    {
        "index": "#85",
        "title": "Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems",
        "link": "/arxiv/2509.24116",
        "arxiv_id": "2509.24116",
        "authors": "Minsoo Kim, Seung-won Hwang",
        "summary": "LLM-based agents have seen promising advances, yet they are still limited in \"hard-exploration\" tasks requiring learning new knowledge through exploration. We present GLoW, a novel approach leveraging dual-scale world models, maintaining a trajectory frontier of high-value discoveries at the global scale, while learning from local trial-and-error in exploration through a Multi-path Advantage Reflection mechanism which infers advantage-based progress signals to guide exploration. To evaluate our framework for hard-exploration, we tackle the Jericho benchmark suite of text-based games, where GLoW achieves a new state-of-theart performance for LLM-based approaches. Compared to state-of-the-art RLbased methods, our approach achieves comparable performance while requiring 100-800x fewer environment interactions.",
        "subjects": "Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.509643",
        "filter_reason": "这篇论文的核心贡献是提出GLoW（双尺度世界模型），一种用于增强LLM智能体在\"hard-exploration\"任务中表现的新方法。根据筛选标准，这篇论文符合我的研究目标，原因如下： 首先，从本质上看，论文专注于提升LLM本身的基础能力，特别是探索和问题解决能力，而不是将LLM作为工具应用到特定领域。论文提出的双尺度世界模型和Multi-path Advantage Reflection机制，旨在增强LLM通过探索学习新知识的能力，这属于提升LLM通用推理能力的范畴。 其次，论文包含多个正面指标：核心概念上明确关注\"LLM-based agents\"；能力方向上涉及\"hard-exploration\"问题，这与问题解决能力直接相关；方法上提出了一种从试错中学习的机制，与强化学习有相似之处；并且明确关注LLM-based agents这一新兴范式。 第三，论文不符合任何排除标准。虽然使用了文本游戏作为评估基准，但这只是作为测试通用探索能力的平台，而不是专注于游戏领域本身。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。 最后，论文提出的是一种通用的智能体框架来增强LLM的探索能力，而不是将智能体应用在特定领域，这符合特殊情况的判断标准。 综上所述，这篇论文致力于提高LLM本身的通用推理能力（特别是探索和问题解决能力），符合我的研究范围。"
    },
    {
        "index": "#86",
        "title": "Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics",
        "link": "/arxiv/2509.24102",
        "arxiv_id": "2509.24102",
        "authors": "Guangliang Liu, Xi Chen, Bocheng Chen, Xitong Zhang, Kristen Johnson",
        "summary": "Moral reasoning has emerged as a promising research direction for Large Language Models (LLMs), yet achieving generalization remains a central challenge. From a linguistic standpoint, this difficulty arises because LLMs are adept at capturing distributional semantics, which fundamentally differs from the morals which operate at the pragmatic level. This paper investigates how LLMs can achieve generalized moral reasoning despite their reliance on distributional semantics. We propose pragmatic inference methods grounded in moral foundations theory, which leverage contextual information at each step to bridge the pragmatic gap and guide LLMs in connecting moral foundations with moral reasoning objectives. Experimental results demonstrate that our approach significantly enhances LLMs' generalization in moral reasoning, providing a foundation for future research grounded in moral foundations theory.",
        "subjects": "Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.510140",
        "filter_reason": "这篇论文符合我的研究目标，因为它的核心是提升大语言模型(LLMs)在道德推理方面的通用能力。根据筛选标准分析：首先，论文本质上是改进LLM的基础推理能力，提出了一种基于道德基础理论的语用推理方法，这属于\"增强其逻辑、多步推理等通用能力\"的范畴。道德推理可以被视为通用推理能力的一个重要子集，涉及逻辑判断和原则应用。其次，论文包含正面指标中的核心概念\"Large language models\"和能力方向\"reasoning\"。第三，论文不符合排除标准，虽然涉及道德推理，但并未将其作为特定应用领域（如社会学）的研究，而是将其视为提升LLM通用推理能力的一种途径。论文提出的语用推理方法旨在弥合LLM依赖的分布语义与道德推理所需的语用层面之间的差距，这实质上是增强模型内在推理机制的研究，符合\"提高大语言模型本身的通用推理能力\"的核心目标。"
    },
    {
        "index": "#89",
        "title": "Large-Scale Constraint Generation - Can LLMs Parse Hundreds of Constraints?",
        "link": "/arxiv/2509.24090",
        "arxiv_id": "2509.24090",
        "authors": "Matteo Boffa, Jiaxuan You",
        "summary": "Recent research has explored the constrained generation capabilities of Large Language Models (LLMs) when explicitly prompted by few task-specific requirements. In contrast, we introduce Large-Scale Constraint Generation (LSCG), a new problem that evaluates whether LLMs can parse a large, fine-grained, generic list of constraints. To examine the LLMs' ability to handle an increasing number constraints, we create a practical instance of LSCG, called Words Checker. In Words Checker, we evaluate the impact of model characteristics (e.g., size, family) and steering techniques (e.g., Simple Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet, a small and dedicated model that parses the original list of constraints into a smaller subset, helping the LLM focus on relevant constraints. Experiments reveal that existing solutions suffer a significant performance drop as the number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.511685",
        "filter_reason": "这篇论文的核心贡献是研究大语言模型(LLMs)处理大量约束的能力，提出了\"大规模约束生成\"(LSCG)这一新问题，并开发了FoCusNet方法来提升LLMs在这一任务上的表现。从本质上看，论文关注的是LLMs的基础能力——约束解析和遵循能力，这属于逻辑推理和问题解决的通用能力范畴，而不是将LLMs作为工具应用到特定领域。论文评估了不同模型特性和技术（包括思维链）对性能的影响，这些都是提升LLM通用推理能力的研究方向。论文不涉及多模态、特定应用领域或模型基础设施等排除标准中的内容。因此，这篇论文符合\"大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#88",
        "title": "GEAR: A General Evaluation Framework for Abductive Reasoning",
        "link": "/arxiv/2509.24096",
        "arxiv_id": "2509.24096",
        "authors": "Kaiyu He, Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Xinya Du, Zhiyu Chen",
        "summary": "Since the advent of large language models (LLMs), research has focused on instruction following and deductive reasoning. A central question remains: can these models discover new knowledge, and how can we evaluate this ability? We address this by studying abductive reasoning-the generation of plausible hypotheses to explain observations-and introduce GEAR (General Evaluation for Abductive Reasoning), a general-purpose, fully automated, transparent, and label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics: consistency (each hypothesis explains the observations), generalizability (consistent hypotheses make meaningful predictions on unseen inputs), and diversity (the set covers distinct predictions and patterns). Built this way, GEAR is scalable (no human gold answers), reliable (deterministic scoring aligned with classical abduction), and open-ended (scores improve only when models produce new plausible hypotheses, unlike static benchmarks that saturate once accuracy is high). Using GEAR, we conduct a fine-grained study of nine LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000 candidate hypotheses and revealing model differences obscured by gold-answer or purely human evaluations. We further propose a momentum-based curriculum that adjusts GEAR-derived training data by learning velocity: it starts with what the model learns quickly and shifts toward harder objectives such as generating diverse hypotheses once the model is confident on foundational objectives. Without gold-label supervision, this strategy improves all GEAR objectives and these gains transfer to established abductive reasoning benchmarks. Taken together, GEAR provides a principled framework that evaluates abduction and supplies label-free, scalable training signals that help LLMs produce more diverse and reliable hypotheses.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.511211",
        "filter_reason": "这篇论文的核心贡献是提出GEAR评估框架和基于动量的课程学习策略，专门用于评估和提升大语言模型的溯因推理能力。溯因推理(abductive reasoning)是一种重要的通用逻辑推理能力，涉及生成合理假设来解释观察到的现象，这直接符合研究目标中\"增强LLM逻辑推理能力\"的要求。 从筛选标准分析： 1. 核心判断：论文本质是改进LLM的基础推理能力，提出了新的评估框架和训练方法（基于动量的课程学习），而非将LLM作为工具应用于特定领域。 2. 正面指标：论文明确包含核心概念\"Large language models\"，能力方向聚焦于\"reasoning\"（特别是溯因推理这种逻辑推理），并提出了新的训练方法。 3. 排除标准：论文不涉及多模态与视觉、特定应用领域或模型基础设施等排除内容。 4. 特殊情况：论文关注提升模型生成可靠假设的能力，这与减少幻觉、增强模型内在可靠性相关，属于提升通用推理质量的范畴。 论文通过GEAR框架评估和改进LLM的溯因推理能力，这种能力是通用推理能力的重要组成部分，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#97",
        "title": "Toward Preference-aligned Large Language Models via Residual-based Model Steering",
        "link": "/arxiv/2509.23982",
        "arxiv_id": "2509.23982",
        "authors": "Lucio La Cava, Andrea Tagarelli",
        "summary": "Preference alignment is a critical step in making Large Language Models (LLMs) useful and aligned with (human) preferences. Existing approaches such as Reinforcement Learning from Human Feedback or Direct Preference Optimization typically require curated data and expensive optimization over billions of parameters, and eventually lead to persistent task-specific models. In this work, we introduce Preference alignment of Large Language Models via Residual Steering (PaLRS), a training-free method that exploits preference signals encoded in the residual streams of LLMs. From as few as one hundred preference pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be applied at inference time to push models toward preferred behaviors. We evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance. Moreover, when compared to DPO-aligned models, they perform better with huge time savings. Our findings highlight that PaLRS offers an effective, much more efficient and flexible alternative to standard preference optimization pipelines, offering a training-free, plug-and-play mechanism for alignment with minimal data.",
        "subjects": "Computation and Language, Artificial Intelligence, Computers and Society, Machine Learning, Neural and Evolutionary Computing",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.521223",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种名为PaLRS的新方法，用于改进大语言模型的基础能力。论文提出了一种训练-free的偏好对齐方法，通过利用LLM残差流中编码的偏好信号，提取轻量级转向向量来增强模型性能。这明显属于改进LLM本身的基础能力和提出新训练范式的研究，而不是将LLM作为工具应用到特定领域。 其次，论文满足多个正面指标： - 核心概念：明确关注大语言模型(LLMs) - 能力方向：特别强调了数学推理能力的提升，这正是通用推理能力的核心组成部分 - 训练方法：讨论了与RLHF等传统偏好对齐方法的对比，并提出了一种新的更高效的方法 第三，论文不符合任何排除标准。它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然论文提到了数学推理和代码生成，但这是作为评估模型通用推理能力的基准，而不是针对特定领域的应用。 特别值得注意的是，论文明确指出PaLRS方法\"保留了基线的通用性能\"，并在\"数学推理和代码生成基准上取得了一致的改进\"，这直接表明其研究目标是提升LLM的通用推理能力，而非特定领域应用。 综上所述，这篇论文的核心贡献是提出了一种新的、高效的偏好对齐方法来增强大语言模型的通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#102",
        "title": "Assessing Large Language Models in Updating Their Forecasts with New Information",
        "link": "/arxiv/2509.23936",
        "arxiv_id": "2509.23936",
        "authors": "Zhangdie Yuan, Zifeng Ding, Andreas Vlachos",
        "summary": "Prior work has largely treated future event prediction as a static task, failing to consider how forecasts and the confidence in them should evolve as new evidence emerges. To address this gap, we introduce EVOLVECAST, a framework for evaluating whether large language models appropriately revise their predictions in response to new information. In particular, EVOLVECAST assesses whether LLMs adjust their forecasts when presented with information released after their training cutoff. We use human forecasters as a comparative reference to analyze prediction shifts and confidence calibration under updated contexts. While LLMs demonstrate some responsiveness to new information, their updates are often inconsistent or overly conservative. We further find that neither verbalized nor logits-based confidence estimates consistently outperform the other, and both remain far from the human reference standard. Across settings, models tend to express conservative bias, underscoring the need for more robust approaches to belief updating.",
        "subjects": "Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.529290",
        "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)如何根据新信息更新预测的能力，属于对LLM通用推理能力的评估和研究。论文提出了EVOLVECAST框架来评估LLMs在面对新证据时如何调整预测和置信度，这直接涉及到模型的推理能力，特别是信念更新(belief updating)这一通用推理能力。从第一步核心判断来看，论文关注的是改进LLM的基础能力，而非将其作为工具应用到特定领域。论文满足正面指标中的核心概念(Large language models)和能力方向(reasoning)两个关键指标。同时，论文不符合任何排除标准，不涉及多模态、特定应用领域或模型基础设施等内容。虽然论文主要是评估性的而非提出新的改进方法，但它对LLM推理能力的深入理解和评估对于进一步改进LLM的通用推理能力具有重要意义，符合\"提高大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#99",
        "title": "HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs",
        "link": "/arxiv/2509.23967",
        "arxiv_id": "2509.23967",
        "authors": "Ken Deng, Zizheng Zhan, Wen Xiang, Wenqiang Zhu, Tianhao Peng, Xinping Lei, Weihao Li, Jingxuan Xu, Kun Wu, Yifan Yao, Haoyang Huang, Huaixi Tang, Kepeng Lei, Zhiyi Lai, Songwei Yu, Zongxian Feng, Zuchen Gao, Weihao Xie, Chenchen Zhang, Yanan Wu, Yuanxing Zhang, Lecheng Huang, Yuqun Zhang, Jie Liu, Zhaoxiang Zhang, Haotian Zhang, Bin Chen, Jiaheng Liu",
        "summary": "Large Language Models (LLMs) increasingly rely on chain-of-thought (CoT) reasoning to improve accuracy on complex tasks. However, always generating lengthy reasoning traces is inefficient, leading to excessive token usage and higher inference costs. This paper introduces the Hybrid Policy Optimization (i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to selectively decide when to engage in detailed reasoning (Think-on) and when to respond directly (Think-off). Specifically, HiPO combines a hybrid data pipelineproviding paired Think-on and Think-off responseswith a hybrid reinforcement learning reward system that balances accuracy and efficiency while avoiding over-reliance on detailed reasoning. Experiments across mathematics and coding benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy. Finally, we hope HiPO a can be a principled approach for efficient adaptive reasoning, advancing the deployment of reasoning-oriented LLMs in real-world, resource-sensitive settings.",
        "subjects": "Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.527735",
        "filter_reason": "根据筛选标准，这篇论文完全符合研究范围。首先，从核心判断来看，论文的本质是改进大语言模型的基础推理能力，提出了混合策略优化(HiPO)这一新框架，使LLM能够自适应地选择推理策略（何时进行详细推理，何时直接响应）。这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、多步推理等通用能力\"的标准。 其次，论文包含多个正面指标：明确关注Large Language Models (LLMs)这一核心概念；专注于reasoning能力（特别是在数学和编码基准测试中验证）；采用了reinforcement learning作为训练方法（混合强化学习奖励系统）。 第三，论文不符合任何排除标准：不涉及多模态与视觉内容；不聚焦于特定应用领域（虽然使用数学和编码基准测试，但这些是通用推理能力的评估，而非特定领域应用）；不关注模型可靠性层面的水印、安全等问题。 论文的核心贡献是提出了一种能动态控制推理过程的新方法，在保持或提高准确性的同时显著提高推理效率，这直接服务于提升LLM的通用推理能力这一研究目标。因此，这篇论文应该被保留。"
    },
    {
        "index": "#103",
        "title": "Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step",
        "link": "/arxiv/2509.23924",
        "arxiv_id": "2509.23924",
        "authors": "Jingyi Yang, Guanxu Chen, Xuhao Hu, Jing Shao",
        "summary": "Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.529793",
        "filter_reason": "这篇论文的核心贡献是提出了解决Masked Diffusion Language Models (MDLMs)训练和推理不一致性的新方法，包括EOSER和ASS解码调度器以及CJ-GRPO强化学习算法。从第一步核心判断来看，论文本质上是关于改进LLM的基础能力和提出新的训练范式的研究，特别是增强模型的推理能力，这符合保留标准。从第二步正面指标看，论文涉及大语言模型(MDLMs)、推理能力(明确在数学和规划基准测试上实验)以及强化学习方法(CJ-GRPO)，满足多个正面指标。从第三步排除标准看，论文虽然标题中提到\"Diffusion\"，但这是指语言模型架构而非视觉领域的扩散模型，且论文没有聚焦于特定应用领域或模型可靠性的应用层面问题。综合分析，这篇论文致力于提高大语言模型本身的通用推理能力，通过新的解码策略和强化学习算法来优化模型性能，完全符合\"大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#106",
        "title": "SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models",
        "link": "/arxiv/2509.23863",
        "arxiv_id": "2509.23863",
        "authors": "Ziyi Yang, Weizhou Shen, Ruijun Chen, Chenliang Li, Fanqi Wan, Ming Yan, Xiaojun Quan, Fei Huang",
        "summary": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models.",
        "subjects": "Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.531494",
        "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出SPELL（一种多角色自我博弈强化学习框架）来提高大语言模型的长上下文推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力（长上下文推理），提出了新的训练范式（自我博弈强化学习），属于增强LLM通用推理能力的研究。从第二步正面指标看，论文包含了所有关键主题：核心概念（LLMs）、能力方向（reasoning）、训练方法（reinforcement learning, self-evolve）和新兴范式（multi-agent systems）。论文不符合第三步的任何排除标准，因为它不涉及多模态、特定应用领域或模型可靠性的应用层面研究。在第四步特殊情况处理中，论文提出的多角色框架是一种通用的智能体协作方法，用于增强LLM的通用问题解决能力，而非针对特定领域。总之，SPELL框架通过自我博弈和强化学习直接提升了LLM的通用推理能力，与研究目标高度一致。"
    },
    {
        "index": "#109",
        "title": "Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions",
        "link": "/arxiv/2509.23782",
        "arxiv_id": "2509.23782",
        "authors": "Yoonah Park, Haesung Pyun, Yohan Jo",
        "summary": "Large Language Models (LLMs) often fail on multiple-choice questions (MCQs) despite demonstrating correct knowledge in other contexts, such as free-form generation. To investigate the mechanism underlying this knowledge-prediction gap on MCQs and alleviate it, we conduct a probing analysis and find that residual streams in certain layers contain a subspace spanned by two important bases: a \\emph{knowledge basis} that encodes the probability of the ground-truth answer for a given MCQ and a \\emph{prediction basis} that encodes the probability of the answer choice predicted by the model. We observe that incorrect predictions arise from a misalignment of the model's hidden states along these two bases. Hence, we introduce \\textbf{KAPPA} (Knowledge-Aligned Prediction through Projection-based Adjustment), a parameter-free intervention that transforms the hidden states to align the prediction coordinate with the knowledge coordinate within this subspace. Experiments on binary-choice reformulations of Big-Bench-Hard and ARC-Challenge show that KAPPA substantially improves accuracy and consistently outperforms baselines. While optimal subspaces differ across tasks, subspaces generalize to some extent, as supported by cross-dataset experiments. Moreover, KAPPA extends its effectiveness to free-form questions beyond MCQs. Our work provides a new geometric understanding of the knowledge-prediction gap and offers a practical method for better aligning model behavior with its latent knowledge.",
        "subjects": "Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.538097",
        "filter_reason": "我按照筛选标准对这篇论文进行了全面分析，判断其符合研究目标。 首先，从核心判断来看，这篇论文的本质是研究大语言模型(LLMs)在多选题场景中的\"知识-预测差距\"问题，并提出了解决方案。论文的核心贡献是发现LLM内部存在知识基和预测基两个重要维度，并提出了KAPPA方法来对齐这两个维度，从而提高模型的推理准确性。这明显属于改进LLM基础推理能力的研究，而非将LLM作为工具应用到特定领域。 其次，从正面指标看，论文明确研究Large language models (LLMs)，并聚焦于reasoning和problem-solving能力，特别是解决多选题这类需要推理的任务。虽然论文没有涉及强化学习、智能体等新兴范式，但已足够表明其与研究目标相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉内容，不专注于医疗、化学等特定应用领域，也不讨论水印、安全等模型可靠性问题。 最后，论文提出的KAPPA方法是一种通用的干预方法，用于对齐模型的预测与其潜在知识，这可以被视为提升LLM通用推理能力的方法，而非针对特定领域的应用。论文还指出该方法的效果可以扩展到多选题以外的自由形式问题，进一步证明其通用性。 综上所述，这篇论文致力于提高大语言模型的通用推理能力，符合研究目标的核心要求。"
    },
    {
        "index": "#111",
        "title": "Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality",
        "link": "/arxiv/2509.23765",
        "arxiv_id": "2509.23765",
        "authors": "Junliang Li, Yucheng Wang, Yan Chen, Yu Ran, Ruiqing Zhang, Jing Liu, Hua Wu, Haifeng Wang",
        "summary": "Hallucination and factuality deficits remain key obstacles to the reliability of large language models (LLMs) in long-form generation. Existing reinforcement learning from human feedback (RLHF) frameworks primarily rely on preference rewards, yet they often overlook the model's internal knowledge boundaries, exacerbating the so-called \"hallucination tax\". To address this challenge, we propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a novel framework that focuses on the knowledge consistency between the policy model's expressed knowledge and the base model's parametric knowledge, and introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall and precision. Specifically, KLCF leverages pretrained knowledge boundaries to construct fact checklist, guiding online reinforcement learning to improve factual coverage and recall; simultaneously, it trains a self-assessment module based on the base model's internal knowledge to enhance factual precision during generation. Unlike prior methods that rely on external retrieval or heavy verification, our reward design is fully external-knowledge-free and lightweight, making KLCF efficient and easily scalable to large-scale training. Experimental results demonstrate that KLCF substantially improves factuality metrics across multiple long-form benchmarks and effectively alleviates model hallucinations.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.539253",
        "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，特别是减少幻觉和提高事实性，这直接关联到LLM的通用推理能力。论文提出了\"Knowledge-Level Consistency Reinforcement Learning Framework (KLCF)\"这一新的训练范式，通过强化学习方法优化模型的事实表达，属于提升LLM核心能力的研究。 从正面指标看，论文明确涉及大语言模型(LLMs)这一核心概念，使用了强化学习(RL)作为训练方法，虽然未直接提及\"reasoning\"一词，但减少幻觉和提高事实性是推理能力的重要组成部分，特别是在长文本生成中保持事实一致性需要强大的推理能力。 论文不符合排除标准，它没有聚焦于多模态与视觉、特定应用领域或应用层面的模型可靠性研究。相反，它针对的是LLM的通用问题——幻觉和事实性缺陷。 在特殊和模糊情况处理上，论文提出了减少幻觉的新方法，通过知识一致性强化学习来提升模型的内在可靠性和推理质量，这正是提升LLM通用推理能力的关键方向。 综上所述，这篇论文的核心贡献是提出了一种新的强化学习框架来提高LLM的事实性和减少幻觉，这直接关联到提升LLM的通用推理能力，因此符合我的研究目标。"
    },
    {
        "index": "#119",
        "title": "Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs",
        "link": "/arxiv/2509.23657",
        "arxiv_id": "2509.23657",
        "authors": "Shulin Huang, Yiran Ding, Junshu Pan, Yue Zhang",
        "summary": "Enhancing the complex reasoning capabilities of Large Language Models (LLMs) attracts widespread attention. While reinforcement learning (RL) has shown superior performance for improving complex reasoning, its impact on cross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains unexplored. We present the first systematic investigation into cross-lingual reasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation model, we conduct experiments on diverse multilingual reasoning benchmarks, including math reasoning, commonsense reasoning, and scientific reasoning. Our investigation yields two significant findings: (1) Tuning with RL not only achieves higher accuracy but also demonstrates substantially stronger cross-lingual generalization capabilities compared to SFT. (2) RL training on non-English data yields better overall performance and generalization than training on English data, which is not observed with SFT. Furthermore, through comprehensive mechanistic analyses, we explore the underlying factors of RL's superiority and generalization across languages. Our results provide compelling evidence that RL enables the model with more robust reasoning strategies, offering crucial guidance for more equitable and effective multilingual reasoning.",
        "subjects": "Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.548440",
        "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是研究如何通过强化学习(RL)来提高大语言模型(LLM)的推理能力，特别是跨语言推理能力，这直接属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。论文不是将LLM作为工具应用到特定领域，而是专注于提升模型本身的通用推理能力。 其次，论文包含多个正面指标：核心概念上明确研究Large language models (LLMs)；能力方向上聚焦于reasoning，特别是math reasoning、commonsense reasoning和scientific reasoning；训练方法上使用reinforcement learning作为主要技术手段。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 论文的核心贡献在于系统研究了强化学习相比监督微调在提升LLM跨语言推理能力方面的优势，并发现RL训练能使模型获得更强大的推理策略。这直接服务于提升大语言模型通用推理能力的研究目标，因此应该被保留。"
    },
    {
        "index": "#120",
        "title": "Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models",
        "link": "/arxiv/2509.23653",
        "arxiv_id": "2509.23653",
        "authors": "Zemin Huang, Yuhang Wang, Zhiyang Chen, Guo-Jun Qi",
        "summary": "Mask-based Diffusion Language Models (DLMs) struggle to revise incorrect tokens: once a token is generated, it typically remains fixed. The key challenge is to identify potential errors in the inputs. In this paper, we propose \\emph{\\underline{Rem}asking-\\underline{e}nabled \\underline{Di}ffusion Language Model (RemeDi}, a mask-based DLM that introduces \\emph{remasking} as another fundamental mechanism, enabling more flexible text refinement in diffusion-based text generation. To achieve this, RemeDi jointly predicts token distributions and per-token confidence scores at each step. The confidence scores determine which tokens to be unmasked after the current step, allowing the model to identify tokens with low quality and remask them. These remasked tokens can be resampled with richer context in subsequent steps. We design a remask-aware pipeline to train this ability, including supervised fine-tuning which teaches the model to detect and remask incorrect tokens in addition to predict mask tokens, and reinforcement learning which optimizes full generation trajectories toward higher rewards. Experiments show that RemeDi achieves the state-of-the-art results among open-source DLMs on multiple datasets.",
        "subjects": "Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.548889",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"RemeDi\"的扩散语言模型，通过引入\"重新掩码\"(remasking)机制，使模型能够识别低质量token并进行重新采样，从而增强模型的自我修正和推理能力。论文设计了一个包含监督微调和强化学习的训练管道来优化这种能力。这种方法本质上是提升大语言模型的基础能力和通用推理能力，特别是通过自反思机制来改进输出质量，符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的要求。论文使用了强化学习作为训练方法，这是正面指标之一。虽然论文没有明确讨论数学或逻辑推理，但其提出的自我修正机制可以提升模型的问题解决能力。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。相反，论文提出的方法可以看作是提升模型内在可靠性和推理质量的新方法，这符合保留标准。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#121",
        "title": "Fast Thinking for Large Language Models",
        "link": "/arxiv/2509.23633",
        "arxiv_id": "2509.23633",
        "authors": "Haoyu Zheng, Zhuonan Wang, Yuqian Yuan, Tianwei Lin, Wenqiao Zhang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang, Hongyang He",
        "summary": "Reasoning-oriented Large Language Models (LLMs) often rely on generating explicit tokens step by step, and their effectiveness typically hinges on large-scale supervised fine-tuning or reinforcement learning. While Chain-of-Thought (CoT) techniques substantially enhance performance on complex reasoning tasks, they remain inefficient, requiring long reasoning traces that increase latency and token usage. In this work, we introduce Latent Codebooks for Fast Thinking, a framework that uses concise CoT sketches only during training to learn a codebook of discrete strategy priors. At inference, the model conditions on a handful of continuous thinking vectors distilled from the codebook in a single pass, enabling strategy-level guidance without producing explicit reasoning tokens. To complement this design, we propose GainRouter, a lightweight routing mechanism that adaptively switches between fast codebook guided inference and slow explicit reasoning, thereby suppressing overthinking and reducing unnecessary token generation. Experiments across multiple reasoning benchmarks show that our approach achieves competitive or superior accuracy while substantially lowering inference cost, offering a practical path toward efficient and controllable reasoning in large language models.",
        "subjects": "Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.549441",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础推理能力，提出了\"Latent Codebooks for Fast Thinking\"框架，旨在解决传统思维链(CoT)技术效率低下的问题。这明显属于改进LLM通用推理能力的研究，而非将LLM应用于特定领域。 其次，论文符合多个正面指标：核心概念上明确关注\"Reasoning-oriented Large Language Models (LLMs)\"；能力方向上专注于推理(reasoning)，并在多个推理基准上进行了实验验证；虽然未直接使用强化学习等方法，但提到了与这些方法的对比。 第三，论文不符合任何排除标准：没有涉及多模态与视觉内容，没有将LLM应用于特定领域，也没有主要关注模型可靠性的应用层面问题。 论文的核心贡献是提出了一种新的训练和推理范式，通过在训练阶段使用简洁的CoT草图学习离散策略先验的码本，在推理阶段使用少量连续思维向量进行策略级引导，从而提高推理效率。这种方法直接增强了LLM的通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#125",
        "title": "Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales",
        "link": "/arxiv/2509.23574",
        "arxiv_id": "2509.23574",
        "authors": "Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Yang Xiang, Buzhou Tang",
        "summary": "Chain-of-thought (CoT) distillation aims to enhance small language models' (SLMs) reasoning by transferring multi-step reasoning capability from the larger teacher models. However, existing work underestimates rationale quality, focusing primarily on data quantity, which may transfer noisy or incorrect information to the student model. To address the above issues, we proposed \\textbf{M}odel-\\textbf{O}riented \\textbf{R}ationale \\textbf{S}election \\textbf{D}istillation (MoRSD), which can discern and select high quality rationales for distillation to improve performance further. We further propose a Rationale Difficulty (RD) metric to measure the ability of the student model to generate the correct answer under a given rationale. Compared to the baseline, we achieved 4.6$\\%$ average improvement on seven datasets over three tasks, using fewer rationales by controlling their accuracy, diversity, and difficulty. Our results reveal that a small portion of the high quality rationales can enhance the reasoning ability of student models than the entire dataset. Our method promises to be a possible solution for efficient CoT distillation. Our code will be released in https://github.com/Leon221220/MoRSD.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.551472",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进语言模型的基础推理能力，它聚焦于思维链(CoT)蒸馏技术，提出了一种新的训练范式MoRSD（模型导向的推理选择蒸馏），旨在通过选择高质量的推理过程来增强小型语言模型的推理能力。这直接符合我们对提高LLM通用推理能力的研究目标。 其次，论文包含多个正面指标：核心概念涉及语言模型(SLMs和大型教师模型)，能力方向明确关注推理(reasoning)特别是多步推理能力，这些都是我们研究范围的核心要素。 第三，论文不涉及任何排除标准领域，它没有关注多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是提出了一种高效的CoT蒸馏方法，通过控制推理过程的准确性、多样性和难度，使用更少的高质量推理就能显著提升小型模型的推理能力。这种方法论研究直接针对提升LLM的通用推理能力，而非将LLM作为工具应用于特定领域，因此完全符合我们的研究目标。"
    },
    {
        "index": "#122",
        "title": "Timber: Training-free Instruct Model Refining with Base via Effective Rank",
        "link": "/arxiv/2509.23595",
        "arxiv_id": "2509.23595",
        "authors": "Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Zenan Xu, Ngai Wong",
        "summary": "Post-training, which elicits a pretrained Base model into the corresponding Instruct model, is widely considered to be superficial. In this work, we first reinforce this hypothesis by providing novel quantitative evidence from the weight level that the effective rank (eRank) remains negligibly changed. However, this superficiality also suffers a critical trade-off, improving the exploitation capabilities at the cost of limiting its exploration. To tackle this issue, we propose Timber, a simple yet effective training-free method that enhances the exploration capability of the Instruct model while preserving its exploitation. The key insight is to partially revert Instruct towards the paired Base model by subtle yet targeted refinement of the weight deltas. Extensive experiments on Llama and Qwen series demonstrate that Timber consistently improves vanilla Instruct models, particularly on Pass@k performance. Our findings offer new insights into the post-training stage at the weight level and practical strategies to refine the Instruct model without training.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.549991",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Timber\"的无训练方法，通过微调权重差异来部分地将Instruct模型恢复到Base模型的状态，以增强Instruct模型的探索能力(exploration capability)同时保持其利用能力(exploitation)。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从本质上看，这篇论文是关于改进LLM基础能力的研究，属于后训练优化范畴，而不是将LLM作为工具应用到特定领域。论文提出的Timber方法直接针对LLM的内在能力进行改进，符合\"改进LLM的基础能力\"这一核心判断标准。 其次，论文明确涉及大语言模型（Llama和Qwen系列）这一核心概念，并通过增强模型的探索能力来提升其整体性能。虽然论文没有直接提及reasoning、planning或problem-solving等具体能力方向，但探索能力的提升往往与模型的推理能力和问题解决能力密切相关，特别是在处理复杂任务时。 第三，论文不涉及任何排除标准中提到的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，虽然论文没有涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况，但其提出的无训练方法为改进LLM的通用能力提供了新的视角，这与研究目标\"提高大语言模型（LLM）本身的通用推理能力\"是一致的。论文中提到的Pass@k性能提升也表明该方法可能对模型的推理能力有积极影响。 综上所述，这篇论文符合研究范围，应该被保留。"
    },
    {
        "index": "#130",
        "title": "The Impact of Role Design in In-Context Learning for Large Language Models",
        "link": "/arxiv/2509.23501",
        "arxiv_id": "2509.23501",
        "authors": "Hamidreza Rouzegar, Masoud Makrehchi",
        "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to generate predictions based on prompts without additional fine-tuning. While prompt engineering has been widely studied, the impact of role design within prompts remains underexplored. This study examines the influence of role configurations in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models' performance across datasets, focusing on tasks like sentiment analysis, text classification, question answering, and math reasoning. Our findings suggest the potential of role-based prompt structuring to enhance LLM performance.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.559043",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究提示工程中的角色设计如何影响大语言模型在上下文学习(ICL)中的表现。论文不是将LLM作为工具应用到特定领域，而是研究如何通过改进提示设计来提升LLM的基础能力。论文特别评估了角色设计对模型在数学推理等任务上的影响，这直接关联到提升LLM的通用推理能力。因此，根据第一步的判断标准，应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念: 明确研究Large Language Models (LLMs)，包括GPT-3.5、GPT-4o、Llama2等模型 - 能力方向: 特别提到了math reasoning（数学推理），这正是通用推理能力的核心组成部分 虽然论文没有涉及reinforcement learning、llm-based agents等新兴范式，但它包含了足够的核心正面指标。 第三步：排除标准 论文不聚焦于任何需要排除的领域： - 不涉及多模态与视觉相关内容 - 不专注于特定应用领域（如医疗、化学等），虽然评估了多种任务，但这些是通用任务而非特定领域应用 - 不讨论模型可靠性方面的watermarking、safety等问题 第四步：特殊和模糊情况 论文研究的是通过角色设计这种提示工程方法来增强LLM的通用能力，特别是推理能力。这与\"智能体/工具使用\"的情况类似，都是提出一种通用方法来增强LLM的问题解决能力，因此应该保留。 综合判断：这篇论文的核心贡献是研究角色设计如何提升LLM在多种任务（包括数学推理）上的表现，这直接关联到提升LLM的通用推理能力，符合研究目标。因此，最终判断为True。"
    },
    {
        "index": "#132",
        "title": "Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models",
        "link": "/arxiv/2509.23441",
        "arxiv_id": "2509.23441",
        "authors": "Xuanming Zhang, Yuxuan Chen, Min-Hsuan Yeh, Yixuan Li",
        "summary": "Large language models (LLMs) excel at complex reasoning but can still exhibit harmful behaviors. Current alignment strategies typically embed safety into model weights, making these controls implicit, static, and difficult to modify. This paper introduces Cognition-of-Thought (CooT), a novel decoding-time framework that equips LLMs with an explicit cognitive self-monitoring loop. CooT couples a standard text Generator with a cognitive Perceiver that continuously monitors the unfolding sequence. The Perceiver uses a structured, precedence-based hierarchy of principles (e.g., safety over obedience) to detect potential misalignments as they arise. When violations are flagged, CooT intervenes by rolling back the generation to the point of error and regenerating under injected guidance that combines universal social priors with context-specific warnings. CooT thus transforms alignment from a fixed property into an explicit, dynamic, and auditable process active during inference, allowing for flexible policy updates without retraining the model. Extensive experiments across multiple benchmarks and model families confirm that CooT consistently improves safety and social reasoning performance.",
        "subjects": "Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.560080",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出Cognition-of-Thought (CooT)框架，这是一种新的解码时推理框架，旨在增强LLM的认知自我监控能力。论文的核心贡献不是将LLM作为工具应用到特定领域，而是改进LLM本身的基础推理能力，特别是社会对齐推理能力。这符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的保留标准。 其次，从正面指标分析，论文明确关注大语言模型(LLMs)这一核心概念，并直接针对推理能力(reasoning)，特别是社会对齐推理(social-aligned reasoning)。虽然不是传统意义上的训练方法，但CooT框架代表了一种新兴的推理范式，通过自我监控循环增强模型的推理过程。 第三，论文不符合排除标准。它不涉及多模态与视觉内容，不专注于特定应用领域（如医疗、化学等），虽然涉及安全性，但是从提升模型内在推理质量的角度出发，而非单纯的应用层面研究。 在处理特殊和模糊情况时，虽然论文涉及安全性，但它是通过提出一种新的推理框架(CooT)来增强模型的社会对齐推理能力，从而提升模型的通用推理质量，这符合\"提出新方法来增强模型内在可靠性，从而提升模型的通用推理质量\"的保留标准。 综上所述，这篇论文的核心贡献是提出一种新的推理框架来增强LLM的通用推理能力，特别是社会对齐推理能力，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#140",
        "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization",
        "link": "/arxiv/2509.23371",
        "arxiv_id": "2509.23371",
        "authors": "Junming Yang, Ning Xu, Biao Liu, Shiqi Qiao, Xin Geng",
        "summary": "Preference optimization is crucial for aligning large language models (LLMs) with human values and intentions. A significant challenge in this process is the distribution mismatch between pre-collected offline preference data and the evolving model policy. Existing methods attempt to reduce this gap using static heuristics or decoupled online sampling strategies, but they often fail to adapt to the model's dynamic learning state. To bridge this gap, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework that dynamically couples data generation with model training. MetaAPO employs a lightweight meta-learner, as an \"alignment gap estimator\", to evaluate the potential benefits of on-policy sampling in relation to offline data. This guides targeted online generation and assigns sample-wise meta-weights to the optimization objective, dynamically balancing the quality and distribution of online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench demonstrate that MetaAPO consistently outperforms existing preference optimization approaches across various settings, while reducing 42% in online annotation costs.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.569353",
        "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式MetaAPO（Meta-Weighted Adaptive Preference Optimization），用于优化大语言模型与人类价值观的对齐过程。这属于改进LLM基础能力的研究，而非将LLM作为工具应用于特定领域。 其次，从正面指标看，论文明确包含\"Large language models, LLMs\"这一核心概念，并且讨论的\"preference optimization\"与强化学习方法（如RLHF）密切相关，是提升LLM性能的重要训练范式。虽然论文没有直接提及推理能力，但通过改进模型对齐过程，可能间接提升模型的整体推理和问题解决能力。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，论文提出的是一种通用的优化框架，旨在解决模型训练中的数据分布不匹配问题，这属于基础模型研究的范畴，与提高LLM通用能力的目标一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#145",
        "title": "Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces",
        "link": "/arxiv/2509.23291",
        "arxiv_id": "2509.23291",
        "authors": "Joseph Marvin Imperial, Harish Tayyar Madabushi",
        "summary": "Policy compliance assessment is a fundamental task of evaluating whether an input case strictly complies with a set of human-defined rules, more generally known as policies. In practice, human experts follow a systematic, step-by-step process to identify violations with respect to specific stipulations outlined in the policy. However, such documentation of gold-standard, expert-level reasoning processes is costly to acquire. In this paper, we introduce Policy Reasoning Traces (PRT), a form of specialized generated reasoning chains that serve as a reasoning bridge to improve an LLM's policy compliance assessment capabilities. Our empirical evaluations demonstrate that the use of PRTs for both inference-time and training-time scenarios significantly enhances the performance of open-weight and commercial models, setting a new state-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also highlight how PRTs can improve an LLM's ability to accurately cite policy clauses, as well as influence compliance decisions through their high utilization from the raw chains of thought.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.571926",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文的核心是提出\"Policy Reasoning Traces (PRT)\"，一种专门的生成推理链形式，作为推理桥梁来提高LLM的政策合规评估能力。这本质上是一种改进LLM推理能力的新方法，类似于思维链(CoT)的变体，专门针对政策合规任务。论文关注的是提升LLM的基础推理能力，而非将LLM作为工具应用到特定领域，因此符合保留标准。 第二步正面指标：论文明确包含多个正面指标： - 核心概念：研究的是语言模型(LLMs)的政策合规评估能力 - 能力方向：直接涉及推理(reasoning)能力，特别是政策合规评估中的逻辑推理(logical reasoning)和问题解决(problem-solving) - 论文提出的PRT方法可以视为一种增强模型推理能力的新范式 第三步排除标准：虽然论文提到了HIPAA和GDPR政策，可能看似涉及特定应用领域，但论文的核心不是解决医疗或数据保护领域的具体问题，而是提高LLM在政策合规评估这一通用推理任务上的能力。政策合规评估本身是一种通用推理任务，类似于逻辑推理，因此不应被排除。 第四步特殊和模糊情况：论文提出的PRT方法能提高LLM\"准确引用政策条款\"的能力，这与增强模型内在的可解释性和推理质量相关，符合保留标准。 综合判断：这篇论文的核心贡献是提出一种新的推理方法(PRT)来增强LLM的通用推理能力，特别是在政策合规评估任务上。虽然论文使用HIPAA和GDPR作为评估案例，但其研究目标是提升LLM的通用推理能力，而非专注于特定应用领域。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#144",
        "title": "Learning to Reason in Structured In-context Environments with Reinforcement Learning",
        "link": "/arxiv/2509.23330",
        "arxiv_id": "2509.23330",
        "authors": "Peng Yu, Zeyuan Zhao, Shao Zhang, Luoyi Fu, Xinbing Wang, Ying Wen",
        "summary": "Large language models (LLMs) have achieved significant advancements in reasoning capabilities through reinforcement learning (RL) via environmental exploration. As the intrinsic properties of the environment determine the abilities that LLMs can learn, the environment plays a important role in the RL finetuning process. An ideal LLM reasoning environment should possess three core characteristics: scalability, generalizable reasoning, and verifiability. However, existing mathematical and coding environments are difficult to scale due to heavy reliance on expert annotation, while the skills learned in game-based environments are too specialized to generalize. To bridge this gap, we introduce the \\textbf{S}tructured \\textbf{I}n-context \\textbf{E}nvironment (SIE) framework. SIE achieves scalability by automatically constructing reasoning environments from large-scale structured data, where the rich compositional patterns naturally support generalizable reasoning. Moreover, the explicit schemas and reasoning chains in structured data provide a foundation for rule-based verifiability. Experimental results show that SIE framework not only achieves substantial improvements in in-domain structured reasoning, but also enables the learned compositional reasoning skills to generalize effectively to out-of-domain mathematical and logical reasoning tasks. We further explored learning in information-limited partial SIEs and found that LLMs can infer the missing information through exploring the environment, leading to robust reasoning improvements and generalization performance.",
        "subjects": "Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.571411",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。以下是我的详细判断过程： 第一步：核心判断——论文的核心是关于改进LLM的推理能力。论文提出了SIE框架，这是一种新的训练/微调范式，通过强化学习和环境设计来增强LLM的推理能力。论文明确关注的是提升模型的逻辑推理和数学推理等通用能力，而不是将LLM作为工具应用于特定领域。 第二步：正面指标——论文包含多个关键正面指标： - 核心概念：明确提及\"Large language models (LLMs)\" - 能力方向：多次强调\"reasoning capabilities\"、\"generalizable reasoning\"、\"mathematical and logical reasoning\" - 训练方法：核心方法为\"reinforcement learning (RL)\"，通过环境探索进行微调 - 新兴范式：虽然未直接提及智能体，但\"environmental exploration\"概念与智能体交互环境的思想一致 第三步：排除标准——论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉内容 - 未专注于医疗、化学、生物等特定应用领域 - 未主要讨论水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况处理——论文中的环境探索概念是为了提升通用推理能力，而非应用于特定领域，符合保留条件。 核心贡献分析：论文提出的SIE框架通过结构化上下文环境和强化学习来提升LLM的推理能力，使模型能够学习可泛化的组合推理技能，并有效应用于数学和逻辑推理任务。这一贡献直接针对提升LLM的通用推理能力，完全符合研究目标。论文不仅关注了推理能力的提升，还通过实验验证了这些能力的泛化性，这正是\"通用推理能力\"研究的核心。"
    },
    {
        "index": "#150",
        "title": "PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness",
        "link": "/arxiv/2509.23206",
        "arxiv_id": "2509.23206",
        "authors": "Huacan Chai, Zijie Cao, Maolin Ran, Yingxuan Yang, Jianghao Lin, pengxin, Hairui Wang, Renjie Ding, Ziyu Wan, Muning Wen, Weiwen Liu, Weinan Zhang, Fei Huang, Ying Wen",
        "summary": "Large language models (LLMs) have achieved impressive success in single-turn function calling, yet real-world applications such as travel planning or multi-stage data analysis typically unfold across multi-turn conversations. In these settings, LLMs must not only issue accurate function calls at each step but also maintain progress awareness, the ability to summarize past interactions and plan future actions to ensure coherent, long-horizon task execution. Existing approaches, however, either reduce multi-turn training to isolated single-turn samples, which neglects task-level planning, or employ end-to-end reinforcement learning (RL) that struggles with redundancy and lacks explicit integration of progress awareness. To overcome these limitations, we introduce PARL-MT, a framework that explicitly incorporates progress awareness into LLM training for multi-turn function calling. PARL-MT combines (i) a Progress Awareness Generation (PAG) pipeline, which automatically constructs datasets coupling conversation summaries with future task planning, and (ii) a Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which integrates progress awareness into RL training to reduce contextual redundancy and improve alignment between local actions and global task completion. Empirical results on two public benchmarks demonstrate that PARL-MT significantly outperforms existing methods, highlighting the effectiveness of progress awareness in enabling robust and efficient multi-turn function calling.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.672588",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：论文的核心是关于改进LLM在多轮对话中调用函数的能力，提出了PARL-MT框架，通过引入进度感知(progress awareness)来增强模型的规划和多步推理能力。这属于提高LLM本身的通用推理能力的研究，特别是规划和多步推理方面，因此应该保留。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确讨论了大型语言模型(LLMs) - 能力方向：强调了\"progress awareness\"（进度感知），涉及总结过去交互和规划未来行动的能力，直接对应planning和problem-solving能力 - 训练方法：提出了\"Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm\"，使用了强化学习方法 - 新兴范式：讨论了\"function calling\"（函数调用），属于tool use的范畴 第三步排除标准：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然提到\"travel planning or multi-stage data analysis\"等应用场景，但只是作为例子，论文本身是提出通用框架而非针对特定领域 - 不主要聚焦于模型可靠性方面的研究 第四步特殊和模糊情况：论文讨论的\"function calling\"属于工具使用范畴，且PARL-MT是一种通用框架，用于增强LLM在多轮对话中调用函数的能力，属于通用的工具使用方法，而非针对特定领域的应用，因此应该保留。 综合判断：这篇论文的核心贡献是提出了一种增强LLM通用推理能力（特别是规划和多步推理）的新方法，通过引入进度感知和强化学习来改进多轮函数调用，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#156",
        "title": "Pretraining LLM with Latent Thoughts in Continuous Space",
        "link": "/arxiv/2509.23184",
        "arxiv_id": "2509.23184",
        "authors": "Boyi Zeng, He Li, Shixiang Song, Yixuan Wang, Ziwei He, Xinbing Wang, Zhouhan Lin",
        "summary": "The remarkable success of Chain-of-Thought (CoT), which enhances performance by scaling generation steps at test-time, inspires us to ask: can we leverage a similar scaling of computational steps during pretraining to improve the generation of each individual token? To address this, we propose a novel pre-training methodology: Pretraining Language Models with Latent Thoughts. Our approach pretrains a language model (LM) to first generate an intermediate latent thought-the last hidden state of the current position-which is then used as input to predict the actual subsequent token. This additional computational step enables the LM to refine its prediction within unconstrained continuous space. Our experiments demonstrate that, at an identical inference cost, a LM that generates one additional latent thought per token outperforms a standard model with double the parameters. For instance, ours-1.4B (Pythia Arch), pretrained on 300B tokens from the Pile, significantly surpasses the vanilla Pythia-2.8B trained on the same data on both language modeling and a range of general downstream tasks. Furthermore, increasing the number of latent thoughts generated before each actual token-forming a chain analogous to CoT-consistently improves the model's performance.",
        "subjects": "Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.681456",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的本质是提出一种新的预训练方法论\"Pretraining Language Models with Latent Thoughts\"，该方法通过在预训练阶段引入中间\"潜在思维\"来增强语言模型的推理能力。这明显属于改进LLM基础能力和提出新训练范式的研究，直接受到思维链(CoT)的启发，旨在提升模型的通用推理能力，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标主题。核心概念方面明确聚焦于大语言模型(LLMs)；能力方向方面，论文直接关联推理能力，特别是通过形成\"类似于CoT的链\"来增强模型推理；训练方法方面，提出了一种新的预训练范式，虽然不是强化学习或进化方法，但同样是改进模型训练的创新方法。 第三步排除标准：论文完全不涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)等排除领域，纯粹关注提升LLM本身的通用能力。 第四步特殊和模糊情况：论文不涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等问题。 核心贡献是提出了一种在连续空间中使用潜在思维预训练LLM的新方法，通过增加计算步骤来增强模型的推理能力，这直接符合提高大语言模型通用推理能力的研究目标。实验证明该方法能显著提升模型性能，且增加潜在思维数量可持续改善模型表现，进一步证实了其对推理能力的增强作用。"
    },
    {
        "index": "#153",
        "title": "From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs",
        "link": "/arxiv/2509.23196",
        "arxiv_id": "2509.23196",
        "authors": "Haonan Wang, Weida Liang, Zihang Fu, Nie Zheng, Yifan Zhang, Yao Tong, Tongyao Zhu, Hao Jiang, Chuang Li, Jiaying Wu, Kenji Kawaguchi",
        "summary": "Recent reasoning LLMs (RLMs), especially those trained with verifier-based reinforcement learning, often perform worse with few-shot CoT than with direct answering. We revisit this paradox using high-quality reasoning traces from DeepSeek-R1 as demonstrations and find that adding more exemplars consistently degrades accuracy, even when demonstrations are optimal. A detailed analysis reveals two mechanisms behind this decline: (i) semantic misguidance, where high textual similarity leads the model to treat the target as the same as the exemplar and to copy intermediate steps verbatim; and (ii) strategy transfer failure, where the model struggles to extract useful reasoning strategies and apply them to target questions. Guided by these, we introduce Insight-to-Solve (I2S), a sequential test-time procedure that turns demonstrations into explicit, reusable insights and derives a target-specific reasoning trace; optionally, the reasoning is self-refined for coherence and correctness (I2S+). Extensive experiments on diverse benchmarks show that I2S and I2S+ consistently outperform both direct answering and test-time scaling baselines across open- and closed-source models. Even for GPT models, our method helps: on AIME'25, GPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on GPQA, indicating that in-context demonstrations can be harnessed effectively via insight-refine-solve framework.",
        "subjects": "Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.674503",
        "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的推理能力，而非将其应用于特定领域。论文提出了Insight-to-Solve (I2S)方法，这是一种测试时程序，将上下文示例转化为明确的、可重用的见解，并推导出针对特定问题的推理轨迹，直接针对提升LLM的通用推理能力。 从正面指标来看，论文明确涉及\"reasoning LLMs (RLMs)\"这一核心概念，并聚焦于reasoning能力（特别是在AIME和GPQA等数学和逻辑推理基准测试上）。同时，论文提到了\"verifier-based reinforcement learning\"和\"self-refined\"等与强化学习和自我进化相关的训练方法。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然论文处理了模型推理过程中的问题（语义误导和策略转移失败），但这是从提升模型内在推理质量的角度出发，而非对这些现象的社会学研究或应用层面讨论。 综上所述，这篇论文的核心贡献是提出了一种新的方法来增强LLM的通用推理能力，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。"
    },
    {
        "index": "#158",
        "title": "Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models",
        "link": "/arxiv/2509.23146",
        "arxiv_id": "2509.23146",
        "authors": "Zichao Yu, Ming Li, Wenyi Zhang, Weiguo Gao",
        "summary": "Tree search has recently emerged as a powerful framework for aligning generative models with task-specific rewards at test time. Applying tree search to Masked Diffusion Language Models, however, introduces two key challenges: (i) parallel unmasking yields highly correlated branches, limiting exploration, and (ii) reward evaluation via sampled completions produces high-variance estimates, making pruning unstable. We propose TReASURe, a tree-search test-time alignment method that addresses these issues. It introduces (i) UnmaskBranch, a branching strategy based on first-hitting unmasking that diversifies both token content and reveal order with a single model call per parent node, and (ii) ResubstituteScore, a pruning rule that uses deterministic resubstitution to score partially masked sequences with low-variance proxy completions. Theoretically, we quantify branching efficiency gains in NFEs (number of function evaluations), show that the scoring rule approximates the true reward with error bounded by predictive uncertainty, and prove improvements with larger tree widths. Empirically, TReASURe achieves state-of-the-art results on perplexity, linguistic acceptability, and control of sentiment and toxicity, outperforming prior methods under matched compute budgets, with especially strong gains in low-NFE regimes.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.682497",
        "filter_reason": "我按照筛选标准对这篇论文进行了详细分析： 第一步：核心判断 这篇论文的本质是提出一种名为TReASURe的树搜索测试时间对齐方法，用于改进掩码扩散语言模型的生成能力。论文的核心贡献是解决树搜索应用于掩码扩散语言模型时的两个关键挑战：(1)并行去掩码导致分支高度相关，限制探索；(2)通过采样完成进行奖励评估产生高方差估计，使修剪不稳定。论文提出的UnmaskBranch分支策略和ResubstituteScore修剪规则，旨在增强语言模型的生成质量和控制能力，这属于改进LLM基础能力和推理能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标 - 核心概念：论文明确研究\"Masked Diffusion Language Models\"，属于LLM范畴。 - 能力方向：树搜索是一种规划方法，与problem-solving相关，论文通过改进树搜索来增强模型的推理和规划能力。 - 训练方法：论文提到\"reward-aligned\"（奖励对齐），这与强化学习中的奖励概念相关。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉领域，虽然提到\"Diffusion\"，但指的是语言模型中的扩散模型，而非视觉扩散模型。 - 不聚焦于特定应用领域，而是关注通用语言模型的改进。 - 虽然提到\"control of sentiment and toxicity\"，但这只是作为评估指标，不是论文主要焦点。 第四步：特殊和模糊情况 论文没有明确讨论智能体/工具使用，也没有专门针对幻觉、可解释性或安全性的改进，因此不适用于这些特殊情况的判断。 最终决策：这篇论文符合研究范围，因为它致力于通过改进树搜索方法来增强大语言模型本身的通用推理能力和生成质量，而非将LLM作为工具应用到特定领域。论文提出的TReASURe方法是一种通用的测试时间对齐技术，能够提升语言模型的基础能力，符合\"提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。"
    },
    {
        "index": "#157",
        "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs",
        "link": "/arxiv/2509.23166",
        "arxiv_id": "2509.23166",
        "authors": "Chenxing Wei, Hong Wang, Ying He, Fei Yu, Yao Shu",
        "summary": "Large Language Models (LLMs) employ multi-turn interaction as a fundamental paradigm for completing complex tasks. However, their performance often degrades in extended interactions, as they are typically trained on static, single-turn data, which hinders their ability to adapt to real-time user feedback. To address this limitation, we first propose a new paradigm: Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes user feedback from the ongoing interaction as a reward signal to estimate a latent optimal policy aligned with user preferences, then updates a small subset of parameters to steer the model toward this policy, ultimately enabling efficient in-conversation self-correction. We then introduce Optimum-Referenced One-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM. ROSA guides the model parameters toward a theoretical optimal policy in a single, efficient update step, avoiding costly iterative gradient-based optimization and minimizing computational overhead. We provide a rigorous theoretical analysis guaranteeing that the policy of ROSA converges to the preference of user as the number of interactions increases. Extensive experiments on challenging benchmark demonstrate that ROSA achieves significant improvements in both task effectiveness and efficiency.",
        "subjects": "Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.681960",
        "filter_reason": "这篇论文完全符合我的研究目标，因为它专注于提高大语言模型本身的通用推理能力。具体分析如下： 第一步：核心判断——论文的本质是改进LLM的基础能力。论文提出了测试时策略自适应(T2PAM)和ROSA算法，使LLM能够在多轮交互中根据用户反馈实时调整自己的策略，实现对话中的自我修正。这明显是关于提升LLM基础推理和适应能力的研究，而非将LLM作为工具应用到特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念: 明确关注Large Language Models (LLMs) - 能力方向: 涉及reasoning和problem-solving，特别是复杂任务的多轮交互推理能力 - 训练方法: 利用用户反馈作为奖励信号来优化模型策略，与强化学习思想一致 - 新兴范式: 关注LLM在多轮交互中的表现，与智能体交互和自我进化相关 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于医疗、化学、生物等特定应用领域 - 不主要讨论水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况——论文提出的是一种通用的自适应机制，用于增强LLM在多轮交互中的推理和适应能力，而不是针对特定领域的应用，因此应该保留。 论文的核心贡献是提出了一种新的范式(T2PAM)和算法(ROSA)，使LLM能够在测试时根据用户反馈进行策略自适应，从而提升其在复杂多轮交互中的推理能力和自我修正能力。这直接关系到提升LLM的通用推理能力，完全符合我的研究目标。"
    },
    {
        "index": "#155",
        "title": "Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts",
        "link": "/arxiv/2509.23188",
        "arxiv_id": "2509.23188",
        "authors": "Guancheng Wan, Leixin Sun, Longxu Dou, Zitong Shi, Fang Wu, Eric Hanchen Jiang, Wenke Huang, Guibin Zhang, Hejia Geng, Xiangru Tang, Zhenfei Yin, Yizhou Sun, Wei Wang",
        "summary": "Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly advanced collaborative reasoning, tool use, and role-specialized coordination in complex tasks. However, reliability-critical deployment remains hindered by a systemic failure mode: hierarchical compliance under instruction conflicts (system-user, peer-peer), where agents misprioritize system-level rules in the presence of competing demands. Moreover, widely used macro-level metrics (e.g., pass@k) obscure these micro-level violations and offer little actionable guidance for remedy. In this work, we present a full-stack, three-stage framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a query-wise, context-aware scoring metric that decomposes role adherence into four measurable dimensions; (2) Localize - attention drift analysis revealing that instruction conflicts are resolved by attention heads that are largely concentrated in middle layers; (3) Align - Surgical Alignment of Instruction Layers (SAIL), which installs LoRA only on the localized focal layers and optimizes a token-weighted DPO-style preference objective that credits tokens by their focal attentional contribution. Across standard benchmarks and MAS frameworks, our surgical approach improves instruction hierarchy compliance (e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.",
        "subjects": "Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.680906",
        "filter_reason": "这篇论文的核心贡献是提出了一种全栈框架（Diagnose, Localize, Align）来解决LLM多智能体系统在指令冲突下的可靠性问题。从本质上看，论文专注于改进LLM多智能体系统的基础能力，特别是协作推理和指令遵从性，这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。 论文包含了多个正面指标：核心概念上明确关注LLM驱动的多智能体系统；能力方向上涉及协作推理；新兴范式上专注于多智能体系统和工具使用。论文提出的CRAS评分、注意力漂移分析和SAIL对齐方法都是为了增强模型对指令层级结构的理解，从而提高其在复杂任务中的推理能力。 虽然论文在实验中使用了MedQA作为评估基准，但这只是验证方法有效性的手段，而不是论文的主要焦点。论文的核心是提出一种通用的框架，可以应用于各种多智能体系统，而不是针对特定领域的问题。因此，这篇论文符合研究范围，应该被保留。"
    },
    {
        "index": "#159",
        "title": "Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning",
        "link": "/arxiv/2509.23140",
        "arxiv_id": "2509.23140",
        "authors": "Song Jin, Juntian Zhang, Yong Liu, Xun Zhang, Yufei Zhang, Fei Jiang, Guojun Yin, Wei Lin, Rui Yan",
        "summary": "Recent advancements have endowed Large Language Models (LLMs) with impressive general reasoning capabilities, yet they often struggle with personalization reasoning - the crucial ability to analyze user history, infer unique preferences, and generate tailored responses. To address this limitation, we introduce TagPR, a novel training framework that significantly enhances an LLM's intrinsic capacity for personalization reasoning through a tagging the thought approach. Our method first develops a data-driven pipeline to automatically generate and semantically label reasoning chains, creating a structured dataset that fosters interpretable reasoning. We then propose a synergistic training strategy that begins with Supervised Fine-Tuning (SFT) on this tagged data to establish foundational reasoning patterns, followed by a multi-stage reinforcement learning (RL) process. This RL phase is guided by a unique composite reward signal, which integrates tag-based constraints and a novel Personalization Reward Model with User Embeddings (PRMU) to achieve fine-grained alignment with user-specific logic. Extensive experiments on the public LaMP benchmark and a self-constructed dataset demonstrate that our approach achieves state-of-the-art results, delivering an average improvement of 32.65% over the base model across all tasks. Our work validates that structured, interpretable reasoning is a highly effective pathway to unlocking genuine personalization capabilities in LLMs.",
        "subjects": "Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.683076",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为TagPR的新训练框架，用于增强大语言模型(LLM)的个性化推理能力。从筛选标准来看，该论文符合我的研究目标，理由如下： 首先，在核心判断层面，论文的本质是改进LLM的基础推理能力，而非将其作为工具应用到特定领域。论文提出了\"标记思维\"的新方法，结合监督微调和多阶段强化学习过程来增强模型的推理能力，这符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理等通用能力\"的保留标准。 其次，论文包含多个正面指标：明确关注大语言模型(LLMs)这一核心概念；聚焦于推理能力(reasoning)，特别是个性化推理；采用了强化学习(RL)作为关键训练方法；通过创建结构化数据集促进可解释推理，这与提升模型内在能力的方向一致。 在排除标准方面，论文没有涉及多模态与视觉、特定应用领域(如医疗、化学等)或模型可靠性的应用层面问题。虽然论文关注\"个性化推理\"，但这应被视为通用推理能力的一个特定方面，而非针对特定领域的应用研究。 在特殊和模糊情况处理上，论文提出的\"标记思维\"方法增强了模型的可解释性，这属于\"增强模型内在的可解释性\"的情况，应当保留。这种方法通过提升推理的可解释性来提高模型的推理质量，而非仅进行社会学研究或应用层面的讨论。 综上所述，该论文致力于通过新方法提升LLM的通用推理能力，符合我的研究范围。"
    },
    {
        "index": "#167",
        "title": "Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents",
        "link": "/arxiv/2509.23040",
        "arxiv_id": "2509.23040",
        "authors": "Yaorui Shi, Yuxin Chen, Siyuan Wang, Sihang Li, Hengxing Cai, Qi Gu, Xiang Wang, An Zhang",
        "summary": "Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory corpus that is dynamically updated during a single-pass document scan, also known as the \"memorize while reading\" methods. While this approach scales efficiently, it suffers from irreversible forward-only processing, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced memory that allows selective retrieval from the entire memory history and allows non-linear reasoning and revisiting of early evidence. To further strengthen training, we propose Reinforcement Learning with Multi-Level Rewards (RLMLR), which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support multi-hop memory utilizing. Experiments on long-document QA show significant gains over existing memory-based approaches, which validates ReMemR1 as an effective solution for long-context reasoning agents.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.692643",
        "filter_reason": "这篇论文的核心贡献是提出ReMemR1（一种带有回调增强记忆的记忆增强LLM智能体）和多级奖励强化学习(RLMLR)训练方法，旨在解决LLM在长上下文推理中的挑战。从第一步核心判断来看，论文本质上是改进LLM的基础推理能力，特别是长上下文推理能力，而不是将LLM作为工具应用到特定领域。论文提出的记忆机制允许非线性推理和重新审视早期证据，这直接增强了LLM的通用推理能力。从第二步正面指标看，论文包含多个相关主题：大型语言模型(LLMs)、推理能力(reasoning)、强化学习(RL)和LLM智能体(llm-based agents)。从第三步排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。在第四步特殊情况处理中，论文提出的是一种通用的智能体框架来增强LLM的推理能力，而非应用于特定领域。综合分析，这篇论文明确符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#164",
        "title": "From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents",
        "link": "/arxiv/2509.23071",
        "arxiv_id": "2509.23071",
        "authors": "Muzhi Li, Jinhu Qi, Yihong Wu, Minghao Zhao, Liheng Ma, Yifan Li, Xinyu Wang, Yingxue Zhang, Ho-fung Leung, Irwin King",
        "summary": "Retrieval-augmented generation agents development is hindered by the lack of process-level supervision to effectively guide agentic capabilities like task decomposition, retriever invocation, and stepwise decision-making. While reinforcement learning offers a potential solution, it suffers from sparse rewards and the limited reasoning capabilities of large language models (LLMs). Meanwhile, existing data synthesis methods only produce chain-of-thought rationales and fail to model environmental interactions. In this paper, we propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG agent development. EviPath comprises: (i) Abductive Subtask Planning, which decomposes the problem into sub-questions and iteratively plans an optimal solution path based on the dependencies between them; (ii) Faithful Sub-question Answering, which uses supporting evidence to construct a proxy environment to generate reasoning thoughts and answers for each sub-question; and (iii) Conversational Fine-Tuning, which formats the complete agent-environment interaction trajectory into a dialogue format suitable for Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and tool-use capabilities directly from synthesized data. Extensive experiments on widely-used question-answering benchmarks show that an 8B parameter model trained with EviPath-synthesized data significantly and consistently outperforms state-of-the-art baselines with a double-digit absolute EM gain of 14.7% in open-domain question answering.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.690954",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。 首先，从核心判断来看，该论文的本质是提出一种新的训练范式(EviPath)来增强大语言模型的推理能力和工具使用能力。论文关注的是如何通过合成数据和特定的训练方法来提升LLM的通用推理能力，包括任务分解、检索器调用和逐步决策等智能体能力。这明显是关于改进LLM基础能力和通用推理能力的研究，而不是将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标主题： - 核心概念：明确讨论large language models (LLMs) - 能力方向：聚焦于溯因推理(Abductive Reasoning)、推理路径合成、任务分解和逐步决策等推理和规划能力 - 新兴范式：研究检索增强生成智能体(RAG agents)和工具使用能力(tool-use capabilities) 第三，论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学等），而是提出通用方法 - 不讨论模型可靠性方面的水印、安全等问题 最后，关于智能体/工具使用的特殊处理，论文提出的是一种通用的智能体协作框架来增强LLM的通用问题解决能力，而不是将智能体应用在特定领域，因此符合保留标准。 综上所述，该论文的核心贡献是提出一种新的证据锚定推理路径合成范式，用于提升大语言模型的通用推理能力和工具使用能力，完全符合研究目标。"
    },
    {
        "index": "#165",
        "title": "Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks",
        "link": "/arxiv/2509.23067",
        "arxiv_id": "2509.23067",
        "authors": "Chunyang Jiang, Yonggang Zhang, Yiyang Cai, Chi-Min Chan, Yulong Liu, Mingming Chen, Wei Xue, Yike Guo",
        "summary": "The rising cost of acquiring supervised data has driven significant interest in self-improvement for large language models (LLMs). Straightforward unsupervised signals like majority voting have proven effective in generating pseudo-labels for verifiable tasks, while their applicability to unverifiable tasks (e.g., translation) is limited by the open-ended character of responses. As a result, self-evaluation mechanisms (e.g., self-judging and entropy minimization) are predominantly used to derive pseudo-labels. However, self-evaluation relying on LLMs typically incurs high computational overhead and introduces overconfidence issues due to intrinsic biases. To address these challenges, we propose a novel self-evaluation-free approach for unverifiable tasks, designed for lightweight yet effective self-improvement. Inspired by majority voting commonly employed in verifiable tasks, we propose semantic voting as a novel mechanism that relaxes the principle of hard matching (i.e., exact matching) toward soft matching (i.e., semantic similarity). Soft matching is achieved by leveraging a lightweight sentence embedding model to quantify semantic similarity, thereby mitigating excessive computational burden and intrinsic bias-associated limitations of self-evaluation. Comprehensive experiments demonstrate that our method achieves substantial gains in computational efficiency and overall better performance than self-evaluation methods across diverse model architectures and tasks.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.691533",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步核心判断：这篇论文的本质是提出一种名为\"语义投票\"(Semantic Voting)的新方法，用于大语言模型在不可验证的开放性任务上的自我改进。这明显属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用于特定领域。论文关注的是LLM的通用能力提升，特别是通过自我改进机制来增强模型性能，符合保留标准。 第二步正面指标：论文明确包含核心概念\"Large language models, LLMs\"；虽然未直接强调推理能力，但开放性任务通常需要推理能力，且论文提出的语义投票方法可以提升LLM在这些任务上的表现；论文关注自我改进(self-improvement)，这与自我进化(self-evolve)概念相关，属于训练方法的范畴。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域，完全避开了这些排除标准。 第四步特殊和模糊情况：论文提到自我评估机制的\"过度自信问题\"和\"内在偏差\"，这与幻觉问题相关。论文提出的方法旨在减轻这些问题，从而提升模型的通用可靠性和推理质量，符合保留标准。 最终决策：论文的核心贡献是提出了一种通用的自我改进方法，通过语义投票机制提升LLM在开放性任务上的表现，减轻了传统自我评估方法的计算负担和内在偏差。这直接关系到提升LLM的通用推理能力，特别是在处理需要复杂推理的开放性任务时。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#177",
        "title": "HEART: Emotionally-driven test-time scaling of Language Models",
        "link": "/arxiv/2509.22876",
        "arxiv_id": "2509.22876",
        "authors": "Gabriela Pinto, Palash Goyal, Yiwen Song, Souradip Chakraborty, Zifeng Wang, Tomas Pfister, Hamid Palangi",
        "summary": "Test-time scaling has shown considerable success in improving the performance of language models on complex reasoning tasks without requiring fine-tuning. However, current strategies such as self-reflection primarily focus on logical or structural refinement. They do not leverage the guiding potential of affective feedback. Inspired by psychological research showing that emotions can modulate cognitive performance, we introduce HEART--a novel framework that uses emotionally-driven prompts for iterative self-correction. HEART provides feedback on a model's incorrect response using a curated set of concise, emotionally charged phrases based on the six universal emotions categorized by Dr. Paul Ekman. By systematically varying the emotional tone of the feedback across iterations, our method guides the model to escape flawed reasoning paths and explore more promising alternatives. We evaluate our framework on challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam, and SimpleQA. Our results reveal a significant new phenomenon: when guided by an oracle verifier, this affective iteration protocol unlocks significantly deeper reasoning, leading to consistent and substantial increases in accuracy over state-of-the-art baselines with the same verifier. However, we also identify a critical bottleneck for practical deployment. In a verifier-free setting, it struggles to harness these gains consistently, highlighting as a key challenge for future work. Our findings suggest that the next frontier in machine reasoning may lie not just in refining logic, but also in understanding and leveraging the `HEART' of the models.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.703105",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为HEART的新框架，通过情感驱动的提示进行迭代自我修正，以提高大语言模型在复杂推理任务上的表现。该方法基于心理学研究，使用情感化的反馈来引导模型摆脱有缺陷的推理路径，探索更有希望的替代方案。论文在多个具有挑战性的推理基准上评估了该方法，结果表明当由验证器引导时，这种情感迭代协议能解锁更深层次的推理，显著提高准确性。这完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求，因为论文关注的是改进LLM的基础推理能力，提出了一种新的测试时扩展范式，而不是将LLM作为工具应用到特定领域。论文的主题与正面指标中的\"大语言模型\"和\"推理能力\"高度吻合，同时不涉及任何排除标准中的领域。"
    },
    {
        "index": "#172",
        "title": "What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?",
        "link": "/arxiv/2509.22947",
        "arxiv_id": "2509.22947",
        "authors": "Mohammed Sabry, Anya Belz",
        "summary": "Does explicitly exercising the induction circuit during pretraining improve in-context learning (ICL), or is natural text sufficient when compute is held constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight curriculum that injects forward-copy (Induction), backward-copy (Anti), or a balanced mix into the pretraining stream. We train models from 0.13B to 1B parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii) head-level telemetry, and (iii) held-out language modeling perplexity. Our findings challenge the assumption that early induction circuit activation directly improves ICL. While Bi-Induct accelerates induction-head emergence at small scales, this does not consistently yield stronger generalization. On standard LM benchmarks, Bi-Induct matches natural-only training; on function-style ICL probes, the 1B natural-only performs best. Stress tests (e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these trends. Telemetry shows larger natural-only models develop broader, earlier induction heads without explicit induction patterns. Anti-induction data fails to elicit meaningful activation. Perplexity penalties from synthetic data shrink with scale, suggesting larger models can absorb non-natural patterns with minimal cost. Crucially, ablating the top 2% of induction heads degrades ICL more than random ablations, especially for natural-only models, indicating more centralized, load-bearing circuits. Bi-Induct variants exhibit more redundant induction activity, implying different circuit utilization. Overall, inducing activation is not sufficient: ICL gains depend on these circuits becoming functionally necessary. These results underscore mechanism-aware pretraining diagnostics and data mixtures that foster load-bearing, not merely present, structure.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.695204",
        "filter_reason": "这篇论文符合我的研究目标，因为它专注于提升大语言模型的基础通用推理能力，而非将LLM作为工具应用于特定领域。 从第一步核心判断来看，论文的本质是研究如何通过预训练数据的设计来增强大语言模型的上下文学习(ICL)能力。ICL是大语言模型的一种核心通用推理能力，允许模型通过少量示例学习新任务。论文提出了Bi-Induct方法，通过在预训练流中注入目标合成数据来研究归纳电路的发展，这属于改进LLM基础能力的研究范畴，而非将LLM应用于特定领域。 从第二步正面指标看，论文明确包含核心概念\"Large language models\"，并研究\"reasoning\"能力方向中的上下文学习(ICL)，这是大语言模型进行多步推理和问题解决的基础能力。 从第三步排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，因此不符合任何排除标准。 论文的核心贡献在于探索了预训练数据设计对LLM通用推理能力(ICL)的影响，挑战了早期归纳电路激活直接改善ICL的假设，并提出了机制感知的预训练诊断方法。这些研究直接关注如何提升大语言模型本身的通用推理能力，符合我的研究目标。"
    },
    {
        "index": "#176",
        "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents",
        "link": "/arxiv/2509.22887",
        "arxiv_id": "2509.22887",
        "authors": "EunJeong Hwang, Yuwei Yin, Giuseppe Carenini, Peter West, Vered Shwartz",
        "summary": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.702547",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：这篇论文的本质是提升LLM的通用推理能力，特别是社交推理能力。论文提出ToMAgent (ToMA)，通过将心智理论(ToM)与对话前瞻配对来训练模型，这是一种新的训练范式，旨在增强LLM的基础推理能力，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确聚焦于\"LLM-based social agents\" - 能力方向：涉及推理能力，特别是社交推理和目标导向的推理行为 - 新兴范式：涉及LLM智能体框架 第三步排除标准：论文不符合任何排除标准。虽然涉及社交智能，但这是在提升LLM的通用推理能力，而非将LLM应用到特定社会学研究领域。社交智能是人类智能的核心组成部分，提升LLM的社交推理能力属于提升其通用推理能力的范畴。 第四步特殊情况处理：论文提出的ToMAgent是一种通用的智能体框架，专注于提升LLM的社交推理能力，符合\"提出通用的智能体协作框架来增强LLM的通用问题解决能力\"的情况。 综合分析，论文的核心贡献是通过引入心智理论和新的训练范式来提升LLM的社交推理能力，这是一种通用推理能力的提升，符合研究目标。"
    },
    {
        "index": "#180",
        "title": "Towards Generalizable Implicit In-Context Learning with Attention Routing",
        "link": "/arxiv/2509.22854",
        "arxiv_id": "2509.22854",
        "authors": "Jiaqian Li, Yanshu Li, Ligong Han, Ruixiang Tang, Wenya Wang",
        "summary": "Implicit in-context learning (ICL) has newly emerged as a promising paradigm that simulates ICL behaviors in the representation space of Large Language Models (LLMs), aiming to attain few-shot performance at zero-shot cost. However, existing approaches largely rely on injecting shift vectors into residual flows, which are typically constructed from labeled demonstrations or task-specific alignment. Such designs fall short of utilizing the structural mechanisms underlying ICL and suffer from limited generalizability. To address this, we propose In-Context Routing (ICR), a novel implicit ICL method that internalizes generalizable ICL patterns at the attention logits level. It extracts reusable structural directions that emerge during ICL and employs a learnable input-conditioned router to modulate attention logits accordingly, enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world datasets spanning diverse domains and multiple LLMs. The results show that ICR consistently outperforms prior implicit ICL methods that require task-specific retrieval or training, while demonstrating robust generalization to out-of-domain tasks where existing methods struggle. These findings position ICR to push the boundary of ICL's practical value.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.704641",
        "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是改进LLM的基础能力，特别是\"in-context learning\"(上下文学习)能力。论文提出的\"In-Context Routing (ICR)\"方法通过在注意力对数级别内化可泛化的ICL模式，增强模型的通用推理能力，这属于改进LLM本身基础能力的研究，而非将LLM作为工具应用于特定领域。 其次，论文包含正面指标中的核心概念\"Large language models, LLMs\"，且研究的in-context learning与推理能力密切相关，因为上下文学习本质上涉及模型如何从给定信息中学习和推理。虽然论文没有直接提到reasoning、planning等关键词，但改进ICL能力实际上是在提升模型的通用推理能力。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性问题。虽然论文在多个领域的真实世界数据集上进行了评估，但其目的是测试方法的泛化能力，而非解决特定领域问题。 最后，论文的核心贡献是提出了一种新的训练/推理范式来增强LLM的通用上下文学习能力，这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标。ICR方法通过改进注意力机制，使模型能够更好地从上下文中学习和推理，从而提升了LLM的通用推理能力。"
    },
    {
        "index": "#187",
        "title": "MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions",
        "link": "/arxiv/2509.22750",
        "arxiv_id": "2509.22750",
        "authors": "Jeonghyun Park, Ingeol Baek, Seunghyun Yoon, Haeun Jang, Aparna Garimella, Akriti Jain, Nedim Lipka, Hwanhee Lee",
        "summary": "Real-world Multi-hop Question Answering (QA) often involves ambiguity that is inseparable from the reasoning process itself. This ambiguity creates a distinct challenge, where multiple reasoning paths emerge from a single question, each requiring independent resolution. Since each sub-question is ambiguous, the model must resolve ambiguity at every step. Thus, answering a single question requires handling multiple layers of ambiguity throughout the reasoning chain. We find that current Large Language Models (LLMs) struggle in this setting, typically exploring wrong reasoning paths and producing incomplete answers. To facilitate research on multi-hop ambiguity, we introduce MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE), a benchmark designed to analyze and evaluate this challenging intersection of ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142 high-quality examples of ambiguous multi-hop questions, categorized under a taxonomy of syntactic, general, and semantic ambiguity, and curated through a rigorous multi-LLM verification pipeline. Our experiments reveal that even state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity combined with multi-step inference is a distinct and significant challenge. To establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning and InstructiON (CLARION), a multi-agent framework that significantly outperforms existing approaches on MIRAGE, paving the way for more adaptive and robust reasoning systems.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.713349",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步核心判断：论文的核心是关于改进LLM在多跳推理(multi-hop reasoning)中处理模糊性(ambiguity)的能力。它提出了MIRAGE基准测试和CLARION多智能体框架，旨在增强LLM的逻辑推理能力，特别是处理复杂、模糊的多步推理问题。这属于改进LLM基础推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确讨论Large Language Models (LLMs)在推理任务中的表现 - 能力方向：专注于reasoning，特别是multi-hop reasoning（多跳推理），属于逻辑推理范畴 - 新兴范式：提出了CLARION多智能体框架(multi-agent framework)，属于llm-based agents和multi-agent systems的研究 第三步排除标准：论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 第四步特殊和模糊情况：论文提出的CLARION框架是一种通用的多智能体协作方法，用于增强LLM处理模糊推理的能力，属于提升通用推理能力的范畴，应予以保留。 最终决策：这篇论文的核心贡献是提出了一种新的评估基准和多智能体框架来提升LLM在模糊多跳推理任务中的表现，这直接符合\"提高大语言模型本身通用推理能力\"的研究目标。论文关注的是LLM的基础推理能力提升，而非特定领域应用，因此应被纳入研究范围。"
    },
    {
        "index": "#183",
        "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning",
        "link": "/arxiv/2509.22824",
        "arxiv_id": "2509.22824",
        "authors": "Chi Ruan, Dongfu Jiang, Yubo Wang, Wenhu Chen",
        "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm, particularly when paired with reasoning models. While effective, it primarily focuses on generating responses and lacks mechanisms to explicitly foster critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT) and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly teaching LLMs how to critique. Motivated by them, we propose Critique Reinforcement Learning (CRL), where the model is tasked with generating a critique for a given (question, solution) pair. The reward is determined solely by whether the final judgment label $c \\in \\{\\texttt{True}, \\texttt{False}\\}$ of the generated critique aligns with the ground-truth judgment $c^*$. Building on this point, we introduce \\textsc{Critique-Coder}, which is trained on a hybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL data. We fine-tune multiple models (\\textsc{Critique-Coder}) and evaluate them on different benchmarks to show their advantages over RL-only models. We show that \\textsc{Critique-Coder} consistently outperforms RL-only baselines on all the evaluated benchmarks. Notably, our \\textsc{Critique-Coder-8B} can reach over 60\\% on LiveCodeBench (v5), outperforming other reasoning models like DeepCoder-14B and GPT-o1. Beyond code generation, \\textsc{Critique-Coder} also demonstrates enhanced general reasoning abilities, as evidenced by its better performance on logic reasoning tasks from the BBEH dataset. This indicates that the application of CRL on coding datasets enhances general reasoning and critique abilities, which are transferable across a broad range of tasks. Hence, we believe that CRL works as a great complement to standard RL for LLM reasoning.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.711258",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Critique Reinforcement Learning (CRL)\"的新训练范式，旨在通过让模型学会评论和反思来增强其推理能力。论文本质上是改进LLM的基础能力，提出新的训练范式来增强其逻辑推理能力，完全符合第一步的\"保留\"标准。从正面指标看，论文明确涉及大语言模型(LLMs)核心概念，聚焦于推理能力(reasoning)，特别是在逻辑推理任务上表现出色，并提出了基于强化学习(RL)的新训练方法。论文强调其方法不仅提高了代码生成能力，还增强了通用推理能力，这种能力可以迁移到广泛的任务中，这正是研究目标所关注的\"通用推理能力\"。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。虽然论文提到了代码生成，但这是作为增强通用推理能力的手段而非特定领域应用。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#195",
        "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression",
        "link": "/arxiv/2509.25176",
        "arxiv_id": "2509.25176",
        "authors": "Haoming Wen, Yushi Bai, Juanzi Li, Jie Tang",
        "summary": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved Compression, a simple yet effective RL approach for Large Reasoning Models (LRMs) that enables more efficient and accurate reasoning. Existing studies have observed repetitive thinking patterns in LRMs, and attempts to reduce them often come at the cost of performance. In this paper, we show that this trade-off can be overcome through a training regime that iteratively alternates between compressing and expanding the reasoning budget, by dynamically adjusting the maximum rollout length during training. The compression phase cuts the rollout length, forcing the model to make precise and valuable decisions within a limited context, which effectively reduces redundant tokens and increases reasoning density. The expansion phase then relaxes the length limit, providing space for the model to explore and plan in long-horizon settings. Remarkably, we find that after each compression-expansion cycle, the model's performance improves even as its output length decreases, steadily pushing it closer to the Pareto frontier in the performance-efficiency trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves performance on AIME24 by 43.2% while reducing token usage by 46.9% after three iterations, and SIRI-high achieves the highest accuracy compared to all other methods (Figure 1). Our findings shed light on the potential of periodically oscillating the LRM's output truncation length during training to dynamically balance exploration and efficiency in reasoning, converging towards an optimal \"sweet spot\" between the two. Our models are publicly available.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.925497",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是提出SIRI（Scaling Iterative Reinforcement Learning with Interleaved Compression），一种针对大型推理模型(LRMs)的强化学习方法。该方法通过在训练过程中交替压缩和扩展推理预算，动态调整最大展开长度，从而提高模型的推理效率和准确性。这明显是关于改进LLM基础推理能力的研究，属于强化学习优化的方法论研究，符合保留标准。 第二步正面指标：论文包含多个正面指标主题： - 核心概念：明确研究大型推理模型(LRMs)，即具有推理能力的大语言模型 - 能力方向：专注于推理能力(reasoning)，特别是在数学推理任务(AIME24)上评估 - 训练方法：核心是强化学习(Reinforcement Learning)方法，通过迭代训练提升模型性能 第三步排除标准：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域（虽然使用数学竞赛作为评估基准，但目的是提升通用推理能力） - 不关注模型基础设施、部署优化或硬件加速 第四步特殊和模糊情况：论文不涉及需要特殊判断的智能体/工具使用或幻觉/可解释性/安全等问题。 综上所述，这篇论文的核心贡献是提出一种新的强化学习训练范式来增强大语言模型的推理能力和效率，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#199",
        "title": "The Era of Real-World Human Interaction: RL from User Conversations",
        "link": "/arxiv/2509.25137",
        "arxiv_id": "2509.25137",
        "authors": "Chuanyang Jin, Jing Xu, Bo Liu, Leitian Tao, Olga Golovneva, Tianmin Shu, Wenting Zhao, Xian Li, Jason Weston",
        "summary": "We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop two complementary methods: (1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses, (2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). Together, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.927030",
        "filter_reason": "这篇论文的核心贡献是提出了一种新的强化学习范式\"Reinforcement Learning from Human Interaction (RLHI)\"，该方法使大语言模型能够直接从真实用户对话中学习，而非依赖预标注的专家反馈。这完全符合我的研究目标，因为它专注于改进LLM的基础能力，提出了新的训练范式来增强模型性能。 具体分析如下： 1. 核心判断：论文本质上是关于改进LLM基础能力的研究，提出了新的强化学习方法(RLHI)来优化模型的对齐能力和指令跟踪能力。论文明确提到这些方法还能\"enhances performance on reasoning benchmarks\"，表明其对推理能力的提升作用。 2. 正面指标：论文符合多个正面指标，包括： - 核心概念：针对对话模型(属于LLM范畴) - 能力方向：明确提及对推理基准性能的提升 - 训练方法：核心是提出新的强化学习方法(RLHI)，与RLHF相关但更先进 3. 排除标准：论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性(应用层面)。 4. 特殊情况：论文虽然涉及用户交互，但不是将LLM作为工具应用于特定领域，而是提出通用方法来增强模型的基础能力，因此应该保留。 综上所述，这篇论文通过提出新的强化学习范式来改进LLM的通用能力(包括推理能力)，完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#202",
        "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones",
        "link": "/arxiv/2509.25123",
        "arxiv_id": "2509.25123",
        "authors": "Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, Hao Peng",
        "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.928189",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究LLM如何通过强化学习(RL)组合已有技能来获取新技能，这直接涉及到改进LLM的基础推理能力和提出新的训练范式。论文探讨了LLM的函数组合推理能力，即从已知的f(x)和g(x)推理出未见过的f(g(x))，这属于逻辑推理和多步推理的核心研究。 其次，论文包含多个正面指标：核心概念明确是LLMs；能力方向聚焦于推理能力，特别是逻辑推理和问题解决；训练方法采用强化学习(RL)；虽然未直接讨论智能体或工具使用，但研究的技能组合机制与这些新兴范式的底层能力相关。 第三，论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题，而是专注于通用推理机制的研究。 最后，论文的核心贡献是揭示了LLM通过RL能够真正获取新技能（而非仅仅激活现有技能），并通过组合已有技能实现更复杂的推理能力，这种能力还能泛化和迁移到不同任务。这些发现对于提升LLM的通用推理能力具有重要意义，完全符合研究目标。"
    },
    {
        "index": "#200",
        "title": "Rethinking Entropy Regularization in Large Reasoning Models",
        "link": "/arxiv/2509.25133",
        "arxiv_id": "2509.25133",
        "authors": "Yuxian Jiang, Yafu Li, Guanxu Chen, Dongrui Liu, Yu Cheng, Jing Shao",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown great promise in enhancing the reasoning abilities of large reasoning models (LRMs). However, it suffers from a critical issue: entropy collapse and premature convergence. Naive entropy regularization, a common approach for encouraging exploration in the traditional RL literature, fails to address this problem in the context of LRM. Our analysis reveals that this failure stems from the vast action space and long trajectories in LRMs, which easily trigger a global entropy explosion as the model indiscriminately explores all possible actions and states. To address this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method that confines exploration to a meaningful subset of actions and states. SIREN achieves this through a two-step entropy masking mechanism, consisting of a top-p mask and a peak-entropy mask. In addition, regularization is transformed into a self-anchored form to stabilize training. Across five mathematical benchmarks, SIREN attains superior average performance over previous entropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on AIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes greater response diversity and maintains entropy at an appropriate level, which helps to preserve the validation pass@k throughout training. This effectively mitigates the premature convergence problem common in RLVR for LRM.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.927368",
        "filter_reason": "这篇论文完全符合我的研究目标和筛选标准。从核心判断来看，论文的本质是关于改进大型推理模型(LRMs)的基础能力，提出了一种新的强化学习训练范式(SIREN)来增强模型的推理能力。论文明确关注解决熵崩溃和过早收敛问题，这些都是提升模型推理能力的关键技术挑战。 在正面指标方面，论文包含了多个相关主题：核心概念上提到了\"Large reasoning models (LRMs)\"；能力方向上明确关注\"reasoning abilities\"，特别是在数学推理方面；训练方法上讨论了\"Reinforcement learning with verifiable rewards (RLVR)\"，这与强化学习方法论直接相关。 从排除标准看，论文没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。虽然论文在数学基准测试上评估了方法，但数学推理被视为通用推理能力的一部分，而非特定应用领域。论文的核心是改进推理模型的一般训练方法，而不是解决特定领域的问题。 论文的核心贡献是提出SIREN方法，通过选择性熵正则化来解决大型推理模型在强化学习训练中遇到的熵崩溃和过早收敛问题，从而提升模型的通用推理能力。这完全符合我的研究目标，即筛选致力于提高大语言模型本身通用推理能力的论文。"
    },
    {
        "index": "#198",
        "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory",
        "link": "/arxiv/2509.25140",
        "arxiv_id": "2509.25140",
        "authors": "Siru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T. Le, Samira Daruki, Xiangru Tang, Vishy Tirumalashetty, George Lee, Mahsan Rofouei, Hangfei Lin, Jiawei Han, Chen-Yu Lee, Tomas Pfister",
        "summary": "With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agent's interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.926657",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，理由如下： 第一步核心判断：这篇论文的本质是提出ReasoningBank，一种新的记忆框架，用于增强大语言模型代理的推理能力。论文核心关注如何让代理从自身经验中提炼可泛化的推理策略，并通过记忆机制实现自我进化，这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理等通用能力\"的范畴，因此应该保留。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确关注\"large language model agents\" - 能力方向：聚焦于\"reasoning strategies\"和\"generalizable reasoning\" - 训练方法：涉及\"self-evolving\"概念 - 新兴范式：研究\"llm-based agents\"及其记忆机制 第三步排除标准：论文没有主要聚焦于任何排除领域。虽然提到了\"web browsing and software engineering benchmarks\"，但这些仅是评估方法的基准测试，而非论文主要焦点。 第四步特殊情况处理：论文提出的是通用的智能体记忆框架，用于增强LLM的通用推理能力和自我进化能力，而不是将智能体应用于特定领域，因此符合保留条件。 综合来看，这篇论文的核心贡献是提出了一种通过记忆机制来增强大语言模型代理通用推理能力的方法，使代理能够从经验中学习并自我进化，这与研究目标\"提高大语言模型的通用推理能力\"高度契合。"
    },
    {
        "index": "#207",
        "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training",
        "link": "/arxiv/2509.24923",
        "arxiv_id": "2509.24923",
        "authors": "Sanxing Chen, Xiaoyin Chen, Yukun Huang, Roy Xie, Bhuwan Dhingra",
        "summary": "While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.934900",
        "filter_reason": "这篇论文符合我的研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的本质是研究如何通过监督微调(SFT)和强化学习(RL)两种训练范式来提升大型语言模型(LLM)在序列决策任务中的探索能力。论文聚焦于多臂老虎机这一经典决策问题，提出了新的奖励设计方法来优化LLM的训练过程，并分析了不同训练方法对LLM行为的影响。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，特别是增强了LLM在决策制定和问题解决方面的通用能力，符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：研究sequential decision-making（序列决策），这涉及到planning和problem-solving能力，是推理能力的重要体现 - 训练方法：重点研究reinforcement learning (RL)作为训练LLM的方法 - 新兴范式：提到LLMs成为autonomous agents（自主智能体）的潜力，与llm-based agents相关 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 研究的是通用的序列决策问题（多臂老虎机任务），而非特定应用领域 - 没有主要聚焦于水印、安全性等模型可靠性问题 第四步：特殊和模糊情况 论文涉及通用智能体框架研究，讨论如何通过训练提高LLM作为自主智能体的决策能力，而不是将智能体应用于特定领域，因此应该保留。 综合判断：这篇论文的核心贡献在于研究如何通过强化学习等训练方法提升LLM在决策制定中的通用能力，探索-利用平衡是推理和决策的关键组成部分。论文提出的奖励设计方法和训练策略分析直接服务于提升LLM的通用推理能力，因此完全符合我的研究目标。"
    },
    {
        "index": "#212",
        "title": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity",
        "link": "/arxiv/2509.24836",
        "arxiv_id": "2509.24836",
        "authors": "Zhen Bi, Zhenlin Hu, Jinnan Yang, Mingyang Chen, Cheng Deng, Yida Xue, Zeyu Yang, Qing Shen, Zhenfang Liu, Kang Zhao, Ningyu Zhang, Jungang Lou",
        "summary": "Recent advances in large language models (LLMs) highlight the importance of training data structure and quality in shaping reasoning behavior. However, most existing approaches focus on transforming data formats while neglecting the internal reasoning complexity of training samples, leaving the reasoning potential of data under-explored and underutilized. In this work, we posit that LLM logical reasoning performance is jointly constrained by the potential of the training data and the cognitive capacity of the model. To make this relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel metric that quantifies the latent logical reasoning complexity of samples by decomposing and aggregating their logical structures. This allows us to analyze how well current LLMs utilize logical reasoning signals and identify performance gaps relative to data potential. Based on this insight, we introduce a re-cognizing optimization strategy that systematically enhances the logical reasoning intensity of training data.Rather than increasing data volume, our method re-optimizes existing samples to better align with the LLM's logical reasoning boundary. Extensive experiments show that our approach significantly improves performance and generalization over data-centric strategies. We further validate our method under a reinforcement learning framework. Our results indicate that prioritizing reasoning complexity in data rather than sheer scale or superficial form is essential to realizing LLMs' full cognitive potential.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.936874",
        "filter_reason": "这篇论文完全符合我的研究目标。从核心判断来看，论文的本质是改进LLM的基础逻辑推理能力，而非将其作为工具应用于特定领域。论文的核心贡献是提出了\"数据推理强度\"(DRI)这一新指标和相应的\"重新认知优化策略\"，通过优化训练数据的逻辑推理复杂性来提升LLM的逻辑推理能力，而不是简单增加数据量或改变数据格式。从正面指标看，论文明确研究大型语言模型(LLMs)，特别关注逻辑推理能力，并在强化学习框架下验证了方法。论文不涉及任何排除标准中的多模态与视觉、特定应用领域或模型可靠性的应用层面研究。这是一篇典型的致力于提高大语言模型本身通用推理能力的研究，通过新的训练范式增强模型的逻辑推理能力，与我的研究目标高度一致。"
    },
    {
        "index": "#214",
        "title": "On the Self-awareness of Large Reasoning Models' Capability Boundaries",
        "link": "/arxiv/2509.24711",
        "arxiv_id": "2509.24711",
        "authors": "Qingjie Zhang, Yujia Fu, Yang Wang, Liu Yan, Tao Wei, Ke Xu, Minlie Huang, Han Qiu",
        "summary": "Large Reasoning Models (LRMs) have shown impressive performance on complex reasoning tasks such as mathematics, yet they also display misbehaviors that expose their limitations. In particular, when faced with hard questions, LRMs often engage in unproductive reasoning until context limit, producing wrong answers while wasting substantial computation. This phenomenon reflects a fundamental issue: current answering paradigms overlook the relationship between questions and LRMs' capability boundaries. In this paper, we investigate whether LRMs possess self-awareness of capability boundaries. We begin by an observation that LRMs may know what they cannot solve through expressed reasoning confidence. For black-box models, we find that reasoning expressions reveal boundary signals, with accelerated growing confidence trajectory for solvable problems but convergent uncertainty trajectory for unsolvable ones. For white-box models, we show that hidden states of the last input token encode boundary information, with solvable and unsolvable problems linearly separable even before reasoning begins. Building on these findings, we propose two simple yet effective optimization strategies: reasoning expression monitoring and hidden states monitoring. Experiments demonstrate that these boundary-aware strategies enable LRMs to avoid unproductive reasoning without sacrificing accuracy, significantly improving reliability and efficiency by cutting token usage up to 62.7 - 93.6%.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.937643",
        "filter_reason": "这篇论文完全符合我的研究目标，核心贡献是研究大型推理模型(LRMs)对其能力边界的自我认知能力，并提出优化策略来提升模型的推理效率和可靠性。 首先，从核心判断角度看，论文本质上是关于改进LLM的基础推理能力的。论文研究的是模型如何认知自身能力边界，并提出两种优化策略（推理表达式监控和隐藏状态监控）来避免无效推理，这直接提升了模型的通用推理能力，而不是将LLM作为工具应用到特定领域。 其次，从正面指标看，论文明确包含核心概念\"Large Reasoning Models (LRMs)\"，并聚焦于\"reasoning\"能力方向，特别是数学推理等复杂推理任务。虽然论文没有明确提到强化学习等训练方法或智能体等新兴范式，但其研究内容与提升LLM通用推理能力直接相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域，也不关注模型可靠性的应用层面（如水印、安全等）。 最后，在特殊情况下，论文关注模型对自身能力边界的认知，这可以视为一种提高模型内在可靠性和推理质量的方法，与减少幻觉和提高可解释性有一定关联，应该保留。 综上所述，这篇论文的核心贡献是通过研究模型对自身能力边界的自我认知，提出方法来优化推理过程，提高效率和可靠性，完全符合\"提高大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#211",
        "title": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval",
        "link": "/arxiv/2509.24869",
        "arxiv_id": "2509.24869",
        "authors": "Junwei Lan, Jianlyu Chen, Zheng Liu, Chaofan Li, Siqi Bao, Defu Lian",
        "summary": "With the growing popularity of LLM agents and RAG, it has become increasingly important to retrieve documents that are essential for solving a task, even when their connection to the task is indirect or implicit. Addressing this problem requires fine-grained reasoning to accurately assess the relevance between the task and each candidate document. This capability, however, poses a significant challenge for existing IR techniques. Despite recent progress in reasoning-enhanced IR, existing approaches still face significant challenges in applicability, scalability, and efficiency. In this work, we propose Retro*, a novel approach for reasoning-intensive document retrieval. Our method introduces a rubric-based relevance scoring mechanism, enabling the model to reason about the relationship between a task and a document based on explicitly defined criteria, whereby producing a fine-grained, interpretable relevance score. Retro* also supports test-time scaling by combining multiple reasoning trajectories via score integration, which produces more reliable relevance estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel reinforcement learning algorithm tailored for its relevance scoring mechanism, which employs two composite rewards to fully exploit the trajectories of each training sample. Our experiments show that Retro* outperforms existing document retrieval methods with notable advantages, leading to state-of-the-art performance on the BRIGHT benchmark.",
        "subjects": "Information Retrieval, Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.936242",
        "filter_reason": "这篇论文的核心贡献是提出Retro*方法，通过基于标准的相关性评分机制和定制的强化学习算法来优化LLM的推理能力。根据筛选标准，我判断这篇论文符合研究目标，原因如下： 首先，从本质上看，这篇论文的核心是改进LLM的基础推理能力，而非将LLM作为工具应用到特定领域。论文提出了一种新的推理方法（基于标准的相关性评分机制）和强化学习算法来增强LLM的推理能力，特别是在处理需要细粒度推理的文档检索任务时。 其次，论文包含多个正面指标：明确关注LLMs的reasoning能力，采用了reinforcement learning作为训练方法，并提到了LLM agents和RAG等新兴范式。 第三，论文不主要聚焦于排除标准中的领域。虽然应用场景是文档检索，但这不是一个特定领域应用（如医疗、化学等），而是一个通用任务。论文的焦点是提高LLM的推理能力，而不是文档检索本身。 最后，虽然论文涉及文档检索这一应用场景，但其提出的方法（基于标准的相关性评分机制、测试时扩展的多个推理轨迹结合、定制的强化学习算法）都是通用的，旨在提高LLM的推理能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。 因此，这篇论文符合研究范围。"
    },
    {
        "index": "#221",
        "title": "Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention",
        "link": "/arxiv/2509.24393",
        "arxiv_id": "2509.24393",
        "authors": "Yichi Zhang, Yue Ding, Jingwen Yang, Tianwei Luo, Dongbai Li, Ranjie Duan, Qiang Liu, Hang Su, Yinpeng Dong, Jun Zhu",
        "summary": "Although Large Reasoning Models (LRMs) have progressed in solving complex problems, their chain-of-thought (CoT) reasoning often contains harmful content that can persist even when the final responses appear safe. We show that this issue still remains in existing methods which overlook the unique significance of safe reasoning, undermining their trustworthiness and posing potential risks in applications if unsafe reasoning is accessible for and exploited by malicious users. We therefore shift our focus to aligning the safety of reasoning itself in this paper and explore process supervision as the solution. However, simply rewarding safe reasoning proves inadequate due to low rollout diversity and limited training signals. To tackle this challenge, we first delve into the characteristics of safe reasoning and uncover several critical insights that 1) safe reasoning is often consolidated by a few critical steps of safety triggers; 2) compliance cues strongly correlate with unsafe continuations; and 3) corrective interventions reliably steer unsafe trajectories towards safer traces. Motivated by these, we propose Intervened Preference Optimization (IPO), an alignment method that enforces safe reasoning by substituting compliance steps with safety triggers and constructing pairs for preference learning with strong signals. Experiments on jailbreak and adversarial safety benchmarks demonstrate that IPO remarkably improves overall safety regarding both reasoning and responses, outperforming SFT-based and RL-based baselines with a relative reduction of over 30% in harmfulness, while preserving excellent performance across diverse reasoning tasks. The results highlight the importance of explicit alignment for reasoning and provide a practical path to safer LRMs.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.940128",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：这篇论文的本质是改进大型推理模型(LRMs)的安全推理能力。论文提出了一种新的训练范式\"干预偏好优化\"(IPO)，专门针对思维链(CoT)推理过程中可能存在的有害内容进行改进。这属于改进LLM基础能力的范畴，特别是增强其推理过程的安全性和可靠性，而不是将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确提到\"Large Reasoning Models (LRMs)\"，这是专注于推理能力的大语言模型 - 能力方向：核心关注\"reasoning\"，特别是\"chain-of-thought (CoT) reasoning\"，这正是通用推理能力的关键表现形式 - 训练方法：涉及\"process supervision\"和\"preference learning\"，这与强化学习方法相关 第三步排除标准：论文不涉及任何应排除的领域： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域（如医疗、化学等） - 虽然涉及安全性，但是从推理过程本身的角度，而非应用层面的水印或安全技术 第四步特殊情况处理：论文属于提出新方法来增强模型内在安全性的情况。作者提出的IPO方法通过改进推理过程的安全性来提升模型的通用可靠性和推理质量，这符合\"如果论文提出一种新方法来增强模型内在的安全性，从而提升模型的通用可靠性和推理质量，应该保留\"的标准。 核心贡献：论文提出了干预偏好优化(IPO)方法，通过将合规步骤替换为安全触发器，并构建偏好学习对来强制执行安全推理，从而提升大型推理模型在推理过程中的安全性。这种方法是通用的，旨在提高LLM的内在推理能力，特别是推理过程的安全性，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#230",
        "title": "SpecExit: Accelerating Large Reasoning Model via Speculative Exit",
        "link": "/arxiv/2509.24248",
        "arxiv_id": "2509.24248",
        "authors": "Rubing Yang, Huajun Bai, Song Liu, Guanghua Yu, Runzhi Fan, Yanbin Dang, Jiejing Zhang, Kai Liu, Jianchen Zhu, Peng Chen",
        "summary": "Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment. To address overthinking, early-exit mechanisms have been proposed to terminate reasoning before typical completion, showing that this approach can effectively shorten generation length with minimal impact on accuracy. However, their reliance on probing mechanisms introduces a detection overhead that limits their end-to-end latency gains and compromises their generalizability across diverse problems. Inspired by the use of hidden states in speculative decoding, we propose SpecExit, a novel framework that predicts both future tokens and an early-exit signal directly from a lightweight draft model without probing overhead. Our method offers significant improvements, reducing average generation length by 66\\% and achieving a 2.5x speedup in end-to-end latency compared to the speculative decoding baseline, without compromising accuracy. Our method leverages the inherent signals from hidden states to provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Our code is available at https://github.com/Tencent/AngelSlim.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.969370",
        "filter_reason": "这篇论文符合我的研究目标，应该被保留。根据筛选标准，我的判断过程如下： 第一步核心判断：论文的本质是关于改进大型推理模型(LRMs)的推理效率，解决\"过度思考\"问题。论文提出的SpecExit框架通过轻量级草稿模型预测未来token和提前退出信号，减少不必要的计算，同时保持准确性。这属于改进LLM基础能力和推理效率的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标。核心概念方面，它明确关注\"large reasoning models (LRMs)\"，这是LLMs的一个子集；能力方向方面，论文直接针对\"reasoning tasks\"和推理效率问题进行研究。 第三步排除标准：论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，而是专注于通用推理模型的效率优化。 第四步特殊和模糊情况：论文不涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等问题。 论文的核心贡献是提出一种新的框架来加速大型推理模型的推理过程，减少计算开销同时保持准确性，这直接提升了大语言模型的通用推理能力，符合我的研究目标。"
    },
    {
        "index": "#233",
        "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends",
        "link": "/arxiv/2509.24203",
        "arxiv_id": "2509.24203",
        "authors": "Chaorui Yao, Yanxi Chen, Yuchang Sun, Yushuo Chen, Wenhao Zhang, Xuchen Pan, Yaliang Li, Bolin Ding",
        "summary": "Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.970418",
        "filter_reason": "这篇论文的核心贡献是对大语言模型强化学习算法的理论分析和改进，特别是关于REINFORCE及其变种的离线策略特性。论文提出了将REINFORCE适应离线设置的两个通用原则：正则化策略更新和主动塑造数据分布，这些改进有助于更好地应用强化学习来训练LLM。强化学习是提升LLM通用推理能力（包括数学推理、逻辑推理等）的关键技术，论文的研究目标（如源代码示例使用的GSM8K数学推理数据集所示）与提升LLM的推理能力直接相关。论文不涉及特定应用领域或模型基础设施优化，而是专注于改进LLM的基础训练方法，因此符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#231",
        "title": "Learning to Ponder: Adaptive Reasoning in Latent Space",
        "link": "/arxiv/2509.24238",
        "arxiv_id": "2509.24238",
        "authors": "Yixin He, Lumingyuan Tang",
        "summary": "Test-time compute has emerged as a key paradigm for enhancing LLM reasoning, yet prevailing approaches like Best-of-N and majority voting apply uniform depth across inputs, wasting computation on simple queries while potentially under-thinking complex ones. We present FR-Ponder, a single-graph, backbone-training-free framework that allocates instance-adaptive reasoning compute via latent steering. A less than 1M-param controller observes hidden states and decides to halt or apply a small ponder step by adding a pre-computed steering vector to frozen representations. Our method extracts the latent steering vector associated with deeper reasoning outputs and direct IO from LLM and re-applies it through a tunable scaling factor, allowing the model to adapt its reasoning depth to the complexity of each input. To balance performance and computational cost, we employ Group Relative Policy Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth, achieving task accuracy while mitigating overreasoning. Through curriculum learning and careful reward engineering, FR-Ponder learns calibrated compute allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder improves the compute-accuracy frontier, delivering lower FLOPs with better matched accuracy and comparing favorably to early-exit baselines, without modifying backbone weights. Analyses visualize interpretable steering directions and show learned compute allocation correlates with problem difficulty.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.969667",
        "filter_reason": "这篇论文的核心贡献是提出FR-Ponder框架，一种通过潜在空间中的自适应推理来分配计算资源的方法。从本质上看，论文直接关注如何提高大语言模型本身的推理能力，特别是通过让模型根据输入复杂性调整推理深度，而不是像现有方法那样对所有输入应用统一的推理深度。论文使用了强化学习方法（GRPO）来平衡性能和计算成本，并在数学推理任务（GSM8K和MATH500）上验证了其有效性。这篇论文不是将LLM作为工具应用到特定领域，而是直接改进LLM本身的推理机制，完全符合\"大语言模型通用推理能力\"的研究范围。论文提出的自适应推理框架可以被视为一种增强LLM通用推理能力的新范式，属于改进LLM基础推理能力的研究。论文不涉及任何排除标准中的领域，如多模态、特定应用领域或模型可靠性的应用层面问题。"
    },
    {
        "index": "#234",
        "title": "Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models",
        "link": "/arxiv/2509.24156",
        "arxiv_id": "2509.24156",
        "authors": "Yuhui Wang, Changjiang Li, Guangke Chen, Jiacheng Liang, Ting Wang",
        "summary": "Large reasoning models (LRMs) exhibit unprecedented capabilities in solving complex problems through Chain-of-Thought (CoT) reasoning. However, recent studies reveal that their final answers often contradict their own reasoning traces. We hypothesize that this inconsistency stems from two competing mechanisms for generating answers: CoT reasoning and memory retrieval. To test this hypothesis, we conduct controlled experiments that challenge LRMs with misleading cues during reasoning and/or corrupted answers during retrieval. Our results across models and datasets confirm that both mechanisms operate simultaneously, with their relative dominance influenced by multiple factors: problem domains, model scales, and fine-tuning approaches (e.g., reinforcement learning vs. distillation). The findings reveal a critical limitation in current reasoning fine-tuning paradigms: models can exploit the retrieval mechanism as a shortcut, effectively \"hacking\" the reward signal and undermining genuine reasoning development. To address this challenge, we introduce FARL, a novel fine-tuning framework that integrates memory unlearning with reinforcement learning. By carefully suppressing retrieval shortcuts during the fine-tuning process, FARL promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.970742",
        "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文本质上是研究大语言模型推理机制的内在工作原理，特别是分析思维链(CoT)推理和记忆检索两种竞争机制如何影响模型的推理过程。这不是将LLM应用于特定领域，而是专注于提升LLM本身的通用推理能力。论文提出的FARL框架是一种新的训练范式，通过结合记忆遗忘和强化学习来抑制检索捷径，从而增强模型的推理主导行为和通用推理能力。其次，论文包含多个正面指标：核心概念涉及大型推理模型(LRMs)，能力方向聚焦于推理能力(特别是CoT推理)，训练方法包含强化学习。论文不涉及任何排除标准中的领域，如多模态视觉、特定应用领域或模型可靠性的应用层面问题。因此，这篇论文明确致力于提高大语言模型的通用推理能力，完全符合我的研究范围。"
    },
    {
        "index": "#239",
        "title": "Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models",
        "link": "/arxiv/2509.23962",
        "arxiv_id": "2509.23962",
        "authors": "Guanxu Chen, Yafu Li, Yuxian Jiang, Chen Qian, Qihan Ren, Jingyi Yang, Yu Cheng, Dongrui Liu, Jing Shao",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning capabilities on tasks with clear correctness criteria, such as mathematical reasoning tasks. Several training metrics, such as entropy or response length, have been observed to correlate with different reasoning behaviors in reinforcement learning. Prior approaches incorporate such priors through reward or advantage shaping, which often relies on hand-crafted penalties and preferences (e.g., higher-is-better or lower-is-better). However, without careful hyperparameter tuning, these directional priors can be overly biased and may lead to failure. To this end, we introduce Conditional advANtage estimatiON (CANON), amplifying the impact of the target metric without presuming its direction. Specifically, CANON regroups the sampled responses into two groups based on the higher or lower value of a target metric, measures which metric trend contributes to better performance through inter-group comparison, and identifies the better response within the same group. In summary, CANON based on entropy consistently outperforms prior methods across three LLMs on both math reasoning and high-complexity logic tasks. When applied to response length, CANON further improves token efficiency, yielding a more favorable Pareto frontier in the performance-cost trade-off.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.981810",
        "filter_reason": "这篇论文完全符合我的研究范围。根据筛选标准，我进行了如下分析： 第一步：核心判断——论文的本质是关于改进LLM的基础推理能力。论文提出了\"Conditional advANtage estimatiON (CANON)\"方法，用于增强大型语言模型在强化学习中的推理能力，特别是在数学推理和复杂逻辑任务上的表现。这属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴，符合保留标准。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确研究Large language models (LLMs) - 能力方向：专注于reasoning，特别是mathematical reasoning和high-complexity logic tasks - 训练方法：研究Reinforcement Learning with Verifiable Rewards (RLVR)，属于强化学习范畴 第三步：排除标准——论文不涉及任何排除领域： - 没有涉及多模态与视觉内容 - 没有专注于特定应用领域（虽然提到数学和逻辑任务，但这些是通用推理能力的测试场景，而非特定领域应用） - 没有关注模型可靠性方面的水印、安全等问题 第四步：特殊和模糊情况——论文没有涉及需要特殊处理的情况，如智能体/工具使用或幻觉/可解释性/安全等。 综上所述，这篇论文的核心贡献是通过改进强化学习中的优势估计方法来提升LLM的通用推理能力，完全符合我筛选\"致力于提高大语言模型本身的通用推理能力\"论文的研究目标。"
    },
    {
        "index": "#240",
        "title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm",
        "link": "/arxiv/2509.23946",
        "arxiv_id": "2509.23946",
        "authors": "Kaisen Yang, Lixuan He, Rushi Shah, Kaicheng Yang, Qinwei Ma, Dianbo Liu, Alex Lamb",
        "summary": "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of execution.This decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: https://github.com/yks23/Explore-Execute-Chain.git",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.982553",
        "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出Explore-Execute Chain（E²C）框架，一种新的结构化推理范式，专门用于改进大语言模型的通用推理能力。从本质上看，论文针对思维链(CoT)的局限性，将推理过程分解为探索和执行两个阶段，通过监督微调和强化学习相结合的训练方法，提升了模型的推理效率、路径探索能力和可解释性。这明显属于改进LLM基础能力和通用推理能力的范畴。 论文包含多个关键正面指标：明确以大语言模型(LLMs)为研究对象；专注于推理(reasoning)、规划(planning)等核心能力方向；采用强化学习(RL)作为训练方法之一。虽然论文提到了在医学基准上的测试结果，但这仅是为了证明其跨领域适应能力，而非将LLM应用于特定领域，因此不触犯排除标准。此外，论文通过分离规划与执行来增强模型的可解释性，这也符合保留标准。 综上所述，该论文直接致力于提升LLM的通用推理能力，提出了新的训练范式和推理框架，与研究目标高度一致。"
    },
    {
        "index": "#244",
        "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR",
        "link": "/arxiv/2509.23808",
        "arxiv_id": "2509.23808",
        "authors": "Fanding Huang, Guanbo Huang, Xiao Fan, Yi He, Xiao Liang, Xiao Chen, Qinting Jiang, Faisal Nadeem Khan, Jingyan Jiang, Zhi Wang",
        "summary": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR) interprets recent progress through the lens of an exploration-exploitation trade-off, a perspective largely shaped by token-level metrics. We re-examine this perspective, proposing that this perceived trade-off may not be a fundamental constraint but rather an artifact of the measurement level. To investigate this, we shift the analysis to the semantically rich hidden-state space, adopting Effective Rank (ER) to quantify exploration and proposing its novel first- and second-order derivatives, named Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our analysis reveals that at the hidden-state level, exploration and exploitation could be decoupled (Sec. 4). This finding reveals an opportunity to enhance both capacities simultaneously. This insight motivates our method, Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the principle of synergistic exploration-exploitation enhancement by directly shaping the RL advantage function. The key innovation is leveraging the theoretically stable ERA as a predictive meta-controller to create a synergistic, dual-channel incentive structure. Instead of forcing a trade-off, VERL prospectively amplifies rewards for exploration to preempt overconfidence and reinforces exploitative gains to consolidate reasoning. Experiments across diverse LLMs and reasoning benchmarks show consistent gains, including up to 21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.985287",
        "filter_reason": "这篇论文完全符合我的研究目标，核心在于提高大语言模型的通用推理能力。首先，论文的本质是改进LLM的基础推理能力，提出了名为VERL（Velocity-Exploiting Rank-Learning）的新方法，通过在隐藏状态空间中解耦探索和利用来增强LLM的推理能力，而非将LLM作为工具应用到特定领域。其次，论文包含多个正面指标：核心概念上明确关注LLMs；能力方向上专注于reasoning和problem-solving；训练方法上采用强化学习框架。第三，论文不符合任何排除标准，没有涉及多模态、特定应用领域或模型可靠性的应用层面研究。论文的核心贡献是通过创新的隐藏状态分析方法，提出了一种能够同时增强探索和利用能力的新训练范式，直接提升了LLM的通用推理能力，在多个推理基准上取得了显著改进，最高达到21.4%的准确率提升。这完全符合我寻找的\"致力于提高大语言模型本身的通用推理能力\"的研究论文。"
    },
    {
        "index": "#247",
        "title": "Anchored Supervised Fine-Tuning",
        "link": "/arxiv/2509.23753",
        "arxiv_id": "2509.23753",
        "authors": "He Zhu, Junyou Su, Peng Lai, Ren Ma, Wenjia Zhang, Linyi Yang, Guanhua Chen",
        "summary": "Post-training of large language models involves a fundamental trade-off between supervised fine-tuning (SFT), which efficiently mimics demonstrations but tends to memorize, and reinforcement learning (RL), which achieves better generalization at higher computational cost. Dynamic Fine-Tuning (DFT) recently emerged as a promising middle ground, reweighting SFT objectives with token probabilities and achieving improvements in certain reasoning domains, though it exhibits instability in other tasks. We provide a analysis of DFT through the reward-weighted regression (RWR) framework, revealing that it corresponds to a specific auxiliary distribution choice that yields provably tighter RL bounds than standard SFT. However, our analysis also uncovers a critical limitation: this construction lacks distributional anchoring, leading to progressive drift that undermines training stability. To address this, we propose Anchored Supervised Fine-Tuning (ASFT), which augments DFT's reweighting with lightweight KL regularization to preserve tightness while ensuring stability. Empirically, ASFT consistently outperforms both SFT and DFT across mathematical reasoning, medical knowledge grounding, and code generation, achieving substantial improvements with minimal computational overhead. Our RWR framework provides a systematic lens for understanding post-training methods and demonstrates that principled theoretical analysis leads to both stronger guarantees and practical gains.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.992425",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式（Anchored Supervised Fine-Tuning, ASFT）来改进大语言模型的后训练过程。论文关注的是LLM的基础能力改进，特别是解决监督微调(SFT)和强化学习(RL)之间的权衡问题，旨在提高模型的推理能力和训练稳定性。论文明确提到在数学推理(mathematical reasoning)方面的改进，这直接符合\"改进LLM的通用推理能力\"的研究目标。 其次，论文包含多个正面指标：核心概念上讨论大语言模型；能力方向上专注于推理能力，特别是数学推理；训练方法上探讨了强化学习与监督微调的结合，提出了新的训练方法。 关于排除标准，虽然论文提到了在\"医学知识基础\"上的测试，但这只是作为评估模型性能的一个领域，而不是论文的主要焦点。论文的核心是提出一种通用的训练方法，而不是专注于医学或其他特定应用领域。论文也不涉及多模态与视觉、模型可靠性等排除领域。 在特殊和模糊情况处理方面，论文提出的ASFT方法通过KL正则化来确保训练稳定性，这间接有助于提高模型的推理质量和可靠性，符合提升模型通用推理能力的目标。 综上所述，这篇论文的核心贡献是提出一种新的通用训练方法来增强大语言模型的推理能力和稳定性，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#215",
        "title": "OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment",
        "link": "/arxiv/2509.24610",
        "arxiv_id": "2509.24610",
        "authors": "Liang Lin, Zhihao Xu, Junhao Dong, Jian Zhao, Yuchen Yuan, Guibin Zhang, Miao Yu, Yiming Zhang, Zhengtao Yao, Huahui Yi, Dongrui Liu, Xinfeng Li, Kun Wang",
        "summary": "Large language model (LLM) alignment faces a critical dilemma when addressing multiple human preferences: improvements in one dimension frequently come at the expense of others, creating unavoidable trade-offs between competing objectives like helpfulness and harmlessness. While prior work mainly focuses on constraint-based optimization algorithms and data selection strategies to mitigate conflicts, these approaches overlook the fundamental issue of resolving conflicts directly at the parameter level. In this paper, we present OrthAlign, an innovative approach that pioneers a new paradigm by leveraging orthogonal subspace decomposition to fundamentally resolve gradient-level conflicts in multi-objective preference alignment. OrthAlign strategically decomposes parameter update spaces into orthogonal subspaces, ensuring that optimization toward different preferences occurs in mathematically non-interfering directions. Building upon this, we provide theoretical guarantees demonstrating that when parameter increments satisfy both orthogonal subspace constraints and spectral norm bounds, the resulting updates exhibit linear Lipschitz growth rather than exponential instability, ensuring stable convergence across all preference dimensions. Extensive experiments show that: I. OrthAlign achieves maximum single-preference improvements ranging from 34.61% to 50.89% after multiple-objective alignment across helpful, harmless, and truthful dimensions. II. With an average overall reward improvement of 13.96%.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.938049",
        "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，该论文的本质是提出一种名为OrthAlign的新方法，通过正交子空间分解技术解决大语言模型在多目标对齐中的参数级冲突问题。这明显属于改进LLM基础能力和提出新训练范式的研究范畴，而非将LLM作为工具应用到特定领域。 其次，论文明确涉及LLM核心概念，并关注模型的基础能力改进。虽然论文没有直接讨论推理能力，但多目标对齐（如同时优化有用性、无害性和真实性）是提升LLM整体能力的基础，间接支持更好的推理和问题解决能力。 第三，论文不涉及任何排除标准中的领域。它既不关注多模态与视觉，也不针对特定应用领域（如医疗、化学等），同时虽然涉及安全性（无害性），但这是作为多目标对齐的一个维度进行研究，而非专门研究模型可靠性的应用层面技术。 最后，在特殊和模糊情况处理上，论文虽然涉及安全性，但提出了基础性的技术解决方案（正交子空间分解），而非进行社会学研究或应用层面的讨论。这种方法通过解决参数更新中的冲突，确保模型在多个维度上都能稳定提升，从而增强模型的通用能力。 因此，这篇论文致力于提高LLM的基础能力，提出新的训练范式，与\"大语言模型通用推理能力\"的研究目标一致。"
    },
    {
        "index": "#252",
        "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",
        "link": "/arxiv/2509.23676",
        "arxiv_id": "2509.23676",
        "authors": "Jue Zhang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang",
        "summary": "Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \\href{https://aka.ms/R2A-code}{this URL}.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:04.995120",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是深入研究大型推理模型(LRMs)中的推理机制，探讨推理轨迹如何影响最终答案生成，这直接关系到LLM的基础推理能力提升。论文通过三个阶段（实证评估、注意力分析和机制干预）来理解推理过程在模型内部的功能作用，属于对LLM内在推理能力的机制性研究，而非将LLM作为工具应用到特定领域。 其次，论文明确符合多个正面指标：它研究的是大型推理模型(LRMs)，直接属于LLMs范畴；核心聚焦于reasoning能力，探索推理过程与答案生成的关系；虽然未明确提及特定训练方法或新兴范式，但研究的是DeepSeek R1蒸馏模型，这类模型通常涉及先进的训练技术。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，论文的特殊情况处理方面，它不是关于智能体/工具使用的研究，也不是主要关注幻觉/可解释性/安全问题，而是专注于推理过程的机制性分析，这恰恰是提升LLM通用推理能力的核心研究方向。 因此，这篇论文的核心贡献是深入理解LLMs如何利用推理标记生成答案，揭示中间推理在塑造模型输出中的功能作用，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#266",
        "title": "Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning",
        "link": "/arxiv/2509.23292",
        "arxiv_id": "2509.23292",
        "authors": "Ningning Xu, Yuxuan Jiang, Shubhashis Roy Dipta",
        "summary": "Tool-integrated reasoning (TIR) has become a key approach for improving large reasoning models (LRMs) on complex problems. Prior work has mainly studied when to invoke tools, while overlooking how tools are applied. We identify two common patterns: a calculator pattern that uses code for direct computation, and an algorithmic pattern that encodes problems as programs. Misaligned choices often cause failures even when reasoning is sound. We propose a two-stage framework that first builds code competence from both patterns and then aligns pattern selection with teacher preferences. Across challenging math datasets, our pattern-aware method substantially improves both code usage and accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a pattern-aware approach for tool-integrated reasoning.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:05.033473",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进大语言模型的基础能力，特别是工具使用能力。论文提出了\"Pattern-Aware Tool-Integrated Reasoning\"这一新框架，专注于提升LLM\"如何使用工具\"而不仅仅是\"何时使用工具\"，这是一种新的训练范式，旨在增强模型的推理能力，特别是在数学推理方面。这完全符合第一步中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。 其次，从正面指标来看，论文包含了多个相关主题： - 核心概念：明确提到了\"large reasoning models (LRMs)\"，与LLMs直接相关 - 能力方向：重点关注\"reasoning\"，特别是\"math reasoning\" - 新兴范式：深入探讨\"tool use\"，这是LLM新兴范式的重要组成部分 第三，论文不符合任何排除标准。它不涉及多模态与视觉内容，不聚焦于特定应用领域（虽然测试于数学数据集，但方法是通用的），也不主要讨论模型可靠性等应用层面问题。 最后，从特殊和模糊情况处理来看，论文提出的是一种通用的工具使用方法，旨在增强LLM的通用问题解决能力，而不是将工具应用在特定领域。这完全符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的保留标准。 综上所述，这篇论文的核心贡献是提出了一种新的、模式感知的工具集成推理框架，通过改进LLM的工具使用能力来提升其通用推理能力，特别是在数学推理方面取得了显著进步。这完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#263",
        "title": "Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking",
        "link": "/arxiv/2509.23392",
        "arxiv_id": "2509.23392",
        "authors": "Jinyi Han, Ying Huang, Ying Liao, Zishang Jiang, Xikun Lu, Haiquan Zhao, Xinyi Wang, Guanghao Zhou, Sihang Jiang, Jiaqing Liang, Weikang Zhou, Zeye Sun, Fei Yu, Yanghua Xiao",
        "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on challenging tasks, yet their deep reasoning often incurs substantial computational costs. To achieve efficient reasoning, existing reinforcement learning methods still struggle to construct short reasoning path during the rollout stage, limiting effective learning. Inspired by Evidence Accumulation Models, we find that LRMs have accumulated sufficient information early in reasoning, making further reasoning steps redundant. Based on this insight, we propose Just-Enough Thinking (JET), which trains models to proactively terminate unnecessary reasoning. JET performs trajectory truncation during rollout to expose the model to short, distributionally consistent reasoning paths. Besides, it uses a quality-controlled length reward to better encourage concise reasoning while maintaining correctness. Extensive experiments demonstrate that JET significantly improves reasoning efficiency without sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6% accuracy gain while reducing output length by 46.3% on the Olympiad benchmark. Our code is available in the GitHub.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:05.006015",
        "filter_reason": "这篇论文完全符合研究目标。论文的核心贡献是提出一种名为\"Just-Enough Thinking\"（JET）的新训练方法，用于优化大型推理模型(LRMs)的推理过程。论文发现这些模型在推理早期已经积累了足够信息，后续步骤往往是冗余的，因此训练模型主动终止不必要的推理步骤。这直接符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的要求。论文不是将LLM作为工具应用到特定领域，而是专注于提升模型本身的通用推理能力，特别是推理效率。它符合多个正面指标，包括核心概念（大型推理模型）、能力方向（推理）和训练方法（强化学习）。同时，论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#269",
        "title": "C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning",
        "link": "/arxiv/2509.23129",
        "arxiv_id": "2509.23129",
        "authors": "Haotian Liu, Shuo Wang, Hongteng Xu",
        "summary": "Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at https://github.com/HaotianLiu123/CCGSPG.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:05.035408",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为C$^2$GSPG的置信度校准组序列策略梯度方法，旨在通过强化学习增强大语言模型的自我意识推理能力。论文直接针对LLM的通用推理能力进行改进，特别是解决现有RL方法中的过度自信问题，提高模型在逻辑和数学推理任务中的表现。它符合筛选标准中的核心判断，即论文本质是关于改进LLM的基础能力和提出新的训练范式。论文包含多个正面指标，如关注LLMs的核心概念、reasoning能力方向以及reinforcement learning训练方法。同时，论文不符合任何排除标准，不涉及多模态、特定应用领域或模型可靠性的应用层面研究。虽然论文讨论了过度自信问题（与幻觉相关），但它是从提升推理能力的角度出发，符合\"提出新方法来减少幻觉，从而提升模型的通用可靠性和推理质量\"的保留标准。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#271",
        "title": "Multiplayer Nash Preference Optimization",
        "link": "/arxiv/2509.23102",
        "arxiv_id": "2509.23102",
        "authors": "Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi",
        "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:05.036757",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础能力，具体来说是通过提出Multiplayer Nash Preference Optimization (MNPO)这一新的训练范式来增强LLM与人类偏好的对齐能力。论文不是将LLM作为工具应用到特定领域，而是直接改进LLM本身的基础能力，这符合第一步的保留标准。 其次，从正面指标分析，论文明确涉及Large language models (LLMs)这一核心概念，并且提出了基于强化学习(RLHF)的新训练方法。虽然论文没有直接提及reasoning、planning等能力方向，但alignment能力的提升对于LLM理解复杂指令、执行多步任务等推理相关能力有重要支撑作用。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面的研究。 最后，论文提出的MNPO框架通过将偏好学习从双人扩展到多人设置，解决了现有方法在处理非传递性和异质性偏好时的局限性，这种基础方法论的改进有助于提升LLM的通用能力，间接支持其推理能力的增强。 综上所述，这篇论文的核心贡献是提出了一种改进LLM基础能力的新训练范式，符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#267",
        "title": "$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding",
        "link": "/arxiv/2509.23234",
        "arxiv_id": "2509.23234",
        "authors": "Runyan Tan, Shuang Wu, Phillip Howard",
        "summary": "Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:05.034075",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"$p$-less sampling\"的无超参数解码方法，用于改进大语言模型的输出质量。根据筛选标准，我判断它符合研究范围，原因如下： 首先，从核心判断来看，这篇论文的本质是改进LLM的基础解码能力，而不是将LLM作为工具应用到特定领域。论文提出了一种新的采样/解码策略，这是一种改进LLM基础能力的方法论研究，符合第一步的保留标准。 其次，论文包含多个正面指标：它明确关注Large Language Models (LLMs)这一核心概念；在能力方向上，论文特别在数学推理(math reasoning)和逻辑推理(logical reasoning)任务上验证了方法的有效性，这些正是通用推理能力的重要组成部分。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，而是专注于通用的解码方法改进。 最后，论文提出的方法通过改进解码策略来提高LLM在推理任务上的表现，这与研究目标\"提高大语言模型本身的通用推理能力\"直接相关。虽然论文也提到了创意写作任务，但其核心贡献在于提升LLM在数学和逻辑推理等通用能力上的表现，因此符合研究范围。"
    },
    {
        "index": "#272",
        "title": "Causally-Enhanced Reinforcement Policy Optimization",
        "link": "/arxiv/2509.23095",
        "arxiv_id": "2509.23095",
        "authors": "Xiangqi Wang, Yue Huang, Yujun Zhou, Xiaonan Luo, Kehan Guo, Xiangliang Zhang",
        "summary": "Large language models (LLMs) trained with reinforcement objectives often achieve superficially correct answers via shortcut strategies, pairing correct outputs with spurious or unfaithful reasoning and degrading under small causal perturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a drop-in reward-shaping framework that augments policy optimization with a differentiable proxy for causal coherence along the generation pathway from prompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal influence with Jacobian-based sensitivities, counterfactually hardens these signals to suppress nuisance cues, and fuses the resulting coherence score with task-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single tunable between accuracy and coherence trade-off. The unified reward integrates with PPO/GRPO without architectural changes. Across reasoning benchmarks and causal stress tests, CE-PO reduces reward hacking and unfaithful chain-of-thought while improving robustness to correlation-causation flips and light counterfactual edits, all at near-parity accuracy. Experimental results across 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on average (up to 9.58%), while improving robustness to correlation-causation flips and light counterfactual edits.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:05.037540",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。首先，从核心判断来看，论文的本质是提出一种名为\"Causally-Enhanced Policy Optimization (CE-PO)\"的新训练框架，用于改进大语言模型的因果推理能力，减少模型使用捷径策略和产生虚假推理的问题。这是一种直接提升LLM基础推理能力的方法，特别是针对逻辑和因果推理能力的增强，属于改进LLM通用推理能力的核心研究。 其次，论文包含多个正面指标：明确关注大语言模型(LLMs)作为核心概念；专注于reasoning能力，特别是causal reasoning（因果推理）这一重要的逻辑推理方向；提出了一种强化学习方法(CE-PO)，是对PPO/GRPO等现有强化学习训练范式的改进。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 最后，虽然论文涉及减少\"不忠实的思维链\"(unfaithful chain-of-thought)，但它不是从应用层面讨论幻觉问题，而是通过增强因果一致性来提升模型内在的推理质量和可靠性，这符合提升模型通用推理能力的研究目标。 论文的核心贡献是提出了一种通用的强化学习训练框架，通过增强因果一致性来提升LLM的推理能力，这直接符合研究课题的核心目标——提高大语言模型的通用推理能力。"
    },
    {
        "index": "#284",
        "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective",
        "link": "/arxiv/2509.22613",
        "arxiv_id": "2509.22613",
        "authors": "Siwei Wang, Yifei Shen, Haoran Sun, Shi Feng, Shang-Hua Teng, Li Dong, Yaru Hao, Wei Chen",
        "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.",
        "subjects": "Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:05.048563",
        "filter_reason": "这篇论文完全符合我的研究目标，核心贡献在于从理论角度分析强化学习(RL)如何提升大语言模型(LLM)的规划能力。根据筛选标准，我判断如下： 第一步核心判断：论文本质上是研究如何通过强化学习方法改进LLM的规划能力，这属于提升LLM通用推理能力的范畴。规划(planning)是通用推理能力的重要组成部分，论文关注的是LLM基础能力的提升，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个关键正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：聚焦于planning能力，这是推理能力的关键组成部分 - 训练方法：研究reinforcement learning (RL)对LLM的影响，包括策略梯度和Q学习 - 新兴范式：规划能力是llm-based agents和tool use的基础能力 第三步排除标准：论文不涉及任何排除标准中的领域，没有研究多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 第四步特殊和模糊情况：论文不存在模糊情况，它明确研究通用规划能力的理论提升，而非特定领域应用。 综合来看，这篇论文通过理论分析探讨了强化学习对LLM规划能力的影响，揭示了不同RL方法的优缺点，属于提升LLM通用推理能力的基础研究，完全符合我的研究目标。"
    },
    {
        "index": "#277",
        "title": "Not only a helper, but also a teacher: Interactive LLM Cascade",
        "link": "/arxiv/2509.22984",
        "arxiv_id": "2509.22984",
        "authors": "Yu Wu, Shuo Wu, Ye Tao, Yansong Li, Anand D. Sarwate",
        "summary": "Large Language Models (LLMs) vary widely in their capabilities, with larger models often having better performance but higher cost: choosing an LLM model often involves trading off performance and cost. The LLM Cascade is a paradigm that defers difficult queries from weak/cheap to strong/expensive models. This approach is nonadaptive: the deferral decision is trained offline. When confronted with similar or repeated queries, the LLM Cascade may then repeatedly consult the expensive model and incur higher cost. To improve the cascading efficiency, we propose Inter-Cascade, an online and interactive LLM Cascade that extends the role of strong model from a backup helper to a long-term teacher. In our system, when a strong model resolves a difficult query, it also distills its solution into a generalized, reusable problem-solving strategy that boosts the weak model on subsequent queries. Adding strategies to queries enables the weak model to dynamically improve its performance over time, avoiding computationally and time-intensive fine-tuning. Empirically, compared with standard LLM Cascade baselines across multiple benchmarks, the Inter-Cascade significantly improves the accuracy of the weak model (by up to 33.06 absolute percentage points) and the overall system (by up to 5.53 absolute percentage points), while reducing the calls to strong models (by up to 48.05% relative reduction) and saving the corresponding fees (by up to 49.63% relative reduction). Inter-Cascade demonstrates the effective in-context knowledge transfer between LLMs, and provides a general, scalable framework applicable to both open-source and API-based LLMs.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:05.040318",
        "filter_reason": "这篇论文符合我的研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断——这篇论文的本质是提出一种名为\"Inter-Cascade\"的在线交互式LLM级联系统，其核心创新在于让强模型不仅作为解决困难问题的助手，还作为\"教师\"角色，将解决方案提炼为通用、可重用的问题解决策略，从而提升弱模型的能力。这明显属于改进LLM基础能力和提出新的训练/交互范式的研究，目的是增强LLM的问题解决能力，而不是将LLM应用到特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确聚焦于Large Language Models (LLMs) - 能力方向：关注problem-solving strategies（问题解决策略），直接涉及LLM的推理能力 - 新兴范式：描述了多个LLM之间的交互和知识传递，类似于multi-agent系统的概念 第三步：排除标准——论文不涉及任何排除领域： - 没有涉及多模态与视觉内容 - 没有针对特定应用领域（如医疗、化学等） - 没有讨论模型可靠性方面的应用问题（如水印、安全等） 第四步：特殊和模糊情况处理—— 论文提出的Inter-Cascade框架是一种通用的多LLM协作方法，让强模型\"教\"弱模型通用问题解决策略，这可以视为一种通用的智能体协作框架，旨在提升LLM的通用问题解决能力，而非针对特定领域的应用。 最终决策：论文的核心贡献是提出了一种让LLM之间进行知识传递和策略提炼的方法，从而提升整体推理能力，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文关注的是如何增强LLM的基础推理和问题解决能力，而非将其作为工具应用到特定领域，因此应该被保留。"
    },
    {
        "index": "#280",
        "title": "Adaptive Margin RLHF via Preference over Preferences",
        "link": "/arxiv/2509.22851",
        "arxiv_id": "2509.22851",
        "authors": "Yaswanth Chittepu, Prasann Singhal, Greg Durrett, Scott Niekum",
        "summary": "Margin-based optimization is fundamental to improving generalization and robustness in classification tasks. In the context of reward model learning from preferences within Reinforcement Learning from Human Feedback (RLHF), existing methods typically rely on no margins, fixed margins, or margins that are simplistic functions of preference ratings. However, such formulations often fail to account for the varying strengths of different preferences, for example some preferences are associated with larger margins between responses, or they rely on noisy margin information derived from ratings. We argue that modeling the strength of preferences can lead to better generalization and more faithful alignment. Furthermore, many existing methods that use adaptive margins assume access to accurate preference scores, which can be difficult for humans to provide reliably. We propose an approach that leverages preferences over preferences, that is annotations indicating which of two preferences reflects a stronger distinction. We use this ordinal signal to infer adaptive margins on a per-datapoint basis. We introduce an extension to Direct Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from preference-over-preference supervision, enabling improved discriminative and generative performance. Empirically, our method outperforms vanilla DPO, DPO with fixed margins, and DPO with ground-truth margins on the UltraFeedback dataset. Additionally, we show that there is a tradeoff between discriminative and generative performance: improving test classification accuracy, particularly by correctly labeling weaker preferences at the expense of stronger ones, can lead to a decline in generative quality. To navigate this tradeoff, we propose two sampling strategies to gather preference-over-preference labels: one favoring discriminative performance and one favoring generative performance.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T20:55:05.041902",
        "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，论文的本质是改进大语言模型的基础训练方法，具体聚焦于强化学习人类反馈(RLHF)和直接偏好优化(DPO)技术。论文提出了一种名为DPO-PoP的新方法，通过利用\"偏好之上的偏好\"(preference over preferences)来推断自适应边界，从而提高模型的泛化能力和对齐质量。这明显属于改进LLM基础能力的范畴，符合研究目标。 从正面指标看，论文涉及核心概念LLMs（虽然未直接提及，但RLHF和DPO是LLM训练的关键技术）和训练方法中的强化学习(RLHF)，这些都是提高LLM通用推理能力的重要方向。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。论文关注的是通用训练方法的改进，而非特定领域应用。 虽然论文没有直接讨论推理、规划或问题解决能力，但通过改进RLHF和DPO这一基础训练范式，论文间接提升了模型的泛化能力和对齐质量，这有助于提高LLM的通用推理能力。因此，这篇论文符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。"
    },
    {
        "index": "#19",
        "title": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts",
        "link": "/arxiv/2509.25020",
        "arxiv_id": "2509.25020",
        "authors": "Jiayu Liu, Zhenya Huang, Anya Sims, Enhong Chen, Yee Whye Teh, Ning Miao",
        "summary": "The current paradigm for reasoning in large language models (LLMs) involves models \"thinking out loud\" via a sequence of tokens, known as chain-of-thought (CoT). This approach, while effective, has several significant drawbacks. Firstly, inference requires autoregressive generation of often thousands of CoT tokens, which is slow and computationally expensive. Secondly, it constrains reasoning to the discrete space of tokens, creating an information bottleneck across reasoning steps. Thirdly, it fundamentally entangles reasoning with token generation, forcing LLMs to \"think while speaking,\" which causes potentially short-sighted reasoning. In light of these limitations, we re-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our approach, rather than autoregressively generating tokens, we model reasoning as a hidden Markov chain of continuous, high-dimensional \"thoughts\". Each reasoning step involves a transition of the internal thoughts, where explicit reasoning steps (which may consist of hundreds of tokens) serve as observable variables, which are windows to peek into the implicit thoughts. Since this latent process is incompatible with the standard supervised learning, we further propose a two-phase variational training scheme. Our experiments on three benchmarks demonstrate that MARCOS outperforms existing continuous reasoning methods and, for the first time, achieves performance comparable to token-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup in inference. Beyond this, MARCOS offers additional advantages, such as step-level instead of token-level control over randomness, opening significant opportunities for reinforcement learning and reasoning in LLMs.",
        "subjects": "Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.219194",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的本质是提出MARCOS，一种新的推理范式来改进大语言模型的基础推理能力。论文针对当前思维链(CoT)推理方法的局限性（速度慢、计算成本高、推理受限于离散token空间），提出将推理建模为连续、高维\"思想\"的隐马尔可夫链。这明显属于改进LLM基础能力和通用推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题。核心概念方面，明确针对大语言模型(LLMs)；能力方向方面，直接针对reasoning能力，并在数学推理基准GSM8K上进行了验证；训练方法方面，提出了两阶段变分训练方案，并提到其方法为强化学习提供了新机会。 第三步排除标准：论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 第四步特殊和模糊情况：论文提出的是一种通用的推理改进方法，不是针对特定领域的应用，因此不需要特殊处理。 核心贡献在于提出了一种全新的推理范式，将LLM的推理过程从离散token生成转变为连续高维思想空间的马尔可夫链，显著提升了推理效率和质量。这直接服务于提升LLM的通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#23",
        "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards",
        "link": "/arxiv/2509.24981",
        "arxiv_id": "2509.24981",
        "authors": "Haoran He, Yuxiao Ye, Qingpeng Cai, Chen Hu, Binxing Jiao, Daxin Jiang, Ling Pan",
        "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving the reasoning abilities of large language models (LLMs). Current methods rely primarily on policy optimization frameworks like PPO and GRPO, which follow generalized policy iteration that alternates between evaluating the current policy's value and improving the policy based on evaluation. While effective, they often suffer from training instability and diversity collapse, requiring complex heuristic tricks and careful tuning. We observe that standard RLVR in math reasoning can be formalized as a specialized finite-horizon Markov Decision Process with deterministic state transitions, tree-structured dynamics, and binary terminal rewards. Though large in scale, the underlying structure is simpler than general-purpose control settings for which popular RL algorithms (e.g., PPO) were developed, suggesting that several sophisticated techniques in existing methods may be reduced or even omitted. Based on this insight, we prove a surprising result: the optimal action can be recovered from the Q-function of a fixed uniformly random policy, thereby bypassing the generalized policy iteration loop and its associated heuristics. We introduce Random Policy Valuation for Diverse Reasoning (ROVER) to translate this principle into a practical and scalable algorithm for LLM math reasoning, a minimalist yet highly effective RL method that samples actions from a softmax over these uniform-policy Q-values. ROVER preserves diversity throughout training, allowing sustained exploration of multiple valid pathways. Across multiple base models and standard math reasoning benchmarks, ROVER demonstrates superior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1, \\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite its radical simplification compared to strong, complicated existing methods.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.226397",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文本质上是关于改进LLM的基础推理能力，提出了一种新的强化学习训练范式ROVER（Random Policy Valuation for Diverse Reasoning），专门针对提升LLM的数学推理能力。论文的核心贡献是发现并证明了一个重要结果：最优动作可以从固定均匀随机策略的Q函数中恢复，从而简化了传统的强化学习训练过程，提高了推理质量和多样性。 其次，从正面指标来看，论文明确包含多个相关主题：核心概念是\"Large language models (LLMs)\"，能力方向聚焦于\"reasoning\"特别是\"math reasoning\"，训练方法涉及\"reinforcement learning (RL)\"，具体是\"RL with Verifiable Rewards (RLVR)\"范式。 从排除标准来看，论文不涉及多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性的应用层面问题。虽然论文聚焦于数学推理，但这被视为LLM通用推理能力的一个重要方面，而非特定领域应用。 综上所述，这篇论文的核心贡献是提出了一种新的、简化的强化学习方法来提升LLM的通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#54",
        "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF",
        "link": "/arxiv/2509.24713",
        "arxiv_id": "2509.24713",
        "authors": "Jing Liu",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) reward models exhibit systematic failures on longtail distributions, leading to reward hacking and misalignment. We propose a mechanistic interpretability framework that identifies specialized neural circuits responsible for rare-event processing in reward models. Drawing from recent advances showing distributed specialization for rare tokens in language models\\citep{liu2025no, liu2025emergent}, we hypothesize that reward models also develop functionally distinct circuits for longtail scenarios. Our theoretical framework establishes formal connections between circuit specialization, reward generalization bounds, and longtail performance. We introduce \\textbf{Circuit-Aware Reward Training (CART)}, which uses circuit analysis to guide data augmentation, regularization, and ensemble strategies. This approach provides both theoretical insights into reward model failures and practical interventions for improving longtail robustness.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.257289",
        "filter_reason": "这篇论文的核心是关于改进RLHF（Reinforcement Learning from Human Feedback）中的奖励模型，特别是提高其在长尾分布上的鲁棒性。RLHF是提升大语言模型对齐能力和通用推理能力的关键训练范式，属于LLM基础能力的改进范畴。论文提出了Circuit-Aware Reward Training (CART)方法，通过机制可解释性框架识别处理罕见事件的专门神经回路，从而改进训练策略。这种方法论研究有助于提升LLM的通用推理能力和鲁棒性，而不是将LLM作为工具应用于特定领域。论文虽然没有直接讨论数学或逻辑推理，但通过改进RLHF这一核心训练方法，可以间接提升LLM的通用推理能力。论文不符合任何排除标准，且在训练方法方面有很强的正面指标，因此符合研究目标。"
    },
    {
        "index": "#58",
        "title": "Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory",
        "link": "/arxiv/2509.24653",
        "arxiv_id": "2509.24653",
        "authors": "Pengxiao Lin, Zheng-An Chen, Zhi-Qin John Xu",
        "summary": "Despite remarkable advances, large language models often fail at compositional reasoning tasks, a phenomenon exemplified by the ``curse of two-hop reasoning''. This paper introduces the Identity Bridge, a simple yet powerful mechanism that resolves this compositionality gap by supervising the model on a zero-hop identity task. We demonstrate empirically that this addition enables models to successfully perform out-of-distribution two-hop reasoning, a task they otherwise completely fail. To explain this phenomenon, we provide a theoretical analysis using a simplified Emb-MLP model, proving that identity supervision reshapes the model's latent geometry. We show this alignment is induced by an implicit nuclear-norm regularization during optimization, which favors low-rank solutions that share structure across tasks. For complex tasks, we use small initialization or weight decay to enhance the regularization effect, which enhances the latent space alignment effect and slows down the generalization decay. Finally, we extend our investigation to large-scale models, observing that they still achieve two-hop reasoning through the latent memory, which provides crucial inspiration for enhancing their implicit reasoning abilities.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.259395",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，该论文的本质是改进大语言模型的推理能力，特别是解决组合推理问题（如\"两跳推理的诅咒\"），这直接针对LLM的基础能力提升。论文提出的\"Identity Bridge\"机制通过在零跳身份任务上监督模型，成功使模型能够执行分布外的两跳推理，这明显属于增强LLM逻辑推理能力的创新方法。 其次，论文包含多个正面指标：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于推理能力(reasoning)，特别是组合推理和两跳推理等逻辑推理形式。虽然论文未涉及强化学习或智能体等新兴范式，但其核心贡献已足够符合研究目标。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉内容，不针对特定应用领域（如医疗、化学等），也不关注模型基础设施或部署优化。 最后，论文没有涉及需要特殊判断的模糊情况，它纯粹聚焦于通过改进模型内部机制（潜在几何结构的重塑）来提升通用推理能力，这正是研究目标所寻求的论文类型。论文的理论分析和实验验证都围绕如何增强LLM的隐式推理能力展开，完全符合\"提高大语言模型本身的通用推理能力\"的核心目标。"
    },
    {
        "index": "#67",
        "title": "Short window attention enables long-term memorization",
        "link": "/arxiv/2509.24552",
        "arxiv_id": "2509.24552",
        "authors": "Loïc Cabannes, Maximilian Beck, Gergely Szilvasy, Matthijs Douze, Maria Lomeli, Jade Copet, Pierre-Emmanuel Mazaré, Gabriel Synnaeve, Hervé Jégou",
        "summary": "Recent works show that hybrid architectures combining sliding window softmax attention layers with linear recurrent neural network (RNN) layers outperform both of these architectures taken separately. However, the impact of the window length and the interplay between softmax attention and linear RNN layers remain under-studied. In this work, we introduce SWAX, a hybrid architecture consisting of sliding-window attention and xLSTM linear RNN layers. A counter-intuitive finding with SWAX is that larger sliding windows do not improve the long-context performance. In fact, short window attention encourages the model to better train the long-term memory of the xLSTM, by relying less on the softmax attention mechanism for long context-retrieval. The issue with small sliding windows is that they are detrimental for short-context tasks, which could be solved with information from moderately larger sliding windows otherwise. Therefore, we train SWAX by stochastically changing the sliding window size, forcing the model to leverage both a longer context window and the xLSTM memory. SWAX trained with stochastic window sizes significantly outperforms regular window attention both on short and long-context problems.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.269323",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为SWAX的混合架构，结合了滑动窗口注意力和xLSTM线性RNN层，并通过随机改变滑动窗口大小来训练模型。论文的本质是改进LLM的基础架构和训练方法，特别是优化模型的长期记忆和上下文处理能力，这些能力是通用推理能力的基础组件。根据第一步的判断标准，这篇论文属于改进LLM基础能力的研究，而非将LLM作为工具应用到特定领域。在正面指标方面，论文涉及大语言模型的核心概念（注意力机制和RNN层），并提出了一种新的训练方法。论文不符合任何排除标准，没有涉及多模态、特定应用领域或模型可靠性等内容。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。"
    },
    {
        "index": "#71",
        "title": "Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models",
        "link": "/arxiv/2509.24510",
        "arxiv_id": "2509.24510",
        "authors": "Jonas Hübotter, Patrik Wolf, Alexander Shevchenko, Dennis Jüni, Andreas Krause, Gil Kur",
        "summary": "Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier explanations mostly focused on the observation that TTT may help when applied to out-of-distribution adaptation or used with privileged data. However, the growing scale of foundation models with most test data being in-distribution questions these explanations. We instead posit that foundation models remain globally underparameterized, with TTT providing a mechanism for specialization after generalization, focusing capacity on concepts relevant to the test task. Specifically, under the linear representation hypothesis, we propose a model in which TTT achieves a substantially smaller in-distribution test error than global training. We empirically validate our model's key assumptions by training a sparse autoencoder on ImageNet, showing that semantically related data points are explained by only a few shared concepts. Finally, we perform scaling studies across image and language tasks that confirm the practical implications of our model, identifying the regimes where specialization is most effective.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.276777",
        "filter_reason": "这篇论文的核心贡献是提出并解释了基础模型（包括大语言模型）中的测试时训练（TTT）机制。作者通过理论分析和实验验证，阐明了TTT如何通过\"先泛化后专业化\"的方式提高模型性能，使模型能够将容量集中在与测试任务相关的概念上。这本质上是一种改进LLM基础能力、增强其通用问题解决能力的研究。虽然论文在ImageNet上进行了视觉领域的实验，但它同时也涵盖了语言任务，并且其理论框架是通用的，不局限于特定领域。TTT可以被视为一种增强模型推理和问题解决能力的新范式，因为它允许模型在测试时根据特定任务进行自适应调整，这与提高LLM通用推理能力的研究目标高度一致。论文没有主要聚焦于需要排除的领域，如特定应用领域或模型可靠性的应用层面问题。因此，这篇论文符合研究范围，应该被保留。"
    },
    {
        "index": "#84",
        "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning",
        "link": "/arxiv/2509.24372",
        "arxiv_id": "2509.24372",
        "authors": "Xin Qiu, Yulu Gan, Conor F. Hayes, Qiyao Liang, Elliot Meyerson, Babak Hodjat, Risto Miikkulainen",
        "summary": "Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: https://github.com/VsonicV/es-fine-tuning-paper.",
        "subjects": "Machine Learning, Artificial Intelligence, Neural and Evolutionary Computing",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.294152",
        "filter_reason": "这篇论文的核心贡献是提出了一种新的LLM微调方法——进化策略(ES)，并证明了它在多个方面优于现有的强化学习微调方法。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从核心判断来看，论文的本质是改进LLM的基础能力和提出新的训练范式，而非将LLM作为工具应用到特定领域。论文专注于LLM的微调技术，这直接关系到提升模型的基础能力。 其次，论文符合多个正面指标：它明确关注Large language models (LLMs)这一核心概念；讨论的训练方法涉及evolution strategies(ES)，这与筛选标准中的\"evolution, self-evolve\"相关；虽然论文没有直接提及reasoning等能力，但改进微调方法通常会提升这些通用能力。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉问题，没有聚焦于特定应用领域，也没有主要关注模型可靠性层面的水印、安全性等问题。 最后，论文提出的是一种通用的微调方法，可以提升LLM在各种任务上的表现，包括推理能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标一致。论文表明ES方法在样本效率、长期奖励容忍度、对不同基础LLM的鲁棒性等方面优于RL方法，这些都是提升LLM通用能力的重要方面。 因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#110",
        "title": "The Impossibility of Inverse Permutation Learning in Transformer Models",
        "link": "/arxiv/2509.24125",
        "arxiv_id": "2509.24125",
        "authors": "Rohan Alur, Chris Hays, Manish Raghavan, Devavrat Shah",
        "summary": "In this technical note, we study the problem of inverse permutation learning in decoder-only transformers. Given a permutation and a string to which that permutation has been applied, the model is tasked with producing the original (``canonical'') string. We argue that this task models a natural robustness property across a variety of reasoning tasks, including long-context retrieval, multiple choice QA and in-context learning. Our primary contribution is an impossibility result: we show that an arbitrary depth, decoder-only transformer cannot learn this task. This result concerns the expressive capacity of decoder-only transformer models and is agnostic to training dynamics or sample complexity. We give a pair of alternative constructions under which inverse permutation learning is feasible. The first of these highlights the fundamental role of the causal attention mask, and reveals a gap between the expressivity of encoder-decoder transformers and the more popular decoder-only architecture. The latter result is more surprising: we show that simply padding the input with ``scratch tokens\" yields a construction under which inverse permutation learning is possible. We conjecture that this may suggest an alternative mechanism by which chain-of-thought prompting or, more generally, intermediate ``thinking'' tokens can enable reasoning in large language models, even when these tokens encode no meaningful semantic information (e.g., the results of intermediate computations).",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.384377",
        "filter_reason": "根据筛选标准，这篇论文符合研究范围，应该被保留。我的判断过程如下： 第一步：核心判断——这篇论文的本质是研究decoder-only transformers在逆置换学习任务上的表达能力限制。论文探讨了大语言模型架构与推理能力之间的关系，并提出添加\"scratch tokens\"可能是一种增强模型推理能力的方法。这本质上是对LLM基础能力的研究，特别是关于其推理能力限制的探索，而不是将LLM作为工具应用到特定领域。论文还明确探讨了这一发现对思维链(CoT)提示如何启用推理的启示，这与提高LLM通用推理能力直接相关。 第二步：正面指标——论文包含多个相关主题： - 核心概念：论文研究的是decoder-only transformers，这是大语言模型的主流架构 - 能力方向：论文明确提到逆置换学习任务模拟了各种推理任务的自然鲁棒性属性，包括长上下文检索、多选QA和上下文学习 - 论文还探讨了思维链(CoT)提示如何启用推理，这是大语言模型推理的重要范式 第三步：排除标准——论文不涉及任何需要排除的领域： - 不涉及多模态与视觉领域 - 不聚焦于任何特定应用领域 - 不涉及模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况——论文没有涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等模糊情况。 综合来看，这篇论文的核心贡献是研究大语言模型在推理任务上的表达能力限制，探讨模型架构与推理能力之间的关系，并提出可能增强模型推理能力的方法。这与\"提高大语言模型的通用推理能力\"的研究目标高度相关，因此应该被保留。"
    },
    {
        "index": "#128",
        "title": "Does Weak-to-strong Generalization Happen under Spurious Correlations?",
        "link": "/arxiv/2509.24005",
        "arxiv_id": "2509.24005",
        "authors": "Chenruo Liu, Yijun Dong, Qi Lei",
        "summary": "We initiate a unified theoretical and algorithmic study of a key problem in weak-to-strong (W2S) generalization: when fine-tuning a strong pre-trained student with pseudolabels from a weaker teacher on a downstream task with spurious correlations, does W2S happen, and how to improve it upon failures? We consider two sources of spurious correlations caused by group imbalance: (i) a weak teacher fine-tuned on group-imbalanced labeled data with a minority group of fraction $\\eta_\\ell$, and (ii) a group-imbalanced unlabeled set pseudolabeled by the teacher with a minority group of fraction $\\eta_u$. Theoretically, a precise characterization of W2S gain at the proportional asymptotic limit shows that W2S always happens with sufficient pseudolabels when $\\eta_u = \\eta_\\ell$ but may fail when $\\eta_u \\ne \\eta_\\ell$, where W2S gain diminishes as $(\\eta_u - \\eta_\\ell)^2$ increases. Our theory is corroborated by extensive experiments on various spurious correlation benchmarks and teacher-student pairs. To boost W2S performance upon failures, we further propose a simple, effective algorithmic remedy that retrains the strong student on its high-confidence data subset after W2S fine-tuning. Our algorithm is group-label-free and achieves consistent, substantial improvements over vanilla W2S fine-tuning.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.439677",
        "filter_reason": "这篇论文的核心是研究弱到强(W2S)泛化问题，特别是在存在虚假相关性的情况下如何提高模型性能。根据筛选标准，我判断它符合研究范围，原因如下： 1. 核心判断：论文的本质是关于改进模型训练范式和泛化能力的理论研究。它探讨了如何通过弱教师模型的伪标签来微调强预训练学生模型，并提出了一种算法来提高W2S性能。这属于改进LLM基础能力的研究，而非将LLM作为工具应用到特定领域。 2. 正面指标：虽然论文没有直接使用\"Large language models, LLMs\"术语，但提到的\"strong pre-trained student\"通常指的就是LLM。论文关注泛化能力(generalization)，这与推理能力密切相关，属于LLM基础能力的重要组成部分。 3. 排除标准：论文不涉及多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性（应用层面）的内容，因此不符合排除标准。 4. 特殊情况处理：论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况，而是专注于模型训练和泛化能力的理论分析与算法改进。 综上所述，这篇论文致力于提高LLM的泛化能力，属于改进LLM基础能力的研究，符合\"提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。"
    },
    {
        "index": "#148",
        "title": "Bayesian Mixture-of-Experts: Towards Making LLMs Know What They Don't Know",
        "link": "/arxiv/2509.23830",
        "arxiv_id": "2509.23830",
        "authors": "Albus Yizhuo Li",
        "summary": "The Mixture-of-Experts (MoE) architecture has enabled the creation of massive yet efficient Large Language Models (LLMs). However, the standard deterministic routing mechanism presents a significant limitation: its inherent brittleness is a key contributor to model miscalibration and overconfidence, resulting in systems that often do not know what they don't know. This thesis confronts this challenge by proposing a structured \\textbf{Bayesian MoE routing framework}. Instead of forcing a single, deterministic expert selection, our approach models a probability distribution over the routing decision itself. We systematically investigate three families of methods that introduce this principled uncertainty at different stages of the routing pipeline: in the \\textbf{weight-space}, the \\textbf{logit-space}, and the final \\textbf{selection-space}. Through a series of controlled experiments on a 3-billion parameter MoE model, we demonstrate that this framework significantly improves routing stability, in-distribution calibration, and out-of-distribution (OoD) detection. The results show that by targeting this core architectural component, we can create a more reliable internal uncertainty signal. This work provides a practical and computationally tractable pathway towards building more robust and self-aware LLMs, taking a crucial step towards making them know what they don't know.",
        "subjects": "Machine Learning, Statistics Theory, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.461388",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是改进LLM的基础架构组件，具体是针对Mixture-of-Experts (MoE)架构中的路由机制进行创新。论文提出了贝叶斯MoE路由框架，通过在路由决策上建模概率分布，而不是强制确定性选择，来解决模型校准不良和过度自信问题。这明显属于改进LLM基础能力的范畴，而非将LLM作为工具应用到特定领域。因此，根据第一步判断标准，应该保留。 第二步：正面指标 论文明确包含以下正面指标： - 核心概念：论文直接关注Large Language Models (LLMs)，特别是基于MoE架构的LLMs。 - 能力方向：虽然论文没有直接讨论推理或规划，但它研究的模型校准和不确定性建模是高质量推理的基础。一个能准确评估自身不确定性的模型更有可能在推理任务中表现出色。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对任何特定应用领域（如医疗、化学、生物等） - 虽然关注模型可靠性，但这是从架构层面（MoE路由机制）来提高模型校准能力，而非应用层面的水印、安全或安全性问题。 第四步：特殊和模糊情况 论文关注模型校准和不确定性建模，这与减少幻觉和提高推理质量密切相关。论文提出的贝叶斯MoE路由框架是一种新的架构改进方法，旨在增强模型对自身不确定性的认知，从而减少过度自信和错误判断。这属于提出新方法来增强模型内在可靠性，从而提升通用推理质量的情况，应该保留。 最终决策 综合分析，这篇论文的核心贡献是通过改进MoE架构的路由机制，增强LLM的校准能力和不确定性建模，使其\"知道它们不知道什么\"。这种改进直接提升了模型的基础能力，为更高质量的推理提供了基础，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#157",
        "title": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement",
        "link": "/arxiv/2509.23799",
        "arxiv_id": "2509.23799",
        "authors": "Anyi Wang, Xuansheng Wu, Dong Shu, Yunpu Ma, Ninghao Liu",
        "summary": "Steering has emerged as a promising approach in controlling large language models (LLMs) without modifying model parameters. However, most existing steering methods rely on large-scale datasets to learn clear behavioral information, which limits their applicability in many real-world scenarios. The steering vectors extracted from small dataset often contain task-irrelevant noising features, which degrades their effectiveness. To refine the steering vectors learned from limited data, we introduce Refinement of Steering Vector via Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise and augment the steering vectors. In our framework, we first remove task-irrelevant features according to their semantics provided by SAEs, and then enrich task-relevant features missing from the small dataset through their semantic similarity to the identified relevant features. Extensive experiments demonstrate that the proposed SAE-RSV substantially outperforms all the baseline methods including supervised fine-tuning. Our findings show that effective steering vector can be constructed from limited training data by refining the original steering vector through SAEs.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.473954",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断——这篇论文的本质是改进LLM的基础能力。论文提出了SAE-RSV方法，通过稀疏自编码器来优化steering向量，这是一种不修改模型参数但能增强LLM控制能力的新技术。这属于改进LLM基础能力的范畴，而非将LLM作为工具应用到特定领域，因此应保留。 第二步：正面指标——论文明确符合\"Large language models, LLMs\"这一核心概念。虽然论文没有直接提及reasoning、planning等能力方向，也没有涉及强化学习等训练方法或智能体等新兴范式，但其提出的steering向量优化技术可以视为提升模型通用能力的一种方法。 第三步：排除标准——论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性（应用层面）的研究。 第四步：特殊和模糊情况——论文不属于智能体/工具使用的特殊范畴。虽然其方法可能间接影响模型的可解释性和控制能力，但这不是论文的主要焦点。 综合判断：这篇论文的核心贡献是提出了一种通过稀疏自编码器优化steering向量的方法，以增强LLM的控制能力。这属于改进LLM基础能力的研究，虽然没有直接针对推理能力，但提供了一种可能间接提升模型在各种任务上表现的新技术，因此符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#177",
        "title": "Why Alignment Must Precede Distillation: A Minimal Working Explanation",
        "link": "/arxiv/2509.23667",
        "arxiv_id": "2509.23667",
        "authors": "Sungmin Cha, Kyunghyun Cho",
        "summary": "For efficiency, preference alignment is often performed on compact, knowledge-distilled (KD) models. We argue this common practice introduces a significant limitation by overlooking a key property of the alignment's reference model: its distributional recall. We show that the standard KD -> Align workflow diminishes the model's capacity to align rare yet desirable behaviors, even under strong preference signals. We instead demonstrate that reversing the pipeline (i.e., Align -> KD) is essential: alignment must first be performed on a high-recall reference before distillation. Our contributions are threefold. First, we provide a minimal working explanation of how the reference model constrains preference alignment objectives at a fundamental level. Second, we validate this theory in a controllable Mixture-of-Gaussians experiment, where low-recall anchoring consistently results in suboptimal model performance. Finally, we demonstrate that the same phenomenon holds in LLM alignment with the SmolLM2 family: models aligned after KD fail to effectively align target behaviors, resulting in substantially lower reward and target precision. In contrast, our proposed Align -> KD pipeline robustly aligns these behaviors, yielding models with superior target-oriented metrics and lower variance. Together, these results establish reference-model recall as a first-order design choice in alignment, offering a clear principle: alignment must precede distillation.",
        "subjects": "Machine Learning",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.481769",
        "filter_reason": "这篇论文的核心是关于改进大语言模型的训练范式，特别是对齐(alignment)和知识蒸馏(knowledge distillation)的顺序问题。论文提出先对齐再蒸馏(Align -> KD)的流程，以保持模型的对齐效果，特别是对于罕见但理想的行为。这属于改进LLM基础能力和训练范式的研究，符合我的研究目标中\"提出新的训练范式\"的范畴。 具体分析： 1. 第一步核心判断：论文本质是改进LLM的基础训练流程，属于模型基础能力的改进，应该保留。 2. 第二步正面指标：论文明确涉及大语言模型(LLMs)这一核心概念，并在SmolLM2系列模型上验证了其方法。 3. 第三步排除标准：论文不涉及多模态、特定应用领域或模型可靠性的应用层面等排除领域。 4. 第四步特殊和模糊情况：论文虽未直接讨论推理能力，但对齐过程本身是提升模型整体性能和可靠性的关键环节，良好的对齐可以间接提高模型在各种任务上的表现，包括推理任务。 论文的核心贡献是提出了一种改进的模型训练流程，通过优化对齐和蒸馏的顺序来提高模型的对齐质量和整体性能。虽然不是直接针对推理能力的研究，但这种基础训练范式的改进可能会对模型的通用能力产生积极影响，包括推理能力。因此，这篇论文符合我的研究范围。"
    },
    {
        "index": "#212",
        "title": "Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought",
        "link": "/arxiv/2509.23365",
        "arxiv_id": "2509.23365",
        "authors": "Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian",
        "summary": "Previous work shows that the chain of continuous thought (continuous CoT) improves the reasoning capability of large language models (LLMs) by enabling implicit parallel thinking, and a subsequent work provided theoretical insight by showing that a two-layer transformer equipped with continuous CoT can efficiently solve directed graph reachability by maintaining a superposition of multiple reasoning traces in the continuous thought. However, it remains unclear how the superposition mechanism is naturally learned from gradient-based training methods. To fill this gap, we theoretically analyze the training dynamics of a simplified two-layer transformer on the directed graph reachability problem to unveil how the superposition mechanism emerges during training in two training stages -- (i) a thought-generation stage that autoregressively expands the continuous thought, and (ii) a prediction stage that converts the thought into the final answer. Our analysis reveals that during training using continuous thought, the index-matching logit, an important quantity which reflects the strength of the model's local search ability, will first increase and then remain bounded under mild assumptions. The bounded index-matching logit effectively balances exploration and exploitation during the reasoning process: the model will exploit local problem structures to identify plausible search traces, and assign comparable weights to multiple such traces to explore when it is uncertain about which solution is correct, which results in superposition. Our experimental results tracking the growth of logits further validate our theory.",
        "subjects": "Machine Learning",
        "date": "2025-09-27",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.503678",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文本质上是研究连续思维链(continuous CoT)如何提高大语言模型的推理能力，属于改进LLM基础能力的研究。论文通过理论分析揭示了超位置机制在训练过程中的形成，这直接关联到LLM的通用推理能力提升。其次，从正面指标看，论文明确涉及大语言模型(LLMs)核心概念，聚焦于推理能力(reasoning)和问题解决(problem-solving)，并研究思维链这一新兴推理范式的训练动态。第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文使用图可达性问题仅作为测试平台，而非特定领域应用。总体而言，该论文通过理论分析揭示了连续思维链如何增强LLM的推理能力，特别是通过超位置机制实现隐式并行思维，这直接贡献于提升LLM的通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#225",
        "title": "NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning",
        "link": "/arxiv/2509.23252",
        "arxiv_id": "2509.23252",
        "authors": "Raviteja Anantha, Soheil Hor, Teodor Nicola Antoniu, Layne C. Price",
        "summary": "We present NanoFlux, a novel adversarial framework for generating targeted training data to improve LLM reasoning, where adversarially-generated datasets containing fewer than 200 examples outperform conventional fine-tuning approaches. The framework employs a competitive dynamic between models alternating as Attacker and Defender, supervised by a tool-augmented Judge, synthesizing multi-step questions with explanatory annotations that target specific reasoning capabilities. Fine-tuning a 4B-parameter model on NanoFlux-generated data yields performance gains across diverse domains compared to full-benchmark fine-tuning: +5.9% on mathematical reasoning (GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical reasoning (MultiMedQA), while reducing computational requirements by 3-14x. Ablation studies reveal a non-monotonic relationship between dataset characteristics and model performance, uncovering domain-specific optimal points for question complexity and reasoning quality. NanoFlux automates training data generation through embedding-based novelty filtering, tool-augmented evaluation, and multi-hop reasoning, suggesting that future model improvements may lie in the intelligent synthesis of small, precisely targeted training datasets.",
        "subjects": "Machine Learning",
        "date": "2025-09-27",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.515634",
        "filter_reason": "根据筛选标准，我对这篇论文进行了详细分析： 第一步核心判断：这篇论文的核心是提出NanoFlux，一个对抗性框架用于生成有针对性的训练数据，以提高LLM的推理能力。论文本质上是关于改进LLM的基础推理能力，提出了一种新的训练范式（对抗性双LLM框架），增强了模型的多步推理能力。这符合\"改进LLM基础能力\"和\"提出新训练范式\"的保留标准。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确针对LLMs - 能力方向：专注于reasoning，特别是mathematical reasoning, scientific reasoning, medical reasoning和multi-step reasoning - 新兴范式：使用了tool-augmented Judge和tool-augmented evaluation，属于工具使用范畴 第三步排除标准：论文虽然提到了在医疗推理(MultiMedQA)上的应用，但这只是作为评估方法效果的基准之一，而非论文的核心焦点。论文的核心是通用的推理能力提升框架，不主要聚焦于特定应用领域，因此不违反排除标准。 第四步特殊和模糊情况：论文中的工具使用(tool-augmented Judge)是为了增强LLM的通用推理能力，属于通用智能体/工具使用方法，而非特定领域应用，因此应该保留。 综上所述，这篇论文的核心贡献是提出了一种通过对抗性框架生成高质量训练数据来增强LLM通用推理能力的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#232",
        "title": "Towards Monotonic Improvement in In-Context Reinforcement Learning",
        "link": "/arxiv/2509.23209",
        "arxiv_id": "2509.23209",
        "authors": "Wenhao Zhang, Shao Zhang, Xihuai Wang, Yang Li, Ying Wen",
        "summary": "In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm for developing agents that can rapidly adapt to new tasks by leveraging past experiences as context, without updating their parameters. Recent approaches train large sequence models on monotonic policy improvement data from online RL, aiming to a continue improved testing time performance. However, our experimental analysis reveals a critical flaw: these models cannot show a continue improvement like the training data during testing time. Theoretically, we identify this phenomenon as Contextual Ambiguity, where the model's own stochastic actions can generate an interaction history that misleadingly resembles that of a sub-optimal policy from the training data, initiating a vicious cycle of poor action selection. To resolve the Contextual Ambiguity, we introduce Context Value into training phase and propose Context Value Informed ICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing the ideal performance theoretically achievable by a policy given the current context. As the context expands, Context Value could include more task-relevant information, and therefore the ideal performance should be non-decreasing. We prove that the Context Value tightens the lower bound on the performance gap relative to an ideal, monotonically improving policy. We fruther propose two methods for estimating Context Value at both training and testing time. Experiments conducted on the Dark Room and Minigrid testbeds demonstrate that CV-ICRL effectively mitigates performance degradation and improves overall ICRL abilities across various tasks and environments. The source code and data of this paper are available at https://github.com/Bluixe/towards_monotonic_improvement .",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.524323",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"上下文价值知情ICRL\"(CV-ICRL)的新方法，用于解决上下文强化学习中的\"上下文歧义\"问题，从而实现智能体性能的单调改进。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，这篇论文研究的是改进序列模型的基础能力，特别是上下文强化学习(ICRL)能力，这是一种让模型能够从过去经验中学习并适应新任务的通用能力。论文提出的CV-ICRL方法是一种新的训练范式，通过引入\"上下文价值\"概念来增强模型的推理和决策能力，这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理等通用能力\"的要求。 其次，从正面指标看，论文涉及了强化学习(RL)这一训练方法，以及问题解决(problem-solving)这一能力方向。虽然论文没有明确提到\"大语言模型\"这一术语，但其研究的\"large sequence models\"和上下文学习能力与大语言模型的核心特性高度相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文研究的是通用的智能体学习框架，旨在增强智能体的推理和决策能力，而不是将智能体应用于特定领域，这符合保留条件。 综上所述，尽管论文没有明确指出其研究对象是大语言模型，但其研究的上下文强化学习能力是大语言模型的核心能力之一，且论文提出的方法旨在增强模型的通用推理能力，因此符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。"
    },
    {
        "index": "#235",
        "title": "ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse",
        "link": "/arxiv/2509.23183",
        "arxiv_id": "2509.23183",
        "authors": "Guohao Chen, Shuaicheng Niu, Deyu Chen, Jiahao Yang, Zitian Zhang, Mingkui Tan, Pengcheng Wu, Zhiqi Shen",
        "summary": "Test-time entropy minimization helps adapt a model to novel environments and incentivize its reasoning capability, unleashing the model's potential during inference by allowing it to evolve and improve in real-time using its own predictions, achieving promising performance. However, pure entropy minimization can favor non-generalizable shortcuts, such as inflating the logit norm and driving all predictions to a dominant class to reduce entropy, risking collapsed solutions (e.g., constant one-hot outputs) that trivially minimize the objective without meaningful learning. In this paper, we introduce ZeroSiam, an efficient asymmetric Siamese architecture tailored for test-time entropy minimization. ZeroSiam prevents collapse through asymmetric divergence alignment, which is efficiently achieved by a learnable predictor and a stop-gradient operator before the classifier. We provide empirical and theoretical evidence that ZeroSiam not only prevents collapse solutions, but also absorbs and regularizes biased learning signals, enhancing performance even when no collapse occurs. Despite its simplicity, extensive results show that ZeroSiam performs more stably over prior methods using negligible overhead, demonstrating efficacy on both vision adaptation and large language model reasoning tasks across challenging test scenarios and diverse models, including tiny models that are particularly collapse-prone.",
        "subjects": "Machine Learning, Networking and Internet Architecture",
        "date": "2025-09-27",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.525922",
        "filter_reason": "这篇论文的核心贡献是提出ZeroSiam，一种用于测试时熵优化的非对称Siamese架构。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，论文聚焦于测试时熵优化技术，这是一种在推理阶段增强模型能力的方法。摘要明确指出该方法旨在\"释放模型在推理过程中的潜力，允许它实时进化和改进\"，这直接关联到提高LLM的通用推理能力，而非将LLM作为工具应用于特定领域。 其次，论文包含多个正面指标：核心概念上明确提到\"large language model reasoning tasks\"；能力方向上关注\"reasoning capability\"；训练方法上涉及\"evolve and improve in real-time\"的自我进化概念。 第三，虽然论文提到视觉适应任务，但这不是其主要焦点，而是作为方法的一个应用场景。论文的核心是提出一种通用的测试时优化方法，同时应用于视觉适应和LLM推理任务，因此不应因涉及视觉而被排除。 最后，论文提出的防止崩溃的方法有助于提高模型的推理质量和可靠性，这与提升LLM通用推理能力的目标一致。论文关注的是模型在推理时的能力提升，而非特定应用或基础设施优化。 综上所述，该论文致力于通过测试时熵优化技术提高大语言模型的通用推理能力，符合研究目标。"
    },
    {
        "index": "#241",
        "title": "Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers",
        "link": "/arxiv/2509.23152",
        "arxiv_id": "2509.23152",
        "authors": "Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Yiwei Wang, Xiaodan Liang, Jing Tang",
        "summary": "Test-time scaling via solution sampling and aggregation has become a key paradigm for improving the reasoning performance of Large Language Models (LLMs). While reward model selection is commonly employed in this approach, it often fails to identify minority-yet-correct answers, which limits its effectiveness beyond that of simple majority voting. We argue that this limitation stems from a lack of informative critique signals during verifier training. To bridge this gap, we introduce Mirror-Critique, a framework that trains a verifier with informative critiques. Our key insight is to leverage the rich critique signal by contrasting model-generated solutions with ground-truth solutions. We deploy a small instruction-tuned model to synthesize high-quality critique data with rejection sampling that teaches the verifier not only what is wrong, but also why. The synthetic data is used to cold-start the LLMs in the RLVR process to further improve the verification ability. The resulting Mirror-Verifier is deployed to evaluate candidate solutions by generating multiple critiques per solution, aggregating them into a verify score used for weighted voting or selective abstention. The experimental results show that our Mirror-Verifier significantly outperforms majority voting in terms of solution accuracy and also improves the solver's honesty to recognize and abstain from answering beyond its capability boundaries.",
        "subjects": "Machine Learning",
        "date": "2025-09-27",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.534189",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是提出Mirror-Critique框架，通过训练带有信息性批判的验证器来提高大语言模型的推理性能。论文的核心贡献是改进LLM的基础推理能力，提出新的训练范式（使用RLVR过程），并增强模型的多步推理和自我评估能力。这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确讨论Large Language Models (LLMs) - 能力方向：专注于reasoning能力的提升，特别是通过验证器增强模型的逻辑推理 - 训练方法：使用了强化学习(RLVR过程)来训练验证器 - 论文虽未明确提及新兴范式，但其验证机制可视为一种增强LLM通用能力的工具使用方法 第三步：排除标准 论文不涉及任何排除标准中的领域： - 没有涉及多模态与视觉内容 - 没有针对特定应用领域（如医疗、化学等） - 虽然提到\"honesty\"，但这是指模型识别自身能力边界的内在可靠性，而非应用层面的水印、安全等问题 第四步：特殊和模糊情况处理 论文提出的验证器可以视为一种通用工具，用于增强LLM的推理能力，而非针对特定领域的应用。论文关注的是提高模型的推理准确性和诚实性，这与减少幻觉和提高模型内在可靠性相关，属于提升模型通用推理能力的范畴。 综上所述，这篇论文明确致力于提高大语言模型本身的通用推理能力，通过创新的验证器训练方法和批判信号利用，增强了模型的逻辑推理和问题解决能力，完全符合研究课题的目标。"
    },
    {
        "index": "#291",
        "title": "On the Capacity of Self-Attention",
        "link": "/arxiv/2509.22840",
        "arxiv_id": "2509.22840",
        "authors": "Micah Adler",
        "summary": "While self-attention is known to learn relations among tokens, we lack a formal understanding of its capacity: how many distinct relations can a single layer reliably recover for a given budget? To formalize this, we introduce Relational Graph Recognition (RGR), where the key-query channel represents a graph on $m$ items with $m'$ directed edges, and, given a context of items, must recover the neighbors of each item. We measure resources by the total key dimension $D_K = h\\,d_k$. Within this framework, we analytically derive a capacity scaling law and validate it empirically. We show that $D_K = \\Theta(m' \\log m' / d_{\\text{model}})$ is both necessary (information-theoretic lower bound) and sufficient (explicit construction) in a broad class of graphs to recover $m'$ relations. This scaling law directly leads to a new, capacity-based rationale for multi-head attention that applies even when each item only attends to a single target. When embeddings are uncompressed ($m = d_{\\text{model}}$) and the graph is a permutation, a single head suffices. However, compression ($m > d_{\\text{model}}$) forces relations into overlapping subspaces, creating interference that a single large head cannot disentangle. Our analysis shows that allocating a fixed $D_K$ across many small heads mitigates this interference, increasing the number of recoverable relations. Controlled single-layer experiments mirror the theory, revealing a sharp performance threshold that matches the predicted capacity scaling and confirms the benefit of distributing $D_K$ across multiple heads. Altogether, these results provide a concrete scaling law for self-attention capacity and a principled design rule for allocating key-query budget across heads.",
        "subjects": "Machine Learning",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.585419",
        "filter_reason": "这篇论文的核心是研究自注意力机制（大语言模型的核心组件）的基础能力和理论限制，提出了关系图识别(RGR)框架，并推导出自注意力机制的能力扩展定律。虽然论文没有直接讨论LLM的推理能力，但它深入研究了LLM的核心组件（自注意力机制）的能力限制，这对于理解和提高LLM的通用推理能力具有重要意义。论文不属于任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。相反，它关注的是模型的基础能力和理论理解，这符合\"改进LLM的基础能力\"的研究目标。通过更好地理解自注意力机制的能力限制，我们可以设计更有效的模型架构，从而提高LLM的通用推理能力。论文还提供了多头注意力的新理论基础，解释了为什么分配固定维度到多个小头比单个大头更有优势，这对于优化LLM架构以提高其推理能力具有直接指导意义。"
    },
    {
        "index": "#283",
        "title": "Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective",
        "link": "/arxiv/2509.22921",
        "arxiv_id": "2509.22921",
        "authors": "Matthieu Zimmer, Xiaotong Ji, Tu Nguyen, Haitham Bou Ammar",
        "summary": "We introduce a novel approach to large language model (LLM) distillation by formulating it as a constrained reinforcement learning problem. While recent work has begun exploring the integration of task-specific rewards into distillation processes, existing methods typically rely on ad-hoc reward weighting. We propose a principled optimization framework that maximizes task-specific rewards while constraining the divergence from the teacher model to remain below a specified threshold. Our approach adapts constrained state augmented reinforcement learning to the distillation setting, introducing a modified reward function that maintains theoretical guarantees of constraint satisfaction without requiring state augmentation or teacher model access during deployment and without the computational overhead of the dual Lagrangian methods. Through extensive experiments on mathematical reasoning tasks, we demonstrate that our method achieves better constraint satisfaction rates and better reasoning compared to the soft Lagrangian relaxation baselines while maintaining competitive task performance. Our framework provides a theoretically grounded and practically efficient solution for reward-aware distillation in resource-constrained settings.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.576309",
        "filter_reason": "这篇论文符合我的研究范围，核心理由如下： 首先，从本质上看，论文的核心是关于改进LLM的基础能力，提出了一种新的训练/优化范式。论文将大语言模型蒸馏问题构建为约束强化学习问题，这是一种新的方法论研究，而非将LLM作为工具应用于特定领域。 其次，论文符合多个正面指标： 1. 核心概念：明确关注大语言模型(LLM)的蒸馏问题 2. 能力方向：论文在数学推理任务上进行了实验，并证明其方法能提高模型的推理能力 3. 训练方法：使用了强化学习方法来优化蒸馏过程，属于新的训练范式探索 第三，论文不符合任何排除标准。虽然论文在数学推理任务上进行了实验，但数学推理被视为通用能力而非特定应用领域。论文的核心是提出一种通用的蒸馏方法，而不是专注于某个特定领域的应用。 论文的核心贡献是提出了一种原则性的优化框架，通过约束强化学习来改进LLM蒸馏过程，从而提高模型的推理能力。这种方法旨在解决如何更有效地从教师模型中提取知识，特别是关注推理能力的保持和提升，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。"
    },
    {
        "index": "#293",
        "title": "In-Context Learning can Perform Continual Learning Like Humans",
        "link": "/arxiv/2509.22764",
        "arxiv_id": "2509.22764",
        "authors": "Liuwang Kang, Fan Wang, Shaoshan Liu, Hung-Chyun Chou, Chuan Lin, Ning Ding",
        "summary": "Large language models (LLMs) can adapt to new tasks via in-context learning (ICL) without parameter updates, making them powerful learning engines for fast adaptation. While extensive research has examined ICL as a few-shot learner, whether it can achieve long-term retention and cross-task knowledge accumulation when multitasks arrive sequentially remains underexplored. Motivated by human memory studies, we investigate the retention characteristics of ICL in multitask settings and extend it to in-context continual learning (ICCL), where continual learning ability emerges through task scheduling and prompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that, for specific large-language models, ICCL benefits from distributed practice (DP) in a manner analogous to humans, consistently revealing a spacing \"sweet spot\" for retention. Beyond retention performance, we propose a human-retention similarity metric to quantify how closely a continual-learning (CL) method aligns with human retention dynamics. Using this metric, we show that linear-attention models such as MAMBA and RWKV exhibit particularly human-like retention patterns, despite their retention performance lagging behind that of Transformer-based LLMs. Overall, our results establish ICCL as both cognitively plausible and practically effective, providing an inference-only CL paradigm that mitigates catastrophic forgetting and addresses the stability-plasticity dilemma in conventional CL methods.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.586388",
        "filter_reason": "这篇论文的核心是研究大型语言模型(LLMs)的上下文学习(ICL)能力，并将其扩展为上下文持续学习(ICCL)，提出了一种新的学习范式。从第一步核心判断来看，这属于改进LLM基础能力和提出新训练范式的研究，符合保留标准。持续学习能力是通用推理能力的重要组成部分，涉及到知识积累、记忆保留和跨任务学习等能力，这些都是通用推理的基础要素。论文没有将LLM作为工具应用到特定领域，而是关注LLM本身的能力提升机制。从第二步正面指标看，论文明确包含LLMs核心概念，虽然未直接提及reasoning、planning等术语，但持续学习本身就是一种通用问题解决能力。从第三步排除标准看，论文未涉及多模态、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。"
    },
    {
        "index": "#308",
        "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning",
        "link": "/arxiv/2509.25052",
        "arxiv_id": "2509.25052",
        "authors": "Sai Wang, Yu Wu, Zhongwen Xu",
        "summary": "The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environment's mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environment's dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T20:55:07.606343",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种名为CEL的新型智能体架构，其核心贡献在于利用大语言模型进行推理和规划来学习，而非简单将LLM作为工具应用到特定领域。论文明确提出了一个新的学习范式，通过让LLM进行显式推理和规划来增强其通用问题解决能力，这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划和多步推理等通用能力\"的保留标准。 其次，论文满足多个正面指标：它明确以大语言模型(LLM)为核心概念；直接聚焦于推理(reasoning)和规划(planning)能力；提出了一种自我学习和进化的训练方法；并且属于基于LLM的智能体(llm-based agents)这一新兴范式。 第三，论文不符合任何排除标准：它不涉及多模态与视觉内容；虽然评估在游戏环境中进行，但这些只是作为测试平台，论文核心是提出通用架构而非特定应用领域；也没有聚焦于模型可靠性的应用层面问题。 最后，在特殊和模糊情况处理上，论文提出的是通用的智能体架构，而非针对特定领域的应用；同时，它强调通过显式推理构建透明和改进的世界模型，增强了模型的可解释性，从而提升通用推理质量。 综上所述，这篇论文的核心贡献是提出一种通过推理和规划来增强LLM通用能力的新方法，完全符合研究目标。"
    },
    {
        "index": "#368",
        "title": "Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models",
        "link": "/arxiv/2509.24261",
        "arxiv_id": "2509.24261",
        "authors": "Yuhua Jiang, Jiawei Huang, Yufeng Yuan, Xin Mao, Yu Yue, Qianchuan Zhao, Lin Yan",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing Large Language Models (LLMs) on complex reasoning tasks. However, existing methods suffer from an exploration dilemma: the sharply peaked initial policies of pre-trained LLMs confine standard RL algorithms to a narrow set of solutions, boosting single-solution accuracy (pass@1) but suppressing solution diversity and multi-solution performance (pass@k). As a result, RLVR often distills existing capabilities rather than discovering new reasoning strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement Learning framework. Our approach employs a risk-seeking objective that interpolates between mean and maximum rewards, leading to a novel algorithm, Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying learning from challenging prompts. Remarkably, RS-GRPO is simple to implement, requiring only minor code modifications. On six mathematical reasoning benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k performance while maintaining or enhancing pass@1 accuracy.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T21:53:08.487753",
        "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。首先，从核心判断来看，该论文的本质是改进大语言模型的基础推理能力，而非将其作为工具应用于特定领域。论文提出了一种新的风险敏感强化学习框架(RS-GRPO)，专门解决LLM在探索过程中的困境，这属于增强LLM通用推理能力的研究。 其次，论文包含多个正面指标：核心概念上明确聚焦于大语言模型(LLMs)；能力方向上专注于数学推理(math reasoning)；训练方法上提出了强化学习(RL)的新范式。这些都是提高LLM通用推理能力的关键要素。 第三，论文不符合任何排除标准。虽然论文使用了数学推理作为测试基准，但数学推理被视为通用推理能力的重要组成部分，而非特定应用领域。论文的核心贡献是提出一种通用的强化学习方法，而非解决特定领域问题。 最后，论文的核心贡献是通过改进强化学习算法来增强LLM的推理策略多样性，提高模型在推理任务上的表现，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#387",
        "title": "Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs",
        "link": "/arxiv/2509.24107",
        "arxiv_id": "2509.24107",
        "authors": "Shreyas Singh, Kunal Singh, Pradeep Moturi",
        "summary": "Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T21:53:08.507628",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。根据我的分析： 首先，从核心判断来看，论文本质上是提出了一种工具集成的推理方法(Fathom-DeepResearch)，通过增强小型语言模型(SLMs)的长期信息检索和综合能力来提高其通用推理能力。这属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用到特定领域。 其次，论文包含了所有重要的正面指标： - 核心概念：明确涉及SLMs(基于Qwen3-4B) - 能力方向：专注于\"tool-integrated reasoning\"，并在多样化推理任务(HLE、AIME-25等)上展示性能 - 训练方法：提出了RAPO强化学习方法和多智能体自我博弈训练策略 - 新兴范式：提出了智能体系统、工具使用和深度研究框架 第三，论文不主要聚焦于任何排除标准中的领域。虽然评估中提到了MedQA，但这只是作为系统泛化能力的测试基准，而非论文主要焦点。 特别地，在智能体/工具使用方面，论文提出的是一种通用的智能体协作框架来增强LLM的通用问题解决能力，而非针对特定领域的应用，完全符合保留标准。 综上所述，该论文的核心贡献是提出了一种新的训练范式和智能体系统，通过工具集成推理和强化学习方法来提升语言模型的通用推理能力，与研究目标高度一致。"
    },
    {
        "index": "#413",
        "title": "How LLMs Learn to Reason: A Complex Network Perspective",
        "link": "/arxiv/2509.23629",
        "arxiv_id": "2509.23629",
        "authors": "Sihan Hu, Xiansheng Cai, Yuan Huang, Zhiyuan Yao, Linfeng Zhang, Pan Zhang, Youjin Deng, Kun Chen",
        "summary": "Training large language models with Reinforcement Learning from Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, V-shaped response-length trajectories, and a pronounced vulnerability to catastrophic forgetting. In this work, we propose that these seemingly disparate phenomena can be explained using a single unifying theory: the model's reasoning process maps to the self-organization of a semantic complex network whose topology remains persistently sparse, with the average degree pinned close to two. This topology imposes a fundamental mechanism for forgetting and learning: it first drives the system into a maximally frustrated state where ``skill islands'' form, slow-learning happens, and forgetting is induced; then it enters a sharp growth phase where the new skills are ``bolted on'', driven by phase-transition-like learning at the web's frontier. Equipped with the theory, we propose \\textit{Annealed-RLVR}, a principled algorithm that introduces an SFT-based ``heating'' step at the point of maximal frustration to resolve the competitive bottleneck and enhance the reasoning capability of the model. Experiments on a 1.5B-parameter model demonstrate that the approach outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks. By recasting RLVR from black-box optimization into a predictable process of structural self-organization, our work provides a new physical intuition for engineering the emergent reasoning capabilities of future AI systems.",
        "subjects": "Artificial Intelligence, Disordered Systems and Neural Networks, Statistical Mechanics, Machine Learning, Physics and Society",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T21:53:08.537905",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究大语言模型(LLM)如何学习推理能力，并提出了一种新的训练方法(Annealed-RLVR)来增强模型的推理能力。论文不是将LLM作为工具应用于特定领域，而是直接关注提升LLM本身的基础推理能力，这符合\"保留\"标准。 其次，论文包含多个正面指标： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：直接关注\"reasoning\"(推理)能力，这是论文的核心主题 - 训练方法：研究了\"Reinforcement Learning from Verifiable Rewards (RLVR)\"这一强化学习方法，并基于此提出了改进算法 第三，论文不符合任何排除标准。它没有涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。 最后，论文不涉及特殊或模糊情况。它明确关注LLM的通用推理能力提升，而不是将智能体/工具应用于特定领域，也不是主要研究幻觉/可解释性/安全等问题。 论文的核心贡献是提出了一种复杂网络视角来理解LLM的推理过程，并基于此设计了新的训练算法来增强模型的推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#449",
        "title": "Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction",
        "link": "/arxiv/2509.23186",
        "arxiv_id": "2509.23186",
        "authors": "Qimin Zhong, Hao Liao, Siwei Wang, Mingyang Zhou, Xiaoqun Wu, Rui Mao, Wei Chen",
        "summary": "Large Language Models (LLMs) have achieved impressive performance across diverse tasks but continue to struggle with learning transitive relations, a cornerstone for complex planning. To address this issue, we investigate the Multi-Token Prediction (MTP) paradigm and its impact to transitive relation learning. We theoretically analyze the MTP paradigm using a Transformer architecture composed of a shared output head and a transfer layer. Our analysis reveals that the transfer layer gradually learns the multi-step adjacency information, which in turn enables the backbone model to capture unobserved transitive reachability relations beyond those directly present in the training data, albeit with some inevitable noise in adjacency estimation. Building on this foundation, we propose two strategies to enhance the transfer layer and overall learning quality: Next-Token Injection (NTI) and a Transformer-based transfer layer. Our experiments on both synthetic graphs and the Blocksworld planning benchmark validate our theoretical findings and demonstrate that the improvements significantly enhance the model's path-planning capability. These findings deepen our understanding of how Transformers with MTP learn in complex planning tasks, and provide practical strategies to overcome the transitivity bottleneck, paving the way toward structurally aware and general-purpose planning models.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-27",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T21:53:08.578833",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：这篇论文的本质是改进LLM的基础能力，特别是规划能力。论文研究多令牌预测(MTP)范式如何增强语言模型的传递关系学习能力，这直接关系到复杂规划这一通用推理能力的核心。作者提出了新的训练策略(Next-Token Injection和基于Transformer的传递层)来提升模型的路径规划能力，这属于改进LLM基础能力和提出新训练范式的研究。 第二步正面指标：论文包含了多个正面指标： - 核心概念：明确研究\"Large Language Models (LLMs)\" - 能力方向：专注于\"planning capability\"和\"complex planning\"，这是推理能力的重要组成部分 - 研究内容涉及\"transitive relation learning\"，这也是逻辑推理的关键方面 第三步排除标准：论文不涉及任何需要排除的领域： - 没有涉及多模态与视觉内容 - 没有针对特定应用领域(如医疗、化学等)，Blocksworld规划基准是通用规划能力的测试平台 - 没有专注于模型可靠性方面的应用层面问题 第四步特殊和模糊情况：论文不涉及需要特殊判断的情况，它明确聚焦于提升LLM的通用规划能力，而非特定领域的应用。 论文的核心贡献是通过理论分析和实验验证，揭示了多令牌预测范式如何帮助语言模型学习传递关系，并提出有效策略来增强模型的路径规划能力，这直接服务于提升LLM的通用推理能力，与研究目标高度一致。"
    },
    {
        "index": "#483",
        "title": "Hilbert: Recursively Building Formal Proofs with Informal Reasoning",
        "link": "/arxiv/2509.22819",
        "arxiv_id": "2509.22819",
        "authors": "Sumanth Varambally, Thomas Voice, Yanchao Sun, Zhifeng Chen, Rose Yu, Ke Ye",
        "summary": "Large Language Models (LLMs) demonstrate impressive mathematical reasoning abilities, but their solutions frequently contain errors that cannot be automatically verified. Formal theorem proving systems such as Lean 4 offer automated verification with complete accuracy, motivating recent efforts to build specialized prover LLMs that generate verifiable proofs in formal languages. However, a significant gap remains: current prover LLMs solve substantially fewer problems than general-purpose LLMs operating in natural language. We introduce Hilbert, an agentic framework that bridges this gap by combining the complementary strengths of informal reasoning and formal verification. Our system orchestrates four components: an informal LLM that excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4 tactics, a formal verifier, and a semantic theorem retriever. Given a problem that the prover is unable to solve, Hilbert employs recursive decomposition to split the problem into subgoals that it solves with the prover or reasoner LLM. It leverages verifier feedback to refine incorrect proofs as necessary. Experimental results demonstrate that Hilbert substantially outperforms existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points above the best publicly available method. Hilbert achieves the best known result on PutnamBench. It solves 462/660 problems (70.0%), outperforming proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement over the best publicly available baseline. Thus, Hilbert effectively narrows the gap between informal reasoning and formal proof generation.",
        "subjects": "Artificial Intelligence, Formal Languages and Automata Theory, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T21:53:08.618986",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，论文的本质是提出Hilbert这一智能体框架，通过结合非形式推理和形式验证来增强LLM的数学推理能力。这明显属于改进LLM基础能力的研究，特别是针对数学推理这一通用推理能力的提升，而非将LLM作为工具应用于特定领域。 其次，论文包含多个正面指标：核心概念上明确涉及Large language models (LLMs)；能力方向专注于reasoning，特别是math reasoning；新兴范式方面提出了llm-based agents框架，通过多个组件协作解决问题。 第三，论文不涉及任何排除标准中的领域：没有涉及多模态与视觉内容；虽然专注于数学证明，但数学推理被视为基础通用推理能力而非特定领域应用；虽然涉及形式验证，但这是作为提升推理能力的手段，而非专注于水印、安全等应用层面的可靠性问题。 最后，在特殊和模糊情况处理上，Hilbert是一个通用的智能体框架，其方法论（递归分解、结合非形式推理和形式验证）可以泛化到其他推理任务，而非仅限于特定领域应用。 论文的核心贡献是提出了一种新的智能体协作框架，通过结合非形式推理和形式验证，显著提高了LLM在数学推理方面的能力，这正是研究目标所关注的\"提高大语言模型本身的通用推理能力\"。"
    },
    {
        "index": "#11",
        "title": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning",
        "link": "/arxiv/2509.25004",
        "arxiv_id": "2509.25004",
        "authors": "Shijie Zhang, Guohao Sun, Kevin Zhang, Xiang Guo, Rujun Guo",
        "summary": "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing methods typically treat all training samples uniformly, overlooking the vast differences in problem difficulty relative to the model's current capabilities. This uniform training strategy leads to inefficient exploration of problems the model has already mastered, while concurrently lacking effective guidance on problems that are challenging its abilities the most, limiting both learning efficiency and upper-bound performance. To address this, we propose CLPO (Curriculum-guided Learning for Policy Optimization), a novel algorithm that creates a dynamic pedagogical feedback loop within the policy optimization process. The core of CLPO leverages the model's own rollout performance to conduct real-time difficulty assessment, thereby constructing an Online Curriculum. This curriculum then guides an Adaptive Problem Restructuring mechanism, where the model acts as its own teacher: it diversifies medium-difficulty problems to promote generalization and simplifies challenging problems to make them more attainable. Our approach transforms the static training procedure into a dynamic process that co-evolves with the model's capabilities. Experiments show that CLPO achieves state-of-the-art performance across eight challenging mathematical and general reasoning benchmarks, with an average pass@1 improvement of 6.96% over other methods, demonstrating its potential for more efficiently training more capable reasoning models.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.180595",
        "filter_reason": "这篇论文的核心贡献是提出CLPO（课程学习与策略优化相结合）的新算法，旨在增强大语言模型的通用推理能力。论文通过创建动态教学反馈循环，利用模型自身性能进行实时难度评估，构建在线课程，从而提高LLM在数学和通用推理任务上的表现。这完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。论文关注的是基础能力改进和新训练范式，而非将LLM作为工具应用于特定领域。同时，论文包含了多个正面指标，如大语言模型、推理能力、强化学习方法等，且不涉及任何排除标准中的领域。因此，该论文非常符合研究范围。"
    },
    {
        "index": "#2",
        "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following",
        "link": "/arxiv/2509.25148",
        "arxiv_id": "2509.25148",
        "authors": "FaQiang Qian, WeiKun Zhang, Ziliang Wang, Kang An, Xuhui Zheng, Liangjian Wen, Mengya Gao, Yong Dai, Yichao Wu",
        "summary": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment. We argue that post-training alignment is fundamentally a unified Preference Learning problem, involving two modalities: demonstrated preferences (e.g., Supervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement Learning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due to a critical distributional mismatch: SFT uses static expert data, but as the policy evolves, its generation distribution drifts, making SFT knowledge brittle. Subsequent RL then explores without direct access to the rich, ground-truth knowledge in expert demonstrations, leading to inefficient, ungrounded updates. This separation prevents mutual regularization between data sources. To address this, we reframe alignment as a constrained optimization problem and propose Unified Adversarial Preference Learning (UniAPL),a novel framework that dynamically aligns the policy's distribution with the expert's. UniAPL implements a single-stage unified training objective, jointly learning from mixed batches of SFT and preference data. In every gradient step, dense expert demonstrations directly ground and regularize online exploration, inherently resolving distributional mismatch and maximizing data synergy.We evaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507 as the teacher. Our models match or exceed strong GRPO baselines: +5.77% on Qwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the teacher. Analyses of response length and log-probability distributions confirm that UniAPL outputs closely mimic expert demonstrations, achieving both stronger performance and better behavioral alignment.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.170215",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式(UniAPL)来改进LLM的基础能力。论文针对的是LLM后训练对齐问题，提出统一对抗偏好学习框架，将监督微调(SFT)和强化学习(RL)两种模态统一在一个框架中，解决分布不匹配问题。这明显属于改进LLM基础能力和提出新训练范式的研究，而不是将LLM作为工具应用到特定领域。 其次，从正面指标看，论文包含多个相关主题： - 核心概念：明确关注LLMs，使用Qwen3系列模型进行实验 - 训练方法：涉及强化学习(RL)作为偏好学习的一种模态，并将其与SFT统一 - 能力方向：虽然不直接针对推理能力，但关注指令遵循能力(instruction-following)，这是一种重要的通用能力，对模型的整体表现和推理能力有基础性影响 第三，从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。虽然提到了\"beneficial and safe\"，但这只是作为背景，论文核心是提出统一训练框架。 最后，论文提出的UniAPL框架通过动态对齐策略分布与专家分布，解决了传统SFT后跟RL的顺序流程中的分布不匹配问题，这种方法论创新有望提升LLM的通用能力，包括其推理和问题解决能力。论文的实验结果也表明，UniAPL能够显著提升模型性能，甚至使小模型达到大模型的效果。 因此，尽管论文没有直接针对推理能力进行研究，但它提出的训练范式创新属于提升LLM基础能力的研究范畴，符合筛选标准。"
    },
    {
        "index": "#24",
        "title": "From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning",
        "link": "/arxiv/2509.24765",
        "arxiv_id": "2509.24765",
        "authors": "Yunyao Zhang, Xinglang Zhang, Junxi Sheng, Wenbing Li, Junqing Yu, Wei Yang, Zikai Song",
        "summary": "Logical reasoning is a fundamental capability of large language models (LLMs). However, existing studies largely overlook the interplay between logical complexity and semantic complexity, resulting in methods that struggle to address challenging scenarios involving abstract propositions, ambiguous contexts, and conflicting stances, which are central to human reasoning. For this gap, we propose LogicAgent, a semiotic-square-guided framework designed to jointly address logical complexity and semantic complexity. LogicAgent explicitly performs multi-perspective deduction in first-order logic (FOL), while mitigating vacuous reasoning through existential import checks that incorporate a three-valued decision scheme (True, False, Uncertain) to handle boundary cases more faithfully. Furthermore, to overcome the semantic simplicity and low logical complexity of existing datasets, we introduce RepublicQA, a benchmark that reaches college-level difficulty (FKGL = 11.94) and exhibits substantially greater lexical and structural diversity than prior benchmarks. RepublicQA is grounded in philosophical concepts, featuring abstract propositions and systematically organized contrary and contradictory relations, making it the most semantically rich resource for evaluating logical reasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art performance on RepublicQA, with a 6.25% average gain over strong baselines, and generalizes effectively to mainstream logical reasoning benchmarks including ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05% average gain. These results highlight the strong effectiveness of our semiotic-grounded multi-perspective reasoning in boosting LLMs' logical performance.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.193071",
        "filter_reason": "根据筛选标准，这篇论文完全符合研究范围。首先，从核心判断来看，该论文的本质是改进大语言模型的基础能力——具体来说是逻辑推理能力。论文提出了LogicAgent，这是一种基于符号学的多视角推理框架，旨在增强LLM处理抽象命题、模糊语境和冲突立场的能力，这属于提升LLM通用推理能力的核心研究。 其次，论文包含多个正面指标：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于逻辑推理(logical reasoning)，这是通用推理能力的重要组成部分；新兴范式上提出了基于LLM的智能体(LLM-based agents)框架。 第三，论文不涉及任何排除标准中的领域：没有研究多模态与视觉问题，没有将LLM应用于特定领域（虽然使用了哲学概念构建数据集，但这只是为了评估逻辑推理能力，而非应用焦点），也没有主要关注模型可靠性的应用层面问题。 在特殊和模糊情况处理上，论文提出的LogicAgent是一种通用的智能体框架，旨在提升LLM的通用推理能力，而非应用于特定领域，因此应该保留。 综上所述，这篇论文的核心贡献是提出了一种新的方法论来增强LLM的逻辑推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#22",
        "title": "Query Circuits: Explaining How Language Models Answer User Prompts",
        "link": "/arxiv/2509.24808",
        "arxiv_id": "2509.24808",
        "authors": "Tung-Yu Wu, Fazl Barez",
        "summary": "Explaining why a language model produces a particular output requires local, input-level explanations. Existing methods uncover global capability circuits (e.g., indirect object identification), but not why the model answers a specific input query in a particular way. We introduce query circuits, which directly trace the information flow inside a model that maps a specific input to the output. Unlike surrogate-based approaches (e.g., sparse autoencoders), query circuits are identified within the model itself, resulting in more faithful and computationally accessible explanations. To make query circuits practical, we address two challenges. First, we introduce Normalized Deviation Faithfulness (NDF), a robust metric to evaluate how well a discovered circuit recovers the model's decision for a specific input, and is broadly applicable to circuit discovery beyond our setting. Second, we develop sampling-based methods to efficiently identify circuits that are sparse yet faithfully describe the model's behavior. Across benchmarks (IOI, arithmetic, MMLU, and ARC), we find that there exist extremely sparse query circuits within the model that can recover much of its performance on single queries. For example, a circuit covering only 1.3% of model connections can recover about 60% of performance on an MMLU questions. Overall, query circuits provide a step towards faithful, scalable explanations of how language models process individual inputs.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.191953",
        "filter_reason": "这篇论文的核心贡献是提出\"查询电路\"(query circuits)方法，用于解释语言模型如何将特定输入映射到输出，追踪模型内部的信息流。论文不仅提出了这一新方法，还开发了评估指标(NDF)和高效识别电路的采样方法。虽然论文不直接提出新的训练范式或直接增强模型推理能力的方法，但它通过提供模型决策的局部、输入级别解释，有助于深入理解LLM的内部推理机制。这种对模型推理过程的理解是提升模型通用推理能力的重要基础，符合\"增强模型内在可解释性，从而提升模型的通用可靠性和推理质量\"的标准。论文不涉及任何排除标准中的领域，如多模态、特定应用领域或应用层面的模型可靠性问题。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#38",
        "title": "Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning with LLMs",
        "link": "/arxiv/2509.24377",
        "arxiv_id": "2509.24377",
        "authors": "Shihao Qi, Jie Ma, Ziang Yin, Lingling Zhang, Jian Zhang, Jun Liu, Feng Tian, Tongliang Liu",
        "summary": "Existing methods usually leverage a fixed strategy, such as natural language reasoning, code-augmented reasoning, tool-integrated reasoning, or ensemble-based reasoning, to guide Large Language Models (LLMs) to perform mathematical reasoning. Our analysis reveals that the single strategy cannot adapt to problem-specific requirements and thus overlooks the trade-off between effectiveness and efficiency. To address these issues, we propose Planning and Routing through Instance-Specific Modeling (PRISM), a novel framework that decouples mathematical reasoning into two stages: strategy planning and targeted execution. Specifically, we first curate a multi-strategy preference dataset, which we call MathStrat, capturing correctness, process quality, and computational efficiency for each problem--strategy pair. Then, we train a lightweight Strategy Adapter based on the dataset to obtain confidence distributions over the mentioned four reasoning strategies. At inference time, an adaptive routing policy dynamically tailors the reasoning approach based on predictor confidence. It directs the model to use single-strategy execution for high-confidence predictions, dual-strategy verification for competitive scenarios, or comprehensive multi-strategy exploration for uncertain cases. Extensive experiments across five mathematical reasoning benchmarks demonstrate that PRISM consistently outperforms individual strategies and ensemble baselines, achieving improvements ranging from 0.9% to 7.6% across different base models. The adaptive routing approach shows particularly strong benefits for mathematical reasoning tasks across diverse model architectures. Our code is released at https://github.com/reml-group/PRISM.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.210916",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础推理能力，特别是数学推理能力。论文提出了PRISM框架，将数学推理分为策略规划和定向执行两个阶段，这是一种新的训练/推理范式，旨在增强LLM的通用推理能力，而非将LLM应用于特定领域。 其次，论文包含多个正面指标：核心概念方面明确关注Large Language Models (LLMs)；能力方向专注于mathematical reasoning和planning；虽然未明确提及强化学习或自我进化，但提出了训练轻量级策略适配器的方法，属于训练方法创新。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。数学推理被视为LLM的基础能力之一，而非特定领域应用。 在特殊情况下，虽然论文提到\"tool-integrated reasoning\"作为现有方法之一，但其核心贡献是提出一种通用的策略路由框架，而非将工具应用在特定领域。PRISM框架是一种通用方法论，适用于各种数学推理问题，旨在增强LLM的通用问题解决能力。 综上所述，这篇论文的核心贡献是提出了一种新的框架来增强LLM的数学推理能力，通过动态选择最适合特定问题的推理策略来提高模型性能，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#35",
        "title": "ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling",
        "link": "/arxiv/2509.24460",
        "arxiv_id": "2509.24460",
        "authors": "Haotian Zhang, Liu Liu, Baosheng Yu, Jiayan Qiu, Likang Xiao, Yanwei Ren, Quan Chen, Xianglong Liu",
        "summary": "Process reward models (PRMs) have demonstrated significant efficacy in enhancing the mathematical reasoning capabilities of large language models (LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit substantial gains in mathematical domains, the scarcity of domain-specific training data and knowledge-based learning patterns limits their generalization ability when faced with other domains. To address this limitation, we shift the learning objective from verifying domain-specific knowledge to modeling domain-agnostic logical flow. Centering on contextual coherence between chain-of-thought (CoT) steps, our approach is realized through a novel data annotation and training framework, which enhances the model's generalization capabilities across diverse domains. For instance, our resulting model, ContextPRM, achieves a notable 6.5% average accuracy improvement over the majority voting baseline via weighted majority voting across nine non-mathematical domains in MMLU-Pro, including law, history, and philosophy, significantly surpassing the 2.2% improvement from VersaPRM and 0.5% gains from other mathematics-focused PRMs, demonstrating consistent performance across both mathematical and non-mathematical domains.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.203969",
        "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，论文本质上是关于改进LLM的基础推理能力的。它提出了一种名为ContextPRM的新方法，将过程奖励模型(PRM)的学习目标从验证领域特定知识转变为建模领域无关的逻辑流，通过关注思维链(CoT)步骤之间的上下文连贯性来增强模型的通用推理能力。这明显属于改进LLM基础能力和提出新训练范式的研究。 其次，从正面指标看，论文包含了多个相关主题：核心概念上明确讨论大语言模型(LLMs)；能力方向上聚焦于推理能力(reasoning)，特别是数学推理(mathematical reasoning)和逻辑推理；训练方法上涉及过程奖励模型(PRM)，这与强化学习相关；新兴范式上采用了思维链(CoT)方法。 第三，论文不主要聚焦于排除标准中的任何领域。虽然论文在法律、历史、哲学等非数学领域进行了测试，但这些是用来验证模型泛化能力的测试领域，而非论文的主要研究焦点。论文的核心是提出一种通用的推理方法，而非针对特定应用领域。 最后，论文不涉及需要特殊判断的模糊情况。它没有将LLM作为工具应用于特定领域，而是直接提升LLM本身的通用推理能力，使其能够在多个领域表现出色。 综上所述，这篇论文完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。"
    },
    {
        "index": "#39",
        "title": "From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision",
        "link": "/arxiv/2509.24351",
        "arxiv_id": "2509.24351",
        "authors": "Jie Ma, Shihao Qi, Rui Xing, Ziang Yin, Bifan Wei, Jun Liu, Tongliang Liu",
        "summary": "The quality of process data plays a key role in training a Process Reward Model (PRM), which can enhance the complex mathematical reasoning capability of large language models. Existing methods estimate the quality of reasoning steps based on a fixed-budget sampling strategy and navigate a vast search space to perform path expansion during the automated data generation process, resulting in their inefficiency and inflexibility. To address these issues, we propose Adaptive Monte Carlo Search (AMCS), a framework that transforms data generation from fixed, static to adaptive, dynamic search at the level of node value estimation and path expansion. On one hand, AMCS adaptively refines estimation by allocating more samples to uncertain reasoning steps while using fewer samples for those that are easier to estimate. On the other hand, it enhances the path expansion through a Monte Carlo algorithm with a temporally adaptive policy that begins with broad exploration and gradually shifts toward exploiting the most promising directions. With AMCS, we construct a large-scale dataset MathSearch-200K of about 200K process supervision examples for training PRMs. To verify the effectiveness of our method, we conduct extensive experiments on four mathematical reasoning benchmarks. Experimental results show that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500 with GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised by Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision. Moreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on out-of-distribution problems, demonstrating strong generalization capability. Our code is available at https://github.com/reml-group/AMCS.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.211437",
        "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是提出一种自适应蒙特卡洛搜索(AMCS)框架，用于改进过程奖励模型(PRM)的训练数据质量，从而增强大语言模型的复杂数学推理能力。这明确属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。论文的核心贡献是将数据生成从固定的静态搜索转变为自适应的动态搜索，在节点值估计和路径扩展层面进行了创新，这是一种提升LLM通用推理能力的新方法。 从正面指标看，论文包含了多个相关主题：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于数学推理(math reasoning)；训练方法上使用了过程奖励模型(PRM)，这与强化学习框架相关。 论文不符合任何排除标准：不涉及多模态与视觉内容，不专注于特定应用领域(数学推理被视为LLM的通用能力而非特定领域应用)，也不关注模型可靠性的应用层面问题。 论文也不涉及需要特殊处理的模糊情况，如智能体/工具使用或幻觉/可解释性/安全等问题。 综上所述，这篇论文直接致力于提高大语言模型的通用推理能力(特别是数学推理)，提出了新的训练范式和方法，完全符合我的研究范围。"
    },
    {
        "index": "#55",
        "title": "Humanline: Online Alignment as Perceptual Loss",
        "link": "/arxiv/2509.24207",
        "arxiv_id": "2509.24207",
        "authors": "Sijia Liu, Niklas Muennighoff, Kawin Ethayarajh",
        "summary": "Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral economics, we propose a human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping -- originally introduced to just stabilize training -- recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.231022",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的训练方法，特别是对齐(alignment)技术。论文提出了一个新的理论框架，解释为什么在线对齐(如GRPO)比离线对齐(如DPO)表现更好，并基于此创建了\"humanline\"变体。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，而不是将LLM作为工具应用到特定领域。 其次，从正面指标来看，论文虽然未直接提及\"Large language models\"，但讨论的DPO、GRPO、PPO等技术都是LLM训练中的核心对齐方法。在训练方法方面，论文明确讨论了强化学习方法(RLHF, RL)，特别是PPO/GRPO等在线对齐方法，这是提高LLM通用能力的重要技术。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是提出了一种新的理论框架和训练方法，通过模拟人类感知来改进模型的对齐过程，从而提高模型在可验证和不可验证任务上的性能。虽然论文没有直接讨论推理、规划等具体能力，但改进对齐方法本质上是提高模型整体性能和通用性的重要途径，这与提高LLM通用推理能力的目标是一致的。论文中提到的\"可验证和不可验证任务\"很可能包括需要推理能力的任务，因此这篇论文对研究大语言模型的通用推理能力具有重要参考价值。"
    },
    {
        "index": "#56",
        "title": "Robust Preference Optimization: Aligning Language Models with Noisy Preference Feedback",
        "link": "/arxiv/2509.24159",
        "arxiv_id": "2509.24159",
        "authors": "Xiaoyang Cao, Zelai Xu, Mo Guang, Kaiwen Long, Michiel A. Bakker, Yu Wang, Chao Yu",
        "summary": "Standard human preference-based alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), are a cornerstone technology for aligning Large Language Models (LLMs) with human values. However, these methods are all underpinned by a critical, yet flawed assumption: human preferences are homogeneous (representing a single, unified preference) and the collected data is noiseless (free from error). In reality, neither is true since human preference is pluralistic and annotators can make mistakes. This creates a discrepancy between the recorded data and the ground-truth preferences, which can misguide the model and degrade its performance. To address this challenge, we introduce Robust Preference Optimization (RPO). RPO employs an Expectation-Maximization (EM) algorithm to infer the posterior probability of each label's correctness, which is used to adaptively re-weigh each data point in the training loss to mitigate noise. We further generalize this approach by establishing a theoretical link between arbitrary preference losses and their corresponding probabilistic models. This generalization enables the systematic transformation of existing alignment algorithms into their robust counterparts, elevating RPO from a specific algorithm to a meta-framework for robust preference alignment. Theoretically, we prove that under the condition of a perfectly calibrated model, RPO is guaranteed to converge to the true noise level of the dataset. Our experiments demonstrate RPO's effectiveness as a meta-framework, consistently enhancing four state-of-the-art alignment algorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3 models, the RPO-enhanced methods achieve substantial win rate gains on AlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% and 5.4%, respectively.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.231552",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进大语言模型的基础能力，特别是提出了一种新的训练范式\"鲁棒偏好优化\"(RPO)。论文关注的是如何更好地将模型与人类价值观对齐，这是提升模型通用能力的重要方面。对齐是LLM推理和行为的基础，更好的对齐可以提升模型在各种任务上的表现，包括推理任务。 其次，从正面指标分析： - 核心概念：论文明确关注大语言模型(LLMs)的对齐问题，提到了Mistral和Llama 3模型。 - 训练方法：论文明确讨论了RLHF（基于人类反馈的强化学习），并提出了改进方法，这符合强化学习优化的指标。 - 能力方向：虽然论文没有直接讨论推理、规划或问题解决，但对齐方法的改进可以间接提升模型在这些方面的表现。 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于任何特定应用领域 - 不是从应用层面讨论水印、安全或安全问题，而是从基础方法层面改进对齐算法 最后，论文提出的是一种通用的训练框架，可以应用于多种对齐算法（DPO, IPO, SimPO,和CPO），提升它们的鲁棒性和效果。这种基础性的方法改进有助于提升大语言模型的通用能力，符合研究目标。 因此，尽管论文没有直接讨论推理能力，但其提出的对齐方法改进可以间接提升模型的推理表现，且属于基础能力改进的范畴，故应保留。"
    },
    {
        "index": "#45",
        "title": "G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge",
        "link": "/arxiv/2509.24276",
        "arxiv_id": "2509.24276",
        "authors": "Linhao Luo, Zicheng Zhao, Junnan Liu, Zhangchi Qiu, Junnan Dong, Serge Panev, Chen Gong, Thuy-Trang Vu, Gholamreza Haffari, Dinh Phung, Alan Wee-Chung Liew, Shirui Pan",
        "summary": "Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. Graphs offer a natural way to model relationships within knowledge, but LLMs are inherently unstructured and cannot effectively reason over graph-structured data. Recent graph-enhanced RAG (GraphRAG) attempts to bridge this gap by constructing tailored graphs and enabling LLMs to reason on them. However, these methods often depend on ad-hoc graph designs, heuristic search, or costly agent pipelines, which hinder scalability and generalization. To address these challenges, we present G-reasoner, a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge. Central to our approach is QuadGraph, a standardized four-layer abstraction that unifies heterogeneous knowledge sources into a common graph representation. Building on this, we introduce a 34M-parameter graph foundation model (GFM) that jointly captures graph topology and textual semantics, and is integrated with LLMs to enhance reasoning in downstream applications. To ensure scalability and efficiency, mixed-precision training and distributed message-passing are implemented to scale GFM with more GPUs. Extensive experiments on six benchmarks show that G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.214670",
        "filter_reason": "这篇论文的核心贡献是提出G-reasoner框架，通过集成图和语言基础模型来增强LLMs对图结构知识的推理能力。从本质上看，论文致力于解决LLMs在处理结构化知识方面的局限性，提出了一种新的方法（QuadGraph抽象和图基础模型）来增强LLMs的通用推理能力，而不是将LLM作为工具应用到特定领域。论文明确关注\"reasoning\"这一核心能力方向，并讨论了如何通过结合图结构知识来提升LLMs的推理性能。虽然涉及图结构知识，但这是一种通用的知识表示方法，不是针对特定应用领域的研究。论文提出的框架具有通用性，旨在提升LLMs的基础推理能力，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。"
    },
    {
        "index": "#68",
        "title": "Rethinking Reward Miscalibration of GRPO in Agentic RL",
        "link": "/arxiv/2509.23870",
        "arxiv_id": "2509.23870",
        "authors": "Jingyu Liu, Xiaopeng Wu, Jingquan Peng, Kehan Chen, Chuan Yu, Lizhong Ding, Yong Liu",
        "summary": "Building autonomous agents capable of solving long-horizon, real-world tasks has garnered significant research interest. But outcome based rewards may cause reward miscalibration which means it might mistakenly allocate positive reward to flawed middle steps which is regarded as the key reason making the bad actions being reinforced during training. However we reveal that outcome based reward ensures expected negative advantage for those flawed middle steps, which means the flawed actions should be punished during training. Even accounting for the ``squeezing effect\", the probability mass of good actions should increase and the actor should gradually get rid of harmful actions. This shows that flawed actions should be punished during training. We further identify gradient coupling between similar samples as a key issue in agentic RL, the input prompt is extremely similar and the output action space is limited, therefore during training, gradients from well-performing samples can inadvertently strengthen suboptimal or incorrect actions due to similar input observation and output actions. We show that with gradient coupling, some flawed actions might be enhanced. To address this, we propose training the actor to classify good or bad actions to separate the embedding of good/bad actions and alleviate the gradient interference, extensive experiments shows its effectiveness.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.243134",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究智能体强化学习(Agentic RL)中的奖励校准问题，提出了一种改进训练方法来增强自主智能体的决策能力。论文核心不是将LLM作为工具应用到特定领域，而是研究如何改进LLM作为智能体的基础训练机制，特别是奖励机制和梯度耦合问题，这直接关系到提升LLM的通用推理能力，因此应该保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：提到\"autonomous agents\"，虽然未直接使用\"LLMs\"术语，但\"Agentic RL\"通常指基于LLM的智能体强化学习 - 能力方向：关注解决\"long-horizon, real-world tasks\"，这隐含了推理和规划能力 - 训练方法：明确涉及\"reinforcement learning (RL)\"，这是提升LLM能力的关键方法 - 新兴范式：讨论\"agentic RL\"，与\"llm-based agents\"直接相关 第三步：排除标准 论文不符合任何排除标准： - 未涉及多模态与视觉内容 - 未聚焦于医疗、化学、生物等特定应用领域 - 未讨论水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况处理 论文讨论的是通用的智能体强化学习方法，而非将智能体应用在特定领域。它研究的是如何通过改进奖励机制和解决梯度耦合问题来提升智能体的决策质量，这属于通用推理能力的提升范畴。 核心贡献：论文揭示了智能体强化学习中奖励校准和梯度耦合的关键问题，并提出了一种训练actor分类好/坏动作的方法，以分离好/坏动作的嵌入并减轻梯度干扰，从而提升智能体的决策能力和推理质量。这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#78",
        "title": "EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance",
        "link": "/arxiv/2509.23730",
        "arxiv_id": "2509.23730",
        "authors": "Siyao Song, Cong Ma, Zhihao Cheng, Shiye Lei, Minghao Li, Ying Zeng, Huaixiao Tou, Kai Jia",
        "summary": "Large language models (LLMs) have recently advanced in reasoning when optimized with reinforcement learning (RL) under verifiable rewards. Existing methods primarily rely on outcome-based supervision to strengthen internal LLM reasoning, often leading to inefficient exploration and sparse rewards. To mitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a novel RL framework that enhances exploration by incorporating multi-turn interactions with external experts during training. Unlike prior methods, where policies reason in isolation, EAPO incentivizes the policy to adaptively determine when and how to consult experts, yielding richer reward signals and more reliable reasoning trajectories. External assistance ultimately internalizes expert knowledge into the policy model, amplifying the model's inherent reasoning capabilities. During evaluation, the policy model has been well-optimized to solve questions independently, producing improved reasoning paths and more accurate solutions. Experiments on mathematical reasoning benchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO consistently outperforms expert-assisted workflow, expert-distilled models, and RL baselines, with an average gain of 5 points over self-exploratory models.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.253817",
        "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是提出一种新的强化学习框架EAPO（Expert-Assisted Policy Optimization），旨在通过外部专家辅助增强LLM的推理能力，这明显属于改进LLM基础能力和通用推理能力的范畴，而非将LLM作为工具应用于特定领域。 其次，论文包含多个关键正面指标：明确涉及大型语言模型(LLMs)这一核心概念；专注于reasoning能力（特别是数学推理）；采用强化学习(RL)作为训练方法；并包含多轮交互与专家咨询的智能体协作元素。 第三，论文不涉及任何排除标准领域：没有多模态与视觉内容，不专注于特定应用领域（数学推理被视为通用推理能力的一部分），也不关注模型基础设施或应用层面的可靠性。 最后，在特殊情况下，论文中提到的与外部专家的多轮交互和咨询机制，是一种通用的智能体协作框架，目的是增强LLM的通用推理能力，而非应用于特定领域。 论文的核心贡献是提出了一种通过外部专家辅助来增强LLM推理能力的新训练范式，这种方法可以提升模型固有的推理能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#85",
        "title": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs",
        "link": "/arxiv/2509.23619",
        "arxiv_id": "2509.23619",
        "authors": "Xiangyu Wen, Junhua Huang, Zeju Li, Min Li, Jianyuan Zhong, Zhijian Xu, Mingxuan Yuan, Yongxiang Huang, Qiang Xu",
        "summary": "The prevailing approach to distilling reasoning from Large Language Models (LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It teaches Small Language Models (SLMs) to mimic surface-level patterns rather than the underlying algorithmic structure of thought, resulting in a critical lack of logical robustness. We argue that instead of cloning text, distillation should transfer this algorithmic structure directly. We introduce Reasoning Scaffolding}, a framework that reframes reasoning as a structured generation process. Our method first abstracts the teacher's thought process into a sequence of discrete, interpretable semantic signals (e.g., Contrast, Addition) that act as a scaffold. The student model is then trained via a multi-task objective to both (1)predict the next semantic signal, anticipating the reasoning flow, and (2)generate the corresponding step, conditioned on that signal. This multi-task scheme acts as a powerful regularizer, compelling the student to internalize the computational patterns of coherent reasoning. On a suite of challenging reasoning benchmarks, our method significantly outperforms state-of-the-art distillation in both accuracy and logical consistency, providing a path towards creating smaller models that are genuine reasoners, not just fluent mimics.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.262876",
        "filter_reason": "这篇论文完全符合我的研究目标，核心原因如下： 首先，从本质上看，论文的核心贡献是提出一种名为\"Reasoning Scaffolding\"（推理支架）的新框架，用于提升语言模型的通用推理能力。论文明确指出当前主流方法（从文本理由中进行行为克隆）的局限性，即只教会小模型模仿表面模式而非思维的底层算法结构。作者提出的解决方案是将推理重构为结构化生成过程，通过多任务训练使模型内化连贯推理的计算模式。这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的保留标准。 其次，论文包含了多个正面指标：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于推理(reasoning)，特别是逻辑推理；虽然论文没有涉及强化学习等特定训练方法，但它提出了一种创新的多任务训练范式来提升推理能力。 第三，论文不符合任何排除标准：它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 最后，论文提出的\"推理支架\"方法通过将思维过程抽象为可解释的语义信号，不仅提升了模型的推理能力，还间接增强了模型的可解释性和逻辑一致性，这与提升模型内在可靠性的目标是一致的。 总之，这篇论文直接针对大语言模型的通用推理能力提升，提出了一种新的训练范式，完全符合我的研究范围。"
    },
    {
        "index": "#91",
        "title": "Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks",
        "link": "/arxiv/2509.23537",
        "arxiv_id": "2509.23537",
        "authors": "Aaron Xuxiang Tian, Ruofan Zhang, Jiayao Tang, Young Min Cho, Xueqian Li, Qiang Yi, Ji Wang, Zhunping Zhang, Danrui Qi, Sharath Chandra Guntuku, Lyle Ungar, Tianyu Shi, Chi Wang",
        "summary": "We study multi-turn multi-agent orchestration, where multiple large language model (LLM) agents interact over multiple turns by iteratively proposing answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we conduct two experiments: (i) benchmarking orchestration against single-LLM baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who authored answers and whether they can observe ongoing votes. Orchestration matches or exceeds the strongest single model and consistently outperforms the others. Analysis of best-achievable orchestration performance shows potential for further gains. The ablations show that revealing authorship increases self-voting and ties, and that showing ongoing votes amplifies herding, which speeds convergence but can sometimes yield premature consensus.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.266255",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究多轮多智能体编排框架，通过多个LLM智能体之间的交互和投票机制来提高问题解决能力，这属于改进LLM基础能力和通用推理能力的研究，而非将LLM作为工具应用到特定领域。其次，论文满足多个正面指标：核心概念明确涉及大型语言模型(LLMs)；能力方向上，虽然摘要未直接提及\"reasoning\"，但多智能体通过交互和投票达成共识的过程本质上是一种推理和问题解决，且在MuSR（多步推理）等基准上进行测试；新兴范式方面，论文明确研究\"multi-turn multi-agent orchestration\"，属于多智能体系统的新兴范式。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面研究。最后，论文提出的是通用的多智能体协作框架来增强LLM的通用问题解决能力，而非针对特定领域的应用。因此，这篇论文的核心贡献——通过多智能体协作框架提升LLM的通用推理能力——完全符合研究目标。"
    },
    {
        "index": "#104",
        "title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning",
        "link": "/arxiv/2509.23285",
        "arxiv_id": "2509.23285",
        "authors": "Yifei Chen, Guanting Dong, Zhicheng Dou",
        "summary": "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal reasoning ability by integrating external tools. However, models employing TIR often display suboptimal behaviors, such as insufficient or excessive tool usage and overthinking after tool calls. The challenge of incentivizing LLMs to perform TIR efficiently and accurately, while stabilizing the reasoning process, remains an open question. In this paper, we start by exploring the impact of tool calls on model reasoning from the perspective of information entropy. Our findings indicate that tool call results lead to a distinct change in the information entropy of subsequent reasoning, with the overall entropy of the reasoning chain varying based on the number of tool calls. Building on these insights, we propose Tool-Light, a framework designed to encourage LLMs to perform TIR efficiently and accurately. Our framework includes dataset construction and multi-stage fine-tuning. For dataset construction, we employ continuous self-evolved sampling using the fine-tuned model, integrating both vanilla sampling and entropy-guided sampling. Besides, we establish strict criteria for selecting positive-negative pairs during sampling. The training process involves a two-stage approach, comprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference Optimization (DPO). Experimental results on 10 datasets demonstrate the effectiveness of Tool-Light, significantly improving the model's efficiency in executing TIR tasks.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.286358",
        "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，论文的本质是关于改进LLM的通用推理能力，具体聚焦于\"Tool-Integrated Reasoning (TIR)\"，即通过集成外部工具来增强大语言模型的内部推理能力。这明显属于提升LLM基础能力的范畴，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标的主题：核心概念上明确研究LLMs；能力方向上专注于reasoning（推理）；训练方法上提出了\"Self-Evolved Preference Learning\"和\"Self-Evolved Direct Preference Optimization (DPO)\"，属于self-evolve（自我进化）的训练方法；新兴范式上关注tool use（工具使用），这些都是高度相关的主题。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。 在特殊和模糊情况处理上，论文提出的是一种通用的工具使用框架（Tool-Light）来增强LLM的通用推理能力，而不是将工具应用在特定领域，这符合保留标准。论文通过提高推理效率和质量，间接有助于提升模型的通用推理能力。 论文的核心贡献是提出了Tool-Light框架，通过数据集构建和多阶段微调（包括SFT和Self-Evolved DPO）来鼓励LLM高效准确地执行工具集成推理，这直接服务于提升LLM通用推理能力的研究目标。因此，这篇论文完全符合我的研究范围。"
    },
    {
        "index": "#112",
        "title": "Limit Analysis for Symbolic Multi-step Reasoning Tasks with Information Propagation Rules Based on Transformers",
        "link": "/arxiv/2509.23178",
        "arxiv_id": "2509.23178",
        "authors": "Tian Qin, Yuhan Chen, Zhiwei Wang, Zhi-Qin John Xu",
        "summary": "Transformers are able to perform reasoning tasks, however the intrinsic mechanism remains widely open. In this paper we propose a set of information propagation rules based on Transformers and utilize symbolic reasoning tasks to theoretically analyze the limit reasoning steps. We show that the limit number of reasoning steps is between $O(3^{L-1})$ and $O(2^{L-1})$ for a model with $L$ attention layers in a single-pass.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.295058",
        "filter_reason": "这篇论文的核心贡献是提出了一套基于Transformer的信息传播规则，并利用符号推理任务来理论上分析推理步骤的极限。论文研究了具有L个注意力层的模型在一次前向传播中能够执行的推理步骤数量限制，这直接关注大语言模型的基础推理能力，特别是多步推理能力的理论分析。根据筛选标准的第一步，这篇论文是关于改进LLM基础能力的研究，符合保留条件。论文明确涉及\"reasoning tasks\"和\"symbolic multi-step reasoning tasks\"，这与我们的研究目标高度相关。同时，论文不涉及任何需要排除的特定应用领域或多模态研究。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#120",
        "title": "Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models",
        "link": "/arxiv/2509.23108",
        "arxiv_id": "2509.23108",
        "authors": "Morgan McCarty, Jorge Morales",
        "summary": "This study offers a novel approach for benchmarking complex cognitive behavior in artificial systems. Almost universally, Large Language Models (LLMs) perform best on tasks which may be included in their training data and can be accomplished solely using natural language, limiting our understanding of their emergent sophisticated cognitive capacities. In this work, we created dozens of novel items of a classic mental imagery task from cognitive psychology. A task which, traditionally, cognitive psychologists have argued is solvable exclusively via visual mental imagery (i.e., language alone would be insufficient). LLMs are perfect for testing this hypothesis. First, we tested several state-of-the-art LLMs by giving text-only models written instructions and asking them to report the resulting object after performing the transformations in the aforementioned task. Then, we created a baseline by testing 100 human subjects in exactly the same task. We found that the best LLMs performed significantly above average human performance. Finally, we tested reasoning models set to different levels of reasoning and found the strongest performance when models allocate greater amounts of reasoning tokens. These results provide evidence that the best LLMs may have the capability to complete imagery-dependent tasks despite the non-pictorial nature of their architectures. Our study not only demonstrates an emergent cognitive capacity in LLMs while performing a novel task, but it also provides the field with a new task that leaves lots of room for improvement in otherwise already highly capable models. Finally, our findings reignite the debate over the formats of representation of visual imagery in humans, suggesting that propositional reasoning (or at least non-imagistic reasoning) may be sufficient to complete tasks that were long-thought to be imagery-dependent.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T21:53:08.304495",
        "filter_reason": "这篇论文的核心贡献是研究大语言模型(LLMs)如何通过命题推理(propositional reasoning)来完成传统上被认为需要视觉心理意象的任务。论文创建了认知心理学中的经典心理意象任务的新项目，测试了几个最先进的LLMs，发现它们的表现显著高于人类平均水平，特别是当模型分配更多的推理token时表现更强。这表明LLMs具有一种新兴的认知能力，能够通过纯文本推理完成依赖意象的任务。论文关注的是LLMs的基础推理能力和认知机制，而不是将LLM作为工具应用到特定领域，也不涉及多模态、特定应用领域或模型可靠性等排除标准。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，特别是关于逻辑推理和问题解决能力的研究。"
    }
]