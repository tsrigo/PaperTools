[
    {
        "index": "#5",
        "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
        "link": "/arxiv/2512.04987",
        "arxiv_id": "2512.04987",
        "authors": "Nex-AGI Team, Yuxuan Cai, Lu Chen, Qiaoling Chen, Yuyang Ding, Liwen Fan, Wenjie Fu, Yufei Gao, Honglin Guo, Pinxue Guo, Zhenhua Han, Zhengfu He, Hanglei Hu, Kai Hu, Shengjia Hua, Tianyu Huai, Baodai Huang, Li Ji, Zhen Jiang, Zhikai Lei, Bufan Li, Jiahang Lin, Lizhi Lin, Jinxiu Liu, Shichun Liu, Ziming Liu, Yuchen Ni, Pengfang Qian, Yujiong Shen, Qingyun Shi, Wentao Shu, Peng Sun, Yiran Suo, Tian Tang, Boyu Tian, Guoteng Wang, Junzhe Wang, Peixin Wang, Zhiheng Xi, Hang Yan, Jie Yang, Zhixiong Yang, Tianchu Yao, Guangze Ye, Qianxi Yu, Shuo Zhang, Xinyue Zhang, Yiqi Zhang, Jiarong Zhao, Miao Zheng, Rui Zheng, Enyu Zhou, Jiazheng Zhou, Maosen Zhou, Yuhao Zhou, Tao Gui, Yining Zheng, Xinchi Chen, Jie Zhou, Siyuan Feng, Qin Chen, Liang He, Qi Zhang, Xuanjing Huang, Xipeng Qiu",
        "summary": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
        "subjects": "Computation and Language",
        "date": "2025-12-04",
        "category": "cs.CL",
        "crawl_time": "2025-12-05T11:00:05.386817",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。以下是我的详细判断过程： **第一步：核心判断——论文的本质是什么？** - **保留**。这篇论文的核心贡献并非简单应用现有智能体，而是提出了一套名为“Nex生态系统”的**全新方法论和框架**，用于**构建和训练**LLM智能体。该生态系统通过三个维度（复杂性、多样性、保真度）系统性地解决了训练高质量智能体所面临的交互信号稀缺问题。其中，`NexAU`是一个灵活的智能体框架，`NexA4A`则能自动生成多样化的智能体层级。这直接对应了您研究目标中的“构建、改进或演化LLM智能体”。 - **排除项分析**： 1.  **非演化型应用**: 论文虽然提到了在SWE-bench等基准上测试，但其核心是提出训练智能体的新范式，而非将智能体应用于软件工程领域。因此，不属于此项排除。 2.  **非Agentic的推理**: 论文明确聚焦于`Agentic Models`和`agent framework`，其目标是提升智能体在复杂任务中的决策能力，而非改进LLM本身的基础推理能力。 3.  **基础设施**: 论文提到的“生态系统”和“基础设施”是指用于生成训练数据和环境的**方法论框架**，而非模型部署、硬件加速等底层技术基础设施。这是构建和演化智能体的关键一环，属于研究范畴。 **第二步：正面指标——论文是否包含我的核心关注点？** 论文包含了大量核心关注点： - **核心范式**: 标题和摘要中明确出现了`Agentic Models`，并讨论了`agent hierarchies`，这与`Multi-Agent Systems (MAS)`高度相关。 - **智能体能力**: 论文旨在训练能在`SWE-bench`等复杂任务上表现出色的智能体，这必然涉及`Planning`和`Tool Use`。其`incentive-driven decision making`的提法也与智能体的自主性相关。 - **多智能体**: `NexA4A`自动生成`diverse agent hierarchies`，直接触及了多智能体的协作与层级结构。 - **演化机制**: 整个生态系统旨在通过大规模、高质量的交互环境进行`policy learning`，这是一种通过环境反馈进行迭代改进的机制，与`Iterative Improvement`和`Self-Improvement`的理念相符。 **第三步：排除标准——是否为我的研究焦点之外？** - **安全与对齐**: 论文未提及`Safety`, `Alignment`, `Interpretability`等相关内容。 - **多模态与视觉**: 论文未涉及`Vision`, `MLLMs`等多模态内容。 **第四步：处理特殊和模糊情况** - **推理/规划**: 论文的核心是构建一个能让智能体学会复杂规划和决策的训练框架，属于“保留”范畴。它不是在改进LLM的数学或逻辑推理本身，而是在构建一个能让智能体应用这些能力的Agentic系统。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一套创新的、系统性的框架（Nex生态系统），用于**构建和演化**更强大的LLM智能体。它不仅包含了单智能体框架（`NexAU`），还涉及了多智能体系统的生成（`NexA4A`），并致力于通过环境交互实现智能体的能力提升。这完全契合您关于“LLM智能体及其演化”的研究课题，特别是“单智能体”、“多智能体”和“自我演化”三个方向。因此，应判定为**True**。"
    },
    {
        "index": "#7",
        "title": "SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs",
        "link": "/arxiv/2512.04868",
        "arxiv_id": "2512.04868",
        "authors": "Hao Wang, Jialun Zhong, Changcheng Wang, Zhujun Nie, Zheng Li, Shunyu Yao, Yanzeng Li, Xinchi Li",
        "summary": "Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework's capacity for robust and scalable conversational reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-12-04",
        "category": "cs.CL",
        "crawl_time": "2025-12-05T11:00:05.387381",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献与您的筛选标准高度契合。我的判断过程如下： **第一步：核心判断——保留** 这篇论文的本质是提出一个名为SEAL的**新框架**，其核心是“自我演化智能体学习”。论文并非简单地将现有LLM或智能体框架应用于知识图谱问答领域，而是**构建了一个具有自我演化能力的智能体方法论**。摘要中明确指出，该框架包含一个“智能体校准模块”和一个关键的“自我演化机制”，这直接命中了您研究目标中的“构建、改进或演化LLM智能体”。因此，它不属于“非演化型应用”的排除范畴。 **第二步：正面指标——高度相关** 论文摘要中包含了大量您关注的核心关键词和概念： - **核心范式**: `Self-Evolving`, `Agentic Learning` - **智能体能力**: 论文的核心是智能体框架，其中包含了`Memory`（“集成了局部和全局记忆”）和`Self-Reflection`（“反思模块”）。其两阶段的处理流程（提取核心、校准补全）本身就是一种复杂的`Planning`和`Tool Use`（将校准模块视为一种工具）。 - **演化机制**: 论文的标题和摘要都反复强调`Self-Evolving`，并具体描述了其机制：“从对话历史和执行反馈中持续适应”，这正是`Self-Improvement`和`Iterative Improvement`的体现。 **第三步：排除标准——未触发** 论文的研究焦点是提升智能体在特定任务上的性能、准确性和效率，并未涉及安全、对齐、可解释性或水印等排除项。同时，它处理的是文本和知识图谱，不涉及视觉或多模态内容，因此也未触发相关的排除标准。 **第四步：处理特殊和模糊情况——符合保留规则** 这篇论文是“自我演化的应用”这一特殊情况的典型范例。虽然它的应用场景是“知识图谱上的对话式问答”，但论文的**核心贡献是提出了一种新的“自我演化”机制**（SEAL框架），而不是仅仅展示如何用现有工具解决该领域问题。根据您的规则“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”，这篇论文必须被保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是构建了一个具有记忆、反思和自我演化能力的LLM智能体框架（SEAL）。它完美地契合了您研究课题中的“单智能体”和“自我演化”两个核心方向。尽管它有一个具体的应用领域，但其方法论的创新性是普适的，完全符合您筛选前沿论文的目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#32",
        "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
        "link": "/arxiv/2512.04220",
        "arxiv_id": "2512.04220",
        "authors": "Wenlong Deng, Yushu Li, Boying Gong, Yi Ren, Christos Thrampoulidis, Xiaoxiao Li",
        "summary": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
        "subjects": "Computation and Language",
        "date": "2025-12-03",
        "category": "cs.CL",
        "crawl_time": "2025-12-05T11:00:05.399880",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于**改进LLM智能体的训练方法**，属于“单智能体”和“自我演化”的交叉领域。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM智能体作为工具去解决某个特定领域的问题，而是深入研究并解决了一个在**构建LLM智能体**过程中遇到的核心技术难题。论文的核心是提出一种新的正则化方法（LLDS）来修复Group Relative Policy Optimization (GRPO)算法在训练工具集成智能体时的崩溃问题。这直接对应了“改进LLM智能体”的目标，而非简单的应用。 2.  **第二步：正面指标** - 论文高度符合您的核心关注点： - **核心范式**: 论文的研究对象是 `Tool-integrated (TI) reinforcement learning (RL)`，这正是构建 `Agentic AI` 的核心范式之一。 - **智能体能力**: 论文的核心是让LLM通过 `Tool Use`（与搜索引擎、检索器交互）来执行 `multi-step reasoning`（多步推理）。这完全命中了“单智能体”方向下的“工具使用”和“规划/推理”能力。 - **演化机制**: 论文识别并解决了训练过程中的一个“自我强化的死亡螺旋”。虽然这不是智能体在任务执行中的自我演化，但它属于**训练过程中的自我完善和迭代优化**，是确保智能体能够成功演化的基础。其提出的LLDS方法旨在实现更稳定的 `Iterative Improvement`。 3.  **第三步：排除标准** - 论文的主要贡献**不涉及**安全与对齐（Safety, Alignment）、可解释性（Interpretability）或多模态（Vision）。它的焦点是算法的稳定性和性能，因此没有被排除。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美符合“保留”条件。它不是在研究如何提升LLM本身的数学或逻辑能力，而是在研究**智能体如何通过使用工具进行规划和多步推理**，以及如何让这个训练过程更稳定。ReAct、ToT等是智能体推理的框架，而本文研究的GRPO及其改进方法，正是训练这类智能体推理能力的底层算法。 **核心依据总结**: 该论文的核心贡献是提出了一种名为LLDS的新方法，用于解决在训练“工具集成LLM智能体”时出现的“GRPO崩溃”问题。这直接对应了您研究目标中的“**改进LLM智能体**”。它聚焦于智能体的核心能力——**工具使用和多步推理**——的训练稳定性问题，属于构建更强大、更可靠的Agentic AI的基础性研究。因此，这篇论文是您研究课题“LLM智能体及其演化”下的高度相关前沿文献。"
    },
    {
        "index": "#34",
        "title": "Multi-LLM Collaboration for Medication Recommendation",
        "link": "/arxiv/2512.05066",
        "arxiv_id": "2512.05066",
        "authors": "Huascar Sanchez, Briland Hitaj, Jules Bergmann, Linda Briesemeister",
        "summary": "As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-12-04",
        "category": "cs.CL",
        "crawl_time": "2025-12-05T11:00:05.400499",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质并非简单地将LLM应用于医疗领域。它的核心贡献是提出了一种名为“LLM Chemistry”的新框架，用于量化和指导多个LLM之间的协作。这属于**构建和改进多智能体系统**的方法论研究。虽然其应用场景是药物推荐，但论文的焦点在于“如何让多个LLM智能体有效协作”，而不是“如何用LLM解决药物推荐问题”。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标** - 论文高度符合我的核心关注点。摘要中明确包含了多个正面指标： - **核心范式**: `Multi-LLM Collaboration` 直接对应 `Multi-Agent Systems (MAS)`。 - **多智能体**: `Collaboration` (协作) 是论文的核心主题。`LLM Chemistry` 和 `interaction modeling` (交互建模) 涉及智能体间的 `Communication` (通信) 和协作机制。 3.  **第三步：排除标准** - 论文不涉及任何排除标准。虽然提到了“reliability”和“trustworthy”，但其主要贡献是关于协作机制，而非安全、对齐或可解释性技术本身。论文也未涉及多模态或视觉内容。 4.  **第四步：处理特殊和模糊情况** - 该论文的情况与“自我演化的应用”的例外规则逻辑相似。规则指出，如果论文的核心是提出一种新的“自我演化”机制，即使应用在特定领域也应保留。同理，本论文的核心是提出一种新的“多智能体协作”机制，因此即使应用在医疗领域，也完全符合我的研究目标。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一种新颖的多智能体协作框架（LLM Chemistry），以提升系统整体的稳定性和可靠性。这直接对应了我研究焦点中的“多智能体”方向，特别是关于智能体间协作与通信的子方向。因此，这篇论文与我的研究课题高度相关，应被筛选出来。"
    },
    {
        "index": "#35",
        "title": "CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent",
        "link": "/arxiv/2512.04949",
        "arxiv_id": "2512.04949",
        "authors": "Leyang Shen, Yang Zhang, Chun Kai Ling, Xiaoyan Zhao, Tat-Seng Chua",
        "summary": "Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-12-04",
        "category": "cs.CL",
        "crawl_time": "2025-12-05T11:00:05.400776",
        "filter_reason": "这篇论文符合我的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献是提出了一种名为CARL的**新的强化学习算法**，其目标是**改进多步骤智能体的训练效率和性能**。这完全符合“构建、改进或演化 LLM智能体”的核心目标。它不是将智能体作为工具去解决某个特定领域的问题，而是专注于智能体本身的学习和优化机制，属于方法论层面的创新。 2.  **第二步：正面指标** - 论文明确聚焦于 **`Agentic AI`**，研究对象是“multi-step agent”（多步骤智能体）。 - 其核心贡献与 **`Planning`**（规划）和**多步推理**紧密相关。CARL算法通过识别和优化“关键动作”，实质上是在改进智能体在复杂任务中的决策和执行规划能力。 - 虽然没有直接使用“Self-Evolving”这个词，但其提出的强化学习算法是一种通过环境反馈（奖励信号）来**迭代改进**智能体策略的机制，这与“自我演化”的内涵（通过经验、环境反馈进行自我完善）高度一致。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或水印等问题。 - 论文也未涉及多模态或视觉，其焦点是智能体的决策算法本身。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是典型的“保留”案例。它不是在提升LLM的基础推理能力（如数学计算），而是在研究**智能体如何在与环境的多次交互中进行有效的规划和行动**。CARL算法本身就是一种新的Agentic框架，用于优化多步决策过程。 - **LLM的提及**: 摘要中未明确提及“LLM”，但这在当前Agentic AI研究中是常见情况。许多关于智能体框架、训练算法（如强化学习）的论文是通用性的，但其最终应用和验证平台往往是LLM-based Agent。该论文解决的是“多步骤智能体”这一普遍存在的核心挑战，其成果可以直接应用于并改进LLM智能体。因此，不应因未直接提及LLM而排除。 **最终决策**: 综合以上分析，该论文的核心贡献是提出了一种用于**改进单智能体在多步骤任务中规划和执行能力的新算法（CARL）**。这直接命中了我的研究焦点“单智能体”方向，并与“自我演化”的迭代改进思想相符。它属于对智能体核心能力的构建和改进，而非应用或基础设施研究。因此，这篇论文高度相关，应被**保留**。"
    },
    {
        "index": "#45",
        "title": "Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space",
        "link": "/arxiv/2512.04601",
        "arxiv_id": "2512.04601",
        "authors": "Joey Hong, Kang Liu, Zhan Ling, Jiecao Chen, Sergey Levine",
        "summary": "Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-12-04",
        "category": "cs.CL",
        "crawl_time": "2025-12-05T11:00:05.408802",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接对应了您设定的“单智能体”和“自我演化”两个方向。我的判断过程如下： 1.  **第一步：核心判断——保留** - 论文的本质是提出一种名为“Natural Language Actor-Critic (NLAC)”的**新算法/新框架**，用于**训练和改进LLM智能体**。它不是将现有智能体作为工具去解决某个特定领域的问题，而是聚焦于如何让智能体本身学得更好、更稳定、更高效。这完全符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **第二步：正面指标——高度匹配** - **核心范式**: 论文明确研究 `LLM-based Agents`。 - **智能体能力**: 论文的研究任务涉及 `Tool Use`、`Reasoning` 和在复杂环境中的长期交互，这些都是智能体的核心能力。 - **自我演化机制**: 这是本文最关键的亮点。NLAC的核心创新在于使用一个生成式LLM作为评论家，它输出的不是简单的奖励分数，而是**自然语言形式的解释**，告诉智能体“为什么某个动作是次优的”。这种机制使得智能体能够进行**自我反思** 和**自我完善**，从而迭代改进其策略。这直接命中了您“自我演化”研究焦点中的“通过经验、反思或环境反馈进行自我完善和迭代”。 3.  **第三步：排除标准——未触发** - 论文的主要贡献是关于提升智能体的训练效率和性能，而非安全、对齐或可解释性。 - 论文完全在语言空间内进行，不涉及视觉或多模态内容。 4.  **第四步：处理特殊和模糊情况——符合保留规则** - **推理/规划**: 论文关注的是智能体在复杂任务（如网页浏览、工具使用）中的多步决策和学习，这属于智能体框架下的推理，而非提升LLM本身的基础数学或逻辑能力，因此应该保留。 - **自我演化的应用**: 本文的核心贡献就是提出一种新的“自我演化”训练机制，即使它在具体的任务上进行验证，其价值在于方法论本身，因此完全符合保留的例外规则。 **总结**: 该论文的核心贡献是提出了一种新颖的Actor-Critic算法，通过引入自然语言评论家来指导LLM智能体的学习。这种方法不仅提升了训练的稳定性和数据效率，更重要的是，它为智能体提供了一种**通过语言反馈进行自我反思和迭代优化**的强大机制。这精准地契合了您在“单智能体”能力提升和“自我演化”机制探索上的研究目标。因此，这篇论文是高度相关且应该保留的前沿研究。"
    },
    {
        "index": "#1",
        "title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets",
        "link": "/arxiv/2512.04988",
        "arxiv_id": "2512.04988",
        "authors": "Christopher Chiu, Simpson Zhang, Mihaela van der Schaar",
        "summary": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-12-04",
        "category": "cs.MA",
        "crawl_time": "2025-12-05T11:00:05.428825",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献与你的三个研究方向高度契合。 1.  **核心判断 (第一步):** 论文的核心是提出一个“开创性的新框架”，用于研究LLM智能体在竞争环境中的行为。这并非将现有智能体作为工具去解决一个外部领域问题，而是**构建和改进智能体本身的能力**。因此，它通过了第一步的核心判断，应被保留。 2.  **正面指标与研究方向匹配 (第二步):** *   **自我演化:** 这是论文最突出的亮点。摘要中明确提到了“战略性自我改进”、“发展技能”、“适应其策略”和“元认知”。这些概念直接对应了你研究目标中的“通过经验、反思或环境反馈进行自我完善和迭代”。论文的核心贡献之一就是揭示和构建了一种让智能体在竞争压力下自我演化的机制。 *   **多智能体:** 论文研究了“竞争性智能体”，它们在市场中“竞争工作”，并且需要具备“竞争意识”（建模对手和市场动态）。这完全属于多智能体系统中的“博弈”和“社会学习”范畴。 *   **单智能体:** 论文强调了智能体需要具备“长期战略规划”和“元认知”（自我评估）能力，这些都是单智能体研究中的核心议题，特别是规划和自我反思。 3.  **排除标准 (第三步):** 论文不涉及安全、对齐、可解释性或多模态技术，因此没有触发任何排除标准。 4.  **特殊与模糊情况处理 (第四步):** *   **推理/规划:** 论文讨论的“长期战略规划”是在复杂、动态的市场环境中进行的多步决策，这属于智能体层面的规划，而非提升LLM基础推理能力，符合保留标准。 *   **自我演化的应用:** 论文虽然应用在“模拟零工经济”这一特定场景，但其核心是提出一种新的“自我演化”机制（战略性自我改进）。根据你的规则，这种情况下应该保留。这个场景是验证机制的试验场，而非研究的最终目的。 **结论:** 该论文的核心贡献在于构建了一个框架，使LLM智能体具备了在竞争环境中进行元认知、战略规划和自我演化的能力。它同时触及了你研究的三个核心方向（单智能体、多智能体、自我演化），并且其贡献是方法论层面的，而非简单的应用。因此，这是一篇高度相关且应被筛选入的前沿论文。"
    },
    {
        "index": "#7",
        "title": "A Modular Cognitive Architecture for Assisted Reasoning: The Nemosine Framework",
        "link": "/arxiv/2512.04500",
        "arxiv_id": "2512.04500",
        "authors": "Edervaldo Melo",
        "summary": "This paper presents the Nemosine Framework, a modular cognitive architecture designed to support assisted reasoning, structured thinking, and systematic analysis. The model operates through functional cognitive modules (\"personas\") that organize tasks such as planning, evaluation, cross-checking, and narrative synthesis. The framework combines principles from metacognition, distributed cognition, and modular cognitive systems to offer an operational structure for assisted problem-solving and decision support. The architecture is documented through formal specification, internal consistency criteria, and reproducible structural components. The goal is to provide a clear conceptual basis for future computational implementations and to contribute to the study of symbolic-modular architectures for reasoning.",
        "subjects": "Artificial Intelligence, Human-Computer Interaction, Multiagent Systems",
        "date": "2025-12-04",
        "category": "cs.MA",
        "crawl_time": "2025-12-05T11:00:05.430386",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献是提出一个名为“Nemosine Framework”的**新框架**，即一个“模块化认知架构”。这完全符合“构建、改进或演化 LLM智能体的方法论或新框架”的保留标准。它不是将现有智能体作为工具去解决某个领域问题，而是专注于智能体本身的架构设计。 2.  **第二步：正面指标** - 论文摘要中包含了多个核心关注点： - **智能体能力**: 明确提到了 `Planning` (规划)、`evaluation` (评估，可视为自我反思的一部分) 和 `cross-checking` (交叉检查，是 `Self-Correction` 或 `Self-Reflection` 的具体体现)。 - **核心范式**: 论文描述的“模块化认知架构”和“功能性认知模块”是 `Agentic AI` 和 `LLM-based Agents` 研究中的典型范式。 - 这些正面指标强烈表明该论文与您的研究焦点高度相关，特别是**单智能体**方向下的规划与自我反思。 3.  **第三步：排除标准** - 论文摘要中没有提及任何关于安全、对齐、可解释性或水印等内容。 - 也没有涉及视觉或多模态相关的研究。 - 因此，该论文不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是关于智能体如何进行规划和结构化推理的典型案例。它提出了一个包含规划、评估、交叉检查等模块的架构，这正是“智能体如何进行规划或在复杂任务中进行多步推理”的保留范畴，而非仅仅是提升LLM本身的基础推理能力。 **最终决策**: 综合以上分析，这篇论文的核心贡献是**构建一个新的、模块化的智能体认知架构**，旨在实现智能体的规划、评估和自我反思等关键能力。它直接对应您研究目标中的“单智能体”方向，特别是关于智能体的规划与自我反思机制。因此，这篇论文是您应该保留的前沿研究。"
    }
]