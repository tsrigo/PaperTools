[
    {
        "index": "#9",
        "title": "Scheming Ability in LLM-to-LLM Strategic Interactions",
        "link": "/arxiv/2510.12826",
        "arxiv_id": "2510.12826",
        "authors": "Thao Pham",
        "summary": "As large language model (LLM) agents are deployed autonomously in diverse contexts, evaluating their capacity for strategic deception becomes crucial. While recent research has examined how AI systems scheme against human developers, LLM-to-LLM scheming remains underexplored. We investigate the scheming ability and propensity of frontier LLM agents through two game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro, Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and without explicit prompting while analyzing scheming tactics through chain-of-thought reasoning. When prompted, most models, especially Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance. Critically, models exhibited significant scheming propensity without prompting: all models chose deception over confession in Peer Evaluation (100% rate), while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These findings highlight the need for robust evaluations using high-stakes game-theoretic scenarios in multi-agent settings.",
        "subjects": "Computation and Language, Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-11",
        "category": "cs.MA",
        "crawl_time": "2025-10-16T11:00:03.923227",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心是研究大语言模型（LLM）在多智能体互动中的一种高级通用推理能力——“谋划能力”。它并非将LLM作为工具应用于某个特定领域，而是通过设计通用的博弈论框架（廉价交谈信号博弈和对等评估对抗博弈），来探测和衡量LLM在策略性互动中的内在能力。这种“谋划”或“策略欺骗”能力，是逻辑推理、规划和多步问题解决能力的复杂体现，属于LLM基础通用能力的范畴。因此，论文通过了核心判断。 **第二步：正面指标——论文是否包含相关主题？** 论文高度符合正面指标： - **核心概念**: 明确研究 \"large language model (LLM) agents\"。 - **能力方向**: 核心研究 \"strategic deception\"（策略欺骗）和 \"scheming ability\"（谋划能力），这属于高级的 \"reasoning\" 和 \"problem-solving\"。 - **新兴范式**: 研究场景是 \"LLM-to-LLM\" 的 \"multi-agent settings\"，并使用 \"chain-of-thought reasoning\" 来分析模型的内在策略。 **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文完全不涉及任何排除标准领域。它没有讨论多模态、特定应用（如医疗、化学），也没有从应用层面讨论水印、安全等问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出了一个通用的多智能体交互框架（基于博弈论），其目的是为了评估和增强LLM的通用策略推理能力，而不是将其应用于特定领域。这完全符合“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的保留标准。 **第五步：最终决策** 综合以上分析，这篇论文的本质是探索和评估LLM的一种前沿且复杂的通用推理能力（策略谋划）。它通过严谨的实验设计，揭示了现有顶尖模型在无提示情况下自发产生此类高级策略行为的倾向。这项研究直接服务于“提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标，因为它首先需要深入理解和评估这种能力。因此，这篇论文是高度相关且应当保留的前沿研究。"
    },
    {
        "index": "#2",
        "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons",
        "link": "/arxiv/2510.13797",
        "arxiv_id": "2510.13797",
        "authors": "Giovanni Monea, Yair Feldman, Shankar Padmanabhan, Kianté Brantley, Yoav Artzi",
        "summary": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.932967",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献是提升大语言模型（LLM）的通用推理能力。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心是解决LLM在进行长上下文推理时面临的一个根本性瓶颈：Transformer架构的KV缓存随上下文长度线性增长导致的内存和计算开销问题。它提出了一种名为“Breadcrumbs Reasoning”的新方法，通过学习和压缩中间推理状态，使得模型能够更高效地进行长序列推理。 - **符合保留标准**: 这项工作直接**改进了LLM的基础能力**。长上下文推理是复杂逻辑、数学证明、代码生成和规划等高级通用推理能力的先决条件。通过提升这一基础能力的效率和可扩展性，论文实质上是在增强LLM的**通用推理能力**。它提出了一种**新的训练范式**（结合了蒸馏和强化学习）来教会模型如何自主压缩其推理“记忆”，这是一种方法论层面的创新，而非应用层面的。 2.  **第二步：正面指标** - 论文标题和摘要明确提到了核心概念 **\"Reasoning\"**。 - 其解决的问题直接关联到 **\"problem-solving\"** 和 **\"multi-step reasoning\"**，因为长上下文是解决复杂问题的关键。 - 论文的训练方法明确使用了 **\"reinforcement learning (RL)\"**，这是你关注的关键训练方法之一。 3.  **第三步：排除标准** - 论文完全未涉及多模态、视觉等。 - 它没有将LLM应用于任何特定领域（如医疗、化学），其方法是通用的，适用于任何需要长文本推理的任务。 - 它关注的不是水印、安全等应用层面的可靠性问题，而是模型推理过程的内在效率和计算可靠性。 4.  **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需特殊判断。 **最终决策**: 该论文并非简单地将LLM作为工具应用，而是深入到LLM推理过程的内部机制，提出了一种创新的、可学习的压缩方法来克服其固有的计算瓶颈，从而直接赋能并增强了模型执行复杂、长序列通用推理任务的能力。这与你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#1",
        "title": "BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis for Fast and Accurate Multi-Hop Reasoning",
        "link": "/arxiv/2510.13799",
        "arxiv_id": "2510.13799",
        "authors": "Jia-Chen Gu, Junyi Zhang, Di Wu, Yuankai Li, Kai-Wei Chang, Nanyun Peng",
        "summary": "As retrieval-augmented generation (RAG) tackles complex tasks, increasingly expanded contexts offer richer information, but at the cost of higher latency and increased cognitive load on the model. To mitigate this bottleneck, especially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a universal, lightweight compressor that distills relevant evidence for a given query from retrieved documents into a concise summary for seamless integration into in-context RAG. Using seed data consisting of relatively short contexts (fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression of extended contexts exceeding 10k words across a wide range of scenarios. Furthermore, BRIEF-Pro offers flexible user control over summary length by allowing users to specify the desired number of sentences. Experiments on four open-domain multi-hop question-answering datasets show that BRIEF-Pro generates more concise and relevant summaries, enhancing performance across small, large, and proprietary language models. With the 70B reader model, 32x compression by BRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x, while requiring only 23% of its computational overhead.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.932478",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的通用能力。** 该论文的核心贡献是提出了一种名为BRIEF-Pro的通用、轻量级上下文压缩器。其根本目的在于解决检索增强生成（RAG）框架中，长上下文信息给大语言模型带来的计算和认知负荷问题，从而提升模型在复杂任务上的表现。这直接关联到您的核心目标——『提高大语言模型（LLM）本身的通用推理能力』。论文明确指出，该方法尤其针对“复杂的多跳问题”，并在四个“开放域多跳问答数据集”上验证了其有效性。“多跳推理”是衡量通用推理能力的关键指标之一，它要求模型能够整合多条信息、进行多步逻辑推导。因此，通过优化输入给LLM的上下文质量，使其更聚焦、更易于理解，BRIEF-Pro实质上是增强而非替代了LLM自身的推理过程，是一种提升LLM通用推理能力的有效方法论。它并非将LLM应用于特定领域，而是提出了一种通用的、能提升LLM基础性能的组件。 2.  **第二步：正面指标——高度相关。** 论文包含了多个关键的正面指标： *   **核心概念**: 明确以 Large Language Models (LLMs) 为研究和应用对象。 *   **能力方向**: 聚焦于 **reasoning**，特别是 **multi-hop reasoning**（多跳推理），这是通用推理能力的核心体现。 *   **新兴范式**: 该研究属于 **Deep Research** 范畴，旨在通过优化信息处理流程来深化模型的研究和推理能力。 3.  **第三步：排除标准——未触发任何排除项。** *   **多模态与视觉**: 论文完全基于文本，不涉及视觉或多模态内容。 *   **特定应用领域**: 论文在“开放域”数据集上进行验证，而非医疗、化学等特定领域。 *   **模型可靠性（应用层面）**: 研究焦点是提升模型性能和效率，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况——适用增强原则。** 该论文提出的方法可以被视为一种增强LLM推理能力的外部“插件”或框架。它没有改变LLM的内部权重，但它改变了LLM处理信息的方式，使其能更好地进行推理。这与思维链通过在输入中添加引导步骤来提升推理质量的逻辑异曲同工。根据筛选标准，提出一种**通用的方法**来增强模型的通用问题解决能力，应当予以保留。 **最终决策**: 综合以上分析，BRIEF-Pro通过一种通用的上下文压缩技术，直接解决了LLM在执行复杂推理任务时的一个核心瓶颈（长上下文处理），从而显著提升了其多步推理的准确性和效率。这是一种致力于提升LLM基础通用能力的纯粹方法论研究，完全符合您为“大语言模型通用推理能力”研究课题设定的筛选要求。"
    },
    {
        "index": "#3",
        "title": "The Mechanistic Emergence of Symbol Grounding in Language Models",
        "link": "/arxiv/2510.13796",
        "arxiv_id": "2510.13796",
        "authors": "Shuyu Wu, Ziqiao Ma, Xiaoxi Luo, Yidong Huang, Josue Torres-Fonseca, Freda Shi, Joyce Chai",
        "summary": "Symbol grounding (Harnad, 1990) describes how symbols such as words acquire their meanings by connecting to real-world sensorimotor experiences. Recent work has shown preliminary evidence that grounding may emerge in (vision-)language models trained at scale without using explicit grounding objectives. Yet, the specific loci of this emergence and the mechanisms that drive it remain largely unexplored. To address this problem, we introduce a controlled evaluation framework that systematically traces how symbol grounding arises within the internal computations through mechanistic and causal analysis. Our findings show that grounding concentrates in middle-layer computations and is implemented through the aggregate mechanism, where attention heads aggregate the environmental ground to support the prediction of linguistic forms. This phenomenon replicates in multimodal dialogue and across architectures (Transformers and state-space models), but not in unidirectional LSTMs. Our results provide behavioral and mechanistic evidence that symbol grounding can emerge in language models, with practical implications for predicting and potentially controlling the reliability of generation.",
        "subjects": "Computation and Language, Computer Vision and Pattern Recognition",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.933468",
        "filter_reason": "这篇论文符合筛选标准，应当保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心并非将LLM应用于特定领域，而是深入探究LLM内部一个基础且关键的现象——符号接地的产生机制。符号接地是语言模型能够理解世界并进行有效推理的基石。如果一个模型无法将符号（词语）与现实世界的含义联系起来，其所谓的“推理”就只是无意义的模式匹配。因此，研究符号接地的内在机制，本质上是在探索通用推理能力的根本来源和实现方式。这属于改进LLM基础能力和理解其核心工作原理的范畴，符合保留标准。 2.  **第二步与第三步：正面与排除指标** - **正面指标**: 论文明确以“Language Models”为核心研究对象。虽然未直接提及“reasoning”，但其研究的“symbol grounding”是推理能力不可或缺的前置条件。论文最后提到的研究意义在于“controlling the reliability of generation”，这与推理的准确性和可靠性直接相关。 - **排除指标**: 论文虽然提到了“vision-language models”和“multimodal dialogue”，但这只是为了验证其发现普适性的实验设置，其核心贡献在于揭示语言模型内部的“mechanistic emergence”，而非研究多模态技术本身。因此，它不属于被排除的多模态研究范畴。论文也未聚焦于任何特定应用领域或应用层面的模型可靠性问题（如水印、安全）。 3.  **第四步：处理特殊和模糊情况** 这篇论文完美地契合了“可解释性”这一特殊情况的保留标准。它提出了一种“controlled evaluation framework”和“mechanistic and causal analysis”方法，这是一种全新的方法论，旨在增强我们对LLM内在工作机制的理解。通过揭示符号接地是如何在模型内部“涌现”的，该研究为未来如何设计、训练和优化具有更强、更可靠推理能力的LLM提供了根本性的理论依据和指导。这种对模型内在可解释性的深化，直接服务于提升模型通用推理质量和可靠性的最终目标。 **最终决策**: 综合来看，这篇论文虽然不直接提出一种新的训练范式来“提高”推理分数，但它通过一种创新的、机制性的分析方法，深入剖析了构成通用推理能力基础的“符号接地”现象。这种对第一性原理的探索，对于顶尖研究者而言，是推动领域前进的关键。它回答了“LLM的推理能力从何而来”这一更深层次的问题，因此完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    },
    {
        "index": "#11",
        "title": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning",
        "link": "/arxiv/2510.13614",
        "arxiv_id": "2510.13614",
        "authors": "Xingyu Tan, Xiaoyang Wang, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang",
        "summary": "Large Language Models (LLMs) have achieved impressive reasoning abilities, but struggle with temporal understanding, especially when questions involve multiple entities, compound operators, and evolving event sequences. Temporal Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a structured format, offer a reliable source for temporal reasoning. However, existing TKG-based LLM reasoning methods still struggle with four major challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving multi-entity temporal synchronization, adapting retrieval to diverse temporal operators, and reusing prior reasoning experience for stability and efficiency. To address these issues, we propose MemoTime, a memory-augmented temporal knowledge graph framework that enhances LLM reasoning through structured grounding, recursive reasoning, and continual experience learning. MemoTime decomposes complex temporal questions into a hierarchical Tree of Time, enabling operator-aware reasoning that enforces monotonic timestamps and co-constrains multiple entities under unified temporal bounds. A dynamic evidence retrieval layer adaptively selects operator-specific retrieval strategies, while a self-evolving experience memory stores verified reasoning traces, toolkit decisions, and sub-question embeddings for cross-type reuse. Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime achieves overall state-of-the-art results, outperforming the strong baseline by up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.948147",
        "filter_reason": "这篇论文完全符合我的研究范围。以下是详细的判断过程： 1.  **第一步：核心判断——保留。** 论文的核心目标是解决大语言模型在“时间理解与推理”这一通用能力上的短板。它提出了一种名为“MemoTime”的框架，其本质并非将LLM应用于某个特定领域，而是旨在通过一种新的方法论（记忆增强、结构化分解、持续学习）来**直接提升LLM本身的基础推理能力**。时间推理是逻辑、规划和多步推理的一个重要子集，属于通用推理能力的核心范畴。因此，在第一步核心判断中，该论文应被保留。 2.  **第二步：正面指标——高度相关。** 论文摘要中包含了多个强相关的正面指标： *   **核心概念**: 明确以 \"Large Language Models (LLMs)\" 为研究对象。 *   **能力方向**: 核心关注点是 \"reasoning abilities\"，特别是 \"temporal understanding\" 和 \"temporal reasoning\"，这是推理能力的关键维度。 *   **训练方法**: 提出了 \"continual experience learning\" 和 \"self-evolving experience memory\"，这与筛选标准中的 \"evolution\" 和 \"self-evolve\" 完全契合，表明这是一种旨在让模型自我进化和优化的新范式。 这些正面指标密集出现，强烈表明论文与我的研究目标一致。 3.  **第三步：排除标准——不触及。** 该论文的研究焦点完全不在排除标准所列的任何领域： *   它不涉及多模态、视觉或机器人控制。 *   它不以医疗、化学等任何特定应用领域为背景，时间推理是一个跨领域的通用技能。 *   它不聚焦于水印、安全等应用层面的可靠性问题，而是关注模型内在的推理质量和稳定性。 4.  **第四步：处理特殊和模糊情况——属于保留情况。** 论文使用了“时序知识图（TKG）”作为一种工具，并结合了记忆机制。这符合筛选标准中关于工具使用的处理逻辑：**它提出的是一个通用的框架（MemoTime）来增强LLM的时间问题解决能力**，而不是将此工具应用于特定领域（如“用于金融分析的时间推理”）。此外，其“自我进化的经验记忆”通过存储和复用已验证的推理轨迹，直接提升了推理过程的稳定性和效率，这与“提升模型通用可靠性和推理质量”的保留标准完全一致。 **总结:** 论文的核心贡献是提出了一种名为MemoTime的通用框架，它通过结构化分解（Tree of Time）、动态检索和自我进化的记忆机制，系统性地解决LLM在通用时间推理上的挑战。其目标是增强模型本身的基础能力，而非在特定场景中应用模型。因此，这篇论文与研究课题“大语言模型通用推理能力”高度相关，应当被保留。"
    },
    {
        "index": "#16",
        "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization",
        "link": "/arxiv/2510.13554",
        "arxiv_id": "2510.13554",
        "authors": "Yang Li, Zhichen Dong, Yuhan Sun, Weixun Wang, Shaopan Xiong, Yijia Luo, Jiashun Liu, Han Lu, Jiamang Wang, Wenbo Su, Bo Zheng, Junchi Yan",
        "summary": "The reasoning pattern of Large language models (LLMs) remains opaque, and Reinforcement learning (RL) typically applies uniform credit across an entire generation, blurring the distinction between pivotal and routine steps. This work positions attention as a privileged substrate that renders the internal logic of LLMs legible, not merely as a byproduct of computation, but as a mechanistic blueprint of reasoning itself. We first distinguish attention heads between locally and globally focused information processing and reveal that locally focused heads produce a sawtooth pattern near the diagonal indicating phrasal chunks, while globally focused heads expose tokens that exert broad downstream influence over future tokens. We formalize these with two metrics: 1) Windowed Average Attention Distance, which measures the extent of backward attention within a clipped window; 2) Future Attention Influence, which quantifies a token's global importance as the average attention it receives from subsequent tokens. Taken together, these signals reveal a recurring preplan-and-anchor mechanism, where the model first performs a long-range contextual reference to generate an introductory token, which is immediately followed by or coincides with a semantic anchor token that organizes subsequent reasoning. Leveraging these insights, we introduce three novel RL strategies that dynamically perform targeted credit assignment to critical nodes (preplan tokens, anchor tokens, and their temporal coupling) and show consistent performance gains across various reasoning tasks. By aligning optimization with the model's intrinsic reasoning rhythm, we aim to transform opaque optimization into an actionable structure-aware process, hoping to offer a potential step toward more transparent and effective optimization of LLM reasoning.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.950509",
        "filter_reason": "这篇论文完全符合您的研究范围，是一篇高质量的前沿研究。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心是**深入探究并优化大语言模型（LLM）的内在推理机制**。它没有将LLM作为工具应用于特定领域，而是聚焦于LLM本身。论文通过分析注意力模式，揭示了LLM在推理过程中存在一种“预计划-锚定”的内在节奏。更重要的是，它基于这一发现，提出了**新的强化学习（RL）训练策略**，以实现更精细、更有效的策略优化。这完全符合“改进LLM的基础能力”和“提出新的训练范式”的核心保留标准。 **第二步：正面指标——论文是否包含相关主题？** 论文命中了多个关键的正面指标： - **核心概念**: 明确以“Large language models (LLMs)”为研究对象。 - **能力方向**: 论文的标题和摘要反复强调“Reasoning”，旨在提升模型在各种推理任务上的表现。 - **训练方法**: 核心贡献之一是引入了“three novel RL strategies”，直接利用强化学习来优化模型。 这些主题与您的研究目标高度契合。 **第三步：排除标准——论文是否聚焦于排除领域？** 该论文完全不涉及任何排除标准领域。它没有讨论多模态、视觉，也没有将研究限定在医疗、化学等特定应用领域，更没有涉及水印、安全等应用层面的可靠性问题。其研究焦点始终是LLM的通用推理能力本身。 **第四步：处理特殊和模糊情况** 这篇论文巧妙地处理了“可解释性”这一模糊情况。它不仅仅是分析注意力模式来“解释”LLM的推理过程（这本身可能偏向观察性研究），而是更进一步，**将这种可解释性的洞察转化为一种可操作的优化方法**。通过识别出推理过程中的关键节点（预计划token、锚定token），论文实现了更精准的强化学习信用分配。这属于“提出一种新方法来……增强模型内在的可解释性……从而提升模型的通用可靠性和推理质量”的保留范畴，而不是单纯的讨论或分析。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于：1）揭示了LLM推理的一种新颖内在机制（预计划-锚定）；2）基于该机制设计了新的强化学习优化方法；3）在通用推理任务上验证了其有效性。这直接服务于“提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，这篇论文是您研究课题的理想候选，应予以保留。"
    },
    {
        "index": "#26",
        "title": "D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree",
        "link": "/arxiv/2510.13363",
        "arxiv_id": "2510.13363",
        "authors": "Xiang Lei, Qin Li, Min Zhang, Min Zhang",
        "summary": "Large Language Models (LLMs) often exhibit factual inconsistencies and logical decay in extended, multi-turn dialogues, a challenge stemming from their reliance on static, pre-trained knowledge and an inability to reason adaptively over the dialogue history. Prevailing mitigation strategies, such as Retrieval-Augmented Generation (RAG) and agentic working memories, improve information recall but still engage with fundamentally static knowledge sources and follow pre-defined single reasoning path. This hinders their ability to preserve factual and logical consistency of their responses in multi-turn dialogues while the context evolves over time. To address this issue, we propose D-SMART, a model-agnostic framework designed to maintain multi-turn dialogue consistency by enabling LLMs to build and reason over a dynamic, structured representation of the conversational context. This is achieved via two synergistic components: (1) a Dynamic Structured Memory (DSM), which incrementally constructs and maintains an authoritative, OWL-compliant knowledge graph of the conversation; and (2) a Reasoning Tree (RT), which executes inferences as an explicit and traceable multi-step search over the graph. As the popular-used quality score (judged by GPT-4) can overlook logical flaws, we introduce new NLI-based metrics to better measure multi-turn dialogue consistency. Comprehensive experiments on the MT-Bench-101 benchmark show that D-SMART significantly outperforms state-of-the-art baselines, elevating the dialogue consistency score by over 48\\% for both proprietary and open-source models, and notably improves the quality score of the latter by up to 10.1\\%.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.960531",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM的通用能力。** 论文的核心贡献是提出D-SMART框架，旨在解决LLM在多轮对话中出现的“事实不一致和逻辑衰退”问题。这并非将LLM应用于某个特定领域，而是直接针对LLM在动态交互环境下的**基础推理能力缺陷**。论文提出的“动态结构化记忆”和“推理树”是一种新的方法论，用于增强模型的自适应推理和逻辑一致性，这完全属于“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的范畴。因此，根据第一步，应予以**保留**。 2.  **第二步：正面指标——论文高度相关。** -   **核心概念**: 论文明确以Large Language Models (LLMs)为核心研究对象。 -   **能力方向**: 论文的核心是解决“logical decay”（逻辑衰退）和“reason adaptively”（自适应推理），这与筛选标准中的“reasoning (尤其是 logical reasoning)”高度契合。 -   **新兴范式**: 论文提出的框架可以被看作是一种增强LLM的通用问题解决能力的“llm-based agents”范式，它使用了外部工具（动态知识图谱）来辅助推理。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文聚焦于通用的文本对话场景，与多模态、医疗、化学、机器人控制等特定应用领域无关。同时，它研究的是如何从模型内部（通过框架增强）提升推理质量，而非水印、安全等应用层面的可靠性议题。 4.  **第四步：处理特殊和模糊情况——论文符合保留条件。** -   **智能体/工具使用**: D-SMART是一个**通用的**智能体框架，它通过构建动态知识图谱（工具）和执行推理树来增强LLM在**通用任务**（多轮对话）中的表现，完全符合保留条件。 -   **幻觉/可解释性/安全**: 论文直接针对“事实不一致”和“逻辑缺陷”提出解决方案，通过构建“显式和可追溯的”推理树来提升模型的内在推理质量和可靠性。这是一种提升模型通用能力的新方法，而非现象讨论，因此符合保留条件。 **最终决策**: 综合以上分析，该论文的本质是提出一种创新的框架（D-SMART），通过引入动态结构化记忆和显式的多步推理树，来系统性地提升大语言模型在复杂、动态场景下的**通用逻辑推理和事实一致性能力**。这与您“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，最终判断为 **True**。"
    },
    {
        "index": "#24",
        "title": "Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment",
        "link": "/arxiv/2510.13387",
        "arxiv_id": "2510.13387",
        "authors": "Buwei He, Yang Liu, Zhaowei Zhang, Zixia Jia, Huijia Wu, Zhaofeng He, Zilong Zheng, Yipeng Kang",
        "summary": "Persuasion, a fundamental social capability for humans, remains a challenge for AI systems such as large language models (LLMs). Current studies often overlook the strategic use of information asymmetry in message design or rely on strong assumptions regarding pre-commitment. In this work, we explore the application of Bayesian Persuasion (BP) in natural language within single-turn dialogue settings, to enhance the strategic persuasion capabilities of LLMs. Our framework incorporates a commitment-communication mechanism, where the persuader explicitly outlines an information schema by narrating their potential types (e.g., honest or dishonest), thereby guiding the persuadee in performing the intended Bayesian belief update. We evaluate two variants of our approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language (FNL) BP, benchmarking them against both naive and strong non-BP (NBP) baselines within a comprehensive evaluation framework. This framework covers a diverse set of persuadees -- including LLM instances with varying prompts and fine-tuning and human participants -- across tasks ranging from specially designed persuasion scenarios to general everyday situations. Experimental results on LLM-based agents reveal three main findings: (1) LLMs guided by BP strategies consistently achieve higher persuasion success rates than NBP baselines; (2) SFNL exhibits greater credibility and logical coherence, while FNL shows stronger emotional resonance and robustness in naturalistic conversations; (3) with supervised fine-tuning, smaller models can attain BP performance comparable to that of larger models.",
        "subjects": "Computation and Language, Computer Science and Game Theory",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.959594",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“贝叶斯说服”的新框架，用以增强大语言模型（LLM）的战略说服能力。其本质并非将LLM作为工具应用于某个外部领域（如金融或法律），而是深入探究并改进LLM在对话交互中的一种高级认知能力。论文将“说服”这一复杂社会行为，建模为一个涉及“贝叶斯信念更新”的战略推理问题。这直接触及了LLM的通用推理能力，特别是逻辑推理、策略规划和多步思考能力。因此，它符合“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标** 论文与多个正面指标高度相关： *   **核心概念**: 论文明确以“large language models (LLMs)”为研究对象。 *   **能力方向**: 论文的核心是“reasoning”，具体表现为“strategic persuasion”和“Bayesian belief update”，这属于高级的逻辑和策略推理范畴。 *   **新兴范式**: 论文在“LLM-based agents”上进行了评估，验证了其方法在智能体场景下的有效性。 *   **训练方法**: 论文探讨了通过“supervised fine-tuning”让小模型掌握BP策略，这属于改进模型能力的训练范式研究。 3.  **第三步：排除标准** 论文完全避开了所有主要的排除标准： *   它不涉及多模态、视觉等内容。 *   它的研究领域是通用的“对话”和“说服”，而非“医疗、化学、生物”等特定应用领域。虽然说服能力可以应用于这些领域，但该论文的研究焦点是能力本身，而非应用。 *   它不关注模型部署、硬件加速或应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 论文涉及“智能体”，但符合保留条件。它使用LLM智能体作为实验平台，是为了验证其提出的“贝叶斯说服”这一通用框架的有效性，而不是构建一个用于特定领域（如化学实验）的智能体。其目标是提升智能体在通用对话任务中的战略推理水平，这与我的研究目标一致。 **最终决策**: 综合以上分析，该论文致力于通过引入一种新的理论框架（贝叶斯说服）和训练方法，来提升LLM在战略对话中的通用推理和规划能力。它不是关于LLM在特定领域的应用，而是对LLM核心认知能力的深度探索和增强。因此，这篇论文完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    },
    {
        "index": "#23",
        "title": "Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models",
        "link": "/arxiv/2510.13395",
        "arxiv_id": "2510.13395",
        "authors": "Agnese Lombardi, Alessandro Lenci",
        "summary": "Language is fundamental to human cooperation, facilitating not only the exchange of information but also the coordination of actions through shared interpretations of situational contexts. This study explores whether the Generative Agent-Based Model (GABM) Concordia can effectively model Theory of Mind (ToM) within simulated real-world environments. Specifically, we assess whether this framework successfully simulates ToM abilities and whether GPT-4 can perform tasks by making genuine inferences from social context, rather than relying on linguistic memorization. Our findings reveal a critical limitation: GPT-4 frequently fails to select actions based on belief attribution, suggesting that apparent ToM-like abilities observed in previous studies may stem from shallow statistical associations rather than true reasoning. Additionally, the model struggles to generate coherent causal effects from agent actions, exposing difficulties in processing complex social interactions. These results challenge current statements about emergent ToM-like capabilities in LLMs and highlight the need for more rigorous, action-based evaluation frameworks.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.959096",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心并非将LLM作为工具应用于特定领域，而是对LLM的一项基础认知能力——心理理论——进行深入的审视和评估。它探究的是LLM是否具备“真正的推断”能力，还是仅仅在进行“浅层的统计关联”。这直接触及了LLM通用推理能力的本质，即模型是真正理解了逻辑和因果关系，还是在模仿表面的语言模式。因此，这篇论文的本质是关于改进和理解LLM的基础推理能力，符合保留标准。 2.  **第二步：正面指标** 论文包含了多个强烈的正面指标： *   **核心概念**: 明确以“Large Language Models”和“GPT-4”为研究对象。 *   **能力方向**: 核心议题是“Theory of Mind (ToM)”，这是一种高级的社会推理能力。摘要中反复强调“genuine inferences”（真正的推断）、“reasoning”（推理）、“causal effects”（因果效应），这些都是通用推理的核心组成部分。 *   **新兴范式**: 论文使用了“Generative Agent-Based Model (GABM)”这一智能体框架来在模拟环境中评估模型，这与“llm-based agents”的研究方向高度相关。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准。它没有讨论多模态、视觉，也没有聚焦于医疗、化学等特定应用领域，更不是关于水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 论文使用智能体框架（Concordia）的目的是为了创建一个可控的实验环境，以评估LLM的通用推理能力，而不是将其应用于某个特定领域（如“用于化学实验的智能体”）。这完全符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的保留精神。 *   **幻觉/可解释性**: 论文的核心发现——LLM的能力可能源于“浅层的统计关联而非真正的推理”——是对模型“幻觉”或“模仿行为”的一种根本性剖析。通过揭示这一缺陷，论文增强了我们对模型内在工作机制的理解（即可解释性），并指明了提升其通用推理可靠性的方向。这符合保留标准。 **最终决策**: 这篇论文的核心贡献在于，它通过严谨的实验设计，挑战了当前关于LLM已具备高级推理能力（如心理理论）的乐观论断。它揭示了LLM在“真正的推断”和“因果理解”上的根本性局限，并呼吁建立更严格的评估框架。这项工作直接服务于“提高大语言模型本身的通用推理能力”这一核心目标，因为它首先精确地定义和测量了当前能力的边界，这是任何改进工作的前提。因此，这篇论文是您研究课题下的高质量前沿文献。"
    },
    {
        "index": "#37",
        "title": "Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation",
        "link": "/arxiv/2510.13272",
        "arxiv_id": "2510.13272",
        "authors": "Zhichao Xu, Zongyu Wu, Yun Zhou, Aosong Feng, Kang Zhou, Sangmin Woo, Kiran Ramnath, Yijun Tian, Xuan Qi, Weikang Qiu, Lin Lee Cheong, Haibo Ding",
        "summary": "Inspired by the success of reinforcement learning (RL) in Large Language Model (LLM) training for domains like math and code, recent works have begun exploring how to train LLMs to use search engines more effectively as tools for retrieval-augmented generation. Although these methods achieve performance improvement across QA benchmarks, many prioritize final answer correctness while overlooking the quality of intermediate reasoning steps, which may lead to chain-of-thought unfaithfulness. In this paper, we first introduce a comprehensive evaluation framework for evaluating RL-based search agents, covering three distinct faithfulness metrics: information-think faithfulness, think-answer faithfulness, and think-search faithfulness. Our evaluations reveal that a prototypical RL-based search agent, Search-R1, has significant room for improvement in this regard. To foster faithful reasoning, we introduce VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in Agentic Search), a novel framework that integrates fine-grained faithfulness rewards into the reinforcement learning process. Our experiments show that models trained with VERITAS not only significantly improve reasoning faithfulness, but also achieve comparable task performance across seven QA benchmarks.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.971405",
        "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一种名为VERITAS的新框架，该框架通过在强化学习（RL）过程中引入细粒度的“忠实度”奖励，来训练大语言模型。其目标是提升模型在使用工具（搜索引擎）进行检索增强生成时，中间推理步骤的质量和忠实度，而不仅仅是最终答案的正确性。这直接触及了LLM的**基础能力——通用推理能力**的改进，特别是多步推理的逻辑一致性和可追溯性。它并非将LLM应用于特定领域，而是致力于优化LLM本身的推理范式，因此应予以保留。 2.  **正面指标（第二步）：** 论文高度契合多个正面指标。 *   **核心概念:** 明确以Large Language Models (LLMs)为研究对象。 *   **能力方向:** 核心主题是\"Faithful Reasoning\"（忠实推理），这直接属于reasoning范畴，旨在提升推理过程的质量。 *   **训练方法:** 核心方法论是强化学习（RL），并提出了新的奖励机制，属于前沿的训练范式。 *   **新兴范式:** 研究内容涉及LLM-based agents（搜索智能体）和tool use（使用搜索引擎），符合新兴范式的研究方向。 3.  **排除标准（第三步）：** 论文未触及任何排除标准。它不涉及多模态、视觉，也不是针对医疗、化学等特定应用领域的研究。其提出的评估框架和训练方法具有通用性。 4.  **特殊和模糊情况（第四步）：** *   **智能体/工具使用:** 论文研究的是如何让LLM作为一个通用的搜索智能体，更忠实地进行推理。这是一种**通用的智能体框架和工具使用方法**，旨在增强LLM的通用问题解决能力，而非应用于特定领域，因此符合保留条件。 *   **幻觉/可解释性:** 论文解决的\"chain-of-thought unfaithfulness\"（思维链不忠实）问题，本质上是推理过程中的一种幻觉或逻辑断裂。作者提出了一种**新方法（VERITAS）来减少这种现象，从而提升模型内在的推理质量和可靠性**，这完全符合保留标准。 **总结：** 该论文直面了当前LLM推理能力中的一个核心痛点——中间推理过程的不可靠性，并提出了一种创新的、基于强化学习的通用训练框架来系统性解决该问题。其工作本质是提升LLM的内在推理质量，与您“提高大语言模型本身的通用推理能力”的核心目标高度一致。因此，这是一篇非常相关且应被筛选入的高质量论文。"
    },
    {
        "index": "#32",
        "title": "ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering",
        "link": "/arxiv/2510.13312",
        "arxiv_id": "2510.13312",
        "authors": "Simon Lupart, Mohammad Aliannejadi, Evangelos Kanoulas",
        "summary": "We present ChatR1, a reasoning framework based on reinforcement learning (RL) for conversational question answering (CQA). Reasoning plays an important role in CQA, where user intent evolves across dialogue turns, and utterances are often underspecified, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Unlike static `rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through RL. To address the challenge of sparse and delayed rewards in RL, we propose an intent-aware reward that provides turn-level feedback by aligning retrieval and reasoning with evolving user goals. Our proposed ChatR1 demonstrates strong performance on both 3B and 7B model backbones, outperforming competitive models on five CQA datasets, measured by different metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA datasets to cover topic shifts, evolving intents, mixed-initiative dialogues, and multi-document grounding, testing ChatR1's performance from various aspects. Ablation studies confirm the effectiveness of the intent-aware reward. Our analyses further reveal diverse reasoning trajectories and effective use of the search tool. ChatR1 also generalizes robustly across domains, demonstrating that RL-based reasoning enables more flexible and context-sensitive behavior than static CQA pipelines.",
        "subjects": "Computation and Language, Information Retrieval",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.968258",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出一个名为ChatR1的**推理框架**，该框架使用**强化学习（RL）**来训练大语言模型，以提升其在**对话式问答（CQA）**中的推理能力。这直接命中了“改进LLM的基础能力”和“提出新的训练范式”这两个核心保留点。论文的本质不是将LLM应用于某个新领域，而是研究如何让LLM在动态、多轮的对话中更好地进行推理，这属于提升模型内在通用能力的研究。 2.  **第二步：正面指标** 论文包含了多个强烈的正面指标： *   **核心概念**: 论文明确基于LLMs（在3B和7B模型主干上进行）。 *   **能力方向**: 标题和摘要反复强调“**Reasoning**”（推理），并深入探讨了对话中所需的“上下文解释、查询改写、动态协调”等复杂推理行为。 *   **训练方法**: 论文的核心方法论是“**Reinforcement Learning (RL)**”，并提出了创新的“intent-aware reward”来解决RL训练中的难题。 *   **新兴范式**: 论文框架涉及“**tool use**”（使用搜索工具），并且其自适应、探索性的行为模式与**llm-based agents**的理念高度一致。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： *   它不涉及多模态或视觉。 *   它的应用场景是**对话式问答（CQA）**，这是一个通用的NLP任务，而非医疗、化学、金融等特定领域。摘要中特别提到“ChatR1 also generalizes robustly across domains”，这进一步证明了其方法的通用性，而非领域特定。 *   它不关注水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文涉及“智能体/工具使用”。根据标准，ChatR1提出的是一种**通用的**、用于增强LLM推理和问题解决能力的框架。它通过RL学习如何自适应地使用“搜索工具”，而不是将智能体或工具绑定在某个特定应用上。因此，这完全符合保留条件。 **最终决策**: 综合以上分析，这篇论文的核心是探索如何通过强化学习这一新的训练范式，来系统性地提升大语言模型在复杂对话场景下的通用推理能力。它提出的方法论（RL框架、意图感知奖励、自适应工具使用）具有通用性，旨在增强模型的基础能力，而非解决特定领域问题。因此，这篇论文与您“提高大语言模型通用推理能力”的研究目标高度契合，应被保留。"
    },
    {
        "index": "#44",
        "title": "Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation",
        "link": "/arxiv/2510.13191",
        "arxiv_id": "2510.13191",
        "authors": "Jiamin Chen, Yuchen Li, Xinyu Ma, Xinran Chen, Xiaokun Zhang, Shuaiqiang Wang, Chen Ma, Dawei Yin",
        "summary": "Retrieval-Augmented Generation (RAG) has become an essential approach for extending the reasoning and knowledge capacity of large language models (LLMs). While prior research has primarily focused on retrieval quality and prompting strategies, the influence of how the retrieved documents are framed, i.e., context format, remains underexplored. We show that seemingly superficial choices, such as delimiters or structural markers in key-value extraction, can induce substantial shifts in accuracy and stability, even when semantic content is identical. To systematically investigate this effect, we design controlled experiments that vary context density, delimiter styles, and positional placement, revealing the underlying factors that govern performance differences. Building on these insights, we introduce Contextual Normalization, a lightweight strategy that adaptively standardizes context representations before generation. Extensive experiments on both controlled and real-world RAG benchmarks across diverse settings demonstrate that the proposed strategy consistently improves robustness to order variation and strengthens long-context utilization. These findings underscore that reliable RAG depends not only on retrieving the right content, but also on how that content is presented, offering both new empirical evidence and a practical technique for better long-context reasoning.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.979766",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种名为“上下文规范化”的新方法。该方法旨在解决检索增强生成（RAG）中的一个关键问题：如何让大语言模型更稳定、更有效地利用检索到的长上下文信息来进行推理。 - **符合目标**: 这并非将LLM应用于某个特定领域，而是直接针对LLM在处理长上下文时的**通用推理能力**进行优化。它通过改进模型对输入信息的处理方式，直接提升了其推理的准确性和鲁棒性。这属于改进LLM基础能力和方法论的研究，完全符合“保留”标准。 2.  **第二步：正面指标** - 论文明确包含了核心概念 **Large language models (LLMs)**。 - 论文的核心研究主题是 **long-context reasoning**，这正是通用推理能力的关键组成部分。 - 虽然没有涉及强化学习或智能体，但它聚焦于 **problem-solving** 的一个核心环节——如何有效利用外部知识进行推理。 3.  **第三步：排除标准** - 论文不涉及任何多模态、视觉内容。 - 论文的研究是在通用的RAG基准上进行的，没有聚焦于医疗、化学等任何特定应用领域。 - 论文关注的是推理的“robustness”（鲁棒性）和“utilization”（利用率），这是模型内在能力的体现，而非应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** - **工具使用**: 论文研究的是RAG（检索增强生成），这本身就是一种工具使用范式。该论文提出的方法是一种**通用的**、旨在增强LLM在RAG框架下通用推理能力的技术，而非应用于特定领域的工具。因此，根据标准，应该保留。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种新的技术方法（Contextual Normalization），用以解决大语言模型在长上下文推理中的一个根本性挑战。它直接提升了LLM的通用推理能力，而非将其作为工具应用于特定领域。因此，这篇论文与您“提高大语言模型本身的通用推理能力”的核心目标高度契合，应被筛选保留。"
    },
    {
        "index": "#47",
        "title": "Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism",
        "link": "/arxiv/2510.13170",
        "arxiv_id": "2510.13170",
        "authors": "Xiaoshu Chen, Sihang Zhou, Ke Liang, Duanyang Yuan, Haoyuan Chen, Xiaoyu Sun, Linyuan Meng, Xinwang Liu",
        "summary": "Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs) with reasoning capabilities by training them on curated reasoning traces. It leverages both supervised and reinforced fine-tuning to cultivate human-like reasoning skills in LLMs, including detailed planning, divergent thinking, intuitive judgment, timely reflection, internal thinking, and fact perception, etc. As CoT fine-tuning has advanced, LLMs have demonstrated substantial improvements in tasks such as mathematical reasoning and code generation. However, existing surveys about CoT fine-tuning primarily focus on technical aspects and overlook a systematic analysis from the perspective of human reasoning mechanisms. Given that the ultimate goal of CoT fine-tuning is to enable LLMs to reason like humans, it is crucial to investigate this technique through the lens of human cognition. To fill this gap, we present the first comprehensive survey of CoT fine-tuning grounded in human reasoning theory. Specifically, inspired by the well-known Six Thinking Hats framework, which systematically characterizes common human thinking modes using six metaphorical hats, we classify and examine CoT fine-tuning methods through this lens. Furthermore, building upon this theory, we outline potential directions for future research in CoT fine-tuning. In addition, we compile a comprehensive overview of existing datasets and model performances, and a real-time GitHub repository \\footnote{https://github.com/AI-Chen/Awesome-CoT-Finetuning} that continuously tracks recent advances in this area is maintained. We hope this survey will serve as a valuable resource to inspire innovation and foster progress in this rapidly evolving field.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.981099",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文本质是提升LLM基础能力** 论文的核心是对“思维链微调”这一技术领域的全面综述。思维链是当前公认的、用于提升大语言模型多步推理、逻辑和数学解题能力的**核心基础方法论之一**。论文的目标是“endow large language models (LLMs) with reasoning capabilities”（赋予大语言模型推理能力），这与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标完全一致。它并非将LLM应用于特定领域，而是聚焦于如何改进LLM的内在推理机制，因此应予以保留。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了多个关键正面指标： *   **核心概念**: 明确提到了 \"Large language models, LLMs\"。 *   **能力方向**: 聚焦于 \"reasoning capabilities\"，并具体列举了 \"mathematical reasoning\" 和 \"code generation\"，这些都是通用推理能力的典型体现。同时，它还关联了人类的高级认知能力，如 \"planning\"（规划）、\"divergent thinking\"（发散思维）、\"reflection\"（反思）等。 *   **训练方法**: 提到了 \"reinforced fine-tuning\"（强化微调），这是提升模型能力的关键训练范式。 这些指标的密集出现，强有力地证明了论文与您研究课题的高度相关性。 3.  **第三步：排除标准——完全规避** 论文的研究焦点完全避开了所有排除标准。它不涉及视觉或多模态，不针对任何特定应用领域（如医疗、化学等），也未讨论水印、安全等应用层面的可靠性问题。论文中提到的数学和代码，是作为衡量通用推理能力的**基准任务**，而非应用领域。 4.  **第四步：特殊情况——不适用** 该论文不涉及智能体/工具使用或幻觉/安全等特殊情况，因此无需特别处理。 5.  **第五步：最终决策** 综合以上分析，这篇论文是一篇关于“如何通过思维链微调来系统性提升LLM通用推理能力”的综述。它从人类认知机制的独特视角，对现有技术进行了梳理和分类，并指明了未来方向。对于一位顶尖AI研究员而言，这类高质量的综述论文是把握领域前沿、启发创新思路的宝贵资源。其核心贡献——系统性地归纳和展望提升LLM推理能力的方法论——与您的研究目标精准匹配。 因此，最终判断为：**True**。"
    },
    {
        "index": "#56",
        "title": "On the Role of Preference Variance in Preference Optimization",
        "link": "/arxiv/2510.13022",
        "arxiv_id": "2510.13022",
        "authors": "Jiacheng Guo, Zihao Li, Jiahao Qiu, Yue Wu, Mengdi Wang",
        "summary": "Direct Preference Optimization (DPO) has emerged as an important approach for learning from human preferences in aligning large language models (LLMs). However, collecting human preference data is costly and inefficient, motivating methods to reduce the required annotations. In this work, we investigate the impact of \\emph{preference variance} (PVar), which measures the variance in model preferences when comparing pairs of responses, on the effectiveness of DPO training. We provide a theoretical insight by establishing an upper bound on the DPO gradient norm for any given prompt, showing it is controlled by the PVar of that prompt. This implies that prompts with low PVar can only produce small gradient updates, making them less valuable for learning. We validate this finding by fine-tuning LLMs with preferences generated by a reward model, evaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental results demonstrate that prompts with higher PVar outperform randomly selected prompts or those with lower PVar. We also show that our PVar-based selection method is robust, when using smaller reward models (1B, 3B) for selection. Notably, in a separate experiment using the original human annotations from the UltraFeedback dataset, we found that training on only the top 10\\% of prompts with the highest PVar yields better evaluation performance than training on the full dataset, highlighting the importance of preference variance in identifying informative examples for efficient LLM alignment.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.990225",
        "filter_reason": "这篇论文完全符合您的筛选标准，应该被保留。 判断过程如下： 1.  **第一步：核心判断** - 论文的核心是关于**改进LLM的基础训练范式**。它聚焦于“直接偏好优化”，这是一种对齐大语言模型的关键技术。论文没有将LLM作为工具应用于特定领域，而是深入研究了如何让这种训练过程本身变得更高效、更有效。它提出了一种新的方法（基于偏好方差PVar的数据选择）来优化DPO训练。这直接属于“改进LLM的基础能力”和“提出新的训练范式”的范畴，因此符合核心保留标准。 2.  **第二步：正面指标** - **核心概念**: 论文明确以Large language models (LLMs)为研究对象。 - **训练方法**: 论文的核心内容是关于Direct Preference Optimization (DPO)，这是Reinforcement Learning from Human Feedback (RLHF)的一种重要替代或简化方法。这直接命中了“reinforcement learning (RLHF, RL)”这一关键正面指标。 - **能力方向**: 虽然摘要没有直接使用\"reasoning\"一词，但模型对齐是模型能够遵循指令、进行有效规划和推理的先决条件。一个经过高效对齐的模型，其通用能力（包括推理能力）的基础会更加坚实。因此，这项工作是对LLM通用能力底层构建的优化。 3.  **第三步：排除标准** - 论文完全避开了所有排除标准。它不涉及多模态、视觉，不针对任何特定应用领域（如医疗、化学），也不讨论模型部署、硬件加速或应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用、幻觉/可解释性等特殊情况，因此无需特殊判断。 **最终决策**: 这篇论文的本质是**方法论创新**，旨在通过理论分析和实验验证，提出一种能够提升LLM对齐训练效率的新策略（基于偏好方差PVar选择数据）。这种对基础训练过程的优化，直接关系到构建更强大、更可靠的LLM，是提升其通用推理能力不可或缺的一环。因此，它非常契合您“致力于提高大语言模型（LLM）本身『通用推理能力』”的核心目标。"
    },
    {
        "index": "#48",
        "title": "CoT-Evo: Evolutionary Distillation of Chain-of-Thought for Scientific Reasoning",
        "link": "/arxiv/2510.13166",
        "arxiv_id": "2510.13166",
        "authors": "Kehua Feng, Keyan Ding, Zhihui Zhu, Lei Liang, Qiang Zhang, Huajun Chen",
        "summary": "While chain-of-thought (CoT) distillation from advanced large language models (LLMs) has proven effective in general reasoning tasks, it struggles in scientific domains where even advanced models often produce incorrect or superficial reasoning due to high complexity and specialized knowledge requirements. Directly distilling from such flawed outputs results in low-quality training data and limits the performance of smaller student models. To overcome this, we propose CoT-Evo, an evolutionary CoT distillation framework. It begins by constructing a diverse pool of reasoning trajectories from multiple LLM thinkers, enriches them with automatically retrieved domain knowledge, and iteratively refines the trajectories using novelty-driven selection, reflective recombination and mutation. The refinement is guided by a fitness function that evaluates answer correctness, coherence, and effective knowledge utilization. This results in a high-quality CoT dataset tailored for scientific reasoning. We employ this evolved dataset to fine-tune a compact model, which achieves state-of-the-art performance on scientific reasoning benchmarks. Our work establishes a scalable approach to synthesizing high-fidelity scientific reasoning data from diverse and fallible LLMs.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.981554",
        "filter_reason": "这篇论文符合我的研究范围，应被保留。判断过程如下： 1.  **第一步：核心判断——论文的本质是提升模型的基础能力。** 论文的核心贡献是提出了一个名为\"CoT-Evo\"的**进化式思维链蒸馏框架**。这是一个全新的**训练范式和方法论**，其目标是通过迭代优化来合成高质量的思维链数据，然后用这些数据去微调（即提升）一个更小的学生模型。论文的本质并非将LLM作为工具应用于科学领域，而是致力于解决当前思维链蒸馏方法在复杂任务中产生低质量数据的问题，从而**从根本上增强LLM的多步推理能力**。这完全符合“改进LLM的基础能力”、“提出新的训练范式”、“增强其逻辑、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——论文包含多个高度相关的主题。** 论文明确提及了多个核心正面指标： *   **核心概念**: \"Large language models (LLMs)\" 是论文的研究对象。 *   **能力方向**: \"chain-of-thought (CoT)\"、\"reasoning tasks\"、\"reasoning trajectories\"、\"scientific reasoning\"等词汇反复出现，表明论文的核心聚焦于**推理能力**。 *   **训练方法**: \"Evolutionary Distillation\"、\"evolutionary\"、\"iteratively refines\"、\"selection, recombination and mutation\"、\"fitness function\"等都指向了**进化学习**这一前沿的训练方法。 *   **新兴范式**: \"multiple LLM thinkers\"可以看作是一种多智能体协作思想在数据生成阶段的应用。 3.  **第三步：排除标准——论文未触及主要排除领域。** 论文的研究焦点不在于多模态、视觉，也不在于医疗、化学、机器人控制等具体的特定应用领域。虽然它以\"科学推理\"作为测试和验证的 benchmark，但这仅仅是作为衡量其通用推理方法有效性的一个极具挑战性的场景，而非其方法论的局限性。论文的核心是提出一种**通用的**数据合成和模型改进方法，这一点与将模型作为工具解决“化学实验自动化”等特定问题的研究有本质区别。 4.  **第四步：处理特殊和模糊情况——论文的焦点是方法论创新。** *   **智能体/工具使用**: 论文中提到的\"multiple LLM thinkers\"和\"automatically retrieved domain knowledge\"，是为了构建和优化训练数据，属于**提升模型通用能力的方法论的一部分**，而非将智能体/工具应用于特定领域任务。这符合保留条件。 *   **幻觉/可解释性/安全**: 论文旨在通过进化框架来修正高级模型产生的\"incorrect or superficial reasoning\"，并提升训练数据的\"coherence\"（连贯性），这实质上是在方法论层面**提升模型推理的内在质量和可靠性**，从而减少推理过程中的“幻觉”现象。这符合保留条件。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种创新的、基于进化思想的训练范式，旨在从源头上提升LLM用于训练的推理数据质量，进而增强模型本身的多步推理能力。尽管其验证场景是科学推理，但其方法论具有通用性，直接服务于“提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，该论文应被保留。"
    },
    {
        "index": "#54",
        "title": "ESI: Epistemic Uncertainty Quantification via Semantic-preserving Intervention for Large Language Models",
        "link": "/arxiv/2510.13103",
        "arxiv_id": "2510.13103",
        "authors": "Mingda Li, Xinyu Li, Weinan Zhang, Longxuan Ma",
        "summary": "Uncertainty Quantification (UQ) is a promising approach to improve model reliability, yet quantifying the uncertainty of Large Language Models (LLMs) is non-trivial. In this work, we establish a connection between the uncertainty of LLMs and their invariance under semantic-preserving intervention from a causal perspective. Building on this foundation, we propose a novel grey-box uncertainty quantification method that measures the variation in model outputs before and after the semantic-preserving intervention. Through theoretical justification, we show that our method provides an effective estimate of epistemic uncertainty. Our extensive experiments, conducted across various LLMs and a variety of question-answering (QA) datasets, demonstrate that our method excels not only in terms of effectiveness but also in computational efficiency.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.989260",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为ESI的新方法，用于量化大语言模型的“认知不确定性”。这并非将LLM作为工具应用于特定领域，而是致力于理解和改进LLM本身的一种基础能力——即对其自身知识边界的认知。一个能够准确评估自身不确定性的模型，是进行可靠推理的前提。因此，这篇论文的本质是改进LLM的基础能力，符合保留标准。 2.  **第二步：正面指标** 论文明确以“Large Language Models (LLMs)”为核心研究对象。虽然它没有直接提出一种新的推理范式（如CoT），但它研究的“不确定性量化”与“推理质量”密切相关。一个在推理时能意识到自己不确定的模型，其输出的可靠性更高，这间接提升了通用推理能力的鲁棒性。 3.  **第三步：排除标准** 论文不涉及多模态、视觉或任何特定应用领域（如医疗、化学）。虽然它触及了“模型可靠性”，但需要结合第四步的特殊情况来判断。 4.  **第四步：处理特殊和模糊情况** 这是判断的关键。论文属于“幻觉/可解释性/安全”的范畴。根据筛选规则：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” - **减少幻觉**：认知不确定性是导致模型产生“幻觉”（即自信地给出错误答案）的根本原因之一。通过量化这种不确定性，ESI方法为识别和缓解幻觉提供了技术基础。 - **提升通用可靠性和推理质量**：论文摘要明确指出，不确定性量化是“提高模型可靠性”的“有前景的方法”。一个知道自己“不知道”的模型，比一个“不懂装懂”的模型，其推理过程更接近理性，推理结果也更可信。因此，这项工作通过提升模型的元认知能力，直接增强了其通用推理的质量和可靠性。 5.  **第五步：最终决策** 综合来看，这篇论文虽然不是直接提出一种新的推理算法，但它从更根本的层面——不确定性量化——入手，为提升LLM的推理可靠性提供了重要的方法论。它通过增强模型的自我评估能力，使其在执行推理任务时更加稳健和可信。这完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#49",
        "title": "A Matter of Representation: Towards Graph-Based Abstract Code Generation",
        "link": "/arxiv/2510.13163",
        "arxiv_id": "2510.13163",
        "authors": "Nyx Iskandar, Hisham Bedri, Andy Tsen",
        "summary": "Most large language models (LLMs) today excel at generating raw, sequential code with minimal abstractions and custom structures. However, there has been little work on graph-based abstract code generation, where significant logic is encapsulated in predefined nodes and execution flow is determined by edges. This is relevant for visual programming languages, and in cases where raw source code is inaccessible to users and LLM training sets. In this work, we propose and evaluate JSON representations for graphs to enable high accuracy graph-based abstract code generation. We evaluate these representations on ScratchTest, a mini-benchmark based on our custom Python re-implementation of Scratch, which tests the LLM in code graph space. Our findings demonstrate that LLMs can indeed perform the aforementioned generation task in a single pass without relying on specialized or complex pipelines, given the correct graph representations. We also show that different representations induce significantly different accuracies, highlighting the instrumental role of representations in this generation task. All in all, this work establishes the first steps towards representation learning for graph-based abstract code generation.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.981964",
        "filter_reason": "这篇论文符合你的研究范围，其核心贡献在于探索如何通过改进『表示方法』来提升大语言模型的『通用推理能力』。我的判断过程如下： 1.  **核心判断（第一步）：** 论文的本质是提升LLM的基础能力。它没有将LLM作为工具应用于某个特定领域（如化学或医学），而是聚焦于一个更根本的问题：如何让LLM生成和理解具有复杂逻辑结构的『抽象代码』。这种“基于图的抽象代码生成”任务，要求模型理解节点（逻辑单元）和边（执行流/依赖关系）之间的关系，这本质上是一种高级的、结构化的逻辑推理和规划能力。论文的核心贡献是提出并验证了不同的JSON表示方法对这种推理能力的关键影响，这直接触及了如何改进模型内在能力的方法论层面，完全符合“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的保留标准。 2.  **正面指标（第二步）：** 论文与多个正面指标高度相关。 - **核心概念**: 明确以Large language models (LLMs)为研究对象。 - **能力方向**: 论文研究的“graph-based abstract code generation”直接对应**reasoning**, **logical reasoning**, 和 **problem-solving**。让LLM理解并构建出节点和边组成的执行图，本身就是一种复杂的逻辑推理过程。 3.  **排除标准（第三步）：** 论文清晰地避开了所有排除标准。 - 它不涉及多模态或视觉，虽然提到了视觉编程语言（Scratch），但其研究方法是通过JSON这种纯文本格式进行的，核心是模型的逻辑而非视觉感知。 - 它不属于任何特定应用领域，ScratchTest只是一个用于验证通用能力的基准，而不是最终的应用目标。 - 它不讨论模型的基础设施、部署、水印或安全等问题。 4.  **特殊情况处理（第四步）：** 本论文不直接涉及智能体或工具使用，但其精神内核是一致的：通过某种外部框架（在这里是图的表示方法）来增强和规范LLM的内部推理过程。论文发现“不同的表示诱导出显著不同的准确性”，这揭示了表示方法对于解锁模型潜在推理能力的决定性作用，这与通过优化提示框架或思维链来提升推理质量的研究思路一脉相承。 **最终决策（第五步）：** 综合来看，这篇论文并非简单地应用LLM，而是深入研究并提升了LLM的一种核心通用能力——结构化逻辑推理。它通过提出新的表示范式，让LLM能够处理比线性代码更复杂的图状逻辑结构，这是对LLM推理能力边界的有效拓展。因此，它完全符合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。"
    },
    {
        "index": "#68",
        "title": "A\\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning",
        "link": "/arxiv/2510.12838",
        "arxiv_id": "2510.12838",
        "authors": "Qianben Chen, Jingyi Cao, Jiayu Zhang, Tianrui Qin, Xiaowan Li, King Zhu, Dingfeng Shi, He Zhu, Minghao Liu, Xiaobo Liang, Ge Zhang, Jian Yang, Yuchen Eleanor Jiang, Wangchunshu Zhou",
        "summary": "Large language models split into two families: reasoning-centric LLMs, which strengthen internal chain-of-thought reasoning but cannot invoke external tools, and agentic LLMs, which learn to interact with environments and leverage tools but often lag in deep reasoning. This divide arises from fundamentally different training objectives, leading to mismatched strengths and inefficiency on simple queries, where both families tend to overthink or over-call tools. In this work, we present Adaptive Agent Foundation Model (A\\textsuperscript{2}FM), a unified framework that follows a route-then-align principle: the model first learns task-aware routing and then aligns mode-specific trajectories under a shared backbone. To address the inefficiency gap, we introduce a third mode-instant-that handles simple queries directly, preventing unnecessary reasoning or tool calls while complementing the agentic and reasoning modes. To jointly enhance accuracy and efficiency, we propose Adaptive Policy Optimization (APO), which enforces adaptive sampling across modes and applies a cost-regularized reward. On the 32B scale, A\\textsuperscript{2}FM achieves 13.4\\% on BrowseComp, 70.4\\% on AIME25, and 16.7\\% on HLE, setting new SOTA among comparable models and performing competitively with frontier LLMs across agentic, reasoning, and general benchmarks. Notably, the adaptive execution achieves a cost of pass of only \\$0.00487 per correct answer-cutting cost by 45.2\\% relative to reasoning and 33.5\\% relative to agentic, thus delivering substantially higher cost efficiency while maintaining comparable accuracy.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:05.073941",
        "filter_reason": "这篇论文完全符合筛选标准，是关于提升大语言模型通用推理能力的前沿研究。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM基础能力。** 论文的核心贡献是提出了一个名为“A²FM”的统一框架，旨在解决当前LLM领域的一个根本性问题：如何将“以推理为中心的LLM”（擅长内部思维链）和“以智能体为中心的LLM”（擅长使用外部工具）的优势结合起来。它提出了一种新的训练范式（“route-then-align”）和新的优化方法（APO），目标是让模型自身学会根据任务难度，自适应地在“内部推理”、“调用工具”和“直接回答”三种模式间进行选择。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——论文高度相关。** 论文命中了多个关键正面指标： -   **核心概念**: 明确以“Large language models (LLMs)”为研究对象。 -   **能力方向**: 论文的核心就是“Hybrid Reasoning”（混合推理），直接讨论了“reasoning-centric LLMs”和“deep reasoning”。 -   **新兴范式**: 论文标题和摘要都聚焦于“Agent Foundation Model”（智能体基础模型）和“Tool-Aware”（工具感知），这正是当前提升LLM推理能力的关键范式。 -   **训练方法**: 提出了“Adaptive Policy Optimization (APO)”，其本质是一种强化学习方法，用于优化模型的自适应决策能力。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容是纯粹的模型和方法论创新，与多模态、视觉、医疗、化学等特定应用领域无关，也非关注模型部署、水印或安全等应用层面的可靠性问题。 4.  **第四步：处理特殊情况——论文属于应保留的“通用智能体框架”。** 论文的研究对象是“通用”的智能体和推理框架。其评估基准（BrowseComp, AIME25, HLE）横跨了智能体任务、数学推理和一般能力，证明了其方法的通用性，而非局限于某个特定垂直领域。因此，它完全符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”的原则。 **最终决策**： 这篇论文的本质是提出一种创新的、自适应的统一框架，通过让模型学习如何在不同推理模式（内部思考、外部工具、直接回答）之间进行动态路由，来系统性地提升LLM在各类任务上的通用推理效率和效果。这直接切中了“提高大语言模型本身的通用推理能力”这一核心研究目标，是高度相关且必须保留的论文。"
    },
    {
        "index": "#58",
        "title": "OPLoRA: Orthogonal Projection LoRA Prevents Catastrophic Forgetting during Parameter-Efficient Fine-Tuning",
        "link": "/arxiv/2510.13003",
        "arxiv_id": "2510.13003",
        "authors": "Yifeng Xiong, Xiaohui Xie",
        "summary": "Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language models but suffers from catastrophic forgetting when learned updates interfere with the dominant singular directions that encode essential pre-trained knowledge. We propose Orthogonal Projection LoRA (OPLoRA), a theoretically grounded approach that prevents this interference through double-sided orthogonal projections. By decomposing frozen weights via SVD, OPLoRA constrains LoRA updates to lie entirely within the orthogonal complement of the top-$k$ singular subspace using projections $P_L = I - U_k U_k^\\top$ and $P_R = I - V_k V_k^\\top$. We prove that this construction exactly preserves the top-$k$ singular triples, providing mathematical guarantees for knowledge retention. To quantify subspace interference, we introduce $\\rho_k$, a metric measuring update alignment with dominant directions. Extensive experiments across commonsense reasoning, mathematics, and code generation demonstrate that OPLoRA significantly reduces forgetting while maintaining competitive task-specific performance on LLaMA-2 7B and Qwen2.5 7B, establishing orthogonal projection as an effective mechanism for knowledge preservation in parameter-efficient fine-tuning.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:04.991053",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： **第一步：核心判断** 这篇论文的本质是提出一种新的参数高效微调方法（OPLoRA），用于解决大语言模型在微调过程中的“灾难性遗忘”问题。这属于对LLM基础能力的改进和训练范式的创新。灾难性遗忘会直接损害模型在预训练阶段学到的通用知识和能力，包括推理能力。因此，防止遗忘是维持并提升LLM通用能力的关键一环。论文的核心贡献不是将LLM应用于某个特定领域，而是改进了LLM本身的学习和知识保留机制。 **第二步：正面指标** 论文明确包含了多个正面指标： - **核心概念**: 论文研究对象是Large Language Models (LLaMA-2, Qwen2.5)。 - **能力方向**: 论文的实验验证部分直接在\"commonsense reasoning\"（常识推理）和\"mathematics\"（数学）任务上进行测试，这直接关联到研究目标中的“通用推理能力”。 - **训练方法**: OPLoRA本身是一种新颖的微调方法，属于训练范式的范畴。 **第三步：排除标准** 论文的主要焦点完全不符合任何排除标准： - 它不涉及多模态或视觉。 - 它不是特定领域的应用，尽管使用了常识、数学等任务进行评估，但其方法是通用的，旨在提升模型本身的能力。 - 它不关注水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** 这篇论文可以与“幻觉/可解释性/安全”类别进行类比。灾难性遗忘可以被看作是模型内在能力的一种“退化”或“失效”。论文提出了一种新的数学方法（正交投影）来从根源上解决这一内在问题，从而提升模型的内在可靠性和知识质量。这完全符合“提出一种新方法来……提升模型的通用可靠性和推理质量，应该保留”的原则。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是OPLoRA，一种旨在通过防止知识遗忘来稳定和提升LLM性能的微调方法。虽然它没有像CoT那样直接提出一种新的推理“技巧”，但它通过解决底层的“灾难性遗忘”问题，为LLM保持和发挥其通用推理能力提供了坚实的基础。一个不会轻易忘记基础知识的模型，其推理、规划和问题解决能力自然会更稳定、更强大。因此，这篇论文直接服务于“提高大语言模型本身的通用推理能力”这一核心目标。"
    },
    {
        "index": "#79",
        "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math",
        "link": "/arxiv/2510.13744",
        "arxiv_id": "2510.13744",
        "authors": "Shrey Pandit, Austin Xu, Xuan-Phi Nguyen, Yifei Ming, Caiming Xiong, Shafiq Joty",
        "summary": "Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, open-ended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond a few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as self-verification and verification-generation dynamics.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:05.091187",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一个名为`Hard2Verify`的、用于评估大语言模型在开放式数学问题中步骤级验证能力的基准数据集。虽然论文没有直接提出一个新的LLM架构或训练算法，但它为提升LLM推理能力提供了至关重要的基础设施和评估工具。论文明确指出，强大的验证器是训练高水平LLM推理器的“必要前提”。通过创建一个高质量的验证基准，该研究直接推动了如何更有效地训练和优化LLM在复杂、多步推理任务中的表现，这正是提升LLM“通用推理能力”的核心环节。因此，这篇论文的本质是服务于改进LLM的基础推理能力，应予以保留。 2.  **第二步：正面指标** 论文内容紧密围绕筛选标准中的正面指标： *   **核心概念**: 明确以 \"Large language model (LLM)\" 为研究对象。 *   **能力方向**: 聚焦于 \"reasoning\"，特别是 \"math reasoning\" 和 \"step-level\" 的推理过程。 *   **训练方法**: 论文评估了 \"process reward models\"，这是强化学习（RL）在训练推理能力时的一种关键技术。 3.  **第三步：排除标准** 论文不涉及任何排除标准中的领域。它没有讨论多模态、视觉，也没有将LLM应用于医疗、化学等特定领域。同时，它研究的“验证”是为了提升推理质量，而非水印、安全等应用层面的可靠性议题。 4.  **第四步：处理特殊和模糊情况** *   **数学作为通用推理基准**: 需要特别说明的是，虽然论文以数学为载体，但其研究目标并非解决某个特定的数学应用，而是将数学作为衡量和提升模型“通用推理”能力的试金石。这与“将LLM应用于特定领域”的排除标准有着本质区别。研究的是“如何验证推理步骤”这一通用方法论，而非“如何解数学题”这一具体任务。 *   **验证与可靠性**: 论文研究的“步骤级验证”本质上是为了提升模型输出的内在逻辑正确性和可靠性，减少推理错误。这与筛选标准中关于“提升模型内在可靠性和推理质量”的要求相符，属于应保留的情况。 **最终决策**: 综合以上分析，该论文通过构建一个关键的评估基准，为改进LLM的通用推理能力（特别是逻辑和数学推理）提供了基础性支持。它直接关联到强化学习、自我验证等前沿训练范式的发展，是推动该领域进步不可或缺的一环。因此，这篇论文完全符合你关于“大语言模型通用推理能力”的研究课题要求。"
    },
    {
        "index": "#71",
        "title": "Mathematics with large language models as provers and verifiers",
        "link": "/arxiv/2510.12829",
        "arxiv_id": "2510.12829",
        "authors": "Hieu Le Duc, Leo Liberti",
        "summary": "During 2024 and 2025 the discussion about the theorem-proving capabilities of large language models started reporting interesting success stories, mostly to do with difficult exercises (such as problems from the International Mathematical Olympiad), but also with conjectures [Feldman & Karbasi, arXiv:2509.18383v1] formulated for the purpose of verifying whether the artificial intelligence could prove it. In this paper we report a theorem proving feat achieved by ChatGPT by using a protocol involving different prover and verifier instances of the gpt-5 model working collaboratively. To make sure that the produced proofs do not suffer from hallucinations, the final proof is formally verified by the lean proof assistant, and the conformance of premises and conclusion of the lean code is verified by a human. Our methodology was able to solve five out of six 2025 IMO problems, and close a third of the sixty-six number theory conjectures in [Cohen, Journal of Integer Sequences, 2025].",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Logic in Computer Science",
        "date": "2025-10-11",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:05.081245",
        "filter_reason": "这篇论文完全符合你的研究范围，是一个高度相关的范例。以下是根据你的筛选标准进行的详细判断： 1.  **第一步：核心判断** - **保留**。这篇论文的本质并非将LLM作为工具应用于某个外部领域，而是聚焦于如何提升LLM在**数学定理证明**这一核心推理任务上的能力。其核心贡献是提出了一种**新的方法论**：一个由“证明者”和“验证者”LLM实例协作的协议。这直接属于“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴。论文的目标是解决LLM在复杂推理中的根本性问题（如幻觉），而非解决某个特定领域的应用问题。 2.  **第二步：正面指标** - 该论文命中了多个关键正面指标： - **核心概念**: 明确以 \"large language models\" (gpt-5) 为研究对象。 - **能力方向**: 核心主题是 \"theorem-proving\"，这是 \"math reasoning\" 和 \"logical reasoning\" 的最高形式之一。 - **新兴范式**: 论文提出的 \"prover and verifier instances working collaboratively\" 是典型的 **llm-based agents** 和 **multi-agent systems** 框架。同时，使用 \"lean proof assistant\" 进行形式化验证是 **tool use** 的绝佳实践。 3.  **第三步：排除标准** - 该论文完全避开了所有排除标准： - **非多模态**: 论文仅涉及文本和数学符号，不涉及视觉。 - **非特定应用领域**: 虽然主题是数学，但在这里数学是作为衡量和提升**通用推理能力**的“试金石”和“训练场”，而非一个应用领域（如金融建模或医疗诊断）。解决IMO问题是衡量顶级逻辑推理能力的经典方式。 - **非应用层面的可靠性**: 论文虽然涉及可靠性（防止幻觉），但它是通过一种**方法论创新**（形式化验证）来提升模型内在的推理质量，而不是讨论水印或安全等应用层技术。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的“证明者-验证者”协作框架是一种**通用的智能体协作框架**，旨在增强LLM解决复杂问题的通用能力，尽管它在数学领域进行验证。这完全符合保留条件。 - **幻觉/可解释性**: 论文直接针对“幻觉”这一核心问题，并提出了一种**新的方法**（结合多智能体协作与形式化验证工具）来确保推理结果的正确性，从而提升了模型的通用推理质量和可靠性。这完全符合保留条件。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一种新颖的、基于多智能体协作和工具使用的方法论，以系统性地提升大语言模型在最具挑战性的数学推理任务上的表现。它直接触及了LLM通用推理能力的核心瓶颈（如幻觉和复杂逻辑链的可靠性），并提供了可行的解决方案。因此，这篇论文是你研究课题“大语言模型通用推理能力”的完美匹配。"
    },
    {
        "index": "#86",
        "title": "EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems",
        "link": "/arxiv/2510.13220",
        "arxiv_id": "2510.13220",
        "authors": "Yufei He, Juncheng Liu, Yue Liu, Yibo Li, Tri Cao, Zhiyuan Hu, Xinxing Xu, Bryan Hooi",
        "summary": "A fundamental limitation of current AI agents is their inability to learn complex skills on the fly at test time, often behaving like \"clever but clueless interns\" in novel environments. This severely limits their practical utility. To systematically measure and drive progress on this challenge, we first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a new evaluation setup where an agent must play the same game for several consecutive episodes, attempting to improve its performance from one episode to the next. On J-TTL, we find that existing adaptation methods like reflection, memory, or reinforcement learning struggle. To address the challenges posed by our benchmark, we present EvoTest, an evolutionary test-time learning framework that improves an agent without any fine-tuning or gradients-by evolving the entire agentic system after every episode. EvoTest has two roles: the Actor Agent, which plays the game, and the Evolver Agent, which analyzes the episode transcript to propose a revised configuration for the next run. This configuration rewrites the prompt, updates memory by logging effective state-action choices, tunes hyperparameters, and learns the tool-use routines. On our J-TTL benchmark, EvoTest consistently increases performance, outperforming not only reflection and memory-only baselines but also more complex online fine-tuning methods. Notably, our method is the only one capable of winning two games (Detective and Library), while all baselines fail to win any.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:05.094903",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出了一种名为“EvoTest”的**进化测试时学习框架**。其本质是解决当前AI智能体（以LLM为核心）的一个根本性局限：无法在测试时动态学习复杂技能。论文并非将LLM作为工具应用于某个特定领域，而是致力于**改进LLM智能体本身的基础能力**，即其在新环境中的自适应学习和问题解决能力。这是一种全新的训练/适应范式，旨在增强模型的通用推理和规划能力，因此完全符合保留标准。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文命中了多个关键正面指标： *   **核心概念**: 论文研究的是“Agentic Systems”，其核心驱动力无疑是LLM。 *   **能力方向**: 论文的核心是让智能体“学习复杂技能”、“提高其性能”，这直接关联到通用推理、规划和问题解决能力。 *   **训练方法**: 论文的核心创新点“Evolutionary test-time learning”和“evolving the entire agentic system”完全符合“进化”和“自我进化”的范畴。 *   **新兴范式**: 论文明确提出了一个基于LLM的智能体框架，并涉及“学习工具使用例程”，这与“llm-based agents”和“tool use”高度相关。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全避开了所有排除标准。它不涉及多模态、视觉，也不是针对医疗、化学等特定应用领域的研究。其提出的基准（J-TTL）是通用的游戏环境，旨在评估通用能力，而非领域知识。同时，论文也未讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 论文提出的EvoTest框架是一个**通用的智能体协作框架**（Actor Agent + Evolver Agent），其目标是增强智能体的**通用问题解决能力**，而非将其应用于特定领域。这完全符合保留条件。论文中提到的“学习工具使用例程”也是作为提升通用能力的一部分，而非特定领域的工具应用。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种创新的方法论，通过进化框架让LLM智能体在测试时实现自我改进，从而提升其在新环境中的通用推理、规划和问题解决能力。这直接命中了您“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。该研究代表了LLM智能体自我进化领域的前沿探索，是您课题筛选的理想论文。"
    },
    {
        "index": "#90",
        "title": "On the Reasoning Abilities of Masked Diffusion Language Models",
        "link": "/arxiv/2510.13117",
        "arxiv_id": "2510.13117",
        "authors": "Anej Svete, Ashish Sabharwal",
        "summary": "Masked diffusion models (MDMs) for text offer a compelling alternative to traditional autoregressive language models. Parallel generation makes them efficient, but their computational capabilities and the limitations inherent to their parallelism remain largely unexplored. To this end, we characterize what types of reasoning problems MDMs can provably solve and how efficiently. We do this by connecting MDMs to the well-understood reasoning frameworks of chain of thought (CoT) and padded looped transformers (PLTs) in the finite-precision log-width setting: We show that MDMs and polynomially-padded PLTs are, in fact, equivalent in this setting, and that MDMs can solve all problems that CoT-augmented transformers can. Moreover, we showcase classes of problems (including regular languages) for which MDMs are inherently more efficient than CoT transformers, where parallel generation allows for substantially faster reasoning.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:05.101945",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心是研究一种新型语言模型架构——掩码扩散语言模型——的**推理能力**。它没有将模型应用于任何特定领域，而是从理论层面分析和证明这种模型在解决通用推理问题上的能力和效率。论文将MDMs与思维链这一公认的推理增强范式进行对比，并证明了其等价性甚至在某些问题上的优越性。这完全符合“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”以及“提出新的训练范式、方法论的研究”这一核心保留标准。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文高度匹配多个正面指标： *   **核心概念**: 论文研究对象是“Masked Diffusion Language Models”，属于大语言模型的范畴。 *   **能力方向**: 论文的标题和摘要反复强调“Reasoning Abilities”、“reasoning problems”和“faster reasoning”，直接命中了“reasoning”这一核心能力方向。 *   **新兴范式**: 论文深入探讨了“chain of thought (CoT)”这一关键的推理范式，并将其与新模型MDMs进行关联分析。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文完全没有触及任何排除标准领域。它不涉及多模态、视觉，不针对任何特定应用领域（如医疗、化学），也不讨论模型部署、水印或应用层面的安全问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需进行特殊判断。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是一项**基础性、理论性的研究**，旨在探索和证明一种新型语言模型架构的**通用推理潜力**。它通过严谨的理论分析，将MDMs与CoT等现有推理方法建立联系，并揭示了其在效率上的优势。这项工作直接服务于“提高大语言模型本身的通用推理能力”这一核心目标，因为它为我们理解和构建更具推理能力的模型提供了新的理论视角和架构选择。因此，这篇论文是您研究课题下的高度相关且有价值的前沿文献。"
    },
    {
        "index": "#3",
        "title": "Training LLM Agents to Empower Humans",
        "link": "/arxiv/2510.13709",
        "arxiv_id": "2510.13709",
        "authors": "Evan Ellis, Vivek Myers, Jens Tuyls, Sergey Levine, Anca Dragan, Benjamin Eysenbach",
        "summary": "Assistive agents should not only take actions on behalf of a human, but also step out of the way and cede control when there are important decisions to be made. However, current methods for building assistive agents, whether via mimicking expert humans or via RL finetuning on an inferred reward, often encourage agents to complete tasks on their own rather than truly assisting the human attain their objectives. Additionally, these methods often require costly explicit human feedback to provide a training signal. We propose a new approach to tuning assistive language models based on maximizing the human's empowerment, their ability to effect desired changes in the environment. Our empowerment-maximizing method, Empower, only requires offline text data, providing a self-supervised method for fine-tuning language models to better assist humans. To study the efficacy of our approach, we conducted an 18-person user study comparing our empowerment assistant with a strong baseline. Participants preferred our assistant 78% of the time (p=0.015), with a 31% higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a new environment for evaluating multi-turn code assistance using simulated humans. Using this environment, we show that agents trained with Empower increase the success rate of a simulated human programmer on challenging coding questions by an average of 192% over an SFT baseline. With this empowerment objective, we provide a framework for useful aligned AI agents at scale using only offline data without the need for any additional human feedback or verifiable rewards.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-16T11:00:05.193108",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断 (第一步)**: 论文的核心贡献是提出了一种名为“Empower”的全新训练范式，用于微调辅助性的语言模型智能体。其本质不是将LLM应用于某个特定领域（如金融、医疗），而是研究如何从根本提升LLM作为“助手”这一通用角色的能力。它专注于改进LLM的基础行为模式——即何时介入、何时退让、如何真正“赋能”人类，这属于改进LLM基础能力和提出新训练范式的范畴，因此符合核心保留标准。 2.  **正面指标 (第二步)**: 论文高度匹配多个正面指标。 *   **核心概念**: 论文标题和摘要反复提及\"LLM Agents\"。 *   **能力方向**: 论文的目标是让智能体更好地辅助人类完成“任务”和解决“挑战性的编码问题”，这涉及到智能体的规划和问题解决能力。为了实现“赋能”，智能体必须进行复杂的推理，判断人类用户的真实意图和最佳介入时机。 *   **训练方法**: 论文直接与\"RL finetuning\"进行对比，并提出了一种新的基于“人类赋能”的自监督微调方法，这属于训练方法论的革新。 *   **新兴范式**: \"llm-based agents\"是这篇论文的绝对核心。 3.  **排除标准 (第三步)**: 论文未触及任何排除标准。它不涉及多模态、视觉，也不是针对医疗、化学等特定应用领域的研究。虽然其评估环境是“代码辅助”，但这只是用来验证其通用方法有效性的一个平台，论文本身的方法论是领域无关的。 4.  **特殊和模糊情况 (第四步)**: 论文是智能体研究中的一个典型案例，完全符合“保留”的条件。它提出了一种**通用的智能体协作框架（Empower）**来增强LLM的通用问题解决能力（即如何成为一个更好的助手）。评估环境虽然用了编程，但论文的目标是构建“有用的对齐AI智能体”，而非“专门用于编程的智能体”，因此属于通用方法论研究，而非特定领域应用。 **总结**: 该论文的核心是提出一种全新的、基于“人类赋能”目标的训练范式，旨在从根本提升LLM智能体的通用辅助能力。它不是应用型研究，而是针对LLM智能体基础行为和对齐方法的探索，完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”的研究目标。其提出的框架具有通用性，评估环境仅为验证手段，不影响其作为方法论研究的本质。因此，该论文应被保留。"
    },
    {
        "index": "#92",
        "title": "Max It or Miss It: Benchmarking LLM On Solving Extremal Problems",
        "link": "/arxiv/2510.12997",
        "arxiv_id": "2510.12997",
        "authors": "Binxin Gao, Jingjun Han",
        "summary": "Test-time scaling has enabled Large Language Models (LLMs) with remarkable reasoning capabilities, particularly in mathematical domains, through intermediate chain-of-thought (CoT) reasoning before generating final answers. However, the specific sources and mechanisms underlying these reasoning capabilities remain insufficiently understood. Optimization reasoning, i.e. finding extrema under constraints, represents a fundamental abstraction that underpins critical applications in planning, control, resource allocation, and prompt search. To systematically evaluate this capability, we introduce ExtremBench, a benchmark dataset for solving mathematical extremal problems, curated from inequality exercises used for Chinese Mathematical Olympiad and transformed into $93$ standardized extrema-finding problems. We conduct extensive evaluations across various state-of-the-art open-source model families, including the Qwen3, GPT-OSS, and DeepSeek. Our results reveal that LLMs' extremal-solving reasoning capabilities do not always align with those of current mathematical benchmarks such as AIME25 and MATH-500, with some models showing strong general mathematical reasoning but poor extremal-solving skills, and vice versa. This discrepancy highlights a critical gap in current evaluation practices and suggests that existing benchmarks may not comprehensively capture the full spectrum of mathematical reasoning abilities.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:05.102886",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是**提出一个新的评测基准（ExtremBench）并系统性地评估了现有LLM在解决数学极值问题上的能力**。虽然它没有直接提出一种新的训练范式或模型架构来“提高”LLM的能力，但它深刻地“剖析”了LLM通用推理能力的一个特定且重要的子集——优化推理。它揭示了现有评测基准（如MATH-500）的盲点，并指出LLM的通用数学推理能力与解决特定类型优化问题的能力之间存在不一致性。这种对能力边界的精确界定和对现有评估体系缺陷的揭示，是推动“提高LLM通用推理能力”这一研究目标向前发展的**关键基础性工作**。它为未来研究者指明了具体的、需要攻克的短板。因此，其本质是服务于“提高LLM通用推理能力”这一核心目标，而非将LLM应用于特定领域。 2.  **第二步：正面指标** - **核心概念**: 论文明确以\"Large Language Models (LLMs)\"为核心研究对象。 - **能力方向**: 论文聚焦于\"reasoning\"能力，特别是\"mathematical reasoning\"和更具体的\"optimization reasoning\"（优化推理），这完全符合筛选标准中“逻辑、数学、多步推理等通用能力”的范畴。 - **新兴范式**: 论文提到了\"chain-of-thought (CoT) reasoning\"，并研究了测试时扩展对这种推理能力的影响。 3.  **第三步：排除标准** - 论文不涉及多模态与视觉。 - 论文不涉及医疗、化学、机器人等特定应用领域。数学是基础科学和通用技能，不属于此处的“特定应用领域”。 - 论文不涉及水印、安全等模型可靠性（应用层面）的研究。 4.  **第四步：处理特殊和模糊情况** 本论文不属于典型的智能体/工具使用或幻觉/可解释性/安全研究。它的情况比较特殊，是一种“评测与发现”类型的研究。根据筛选标准的精神，如果一个评测工作能够精准地揭示出当前通用能力上的不足，并为未来的“提高”指明方向，它就应该被视为该研究课题的前沿和相关论文。这篇论文正是如此，它没有停留在现有评测上，而是创造性地构建了一个新基准来挖掘更深层次的推理能力，其发现对整个领域具有指导意义。 5.  **第五步：最终决策** 综合以上分析，尽管这篇论文的直接贡献是评测而非模型改进，但它精准地切入了“大语言模型通用推理能力”这一核心主题，特别是其中的数学优化推理子领域。它通过创造新的评测工具和揭示现有能力的短板，为未来如何“提高”LLM的通用推理能力设定了新的研究议程和明确的靶点。对于一个致力于“提高”LLM推理能力的研究项目来说，这篇论文是必读的、具有高度指导价值的前沿文献。因此，最终判断为**符合要求**。"
    },
    {
        "index": "#5",
        "title": "Tandem Training for Language Models",
        "link": "/arxiv/2510.13551",
        "arxiv_id": "2510.13551",
        "authors": "Robert West, Ashton Anderson, Ece Kamar, Eric Horvitz",
        "summary": "As language models continue to rapidly improve, we can expect their actions and reasoning to become difficult or impossible for weaker agents and humans to follow, undermining interpretability and oversight. With an eye on long-term futures, we pursue methods that encourage models to produce solutions that remain intelligible to weaker collaborators. We formalize intelligibility as handoff robustness: a strong model's solution is intelligible to a weaker model if randomly handing off control to the weaker model along the solution path does not cause failure. Building on this criterion, we introduce tandem training for language models, a reinforcement learning (RL) paradigm in which rollout tokens are intermittently and randomly sampled from a frozen weak model rather than the strong model being trained. Because rollouts succeed only when the strong model's actions and reasoning process can be continued by the weak model -- when the two can co-construct a successful solution -- optimizing standard RL objectives with tandem training implicitly incentivizes both correctness and intelligibility. In the GSM8K math reasoning task, tandem training reliably teaches models to abandon jargon and adapt their language to weaker partners while keeping task accuracy high. Our results demonstrate a promising route to building AI systems that remain auditable by weaker agents, with implications for human--AI collaboration and multi-agent communication.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-16T11:00:05.194255",
        "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **第一步：核心判断（保留）** 论文的核心贡献是提出了一种名为“tandem training”的全新强化学习训练范式。其本质并非将LLM应用于某个特定领域，而是直接改进LLM的推理过程本身。该方法的目标是让强大的模型生成对较弱模型也“可理解”的推理路径，这直接触及了LLM推理能力的内在质量和结构，属于对LLM基础能力的增强。 2.  **第二步：正面指标（高度相关）** 论文明确命中了多个关键正面指标： *   **核心概念**: \"Language models\"。 *   **能力方向**: 明确聚焦于 \"reasoning\"，并在 \"math reasoning\" 的标准基准GSM8K上进行了验证。 *   **训练方法**: 提出的核心方法是 \"reinforcement learning (RL)\"。 *   **新兴范式**: 论文的结论部分明确提到了对 \"multi-agent communication\" 的启示，其方法本身也涉及强弱两个模型的协同。 3.  **第三步：排除标准（不涉及）** 该论文完全不涉及任何排除标准中的领域。它没有讨论多模态、视觉，也没有针对医疗、化学等特定应用。论文关注的是模型内在的推理过程，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：特殊和模糊情况（符合保留条件）** *   **可解释性**: 论文深入探讨了“可解释性”和“可理解性”。但它不是停留在现象讨论，而是提出了一种具体的、可操作的训练方法来从根源上提升模型推理过程的可审计性和可理解性。这完全符合“提出新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量”的保留条件。 *   **智能体**: 论文中的强弱模型交互可以被看作一个简单的多智能体系统。但其目的是为了构建一个通用的、能够增强模型通用问题解决能力的协作框架，而非应用于特定领域。 **结论:** 该论文提出了一种创新的训练方法，旨在通过强化学习优化LLM的推理过程，使其推理路径不仅正确，而且易于理解和跟进。这直接服务于“提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标，是一项关于LLM基础方法论的前沿研究，因此应被保留。"
    },
    {
        "index": "#7",
        "title": "Confidence as a Reward: Transforming LLMs into Reward Models",
        "link": "/arxiv/2510.13501",
        "arxiv_id": "2510.13501",
        "authors": "He Du, Bowen Li, Chengxing Xie, Chang Gao, Kai Chen, Dacheng Tao",
        "summary": "Reward models can significantly enhance the reasoning capabilities of large language models (LLMs), but they typically require extensive curated data and costly training. To mitigate these challenges, training-free approaches such as LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate responses, achieving promising results. Recent works have also indicated that model confidence can serve effectively as a reward metric, distinguishing between chain-of-thought (CoT) and non-CoT paths. However, the concept of using confidence as a reward has not been comprehensively studied. In this work, we systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful training-free method that utilizes token-level confidence in the model's final answers as a proxy for reward, especially suitable for close-ended tasks. Through extensive experiments on mathematical reasoning tasks, we demonstrate that CRew outperforms existing training-free reward approaches on the MATH500 and RewardMATH benchmarks, and even surpasses most trained reward models. We further identify a strong correlation between CRew scores and the actual reasoning performance of the model. Additionally, we find that CRew can effectively filter high-quality training data. Building upon these insights, we propose CRew-DPO, a training strategy that constructs preference data from confidence scores combined with correctness signals. Finetuning with CRew-DPO further enhances the model's judging capabilities and consistently outperforms existing self-training methods.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-16T11:00:05.195667",
        "filter_reason": "这篇论文完全符合您的研究范围和筛选标准。 **第一步：核心判断** 论文的核心贡献是提出了一种名为“置信度即奖励”的新方法，并基于此构建了CRew-DPO训练策略。其本质是探索如何利用模型自身的内在信号（置信度）作为奖励，来改进和提升大语言模型的推理能力。这直接触及了“改进LLM的基础能力”和“提出新的训练范式”这两个核心要点，旨在增强其数学和逻辑推理等通用能力，而非将其应用于特定领域。因此，该论文应被保留。 **第二步：正面指标** 论文命中了多个关键的正面指标： - **核心概念**: 论文研究对象明确为“大语言模型”。 - **能力方向**: 论文的核心目标是提升“reasoning capabilities”，并特别在“mathematical reasoning”任务上进行了验证。 - **训练方法**: 论文不仅探讨了免训练的奖励方法，还提出了“CRew-DPO”这一新的训练策略，这与强化学习（RL）和偏好优化（DPO）等前沿训练方法紧密相关。 **第三步：排除标准** 论文的研究焦点完全避开了所有排除标准： - 它不涉及多模态、视觉或特定应用领域（如医疗、化学等）。 - 它关注的是提升模型内在的推理质量，而非应用层面的水印、安全或可靠性问题。 **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊模糊情况，其研究焦点非常清晰。 **第五步：最终决策** 综合来看，这篇论文的出发点是解决奖励模型训练成本高的问题，其提出的CRew方法是一种创新的、旨在提升LLM通用推理能力（特别是数学推理）的通用方法论。它不仅提出了一种新的评估和奖励机制，还进一步将其转化为一种新的训练范式（CRew-DPO），这与您寻找“致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标高度一致。因此，最终判断为符合要求。"
    },
    {
        "index": "#94",
        "title": "DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping",
        "link": "/arxiv/2510.12979",
        "arxiv_id": "2510.12979",
        "authors": "Wei Fan, Wenlin Yao, Zheng Li, Feng Yao, Xin Liu, Liang Qiu, Qingyu Yin, Yangqiu Song, Bing Yin",
        "summary": "Large language models (LLMs) augmented with multi-step reasoning and action generation abilities have shown promise in leveraging external tools to tackle complex tasks that require long-horizon planning. However, existing approaches either rely on implicit planning in the reasoning stage or introduce explicit planners without systematically addressing how to optimize the planning stage. As evidence, we observe that under vanilla reinforcement learning (RL), planning tokens exhibit significantly higher entropy than other action tokens, revealing uncertain decision points that remain under-optimized. To address this, we propose DeepPlanner, an end-to-end RL framework that effectively enhances the planning capabilities of deep research agents. Our approach shapes token-level advantage with an entropy-based term to allocate larger updates to high entropy tokens, and selectively upweights sample-level advantages for planning-intensive rollouts. Extensive experiments across seven deep research benchmarks demonstrate that DeepPlanner improves planning quality and achieves state-of-the-art results under a substantially lower training budget.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:05.103977",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。判断依据如下： 1.  **核心判断（第一步）：** 论文的核心是提出一种名为DeepPlanner的新训练范式，旨在解决现有LLM智能体在“规划”这一核心能力上的不足。它通过一种新颖的强化学习方法（优势塑造）来系统性地优化LLM的规划过程。这直接命中了“改进LLM的基础能力”、“提出新的训练范式”和“增强其……规划、多步推理等通用能力”的保留标准。它并非将LLM应用于特定领域，而是聚焦于提升LLM智能体本身的能力。 2.  **正面指标（第二步）：** 论文摘要中包含了多个高相关性的正面指标： *   **核心概念:** \"Large language models (LLMs)\", \"llm-based agents\" *   **能力方向:** \"planning\", \"multi-step reasoning\", \"complex tasks\" *   **训练方法:** \"reinforcement learning (RL)\" *   **新兴范式:** \"deep research agents\", \"tool use\" 这些关键词密集地出现在摘要中，表明论文与你的研究目标高度相关。 3.  **排除标准（第三步）：** 论文的研究焦点完全避开了所有排除标准。它不涉及多模态、视觉，也不是针对医疗、化学等特定应用领域的研究，更不关注水印、安全等模型可靠性问题。 4.  **特殊和模糊情况（第四步）：** 论文属于“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的情况。DeepPlanner是一个旨在提升“规划能力”的通用框架，虽然它在“深度研究”基准上测试，但其方法论本身是领域无关的，旨在增强LLM的底层推理技能，而非解决某个特定领域的问题。 **最终决策（第五步）：** 综合分析，这篇论文的核心贡献是提出了一种创新的、通用的强化学习框架，用以提升LLM智能体的长时程规划能力。规划是通用推理能力的关键一环。因此，该论文直接且有力地服务于“提高大语言模型本身的通用推理能力”这一核心目标，是高质量的前沿研究，应被筛选入内。"
    },
    {
        "index": "#98",
        "title": "AutoCode: LLMs as Problem Setters for Competitive Programming",
        "link": "/arxiv/2510.12803",
        "arxiv_id": "2510.12803",
        "authors": "Shang Zhou, Zihan Zheng, Kaiyuan Liu, Zeyu Shen, Zerui Cheng, Zexing Chen, Hansen He, Jianzhu Yao, Huanzhi Mao, Qiuyang Mang, Tianfu Fu, Beichen Li, Dongruixuan Li, Wenhao Chai, Zhuang Liu, Aleksandra Korolova, Peter Henderson, Natasha Jaques, Pramod Viswanath, Saining Xie, Jingbo Shang",
        "summary": "Writing competitive programming problems is exacting. Authors must: set constraints, input distributions, and edge cases that rule out shortcuts; target specific algorithms (e.g., max-flow, dynamic programming, data structures); and calibrate complexity beyond the reach of most competitors. We argue that this makes for an ideal test of general large language model capabilities and study whether they can do this reliably. We introduce AutoCode, which uses multiple rounds of validation to yield competition-grade problem statements and test cases. On held-out problems, AutoCode test suites approach 99% consistency with official judgments, a significant improvement over current state-of-the-art methods like HardTests, which achieve less than 81%. Furthermore, starting with a random seed problem, AutoCode can create novel variants with reference and brute-force solutions. By cross-verifying these generated solutions against test cases, we can further filter out malformed problems. Our system ensures high correctness, as verified by human experts. AutoCode successfully produces novel problems judged by Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.",
        "subjects": "Software Engineering, Artificial Intelligence, Computation and Language, Programming Languages",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:05.153063",
        "filter_reason": "我的判断过程严格遵循您提供的筛选标准，具体分析如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心贡献是提出了一种名为`AutoCode`的方法，让大语言模型（LLM）能够高质量地**生成和验证**竞技编程题目及其测试用例。这并非简单地将LLM用作一个“解题工具”，而是让LLM扮演“出题人”的角色。这个过程要求模型具备深度的**通用推理能力**： 1.  **逻辑推理与规划**：设计一个严谨的、无歧义的、有明确解法的问题，需要极强的逻辑构建和规划能力。模型需要设定约束条件、输入分布和边界情况，以排除“投机取巧”的解法，这本质上是在构建一个逻辑自洽的微型世界。 2.  **元认知与自我验证**：论文中的“多轮验证”和“交叉验证生成的解决方案”是关键。这表明模型不仅在生成，还在进行自我批判和验证，检查自己生成的问题是否“格式良好”、解法是否正确。这是一种高级的自我进化或自我修正机制，是提升通用推理能力的重要方向。 因此，论文的本质是探索和提升LLM在**问题构建、逻辑验证和自我修正**等高级认知任务上的能力，这完全符合“改进LLM的基础能力、增强其逻辑、规划等通用能力”的核心要求。 **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文明确以LLMs为核心研究对象。 - **能力方向**: 论文的核心任务“出题”深度涉及了**reasoning**（特别是逻辑推理）、**planning**（问题设计）和**problem-solving**（元问题解决）。 - **训练方法**: 虽然没有明确提及RLHF，但其“多轮验证”和“交叉验证”的流程，是一种迭代式、基于反馈的优化过程，与自我进化的思想高度一致。 - **新兴范式**: `AutoCode`可以被视为一个复杂的、用于解决高级认知任务的自动化系统，与“深度研究”和“工具使用”（将验证和交叉验证作为内部工具）的理念相符。 论文满足了多个关键的正面指标。 **第三步：排除标准——论文是否主要聚焦于以下领域？** - **多模态与视觉**: 不涉及，论文专注于文本和代码。 - **特定应用领域**: 这是需要辨析的关键点。“竞技编程”看似一个特定领域，但它本质上是一个**衡量和测试通用算法、逻辑和数学推理能力的标准基准**，而非像医疗、化学那样拥有特定领域知识体系。论文摘要中也明确指出，他们选择这个任务是因为它“makes for an ideal test of **general** large language model capabilities”。因此，这篇论文的目的是利用这个基准来探索**通用能力**，而不是解决一个特定的化学或医疗问题。它没有违反此排除标准。 - **模型可靠性（应用层面）**: 不涉及水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: `AutoCode`系统通过多轮验证和交叉验证来增强LLM的能力，这是一种提升模型内在推理质量和可靠性的方法，符合“保留”的条件。它不是将智能体应用于特定领域，而是构建一个通用的、能提升模型内在能力的框架。 - **幻觉/可解释性/安全**: 论文中的验证机制正是为了解决模型输出（生成的问题和代码）可能存在的“幻觉”或错误。通过强制模型生成可验证的、逻辑自洽的输出，它直接提升了模型的内在可靠性和推理质量。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一种方法论，通过让LLM执行“出题并验证”这一高度复杂的元认知任务，来显著提升其逻辑构建、规划和自我验证等**通用推理能力**。它将竞技编程作为测试通用能力的理想载体，而非一个孤立的应用领域。其技术核心（多轮验证、交叉验证）也直接指向了增强模型内在可靠性和推理质量的前沿方向。 因此，这篇论文完全符合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的研究目标。"
    },
    {
        "index": "#15",
        "title": "Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning",
        "link": "/arxiv/2510.13214",
        "arxiv_id": "2510.13214",
        "authors": "Zehui Ling, Deshu Chen, Yichi Zhang, Yuchen Liu, Xigui Li, Xin Guo, Yuan Cheng",
        "summary": "Recent advances in Large Language Models (LLMs) demonstrate that chain-of-thought prompting and deep reasoning substantially enhance performance on complex tasks, and multi-agent systems can further improve accuracy by enabling model debates. However, applying deep reasoning to all problems is computationally expensive. To mitigate these costs, we propose a complementary agent system integrating small and large LLMs. The small LLM first generates an initial answer, which is then verified by the large LLM. If correct, the answer is adopted directly; otherwise, the large LLM performs in-depth reasoning. Experimental results show that, for simple problems, our approach reduces the computational cost of the large LLM by more than 50% with negligible accuracy loss, while consistently maintaining robust performance on complex tasks.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-16T11:00:05.205036",
        "filter_reason": "这篇论文完全符合您的筛选标准，其核心贡献与研究目标高度一致。 **第一步：核心判断** 该论文的本质是提出一种新的方法来**增强大语言模型本身的推理效率和成本效益**。它没有将LLM应用于任何特定领域（如医疗、金融），而是聚焦于优化LLM执行“深度推理”这一通用能力的方式。论文提出的“协作智能体系统”是一种新的方法论框架，旨在解决现有推理范式（如思维链）计算开销过大的问题。这完全属于“增强其逻辑、数学、规划、多步推理等通用能力”以及“智能体协作框架”的研究范畴。因此，根据第一步的核心判断，应予以**保留**。 **第二步：正面指标** 论文包含了多个强相关的正面指标： - **核心概念**: 明确以 \"Large Language Models (LLMs)\" 为研究对象。 - **能力方向**: 标题和摘要的核心就是 \"Reasoning\"（推理），并提到了 \"deep reasoning\"（深度推理）和 \"complex tasks\"（复杂任务）。 - **新兴范式**: 提出了 \"Collaborative Agent System\"（协作智能体系统）和 \"multi-agent systems\"（多智能体系统）的概念，这是提升LLM能力的前沿方向。 **第三步：排除标准** 论文完全不涉及任何排除标准中列出的领域。它没有讨论多模态、特定应用场景（如生物、化学），也没有关注水印、安全等应用层面的可靠性问题。其焦点始终保持在LLM的通用推理能力上。 **第四步：处理特殊和模糊情况** 论文提出的智能体系统是典型的“通用智能体协作框架”。其目的不是解决某个特定领域的问题，而是为了**提升LLM进行通用问题解决的效率**。通过动态分配小模型和大模型来处理不同难度的问题，这是一种新颖的、旨在增强LLM内在推理能力（效率和成本控制）的框架设计，完全符合保留条件。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种创新的、基于多智能体协作的推理框架，旨在提升LLM通用推理能力的效率和实用性。它直接回应了“提高大语言模型本身『通用推理能力』”这一核心目标，是方法论层面的前沿研究。因此，最终判断为**符合**。"
    },
    {
        "index": "#26",
        "title": "The Art of Scaling Reinforcement Learning Compute for LLMs",
        "link": "/arxiv/2510.13786",
        "arxiv_id": "2510.13786",
        "authors": "Devvrit Khatri, Lovish Madaan, Rishabh Tiwari, Rachit Bansal, Sai Surya Duvvuri, Manzil Zaheer, Inderjit S. Dhillon, David Brandfonbrener, Rishabh Agarwal",
        "summary": "Reinforcement learning (RL) has become central to training large language models (LLMs), yet the field lacks predictive scaling methodologies comparable to those established for pre-training. Despite rapidly rising compute budgets, there is no principled understanding of how to evaluate algorithmic improvements for scaling RL compute. We present the first large-scale systematic study, amounting to more than 400,000 GPU-hours, that defines a principled framework for analyzing and predicting RL scaling in LLMs. We fit sigmoidal compute-performance curves for RL training and ablate a wide range of common design choices to analyze their effects on asymptotic performance and compute efficiency. We observe: (1) Not all recipes yield similar asymptotic performance, (2) Details such as loss aggregation, normalization, curriculum, and off-policy algorithm primarily modulate compute efficiency without materially shifting the asymptote, and (3) Stable, scalable recipes follow predictable scaling trajectories, enabling extrapolation from smaller-scale runs. Combining these insights, we propose a best-practice recipe, ScaleRL, and demonstrate its effectiveness by successfully scaling and predicting validation performance on a single RL run scaled up to 100,000 GPU-hours. Our work provides both a scientific framework for analyzing scaling in RL and a practical recipe that brings RL training closer to the predictability long achieved in pre-training.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-16T11:00:05.216020",
        "filter_reason": "这篇论文完全符合您的筛选标准，其核心贡献与您的研究目标高度一致。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质并非应用LLM解决特定领域问题，也不是关于模型基础设施。其核心是提出了一种**新的训练范式方法论**。论文致力于解决“如何高效、可预测地扩展用于训练LLM的强化学习（RL）计算量”这一基础性问题。通过建立原则性框架和最佳实践（ScaleRL），它直接改进了用于提升LLM能力的关键技术——强化学习。这属于“改进LLM的基础能力、提出新的训练范式”的范畴，完全符合保留标准。 **第二步：正面指标——论文是否包含相关主题？** - **核心概念**: 论文明确以“Large language models (LLMs)”为核心研究对象。 - **训练方法**: 论文的研究焦点是“Reinforcement learning (RL)”，这是提升LLM推理和对齐能力的关键技术。摘要中反复提及RL training, RL scaling, RL compute，是论文的绝对核心主题。 - **能力方向**: 虽然论文没有直接评估在数学或逻辑推理任务上的性能提升，但它优化的是RL这一**实现这些能力提升的底层方法**。一个更高效、可预测的RL训练过程，是提升LLM在各种推理、规划和问题解决任务上表现的根本保障。因此，其研究内容与“reasoning, planning, problem-solving”等能力方向紧密相关。 **第三步：排除标准——论文是否主要聚焦于排除领域？** - **多模态与视觉**: 完全不涉及。 - **特定应用领域**: 完全不涉及。其提出的框架是通用的，不局限于任何特定垂直领域。 - **模型可靠性（应用层面）**: 论文关注的是训练过程的计算效率和性能预测，而非水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 虽然不直接研究智能体，但其优化的RL训练方法是构建高性能、具备通用问题解决能力的LLM智能体的基石。因此，该研究为相关领域提供了支持，应被保留。 - **幻觉/可解释性/安全**: 不涉及。 **第五步：最终决策** 综合以上分析，这篇论文《The Art of Scaling Reinforcement Learning Compute for LLMs》是一篇典型的、高质量的**方法论研究**。它的核心贡献是解决了LLM训练中一个非常基础且关键的问题：如何科学地扩展强化学习计算。通过使RL训练更高效、更可预测，它直接推动了LLM基础能力的提升边界，为后续所有依赖RL来增强模型推理、规划和问题解决能力的研究铺平了道路。这与您“筛选出致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标完全契合。因此，应该保留。"
    },
    {
        "index": "#97",
        "title": "From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models",
        "link": "/arxiv/2510.12864",
        "arxiv_id": "2510.12864",
        "authors": "Imran Khan",
        "summary": "Large Language Models (LLMs) are increasingly being deployed as the reasoning engines for agentic AI systems, yet they exhibit a critical flaw: a rigid adherence to explicit rules that leads to decisions misaligned with human common sense and intent. This \"rule-rigidity\" is a significant barrier to building trustworthy autonomous agents. While prior work has shown that supervised fine-tuning (SFT) with human explanations can mitigate this issue, SFT is computationally expensive and inaccessible to many practitioners. To address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a novel, low-compute meta-prompting technique designed to elicit human-aligned exception handling in LLMs in a zero-shot manner. The RID framework provides the model with a structured cognitive schema for deconstructing tasks, classifying rules, weighing conflicting outcomes, and justifying its final decision. We evaluated the RID framework against baseline and Chain-of-Thought (CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced judgment across diverse domains. Our human-verified results demonstrate that the RID framework significantly improves performance, achieving a 95% Human Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT. Furthermore, it consistently produces higher-quality, intent-driven reasoning. This work presents a practical, accessible, and effective method for steering LLMs from literal instruction-following to liberal, goal-oriented reasoning, paving the way for more reliable and pragmatic AI agents.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-16T11:00:05.105447",
        "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“规则-意图区分框架”的**元提示技术**。这是一种全新的方法论，旨在解决LLM在推理过程中的一个根本性缺陷——“规则僵化”。其目标是引导模型从字面遵循指令转变为进行更灵活、更符合人类意图的**目标导向推理**。这直接触及并致力于改进LLM的**基础推理能力**，而不是将LLM作为工具应用于某个特定领域。因此，根据第一步的核心判断标准，这篇论文应该被**保留**。 2.  **第二步：正面指标** 论文摘要中包含了多个强烈的正面指标： *   **核心概念**: 明确提到了 \"Large Language Models (LLMs)\"。 *   **能力方向**: 论文的核心是提升模型的 \"reasoning engines\" 能力，具体聚焦于 \"exception handling\"（异常处理）、\"nuanced judgment\"（细致判断）和 \"goal-oriented reasoning\"（目标导向推理），这些都是通用推理能力的关键组成部分。 *   **新兴范式**: 论文提出的方法 \"meta-prompting\" 与 \"Chain-of-Thought (CoT)\" 类似，属于一种新的推理增强范式。同时，论文将这项工作定位为提升 \"agentic AI systems\" 的核心能力，这与 \"llm-based agents\" 的主题高度相关。 3.  **第三步：排除标准** 论文完全没有触及任何排除标准： *   它不涉及多模态或视觉。 *   它的研究是**跨领域**的，旨在提升模型的通用能力，而非聚焦于医疗、化学等特定应用领域。 *   它虽然提到了 \"trustworthy\" 和 \"reliable\"，但这是通过改进模型内在的推理逻辑来实现的，而不是研究水印、安全等应用层面的可靠性技术。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 论文明确指出其研究背景是LLM作为“智能体AI系统的推理引擎”。它提出的RID框架是一种**通用的**方法，用于增强这些智能体的核心推理质量，使其能更好地处理规则冲突和例外情况。这完全符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的保留条件。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种创新的、低计算成本的提示框架，以解决LLM在通用推理中的一个核心瓶颈——即在面对规则与人类意图冲突时的僵化问题。它通过赋予模型一种结构化的认知图式，显著提升了模型进行细致、灵活且与人类对齐的推理能力。这项工作直接贡献于“大语言模型通用推理能力”这一核心研究目标，因此最终判断为 **True**。"
    },
    {
        "index": "#75",
        "title": "To Steer or Not to Steer? Mechanistic Error Reduction with Abstention for Language Models",
        "link": "/arxiv/2510.13290",
        "arxiv_id": "2510.13290",
        "authors": "Anna Hedström, Salim I. Amoukou, Tom Bewley, Saumitra Mishra, Manuela Veloso",
        "summary": "We introduce Mechanistic Error Reduction with Abstention (MERA), a principled framework for steering language models (LMs) to mitigate errors through selective, adaptive interventions. Unlike existing methods that rely on fixed, manually tuned steering strengths, often resulting in under or oversteering, MERA addresses these limitations by (i) optimising the intervention direction, and (ii) calibrating when, and how much to steer, thereby provably improving performance or abstaining when no confident correction is possible. Experiments across diverse datasets, and LM families demonstrate safe, effective, non-degrading error correction, and that MERA outperforms existing baselines. Moreover, MERA can be applied on top of existing steering techniques to further enhance their performance, establishing it as a general-purpose, and efficient approach to mechanistic activation steering.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-16T11:00:05.274418",
        "filter_reason": "这篇论文完全符合您的筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 该论文的核心贡献是提出了一种名为MERA的框架，旨在通过“steering”（引导）语言模型的内部机制来“mitigate errors”（减少错误）。这并非将LLM作为工具应用于特定领域，而是直接针对LLM本身的一种基础能力改进。减少错误、提升输出的准确性是通用推理能力（尤其是逻辑推理和多步推理）的基石。一个更少犯错的模型，其推理过程自然更可靠。因此，这篇论文的本质完全符合“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的要求。 2.  **第二步：正面指标——论文包含高度相关的主题。** 论文明确以“language models (LMs)”为核心研究对象。其目标“mitigate errors”直接关联到“reasoning”和“problem-solving”的质量。虽然未直接提及RLHF或Agents，但其提出的方法论“mechanistic activation steering”是一种新颖的模型内部干预范式，与您关注的“提出新的训练范式”和“方法论的研究”精神一致。 3.  **第三步：排除标准——论文不聚焦于排除领域。** 该论文是纯粹的关于语言模型内部机制的研究，不涉及多模态、视觉，也没有限定在任何特定应用领域（如医疗、化学）。它强调其方法的通用性，在“diverse datasets, and LM families”上进行了验证。 4.  **第四步：处理特殊和模糊情况——论文属于应保留的“可靠性/幻觉”研究。** 这篇论文可以被视为一种减少“错误”或“幻觉”的方法。根据筛选标准，如果论文提出一种新方法来减少幻觉，从而“提升模型的通用可靠性和推理质量”，就应该保留。MERA正是这样一个方法：它不是从社会学角度讨论错误，而是提出一种技术框架，通过优化干预方向和时机，主动修正模型可能产生的错误，或者在无法自信修正时选择“abstention”（放弃回答）。这直接提升了模型输出的内在可靠性，是增强其通用推理能力的关键一环。 **最终决策：** 综合以上分析，该论文的核心贡献是提出一种通用的、模型内部的方法（MERA）来减少LLM的错误。这种对模型基础能力的直接改进，旨在提升其输出的准确性和可靠性，是增强LLM通用推理能力的重要组成部分。它完全符合您的研究目标，应被判定为 **True**。"
    },
    {
        "index": "#70",
        "title": "Thompson Sampling via Fine-Tuning of LLMs",
        "link": "/arxiv/2510.13328",
        "arxiv_id": "2510.13328",
        "authors": "Nicolas Menet, Aleksandar Terzić, Andreas Krause, Abbas Rahimi",
        "summary": "Bayesian optimization in large unstructured discrete spaces is often hindered by the computational cost of maximizing acquisition functions due to the absence of gradients. We propose a scalable alternative based on Thompson sampling that eliminates the need for acquisition function maximization by directly parameterizing the probability that a candidate yields the maximum reward. Our approach, Thompson Sampling via Fine-Tuning (ToSFiT) leverages the prior knowledge embedded in prompt-conditioned large language models, and incrementally adapts them toward the posterior. Theoretically, we derive a novel regret bound for a variational formulation of Thompson Sampling that matches the strong guarantees of its standard counterpart. Our analysis reveals the critical role of careful adaptation to the posterior probability of maximality--a principle that underpins our ToSFiT algorithm. Empirically, we validate our method on three diverse tasks: FAQ response refinement, thermally stable protein search, and quantum circuit design. We demonstrate that online fine-tuning significantly improves sample efficiency, with negligible impact on computational efficiency.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-16T11:00:05.265167",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **论文的核心贡献**：提出了一种名为“Thompson Sampling via Fine-Tuning (ToSFiT)”的新方法。这个方法的核心不是将LLM作为一个黑箱工具去解决某个领域问题，而是通过**在线微调**这一新的训练范式，让LLM本身学会并执行一种复杂的推理算法——Thompson Sampling。 - **是否符合目标**：这完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。论文的本质是增强LLM在复杂、非结构化空间中进行高效搜索和决策的内在能力，这是一种高级的通用推理和问题解决能力。它提出的是一种新的、可泛化的训练方法论，而非特定应用。 2.  **第二步：正面指标** - **核心概念**: 论文明确以 \"large language models\" 为研究对象。 - **能力方向**: 论文聚焦于 \"problem-solving\"，具体是通过贝叶斯优化和Thompson Sampling来提升模型在复杂任务中的决策和搜索效率，这属于高级推理能力的范畴。 - **训练方法**: 论文的核心是 \"Fine-Tuning\"，并且是 \"incrementally adapts them\" 的在线微调，这是一种新颖的训练范式，旨在让模型动态进化。 3.  **第三步：排除标准** - **特定应用领域**: 这是本论文最需要仔细辨析的一点。虽然论文在 \"thermally stable protein search\" 和 \"quantum circuit design\" 等具体任务上进行了验证，但这些任务是作为**实证案例**来证明其方法的通用性和有效性。论文的标题、摘要和理论分析都聚焦于ToSFiT这一**通用方法论**本身，而不是为了解决蛋白质或量子问题。因此，它不属于“将LLM作为工具应用到特定领域”的排除范畴。 - **其他排除项**: 论文不涉及多模态、模型基础设施或应用层面的安全水印等问题。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: ToSFiT可以被理解为一种赋予LLM通用“搜索和优化”能力的框架。它不是针对特定领域的智能体，而是提升模型内在的、通用的决策推理能力，因此应该保留。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种新的微调范式（ToSFiT），旨在通过训练来增强LLM在复杂决策空间中的通用推理和问题解决能力。尽管它在特定领域进行了实验验证，但其贡献是方法论层面的、通用的，直接服务于提升LLM的基础推理能力这一核心目标。因此，这篇论文高度符合你的研究范围。"
    },
    {
        "index": "#11",
        "title": "Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating Reward Hacking",
        "link": "/arxiv/2510.13694",
        "arxiv_id": "2510.13694",
        "authors": "Yuchun Miao, Liang Ding, Sen Zhang, Rong Bao, Lefei Zhang, Dacheng Tao",
        "summary": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in aligning language models with human values, reward hacking-or reward over-optimization-remains a major challenge. We identify two key obstacles to its mitigation: (1) reward misgeneralization in reward modeling, where reward models overfit to spurious, preference-irrelevant features; and (2) the lack of suitable regularization during RL optimization, as existing token-level constraints often over-restrict the policy space. To address these issues, we propose InfoRM, an information-theoretic reward modeling framework based on the Information Bottleneck (IB) principle, which filters out preference-irrelevant information to alleviate reward misgeneralization. We further observe that reward-hacked responses manifest as pronounced outliers in InfoRM's IB latent space, measured by Mahalanobis distance from the SFT-induced distribution. Motivated by this, we introduce IBL, a distribution-level regularization that penalizes such deviations, effectively expanding the optimization landscape while maintaining alignment. We prove that IBL is theoretically equivalent to the pessimistic RL objective within the IB latent space. Finally, we present Mahalanobis Outlier Probability (MOP), a statistical metric for quantifying reward hacking severity, enabling principled hyperparameter tuning and online mitigation such as early stopping. Extensive experiments across diverse LLMs and datasets confirm the generality of our findings, the effectiveness of InfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively advancing the state of RLHF.",
        "subjects": "Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-16T11:00:05.382194",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心是改进RLHF（Reinforcement Learning from Human Feedback）这一训练范式。RLHF是当前提升大语言模型（尤其是其与人类意图对齐和推理能力）的关键技术之一。论文针对RLHF中的核心难题——“奖励黑客”（reward hacking），提出了一套全新的信息论框架（InfoRM）和一种新的正则化方法（IBL）。其根本目标是使强化学习过程更稳定、更有效，从而让LLM在遵循人类指令时能学到更本质、更通用的能力，而不是通过钻奖励模型的空子来“作弊”。这直接关系到提升LLM的基础能力和训练质量，属于改进LLM本身通用能力的范畴，而非将其作为工具应用于特定领域。 **第二步：正面指标——论文是否包含相关主题？** 论文高度契合多个正面指标： - **核心概念**: 论文的研究对象是“Large language models (LLMs)”。 - **训练方法**: 论文的核心贡献是关于“Reinforcement Learning (RLHF)”的改进，提出了新的建模和优化方法。 - **能力方向**: 虽然摘要未直接使用“reasoning”一词，但RLHF的稳定性和有效性直接决定了模型在复杂任务（如多步推理、规划）中的表现。防止奖励模型被“黑客攻击”，意味着模型必须真正理解任务并生成高质量、有逻辑的回应，而不是利用表面特征获得高分。因此，这项工作是提升模型内在推理质量和问题解决能力的基础。 **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文完全不涉及任何排除标准领域。它没有讨论多模态、视觉，也没有针对医疗、化学等特定应用，更不涉及模型基础设施或部署优化。其焦点纯粹集中在LLM的训练算法层面。 **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/安全等特殊情况的讨论，因此无需进行此步判断。 **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种新的训练范式（基于信息论的RLHF框架），旨在解决RLHF中的根本性挑战（奖励黑客），从而提升大语言模型训练的稳定性和有效性。这项工作直接作用于LLM的核心能力培养过程，是提升其通用推理与问题解决能力的重要基础研究。因此，它完全符合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。"
    },
    {
        "index": "#16",
        "title": "What is the objective of reasoning with reinforcement learning?",
        "link": "/arxiv/2510.13651",
        "arxiv_id": "2510.13651",
        "authors": "Damek Davis, Benjamin Recht",
        "summary": "We show that several popular algorithms for reinforcement learning in large language models with binary rewards can be viewed as stochastic gradient ascent on a monotone transform of the probability of a correct answer given a prompt. In particular, the transformation associated with rejection sampling algorithms is the logarithm and that associated with the GRPO algorithm is the arcsine of the square root.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-16T11:00:05.383561",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断** 论文的核心是理论分析，而非应用。它深入探讨了用于提升LLM推理能力的强化学习算法的数学本质。论文标题直接点明主题“推理与强化学习的目标”，摘要内容则揭示了其核心贡献：将几种流行的LLM强化学习算法（如GRPO）统一为在“正确答案概率”的某种单调变换上进行梯度上升。这属于“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴，是典型的致力于改进LLM基础能力的方法论研究。它不是将LLM作为工具应用于特定领域，因此应予以保留。 2.  **第二步：正面指标** 论文完美命中了多个关键正面指标： *   **核心概念**: 明确研究 \"large language models\"。 *   **能力方向**: 标题和内容都直指 \"reasoning\"（推理）。 *   **训练方法**: 核心内容是关于 \"reinforcement learning\" (RL)，并具体分析了相关算法。 这些关键词的高度相关性，强有力地证明了论文与您研究课题的契合度。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准中的领域。它没有讨论多模态、视觉模型，也没有聚焦于任何特定应用领域（如医疗、化学等），更不涉及模型部署、水印或应用层面的安全问题。这进一步确认了其通用性和基础性。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况的讨论，因此无需进行特殊判断。其纯粹的理论分析性质使其定位非常清晰。 **最终决策**: 综合以上分析，这篇论文是一篇高质量的理论研究，其核心贡献是为“如何通过强化学习提升LLM的推理能力”这一前沿问题提供了深刻的数学洞察和统一的理论框架。它直接回应了您“提高大语言模型本身的通用推理能力”的核心目标，是一篇非常值得保留和深入研究的论文。"
    },
    {
        "index": "#72",
        "title": "Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games",
        "link": "/arxiv/2510.13060",
        "arxiv_id": "2510.13060",
        "authors": "Anupam Nayak, Tong Yang, Osman Yagan, Gauri Joshi, Yuejie Chi",
        "summary": "Reverse Kullback-Leibler (KL) divergence-based regularization with respect to a fixed reference policy is widely used in modern reinforcement learning to preserve the desired traits of the reference policy and sometimes to promote exploration (using uniform reference policy, known as entropy regularization). Beyond serving as a mere anchor, the reference policy can also be interpreted as encoding prior knowledge about good actions in the environment. In the context of alignment, recent game-theoretic approaches have leveraged KL regularization with pretrained language models as reference policies, achieving notable empirical success in self-play methods. Despite these advances, the theoretical benefits of KL regularization in game-theoretic settings remain poorly understood. In this work, we develop and analyze algorithms that provably achieve improved sample efficiency under KL regularization. We study both two-player zero-sum Matrix games and Markov games: for Matrix games, we propose OMG, an algorithm based on best response sampling with optimistic bonuses, and extend this idea to Markov games through the algorithm SOMG, which also uses best response sampling and a novel concept of superoptimistic bonuses. Both algorithms achieve a logarithmic regret in $T$ that scales inversely with the KL regularization strength $\\beta$ in addition to the standard $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret independent of $\\beta$ which is attained in both regularized and unregularized settings",
        "subjects": "Machine Learning, Computer Science and Game Theory, Optimization and Control, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-16T11:00:05.399115",
        "filter_reason": "这篇论文符合筛选标准，应当保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的本质是研究一种先进的训练范式——**KL正则化自我博弈**——的理论基础。摘要明确指出，这种范式在“对齐”任务中，将“预训练语言模型作为参考策略”，并取得了“显著的经验性成功”。论文的核心贡献并非将LLM应用于某个外部领域，而是深入分析这种**用于提升LLM自身能力的训练方法**的理论性质（如样本效率和后悔界）。这完全符合“改进LLM的基础能力、提出新的训练范式”这一核心保留标准。 2.  **第二步：正面指标** 论文命中了多个关键正面指标： *   **核心概念**: 明确提到了“pretrained language models”。 *   **训练方法**: 核心内容是关于“KL regularization”，这是一种在RLHF中至关重要的技术。同时，论文的背景是“self-play methods”，这是一种通过多智能体交互来迭代提升模型策略的强化学习方法。 *   **新兴范式**: “Self-play”是“llm-based agents”和“multi-agent systems”的一种具体实现形式，旨在通过内部竞争与合作来催生出更强的通用问题解决能力。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准中的领域。它研究的是抽象的数学模型（Matrix games, Markov games），而非特定应用领域（如医疗、化学），也未涉及多模态或应用层面的安全水印等。 4.  **第四步：处理特殊和模糊情况** 论文研究的“自我博弈”框架，正是一种**通用的智能体协作/对抗框架**。其目的不是解决某个特定领域的问题，而是通过理论分析和算法设计，来**增强LLM在这种框架下的策略学习效率**，从而提升其通用能力。因此，根据特殊情况的判断准则，应该保留。 **最终决策**: 尽管这篇论文高度理论化，没有直接展示一个在推理基准上得分更高的LLM，但它为我们所关心的**一种核心提升方法（自我博弈）提供了坚实的理论支撑**。对于一位顶尖的人工智能研究员而言，理解这些前沿训练范式的理论基础，是进一步设计出更强大、更高效算法的前提。因此，这篇论文直接服务于“提高大语言模型通用推理能力”这一核心目标，符合筛选要求。"
    }
]