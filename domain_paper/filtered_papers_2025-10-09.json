[
    {
        "index": "#2",
        "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
        "link": "/arxiv/2510.08529",
        "arxiv_id": "2510.08529",
        "authors": "Xiangyuan Xue, Yifan Zhou, Guibin Zhang, Zaibin Zhang, Yijiang Li, Chen Zhang, Zhenfei Yin, Philip Torr, Wanli Ouyang, Lei Bai",
        "summary": "Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.717417",
        "filter_reason": "这篇论文完全符合你的筛选要求。 **第一步：核心判断** 论文的核心是提出一种名为CoMAS（协同进化多智能体系统）的新颖框架。其本质并非将LLM应用于某个特定领域，而是致力于解决LLM智能体在预训练后如何“持续提升其能力”这一基础性问题。这是一种全新的训练范式（基于互动的协同进化），旨在通过方法论创新来增强LLM的基础能力，这完全符合“改进LLM的基础能力、提出新的训练范式”的保留标准。 **第二步：正面指标** 该论文命中了多个核心正面指标： - **核心概念**: 论文明确聚焦于 \"large language model (LLM)-based agents\"。 - **能力方向**: 其目标是让智能体 \"continually improve their capabilities\"（持续提升能力），这与提升通用推理、规划、问题解决能力直接相关。 - **训练方法**: 论文的核心机制之一是 \"reinforcement learning (RL)\"，并以此优化策略。同时，其主题 \"Self-evolution\"（自我进化）也是筛选标准中的关键概念。 - **新兴范式**: 论文的研究对象是 \"Multi-Agent Systems\"（多智能体系统），这正是筛选标准中提到的新兴范式。 **第三步：排除标准** 论文完全不涉及任何排除标准中的领域： - 摘要中没有提及任何多模态（如Vision）、特定应用领域（如Medical, Chemical）或模型基础设施。 - 论文讨论的是提升智能体能力的内在机制，而非水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** 论文恰好是“智能体”这一特殊情况的典型正面案例。它提出的是一个“通用的智能体协作框架”，目的是让智能体通过互相学习来实现“自我进化”，从而增强其“通用问题解决能力”。这与排除标准中“将智能体应用在特定领域”的情况完全不同，而是属于保留标准中“增强其逻辑、规划、多步推理等通用能力”的研究。 **最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种创新的、通用的训练框架，通过多智能体间的互动和强化学习，驱动LLM实现自我进化，从而提升其通用能力。这精准地契合了你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心研究目标。因此，应该保留。"
    },
    {
        "index": "#3",
        "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
        "link": "/arxiv/2510.08525",
        "arxiv_id": "2510.08525",
        "authors": "Wenjie Du, Li Jiang, Keda Tao, Xue Liu, Huan Wang",
        "summary": "Reasoning large language models exhibit complex reasoning behaviors through the extended chain-of-thought generation, creating unprecedented Key-Value (KV) cache overhead during the decoding phase. Existing KV cache compression methods underperform on reasoning models: token-dropping methods break reasoning integrity by discarding critical information, while head-reallocating methods mistakenly compress reasoning-critical heads since they are designed for retrieval tasks, resulting in significant performance degradation as compression rates increase. We hypothesize that KV heads exhibit functional heterogeneity in reasoning models-some heads are critical for chain-of-thought consistency while others are compressible. To validate and exploit this insight, we propose RLKV, a novel reasoning-critical head identification framework, which uses reinforcement learning to directly optimize the relationship between each head's cache usage and reasoning quality. As RLKV produces rewards from actual generated samples during training, it naturally identifies heads relevant to reasoning behaviors. We then allocate full KV cache to these heads while applying compressed constant KV cache to others for efficient inference. Our experiments reveal that only a small fraction of attention heads is essential for reasoning, enabling our KV compression approach to outperform baseline methods while achieving 20-50% cache reduction with near lossless performance compared to uncompressed results.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.718030",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是我的详细判断过程： 1.  **核心判断（第一步）：论文的本质是改进LLM的基础能力。** 这篇论文的核心贡献并非单纯的“KV缓存压缩”这一工程优化，而是提出了一种名为RLKV的**新方法论**，用于**识别和理解LLM内部哪些组件（注意力头）对推理能力至关重要**。它通过强化学习，直接将模型内部的“KV头”与“推理质量”这一核心能力挂钩进行优化。这本质上是对LLM推理机理的深度探索和利用，其最终目的是在提升推理效率的同时，无损地保留甚至增强模型的推理能力。因此，它并非将LLM作为工具应用于特定领域，也不是纯粹的模型基础设施研究，而是聚焦于提升LLM自身的通用推理能力。**此项符合保留标准。** 2.  **正面指标（第二步）：论文高度匹配相关主题。** - **核心概念**: 明确以\"Reasoning large language models\"为研究对象。 - **能力方向**: 论文的核心就是\"Reasoning\"，并深入探讨了\"chain-of-thought consistency\"（思维链一致性）等推理细节。 - **训练方法**: 提出的核心方法RLKV是基于\"reinforcement learning\" (RL)的。 - **新兴范式**: 虽然没有直接提Agent，但对模型内部推理过程的精细化分析和优化，是构建更强大智能体和问题解决系统的基础。 **此项满足多个关键正面指标。** 3.  **排除标准（第三步）：论文未触及任何排除领域。** 论文完全不涉及多模态、视觉、医疗、化学、机器人等特定应用领域，也未讨论水印、安全等应用层面的可靠性问题。**此项完全符合。** 4.  **特殊和模糊情况（第四步）：论文提供了增强模型内在可靠性的新方法。** 这篇论文可以被视为一种增强模型内在可解释性和可靠性的研究。它没有停留在表面现象，而是深入模型内部，通过一种新颖的RL框架，识别出对推理行为“至关重要”的头部。这种“识别关键组件”的方法，本身就是一种高级的可解释性分析，并且通过在推理时保留这些关键部分，直接提升了模型推理过程的**内在可靠性**和**质量**。这远超于应用层面的讨论，而是提出了提升模型通用推理质量的新方法。**此项符合保留标准。** **最终决策（第五步）：** 综合以上分析，这篇论文虽然以“KV缓存压缩”为技术应用点，但其**本质和核心贡献**在于提出了一种基于强化学习的框架，来**识别并保留对LLM通用推理能力至关重要的内部组件**。这是一种对LLM推理机理的深度探索和方法论创新，直接服务于“提高大语言模型本身的通用推理能力”这一核心目标。因此，这篇论文高度相关，应当保留。"
    },
    {
        "index": "#6",
        "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
        "link": "/arxiv/2510.08483",
        "arxiv_id": "2510.08483",
        "authors": "Shangqing Tu, Yaxuan Li, Yushi Bai, Lei Hou, Juanzi Li",
        "summary": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io/",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.719893",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心是提出一种名为 `DeepPrune` 的新框架，旨在解决现有“并行扩展”推理方法中的计算冗余问题。并行扩展通过生成多个思维链来增强LLM的推理能力，而本文的工作是对这一推理范式本身的优化和改进。它提出了一种新的方法论（包含一个judge模型和一个在线聚类算法），让LLM的推理过程更高效，这直接关联到“改进LLM的基础能力”和“提出新的训练范式”。 - **结论**: 论文的核心是方法论创新，旨在提升LLM的通用推理能力，而非将其应用于特定领域。**因此，符合保留标准。** 2.  **第二步：正面指标** - **核心概念**: 论文摘要中明确提到了 \"Large language models (LLMs)\"。 - **能力方向**: 论文的核心主题是 \"reasoning capabilities\"，并具体讨论了 \"Chain-of-Thought (CoT) traces\"，这是多步推理的典型代表。评估基准（AIME, GPQA）也直接指向了数学和科学推理能力。 - **新兴范式**: 论文聚焦于 \"Parallel scaling\" 这一新兴的推理增强范式。 - **结论**: 论文包含了多个关键正面指标，尤其是与LLM和推理能力直接相关的主题。**这进一步确认了其高度相关性。** 3.  **第三步：排除标准** - **多模态**: 论文内容完全不涉及视觉、多模态等领域。 - **特定应用领域**: 论文使用的评估基准是通用的数学和科学问题，而非医疗、化学、金融等特定领域。 - **模型可靠性（应用层面）**: 论文关注的是推理过程的效率和性能，而非水印、安全等应用层面的可靠性问题。 - **结论**: 论文不触及任何排除标准。**因此，不应被排除。** 4.  **第四步：处理特殊和模糊情况** - 本文不涉及智能体/工具使用或幻觉/安全等特殊情况，但其精神内核与筛选标准一致：它提出了一种通用的、领域无关的框架来增强LLM的内在能力（推理效率和质量）。 5.  **第五步：最终决策** - **综合分析**: 这篇论文精准地定位在“提升大语言模型通用推理能力”这一核心目标上。它没有将LLM作为工具去解决外部问题，而是深入到LLM的推理过程内部，通过提出一种新颖的框架来优化现有的推理范式（并行CoT），使其在保持高性能的同时更加高效。这种对推理方法论本身的改进，正是你研究课题所需要的前沿工作。 **最终判断**: 这篇论文是关于如何让LLM的通用推理过程（特别是并行推理）变得更高效、更强大的方法论研究，完全符合你的筛选要求。"
    },
    {
        "index": "#5",
        "title": "Neologism Learning for Controllability and Self-Verbalization",
        "link": "/arxiv/2510.08506",
        "arxiv_id": "2510.08506",
        "authors": "John Hewitt, Oyvind Tafjord, Robert Geirhos, Been Kim",
        "summary": "Humans invent new words when there is a rising demand for a new useful concept (e.g., doomscrolling). We explore and validate a similar idea in our communication with LLMs: introducing new words to better understand and control the models, expanding on the recently introduced neologism learning. This method introduces a new word by adding a new word embedding and training with examples that exhibit the concept with no other changes in model parameters. We show that adding a new word allows for control of concepts such as flattery, incorrect answers, text length, as well as more complex concepts in AxBench. We discover that neologisms can also further our understanding of the model via self-verbalization: models can describe what each new word means to them in natural language, like explaining that a word that represents a concept of incorrect answers means ``a lack of complete, coherent, or meaningful answers...'' To validate self-verbalizations, we introduce plug-in evaluation: we insert the verbalization into the context of a model and measure whether it controls the target concept. In some self-verbalizations, we find machine-only synonyms: words that seem unrelated to humans but cause similar behavior in machines. Finally, we show how neologism learning can jointly learn multiple concepts in multiple words.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.719208",
        "filter_reason": "这篇论文符合你的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“新造词学习”的新方法。这并非将LLM应用于某个特定领域，而是提出了一种全新的、轻量级的训练/微调范式，通过引入新词来直接控制和理解模型本身的行为。这种方法旨在增强模型的基础能力——即可控性和可解释性。这两者是构建更强大、更可靠的通用推理模型的重要基石。因此，从本质上讲，这篇论文致力于改进LLM本身的基础能力，应予以保留。 2.  **第二步：正面指标** - 论文的核心概念明确是 **Large language models (LLMs)**。 - 论文涉及的能力方向包括 **problem-solving**（通过控制概念来解决问题），并与 **reasoning** 间接相关。例如，控制“错误答案”这一概念，直接关系到模型推理输出的质量和可靠性。 - 论文提出的方法是一种新颖的训练方式，虽然不是直接的RL，但它属于增强模型能力的训练方法论探索。 3.  **第三步：排除标准** - 论文完全不涉及多模态与视觉。 - 论文的应用场景（如控制奉承、文本长度）是通用的、与领域无关的，并非聚焦于医疗、化学等特定应用领域。 - 论文虽然触及“错误答案”，但其目标是提供一种控制机制，而非研究应用层面的安全或水印技术。 4.  **第四步：处理特殊和模糊情况** - 这是判断的关键。论文的“自我言语化”部分，让模型用自然语言解释新词的含义，这完全符合“**如果论文提出一种新方法来……增强模型内在的可解释性……从而提升模型的通用可靠性和推理质量，应该保留**”这一标准。通过理解模型如何“定义”一个概念（如“不正确”），我们获得了洞察其内部逻辑和推理过程的窗口。这种对模型内在世界的探索，是提升其通用推理能力的重要研究路径，远超应用层面的讨论。 5.  **第五步：最终决策** 综合分析，这篇论文虽然不像思维链那样直接提出一种“推理算法”，但它从更基础的层面——“可控性”和“可解释性”——入手，提出了一种新颖的范式来增强我们对LLM的理解和操控能力。能够精确地注入、控制并让模型“解释”一个抽象概念，是构建具有高级通用推理能力模型的关键一步。因此，这篇论文与“提高大语言模型本身的通用推理能力”这一核心目标高度相关，应当被筛选出来。"
    },
    {
        "index": "#9",
        "title": "Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT",
        "link": "/arxiv/2510.08404",
        "arxiv_id": "2510.08404",
        "authors": "Noor Ul Zain, Mohsin Raza, Ahsan Adeel",
        "summary": "We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$ is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2 (124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude greater training efficiency on 10M tokens, demonstrating highly sample efficient pretraining. Using the BabyLM challenge evaluation pipeline across complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out of 7 metrics in both cases. These results suggest the need to rethink prevailing deep learning paradigms and associated scaling laws.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.726765",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。判断依据如下： 1.  **核心判断（第一步）：符合保留标准。** 论文的核心贡献是提出了一种名为Co$^4$的**全新模型架构**。它并非将现有LLM作为工具应用于特定领域，而是从根本上探索如何构建一个更高效、更强的语言模型。提出新的模型架构属于“改进LLM的基础能力”和“提出新的训练范式”的范畴，是提升模型通用能力的根本性方法之一。论文通过在通用基准（SuperGLUE）上超越GPT-2等模型，证明了其架构的有效性，这直接关联到提升LLM的内在能力。 2.  **正面指标（第二步）：满足关键指标。** 虽然摘要没有直接使用\"reasoning\"一词，但其评估标准是**SuperGLUE**。SuperGLUE是公认的、极具挑战性的通用语言理解基准，其下的许多任务（如BoolQ、COPA、WinoGrande）都深度依赖**逻辑推理、常识推理和问题解决能力**。因此，在SuperGLUE上取得优异表现，是证明模型具备强大**通用问题解决能力**的有力证据。论文的核心是关于语言模型（LLMs）的改进，这一点也与“核心概念”指标相符。 3.  **排除标准（第三步）：不涉及排除领域。** 论文的研究焦点是文本语言模型，没有涉及多模态、视觉或特定应用领域（如医疗、化学）。它也非关注模型部署或安全等应用层面的可靠性问题。 4.  **特殊和模糊情况（第四步）：不适用。** 该论文不涉及智能体、工具使用、幻觉或可解释性等特殊话题。 **最终决策（第五步）：** 该论文的本质是通过**创新模型架构**这一基础性方法，提升了模型在通用语言理解基准上的表现。SuperGLUE的高分直接映射了模型在复杂推理任务上的能力。论文中提到的“rethink prevailing deep learning paradigms and associated scaling laws”也表明其研究目标是探索构建更强通用能力LLM的根本路径，这与我寻找“致力于提高大语言模型（LLM）本身『通用推理能力』”的论文的核心目标高度一致。因此，应判定为符合。"
    },
    {
        "index": "#11",
        "title": "On the Relationship Between the Choice of Representation and In-Context Learning",
        "link": "/arxiv/2510.08372",
        "arxiv_id": "2510.08372",
        "authors": "Ioana Marinescu, Kyunghyun Cho, Eric Karl Oermann",
        "summary": "In-context learning (ICL) is the ability of a large language model (LLM) to learn a new task from a few demonstrations presented as part of the context. Past studies have attributed a large portion of the success of ICL to the way these in-context demonstrations are represented, particularly to how labels are represented in classification tasks. On the other hand, observations of the learning capacity of ICL (i.e., the extent to which more in-context demonstrations can lead to higher performance) have been mixed, and ICL is often thought to occur only under specific conditions. The interaction between these two aspects in ICL, representation and learning, has not been studied in depth until now. We hypothesize that they are largely independent of one another, such that the representation of demonstrations determines the baseline accuracy of ICL, while learning from additional demonstrations improves only on top of this baseline. We validate this hypothesis by developing an optimization algorithm that can enumerate a spectrum of possible label sets (representations) varying in semantic relevance. We then perform ICL with varying numbers of in-context demonstrations for each of these label sets. We observed that learning happens regardless of the quality of the label set itself, although its efficiency, measured by the slope of improvement over in-context demonstrations, is conditioned on both the label set quality and the parameter count of the underlying language model. Despite the emergence of learning, the relative quality (accuracy) of the choice of a label set (representation) is largely maintained throughout learning, confirming our hypothesis and implying their orthogonality. Our work reveals a previously underexplored aspect of ICL: the independent effects of learning from demonstrations and their representations on ICL performance.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.727681",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质** 这篇论文的本质是**对大语言模型（LLM）一项基础能力——上下文学习——的内在机理进行深入剖析和探索**。它没有将LLM作为工具去解决某个特定领域的问题，而是聚焦于LLM本身。论文的核心贡献是揭示了ICL中“表征”和“学习”两个关键因素的独立性。这种对基础能力的机理研究，正是提升LLM通用推理能力的基石。理解了ICL为何有效以及其局限性，才能更好地设计和优化未来的方法来增强这种能力。因此，这篇论文通过了核心判断，应予以保留。 2.  **第二步：正面指标** - **核心概念**: 论文明确以“large language model (LLM)”为研究对象。 - **能力方向**: 论文研究的“In-context learning (ICL)”是LLM实现快速学习和推理的关键范式，属于通用问题解决和学习能力的重要组成部分。虽然摘要未直接使用“reasoning”一词，但ICL本身就蕴含了一定的推理和学习过程。 - **训练方法**: 虽然没有提出新的训练范式，但其研究结论对未来如何设计更好的提示策略以激发模型潜力至关重要，这与优化模型表现的目标一致。 3.  **第三步：排除标准** - 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）等排除领域。 4.  **第四步：处理特殊和模糊情况** - 本论文情况不属于特殊模糊类别，其研究焦点非常清晰，即LLM的基础认知机制。 5.  **第五步：最终决策** 综合来看，这篇论文是一篇高质量的、专注于LLM基础能力机理的研究。它通过严谨的实验设计和分析，揭示了ICL的一个重要内在规律，即表征与学习的正交性。这种基础性的洞见，对于整个社区理解并最终提升LLM的通用推理和学习能力具有直接的指导意义。它完全符合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。"
    },
    {
        "index": "#2",
        "title": "Opponent Shaping in LLM Agents",
        "link": "/arxiv/2510.08255",
        "arxiv_id": "2510.08255",
        "authors": "Marta Emili Garcia Segura, Stephen Hailes, Mirco Musolesi",
        "summary": "Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Multiagent Systems",
        "date": "2025-10-09",
        "category": "cs.MA",
        "crawl_time": "2025-10-10T11:00:05.950858",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为 `ShapeLLM` 的新方法，用于实现“对手塑形”。这并非将LLM应用于某个特定领域，而是致力于提升LLM作为智能体在多智能体交互环境中的**高级策略推理能力**。具体来说，它研究的是LLM智能体如何通过交互来影响和塑造其他智能体的学习过程与行为策略。这是一种比单步逻辑或数学推理更复杂的、涉及博弈论、规划和长期影响的通用推理能力。因此，论文的本质是改进LLM的基础能力，属于“增强其逻辑、规划、多步推理等通用能力”的范畴，与“智能体协作框架”的研究方向高度一致。 2.  **第二步：正面指标** 论文与多个正面指标高度匹配： *   **核心概念**: 论文的核心研究对象就是 `Large language models (LLMs)`。 *   **能力方向**: 论文研究的“对手塑形”是 `reasoning` 和 `planning` 的延伸和深化，属于在复杂动态环境中的问题解决能力。 *   **训练方法**: 论文明确提出 `ShapeLLM` 是对 `model-free OS methods`（一种强化学习方法）的改造，以适应Transformer架构，与 `reinforcement learning (RL)` 紧密相关。 *   **新兴范式**: 论文的研究焦点是 `multi-agent systems`，其提出的 `ShapeLLM` 是一个通用的 `llm-based agents` 框架，旨在增强智能体的通用能力。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： *   **多模态与视觉**: 论文仅涉及基于文本的LLM智能体，未提及任何视觉或多模态内容。 *   **特定应用领域**: 论文使用的实验环境是“囚徒困境”等博弈论游戏。这些是用于测试和衡量智能体通用策略能力的**标准化测试平台**，而非生物、医疗、化学等特定应用领域。研究目标是揭示LLM的通用能力，而非解决某个领域的具体问题。 *   **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的典型范例。`ShapeLLM` 是一个通用方法论，旨在提升LLM智能体在多智能体交互中的策略水平，而不是将其局限于某个特定领域。因此，完全符合保留条件。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种新的方法论（`ShapeLLM`），用以增强LLM在多智能体环境下的高级策略推理和影响力。它直接触及了LLM通用推理能力的前沿，即如何让模型不仅能自己推理，还能理解和影响其他智能体的推理过程。这项研究对于提升LLM的自主性和在复杂现实世界中的表现具有基础性意义，与“提高大语言模型本身的通用推理能力”这一核心目标高度契合。因此，最终判断为 **True**。"
    },
    {
        "index": "#15",
        "title": "Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window",
        "link": "/arxiv/2510.08276",
        "arxiv_id": "2510.08276",
        "authors": "Qiaoyu Tang, Hao Xiang, Le Yu, Bowen Yu, Yaojie Lu, Xianpei Han, Le Sun, WenJuan Zhang, Pengbo Wang, Shixuan Liu, Zhenru Zhang, Jianhong Tu, Hongyu Lin, Junyang Lin",
        "summary": "While recent advances in reasoning models have demonstrated cognitive behaviors through reinforcement learning, existing approaches struggle to invoke deep reasoning capabilities in multi-turn agents with long-horizon interactions. We propose DeepMiner, a novel framework that elicits such abilities by introducing high-difficulty training tasks and dynamic context window. DeepMiner presents a reverse construction method to generate complex but verifiable question-answer pairs from authentic web sources, which ensures the challenge and reliability of training data while injecting cognitive capabilities into multi-turn reasoning scenarios. We further design an elegant yet effective dynamic context management strategy for both training and inference, utilizing sliding window mechanisms while eliminating the dependency on external summarization models, thereby efficiently empowering the model to handle continuously expanding long-horizon contexts. Through reinforcement learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial performance improvements across multiple search agent benchmarks. DeepMiner attains 33.5% accuracy on BrowseComp-en, surpassing the previous best open-source agent by almost 20 percentage points, and demonstrates consistent improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our dynamic context management enables sustained interactions of nearly 100 turns within standard 32k context length, effectively addressing the context limitations that constrain existing multi-turn interaction systems.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.729819",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是基于筛选标准的详细判断过程： 1.  **第一步：核心判断** 论文的核心本质是提出一种名为`DeepMiner`的新框架，用于**训练**大语言模型，使其成为具备深度推理能力的搜索智能体。其核心贡献包括两点：1）通过“逆向构建方法”生成高难度训练数据，以激发模型的深度推理能力；2）设计了“动态上下文管理策略”，使模型能够处理超长交互中的信息。这完全属于“改进LLM的基础能力”和“提出新的训练范式”的范畴，其目标是增强模型在通用场景下的多步、长期推理能力，而非将其应用于特定领域。 2.  **第二步：正面指标** 论文摘要中包含了多个强烈的正面指标： - **核心概念**: 论文基于Qwen3-32B模型进行研究，明确属于大语言模型（LLMs）范畴。 - **能力方向**: 标题和摘要反复强调“Deep Search Agents”（深度搜索智能体）、“deep reasoning capabilities”（深度推理能力）、“multi-turn reasoning scenarios”（多轮推理场景），这些都与“reasoning”和“problem-solving”高度相关。 - **训练方法**: 明确提到通过“reinforcement learning”（强化学习）来训练模型。 - **新兴范式**: 论文的研究对象是“search agent”（搜索智能体），并提出了通用的智能体框架和交互策略，这与“llm-based agents”和“deep research”主题完全契合。 3.  **第三步：排除标准** 论文完全没有触及任何排除标准领域： - 它不涉及多模态、视觉或扩散模型。 - 它的应用场景是通用的网络搜索和问题回答，而不是医疗、化学、生物等特定领域。 - 它的研究重点是提升推理能力，而非水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的`DeepMiner`框架是一个**通用的智能体训练和推理框架**，旨在提升智能体在“深度搜索”这一通用任务上的表现。它不是为某个特定垂直领域（如化学实验自动化）设计的智能体，因此完全符合保留条件。其动态上下文窗口管理策略更是一种增强通用问题解决能力的方法论创新。 **总结**: 该论文的本质是方法论创新，其核心贡献`DeepMiner`框架和动态上下文管理策略，都是为了**从训练和推理机制上直接提升LLM的通用深度推理和长程规划能力**。它精准地命中了你研究目标中关于“提高大语言模型本身的『通用推理能力』”的核心，因此应被筛选并保留。"
    },
    {
        "index": "#6",
        "title": "Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization",
        "link": "/arxiv/2510.07517",
        "arxiv_id": "2510.07517",
        "authors": "Hyeong Kyu Choi, Xiaojin Zhu, Yixuan Li",
        "summary": "Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning by letting multiple agents exchange answers and then aggregate their opinions. Yet recent studies reveal that agents are not neutral: they are prone to identity-driven sycophancy and self-bias, uncritically adopting a peer's view or stubbornly adhering to their own prior output, undermining the reliability of debate. In this work, we present the first principled framework that joins sycophancy and self-bias to mitigate and quantify identity bias in MAD. First, we formalize the debate dynamics as an identity-weighted Bayesian update process. Second, we propose response anonymization: by removing identity markers from prompts, agents cannot distinguish \"self\" from \"peer\", which forces equal weights on agent identity, thereby reducing bias. Third, we define the Identity Bias Coefficient (IBC), a principled metric that measures how often an agent follows a peer versus itself. Empirical studies across multiple models, datasets and debate rounds confirm that identity bias is widespread, with sycophancy far more common than self-bias. Our findings highlight the need to \"mask\" identity to ensure that MAD systems reason based on content rather than source identity. Code is released in https://github.com/deeplearning-wisc/MAD-identity-bias.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-08",
        "category": "cs.MA",
        "crawl_time": "2025-10-10T11:00:05.952071",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于提出一种方法来提升一种重要的大语言模型推理范式——多智能体辩论——的质量和可靠性。 以下是根据你的筛选标准进行的详细判断过程： 1.  **第一步：核心判断** - 论文的本质是**改进LLM的基础能力**。它没有将LLM应用于医疗、化学等特定领域，而是聚焦于“多智能体辩论”这一旨在提升LLM推理能力的通用方法论。 - 论文识别出该方法论中的一个关键缺陷（身份偏见），并提出了一种新颖的解决方案（响应匿名化）来修复它。这直接关系到如何让LLM通过协作进行更高质量、更可靠的推理，属于提升模型“通用推理能力”的范畴。 - 因此，根据第一步标准，应**保留**。 2.  **第二步：正面指标** - **核心概念**: 论文明确以 \"Large language models (LLMs)\" 为研究对象。 - **能力方向**: 论文开篇即点明其目标是 \"improve large language model (LLM) reasoning\"，后续所有工作都围绕如何提升辩论这一推理过程的质量展开。 - **新兴范式**: 论文的核心是 \"Multi-agent debate (MAD)\"，这正是你筛选标准中提到的 \"llm-based agents\" 和 \"multi-agent systems\" 的前沿研究方向。 - 论文在多个正面指标上高度吻合。 3.  **第三步：排除标准** - 论文不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施。 - 虽然提到了 \"reliability\"，但它是在提升通用推理过程质量的语境下讨论的，而非应用层面的水印、安全等问题。 - 因此，未触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架...来增强LLM的通用问题解决能力”的绝佳范例。它研究的不是“用于化学实验的智能体”，而是通用的“多智能体辩论”框架本身，旨在让这个框架变得更好、更通用。 - **幻觉/可解释性/安全**: 论文研究的“身份偏见”是一种系统性推理缺陷，类似于“偏见”或“幻觉”。它提出的“匿名化”方法，是一种**新的技术手段来减少这种内在缺陷**，从而提升模型的通用推理质量。其目标是让推理“基于内容而非来源身份”，这与提升模型内在可靠性和推理逻辑性的目标完全一致。因此，它属于应该保留的情况。 5.  **第五步：最终决策** - 综合来看，这篇论文精准地聚焦于提升LLM的一种通用推理范式（多智能体辩论）。它不仅发现了该范式中的一个重要问题（身份偏见），还提出了创新的解决方案（匿名化）和量化指标（IBC），其最终目标是让LLM的推理过程更加客观、理化和可靠。这完全契合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，这篇论文应该被筛选出来。"
    },
    {
        "index": "#16",
        "title": "Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling",
        "link": "/arxiv/2510.08245",
        "arxiv_id": "2510.08245",
        "authors": "Jannek Ulm, Kevin Du, Vésteinn Snæbjarnarson",
        "summary": "Large language models (LLMs) are trained on huge amounts of textual data, and concerns have been raised that the limits of such data may soon be reached. A potential solution is to train on synthetic data sampled from LLMs. In this work, we build on this idea and investigate the benefits of contrastive decoding for generating synthetic corpora. In a controlled setting, we experiment with sampling corpora using the relative difference between a good and bad model trained on the same original corpus of 100 million words. By amplifying the signal from a model that has better performance, we create a synthetic corpus and mix it with the original training data. Our findings show that training on a mixture of synthesized and real data improves performance on the language modeling objective and a range of downstream tasks. In particular, we see that training with a mix of synthetic data from contrastive decoding benefits tasks that require more reasoning skills, while synthetic data from traditional sampling helps more on tasks dependent on surface level linguistic capabilities.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.730306",
        "filter_reason": "这篇论文符合您的筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 这篇论文的本质是提出一种**新的训练范式**。它没有将LLM应用于特定领域，而是聚焦于如何通过改进训练数据本身来提升模型能力。其核心贡献是“使用对比解码生成合成数据”，这是一种增强LLM训练过程的基础方法论。最关键的是，论文明确指出，这种方法生成的数据能够**“benefits tasks that require more reasoning skills”（有益于需要更多推理技能的任务）**。这直接命中了您核心目标中“提高大语言模型本身的『通用推理能力』”。因此，根据第一步的核心判断标准，该论文应被**保留**。 2.  **第二步：正面指标** - **核心概念**: 论文明确围绕 \"Large language models (LLMs)\" 展开。 - **能力方向**: 论文的核心发现是其方法对 \"reasoning skills\" 有益，这与您的筛选方向高度一致。 - **训练方法**: 论文提出了一种新颖的数据生成方法，这属于广义上的“新的训练范式”，虽然不是RL或进化，但同样旨在从数据层面优化模型。 3.  **第三步：排除标准** - 论文未涉及多模态、视觉、化学、医疗、机器人等任何特定应用领域。 - 论文也未讨论模型基础设施、部署优化或水印、安全等应用层面的可靠性问题。 - 因此，该论文未触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - 该论文不涉及智能体/工具使用，也不涉及幻觉/可解释性/安全等特殊主题，因此无需进行特殊判断。 5.  **第五步：最终决策** 综合分析，这篇论文的核心贡献是提出了一种通用的、基础性的方法（对比解码生成合成数据），用以改进LLM的训练过程。其最关键的实验结果证明，该方法能够有效提升模型在需要推理能力任务上的表现。这完全符合您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#18",
        "title": "Investigating Counterclaims in Causality Extraction from Text",
        "link": "/arxiv/2510.08224",
        "arxiv_id": "2510.08224",
        "authors": "Tim Hagen, Niklas Deckers, Felix Wolter, Harrisen Scells, Martin Potthast",
        "summary": "Research on causality extraction from text has so far almost entirely neglected counterclaims. Existing causality extraction datasets focus solely on \"procausal\" claims, i.e., statements that support a relationship. \"Concausal\" claims, i.e., statements that refute a relationship, are entirely ignored or even accidentally annotated as procausal. We address this shortcoming by developing a new dataset that integrates concausality. Based on an extensive literature review, we first show that concausality is an integral part of causal reasoning on incomplete knowledge. We operationalize this theory in the form of a rigorous guideline for annotation and then augment the Causal News Corpus with concausal statements, obtaining a substantial inter-annotator agreement of Cohen's $\\kappa=0.74$. To demonstrate the importance of integrating concausal statements, we show that models trained without concausal relationships tend to misclassify these as procausal instead. Based on our new dataset, this mistake can be mitigated, enabling transformers to effectively distinguish pro- and concausality.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.736470",
        "filter_reason": "这篇论文符合我的研究范围。 我的判断过程如下： **第一步：核心判断** 这篇论文的本质是研究如何提升模型在“因果抽取”任务上的能力。因果抽取是逻辑推理和自然语言理解中的一个核心基础任务。论文指出现有模型和数据集的一个关键缺陷：无法处理“反驳性因果主张”。论文的核心贡献是构建了一个新的数据集，并证明了使用该数据集可以训练模型（transformers）有效地区分支持性和反驳性的因果关系。这并非将LLM应用于某个特定领域（如金融、医疗），而是直接针对模型在执行一项基础性推理任务时的能力短板进行改进，旨在提升模型本身对复杂逻辑关系的理解能力。因此，它通过了第一步的核心判断。 **第二步：正面指标** 论文明确命中了多个关键的正面指标： - **能力方向**: 论文的核心主题是“causality extraction”（因果抽取），这直接隶属于“reasoning”（推理）和“logical reasoning”（逻辑推理）的范畴。论文标题和摘要中反复强调的“counterclaims”（反驳主张）更是高级逻辑推理能力的体现。 - **核心概念**: 论文的实验对象是“transformers”，这是大语言模型（LLMs）的基础架构。研究如何让transformers更好地进行因果推理，本质上就是在研究如何提升LLM的推理能力。 **第三步：排除标准** 论文完全避开了所有排除标准： - **多模态与视觉**: 论文仅处理文本，不涉及任何视觉或多模态内容。 - **特定应用领域**: 论文的数据来源是“Causal News Corpus”，属于通用新闻文本领域，并非医疗、化学、机器人等高度专业化的特定应用领域。其研究目标是提升通用的文本理解能力，而非解决特定领域的业务问题。 - **模型可靠性（应用层面）**: 论文不涉及水印、安全等议题。 **第四步：处理特殊和模糊情况** 本论文的情况不涉及智能体/工具使用，也不涉及幻觉/安全等模糊议题。其研究目标非常清晰，即通过改进数据和训练方法来增强模型的逻辑推理能力。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于通过构建一个包含“反驳性因果”的新数据集，弥补了现有模型在复杂逻辑推理上的一个重要盲点。这项工作直接推动了LLM在“因果推理”这一通用能力上的进步，完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，最终判断为符合要求。"
    },
    {
        "index": "#21",
        "title": "Memory Retrieval and Consolidation in Large Language Models through Function Tokens",
        "link": "/arxiv/2510.08203",
        "arxiv_id": "2510.08203",
        "authors": "Shaohua Zhang, Yuan Lin, Hang Li",
        "summary": "The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.737978",
        "filter_reason": "这篇论文完全符合您的筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——论文本质** 这篇论文的本质是**基础理论研究**，旨在揭示大语言模型（LLM）内部工作机制的核心环节——记忆的检索与巩固。它并非将LLM作为工具应用于某个特定领域，也不是关于模型的基础设施或部署优化。其研究的“功能词假说”直接触及了LLM如何激活和运用其内部知识，这是实现包括推理在内的各种高级能力的**基础**。因此，该论文的核心是探索和解释LLM的基础能力，完全符合保留条件。 2.  **第二步：正面指标** 论文标题和摘要中多次出现核心概念“Large language models (LLMs)”。更重要的是，摘要中明确指出，LLM通过记忆检索实现的能力包括“**reasoning**”（推理）。虽然论文的重点是解释记忆机制，但它的出发点是为了理解和解释这些高级能力的来源。因此，它与“reasoning”这一能力方向高度相关。 3.  **第三步与第四步：排除标准与特殊情况** 该论文不涉及任何第三步中的排除领域（如多模态、医疗、化学、机器人等）。同时，它属于第四步中提到的特殊且应保留的情况：**增强模型内在的可解释性**。通过提出“功能词假说”并用实验证据支持，论文旨在阐明LLM“黑箱”内部的一个重要工作机制。这种基础性的可解释性研究，能够帮助研究者更好地理解模型为何能（或为何不能）进行有效推理，从而为未来设计出推理能力更强的模型提供理论指导。 5.  **第五步：最终决策** 综合来看，这篇论文的核心贡献是提出了一个关于LLM记忆检索与巩固机制的理论假说，并提供了实验支持。虽然它没有像思维链那样直接提出一种新的推理*方法*，但它深刻地揭示了LLM实现推理所依赖的**底层知识操作过程**。理解功能词如何激活特定知识、引导预测，就是理解推理链条中“回忆相关知识”这一环节是如何发生的。这种对模型基础能力的深入探索，是提升其通用推理能力的必经之路，与您“致力于提高大语言模型本身的通用推理能力”的核心目标高度契合。 因此，**结果为 True**。"
    },
    {
        "index": "#22",
        "title": "Training-Free Group Relative Policy Optimization",
        "link": "/arxiv/2510.08191",
        "arxiv_id": "2510.08191",
        "authors": "Yuzheng Cai, Siqi Cai, Yuchen Shi, Zihan Xu, Lichao Chen, Yulei Qin, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Yong Mao, Ke Li, Xing Sun",
        "summary": "Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.738579",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心贡献是提出了一种名为“Training-Free Group Relative Policy Optimization (Training-Free GRPO)”的新方法。该方法旨在通过一种轻量级、无需更新模型参数的方式，来增强大语言模型智能体的性能。其本质是改变和引导模型的输出分布，从而提升其解决问题的能力。这直接触及了LLM的基础能力优化，而非将其作为工具应用于特定领域。因此，根据第一步标准，应**保留**。 2.  **第二步：正面指标——论文高度相关。** 论文摘要中包含了多个关键的正面指标： *   **核心概念**: 明确提到了 \"Large Language Model (LLM) agents\"。 *   **能力方向**: 实验验证了在 \"mathematical reasoning\"（数学推理）任务上的效果，这正是通用推理能力的核心组成部分。 *   **训练方法**: 论文与 \"Reinforcement Learning (RL)\" 和 \"Group Relative Policy Optimization (GRPO)\" 紧密相关，提出了一种新的优化范式。 *   **新兴范式**: 论文的研究背景是 \"LLM agents\"，并探讨了如何提升其性能，这与智能体框架的研究方向一致。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文内容完全聚焦于LLM本身的方法论，没有涉及多模态、视觉、医疗、化学等特定应用领域，也未讨论水印、安全等应用层面的可靠性问题。因此，根据第三步标准，不应**排除**。 4.  **第四步：处理特殊和模糊情况——论文属于应保留的范畴。** 论文讨论了LLM智能体和工具使用。它并非提出一个用于特定领域（如化学）的智能体，而是提出一种**通用的**方法来提升智能体在整合外部工具和策略时的性能，从而增强其通用问题解决能力。这完全符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”的原则。 5.  **第五步：最终决策。** 综合以上分析，这篇论文的核心是提出一种创新的、轻量级的优化方法（Training-Free GRPO），旨在直接提升LLM智能体的通用推理和问题解决能力（以数学推理和域外性能为证）。它不涉及特定应用领域，完全符合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为**True**。"
    },
    {
        "index": "#24",
        "title": "ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code",
        "link": "/arxiv/2510.08163",
        "arxiv_id": "2510.08163",
        "authors": "Jian Xie, Zhendong Chu, Aoxiao Zhong, Kai Zhang, Mingzhe Han, Xin Fang, Jialie Shen, Qingsong Wen",
        "summary": "Large Reasoning Models (LRMs) often suffer from the ``over-thinking'' problem, generating unnecessarily long reasoning on simple tasks. Some strategies have been proposed to mitigate this issue, such as length penalties or routing mechanisms, but they are typically heuristic and task-specific, lacking a general framework for adaptive reasoning. In this paper, we present ARM2, a unified model that adaptively balances reasoning performance and efficiency across multiple formats through a reinforcement learning framework augmented with length-aware optimization. Beyond conventional natural language inference, ARM2 integrates vision understanding, extending its applicability to multimodal. Moreover, ARM2 integrates executable code into reasoning, enabling substantial reductions in token cost while preserving task performance compared to long CoT. Experiments demonstrate that ARM2 achieves performance on par with traditional reasoning models trained with GRPO, while reducing token usage by over 70% on average. We further conduct extensive analyses to validate the effectiveness of ARM2 and the soundness of its design.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.739550",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是解决大型推理模型（LRM）的“过度思考”问题。它提出了一种名为ARM2的统一模型，通过强化学习框架来**自适应地平衡推理性能和效率**。这直接触及了大语言模型推理能力的核心——如何让模型在保证效果的同时，更高效、更智能地进行推理。这并非将LLM作为工具应用于特定领域，而是致力于改进LLM的**基础推理范式**，因此符合“保留”标准。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **能力方向**: 标题和摘要都明确聚焦于“Reasoning”。 *   **训练方法**: 明确提出了“reinforcement learning framework”。 *   **新兴范式**: 集成了“executable code”到推理过程中，这是一种典型的“tool use”形式，用以增强模型的问题解决能力。 这些指标都强烈表明该论文与“提高LLM通用推理能力”的研究目标高度相关。 3.  **第三步：排除标准** 论文摘要中提到了“vision understanding”，这触及了“多模态与视觉”的排除标准。然而，需要判断这是否是论文的**主要焦点**。从摘要的行文逻辑来看，“Beyond conventional natural language inference, ARM2 integrates vision understanding...”表明视觉理解是其核心推理框架的一个**扩展能力**，用以证明该框架的通用性，而不是论文本身的研究主体。论文的核心贡献是“Adaptive Reasoning Model”，而非一个新颖的视觉模型。 4.  **第四步：处理特殊和模糊情况** 这篇论文恰好属于“智能体/工具使用”和“多模态”的模糊情况。 *   **工具使用**: 论文将“可执行代码”集成到推理中以降低成本，这完全符合“提出一种通用的工具使用方法来增强LLM的通用问题解决能力”的保留条件。 *   **多模态**: 尽管包含了视觉，但其目的是为了展示其提出的**通用推理框架**的广泛适用性。研究的本质是“如何让推理过程自适应”，而不是“如何让模型看懂图片”。因此，这应被视为对通用推理能力的一种增强和验证，而非一个特定领域的应用。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种通过强化学习来优化LLM推理过程效率的新框架（ARM2），旨在解决“过度思考”这一通用推理瓶颈。虽然论文将视觉理解作为其框架能力的一个展示，但这并未改变其研究本质是关于**通用推理方法论**的事实。其目标与我的研究课题“提高大语言模型（LLM）本身的『通用推理能力』”高度契合。因此，最终判断为保留。"
    },
    {
        "index": "#32",
        "title": "The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models",
        "link": "/arxiv/2510.08098",
        "arxiv_id": "2510.08098",
        "authors": "Sherzod Hakimov, Roland Bernard, Tim Leiber, Karl Osswald, Kristina Richert, Ruilin Yang, Raffaella Bernardi, David Schlangen",
        "summary": "Negotiation is a fundamental challenge for AI agents, as it requires an ability to reason strategically, model opponents, and balance cooperation with competition. We conduct the first comprehensive study systematically evaluating the effect of (LLM-)reasoning on the negotiation abilities of both commercial and open-weight LLMs, and do this across three languages. Using a self-play setup across three diverse dialogue games, we analyse trade-offs between performance and cost, the language consistency of reasoning processes, and the nature of strategic adaptation exhibited by models. Our findings show that enabling reasoning-that is, scaling test time compute-significantly improves negotiation outcomes by enhancing collaboration and helping models overcome task complexities, but comes at a substantial computational cost: reasoning improves GPT-5's performance by 31.4 % while increasing its cost by nearly 400 %. Most critically, we uncover a significant multilingual reasoning distinction: open-weight models consistently switch to English for their internal reasoning steps, even when negotiating in German or Italian (and thus possibly impacting potential explainability gains through the disclosure of reasoning traces), while leading commercial models maintain language consistency between their reasoning and final output.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.749103",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文本质是提升LLM通用能力。** 论文的核心并非将LLM应用于某个特定垂直领域（如金融或医疗），而是研究一项非常通用的认知能力——**谈判**。谈判本身是一项复杂的推理活动，需要战略性思考、对手建模、多步规划和权衡利弊。论文通过系统性地研究“推理机制”（即启用思维链等，摘要中称为“scaling test time compute”）如何影响LLM在这项通用任务上的表现，其本质是在**探索和量化一种增强LLM通用推理能力的方法及其效果**。这完全符合“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的核心要求。 2.  **第二步：正面指标——论文高度相关。** 论文标题和摘要中明确包含了多个核心正面指标： *   **核心概念**: \"Large Language Models\" (LLMs)。 *   **能力方向**: \"Reasoning\"是论文的绝对核心，\"Negotiation\"被定义为一种需要\"reason strategically\"的挑战，属于高级问题解决能力。 *   **新兴范式**: 论文采用了\"self-play setup\"，这是研究基于LLM的智能体和多智能体系统的经典范式，用于探究模型在交互环境中的能力。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 该论文是纯粹的语言模型研究，不涉及多模态、视觉，也不聚焦于任何特定应用领域（如医疗、化学等）。同时，其关注点并非模型可靠性的应用层面（如水印或安全），因此完全避开了所有排除标准。 4.  **第四步：特殊和模糊情况处理。** *   **智能体/工具使用**: 论文中的智能体框架是通用的，用于研究谈判这一通用智能行为，而非应用于特定领域。因此，这符合保留条件。 *   **可解释性**: 论文发现开源模型在多语言推理时会切换到英语，并指出这可能影响“通过披露推理轨迹获得的可解释性收益”。这是对推理过程内在特性的深刻洞察，直接关系到推理质量的可解释性，属于保留范畴。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于**系统性地评估和揭示了“推理”这一机制对LLM通用能力（谈判）的具体影响、性能收益与计算成本之间的权衡，以及其在多语言环境下的内在行为模式**。它没有提出一种全新的训练范式，但它对现有推理增强方法的效果、代价和局限性进行了深入、量化的分析。这种基础性的、对通用推理能力本身的深刻理解，是推动该领域前进不可或缺的一环，与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#35",
        "title": "A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models",
        "link": "/arxiv/2510.08049",
        "arxiv_id": "2510.08049",
        "authors": "Congming Zheng, Jiachen Zhu, Zhuoying Ou, Yuxiang Chen, Kangning Zhang, Rong Shan, Zeyu Zheng, Mengyue Yang, Jianghao Lin, Yong Yu, Weinan Zhang",
        "summary": "Although Large Language Models (LLMs) exhibit advanced reasoning ability, conventional alignment remains largely dominated by outcome reward models (ORMs) that judge only final answers. Process Reward Models(PRMs) address this gap by evaluating and guiding reasoning at the step or trajectory level. This survey provides a systematic overview of PRMs through the full loop: how to generate process data, build PRMs, and use PRMs for test-time scaling and reinforcement learning. We summarize applications across math, code, text, multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our goal is to clarify design spaces, reveal open challenges, and guide future research toward fine-grained, robust reasoning alignment.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.750646",
        "filter_reason": "这篇论文完全符合您的研究范围，应被保留。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心是关于**过程奖励模型（PRMs）**。PRMs是一种全新的、用于对齐和增强大语言模型推理能力的技术范式。它不同于传统只关注最终答案正确与否的奖励模型（ORMs），而是深入到推理的**“过程”**或**“步骤”**层面进行评估和引导。 - **符合保留标准**: 这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。PRMs本身就是为了解决LLM在复杂推理任务中的“过程错误”和“逻辑跳跃”问题而提出的，其核心目标就是提升模型的通用推理质量。 - **不符合排除标准**: 论文并非将LLM作为工具应用于特定领域，也不是关于基础设施或部署的研究。它聚焦于LLM内在的推理机制和对齐方法。 **第二步：正面指标——论文包含哪些相关主题？** 论文摘要中包含了多个强有力的正面指标： - **核心概念**: 明确以“Large Language Models (LLMs)”为核心研究对象。 - **能力方向**: 核心主题就是“reasoning”，并直接点出其对“advanced reasoning ability”的提升，目标是实现“fine-grained, robust reasoning alignment”。 - **训练方法**: 明确提到了“reinforcement learning”，并将PRMs的应用与RL和测试时扩展相结合，这直接对应了您关注的新训练范式。 - **新兴范式**: 论文总结了PRMs在“agents”领域的应用，表明了其与前沿智能体研究的关联。 **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文摘要中提到了“多模态推理”和“机器人”，但这**不应作为排除的理由**。 - 关键在于，这些领域是作为PRMs方法**应用的示例**出现的，用以展示PRMs方法的广泛适用性。论文的**主体和核心贡献**是PRMs这一方法论本身，而不是关于多模态或机器人的研究。论文的目标是“澄清设计空间，揭示开放挑战，并指导未来的研究”，这些都集中在PRM这一通用技术上。因此，论文的主要焦点并非这些特定领域。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文涉及了PRMs在“智能体”中的应用。根据标准，如果这是为了增强LLM的通用问题解决能力，则应保留。PRMs通过提供更精确的步骤级反馈，能够让智能体在执行复杂任务时进行更可靠的规划和推理，这正是一种增强通用能力的方式。 **第五步：最终决策** 综合以上分析，这篇论文是一篇关于“过程奖励模型”的综述，其核心贡献是系统性地梳理了一种能够**从“过程”层面精细化管理、引导和提升LLM通用推理能力**的前沿方法。它直接针对您研究目标中的“通用推理能力”这一核心，并探讨了通过新的训练范式（如过程监督的RL）来实现这一目标。尽管它提及了多个应用领域作为例子，但其本质是方法论的研究，完全符合您的筛选要求。因此，应判定为符合。"
    },
    {
        "index": "#40",
        "title": "Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning",
        "link": "/arxiv/2510.07974",
        "arxiv_id": "2510.07974",
        "authors": "Jialu Du, Guiyang Hou, Yihui Fu, Chen Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu",
        "summary": "While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like \"tricky\" and \"confused\" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.758419",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献是提出了一种新的方法论来增强大语言模型的通用推理能力。 我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是研究并改进LLM在“社会推理”这一通用认知能力上的缺陷。它没有将LLM作为工具应用于社会学领域，而是深入分析了LLM在处理涉及多主体、多时间线的复杂推理任务时出现的“认知混淆”问题。其核心贡献是提出了一种“自适应世界模型增强推理机制”，这是一种全新的、旨在提升模型内在推理质量的框架。这完全符合“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的要求。 2.  **第二步：正面指标** - 论文明确包含了核心概念“Large language models (LLMs)”。 - 其能力方向聚焦于“reasoning”，特别是更复杂的“social reasoning”。 - 论文提出的新机制（动态监控推理轨迹、及时干预）可以被看作是一种提升模型自我纠错和问题解决能力的新范式。 3.  **第三步：排除标准** - 论文不涉及多模态、视觉等内容。 - 论文的研究焦点是“社会推理”这一通用认知能力，而非医疗、化学、法律等特定应用领域。虽然评估使用了社会推理基准，但其目标是提升模型的基础能力，而非解决某个具体的社会学问题。 - 论文不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** - 论文研究的“认知混淆”问题，可以被视为一种特定形式的推理失败或“幻觉”。作者提出的解决方案——通过构建世界模型来帮助模型区分客观现实与主观信念——是从根本上提升模型的内在逻辑一致性和推理质量。这符合“提出一种新方法来减少幻觉、增强模型内在的可解释性……从而提升模型的通用可靠性和推理质量”的保留标准。 **最终决策**：该论文精准地定位了LLM在通用推理能力（特别是社会推理）上的一个核心缺陷，并提出了一种创新的、通用的技术机制（自适应世界模型）来修复它。其研究目标是增强模型本身的基础认知能力，而非将其应用于特定领域。因此，这篇论文与你的研究课题“大语言模型通用推理能力”高度相关，应当保留。"
    },
    {
        "index": "#38",
        "title": "Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks",
        "link": "/arxiv/2510.08002",
        "arxiv_id": "2510.08002",
        "authors": "Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, Yu Qiao, Haifeng Li",
        "summary": "Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.757391",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为MUSE的**自进化智能体框架**。其本质是解决现有LLM智能体“测试时静态、无法从经验中学习”的根本性局限。论文提出的方法——通过分层记忆模块反思任务轨迹、将经验结构化并用于持续改进——是一种**全新的训练/进化范式**，旨在增强LLM在长时程任务中的通用规划和问题解决能力。这直接触及并试图提升LLM的**基础能力**，而非将其作为工具应用于特定领域。因此，在第一步判断中，该论文应被**保留**。 2.  **第二步：正面指标** 论文摘要中包含了大量与您研究方向高度相关的正面指标： -   **核心概念**: 明确提及 \"Large Language Models\" 和 \"AI agents\"。 -   **能力方向**: 聚焦于 \"long-horizon tasks\"，这必然涉及 \"planning\" 和 \"problem-solving\"。 -   **训练方法**: 提出了 \"self-evolving\" 和 \"continuous learning\" 机制，这与强化学习、进化的思想一脉相承，旨在实现模型在部署后的自主能力提升。 -   **新兴范式**: 论文本身就是关于 \"llm-based agents\" 的研究，并提出了一个通用框架。 这些指标进一步确认了论文与您研究目标的高度相关性。 3.  **第三步：排除标准** 该论文的焦点非常清晰，完全避开了所有主要的排除标准： -   它不涉及多模态、视觉。 -   虽然在生产力任务上测试，但其方法是**通用框架**，并非针对医疗、化学等特定领域。 -   它不讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文的核心是关于**智能体**的。根据筛选标准，如果提出的是一种**通用的智能体框架**来增强LLM的通用问题解决能力，则应保留。MUSE正是这样一个框架，其目标是“real-world productivity task automation”，这是一个非常宽泛的领域，其强调的是框架的通用性和泛化能力（\"zero-shot improvement on new tasks\"），而非在某个垂直领域的应用。因此，该情况符合保留条件。 5.  **第五步：最终决策** 综合以上分析，这篇论文的研究前沿且高度契合。它没有停留在应用LLM，而是深入探索如何让LLM本身具备“在工作中学习”和“自我进化”的能力，这直接关系到LLM在复杂、长时程任务中的**通用推理和规划能力**的根基。论文提出的方法论（经验驱动的自进化）是提升LLM通用能力的核心探索，完全符合您筛选“致力于提高大语言模型（LLM）本身的『通用推理能力』”的论文的目标。"
    },
    {
        "index": "#41",
        "title": "LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?",
        "link": "/arxiv/2510.07962",
        "arxiv_id": "2510.07962",
        "authors": "Jingyuan Wang, Yankai Chen, Zhonghang Li, Chao Huang",
        "summary": "Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: https://github.com/HKUDS/LightReasoner",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.758898",
        "filter_reason": "这篇论文完全符合研究范围，应被保留。 **详细判断过程如下:** 1.  **第一步：核心判断** *   **符合保留条件**。这篇论文的核心本质是提出一种名为“LightReasoner”的**新训练范式**，其直接目标是**提升大语言模型（LLM）的推理能力**。论文的核心贡献不是将LLM应用于某个新领域，而是改进了LLM的学习和优化过程本身。它通过“让小模型教大模型”的思路，解决传统监督微调（SFT）在提升推理能力时资源消耗大的痛点。这直接命中了“改进LLM的基础能力”、“提出新的训练范式”、“增强其...多步推理等通用能力”的核心要求。 2.  **第二步：正面指标** *   论文高度匹配多个正面指标： *   **核心概念**: 论文标题和摘要反复提及 \"Large language models (LLMs)\"。 *   **能力方向**: 论文的研究重点是 \"reasoning\"，并在七个 \"mathematical benchmarks\" 上进行了验证，这完全符合 \"math reasoning\" 的能力方向。 *   **训练方法**: 论文提出的 \"LightReasoner\" 是一种全新的、高效的微调框架，其核心思想是利用模型间的行为差异来构建监督信号，这与 \"强化学习优化\"、\"自我进化\" 等方法在目标上是一致的——都是为了更好地训练和优化模型。 3.  **第三步：排除标准** *   **不涉及任何排除领域**。论文没有涉及多模态、视觉（Vision, VLMs等），没有聚焦于任何特定应用领域（如医疗、化学、机器人），也没有讨论模型可靠性层面的水印、安全等问题。它的评估基准是通用的数学推理数据集，而非特定领域数据集。 4.  **第四步：处理特殊和模糊情况** *   本论文不涉及特殊模糊情况。它清晰地聚焦于一个通用的训练方法论，而不是一个特定应用或一个应用层面的可靠性问题。 5.  **第五步：最终决策** *   **综合分析**，这篇论文的标题、摘要和核心贡献明确指向了“如何通过创新的方法论来提升LLM的通用推理能力”。LightReasoner框架通过一种巧妙的“专家-业余者”对比机制，高效地识别和放大了模型在推理过程中的优势，这是一种对模型基础能力的直接增强。它完全符合您筛选标准中“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标，因此应被判定为**True**。"
    },
    {
        "index": "#50",
        "title": "Contrastive Weak-to-strong Generalization",
        "link": "/arxiv/2510.07884",
        "arxiv_id": "2510.07884",
        "authors": "Houcheng Jiang, Junfeng Fang, Jiaxin Wu, Tianyu Zhang, Chen Gao, Yong Li, Xiang Wang, Xiangnan He, Yang Deng",
        "summary": "Weak-to-strong generalization provides a promising paradigm for scaling large language models (LLMs) by training stronger models on samples from aligned weaker ones, without requiring human feedback or explicit reward modeling. However, its robustness and generalization are hindered by the noise and biases in weak-model outputs, which limit its applicability in practice. To address this challenge, we leverage implicit rewards, which approximate explicit rewards through log-likelihood ratios, and reveal their structural equivalence with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in LLM generation. Building on this connection, we propose Contrastive Weak-to-Strong Generalization (ConG), a framework that employs contrastive decoding between pre- and post-alignment weak models to generate higher-quality samples. This approach enables more reliable capability transfer, denoising, and improved robustness, substantially mitigating the limitations of traditional weak-to-strong methods. Empirical results across different model families confirm consistent improvements, demonstrating the generality and effectiveness of ConG. Taken together, our findings highlight the potential of ConG to advance weak-to-strong generalization and provide a promising pathway toward AGI.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.768579",
        "filter_reason": "根据您的筛选标准，这篇论文完全符合要求，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种名为“Contrastive Weak-to-strong Generalization (ConG)”的**新训练范式**。其目标是解决“弱到强泛化”这一前沿研究方向中的关键挑战（弱模型输出的噪声和偏见），从而使更强的模型能够更有效地从弱模型中学习并实现能力提升。 - **是否符合**: 这完全符合“改进LLM的基础能力”和“提出新的训练范式”的标准。它不是将LLM应用于特定领域，而是专注于如何让LLM本身变得更强、能力更可靠。论文中提到的“更可靠的能力转移”和“通向AGI”的愿景，都表明其目标是提升模型的**通用能力**，这自然包括了通用推理能力。 2.  **第二步：正面指标** - **核心概念**: 论文明确聚焦于“Large language models (LLMs)”。 - **能力方向**: 虽然摘要没有直接使用“reasoning”这个词，但其核心目标“能力转移”和“通向AGI”内在地包含了提升模型逻辑、规划和问题解决等通用能力的意图。一个能实现可靠能力泛化的模型，其推理能力必然会得到增强。 - **训练方法**: 论文提出的方法是“弱到强泛化”的一种改进，并且明确指出这是一种**不依赖人类反馈或显式奖励建模**的范式，这使其与强化学习（RLHF）等训练优化方法处于同一类别，都属于提升模型基础能力的方法论研究。 - **新兴范式**: “弱到强泛化”本身就是一个当前非常热门的、旨在实现模型自我进化和能力提升的新兴范式。 3.  **第三步：排除标准** - 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型基础设施（部署、硬件）等。因此，它没有触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: 论文通过“对比解码”来“去噪”，减少弱模型输出的噪声和偏见。这可以被看作是一种从技术层面提升模型输出质量和内在可靠性的方法，而不是应用层面的讨论。提升输出的可靠性是进行高质量推理的基础，因此这一点反而是保留论文的有力依据。 **最终决策**: 综合分析，这篇论文的研究核心是提出一种创新的训练框架（ConG）来改进“弱到强泛化”这一前沿范式。其直接目标是通过更可靠、更少噪声的能力转移过程，来提升大语言模型的基础和通用能力。这与您的核心目标——“致力于提高大语言模型（LLM）本身的『通用推理能力』”——高度一致。该论文属于方法论层面的基础研究，而非应用层面的研究，因此完全符合您的筛选要求。"
    },
    {
        "index": "#56",
        "title": "Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models",
        "link": "/arxiv/2510.07799",
        "arxiv_id": "2510.07799",
        "authors": "Eric Hanchen Jiang, Guancheng Wan, Sophia Yin, Mengting Li, Yuchen Wu, Xiao Liang, Xinfeng Li, Yizhou Sun, Wei Wang, Kai-Wei Chang, Ying Nian Wu",
        "summary": "The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as task performance, communication cost, and robustness. Existing frameworks often rely on static or hand-crafted topologies, which inherently fail to adapt to diverse task requirements, leading to either excessive token consumption for simple problems or performance bottlenecks for complex ones. To address this challenge, we introduce a novel generative framework called \\textit{Guided Topology Diffusion (GTD)}. Inspired by conditional discrete graph diffusion models, GTD formulates topology synthesis as an iterative construction process. At each step, the generation is steered by a lightweight proxy model that predicts multi-objective rewards (e.g., accuracy, utility, cost), enabling real-time, gradient-free optimization towards task-adaptive topologies. This iterative, guided synthesis process distinguishes GTD from single-step generative frameworks, enabling it to better navigate complex design trade-offs. We validated GTD across multiple benchmarks, and experiments show that this framework can generate highly task-adaptive, sparse, and efficient communication topologies, significantly outperforming existing methods in LLM agent collaboration.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.771788",
        "filter_reason": "根据您的筛选标准，我对这篇论文进行了详细分析，判断其符合您的研究范围。 1.  **第一步：核心判断** 这篇论文的本质并非将LLM作为工具应用于特定领域，而是提出了一种新的框架（GTD）来优化由LLM驱动的多智能体系统。其核心贡献是解决“如何让多个LLM智能体更有效地协作”这一基础性问题。这完全符合您筛选标准中“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”以及“智能体协作框架”的范畴。论文的目标是提升LLM系统在通用任务上的性能，而非解决某个垂直领域的问题。 2.  **第二步：正面指标** 论文包含了多个关键的正面指标： *   **核心概念**: 明确以 \"large language models (LLMs)\" 为驱动单元。 *   **能力方向**: 虽然未直接使用\"reasoning\"一词，但其目标是通过优化通信拓扑来提升\"task performance\"和\"LLM agent collaboration\"，这本质上是在提升整个系统的**通用问题解决能力**，是多步推理和规划在系统层面的体现。 *   **新兴范式**: 论文的主题就是关于 **\"llm-based agents\"** 和 **\"multi-agent systems\"**，这正是您关注的前沿范式。 3.  **第三步：排除标准** 该论文完全避开了所有的排除标准： *   它不涉及多模态、视觉。 *   它不局限于任何特定应用领域（如医疗、化学），而是在\"multiple benchmarks\"上验证其通用性。 *   它不关注模型的基础设施、部署或应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文是**智能体/工具使用**情况的典型范例。它提出的GTD框架是一个**通用的智能体协作框架**，其目的是通过动态生成通信结构来增强LLM系统的通用问题解决能力。这与“用于化学实验自动化的智能体”等特定领域应用有本质区别，因此应被**保留**。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种新颖的、通用的方法论，通过优化多LLM智能体间的通信结构来提升其在多样化任务上的协作效率和整体性能。这直接关联到提升LLM系统的通用推理与问题解决能力，完全符合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，应判定为符合要求。"
    },
    {
        "index": "#54",
        "title": "AdaSwitch: Adaptive Switching Generation for Knowledge Distillation",
        "link": "/arxiv/2510.07842",
        "arxiv_id": "2510.07842",
        "authors": "Jingyu Peng, Maolin Wang, Hengyi Cai, Yuchen Li, Kai Zhang, Shuaiqiang Wang, Dawei Yin, Xiangyu Zhao",
        "summary": "Small language models (SLMs) are crucial for applications with strict latency and computational constraints, yet achieving high performance remains challenging. Knowledge distillation (KD) can transfer capabilities from large teacher models, but existing methods involve trade-offs: off-policy distillation provides high-quality supervision but introduces a training-inference mismatch, while on-policy approaches maintain consistency but rely on low-quality student outputs. To address these issues, we propose AdaSwitch, a novel approach that dynamically combines on-policy and off-policy generation at the token level. AdaSwitch allows the student to first explore its own predictions and then selectively integrate teacher guidance based on real-time quality assessment. This approach simultaneously preserves consistency and maintains supervision quality. Experiments on three datasets with two teacher-student LLM pairs demonstrate that AdaSwitch consistently improves accuracy, offering a practical and effective method for distilling SLMs with acceptable additional overhead.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.770706",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出了一种名为“AdaSwitch”的新颖知识蒸馏方法。知识蒸馏是一种重要的模型训练范式，其目标是让一个较小的学生模型学习到一个较大教师模型的能力。这完全符合筛选标准中“提出新的训练范式”和“改进LLM的基础能力”的范畴。论文的本质不是将LLM作为工具应用于特定领域，而是研究如何从根本上提升模型（此处特指小型语言模型SLM）本身的能力和效率。因此，从第一步判断，这篇论文应该被保留。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文明确提到了“Large language models (LLMs)”。虽然摘要中没有直接使用“reasoning”一词，但其目标是“transfer capabilities”和“consistently improves accuracy”，这内在地包含了提升模型在各项任务（包括推理任务）上的表现。一个能力更强的模型，其通用推理能力自然会更强。因此，该论文与“reasoning”和“problem-solving”方向是正相关的。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文的研究内容是纯粹的语言模型训练方法，完全不涉及多模态、视觉、任何特定应用领域（如医疗、化学），也未讨论水印、安全等应用层面的可靠性议题。因此，它触发了所有排除标准中的“不应排除”条件。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体、工具使用或幻觉等特殊情况，因此无需进行特殊判断。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心是提出一种创新的训练范式（自适应知识蒸馏），旨在提升小型语言模型的基础性能。虽然它不像思维链（CoT）那样直接研究一种推理“技术”，但它通过改进模型能力的“转移”过程，为构建更强、更高效的通用推理模型提供了关键的底层支持。这完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，最终决策是保留。"
    },
    {
        "index": "#47",
        "title": "ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall",
        "link": "/arxiv/2510.07896",
        "arxiv_id": "2510.07896",
        "authors": "Jiayu Yang, Yuxuan Fan, Songning Lai, Shengen Wu, Jiaqi Tang, Chun Kang, Zhijiang Guo, Yutao Yue",
        "summary": "Large Language Models (LLMs) require efficient knowledge editing (KE) to update factual information, yet existing methods exhibit significant performance decay in multi-hop factual recall. This failure is particularly acute when edits involve intermediate implicit subjects within reasoning chains. Through causal analysis, we reveal that this limitation stems from an oversight of how chained knowledge is dynamically represented and utilized at the neuron level. We discover that during multi hop reasoning, implicit subjects function as query neurons, which sequentially activate corresponding value neurons across transformer layers to accumulate information toward the final answer, a dynamic prior KE work has overlooked. Guided by this insight, we propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall, a framework that leverages neuron-level attribution to identify and edit these critical query-value (Q-V) pathways. ACE provides a mechanistically grounded solution for multi-hop KE, empirically outperforming state-of-the-art methods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals more fine-grained activation patterns in Qwen3 and demonstrates that the semantic interpretability of value neurons is orchestrated by query-driven accumulation. These findings establish a new pathway for advancing KE capabilities based on the principled understanding of internal reasoning mechanisms.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.767073",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是改进LLM的基础能力。** 论文的核心并非将LLM应用于某个特定领域，而是聚焦于LLM在执行“多跳事实回忆”任务时的内在缺陷。多跳事实回忆是通用推理能力的一个关键组成部分，它要求模型能够连接和整合多个信息片段来得出最终答案。论文通过“知识编辑”这一技术手段，直接干预和修正模型内部的推理机制，其本质是提升模型底层的、通用的多步推理能力。因此，这属于“改进LLM的基础能力”的范畴，应当保留。 2.  **第二步：正面指标——论文高度相关。** -   **核心概念**: 论文明确以 Large Language Models (LLMs) 为研究对象。 -   **能力方向**: 论文的核心是解决“多跳事实回忆”问题，这直接对应了筛选标准中的“多步推理”。其目标是通过优化模型内部机制来提升这一通用能力。 -   论文虽然未直接提及RL或智能体，但其提出的“知识编辑”是一种新颖的训练/优化范式，旨在从模型内部增强能力，这与研究目标高度一致。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容完全不涉及多模态、视觉，也并非针对医疗、化学、机器人等特定应用领域。同时，它研究的也不是水印、安全等应用层面的可靠性问题。因此，所有排除标准均不适用。 4.  **第四步：处理特殊和模糊情况——论文符合“可解释性”的保留原则。** 这篇论文是“可解释性”驱动下提升推理能力的绝佳范例。它没有停留在表面现象，而是通过“因果分析”和“神经元级归因”，深入探究了多跳推理失败的根本原因（即“查询-神经元”和“价值-神经元”的动态激活机制）。基于这种对内部推理机制的深刻理解，它提出了ACE框架来精准地修复问题。这完全符合“提出一种新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量”的保留标准。 **最终决策**: 该论文的核心贡献是提出了一种基于模型内部机理（神经元级通路）的知识编辑框架ACE，专门用于解决LLM在多跳推理这一通用能力上的短板。它通过深入理解并干预模型的内部推理过程来提升其性能，而非将其作为工具应用于外部任务。因此，这篇论文与“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标高度契合，应被筛选保留。"
    },
    {
        "index": "#42",
        "title": "A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning",
        "link": "/arxiv/2510.07958",
        "arxiv_id": "2510.07958",
        "authors": "Fengji Zhang, Xinyao Niu, Chengyang Ying, Guancheng Lin, Zhongkai Hao, Zhou Fan, Chengen Huang, Jacky Keung, Bei Chen, Junyang Lin",
        "summary": "Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A$^2$Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed $\\mathrm{AnsF1}$ reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A$^2$Search achieves new state-of-the-art performance. With only a single rollout, A$^2$Search-7B yields an average $\\mathrm{AnsF1}@1$ score of $48.4\\%$ across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B ($46.2\\%$). Extensive analyses further show that A$^2$Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.759452",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一个名为A²Search的**训练框架**，旨在提升大语言模型在处理**模糊问题**时的能力。这并非将LLM作为工具应用于某个特定领域，而是直接针对LLM在**通用推理**中的一个核心短板——即面对存在多个正确答案的复杂问题时表现不佳——进行改进。通过设计新的训练范式（使用强化学习和自定义奖励函数），论文直接增强了模型的基础推理能力。因此，根据第一步的核心判断标准，这篇论文应该被**保留**。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 明确以\"Large Language Models (LLMs)\"为研究对象。 *   **能力方向**: 论文聚焦于\"question answering (QA)\"，特别是多跳QA，这本身就是一种复杂的**问题解决**和**推理**任务。处理歧义是高级**推理能力**的体现。 *   **训练方法**: 论文的核心方法论是**强化学习**，并设计了新的奖励函数来优化模型，这完全符合筛选标准中提到的训练方法。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： *   它不涉及多模态、视觉等内容。 *   其应用场景是通用的\"开放域问答\"，而非医疗、化学等特定领域。 *   它虽然提升了系统的\"可靠性\"，但这是通过改进模型内在的推理机制实现的，而非通过添加水印等应用层面的技术。 4.  **第四步：处理特殊和模糊情况** 论文的研究内容与“提升模型内在可靠性”的特殊情况高度相关。它通过一种新的训练方法（RL）来减少模型在面对歧义时可能产生的“错误”（即只给出一个答案而忽略其他可能性），从而提升了模型的**通用推理质量和可靠性**。这并非对现象的社会学研究，而是提出了一种可操作的、改进模型核心能力的技术方案。 **最终决策**: 综合以上分析，这篇论文的本质是提出一种新的训练范式（基于强化学习的A²Search框架），用以提升大语言模型在通用问答任务中的推理能力，特别是处理模糊性和多答案问题的能力。这直接命中了你“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，这篇论文是高度相关且应该被筛选出来的前沿研究。"
    },
    {
        "index": "#60",
        "title": "Drift No More? Context Equilibria in Multi-Turn LLM Interactions",
        "link": "/arxiv/2510.07777",
        "arxiv_id": "2510.07777",
        "authors": "Vardhan Dongre, Ryan A. Rossi, Viet Dac Lai, David Seunghyun Yoon, Dilek Hakkani-Tür, Trung Bui",
        "summary": "Large Language Models (LLMs) excel at single-turn tasks such as instruction following and summarization, yet real-world deployments require sustained multi-turn interactions where user goals and conversational context persist and evolve. A recurring challenge in this setting is context drift: the gradual divergence of a model's outputs from goal-consistent behavior across turns. Unlike single-turn errors, drift unfolds temporally and is poorly captured by static evaluation metrics. In this work, we present a study of context drift in multi-turn interactions and propose a simple dynamical framework to interpret its behavior. We formalize drift as the turn-wise KL divergence between the token-level predictive distributions of the test model and a goal-consistent reference model, and propose a recurrence model that interprets its evolution as a bounded stochastic process with restoring forces and controllable interventions. We instantiate this framework in both synthetic long-horizon rewriting tasks and realistic user-agent simulations such as in $\\tau$-Bench, measuring drift for several open-weight LLMs that are used as user simulators. Our experiments consistently reveal stable, noise-limited equilibria rather than runaway degradation, and demonstrate that simple reminder interventions reliably reduce divergence in line with theoretical predictions. Together, these results suggest that multi-turn drift can be understood as a controllable equilibrium phenomenon rather than as inevitable decay, providing a foundation for studying and mitigating context drift in extended interactions.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.789777",
        "filter_reason": "这篇论文符合您的研究范围，核心判断依据如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种理论框架来理解和解决LLM在多轮交互中的“上下文漂移”问题。这并非将LLM作为工具应用于特定领域，而是直接针对LLM在执行通用任务（多轮对话、目标导向交互）时出现的一个基础性、普遍性的能力缺陷。一个无法在多轮对话中保持目标一致性的模型，其进行复杂推理、规划和问题解决的能力将大打折扣。因此，研究并缓解上下文漂移，本质上是在提升LLM的**通用推理和问题解决能力**的鲁棒性和持续性。这完全符合“改进LLM的基础能力”这一核心要求。 2.  **第二步：正面指标** 论文明确包含了多个正面指标： *   **核心概念**: 论文研究对象是 \"Large Language Models (LLMs)\"。 *   **能力方向**: 上下文漂移直接关系到LLM在多轮对话中的**问题解决**能力。论文中提到的 \"goal-consistent behavior\" 和 \"user goals\" 都是规划和问题解决的核心要素。 *   **新兴范式**: 论文的实验设置涉及 \"user-agent simulations\"，这是当前LLM智能体研究的热点范式。解决上下文漂移问题是构建高效、可靠的LLM智能体的关键一步。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准领域。它没有讨论多模态、视觉，也没有聚焦于医疗、化学等特定应用，更不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 论文虽然使用了“用户-智能体模拟”作为实验环境，但其目标是提出一种**通用的**理解和缓解上下文漂移的方法，而非为特定领域（如化学）设计智能体。这符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的保留原则。 *   **幻觉/可解释性/安全**: 上下文漂移可以被视为一种在时间维度上展开的、与事实或目标不一致的“幻觉”。论文提出了一种新的方法（动态系统框架）来形式化、测量和缓解这种现象，这直接提升了模型的**内在可靠性**和**推理质量**，因此应该保留。 **最终决策**: 综合以上分析，该论文通过提出一个新颖的动态系统理论框架，深入研究了LLM在多轮交互中保持上下文一致性的根本问题。这项工作直接提升了LLM在复杂、持续的任务中进行通用推理和问题解决的能力，是提升LLM核心能力的前沿研究，而非特定领域的应用。因此，这篇论文完全符合您的筛选要求。"
    },
    {
        "index": "#63",
        "title": "Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards",
        "link": "/arxiv/2510.07774",
        "arxiv_id": "2510.07774",
        "authors": "Youliang Yuan, Qiuyang Mang, Jingbang Chen, Hong Wan, Xiaoyuan Liu, Junjielong Xu, Jen-tse Huang, Wenxuan Wang, Wenxiang Jiao, Pinjia He",
        "summary": "Large language models for mathematical reasoning are typically trained with outcome-based rewards, which credit only the final answer. In our experiments, we observe that this paradigm is highly susceptible to reward hacking, leading to a substantial overestimation of a model's reasoning ability. This is evidenced by a high incidence of false positives - solutions that reach the correct final answer through an unsound reasoning process. Through a systematic analysis with human verification, we establish a taxonomy of these failure modes, identifying patterns like Miracle Steps - abrupt jumps to a correct output without a valid preceding derivation. Probing experiments suggest a strong association between these Miracle Steps and memorization, where the model appears to recall the answer directly rather than deriving it. To mitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a process-oriented reward function that evaluates the entire reasoning trajectory against problem-specific rubrics. The generative RRM provides fine-grained, calibrated rewards (0-1) that explicitly penalize logical flaws and encourage rigorous deduction. When integrated into a reinforcement learning pipeline, RRM-based training consistently outperforms outcome-only supervision across four math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from 26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work demonstrates that rewarding the solution process is crucial for building models that are not only more accurate but also more reliable.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.791760",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。我的判断依据如下： 1.  **第一步：核心判断** 这篇论文的核心贡献是提出了一种名为“Rubric Reward Model (RRM)”的**全新训练范式**。其目标是解决现有LLM在数学推理中的一个根本性缺陷：模型通过“Miracle Steps”（奇迹步骤）等不严谨的推理过程“作弊”得到正确答案，而非真正地进行逻辑推导。RRM通过奖励整个**推理过程**而非仅奖励最终结果，来**增强模型的逻辑、数学和多步推理能力**。这直接触及并改进了LLM的基础能力，属于提升“通用推理能力”的核心研究，而非将LLM作为工具应用于特定领域。 2.  **第二步：正面指标** 论文高度符合所有关键正面指标： *   **核心概念**: 明确以 \"Large language models for mathematical reasoning\" 为研究对象。 *   **能力方向**: 聚焦于 \"mathematical reasoning\" 和 \"logical flaws\"，直接命中 \"reasoning\" 方向。 *   **训练方法**: 提出的RRM被 \"integrated into a reinforcement learning pipeline\"，明确使用了强化学习方法。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准领域： *   它没有讨论多模态或视觉。 *   它的研究领域是数学，这是一个基础学科，而非医疗、化学、机器人等特定应用领域。其贡献是普适性的方法论，可以推广到其他需要严密推理的场景。 *   它讨论的“可靠性”是指模型内在推理过程的可靠性，而非应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 论文对“Miracle Steps”的分析和解决，可以看作是对模型推理过程中一种特殊“幻觉”（即产生不合逻辑的推导步骤）的深度探究和修复。它提出了一种新的方法（RRM）来根除这一问题，从而提升了模型的内在可靠性和推理质量。这完全符合“提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量”的保留标准。 **总结**：该论文的本质是提出一种创新的、基于过程的强化学习奖励机制，以根治LLM在数学推理中的逻辑缺陷，从而提升其通用推理能力的严谨性和可靠性。这与你的核心目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度一致，因此是一篇非常相关且高质量的前沿论文。"
    },
    {
        "index": "#62",
        "title": "The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs",
        "link": "/arxiv/2510.07775",
        "arxiv_id": "2510.07775",
        "authors": "Omar Mahmoud, Ali Khalil, Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana",
        "summary": "Hallucination in large language models (LLMs) has been widely studied in recent years, with progress in both detection and mitigation aimed at improving truthfulness. Yet, a critical side effect remains largely overlooked: enhancing truthfulness can negatively impact safety alignment. In this paper, we investigate this trade-off and show that increasing factual accuracy often comes at the cost of weakened refusal behavior. Our analysis reveals that this arises from overlapping components in the model that simultaneously encode hallucination and refusal information, leading alignment methods to suppress factual knowledge unintentionally. We further examine how fine-tuning on benign datasets, even when curated for safety, can degrade alignment for the same reason. To address this, we propose a method that disentangles refusal-related features from hallucination features using sparse autoencoders, and preserves refusal behavior during fine-tuning through subspace orthogonalization. This approach prevents hallucinations from increasing while maintaining safety alignment.We evaluate our method on commonsense reasoning tasks and harmful benchmarks (AdvBench and StrongReject). Results demonstrate that our approach preserves refusal behavior and task utility, mitigating the trade-off between truthfulness and safety.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.791042",
        "filter_reason": "这篇论文符合我的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心并非将LLM应用于特定领域，而是深入探究了LLM内部的一个根本性矛盾：**提升事实准确性（减少幻觉）与维持安全对齐（拒绝有害指令）之间的冲突**。它揭示了这两种能力在模型内部存在重叠的表征组件，并因此导致对齐训练会意外地抑制事实知识。这直接关系到LLM的基础能力和内在机制。 2.  **第二步与第四步：正面指标与特殊情况处理** - **核心贡献是方法论创新**：论文的核心贡献不是对安全问题的社会学讨论，而是提出了一种**新颖的技术方法**——“使用稀疏自编码器解耦拒绝相关特征与幻觉特征，并通过子空间正交化在微调过程中保留拒绝行为”。这是一种针对模型内部表征和训练过程的创新，旨在从根本上提升模型的内在能力和可靠性。 - **符合“幻觉”特殊情况处理规则**：根据第四步的筛选标准，“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留”。这篇论文完美符合此条。它提出的新方法直接作用于减少幻觉，其最终目标是在不损害安全的前提下，提升模型的“任务效用”，并在“常识推理任务”上进行了验证。这表明其工作直接指向提升模型通用的、可靠的推理表现。 3.  **与“通用推理能力”的关联** 通用推理能力不仅包括逻辑链条的正确性，也包含推理过程的真实性和可靠性。幻觉是LLM高质量推理的最大障碍之一。一个频繁产生幻觉的模型，其推理能力是不可靠和无用的。这篇论文通过解决“真实性”与“安全性”的内在冲突，旨在让LLM在**保持安全的同时，更准确、更真实地进行推理**。因此，这项工作是提升LLM通用推理能力质量与鲁棒性的关键一环，它让推理能力变得更“可用”和“可信”。 **总结**：尽管论文标题和摘要中频繁出现“Safety”和“Hallucination”等词，看似偏向模型可靠性，但其本质是提出了一种创新的、作用于模型内部表征的训练/微调范式，旨在解决两个核心通用能力（事实准确性与安全性）之间的冲突。这种对模型内在机制进行解耦和优化的方法论研究，直接提升了模型通用推理的可靠性和质量，完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，应该保留。"
    },
    {
        "index": "#64",
        "title": "ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning",
        "link": "/arxiv/2510.07768",
        "arxiv_id": "2510.07768",
        "authors": "Murong Yue, Zhiwei Liu, Liangwei Yang, Jianguo Zhang, Zuxin Liu, Haolin Chen, Ziyu Yao, Silvio Savarese, Caiming Xiong, Shelby Heinecke, Huan Wang",
        "summary": "Large Language Models (LLMs) equipped with external tools have demonstrated enhanced performance on complex reasoning tasks. The widespread adoption of this tool-augmented reasoning is hindered by the scarcity of domain-specific tools. For instance, in domains such as physics question answering, suitable and specialized tools are often missing. Recent work has explored automating tool creation by extracting reusable functions from Chain-of-Thought (CoT) reasoning traces; however, these approaches face a critical scalability bottleneck. As the number of generated tools grows, storing them in an unstructured collection leads to significant retrieval challenges, including an expanding search space and ambiguity between function-related tools. To address this, we propose a systematic approach to automatically refactor an unstructured collection of tools into a structured tool library. Our system first generates discrete, task-specific tools and clusters them into semantically coherent topics. Within each cluster, we introduce a multi-agent framework to consolidate scattered functionalities: a code agent refactors code to extract shared logic and creates versatile, aggregated tools, while a reviewing agent ensures that these aggregated tools maintain the complete functional capabilities of the original set. This process transforms numerous question-specific tools into a smaller set of powerful, aggregated tools without loss of functionality. Experimental results demonstrate that our approach significantly improves tool retrieval accuracy and overall reasoning performance across multiple reasoning tasks. Furthermore, our method shows enhanced scalability compared with baselines as the number of question-specific increases.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.802752",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据您的筛选标准进行的详细判断： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一种新的方法论（`ToolLibGen`）来增强大语言模型的推理能力。它没有将LLM作为工具应用于某个特定领域，而是专注于改进“工具使用”这一通用推理范式本身。其核心贡献是解决工具增强推理中的可扩展性瓶颈，通过自动化的工具创建、聚类和聚合，构建一个更高效、更强大的工具库，从而提升LLM在复杂推理任务上的表现。这完全符合“改进LLM的基础能力”和“增强其逻辑、多步推理等通用能力”的要求。 2.  **第二步：正面指标** - 论文包含了多个强烈的正面指标： - **核心概念**: 明确以 \"Large Language Models (LLMs)\" 为研究对象。 - **能力方向**: 核心目标是提升 \"LLM Reasoning\" 和 \"complex reasoning tasks\" 的性能。 - **新兴范式**: 论文的核心是 \"tool use\"，并提出了一个 \"multi-agent framework\" 来实现工具聚合，这属于前沿的智能体协作框架研究。 3.  **第三步：排除标准** - 论文的主要焦点不涉及任何排除标准领域： - 它不是关于多模态或视觉的。 - 虽然摘要中提到了 \"physics question answering\" 作为一个工具稀缺的例子，但这只是为了引出问题的动机。论文提出的方法（`ToolLibGen`）和实验验证都是在 \"multiple reasoning tasks\" 上进行的，是一个通用框架，而非特定于物理领域的应用。 - 它不关注模型基础设施、部署优化或应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的绝佳范例。它设计了一个包含“代码智能体”和“审查智能体”的多智能体系统，其目标是通用地重构和聚合工具，以提升通用推理能力，而不是将其应用于化学、生物等特定领域。因此，根据此规则，应明确保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种可扩展的、自动化的工具库生成与聚合方法，旨在通过优化“工具使用”这一关键推理范式，来系统性地提升大语言模型的通用推理能力。其研究内容、方法和目标与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#67",
        "title": "OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment",
        "link": "/arxiv/2510.07743",
        "arxiv_id": "2510.07743",
        "authors": "Tianci Liu, Ran Xu, Tony Yu, Ilgee Hong, Carl Yang, Tuo Zhao, Haoyu Wang",
        "summary": "Reward modeling lies at the core of reinforcement learning from human feedback (RLHF), yet most existing reward models rely on scalar or pairwise judgments that fail to capture the multifaceted nature of human preferences. Recent studies have explored rubrics-as-rewards (RaR) that uses structured natural language criteria that capture multiple dimensions of response quality. However, producing rubrics that are both reliable and scalable remains a key challenge. In this work, we introduce OpenRubrics, a diverse, large-scale collection of (prompt, rubric) pairs for training rubric-generation and rubric-based reward models. To elicit discriminative and comprehensive evaluation signals, we introduce Contrastive Rubric Generation (CRG), which derives both hard rules (explicit constraints) and principles (implicit qualities) by contrasting preferred and rejected responses. We further improve reliability by enforcing preference-label consistency via rejection sampling to remove noisy rubrics. Across multiple reward-modeling benchmarks, our rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines by 6.8%. These gains transfer to policy models on instruction-following and biomedical benchmarks. Our results show that rubrics provide scalable alignment signals that narrow the gap between costly human evaluation and automated reward modeling, enabling a new principle-driven paradigm for LLM alignment.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.804260",
        "filter_reason": "根据筛选标准，这篇论文完全符合研究范围。我的判断过程如下： 1.  **第一步：核心判断** 这篇论文的本质是**改进LLM的基础训练范式**。其核心贡献是提出了一种名为“Contrastive Rubric Generation (CRG)”的新方法，用于为LLM对齐过程中的关键环节——奖励建模——生成更高质量、更多维度的训练信号（即评分标准）。这不是将LLM作为工具应用于某个领域，而是直接作用于LLM本身的训练和对齐机制，旨在提升模型生成高质量、符合人类复杂偏好响应的能力。因此，该论文通过了第一步的核心判断，应予以保留。 2.  **第二步：正面指标分析** 论文高度符合多个正面指标： *   **核心概念**: 论文明确以\"Large language models, LLM alignment\"为核心。 *   **训练方法**: 论文的核心是\"Reward modeling\"，而这正是\"Reinforcement Learning from Human Feedback (RLHF)\"的关键组成部分。RLHF是当前提升LLM通用能力的核心训练技术之一。 *   **能力方向**: 虽然没有直接使用\"reasoning\"一词，但论文的目标是提升\"instruction-following\"能力，并捕捉\"response quality\"的多个维度（如逻辑性、完整性等）。一个能更好地遵循复杂指令、产出高质量响应的模型，其内在的通用推理和问题解决能力必然得到了增强。论文旨在通过改进对齐来提升模型输出的“原则性”，这与高质量的逻辑推理紧密相关。 3.  **第三步：排除标准分析** 论文成功规避了所有排除标准： *   **多模态与视觉**: 完全不涉及。 *   **特定应用领域**: 论文中提到了\"biomedical benchmarks\"，这是一个需要仔细辨析的点。然而，这里的生物医学基准**仅作为评估其通用方法有效性的测试平台**，而非论文的研究焦点。论文的核心是“一种通用的、基于原则的对齐新范式”，而不是“一个用于生物医学领域的应用”。其增益可以“transfer to”（迁移到）政策模型上，证明了该方法的通用性。因此，这不构成排除理由。 *   **模型可靠性（应用层面）**: 论文关注的是通过改进奖励模型来提升模型输出的内在质量和一致性，这与应用层面的水印、安全等不同。 4.  **第四步：特殊和模糊情况处理** 本论文不属于模糊情况，其定位非常清晰。它通过提出一种新的、更精细的奖励建模方法，直接作用于LLM的训练过程，从而提升模型的通用能力和对齐质量。这是一种“治本”的方法，而非“治标”的应用。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种**提升LLM对齐质量和训练效果的通用方法论**。它通过改进RLHF中的奖励建模环节，使模型能够学习到更复杂、更符合人类原则的偏好，从而提升其生成高质量响应的通用能力。这直接服务于“提高大语言模型本身的通用推理能力”这一核心目标。因此，最终判断为**符合**。"
    },
    {
        "index": "#66",
        "title": "Parallel Test-Time Scaling for Latent Reasoning Models",
        "link": "/arxiv/2510.07745",
        "arxiv_id": "2510.07745",
        "authors": "Runyang You, Yongqi Li, Meng Liu, Wenjie Wang, Liqiang Nie, Wenjie Li",
        "summary": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \\ This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.803750",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 论文的核心贡献是提出了一种名为“并行测试时缩放”的新方法，专门用于增强“潜在推理模型”的性能。潜在推理是一种在连续向量空间中进行的多步推理范式，是显式思维链的一种高效替代方案。论文直接解决了如何在这种新兴的推理范式上进行有效扩展和优化的根本性问题。这完全属于“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的范畴，而不是将LLM应用于特定领域。因此，根据第一步的核心判断，这篇论文应该被**保留**。 2.  **第二步：正面指标——论文高度相关。** 论文摘要中明确包含了多个关键正面指标： -   **核心概念**: \"Large language models (LLMs)\" -   **能力方向**: \"reasoning\", \"latent reasoning\", \"intermediate reasoning\" -   **新兴范式**: 论文建立在 \"chains-of-thought\" 的基础上，并对其进行了创新，提出了 \"latent reasoning\" 这一新范式。其研究内容 \"Parallel test-time scaling\" 也是当前提升模型推理能力的前沿探索方向。 这些关键词和主题与你的研究目标高度契合。 3.  **第三步：排除标准——论文未触及任何排除领域。** 论文的研究内容纯粹聚焦于模型本身的推理机制和推理时优化方法。它不涉及： -   **多模态与视觉**: 完全没有提及视觉或多模态内容。 -   **特定应用领域**: 没有将方法应用于医疗、化学、机器人等任何特定领域。 -   **模型可靠性（应用层面）**: 研究目标是提升推理性能，而非水印、安全等应用层面的可靠性问题。 因此，论文完全避开了所有排除标准。 4.  **第四步：处理特殊和模糊情况——不适用。** 该论文虽然涉及智能体推理的某些元素（如多路径探索和选择），但其核心是提出一种通用的、领域无关的推理增强框架，而非针对特定领域的智能体应用。它也未涉及幻觉、可解释性等特殊情况的讨论。 **最终决策:** 综合以上分析，这篇论文的本质是提出一种创新的方法论（并行测试时缩放），用以增强大语言模型在一种新兴推理范式（潜在推理）下的通用推理能力。它直接回应了“如何提升LLM通用推理能力”这一核心问题，属于前沿的基础能力研究，而非应用或工程优化研究。因此，这篇论文与你的研究课题高度相关，应被筛选**通过 (True)**。"
    },
    {
        "index": "#65",
        "title": "Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers",
        "link": "/arxiv/2510.07761",
        "arxiv_id": "2510.07761",
        "authors": "Nishant Balepur, Atrey Desai, Rachel Rudinger",
        "summary": "Large language models (LLMs) now give reasoning before answering, excelling in tasks like multiple-choice question answering (MCQA). Yet, a concern is that LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed in MCQA without using the question, i.e., choices-only. Such partial-input success is often deemed problematic, but reasoning traces could reveal if these strategies are truly shallow in choices-only settings. To study these strategies, reasoning LLMs solve MCQs in full and choices-only inputs; test-time reasoning often boosts accuracy on full and in choices-only half the time. While possibly due to shallow shortcuts, choices-only success is barely affected by the length of reasoning traces, and after finding traces pass faithfulness tests, we show they use less problematic strategies like inferring missing questions. In all, we challenge claims that partial-input success is always a flaw, so we discuss how reasoning traces could separate problematic data from less problematic reasoning.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.803192",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心并非将LLM应用于某个特定领域，而是对LLM在执行通用推理任务（多项选择问答）时的行为进行深入的**分析和解构**。它研究的不是“如何用LLM解决化学问题”，而是“LLM在推理时，其内部过程究竟是怎样的？”。具体来说，它探讨了“推理轨迹”的真实作用，挑战了“模型仅靠选项就能成功是浅层表现”的简单论断，并提出了一种更精细的视角来评估模型的推理策略。这种对LLM推理机制本身的深刻洞察，是提升其通用推理能力的基础和前提，因此完全符合“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的核心目标。 2.  **第二步：正面指标** 论文高度匹配多个正面指标： *   **核心概念**: 明确以“Large language models (LLMs)”为研究对象。 *   **能力方向**: 核心主题就是“reasoning”，特别是对“reasoning traces”（推理轨迹）的分析。 *   **新兴范式**: “Test-Time Reasoners”和“reasoning traces”是当前提升LLM推理能力（如思维链CoT）的核心研究范式。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准。它没有讨论多模态、视觉，也没有聚焦于医疗、化学等特定应用领域，更不是关于模型基础设施或应用层面的安全水印。 4.  **第四步：处理特殊和模糊情况** 这篇论文可以归类于对“可解释性”的探讨。它没有停留在表面现象，而是通过分析“reasoning traces”来增强我们对模型推理过程的理解，试图区分“有问题的捷径”和“不那么有问题的推理策略”。这种对模型内在推理质量的探究，正是为了提升模型的通用可靠性，符合“如果论文提出一种新方法来增强模型内在的可解释性...从而提升模型的通用可靠性和推理质量，应该保留”的标准。 **最终决策**: 该论文的核心贡献在于，它提供了一种新的分析框架来理解和评估LLM的推理过程。它没有提出一种新的训练方法，但它对现有推理方法（如CoT）的内在机制、优势和潜在缺陷进行了深刻的剖析。对于任何致力于“提高大语言模型通用推理能力”的研究者来说，理解模型当前是如何进行推理的，是设计更好方法的第一步。因此，这篇论文是高度相关且极具价值的前沿研究，应当被保留。"
    },
    {
        "index": "#57",
        "title": "HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation",
        "link": "/arxiv/2510.07794",
        "arxiv_id": "2510.07794",
        "authors": "Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Kaiyu He, Xinya Du, Zhiyu Chen",
        "summary": "Agentic RAG is a powerful technique for incorporating external information that LLMs lack, enabling better problem solving and question answering. However, suboptimal search behaviors exist widely, such as over-search (retrieving information already known) and under-search (failing to search when necessary), which leads to unnecessary overhead and unreliable outputs. Current training methods, which typically rely on outcome-based rewards in a RL framework, lack the fine-grained control needed to address these inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for Efficient agentic RAG (HiPRAG), a training methodology that incorporates a fine-grained, knowledge-grounded process reward into the RL training. Our approach evaluates the necessity of each search decision on-the-fly by decomposing the agent's reasoning trajectory into discrete, parsable steps. We then apply a hierarchical reward function that provides an additional bonus based on the proportion of optimal search and non-search steps, on top of commonly used outcome and format rewards. Experiments on the Qwen2.5 and Llama-3.2 models across seven diverse QA benchmarks show that our method achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished while improving search efficiency, reducing the over-search rate to just 2.3% and concurrently lowering the under-search rate. These results demonstrate the efficacy of optimizing the reasoning process itself, not just the final outcome. Further experiments and analysis demonstrate that HiPRAG shows good generalizability across a wide range of RL algorithms, model families, sizes, and types. This work demonstrates the importance and potential of fine-grained control through RL, for improving the efficiency and optimality of reasoning for search agents.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.787830",
        "filter_reason": "这篇论文完全符合你的筛选标准。 1.  **核心判断 (第一步):** 论文的核心是提出一种名为 **HiPRAG** 的新训练方法论。该方法通过引入分层过程奖励，在强化学习（RL）框架中优化智能体检索增强生成（Agentic RAG）的决策过程。其本质是改进LLM作为智能体时，在解决问题过程中的规划、决策和多步推理能力，特别是关于“何时搜索”以及“搜索什么”的推理能力。这直接关系到提升LLM的通用推理能力，属于增强LLM基础能力和提出新训练范式的研究，而非将LLM应用于特定领域。 2.  **正面指标 (第二步):** 论文高度符合多个正面指标。 *   **核心概念:** 研究对象是基于LLM的智能体。 *   **能力方向:** 摘要中明确提到其目标是“enabling better problem solving”（实现更好的问题解决），并且核心是“optimizing the reasoning process itself”（优化推理过程本身），这直接命中了`reasoning`和`problem-solving`。 *   **训练方法:** 论文的核心贡献是一种新的`reinforcement learning (RL)`训练方法，它使用了“分层过程奖励”。 *   **新兴范式:** 论文聚焦于`llm-based agents`和`tool use`（搜索作为一种工具）。 3.  **排除标准 (第三步):** 论文不涉及任何排除标准中的领域。它研究的是纯文本问答任务，没有涉及视觉、多模态，也没有专注于任何特定的应用领域（如医疗、化学等）。其关注点是优化推理过程的效率，而非模型的安全、水印等应用层面的可靠性问题。 4.  **特殊和模糊情况 (第四步):** *   **智能体/工具使用:** 该论文是提出一种**通用的**智能体工具使用（搜索）的训练方法，旨在提升其通用问题解决的效率和可靠性，而不是将其应用于某个特定领域。这完全符合“应该保留”的情况。 **最终决策 (第五步):** 综合以上分析，HiPRAG论文致力于通过改进训练范式（带过程奖励的RL）来提升LLM作为智能体时的核心规划与推理能力。它提出的方法论是通用的，旨在解决智能体在工具使用过程中的次优决策问题，这直接推动了LLM通用推理能力的前沿。因此，这篇论文精准地符合你的研究范围。"
    },
    {
        "index": "#52",
        "title": "Do LLMs Really Need 10+ Thoughts for \"Find the Time 1000 Days Later\"? Towards Structural Understanding of LLM Overthinking",
        "link": "/arxiv/2510.07880",
        "arxiv_id": "2510.07880",
        "authors": "Xinliang Frederick Zhang, Anhad Mohananey, Alexandra Chronopoulou, Pinelopi Papalampidi, Somit Gupta, Tsendsuren Munkhdalai, Lu Wang, Shyam Upadhyay",
        "summary": "Models employing long chain-of-thought (CoT) reasoning have shown superior performance on complex reasoning tasks. Yet, this capability introduces a critical and often overlooked inefficiency -- overthinking -- models often engage in unnecessarily extensive reasoning even for simple queries, incurring significant computations without accuracy improvements. While prior work has explored solutions to mitigate overthinking, a fundamental gap remains in our understanding of its underlying causes. Most existing analyses are limited to superficial, profiling-based observations, failing to delve into LLMs' inner workings. This study introduces a systematic, fine-grained analyzer of LLMs' thought process to bridge the gap, TRACE. We first benchmark the overthinking issue, confirming that long-thinking models are five to twenty times slower on simple tasks with no substantial gains. We then use TRACE to first decompose the thought process into minimally complete sub-thoughts. Next, by inferring discourse relationships among sub-thoughts, we construct granular thought progression graphs and subsequently identify common thinking patterns for topically similar queries. Our analysis reveals two major patterns for open-weight thinking models -- Explorer and Late Landing. This finding provides evidence that over-verification and over-exploration are the primary drivers of overthinking in LLMs. Grounded in thought structures, we propose a utility-based definition of overthinking, which moves beyond length-based metrics. This revised definition offers a more insightful understanding of LLMs' thought progression, as well as practical guidelines for principled overthinking management.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.769646",
        "filter_reason": "这篇论文完全符合你的研究范围，是一篇关于提升大语言模型（LLM）通用推理能力的高质量前沿研究。我的判断依据如下： 1.  **核心判断 (第一步): 论文本质是改进LLM的基础推理能力。** 论文的核心是解决LLM在应用“思维链”这一核心推理范式时出现的“overthinking”（过度思考）问题。它没有将LLM作为工具应用于特定领域，而是深入分析LLM推理过程的内部机制。其目标是让LLM的推理过程更高效、更精准，这直接属于“改进LLM的基础能力”和“增强其...多步推理等通用能力”的范畴。 2.  **正面指标 (第二步): 论文高度契合关键主题。** 论文明确围绕`Large language models (LLMs)`的`reasoning`能力展开，特别是`chain-of-thought (CoT)`这一多步推理技术。它通过提出新的分析框架`TRACE`，对LLM的“inner workings”（内部运作）进行深度研究，这本身就是一种`deep research`。其发现对于理解和优化LLM的`problem-solving`过程至关重要。 3.  **排除标准 (第三步): 论文不涉及任何排除领域。** 该研究是纯粹基于文本的推理分析，完全不涉及`多模态与视觉`。它的研究问题是通用的（如“1000天后是星期几”），而非聚焦于`医疗、化学`等`特定应用领域`。同时，它研究的是推理过程的效率和质量，而非模型部署后的`水印、安全`等应用层面的可靠性问题。 4.  **特殊和模糊情况 (第四步): 论文属于基础性研究，而非应用。** 这篇论文可以看作是对LLM推理过程的一种“可解释性”研究。它不是简单地讨论现象，而是提出了一种新的分析方法（`TRACE`），揭示了“overthinking”背后的结构性原因（`over-verification`和`over-exploration`），并提出了基于此的新定义和改进原则。这种对内在机制的深刻洞察，旨在从根本上提升模型的推理质量和效率，完全符合保留标准。 **总结:** 该论文的核心贡献在于，它没有停留在“CoT有效”的表面，而是深入到“CoT为何有时低效”这一更深层次的问题。通过系统性的分析，它揭示了LLM推理过程中的结构性缺陷，并为更高效、更智能的推理机制提供了理论基础和实践指导。这直接服务于“提高大语言模型本身的『通用推理能力』”这一核心目标，因此应被保留。"
    },
    {
        "index": "#68",
        "title": "ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs",
        "link": "/arxiv/2510.07737",
        "arxiv_id": "2510.07737",
        "authors": "Fu Chen, Peng Wang, Xiyin Li, Wen Li, Shichi Lei, Dongdong Xiang",
        "summary": "Training Large Language Models (LLMs) with Group Relative Policy Optimization (GRPO) encounters a significant challenge: models often fail to produce accurate responses, particularly in small-scale architectures. This limitation not only diminishes performance improvements and undermines the potential of GRPO but also frequently leads to mid-training collapse, adversely affecting stability and final efficacy. To address these issues, we propose ToolExpander, a novel framework that advances tool-oriented reinforcement learning for resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round Hard Sampling, which dynamically substitutes challenging samples(those without correct outputs over 10 rollouts) with high-quality few-shot demonstrations during training, coupled with an exponential learning rate decay strategy to mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO framework that eliminates KL divergence and incorporates adjusted clipping coefficients, encouraging models to autonomously generate and analyze few-shot examples via a minimal additional reward (0.01).Experimental results demonstrate that ToolExpander significantly enhances tool-using capabilities in LLMs, especially in weaker small-scale models, improving both training stability and overall performance.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.804766",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一个名为“ToolExpander”的新框架。这个框架旨在通过改进强化学习训练过程（GRPO），来提升大语言模型（尤其是规模较小的模型）的**工具使用能力**。工具使用是LLM实现复杂规划和多步推理的关键能力之一，属于其**通用推理能力**的核心组成部分。因此，这篇论文的本质是**改进LLM的基础能力**和**提出新的训练范式**，而不是将LLM应用于特定领域。这完全符合您的核心目标。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 明确聚焦于 \"Large Language Models (LLMs)\"。 *   **能力方向**: 致力于提升 \"tool-using capabilities\"，这直接关联到 \"problem-solving\" 和 \"reasoning\"。 *   **训练方法**: 核心方法是 \"Group Relative Policy Optimization (GRPO)\"，属于 \"reinforcement learning (RL)\" 的范畴。 *   **新兴范式**: 论文主题是 \"Tool-Using\"，这是当前提升LLM能力的重要新兴范式。 3.  **第三步：排除标准** 论文完全没有触及任何排除标准： *   它不涉及多模态、视觉或机器人控制。 *   它的研究是通用的，没有限定在医疗、化学等任何特定应用领域。 *   它关注的是训练过程中的性能和稳定性，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文主题恰好是“工具使用”。根据您的标准，这篇论文提出的是一种**通用的工具使用训练框架**，旨在增强LLM的**通用问题解决能力**，而不是将其应用于某个特定领域（如“用于化学实验的智能体”）。因此，它应该被**保留**。 **最终决策**: 综合以上分析，这篇论文的核心是提出一种创新的训练方法（ToolExpander），以增强LLM的通用工具使用能力，从而提升其整体的推理和问题解决水平。这与您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#70",
        "title": "SUBQRAG: sub-question driven dynamic graph rag",
        "link": "/arxiv/2510.07718",
        "arxiv_id": "2510.07718",
        "authors": "Jiaoyang Li, Junhao Ruan, Shengwei Tang, Saihan Chen, Kaiyan Chang, Yuan Ge, Tong Xiao, Jingbo Zhu",
        "summary": "Graph Retrieval-Augmented Generation (Graph RAG) effectively builds a knowledge graph (KG) to connect disparate facts across a large document corpus. However, this broad-view approach often lacks the deep structured reasoning needed for complex multi-hop question answering (QA), leading to incomplete evidence and error accumulation. To address these limitations, we propose SubQRAG, a sub-question-driven framework that enhances reasoning depth. SubQRAG decomposes a complex question into an ordered chain of verifiable sub-questions. For each sub-question, it retrieves relevant triples from the graph. When the existing graph is insufficient, the system dynamically expands it by extracting new triples from source documents in real time. All triples used in the reasoning process are aggregated into a \"graph memory,\" forming a structured and traceable evidence path for final answer generation. Experiments on three multi-hop QA benchmarks demonstrate that SubQRAG achieves consistent and significant improvements, especially in Exact Match scores.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.805850",
        "filter_reason": "这篇论文完全符合研究范围，理由如下： 1.  **核心判断（符合保留标准）**: 论文的核心是提出一种名为“SubQRAG”的新框架，旨在解决LLM在“复杂多跳问答”任务中“深度结构化推理”不足的问题。这直接命中了我筛选标准的核心——『提升大语言模型的通用推理能力』。它不是将LLM作为工具应用于某个垂直领域，而是提出一种普适性的方法论来增强LLM在处理复杂问题时的多步推理、规划和证据链构建能力。 2.  **正面指标（高度相关）**: *   **能力方向**: 论文明确聚焦于 \"reasoning\"（推理），特别是 \"complex multi-hop question answering\"（复杂多跳问答），这是衡量LLM逻辑和规划能力的核心场景。 *   **核心概念**: 论文的技术基础是 \"Graph Retrieval-Augmented Generation\" (Graph RAG)，这是一种与LLM紧密协同工作的技术，其目标就是为了增强LLM的知识获取和推理能力。 *   **新兴范式**: SubQRAG框架本身可以被看作是一种高级的『工具使用』和『深度研究』范式。它引导LLM动态地使用“知识图谱”和“原始文档”作为工具，并构建结构化的“图记忆”来支持其推理过程。 3.  **排除标准（未触及）**: *   论文研究的是纯文本领域的通用问答框架，不涉及多模态、视觉等内容。 *   实验是在通用的多跳QA基准上进行的，而非特定应用领域（如医疗、化学等）。 *   论文虽然提升了答案质量，但其核心贡献是方法论，而非专注于水印、安全等应用层面的可靠性研究。 4.  **特殊/模糊情况处理（符合保留逻辑）**: *   **智能体/工具使用**: SubQRAG提出的是一个通用的、用于增强推理的框架，它引导LLM如何与外部知识结构（知识图谱）进行交互和扩展，这完全符合保留条件。 *   **幻觉/可解释性**: 论文通过构建“结构化且可追踪的证据路径”，极大地增强了LLM推理过程的透明度和可解释性，有效减少了因证据不全或错误累积导致的推理失误。这正是一种通过提升内在可靠性来优化通用推理质量的方法。 **核心依据**: 该论文的本质是提出一种创新的、系统化的方法论，直接优化和增强了LLM在处理复杂问题时的核心推理过程。它通过问题分解、动态检索和证据聚合，提升了LLM的推理深度和准确性，这与“提高大语言模型本身的通用推理能力”这一核心目标完全一致。"
    },
    {
        "index": "#90",
        "title": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding",
        "link": "/arxiv/2510.07486",
        "arxiv_id": "2510.07486",
        "authors": "Shuqing Luo, Yilin Guan, Pingzhi Li, Hanrui Wang, Tianlong Chen",
        "summary": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT), but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM decoding. Query-aware page-level sparse decoding can achieve state-of-the-art performance under constrained FLOPs budgets, but is limited by both sequential-dependent page filtering and coarse-grained token selection, hampering serving efficiency and model performance on TTS tasks under high concurrency and long CoT scenarios (consuming even higher runtime than the forward pipeline itself). In this paper, we first find that the current-step query state can be accurately approximated in a unified manner from a short window of recent queries, enabling training-free query-aware sparsity without waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework for efficient TTS built on two core components: (1) a novel light-weight temporal-regressive module that predicts the next-token query state; (2) an asynchronous and disaggregated framework that decouples the KV cache filtering from the auto-regressive decoding loop, overlapping the token-level KV selection with the forward inference computation through asynchronism. To our knowledge, AsyncSpade is the first to eliminate the sequential dependence without sacrificing model performance. We validate the effectiveness of AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade fully overlaps KV-cache operations with the inference pipeline, achieving theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and at least 50% TPOT reduction compared to full attention on Qwen3-8B and Qwen3-32B models, while matching or surpassing their accuracy on various TTS benchmarks (AIME-24/25, GPQA-Diamond, MATH-500).",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.844024",
        "filter_reason": "这篇论文完全符合关于\"大语言模型通用推理能力\"的研究范围。我的判断依据如下： **第一步：核心判断** *   **论文本质**: 这篇论文的核心贡献是提出了一种名为`AsyncSpade`的异步稀疏解码框架。其根本目标是解决\"Test-Time Scaling (TTS)\"过程中的效率瓶颈。TTS是一种通过生成长思维链来显著提升LLM推理能力的前沿范式。因此，该论文的本质并非通用的模型部署优化，而是**为提升LLM推理能力这一核心目标而设计的专用优化方法**。它致力于让一种关键的推理技术（长CoT）变得更高效、更具可扩展性，这直接属于改进LLM基础能力的范畴。 **第二步：正面指标** *   **核心概念**: 论文明确以LLM（Qwen3-8B/32B）为研究对象。 *   **能力方向**: 论文的核心是提升\"reasoning\"能力。它通过优化TTS来增强LLM的推理，并在顶级的通用推理基准上进行了验证，包括数学推理（AIME-24/25, MATH-500）和科学逻辑推理（GPQA-Diamond）。这些都是衡量通用推理能力的黄金标准。 *   **新兴范式**: 论文聚焦于\"Test-Time Scaling\"（测试时缩放），这是当前提升LLM推理能力的一个核心新兴范式，与思维链紧密相关。 **第三步：排除标准** *   **多模态与视觉**: 论文完全不涉及视觉或多模态内容。 *   **特定应用领域**: 论文的验证基准（AIME, MATH, GPQA）是通用的、跨领域的推理任务，而非医疗、化学等特定应用领域。 *   **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** *   **基础设施 vs. 核心能力**: 这是最关键的判断点。虽然论文涉及了KV-cache、TPOT（每输出词元时间）、A100节点部署等看似属于基础设施的术语，但其**动机和落脚点**是服务于\"推理\"这一核心能力。它不是一篇泛泛而谈的\"如何加速LLM推理\"的工程论文，而是一篇\"如何通过优化计算流程来赋能深度推理\"的方法论论文。其创新点（异步解耦、状态预测）都是为了在不牺牲推理准确性的前提下，让长CoT这种高成本推理方法变得实用。这种优化与推理范式本身是深度耦合的，因此应被视为对推理能力的一种增强，而非单纯的部署优化。 **第五步：最终决策** 综合以上分析，该论文提出了一种新颖的异步解码框架，旨在解决长思维链推理过程中的计算瓶颈，从而高效地提升大语言模型的通用推理能力。它直接针对一种核心的推理范式进行优化，并在顶级的通用推理基准上证明了其有效性。因此，这篇论文与您的研究课题高度相关，应当保留。"
    },
    {
        "index": "#87",
        "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs",
        "link": "/arxiv/2510.07499",
        "arxiv_id": "2510.07499",
        "authors": "Soyeong Jeong, Taehee Jung, Sung Ju Hwang, Joo-Kyung Kim, Dongyeop Kang",
        "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands of tokens in a single prompt, enabling new opportunities for knowledge-intensive multi-hop reasoning by integrating large sets of retrieved documents or, in some cases, directly all necessary information. However, simply feeding more documents into the context window fails to capture how evidence should be connected. We address this gap with thought templates, which recast reasoning as reusable thought caches, derived from prior problem solving traces, structuring how evidence is combined and guiding multi-hop inference with factual documents. To keep these templates effective, we propose an update strategy that iteratively refines templates derived from training data through natural-language feedback. Across diverse benchmarks and LCLM families, our approach delivers consistent gains over strong baselines in both retrieval-based and retrieval-free settings. Furthermore, we show that optimized templates can be distilled into smaller open-source models, demonstrating its broad applicability and transparent reasoning reuse. We refer to our framework as Thought Template Augmented LCLMs (ToTAL).",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.842522",
        "filter_reason": "这篇论文完全符合筛选要求，其核心贡献直接指向提升大语言模型的通用推理能力。 以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心是提出一种名为“思维模板”的新方法，旨在解决长上下文语言模型（LCLM）在进行多跳推理时的核心痛点——无法有效连接和整合证据。这并非将LLM作为工具应用于特定领域，而是直接改进模型本身的推理机制和过程。它提出了一种新的推理范式，将推理过程结构化、可重用化，这与思维链（CoT）等提升模型基础能力的研究一脉相承。因此，根据第一步标准，应**保留**。 2.  **第二步：正面指标** - **核心概念**: 论文明确聚焦于“Long-Context Language Models (LCLMs)”，属于LLMs范畴。 - **能力方向**: 论文的核心是“multi-hop reasoning”（多跳推理），这是通用推理能力的关键组成部分。摘要中反复提及“reasoning”、“inference”，完全符合筛选目标。 - **训练方法**: 论文提出了一种“通过自然语言反馈迭代优化模板”的更新策略，这是一种模型自我改进和优化的方法，与“self-evolve”的理念相符。 - **新兴范式**: “思维模板”可以被看作是一种增强LLM推理能力的通用框架或工具，它指导模型如何使用上下文中的事实信息，这与“tool use”和“deep research”的范式精神一致。 3.  **第三步：排除标准** - 论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）。其评估是在“多样化的基准测试”上进行的，证明了其通用性。因此，不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: “思维模板”是一种通用的推理增强框架，它不绑定任何特定领域，旨在提升模型在知识密集型任务中的通用问题解决能力。这符合“提出一种通用的...工具使用方法来增强LLM的通用问题解决能力”的保留条件。 - **幻觉/可解释性/安全**: 论文通过“结构化证据组合”和“透明的推理重用”，内在地提升了推理过程的可解释性和质量，从而有助于减少无根据的推理（幻觉的一种形式）。这属于“提升模型的通用可靠性和推理质量”的范畴，应予以保留。 **最终决策**: 综合以上分析，该论文的核心贡献是提出了一种创新的、通用的方法论（思维模板），直接针对并增强了大语言模型在长上下文下的多步推理能力。它不涉及特定应用领域，而是聚焦于模型基础能力的提升，完全契合“提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#91",
        "title": "MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference",
        "link": "/arxiv/2510.07475",
        "arxiv_id": "2510.07475",
        "authors": "Zheyuan Zhang, Lin Ge, Hongjiang Li, Weicheng Zhu, Chuxu Zhang, Yanfang Ye",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, and LLM-based agents further extend these abilities to various practical workflows. While recent progress shows that multi-agent systems (MAS) can outperform single agents by coordinating specialized roles, designing effective MAS remains difficult due to prompt sensitivity and the compounded instability MAS creates. To cope with the challenge, recent efforts in automated prompt design have reduced manual effort. However, multi-agent prompt optimization remains largely unexplored. Challenges like exponentially expanding search space and ambiguous credit assignment together make systematic design intractable without principled methods. Therefore, we introduce M}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first formulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference problem and solves it using a language-guided variant of max-product belief propagation algorithm. To address credit assignment and updates the system iteratively, MAPRO employs a topology-aware refinement mechanism that integrates execution feedback and downstream blames to selectively update agent prompts. Through this process, MAPRO progressively converges to a coordinated set of agent-specific prompt policies. Across benchmarks in various tasks, MAPRO achieves state-of-the-art performance, consistently surpassing manually engineered baselines and recent automated alternatives. Beyond performance, our MAP-based formulation also delivers general guidelines for building more reliable and principled multi-agent systems in the future",
        "subjects": "Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.844503",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一个名为MAPRO的框架，用于解决多智能体系统（MAS）中的提示优化问题。这并非将LLM作为工具应用于某个特定领域，而是提出了一种**新的方法论（框架）**来提升由LLM组成的智能体系统的整体性能。通过优化智能体间的协作与提示，该研究直接致力于增强LLM在复杂任务中的**通用问题解决和协调推理能力**。这完全符合“改进LLM基础能力”和“增强智能体协作框架”的保留标准。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 明确以 \"Large language models (LLMs)\" 为基础。 *   **能力方向**: 虽然未直接使用\"math reasoning\"等词，但\"multi-agent systems\"、\"coordination\"、\"problem-solving\"以及优化整个系统以在\"various tasks\"上取得更好表现，都指向了提升模型的**通用规划和多步推理能力**。 *   **新兴范式**: 论文的主题就是 \"multi-agent systems\" 和 \"llm-based agents\"，这正是你关注的前沿范式。 3.  **第三步：排除标准** 论文完全没有触及任何排除标准。它不涉及多模态、视觉，也不是针对医疗、化学等特定应用领域，更不讨论模型部署或水印等基础设施或应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 该论文是“智能体/工具使用”这一特殊情况的完美范例。它提出的是一个**通用的智能体协作框架（MAPRO）**，旨在通过系统化的方法优化多智能体协作，从而提升LLM的通用问题解决能力，而不是将智能体应用于某个垂直领域。因此，它应该被保留。 **最终决策**: 综合以上分析，这篇论文的本质是提出一种创新的、基于概率推断的框架来优化多智能体系统的提示，其根本目标是提升LLM在复杂任务中的协作与推理表现。这与你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，这篇论文应被筛选为**符合要求**。"
    },
    {
        "index": "#89",
        "title": "Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics",
        "link": "/arxiv/2510.07488",
        "arxiv_id": "2510.07488",
        "authors": "Rasika Muralidharan, Jaewoon Kwak, Jisun An",
        "summary": "Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are gaining attention, yet fewer studies explore their team dynamics. Inspired by human team science, we propose a multi-agent framework to examine core aspects of team science: structure, diversity, and interaction dynamics. We evaluate team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and Latent Implicit Hate, spanning commonsense and social reasoning. Our results show that flat teams tend to perform better than hierarchical ones, while diversity has a nuanced impact. Interviews suggest agents are overconfident about their team performance, yet post-task reflections reveal both appreciation for collaboration and challenges in integration, including limited conversational coordination.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.843544",
        "filter_reason": "该论文完全符合您的研究目标。我的判断过程如下： 1.  **核心判断（第一步）**：论文的核心是提出一个**多智能体框架**，旨在研究如何通过优化团队结构、多样性和交互动态来提升LLM智能体团队的性能。这并非将LLM作为工具应用于特定领域，而是直接探索一种**新的方法论（多智能体协作）来增强LLM的通用能力**。其评估任务（CommonsenseQA, StrategyQA等）是典型的**通用推理能力**（常识推理、策略推理）基准测试，而非特定领域问题。因此，这篇论文的本质是改进LLM的基础能力，应予以保留。 2.  **正面指标（第二步）**：论文包含了多个强相关的正面指标： *   **核心概念**: 明确以“Large Language Model (LLM)-powered agents”为核心。 *   **能力方向**: 研究任务直接指向“commonsense and social reasoning”，这正是通用推理能力的核心组成部分。 *   **新兴范式**: 论文的研究主题是“Multi-Agent Systems (MAS)”，是当前提升LLM能力的前沿范式之一。 3.  **排除标准（第三步）**：论文完全不涉及任何排除标准中的领域。它聚焦于纯文本的推理任务，没有涉及多模态、视觉、特定应用领域（如医疗、化学）或应用层面的模型可靠性问题。 4.  **特殊和模糊情况（第四步）**：该论文是关于智能体框架的典型案例。它提出的是一个**通用的多智能体协作框架**，旨在解决通用推理问题，而非应用于特定领域（如“用于化学实验的智能体”）。因此，根据筛选规则，这种情况应当保留。 **最终决策**：综合以上分析，这篇论文的核心贡献在于探索如何通过借鉴人类团队科学，构建更高效的LLM多智能体系统，以提升其在通用推理任务上的表现。这直接契合了您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为符合要求。"
    },
    {
        "index": "#96",
        "title": "Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation",
        "link": "/arxiv/2510.07414",
        "arxiv_id": "2510.07414",
        "authors": "Mufei Li, Dongqi Fu, Limei Wang, Si Zhang, Hanqing Zeng, Kaan Sancak, Ruizhong Qiu, Haoyu Wang, Xiaoxin He, Xavier Bresson, Yinglong Xia, Chonglin Sun, Pan Li",
        "summary": "Modern long-context large language models (LLMs) perform well on synthetic \"needle-in-a-haystack\" (NIAH) benchmarks, but such tests overlook how noisy contexts arise from biased retrieval and agentic workflows. We argue that haystack engineering is necessary to construct noisy long contexts that faithfully capture key real-world factors -- distraction from heterogeneous biased retrievers and cascading errors in agentic workflows -- to test models' long-context robustness. We instantiate it through HaystackCraft, a new NIAH benchmark built on the full English Wikipedia hyperlink network with multi-hop questions. HaystackCraft evaluates how heterogeneous retrieval strategies (e.g., sparse, dense, hybrid, and graph-based) affect distractor composition, haystack ordering, and downstream LLM performance. HaystackCraft further extends NIAH to dynamic, LLM-dependent settings that simulate agentic operations, where models refine queries, reflect on their past reasonings, and decide when to stop. Experiments with 15 long-context models show that (1) while stronger dense retrievers can introduce more challenging distractors, graph-based reranking simultaneously improves retrieval effectiveness and mitigates more harmful distractors; (2) in agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated distractors or struggle to perform early stops. These results highlight persistent challenges in agentic long-context reasoning and establish HaystackCraft as a valuable testbed for future progress.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.867513",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： **第一步：核心判断** 这篇论文的本质是提出一种新的评估基准，用于更精准地衡量大语言模型在复杂、真实场景下的通用推理能力。虽然它没有直接提出一种新的训练方法或模型架构来“改进”LLM，但它通过构建一个更接近现实世界挑战的“试炼场”，来诊断当前LLM在通用推理上的核心弱点。这种对能力的“诊断”和“度量”是推动能力“提升”不可或缺的一环。论文的核心贡献是揭示了LLM在“异构检索干扰”和“智能体工作流中的级联错误”这两类通用推理场景下的脆弱性，这直接指向了未来需要攻克的研究方向。因此，它的本质是推动LLM通用推理能力发展的基础性研究，而非应用性研究。 **第二步：正面指标** 论文高度符合多个正面指标： - **核心概念**: 论文明确聚焦于 \"large language models (LLMs)\"。 - **能力方向**: 论文的核心是评估模型的 \"agentic long-context reasoning\"（智能体长上下文推理）能力，这直接属于通用推理的范畴。它通过 \"multi-hop questions\"（多跳问题）来测试模型的复杂问题解决能力。 - **新兴范式**: 论文的核心贡献之一就是将评估扩展到 \"agentic workflows\"（智能体工作流）中，模拟了模型“精炼查询、反思推理、决定何时停止”等智能体行为。这正是当前提升LLM通用推理能力的前沿范式。 **第三步：排除标准** 论文完全避开了所有排除标准： - 论文与多模态、视觉无关。 - 论文使用的语料是通用的维基百科，而非任何特定应用领域（如医疗、化学等）。 - 论文讨论的模型可靠性是“推理鲁棒性”和“级联错误”，属于推理能力的内在质量问题，而非应用层面的水印、安全或社会学研究。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文是提出一种通用的框架来评估智能体的推理过程，而不是将智能体应用于特定领域。它测试的是智能体在通用任务（基于维基百科回答问题）中的推理质量和鲁棒性，这完全符合保留标准。论文发现Gemini 2.5 Pro和GPT-5等先进模型在智能体推理中仍存在“cascading failures”（级联失败），这正是对当前LLM通用推理能力瓶颈的深刻洞察。 **第五步：最终决策** 综合分析，这篇论文虽然是一篇评估（Benchmark）论文，但其核心目标与你的研究课题“提高大语言模型（LLM）本身的『通用推理能力』”高度一致。它没有将LLM视为工具去解决外部问题，而是深入探究LLM在模拟真实、复杂的通用推理场景（特别是智能体工作流）中的内在能力边界和失败模式。通过构建一个更科学的“尺子”，它为整个领域指明了提升LLM通用推理能力的具体方向。因此，这篇论文是高度相关且极具价值的前沿研究，应当保留。"
    },
    {
        "index": "#88",
        "title": "Can Speech LLMs Think while Listening?",
        "link": "/arxiv/2510.07497",
        "arxiv_id": "2510.07497",
        "authors": "Yi-Jen Shih, Desh Raj, Chunyang Wu, Wei Zhou, SK Bong, Yashesh Gaur, Jay Mahadeokar, Ozlem Kalinli, Mike Seltzer",
        "summary": "Recent advances in speech large language models (speech LLMs) have enabled seamless spoken interactions, but these systems still struggle with complex reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning has been to shown to significantly improve the reasoning abilities of text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for multi-stream speech LLMs, demonstrating that reasoning in text space improves the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken reasoning tasks. Beyond accuracy, the latency of the spoken response is a crucial factor for interacting with voice-based agents. Inspired by the human behavior of \"thinking while listening,\" we propose methods to reduce the additional latency from reasoning by allowing the model to start reasoning before the user query has ended. To achieve this, we introduce an entropy-based metric, \"question completeness,\" which acts as an indicator to guide the model on the optimal time to start reasoning. This method provides greater control over the accuracy-latency trade-off compared with heuristic-based approaches and, under equivalent latency conditions, yields a 4% accuracy gain on ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference data created using rejection sampling to push the accuracy-latency pareto frontier further, resulting in a 70% reduction in latency without loss in accuracy.",
        "subjects": "Computation and Language, Artificial Intelligence, Audio and Speech Processing",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.843088",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献直接聚焦于提升大语言模型的通用推理能力。 1.  **核心判断（第一步）**: 论文的本质是改进LLM的基础能力，而非将其作为工具应用于特定领域。论文的核心问题是如何让“语音大语言模型”更好地进行“复杂推理任务”。它没有深入研究语音识别或合成本身，而是将语音作为交互的入口，其研究重心和核心创新全部围绕着如何优化和增强模型内部的“推理”过程。这完全符合“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的保留标准。 2.  **正面指标（第二步）**: 论文高度契合多个正面指标。 *   **核心概念**: 论文研究对象是“speech large language models”，属于LLM范畴。 *   **能力方向**: “reasoning”, “complex reasoning tasks”, “spoken reasoning tasks”等关键词反复出现，明确指向推理能力。 *   **训练方法**: 论文使用了“Direct Preference Optimization (DPO)”，这是一种先进的对齐/强化学习方法，用于优化模型的推理表现。 *   **新兴范式**: 论文探讨了“voice-based agents”的推理延迟问题，其提出的“thinking while listening”机制可以被视为一种提升智能体实时问题解决效率的新范式。 3.  **排除标准（第三步）**: 论文不触及任何排除标准。 *   **多模态与视觉**: 尽管论文涉及“Speech”，但其核心贡献并非语音处理技术或多模态融合。作者明确指出“reasoning in text space”，说明其方法论是在文本推理层面进行的，语音只是I/O模态。研究的焦点是推理过程本身，而非模态。因此，不应被排除。 *   **特定应用领域**: 论文在通用的“spoken reasoning tasks”和“ARC-Easy”等基准上测试，没有局限于任何特定垂直领域。 *   **模型可靠性（应用层面）**: 论文虽然讨论了延迟，但这是从推理过程优化的角度出发，旨在提升推理效率和质量，而非讨论水印、安全等应用层面的可靠性问题。 4.  **特殊和模糊情况（第四步）**: *   **智能体/工具使用**: 论文虽然提到了“voice-based agents”，但其核心是提出一种通用的、旨在提升推理效率的方法（“thinking while listening”），而不是设计一个用于特定领域的智能体。这符合“保留”的情况。 **最终决策（第五步）**: 综上所述，这篇论文的核心贡献在于： 1.  **验证并提升了语音LLM的推理能力**：通过应用CoT，显著提升了模型在口语推理任务上的准确性。 2.  **提出了一种优化推理过程效率的新方法**：受“边听边想”启发，引入“问题完整性”指标，让模型在输入未完成时就提前启动推理，从而在准确性和延迟之间取得更好的平衡。 3.  **使用先进的训练技术进一步优化**：采用DPO来推动准确率-延迟的帕累托前沿，实现了在不损失精度的情况下大幅降低延迟。 这些贡献都是直接、深入地针对LLM的**通用推理能力**（包括准确性和效率）进行的方法论创新，完全符合你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，应予以保留。"
    },
    {
        "index": "#102",
        "title": "CaRT: Teaching LLM Agents to Know When They Know Enough",
        "link": "/arxiv/2510.08517",
        "arxiv_id": "2510.08517",
        "authors": "Grace Liu, Yuxiao Qu, Jeff Schneider, Aarti Singh, Aviral Kumar",
        "summary": "Many tasks require learned models to strategically gather relevant information over multiple rounds of interaction before actually acting on a task. Strategic information gathering requires models to know not only how to effectively acquire information, but also when to stop gathering information and make a decision, in order to avoid overthinking or getting derailed when acting. In this paper, we formalize this problem and introduce Counterfactuals and Reasoning for Termination (CaRT), an approach for teaching LLMs when to stop seeking information. To appropriately learn when to terminate, CaRT fine-tunes LLMs using counterfactual pairs of trajectories, one where termination is appropriate and a minimally modified version of the same trajectory where it is not. It trains the LLM to explain the rationale for the termination decision in either case via verbal reasoning, and imbues this capability into the base LLM via fine-tuning. We instantiate CaRT in two domains: interactive medical diagnosis and math problem solving. In both domains, we find that CaRT improves the efficiency of information gathering and task success rate compared to other fine-tuning methods.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.871700",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献是提出了一种提升大语言模型通用推理能力的新方法论。以下是基于您筛选标准的详细判断过程： 1.  **第一步：核心判断** - 论文的核心是提出一种名为CaRT（Counterfactuals and Reasoning for Termination）的新方法，用于**教导LLM智能体在信息收集过程中“何时停止”**。这个“何时停止”的决策能力，是高级推理和问题解决过程中的一个关键**元认知能力**。它直接关系到模型能否高效地进行多步推理、规划，并避免在无关信息上过度消耗计算资源。因此，论文的本质是**增强LLM的基础通用能力（决策与规划），并提出了新的训练范式（基于反事实的微调）**。这与您的核心目标高度一致。 2.  **第二步：正面指标** - **核心概念**: 论文明确聚焦于“LLM Agents”。 - **能力方向**: 论文的核心是提升“strategic information gathering”的效率，这属于**推理** 和 **问题解决** 的范畴。它直接解决了多步推理中的一个关键瓶颈。 - **训练方法**: 提出了一种新颖的微调方法，通过“counterfactual pairs of trajectories”和“verbal reasoning”来训练模型，这是一种创新的训练方法论。 - **新兴范式**: 研究对象是“LLM-based agents”，探讨了它们如何更智能地与环境交互和做决策。 3.  **第三步：排除标准** - 论文虽然提到了“interactive medical diagnosis”作为其应用领域之一，但这并**不是论文的主要焦点**。作者选择医疗诊断和数学问题求解这两个截然不同的领域，是为了**验证CaRT方法的通用性**，而不是为了解决医疗领域的特定问题。论文的贡献是CaRT这个通用方法论本身，而非其在医疗领域的应用。因此，它不应被归类为“特定应用领域”的论文而被排除。 - 论文不涉及多模态、视觉、模型基础设施或应用层面的安全水印等问题。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的方法CaRT是一种**通用的智能体决策框架**，旨在增强LLM智能体进行通用问题解决时的效率。它教给智能体一个通用的原则（何时停止信息寻求），而不是一个特定领域的技能。这完全符合“保留”标准。 - **幻觉/可解释性**: 论文通过让模型“explain the rationale for the termination decision via verbal reasoning”，将可解释性融入训练过程。这种做法**增强了模型内在的推理透明度和可靠性**，从而提升了其推理质量，符合“保留”标准。 5.  **第五步：最终决策** - 综合来看，这篇论文的核心贡献是提出了一种创新的训练方法（CaRT），用于提升LLM智能体在通用问题解决任务中的一个关键推理子能力——决策终止。它不是将LLM作为工具应用于特定领域，而是致力于从方法论层面增强LLM本身的通用推理效率和智能水平。因此，这篇论文与您的研究课题“大语言模型通用推理能力”高度相关，应当被保留。"
    },
    {
        "index": "#99",
        "title": "Agent Learning via Early Experience",
        "link": "/arxiv/2510.08558",
        "arxiv_id": "2510.08558",
        "authors": "Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, Jian Xie, Yuxuan Sun, Boyu Gou, Qi Qi, Zihang Meng, Jianwei Yang, Ning Zhang, Xian Li, Ashish Shah, Dat Huynh, Hengduo Li, Zi Yang, Sara Cao, Lawrence Jang, Shuyan Zhou, Jiacheng Zhu, Huan Sun, Jason Weston, Yu Su, Yifan Wu",
        "summary": "A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.",
        "subjects": "Artificial Intelligence, Computation and Language, Information Retrieval, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.869670",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是基于你筛选标准的详细判断过程： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出一种**新的训练范式**。它没有将LLM作为工具应用在特定领域，而是聚焦于解决LLM智能体自身学习能力的根本性问题。论文的核心贡献是“early experience”这一中间范式，旨在让智能体通过自身交互产生的数据进行学习，从而提升其**基础能力**。具体来说，它通过“self-reflection”（自我反思）机制来**改进智能体的推理和决策能力**。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划、多步推理等通用能力”的保留标准。 **第二步：正面指标——论文是否包含以下主题？** 该论文命中了多个关键的正面指标： - **核心概念**: 论文研究对象是“language agents”，其基础就是LLMs。 - **能力方向**: 摘要中明确指出，其方法旨在“improve reasoning and decision-making”（提升推理和决策能力），这正是你关注的核心。 - **训练方法**: 论文提出了一种介于模仿学习和强化学习之间的新方法，属于“self-evolve”（自我进化）的范畴，旨在解决传统RL训练的难题。 - **新兴范式**: 整篇论文都是关于“llm-based agents”如何通过自身经验进行学习，是智能体研究的前沿方向。 **第三步：排除标准——论文是否主要聚焦于以下领域？** 该论文**不涉及**任何排除标准领域： - 它没有涉及视觉或多模态内容。 - 它明确强调在“八个多样化的环境”（eight diverse environments）中进行评估，这表明其方法是**通用的**，而非针对特定领域（如医疗、化学等）。 - 它研究的不是模型可靠性（如水印、安全），而是提升智能体内在的学习和推理效能。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 该论文提出了一个**通用的智能体学习框架**。虽然它提到了“multi-turn tool use”作为一个挑战性的环境类型，但其提出的“early experience”范式是普适的，旨在增强智能体在任何需要多步规划和工具使用的场景下的通用问题解决能力，而非仅限于某个特定领域。因此，这完全符合保留条件。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是一种方法论创新，它提出了一种名为“early experience”的新训练范式，通过让LLM智能体从自身行为中学习并进行“自我反思”，来系统性地提升其内在的通用推理和决策能力。论文的研究目标、方法、实验和结论都紧密围绕“增强LLM通用推理能力”这一核心主题，是高度相关的前沿研究，因此应予以保留。"
    },
    {
        "index": "#111",
        "title": "Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization",
        "link": "/arxiv/2510.08256",
        "arxiv_id": "2510.08256",
        "authors": "Jason Bohne, Pawel Polak, David Rosenberg, Brian Bloniarz, Gary Kazantsev",
        "summary": "Direct Preference Optimization (DPO) has recently emerged as a simple and effective alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with user preferences. However, existing DPO formulations rely on a single monolithic model, which limits their expressivity in multi-task settings and their adaptability to heterogeneous or diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a framework that extends DPO with both soft mixture models and mixture-of-experts (MoE) architectures, using a stochastic variational inference approach. Our method introduces a latent-variable model over expert assignments and optimizes a variational evidence lower bound (ELBO), enabling stable and efficient learning of specialized expert policies from preference data. Mix- and MoE-DPO provides three key advantages over standard DPO: (i) generalization via universal function approximation through mixtures; (ii) reward and policy specialization through expert components tailored to distinct preference modes; and (iii) contextual alignment through input-dependent soft gating that enables user-specific mixture policies. Our framework supports both shared base architectures with expert-specific policy heads and fully independent expert models, allowing flexible trade-offs between parameter efficiency and specialization. We validate our approach on a variety of model sizes and multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a powerful and scalable method for preference-based LLM alignment.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.888004",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心是提出一种名为“Mix- and MoE-DPO”的新训练范式，用于改进Direct Preference Optimization (DPO)方法。DPO是替代RLHF的LLM对齐技术。因此，这篇论文的本质是**改进LLM的基础训练与对齐方法**，而不是将其应用于特定领域。这完全符合“保留”标准中关于“提出新的训练范式，增强其……通用能力”的要求。LLM对齐是模型能够准确遵循人类指令、进行有效沟通和推理的基石，因此提升对齐能力就是提升其通用能力的基础。 2.  **第二步：正面指标** - **核心概念**: 论文明确以“large language models (LLMs)”为核心研究对象。 - **训练方法**: 论文聚焦于“Direct Preference Optimization (DPO)”，这是“reinforcement learning from human feedback (RLHF)”的替代方案，完全符合“reinforcement learning (RLHF, RL)”这一正面指标。 - **能力方向**: 虽然摘要没有直接使用“reasoning”一词，但其目标是“LLM alignment”。一个能够更好地对齐复杂、多样化用户偏好的模型，必然在理解意图、生成连贯且有逻辑的内容方面表现更出色。这直接关系到模型的通用问题解决能力。论文中提到的“multi-task settings”和“contextual alignment”也暗示了其目标是提升模型的泛化与适应能力，这是通用推理能力的重要组成部分。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准领域。它没有讨论多模态、视觉，没有针对任何特定应用领域（如医疗、化学），也没有关注水印、安全等应用层面的可靠性议题。其焦点纯粹在于模型本身的训练机制和架构。 4.  **第四步：处理特殊和模糊情况** 本论文不属于智能体/工具使用或幻觉/可解释性等特殊情况，因此无需特殊处理。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种创新的训练框架，通过引入混合模型和混合专家架构，来提升LLM在多样化偏好和多任务场景下的对齐效果。这直接作用于LLM的基础能力，旨在让模型变得更“通用”、更“智能”，能够更好地适应不同用户的复杂需求。这种对基础训练范式的改进，是提升大语言模型通用推理能力的根本性研究之一，与您的核心目标高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#115",
        "title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?",
        "link": "/arxiv/2510.08189",
        "arxiv_id": "2510.08189",
        "authors": "Yi Lu, Jianing Wang, Linsen Guo, Wei He, Hongyin Tang, Tao Gui, Xuanjing Huang, Xuezhi Cao, Wei Wang, Xunliang Cai",
        "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.895183",
        "filter_reason": "这篇论文完全符合您的筛选标准，核心贡献在于提升大语言模型本身的通用推理能力。 1.  **第一步：核心判断** 这篇论文的本质是提出一种新的训练范式来增强LLM的**长视界推理能力**。它不仅仅是一个评估基准，其核心贡献在于利用提出的R-HORIZON方法**构建训练数据**，并通过**强化学习（RLVR）**来训练模型。这直接改进了LLM在处理复杂、多步骤、长链条推理任务时的基础能力。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其……多步推理等通用能力”的保留标准。论文的研究对象是“Large Reasoning Models (LRMs)”本身，而非将其作为工具应用于特定领域。 2.  **第二步：正面指标** 论文高度符合多个正面指标： *   **核心概念**: 明确以“Large Reasoning Models (LRMs)”为研究对象。 *   **能力方向**: 核心议题是“reasoning”，特别是“long-horizon reasoning”和“multi-step reasoning”。在AIME2024（美国数学邀请赛）上的性能提升也直接证明了其在**数学推理**上的增强。 *   **训练方法**: 明确使用了“reinforcement learning with verified rewards (RLVR)”，这是一种强化学习方法，用于优化模型的推理过程。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准领域。它没有讨论多模态、视觉，没有将模型应用于医疗、化学等特定领域，也未涉及水印、安全等模型可靠性（应用层面）的问题。 4.  **第四步：处理特殊和模糊情况** 本论文不直接涉及智能体或幻觉/安全等特殊情况，但其方法论精神与这些情况下的“保留”原则一致：即提出一种**根本性的新方法**来提升模型的内在能力，而不是进行应用层面的探讨。 5.  **第五步：最终决策** 综合来看，这篇论文的核心是提出了一种名为R-HORIZON的范式，通过**数据生成**和**强化学习训练**两个环节，系统性地提升大语言模型在长视界、多步推理这一通用核心能力上的表现。其研究目标、方法和贡献都与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致，是一篇非常相关的前沿研究论文。"
    },
    {
        "index": "#110",
        "title": "Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries",
        "link": "/arxiv/2510.08325",
        "arxiv_id": "2510.08325",
        "authors": "Marius Dragoi, Ioana Pintilie, Florin Gogianu, Florin Brad",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm to improve Large Language Models on reasoning tasks such as coding, math or logic. To assess the reasoning boundary (the fraction of problems a model can solve) researchers often report Pass@k at large sampling budgets. Recent results reveal a crossover phenomenon: while RLVR models outperform the base model at small k values, the base model usually outperforms them when sampling a very large number of completions. This has been interpreted as evidence that base models have a larger reasoning boundary. We argue that on tasks with discrete answer spaces, such as math with numeric outputs, Pass@k at large k reflects the increasingly higher chance of success in the limit of the number of trials rather than genuine reasoning, and can therefore be misleading. We propose Cover@tau, which measures the fraction of problems that a model can solve for which at least a tau proportion of completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an explicit reliability threshold: models that rely on random guessing degrade rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based metrics and illustrate how the relative rankings of popular algorithms change compared to Pass@1, offering a different perspective on reasoning boundaries.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.887461",
        "filter_reason": "该论文完全符合我的研究范围，是一篇高质量的前沿论文。我的判断依据如下： 1.  **核心判断 (第一步):** 该论文的本质并非将LLM作为工具应用于特定领域，而是对『如何有效评估LLM的通用推理能力』这一基础性问题进行深入探讨。它没有研究如何解决医疗、化学或法律问题，而是聚焦于推理能力本身的度量边界。论文的核心贡献是提出了一种新的评估指标 `Cover@τ`，旨在更准确、更可靠地衡量LLM的推理边界。这种对基础方法论（评估范式）的改进，直接服务于“提高LLM通用推理能力”这一长远目标，因为它为整个领域提供了一个更精确的“尺子”，用以判断各种改进方法的有效性。 2.  **正面指标 (第二步):** 论文高度相关。摘要中明确包含了核心概念 \"Large Language Models\"，能力方向 \"reasoning\" (并具体到了 \"coding, math or logic\")，以及训练方法 \"Reinforcement Learning\"。它研究的“reasoning boundaries”正是通用推理能力的核心体现。 3.  **排除标准 (第三步):** 论文不涉及任何排除标准。它没有讨论视觉、多模态，没有应用于医疗、化学等特定领域，也不是关于水印或安全等应用层面的可靠性。 4.  **特殊与模糊情况 (第四步):** 论文与“幻觉”问题有间接但深刻的联系。它提出的 `Cover@τ` 指标通过要求一个较高比例的生成结果都正确，有效地区分了模型是“真正会”还是“偶尔蒙对”。这实际上是在衡量模型输出的一致性和可靠性，是减少“随机性错误”（一种广义的幻觉）的关键。它不是社会学讨论，而是提出了一个可量化的技术方案来提升对模型内在推理质量的认知。 **最终决策 (第五步):** 这篇论文的核心价值在于它对现有评估范式的批判性思考和创新性改进。它指出了广泛使用的 `Pass@k` 指标在衡量模型“推理边界”时可能存在的误导性——即将大规模采样带来的成功率误认为模型固有的推理能力。通过提出 `Cover@τ`，论文为社区提供了一个能更好地区分“真正的推理”与“暴力搜索/随机猜测”的工具。这项工作对于任何致力于提升LLM内在推理能力的研究者都至关重要，因为它能帮助我们更清晰地识别出那些真正使模型变得更“聪明”而非更“会猜”的训练方法。因此，它是一篇必须保留的、与我的研究目标高度契合的前沿论文。"
    },
    {
        "index": "#124",
        "title": "Self-Improving LLM Agents at Test-Time",
        "link": "/arxiv/2510.07841",
        "arxiv_id": "2510.07841",
        "authors": "Emre Can Acikgoz, Cheng Qian, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur",
        "summary": "One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.905074",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。我的判断过程如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一种名为“测试时自我提升”的新范式。它不是将LLM应用于某个特定领域，而是致力于改进LLM智能体本身的学习和适应能力。该方法通过“自我意识”、“自我数据增强”和“自我提升”三个步骤，让模型在测试时动态地、高效地弥补自身能力的短板。这直接属于“改进LLM的基础能力、提出新的训练范式”的范畴，其目标是让模型变得更“有效和具有泛化性”，这正是提升通用推理能力的核心。 2.  **正面指标（第二步）：** 论文命中了多个关键的正面指标。 *   **核心概念:** 论文标题和摘要明确聚焦于“LLM Agents”。 *   **能力方向:** 论文旨在提升“agentic LMs”在“agent benchmarks”上的表现，这直接关联到推理、规划和问题解决等通用能力。 *   **训练方法:** 论文的核心是“Self-Improving”和“self-evolution”，这是一种新颖的、非传统的训练/优化方法，与强化学习、进化等方法论在目标上一致。 *   **新兴范式:** 论文的研究对象是“LLM Agents”，并探索其“self-evolution”的潜力，完全符合筛选标准中的新兴范式。 3.  **排除标准（第三步）：** 论文完全没有涉及任何排除标准中的领域。它不涉及多模态、视觉，不针对医疗、化学等特定应用领域，也不讨论水印、安全等应用层面的可靠性问题。 4.  **处理特殊和模糊情况（第四步）：** 论文完美地符合“智能体/工具使用”的保留条件。它提出的是一种**通用的**智能体自我提升框架，旨在构建“更有能力的智能体”并实现“自我进化”，而不是将智能体应用于特定领域。因此，这篇论文应该被保留。 **最终决策（第五步）：** 综合以上分析，这篇论文的本质是提出一种创新的训练/适应范式（测试时自我提升），以增强LLM智能体的通用推理和问题解决能力。它不涉及特定应用领域，完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。因此，最终判断为符合要求。"
    },
    {
        "index": "#136",
        "title": "Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts",
        "link": "/arxiv/2510.07358",
        "arxiv_id": "2510.07358",
        "authors": "Yeskendir Koishekenov, Aldo Lipani, Nicola Cancedda",
        "summary": "Most efforts to improve the reasoning capabilities of large language models (LLMs) involve either scaling the number of parameters and the size of training data, or scaling inference computation by letting models generate complex chains of thought. Motivated by interpretability studies showing that the crucial computation required for reasoning tasks is concentrated in a limited range of layers, we introduce Encode-Think-Decode (ETD), a method that enhances the reasoning capabilities of a base model by training it to iterate over a small subset of reasoning-relevant layers during the mid-training stage. ETD amplifies latent reasoning while preserving the original architecture, parameter count, hyperparameters, and training data composition. When iterating on the selected layers at inference time, ETD models yield substantial gains on 17 reasoning benchmarks, including +28.4% relative accuracy improvement on GSM8K and +36% on MATH with the OLMo-2 1B Base model. We also explore an adaptive depth strategy that adjusts the computation per input token. Our results show that recursive latent reasoning offers a simple and effective path to stronger LLM reasoning.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-08",
        "category": "cs.CL",
        "crawl_time": "2025-10-10T11:00:07.916264",
        "filter_reason": "这篇论文完全符合您的研究目标和筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** *   **论文的本质是什么？** 这篇论文的核心贡献是提出了一种名为“Encode-Think-Decode (ETD)”的新方法。该方法不是将LLM应用于某个特定领域，而是直接作用于LLM本身。它通过在训练过程中让模型学习迭代一个与推理相关的“思维层”，并在推理时进行递归调用，从而**从根本上增强了模型内在的、通用的推理能力**。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”这一核心保留标准。 2.  **第二步：正面指标** *   **核心概念:** 论文明确以“大语言模型”为基础。 *   **能力方向:** 论文的核心主题就是“reasoning”，并特别在数学推理数据集上进行了验证，这与筛选标准中的“reasoning (尤其是 math reasoning...)”高度吻合。 *   **训练方法:** ETD是一种全新的训练范式，它在不改变模型架构和参数的情况下，通过改变训练和推理的计算方式来提升能力，这是一种创新的模型优化方法。 *   **新兴范式:** 论文与“思维链”相关联，但提出了更深入、更内在的“递归潜在思维”，旨在放大模型内部的推理计算，这与探索更强大推理范式的目标一致。 3.  **第三步：排除标准** *   **多模态与视觉:** 论文完全不涉及视觉或多模态内容。 *   **特定应用领域:** 论文的实验是在通用的数学和逻辑推理基准上进行的，没有将其方法局限于任何特定行业或领域。 *   **模型可靠性（应用层面）:** 论文的目标是提升推理准确性，而不是研究水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** *   该论文的情况非常清晰，不涉及智能体/工具在特定领域的应用，也不属于对幻觉或安全性的社会学研究。它提出的ETD方法是一种直接作用于模型核心推理过程的通用技术。 **最终决策**: 这篇论文的标题“Encode, Think, Decode”和摘要都清晰地表明，其研究焦点是**如何通过创新的模型训练和推理机制（递归潜在思维）来提升LLM的通用推理能力**。它提出了一种基础性的方法论，显著提升了模型在数学和逻辑等通用推理任务上的表现。这与您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标完全一致。因此，最终判断为符合要求。"
    },
    {
        "index": "#3",
        "title": "FlowSearch: Advancing deep research with dynamic structured knowledge flow",
        "link": "/arxiv/2510.08521",
        "arxiv_id": "2510.08521",
        "authors": "Yusong Hu, Runmin Ma, Yue Fan, Jinxin Shi, Zongsheng Cao, Yuhao Zhou, Jiakang Yuan, Xiangchao Yan, Wenlong Zhang, Lei Bai, Bo Zhang",
        "summary": "Deep research is an inherently challenging task that demands both breadth and depth of thinking. It involves navigating diverse knowledge spaces and reasoning over complex, multi-step dependencies, which presents substantial challenges for agentic systems. To address this, we propose FlowSearch, a multi-agent framework that actively constructs and evolves a dynamic structured knowledge flow to drive subtask execution and reasoning. FlowSearch is capable of strategically planning and expanding the knowledge flow to enable parallel exploration and hierarchical task decomposition, while also adjusting the knowledge flow in real time based on feedback from intermediate reasoning outcomes and insights. FlowSearch achieves state-of-the-art performance on both general and scientific benchmarks, including GAIA, HLE, GPQA and TRQA, demonstrating its effectiveness in multi-disciplinary research scenarios and its potential to advance scientific discovery. The code is available at https://github.com/Alpha-Innovator/InternAgent.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.096731",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出一个名为 **FlowSearch** 的**多智能体框架**。这个框架的本质并非将LLM应用于某个特定领域，而是致力于解决一个更根本、更通用的认知挑战——“深度研究”。论文明确指出，深度研究需要“对复杂的、多步骤的依赖关系进行推理”，这正是通用推理能力的核心体现。FlowSearch通过构建和演化动态知识流来驱动子任务执行和推理，其目标是增强LLM系统在处理复杂、多步问题时的规划、分解和推理能力。因此，这篇论文的本质是**改进LLM的基础通用推理与规划能力**，符合保留标准。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文高度契合多个正面指标： *   **能力方向**: 摘要中明确提到了 **reasoning**（“reasoning over complex, multi-step dependencies”）、**planning**（“strategically planning”）和 **problem-solving**（整个框架的目标）。 *   **新兴范式**: 论文的核心是一个 **multi-agent framework**（多智能体框架），并且其目标是 **deep research**（深度研究），这正是当前LLM前沿研究的热点范式。 *   **核心概念**: 虽然摘要未直接重复“Large language models”，但“agentic systems”和上下文强烈暗示其底层驱动力是LLM，且提供的GitHub链接（InternAgent）进一步证实了这一点。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文成功避开了所有主要的排除领域： *   **多模态与视觉**: 未涉及。 *   **特定应用领域**: 论文虽然在“科学基准”上进行了测试，但其方法本身是**领域无关的**。它解决的是“如何进行深度研究”这一通用方法论问题，而非“如何解决化学问题”。论文强调其在“多学科研究场景”中的有效性，这恰恰证明了其通用性，而非特定性。 *   **模型可靠性（应用层面）**: 未涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 这篇论文是“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的典型范例。FlowSearch框架本身是通用的，旨在提升深度研究这一通用任务的能力，而不是为某个特定领域（如化学、法律）定制的智能体。因此，根据筛选标准，应该保留。 5.  **第五步：最终决策** 综合以上分析，论文《FlowSearch》的核心贡献是一个旨在提升LLM在复杂、多步任务上通用推理与规划能力的多智能体框架。它直接回应了“提高大语言模型通用推理能力”这一核心目标，并且其方法是通用方法论，而非特定领域应用。因此，这篇论文与你的研究课题高度相关，应当被筛选出来。"
    },
    {
        "index": "#8",
        "title": "QAgent: A modular Search Agent with Interactive Query Understanding",
        "link": "/arxiv/2510.08383",
        "arxiv_id": "2510.08383",
        "authors": "Yi Jiang, Lei Shen, Lujie Niu, Sendong Zhao, Wenbo Su, Bo Zheng",
        "summary": "Large language models (LLMs) excel at natural language tasks but are limited by their static parametric knowledge, especially in knowledge-intensive task. Retrieval-augmented generation (RAG) mitigates this by integrating external information. However, (1) traditional RAG struggles with complex query understanding, and (2) even search agents trained with reinforcement learning (RL), despite their promise, still face generalization and deployment challenges. To address these limitations, we propose QAgent, a unified agentic RAG framework that employs a search agent for adaptive retrieval. This agent optimizes its understanding of the query through interactive reasoning and retrieval. To facilitate real-world application, we focus on modular search agent for query understanding that are plug-and-play in complex systems. Secifically, the agent follows a multi-step decision process trained with RL to maximize retrieval quality and support accurate downstream answers. We further analyze the strengths and weaknesses of end-to-end RL and propose a strategy that focuses on effective retrieval, thereby enhancing generalization in LLM applications. Experiments show QAgent excels at QA and serves as a plug-and-play module for real-world deployment.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.099267",
        "filter_reason": "这篇论文完全符合筛选要求，其核心贡献直接指向提升大语言模型的通用推理能力。 判断过程如下： **第一步：核心判断** 这篇论文的本质是提出一种新的方法论（QAgent框架）来改进检索增强生成（RAG）范式中的一个核心瓶颈——复杂查询的理解。它不是将LLM应用于某个特定垂直领域，而是致力于提升LLM在处理知识密集型任务时的通用能力。论文的核心贡献在于，通过一个基于强化学习（RL）训练的智能体，让模型能够进行“交互式推理”和“多步决策”来深化对查询的理解，这直接对应了筛选标准中“增强其逻辑、多步推理等通用能力”和“提出新的训练范式”的要求。因此，应予以保留。 **第二步：正面指标** 论文摘要中包含了大量符合研究范围的正面指标： - **核心概念**: 明确提及 \"Large language models (LLMs)\"。 - **能力方向**: 核心是解决 \"knowledge-intensive task\"，通过 \"interactive reasoning\" 和 \"multi-step decision process\" 来提升 \"query understanding\"，这本质上是一种复杂的推理和问题解决能力。 - **训练方法**: 明确提出使用 \"reinforcement learning (RL)\" 来训练智能体，并讨论了RL策略对泛化性的影响。 - **新兴范式**: 论文本身就是一个关于 \"llm-based agents\" 的研究，提出了一个 \"agentic RAG framework\"，并且涉及 \"tool use\"（检索作为一种工具）。 **第三步：排除标准** 论文内容完全不涉及任何排除标准： - **多模态与视觉**: 未提及。 - **特定应用领域**: 研究的是通用的知识密集型问答（QA）任务，而非医疗、化学等特定领域。 - **模型可靠性（应用层面）**: 未提及水印、安全等内容。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: QAgent是一个典型的“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的案例。它被设计成一个“模块化”、“即插即用”的组件，用于增强LLM在“LLM应用”中的泛化能力，而不是针对某个特定领域。这完全符合保留标准。 **第五步：最终决策** 综合以上分析，这篇论文的核心是利用强化学习训练一个智能体，通过多步交互式推理来优化LLM的查询理解过程，从而提升其在知识密集型任务上的通用推理和问题解决能力。这是一个方法论层面的创新，直接作用于LLM本身的能力增强，而非特定领域的应用。因此，这篇论文高度契合“大语言模型通用推理能力”这一研究课题。"
    },
    {
        "index": "#13",
        "title": "Co-TAP: Three-Layer Agent Interaction Protocol Technical Report",
        "link": "/arxiv/2510.08263",
        "arxiv_id": "2510.08263",
        "authors": "Shunyu An, Miao Wang, Yongchao Li, Dong Wan, Lina Wang, Ling Qin, Liqin Gao, Congyao Fan, Zhiyong Mao, Jiange Pu, Wenji Xia, Dong Zhao, Rui Hu, Ji Lu, Guiyue Zhou, Baoyu Tang, Yanqin Gao, Yongsheng Du, Daigang Xu, Lingjun Huang, Baoli Wang, Xiwen Zhang, Luyao Wang, Shilong Liu",
        "summary": "This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer agent interaction protocol designed to address the challenges faced by multi-agent systems across the three core dimensions of Interoperability, Interaction and Collaboration, and Knowledge Sharing. We have designed and proposed a layered solution composed of three core protocols: the Human-Agent Interaction Protocol (HAI), the Unified Agent Protocol (UAP), and the Memory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction layer, standardizing the flow of information between users, interfaces, and agents by defining a standardized, event-driven communication paradigm. This ensures the real-time performance, reliability, and synergy of interactions. As the core of the infrastructure layer, UAP is designed to break down communication barriers among heterogeneous agents through unified service discovery and protocol conversion mechanisms, thereby enabling seamless interconnection and interoperability of the underlying network. MEK, in turn, operates at the cognitive layer. By establishing a standardized ''Memory (M) - Extraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the ability to learn from individual experiences and form shareable knowledge, thereby laying the foundation for the realization of true collective intelligence. We believe this protocol framework will provide a solid engineering foundation and theoretical guidance for building the next generation of efficient, scalable, and intelligent multi-agent applications.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.107176",
        "filter_reason": "这篇论文符合筛选要求，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出一个名为Co-TAP的三层智能体交互协议。这个协议并非将LLM作为工具应用于特定领域，而是致力于解决多智能体系统在**互操作性、交互协作和知识共享**方面的根本性挑战。特别是其第三层协议MEK（Memory-Extraction-Knowledge），直接作用于**认知层**，旨在让智能体能够从经验中学习、形成可共享的知识，最终实现**集体智能**。这本质上是在构建一个能够提升智能体（通常由LLM驱动）群体通用问题解决和学习能力的框架，完全符合“改进LLM基础能力”和“增强其通用能力”的核心目标。它提出的不是具体应用，而是一种增强智能体通用能力的方法论。 2.  **第二步：正面指标** 论文高度符合多个正面指标： *   **核心概念**: 论文主题是“Agent Interaction Protocol”，与“llm-based agents”和“multi-agent systems”紧密相关。 *   **能力方向**: 论文明确提到了“Knowledge Sharing”（知识共享）、“collective intelligence”（集体智能）和“cognitive layer”（认知层），这些都是提升通用推理和问题解决能力的关键组成部分。 *   **新兴范式**: 论文的核心就是关于“multi-agent systems”的协作框架。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准中的领域。它没有讨论多模态、特定应用（如医疗、化学），也未涉及模型部署、水印或安全等问题。其框架被描述为通用的，旨在构建“下一代高效、可扩展且智能的多智能体应用”，而非针对某一垂直领域。 4.  **第四步：处理特殊和模糊情况** 本论文是“智能体/工具使用”特殊情况的典型范例。它提出的Co-TAP协议是一个**通用的智能体协作框架**，其最终目的是通过标准化交互和知识共享来增强智能体的集体智能，这直接提升了LLM作为智能体核心时的通用问题解决能力。因此，根据筛选标准，应该保留。 **最终决策**: 综合以上分析，该论文提出的是一个旨在增强多智能体系统（通常由LLM驱动）通用认知与协作能力的基础框架。它通过标准化的协议来促进知识共享和集体智能的形成，这直接关系到提升LLM的通用推理和问题解决能力。因此，这篇论文完全符合你的研究范围。"
    },
    {
        "index": "#11",
        "title": "First Try Matters: Revisiting the Role of Reflection in Reasoning Models",
        "link": "/arxiv/2510.08308",
        "arxiv_id": "2510.08308",
        "authors": "Liwei Kang, Yue Deng, Yao Xiao, Zhanfeng Mo, Wee Sun Lee, Lidong Bing",
        "summary": "Large language models have recently demonstrated significant gains in reasoning ability, often attributed to their capacity to generate longer chains of thought and engage in reflective reasoning. However, the contribution of reflections to performance improvement remains unclear. In this paper, we systematically analyze the rollouts of eight reasoning models on five mathematical datasets. We focus on reflective behaviours where the model has already produced an answer but continues reflecting before finalizing its output. Our analysis reveals that reflections are predominantly confirmatory and rarely alter the model's initial answer, a pattern consistent across models and datasets. To understand the role of reflections in training, we construct supervised fine-tuning (SFT) datasets with varying amounts of reflection steps. We observe that training models on rollouts with more reflection steps primarily enhances first-answer correctness rather than the ability to correct initially wrong answers through reflections. This motivates us to propose a question-aware early-stopping method that enhances inference-time token efficiency by stopping the reasoning process once a few plausible candidate answers are generated, thereby reducing unnecessary reflection steps. Motivated by this, we further propose to dynamically truncate the reflections after a candidate answer has appeared during generation, which reduces reasoning tokens by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.105981",
        "filter_reason": "这篇论文完全符合你的研究范围，核心原因在于它并非将LLM作为应用工具，而是深入探究并试图改进LLM的内在推理机制。以下是根据筛选标准的详细分析： **第一步：核心判断** 这篇论文的本质是研究LLM的『反思』这一推理行为。它首先通过系统性分析，揭示了现有推理模型中“反思”的真实作用（主要是确认而非纠错），然后基于这一发现，提出了一种新的方法（感知问题的早停/动态截断）来优化推理过程。这直接触及并改进了LLM的『通用推理能力』（特别是数学推理）的效率和机制，属于对模型基础能力的增强，而非特定领域的应用。因此，应该保留。 **第二步：正面指标** 论文摘要中包含了大量与你研究目标高度相关的正面指标： - **核心概念**: \"Large language models\" (明确指出研究对象) - **能力方向**: \"reasoning\", \"reasoning models\", \"mathematical datasets\", \"math reasoning\" (全文围绕推理能力展开) - **训练方法**: \"supervised fine-tuning (SFT)\" (论文通过构建SFT数据集来探究训练与推理能力的关系) - **新兴范式**: \"chains of thought\", \"reflective reasoning\" (论文聚焦于这些推理范式的具体机制) **第三步：排除标准** 该论文完全避开了所有排除标准： - 它不涉及任何**多模态与视觉**内容。 - 它使用的数学数据集是作为评估通用推理能力的基准，而非**特定应用领域**（如医疗、化学）的研究。 - 它关注的是推理过程的内在效率和正确性，而不是**模型可靠性**的应用层问题，如水印或安全。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 虽然论文没有直接讨论智能体，但它所研究的“反思”和“推理过程”是构建高效智能体的核心基础。论文提出的方法（动态截断）可以被看作是一种通用的、提升推理效率的元技术，可以被广泛应用于各种需要多步推理的智能体框架中，而非局限于某个领域。 - **幻觉/可解释性/安全**: 论文通过对“反思”行为的分析，加深了对模型推理过程的理解，这本身就增强了模型的可解释性。同时，它提出的优化方法减少了不必要的思考步骤，使推理过程更聚焦，间接提升了推理输出的质量和可靠性。这属于提升模型内在推理质量的范畴，应当保留。 **最终决策** 综合以上分析，这篇论文的核心贡献是：1）通过实证分析，澄清了“反思”在LLM推理中的真实作用；2）基于此发现，提出了一种通用且有效的方法来提升LLM推理过程的token效率。这两点都紧密围绕着“提升大语言模型本身通用推理能力”这一核心目标。因此，这篇论文是高度相关且应该被保留的前沿研究。"
    },
    {
        "index": "#7",
        "title": "Revisiting Hallucination Detection with Effective Rank-based Uncertainty",
        "link": "/arxiv/2510.08389",
        "arxiv_id": "2510.08389",
        "authors": "Rui Wang, Zeming Wei, Guanzhang Yue, Meng Sun",
        "summary": "Detecting hallucinations in large language models (LLMs) remains a fundamental challenge for their trustworthy deployment. Going beyond basic uncertainty-driven hallucination detection frameworks, we propose a simple yet powerful method that quantifies uncertainty by measuring the effective rank of hidden states derived from multiple model outputs and different layers. Grounded in the spectral analysis of representations, our approach provides interpretable insights into the model's internal reasoning process through semantic variations, while requiring no extra knowledge or additional modules, thus offering a combination of theoretical elegance and practical efficiency. Meanwhile, we theoretically demonstrate the necessity of quantifying uncertainty both internally (representations of a single response) and externally (different responses), providing a justification for using representations among different layers and responses from LLMs to detect hallucinations. Extensive experiments demonstrate that our method effectively detects hallucinations and generalizes robustly across various scenarios, contributing to a new paradigm of hallucination detection for LLM truthfulness.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.098791",
        "filter_reason": "这篇论文符合筛选标准，应当保留。判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出一种新的方法论来检测和解释大语言模型（LLM）的幻觉现象。虽然它没有直接通过训练来“提高”LLM的推理能力，但它触及了通用推理能力的一个核心且根本性的问题——**推理的可靠性**。该论文并非将LLM应用于特定领域，而是深入分析模型内部的表示（隐藏状态），旨在理解模型“为什么”会产生不正确的推理（即幻觉）。这种对模型内在机制的诊断和洞察，是提升模型基础能力的关键前置步骤，因此其本质符合“改进LLM的基础能力”这一范畴。 **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文明确以 \"large language models (LLMs)\" 为研究对象。 - **能力方向**: 摘要中提到 \"model's internal reasoning process\"，直接与“推理”相关。其研究的“幻觉”问题正是逻辑、事实等推理失败的具体表现。 - **训练方法**: 虽然不涉及训练，但其方法可以被视为一种评估或诊断工具，为未来的训练范式（如根据检测到的不确定性进行RL优化）提供依据。 - 该论文命中了关键的正面指标，尤其是与“推理”直接相关。 **第三步：排除标准——论文是否主要聚焦于以下领域？** - 论文不涉及多模态、特定应用领域。 - 它触及了“模型可靠性”，但并未被排除。原因见下一步分析。 **第四步：处理特殊和模糊情况——幻觉/可解释性/安全** 这是判断的关键。根据筛选标准：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” 这篇论文完美符合该条件： 1.  **提出新方法**: 它提出了基于隐藏状态“有效秩”的新方法来量化不确定性并检测幻觉。 2.  **增强模型内在的可解释性**: 论文核心贡献之一是提供了“interpretable insights into the model's internal reasoning process”。它不是简单地给输出打分，而是试图通过分析模型内部的语义变化来解释产生幻觉的原因，这正是内在可解释性的体现。 3.  **提升通用推理质量**: 通过提供一个诊断推理失败（幻觉）的强大工具，该方法为研究者指明了模型在何处、为何推理不可靠。这份洞察是未来改进模型架构、训练数据和优化算法，从而从根本上提升其通用推理质量的宝贵基础。 **第五步：最终决策** 综合以上分析，这篇论文的贡献不是应用LLM，而是开发了一种深入理解LLM内在推理过程的诊断工具。它通过对幻觉这一核心挑战的深入分析，为提升LLM的通用推理能力和可靠性提供了理论依据和实践方法。因此，这篇论文与“致力于提高大语言模型本身的『通用推理能力』”的核心目标高度一致，应当被保留。"
    },
    {
        "index": "#15",
        "title": "Selection, Reflection and Self-Refinement: Revisit Reasoning Tasks via a Causal Lens",
        "link": "/arxiv/2510.08222",
        "arxiv_id": "2510.08222",
        "authors": "Yunlong Deng, Boyang Sun, Yan Li, Lingjing Kong, Zeyu Tang, Kun Zhang, Guangyi Chen",
        "summary": "Due to their inherent complexity, reasoning tasks have long been regarded as rigorous benchmarks for assessing the capabilities of machine learning models, especially large language models (LLMs). Although humans can solve these tasks with ease, existing models, even after extensive pre-training and post-training at scale, still fail to perform reasoning reliably. In this paper, we revisit reasoning tasks from a causal perspective, seeking to understand their behavior in latent space and to offer insights for addressing their challenges. Specifically, we cast reasoning tasks as a selection mechanism, in which high-level logical concepts function as selection operators on the given observations, such as, identifying the correct answer in a math problem or filling the appropriate entry in Sudoku. We emphasize two key properties of this formulation that shed light on the difficulty of reasoning tasks. First, the latent space exceeds the observation space in complexity, even when the correct answer is fully determined by the observed input. Second, the latent variables, corresponding to logical thought, are densely structured and exhibit strong dependencies. Building on this formulation, we introduce a framework, called SR$^2$, that incorporates the estimated latent variables as feedback into the selection mechanism, thereby facilitating the learning of dense dependencies among latent representations. The framework consists of three key modules: reflective representation learning, dependency self-refinement, and periodic intermediate alignment. Experimentally, we show that our approach yields significant gains in reasoning accuracy, for example, attaining over 10$\\%$ improvement in performance with 8$\\times$ fewer parameters on the Sudoku and Maze tasks over the recent advances.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.108171",
        "filter_reason": "这篇论文完全符合您的研究范围，核心原因在于它致力于从理论和实践两个层面提升大语言模型（LLM）的**通用推理能力**，而非将其作为工具应用于特定领域。 以下是根据您提供的筛选标准进行的详细判断： 1.  **第一步：核心判断——这篇论文的本质是什么？** - **结论：保留。** - **依据：** 论文的本质是提出一种新的方法论来增强LLM的推理能力。它没有将LLM应用于生物、医疗等特定领域，而是直面LLM在推理任务上的核心挑战。论文的核心贡献是： - **理论层面：** 从因果视角重新审视推理任务，提出“选择机制”这一新范式来解释推理的内在困难（如潜在空间的复杂性、逻辑变量的强依赖关系）。这属于对LLM基础能力的深刻洞察。 - **方法层面：** 基于该理论，提出了一个名为SR²的框架，包含“反思性表示学习”、“依赖关系自我优化”等模块，这是一种全新的、旨在提升模型内在逻辑推理能力的训练/优化范式。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。 2.  **第二步：正面指标——论文是否包含以下主题？** - **结论：高度相关。** - **依据：** 论文命中了多个核心正面指标： - **核心概念:** 明确以“Large language models (LLMs)”为研究对象。 - **能力方向:** 论文的核心就是“reasoning”，并具体探讨了“math reasoning”（数学问题）和“logical reasoning”（数独、迷宫任务中的逻辑）。 - **训练方法:** 提出的SR²框架，特别是“自我优化”和“反思性学习”模块，属于一种新颖的训练方法论，旨在提升模型性能。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** - **结论：不涉及。** - **依据：** 论文完全避开了所有排除标准。 - 它不涉及多模态、视觉等。 - 它的应用场景是数独和迷宫，这些是用于衡量通用逻辑和规划能力的标准基准，而非医疗、化学等特定领域。 - 它不关注水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** - **结论：不适用，但符合保留精神。** - **依据：** 论文虽然未直接涉及“智能体/工具使用”，但其通过“因果透镜”分析模型内在推理机制，并提出“反思”、“自我优化”等框架来改进它，这与“提升模型内在可靠性和推理质量”的保留原则精神一致。它不是在讨论现象，而是在提出解决方案。 5.  **第五步：最终决策** - **综合以上分析，这篇论文是一篇典型的、致力于提升LLM通用推理能力的前沿研究。** 它从理论创新出发，构建了新的方法论框架，并在通用的推理基准任务上验证了其有效性。其目标与您的“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标完全契合。因此，应予以保留。"
    },
    {
        "index": "#28",
        "title": "Language Models Do Not Embed Numbers Continuously",
        "link": "/arxiv/2510.08009",
        "arxiv_id": "2510.08009",
        "authors": "Alex O. Davies, Roussel Nzoyem, Nirav Ajmeri, Telmo M. Silva Filho",
        "summary": "Recent research has extensively studied how large language models manipulate integers in specific arithmetic tasks, and on a more fundamental level, how they represent numeric values. These previous works have found that language model embeddings can be used to reconstruct the original values, however, they do not evaluate whether language models actually model continuous values as continuous. Using expected properties of the embedding space, including linear reconstruction and principal component analysis, we show that language models not only represent numeric spaces as non-continuous but also introduce significant noise. Using models from three major providers (OpenAI, Google Gemini and Voyage AI), we show that while reconstruction is possible with high fidelity ($R^2 \\geq 0.95$), principal components only explain a minor share of variation within the embedding space. This indicates that many components within the embedding space are orthogonal to the simple numeric input space. Further, both linear reconstruction and explained variance suffer with increasing decimal precision, despite the ordinal nature of the input space being fundamentally unchanged. The findings of this work therefore have implications for the many areas where embedding models are used, in-particular where high numerical precision, large magnitudes or mixed-sign values are common.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.119543",
        "filter_reason": "这篇论文符合研究筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献并非提出一个新模型或新训练方法来直接“提高”LLM的推理能力，而是深入**分析和诊断**了LLM在数学推理这一核心能力上的一个基础性缺陷：数字嵌入的非连续性。这完全符合“改进LLM的基础能力”这一核心目标的范畴。要提升一项能力，首先必须深刻理解其内在机制和局限性。这篇论文精准地指出了LLM在处理精确数值时的根本瓶颈，为后续如何针对性地改进其数学推理能力（例如，通过设计新的嵌入方法或训练目标）提供了关键的、基础性的洞见。因此，它不是将LLM作为工具应用，而是对LLM本身进行深入的基础研究。 2.  **第二步 & 第三步：正面与排除指标** - **正面指标**: 论文明确聚焦于`Large language models`和`math reasoning`能力。它虽然不包含`reinforcement learning`或`agents`等关键词，但其对数学推理底层机制的剖析，正是提升该能力的前沿研究方向。 - **排除标准**: 论文完全不涉及多模态、特定应用领域（如医疗、化学）或应用层面的模型可靠性（如水印、安全）。它是一个纯粹的、针对LLM内部表征的基础研究。 3.  **第四步：处理特殊和模糊情况** 这篇论文可以被看作是一种深入到模型内部的“可解释性”研究。它揭示了模型在数值任务上表现不佳的潜在原因（embedding space的噪声和非连续性），这种解释性研究的直接目的就是为了启发如何提升模型内在的“推理质量”。它不是对现象的泛泛而谈，而是通过严谨的实验设计（线性重建、PCA分析）来验证其假设，为未来的改进工作指明了方向。 **核心依据总结**: 该论文的核心价值在于其对LLM“通用推理能力”（特别是数学推理）的**基础性诊断**。它没有停留在“LLM数学能力不行”的表面现象，而是深入揭示了“为什么不行”的内在机制。这种旨在揭示根本瓶颈的研究，是驱动领域前进、并为后续“提升”方法铺平道路的关键一环。因此，它完全符合“致力于提高大语言模型本身的通用推理能力”这一核心研究目标。"
    },
    {
        "index": "#20",
        "title": "Prepared mind, fast response: A temporal decoupling framework for adaptive knowledge orchestration in open-domain dialogue",
        "link": "/arxiv/2510.08175",
        "arxiv_id": "2510.08175",
        "authors": "Jinling Gan, Churong Liang, Runnan Li",
        "summary": "The latency-quality tradeoff is a fundamental constraint in open-domain dialogue AI systems, since comprehensive knowledge access necessitates prohibitive response delays. Contemporary approaches offer two inadequate solutions: lightweight instruct models achieve sub-second latency but lack reasoning depth, while tool-augmented ReAct agents enhance factuality through external knowledge at the cost of synchronous execution that blocks interaction during re- trieval processes. PMFR is thus proposed, with a tempo- ral decoupling framework that fundamentally resolves the contradiction through asynchronous knowledge orchestra- tion. PMFR employs three coordinated components: (1) a Knowledge Adequacy Evaluator for real-time sufficiency assessment, (2) a Lightweight Response Generator for imme- diate user interaction, and (3) an Asynchronous Knowledge Refinement Agent for background knowledge enhancement. This architecture maintains continuous conversational flow while progressively enriching knowledge coverage through intelligent triggering mechanisms. Evaluation results on Top- iOCQA demonstrate PMFR outperforms brute-force scaling: PMFR achieves 95.3% latency reduction (23.38s -> 1.09s) while preserving response quality comparable to heavyweight synchronous baselines (GEval-C: 0.613 vs. 0.620).",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.115683",
        "filter_reason": "这篇论文符合研究范围，应当被保留。我的判断依据如下： 1.  **核心判断 (第一步)**: 这篇论文的本质并非将LLM应用于某个特定垂直领域，而是提出了一种新颖的**系统框架 (PMFR)**来解决LLM在实际应用中的一个根本瓶颈：延迟与推理深度的矛盾。论文的核心贡献是一种“时域解耦框架”和“异步知识编排”的方法论，这属于“增强其逻辑、…、多步推理等通用能力”的范畴，特别是通过一种新的智能体和工具使用范式来实现。它不属于特定领域应用，也不关注基础设施或硬件。 2.  **正面指标 (第二步)**: 论文显著地包含了多个正面指标。 *   **能力方向**: 论文明确指出其目标是“推理深度”，并与“重型同步基线”在质量上做对比，这直接关联到`reasoning`能力。 *   **新兴范式**: 论文的核心是提出一个“异步知识精炼智能体”，这完全符合`llm-based agents`的定义。同时，它通过异步调用外部知识来精炼回答，这与`tool use`密切相关。 3.  **特殊和模糊情况处理 (第四步)**: 这篇论文是“智能体/工具使用”特殊情况的典型案例。 *   论文提出的是一种**通用的智能体协作/工具使用方法**（异步知识编排），旨在解决开放域对话这一**通用场景**下的通用问题（如何在保证推理质量的同时实现快速响应）。它的方法具有普适性，不局限于“化学”、“医疗”等特定领域。 *   其本质是通过优化LLM使用外部知识（工具）的**策略和流程**来增强其整体的**问题解决和推理能力**。这正是一种提升LLM通用推理能力的前沿探索。 **综合结论**: 尽管这篇论文的重点不是改变模型内部的结构或训练数据，但它提出了一种创新的框架（PMFR），使得LLM能够更高效地组织和执行其推理过程。通过异步知识精炼智能体，它在不牺牲对话流畅性的前提下，实现了与重度依赖外部工具和深度推理模型相媲美的质量。这是一种系统层面的方法论创新，直接服务于“提升大语言模型通用推理能力”这一核心目标，因为它解决了深度推理在实时交互场景下的部署难题。因此，该论文高度相关，应予以保留。"
    },
    {
        "index": "#27",
        "title": "PEAR: Phase Entropy Aware Reward for Efficient Reasoning",
        "link": "/arxiv/2510.08026",
        "arxiv_id": "2510.08026",
        "authors": "Chen Huang, Wei Lu, Wenxuan Zhang",
        "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on complex reasoning tasks by generating detailed chain-of-thought (CoT) explanations. However, these responses are often excessively long, containing redundant reasoning steps that inflate inference cost and reduce usability. Controlling the length of generated reasoning without sacrificing accuracy remains an open challenge. Through a systematic empirical analysis, we reveal a consistent positive correlation between model entropy and response length at different reasoning stages across diverse LRMs: the thinking phase exhibits higher entropy, reflecting exploratory behavior of longer responses, while the final answer phase shows lower entropy, indicating a more deterministic solution.This observation suggests that entropy at different reasoning stages can serve as a control knob for balancing conciseness and performance. Based on this insight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism that incorporating phase-dependent entropy into the reward design. Instead of treating all tokens uniformly, PEAR penalize excessive entropy during the thinking phase and allowing moderate exploration at the final answer phase, which encourages models to generate concise reasoning traces that retain sufficient flexibility to solve the task correctly. This enables adaptive control of response length without relying on explicit length targets or rigid truncation rules. Extensive experiments across four benchmarks demonstrate that PEAR consistently reduces response length while sustaining competitive accuracy across model scales. In addition, PEAR demonstrates strong out-of-distribution (OOD) robustness beyond the training distribution. Our code is available at: https://github.com/iNLP-Lab/PEAR.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.119070",
        "filter_reason": "这篇论文完全符合筛选要求，应予以保留。 以下是详细的判断过程： **第一步：核心判断** - 论文的核心是改进大语言模型在推理过程中的一个固有缺陷：生成的思维链过长、冗余，导致推理效率低下。它提出了一种新的奖励机制（PEAR），通过在训练阶段对模型不同推理阶段的熵进行调控，来引导模型生成更简洁且不失准确性的推理路径。 - 这本质上是对LLM基础推理能力的优化和增强，属于提出新训练范式以改进模型通用能力的范畴。它并非将LLM作为工具应用于特定领域，因此符合保留标准。 **第二步：正面指标** - **核心概念**: 论文明确研究“Large Reasoning Models (LRMs)”，这是LLMs在推理任务上的直接体现。 - **能力方向**: 标题和摘要反复强调“Reasoning”，致力于提升“Efficient Reasoning”能力，这与研究目标中的“通用推理能力”高度一致。 - **训练方法**: 论文提出了“Reward”（奖励）机制，并旨在影响模型的生成行为。这直接指向了强化学习（RL）或类似的训练优化范式，是提升模型能力的关键方法之一。 - **新兴范式**: 论文建立在“chain-of-thought (CoT)”这一核心推理范式之上，并对其进行改进，属于对现有范式的深化和优化。 **第三步：排除标准** - **多模态与视觉**: 论文全文未提及任何视觉、多模态相关内容，完全聚焦于文本推理。 - **特定应用领域**: 论文在四个通用推理基准上进行实验，没有绑定任何生物、医疗、金融等特定应用领域。 - **模型可靠性（应用层面）**: 论文关注的是推理的“效率”和“准确性”，这是一个内在能力问题，而非水印、安全等应用层面的可靠性议题。 **第四步：处理特殊和模糊情况** - 论文不涉及智能体、工具使用等特殊情况的模糊地带。其目标非常明确：通过优化训练过程，直接提升模型本身的推理质量。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种名为PEAR的新颖奖励机制，用以解决大语言模型在进行链式推理时效率低下的问题。这直接触及了“提升LLM通用推理能力”的核心目标，它通过方法论创新（新的奖励设计）来增强模型的基础能力（推理）。因此，该论文与研究课题高度相关，是理想的前沿文献，应予以保留。"
    },
    {
        "index": "#42",
        "title": "An approach for systematic decomposition of complex llm tasks",
        "link": "/arxiv/2510.07772",
        "arxiv_id": "2510.07772",
        "authors": "Tianle Zhou, Jiakai Xu, Guanhong Liu, Jiaxiang Liu, Haonan Wang, Eugene Wu",
        "summary": "Large Language Models (LLMs) suffer from reliability issues on complex tasks, as existing decomposition methods are heuristic and rely on agent or manual decomposition. This work introduces a novel, systematic decomposition framework that we call Analysis of CONstraint-Induced Complexity (ACONIC), which models the task as a constraint problem and leveraging formal complexity measures to guide decomposition. On combinatorial (SATBench) and LLM database querying tasks (Spider), we find that by decomposing the tasks following the measure of complexity, agent can perform considerably better (10-40 percentage point).",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.136871",
        "filter_reason": "这篇论文完全符合你的筛选标准，应予以保留。以下是详细的判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为ACONIC的**系统化任务分解框架**。这并非将LLM作为工具应用于某个特定领域，而是直接针对LLM在处理复杂任务时的一个**根本性短板**——无法系统性地拆解问题。通过引入形式化的复杂度度量来指导分解，该论文旨在提升LLM（或基于LLM的智能体）的**内在问题解决和规划能力**。这直接关乎LLM的通用推理能力，属于改进其基础能力的范畴，因此完全符合“保留”标准。 2.  **第二步：正面指标** 论文完美命中了多个关键正面指标： *   **核心概念**: 明确以 \"Large Language Models (LLMs)\" 为研究对象。 *   **能力方向**: 论文的核心是解决 \"complex tasks\"，这直接关联到 \"reasoning\", \"planning\", 和 \"problem-solving\"。其采用的组合问题（SATBench）和数据库查询（Spider）都是衡量逻辑和结构化推理能力的经典基准。 *   **新兴范式**: 摘要中提到 \"agent can perform considerably better\"，表明该分解方法是作为一种增强**LLM-based agent**性能的通用策略提出的，这属于智能体研究范畴。 3.  **第三步：排除标准** 该论文完全避开了所有排除标准： *   它不涉及任何多模态或视觉内容。 *   虽然在数据库查询任务上测试，但其方法（基于约束和复杂度分析的分解）是**通用方法论**，而非局限于数据库或金融等特定领域。Spider是公认的通用推理基准，而非特定应用数据集。 *   它关注的是提升模型解决复杂问题的内在可靠性，而非水印、安全等应用层面的保障措施。 4.  **第四步：处理特殊和模糊情况** 这篇论文是“智能体/工具使用”规则的绝佳范例。它提出的ACONIC框架可以被视为一种**通用的工具或思维策略**，供LLM智能体在面临任何复杂问题时使用，以提升其通用的规划和推理表现。这与“用于化学实验自动化的智能体”等特定领域应用有本质区别。因此，根据规则，这种情况应该**保留**。 **最终决策**: 综合以上分析，这篇论文的本质是提出一种新颖的、系统化的方法论来增强LLM的通用问题分解和规划能力，这是通用推理的核心组成部分。它不涉及特定领域应用或多模态，并且其提出的框架能够作为通用工具提升智能体的表现。因此，这篇论文与你的研究目标“提高大语言模型（LLM）本身的『通用推理能力』”高度相关，必须保留。"
    },
    {
        "index": "#41",
        "title": "GCPO: When Contrast Fails, Go Gold",
        "link": "/arxiv/2510.07790",
        "arxiv_id": "2510.07790",
        "authors": "Hao Wu, Wei Liu",
        "summary": "Reinforcement learning has been widely applied to enhance the reasoning capabilities of large language models. Extending the inference limits of smaller models has become a prominent research focus. However, algorithms such as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the upper bound of a model's rollout responses is entirely determined by the model itself, preventing the acquisition of knowledge from samples that are either all incorrect or all correct. In this paper, we introduce Group Contrastive Policy Optimization (GCPO), a method that incorporates external standard reference answers. When the model cannot solve a problem, the reference answer supplies the correct response, steering the model toward an unequivocally accurate update direction. This approach offers two main advantages: (1) it improves training efficiency by fully utilizing every sample; (2) it enables the model to emulate the problem solving strategy of the reference answer during training, thereby enhancing generalization in reasoning. GCPO achieves outstanding results across multiple benchmark datasets, yielding substantial improvements over the baseline model. Our code is available at: https://github.com/AchoWu/GCPO.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.136370",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。 1.  **核心判断 (第一步)**: 该论文的本质是提出一种新的强化学习训练范式（GCPO），旨在解决现有方法（如GRPO）在提升大语言模型推理能力时的局限性。论文的核心贡献在于，通过引入外部标准参考答案，为模型提供了超越其自身生成能力上限的学习信号，从而更有效地学习问题解决策略，增强其通用推理能力。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **正面指标 (第二步)**: 论文摘要中明确包含了多个关键的正面指标： *   **核心概念**: \"large language models\" *   **能力方向**: \"reasoning capabilities\", \"enhancing generalization in reasoning\", \"problem solving strategy\" *   **训练方法**: \"Reinforcement learning\", \"Group Relative Policy Optimization (GRPO)\", \"Group Contrastive Policy Optimization (GCPO)\" 这些关键词表明论文直接聚焦于使用强化学习方法来提升LLM的推理能力。 3.  **排除标准 (第三步)**: 该论文的研究内容不涉及任何排除标准中提到的领域。它没有讨论多模态、视觉，也没有将LLM应用于医疗、化学、机器人等特定领域，更不关注模型基础设施或水印、安全等应用层面的可靠性。其目标是提升模型的通用能力，而非解决某个垂直领域的问题。 4.  **特殊和模糊情况 (第四步)**: 第四步的特殊情况在此不适用，论文的研究焦点非常清晰，即通过改进训练算法来提升通用推理能力。 **最终决策 (第五步)**: 综合以上分析，GCPO这篇论文致力于从训练算法层面改进LLM的通用推理能力，与研究课题“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，应被判定为符合要求。"
    },
    {
        "index": "#53",
        "title": "AgentAsk: Multi-Agent Systems Need to Ask",
        "link": "/arxiv/2510.07593",
        "arxiv_id": "2510.07593",
        "authors": "Bohan Lin, Kuo Yang, Yingchuan Lai, Yudong Zhang, Chen Zhang, Guibin Zhang, Xinlei Yu, Miao Yu, Xu Wang, Yang Wang",
        "summary": "Multi-agent systems built on large language models (LLMs) promise enhanced problem-solving capabilities through collaborative division of labor. However, they frequently underperform single-agent baselines due to edge-level error cascades: minor inaccuracies at one message handoff propagate across the entire chain. We propose AgentAsk, a lightweight and plug-and-play clarification module that treats every inter-agent message as a potential failure point and inserts minimally necessary questions to arrest error propagation. AgentAsk follows a three-stage pipeline: (i) distilling edge-level judgments from curated failure traces into a compact policy, (ii) supervising the policy to determine when/what/whom/how to ask, and (iii) optimizing online with E-GRPO, a reinforcement learning objective that balances accuracy, latency, and cost. The module is architecture-agnostic and easy to integrate into existing orchestration. Across math, reasoning, and coding benchmarks, AgentAsk consistently improves accuracy and robustness over public multi-agent implementations while keeping overhead minimal, with latency and extra cost all less than 5%, approaching the performance of a strong evaluator. Beyond empirical improvements, we contribute a principled taxonomy of edge-level errors and a practical recipe for link-local intervention, offering a scalable pathway toward more reliable LLM-based multi-agent systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.147476",
        "filter_reason": "这篇论文完全符合您关于“大语言模型通用推理能力”的研究范围。我的判断过程如下： **第一步：核心判断——论文的本质** 这篇论文的核心贡献是提出了一种名为“AgentAsk”的轻量级、即插即用的澄清模块。其本质是解决多智能体系统中的一个根本性问题——“边缘级错误级联”，即一个环节的微小错误会沿着协作链条传播，导致整个系统性能下降。通过在智能体间的信息传递点插入必要的提问和确认，该方法直接提升了多智能体协作解决问题的准确性和鲁棒性。这并非将LLM应用于特定领域，而是致力于改进LLM在多智能体协作这一新兴范式下的基础推理过程和可靠性，属于改进LLM通用问题解决能力的核心范畴。 **第二步：正面指标——论文包含的关键主题** 论文明确命中了多个关键的正面指标： - **核心概念**: 论文研究对象是“Multi-agent systems built on large language models (LLMs)”。 - **能力方向**: 实验验证在“math, reasoning, and coding benchmarks”上进行，直接对应了您关注的“reasoning (尤其是 math reasoning)”和“problem-solving”能力。 - **训练方法**: 论文提出了一个包含“reinforcement learning objective (E-GRPO)”的在线优化流程，这与您关注的“reinforcement learning (RL)”高度相关。 - **新兴范式**: 论文的核心是“LLM-based multi-agent systems”，这正是您关注的新兴范式之一。 **第三步：排除标准——论文未聚焦于排除领域** 论文完全没有触及任何排除标准： - 未涉及多模态、视觉等技术。 - 应用场景为通用的数学、推理和编码，而非医疗、化学、机器人等特定领域。 - 讨论的可靠性是针对推理过程中的错误传播，而非水印、安全等应用层面的议题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的AgentAsk是一个“architecture-agnostic and easy to integrate”的通用模块，旨在提升多智能体系统在通用任务上的性能。这完全符合“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的保留条件。 - **幻觉/可解释性/安全**: 论文解决的“edge-level error cascades”可以看作是导致最终输出错误（包括事实性错误或幻觉）的一种内在机制。论文提出的新方法直接干预并修复了这一推理链路中的缺陷，从而提升了模型输出的“准确性和鲁棒性”，这属于通过改进内在机制来提升通用推理质量和可靠性的研究，因此应该保留。 **第五步：最终决策** 综合以上分析，论文《AgentAsk: Multi-Agent Systems Need to Ask》的核心贡献是提出了一种新颖的方法论，通过在多智能体协作链路中引入智能化的澄清机制，有效遏制了错误传播，从而系统性地提升了LLM在多智能体框架下的通用推理能力（尤其是在数学、逻辑和编码方面）。这项研究直接面向LLM基础能力的增强，完全契合您筛选“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。因此，最终判断为符合要求。"
    },
    {
        "index": "#75",
        "title": "On the optimization dynamics of RLVR: Gradient gap and step size thresholds",
        "link": "/arxiv/2510.08539",
        "arxiv_id": "2510.08539",
        "authors": "Joe Suk, Yaqi Duan",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple binary feedback to post-train large language models, has shown significant empirical success. However, a principled understanding of why it works has been lacking. This paper builds a theoretical foundation for RLVR by analyzing its training process at both the full-response (trajectory) and token levels. Central to our analysis is a quantity called the Gradient Gap, which formalizes the direction of improvement from low-reward to high-reward regions of the response space. We prove that convergence critically depends on aligning the update direction with this Gradient Gap. Moreover, we derive a sharp step-size threshold based on the magnitude of the Gradient Gap: below it, learning converges, whereas above it, performance collapses. Our theory further predicts how the critical step size must scale with response length and the success rate, thereby explaining why practical heuristics such as length normalization improve stability and showing that, with a fixed learning rate, the success rate can stagnate strictly below $100\\%$. We validate these predictions through controlled bandit simulations and LLM experiments, including training Qwen2.5-7B with GRPO.",
        "subjects": "Machine Learning, Artificial Intelligence, Information Theory, Optimization and Control, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.168789",
        "filter_reason": "这篇论文完全符合你的研究目标。我的判断过程如下： 1.  **第一步：核心判断** 这篇论文的本质是**对一种名为RLVR（Reinforcement Learning with Verifiable Rewards）的大语言模型后训练方法进行理论分析**。它的核心贡献不是将LLM应用于某个领域，而是深入探究一种旨在提升模型性能的训练范式（基于强化学习）的内在机制和优化动态。这完全符合筛选标准中“提出新的训练范式”和“改进LLM的基础能力”的要求。 2.  **第二步：正面指标** 论文命中了多个关键的正面指标： *   **核心概念**: 论文明确研究“large language models (LLMs)”。 *   **训练方法**: 论文的核心是关于“Reinforcement Learning (RL)”的，具体是RLVR和GRPO。这属于通过强化学习优化模型能力的范畴。 *   **能力方向**: 虽然摘要没有直接使用\"reasoning\"一词，但“Verifiable Rewards”（可验证奖励）通常应用于有明确正确或错误答案的任务，例如数学推理、逻辑证明、代码生成等。因此，该论文的研究内容与提升模型的**推理和问题解决能力**高度相关。 3.  **第三步：排除标准** 论文完全没有涉及任何排除标准中的领域。它不研究多模态、不针对特定应用领域（如医疗、化学），也不讨论模型部署、水印或安全等应用层面的可靠性。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是**为大语言模型的一种强化学习训练方法（RLVR）提供了坚实的理论基础**。它通过分析“梯度差”和推导“步长阈值”，解释了为什么这种方法能有效提升模型性能，以及如何更稳定地使用它。这种对训练方法本身的深入探究，正是为了从根本上增强LLM的能力，特别是那些可以通过可验证反馈来提升的通用推理和问题解决能力。因此，它与你“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。"
    },
    {
        "index": "#67",
        "title": "Truth-Aware Decoding: A Program-Logic Approach to Factual Language Generation",
        "link": "/arxiv/2510.07331",
        "arxiv_id": "2510.07331",
        "authors": "Faruk Alpay, Hamdi Alakkad",
        "summary": "This paper introduces Truth-Aware Decoding (TAD), a verification-oriented decoding scheme that aligns neural language generation with knowledge bases. Situated in the tradition of probabilistic program semantics for sequence models, TAD augments modern instruction-tuned systems with a lattice of semantic guards that operate at decode time. Our contributions are fourfold: (i) a constraint-based semantics that renders oracle filtering as a program-logic judgment, (ii) a proof that greedy selection enjoys local likelihood dominance under sound and complete guards (Theorem 2.7), (iii) an entropy-style invariant that quantifies factual risk via knowledge-aware safe mass, and (iv) a multi-agent operational calculus with verified Lean artefacts to certify implementation behaviour. Numerical and algorithmic case studies confirm that the resulting guardrails reduce hallucinations without sacrificing throughput, yielding a pragmatic bridge between large-scale empirical models and formal verification.",
        "subjects": "Artificial Intelligence, Logic in Computer Science",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.159376",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为“Truth-Aware Decoding (TAD)”的新型解码方案。我的判断过程如下： 1.  **第一步：核心判断** 论文的本质是**改进LLM的基础能力**。它并非将LLM作为工具应用于某个特定领域（如医疗、金融），而是针对LLM在生成内容时普遍存在的一个核心缺陷——“幻觉”，提出了一种通用的解决方案。TAD通过在解码时引入基于“知识库”的“语义防护”，直接干预和优化模型的生成过程。这可以被看作是一种增强模型内在逻辑一致性和事实准确性的方法论，属于提升LLM通用推理能力（特别是事实推理）的范畴，因此符合保留标准。 2.  **第二步：正面指标** 论文高度契合多个正面指标： *   **核心概念**: 明确针对“neural language generation”和“modern instruction-tuned systems”，即LLM。 *   **能力方向**: 虽然没有直接使用“reasoning”一词，但其核心目标“Factual Language Generation”和“reduce hallucinations”是高质量、可靠推理的基础。一个无法保证事实正确的模型，其逻辑推理能力也无从谈起。因此，该研究直接服务于提升推理质量。 *   **新兴范式**: 论文明确提到了“multi-agent operational calculus”，这表明其方法论与多智能体系统相关。它将验证过程形式化，可以视为一种通用的、可增强LLM能力的框架或工具使用方法。 3.  **第三步：排除标准** 论文不涉及任何排除标准所列的领域。它没有讨论多模态、特定应用领域，也不是关于模型基础设施或部署优化的研究。 4.  **第四步：处理特殊和模糊情况** 论文完美地符合“幻觉/可解释性/安全”的保留情况。它不是泛泛而谈幻觉的社会影响，而是**提出了一种全新的、基于程序逻辑和形式化验证的方法（TAD）来从根本上减少幻觉**。这种方法通过提升模型输出的内在可靠性，直接增强了其通用推理的质量和可信度。使用“程序逻辑”和“Lean构件”进行验证，也极大地增强了模型行为的可解释性，这与研究目标高度一致。 **最终决策**： 综合来看，这篇论文通过提出一种新颖的、形式化的解码时干预方法，致力于解决LLM普遍存在的幻觉问题。这直接提升了模型的事实准确性和内在可靠性，而这两者是通用推理能力的基石。其研究方法是通用的，不局限于任何特定领域，并且与智能体、工具使用等前沿范式相关。因此，该论文完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心研究目标。"
    },
    {
        "index": "#63",
        "title": "ProSEA: Problem Solving via Exploration Agents",
        "link": "/arxiv/2510.07423",
        "arxiv_id": "2510.07423",
        "authors": "William Nguyen, Vinh Luong, Christopher Nguyen",
        "summary": "Large language models (LLMs) have empowered AI agents to tackle increasingly complex tasks. However, most existing agents remain limited to static planning and brittle interactions, falling short of true collaboration or adaptive reasoning. We introduce ProSEA, a modular, general-purpose multi-agent framework designed for iterative problem solving through exploration and plan evolution. ProSEA features a hierarchical architecture in which a Manager Agent orchestrates domain-specialized Expert Agents, decomposes tasks, and adaptively replans based on structured feedback from failed attempts. Unlike prior systems, ProSEA agents report not only success or failure but also detailed reasons for failure and newly discovered constraints, enabling dynamic plan refinement informed by exploratory traces. The framework operates autonomously but supports seamless integration with human collaborators when needed. Experiments on the challenging FinanceBench benchmark demonstrate that ProSEA, even without human feedback, outperforms state-of-the-art baselines and achieves robust performance across reasoning-heavy tasks. These results underscore ProSEA's potential as a foundation for more transparent, adaptive, and human-aligned AI agents.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.157553",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出了一种名为ProSEA的、**模块化的、通用的多智能体框架**。这个框架旨在通过“探索”和“计划演化”来提升LLM的**迭代问题解决**和**自适应推理**能力。它的本质不是将LLM应用于金融领域，而是提出一种**新的方法论/范式**，让LLM驱动的智能体能够进行更复杂的动态规划和推理。这完全符合“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的核心保留标准。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文命中了多个关键的正面指标： *   **核心概念**: 明确以 \"Large language models (LLMs)\" 为基础。 *   **能力方向**: 核心关注 \"problem solving\", \"adaptive reasoning\", \"planning\"。 *   **新兴范式**: 提出了一种新的 \"llm-based agents\" 和 \"multi-agent systems\" 框架。 这些关键词都直接指向了您的核心研究目标——提升LLM的通用推理能力。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文虽然在 \"FinanceBench\" 这个金融领域的基准测试上进行了实验，但这**并不等同于论文的主要焦点是金融领域**。摘要中明确强调ProSEA是一个 \"general-purpose\"（通用目的）框架，选择FinanceBench是因为它包含 \"reasoning-heavy tasks\"（推理密集型任务），足以挑战和证明框架的通用推理能力。因此，这不应被视为“将LLM应用于特定领域”而被排除。 4.  **第四步：处理特殊和模糊情况——智能体/工具使用** 这篇论文是处理“智能体”情况的绝佳范例。它提出的是一种**通用的智能体协作框架**（管理者智能体+专家智能体），其核心机制（如根据失败反馈进行自适应重规划、动态精炼计划）旨在增强LLM的**通用问题解决能力**。这完全符合筛选标准中“应该保留”的情况，而非“用于特定领域的智能体”。 5.  **第五步：最终决策** 综合以上分析，ProSEA论文的核心创新点在于提出了一种能够通过探索、反馈和动态重规划来增强LLM多步推理和规划能力的通用框架。它解决的是LLM在复杂任务中“静态规划和脆弱交互”的根本性推理短板。虽然评估使用了一个特定领域的基准，但其方法论和目标是通用的，旨在提升LLM本身的推理上限。因此，这篇论文高度契合您关于“大语言模型通用推理能力”的研究课题。"
    },
    {
        "index": "#89",
        "title": "Prompts Generalize with Low Data: Non-vacuous Generalization Bounds for Optimizing Prompts with More Informative Priors",
        "link": "/arxiv/2510.08413",
        "arxiv_id": "2510.08413",
        "authors": "David Madras, Joshua Safyan, Qiuyi, Zhang",
        "summary": "Many prompt engineering techniques have been successful in practice, even when optimizing over a large prompt space with with a small amount of task-specific data. Recent work has partially explained this success by showing generalization bounds which apply PAC-Bayes theory to the discrete prompt space, but they are non-vacuous only in data-rich scenarios. We argue that such widespread success can be more fully explained through more carefully considering data- or distribution-dependent perplexity, which acts as an effective prior and steers the optimization towards prompts that are more ``natural'' for the task at hand. We derive novel generalization bounds that are non-vacuous for data-scarce prompt optimization via more useful priors, formally analyzing how perplexity regularization tightens these bounds by limiting exploration. Empirically, we explore both the bounds' effectiveness and the practical benefits of perplexity regularization in improving prompt generalization.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.181125",
        "filter_reason": "这篇论文符合筛选标准，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是关于**提示工程**和**提示优化**的。它研究如何通过理论（PAC-Bayes）和实践（困惑度正则化）的方法，让在少量数据上优化出的提示能够更好地泛化到未见过的数据上。这属于一种**新的方法论研究**，旨在更有效地激发和利用大语言模型已有的能力。虽然它没有直接改变模型权重（如通过强化学习微调），但它直接解决了如何更可靠、更高效地引导LLM进行问题解决的关键问题。提示是引导LLM进行复杂推理（如思维链CoT）的核心手段，因此，优化提示的泛化能力，本质上就是**提升我们运用LLM进行通用推理的能力和效率**。这不属于将LLM作为工具应用于特定领域，也不属于基础设施研究。 2.  **第二步：正面指标** 论文的核心主题是关于“Prompts”，这与应用大语言模型（LLMs）紧密相关。其目标是提升在特定“task”上的表现，而问题解决能力是通用推理的核心组成部分。虽然摘要未直接出现\"reasoning\"一词，但提升提示的泛化能力是确保LLM在各类推理任务上表现稳定的基础。 3.  **第三步：排除标准** 该论文完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）。其研究内容是通用的，适用于任何基于文本的任务，因此完全不符合排除标准。 4.  **第四步：处理特殊和模糊情况** 该论文的研究内容不属于特殊和模糊情况的范畴，它聚焦于一个更基础的层面：提示的泛化性。 5.  **第五步：最终决策** 综合来看，这篇论文的核心贡献在于**从理论和实践上解决了提示优化的泛化问题**。通过提出更有效的先验（基于困惑度），它使得我们能够用更少的数据找到更强大、更通用的提示。这直接提升了我们驾驭LLM进行通用问题解决的能力边界，是LLM方法论研究的重要组成部分。对于“提高LLM通用推理能力”这一目标而言，研究如何更有效地**引导**和**激发**这种能力，与研究如何**内化**这种能力（如通过RLHF）同等重要。因此，这篇论文高度相关，应当保留。"
    },
    {
        "index": "#65",
        "title": "Base Models Know How to Reason, Thinking Models Learn When",
        "link": "/arxiv/2510.07364",
        "arxiv_id": "2510.07364",
        "authors": "Constantin Venhoff, Iván Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda",
        "summary": "Why do thinking language models like DeepSeek R1 outperform their base counterparts? Despite consistent performance gains, it remains unclear to what extent thinking models learn entirely new reasoning capabilities or repurpose pre-existing base model ones. In this work, we propose a hybrid model where we activate reasoning mechanisms in base models at the right time to elicit thinking-model-level reasoning chains, implying that thinking models exploit already existing capabilities. To ground our analysis, we introduce an unsupervised, bottom-up approach for uncovering human-interpretable reasoning behaviors in thinking models. This approach provides an unbiased method to discover reasoning behaviors without imposing manual or LLM-derived assumptions. Across three base and four thinking models, using GSM8K and MATH500, our hybrid model recovers up to 91% of the performance gap to thinking models without any weight updates while steering only 12% of tokens. Concretely, our empirical setup provides a simple, causal way to test the effectiveness of existing reasoning mechanisms in base models by invoking them directly and measuring the resulting task performance. More broadly, these results reframe our understanding of how thinking models are trained: pre-training is when models acquire most of their reasoning mechanisms, and post-training teaches efficient deployment of these mechanisms at the right time, enabling efficient use of their inference-time compute.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-08",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.158471",
        "filter_reason": "这篇论文完全符合我的研究范围，是一篇关于大语言模型通用推理能力的核心研究。我的判断依据如下： **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 这篇论文的核心贡献并非将LLM应用于某个特定领域，而是深入探究了“思考模型”（如DeepSeek R1）相对于其“基础模型”性能提升的根本原因。它提出了一个深刻的洞见：基础模型在预训练阶段已经学会了“如何推理”（know how to reason），而后续的“思考”训练（如RL）主要是让模型学会了“何时进行推理”（learn when to reason）。为了验证这一假设，论文提出了一种创新的“混合模型”方法，通过在正确的时间激活基础模型中已有的推理机制，无需任何权重更新就能达到接近思考模型的性能。这直接触及了LLM推理能力的本质，属于改进LLM基础能力、增强其通用推理能力的核心研究。 **第二步：正面指标——论文高度相关。** 论文内容与多个正面指标高度吻合： - **核心概念**: 明确研究 `Large language models (LLMs)` 的 `base models` 和 `thinking models`。 - **能力方向**: 论文的核心就是 `reasoning`，特别是在 `math reasoning` 基准上进行验证。 - **新兴范式**: 论文直接分析当前最前沿的 `thinking` 范式（类似CoT的延伸），并对其工作机制进行了解构和验证。 **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容完全排除了以下领域： - **多模态与视觉**: 研究仅限于纯文本语言模型。 - **特定应用领域**: 实验在通用的数学推理数据集上进行，不涉及医疗、化学、机器人等任何垂直领域。 - **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况——论文增强了内在的可解释性。** 论文提出了一种“无监督、自下而上的方法来发现人类可解释的推理行为”。这完全符合筛选标准中“如果论文提出一种新方法来增强模型内在的可解释性……从而提升模型的通用可靠性和推理质量，应该保留”的原则。它不是对现象的表面讨论，而是提供了一种深入模型内部、理解其推理过程的新工具。 **第五步：最终决策——这是一篇必须保留的高质量论文。** 综合以上分析，这篇论文不仅符合，而且是“大语言模型通用推理能力”这一研究课题下的典范之作。它没有停留在“如何让模型推理得更好”的表象，而是深入探究了“模型推理能力的来源和触发机制”这一更根本的问题。其核心贡献——区分“推理能力的获取”与“推理能力的部署”——为未来如何更高效地训练和激发LLM的推理潜力提供了全新的理论框架和实证方法。因此，这篇论文对我的研究课题具有极高的参考价值。"
    },
    {
        "index": "#96",
        "title": "Learning What's Missing: Attention Dispersion and EMA Stabilization in Length Generalization",
        "link": "/arxiv/2510.08341",
        "arxiv_id": "2510.08341",
        "authors": "Pál Zsámboki, Benjamin Levi, David Ansel Josef Smith, Mitansh Kagalwala, Arlington Kell, Samuel Liechty, Cong Wang",
        "summary": "We study length generalization in transformers through the set complement task, where a model must predict a uniform distribution over tokens absent from an input sequence -- an ability central to board-game style reasoning. Our main theoretical result establishes two statements. First, we prove tight bounds on embedding and value dimensions for single-layer attention-only transformers. Second, we show that if such a model achieves balanced logit displacement at lengths 1 and 2, then it must generalize to longer sequences, though with reduced precision. A mechanistic reading of the proof explains this limitation: as more tokens are attended to, softmax compresses logit displacements, eroding separation between valid and invalid outputs. Training dynamics also suggest a second obstacle: when many next tokens are possible, updates become noisy. We hypothesize that dropout can counteract the first effect and Exponential Moving Average (EMA) the second. We validate these hypotheses through random hyperparameter search on the set complement task, which confirms both mechanisms. We then test OthelloGPT, a GPT-1 style model trained on random Othello moves, and find that EMA again improves length generalization in this more complex setting.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.189696",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心是研究并提升Transformer模型在“长度泛化”上的能力。长度泛化指的是模型处理比训练时所见序列更长的序列的能力，这是一种非常基础且核心的通用推理能力。论文通过理论分析和实验，揭示了模型在处理长序列时遇到的内在机制问题（如softmax压缩导致的logit位移减小），并提出了一种新的训练方法（使用EMA来稳定训练）来克服这一障碍。这完全符合“改进LLM的基础能力”和“提出新的训练范式”的标准，其目标是增强模型本身的通用推理能力，而非将其应用于特定领域。 **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文研究对象是 \"transformers\"，属于大语言模型的核心架构。 - **能力方向**: 论文研究的 \"length generalization\" 被明确描述为 \"an ability central to board-game style reasoning\"，直接关联到 \"reasoning\" 和 \"problem-solving\"。 - **训练方法**: 论文的核心贡献之一是提出并验证了 \"Exponential Moving Average (EMA)\" 作为一种提升模型泛化能力的训练优化技术。 **第三步：排除标准——论文是否主要聚焦于以下领域？** - **多模态与视觉**: 否。论文完全专注于文本序列。 - **特定应用领域**: 否。论文使用的任务是 \"set complement task\" 和 \"OthelloGPT\"，这些都是用于研究模型基础能力的合成或抽象任务，而非医疗、化学等特定领域。 - **模型可靠性（应用层面）**: 否。论文关注的是模型内在的推理能力极限，而非水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 不适用。 - **幻觉/可解释性/安全**: 不适用。 **第五步：最终决策** 综合以上分析，该论文深入探究了大语言模型（Transformer）在通用推理能力（具体表现为长度泛化）上的一个核心瓶颈，从理论层面解释了其内在机制，并提出了一种有效的训练优化方法（EMA）来提升该能力。其研究目标、方法和贡献都与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#116",
        "title": "Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning",
        "link": "/arxiv/2510.08146",
        "arxiv_id": "2510.08146",
        "authors": "Aman Sharma, Paras Chopra",
        "summary": "We introduce a simple, yet novel entropy-based framework to drive token efficiency in large language models during reasoning tasks. Our approach uses Shannon entropy from token-level logprobs as a confidence signal to enable early stopping, achieving 25-50% computational savings while maintaining task accuracy. Crucially, we demonstrate that entropy-based confidence calibration represents an emergent property of advanced post-training optimization present in modern reasoning models but notably absent in standard instruction-tuned and pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop reasoning varies from model to model but can be calculated easily in one shot using only a few examples from existing reasoning datasets. Our results indicate that advanced reasoning models often know that they've gotten a correct answer early on, and that this emergent confidence awareness can be exploited to save tokens and reduce latency. The framework demonstrates consistent performance across reasoning-optimized model families with 25-50% computational cost reduction while preserving accuracy, revealing that confidence mechanisms represent a distinguishing characteristic of modern post-trained reasoning systems versus their predecessors.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.214237",
        "filter_reason": "该论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出了一种基于序列级熵的框架，用于提升大语言模型在推理任务中的效率。它并非将LLM应用于某个特定领域，而是聚焦于LLM执行推理任务时的**内在过程和机制**。具体来说，它通过分析模型在生成过程中的置信度（以熵为指标）来实现早期停止，这是一种优化模型**推理过程本身**的方法论。这直接属于“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的范畴。因此，根据第一步的核心判断，这篇论文应该**保留**。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文摘要中明确包含了多个关键正面指标： *   **核心概念**: \"large language models\", \"LLMs\"。 *   **能力方向**: \"reasoning tasks\", \"reasoning models\"。 *   论文深入探讨了现代推理模型（如经过高级后训练优化的模型）所涌现出的“置信度感知”能力，这直接关系到对LLM推理能力的理解和优化。 3.  **第三步：排除标准——论文是否聚焦于排除领域？** 该论文完全不涉及任何排除标准中提到的领域。它没有讨论多模态、视觉，也没有针对医疗、化学等特定应用。其研究焦点是通用推理模型，而非模型基础设施或应用层面的安全水印等。 4.  **第四步：处理特殊和模糊情况** 这篇论文可以被视为对“可解释性”和“推理质量”的深入研究。它没有停留在表面讨论，而是提出了一种具体的技术方法（序列级熵）来量化和利用模型的“置信度”。这种方法揭示了模型“知道何时已得出正确答案”的内在机制，这是一种增强模型内在可解释性、从而提升推理质量和效率的新方法。这完全符合“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留”的原则。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是一项方法论研究，旨在**优化和理解LLM的通用推理过程**。它提出了一种新颖的置信度信号（序列级熵），不仅实现了显著的计算效率提升，更揭示了现代推理模型区别于普通模型的一个关键涌现属性。这项工作直接推动了我们对“LLM如何推理”这一核心问题的理解，并提供了增强其推理效率的有效手段。因此，它完全符合您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心研究目标。"
    },
    {
        "index": "#177",
        "title": "MeSH: Memory-as-State-Highways for Recursive Transformers",
        "link": "/arxiv/2510.07739",
        "arxiv_id": "2510.07739",
        "authors": "Chengting Yu, Xiaobo Shu, Yadao Wang, Yizhen Zhang, Haoyi Wu, Jiaang Li, Rujiao Long, Ziheng Chen, Yuchi Xu, Wenbo Su, Bo Zheng",
        "summary": "Recursive transformers reuse parameters and iterate over hidden states multiple times, decoupling compute depth from parameter depth. However, under matched compute, recursive models with fewer parameters often lag behind non-recursive counterparts. By probing hidden states, we trace this performance gap to two primary bottlenecks: undifferentiated computation, where the core is forced to adopt a similar computational pattern at every iteration, and information overload, where long-lived and transient information must coexist in a single hidden state. To address the issues, we introduce a Memory-as-State-Highways (MeSH) scheme, which externalizes state management into an explicit memory buffer and employs lightweight routers to dynamically diversify computation across iterations. Probing visualizations confirm that MeSH successfully resolves the pathologies by inducing functional specialization across iterations. On the Pythia suite (160M-1.4B), MeSH-enhanced recursive transformers consistently improve over recursive baselines and outperforms its larger non-recursive counterpart at the 1.4B scale, improving average downstream accuracy by +1.06% with 33% fewer non-embedding parameters. Our analysis establishes MeSH as a scalable and principled architecture for building stronger recursive models.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-10T11:00:08.249368",
        "filter_reason": "这篇论文符合研究范围，应予以保留。 判断过程如下： 1.  **第一步：核心判断——论文本质是提升模型基础能力。** 论文的核心贡献是提出了一种名为“Memory-as-State-Highways (MeSH)”的全新架构方案，用于改进“递归Transformer”模型。作者指出现有递归模型存在“无差别计算”和“信息过载”两个内在瓶颈，并通过MeSH方案，将状态管理外化，并动态地实现计算多样化。这完全是针对模型内部计算机制和架构的优化，旨在从最基础的层面提升模型处理复杂信息的能力。这完全符合“改进LLM的基础能力”的要求，而非将LLM作为工具应用于特定领域。 2.  **第二步：正面指标——论文主题高度相关。** 论文虽然未直接使用“reasoning”一词，但其解决的问题是实现通用推理能力的关键前置条件。 *   **能力方向 (隐性匹配):** 论文旨在解决递归模型中的“无差别计算”和“信息过载”问题，并通过引入“功能专门化”来改善模型。这意味着模型在不同的计算迭代中可以执行不同类型的计算任务，这正是多步推理的核心机制——将一个复杂问题分解为多个不同性质的子步骤并依次解决。因此，该研究实质上是在增强模型的底层推理基础设施。 *   **核心概念:** 论文的研究对象是Transformer架构的变体（Recursive Transformers），这是当前LLM的主流技术基础。其成果直接应用于提升LLM类型的模型。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文研究的是纯粹的模型架构，不涉及任何多模态、视觉或特定应用领域（如医疗、化学等）。同时，它也不讨论模型部署、硬件加速或应用层面的可靠性问题（如水印、安全）。 4.  **第四步：处理特殊情况——不适用。** 该论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊情况，因此无需进行额外判断。 5.  **第五步：最终决策——保留。** 综合分析，这篇论文的本质是提出一种创新的、可扩展的架构方案，通过优化模型内部的计算流程和信息管理方式，来增强其基础能力。这种对“功能专门化”和“计算多样化”的改进，直接提升了模型执行复杂、多步计算的能力，是通向更强通用推理能力的关键一步。因此，它完全符合“致力于提高大语言模型（LLM）本身的通用推理能力”这一核心目标。"
    },
    {
        "index": "#2",
        "title": "Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization",
        "link": "/arxiv/2510.08554",
        "arxiv_id": "2510.08554",
        "authors": "Kevin Rojas, Jiahe Lin, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Molei Tao, Wei Deng",
        "summary": "Diffusion language models (DLMs) enable parallel, order-agnostic generation with iterative refinement, offering a flexible alternative to autoregressive large language models (LLMs). However, adapting reinforcement learning (RL) fine-tuning to DLMs remains an open challenge because of the intractable likelihood. Pioneering work such as diffu-GRPO estimated token-level likelihoods via one-step unmasking. While computationally efficient, this approach is severely biased. A more principled foundation lies in sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a surrogate. Yet, despite this clean mathematical connection, ELBO-based methods have seen limited adoption due to the prohibitive cost of likelihood evaluation. In this work, we revisit ELBO estimation and disentangle its sources of variance. This decomposition motivates reducing variance through fast, deterministic integral approximations along a few pivotal dimensions. Building on this insight, we introduce \\textbf{Group Diffusion Policy Optimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the variance explosion of ELBO estimators under vanilla double Monte Carlo sampling, yielding a provably lower-variance estimator under tight evaluation budgets. Empirically, GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines, on the majority of math, reasoning, and coding benchmarks.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-09",
        "category": "cs.LG",
        "crawl_time": "2025-10-10T11:00:08.232756",
        "filter_reason": "这篇论文完全符合你的研究范围，是一篇关于提升大语言模型（LLM）基础推理能力的前沿研究。以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断** 论文的核心本质是提出一种新的训练算法（GDPO），用于改进一种新兴的语言模型架构（扩散语言模型 DLMs）。其目标是解决将强化学习（RL）应用于DLMs时的技术难题，从而提升模型性能。这完全属于“改进LLM的基础能力、提出新的训练范式”的范畴。论文并非将LLM作为工具应用于特定领域，而是聚焦于模型本身的能力提升。 2.  **第二步：正面指标** 论文高度匹配多个正面指标： *   **核心概念**: 论文研究对象是“扩散语言模型”，这是大语言模型（LLMs）的一个新兴分支。 *   **能力方向**: 论文标题和摘要都明确指出其目标是“提升推理能力”，并在“数学、推理和编码基准测试”上验证了其效果。这正是你核心目标中关注的“通用推理能力”。 *   **训练方法**: 论文的核心贡献是一种新的“强化学习”算法——“群组扩散策略优化”（GDPO）。RL是提升模型能力的关键训练方法之一。 3.  **第三步：排除标准** 论文没有触及任何主要的排除标准： *   **多模态与视觉**: 论文研究的是“扩散语言模型”，而非视觉或多模态模型。 *   **特定应用领域**: 论文的评估基准是通用的数学、推理和编程任务，而非医疗、化学等特定领域。 *   **模型可靠性**: 论文焦点是提升模型性能，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，其研究焦点非常清晰。 5.  **第五步：最终决策** 综合分析，这篇论文的核心贡献是提出了一种名为GDPO的、专为扩散语言模型设计的新型强化学习训练算法。该算法通过技术创新解决了现有方法的偏差和高方差问题，并最终在数学、逻辑推理和编程等多个通用基准上取得了性能提升。这直接响应了你“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标。它属于方法论层面的基础研究，旨在增强模型内在的、通用的推理能力，因此应被保留。"
    },
    {
        "index": "#32",
        "title": "Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization",
        "link": "/arxiv/2510.08233",
        "arxiv_id": "2510.08233",
        "authors": "Yuchen Zhu, Wei Guo, Jaemoo Choi, Petr Molodyk, Bo Yuan, Molei Tao, Yongxin Chen",
        "summary": "Diffusion large language models (dLLMs) are promising alternatives to autoregressive large language models (AR-LLMs), as they potentially allow higher inference throughput. Reinforcement learning (RL) is a crucial component for dLLMs to achieve comparable performance with AR-LLMs on important tasks, such as reasoning. However, RL algorithms that are well-suited for dLLMs' unique characteristics have yet to be developed. This paper proposes Distribution Matching Policy Optimization (DMPO), a principled and theoretically grounded RL fine-tuning method specifically designed to enhance the reasoning capabilities of dLLMs by matching the dLLM policy distribution to the optimal, reward-tilted one through cross-entropy optimization. We identify a key challenge in the implementation with a small training batch size and propose several effective solutions through a novel weight baseline subtraction technique. DMPO exhibits superior performance on multiple reasoning benchmarks without supervised fine-tuning, with an accuracy improvement of up to $42.9\\%$ over previously SOTA baselines and $55.8\\%$ over the base model, underscoring the effectiveness of the distribution matching framework. Our code is available at https://github.com/yuchen-zhu-zyc/DMPO.",
        "subjects": "Machine Learning",
        "date": "2025-10-09",
        "category": "cs.LG",
        "crawl_time": "2025-10-10T11:00:08.263355",
        "filter_reason": "这篇论文完全符合你的研究范围。以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断** 论文的核心是提出一种名为“分布匹配策略优化”（DMPO）的全新强化学习（RL）微调方法。该方法专门为扩散大语言模型设计，其唯一且明确的目标是“增强推理能力”。这直接对应了你筛选标准中的“提出新的训练范式”和“增强其逻辑、数学、规划、多步推理等通用能力”。论文的本质是改进LLM的基础能力，而非将其应用于特定领域。 2.  **第二步：正面指标** 论文与多个正面指标高度匹配： *   **核心概念**: 论文研究对象是“扩散大语言模型”，属于LLMs范畴。 *   **能力方向**: 论文标题和摘要反复强调“Enhancing Reasoning”，并在“multiple reasoning benchmarks”上验证效果，完全命中“reasoning”这一核心能力方向。 *   **训练方法**: 论文的核心贡献是一种“强化学习”微调方法，这与“reinforcement learning (RLHF, RL)”指标完全吻合。 3.  **第三步：排除标准** 论文的研究内容完全避开了所有排除标准： *   它不涉及任何多模态或视觉内容。 *   它没有将模型应用于医疗、化学、机器人等任何特定领域，其评估基准是通用的推理任务。 *   它不讨论水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等需要特殊判断的模糊情况。其研究焦点非常清晰：通过一种新的RL算法来提升dLLM的推理性能。 **最终决策**: 综合以上分析，该论文并非将LLM作为工具应用于特定领域，而是致力于从算法和训练范式的层面，从根本上提升一种新型LLM（扩散LLM）的通用推理能力。这与你的核心目标——筛选致力于提高LLM本身『通用推理能力』的论文——高度一致。因此，这篇论文应被保留。"
    },
    {
        "index": "#43",
        "title": "Arbitrary Entropy Policy Optimization: Entropy Is Controllable in Reinforcement Finetuning",
        "link": "/arxiv/2510.08141",
        "arxiv_id": "2510.08141",
        "authors": "Chen Wang, Zhaochun Li, Jionghao Bai, Yuzhi Zhang, Shisheng Cui, Zhou Zhao, Yue Wang",
        "summary": "Reinforcement finetuning (RFT) is essential for enhancing the reasoning capabilities of large language models (LLM), yet the widely adopted Group Relative Policy Optimization (GRPO) suffers from entropy collapse, where entropy monotonically decreases, exploration vanishes, and policies converge prematurely. Existing entropy-regularized methods only partially alleviate this issue while introducing bias and instability, leaving entropy control unresolved and the connection between entropy, exploration, and performance unclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which eliminates entropy collapse by replacing entropy bonuses with REINFORCE policy gradient on temperature-adjusted distributions and stabilizing entropy through temperature regulation. AEPO integrates three key designs: policy gradient as regularization, distribution as regularization, and REINFORCE as regularization, enabling precise entropy control without distorting optimization. Experiments demonstrate three major contributions: AEPO (1) stabilizes entropy at arbitrary target levels, effectively removing collapse in GRPO; (2) reveals a non-monotonic relation where performance first improves then declines with increasing entropy, clarifying the link between entropy, exploration, and reasoning; and (3) generalizes beyond entropy, providing a broader RFT paradigm where superior target distributions can serve as REINFORCE regularizers.",
        "subjects": "Machine Learning",
        "date": "2025-10-09",
        "category": "cs.LG",
        "crawl_time": "2025-10-10T11:00:08.273890",
        "filter_reason": "这篇论文完全符合筛选标准，应予以保留。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出一种新的训练范式（Arbitrary Entropy Policy Optimization, AEPO）来解决现有强化微调（RFT）方法中的一个核心问题。论文开篇即明确指出其目标是“enhancing the reasoning capabilities of large language models (LLM)”。它并非将LLM应用于某个特定领域，而是聚焦于改进LLM的训练过程本身，以提升其内在的通用推理能力。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 **第二步：正面指标——论文是否包含相关主题？** 论文包含了多个关键的正面指标： - **核心概念**: 明确以 \"Large language models, LLMs\" 为研究对象。 - **能力方向**: 核心目标是提升 \"reasoning capabilities\"，并探讨了 \"exploration\" 与 \"reasoning\" 之间的关系。 - **训练方法**: 论文的核心是关于 \"Reinforcement finetuning (RFT)\"，并提出了对现有 \"Group Relative Policy Optimization (GRPO)\" 的改进，属于强化学习优化的范畴。 **第三步：排除标准——论文是否聚焦于排除领域？** 论文完全不涉及任何排除标准中的领域： - **多模态与视觉**: 全文未提及视觉、多模态等相关内容。 - **特定应用领域**: 研究是通用的方法论，没有绑定任何特定应用场景（如医疗、化学等）。 - **模型可靠性（应用层面）**: 论文讨论的“stability”是算法层面的优化稳定性，而非应用层面的水印、安全等问题。 **第四步：处理特殊和模糊情况** 本论文不属于特殊和模糊情况的范畴，但其研究精神与“提升模型内在可靠性”的目标一致。通过解决“熵崩溃”这一训练过程中的根本性问题，论文从算法层面提升了模型探索更优解的能力，这直接关系到推理的质量和鲁棒性，是一种更深层次的可靠性提升。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种名为“任意熵策略优化”（AEPO）的新方法，用于解决强化微调过程中的“熵崩溃”问题。通过稳定和控制训练过程中的熵，AEPO能够增强模型的探索能力，防止策略过早收敛，从而直接提升了模型的推理性能。这完全契合“提高LLM本身通用推理能力”这一核心研究目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#92",
        "title": "Expanding the Action Space of LLMs to Reason Beyond Language",
        "link": "/arxiv/2510.07581",
        "arxiv_id": "2510.07581",
        "authors": "Zhongqi Yue, Weishi Wang, Yundaichuan Zhan, Juncheng Li, Daniel Dahlmeier, Fredrik D. Johansson",
        "summary": "Large Language Models (LLMs) are powerful reasoners in natural language, but their actions are typically confined to outputting vocabulary tokens. As a result, interactions with external environments -- such as symbolic operators or simulators -- must be expressed through text in predefined formats, parsed, and routed to external interfaces. This overloads the model's language with both reasoning and control duties, and requires a hand-crafted parser, external to the LLM. To address this, we decouple environment interactions from language by internalizing them in an Expanded Action space (ExpA), beyond the vocabulary. The model starts reasoning in the default language environment, but may trigger routing actions and switch to an external environment at any time. From there, the model can only invoke environment-specific actions, receive feedback from the environment, and potentially route back to language as a result. To promote effective exploration of the expanded action space and new environments, we introduce ExpA Reinforcement Learning (EARL) with counterfactual policy optimization. On tasks requiring multi-turn interactions and contingent planning, EARL outperforms strong baselines with vocabulary-constrained actions. It performs robustly across calculator-based multi-task learning and, in the partially observed sorting problem, achieves perfect Sort-4 accuracy while self-discovering an efficient algorithm competitive with classical designs.",
        "subjects": "Machine Learning",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-10T11:00:08.304670",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献与你的筛选标准高度契合。以下是我的详细判断过程： 1.  **第一步：核心判断——论文的本质是提升LLM的通用推理能力。** 论文的核心贡献是提出了一种名为“扩展动作空间”的新范式。它没有将LLM应用于某个特定领域，而是从根本上解决了LLM在推理和执行动作时的一个根本性限制：即LLM只能通过输出文本来与外部世界交互。论文通过将外部环境（如计算器、模拟器）的动作“内化”到模型的动作空间中，使LLM能够直接调用和执行这些动作，而无需通过文本进行中转。这是一种**全新的方法论和训练范式**，其直接目标是增强LLM的**多步推理、规划和与环境交互的能力**。这完全属于“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴。 2.  **第二步：正面指标——论文命中多个关键主题。** 论文摘要中明确包含了大量正面指标的关键词： - **核心概念**: \"Large Language Models (LLMs)\" - **能力方向**: \"reasoners\", \"reason beyond language\", \"contingent planning\"（应急规划，是推理和规划的高级形式） - **训练方法**: \"Reinforcement Learning (EARL)\" - **新兴范式**: 整个框架可以看作是一种更强大的**llm-based agents**和**tool use**方法。它不是简单地“用文本描述工具使用”，而是让模型直接“执行工具动作”，这是对智能体和工具使用范式的深化。 这些指标表明，论文的研究方向与你的目标高度一致。 3.  **第三步：排除标准——论文未触及任何排除领域。** 论文的研究焦点纯粹集中在语言模型的动作空间和推理机制上，完全不涉及多模态、视觉、医疗、化学、机器人等特定应用领域，也未讨论模型部署或水印等可靠性问题。因此，它被排除的风险为零。 4.  **第四步：处理特殊情况——论文是“通用智能体/工具使用”的典型。** 论文提出的ExpA框架完美地符合特殊情况中的“保留”条款。它提出的是一种**通用的智能体协作和工具使用框架**，旨在增强LLM的**通用问题解决能力**。虽然在实验中使用了计算器和排序任务作为评测基准，但这些任务是用于验证其通用推理能力（数学、逻辑规划），而非应用本身。该框架的设计是领域无关的，可以潜在地应用于任何需要符号推理或环境交互的场景，其核心价值在于框架本身，而非某个特定应用。 **最终决策:** 综上所述，该论文精准地聚焦于提升LLM的内在通用推理能力，通过提出创新的“扩展动作空间”范式和相应的强化学习方法，从根本上改进了LLM进行多步推理和规划的方式。它不是应用型研究，而是对LLM核心能力的增强方法论研究，完全符合你的筛选要求。因此，判断为True。"
    },
    {
        "index": "#186",
        "title": "From Data to Rewards: a Bilevel Optimization Perspective on Maximum Likelihood Estimation",
        "link": "/arxiv/2510.07624",
        "arxiv_id": "2510.07624",
        "authors": "Abdelhakim Benechehab, Gabriel Singer, Corentin Léger, Youssef Attia El Hili, Giuseppe Paolo, Albert Thomas, Maurizio Filippone, Balázs Kégl",
        "summary": "Generative models form the backbone of modern machine learning, underpinning state-of-the-art systems in text, vision, and multimodal applications. While Maximum Likelihood Estimation has traditionally served as the dominant training paradigm, recent work have highlighted its limitations, particularly in generalization and susceptibility to catastrophic forgetting compared to Reinforcement Learning techniques, such as Policy Gradient methods. However, these approaches depend on explicit reward signals, which are often unavailable in practice, leaving open the fundamental problem of how to align generative models when only high-quality datasets are accessible. In this work, we address this challenge via a Bilevel Optimization framework, where the reward function is treated as the optimization variable of an outer-level problem, while a policy gradient objective defines the inner-level. We then conduct a theoretical analysis of this optimization problem in a tractable setting and extract insights that, as we demonstrate, generalize to applications such as tabular classification and model-based reinforcement learning. We release the code at https://github.com/abenechehab/nll_to_po .",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-08",
        "category": "cs.LG",
        "crawl_time": "2025-10-10T11:00:08.334401",
        "filter_reason": "这篇论文符合我的研究范围，理由如下： 1.  **核心判断（第一步）：论文本质是方法论创新，而非领域应用。** 论文的核心贡献是提出了一种名为“双层优化”的新训练框架。它的目标是解决一个基础性挑战：当只有高质量数据而没有显式奖励信号时，如何有效地训练（或“对齐”）生成模型。这直接关系到改进模型的基础训练范式。它将强化学习的思想（优化奖励函数）与监督学习的数据可用性结合起来，旨在生成泛化能力更强、不易灾难性遗忘的模型。这正是“致力于提高LLM本身基础能力”的研究，完全符合保留标准。 2.  **正面指标（第二步）：论文内容与研究目标高度相关。** 论文明确讨论了与LLM能力直接相关的核心概念。摘要中提到的“生成模型”在“文本”领域的应用，直接指向了LLM。它将最大似然估计（一种传统训练范式）与强化学习（如策略梯度，一种已知的能提升模型复杂能力的方法）进行对比，并试图融合二者的优点。提出新的训练方法论来提升模型的泛化能力，是提升通用推理能力的前置条件和关键一步。 3.  **排除标准（第三步）：论文焦点并非被排除领域。** 虽然摘要开头提到了“视觉”和“多模态”，但这只是作为背景介绍，说明生成模型应用的广泛性。论文的核心方法论——双层优化框架——是通用且不依赖于特定模态的。更重要的是，论文的实验验证部分明确是在“表格分类”和“基于模型的强化学习”等通用机器学习任务上进行的，而非视觉、医疗或化学等特定应用领域。因此，论文的主要焦点并未落在任何一项排除标准上。 4.  **特殊和模糊情况（第四步）：不适用。** 论文不涉及智能体/工具使用或幻觉/安全性的特殊讨论，因此无需进行特殊判断。 5.  **最终决策（第五步）：综合判断为“符合”。** 综合来看，这篇论文的本质是一种关于如何训练生成模型的基础性方法论研究。它通过创新的优化框架，试图解决数据驱动训练与强化学习训练之间的鸿沟，目标是从根本上提升模型的泛化能力和性能。这种对模型核心训练范式的探索，正是实现更强“通用推理能力”的必经之路和研究前沿。因此，这篇论文与研究目标高度契合，应被筛选。"
    }
]