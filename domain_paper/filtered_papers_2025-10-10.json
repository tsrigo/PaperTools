[
    {
        "index": "#1",
        "title": "Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation",
        "link": "/arxiv/2510.09599",
        "arxiv_id": "2510.09599",
        "authors": "Sondos Mahmoud Bsharat, Zhiqiang Shen",
        "summary": "Large language models (LLMs) have demonstrated impressive reasoning capabilities when provided with chain-of-thought exemplars, but curating large reasoning datasets remains laborious and resource-intensive. In this work, we introduce Prompting Test-Time Scaling (P-TTS), a simple yet effective inference-time data augmentation strategy for enhancing LLM reasoning through finetuning. Rather than collecting thousands or even millions of examples, P-TTS leverages a small pool of only 90 manually selected reasoning instances and systematically varies exemplar augmentation through principled instruction prompting intensities at test time to synthesize diverse reasoning trajectory contexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data. Across a suite of mathematical reasoning AIME2024 & 25, MATH500, and GPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive baselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of +26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B); P-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and +3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better performance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances zero-shot generalization accuracy on out-of-domain reasoning benchmarks of Gaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our analysis suggests that test-time scaling effectively explores the latent space of reasoning patterns, amplifying LLM problem-solving with minimal annotation overhead, and further unlocking the reasoning potential and capabilities of LLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit LLM reasoning in resource-constrained or rapidly evolving domains.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.307342",
        "filter_reason": "这篇论文完全符合你的筛选标准，应该被保留。以下是我的详细判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心是提出了一种名为“提示测试时缩放”的全新数据增强和微调策略。其本质目标并非将LLM应用于某个特定领域，而是通过一种创新的、低成本的方法来**增强LLM自身的通用推理能力**。具体来说，它解决了高质量推理数据稀缺的问题，通过在测试时动态生成多样化的推理轨迹来微调模型，从而提升其在数学和问题解决等通用推理任务上的表现。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——论文是否包含相关主题？** 该论文在摘要中大量出现了核心正面指标： - **核心概念**: 明确提到了 \"Large language models (LLMs)\"。 - **能力方向**: 反复强调 \"reasoning capabilities\", \"mathematical reasoning\", \"problem-solving\", \"reasoning patterns\", \"reasoning potential\"。 - **训练方法**: 提出了一种新的 \"finetuning\" 方法，这属于增强模型能力的训练范式研究。 - **新兴范式**: P-TTS本身就可以被看作是一种激发和提升LLM推理能力的新兴范式。 这些正面指标的存在，极大地强化了论文与你的研究目标的关联性。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文完全不涉及任何排除标准中的领域。它的评估基准是通用的数学和问题解决数据集（如AIME, MATH500, GPQA-Diamond），而非医疗、化学、法律等特定领域。同时，它也未涉及多模态、视觉或模型基础设施等问题。因此，该论文安全地避开了所有排除项。 4.  **第四步：处理特殊和模糊情况** 本论文不属于需要特殊处理的模糊情况。它没有讨论特定领域的智能体，也没有从应用层面讨论幻觉或安全问题。其焦点非常清晰：**一种提升模型内在通用推理能力的通用方法论**。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种创新的训练范式（P-TTS）来增强LLM的通用推理能力，这与你的核心目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度一致。论文内容与所有正面指标都高度相关，并且完全避开了所有排除标准。因此，这篇论文是与你研究课题高度相关的前沿文献，应予以保留。"
    },
    {
        "index": "#6",
        "title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare",
        "link": "/arxiv/2510.08872",
        "arxiv_id": "2510.08872",
        "authors": "Siqi Zhu, David Zhang, Pedro Cisneros-Velarde, Jiaxuan You",
        "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning, yet sometimes produce responses that are suboptimal for users in tasks such as writing, information seeking, or providing practical guidance. Conventional alignment practices typically assume that maximizing model reward also maximizes user welfare, but this assumption frequently fails in practice: models may over-clarify or generate overly verbose reasoning when users prefer concise answers. Such behaviors resemble the prisoner's dilemma, where individually rational choices lead to socially suboptimal outcomes. The fundamental challenge is the lack of a principled decision making mechanism that mutually benefits both the LLM and the user. We propose Game-Theoretic Alignment (GTAlign), an alignment framework that integrates game-theoretic decision making into both reasoning and training. During reasoning, the model explicitly treats user-LLM interaction as a strategic game: it constructs payoff matrices within its reasoning chain to estimate welfare for both itself and the user, and then selects actions that are mutually beneficial. During training, we introduce a mutual welfare reward that reinforces cooperative responses, aligning model behavior with socially efficient outcomes. In addition, we introduce an inference technique that leverages game-theoretic reasoning to dynamically adapt LLM's response when pricing policies of LLM service change. Extensive experiments demonstrate that GTAlign substantially improves reasoning efficiency, answer quality, and mutual welfare compared to baselines across diverse tasks. The code is available at https://github.com/ulab-uiuc/GTAlign .",
        "subjects": "Artificial Intelligence, Computer Science and Game Theory, Human-Computer Interaction, Machine Learning, Multiagent Systems",
        "date": "2025-10-10",
        "category": "cs.MA",
        "crawl_time": "2025-10-13T11:00:04.642696",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献是提出了一种新颖的框架来增强大语言模型的通用推理能力。以下是根据你的筛选标准进行的详细判断： 1.  **第一步：核心判断** 这篇论文的本质是关于改进LLM的基础能力，而非将其应用于特定领域。论文的核心贡献是**GTAlign框架**，它通过将博弈论决策机制直接整合到LLM的**推理过程**和**训练范式**中，来解决模型输出与用户福祉不匹配的问题。这直接命中了你筛选标准中“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。它不是把LLM当作工具用在医疗、金融等领域，而是从根本上提升模型自身的“思考”质量——即如何产出对用户而言更优、更高效的回答。这是一种更高层次的通用推理能力的体现。 2.  **第二步：正面指标** 该论文几乎完美地覆盖了所有关键的正面指标： - **核心概念**: 论文明确聚焦于 \"Large Language Models (LLMs)\"。 - **能力方向**: 论文的核心是提升 \"reasoning\" 能力，具体表现为改善 \"reasoning efficiency\" (推理效率) 和 \"answer quality\" (回答质量)，并将其应用于广泛的 \"problem-solving\" 场景（如写作、信息搜寻）。 - **训练方法**: 论文提出了一种新的训练方法，即引入 \"mutual welfare reward\" (共同福祉奖励)，这属于对强化学习（RL）的创新应用，旨在优化模型的内在行为。 - **新兴范式**: 论文将用户-LLM的交互视为一个 \"strategic game\" (战略博弈)，这与 \"llm-based agents\" 和 \"multi-agent systems\" 的思想高度契合，探索了智能体在交互中的决策机制。 3.  **第三步：排除标准** 该论文完全避开了所有的排除领域。它不涉及多模态、视觉，不针对任何特定的应用领域（如医疗、化学），也不关注模型部署、水印或安全等应用层面的可靠性问题。其焦点始终在模型核心的推理与对齐机制上。 4.  **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的博弈论框架是一种**通用的**LLM与用户交互的决策框架，旨在提升模型在通用任务中的表现，而不是将其应用于某个特定领域（如“用于化学实验的智能体”）。因此，这完全符合保留标准。 - **幻觉/可解释性/安全**: 论文虽然涉及“对齐”，但其方法是通过改进模型的内在推理过程（在思维链中构建收益矩阵）和训练目标（共同福祉奖励）来实现的。这属于提出一种新方法来提升模型的内在质量和推理可靠性的范畴，因此符合保留标准。 **最终决策**: 综合以上分析，这篇论文提出了一种创新的、基于博弈论的框架（GTAlign），通过改进模型的训练和推理过程，使其在生成回答时能更好地权衡自身行为与用户福祉，从而显著提升了通用推理的效率和答案质量。这项工作直接致力于提高LLM本身的核心能力，完全符合你“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标。因此，应予以保留。"
    },
    {
        "index": "#7",
        "title": "What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment",
        "link": "/arxiv/2510.08847",
        "arxiv_id": "2510.08847",
        "authors": "Allison Sihan Jia, Daniel Huang, Nikhil Vytla, Nirvika Choudhury, John C Mitchell, Anupam Datta",
        "summary": "We introduce the Agent GPA (Goal-Plan-Action) framework: an evaluation paradigm based on an agent's operational loop of setting goals, devising plans, and executing actions. The framework includes five evaluation metrics: Goal Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan Adherence. Logical Consistency checks that an agent's actions are consistent with its prior actions. Execution Efficiency checks whether the agent executes in the most efficient way to achieve its goal. Plan Quality checks whether an agent's plans are aligned with its goals; Plan Adherence checks if an agent's actions are aligned with its plan; and Goal Fulfillment checks that agent's final outcomes match the stated goals. Our experimental results on two benchmark datasets - the public TRAIL/GAIA dataset and an internal dataset for a production-grade data agent - show that this framework (a) provides a systematic way to cover a broad range of agent failures, including all agent errors on the TRAIL/GAIA benchmark dataset; (b) supports LLM-judges that exhibit strong agreement with human annotation, covering 80% to over 95% errors; and (c) localizes errors with 86% agreement to enable targeted improvement of agent performance.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-09",
        "category": "cs.MA",
        "crawl_time": "2025-10-13T11:00:04.642972",
        "filter_reason": "这篇论文符合筛选要求。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心贡献是提出了一个名为“Agent GPA”的**通用评估框架**，用于系统性地评估LLM智能体在执行任务时的推理和规划过程。它并非将LLM作为工具应用于某个特定领域（如医疗、金融），而是聚焦于智能体本身的“目标-计划-行动”这一核心推理循环。其最终目的是为了“实现针对智能体性能的靶向改进”，这直接指向了提升LLM的内在通用能力。因此，论文的本质是改进LLM的基础推理与规划能力，符合保留标准。 2.  **第二步：正面指标** 论文与多个正面指标高度相关： -   **核心概念**: 论文的研究对象是基于LLM的智能体。 -   **能力方向**: 论文的核心是评估智能体的 **reasoning (推理)** 和 **planning (规划)** 能力。其提出的指标“逻辑一致性”、“计划质量”、“计划遵循”都是通用推理能力的具体体现。 -   **新兴范式**: 论文完全聚焦于 **llm-based agents (基于LLM的智能体)** 这一前沿范式。 3.  **第三步：排除标准** 论文未触及任何排除标准： -   它不涉及多模态、视觉等。 -   它的框架是通用的，虽然在生产级数据智能体上做了测试，但框架本身并非为特定领域（如化学、生物）设计。 -   它关注的不是应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** -   **智能体/工具使用**: 这篇论文属于“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的情况。虽然它是一个评估框架而非执行框架，但它通过精确诊断推理过程中的失败（如逻辑不一致、计划与目标脱节），为提升通用问题解决能力提供了关键的反馈和改进方向。因此，应当保留。 -   **幻觉/可解释性/安全**: 论文提出的“逻辑一致性”和“计划遵循”指标，本质上是在探测和量化智能体在多步推理过程中的某种内在“不一致性”，这与减少幻觉、提升内在可解释性密切相关。它是一种新的、针对推理过程质量的方法论研究，而非应用层面的讨论，因此应当保留。 **最终决策**: 综合以上分析，这篇论文虽然是一个评估框架，但其核心是构建一套方法论来精确度量和诊断LLM智能体的通用推理与规划能力。这种基础性的、旨在“理解并改进”模型内在能力的研究，正是“提高大语言模型（LLM）本身的『通用推理能力』”这一课题所需要的前沿工作。因此，该论文高度符合研究范围。"
    },
    {
        "index": "#5",
        "title": "WUGNECTIVES: Novel Entity Inferences of Language Models from Discourse Connectives",
        "link": "/arxiv/2510.09556",
        "arxiv_id": "2510.09556",
        "authors": "Daniel Brubaker, William Sheffield, Junyi Jessy Li, Kanishka Misra",
        "summary": "The role of world knowledge has been particularly crucial to predict the discourse connective that marks the discourse relation between two arguments, with language models (LMs) being generally successful at this task. We flip this premise in our work, and instead study the inverse problem of understanding whether discourse connectives can inform LMs about the world. To this end, we present WUGNECTIVES, a dataset of 8,880 stimuli that evaluates LMs' inferences about novel entities in contexts where connectives link the entities to particular attributes. On investigating 17 different LMs at various scales, and training regimens, we found that tuning an LM to show reasoning behavior yields noteworthy improvements on most connectives. At the same time, there was a large variation in LMs' overall performance across connective type, with all models systematically struggling on connectives that express a concessive meaning. Our findings pave the way for more nuanced investigations into the functional role of language cues as captured by LMs. We release WUGNECTIVES at https://github.com/sheffwb/wugnectives.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.310056",
        "filter_reason": "这篇论文完全符合你的研究范围。 **判断过程如下:** 1.  **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是**探究并提升大语言模型的一种基础推理能力**。它并非将LLM应用于某个特定领域，而是反向思考一个基础的语言学问题：模型能否从语篇连接词（如“虽然”、“因为”、“所以”）这类语言线索中推断出关于世界的新知识？论文的核心贡献之一是发现“**将一个LM微调以展现推理行为，在大多数连接词上带来了显著的改进**”。这直接触及了如何通过训练方法（微调）来增强模型的逻辑推理能力，属于改进LLM基础能力的范畴。因此，根据第一步标准，应**保留**。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文摘要中明确包含了多个正面指标： *   **核心概念**: \"language models (LMs)\" *   **能力方向**: \"inferences\", \"reasoning behavior\"。论文研究的“从连接词推断实体属性”是一种典型的逻辑推理能力。 *   **训练方法**: \"tuning an LM to show reasoning behavior\"，这是一种新的训练/微调范式。 这些指标的出现，进一步确认了论文与你的研究目标高度相关。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 该论文的研究焦点是纯粹的语言学和认知科学问题，即模型如何理解和运用语言中的逻辑关系。它完全不涉及多模态、视觉，也没有应用于医疗、化学、机器人等任何特定领域。同时，它也不是关于水印、安全等模型可靠性问题。因此，所有排除标准均不适用。 4.  **第四步：处理特殊和模糊情况** 该论文不涉及智能体/工具使用或幻觉/安全等特殊模糊情况，其研究主线非常清晰。 **最终决策:** 综合以上分析，这篇论文《WUGNECTIVES: Novel Entity Inferences of Language Models from Discourse Connectives》是一项高质量的基础研究。它通过构建新的数据集来评估LLM在特定逻辑推理任务上的表现，并发现了一种通过微调来提升该推理能力的有效方法。其核心贡献直接指向“**提高大语言模型本身的通用推理能力**”这一目标，特别是其中的逻辑推理维度。因此，它完全符合你的筛选要求。"
    },
    {
        "index": "#2",
        "title": "Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in Spoken Language Models",
        "link": "/arxiv/2510.09592",
        "arxiv_id": "2510.09592",
        "authors": "Donghang Wu, Haoyang Zhang, Jun Chen, Xiangyu, Zhang, Hexin Liu, Eng Siong Chng, Fei Tian, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Gang Yu",
        "summary": "Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought (CoT) reasoning due to the prohibitive latency of generating the entire thought process sequentially. Enabling SLMs to think while speaking, similar to humans, is attracting increasing attention. We present, for the first time, Mind-Paced Speaking (MPS), a brain-inspired framework that enables high-fidelity, real-time reasoning. Similar to how humans utilize distinct brain regions for thinking and responding, we propose a novel dual-brain approach, employing a \"Formulation Brain\" for high-level reasoning to pace and guide a separate \"Articulation Brain\" for fluent speech generation. This division of labor eliminates mode-switching, preserving the integrity of the reasoning process. Experiments show that MPS significantly outperforms existing think-while-speaking methods and achieves reasoning performance comparable to models that pre-compute the full CoT before speaking, while drastically reducing latency. Under a zero-latency configuration, the proposed method achieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and attains a score of 82.5 on the speech conversation task URO-Bench. Our work effectively bridges the gap between high-quality reasoning and real-time interaction.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.307991",
        "filter_reason": "这篇论文完全符合筛选要求，应予以保留。 1.  **核心判断（第一步）**: 论文的核心本质是提出一种名为“Mind-Paced Speaking (MPS)”的新框架，用于解决口语语言模型（SLMs）在进行“思维链”推理时面临的根本性延迟问题。它没有将LLM作为工具应用在特定领域，而是直接改进LLM（特别是其子类SLM）的基础推理能力。论文提出的“双脑”架构（一个负责推理的“Formulation Brain”和一个负责表达的“Articulation Brain”）是一种全新的方法论，旨在增强模型的逻辑推理和多步思考能力，同时满足实时性要求。这直接命中了“改进LLM的基础能力”和“增强其逻辑、数学、多步推理等通用能力”的核心目标。 2.  **正面指标（第二步）**: 论文明确包含了多个高度相关的正面指标。 *   **核心概念**: 论文研究对象是口语语言模型（SLMs），属于大语言模型（LLMs）的范畴。 *   **能力方向**: 标题和摘要中反复出现“Real-Time Reasoning”、“Chain-of-Thought (CoT) reasoning”、“mathematical reasoning task”，清晰地表明其研究核心就是“推理”能力。 *   **新兴范式**: “dual-brain approach”是一种新颖的架构范式，通过分工协作来提升整体性能，这可以被视为一种增强LLM问题解决能力的通用框架。 3.  **排除标准（第三步）**: 论文不涉及任何排除标准。 *   它不涉及视觉或多模态内容，处理的是语音和文本，属于语言模型的内部范畴。 *   其评估任务虽然是数学推理和对话，但这些是衡量“通用能力”的基准测试，而非医疗、化学等特定应用领域。 *   论文焦点是提升推理质量和效率，而非水印、安全等应用层面的可靠性问题。 4.  **特殊和模糊情况（第四步）**: 论文提出的“双脑”框架可以被视为一种通用的、增强模型内在能力的架构。它并非应用于特定领域的智能体，而是通过改变模型内部的思考和协同方式来提升其通用的实时推理能力，这与“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的精神内核一致，因此应当保留。 **综上所述**，该论文提出了一种创新的、以提升LLM通用推理能力（特别是实时、多步推理）为唯一目标的架构范式。它解决了现有技术（如CoT）在特定场景（实时交互）下的一个核心瓶颈，其贡献是方法论层面的，直接服务于提升模型基础能力这一核心目标。因此，这篇论文与您的研究范围高度契合。"
    },
    {
        "index": "#8",
        "title": "Beyond Surface Reasoning: Unveiling the True Long Chain-of-Thought Capacity of Diffusion Large Language Models",
        "link": "/arxiv/2510.09544",
        "arxiv_id": "2510.09544",
        "authors": "Qiguang Chen, Hanjing Li, Libo Qin, Dengyun Peng, Jinhao Liu, Jiangyi Wang, Chengyue Wu, Xie Chen, Yantao Du, Wanxiang Che",
        "summary": "Recently, Diffusion Large Language Models (DLLMs) have offered high throughput and effective sequential reasoning, making them a competitive alternative to autoregressive LLMs (ALLMs). However, parallel decoding, which enables simultaneous token updates, conflicts with the causal order often required for rigorous reasoning. We first identify this conflict as the core Parallel-Sequential Contradiction (PSC). Behavioral analyses in both simple and complex reasoning tasks show that DLLMs exhibit genuine parallelism only for directly decidable outputs. As task difficulty increases, they revert to autoregressive-like behavior, a limitation exacerbated by autoregressive prompting, which nearly doubles the number of decoding steps with remasking without improving quality. Moreover, PSC restricts DLLMs' self-reflection, reasoning depth, and exploratory breadth. To further characterize PSC, we introduce three scaling dimensions for DLLMs: parallel, diffusion, and sequential. Empirically, while parallel scaling yields consistent improvements, diffusion and sequential scaling are constrained by PSC. Based on these findings, we propose several practical mitigations, parallel-oriented prompting, diffusion early stopping, and parallel scaling, to reduce PSC-induced ineffectiveness and inefficiencies.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.322349",
        "filter_reason": "这篇论文完全符合您的筛选要求，应被保留。以下是基于您提供的筛选标准的详细判断过程： **第一步：核心判断** 论文的核心是**关于改进LLM的基础能力**。它聚焦于一种新兴的LLM架构——扩散大语言模型（DLLMs），并深入探究其在执行“长链式思维”推理任务时的内在机制和局限性。论文的核心贡献在于识别并分析了DLLMs中存在的“并行-序列矛盾”（PSC）问题，这个问题从根本上限制了模型的推理深度、自我反思和探索能力。这并非将LLM作为工具应用于特定领域，而是对LLM本身的基础推理范式进行剖析和优化，旨在提升其通用的、严谨的、多步的推理能力。因此，论文的本质完全符合保留标准。 **第二步：正面指标** 论文命中了多个关键的正面指标： - **核心概念**: 论文明确以“Diffusion Large Language Models (DLLMs)”为核心研究对象。 - **能力方向**: 论文的标题和摘要通篇都在讨论“推理”能力，具体包括“sequential reasoning”（序列推理）、“rigorous reasoning”（严谨推理）、“long Chain-of-Thought”（长链式思维）、“reasoning depth”（推理深度）和“self-reflection”（自我反思）。这些都是通用推理能力的核心组成部分。 - **新兴范式**: 虽然没有直接提及“Agents”或“Tool Use”，但对“self-reflection”和“exploratory breadth”（探索广度）的分析，与构建高级LLM智能体所需的基础能力高度相关。论文致力于解决的是这些高级能力背后的根本性推理瓶颈。 **第三步：排除标准** 论文不符合任何一项排除标准： - **多模态与视觉**: 论文研究的是纯语言模型，不涉及视觉或多模态内容。 - **特定应用领域**: 论文的评估任务是“简单和复杂的推理任务”，属于通用能力范畴，而非医疗、化学等特定领域。 - **模型可靠性（应用层面）**: 论文讨论的“ineffectiveness and inefficiencies”（无效性和低效率）是模型在执行推理任务时的内在性能缺陷，而不是水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** 论文的情况不属于模糊范畴，其目标非常清晰。它所做的工作——分析并缓解PSC问题——与“提出一种新方法来减少幻觉、增强模型内在的可解释性”的目标异曲同工。通过解决PSC这个根本性的推理瓶颈，论文直接提升了DLLM在长程推理任务中的质量和可靠性，从而增强了其通用推理能力。 **第五步：最终决策** 综合分析，这篇论文是关于“大语言模型通用推理能力”的高质量前沿研究。它没有停留在表面应用，而是深入到模型架构与推理过程的交互层面，揭示了一个关键的科学问题（PSC），并提出了针对性的解决方案（并行导向的提示、扩散提前停止等）。这项工作不仅加深了我们对DLLMs推理能力的理解，更为如何有效提升其通用推理能力提供了明确的指导方向，完全契合您的研究目标。"
    },
    {
        "index": "#6",
        "title": "A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages",
        "link": "/arxiv/2510.09555",
        "arxiv_id": "2510.09555",
        "authors": "Raoyuan Zhao, Yihong Liu, Hinrich Schütze, Michael A. Hedderich",
        "summary": "Large reasoning models (LRMs) increasingly rely on step-by-step Chain-of-Thought (CoT) reasoning to improve task performance, particularly in high-resource languages such as English. While recent work has examined final-answer accuracy in multilingual settings, the thinking traces themselves, i.e., the intermediate steps that lead to the final answer, remain underexplored. In this paper, we present the first comprehensive study of multilingual CoT reasoning, evaluating three key dimensions: performance, consistency, and faithfulness. We begin by measuring language compliance, answer accuracy, and answer consistency when LRMs are explicitly instructed or prompt-hacked to think in a target language, revealing strong language preferences and divergent performance across languages. Next, we assess crosslingual consistency of thinking traces by interchanging them between languages. We find that the quality and effectiveness of thinking traces vary substantially depending on the prompt language. Finally, we adapt perturbation-based techniques -- i.e., truncation and error injection -- to probe the faithfulness of thinking traces across languages, showing that models rely on traces to varying degrees. We release our code and data to support future research.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.310645",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的本质是对“思维链”这一通用推理方法进行深入、全面的评估。思维链是提升大语言模型多步推理能力的核心范式之一。虽然这篇论文没有提出一种全新的训练方法来*直接提升*推理能力，但它系统地分析和揭示了现有CoT方法在多语言环境下的表现、一致性和忠实度等关键问题。这种对核心推理机制本身的深度剖析，是未来改进和增强LLM通用推理能力的基础和前提。它不属于将LLM应用于特定领域的研究，而是聚焦于LLM的基础能力。因此，其核心贡献属于“增强其逻辑、...多步推理等通用能力”的研究范畴。 2.  **第二步：正面指标** 论文包含了多个高相关性的正面指标： - **核心概念**: 明确以“Large reasoning models (LRMs)”为研究对象。 - **能力方向**: 核心主题就是“Chain-of-Thought (CoT) reasoning”，这正是通用推理能力的关键体现。 - 论文内容直接围绕reasoning的多个维度（performance, consistency, faithfulness）展开，与筛选标准高度吻合。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： - 不涉及多模态、视觉。 - 不聚焦于任何特定应用领域（如医疗、化学等）。 - 不讨论模型基础设施或部署优化。 - 虽然提到了“faithfulness”，但这是从推理过程可靠性的角度进行探究，而非应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 论文对“忠实度”的研究可以归入对“模型内在可解释性”的探讨。它通过扰动实验来探测模型是否真的依赖于其生成的推理步骤，这对于理解和提升模型的推理质量至关重要。这符合“如果论文提出一种新方法来...增强模型内在的可解释性...从而提升模型的通用可靠性和推理质量，应该保留”的原则。本文提出的评估方法就是一种新的探测方法。 **最终决策**: 综合来看，这篇论文虽然是一篇评估性研究，但其研究对象（CoT）是LLM通用推理能力的基石。它通过严谨的实验揭示了这一核心范式在多语言环境下的内在特性和缺陷，为后续如何“提高”LLM的通用推理能力提供了关键的数据支持和研究方向。因此，这篇论文是你研究课题中非常有价值的前沿文献，应当保留。"
    },
    {
        "index": "#3",
        "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents",
        "link": "/arxiv/2510.09577",
        "arxiv_id": "2510.09577",
        "authors": "Xiao Yu, Baolin Peng, Michel Galley, Hao Cheng, Qianhui Wu, Janardhan Kulkarni, Suman Nath, Zhou Yu, Jianfeng Gao",
        "summary": "Reasoning models have recently shown remarkable progress in domains such as math and coding. However, their expert-level abilities in math and coding contrast sharply with their performance in long-horizon, interactive tasks such as web navigation and computer/phone-use. Inspired by literature on human cognition, we argue that current AI agents need ''vicarious trial and error'' - the capacity to mentally simulate alternative futures before acting - in order to enhance their understanding and performance in complex interactive environments. We introduce Dyna-Mind, a two-stage training framework that explicitly teaches (V)LM agents to integrate such simulation into their reasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which trains the agent to generate structured reasoning traces from expanded search trees built from real experience gathered through environment interactions. ReSim thus grounds the agent's reasoning in faithful world dynamics and equips it with the ability to anticipate future states in its reasoning. In stage 2, we propose Dyna-GRPO, an online reinforcement learning method to further strengthen the agent's simulation and decision-making ability by using both outcome rewards and intermediate states as feedback from real rollouts. Experiments on two synthetic benchmarks (Sokoban and ALFWorld) and one realistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively infuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome and interaction-level signals to learn better policies for long-horizon, planning-intensive tasks. Together, these results highlight the central role of simulation in enabling AI agents to reason, plan, and act more effectively in the ever more challenging environments.",
        "subjects": "Computation and Language, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.308723",
        "filter_reason": "这篇论文的核心贡献是提出了一个名为 Dyna-Mind 的两阶段训练框架（ReSim 和 Dyna-GRPO），该框架旨在通过教会 AI 智能体进行“心像模拟”，从而提升其在复杂、长时程交互任务中的通用推理和规划能力。这与您筛选『大语言模型通用推理能力』的核心目标高度一致。 以下是根据您的筛选标准进行的详细判断过程： 1.  **第一步：核心判断** 论文的本质是提出一种新的训练范式，来增强模型的基础能力。它并非将LLM作为工具应用于特定领域（如医疗、金融），而是直接针对LLM/VLM在“长时程、交互式任务”上表现不佳这一核心痛点进行改进。论文的核心方法——教会模型进行“心像模拟”——是一种旨在增强其规划、决策和未来预测等通用推理能力的根本性方法，与“思维链(CoT)”、“强化学习优化”等属于同一类别的、旨在提升模型内在能力的研究。因此，该论文应被**保留**。 2.  **第二步：正面指标** 论文摘要中包含了多个强烈的正面指标： *   **核心概念**: 明确提到了 \"(V)LM agents\"。 *   **能力方向**: 核心关注点就是 \"reasoning\", \"planning\", \"long-horizon, interactive tasks\"。这些都是通用推理能力的核心组成部分。 *   **训练方法**: 提出了两个新的训练方法，其中 \"Dyna-GRPO\" 是一种 \"online reinforcement learning method\"，完全命中 \"reinforcement learning\" 指标。 *   **新兴范式**: 整篇论文都在讨论如何构建更好的 \"AI agents\"，使其具备更强的通用问题解决能力。 3.  **第三步：排除标准** 该论文并未触及任何主要的排除标准： *   **多模态与视觉**: 虽然论文提到了 (V)LM agents，但它的核心贡献并非一种新的视觉或多模态技术。视觉只是其评测环境 的一种输入模态，论文的精髓在于模拟和规划的*算法与训练框架*，这是一种可以脱离视觉存在的通用推理方法论。 *   **特定应用领域**: 论文的实验环境是 Sokoban (经典规划游戏)、ALFWorld (家居交互) 和 AndroidWorld (手机操作)，这些都是通用的、非特定领域的交互式任务，旨在测试模型的通用规划能力，而非在医疗、化学等专业领域的应用。 *   **模型可靠性**: 论文未涉及水印、安全等议题。 4.  **第四步：处理特殊和模糊情况** *   **智能体**: 这篇论文是“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的典型案例。Dyna-Mind 是一个通用的框架，其目标是提升智能体在“长时程、规划密集型任务”中的表现，而不是将其局限于某个垂直领域。因此，完全符合保留条件。 **最终决策**: 综合以上分析，这篇论文精准地聚焦于提升大语言模型（或视觉语言模型）在复杂、交互式环境下的通用推理与规划能力。它提出了新颖的训练范式，旨在从根源上增强模型的心智模拟能力，这直接对应了您研究课题的核心。因此，这篇论文完全符合您的筛选要求。"
    },
    {
        "index": "#9",
        "title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models",
        "link": "/arxiv/2510.09541",
        "arxiv_id": "2510.09541",
        "authors": "Chengyu Wang, Paria Rashidinejad, DiJia Su, Song Jiang, Sid Wang, Siyan Zhao, Cai Zhou, Shannon Zejiang Shen, Feiyu Chen, Tommi Jaakkola, Yuandong Tian, Bo Liu",
        "summary": "Diffusion large language models (dLLMs) are emerging as an efficient alternative to autoregressive models due to their ability to decode multiple tokens in parallel. However, aligning dLLMs with human preferences or task-specific rewards via reinforcement learning (RL) is challenging because their intractable log-likelihood precludes the direct application of standard policy gradient methods. While prior work uses surrogates like the evidence lower bound (ELBO), these one-sided approximations can introduce significant policy gradient bias. To address this, we propose the Sandwiched Policy Gradient (SPG) that leverages both an upper and a lower bound of the true log-likelihood. Experiments show that SPG significantly outperforms baselines based on ELBO or one-step estimation. Specifically, SPG improves the accuracy over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500, 18.4% in Countdown and 27.0% in Sudoku.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.323008",
        "filter_reason": "这篇论文完全符合您的筛选标准。 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献是提出了一种新的训练方法——\"三明治策略梯度\"(SPG)，用于解决扩散大语言模型在强化学习对齐上的难题。这属于改进LLM基础训练范式的范畴，旨在从根本上提升模型的能力。 - **目标契合**: 论文的目标是通过改进训练算法，来提升模型在数学和逻辑推理任务上的表现。这直接对应了您研究目标中的“提高大语言模型（LLM）本身的『通用推理能力』”，特别是数学推理和逻辑规划能力。它不是将LLM作为工具应用于某个领域，而是致力于打磨LLM这把“工具”本身。 2.  **第二步：正面指标** - 论文明确包含了多个关键正面指标： - **核心概念**: \"Diffusion large language models (dLLMs)\"，即大语言模型。 - **能力方向**: 论文的实验部分直接在**数学推理**的黄金标准数据集（GSM8K, MATH500）和逻辑规划问题（Countdown, Sudoku）上进行验证，这与\"reasoning (math reasoning, logical reasoning)\"高度相关。 - **训练方法**: 论文的核心是提出一种新的**强化学习**方法，属于\"reinforcement learning (RL)\"的范畴。 3.  **第三步：排除标准** - 该论文未涉及任何排除标准领域。它专注于纯文本语言模型，而非多模态或视觉。其应用场景是通用的数学和逻辑问题，而非医疗、化学等特定领域。研究内容是训练算法，而非水印、安全或基础设施。 **最终决策**: 综合来看，这篇论文是一篇典型的、高质量的方法论研究。它提出了一种新颖的强化学习技术，专门用于解决新型扩散语言模型的训练瓶颈，并显著提升了其在数学和逻辑推理这一核心通用能力上的表现。这与您筛选“致力于提高大语言模型本身通用推理能力”的论文的目标高度一致。因此，应判定为符合要求。"
    },
    {
        "index": "#19",
        "title": "KORMo: Korean Open Reasoning Model for Everyone",
        "link": "/arxiv/2510.09426",
        "arxiv_id": "2510.09426",
        "authors": "Minjun Kim, Hyeonseok Lim, Hangyeol Yoo, Inho Won, Seungwoo Song, Minkyung Cho, Junhun Yuk, Changsu Choi, Dongjae Shin, Huige Lee, Hoyun Song, Alice Oh, Kyungtae Lim",
        "summary": "This work presents the first large-scale investigation into constructing a fully open bilingual large language model (LLM) for a non-English language, specifically Korean, trained predominantly on synthetic data. We introduce KORMo-10B, a 10.8B-parameter model trained from scratch on a Korean-English corpus in which 68.74% of the Korean portion is synthetic. Through systematic experimentation, we demonstrate that synthetic data, when carefully curated with balanced linguistic coverage and diverse instruction styles, does not cause instability or degradation during large-scale pretraining. Furthermore, the model achieves performance comparable to that of contemporary open-weight multilingual baselines across a wide range of reasoning, knowledge, and instruction-following benchmarks. Our experiments reveal two key findings: (1) synthetic data can reliably sustain long-horizon pretraining without model collapse, and (2) bilingual instruction tuning enables near-native reasoning and discourse coherence in Korean. By fully releasing all components including data, code, training recipes, and logs, this work establishes a transparent framework for developing synthetic data-driven fully open models (FOMs) in low-resource settings and sets a reproducible precedent for future multilingual LLM research.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.333315",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。 **判断过程分析:** 1.  **第一步：核心判断** 这篇论文的本质并非将LLM作为工具应用于特定领域，而是提出并验证了一种**新的训练范式**——即在低资源语言环境下，大规模使用合成数据来从零开始训练一个强大的大语言模型。论文的核心贡献是证明了这种方法的可行性和有效性，并揭示了“合成数据可以稳定支撑长周期预训练”以及“双语指令微调能实现近乎原生的推理能力”这两个关键发现。这直接属于“改进LLM的基础能力、提出新的训练范式”的范畴，因此应予以保留。 2.  **第二步：正面指标** 论文明确包含了以下正面指标： *   **核心概念**: 摘要中多次提及 \"large language model (LLM)\"。 *   **能力方向**: 论文的副标题和摘要中都强调了 \"Reasoning\"，并在多个 \"reasoning\" 基准上评估了模型性能。 *   **训练方法**: 论文的核心创新点在于使用 \"synthetic data\" 进行训练，这是一种新颖的训练方法，虽然不是RL，但同样属于训练范式的研究。 3.  **第三步：排除标准** 论文不符合任何一项硬性排除标准： *   **多模态**: 这是一个纯文本模型，不涉及视觉。 *   **特定应用领域**: 尽管论文以“韩语”这个特定语言为例，但语言本身是一个基础维度，而非像医疗、化学那样的垂直应用领域。论文的目标是解决“如何为低资源语言构建具备通用推理能力的模型”这一基础问题，而非“如何用LLM解决韩国的某个具体业务问题”。其方法和发现具有普适性，可推广到其他低资源语言。 *   **模型可靠性**: 论文关注的是训练过程的稳定性，而非应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 这里最关键的一点是区分“特定领域”和“以特定领域为案例研究基础方法”。这篇论文属于后者。它以韩语为案例，来验证其提出的“合成数据驱动”这一通用训练框架的有效性。其最终目标是建立一个可复制的、面向低资源语言的通用模型开发框架，这与我们的研究目标——“提高LLM本身的通用推理能力”——高度一致。它通过提供一种新的构建和训练LLM的方法，间接但有力地推动了通用推理能力的边界。 **最终决策:** 综合来看，尽管论文标题和案例聚焦于韩语，但其核心贡献是一种创新的、旨在提升LLM基础能力（尤其是推理能力）的通用训练方法论。该论文为在数据稀缺条件下如何构建强大的推理模型提供了宝贵的见解和可复现的框架，完全符合“致力于提高大语言模型（LLM）本身的通用推理能力”这一核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#16",
        "title": "Hybrid Models for Natural Language Reasoning: The Case of Syllogistic Logic",
        "link": "/arxiv/2510.09472",
        "arxiv_id": "2510.09472",
        "authors": "Manuel Vargas Guzmán, Jakub Szymanik, Maciej Malicki",
        "summary": "Despite the remarkable progress in neural models, their ability to generalize, a cornerstone for applications like logical reasoning, remains a critical challenge. We delineate two fundamental aspects of this ability: compositionality, the capacity to abstract atomic logical rules underlying complex inferences, and recursiveness, the aptitude to build intricate representations through iterative application of inference rules. In the literature, these two aspects are often confounded together under the umbrella term of generalization. To sharpen this distinction, we investigated the logical generalization capabilities of pre-trained large language models (LLMs) using the syllogistic fragment as a benchmark for natural language reasoning. Though simple, this fragment provides a foundational yet expressive subset of formal logic that supports controlled evaluation of essential reasoning abilities. Our findings reveal a significant disparity: while LLMs demonstrate reasonable proficiency in recursiveness, they struggle with compositionality. To overcome these limitations and establish a reliable logical prover, we propose a hybrid architecture integrating symbolic reasoning with neural computation. This synergistic interaction enables robust and efficient inference, neural components accelerate processing, while symbolic reasoning ensures completeness. Our experiments show that high efficiency is preserved even with relatively small neural components. As part of our proposed methodology, this analysis gives a rationale and highlights the potential of hybrid models to effectively address key generalization barriers in neural reasoning systems.",
        "subjects": "Computation and Language, Machine Learning, Logic in Computer Science",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.326539",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是详细的判断过程和依据： 1.  **第一步：核心判断（符合保留标准）** 论文的本质是研究并改进大语言模型（LLM）在逻辑推理这一核心通用能力上的缺陷。它并非将LLM作为工具应用于某个特定领域，而是深入剖析了LLM在“自然语言推理”任务中的泛化能力瓶颈。论文明确指出了LLM在逻辑推理的“组合性”上的不足，并提出了一个全新的“神经-符号混合架构”来克服这一限制。这种旨在增强模型基础逻辑推理能力的方法论研究，直接命中了您筛选标准中“改进LLM的基础能力、增强其逻辑、……多步推理等通用能力”的核心目标。 2.  **第二步：正面指标（高度匹配）** 论文与多个正面指标高度相关： *   **核心概念**: 明确以“pre-trained large language models (LLMs)”为研究对象。 *   **能力方向**: 聚焦于“reasoning”，特别是“logical reasoning”和“natural language reasoning”，这正是您关注的核心方向。 *   **新兴范式**: 提出的“Hybrid Models”（混合模型）是一种新的方法论范式，旨在通过结合神经网络的效率和符号推理的严谨性，来提升模型的推理能力，这与您关注的“新的训练范式”、“方法论研究”等范畴一致。 3.  **第三步：排除标准（完全避开）** 该论文与所有排除标准均无关系。它不涉及多模态与视觉，不聚焦于任何特定应用领域（如医疗、化学等），也并非讨论模型部署、硬件加速或应用层面的水印、安全等问题。其使用的“三段论”只是一个用于衡量通用逻辑推理能力的基准，而非特定应用场景。 4.  **第四步：处理特殊和模糊情况（适用保留逻辑）** 论文虽然未直接提及“幻觉”或“可解释性”，但它通过引入符号推理器来确保推理的“完整性”，这从根本上提升了模型推理结果的可靠性和正确性，属于“提升模型的通用可靠性和推理质量”的范畴。其提出的混合架构，本质上是一种为了增强通用问题解决能力（逻辑证明）而设计的新框架，其精神与保留“通用的智能体协作框架或工具使用方法”的逻辑是一致的。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于精准定位了LLM在通用逻辑推理能力上的一个关键短板（组合性），并提出了一种创新的、旨在弥补该短板的混合架构。它的研究目标、方法和结论都紧紧围绕着“如何提升大语言模型本身的通用推理能力”这一核心议题，是您研究课题下的高度相关和高质量的前沿文献，应当被保留。"
    },
    {
        "index": "#15",
        "title": "Multimodal Policy Internalization for Conversational Agents",
        "link": "/arxiv/2510.09474",
        "arxiv_id": "2510.09474",
        "authors": "Zhenhailong Wang, Jiateng Liu, Amin Fazel, Ritesh Sarkhel, Xing Fan, Xiang Li, Chenlei Guo, Heng Ji, Ruhi Sarikaya",
        "summary": "Modern conversational agents like ChatGPT and Alexa+ rely on predefined policies specifying metadata, response styles, and tool-usage rules. As these LLM-based systems expand to support diverse business and user queries, such policies, often implemented as in-context prompts, are becoming increasingly complex and lengthy, making faithful adherence difficult and imposing large fixed computational costs. With the rise of multimodal agents, policies that govern visual and multimodal behaviors are critical but remain understudied. Prior prompt-compression work mainly shortens task templates and demonstrations, while existing policy-alignment studies focus only on text-based safety rules. We introduce Multimodal Policy Internalization (MPI), a new task that internalizes reasoning-intensive multimodal policies into model parameters, enabling stronger policy-following without including the policy during inference. MPI poses unique data and algorithmic challenges. We build two datasets spanning synthetic and real-world decision-making and tool-using tasks and propose TriMPI, a three-stage training framework. TriMPI first injects policy knowledge via continual pretraining, then performs supervised finetuning, and finally applies PolicyRollout, a GRPO-style reinforcement learning extension that augments rollouts with policy-aware responses for grounded exploration. TriMPI achieves notable gains in end-to-end accuracy, generalization, and robustness to forgetting. As the first work on multimodal policy internalization, we provide datasets, training recipes, and comprehensive evaluations to foster future research. Project page: https://mikewangwzhl.github.io/TriMPI.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.326076",
        "filter_reason": "这篇论文符合你的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心贡献是提出了一种名为“多模态策略内化”的新任务和一套名为“TriMPI”的三阶段训练框架。其本质并非将LLM应用于某个特定领域，而是**提出一种新的训练范式，旨在改进LLM本身的基础能力**。具体来说，它致力于解决LLM在遵循复杂、冗长且“推理密集型”的指令策略时遇到的困难。通过将这些策略知识内化到模型参数中，论文旨在提升模型在无需长提示的情况下的规划、决策和工具使用能力。这完全符合你筛选标准中“改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划、多步推理等通用能力”的要求。 **第二步：正面指标——论文是否包含相关主题？** 论文命中了多个关键的正面指标： - **核心概念**: 论文明确聚焦于LLM，特别是基于LLM的对话代理。 - **能力方向**: 摘要中直接提到处理“reasoning-intensive multimodal policies”，这直接关联到推理、规划和问题解决能力。 - **训练方法**: 论文的核心方法TriMPI包含了“GRPO-style reinforcement learning extension”，这与你关注的“强化学习优化”高度契合。 - **新兴范式**: 研究对象是“conversational agents”，并涉及“tool-using tasks”，属于智能体和工具使用的范畴。 **第三步：排除标准——论文是否主要聚焦于排除领域？** 这里需要仔细辨析。论文标题和摘要中确实提到了“Multimodal”，这触发了排除标准的警报。然而，**这篇论文的重点并非多模态技术本身（如新的视觉编码器、视觉-语言融合机制等），而是将多模态作为其研究的策略所涉及的一个领域**。论文的核心问题是“如何让LLM更好地遵守复杂规则”，而不是“如何让LLM看懂图像”。多模态只是这些复杂规则的一个应用场景，用以证明其提出的方法在更复杂条件下的有效性。因此，它不应被归类于“多模态与视觉”这一排除类别。论文也未聚焦于医疗、化学等特定应用领域或模型基础设施。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出了一种通用的训练框架（TriMPI）来增强智能体遵循策略的能力，这属于“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”，因此应该保留。它并非将智能体应用于特定领域，而是研究智能体本身的能力。 **第五步：最终决策** 综合以上分析，尽管论文标题包含“Multimodal”一词，容易引起误判，但其**核心贡献是方法论层面的创新**——一种通过持续预训练、监督微调和强化学习相结合，来内化复杂策略、提升LLM通用推理和规划能力的新范式。这项工作直接回应了“提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，这篇论文高度相关，应被筛选出来。"
    },
    {
        "index": "#11",
        "title": "Mitigating Overthinking through Reasoning Shaping",
        "link": "/arxiv/2510.09535",
        "arxiv_id": "2510.09535",
        "authors": "Feifan Song, Shaohang Wei, Bofei Gao, Yejie Wang, Wen Luo, Wei Li, Linli Yao, Weimin Xiong, Liang Chen, Tianyu Liu, Houfeng Wang",
        "summary": "Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier Reward (RLVR) have shown great power in problem solving, yet they often cause overthinking: excessive, meandering reasoning that inflates computational cost. Prior designs of penalization in RLVR manage to reduce token consumption while often harming model performance, which arises from the oversimplicity of token-level supervision. In this paper, we argue that the granularity of supervision plays a crucial role in balancing efficiency and accuracy, and propose Group Relative Segment Penalization (GRSP), a step-level method to regularize reasoning. Since preliminary analyses show that reasoning segments are strongly correlated with token consumption and model performance, we design a length-aware weighting mechanism across segment clusters. Extensive experiments demonstrate that GRSP achieves superior token efficiency without heavily compromising accuracy, especially the advantages with harder problems. Moreover, GRSP stabilizes RL training and scales effectively across model sizes.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.324023",
        "filter_reason": "这篇论文完全符合筛选标准，应被保留。我的判断过程如下： **第一步：核心判断** 这篇论文的本质是**改进LLM的基础推理能力**。它没有将LLM作为工具应用到特定领域，而是直接针对LLM在推理过程中出现的一个具体问题——“过度思考”——提出解决方案。其核心贡献是提出了一种名为“分组相对分段惩罚（GRSP）”的新方法，这是一种**新的训练范式/方法论**，用于优化和规范化LLM的推理过程。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 **第二步：正面指标** 论文明确命中了多个正面指标： - **核心概念**: 论文研究对象是“Large reasoning models (LRMs)”，这是大语言模型（LLMs）在推理任务上的具体形态。 - **能力方向**: 论文的核心是“reasoning”（推理），旨在解决推理过程中的“overthinking”问题，以提升推理的效率和准确性。 - **训练方法**: 论文建立在“Reinforcement Learning from Verifier Reward (RLVR)”这一强化学习方法之上，并对其进行了改进，这直接命中了强化学习（RL）这一关键指标。 **第三步：排除标准** 论文没有触及任何排除标准： - 它不涉及多模态、视觉或特定应用领域（如医疗、化学、机器人学）。其提出的方法是通用的，旨在提升模型在各类问题上的推理表现。 - 它也不属于模型基础设施、部署优化或应用层面的可靠性（如水印、安全）研究。 **第四步：处理特殊和模糊情况** 此论文的情况不属于特殊模糊类别，但其内容与“提升模型内在可靠性”的精神相通。它解决“过度思考”问题，这可以看作是提升模型内在逻辑严谨性和推理效率的一种方式，从而提高了模型整体的推理质量和可靠性。这与应用层面的安全讨论有本质区别。 **第五步：最终决策** 综合以上分析，这篇论文的**核心贡献是提出了一种创新的训练方法（GRSP）来优化LLM的推理过程，直接解决了其通用推理能力中的一个关键缺陷（过度思考）**。其研究目标、方法和内容与“提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标高度一致。因此，该论文是极其相关且应被保留的前沿研究。"
    },
    {
        "index": "#25",
        "title": "Token-Level Policy Optimization: Linking Group-Level Rewards to Token-Level Aggregation via Markov Likelihood",
        "link": "/arxiv/2510.09369",
        "arxiv_id": "2510.09369",
        "authors": "Xingyu Lin, Yilin Wen, En Wang, Du Su, Wenbin Liu, Chenfu Bao, Zhonghou Lv",
        "summary": "Group Relative Policy Optimization (GRPO) has significantly advanced the reasoning ability of large language models (LLMs), particularly by boosting their mathematical performance. However, GRPO and related entropy-regularization methods still face challenges rooted in the sparse token rewards inherent to chain-of-thought (CoT). Current approaches often rely on undifferentiated token-level entropy adjustments, which frequently lead to entropy collapse or model collapse. In this work, we propose TEPO, a novel token-level framework that incorporates Markov Likelihood (sequence likelihood) links group-level rewards with tokens via token-level aggregation. Experiments show that TEPO consistently outperforms existing baselines across key metrics (including @k and accuracy). It not only sets a new state of the art on mathematical reasoning tasks but also significantly enhances training stability.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.336196",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断** 论文的核心是提出一种名为TEPO（Token-Level Policy Optimization）的新颖训练框架。它旨在解决现有强化学习方法（如GRPO）在训练LLM进行推理时遇到的“熵崩溃”问题。这直接触及了“改进LLM的基础能力”和“提出新的训练范式”这一核心目标，其目的是从根本上提升模型的推理质量和训练稳定性，而非将其应用于特定领域。因此，根据第一步判断，应予以**保留**。 2.  **第二步：正面指标** 论文明确包含了多个正面指标： *   **核心概念**: 摘要中直接提及“large language models (LLMs)”。 *   **能力方向**: 论文的核心目标是提升“reasoning ability”，特别是“mathematical reasoning”。 *   **训练方法**: 论文属于“reinforcement learning (RL)”的范畴，是对现有RL方法（GRPO）的改进，并建立在“chain-of-thought (CoT)”范式之上。 这些指标都强烈表明该论文与您的研究目标高度相关。 3.  **第三步：排除标准** 该论文不涉及任何排除标准中的内容。它没有讨论多模态、视觉，也没有将模型应用于医疗、化学等特定领域，更不关注模型部署或水印等应用层面的可靠性问题。因此，根据第三步判断，不应被排除。 4.  **第四步：处理特殊和模糊情况** 该论文的研究焦点是训练算法本身的改进，属于模型内在能力的增强，因此不涉及特殊情况的排除范畴。它虽然以数学推理为评估基准，但数学推理通常被视为衡量LLM通用推理能力的核心标准之一，而非一个特定的应用领域。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是一项致力于通过优化强化学习训练过程来增强LLM通用推理能力（特别是数学推理）的基础性、方法论研究。它提出的新方法（TEPO）直接解决了现有技术（GRPO）在提升推理能力时的核心瓶颈，与您“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标完全一致。因此，最终判断为**符合要求**。"
    },
    {
        "index": "#28",
        "title": "Logit Arithmetic Elicits Long Reasoning Capabilities Without Training",
        "link": "/arxiv/2510.09354",
        "arxiv_id": "2510.09354",
        "authors": "Yunxiang Zhang, Muhammad Khalifa, Lechen Zhang, Xin Liu, Ayoung Lee, Xinliang Frederick Zhang, Farima Fatahi Bayat, Lu Wang",
        "summary": "Large reasoning models exhibit long chain-of-thought reasoning with strategies such as backtracking and self-correction, though recent studies suggest that these abilities typically require additional training. We first investigate whether such behaviors can be elicited without any training. To this end, we propose a decoding-time approach, ThinkLogit, which utilizes logit arithmetic to tune a target large non-reasoning model for long reasoning using a substantially smaller reasoning model as the guider. We then show that we can further boost its performance by training the guider model with preference optimization over correct/incorrect reasoning pairs sampled from both the target and guider model, a setup we refer to as ThinkLogit-DPO. Our experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative improvement in average accuracy by 24.5% and 29.1%, respectively, over five reasoning benchmarks using the Qwen2.5-32B guided by R1-Distill-Qwen-1.5B, a model 21x smaller. Moreover, we find that ThinkLogit remains effective when the guider and target come from different model families. It is also orthogonal to post-training methods for small models, as guiders improved through supervised distillation or reinforcement learning can be directly plugged in to yield stronger large models, offering a practical path to unlock long reasoning in large-scale models without costly post-training.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.342868",
        "filter_reason": "这篇论文完全符合你的研究目标，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一种名为“ThinkLogit”的解码时新方法，旨在激发和增强大语言模型（LLM）的长程推理能力。它通过一个小的推理模型来引导一个大的非推理模型，从而让后者无需经过昂贵的额外训练就能具备复杂的推理能力（如回溯和自我修正）。这直接对应了你筛选标准第一步中“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的要求。这篇论文的本质就是提升LLM的通用推理能力，而不是将其应用于特定领域。 2.  **第二步：正面指标** 论文的摘要和标题明确包含了大量正面指标： *   **核心概念**: \"Large reasoning models\", \"Large language models\"。 *   **能力方向**: \"Long Reasoning Capabilities\", \"chain-of-thought reasoning\", \"backtracking and self-correction\", \"reasoning benchmarks\"。 *   **训练方法**: 虽然主方法无需训练，但其增强版\"ThinkLogit-DPO\"使用了\"preference optimization\"，这与强化学习（RL）的理念一致，用于优化模型的推理能力。 3.  **第三步：排除标准** 该论文完全不涉及任何排除标准中的领域。它没有涉及多模态、视觉，没有聚焦于任何特定应用领域（如医疗、化学、机器人），也没有研究模型基础设施或应用层面的安全水印等问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文不属于需要特殊处理的模糊情况。它不是智能体或工具使用研究，而是提出了一种新颖的模型内在推理增强机制。它关注的是提升模型内在的推理质量和准确性，这属于提升模型通用可靠性的范畴，而非应用层面的可靠性讨论。 **最终决策**：该论文提出了一种创新的、非训练范式来提升LLM的通用推理能力，与你的研究课题“提高大语言模型（LLM）本身的『通用推理能力』”高度契合，是典型的、需要保留的前沿研究。其核心贡献——通过解码时模型引导来解锁推理能力——正是该领域最前沿的探索方向之一。"
    },
    {
        "index": "#29",
        "title": "ReTraceQA: Evaluating Reasoning Traces of Small Language Models in Commonsense Question Answering",
        "link": "/arxiv/2510.09351",
        "arxiv_id": "2510.09351",
        "authors": "Francesco Maria Molfese, Luca Moroni, Ciro Porcaro, Simone Conia, Roberto Navigli",
        "summary": "While Small Language Models (SLMs) have demonstrated promising performance on an increasingly wide array of commonsense reasoning benchmarks, current evaluation practices rely almost exclusively on the accuracy of their final answers, neglecting the validity of the reasoning processes that lead to those answers. To address this issue, we introduce ReTraceQA, a novel benchmark that introduces process-level evaluation for commonsense reasoning tasks. Our expert-annotated dataset reveals that in a substantial portion of instances (14-24%), SLMs provide correct final answers despite flawed reasoning processes, suggesting that the capabilities of SLMs are often overestimated by evaluation metrics that focus only on comparing the final answer with the ground truth. Indeed, we show that when employing strong Large Language Models (LLMs) as automated judges for reasoning-aware evaluation rather than answer-only metrics, SLM performance drops significantly across all models and datasets, with scores decreasing by up to 25%.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.343464",
        "filter_reason": "这篇论文符合你的研究范围，应该被保留。判断依据如下： 1.  **第一步（核心判断）**: 论文的核心并非将LLM作为工具应用于特定领域，而是聚焦于LLM（此处是SLM）的**核心能力——推理**的评估方法。它提出了一个名为ReTraceQA的新基准，旨在评估模型的**推理过程**而非仅仅是最终答案。虽然论文没有直接提出一种新的训练方法来“提高”模型能力，但它为“提高”这一目标提供了**至关重要的前提和指导**：一个更精准、更细粒度的评估范式。它揭示了当前模型在推理过程中存在的“正确答案，错误过程”的问题，这为后续研究如何真正优化模型的推理链条指明了方向。这种对推理过程本身质量的关注，完全符合你筛选“改进LLM基础能力”相关论文的目标。 2.  **第二步（正面指标）**: 论文高度契合多个正面指标。核心概念明确是“Small Language Models (SLMs)”和“Large Language Models (LLMs)”。能力方向直接指向“reasoning”（推理），特别是“commonsense reasoning”（常识推理）。这满足了筛选标准中最核心的关键词。 3.  **第三步（排除标准）**: 论文的研究内容完全不涉及多模态、视觉、医疗、化学等特定应用领域，也不关注水印、安全等模型可靠性问题。因此，它未被任何排除标准命中。 4.  **第四步（特殊和模糊情况）**: 这篇论文可以被视为对**模型可解释性**和**推理质量**的一种方法论研究。它提出的“过程级评估”是一种新的评估框架，旨在增强我们对模型内在推理过程的理解和判断。这完全符合“如果论文提出一种新方法来……增强模型内在的可解释性……从而提升模型的通用可靠性和推理质量，应该保留”的规则。通过揭示推理缺陷，它直接服务于提升模型通用推理质量的最终目标。 **最终决策**: 综合来看，这篇论文虽然主题是“评估”而非直接“提升”，但它对LLM通用推理能力的研究具有基础性的推动作用。它提供了一个更强大的“显微镜”来观察模型的推理缺陷，这对于任何致力于改进LLM推理能力的研究者来说都是不可或缺的。因此，它完全符合你关于“大语言模型通用推理能力”的研究课题。"
    },
    {
        "index": "#32",
        "title": "Verifying Chain-of-Thought Reasoning via Its Computational Graph",
        "link": "/arxiv/2510.09312",
        "arxiv_id": "2510.09312",
        "authors": "Zheng Zhao, Yeskendir Koishekenov, Xianjun Yang, Naila Murray, Nicola Cancedda",
        "summary": "Current Chain-of-Thought (CoT) verification methods predict reasoning correctness based on outputs (black-box) or activations (gray-box), but offer limited insight into why a computation fails. We introduce a white-box method: Circuit-based Reasoning Verification (CRV). We hypothesize that attribution graphs of correct CoT steps, viewed as execution traces of the model's latent reasoning circuits, possess distinct structural fingerprints from those of incorrect steps. By training a classifier on structural features of these graphs, we show that these traces contain a powerful signal of reasoning errors. Our white-box approach yields novel scientific insights unattainable by other methods. (1) We demonstrate that structural signatures of error are highly predictive, establishing the viability of verifying reasoning directly via its computational graph. (2) We find these signatures to be highly domain-specific, revealing that failures in different reasoning tasks manifest as distinct computational patterns. (3) We provide evidence that these signatures are not merely correlational; by using our analysis to guide targeted interventions on individual transcoder features, we successfully correct the model's faulty reasoning. Our work shows that, by scrutinizing a model's computational process, we can move from simple error detection to a deeper, causal understanding of LLM reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.345028",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出一种全新的方法论（Circuit-based Reasoning Verification, CRV）来理解和改进LLM的推理过程。它没有将LLM应用于某个特定领域，而是深入探究LLM在进行思维链（CoT）推理时的内部计算机制。论文的核心贡献不仅仅是“验证”推理的正确性，更重要的是，它通过分析计算图的结构特征，揭示了推理错误的因果机制，并最终**成功地利用这些分析来修正模型的错误推理**。这直接对应了筛选标准中“改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力”的目标。它是一种旨在提升模型内在推理质量的方法论研究。 **第二步：正面指标——论文是否包含相关主题？** 论文高度符合多个正面指标： - **核心概念**: 论文的研究对象是LLM的Chain-of-Thought推理。 - **能力方向**: 论文的核心是“reasoning”，并明确涉及“math reasoning”和“logical reasoning”。 - **新兴范式**: 论文是对“思维链”这一重要范式的深化和拓展，提供了更深层次的白盒分析视角。 **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文完全不涉及任何排除标准领域： - 它不涉及多模态、视觉或特定应用领域（如医疗、化学）。 - 它的研究焦点不是模型的基础设施、部署优化或水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况——可解释性与推理质量** 这篇论文是“可解释性”研究如何服务于“提升推理能力”的绝佳范例。根据筛选标准，如果论文提出一种新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量，就应该保留。本论文恰恰做到了这一点： 1.  **增强内在可解释性**: 它通过分析模型的“计算图”和“推理电路”，提供了一种白盒的、深入模型内部的可解释性方法。 2.  **提升推理质量**: 最关键的是，这种可解释性不是终点。论文明确指出，他们利用分析结果“成功地修正了模型的错误推理”，实现了从错误检测到因果理解和能力提升的跨越。这完全符合“提升模型的通用可靠性和推理质量”的要求。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种能够诊断并修正LLM推理错误的白盒方法。它直接致力于提升LLM的通用推理能力，是方法论上的创新，而非特定领域的应用。因此，这篇论文是关于“大语言模型通用推理能力”研究的理想筛选对象。"
    },
    {
        "index": "#37",
        "title": "CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts",
        "link": "/arxiv/2510.09278",
        "arxiv_id": "2510.09278",
        "authors": "Jiuheng Lin, Cong Jiang, Zirui Wu, Jiarui Sun, Yansong Feng",
        "summary": "Training expert LLMs in domains with scarce data is difficult, often relying on multiple-choice questions (MCQs). However, standard outcome-based reinforcement learning (RL) on MCQs is risky. While it may improve accuracy, we observe it often degrades reasoning quality such as logical consistency. Existing solutions to supervise reasoning, such as large-scale Process Reward Models (PRMs), are prohibitively expensive. To address this, we propose CLARity, a cost-effective RL framework that enhances reasoning quality using only a small, general-purpose LLM. CLARity integrates a consistency-aware reward mechanism with a 2-stage refine-then-monitor training pipeline to enhance reasoning consistency, and a dynamic data reformulation strategy to to better exploit limited data. Experiments demonstrate that CLARity improves response consistency by 16.5% and accuracy by 7.5% over baselines. Human evaluations further confirm holistic improvements in coherence and professionalism. Thus, CLARity offers a generalizable solution that enables smaller models to effectively guide expert models by reasoning consistency.Our code is open sourced at: https://github.com/Infinite-set/CLARity",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.347351",
        "filter_reason": "这篇论文完全符合你的研究范围。 **判断过程如下:** 1.  **第一步：核心判断** 论文的核心是提出一种名为 **CLARity 的新强化学习（RL）训练框架**。其目标是解决传统基于结果的RL在训练专家模型时会**损害推理质量**的问题。论文的本质并非将LLM应用于某个特定领域，而是提出一种**通用的方法论**来提升LLM的**推理一致性**，并进而提升其推理质量和准确性。这完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、多步推理等通用能力”的要求。 2.  **第二步：正面指标** 论文摘要中包含了大量高度相关的正面指标： *   **核心概念**: \"expert LLMs\", \"smaller models\" *   **能力方向**: \"reasoning quality\", \"logical consistency\", \"reasoning consistency\", \"coherence\" *   **训练方法**: \"reinforcement learning (RL)\", \"outcome-based reinforcement learning\", \"training pipeline\" 这些关键词密集地出现在摘要中，清晰地表明论文聚焦于LLM的推理能力和训练方法。 3.  **第三步：排除标准** 论文没有被任何排除标准命中。 *   它不涉及多模态或视觉。 *   虽然提到了“domains with scarce data”和“expert LLMs”，但这只是为了引出问题背景（在数据稀缺领域训练模型困难）。论文的**核心贡献是CLARity这个通用的训练框架**，而不是在某个特定领域（如医疗、化学）的应用。摘要最后明确指出这是一个“generalizable solution”，证明了其通用性。 *   它关注的是通过改进训练过程来提升模型内在的推理一致性，而非应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 本论文的情况与“幻觉/可解释性/安全”的模糊情况处理规则高度一致。论文旨在通过“reasoning consistency”来提升模型质量，这本质上是一种减少逻辑错误、提升推理可靠性的方法。因为它提出的是一种**新的训练范式**来**内在地**增强模型的推理能力，而不是做表面应用或社会学研究，因此**应该保留**。 **最终决策:** 综合以上分析，这篇论文的核心贡献是提出了一种新颖的、成本效益高的强化学习框架（CLARity），通过提升推理一致性来直接增强大语言模型的通用推理质量。它完全符合你筛选“致力于提高大语言模型（LLM）本身的『通用推理能力』”的前沿论文的目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#40",
        "title": "Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models",
        "link": "/arxiv/2510.09259",
        "arxiv_id": "2510.09259",
        "authors": "Yongding Tao, Tian Wang, Yihong Dong, Huanyu Liu, Kechi Zhang, Xiaolong Hu, Ge Li",
        "summary": "Data contamination poses a significant threat to the reliable evaluation of Large Language Models (LLMs). This issue arises when benchmark samples may inadvertently appear in training sets, compromising the validity of reported performance. While detection methods have been developed for the pre-training and Supervised Fine-Tuning stages, a critical research gap exists for the increasingly significant phase of Reinforcement Learning (RL) post-training. As RL post-training becomes pivotal for advancing LLM reasoning, the absence of specialized contamination detection methods in this paradigm presents a critical vulnerability. To address this, we conduct the first systematic study of data detection within RL post-training scenario and propose Self-Critique. Our method is motivated by a key observation: after RL phase, the output entropy distribution of LLMs tends to collapse into highly specific and sparse modes. Self-Critique probes for the underlying policy collapse, i.e., the model's convergence to a narrow reasoning path, which causes this entropy reduction. To facilitate this research, we also introduce RL-MIA, a benchmark constructed to simulate this specific contamination scenario. Extensive experiments show that Self-Critique significantly outperforms baseline methods across multiple models and contamination tasks, achieving an AUC improvement of up to 30%. Whereas existing methods are close to a random guess for RL-phase contamination, our method makes detection possible.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.359369",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心并非直接提出一种新的训练范式来提升LLM的推理能力，而是提出了一种**评估方法**（Self-Critique）来检测在强化学习（RL）后训练阶段的数据污染。初看之下，这似乎不属于“提高能力”的范畴。然而，论文的出发点是：RL后训练是“推进LLM推理”的关键阶段，而数据污染会严重威胁这一阶段性能评估的**可靠性**。因此，这篇论文的本质是**为提升LLM推理能力的关键技术（RL训练）提供可靠的评估保障**。一个无法被准确衡量的能力提升是无效的。从这个角度看，该论文是整个“提升LLM推理能力”研究生态中不可或缺的一环，它确保了该领域研究的科学性和有效性。 2.  **第二步：正面指标** 论文高度符合多个正面指标： *   **核心概念**: 论文明确围绕“Large Language Models (LLMs)”展开。 *   **能力方向**: 论文摘要开篇即点明，RL后训练对于“advancing LLM reasoning”至关重要，其研究目标直接服务于推理能力的评估。 *   **训练方法**: 论文的核心研究对象就是“Reinforcement Learning (RL) post-training”，这是当前提升LLM推理能力的主流方法之一。 3.  **第三步：排除标准** 论文不涉及任何排除标准中的领域： *   它不涉及多模态、视觉。 *   它不针对任何特定应用领域（如医疗、化学），而是提出了一种通用的检测方法。 *   它虽然涉及“可靠性”，但并非应用层面的水印、安全或安保问题，而是**评估方法论层面的可靠性**，这与排除标准中的“模型可靠性（应用层面）”有本质区别。 4.  **第四步：处理特殊和模糊情况** 这篇论文的情况可以与“幻觉/可解释性/安全”的特殊情况类比。正如筛选标准所述：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” 本文提出的Self-Critique方法，通过检测数据污染，**防止了因模型“记忆”而非“推理”而导致的虚假性能提升**。这直接提升了我们对模型“推理质量”评估的信心，确保了其通用可靠性。因此，尽管它不直接训练模型，但它通过提升评估的保真度，为整个研究目标提供了关键支持，其作用与提升推理质量的新方法是同构的。 **最终决策**: 综合以上分析，这篇论文虽然是一篇“元研究”论文，但它精确地切中了当前提升LLM推理能力（特别是通过RL方法）的核心痛点——评估的可靠性。它的贡献（Self-Critique方法）为整个研究社区提供了一个强大的工具，用以甄别真正的推理进步和虚假的数据记忆效果。这对于准确衡量和推动LLM通用推理能力的发展具有基础性和先导性的重要意义。因此，它完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标的内涵。"
    },
    {
        "index": "#41",
        "title": "DSPO: Stable and Efficient Policy Optimization for Agentic Search and Reasoning",
        "link": "/arxiv/2510.09255",
        "arxiv_id": "2510.09255",
        "authors": "Chenyang Gu, Yewen Pu, Bruce Yang, Xiaofan Li, Huan Gao",
        "summary": "Enhancing LLMs with the ability to actively search external knowledge is crucial for complex and real-world tasks. Current approaches either rely on prompting to elicit the model's innate agent capabilities, or suffer from performance ceilings and collapse when applying RL to complex interactive tasks, leaving their true agentic potential untapped. To address this, we introduce \\textbf{D}ynamic-filter \\textbf{S}equence-level \\textbf{P}olicy \\textbf{O}ptimization (DSPO), an improved RL algorithm designed for robust agent training through sequence-level optimization and dynamic sample filtering. We train our model purely through RL to interleave multi-turn search and reasoning, obviating the need for supervised demonstration data. Across multiple QA benchmarks, our DSPO-trained 7B model improves over a comparable previous work by \\textbf{34.1\\%}, and even outperforms the 14B model from previous work in complex multihop QA such as HotpotQA by nearly \\textbf{9\\% relative}, maintaining exceptional training stability.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.359835",
        "filter_reason": "这篇论文完全符合你的研究范围，核心判断依据如下： 1.  **第一步：核心判断 (保留)** 论文的本质是提出一种新的训练范式（DSPO，一种改进的强化学习算法），其核心目标是提升大语言模型作为智能体的通用推理与问题解决能力。摘要中明确指出，该算法用于“robust agent training through sequence-level optimization”，并训练模型“interleave multi-turn search and reasoning”。这直接触及了LLM在规划、多步推理和工具使用等方面的基础能力提升，而非将其应用于特定领域。因此，这篇论文的核心贡献是方法论层面的创新，旨在增强LLM本身的通用推理能力。 2.  **第二步：正面指标 (高度匹配)** 论文摘要中包含了大量的正面指标关键词，进一步印证了其相关性： - **核心概念**: \"Enhancing LLMs\" - **能力方向**: \"agentic search and reasoning\", \"complex interactive tasks\", \"complex multihop QA\"，这些都直接指向了`reasoning`和`problem-solving`。 - **训练方法**: \"reinforcement learning (RL)\", \"policy optimization\"，表明其核心是基于`RL`的训练方法论。 - **新兴范式**: \"Agentic Search\", \"interleave multi-turn search and reasoning\"，这完全符合`llm-based agents`和`tool use`的范式，并且是一种通用的框架。 3.  **第三步：排除标准 (不匹配)** 论文的研究焦点完全避开了所有排除标准。它不涉及多模态、视觉；没有限定在医疗、化学等特定应用领域；其评估基准是通用的QA数据集而非特定领域任务；研究内容也不是水印、安全等模型可靠性应用层面的问题。 4.  **第四步：特殊和模糊情况 (确认保留)** 论文提出的“智能体搜索与推理”框架是一种通用的能力提升方法。它通过强化学习训练模型自主进行多轮搜索和推理，这是一种增强LLM通用问题解决能力的内在机制，而非针对特定领域的应用。因此，符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的保留条件。 **最终决策**: 综上所述，该论文的核心贡献是提出一种新的、更稳定高效的强化学习算法（DSPO），专门用于训练LLM执行复杂的多轮搜索和推理任务，从而提升其作为智能体的通用能力。这与你的核心目标——“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”——高度一致，是一篇非常相关的、应该被保留的前沿研究论文。"
    },
    {
        "index": "#44",
        "title": "DICE: Structured Reasoning in LLMs through SLM-Guided Chain-of-Thought Correction",
        "link": "/arxiv/2510.09211",
        "arxiv_id": "2510.09211",
        "authors": "Yiqi Li, Yusheng Liao, Zhe Chen, Yanfeng Wang, Yu Wang",
        "summary": "When performing reasoning tasks with user-specific requirements, such as strict output formats, large language models (LLMs) often prioritize reasoning over adherence to detailed instructions. Fine-tuning LLMs on supervised datasets to address this is impractical due to high computational costs and limited parameter access. To tackle this, we propose DICE, a lightweight framework that guides small language models (SLMs) to refine LLMs' outputs through chain-of-thought (CoT) correction. DICE decouples the process by first prompting LLMs to generate natural language responses, then using trained SLMs to analyze and refine these outputs to meet structured output specifications. This framework preserves LLMs' broad knowledge and reasoning capabilities while ensuring the outputs conform to user demands. Specifically, DICE first constructs structured CoT adaptation datasets via a two-stage method and subsequently applies a dual-tuning strategy to fine-tune SLMs for generating structured outputs in an analyze-then-answer pattern. Experiments demonstrate that DICE improves the average format accuracy and content correctness of LLM outputs by 35.4\\% and 29.4\\%, respectively, achieving state-of-the-art (SOTA) performance over other competitive baselines.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.361233",
        "filter_reason": "这篇论文完全符合您的筛选标准，其核心目标在于提升大语言模型的通用推理能力。 **第一步：核心判断——这篇论文的本质是什么？** - **保留**。这篇论文的本质是提出一种新的方法来改进LLM的推理输出质量。它没有将LLM应用于任何特定领域（如生物、金融），而是聚焦于LLM在执行通用推理任务（特别是思维链CoT）时遇到的一个普遍问题：为了追求推理的正确性而忽略了格式等结构性要求。论文提出的DICE框架，通过引入一个SLM（小语言模型）作为“修正器”，来优化和结构化LLM的推理过程和最终输出。这属于“提出新的训练范式、增强其逻辑、多步推理等通用能力”的范畴，是一种方法论层面的创新。 **第二步：正面指标——论文是否包含以下主题？** - **高度相关**。论文明确包含了以下正面指标： - **核心概念**: Large language models (LLMs), Small language models (SLMs)。 - **能力方向**: 论文标题和摘要中反复强调 \"Structured Reasoning\" 和 \"Chain-of-Thought (CoT)\"，这正是通用推理能力的核心。其目标是提升推理输出的 \"format accuracy\" 和 \"content correctness\"。 - **训练方法**: 论文提出了 \"dual-tuning strategy\" 来微调SLM，这是一种新颖的训练范式。 - **新兴范式**: DICE框架本身可以看作是一种LLM-SLM协作的“工具使用”或“智能体”框架的雏形，其中SLM被用作一个专门化的工具，来增强LLM的通用问题解决能力。 **第三步：排除标准——论文是否主要聚焦于以下领域？** - **不排除**。论文完全没有涉及多模态、特定应用领域（医疗、化学等），也未聚焦于水印、安全等应用层面的可靠性问题。其研究对象是通用的文本推理任务。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出的DICE框架完美符合保留条件。它提出了一种**通用的**SLM引导的CoT修正方法，旨在提升LLM在**任何**需要结构化输出的推理任务上的表现，而不是限定在某个特定领域。这里的SLM扮演了一个通用的“推理输出修正工具”角色。 - **幻觉/可解释性/安全**: 论文中提到的提升 \"content correctness\"（内容正确性）与减少“幻觉”直接相关。它提出了一种具体的技术方案（DICE框架）来达成这一目标，从而提升了模型推理的内在可靠性，这符合保留标准。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是DICE框架，它通过一种新颖的LLM-SLM协作与微调范式，直接解决了LLM在通用推理过程中的一类关键问题（结构化与正确性的平衡）。该方法具有通用性，旨在增强LLM基础的、跨领域的推理能力，而非局限于某个应用场景。因此，该论文与您的研究课题“大语言模型通用推理能力”高度契合，应予以保留。"
    },
    {
        "index": "#49",
        "title": "When Retrieval Succeeds and Fails: Rethinking Retrieval-Augmented Generation for LLMs",
        "link": "/arxiv/2510.09106",
        "arxiv_id": "2510.09106",
        "authors": "Yongjie Wang, Yue Yu, Kaisong Song, Jun Lin, Zhiqi Shen",
        "summary": "Large Language Models (LLMs) have enabled a wide range of applications through their powerful capabilities in language understanding and generation. However, as LLMs are trained on static corpora, they face difficulties in addressing rapidly evolving information or domain-specific queries. Retrieval-Augmented Generation (RAG) was developed to overcome this limitation by integrating LLMs with external retrieval mechanisms, allowing them to access up-to-date and contextually relevant knowledge. However, as LLMs themselves continue to advance in scale and capability, the relative advantages of traditional RAG frameworks have become less pronounced and necessary. Here, we present a comprehensive review of RAG, beginning with its overarching objectives and core components. We then analyze the key challenges within RAG, highlighting critical weakness that may limit its effectiveness. Finally, we showcase applications where LLMs alone perform inadequately, but where RAG, when combined with LLMs, can substantially enhance their effectiveness. We hope this work will encourage researchers to reconsider the role of RAG and inspire the development of next-generation RAG systems.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.368788",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断——这篇论文的本质是什么？** 论文的核心是**分析和重新审视“检索增强生成（RAG）”这一方法论**。RAG本身是一种旨在提升大语言模型能力的技术框架，它通过引入外部知识库，来弥补LLM固有知识的局限性和时效性不足。这并非将LLM直接应用于某个特定垂直领域（如生物、金融），而是**致力于改进LLM获取和处理信息的基础能力**。一个能够准确、高效地检索并利用外部知识的LLM，其在解决开放性、时效性问题时的推理能力自然会得到增强。因此，论文本质上是关于**改进LLM通用能力的一种核心方法论**的研究，符合第一步的保留标准。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文明确包含以下正面指标： *   **核心概念**: 论文标题和摘要中反复提及 \"Large Language Models (LLMs)\"。 *   **能力方向**: RAG的根本目的就是为了提升LLM在需要外部知识的**问题解决** 上的表现。虽然摘要未直接使用\"reasoning\"一词，但通过提供准确的知识背景，RAG直接服务于更高质量的推理输出，避免因知识陈旧或缺失导致的推理失败。 *   **新兴范式**: RAG是当前LLM领域最核心的**工具使用** 和**深度研究** 范式之一。论文旨在“rethink”并“inspire the development of next-generation RAG systems”，这属于对该范式的深化和创新。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 论文不触及任何排除标准所列出的主要领域。它不是关于多模态、医疗、化学、机器人或模型基础设施（如硬件部署）的研究。摘要中虽然提到“domain-specific queries”，但这只是为了说明RAG可以解决的问题类型，论文的焦点是RAG框架本身，而非某个特定领域。 4.  **第四步：处理特殊和模糊情况——智能体/工具使用** 这篇论文完美地契合了“智能体/工具使用”的保留规则。RAG就是一种LLM使用“检索”这一工具来增强自身能力的通用框架。这篇论文并非“用于化学实验自动化的智能体”，而是对“用于增强通用问题解决能力的工具（检索）”进行系统性的回顾、反思和展望，其目标是提升LLM的**通用性**而非**领域专用性**。通过解决LLM因知识不足而无法有效推理的问题，RAG直接提升了其推理的可靠性和质量，这与第四条中“提升模型的通用可靠性和推理质量”的保留原则一致。 5.  **第五步：最终决策** 综合以上分析，这篇论文虽然是一篇综述性文章，但其核心贡献在于**系统性地剖析和重新定义RAG这一提升LLM通用能力的关键技术**。它探讨的不是RAG在某个狭窄场景的应用，而是RAG作为一种通用方法论的价值、挑战和未来方向。这与“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，这篇论文是研究前沿中非常重要的一环，应该被筛选出来。"
    },
    {
        "index": "#51",
        "title": "ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability",
        "link": "/arxiv/2510.09062",
        "arxiv_id": "2510.09062",
        "authors": "Chung-En Sun, Ge Yan, Akshay Kulkarni, Tsui-Wei Weng",
        "summary": "Recent advances in long chain-of-thought (CoT) reasoning have largely prioritized answer accuracy and token efficiency, while overlooking aspects critical to trustworthiness. We argue that usable reasoning systems must be trustworthy, characterized by three properties: interpretability, faithfulness, and reliability. To this end, we propose ReFIne, a new training framework that integrates supervised fine-tuning with GRPO to encourage models to: (i) improve interpretability by producing structured, tag-based traces with high-level planning that are easier for humans to follow; (ii) enhance faithfulness by explicitly disclosing the decisive information guiding each solution, with consistent cross-section references; and (iii) promote reliability by providing self-assessments of both the derivation's soundness and the confidence of the final answer. We apply ReFIne to the Qwen3 models at multiple scales (1.7B/4B/8B) and evaluate across mathematical benchmarks of varying difficulty. Our experimental results show that ReFIne models generate clearer and better-structured reasoning traces (interpretability +44.0%), more faithfully expose their underlying decision process (faithfulness +18.8%), and offer informative confidence estimates (reliability +42.4%). These findings highlight an overlooked but important direction: reasoning models should be optimized not only for accuracy, but also for broader dimensions of trustworthiness. Our code is available at: https://github.com/Trustworthy-ML-Lab/Training_Trustworthy_LRM_with_Refine",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.369687",
        "filter_reason": "这篇论文完全符合您的筛选标准，应被保留。以下是基于您提供的五步筛选标准的详细判断过程： **第一步：核心判断——这篇论文的本质是什么？** - 论文的核心贡献是提出了一个名为“ReFIne”的**新训练框架**。这个框架并非将LLM作为工具应用于特定领域，而是直接作用于LLM本身，旨在改进其推理过程。 - 它通过结合监督微调（SFT）和一种强化学习方法（GRPO），来优化模型在生成推理轨迹时的三个关键属性：可解释性、忠实性和可靠性。 - 这本质上是在**增强LLM的基础推理能力**，使其推理过程不仅是正确的，而且是清晰、可信、可靠的。这与您筛选标准中“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的目标高度一致。 **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文明确研究对象是“Large Reasoning Models”（大型推理模型），并在Qwen3系列模型上进行实验，完全符合。 - **能力方向**: 论文的核心是“reasoning”，特别是“long chain-of-thought (CoT) reasoning”，并在“mathematical benchmarks”上进行评估。这直接命中了“reasoning”和“math reasoning”等关键词。 - **训练方法**: 论文明确提出了一个结合“supervised fine-tuning”与“GRPO”（一种强化学习方法）的新训练范式，命中了“reinforcement learning (RL)”。 **第三步：排除标准——论文是否主要聚焦于以下领域？** - **多模态与视觉**: 论文完全不涉及视觉或多模态内容，排除此项。 - **特定应用领域**: 虽然论文在数学基准上测试，但数学能力通常被视为衡量通用推理能力的标准，而非一个“特定应用领域”（如医疗、化学）。ReFIne框架本身是通用的，旨在提升推理过程的质量，而非解决某个特定领域的问题。 - **模型可靠性（应用层面）**: 这是最需要辨析的一点。论文的标题和摘要中提到了“Reliability”和“Trustworthy”。然而，它讨论的并非应用层面的水印、安全或安保。相反，它将“可靠性”定义为模型对其推理过程**自我评估**的能力，即评估推导过程的合理性和最终答案的置信度。这是一种内嵌于推理过程中的**元认知能力**，旨在提升推理质量本身，而不是一个外部的安全防护措施。因此，这不属于排除标准中的“模型可靠性（应用层面）”。 **第四步：处理特殊和模糊情况** - **幻觉/可解释性/安全**: 这篇论文是第四步规则的一个完美例证。它提出了一种**新方法（ReFIne框架）**来**增强模型内在的可解释性**（通过结构化的推理轨迹）和**可靠性**（通过自我评估），其直接目的就是**提升模型的通用推理质量**。这完全符合“如果论文提出一种新方法来...增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留”的规则。 **第五步：最终决策** 综合以上分析，该论文的核心是提出一种新的训练范式，通过增强LLM推理过程的可解释性、忠实性和可靠性，来**从根本上提升其通用推理能力的质量**。它并非特定领域应用，也非基础设施或应用层安全研究。其研究方向（提升推理过程的内在质量和可信度）是对“大语言模型通用推理能力”这一课题的深化和补充，属于前沿且高度相关的研究。 因此，最终判断为：**True**。"
    },
    {
        "index": "#53",
        "title": "Large Language Models Do NOT Really Know What They Don't Know",
        "link": "/arxiv/2510.09033",
        "arxiv_id": "2510.09033",
        "authors": "Chi Seng Cheang, Hou Pong Chan, Wenxuan Zhang, Yang Deng",
        "summary": "Recent work suggests that large language models (LLMs) encode factuality signals in their internal representations, such as hidden states, attention weights, or token probabilities, implying that LLMs may \"know what they don't know\". However, LLMs can also produce factual errors by relying on shortcuts or spurious associations. These error are driven by the same training objective that encourage correct predictions, raising the question of whether internal computations can reliably distinguish between factual and hallucinated outputs. In this work, we conduct a mechanistic analysis of how LLMs internally process factual queries by comparing two types of hallucinations based on their reliance on subject information. We find that when hallucinations are associated with subject knowledge, LLMs employ the same internal recall process as for correct responses, leading to overlapping and indistinguishable hidden-state geometries. In contrast, hallucinations detached from subject knowledge produce distinct, clustered representations that make them detectable. These findings reveal a fundamental limitation: LLMs do not encode truthfulness in their internal states but only patterns of knowledge recall, demonstrating that \"LLMs don't really know what they don't know\".",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.370632",
        "filter_reason": "这篇论文符合筛选标准，应该保留。判断依据如下： 1.  **核心判断（第一步）：** 这篇论文的本质不是将LLM作为工具应用于特定领域，也不是关于模型基础设施。它的核心是对LLM进行一次深入的、基础性的**机制性分析**。论文的核心贡献在于揭示了LLM在处理事实性信息时一个根本性的内部运作机制：它并不真正“知道”自己不知道什么，因为其正确回答和部分错误回答在内部表示上是无法区分的。这项工作直接触及了LLM认知能力的核心，即它如何“回忆”和“生成”信息，这是所有通用推理能力的基础。 2.  **与核心目标的关联：** 我的核心目标是“提高LLM的通用推理能力”。虽然这篇论文没有直接提出一种新的训练方法来“提高”能力，但它**精确地诊断了一个关键的根本性缺陷**。一个无法区分自身知识与幻觉的模型，其推理能力的上限是极低的，可靠的自我纠错、规划等高级能力也就无从谈起。因此，这篇论文的研究成果为未来如何“提高”推理能力指明了方向——即必须设法让模型在内部建立起可区分的、表征“真实性”的机制。对于一位顶尖研究员来说，这种基础性的诊断研究与提出新方法具有同等甚至更高的价值。 3.  **正面指标（第二步）：** 论文的核心概念是LLMs，其研究方向虽然是关于事实性，但其分析方法（隐藏状态几何）和结论（关于模型内在可靠性）直接关联到**reasoning**的质量和**problem-solving**的可靠性。它深化了我们对LLM内在认知过程的**可解释性**理解。 4.  **特殊与模糊情况处理（第四步）：** 这篇论文完美地契合了“幻觉/可解释性”这一特殊情况。它并非应用层面的讨论，而是通过深入的内部机制分析，揭示了幻觉产生的根本原因。这项发现极大地**增强了模型的内在可解释性**，从而为未来提升模型的**通用可靠性和推理质量**奠定了理论基础。它解释了为什么简单的“置信度”或内部信号不足以解决幻觉问题，这对于设计更有效的自我纠错或推理增强框架至关重要。 综上所述，这篇论文虽然没有直接“提升”模型的能力，但它通过深入的机制性研究，揭示了限制LLM通用推理能力的一个核心瓶颈。这种对模型内在工作原理的深刻洞见，是推动该领域向前发展的关键基石，完全符合我为“大语言模型通用推理能力”这一前沿课题筛选论文的要求。"
    },
    {
        "index": "#58",
        "title": "DARO: Difficulty-Aware Reweighting Policy Optimization",
        "link": "/arxiv/2510.09001",
        "arxiv_id": "2510.09001",
        "authors": "Jingyu Zhou, Lu Ma, Hao Liang, Chengyu Shen, Bin Cui, Wentao Zhang",
        "summary": "Recent advances in large language models (LLMs) have shown that reasoning ability can be significantly enhanced through Reinforcement Learning with Verifiable Rewards (RLVR). Group Relative Policy Optimization (GRPO) has emerged as the de facto approach for RLVR, inspiring numerous variants. However, our mathematical analysis reveals that these methods are fundamentally weighted variations of GRPO. We provide a unified view, demonstrating that their reliance on static or overly simplistic weighting schemes tied to sample difficulty prevents adaptation to a model's evolving capabilities. This creates a significant loss scale issue, where training disproportionately focuses on certain difficulty levels at the expense of others, hindering overall performance. To address these limitations, we introduce \\textbf{Difficulty-Aware Reweighting Policy Optimization (DARO)}, a method that dynamically adjusts the loss contribution of each difficulty group based on the model's learning state. Extensive experiments on Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, and Llama3.1-8B show that DARO outperforms four leading baselines across six math benchmarks, achieving significantly faster convergence and superior final performance.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.372938",
        "filter_reason": "这篇论文完全符合我的研究范围，应予以保留。以下是我的详细判断过程： 1.  **第一步：核心判断** - **核心贡献**: 该论文的核心是提出了一种名为“DARO”的新方法，这是一种“Difficulty-Aware Reweighting Policy Optimization”（难度感知的重新加权策略优化）方法。 - **本质分析**: 论文的本质是改进大语言模型的**训练范式**。它针对当前通过强化学习（特别是“Reinforcement Learning with Verifiable Rewards”, RLVR）来提升模型推理能力的方法（如GRPO）进行了深入分析，指出了其静态加权方案的局限性，并提出了一种动态调整不同难度样本损失贡献的新策略。 - **与目标匹配度**: 这直接命中了我的核心目标——“致力于提高大语言模型（LLM）本身的『通用推理能力』”。论文不是在应用LLM，而是在研究如何**更好地训练LLM**，使其在推理任务上表现更佳。它解决的是训练过程中的一个根本性问题（损失规模问题），从而提升模型的通用能力。 2.  **第二步：正面指标** - **核心概念**: 论文明确以“large language models (LLMs)”为研究对象。 - **能力方向**: 论文的核心目标是提升“reasoning ability”，并且实验部分聚焦于“math reasoning”，这是通用推理能力的关键组成部分。 - **训练方法**: 论文的核心内容是关于“Reinforcement Learning”和“Policy Optimization”，完全符合筛选标准。 - **新兴范式**: 虽然没有直接提及Agent或Tool，但通过强化学习优化推理能力是构建高级LLM智能体的基础技术，其重要性不言而喻。 3.  **第三步：排除标准** - **多模态与视觉**: 论文全文未涉及视觉、多模态等内容，排除此项。 - **特定应用领域**: 论文虽然在数学基准上测试，但其目的不是解决某个特定的数学应用问题（如解方程），而是将数学推理作为一个**公认的、可验证的测试平台**，来衡量其提出的通用训练方法的有效性。这是一种方法论研究，而非应用研究，因此不排除。 - **模型可靠性（应用层面）**: 论文关注的是模型性能优化，而非水印、安全等应用层面的可靠性问题，排除此项。 4.  **第四步：处理特殊和模糊情况** - 本论文情况清晰，不属于需要特殊处理的模糊范畴。它是一个纯粹的、关于LLM核心推理能力训练方法的研究。 5.  **第五步：最终决策** - **综合分析**: 该论文提出了一种创新的训练算法（DARO），通过动态调整训练样本的权重，解决了现有强化学习训练方法在提升LLM推理能力时的一个关键瓶颈。其研究焦点是**方法论创新**，旨在增强LLM的**内在通用推理能力**，并且通过数学推理这一核心领域进行了验证。这与我筛选“关于大语言模型通用推理能力的前沿论文”的核心目标高度一致，甚至可以说是非常典型的目标论文。因此，最终决策是**保留**。"
    },
    {
        "index": "#59",
        "title": "MASA: LLM-Driven Multi-Agent Systems for Autoformalization",
        "link": "/arxiv/2510.08988",
        "arxiv_id": "2510.08988",
        "authors": "Lan Zhang, Marco Valentino, André Freitas",
        "summary": "Autoformalization serves a crucial role in connecting natural language and formal reasoning. This paper presents MASA, a novel framework for building multi-agent systems for autoformalization driven by Large Language Models (LLMs). MASA leverages collaborative agents to convert natural language statements into their formal representations. The architecture of MASA is designed with a strong emphasis on modularity, flexibility, and extensibility, allowing seamless integration of new agents and tools to adapt to a fast-evolving field. We showcase the effectiveness of MASA through use cases on real-world mathematical definitions and experiments on formal mathematics datasets. This work highlights the potential of multi-agent systems powered by the interaction of LLMs and theorem provers in enhancing the efficiency and reliability of autoformalization, providing valuable insights and support for researchers and practitioners in the field.",
        "subjects": "Computation and Language, Formal Languages and Automata Theory",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.378517",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。以下是我的详细判断过程： 1.  **第一步：核心判断——论文的本质是什么？** - **核心贡献**: 该论文提出了一个名为MASA的新颖框架，这是一个由大语言模型驱动的多智能体系统，专门用于自动形式化。 - **本质分析**: 自动形式化是将自然语言陈述（如数学定义）转换为形式化语言（如机器可验证的证明代码）的过程。这个过程本质上是要求模型进行高精度的**逻辑和数学推理**。论文的核心不是将LLM作为一个黑盒工具去解决一个外部的、特定领域的问题（如诊断疾病），而是**构建一个协作框架来增强LLM自身在形式化推理这一核心任务上的能力**。这完全符合“改进LLM的基础能力、增强其逻辑、数学、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——论文是否包含相关主题？** - **核心概念**: 论文明确以“Large Language Models (LLMs)”为核心驱动力。 - **能力方向**: 研究内容直接关联“formal reasoning”（形式化推理）和“mathematical definitions”（数学推理），这些都是通用推理能力的关键组成部分。 - **新兴范式**: 论文的创新点在于提出了一个“Multi-Agent Systems”（多智能体系统）框架，这与你筛选标准中的“智能体协作框架”高度吻合。其“flexibility and extensibility”的设计也表明这是一个通用性的方法论，而非特定解决方案。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** - 论文的研究内容不涉及多模态、视觉、医疗、化学、机器人等任何被列出的排除领域。它聚焦于数学和逻辑，这是AI基础研究的核心领域，而非特定应用领域。 4.  **第四步：处理特殊和模糊情况——智能体/工具使用** - 论文提出的MASA框架是一个**通用的智能体协作框架**，旨在解决“自动形式化”这一类通用推理问题。它强调模块化和可扩展性，意味着该框架可以被适配和扩展到其他形式化或推理任务中。这完全符合“提出一种通用的智能体协作框架来增强LLM的通用问题解决能力”的保留条件，而不是一个应用于特定领域的智能体。 5.  **第五步：最终决策** - 综合来看，这篇论文的核心是提出一种新的方法论（多智能体协作框架）来提升LLM在形式化数学和逻辑推理方面的能力。这直接触及了“大语言模型通用推理能力”这一研究课题的核心。它不是在应用LLM，而是在**增强LLM**。因此，该论文高度相关，应被筛选出来。"
    },
    {
        "index": "#70",
        "title": "Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning on Knowledge Graphs",
        "link": "/arxiv/2510.08825",
        "arxiv_id": "2510.08825",
        "authors": "Jia Ao Sun, Hao Yu, Fabrizio Gotti, Fengran Mo, Yihong Wu, Yuchen Hui, Jian-Yun Nie",
        "summary": "Large language models (LLMs) have demonstrated impressive reasoning abilities yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss long-tail facts, hallucinate when uncertain, and their internal knowledge lags behind real-world change. Knowledge graphs (KGs) offer a structured source of relational evidence, but existing KGQA methods face fundamental trade-offs: compiling complete SPARQL queries without knowing available relations proves brittle, retrieving large subgraphs introduces noise, and complex agent frameworks with parallel exploration exponentially expand search spaces. To address these limitations, we propose Search-on-Graph (SoG), a simple yet effective framework that enables LLMs to perform iterative informed graph navigation using a single, carefully designed \\textsc{Search} function. Rather than pre-planning paths or retrieving large subgraphs, SoG follows an ``observe-then-navigate'' principle: at each step, the LLM examines actual available relations from the current entity before deciding on the next hop. This approach further adapts seamlessly to different KG schemas and handles high-degree nodes through adaptive filtering. Across six KGQA benchmarks spanning Freebase and Wikidata, SoG achieves state-of-the-art performance without fine-tuning. We demonstrate particularly strong gains on Wikidata benchmarks (+16\\% improvement over previous best methods) alongside consistent improvements on Freebase benchmarks.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.389133",
        "filter_reason": "这篇论文完全符合筛选标准，应当被保留。 以下是根据筛选标准进行的详细判断过程： **第一步：核心判断** 论文的核心贡献是提出了“Search-on-Graph (SoG)”这个**新的推理框架**。这个框架不是将LLM作为工具去解决某个特定领域的问题，而是致力于解决LLM在**知识密集型、多跳推理**这一通用任务上的内在缺陷（如事实滞后、长尾事实缺失、幻觉）。它通过改进LLM的推理范式（从“预先规划”或“大规模检索”转变为“迭代式、信息驱动的导航”），直接增强了LLM的**基础推理能力**。这完全符合“改进LLM基础能力、增强其逻辑、多步推理等通用能力”的保留标准。 **第二步：正面指标** 这篇论文命中了多个关键的正面指标： - **核心概念**: 论文标题和摘要中明确提到了“Large language models (LLMs)”。 - **能力方向**: 论文的核心是“reasoning”，特别是“knowledge-intensive, multi-hop questions”，这是通用推理能力的重要组成部分。 - **新兴范式**: SoG框架本质上是一种**LLM-based agent**的实现。它将LLM视为一个能够在知识图谱（KG）这个环境中进行导航和决策的智能体，通过一个精心设计的`Search`函数（工具）与环境交互。这正是“智能体协作框架”和“工具使用”的体现，旨在提升模型的通用问题解决能力。 **第三步：排除标准** 论文的焦点不包含任何需要排除的领域： - **多模态与视觉**: 不涉及。 - **特定应用领域**: 虽然论文在KGQA（Knowledge Graph Question Answering）基准上进行测试，但KGQA本身被广泛用作衡量模型**事实推理和逻辑链接能力**的标准任务，而非一个像医疗或金融那样的特定应用领域。Freebase和Wikidata是通用知识图谱，而非领域知识库。因此，这篇论文是在用通用推理任务来评估其方法的通用性，而不是在解决特定领域问题。 - **模型可靠性（应用层面）**: 论文不关注水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况** - **智能体/工具使用**: 论文提出了一个**通用的智能体框架**。SoG方法不局限于某个特定的知识图谱或问题类型，其“观察-导航”原则是一种通用的、可迁移的推理策略。这符合“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力”的保留条件。 - **幻觉/可解释性**: 论文在开篇就指出LLM会“hallucinate when uncertain”，而SoG框架通过迫使LLM每一步都基于知识图谱中**真实存在的关系**进行决策，从根本上减少了在知识密集型推理中产生幻觉的可能性。这是一种通过改进推理过程来提升模型内在可靠性和质量的方法，符合保留标准。 **第五步：最终决策** 综合以上分析，该论文的本质是提出一种新颖的、基于智能体和工具使用思想的推理框架（SoG），用以系统性地提升大语言模型在多跳、知识密集型任务上的通用推理能力。它直面LLM的核心短板，并提供了一种方法论层面的解决方案，而非特定应用。因此，这篇论文与“大语言模型通用推理能力”的研究课题高度相关，应予以保留。"
    },
    {
        "index": "#66",
        "title": "Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR",
        "link": "/arxiv/2510.08892",
        "arxiv_id": "2510.08892",
        "authors": "Haomin Zhuang, Yujun Zhou, Taicheng Guo, Yue Huang, Fangxu Liu, Kai Song, Xiangliang Zhang",
        "summary": "Reinforcement Learning has demonstrated substantial improvements in the reasoning abilities of Large Language Models (LLMs), exhibiting significant applicability across various domains. Recent research has identified that tokens within LLMs play distinct roles during reasoning tasks, categorizing them into high-entropy reasoning tokens and low-entropy knowledge tokens. Prior approaches have typically focused on restricting updates to indirectly encourage exploration, yet they do not explicitly facilitate exploratory behavior during the token generation stage itself. In this work, we introduce a complementary approach that explicitly promotes exploration during sampling by applying distinct temperature settings for different token types. Specifically, our method employs higher temperatures for reasoning tokens to actively encourage exploration, while retaining lower temperatures for knowledge tokens to maintain factual correctness. Furthermore, we systematically investigate various multi-temperature scheduling strategies and their impacts within reinforcement learning contexts. Empirical evaluations on several reasoning benchmarks demonstrate that our approach significantly enhances the reasoning performance of LLMs. The code is available at https://github.com/zhmzm/Multi_Temperature_Verl.git.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.381985",
        "filter_reason": "这篇论文完全符合您的筛选标准。以下是基于您提供的筛选标准进行的详细判断： 1.  **第一步：核心判断——论文的本质是什么？** 这篇论文的本质是提出一种新的方法论来**提升LLM自身的通用推理能力**。它的核心贡献并非将LLM应用于某个特定领域，而是深入到LLM的推理生成过程内部。论文通过区分“推理token”和“知识token”，并应用不同的采样温度，来优化模型在强化学习训练过程中的探索与利用平衡。这是一种**基础性的、模型内在的改进**，旨在增强模型完成逻辑、数学等通用推理任务时的表现，完全符合“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的保留标准。 2.  **第二步：正面指标——论文是否包含以下主题？** 论文摘要中明确包含了多个高相关性的正面指标： *   **核心概念**: 明确提到 \"Large Language Models (LLMs)\"。 *   **能力方向**: 反复强调 \"reasoning abilities\", \"reasoning tasks\", \"reasoning tokens\", \"reasoning benchmarks\", \"reasoning performance\"，完全聚焦于“推理”这一核心能力。 *   **训练方法**: 论文的工作背景是 \"Reinforcement Learning\"，并探讨了在强化学习框架下的应用，与 \"reinforcement learning (RLHF, RL)\" 高度相关。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** 该论文完全没有触及任何排除标准领域： *   它不涉及多模态与视觉。 *   它是在通用的推理基准上进行评估，而非医疗、化学、生物等特定应用领域。 *   它关注的是通过技术手段提升推理质量，而不是水印、安全等应用层面的可靠性研究。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 不适用。 *   **幻觉/可解释性/安全**: 论文中提到的“maintain factual correctness”（保持事实正确性）是通过在知识token上使用较低温度这一**技术方法**来实现的，其目的是为了提升推理过程的整体质量和可靠性，而不是作为一项独立的应用层安全研究。因此，这属于“提升模型的通用可靠性和推理质量”的范畴，应当保留。 **最终决策**: 该论文的核心贡献是提出了一种创新的、针对LLM推理过程的内在改进方法（多温度采样策略），直接致力于提升其通用推理能力。它属于方法论研究，旨在增强模型的基础能力，而非将其作为工具应用于特定领域。因此，这篇论文与您“提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致，应当被保留。"
    },
    {
        "index": "#84",
        "title": "Upfront Chain-of-Thought: A Cooperative Framework for Chain-of-Thought Compression",
        "link": "/arxiv/2510.08647",
        "arxiv_id": "2510.08647",
        "authors": "Chengzhengxu Li, Xiaoming Liu, Zhaohan Zhang, Shaochu Zhang, Shengchao Liu, Guoxin Ma, Yu Lan, Chao Shen",
        "summary": "Recent developments have enabled advanced reasoning in Large Language Models (LLMs) via long Chain-of-Thought (CoT), while long CoT suffers from high computational costs and significant latency losses owing to the autoregressive nature of generative LLMs. CoT compression aims to improve efficiency in the reasoning process by reducing output length. Previous works trade reasoning efficiency by either laborious discrete prompt designing or the construction of external compressed CoT datasets that sacrifice key reasoning details. In this work, we propose Upfront CoT (UCoT): an efficient reasoning framework with upfront thought embedding to automate CoT compression. UCoT is a cooperative workflow involving a small model (compressor) and a large model (executor). The first stage of UCoT trains compressor to generate upfront thought embeddings rich in reasoning information for the executor, avoiding the drawbacks of manually designed prompts. The second stage optimizes executor to utilize upfront thought embeddings to derive the correct answer with short reasoning, using a reward mechanism. Extensive experiments show that UCoT maintains the powerful reasoning ability of executor while significantly reducing the length of CoT. It is worth mentioning that when applying UCoT to the Qwen2.5-7B-Instruct model, the usage of tokens on GSM8K dataset is reduced by 50\\%, while the performance is 3.08\\% higher than that of the state-of-the-art (SOTA) method. The code and dataset are in supplementary material.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.401025",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为“Upfront CoT (UCoT)”的新框架，旨在解决大语言模型（LLM）在应用长思维链（CoT）进行推理时面临的计算成本高和延迟大的问题。其本质是通过一种“小模型压缩、大模型执行”的协作工作流，来**优化LLM的推理过程本身**，使其在保持甚至提升推理能力的同时，大幅提高推理效率。这直接属于“改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴，而不是将LLM作为工具应用于特定领域。 2.  **第二步：正面指标** 论文与多个正面指标高度吻合： *   **核心概念**: 论文明确以“Large Language Models (LLMs)”为研究对象。 *   **能力方向**: 论文的核心主题是“reasoning”，并在数学推理基准数据集GSM8K上进行了验证，这直接对应了“math reasoning”和“problem-solving”。 *   **训练方法**: 论文提到了使用“reward mechanism”来优化执行器模型，这强烈暗示了其采用了强化学习（RL）的思想，这与“reinforcement learning (RL)”指标相符。 *   **新兴范式**: 论文提出的“cooperative workflow”涉及一个压缩器模型和一个执行器模型的协作，可以被视为一种新颖的“multi-agent systems”或协作框架，旨在增强LLM的通用问题解决能力。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准领域： *   它没有涉及视觉、多模态等内容。 *   它的研究目标是通用的数学推理能力，而非医疗、化学、机器人等任何特定应用领域。 *   它关注的是推理效率和性能，而非水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况** 论文提出的“cooperative framework”是一个**通用的智能体协作框架**，其目的是增强LLM的通用推理效率和能力，而不是将其应用于某个特定领域。因此，根据筛选标准，这种情况应该被保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种创新的训练和推理框架（UCoT），通过自动化压缩思维链来提升LLM的通用推理效率和能力。它直接针对LLM的核心推理能力进行优化，与您的研究目标“致力于提高大语言模型（LLM）本身的『通用推理能力』”高度一致。因此，这篇论文应该被保留。"
    },
    {
        "index": "#80",
        "title": "Scaling Laws for Code: A More Data-Hungry Regime",
        "link": "/arxiv/2510.08702",
        "arxiv_id": "2510.08702",
        "authors": "Xianzhen Luo, Wenzhen Zheng, Qingfu Zhu, Rongyi Zhang, Houyi Li, Siming Huang, YuanTao Fan, Wanxiang Che",
        "summary": "Code Large Language Models (LLMs) are revolutionizing software engineering. However, scaling laws that guide the efficient training are predominantly analyzed on Natural Language (NL). Given the fundamental differences like strict syntax between code and NL, it is unclear whether these laws are directly applicable to code. To address this gap, we conduct the first large-scale empirical study of scaling laws for code, comprising 117 experimental runs with model sizes from 0.2B to 3.8B and training tokens from 2B to 128B. We fit the Chinchilla law and the Farsser law. First, the results show that the more expressive Farseer law offers greater accuracy. Second, the analysis reveals that Code LLMs scale effectively with model size. Crucially, code represents a more data-hungry regime, requiring a substantially higher data-to-parameter ratio than NL. Finally, two additional sets of experiments on code-NL mixtures show that NL benefits resource-constrained scenarios, but becomes a detriment at higher compute budgets.",
        "subjects": "Computation and Language",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.398865",
        "filter_reason": "这篇论文符合筛选标准。我的判断过程如下： 1.  **第一步：核心判断** - **论文本质**: 这篇论文的核心贡献并非将LLM作为工具应用于特定领域，而是对LLM本身（特别是代码LLM）的基础训练规律进行实证研究。它探究的是“如何更高效地训练一个强大的代码大语言模型”这一根本性问题，具体聚焦于“缩放定律”。缩放定律是指导大模型训练和资源配置的基础理论，直接关系到模型能力的上限和训练效率，因此这属于改进LLM基础能力和训练范式的研究范畴。 - **与通用推理能力的关联**: 论文研究的是“代码”这一特定模态的LLM。代码是逻辑、算法和结构化推理的极致体现。一个更强大的代码LLM，其内在的逻辑演绎、符号操作和规划能力必然更强。因此，研究如何提升代码LLM的性能，本质上就是在探索如何提升LLM在逻辑和结构化推理这一通用能力上的表现。这与“提高大语言模型本身的通用推理能力”的核心目标高度一致。 2.  **第二步：正面指标** - **核心概念**: 论文明确以“Code Large Language Models (LLMs)”为核心研究对象。 - **能力方向**: 虽然摘要未直接使用\"reasoning\"一词，但其研究对象\"code\"本身就是一种严谨的推理和问题求解形式。论文的目标是通过优化训练来提升代码模型的能力，这直接等同于提升其内在的逻辑和算法推理能力。 - **训练方法**: 论文研究的“缩放定律”是关于如何平衡模型大小、数据量和计算预算以获得最佳性能的训练方法论，是训练范式研究的重要组成部分。 3.  **第三步：排除标准** - **多模态与视觉**: 论文仅涉及代码文本，不涉及任何视觉或多模态内容。 - **特定应用领域**: 这是关键点。虽然“代码”属于软件工程领域，但论文的焦点并非“用LLM解决某个具体的软件开发任务”，而是“研究代码LLM这一模型类别的普适性训练规律”。它探究的是模型本身的发展规律，而非模型在下游领域的应用效果。因此，它不应被归类为应被排除的“特定应用领域”研究。 - **模型可靠性（应用层面）**: 论文未涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** - 本论文不属于智能体、工具使用或幻觉等特殊情况。 5.  **第五步：最终决策** - 综合来看，这篇论文虽然聚焦于代码这一特定数据模态，但其研究问题是关于如何更有效地训练和提升LLM基础能力的。它揭示了代码LLM独特的“数据饥渴”特性，为未来构建在逻辑推理方面更强大的LLM提供了关键的指导原则。这种对模型基础训练规律的探索，完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。因此，应予以保留。"
    },
    {
        "index": "#96",
        "title": "GraphGhost: Tracing Structures Behind Large Language Models",
        "link": "/arxiv/2510.08613",
        "arxiv_id": "2510.08613",
        "authors": "Xinnan Dai, Kai Guo, Chung-Hsiang Lo, Shenglai Zeng, Jiayuan Ding, Dongsheng Luo, Subhabrata Mukherjee, Jiliang Tang",
        "summary": "Large Language Models (LLMs) demonstrate remarkable reasoning capabilities, yet the structural mechanisms underlying these abilities remain under explored. In this work, we introduce GraphGhost, a unified framework that represents neuron activations and their signal propagation as graphs, explaining how LLMs capture structural semantics from sequential inputs and generate outputs through structurally consistent mechanisms. This graph-based perspective enables us to employ graph algorithms such as PageRank to characterize the properties of LLMs, revealing both shared and model-specific reasoning behaviors across diverse datasets. We further identify the activated neurons within GraphGhost and evaluate them through structural interventions, showing that edits to key neuron nodes can trigger reasoning collapse, altering both logical flow and semantic understanding. Together, these contributions position GraphGhost as a powerful tool for analyzing, intervening in, and ultimately understanding the structural foundations of reasoning in LLMs.",
        "subjects": "Computation and Language",
        "date": "2025-10-07",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.437878",
        "filter_reason": "这篇论文完全符合你的研究范围。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是提出了一个名为“GraphGhost”的统一框架。这个框架的本质不是将LLM作为工具去解决某个外部领域的问题，而是**深入LLM内部，通过将神经元激活表示为图结构，来分析和干预其推理过程**。论文明确指出了其目标是“理解LLM中推理的结构基础”，并通过“结构性干预”实验（如编辑关键神经元导致“推理崩溃”）来证明这种结构与推理能力的直接因果关系。这直接触及了LLM通用推理能力的根本机制，属于改进LLM基础能力的范畴，因此应予以保留。 2.  **第二步：正面指标** 论文高度符合正面指标： *   **核心概念**: 标题和摘要中明确提到了“Large Language Models (LLMs)”。 *   **能力方向**: 论文的核心就是围绕“reasoning”展开，讨论了“reasoning capabilities”、“reasoning behaviors”以及“reasoning collapse”。 *   **新兴范式**: 虽然没有直接提到智能体或工具使用，但其提出的“分析和干预”框架本身就是一种理解LLM的新兴研究范式。 3.  **第三步：排除标准** 论文完全不涉及任何排除标准。它没有讨论多模态、特定应用领域（如医疗、化学），也没有关注模型部署、水印等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文恰好命中了“可解释性”这一特殊情况的保留条件。它不是对社会学现象的讨论，而是提出了一种**全新的技术方法（GraphGhost框架）来增强模型内在的可解释性**。通过揭示推理背后的结构基础，该论文为未来如何针对性地提升模型的逻辑流畅性和语义理解（即推理质量）提供了明确的路径和工具。这种对内在机制的深刻洞察，是提升通用推理能力不可或缺的一步。 5.  **第五步：最终决策** 综合以上分析，这篇论文虽然不像CoT或RLHF那样直接提出一个“训练”方法来提升模型分数，但它提供了一个更底层的**“分析与干预”方法论**。它致力于回答“LLM的推理能力究竟是如何产生的”这一根本问题。对于你“致力于提高大语言模型本身的通用推理能力”这一核心目标而言，理解其内在结构是实现有效改进的前提。因此，这篇论文提供了极具价值的洞见和工具，完全符合你的筛选要求。"
    },
    {
        "index": "#106",
        "title": "Systematic Diagnosis of Brittle Reasoning in Large Language Models",
        "link": "/arxiv/2510.08595",
        "arxiv_id": "2510.08595",
        "authors": "V. S. Raghu Parupudi",
        "summary": "A central question in artificial intelligence is the extent to which machine learning models comprehend mathematics. To address this, we propose a novel framework for measuring mathematical reasoning that moves beyond standard benchmarks to diagnose specific failure points. Our method first generates structured, step-by-step reasoning from gpt-3.5-turbo on the GSM8K dataset. We then use a more capable analyst model, gpt-4o-mini, to categorize errors and, crucially, perform an unsupervised clustering of every reasoning sentence to identify emergent \"reasoning modes.\" This analysis reveals a cognitive profile with a stark, nonhuman-like brittleness: while the model achieves near-perfect accuracy on procedural modes like sequential calculation, its performance on modes requiring combinatorial reasoning with restrictions plummets. By identifying and quantifying the reliability of these distinct reasoning skills, our work provides a more granular method to evaluate mathematical comprehension and offers a precise roadmap for developing new capabilities and more reliable future applications.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.448523",
        "filter_reason": "这篇论文完全符合您的研究范围。 判断过程如下： 1.  **第一步：核心判断** 这篇论文的本质并非将LLM作为工具应用于特定领域，而是对LLM的**核心能力之一——数学推理能力**进行深入的、系统性的诊断和分析。它提出了一种新的评估框架，旨在揭示模型在推理过程中的具体失败模式（如“脆弱的组合推理”）。这种诊断工作是“提高LLM通用推理能力”的必要前提和基础。论文摘要最后明确指出，其工作为“开发新能力和更可靠的未来应用提供了精确的路线图”，这直接呼应了您筛选那些“致力于提高大语言模型本身通用推理能力”的论文的核心目标。因此，它应被保留。 2.  **第二步：正面指标** 论文高度契合正面指标： *   **核心概念**: 明确研究Large language models (LLMs)，以GPT-3.5和GPT-4o-mini为例。 *   **能力方向**: 核心主题是**数学推理**，并深入探讨了推理的不同“模式”，这直接属于`reasoning`和`problem-solving`的范畴。 3.  **第三步：排除标准** 该论文完全没有触及任何排除标准： *   它不涉及多模态、视觉等内容。 *   它的研究领域是通用的数学，而非医疗、化学、机器人等特定应用领域。 *   它讨论的“脆弱性”是关于模型内在的推理能力缺陷，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文可以被视为在**可解释性**方面的一种探索。它提出了一种新的方法（通过聚类推理步骤来识别“推理模式”），以增强我们对模型为何以及如何在推理任务中失败的理解。根据筛选规则，“如果论文提出一种新方法来增强模型内在的可解释性……从而提升模型的通用可靠性和推理质量，应该保留。” 该论文正是如此，它通过提供更深层次的可解释性，为未来提升推理质量指明了方向。 **最终决策**: 该论文的核心贡献是提出了一套系统性的诊断框架，用于量化和分类LLM在数学推理中的具体失败模式。它没有直接提出一种新的训练方法，但它为未来的研究提供了至关重要的“诊断工具”和“改进路线图”。这项基础性、诊断性的研究，是任何旨在系统性提升LLM通用推理能力的工作所不可或缺的。因此，它精准地符合您的研究目标。"
    },
    {
        "index": "#113",
        "title": "HINT: Helping Ineffective Rollouts Navigate Towards Effectiveness",
        "link": "/arxiv/2510.09388",
        "arxiv_id": "2510.09388",
        "authors": "Xinyi Wang, Jinyi Han, Zishang Jiang, Tingyun Li, Jiaqing Liang, Sihang Jiang, Zhaoqian Dai, Shuguang Ma, Fei Yu, Yanghua Xiao",
        "summary": "Reinforcement Learning (RL) has become a key driver for enhancing the long chain-of-thought (CoT) reasoning capabilities of Large Language Models (LLMs). However, prevalent methods like GRPO often fail when task difficulty exceeds the model's capacity, leading to reward sparsity and inefficient training. While prior work attempts to mitigate this using off-policy data, such as mixing RL with Supervised Fine-Tuning (SFT) or using hints, they often misguide policy updates In this work, we identify a core issue underlying these failures, which we term low training affinity. This condition arises from a large distributional mismatch between external guidance and the model's policy. To diagnose this, we introduce Affinity, the first quantitative metric for monitoring exploration efficiency and training stability. To improve Affinity, we propose HINT: Helping Ineffective rollouts Navigate Towards effectiveness, an adaptive hinting framework. Instead of providing direct answers, HINT supplies heuristic hints that guide the model to discover solutions on its own, preserving its autonomous reasoning capabilities. Extensive experiments on mathematical reasoning tasks show that HINT consistently outperforms existing methods, achieving state-of-the-art results with models of various scales, while also demonstrating significantly more stable learning and greater data efficiency.Code is available on Github.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.457248",
        "filter_reason": "这篇论文完全符合您的研究范围。以下是我的详细判断过程： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献是提出了一种名为 **HINT** 的新颖训练框架，旨在解决强化学习（RL）在训练大语言模型进行长链推理时遇到的**低训练亲合力**问题。其本质是**改进LLM的训练范式**，从而直接增强其**通用推理能力**（特别是长链思维链推理）。它不是将LLM应用于特定领域，而是致力于提升模型本身的基础能力。因此，这一步的判断是**保留**。 2.  **第二步：正面指标——论文是否包含相关主题？** 论文摘要中包含了多个关键正面指标，且均为论文核心： - **核心概念**: 明确提到了 \"Large Language Models (LLMs)\"。 - **能力方向**: 核心议题是 \"long chain-of-thought (CoT) reasoning capabilities\" 和 \"mathematical reasoning\"。 - **训练方法**: 论文的核心是基于 \"Reinforcement Learning (RL)\"，并讨论了 GRPO、SFT 等方法，提出了自己的改进框架。 这些主题与您的研究目标高度契合。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文不涉及任何排除标准中的领域。它没有讨论多模态、视觉问题；没有将研究应用于医疗、化学、法律等特定领域；也不关注水印、安全等模型可靠性（应用层面）的问题。它使用的“数学推理任务”是评估模型**通用推理能力**的标准基准，而非特定应用。 4.  **第四步：处理特殊和模糊情况** 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况。其焦点非常清晰：通过改进RL训练过程来提升模型的内在推理能力。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种新的训练方法论（HINT框架），通过解决RL训练中的核心瓶颈（低训练亲合力），来提升大语言模型的通用推理能力（长链CoT）。这与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标完全一致。因此，这篇论文是您应该重点关注的、高度相关的前沿研究。"
    },
    {
        "index": "#123",
        "title": "Diagnosing and Mitigating System Bias in Self-Rewarding RL",
        "link": "/arxiv/2510.08977",
        "arxiv_id": "2510.08977",
        "authors": "Chuyi Tan, Peiwen Yuan, Xinglin Wang, Yiwei Li, Shaoxiong Feng, Yueqi Zhang, Jiayi Shi, Ji Zhang, Boyuan Pan, Yao Hu, Kan Li",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) scales the reasoning ability of large language models (LLMs) but remains bottlenecked by limited labeled samples for continued data scaling. Reinforcement learning with intrinsic rewards (RLIR), where the policy model assigns rewards to its own rollouts, enables sustainable scaling in unlabeled settings, yet its performance and stability lag behind RLVR. We trace this gap to a system bias: the model tends to overestimate its high-confidence rollouts, leading to biased and unstable reward estimation. This bias accumulates as training progresses, with deviations from the oracle drifting toward over-reward, causing unstable training. We characterize this bias using three metrics: $\\rho_{\\text{noise}}$, $\\rho_{\\text{selfbias}}$, and $\\rho_{\\text{symbias}}$. We find that $\\rho_{\\text{noise}}$ and $\\rho_{\\text{symbias}}$ impact convergence, while $\\rho_{\\text{selfbias}}$ amplifies both correct and incorrect updates, leading to instability. To mitigate this, we propose reinforcement learning with ensembled rewards (RLER), which aggregates diverse models and adapts reward interpolation and rollout selection. Extensive experiments show that RLER improves by +13.6% over RLIR and is only 3.6% below RLVR, achieving stable scaling on unlabeled samples, making it highly applicable.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.467833",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断依据如下： 1.  **第一步：核心判断——论文本质是改进LLM的基础能力。** 论文的核心贡献是提出了一种名为“RLER”（reinforcement learning with ensembled rewards）的新方法。该方法旨在解决“自奖励强化学习”（Self-Rewarding RL）这一训练范式中的系统偏差和不稳定性问题。论文摘要开篇就明确指出，其研究背景是“扩展大语言模型（LLM）的推理能力”。因此，这篇论文的本质不是将LLM应用于某个特定领域，而是致力于改进一种能够提升LLM通用推理能力的**基础训练方法论**。这完全符合“保留”标准。 2.  **第二步：正面指标——论文高度匹配核心主题。** 论文包含了多个关键的正面指标： *   **核心概念**: 明确以“large language models (LLMs)”为研究对象。 *   **能力方向**: 直接点明研究目标是“scales the reasoning ability”（扩展推理能力）。 *   **训练方法**: 论文的核心是关于“Reinforcement learning (RL)”的，特别是“Self-Rewarding RL”和“Ensembled Rewards”，属于前沿的训练范式研究。 *   **新兴范式**: “Self-Rewarding”本身可以看作是一种“自我进化”的雏形，论文旨在让这种进化过程更稳定、更有效。 3.  **第三步：排除标准——论文未触及任何排除领域。** 论文的研究内容纯粹聚焦于LLM的训练算法和动态，完全不涉及多模态、视觉、特定应用领域（如医疗、化学）或应用层面的模型可靠性（如水印、安全）。 4.  **第四步：处理特殊情况——论文属于应保留的范畴。** 论文研究的“系统偏差”和“不稳定性”是训练过程中的内在技术挑战。解决这些问题能够直接提升模型自我评估和自我优化的质量，从而增强其推理过程的稳定性和可靠性。这属于“提出一种新方法来……提升模型的通用可靠性和推理质量”的情况，因此应该保留。它不是对这些现象的宏观讨论，而是深入到训练机制内部的技术创新。 **总结**: 该论文精准地定位在“如何通过改进强化学习训练范式来提升大语言模型的通用推理能力”这一核心议题上。它识别了现有方法（自奖励RL）的关键缺陷（系统偏差），并提出了一种有效的解决方案（RLER），最终目标是实现更稳定的模型能力扩展。这与您筛选“致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标高度一致。"
    },
    {
        "index": "#134",
        "title": "Robust Heuristic Algorithm Design with LLMs",
        "link": "/arxiv/2510.08755",
        "arxiv_id": "2510.08755",
        "authors": "Pantea Karimi, Dany Rouhana, Pooria Namyar, Siva Kesava Reddy Kakarla, Venkat Arun, Behnaz Arzani",
        "summary": "We posit that we can generate more robust and performant heuristics if we augment approaches using LLMs for heuristic design with tools that explain why heuristics underperform and suggestions about how to fix them. We find even simple ideas that (1) expose the LLM to instances where the heuristic underperforms; (2) explain why they occur; and (3) specialize design to regions in the input space, can produce more robust algorithms compared to existing techniques~ -- ~the heuristics we produce have a $\\sim28\\times$ better worst-case performance compared to FunSearch, improve average performance, and maintain the runtime.",
        "subjects": "Artificial Intelligence, Computation and Language, Networking and Internet Architecture",
        "date": "2025-10-09",
        "category": "cs.CL",
        "crawl_time": "2025-10-13T11:00:06.484327",
        "filter_reason": "这篇论文完全符合您的筛选标准，其核心贡献在于提出了一种新颖的框架，用以提升大语言模型的通用推理与问题解决能力。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心并非将LLM作为一个现成工具去解决某个特定领域（如化学、金融）的问题，而是提出了一种**方法论**来增强LLM本身的高级认知能力。论文研究的是如何让LLM去“设计”和“改进”启发式算法，这本质上是一个复杂的**规划、问题解决和迭代优化**过程。该过程包括“识别失败案例”、“分析原因”和“针对性修复”，这正是一种多步推理和自我修正能力的体现，属于提升LLM通用推理能力的核心范畴。因此，应予以**保留**。 2.  **第二步：正面指标** 论文与多个正面指标高度相关： *   **核心概念**: 论文明确以LLMs为核心。 *   **能力方向**: 论文致力于提升LLM在“问题解决”和“规划”方面的能力，具体表现为算法设计与优化。 *   **新兴范式**: 论文完美契合“工具使用”范式。它让LLM使用“解释失败原因”和“提供修复建议”的工具来提升自身表现。同时，其迭代改进的闭环机制也带有“自我进化”的意味。 *   **训练方法**: 虽然未直接提及RL，但其“反馈-改进”的流程与强化学习的思想内核一致。 3.  **第三步：排除标准** 论文不涉及任何排除标准中的内容： *   它不关注多模态。 *   其焦点是通用的算法设计方法论，而非任何特定应用领域（如医疗、机器人）。 *   它讨论的是提升模型性能的方法，而非水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** 该论文是“智能体/工具使用”标准的典型案例。它提出的是一个**通用的**智能体框架，其核心是利用工具来赋能LLM进行更高效的推理和创造（设计算法），而不是将智能体限定在某个垂直领域。这种通过外部工具和反馈循环来增强LLM内在推理质量的研究，正是您所关注的前沿方向。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种通过工具辅助和反馈机制来增强LLM通用问题解决与推理能力的新范式。它探索了如何让LLM从简单的生成器进化为能够分析、反思和自我改进的“算法设计师”，这与您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标高度一致。因此，最终判断为**True**。"
    },
    {
        "index": "#1",
        "title": "BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative Rewards",
        "link": "/arxiv/2510.09596",
        "arxiv_id": "2510.09596",
        "authors": "Sangyun Lee, Brandon Amos, Giulia Fanti",
        "summary": "Today's generative models thrive with large amounts of supervised data and informative reward functions characterizing the quality of the generation. They work under the assumptions that the supervised data provides knowledge to pre-train the model, and the reward function provides dense information about how to further improve the generation quality and correctness. However, in the hardest instances of important problems, two problems arise: (1) the base generative model attains a near-zero reward signal, and (2) calls to the reward oracle are expensive. This setting poses a fundamentally different learning challenge than standard reward-based post-training. To address this, we propose BaNEL (Bayesian Negative Evidence Learning), an algorithm that post-trains the model using failed attempts only, while minimizing the number of reward evaluations (NREs). Our method is based on the idea that the problem of learning regularities underlying failures can be cast as another, in-loop generative modeling problem. We then leverage this model to assess whether new data resembles previously seen failures and steer the generation away from them. We show that BaNEL can improve model performance without observing a single successful sample on several sparse-reward tasks, outperforming existing novelty-bonus approaches by up to several orders of magnitude in success rate, while using fewer reward evaluations.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.497080",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。以下是我的详细判断过程： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一种名为BaNEL的**新型后期训练算法**。其本质不是将LLM应用于某个特定领域，而是致力于解决生成模型在面临“困难任务”（即奖励信号稀疏、成功样本极少）时的训练难题。这种方法直接旨在**提升模型生成过程的质量和正确性**，这属于改进LLM基础能力的范畴，特别是其在复杂问题上的推理和求解能力。因此，它通过了第一步的核心判断。 2.  **正面指标（第二步）：** 该论文在多个正面指标上表现出色： *   **核心概念:** 虽然摘要中使用了更广泛的“Generative Models”，但其描述的后期训练、奖励优化等场景与LLM的研究高度相关，其方法论可以直接应用于提升LLM。 *   **能力方向:** 论文聚焦于“稀疏奖励任务”和提升“生成正确性”。解决这类任务通常需要模型具备强大的多步推理、规划和问题解决能力，因为正确的答案并非一步到位。论文的核心目标——通过学习失败来提升成功率——本质上就是增强模型的内在推理和纠错能力。 *   **训练方法:** 这是论文最突出的亮点。BaNEL是一种创新的**强化学习后期训练范式**。它不依赖传统的正向奖励，而是巧妙地利用“负奖励”（即失败样本）进行学习，这是一种非常前沿的**自我进化/自我修正**思想，直接旨在提升模型的内在能力。 3.  **排除标准（第三步）：** 该论文未触及任何排除标准。其内容是纯粹的方法论研究，不涉及多模态、特定领域应用或应用层面的安全水印等问题。 4.  **特殊和模糊情况（第四步）：** 该论文可以被视为对“模型可靠性”的一种深层次探索。它不是从应用层面讨论安全性，而是提出了一种新的训练方法来**从根本上减少模型输出的“失败”或“错误”**，从而提升其在复杂任务上的内在可靠性。这正符合“提升模型内在可靠性或安全性，从而提升模型的通用可靠性和推理质量”的保留标准。 **最终决策（第五步）：** 综合分析，这篇论文提出了一种创新的、通用的后期训练算法（BaNEL），其核心目标是让大语言模型能够从失败中学习，从而在奖励信号稀疏的复杂推理和问题解决任务中表现得更好。这直接命中了您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。它是一种基础性的方法论贡献，而非特定领域的应用，因此完全符合您的筛选要求。"
    },
    {
        "index": "#22",
        "title": "Task-Level Insights from Eigenvalues across Sequence Models",
        "link": "/arxiv/2510.09379",
        "arxiv_id": "2510.09379",
        "authors": "Rahel Rickenbach, Jelena Trisovic, Alexandre Didier, Jerome Sieber, Melanie N. Zeilinger",
        "summary": "Although softmax attention drives state-of-the-art performance for sequence models, its quadratic complexity limits scalability, motivating linear alternatives such as state space models (SSMs). While these alternatives improve efficiency, their fundamental differences in information processing remain poorly understood. In this work, we leverage the recently proposed dynamical systems framework to represent softmax, norm and linear attention as dynamical systems, enabling a structured comparison with SSMs by analyzing their respective eigenvalue spectra. Since eigenvalues capture essential aspects of dynamical system behavior, we conduct an extensive empirical analysis across diverse sequence models and benchmarks. We first show that eigenvalues influence essential aspects of memory and long-range dependency modeling, revealing spectral signatures that align with task requirements. Building on these insights, we then investigate how architectural modifications in sequence models impact both eigenvalue spectra and task performance. This correspondence further strengthens the position of eigenvalue analysis as a principled metric for interpreting, understanding, and ultimately improving the capabilities of sequence models.",
        "subjects": "Machine Learning, Artificial Intelligence, Systems and Control",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.518535",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心并非将LLM应用于特定领域，而是深入探究序列模型（包括LLM）的内在工作机制。它提出了一种基于“特征值分析”的理论框架，来理解和比较不同架构（如Attention和SSM）在处理信息时的根本差异。论文的最终目标是“解释、理解并最终改进序列模型的能力”。这完全符合“改进LLM的基础能力”这一核心要求，因为它提供了一种原则性的方法来指导模型架构的优化，从而从根本上提升模型性能，这包括推理能力所依赖的记忆和长程依赖等基础能力。 2.  **第二步：正面指标** - **核心概念**: 论文研究的“序列模型”是LLM的所属范畴，其分析直接适用于LLM。 - **能力方向**: 论文明确指出其研究揭示了模型在“记忆和长程依赖建模”方面的特性。这两者是进行复杂、多步推理的基石。通过理解并优化这些基础能力，论文间接但有力地促进了通用推理能力的提升。 - **可解释性**: 论文的核心贡献之一是提出了一种新的可解释性方法（特征值分析），用于“解释和理解”模型行为。根据筛选标准第四条的特殊情况，这种旨在增强模型内在可解释性以指导改进的研究，是应该保留的。 3.  **第三步：排除标准** 论文完全不涉及多模态、视觉、医疗、化学等特定应用领域，也未讨论水印、安全等应用层面的可靠性问题。因此，它没有触犯任何排除标准。 4.  **第四步：处理特殊和模糊情况** 如上所述，这篇论文是关于“可解释性”的典型案例。它不是对现象的讨论，而是提出了一种新的分析工具（特征值谱），旨在从数学和动力学系统的角度揭示模型能力的来源。这种基础性的理解是未来设计出推理能力更强的模型的关键一步，因此完全符合保留条件。 **最终决策**: 这篇论文虽然没有直接提出一种新的“推理技巧”（如CoT），但它提供了一种更深层次、更根本的视角来审视和改进LLM的核心架构与能力。它通过分析模型的“记忆”和“长程依赖”等基础能力，为提升模型的通用推理能力提供了理论依据和改进方向。这种基础性、方法论的研究对于推动“大语言模型通用推理能力”这一前沿课题的发展至关重要，因此应被判定为符合要求。"
    },
    {
        "index": "#43",
        "title": "Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting",
        "link": "/arxiv/2510.09152",
        "arxiv_id": "2510.09152",
        "authors": "Suming Qiu, Jing Li, Zhicheng Zhou, Junjie Huang, Linyuan Qiu, Zhijie Sun",
        "summary": "Large language models (LLMs) often face a trade-off in post-training: improvements on specialized domains frequently come at the expense of general capabilities. Existing solutions attempt to mitigate this tension via regularization, selective parameter updates, or data-centric replay, but each imposes significant costs in computation, data access, or adaptability. Recent work has shown that training signals can be compressed to subsets of logits without severe accuracy loss, suggesting a path toward efficient adaptation. However, naive truncation destabilizes optimization and exacerbates forgetting. We introduce Logits Replay + MoClip, a two-stage framework that compresses supervision in the logit space and stabilizes optimization at the update level. In Stage 0, we record dynamic Top-K token subsets that cover a probability threshold, always including the gold label. In Stage 1, we replay these compact subsets to compute exact renormalized losses, avoiding full softmax computation and implicitly regularizing. To ensure stability, we design MoClip, an optimizer that caps gradient-momentum rotation and applies an arctan2-based rescaling of updates. Empirically, our method improves domain performance on Communication Technology (CT) and NL2SQL tasks while mitigating forgetting on general benchmarks (MMLU, BBH, GPQA, MATH), and reduces training cost by over 40%. Together, these contributions offer a scalable, architecture-agnostic path for domain adaptation of LLMs without sacrificing generalization.",
        "subjects": "Machine Learning",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.542804",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。判断依据如下： 1.  **核心判断（符合保留标准）**: 论文的核心贡献是提出了一种名为“Logits Replay + MoClip”的后期训练框架。其本质是解决LLM在针对特定领域进行微调时，会“遗忘”通用能力的关键问题。这是一种直接作用于LLM训练过程本身，旨在**保护和维持**其通用能力的方法论研究。这与我的核心目标——“提高大语言模型（LLM）本身的『通用推理能力』”高度相关，因为一个无法在持续学习中保持自身通用能力的模型，其推理能力的提升也就无从谈起。保护通用能力是提升通用能力的前提和基础。 2.  **正面指标（高度相关）**: *   **核心概念**: 论文明确围绕“Large language models (LLMs)”展开。 *   **能力方向**: 论文通过在多个通用基准上验证其方法效果来证明其有效性，其中**MATH**、**BBH** (BIG-Bench Hard, 包含大量复杂推理任务) 和 **GPQA** (研究生水平的科学问答) 都是衡量模型高级数学、逻辑和推理能力的核心数据集。论文的核心目标就是“mitigating forgetting on general benchmarks”，这直接对应了通用推理能力的保持。 *   **训练方法**: 提出了一种新的、低成本且稳定的后期训练范式，这属于“提出新的训练范式”的范畴。 3.  **排除标准（未触犯）**: *   **特定应用领域**: 论文虽然提到了“Communication Technology (CT)”和“NL2SQL”这两个特定领域，但它们是作为引出问题（领域提升导致通用能力下降）的背景和验证方法有效性的案例。论文的**核心贡献**并非解决CT或NL2SQL问题本身，而是解决这个普遍存在的“遗忘”问题。因此，它不是一篇领域应用论文，而是一篇关于LLM训练机制的基础研究论文。 *   **多模态与安全**: 论文内容完全不涉及视觉、多模态、水印、安全等排除领域。 4.  **最终决策**: 综合来看，该论文精准地切入了一个提升LLM通用推理能力的关键痛点——在持续学习和领域适应中如何防止核心能力的退化。它提出的方法论（Logits Replay + MoClip）直接针对LLM的训练过程，旨在稳定和保留其在数学、逻辑等通用推理任务上的表现。这种对模型基础能力的保护性增强，完全符合我为“大语言模型通用推理能力”课题筛选前沿论文的目标。"
    },
    {
        "index": "#42",
        "title": "Agentic-KGR: Co-evolutionary Knowledge Graph Construction through Multi-Agent Reinforcement Learning",
        "link": "/arxiv/2510.09156",
        "arxiv_id": "2510.09156",
        "authors": "Jing Li, Zhijie Sun, Zhicheng Zhou, Suming Qiu, Junjie Huang, Haijia Sun, Linyuan Qiu",
        "summary": "Current knowledge-enhanced large language models (LLMs) rely on static, pre-constructed knowledge bases that suffer from coverage gaps and temporal obsolescence, limiting their effectiveness in dynamic information environments. We present Agentic-KGR, a novel framework enabling co-evolution between LLMs and knowledge graphs (KGs) through multi-round reinforcement learning (RL). Our approach introduces three key innovations: (1) a dynamic schema expansion mechanism that systematically extends graph ontologies beyond pre-defined boundaries during training; (2) a retrieval-augmented memory system enabling synergistic co-evolution between model parameters and knowledge structures through continuous optimization; (3) a learnable multi-scale prompt compression approach that preserves critical information while reducing computational complexity through adaptive sequence optimization. Experimental results demonstrate substantial improvements over supervised baselines and single-round RL approaches in knowledge extraction tasks. When integrated with GraphRAG, our method achieves superior performance in downstream QA tasks, with significant gains in both accuracy and knowledge coverage compared to existing methods.",
        "subjects": "Machine Learning",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.542312",
        "filter_reason": "这篇论文符合筛选标准，应当保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一个名为“Agentic-KGR”的新框架，其本质是一种**新的训练和优化范式**。该框架通过多智能体强化学习（Multi-Agent RL）让大语言模型（LLM）和知识图谱（KG）进行“协同进化”。这并非将LLM作为工具应用于特定领域，而是致力于从根本上解决LLM依赖静态知识库的局限性，从而增强其内在能力。一个能够动态更新和扩展知识基础的LLM，其通用推理和问题解决能力必然会得到增强。因此，这篇论文的核心是改进LLM的基础能力，符合保留标准。 2.  **第二步：正面指标** 论文包含了多个高相关性的正面指标： *   **核心概念**: 明确以“knowledge-enhanced large language models (LLMs)”为研究对象。 *   **能力方向**: 虽然没有直接以“reasoning”为标题，但其最终目标是通过提升知识的质量和覆盖度，来提升在“downstream QA tasks”上的表现，而问答是衡量推理能力的关键任务。这属于通用问题解决能力的范畴。 *   **训练方法**: 核心方法是“multi-round reinforcement learning (RL)”和“continuous optimization”，这正是筛选标准中明确指出的关键训练方法。 *   **新兴范式**: 论文提出了一个“multi-agent”系统，并包含“retrieval-augmented memory system”，这完全符合“llm-based agents”和“multi-agent systems”的新兴范式。 3.  **第三步：排除标准** 论文不涉及任何排除标准中的领域。它不是关于多模态、视觉，也不是关于医疗、化学、机器人等特定应用领域，更不关注水印、安全等应用层面的可靠性。其方法具有通用性。 4.  **第四步：处理特殊和模糊情况** *   **智能体/工具使用**: 论文提出的智能体框架是用于**通用的知识图谱构建和进化**，而非应用于特定领域（如“用于化学实验的智能体”）。其目的是增强LLM的通用知识基础，从而提升其通用能力，因此应该保留。这与“用于化学实验自动化的智能体”有本质区别。 5.  **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一种创新的“协同进化”框架，通过多智能体强化学习让LLM和其外部知识库共同成长。这是一种方法论上的创新，直接作用于增强LLM的基础设施——知识动态性。虽然它没有直接优化思维链（CoT）或数学推理的某个具体环节，但它通过解决知识时效性和覆盖度的根本问题，为LLM的通用推理能力提供了更强的支撑。这完全符合筛选那些“致力于提高大语言模型本身通用推理能力”论文的目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#59",
        "title": "Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers",
        "link": "/arxiv/2510.09017",
        "arxiv_id": "2510.09017",
        "authors": "Rui Bu, Haofeng Zhong, Wenzheng Chen, Yangyan Li",
        "summary": "Large models based on the Transformer architecture are susceptible to extreme-token phenomena, such as attention sinks and value-state drains. These issues, which degrade model performance, quantization fidelity, and interpretability, arise from a problematic mutual reinforcement mechanism where the model learns an inefficient 'no-op' behavior by focusing attention on tokens with near-zero value states. In this paper, we propose Value-State Gated Attention (VGA), a simple, dedicated, and stable architectural mechanism for performing 'no-op' attention efficiently by directly breaking this cycle. VGA introduces a learnable, data-dependent gate, computed directly from the value vectors (V), to modulate the output. Through a theoretical analysis of the underlying gradients, we show that gating the value-state with a function of itself is more effective at decoupling value and attention score updates than prior methods that gate on input embeddings. This creates a direct regulatory pathway that allows the model to suppress a token's contribution based on its emergent value representation. Our experiments demonstrate that VGA significantly mitigates the formation of attention sinks and stabilizes value-state norms, leading to improved performance, robust quantization fidelity, and enhanced model interpretability.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.567253",
        "filter_reason": "这篇论文符合我的研究范围。我的判断过程如下： **第一步：核心判断** 这篇论文的核心是针对Transformer架构本身提出一种改进。它识别并解决了一个名为“极端token现象”（如注意力汇聚）的底层架构问题，这个问题会“降低模型性能”。论文提出的“值状态门控注意力（VGA）”是一种直接的架构修改，旨在打破模型学习低效行为的循环。这完全符合筛选标准中“改进LLM的基础能力”的要求，因为它通过优化模型的核心计算机制来提升其整体表现，而不是将模型应用于某个特定领域。 **第二步与第三步：指标与排除项核对** - **正面指标**: 论文明确研究“Large models based on the Transformer architecture”，即LLM的核心架构。虽然摘要没有直接使用“reasoning”一词，但它解决的问题（模型性能下降）和带来的好处（性能提升）是所有高级能力（包括推理）的基石。一个性能更稳定、更高效的模型，其推理能力自然会更强。 - **排除标准**: 论文完全不涉及多模态、视觉、医疗、化学等任何特定应用领域，也未聚焦于水印、安全等应用层面的可靠性问题。它关注的是模型内部的、通用的计算机制。 **第四步：处理特殊和模糊情况** 论文提到了“enhanced model interpretability”和“robust quantization fidelity”。根据筛选标准，如果这些是提升模型内在质量所带来的“结果”，而非论文的“主要目标”，则应保留。本文的核心贡献是VGA这一架构机制，可解释性和量化保真度是验证该机制有效性的实验结果和附带好处。因此，这不应成为排除的理由。其本质是提出一种方法来提升模型的内在质量和稳定性，这与“提升模型的通用可靠性和推理质量”的目标是一致的。 **第五步：最终决策** 综合来看，这篇论文的本质是提出一种新的、通用的架构改进方法（VGA），用于解决Transformer模型内部的一个根本性问题。这种改进旨在提升模型的基础性能、稳定性和可解释性，从而为模型实现更强的通用推理能力奠定坚实的基础。它并非应用型研究，而是对LLM核心能力的直接增强。因此，这篇论文高度符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标。"
    },
    {
        "index": "#63",
        "title": "Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search",
        "link": "/arxiv/2510.08992",
        "arxiv_id": "2510.08992",
        "authors": "Kamel Alrashedy, Vriksha Srihari, Zulfiqar Zaidi, Ridam Srivastava, Pradyumna Tambwekar, Matthew Gombolay",
        "summary": "While researchers have made significant progress in enabling large language models (LLMs) to perform multi-step planning, LLMs struggle to ensure that those plans align with high-level user intent and satisfy symbolic constraints, especially in complex, multi-step domains. Existing reasoning approaches such as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and verifier-augmented methods, expand the search space but often yield infeasible actions or hallucinated steps. To overcome these limitations, we propose Constraints-of-Thought (Const-o-T), a framework that provides a structured prior that enables Monte Carlo Tree Search (MCTS) focus search on semantically meaningful paths. Each reasoning step is represented as an (intent, constraint) pair, which serves both to compress the search space and enforce validity. Unlike prior methods that merely generate reasoning traces or validate outputs post hoc, Const-o-T uses (intent, constraint)pairs to actively focus the search toward feasible and meaningful plans. We integrate Const-o-T into MCTS using a structured representation of intent-constraint pairs constraints prune infeasible branches and guide exploration toward semantically valid actions, improving planning efficiency and verifiable decision-making. We demonstrate across three domains Risk game, CAD code generation, and arithmetic reasoning that our approach outperforms baselines, yielding higher accuracy and stronger structural alignment. Our contribution is to demonstrate that Const-of-T offers a generalizable foundation for constraint-guided reasoning, enabling more efficient, constraint-aligned, and domain-adaptable planning with LLMs.",
        "subjects": "Machine Learning",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.569197",
        "filter_reason": "这篇论文完全符合您的筛选标准，是关于提升大语言模型通用推理能力的前沿研究。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为“Constraints-of-Thought (Const-o-T)”的新框架，旨在解决现有LLM推理方法（如CoT、ToT）在多步规划中无法满足符号约束和用户意图的局限性。这并非将LLM作为工具应用于特定领域，而是直接改进LLM的**基础推理和规划能力**，使其生成的计划更可行、更符合逻辑。因此，论文本质符合核心要求，应予以保留。 2.  **第二步：正面指标** 论文高度契合正面指标： - **核心概念**: 明确以 \"Large language models (LLMs)\" 为研究对象。 - **能力方向**: 聚焦于 \"reasoning\"（特别是约束推理）和 \"planning\"（多步规划）。 - **新兴范式**: 提出的框架是对现有思维链等推理范式的革新，旨在提升LLM作为问题解决者的能力，这与 \"llm-based agents\" 的核心目标一致。 3.  **第三步：排除标准** 论文未触发任何排除标准： - 它不涉及多模态、视觉或任何特定应用领域（如医疗、化学）。 - 虽然论文在Risk游戏、CAD代码生成等领域进行了实验验证，但其目的并非解决这些特定领域的问题，而是为了证明其框架的 \"generalizable foundation\"（可泛化的基础）和 \"domain-adaptable planning\"（领域自适应规划）能力。因此，它并未被排除。 - 它不关注模型基础设施或应用层面的安全性、水印等。 4.  **第四步：处理特殊和模糊情况** 论文提出的框架本质上是一种通用的、增强LLM问题解决能力的方法。它通过引入（意图，约束）对来引导搜索，这是一种提升模型内在逻辑严谨性和规划有效性的通用方法论，完全符合筛选标准中关于“通用智能体协作框架或工具使用方法”的保留条件。它解决的是“幻觉步骤”和“不可行行动”等内在推理缺陷，而非应用层面的讨论。 5.  **第五步：最终决策** 综合以上分析，该论文致力于通过提出一种新的推理框架来提升LLM的通用规划和约束满足能力，这直接服务于您“提高大语言模型本身通用推理能力”的核心研究目标。其贡献是方法论层面的，具有通用性和可泛化性，是典型的LLM基础能力增强研究。因此，最终判断为True。"
    },
    {
        "index": "#77",
        "title": "Pinpointing crucial steps: Attribution-based Credit Assignment for Verifiable Reinforcement Learning",
        "link": "/arxiv/2510.08899",
        "arxiv_id": "2510.08899",
        "authors": "Junxi Yin, Haisen Luo, Zhenyu Li, Yihua Liu, Dan Liu, Zequn Li, Xiaohang Xu",
        "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) enhances complex reasoning in LLMs, current methods struggle to balance exploration and exploitation. This leads to critical issues like inaccurate credit assignment for intermediate steps and premature entropy collapse, limiting model performance. To address this, we introduce Attribution-based Contribution to Policy Optimization (ACPO), a phased framework that incorporates a difficulty-aware curriculum. ACPO improves exploration by using trajectory semantic segmentation and an attribution-based representation to dynamically regulate policy entropy, thus mitigating its collapse. Concurrently, it enhances exploitation with a factorized reward system that precisely quantifies the hierarchical contribution of each reasoning step, ensuring accurate credit assignment. Extensive experiments on challenging benchmarks, including AIME, MATH, and AMC, demonstrate that ACPO significantly outperforms existing state-of-the-art approaches.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.576647",
        "filter_reason": "这篇论文完全符合我的研究范围，应予以保留。我的判断过程如下： 1.  **核心判断 (第一步)**: 论文的核心是提出一种新的强化学习框架（ACPO），旨在解决大语言模型在复杂推理任务中遇到的“信用分配不准确”和“过早熵崩溃”等根本性问题。这直接关系到如何改进LLM的**基础训练范式**和**多步推理能力**。论文的本质不是将LLM应用于某个领域，而是致力于优化LLM自身的推理过程，因此完全符合保留标准。 2.  **正面指标 (第二步)**: 论文高度匹配多个正面指标。 *   **核心概念**: 论文研究对象明确是LLMs。 *   **能力方向**: 论文聚焦于“complex reasoning”、“math reasoning”和“reasoning steps”，这正是我们关注的核心能力。 *   **训练方法**: 论文的核心贡献是“Reinforcement Learning with Verifiable Rewards (RLVR)”和新的“Attribution-based Contribution to Policy Optimization (ACPO)”框架，属于强化学习（RL）的范畴。 3.  **排除标准 (第三步)**: 论文不涉及任何排除标准中的领域。 *   它不涉及多模态、视觉或特定应用领域（如医疗、化学）。 *   虽然论文在AIME、MATH等数学基准上测试，但这是为了验证其方法在**复杂、可验证推理**任务上的通用有效性，而不是为了解决一个特定的数学应用问题。数学在这里是作为衡量通用推理能力的“试金石”，而非论文的应用终点。 4.  **特殊和模糊情况 (第四步)**: *   论文提出的方法ACPO通过“精确量化每个推理步骤的层次贡献”来确保准确的信用分配，这从根本上提升了模型推理过程的稳健性和准确性。这可以被看作是一种提升模型内在可靠性和推理质量的方法，因此即使从“幻觉/可解释性/安全”的角度看，它也偏向于应被保留的类别。 **最终决策 (第五步)**: 综合以上分析，这篇论文的核心贡献是提出了一种通用的、基于强化学习的新训练框架（ACPO），用以增强LLM在复杂、多步推理任务中的表现。它直接针对LLM推理过程中的关键瓶颈（信用分配），并通过在数学推理这一高难度基准上验证了其有效性。该研究完全符合“提高大语言模型本身的通用推理能力”这一核心目标，是一篇高质量的相关论文。"
    },
    {
        "index": "#108",
        "title": "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting",
        "link": "/arxiv/2510.08696",
        "arxiv_id": "2510.08696",
        "authors": "Yunzhen Feng, Parag Jain, Anthony Hartshorn, Yaqi Duan, Julia Kempe",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compute on negative groups: groups in which no sampled response is correct yield zero advantage and thus no gradient. We ask whether negative groups can be leveraged without extra supervision. Starting from a maximum-likelihood (MLE) objective in reward modeling, we show that the MLE gradient is equivalent to a policy gradient for a modified value function. This value function adds a confidence-weighted penalty on incorrect responses, imposing larger penalties on more confident mistakes. We refer to this as \\textbf{L}ikelihood \\textbf{E}stimation with \\textbf{N}egative \\textbf{S}amples (\\textbf{LENS}). LENS modifies GRPO to assign non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative and converting previously wasted samples into useful gradient updates. On the MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently outperforms GRPO baseline, with significant gains on harder items. These results demonstrate a principled and practical way to \"rescue\" negative groups, improving efficiency and performance in RLVR.",
        "subjects": "Machine Learning",
        "date": "2025-10-09",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.610161",
        "filter_reason": "这篇论文完全符合您的筛选标准，其核心贡献在于提出了一种新的训练范式来提升大语言模型的通用推理能力。 1.  **第一步：核心判断** - **保留**。该论文的本质是改进LLM的训练方法。它没有将LLM作为工具应用于某个特定领域，而是聚焦于强化学习（RL）这一核心训练技术本身。论文的核心贡献是提出了一种名为LENS的新方法，它改进了现有的GRPO算法，通过利用“负样本”（即完全错误的回答组）来提升模型在推理任务上的学习效率和性能。这直接对应了您标准中“提出新的训练范式”、“增强其逻辑、数学、规划、多步推理等通用能力”的要求。 2.  **第二步：正面指标** - 论文高度符合多个正面指标： - **核心概念**: 论文明确以Large language models (LLMs)为研究对象。 - **能力方向**: 研究目标是提升LLM在reasoning tasks上的表现，并在MATH benchmark上进行了验证，这正是数学推理能力的核心体现。 - **训练方法**: 论文的核心是关于Reinforcement learning (RL)，具体是对Group Relative Policy Optimization (GRPO)的改进。 3.  **第三步：排除标准** - 论文完全不涉及任何排除标准领域。它没有讨论多模态、视觉，也没有将模型应用于医疗、化学、机器人等特定领域，更不涉及水印、安全等应用层面的可靠性问题。 4.  **第四步：处理特殊和模糊情况** - 本论文不涉及智能体/工具使用或幻觉/可解释性等特殊情况，因此无需进行特殊判断。 5.  **第五步：最终决策** - **综合判断**：该论文是一项典型的、致力于提升LLM基础能力的研究。它通过一种创新的、基于理论推导的强化学习方法（LENS），解决了现有训练方法（GRPO）在利用错误样本上的不足，从而更高效地提升了模型在数学推理这一通用能力上的表现。这与您“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”的核心目标高度一致。因此，应保留。"
    },
    {
        "index": "#104",
        "title": "Transmuting prompts into weights",
        "link": "/arxiv/2510.08734",
        "arxiv_id": "2510.08734",
        "authors": "Hanna Mazzawi, Benoit Dherin, Michael Munn, Michael Wunder, Javier Gonzalvo",
        "summary": "A growing body of research has demonstrated that the behavior of large language models can be effectively controlled at inference time by directly modifying their internal states, either through vector additions to their activations or through updates to their weight matrices. These techniques, while powerful, are often guided by empirical heuristics, such as deriving steering vectors from the average activations of contrastive prompts. This work provides a theoretical foundation for these interventions, explaining how they emerge from the fundamental computations of the transformer architecture. Building on the recent finding that a prompt's influence can be mathematically mapped to implicit weight updates (Dherin et al., 2025), we generalize this theory to deep, multi-block transformers. We show how the information contained in any chunk of a user prompt is represented and composed internally through weight vectors and weight matrices. We then derive a principled method for condensing this information into token-independent thought vectors and thought matrices. These constructs provide a theoretical explanation for existing vector- and matrix-based model editing techniques and offer a direct, computationally-grounded method for transmuting textual input into reusable weight updates.",
        "subjects": "Machine Learning",
        "date": "2025-10-09",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.608985",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——论文的本质是什么？** 论文的核心贡献并非将LLM应用于特定领域，而是深入LLM的内部工作机制，为“如何通过修改内部状态（激活或权重）来控制模型行为”这一前沿方向提供了**理论基础**。它提出了一种将文本提示“转译”为可复用权重更新的原则性方法。这属于改进LLM基础能力和提出新训练/干预范式的研究，直接触及了模型如何“思考”和“响应”的根本问题，与提升其通用能力的目标高度一致。 2.  **第二步：正面指标——论文是否包含相关主题？** - **核心概念**: 论文明确以大语言模型为研究对象。 - **能力方向**: 虽然摘要未直接使用\"reasoning\"一词，但其核心概念“思维向量”和“思维矩阵”直接指向了模型的内部信息处理和决策过程。将提示（如思维链CoT）转化为权重更新，本质上是在探索一种更直接、更底层的引导和增强模型推理能力的方法。 - **新兴范式**: 这项研究为“模型编辑”和“推理时干预”等新兴范式提供了理论支撑，这些都是当前提升LLM通用能力的关键研究方向。 3.  **第三步：排除标准——论文是否主要聚焦于排除领域？** 论文完全不涉及多模态、特定应用领域（如医疗、化学）或应用层面的模型可靠性（如水印、安全）。它纯粹聚焦于LLM的内部计算原理。 4.  **第四步：处理特殊和模糊情况** 这篇论文可以被视为对“可解释性”的深度探索。它不是停留在现象描述，而是提供了计算层面的理论解释，阐明了提示信息如何在模型内部被表征和组合。这种对模型内在机理的深刻理解，是提升模型通用推理质量和可靠性的基石，因此完全符合保留条件。 **最终决策**: 该论文提出了一种将提示“转译”为权重更新的理论和方法，这为从根本上控制和增强LLM的内部处理过程（包括推理）开辟了新的途径。它是一项基础性的方法论研究，旨在提升LLM本身的通用能力，而非将其作为工具应用。因此，它精准地契合了您“致力于提高大语言模型（LLM）本身的『通用推理能力』”的核心目标。"
    },
    {
        "index": "#136",
        "title": "Localist LLMs -- A Mathematical Framework for Dynamic Locality Control",
        "link": "/arxiv/2510.09338",
        "arxiv_id": "2510.09338",
        "authors": "Joachim Diederich",
        "summary": "We present a novel framework for training large language models with continuously adjustable internal representations that span the full spectrum from localist (interpretable, rule-based) to distributed (generalizable, efficient) encodings. The key innovation is a locality dial, a tunable parameter that dynamically controls the degree of localization during both training and inference without requiring model retraining. This is achieved through group sparsity penalties on attention mechanisms, information-theoretic anchor design, and dynamic rule injection. We provide rigorous mathematical proofs establishing explicit threshold conditions under which attention provably concentrates on semantically relevant blocks, with exponential bounds on attention entropy and pointer fidelity. Specifically, we prove that when group sparsity penalties exceed certain threshold values, the model's attention mechanisms concentrate on semantically relevant blocks, achieving low entropy and high fidelity with negligible error. This framework enables practitioners to continuously interpolate between interpretable and high-performance modes, supporting applications in regulated domains requiring both transparency and capability.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.619151",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接指向提升大语言模型的基础能力，特别是与通用推理能力相关的方面。 **第一步：核心判断——论文的本质是改进LLM的基础能力。** 这篇论文的本质是提出一种新颖的训练和推理框架（\"locality dial\"），用于动态控制LLM内部表征的“局部性”。这并非将LLM作为工具应用于特定领域，而是直接作用于模型的核心机制——注意力机制和内部表征方式。通过让模型能够在“局部主义”（可解释、基于规则）和“分布式”（可泛化、高效）之间进行插值，该框架本质上是在探索并增强模型的一种基础能力：即进行结构化、规则化逻辑推理的能力。一个能够更倾向于“基于规则”进行思考和表征的模型，其逻辑推理能力的上限会更高。因此，这完全符合“改进LLM的基础能力、增强其逻辑、多步推理等通用能力”的保留标准。 **第二步：正面指标——论文高度相关。** - **核心概念**: 论文直接聚焦于 \"Large language models\"。 - **能力方向**: 虽然摘要未直接使用 \"reasoning\" 一词，但其核心思想“localist (interpretable, rule-based)”与逻辑推理和规则应用能力紧密相关。通过“group sparsity penalties”迫使注意力集中于“semantically relevant blocks”，并辅以“dynamic rule injection”，这些都是旨在增强模型结构化、逻辑化处理信息能力的具体技术手段，直接服务于提升推理质量。 - **训练方法**: 论文提出了全新的训练范式，即通过可调参数在训练和推理中动态控制模型行为，这是一种根本性的方法论创新。 **第三步：排除标准——论文未触犯排除红线。** - **多模态与视觉**: 论文完全未涉及视觉或多模态内容。 - **特定应用领域**: 论文提及了“regulated domains”作为潜在应用场景，但这只是为了说明其框架的价值（即可解释性），论文的**核心贡献**是框架本身，而非在某个特定领域的应用。这与“将LLM作为工具应用到特定领域”的排除标准有本质区别。 - **模型可靠性（应用层面）**: 论文不涉及水印、安全等应用层面的可靠性问题。 **第四步：处理特殊和模糊情况——论文是关于可解释性的基础研究。** 这篇论文是“可解释性”相关研究的绝佳范例，应被保留。它不是在讨论或评估LLM的可解释性现象，而是提出了一种**新的方法**（locality dial）来**主动控制和增强**模型的可解释性和规则遵循能力。这种对模型内部表征的主动塑造，直接关联到提升其内在的推理质量和逻辑严谨性，符合“提出一种新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量”的保留原则。 **第五步：最终决策** 综合分析，该论文提出了一种创新的数学框架和训练范式，通过动态控制模型的内部表征方式，旨在使其在保持泛化能力的同时，能够切换到一种更接近“可解释、基于规则”的模式。这直接触及了LLM通用推理能力的核心——逻辑与规则应用能力。因此，这是一篇与您研究课题“大语言模型通用推理能力”高度相关的前沿论文，应予以保留。"
    },
    {
        "index": "#158",
        "title": "PAC Reasoning: Controlling the Performance Loss for Efficient Reasoning",
        "link": "/arxiv/2510.09133",
        "arxiv_id": "2510.09133",
        "authors": "Hao Zeng, Jianguo Huang, Bingyi Jing, Hongxin Wei, Bo An",
        "summary": "Large reasoning models (LRMs) have achieved remarkable progress in complex problem-solving tasks. Despite this success, LRMs typically suffer from high computational costs during deployment, highlighting a need for efficient inference. A popular direction of efficiency improvement is to switch the LRM between thinking and nonthinking modes dynamically. However, such approaches often introduce additional reasoning errors and lack statistical guarantees for the performance loss, which are critical for high-stakes applications. In this work, we propose Probably Approximately Correct (PAC) reasoning that controls the performance loss under the user-specified performance loss tolerance. In particular, we construct an upper confidence bound on the performance loss, formulated as a monotone function of the uncertainty score, and subsequently determine a threshold for switching to the nonthinking model. Theoretically, using the threshold to switch between the thinking and nonthinking modes ensures bounded performance loss in a distribution-free manner. Our comprehensive experiments on reasoning benchmarks show that the proposed method can save computational budgets and control the user-specified performance loss.",
        "subjects": "Artificial Intelligence, Machine Learning, Statistics Theory",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.631063",
        "filter_reason": "这篇论文符合筛选要求，应被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的核心是提出一种名为“PAC Reasoning”的新方法，用于优化大型推理模型在执行推理任务时的效率。它并非将LLM作为工具应用于某个特定领域，也不是研究模型的基础设施或硬件加速。相反，它的核心贡献在于**改进模型执行推理过程的范式本身**。通过动态切换“思考”和“非思考”模式，并提供理论上的性能损失保证，该方法直接干预和优化了LLM的推理行为。这属于“提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力”的范畴，因为它关注的是推理过程的效率和可控性，这是通用推理能力的一个重要维度。 2.  **第二步：正面指标** 论文高度符合正面指标： - **核心概念**: 明确以“Large reasoning models (LRMs)”为研究对象，这是LLM的一个子集。 - **能力方向**: 论文的核心主题就是“reasoning”，并在“reasoning benchmarks”上进行验证，完全符合研究目标。 - **新兴范式**: 论文提出的“动态切换思考/非思考模式”是一种新颖的推理范式，旨在解决模型在部署时的实际问题，属于对模型推理能力的增强方法。 3.  **第三步：排除标准** 论文不涉及任何排除标准中列出的领域： - 它不涉及多模态、视觉等内容。 - 它使用的是通用的“reasoning benchmarks”，而非医疗、化学等特定领域的数据集。 - 它关注的是模型的“performance loss”（性能损失，即准确性），这是模型内在能力的体现，而非应用层面的水印、安全等问题。 4.  **第四步：处理特殊和模糊情况** 本论文的关键模糊点在于“效率提升”是否属于“部署优化”而被排除。我的判断是：**不属于**。论文的切入点并非系统层面的优化（如量化、剪枝、分布式计算），而是**算法层面的创新**。它提出了一种新的推理决策机制，并为其提供了统计学理论保证（PAC框架）。这种对模型“何时思考、何时不思考”的内在逻辑进行设计和控制，是对模型推理能力本身的直接增强，使其在保持性能的同时更具实用性和可控性。这完全符合“提高LLM本身的通用推理能力”的目标，因为它让推理这一能力变得更高效、更可靠。 5.  **第五步：最终决策** 综合以上分析，这篇论文的本质是提出一种新的、有理论保证的推理范式，以增强LLM在执行通用推理任务时的效率和可控性。它直接作用于模型的推理过程，而非外部应用或基础设施。因此，它精准地契合了“筛选出那些致力于提高大语言模型（LLM）本身的『通用推理能力』的论文”这一核心目标。"
    },
    {
        "index": "#169",
        "title": "RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows",
        "link": "/arxiv/2510.09021",
        "arxiv_id": "2510.09021",
        "authors": "Hamed Mahdavi, Pouria Mahdavinia, Samira Malek, Pegah Mohammadipour, Alireza Hashemi, Majid Daliri, Alireza Farhadi, Amir Khasahmadi, Niloofar Mireshghallah, Vasant Honavar",
        "summary": "State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based Olympiad problems to solving most of the IMO 2025 problems, with leading systems reportedly handling 5 of 6 problems. Given this progress, we assess how well these models can grade proofs: detecting errors, judging their severity, and assigning fair scores beyond binary correctness. We study proof-analysis capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we grade on a 1-4 scale with detailed error annotations, and on MathArena solution sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models can reliably flag incorrect (including subtly incorrect) solutions but exhibit calibration gaps in how partial credit is assigned. To address this, we introduce agentic workflows that extract and analyze reference solutions and automatically derive problem-specific rubrics for a multi-step grading process. We instantiate and compare different design choices for the grading workflows, and evaluate their trade-offs. Across our annotated corpus and MathArena, our proposed workflows achieve higher agreement with human grades and more consistent handling of partial credit across metrics. We release all code, data, and prompts/logs to facilitate future research.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.634563",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** 论文的标题和摘要初看似乎是在做一个应用（自动评分），但其核心贡献并非“将LLM应用于教育领域”，而是**提出了一种新的方法论来增强LLM的复杂推理能力**。论文的核心是“agentic workflows”（智能体工作流），这是一种让LLM通过多步骤、结构化的流程（提取参考方案、分析、推导评分标准、执行评分）来完成一个复杂分析任务的方法。这种工作流本身就是一种对LLM推理范式的创新，旨在提升模型进行细致、多步、有逻辑的分析和判断能力，这直接触及了“通用推理能力”的核心。因此，论文的本质是方法论创新，而非特定领域应用。 2.  **第二步：正面指标** 论文高度符合正面指标： *   **核心概念**: 明确以大语言模型为研究对象。 *   **能力方向**: 聚焦于数学推理，特别是对证明的深层分析，这属于高级推理能力的范畴。 *   **新兴范式**: 论文的核心贡献是“agentic workflows”，这完全符合“llm-based agents”这一前沿范式。它研究如何通过智能体框架来提升模型在复杂任务上的表现。 3.  **第三步：排除标准** 论文不触发排除标准： *   **特定应用领域**: 虽然论文的测试场景是数学证明评分，但数学推理通常被视为衡量通用推理能力的“基准领域”，而非像医疗、化学那样的封闭特定领域。论文提出的方法（智能体工作流）具有通用性，其目标是提升模型的分析和规划能力，而非仅仅解决数学问题。 *   **多模态**: 不涉及。 *   **模型可靠性**: 论文关注的是推理质量，而非水印、安全等应用层面的可靠性。 4.  **第四步：处理特殊和模糊情况** 论文完美地契合了“智能体/工具使用”的保留条件。它提出的“agentic workflows”是一种**通用的智能体协作框架**，旨在通过结构化的多步流程来增强LLM的通用问题解决能力。虽然该框架在“数学证明评分”这个具体任务上进行验证和展示，但其方法论本身是领域无关的，可以被迁移到其他需要复杂分析和规划的通用推理任务上。这与“用于化学实验自动化的智能体”有本质区别，后者的核心是化学领域的自动化，而本文的核心是智能体工作流这一通用推理范式。 **最终决策**: 综合来看，这篇论文的核心贡献是提出了一种名为“智能体工作流”的新范式，通过让LLM执行结构化的多步骤分析流程，显著提升了其在复杂任务（如细致的数学证明评分）上的推理表现。这项研究直接推动了LLM通用推理能力的前沿，完全符合“致力于提高大语言模型本身的通用推理能力”这一核心目标。因此，应判定为 **True**。"
    },
    {
        "index": "#173",
        "title": "RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation",
        "link": "/arxiv/2510.08931",
        "arxiv_id": "2510.08931",
        "authors": "Ashish Kattamuri, Harshwardhan Fartale, Arpita Vats, Rahul Raja, Ishita Prasad",
        "summary": "Data contamination poses a significant challenge to reliable LLM evaluation, where models may achieve high performance by memorizing training data rather than demonstrating genuine reasoning capabilities. We introduce RADAR (Recall vs. Reasoning Detection through Activation Representation), a novel framework that leverages mechanistic interpretability to detect contamination by distinguishing recall-based from reasoning-based model responses. RADAR extracts 37 features spanning surface-level confidence trajectories and deep mechanistic properties including attention specialization, circuit dynamics, and activation flow patterns. Using an ensemble of classifiers trained on these features, RADAR achieves 93\\% accuracy on a diverse evaluation set, with perfect performance on clear cases and 76.7\\% accuracy on challenging ambiguous examples. This work demonstrates the potential of mechanistic interpretability for advancing LLM evaluation beyond traditional surface-level metrics.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-13T11:00:06.635831",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。以下是我的详细判断过程： 1.  **第一步：核心判断** 论文的核心并非直接提出一种新的训练方法来“提高”LLM的推理能力，而是致力于解决一个更根本性的问题：如何“准确评估”LLM是否真正具备推理能力。论文明确指出，当前LLM评估中存在的“数据污染”问题，使得模型可能通过“记忆”而非“推理”来获得高分。因此，这篇论文的本质是提出一种新的评估框架（RADAR），用以区分模型是基于“回忆”还是基于“推理”来回答问题。对于一个致力于“提高大语言模型通用推理能力”的研究课题而言，能够**准确、可靠地衡量**推理能力是所有研究工作的前提和基石。如果一个评估方法本身不可靠，那么任何关于“能力提升”的结论都将站不住脚。因此，这篇关于评估方法论的研究，其目标直接服务于“提升推理能力”这一核心目标，属于基础性、支撑性的研究，应当被保留。 2.  **第二步：正面指标** 论文高度符合正面指标： - **核心概念**: 论文明确聚焦于“Large language models, LLMs”。 - **能力方向**: 论文的中心议题是“reasoning”，特别是区分“recall-based”与“reasoning-based”的响应，这直接触及了推理能力的本质。 - **新兴范式**: 论文虽然不直接研究Agent，但其使用的“mechanistic interpretability”（机制可解释性）方法是当前理解LLM内部工作原理、探索其能力来源的前沿范式，与“deep research”的精神相符。 3.  **第三步：排除标准** 论文不涉及任何排除标准领域： - 它不涉及多模态或视觉。 - 它并非针对医疗、化学等特定应用领域，而是通用性的评估方法。 - 它虽然涉及“reliability”（可靠性），但并非应用层面的“Watermarking, Safety, Security”，而是评估可靠性。 4.  **第四步：处理特殊和模糊情况** 本论文是“可解释性/可靠性”领域的一个典型“应保留”案例。根据筛选标准：“如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量，应该保留。” - RADAR框架正是提出了一种“新方法”（基于机制可解释性）。 - 其目的是“检测数据污染”，这可以类比为一种识别“高级别记忆/幻觉”的方法。 - 通过区分“回忆”与“推理”，它直接“增强了模型内在的可解释性”（让我们知道模型是怎么得出答案的）。 - 最终目标是“advancing LLM evaluation beyond traditional surface-level metrics”（推进LLM评估超越传统的表层指标），这最终将“提升模型的通用可靠性和推理质量”的评估水平。 5.  **第五步：最终决策** 综合分析，这篇论文的核心贡献是提供了一把能够精准度量“真正推理能力”的“尺子”。它没有直接去“造一栋更高的楼”，但它提供了一个更准确的“测量工具”，确保我们知道未来的楼是否真的建得更高。对于任何严肃的、旨在提升LLM推理能力的研究项目来说，这样的评估基础论文是不可或缺的。因此，它完全符合你的研究范围。"
    },
    {
        "index": "#9",
        "title": "Fundamentals of Building Autonomous LLM Agents",
        "link": "/arxiv/2510.09244",
        "arxiv_id": "2510.09244",
        "authors": "Victor de Lamo Castrillo, Habtom Kahsay Gidey, Alexander Lenz, Alois Knoll",
        "summary": "This paper reviews the architecture and implementation methods of agents powered by large language models (LLMs). Motivated by the limitations of traditional LLMs in real-world tasks, the research aims to explore patterns to develop \"agentic\" LLMs that can automate complex tasks and bridge the performance gap with human capabilities. Key components include a perception system that converts environmental percepts into meaningful representations; a reasoning system that formulates plans, adapts to feedback, and evaluates actions through different techniques like Chain-of-Thought and Tree-of-Thought; a memory system that retains knowledge through both short-term and long-term mechanisms; and an execution system that translates internal decisions into concrete actions. This paper shows how integrating these systems leads to more capable and generalized software bots that mimic human cognitive processes for autonomous and intelligent behavior.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.AI",
        "crawl_time": "2025-10-13T11:00:06.630208",
        "filter_reason": "这篇论文完全符合研究范围，应被保留。我的判断过程如下： 1.  **第一步（核心判断）：论文的核心贡献是构建通用LLM智能体的框架。** 论文的本质不是将LLM应用在某个特定领域，而是探讨如何从根本上增强LLM自身的能力，使其能够胜任更复杂的任务。它提出了一个包含感知、推理、记忆和执行四大系统的通用架构，旨在将传统的LLM升级为能够自主规划和解决问题的“智能体化”LLM。这直接回应了“改进LLM的基础能力、增强其逻辑、规划、多步推理等通用能力”的核心要求，属于方法论和框架层面的研究，因此符合保留条件。 2.  **第二步（正面指标）：论文高度匹配多个正面指标。** - **核心概念**: 论文主题是 \"LLM Agents\"，与 \"Large language models\" 紧密相关。 - **能力方向**: 论文明确提到了 \"reasoning system\"、\"formulates plans\"，并引用了 \"Chain-of-Thought\" 和 \"Tree-of-Thought\" 等具体技术，这些都是通用推理能力（特别是规划和问题解决）的核心体现。 - **新兴范式**: 论文本身就是对 \"llm-based agents\" 这一新兴范式的综述和构建基础的探讨，这正是当前提升LLM通用能力的前沿方向。 3.  **第三步（排除标准）：论文未触及任何排除标准。** 论文聚焦于通用的智能体架构，不涉及多模态、视觉，也没有限定在医疗、化学等特定应用领域。同时，它也不属于模型基础设施、部署优化或应用层面的安全（如水印）研究。 4.  **第四步（特殊和模糊情况）：论文完美符合“智能体/工具使用”的保留规则。** 根据规则，“如果是提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留。” 这篇论文正是如此。它提出的是一个通用的、旨在创造“更通用化软件机器人”的框架，其目标是“自动化复杂任务”和“弥合与人类能力的差距”，而不是解决某个特定领域的问题。因此，它完全符合保留条件。 **最终决策**：综合以上分析，该论文的核心贡献在于提出一个通用的框架，通过赋予LLM感知、推理、记忆和执行的能力，系统性地提升其自主规划和解决复杂问题的通用推理能力。这与研究课题“大语言模型通用推理能力”的目标高度一致，因此最终判断为 **True**。"
    },
    {
        "index": "#7",
        "title": "Toward Mechanistic Explanation of Deductive Reasoning in Language Models",
        "link": "/arxiv/2510.09340",
        "arxiv_id": "2510.09340",
        "authors": "Davide Maltoni, Matteo Ferrara",
        "summary": "Recent large language models have demonstrated relevant capabilities in solving problems that require logical reasoning; however, the corresponding internal mechanisms remain largely unexplored. In this paper, we show that a small language model can solve a deductive reasoning task by learning the underlying rules (rather than operating as a statistical learner). A low-level explanation of its internal representations and computational circuits is then provided. Our findings reveal that induction heads play a central role in the implementation of the rule completion and rule chaining steps involved in the logical inference required by the task.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.AI",
        "crawl_time": "2025-10-13T11:00:06.629309",
        "filter_reason": "这篇论文完全符合研究范围。 **判断过程如下:** 1.  **第一步：核心判断** 论文的核心并非将LLM作为工具应用于特定领域，而是深入探究LLM在进行“演绎推理”这一通用能力时的“内部机制”。它试图回答“LLM是如何进行逻辑推理的？”这一根本性问题。虽然它没有提出一种新的训练范式来直接提升模型性能，但这种对模型内部工作原理的“机制性解释”是理解和改进LLM通用推理能力的基石。它直接服务于“增强其逻辑、多步推理等通用能力”这一核心目标，因此属于保留范围。 2.  **第二步：正面指标** 论文高度匹配多个正面指标： *   **核心概念**: 明确以 \"language models\" 为研究对象。 *   **能力方向**: 核心聚焦于 \"deductive reasoning\"（演绎推理）和 \"logical reasoning\"（逻辑推理），这正是通用推理能力的核心组成部分。 3.  **第三步：排除标准** 论文完全避开了所有排除标准： *   它不涉及多模态、视觉等内容。 *   它研究的是通用的演绎推理任务，而非医疗、化学等特定应用领域。 *   它不讨论水印、安全等模型可靠性问题。 4.  **第四步：处理特殊和模糊情况** 这篇论文恰好命中了“可解释性”这一特殊情况。摘要中提到，论文提供了“对其内部表征和计算电路的低层次解释”，并揭示了“归纳头”在逻辑推理中的核心作用。这正是一种“增强模型内在的可解释性”的方法。通过理解推理是如何在模型内部实现的（例如，通过规则补全和规则链接），未来的研究才能更有针对性地去设计和优化模型，从而“提升模型的通用可靠性和推理质量”。因此，这种基础性的、机制层面的可解释性研究，完全符合保留条件。 **最终决策:** 综合来看，这篇论文的核心贡献在于揭示了LLM执行通用逻辑推理的内在计算机制。它不是应用层面的研究，而是对LLM核心能力本身的基础性剖析。这种深刻的理解是未来所有旨在“提高LLM通用推理能力”的研究工作的理论前提和重要参考。因此，它精准地契合了您的研究目标。"
    },
    {
        "index": "#25",
        "title": "DualResearch: Entropy-Gated Dual-Graph Retrieval for Answer Reconstruction",
        "link": "/arxiv/2510.08959",
        "arxiv_id": "2510.08959",
        "authors": "Jinxin Shi, Zongsheng Cao, Runmin Ma, Yusong Hu, Jie Zhou, Xin Li, Lei Bai, Liang He, Bo Zhang",
        "summary": "The deep-research framework orchestrates external tools to perform complex, multi-step scientific reasoning that exceeds the native limits of a single large language model. However, it still suffers from context pollution, weak evidentiary support, and brittle execution paths. To address these issues, we propose DualResearch, a retrieval and fusion framework that matches the epistemic structure of tool-intensive reasoning by jointly modeling two complementary graphs: a breadth semantic graph that encodes stable background knowledge, and a depth causal graph that captures execution provenance. Each graph has a layer-native relevance function, seed-anchored semantic diffusion for breadth, and causal-semantic path matching with reliability weighting for depth. To reconcile their heterogeneity and query-dependent uncertainty, DualResearch converts per-layer path evidence into answer distributions and fuses them in log space via an entropy-gated rule with global calibration. The fusion up-weights the more certain channel and amplifies agreement. As a complement to deep-research systems, DualResearch compresses lengthy multi-tool execution logs into a concise reasoning graph, and we show that it can reconstruct answers stably and effectively. On the scientific reasoning benchmarks HLE and GPQA, DualResearch achieves competitive performance. Using log files from the open-source system InternAgent, its accuracy improves by 7.7% on HLE and 6.06% on GPQA.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.AI",
        "crawl_time": "2025-10-13T11:00:06.648574",
        "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **核心判断（第一步）**: 论文的核心是提出一种名为“DualResearch”的**检索与融合框架**，旨在解决基于工具的“深度研究”框架中的问题。这个框架本身并不是在训练或微调LLM的内部参数，但它致力于**优化和增强LLM在执行复杂、多步推理任务时的过程和结果**。它通过构建语义图和因果图来更好地组织和融合外部工具产生的信息，最终目标是提升LLM驱动系统的推理质量。这属于“提出新的训练范式、增强其逻辑、规划、多步推理等通用能力”的范畴，特别是“智能体协作框架、工具使用”这一新兴范式。因此，从本质上讲，它是在提升LLM的通用推理能力，而不是将其作为特定领域的工具。 2.  **正面指标（第二步）**: 论文高度符合多个正面指标。 *   **核心概念**: 论文的研究背景是“deep-research framework”，其核心驱动力是LLM。 *   **能力方向**: 论文明确聚焦于“complex, multi-step scientific reasoning”，这直接对应“reasoning”和“problem-solving”。 *   **新兴范式**: 论文的研究对象是“deep-research”系统，并且使用了“InternAgent”的日志，这完全属于“llm-based agents”和“tool use”的范畴。 3.  **排除标准（第三步）**: 论文完全不符合排除标准。 *   它不涉及多模态与视觉。 *   虽然在“科学推理基准”上测试，但其方法本身是通用的，并未限定于医疗、化学等特定应用领域。 *   它不关注水印或安全等应用层面的可靠性问题。 4.  **特殊和模糊情况（第四步）**: *   **智能体/工具使用**: 这是判断此论文的关键。论文提出的“DualResearch”框架正是一个**通用的智能体推理增强框架**。它并非为“化学实验自动化”等特定领域设计，而是为了解决所有工具密集型推理任务中普遍存在的“上下文污染”、“证据支持薄弱”和“执行路径脆弱”问题。它通过压缩执行日志、构建推理图来增强LLM驱动的智能体的通用问题解决能力，完全符合“应该保留”的条件。 *   **幻觉/可解释性/可靠性**: 论文通过“深度因果图”来捕获“执行溯源”，并提供“可靠性加权”，这本质上是在**提升推理过程的可解释性和内在可靠性**，从而增强推理质量。这也符合“应该保留”的条件。 5.  **最终决策（第五步）**: 综合以上分析，这篇论文的核心贡献是提出了一种新颖的框架，用以优化LLM在工具使用场景下的多步推理过程。它通过改进信息融合和证据链构建的方式，直接提升了LLM驱动系统的推理能力和可靠性。这完全符合“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标，特别是针对当前前沿的智能体与工具使用范式。因此，应将其保留。"
    },
    {
        "index": "#26",
        "title": "EcphoryRAG: Re-Imagining Knowledge-Graph RAG via Human Associative Memory",
        "link": "/arxiv/2510.08958",
        "arxiv_id": "2510.08958",
        "authors": "Zirui Liao",
        "summary": "Cognitive neuroscience research indicates that humans leverage cues to activate entity-centered memory traces (engrams) for complex, multi-hop recollection. Inspired by this mechanism, we introduce EcphoryRAG, an entity-centric knowledge graph RAG framework. During indexing, EcphoryRAG extracts and stores only core entities with corresponding metadata, a lightweight approach that reduces token consumption by up to 94\\% compared to other structured RAG systems. For retrieval, the system first extracts cue entities from queries, then performs a scalable multi-hop associative search across the knowledge graph. Crucially, EcphoryRAG dynamically infers implicit relations between entities to populate context, enabling deep reasoning without exhaustive pre-enumeration of relationships. Extensive evaluations on the 2WikiMultiHop, HotpotQA, and MuSiQue benchmarks demonstrate that EcphoryRAG sets a new state-of-the-art, improving the average Exact Match (EM) score from 0.392 to 0.474 over strong KG-RAG methods like HippoRAG. These results validate the efficacy of the entity-cue-multi-hop retrieval paradigm for complex question answering.",
        "subjects": "Artificial Intelligence, Information Retrieval",
        "date": "2025-10-10",
        "category": "cs.AI",
        "crawl_time": "2025-10-13T11:00:06.649000",
        "filter_reason": "这篇论文符合筛选标准，应该被保留。 1.  **第一步：核心判断** 这篇论文的本质是提出一种新的、名为EcphoryRAG的RAG（检索增强生成）框架。其核心贡献并非将LLM应用于特定领域，而是通过改进检索机制来提升LLM处理复杂问题的能力。论文的核心是“动态推断隐式关系”和“多跳关联搜索”，其直接目标是实现“深度推理”。这属于一种为增强LLM通用能力而提出的新方法论，与改进模型基础能力的目标一致，因此应予以保留。它不是关于模型基础设施或特定领域应用的。 2.  **第二步：正面指标** 论文摘要中明确包含了多个高相关度的正面指标： *   **能力方向**: 直接提到了 \"complex, multi-hop recollection\"（复杂的多跳回忆）, \"deep reasoning\"（深度推理）, 和 \"complex question answering\"（复杂问答）。这些都属于通用推理能力的核心范畴。 *   **新兴范式**: RAG本身是增强LLM能力的重要新兴范式。本文提出的Knowledge-Graph RAG是该范式下的一个创新，旨在解决多步推理中的知识整合问题。 3.  **第三步：排除标准** 论文的主要焦点完全不涉及任何排除标准中的领域。它是一个纯文本框架，没有涉及多模态、视觉、特定应用领域（如医疗、化学）或模型可靠性（如水印、安全）。因此，排除标准不适用。 4.  **第四步：处理特殊和模糊情况** 这篇论文可以被看作是一种高级的“工具使用”。在这里，知识图谱（KG）被用作增强LLM推理能力的工具。论文提出的是一个通用的框架，旨在提升在通用多跳问答基准（如HotpotQA）上的表现，而非应用于特定领域。根据筛选标准“提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力，应该保留”，这篇论文符合保留条件。其动态推断关系的能力，本质上是通过一种新颖的外部信息整合方式，来弥补和增强LLM自身的逻辑推理链条。 5.  **第五步：最终决策** 综合分析，这篇论文的核心贡献是提出了一种创新的RAG框架，通过模仿人类联想记忆的机制，实现更高效、更深入的多跳知识检索与整合。其最终目的和验证效果都集中在提升LLM的“深度推理”和“复杂问题解决”能力上。这是一种旨在增强LLM通用推理能力的、通用的方法论研究，与研究课题“大语言模型通用推理能力”高度契合。因此，最终判断为 **True**。"
    },
    {
        "index": "#34",
        "title": "COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context",
        "link": "/arxiv/2510.08790",
        "arxiv_id": "2510.08790",
        "authors": "Guangya Wan, Mingyang Ling, Xiaoqi Ren, Rujun Han, Sheng Li, Zizhao Zhang",
        "summary": "Long-horizon tasks that require sustained reasoning and multiple tool interactions remain challenging for LLM agents: small errors compound across steps, and even state-of-the-art models often hallucinate or lose coherence. We identify context management as the central bottleneck -- extended histories cause agents to overlook critical evidence or become distracted by irrelevant information, thus failing to replan or reflect from previous mistakes. To address this, we propose COMPASS (Context-Organized Multi-Agent Planning and Strategy System), a lightweight hierarchical framework that separates tactical execution, strategic oversight, and context organization into three specialized components: (1) a Main Agent that performs reasoning and tool use, (2) a Meta-Thinker that monitors progress and issues strategic interventions, and (3) a Context Manager that maintains concise, relevant progress briefs for different reasoning stages. Across three challenging benchmarks -- GAIA, BrowseComp, and Humanity's Last Exam -- COMPASS improves accuracy by up to 20% relative to both single- and multi-agent baselines. We further introduce a test-time scaling extension that elevates performance to match established DeepResearch agents, and a post-training pipeline that delegates context management to smaller models for enhanced efficiency.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-13T11:00:06.657950",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献是提升大语言模型智能体的通用推理能力。我的判断过程如下： 1.  **第一步：核心判断——论文本质是提升LLM基础能力。** 论文的核心是解决LLM智能体在执行“长时程任务”时遇到的根本性挑战：推理连贯性差、错误累积、上下文管理瓶颈。它提出的COMPASS框架，是一种全新的、轻量级的分层架构，旨在通过优化智能体的内部工作机制（战术执行、战略监督、上下文组织）来增强其“持续推理”和“规划”能力。这完全属于“改进LLM的基础能力”和“增强其逻辑、规划、多步推理等通用能力”的范畴，而不是将LLM作为工具应用于某个特定领域。 2.  **第二步：正面指标——论文高度相关。** 论文摘要中包含了多个关键的正面指标： *   **核心概念**: 明确以 \"LLM agents\" 为研究对象。 *   **能力方向**: 核心聚焦于 \"Long-horizon reasoning\", \"sustained reasoning\", \"planning\", \"replan\", \"reflect\"，这些都是通用推理能力的核心体现。 *   **新兴范式**: 论文本身就是一个 \"multi-agent systems\" 的研究，涉及 \"tool use\"，并与 \"DeepResearch agents\" 进行比较，这些都是当前提升LLM能力的前沿范式。 3.  **第三步：排除标准——论文不涉及任何排除领域。** 论文的研究内容与多模态、视觉、医疗、化学、机器人等特定应用领域完全无关。它关注的是通用的推理框架，并在通用基准（GAIA, BrowseComp, Humanity's Last Exam）上进行验证。同时，它虽然提到了“幻觉”，但将其作为推理过程中的一个待解决的技术问题，而非研究水印、安全等应用层面的可靠性。 4.  **第四步：处理特殊和模糊情况——论文属于应保留的情况。** *   **智能体/工具使用**: 论文提出的COMPASS是一个“通用的智能体协作框架”，其目标是增强LLM的“通用问题解决能力”，而非应用于特定领域。因此，完全符合保留标准。 *   **幻觉/可解释性**: 论文通过改进上下文管理和引入战略监督来减少错误累积和幻觉，这是一种从模型内部推理机制入手提升其“通用可靠性和推理质量”的新方法，符合保留标准。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种新颖的架构（COMPASS），通过分离战术、战略和上下文管理，有效解决了LLM智能体在长时程推理中的核心瓶颈。这直接且有力地推动了“大语言模型通用推理能力”这一研究目标的前沿。因此，这篇论文应被**保留**。"
    },
    {
        "index": "#10",
        "title": "RegexPSPACE: A Benchmark for Evaluating LLM Reasoning on PSPACE-complete Regex Problems",
        "link": "/arxiv/2510.09227",
        "arxiv_id": "2510.09227",
        "authors": "Hyundong Jin, Joonghyuk Hahn, Yo-Sub Han",
        "summary": "Large language models (LLMs) show strong performance across natural language processing (NLP), mathematical reasoning, and programming, and recent large reasoning models (LRMs) further emphasize explicit reasoning. Yet their computational limits, particularly spatial complexity constrained by finite context windows, remain poorly understood. While recent works often focus on problems within the NP complexity class, we push the boundary by introducing a novel benchmark grounded in two PSPACE-complete regular expression (regex) problems: equivalence decision (RegexEQ) and minimization (RegexMin). PSPACE-complete problems serve as a more rigorous standard for assessing computational capacity, as their solutions require massive search space exploration. We perform a double-exponential space exploration to construct a labeled dataset of over a million regex instances with a sound filtering process to build the benchmark. We conduct extensive evaluations on 6 LLMs and 5 LRMs of varying scales, revealing common failure patterns such as verbosity and repetition. With its well-defined structure and quantitative evaluation metrics, this work presents the first empirical investigation into the spatial computational limitations of LLMs and LRMs, offering a new framework for evaluating their advanced reasoning capabilities. Our code is available at https://github.com/hyundong98/RegexPSPACE .",
        "subjects": "Artificial Intelligence, Formal Languages and Automata Theory",
        "date": "2025-10-10",
        "category": "cs.AI",
        "crawl_time": "2025-10-13T11:00:06.630673",
        "filter_reason": "这篇论文符合我的研究范围。 以下是详细的判断过程: 1.  **第一步：核心判断——这篇论文的本质是什么？** 这篇论文的本质不是提出一种直接“改进”LLM推理能力的训练方法（如新的CoT或RL算法），而是提出了一种全新的、更严格的**评估基准**来衡量LLM的推理能力极限。然而，这种评估工作本身是“提高LLM通用推理能力”这一宏大课题中不可或缺的一环。没有精确、有挑战性的“尺子”，我们就无法衡量现有模型的不足，也无法验证未来改进方法的有效性。论文的核心贡献是创建了一个基于PSPACE计算复杂度的基准，旨在“评估其高级推理能力”和“调查空间计算局限性”，这直接服务于理解和提升LLM通用推理能力的最终目标。因此，它属于研究范围内的基础性工作，应予以保留。 2.  **第二步：正面指标——论文是否包含以下主题？** - **核心概念**: 论文明确以“Large language models (LLMs)”和“large reasoning models (LRMs)”为研究对象。 - **能力方向**: 论文的核心主题就是“reasoning”，特别是更高维度的“计算推理”和“空间复杂性推理”，这远超于一般的数学或逻辑推理。摘要中反复强调“mathematical reasoning”、“explicit reasoning”、“advanced reasoning capabilities”。 - **训练方法**: 不涉及。 - **新兴范式**: 不涉及。 论文满足了最关键的两个指标，紧扣“LLM”和“推理”两大核心。 3.  **第三步：排除标准——论文是否主要聚焦于以下领域？** - **多模态与视觉**: 完全不涉及。 - **特定应用领域**: 论文使用的“正则表达式”问题源于理论计算机科学，是一个高度抽象和通用的计算问题，而非医疗、化学、金融等特定应用领域。它被用作衡量普适性计算能力的探针，而非解决某个领域问题的工具。 - **模型可靠性（应用层面）**: 论文关注的是模型内在的“计算极限”，而非水印、安全等应用层面的可靠性问题。 因此，论文未触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** 此处不涉及智能体或安全等问题，主要的模糊点在于“评估”与“改进”的关系。如第一步所述，为通用推理能力设定一个更高标准（从NP问题到PSPACE问题）的评估框架，本身就是推动该领域向前发展的关键一步。它揭示了当前模型的短板（“verbosity and repetition”），为未来的研究方向指明了道路。因此，应将此视为对“提高通用推理能力”这一目标的直接贡献。 5.  **第五步：最终决策** 综合来看，该论文通过创建一个基于PSPACE完备问题的基准，为衡量和推动LLM的“通用推理能力”提供了一个更严格、更深刻的视角。它的核心贡献是“评估”，但这种评估是“改进”的前提和方向标，是整个研究链条中至关重要的一环。论文精准地聚焦于LLM的内在推理极限，而非将其作为工具应用，完全符合我的筛选要求。"
    },
    {
        "index": "#148",
        "title": "RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution",
        "link": "/arxiv/2510.08665",
        "arxiv_id": "2510.08665",
        "authors": "Aofan Liu, Haoxuan Li, Bin Wang, Ao Yang, Hui Li",
        "summary": "Code generation models based on large language models (LLMs) have gained wide adoption, but challenges remain in ensuring safety, accuracy, and controllability, especially for complex tasks. Existing methods often lack dynamic integration of external tools, transparent reasoning, and user control over safety. To address these issues, we propose a controllable code generation framework utilizing the ReAct paradigm for multi-agent task execution. This framework is a multi-agent system designed to enable efficient, precise, and interpretable code generation through dynamic interactions between LLMs and external resources. The framework adopts a collaborative architecture comprising four specialized agents: a Planner for task decomposition, a Searcher that leverages the ReAct framework for reasoning and tool integration, a CodeGen agent for accurate code generation, and an Extractor for structured data retrieval. The ReAct-based Searcher alternates between generating reasoning traces and executing actions, facilitating seamless integration of internal knowledge with external tools (such as search engines) to enhance accuracy and user control. Experimental results show the framework's effectiveness across multiple languages, achieving a 94.8% security rate on the SVEN dataset with CodeQL, outperforming existing approaches. Its transparent reasoning process fosters user trust and improves controllability.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-13T11:00:06.706874",
        "filter_reason": "这篇论文符合我的研究范围，应予以保留。判断依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出一个名为“RA-Gen”的**多智能体框架**，旨在通过ReAct范式来增强LLM在复杂任务中的推理和执行能力。其本质并非将LLM作为工具应用于某个外部领域（如医疗、化学），而是**改进LLM自身在执行“代码生成”这一复杂认知任务时的方法论**。代码生成被视为衡量和提升LLM逻辑、规划和多步推理能力的重要基准任务。因此，该论文致力于提升LLM的通用推理能力，符合保留标准。 2.  **正面指标（第二步）：** 论文与多个正面指标高度契合。 *   **核心概念**: 明确基于 \"large language models (LLMs)\"。 *   **能力方向**: 核心聚焦于 **reasoning**（通过ReAct框架生成推理痕迹）、**planning**（通过Planner智能体进行任务分解）和 **problem-solving**（多智能体协作执行任务）。 *   **新兴范式**: 论文的核心是 **llm-based agents** 和 **multi-agent systems**，并明确讨论了 **tool use**（集成搜索引擎等外部工具）。 3.  **排除标准与特殊情况处理（第三、四步）：** *   **特定应用领域**: 论文的研究领域是“代码生成”，但这应被视为一个基础的人工智能能力测试场，而非像金融或法律那样的垂直应用领域。提升代码生成能力直接关联到提升模型的逻辑推理、规划和遵循指令等通用能力。 *   **智能体/工具使用**: 论文提出的框架是一个**通用的多智能体协作与工具使用方法**，旨在提升模型在复杂任务中的通用问题解决能力。它不是为特定领域（如“用于化学实验的智能体”）设计的，因此符合保留条件。 *   **模型可靠性**: 论文提到了“安全”和“可控性”。根据第四步的规则，这不应成为排除理由。因为论文并非单纯研究应用层的安全水印或对抗攻击，而是**通过提出一种新框架（ReAct+多智能体）来增强推理过程的透明度和可控性，从而提升模型内在的可靠性和推理质量**。这种对可解释性和可控性的内在提升，正是增强通用推理能力的一部分。 **核心依据总结**: 该论文的核心贡献在于提出了一种新的方法论框架（基于ReAct的多智能体系统），通过结构化的推理、规划和动态工具集成，来系统性地提升LLM在复杂任务中的表现。这直接命中了“致力于提高大语言模型（LLM）本身的『通用推理能力』”这一核心目标，因此应被筛选入内。"
    }
]