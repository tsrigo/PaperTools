[
    {
        "index": "#8",
        "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
        "link": "/arxiv/2510.18866",
        "arxiv_id": "2510.18866",
        "authors": "Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang",
        "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.",
        "subjects": "Computation and Language, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning, Multiagent Systems",
        "date": "2025-10-21",
        "category": "cs.MA",
        "crawl_time": "2025-10-22T11:00:04.625506",
        "filter_reason": "这篇论文符合研究范围，应予以保留。判断依据如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是构建了一个名为 `LightMem` 的新记忆系统。记忆（Memory）是LLM智能体（Agentic AI）的核心能力之一，它使智能体能够超越无状态的交互，在动态和复杂环境中利用历史信息。因此，这篇论文的本质是关于**改进LLM智能体的一个关键组件（记忆）**，属于“构建、改进LLM智能体”的范畴，符合保留标准。 2.  **第二步：正面指标——高度相关** 论文明确包含了多个核心关注点： *   **智能体能力**: 论文的核心是 `Memory` 系统。它通过引入持久化信息存储、检索和利用机制，使LLM具备了处理历史信息的能力，这是智能体进行规划和长期任务执行的基础。 *   **核心范式**: 论文的研究内容直接服务于 `Agentic AI` 和 `LLM-based Agents`，因为它解决了智能体在动态环境中的一个根本性挑战。 3.  **第三步：排除标准——不适用** 论文的主要贡献并非关于安全、对齐、可解释性或多模态。它专注于提升智能体记忆机制的效率和性能，因此不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况——不适用** 论文不涉及推理/规划的模糊情况，也不是一个特定领域的应用。它提出的是一个通用的、基础性的智能体组件改进方案。 **核心依据总结**: 该论文的核心贡献是 `LightMem`，一个为LLM设计的轻量级、高效的记忆系统。记忆是定义一个LLM是否为“智能体”（Agent）的关键能力之一，它赋予了智能体状态和上下文学习能力。这篇论文并非将LLM作为工具应用，也非提升LLM的基础推理能力，而是直接**构建和改进了LLM智能体的核心基础设施（记忆模块）**。因此，它完全符合“构建、改进或演化LLM智能体”这一核心研究目标。"
    },
    {
        "index": "#3",
        "title": "Socialized Learning and Emergent Behaviors in Multi-Agent Systems based on Multimodal Large Language Models",
        "link": "/arxiv/2510.18515",
        "arxiv_id": "2510.18515",
        "authors": "Sureyya Akin, Shruti T. Tiwari, Ram Bhattacharya, Sagar A. Raman, Kiran Mohanty, Sita Krishnan",
        "summary": "This search introduces the Multimodal Socialized Learning Framework (M-S2L), designed to foster emergent social intelligence in AI agents by integrating Multimodal Large Language Models (M-LLMs) with social learning mechanisms. The framework equips agents with multimodal perception (vision and text) and structured action capabilities, enabling physical manipulation and grounded multimodal communication (e.g., text with visual pointers). M-S2L combines direct reinforcement learning with two novel social learning pathways: multimodal observational learning and communication-driven learning from feedback, augmented by an episodic memory system for long-term social context. We evaluate M-S2L in a Collaborative Assembly Environment (CAE), where agent teams must construct complex devices from ambiguous blueprints under informational asymmetry. Across tasks of increasing complexity, M-S2L agents consistently outperform Text-Only and No-Social-Learning baselines in Task Completion Rate and Time to Completion, particularly in dynamic problem-solving scenarios. Ablation studies confirm the necessity of both multimodality and socialized learning. Our analysis reveals the emergence of efficient communication protocols integrating visual pointers with concise text, alongside rapid role specialization leading to stable labor division. Qualitative case studies demonstrate agents' abilities for shared awareness, dynamic re-planning, and adaptive problem-solving, suggesting a nascent form of machine social cognition. These findings indicate that integrating multimodal perception with explicit social learning is critical for developing human-like collaborative intelligence in multi-agent systems.",
        "subjects": "Multiagent Systems",
        "date": "2025-10-21",
        "category": "cs.MA",
        "crawl_time": "2025-10-22T11:00:04.623995",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——保留** 这篇论文的本质是构建一个新的多智能体系统框架。其核心贡献是提出了“多模态社交学习框架 (M-S2L)”，这是一个旨在促进AI智能体涌现社交智能的方法论和框架。这直接命中了您研究目标中的“构建、改进或演化 LLM智能体”以及“多智能体 (Multi-Agent)”方向。它并非简单应用现有框架，而是提出了一个包含新机制（如多模态观察学习、基于反馈的通信学习）的原创性框架，因此不属于“非演化型应用”的排除范畴。 **第二步：正面指标——高度相关** 论文包含了大量您的核心关注点： *   **核心范式**: `Multi-Agent Systems (MAS)` 是论文的绝对核心。摘要中明确指出研究目标是“developing human-like collaborative intelligence in multi-agent systems”。 *   **多智能体**: 论文深入探讨了 `Collaboration` (协作)、`Communication` (通信)、`Social Learning` (社会学习)。其提出的两种社会学习路径（观察学习和反馈学习）以及涌现出的角色分工和劳动划分，都是多智能体研究的核心议题。 *   **智能体能力**: 论文涉及 `Planning` (动态重新规划)、`Memory` (情景记忆系统) 和 `Tool Use` (结构化动作能力，实现物理操作)。 *   **演化机制**: 论文研究了“涌现行为 (Emergent Behaviors)”和“角色专业化 (Role Specialization)”，这属于智能体在交互和学习过程中的自我演化和迭代，与您的“自我演化”方向高度契合。 **第三步：排除标准——不适用** 论文的主要贡献并非安全与对齐，也未将多模态作为研究核心。虽然论文使用了“多模态 (Multimodal)”，但根据您的筛选规则，这里的视觉和文本能力是作为智能体感知环境和进行“具身多模态通信 (grounded multimodal communication)”的工具，其目的是为了实现更高级的社交学习和协作，而不是研究视觉模型本身。因此，这不构成排除理由。 **第四步：处理特殊和模糊情况——不适用** 论文的核心是关于多智能体系统中的协作与学习，不涉及需要特殊处理的推理/规划或自我演化应用的模糊情况。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一种新颖的多智能体框架（M-S2L），该框架通过整合多模态感知和社交学习机制，促进了智能体间协作、通信和复杂社会行为的涌现。这完全符合您对“多智能体 (Multi-Agent)”方向的研究焦点，并且触及了“自我演化”的范畴。因此，最终判断为 **True**，应保留该论文。"
    },
    {
        "index": "#6",
        "title": "Adaptive Coopetition: Leveraging Coarse Verifier Signals for Resilient Multi-Agent LLM Reasoning",
        "link": "/arxiv/2510.18179",
        "arxiv_id": "2510.18179",
        "authors": "Rui Jerry Huang, Wendy Liu, Anastasia Miin",
        "summary": "Inference-time computation is a critical yet challenging paradigm for enhancing the reasoning performance of large language models (LLMs). While existing strategies improve reasoning stability and consistency, they suffer from notable limitations: self-correction often reinforces the model's initial biases, and Multi-Agent Collaboration (MAC) often fails due to the lack of efficient coordination mechanisms, leading to collective errors. Although high-performing verifiers can detect reasoning errors, making them reliable requires substantial training. To address these challenges, we introduce a novel inference-time framework, Adaptive Coopetition (AdCo), in which LLM agents utilize an adaptive, UCB-based \"coopetition\" mechanism. At each round, agents leverage coarse verifier signals to determine whether to collaborate or compete, and iteratively refine their reasoning based on peer feedback. Without relying on high-performance verifiers, our adaptive strategy achieves significant performance gains on mathematical reasoning benchmarks, yielding a 20% relative improvement over baselines on the more challenging dataset. Our approach remains robust and consistent in terms of accuracy under different sample sizes and configurations. This adaptive, signal-guided \"coopetition\" framework enhances reasoning robustness by leveraging both model knowledge diversity and reasoning trace measures, while also promoting uncertainty-driven exploration, especially when participants have comparable capabilities. From this perspective, our work offers a fresh lens on inference-time computation and paves the way for more resilient multi-agent LLM systems. Our code is available at: https://github.com/AdCo-Research/adaptive-coopetition.",
        "subjects": "Multiagent Systems",
        "date": "2025-10-21",
        "category": "cs.MA",
        "crawl_time": "2025-10-22T11:00:04.624878",
        "filter_reason": "这篇论文完全符合你的筛选要求。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心本质是**构建和改进多智能体系统**。它没有将一个已有的智能体框架应用到特定领域，而是针对现有多智能体协作（MAC）中“缺乏有效协调机制”的痛点，提出了一种全新的、名为“Adaptive Coopetition (AdCo)”的推理时框架。这个框架的核心是设计一种智能体间的“竞争合作”机制，让智能体能够自适应地决定是协作还是竞争，并基于同伴反馈迭代优化推理过程。这完全符合你“构建、改进或演化 LLM智能体”的核心目标。 2.  **第二步：正面指标——高度相关** 论文包含了大量与你研究焦点相关的正面指标： *   **核心范式**: 明确属于 `Multi-Agent Systems (MAS)`。 *   **多智能体**: 核心贡献围绕 `Collaboration`（协作）和一种新的交互模式（coopetition）。`peer feedback`（同伴反馈）也体现了智能体间的`Communication`（通信）。 *   **演化机制**: 论文的核心机制是“iteratively refine their reasoning”（迭代优化其推理），这直接对应了`Iterative Improvement`和`Self-Correction`（自我修正）的概念，属于自我演化的范畴。 3.  **第三步：排除标准——未触发** 论文的主要贡献是提升多智能体系统的推理鲁棒性和性能，不涉及安全、对齐、可解释性或多模态等排除领域。 4.  **第四步：处理特殊和模糊情况——符合保留规则** *   **推理/规划**: 这篇论文是典型的“保留”案例。它研究的不是单个LLM的基础数学或逻辑推理能力，而是**多个智能体如何通过一个新颖的框架进行复杂任务的多步推理**。AdCo框架本身就是一种高级的、分布式的规划和推理策略，远超非Agentic的CoT变体。 *   **自我演化的应用**: 论文提出的“迭代优化”机制本身就是一种自我演化的体现，虽然应用在数学推理基准上，但其核心是提出了一种通用的、可演化的多智能体交互框架，因此符合保留规则。 **最终决策**: 这篇论文的核心贡献是提出了一种创新的、自适应的多智能体协作与竞争框架，旨在解决多智能体LLM系统中的协调难题，并通过迭代反馈实现推理能力的自我完善。其研究焦点、方法论和核心贡献与你的研究课题“LLM智能体及其演化”中的**多智能体**和**自我演化**方向高度契合，是一篇非常前沿且相关的论文。因此，最终判断为 **True**。"
    },
    {
        "index": "#7",
        "title": "TACLA: An LLM-Based Multi-Agent Tool for Transactional Analysis Training in Education",
        "link": "/arxiv/2510.17913",
        "arxiv_id": "2510.17913",
        "authors": "Monika Zamojska, Jarosław A. Chudziak",
        "summary": "Simulating nuanced human social dynamics with Large Language Models (LLMs) remains a significant challenge, particularly in achieving psychological depth and consistent persona behavior crucial for high-fidelity training tools. This paper introduces TACLA (Transactional Analysis Contextual LLM-based Agents), a novel Multi-Agent architecture designed to overcome these limitations. TACLA integrates core principles of Transactional Analysis (TA) by modeling agents as an orchestrated system of distinct Parent, Adult, and Child ego states, each with its own pattern memory. An Orchestrator Agent prioritizes ego state activation based on contextual triggers and an agent's life script, ensuring psychologically authentic responses. Validated in an educational scenario, TACLA demonstrates realistic ego state shifts in Student Agents, effectively modeling conflict de-escalation and escalation based on different teacher intervention strategies. Evaluation shows high conversational credibility and confirms TACLA's capacity to create dynamic, psychologically-grounded social simulations, advancing the development of effective AI tools for education and beyond.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Computers and Society",
        "date": "2025-10-19",
        "category": "cs.MA",
        "crawl_time": "2025-10-22T11:00:04.625144",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** - **保留 (Keep)**。这篇论文的核心贡献是构建了一个名为 TACLA 的**新型多智能体架构（Multi-Agent architecture）**。它并非简单地将现有框架应用于教育领域，而是提出了一种创新的、基于心理学理论（交互分析理论）的智能体内部状态建模方法。其本质是关于**如何构建和改进LLM智能体**，特别是如何让智能体在多智能体交互中表现出更复杂、更真实的行为。这完全符合您“构建、改进或演化LLM智能体”的核心目标。 **第二步：正面指标——论文是否包含我的核心关注点？** - **核心范式**: 论文明确提出了一个 `LLM-based Multi-Agent` 架构，直接命中您的核心研究焦点。 - **多智能体**: 论文的核心是关于多智能体系统，涉及 `Agent Society` 的概念。它设计了多个智能体（学生、教师）并让它们在一个模拟环境中进行交互。 - **智能体能力**: 论文深入探讨了智能体的内部机制，特别是通过 `Parent, Adult, and Child ego states` 和 `pattern memory` 来实现一种高级的**记忆（Memory）**和**自我状态管理**。`Orchestrator Agent` 的作用是根据上下文触发器来决定激活哪个自我状态，这本质上是一种复杂的**规划（Planning）**和决策机制，确保了行为的连贯性和真实性。 - **演化机制**: 虽然论文没有明确提出“自我演化”，但其设计的 `ego state shifts`（自我状态转换）和基于不同干预策略产生的行为变化（冲突升级或降级），展示了智能体在交互中动态调整其行为模式的能力，这与“通过环境反馈进行迭代”的思想高度相关。 **第三步：排除标准——是否为我的研究焦点之外？** - **安全与对齐**: 论文的主要贡献不是关于安全、对齐或可解释性。虽然它追求“心理真实性”，但这服务于构建更好的智能体，而非为了对齐研究。 - **多模态与视觉**: 论文完全基于文本交互，不涉及多模态或视觉内容。 **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的 `Orchestrator Agent` 根据上下文和“生命脚本”来激活自我状态，这是一种典型的**智能体自主规划**行为，而非提升LLM基础推理能力。因此，符合保留条件。 - **自我演化的应用**: 这篇论文是一个绝佳的“例外”案例。尽管它被应用在“教育”这一特定领域，但其核心贡献是提出了一种**新的智能体架构和交互机制**。它不是在用智能体解决教育问题，而是在用教育场景来验证其智能体架构的有效性。因此，它完全符合“保留”的例外规则。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于提出了一种创新的、基于心理学理论的LLM多智能体架构（TACLA），以实现更真实的社会动态模拟。它直接命中了您研究范围中的“多智能体（Multi-Agent）”方向，并深入探讨了智能体的“记忆”和“规划”能力。虽然应用场景是教育，但其本质是方法论创新，而非简单的应用部署。因此，这篇论文与您的研究目标高度契合，应被**保留**。"
    },
    {
        "index": "#2",
        "title": "Fetch.ai: An Architecture for Modern Multi-Agent Systems",
        "link": "/arxiv/2510.18699",
        "arxiv_id": "2510.18699",
        "authors": "Michael J. Wooldridge, Attila Bagoly, Jonathan J. Ward, Emanuele La Malfa, Gabriel Paludo Licks",
        "summary": "Recent surges in LLM-driven intelligent systems largely overlook decades of foundational multi-agent systems (MAS) research, resulting in frameworks with critical limitations such as centralization and inadequate trust and communication protocols. This paper introduces the Fetch.ai architecture, an industrial-strength platform designed to bridge this gap by facilitating the integration of classical MAS principles with modern AI capabilities. We present a novel, multi-layered solution built on a decentralized foundation of on-chain blockchain services for verifiable identity, discovery, and transactions. This is complemented by a comprehensive development framework for creating secure, interoperable agents, a cloud-based platform for deployment, and an intelligent orchestration layer where an agent-native LLM translates high-level human goals into complex, multi-agent workflows. We demonstrate the deployed nature of this system through a decentralized logistics use case where autonomous agents dynamically discover, negotiate, and transact with one another securely. Ultimately, the Fetch.ai stack provides a principled architecture for moving beyond current agent implementations towards open, collaborative, and economically sustainable multi-agent ecosystems.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.MA",
        "crawl_time": "2025-10-22T11:00:04.623661",
        "filter_reason": "这篇论文完全符合您的筛选标准，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **核心贡献**: 本文的核心贡献是提出一个名为 Fetch.ai 的、面向现代多智能体系统的去中心化架构和平台。它旨在将经典的多智能体系统（MAS）原则与现代LLM能力相结合，创建一个能让智能体安全、自主地发现、通信、谈判和交易的生态系统。 - **判断**: 这完全符合“保留”标准，即“论文的核心是关于构建...多智能体系统...的方法论或新框架”。它不是将现有智能体作为工具应用，而是提出了一种构建和运行多智能体系统的基础性新架构。 2.  **第二步：正面指标** - 论文摘要中包含了大量与您研究焦点高度相关的正面指标： - **核心范式**: `Multi-Agent Systems (MAS)` 是论文的标题和核心主题。 - **多智能体**: 明确提到了 `Collaboration` (协作)、`Communication` (通信)、`Negotiation` (谈判)，并构建了 `Agent Society` (智能体生态系统)。 - **智能体能力**: 论文中的“智能编排层”由一个“agent-native LLM”驱动，其功能是将高层人类目标“转化为复杂的多智能体工作流”，这本质上是一种高级的 `Planning` (规划) 能力。 - 这些指标强烈表明该论文与您的“多智能体”研究方向高度契合。 3.  **第三步：排除标准** - 论文的主要贡献并非关于 `Safety`、`Alignment` 或 `Interpretability`。虽然它利用区块链技术实现了“可验证身份”和“安全交易”，但这是其架构设计的一部分，旨在解决多智能体交互中的信任问题，而非论文的研究核心。论文的核心是架构本身，而非安全协议。 - 论文不涉及多模态或视觉内容。 - 因此，该论文未触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文符合“保留”条件。它不是在提升LLM的基础数学或逻辑能力，而是在构建一个Agentic框架，其中的LLM负责将高层目标分解为多智能体工作流，这是一种典型的智能体规划行为。 - **基础设施**: 这是一个需要辨析的关键点。您要求排除“模型基础设施、部署优化、硬件加速”的研究。Fetch.ai 论文虽然涉及“云平台”和“区块链”等基础设施，但其焦点并非模型训练或推理的效率优化。相反，它提出的是**智能体生态系统级别的基础设施**，用于解决智能体之间的身份、发现、通信和交易等根本性问题。这属于“构建和改进LLM智能体”的范畴，特别是为多智能体系统提供了一个基础性的运行框架和方法论，因此不应被排除。 5.  **第五步：最终决策** - 综合以上分析，这篇论文的核心是为构建和运行现代化的、去中心化的多智能体系统提供一个全新的、原则性的架构和平台。它直接命中了您研究焦点中的“多智能体”方向，并深入探讨了智能体间的协作、通信和谈判机制。其贡献是方法论和框架层面的，而非简单的应用或底层模型优化。因此，这篇论文是您研究课题“LLM智能体及其演化”中关于多智能体系统的理想前沿文献。"
    },
    {
        "index": "#13",
        "title": "OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning",
        "link": "/arxiv/2510.18032",
        "arxiv_id": "2510.18032",
        "authors": "Zhenyu Bi, Meng Lu, Yang Li, Swastik Roy, Weijie Guan, Morteza Ziyadi, Xuan Wang",
        "summary": "Large Language Models (LLMs) have shown remarkable reasoning capabilities in mathematical and scientific tasks. To enhance complex reasoning, multi-agent systems have been proposed to harness the collective intelligence of LLM agents. However, existing collaboration structures are either predefined or rely on majority voting or round-table debates, which can suppress correct but less dominant agent contributions. Recent approaches model multi-agent systems as graph networks but optimize purely for agent performance, neglecting the quality of interactions. We hypothesize that effective agent communication is crucial for multi-agent reasoning and that debating quality plays a significant role. To address this, we propose $\\ours$, a multi-agent verbal reinforcement learning algorithm that dynamically constructs and refines multi-agent collaboration structures. Our method defines action spaces and a feedback mechanism that evaluates communication robustness and coherence throughout the debate. The final decision is achieved through a majority vote over all the agents. We assess $\\ours$ on various reasoning tasks, including mathematical reasoning, creative writing, scientific reasoning, and numerical sorting. Results demonstrate that our approach significantly outperforms single-agent prompting methods and state-of-the-art multi-agent frameworks on diverse tasks.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-20",
        "category": "cs.MA",
        "crawl_time": "2025-10-22T11:00:04.626944",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建和改进多智能体LLM系统。以下是根据您的筛选标准进行的详细判断过程： **第一步：核心判断——保留 (Keep)** - **论文本质**: 论文的核心是提出一种名为 `OPTAGENT` 的新方法论，用于**优化多智能体LLM之间的交互**。它不是简单地将现有框架应用于某个领域，而是提出了一种全新的、动态构建和优化多智能体协作结构的算法。这直接命中了您研究目标中的“构建、改进或演化 LLM智能体”以及“多智能体”方向。 - **排除项检查**: 1. **非演化型应用**: 论文虽然评估了数学、科学等任务，但其核心贡献是`OPTAGENT`这个**多智能体协作框架本身**，而不是它在某个领域的应用结果。因此，不属于此项排除。 2. **非Agentic的推理**: 论文明确关注的是**多智能体系统**如何通过**辩论（debate）**和**通信（communication）**来增强推理，这属于典型的Agentic AI范畴，而非提升LLM单体的基础推理能力。 3. **基础设施**: 论文内容与模型基础设施、部署优化无关。 **第二步：正面指标——高度匹配** 论文包含了大量您关注的核心指标： - **核心范式**: `Multi-Agent Systems (MAS)` 是论文的绝对核心。 - **智能体能力**: 论文的核心是优化智能体间的 `Communication`，并通过 `Verbal Reinforcement Learning` 进行 `Self-Refine`（自我精炼），这可以看作是一种在交互层面的自我演化。 - **多智能体**: `Collaboration` 和 `Communication` 是论文的标题和摘要中反复出现的关键词，是 `OPTAGENT` 算法要解决的核心问题。 - **演化机制**: 论文提出的算法能够“**dynamically constructs and refines multi-agent collaboration structures**”（动态构建和精炼多智能体协作结构），这完全符合“自我演化”和“迭代改进”的定义。 **第三步：排除标准——不适用** - **安全与对齐**: 论文的主要贡献是提升推理性能，而非安全、对齐或可解释性。 - **多模态与视觉**: 论文专注于语言模型和推理，不涉及视觉或多模态内容。 **第四步：处理特殊和模糊情况** - **推理/规划**: 论文属于“保留”情况。它研究的是**多智能体如何通过结构化的辩论和交互进行复杂推理**，这是一个典型的Agentic框架下的推理问题，而非提升LLM本身的基础数学或逻辑能力。 **第五步：最终决策** 综合以上分析，论文 `OPTAGENT` 的核心贡献是提出了一种新颖的、基于口头强化学习的多智能体协作优化算法。该算法通过动态调整智能体间的交互结构来提升集体推理能力，这直接对应了您研究课题中的“**多智能体 (Multi-Agent)**”方向，并触及了“**自我演化 (Self-Evolving)**”的机制（在系统层面进行精炼和优化）。因此，这篇论文是您研究范围内的前沿且高度相关的论文，应予以保留。"
    },
    {
        "index": "#15",
        "title": "Modeling Layered Consciousness with Multi-Agent Large Language Models",
        "link": "/arxiv/2510.17844",
        "arxiv_id": "2510.17844",
        "authors": "Sang Hun Kim, Jongmin Lee, Dongkyu Park, So Young Lee, Yosep Chong",
        "summary": "We propose a multi-agent framework for modeling artificial consciousness in large language models (LLMs), grounded in psychoanalytic theory. Our \\textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and unconsciousness through agent interaction, guided by a Personalization Module combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning on emotionally rich dialogues, the system was evaluated across eight personalized conditions. An LLM as a judge approach showed a 71.2\\% preference for the fine-tuned model, with improved emotional depth and reduced output variance, demonstrating its potential for adaptive, personalized cognition.",
        "subjects": "Computation and Language, Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-10",
        "category": "cs.MA",
        "crawl_time": "2025-10-22T11:00:04.627516",
        "filter_reason": "这篇论文符合您的研究范围，应被保留。我的判断过程如下： **第一步：核心判断** - **保留 (Keep)**。这篇论文的本质是提出一个**新的多智能体框架（Multi-agent framework）**，即“心理动力学模型（Psychodynamic Model）”。其核心贡献并非将现有框架应用于某个领域，而是**构建**一个由多个LLM智能体交互来模拟人工意识的新方法论。这完全符合您筛选标准中“构建、改进或演化LLM智能体”的核心目标。 **第二步：正面指标分析** - **核心范式**: 论文明确提出了一个`Multi-Agent Systems (MAS)`框架，这是您关注的三个核心方向之一。 - **智能体能力**: 论文通过智能体间的交互来模拟“自我意识（self-awareness）”，这与智能体的`Self-Reflection`（自我反思）能力高度相关。 - **多智能体**: 论文的核心机制就是“智能体交互（agent interaction）”，这直接对应了您关注的多智能体方向中的`Communication`（通信）和`Agent Society`（智能体社会）等概念。 - **演化机制**: 论文中的“个性化模块（Personalization Module）”结合了“固定特质和动态需求（fixed traits and dynamic needs）”，并且系统通过参数高效微调展现出“自适应的、个性化的认知（adaptive, personalized cognition）”，这体现了智能体基于内部状态和外部反馈进行动态调整和迭代，与`Self-Improvement`（自我完善）和`Iterative Improvement`（迭代改进）的演化思想相符。 **第三步：排除标准分析** - **安全与对齐**: 论文的主要贡献是关于构建模拟意识的智能体框架，而非安全、对齐或可解释性。虽然提到了“情感深度”，但其目的是为了验证模型的个性化认知能力，而非研究情感安全或对齐问题。因此，不触及排除标准。 - **多模态与视觉**: 论文完全基于文本对话，不涉及视觉或多模态内容。因此，不触及排除标准。 **第四步：特殊和模糊情况处理** - **推理/规划**: 论文的核心不在于提升LLM的基础推理能力，而在于通过多智能体架构实现更高层次的认知功能模拟（如意识），这属于Agentic框架的构建，而非单纯的推理技巧优化。 - **自我演化的应用**: 论文的核心贡献是提出多智能体框架这一新方法论，而非其在特定领域的应用。评估部分虽然使用了“个性化对话”，但这只是为了验证框架有效性，属于方法论研究的常规环节，不属于“非演化型应用”的排除范畴。 **第五步：最终决策** 综合以上分析，该论文的核心贡献是**提出一个新颖的多智能体框架来模拟人工意识**，这直接命中了您研究焦点中的“多智能体 (Multi-Agent)”方向，并触及了“自我演化 (Self-Evolving)”中的自适应和迭代改进思想。论文的目的是构建和探索新的Agentic AI范式，而非将其作为工具解决特定领域问题，也非关注安全或多模态。因此，这篇论文与您的研究目标高度相关，应被保留。"
    },
    {
        "index": "#8",
        "title": "KAT-Coder Technical Report",
        "link": "/arxiv/2510.18779",
        "arxiv_id": "2510.18779",
        "authors": "Zizheng Zhan, Ken Deng, Xiaojiang Zhang, Jinghui Wang, Huaixi Tang, Zhiyi Lai, Haoyang Huang, Wen Xiang, Kun Wu, Wenhao Zhuang, Minglei Zhang, Shaojie Wang, Shangpeng Yan, Kepeng Lei, Zongxian Feng, Huiming Wang, Zheng Lin, Mengtong Li, Mengfei Xie, Yinghan Cui, Xuxing Chen, Chao Wang, Weihao Li, Wenqiang Zhu, Jiarong Zhang, Jingxuan Xu, Songwei Yu, Yifan Yao, Xinping Lei, Han Li, Junqi Xiong, Zuchen Gao, Dailin Li, Haimo Li, Jiaheng Liu, Yuqun Zhang, Junyi Peng, Haotian Zhang, Bin Chen",
        "summary": "Recent advances in large language models (LLMs) have enabled progress in agentic coding, where models autonomously reason, plan, and act within interactive software development workflows. However, bridging the gap between static text-based training and dynamic real-world agentic execution remains a core challenge. In this technical report, we present KAT-Coder, a large-scale agentic code model trained through a multi-stage curriculum encompassing Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning (RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances reasoning, planning, and reflection capabilities through a corpus of real software engineering data and synthetic agentic interactions. The SFT stage constructs a million-sample dataset balancing twenty programming languages, ten development contexts, and ten task archetypes. The RFT stage introduces a novel multi-ground-truth reward formulation for stable and sample-efficient policy optimization. Finally, the Reinforcement-to-Deployment phase adapts the model to production-grade IDE environments using Error-Masked SFT and Tree-Structured Trajectory Training. In summary, these stages enable KAT-Coder to achieve robust tool-use reliability, instruction alignment, and long-context reasoning, forming a deployable foundation for real-world intelligent coding agents. Our KAT series 32B model, KAT-Dev, has been open-sourced on https://huggingface.co/Kwaipilot/KAT-Dev.",
        "subjects": "Computation and Language",
        "date": "2025-10-21",
        "category": "cs.CL",
        "crawl_time": "2025-10-22T11:00:05.530455",
        "filter_reason": "这篇论文完全符合您的研究范围。 1.  **核心判断 (第一步):** 论文的核心贡献是提出了一种名为KAT-Coder的大规模智能体代码模型，以及一个用于训练它的多阶段课程框架。这直接对应了您研究目标中的“构建、改进LLM智能体”。论文的本质是方法论创新，旨在通过一种新的训练范式来创建一个更强大的Agentic LLM，而非简单应用。因此，根据第一步的“保留”标准，该论文应被保留。 2.  **正面指标 (第二步):** 论文包含了您核心关注点的多个关键指标： *   **核心范式**: 论文标题和摘要中明确提到了 `agentic coding` 和 `agentic code model`，完全属于 `Agentic AI` 和 `LLM-based Agents` 范畴。 *   **智能体能力**: 摘要明确指出，该模型通过Mid-Term Training阶段增强了 `reasoning, planning, and reflection capabilities`（推理、规划和反思能力），并通过RFT和部署适应阶段实现了 `robust tool-use reliability`（稳健的工具使用可靠性）。这些都是您在“单智能体”方向下关注的核心能力。 3.  **排除标准 (第三步):** 论文内容不涉及安全、对齐、可解释性或视觉等多模态领域。它虽然提到了`instruction alignment`（指令对齐），但这是作为模型的一项能力指标，而非论文研究的核心贡献。因此，没有触碰到任何排除标准。 4.  **特殊和模糊情况处理 (第四步):** *   **推理/规划**: 这篇论文是典型的“保留”情况。它研究的不是LLM基础的数学或逻辑能力，而是智能体在复杂任务（软件开发）中进行多步`reasoning`、`planning`和`reflection`的框架和方法。这与ReAct、ToT等Agentic规划框架的研究精神一致。 *   **自我演化**: 虽然论文不完全是关于智能体在运行时的自主“自我演化”，但其提出的多阶段训练方法论可以被看作是一种结构化的、迭代的改进过程，旨在创建一个能够适应复杂环境的、更强大的智能体基础模型。这属于您所关心的“自我演化”的广义范畴，即如何让智能体变得更完善和迭代。 **最终决策 (第五步):** 综合来看，KAT-Coder这篇技术报告的核心工作是提出了一套系统性的方法论来构建和改进一个面向编程领域的LLM智能体，重点提升了其规划、工具使用和自我反思能力。这与您的研究课题“LLM智能体及其演化”高度契合，特别是其中的“单智能体”方向。因此，最终判断为 **True**，应保留该论文。"
    },
    {
        "index": "#11",
        "title": "Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation",
        "link": "/arxiv/2510.18731",
        "arxiv_id": "2510.18731",
        "authors": "Ming Li",
        "summary": "Large Language Models demonstrate strong capabilities in single-turn instruction following but suffer from Lost-in-Conversation (LiC), a degradation in performance as information is revealed progressively in multi-turn settings. Motivated by the current progress on Reinforcement Learning with Verifiable Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not only to generate correct answers, but also to judge the solvability of questions in the multi-turn conversation setting. Our approach employs a competence-gated curriculum that incrementally increases dialogue difficulty (in terms of instruction shards), stabilizing training while promoting reliability. Using multi-turn, on-policy rollouts and a mixed-reward system, RLAAR teaches models to balance problem-solving with informed abstention, reducing premature answering behaviors that cause LiC. Evaluated on LiC benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to 75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together, these results provide a practical recipe for building multi-turn reliable and trustworthy LLMs.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-21",
        "category": "cs.CL",
        "crawl_time": "2025-10-22T11:00:05.532283",
        "filter_reason": "这篇论文符合你的研究范围，其核心贡献在于构建和改进LLM智能体的能力。我的判断依据如下： 1.  **第一步：核心判断（保留）** 论文的本质是提出一个名为RLAAR的**新框架**，用于通过课程强化学习来训练和改进LLM。其核心目标是解决LLM在多轮对话中的性能退化问题（Lost-in-Conversation）。这并非简单地将LLM作为一个工具应用到特定领域，而是直接针对LLM在多轮交互这一**智能体核心场景**中的行为进行优化和增强。因此，它符合“构建、改进或演化LLM智能体”的核心要求，应予以保留。 2.  **第二步：正面指标（高度相关）** 该论文的研究内容与你关注的多个核心指标高度契合： *   **Agentic AI**: 整个研究聚焦于LLM在多轮对话中的表现，这是LLM智能体最基本和核心的应用场景之一。 *   **自我修正/自我反思**: 论文的核心创新点之一是训练模型“judge the solvability of questions”（判断问题的可解性）和进行“informed abstention”（知情拒答）。这是一种典型的**自我评估和自我反思**能力。智能体不再盲目回答，而是学会了在信息不足或能力不够时主动暂停或拒绝，这是高级智能体的重要特征。 *   **迭代改进**: 论文采用的“Curriculum Reinforcement Learning”（课程强化学习）和“competence-gated curriculum”（能力门控课程）本质上就是一种**迭代改进**机制，使智能体能够循序渐进地提升其在复杂对话任务中的表现。 3.  **第三步：排除标准（未触发）** *   **安全与对齐**: 尽管论文提到了“reliable”和“trustworthy”，但这是其方法带来的**结果**，而非研究的主要贡献。论文的核心是提出一种提升性能的**训练方法论（RLAAR）**，而不是探讨安全、可解释性或对齐的理论。如果一篇论文的贡献是“一种新的对齐算法”，那它会被排除；但这篇论文的贡献是“一种提升多轮对话性能的训练框架”，其可靠性提升是性能指标的一部分，因此不应被排除。 *   **多模态与视觉**: 论文完全聚焦于文本对话，未涉及视觉或多模态内容。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 该论文的研究内容不属于“提高LLM本身基础Token预测的数学或逻辑能力”。相反，它研究的是智能体在**多步、多轮的交互环境**中如何做出决策——是继续回答还是选择拒答。这是一种更高层次的、基于环境和自身状态评估的**智能体决策行为**，完全符合“保留”标准。 **总结**: 这篇论文的核心贡献是提出了一种新的训练框架（RLAAR），用于**赋予LLM智能体在多轮对话中进行自我反思和知情拒答的能力**。这直接对应了你研究范围中的“单智能体”方向，特别是“自我反思”和“自我修正”子方向。它不是对已有智能体的简单应用，而是对智能体核心能力的**构建和改进**，因此完全符合你的筛选要求。"
    },
    {
        "index": "#7",
        "title": "WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection",
        "link": "/arxiv/2510.18798",
        "arxiv_id": "2510.18798",
        "authors": "Guanzhong He, Zhen Yang, Jinxin Liu, Bin Xu, Lei Hou, Juanzi Li",
        "summary": "Search agents have achieved significant advancements in enabling intelligent information retrieval and decision-making within interactive environments. Although reinforcement learning has been employed to train agentic models capable of more dynamic interactive retrieval, existing methods are limited by shallow tool-use depth and the accumulation of errors over multiple iterative interactions. In this paper, we present WebSeer, a more intelligent search agent trained via reinforcement learning enhanced with a self-reflection mechanism. Specifically, we construct a large dataset annotated with reflection patterns and design a two-stage training framework that unifies cold start and reinforcement learning within the self-reflection paradigm for real-world web-based environments, which enables the model to generate longer and more reflective tool-use trajectories. Our approach substantially extends tool-use chains and improves answer accuracy. Using a single 14B model, we achieve state-of-the-art results on HotpotQA and SimpleQA, with accuracies of 72.3% and 90.0%, respectively, and demonstrate strong generalization to out-of-distribution datasets. The code is available at https://github.com/99hgz/WebSeer",
        "subjects": "Computation and Language",
        "date": "2025-10-21",
        "category": "cs.CL",
        "crawl_time": "2025-10-22T11:00:05.529281",
        "filter_reason": "根据您提供的筛选标准，这篇论文完全符合研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——论文的本质是构建和改进LLM智能体。** 这篇论文的核心贡献是提出了一个名为 **WebSeer** 的新框架，用于训练更强大的搜索智能体。它并非简单地将现有智能体应用于某个领域，而是针对现有智能体“工具使用深度浅”和“错误累积”的缺陷，提出了一种结合了**强化学习**和**自我反思**的全新训练范式。这完全符合“构建、改进或演化LLM智能体的方法论或新框架”的保留标准。它不属于非演化型应用、非Agentic推理或基础设施研究的排除范畴。 **第二步：正面指标——论文包含多个核心关注点。** 论文的标题和摘要中明确包含了多个您关注的核心关键词和概念： *   **核心范式**: 论文直接研究 `LLM-based Agents`。 *   **智能体能力**: 论文的核心是 `Self-Reflection`（自我反思），并旨在改进 `Tool Use`（工具使用）和 `Planning`（规划，体现为生成更长的工具使用轨迹）能力。其训练方法与ReAct等Agentic框架一脉相承，但进行了深化。 *   **演化机制**: `Self-Reflection` 本质上是一种 `Self-Correction` 和 `Self-Improvement` 机制，通过反思过去的行动轨迹来优化未来的决策，这与“自我演化”的核心理念高度一致。 **第三步：排除标准——论文不涉及任何排除焦点。** 论文的主要贡献是提升智能体的任务执行能力，而不是研究其安全性、对齐性或可解释性。同时，论文聚焦于基于文本的网页搜索环境，不涉及视觉或多模态内容，因此不触及相关排除标准。 **第四步：处理特殊情况——论文属于典型的智能体规划/推理研究。** 论文的研究重点在于智能体如何通过反思机制进行更复杂、更深入的多步推理和规划（即生成“longer and more reflective tool-use trajectories”）。这完全符合“保留关于智能体如何进行规划或在复杂任务中进行多步推理的论文”这一特殊规则。它不是在提升LLM本身的基础数学或逻辑能力，而是在构建一个能让LLM在交互环境中更好地思考和行动的框架。 **最终决策：** 综上所述，**WebSeer** 论文的核心贡献是提出了一种创新的、融合了自我反思机制的智能体训练框架，旨在提升LLM智能体的规划和工具使用能力。这直接命中了您研究课题中的“单智能体”和“自我演化”两个核心方向，是一篇与您研究目标高度相关的前沿论文。因此，最终判断为 **True**。"
    },
    {
        "index": "#32",
        "title": "MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models",
        "link": "/arxiv/2510.18383",
        "arxiv_id": "2510.18383",
        "authors": "ChangSu Choi, Hoyun Song, Dongyeon Kim, WooHyeon Jung, Minkyung Cho, Sunjin Park, NohHyeob Bae, Seona Yu, KyungTae Lim",
        "summary": "Distilling the tool-using capabilities of large language models (LLMs) into smaller, more efficient small language models (SLMs) is a key challenge for their practical application. The predominant approach, supervised fine-tuning (SFT), suffers from poor generalization as it trains models to imitate a static set of teacher trajectories rather than learn a robust methodology. While reinforcement learning (RL) offers an alternative, the standard RL using sparse rewards fails to effectively guide SLMs, causing them to struggle with inefficient exploration and adopt suboptimal strategies. To address these distinct challenges, we propose MENTOR, a framework that synergistically combines RL with teacher-guided distillation. Instead of simple imitation, MENTOR employs an RL-based process to learn a more generalizable policy through exploration. In addition, to solve the problem of reward sparsity, it uses a teacher's reference trajectory to construct a dense, composite teacher-guided reward that provides fine-grained guidance. Extensive experiments demonstrate that MENTOR significantly improves the cross-domain generalization and strategic competence of SLMs compared to both SFT and standard sparse-reward RL baselines.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.CL",
        "crawl_time": "2025-10-22T11:00:05.552554",
        "filter_reason": "这篇论文完全符合您的研究范围，核心判断为“保留”。以下是我的详细分析： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一种名为 MENTOR 的强化学习框架，其目标是**将大型语言模型（LLM）的工具使用能力蒸馏并增强到小型语言模型（SLM）中**。这并非一个简单的应用型论文，而是提出了一个**构建和改进LLM智能体（特指其工具使用能力）的新方法论和新框架**。因此，它通过了第一步的核心判断，应被保留。 2.  **与核心关注点的匹配度（第二步）：** *   **单智能体：** 论文的核心是“蒸馏工具使用能力”，这直接命中了您研究焦点中“单智能体”方向的核心子方向——`Tool Use / Tool Augmentation`。它致力于让智能体（SLM）更擅长使用工具。 *   **自我演化：** MENTOR 框架通过强化学习（RL）的探索机制，让 SLM 在任务中不断试错、学习并优化其策略。更重要的是，它通过“教师引导的奖励”来指导这一过程。这种通过**迭代优化和经验反馈来完善模型策略**的机制，与您“自我演化”方向中的`Self-Improvement`和`Iterative Improvement`高度契合。它描述了一种在训练阶段实现智能体能力演化的有效路径。 3.  **排除标准检查（第三步）：** 论文的研究焦点是提升智能体的能力（工具使用的策略性和泛化性），完全不涉及安全、对齐、可解释性或多模态等排除标准。因此，它没有被第三步的规则排除。 4.  **特殊和模糊情况处理（第四步）：** *   **推理/规划：** 论文的核心是“工具使用策略”，这本身就是一种高级的规划和推理能力。它不是在提升LLM的Token级基础数学或逻辑能力，而是在研究**智能体如何在多步任务中有效规划和执行（使用工具）**，这属于应被保留的范畴。 **最终决策（第五步）：** 综合以上分析，MENTOR 论文的核心是提出一个新框架来**改进LLM智能体的工具使用能力**，并利用强化学习的迭代机制实现其策略的**自我优化和演化**。这精确地覆盖了您研究课题中的“单智能体”和“自我演化”两个核心方向。它不是应用研究，也不是基础模型或安全研究，而是关于Agentic AI方法论的前沿探索。因此，这篇论文与您的研究目标高度相关，应被筛选出来。"
    },
    {
        "index": "#42",
        "title": "DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization",
        "link": "/arxiv/2510.18257",
        "arxiv_id": "2510.18257",
        "authors": "Tao Tao, Guanghui Zhu, Lang Guo, Hongyi Chen, Chunfeng Yuan, Yihua Huang",
        "summary": "Prompt Optimization has emerged as a crucial approach due to its capabilities in steering Large Language Models to solve various tasks. However, current works mainly rely on the random rewriting ability of LLMs, and the optimization process generally focus on specific influencing factors, which makes it easy to fall into local optimum. Besides, the performance of the optimized prompt is often unstable, which limits its transferability in different tasks. To address the above challenges, we propose $\\textbf{DelvePO}$ ($\\textbf{D}$irection-Guid$\\textbf{e}$d Se$\\textbf{l}$f-E$\\textbf{v}$olving Framework for Fl$\\textbf{e}$xible $\\textbf{P}$rompt $\\textbf{O}$ptimization), a task-agnostic framework to optimize prompts in self-evolve manner. In our framework, we decouple prompts into different components that can be used to explore the impact that different factors may have on various tasks. On this basis, we introduce working memory, through which LLMs can alleviate the deficiencies caused by their own uncertainties and further obtain key insights to guide the generation of new prompts. Extensive experiments conducted on different tasks covering various domains for both open- and closed-source LLMs, including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini. Experimental results show that DelvePO consistently outperforms previous SOTA methods under identical experimental settings, demonstrating its effectiveness and transferability across different tasks.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.CL",
        "crawl_time": "2025-10-22T11:00:05.562567",
        "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： **第一步：核心判断——保留** 该论文的核心贡献是提出了一种名为DelvePO的、用于提示优化的自我演化框架。这完全符合你研究范围中的第三个核心方向：**“自我演化”**。论文的本质是构建一个新的方法论框架，以解决现有提示优化方法的局限性（如易陷入局部最优、性能不稳定），而不是简单地将LLM应用到一个特定领域。因此，它不属于“非演化型应用”或“非Agentic的推理”。 **第二步：正面指标——高度匹配** 论文摘要中包含了多个你关注的核心关键词和概念： *   **核心范式**: `Self-Evolving` (在标题和摘要中多次出现)、`Self-Evolving Framework`。 *   **智能体能力**: `Memory` (明确引入了“working memory”工作记忆机制)。 *   **演化机制**: `Self-Improvement` / `Self-Refine` (整个“self-evolve manner”就是一种自我完善机制)。 这些正面指标强烈表明该论文与你的研究焦点高度相关。 **第三步：排除标准——未触发** 论文摘要中没有提及任何关于安全、对齐、可解释性或幻觉的内容。研究内容完全集中在文本提示的优化上，不涉及视觉或多模态。因此，它没有触及任何排除标准。 **第四步：处理特殊和模糊情况——强化保留判断** 根据你对“自我演化的应用”的特殊规则，这篇论文是**规则中的“例外”情况，是应该被保留的核心类型**。论文的核心贡献**就是**提出一种新的“自我演化”机制，即使它被用来解决“提示优化”这个相对具体的问题，其本质仍然是对智能体自我演化能力的探索和构建。这个机制本身是任务无关的，证明了其通用性和基础研究价值。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于构建一个具有**自我演化**和**记忆**能力的新框架，以改进LLM智能体的核心能力之一（提示优化）。这与你寻找“构建、改进或演化LLM智能体”的论文的目标高度一致。它不仅属于你的研究范畴，而且在其核心方向（自我演化）上具有很高的相关性和前沿性。因此，最终决策为保留。"
    },
    {
        "index": "#60",
        "title": "Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models",
        "link": "/arxiv/2510.17922",
        "arxiv_id": "2510.17922",
        "authors": "Shuodi Liu, Yingzhuo Liu, Zi Wang, Yusheng Wang, Huijia Wu, Liuyu Xiang, Zhaofeng He",
        "summary": "Large language models (LLMs) have demonstrated remarkable reasoning and planning capabilities, driving extensive research into task decomposition. Existing task decomposition methods focus primarily on memory, tool usage, and feedback mechanisms, achieving notable success in specific domains, but they often overlook the trade-off between performance and cost. In this study, we first conduct a comprehensive investigation on task decomposition, identifying six categorization schemes. Then, we perform an empirical analysis of three factors that influence the performance and cost of task decomposition: categories of approaches, characteristics of tasks, and configuration of decomposition and execution models, uncovering three critical insights and summarizing a set of practical principles. Building on this analysis, we propose the Select-Then-Decompose strategy, which establishes a closed-loop problem-solving process composed of three stages: selection, execution, and verification. This strategy dynamically selects the most suitable decomposition approach based on task characteristics and enhances the reliability of the results through a verification module. Comprehensive evaluations across multiple benchmarks show that the Select-Then-Decompose consistently lies on the Pareto frontier, demonstrating an optimal balance between performance and cost. Our code is publicly available at https://github.com/summervvind/Select-Then-Decompose.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-20",
        "category": "cs.CL",
        "crawl_time": "2025-10-22T11:00:05.581768",
        "filter_reason": "这篇论文符合您的研究范围，应被保留。我的判断过程如下： **第一步：核心判断——论文的本质是什么？** 这篇论文的核心贡献是提出了一种名为“Select-Then-Decompose”的新策略，用于改进LLM在复杂任务中的表现。该策略建立了一个包含“选择、执行、验证”的闭环问题解决流程。这本质上是一个关于**智能体如何进行规划和多步推理**的新方法论框架。它不是简单地将LLM作为工具应用，而是构建了一个更智能、更具适应性的任务处理流程。因此，根据第一步的“保留”标准（构建、改进LLM智能体的方法论或新框架），这篇论文应被保留。 **第二步：正面指标——论文是否包含我的核心关注点？** 论文明确包含了多个核心关注点： 1.  **智能体能力**: 论文的核心是“任务分解”（Task Decomposition），这是**规划（Planning）**能力的关键组成部分。其提出的“选择、执行、验证”闭环流程，与经典的**ReAct**（Reasoning and Acting）范式高度相似，都涉及推理、行动和验证的循环。 2.  **核心范式**: 论文的研究内容直接服务于**Agentic AI**和**LLM-based Agents**，因为它旨在提升智能体自主解决复杂任务的能力。 3.  **演化机制**: 论文提出的策略能够“动态选择最合适的分解方法”，这体现了智能体根据任务特性进行**自我调整（Self-Adjustment）**和**迭代改进（Iterative Improvement）**的能力，是自我演化的一种体现。 **第三步：排除标准——是否为我的研究焦点之外？** 论文不涉及任何排除标准中的内容。它的主要贡献不是关于安全、对齐、可解释性，也不是关于多模态或视觉模型。其焦点完全集中在提升智能体的任务规划和执行效率上。 **第四步：处理特殊和模糊情况** 这篇论文恰好处于“推理/规划”这一特殊情况的“保留”范畴。 - **保留**: 论文是关于智能体如何在复杂任务中进行规划和多步推理。它不是在研究如何提升LLM的基础数学或逻辑能力，而是在构建一个让LLM能够更好地进行规划和分解任务的**Agentic框架**。其“选择-分解-验证”的闭环流程，正是智能体自主规划和执行的体现。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种新的、自适应的LLM智能体任务分解与规划策略。它直接命中了您研究焦点中的“单智能体 (Agentic)”方向，特别是“规划”和“自我反思/验证”子方向。论文通过构建一个闭环的、动态选择的框架来改进智能体的能力，完全符合您“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的核心目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#72",
        "title": "Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents",
        "link": "/arxiv/2510.18476",
        "arxiv_id": "2510.18476",
        "authors": "Feifan Xia, Yuyang Fang, Defang Li, Yantong Xie, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang",
        "summary": "We present a probabilistic intent modeling framework for large language model (LLM) agents in multi-turn social dialogue. The framework maintains a belief distribution over a partner's latent intentions, initialized from contextual priors and dynamically updated through likelihood estimation after each utterance. The evolving distribution provides additional contextual grounding for the policy, enabling adaptive dialogue strategies under uncertainty. Preliminary experiments in the SOTOPIA environment show consistent improvements: the proposed framework increases the Overall score by 9.0% on SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and slightly surpasses an oracle agent that directly observes partner intentions. These early results suggest that probabilistic intent modeling can contribute to the development of socially intelligent LLM agents.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-21",
        "category": "cs.CL",
        "crawl_time": "2025-10-22T11:00:05.593447",
        "filter_reason": "这篇论文完全符合你的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM智能体作为工具去解决一个外部领域问题，而是**构建并改进LLM智能体本身**。其核心贡献是提出了一个“概率意图建模框架”，这是一个用于增强LLM智能体在社交互动中能力的新方法论。这直接命中了你“构建、改进或演化LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **多智能体**: 论文明确研究“multi-turn social dialogue”中的“partner's latent intentions”，这完全属于**多智能体系统**中的**通信**和**协作**范畴。 - **智能体能力**: 该框架通过维护一个“belief distribution”来增强智能体的**记忆**和状态感知能力。同时，它为“policy”提供上下文，以实现“adaptive dialogue strategies”，这直接关联到智能体的**规划**能力。 - **演化机制**: 摘要中提到“dynamically updated”和“evolving distribution”，虽然不是完整的自我迭代，但描述了一个在交互中动态演化的内部状态，这与**自我演化**的理念高度相关。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或多模态。它的焦点是智能体的社交能力和策略，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文研究的“adaptive dialogue strategies”是典型的智能体**规划**行为。它不是在提升LLM的基础数学或逻辑推理，而是在研究智能体如何在不确定的社交环境中进行多步决策和规划，这符合保留条件。 **核心依据总结**: 该论文的核心贡献在于为LLM智能体设计了一个全新的、用于理解社交伙伴意图的概率模型。这项工作直接推动了**多智能体**领域中的**通信**和**协作**能力，并涉及了智能体的**规划**和**记忆**机制。它完全符合你筛选标准中关于“构建、改进LLM智能体”和“多智能体”的研究方向，因此是一篇高度相关的前沿论文。"
    },
    {
        "index": "#79",
        "title": "SMaRT: Select, Mix, and ReinvenT - A Strategy Fusion Framework for LLM-Driven Reasoning and Planning",
        "link": "/arxiv/2510.18095",
        "arxiv_id": "2510.18095",
        "authors": "Nikhil Verma, Manasa Bharadwaj, Wonjun Jang, Harmanpreet Singh, Yixiao Wang, Homa Fashandi, Chul Lee",
        "summary": "Large Language Models (LLMs) have redefined complex task automation with exceptional generalization capabilities. Despite these advancements, state-of-the-art methods rely on single-strategy prompting, missing the synergy of diverse reasoning approaches. No single strategy excels universally, highlighting the need for frameworks that fuse strategies to maximize performance and ensure robustness. We introduce the Select, Mix, and ReinvenT (SMaRT) framework, an innovative strategy fusion approach designed to overcome this constraint by creating balanced and efficient solutions through the seamless integration of diverse reasoning strategies. Unlike existing methods, which employ LLMs merely as evaluators, SMaRT uses them as intelligent integrators, unlocking the \"best of all worlds\" across tasks. Extensive empirical evaluations across benchmarks in reasoning, planning, and sequential decision-making highlight the robustness and adaptability of SMaRT. The framework consistently outperforms state-of-the-art baselines in solution quality, constraint adherence, and performance metrics. This work redefines LLM-driven decision-making by pioneering a new paradigm in cross-strategy calibration, unlocking superior outcomes for reasoning systems and advancing the boundaries of self-refining methodologies.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-20",
        "category": "cs.CL",
        "crawl_time": "2025-10-22T11:00:05.603101",
        "filter_reason": "这篇论文完全符合您的研究范围。我的判断依据如下： 1.  **第一步：核心判断 (保留)** 论文的核心贡献是提出了一个名为 **SMaRT** 的 **“策略融合框架”**。这并非简单地将现有LLM或智能体框架应用于某个领域，而是提出了一种全新的方法论来构建和改进LLM智能体的核心能力。其本质是关于如何构建一个更强大的智能体系统，使其能够自主地选择、混合和创造推理策略，这直接属于“构建、改进LLM智能体”的范畴。因此，它通过了第一步的核心判断。 2.  **第二步：正面指标 (高度匹配)** 论文摘要中包含了多个核心关注点： *   **智能体能力**: 明确提到了 `Planning` (规划)、`Reasoning` (推理) 和 `sequential decision-making` (序列决策)，这些都是单智能体的核心能力。 *   **演化机制**: 摘要结尾处明确指出，该工作“advancing the boundaries of **self-refining methodologies**”（推进了自我完善方法的边界）。框架中的“ReinvenT”（再创造）部分也暗示了智能体能够动态地调整和生成新策略，这与“自我演化”和“自我反思”的焦点高度契合。 3.  **第三步：排除标准 (未触发)** 论文的研究焦点是提升智能体的决策和推理框架，没有涉及安全、对齐、可解释性或视觉等多模态内容。因此，它没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况 (符合保留规则)** 论文的研究内容是关于 **推理/规划** 的。根据规则，需要区分是“智能体的规划”还是“LLM的基础推理能力”。这篇论文明确提出了一个框架来整合多种策略（如ReAct、ToT等），其目标是提升智能体在复杂任务中的多步决策能力，而不是通过微调或新数据集来提升LLM本身的数学或逻辑能力。因此，它属于“保留”的范畴，即“关于智能体如何进行规划或在复杂任务中进行多步推理”。 **最终决策**: 该论文的核心贡献是提出了一种创新的框架（SMaRT），通过融合多种策略来增强LLM智能体的规划、推理和自我完善能力。这完全符合您研究课题中的“单智能体”和“自我演化”方向。它不是简单的应用，而是对智能体架构和能力的根本性改进，是典型的Agentic AI研究。因此，最终判断为 **True**。"
    },
    {
        "index": "#5",
        "title": "Search Self-play: Pushing the Frontier of Agent Capability without Supervision",
        "link": "/arxiv/2510.18821",
        "arxiv_id": "2510.18821",
        "authors": "Hongliang Lu, Yuhang Wen, Pengyu Cheng, Ruijin Ding, Haotian Xu, Jiaqi Guo, Chutian Wang, Haonan Chen, Xiaoxi Jiang, Guanjun Jiang",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has become the mainstream technique for training LLM agents. However, RLVR highly depends on well-crafted task queries and corresponding ground-truth answers to provide accurate rewards, which requires massive human efforts and hinders the RL scaling processes, especially under agentic scenarios. Although a few recent works explore task synthesis methods, the difficulty of generated agentic tasks can hardly be controlled to provide effective RL training advantages. To achieve agentic RLVR with higher scalability, we explore self-play training for deep search agents, in which the learning LLM utilizes multi-turn search engine calling and acts simultaneously as both a task proposer and a problem solver. The task proposer aims to generate deep search queries with well-defined ground-truth answers and increasing task difficulty. The problem solver tries to handle the generated search queries and output the correct answer predictions. To ensure that each generated search query has accurate ground truth, we collect all the searching results from the proposer's trajectory as external knowledge, then conduct retrieval-augmentation generation (RAG) to test whether the proposed query can be correctly answered with all necessary search documents provided. In this search self-play (SSP) game, the proposer and the solver co-evolve their agent capabilities through both competition and cooperation. With substantial experimental results, we find that SSP can significantly improve search agents' performance uniformly on various benchmarks without any supervision under both from-scratch and continuous RL training setups. The code is at https://github.com/Alibaba-Quark/SSP.",
        "subjects": "Machine Learning",
        "date": "2025-10-21",
        "category": "cs.LG",
        "crawl_time": "2025-10-22T11:00:06.289713",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于提出了一种新颖的LLM智能体自我演化机制。以下是根据您的筛选标准进行的详细判断过程： **第一步：核心判断——这篇论文的本质是什么？** - **保留 (Keep)**。这篇论文的本质不是将现有智能体应用于某个领域，而是提出了一种名为“搜索自我对弈”（Search Self-play, SSP）的**新方法论和训练框架**。该框架的核心是让LLM智能体通过自我博弈的方式进行训练，一个智能体作为“任务提出者”，另一个作为“问题解决者”，两者共同演化。这直接对应了您研究目标中的“构建、改进或演化 LLM智能体”，特别是“自我演化”方向。 **第二步：正面指标——论文是否包含我的核心关注点？** 该论文包含了大量您关注的核心指标，相关性极高： - **核心范式**: 论文明确提出了一个**自我演化（Self-Evolving）**的框架，并且其机制涉及**多智能体系统（Multi-Agent Systems）**，尽管这两个角色由同一个LLM扮演。 - **智能体能力**: 论文聚焦于**搜索智能体（Search Agents）**，这必然涉及复杂的**规划（Planning）**（多轮搜索调用）和**工具使用（Tool Use）**（调用搜索引擎）。 - **多智能体**: 论文的核心机制是“自我对弈”，其中包含了**协作（Cooperation）**（共同提升）和**博弈（Competition）**（提出更难的题 vs 解决难题）的元素。 - **演化机制**: 论文的标题和摘要都强调了其核心贡献是**自我演化**。具体机制是“**共同演化（co-evolve）**”，通过迭代式的自我博弈实现**迭代改进（Iterative Improvement）**。 **第三步：排除标准——是否为我的研究焦点之外？** 该论文不触及任何排除标准： - **安全与对齐**: 论文焦点是提升智能体能力，而非安全、对齐或可解释性。 - **多模态与视觉**: 论文专注于文本搜索任务，不涉及视觉或多模态内容。 **第四步：处理特殊和模糊情况** - **推理/规划**: 论文研究的是智能体如何通过多轮调用工具（搜索引擎）来完成复杂任务，这属于**智能体层面的规划和推理**，而非提升LLM本身的基础数学或逻辑能力，因此符合保留条件。 - **自我演化的应用**: 论文的核心贡献就是**提出一种新的“自我演化”机制**（SSP框架）。即使它被应用在“搜索”这个特定领域，根据您的规则（“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”），也必须保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种创新的、基于自我对弈的LLM智能体训练框架，使智能体能够在无监督的情况下通过自我博弈实现能力的共同演化。这完美契合您研究课题中的“自我演化（Self-Evolving）”方向，并与“单智能体（Agentic）”和“多智能体（Multi-Agent）”的多个子方向（规划、工具使用、协作、博弈）紧密相关。因此，这篇论文是您研究范围内的前沿高质量论文。"
    },
    {
        "index": "#6",
        "title": "Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards",
        "link": "/arxiv/2510.18814",
        "arxiv_id": "2510.18814",
        "authors": "Mengqi Li, Lei Zhao, Anthony Man-Cho So, Ruoyu Sun, Xiao Li",
        "summary": "We present a simple, self-help online supervised finetuning (OSFT) paradigm for LLM reasoning. In this paradigm, the model generates its own responses and is immediately finetuned on this self-generated data. OSFT is a highly efficient training strategy for LLM reasoning, as it is reward-free and uses just one rollout by default. Experiment results show that OSFT achieves downstream performance on challenging mathematical reasoning tasks comparable to strong reinforcement learning with verifiable rewards (RLVR) methods such as GRPO. Our ablation study further demonstrates the efficiency and robustness of OSFT. The major mechanism of OSFT lies in facilitating the model's own existing preference (latent knowledge) learned from pretraining, which leads to reasoning ability improvement. We believe that OSFT offers an efficient and promising alternative to more complex, reward-based training paradigms. Our code is available at https://github.com/ElementQi/OnlineSFT.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.LG",
        "crawl_time": "2025-10-22T11:00:06.290417",
        "filter_reason": "这篇论文完全符合我的研究范围，核心依据在于其贡献的本质属于“自我演化”方向。 1.  **第一步核心判断 (保留)**: 论文的核心贡献并非简单应用，而是提出了一种全新的训练范式——在线监督微调 (OSFT)。这个范式的核心机制是“模型生成自己的响应，并立即基于这些自生成的数据进行微调”。这是一个明确的、迭代的自我完善过程，符合“自我演化”的定义。它不是将已有框架应用于新领域，也不是非演化的应用。 2.  **第二步正面指标 (高度相关)**: 论文的研究重点与我的核心关注点高度契合。 *   **演化机制**: 论文的标题和摘要反复强调 \"Self-Tuning\" 和 \"self-help\"。其核心机制 `Self-Improvement`, `Self-Refine`, `Iterative Improvement` 的特征非常明显。模型通过自我生成的数据进行迭代优化，这正是自我演化的体现。 *   **智能体能力**: 虽然论文聚焦于 \"Reasoning\"，但它实现推理提升的方式不是通过静态的数据集或提示工程，而是通过一个动态的、自我调节的训练循环。这可以被看作是智能体在“元层面”上进行自我反思和改进，以提升其基础能力。 3.  **第三步排除标准 (不涉及)**: 论文的研究内容是提升模型的推理能力和训练效率，完全不涉及安全、对齐、可解释性或视觉等多模态领域，因此没有触及任何排除标准。 4.  **第四步特殊情况处理 (适用)**: *   **推理/规划**: 这是最关键的一点。虽然论文表面上是关于“LLM推理”，但它不属于“非Agentic的推理”。传统的非Agentic推理研究可能提出一个新的CoT变体或一个用于微调的静态数据集。而这篇论文提出的是一个**过程性、动态化的自我调优机制**。模型主动生成数据并利用这些数据来更新自身，这赋予了模型一种自主完善的属性，使其超越了单纯的“能力提升”，进入了“机制演化”的范畴，这正是我关注的Agentic AI的核心。 **综上所述**，这篇论文的核心贡献是提出了一种名为OSFT的、能让LLM通过自身生成的数据进行迭代优化的新范式。这是一种典型的“自我演化”机制，虽然其应用目标是提升基础推理能力，但其方法论的原创性和演化特性，使其完全符合我关于“LLM智能体及其演化”的研究课题，特别是“自我演化”这一核心方向。因此，应予以保留。"
    },
    {
        "index": "#75",
        "title": "SPACeR: Self-Play Anchoring with Centralized Reference Models",
        "link": "/arxiv/2510.18060",
        "arxiv_id": "2510.18060",
        "authors": "Wei-Jer Chang, Akshay Rangesh, Kevin Joseph, Matthew Strong, Masayoshi Tomizuka, Yihan Hu, Wei Zhan",
        "summary": "Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose SPACeR, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10x faster at inference and 50x smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.",
        "subjects": "Machine Learning, Artificial Intelligence, Robotics",
        "date": "2025-10-20",
        "category": "cs.LG",
        "crawl_time": "2025-10-22T11:00:06.361789",
        "filter_reason": "这篇论文符合您的筛选标准，应被保留。我的判断过程如下： 1.  **第一步：核心判断——论文本质是方法论框架** 论文的核心贡献是提出了一种名为 **SPACeR** 的新**框架**。这个框架并非简单地将现有智能体应用于自动驾驶领域，而是提出了一种**新的训练范式**：利用一个预训练的类LLM模型作为“中央参考策略”，来指导和锚定多智能体的自我博弈强化学习过程。这完全符合“构建、改进或演化 LLM智能体”的核心目标。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步：正面指标——高度匹配研究焦点** 该论文与您的两个核心研究方向高度相关： *   **多智能体:** 论文明确在“多智能体设置”中工作，其核心是解决“多智能体交互”问题。它通过“去中心化的自我博弈”来训练智能体，这是多智能体系统研究的经典方法。论文的目标是生成能够进行社会性交互的“模拟智能体策略”。 *   **自我演化:** 论文的训练机制“自我博弈强化学习”本身就是一种典型的自我演化形式，智能体通过与自身及环境的交互不断迭代和改进。SPACeR框架的创新之处在于，它引入了一种**新的演化引导机制**（通过参考模型的KL散度奖励），使得演化过程更贴近人类行为规范，这是一种对自我演化机制的**改进**。 3.  **第三步：排除标准——未触发** - 论文虽然涉及自动驾驶的“安全”目标，但其主要贡献是SPACeR训练框架，而非新的安全算法，因此不属于安全与对齐的排除范围。 - 论文处理的是“分词自回归运动模型”，而非原始视觉数据，因此不涉及多模态与视觉的排除标准。 4.  **第四步：处理特殊和模糊情况——适用“自我演化的应用”例外规则** 这是判断的关键。虽然论文的应用领域是“自动驾驶”，但根据您设定的特殊规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。” - SPACeR框架的核心创新点正是**一种新的自我演化机制**：它将模仿学习（来自参考模型）与强化学习（自我博弈）相结合，以引导智能体种群的演化。这完全符合该例外规则，因此即使应用在自动驾驶领域，也应被保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出了一种新颖的框架（SPACeR），用于**改进多智能体系统**的训练过程。其训练机制（带锚定的自我博弈）本质上是一种**创新的自我演化方法**。尽管其应用场景是自动驾驶，但其方法论贡献完全契合您在“多智能体”和“自我演化”方向的研究目标。因此，这篇论文高度相关，应被保留。"
    },
    {
        "index": "#85",
        "title": "Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning",
        "link": "/arxiv/2510.17923",
        "arxiv_id": "2510.17923",
        "authors": "Chenwei Tang, Jingyu Xing, Xinyu Liu, Wei Ju, Jiancheng Lv, Deng Xiong, Ziyue Qiao",
        "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing Large Language Models (LLMs), achieving remarkable performance in complex reasoning domains such as mathematics and code generation. However, current RL methods face a fundamental scalability bottleneck due to their heavy reliance on human-curated preference data or labeled datasets for reward modeling. To overcome this limitation, we explore RL on unlabeled data where models learn autonomously from continuous experience streams. The core challenge in this setting lies in reliable reward estimation without ground-truth supervision. Existing approaches like Test-Time RL address this through self-consistent consensus, but risk reinforcing incorrect pseudo-labels derived from majority voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel test-time reward mechanism that operates without external supervision. COMPASS integrates two complementary components: the Dual-Calibration Answer Reward (DCAR), which stabilizes training by establishing trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which directly optimizes the reasoning process quality beyond mere outcome supervision. By jointly reinforcing trustworthy consensus answers and highly decisive reasoning chains, the COMPASS systematically enhances the model's analytical capabilities. Extensive experiments show that COMPASS achieves significant and consistent performance gains across diverse reasoning tasks and model architectures, advancing a more scalable direction for LLMs to learn from continuous experience.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-20",
        "category": "cs.LG",
        "crawl_time": "2025-10-22T11:00:06.379515",
        "filter_reason": "这篇论文完全符合您的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献是提出了一种名为COMPASS的新型奖励机制，用于在测试时进行强化学习。其本质是让LLM在没有外部监督的情况下，通过“从连续的经验流中自主地学习”来提升自身能力。这直接对应了您研究目标中的“自我演化”方向。它不是一个简单的应用，而是一个让智能体进行自我完善和迭代的**方法论或新框架**。 2.  **第二步：正面指标** - 论文与您的核心关注点高度契合： - **自我演化**: 论文的核心是让模型“learn autonomously from continuous experience streams”，这正是自我演化的定义。关键词如`Self-Scoring`、`Self-Improvement`、`Iterative Improvement`都体现在其方法中。 - **智能体能力**: 论文强调“directly optimizes the reasoning process quality beyond mere outcome supervision”，并关注“reasoning chains”。这表明它不仅仅关注最终答案，更关注智能体的**规划**和**推理过程**，这是单智能体研究的核心能力。 3.  **第三步：排除标准** - 论文不涉及任何关于安全、对齐、可解释性或多模态的内容。其焦点完全集中在提升模型的自主推理和演化能力上，因此不触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是关于智能体推理的典型范例。它没有停留在提升LLM的基础Token预测能力，而是构建了一个框架（Test-Time RL + COMPASS奖励机制）来优化整个推理链的质量。这符合“保留”的条件，即“关于智能体如何进行规划或在复杂任务中进行多步推理”。 - **自我演化的应用**: 论文虽然应用于数学和代码等推理领域，但其核心贡献是COMPASS这个**新的“自我演化”机制**本身。根据您的规则，这种情况下应该保留。 **核心依据总结**: 该论文的核心贡献是提出了一种**无监督的自我演化机制（COMPASS）**，使LLM能够通过自主学习和迭代优化其**推理过程**，从而实现自我完善。这精准地命中了您研究课题的“自我演化”和“单智能体（规划/推理）”两个核心方向。它不是对LLM的简单应用，而是对LLM智能体能力构建和演化的根本性探索，因此是高度相关的前沿论文。"
    },
    {
        "index": "#97",
        "title": "GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing",
        "link": "/arxiv/2510.17843",
        "arxiv_id": "2510.17843",
        "authors": "Zongze Wu, Yani Guo, Churong Liang, Runnan Li",
        "summary": "Despite remarkable advances in Large Language Model capabilities, tool retrieval for agent-based systems remains fundamentally limited by reliance on semantic similarity, which fails to capture functional viability. Current methods often retrieve textually relevant but functionally inoperative tools due to parameter mismatches, authentication failures, and execution constraints--a phenomenon we term the semantic-functional gap. We introduce GRETEL, to address this gap through systematic empirical validation. GRETEL implements an agentic workflow that processes semantically retrieved candidates through sandboxed plan-execute-evaluate cycles, generating execution-grounded evidence to distinguish truly functional tools from merely descriptive matches. Our comprehensive evaluation on the ToolBench benchmark demonstrates substantial improvements across all metrics: Pass Rate (at 10) increases from 0.690 to 0.826, Recall (at 10) improves from 0.841 to 0.867, and NDCG (at 10) rises from 0.807 to 0.857.. These results establish that execution-based validation provides a more reliable foundation for tool selection than semantic similarity alone, enabling more robust agent performance in real-world applications.",
        "subjects": "Machine Learning, Artificial Intelligence, Software Engineering",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-22T11:00:06.404111",
        "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出一个名为 **GRETEL** 的新框架，其目标是解决LLM智能体在工具选择上的根本性缺陷（“语义-功能鸿沟”）。这并非将现有智能体简单地应用到一个新领域，而是**构建和改进LLM智能体本身的核心能力**——即更有效地使用工具。因此，它完全符合“构建、改进LLM智能体”的核心要求。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了多个您关注的核心范式和能力关键词： *   **核心范式**: 明确提到了 `agent-based systems` 和 `agentic workflow`。 *   **智能体能力**: 论文的焦点是 `Tool Use / Tool Augmentation`，并提出了一个 `plan-execute-evaluate cycles` 的工作流。这个循环机制本质上是一种**规划** 和 **自我反思/自我修正** 的体现，智能体通过执行和评估来验证自己的选择，从而进行迭代和优化。 3.  **第三步：排除标准——不涉及** 论文的主要贡献是提升智能体的功能性表现和鲁棒性，而非研究安全、对齐、可解释性或多模态。因此，它没有触及任何主要的排除标准。 4.  **第四步：处理特殊和模糊情况——符合保留规则** 论文提出的 `plan-execute-evaluate` 循环，是一种典型的智能体在复杂任务中进行多步推理和行动的框架。这与您保留标准中提到的“智能体如何进行规划或在复杂任务中进行多步推理（如 ReAct、ToT 或新的Agentic框架）”完全一致。它不是在提升LLM的基础Token预测能力，而是在构建一个让LLM能够更好地与外部世界（工具）交互的智能体框架。 **结论**: 综合以上分析，这篇论文的核心是提出一种创新的、基于执行验证的智能体框架，以增强LLM智能体的工具使用能力。这直接对您研究焦点中的**“单智能体”**方向，特别是**“工具使用”**和**“规划/自我反思”**子方向，做出了明确的方法论贡献。因此，该论文与您的研究课题高度相关。"
    },
    {
        "index": "#134",
        "title": "Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning",
        "link": "/arxiv/2510.18254",
        "arxiv_id": "2510.18254",
        "authors": "Sion Weatherhead, Flora Salim, Aaron Belbasis",
        "summary": "Humans do not just find mistakes after the fact -- we often catch them mid-stream because 'reflection' is tied to the goal and its constraints. Today's large language models produce reasoning tokens and 'reflective' text, but is it functionally equivalent with human reflective reasoning? Prior work on closed-ended tasks -- with clear, external 'correctness' signals -- can make 'reflection' look effective while masking limits in self-correction. We therefore test eight frontier models on a simple, real-world task that is open-ended yet rule-constrained, with auditable success criteria: to produce valid scientific test items, then revise after considering their own critique. First-pass performance is poor (often zero valid items out of 4 required; mean $\\approx$ 1), and reflection yields only modest gains (also $\\approx$ 1). Crucially, the second attempt frequently repeats the same violation of constraint, indicating 'corrective gains' arise largely from chance production of a valid item rather than error detection and principled, constraint-sensitive repair. Performance before and after reflection deteriorates as open-endedness increases, and models marketed for 'reasoning' show no advantage. Our results suggest that current LLM 'reflection' lacks functional evidence of the active, goal-driven monitoring that helps humans respect constraints even on a first pass. Until such mechanisms are instantiated in the model itself, reliable performance requires external structure that enforces constraints.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-21",
        "category": "cs.LG",
        "crawl_time": "2025-10-22T11:00:06.422817",
        "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质并非将LLM作为工具应用于某个特定领域，也不是关于模型基础设施。它的核心贡献是**系统性地评估和揭示了当前LLM在“反思”这一核心智能体能力上的根本性缺陷**。虽然它没有提出一个“新”的智能体框架，但它对现有智能体能力（自我反思/自我修正）的深刻剖析，为“改进”和“演化”未来的LLM智能体提供了至关重要的方向性指导和理论基础。这属于对智能体核心能力的“改进”研究范畴，是构建更高级智能体（尤其是自我演化智能体）的必要前置工作。 2.  **第二步：正面指标** - 论文高度契合你的核心关注点。摘要中明确包含了大量正面指标：`Self-Reflection` (反思), `Self-Correction` (自我修正), `Reasoning` (推理)。论文设计的任务——“生成、自我批判、修订”——是一个典型的智能体自我迭代循环，直接触及了 `Self-Refine` 和 `Iterative Improvement` 的核心机制。 3.  **第三步：排除标准** - 论文不涉及任何排除标准。它的主要贡献不是关于安全、对齐、可解释性，也未涉及多模态或视觉模型。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美地符合“保留”条件。它研究的不是LLM的基础数学或逻辑推理能力，而是**智能体在复杂任务中如何进行自我修正的元认知过程**。论文指出现有的“反思”缺乏“主动的、目标驱动的监控”，这正是构建有效Agentic框架的关键。这项研究为如何设计真正具备自我反思和修正能力的智能体规划了路径。 - **自我演化的应用**: 论文虽然使用了一个具体任务（生成科学测试项），但这个任务是作为**探针**来研究“反思”这一通用机制的，而非论文的核心贡献。其核心发现——LLM反思的局限性——是普适性的，对任何需要自我演化的智能体都具有指导意义。 **最终决策**： 综合以上分析，这篇论文的核心贡献在于**深刻剖析了LLM智能体的一项关键能力——自我反思——的内在缺陷和失效模式**。它没有构建一个新系统，但它为所有试图构建、改进或演化具备自我反思和自我修正能力的LLM智能体的研究者，提供了不可或缺的见解和警告。这种对核心能力的“诊断性”研究，直接服务于“改进或演化LLM智能体”这一核心目标，因此是高度相关且极具价值的前沿论文。"
    },
    {
        "index": "#151",
        "title": "From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern Autonomous Recommender-Systems Research Labs",
        "link": "/arxiv/2510.18104",
        "arxiv_id": "2510.18104",
        "authors": "Joeran Beel, Bela Gipp, Tobias Vente, Moritz Baumgart, Philipp Meister",
        "summary": "Recommender-systems research has accelerated model and evaluation advances, yet largely neglects automating the research process itself. We argue for a shift from narrow AutoRecSys tools -- focused on algorithm selection and hyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab (AutoRecLab) that integrates end-to-end automation: problem ideation, literature analysis, experimental design and execution, result interpretation, manuscript drafting, and provenance logging. Drawing on recent progress in automated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems), we outline an agenda for the RecSys community: (1) build open AutoRecLab prototypes that combine LLM-driven ideation and reporting with automated experimentation; (2) establish benchmarks and competitions that evaluate agents on producing reproducible RecSys findings with minimal human input; (3) create review venues for transparently AI-generated submissions; (4) define standards for attribution and reproducibility via detailed research logs and metadata; and (5) foster interdisciplinary dialogue on ethics, governance, privacy, and fairness in autonomous research. Advancing this agenda can increase research throughput, surface non-obvious insights, and position RecSys to contribute to emerging Artificial Research Intelligence. We conclude with a call to organise a community retreat to coordinate next steps and co-author guidance for the responsible integration of automated research systems.",
        "subjects": "Information Retrieval, Artificial Intelligence, Machine Learning",
        "date": "2025-10-20",
        "category": "cs.LG",
        "crawl_time": "2025-10-22T11:00:06.441424",
        "filter_reason": "这篇论文符合您的研究范围，应被保留。我的判断过程如下： **第一步：核心判断** - **保留 (Keep)**。这篇论文的本质不是将LLM作为工具应用于推荐系统领域，而是**提出一个全新的、自主的、基于LLM智能体的研究框架（AutoRecLab）**。其核心贡献是构建一个能够自动化整个推荐系统研究流程的“实验室”，这本质上是一个复杂的LLM智能体系统。论文明确指出要借鉴“多智能体AI科学家”等系统的进展，其目标是构建一个能够进行“问题构思、文献分析、实验设计、结果解释、手稿起草”的自主智能体。这完全符合您“构建、改进或演化LLM智能体”的核心目标。 **第二步：正面指标** - 论文包含了大量您的核心关注点： - **核心范式**: 论文的核心是构建一个`LLM-based Agents`系统，并明确提到了`Multi-Agent AI Scientist`作为其灵感来源。 - **智能体能力**: AutoRecLab需要具备`Planning`（规划整个研究流程）、`Tool Use`（使用自动化实验工具）、`Memory`（记录研究过程和结果）等高级能力。 - **多智能体**: 虽然论文本身是提出一个概念，但它明确引用了多智能体系统作为其技术基础，表明其构建方向是多智能体协作的。 - **演化机制**: 论文提出的系统能够“产生可复现的发现”和“揭示非显而易见的见解”，这暗示了系统具有通过实验进行迭代和自我完善的能力，属于`Iterative Improvement`的范畴。 **第三步：排除标准** - **安全与对齐**: 论文虽然提到了“伦理、治理、隐私和公平”，但这并非其核心贡献。它是在呼吁社区在构建这类系统时需要考虑这些问题，属于对未来的展望和讨论，而不是一篇关于安全对齐技术的研究论文。因此，不触发排除标准。 - **多模态与视觉**: 论文内容不涉及多模态或视觉，不触发排除标准。 **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文完美地符合“自我演化的应用”的例外规则。它的应用领域是推荐系统研究（RecSys），但其核心贡献是提出一种**新的“自主研究”机制**。它不是简单地用LLM做推荐，而是构建一个能自主进行科学研究的智能体。这与您要求的“用于化学实验的自我演化智能体”是同一类贡献，即**提出一种新的智能体范式，并将其应用于特定领域**。因此，必须保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出一个名为AutoRecLab的自主研究实验室框架。该框架本质上是一个复杂的、多功能的LLM智能体系统，旨在自动化整个科学研究流程。它直接触及了您研究焦点的“单智能体”（规划、工具使用）和“多智能体”（借鉴多智能体系统）方向，并蕴含了“自我演化”（通过实验迭代改进）的潜力。尽管其应用场景是推荐系统领域，但其贡献在于智能体构建方法论本身，而非领域应用。因此，这篇论文高度符合您的研究范围。"
    },
    {
        "index": "#3",
        "title": "Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval",
        "link": "/arxiv/2510.18659",
        "arxiv_id": "2510.18659",
        "authors": "Dong Yun, Marco Schouten, Dim Papadopoulos",
        "summary": "User queries in information retrieval are often ambiguous, making it challenging for systems to identify a user's target from a single query. While recent dialogue-based interactive retrieval systems can clarify user intent, they are inefficient as they often lack an explicit strategy to ask the most informative questions. To address this limitation, we propose SherlockLLM, a dialogue-driven retrieval framework that learns an optimal questioning strategy via Reinforcement Learning (RL) and avoids the need for large-scale annotated dialogue data. In our framework, an agent is trained to generate a sequence of binary questions to efficiently narrow down the search space. To validate our approach, we introduce a benchmark with both structured and unstructured tasks. Experimental results show that SherlockLLM is a robust and efficient solution. On the structured tasks, its performance matches strong baselines and approaches the theoretical optimal defined by binary search. On the challenging unstructured task, our agent significantly outperforms these baselines, showcasing its ability to learn a highly effective information-seeking dialogue policy.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.427512",
        "filter_reason": "这篇论文符合我的研究范围，应被保留。我的判断过程严格遵循了您提供的筛选标准： **第一步：核心判断** - **保留 (Keep)**。这篇论文的本质是构建一个名为 `SherlockLLM` 的LLM智能体框架。其核心贡献并非简单地将LLM应用于信息检索领域，而是提出了一种**方法论**，让智能体通过强化学习（RL）**学习一种最优的提问策略**。论文明确指出，该框架的核心是一个被训练来生成一系列问题的“agent”，这完全符合“构建、改进或演化LLM智能体”的核心目标。它不是对现有智能体框架的简单应用，而是对智能体能力（信息获取策略）的构建和改进。 **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `Agentic AI`, `LLM-based Agents`。论文的核心就是构建一个基于LLM的智能体。 - **智能体能力**: `Planning`。智能体需要规划一个提问序列来最高效地缩小搜索范围，这是一种高级的规划能力。虽然摘要未直接提及 `Tool Use`，但“提问”本身可以被视为一种与用户（环境）交互以获取信息的工具。 - **演化机制**: `Self-Improvement`, `Iterative Improvement`。论文的核心机制是使用强化学习（RL）来训练智能体，使其通过与环境（用户和知识库）的交互和反馈（奖励信号）来**自我完善**其提问策略。这完全符合“自我演化”的定义，即智能体通过经验进行自我完善和迭代。 **第三步：排除标准** - 论文不涉及任何排除标准。其核心贡献不是关于安全、对齐、可解释性，也不是关于多模态或视觉模型。它的焦点始终在智能体的决策和学习机制上。 **第四步：处理特殊和模糊情况** - **推理/规划 (Reasoning/Planning)**: 这篇论文是典型的“保留”案例。它研究的是智能体如何进行**多步规划**（生成一个提问序列）来完成一个复杂任务（高效检索），而不是提升LLM本身的基础推理能力。这与 `ReAct` 等范式在精神上是一致的，都是关于智能体的行动策略。 - **自我演化的应用 (Self-Evolving Applications)**: 这篇论文恰好命中了“保留 (例外)”的规则。虽然它被应用在“对话式检索”这个特定领域，但其核心贡献是提出了一种**新的“自我演化”机制**（通过RL学习提问策略）。因此，即使有应用背景，也应被保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献在于构建一个能够通过强化学习自我演化其规划策略（提问序列）的LLM智能体。它直接触及了“单智能体”的规划能力和“自我演化”的核心机制，完全符合您的研究目标。因此，最终判断为 **True**。"
    },
    {
        "index": "#8",
        "title": "QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework",
        "link": "/arxiv/2510.18569",
        "arxiv_id": "2510.18569",
        "authors": "Junhyeog Yun, Hyoun Jun Lee, Insu Jeon",
        "summary": "Automating quantitative trading strategy development in dynamic markets is challenging, especially with increasing demand for personalized investment solutions. Existing methods often fail to explore the vast strategy space while preserving the diversity essential for robust performance across changing market conditions. We present QuantEvolve, an evolutionary framework that combines quality-diversity optimization with hypothesis-driven strategy generation. QuantEvolve employs a feature map aligned with investor preferences, such as strategy type, risk profile, turnover, and return characteristics, to maintain a diverse set of effective strategies. It also integrates a hypothesis-driven multi-agent system to systematically explore the strategy space through iterative generation and evaluation. This approach produces diverse, sophisticated strategies that adapt to both market regime shifts and individual investment needs. Empirical results show that QuantEvolve outperforms conventional baselines, validating its effectiveness. We release a dataset of evolved strategies to support future research.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.435856",
        "filter_reason": "这篇论文完全符合您的研究范围，应保留。判断依据如下： 1.  **第一步：核心判断** - 论文的核心贡献是构建了一个名为 **QuantEvolve 的演化框架**。这个框架的本质不是简单地将一个已有的智能体应用到金融领域，而是提出了一种**新的方法论**，即通过一个**多智能体系统**来**迭代地生成、评估和演化**量化交易策略。这完全符合您筛选标准中“构建、改进或演化 LLM智能体”或“多智能体系统”的核心要求。 - 这不属于“非演化型应用”的排除范畴，因为其核心创新点在于“演化”机制本身，而不是应用结果。 2.  **第二步：正面指标** - 论文命中了多个核心关注点： - **核心范式**: 明确提到了 `Multi-Agent System (MAS)` 和 `Evolutionary` Framework。 - **多智能体**: 描述了一个`hypothesis-driven multi-agent system`，智能体之间通过协作来`系统性地探索策略空间`，这涉及到协作和探索。 - **演化机制**: 整个框架就是围绕`演化`构建的，通过`迭代生成和评估`实现了`Self-Improvement`或`Iterative Improvement`。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全与对齐，也未聚焦于多模态或视觉技术。因此，不触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这是判断本论文的关键。尽管论文的应用领域是**量化金融**（一个特定垂直领域），但它完美地符合您设定的**例外规则**：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。” QuantEvolve 的核心正是这个“多智能体演化框架”，而非其在金融市场的具体表现。因此，应予以保留。 **最终决策**: 这篇论文的核心贡献在于提出了一种新颖的**多智能体**合作下的**自我演化**框架。它通过多个智能体的协作与迭代，实现了一个能够自我完善和发现的系统。这精准地命中了您研究范围中的“多智能体”和“自我演化”两个核心方向。尽管其应用场景是金融，但其方法论的创新性完全符合您的研究目标。因此，最终判断为 **True (保留)**。"
    },
    {
        "index": "#13",
        "title": "Crucible: Quantifying the Potential of Control Algorithms through LLM Agents",
        "link": "/arxiv/2510.18491",
        "arxiv_id": "2510.18491",
        "authors": "Lianchen Jia, Chaoyang Li, Qian Houde, Tianchi Huang, Jiangchuan Liu, Lifeng Sun",
        "summary": "Control algorithms in production environments typically require domain experts to tune their parameters and logic for specific scenarios. However, existing research predominantly focuses on algorithmic performance under ideal or default configurations, overlooking the critical aspect of Tuning Potential. To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven, multi-level expert simulation to turn algorithms and defines a formalized metric to quantitatively evaluate their Tuning Potential. We demonstrate Crucible's effectiveness across a wide spectrum of case studies, from classic control tasks to complex computer systems, and validate its findings in a real-world deployment. Our experimental results reveal that Crucible systematically quantifies the tunable space across different algorithms. Furthermore, Crucible provides a new dimension for algorithm analysis and design, which ultimately leads to performance improvements. Our code is available at https://github.com/thu-media/Crucible.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.438228",
        "filter_reason": "这篇论文符合您的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断** - **保留 (Keep)**。这篇论文的核心贡献是构建了一个名为 \"Crucible\" 的LLM智能体。该智能体的核心功能不是解决某个特定领域（如生物、金融）的问题，而是提出了一种新的方法论：通过LLM驱动的多级专家模拟来“测试”和“量化”其他算法的“调优潜力”（Tuning Potential）。这本质上是一个关于**如何构建一个具有特定能力的智能体**的研究，完全符合“构建、改进或演化LLM智能体”的核心目标。它不是简单地将LLM作为工具应用，而是将LLM作为智能体的核心“大脑”，设计了一套完整的智能体框架。 **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: 论文明确提出了一个 `LLM-based Agent` (Crucible)。 - **智能体能力**: Crucible智能体通过“多级专家模拟”来完成任务，这隐含了复杂的**规划**（Planning）能力，即如何设计模拟流程、如何与被测算法交互。同时，它将“调优”这一行为抽象为一种可以被智能体执行的任务，这是一种高级的**工具使用**（Tool Use），即把被测算法当作工具来探索其性能边界。 - **多智能体**: “多级专家模拟”（multi-level expert simulation）强烈暗示了**多智能体**（Multi-Agent）的范式。Crucible内部可能模拟了多个具有不同专业知识和策略的“专家”智能体，通过它们的协作或博弈来全面探索算法的调优空间。这属于“智能体间的协作、通信”等子方向。 **第三步：排除标准** - 论文不涉及主要的排除标准。其核心贡献不是关于安全、对齐、可解释性，也不是关于多模态或视觉模型。虽然它应用在“控制算法”和“计算机系统”领域，但根据第一步的判断，这些领域是作为验证其智能体能力的“试验场”，而非研究焦点本身。 **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是关于智能体如何进行规划和在复杂任务中进行多步推理的典型案例。Crucible的“多级专家模拟”过程就是一个复杂的、自主的规划和执行过程，远超非Agentic的基础推理能力提升。因此，符合保留条件。 - **自我演化的应用**: 虽然论文标题和摘要没有直接使用“自我演化”（Self-Evolving）这个词，但其核心思想——通过智能体系统性地探索和量化一个算法的“可调优空间”，并最终“带来性能提升”——与演化的精神内核（通过探索和迭代实现优化）高度相关。更重要的是，根据筛选规则，即使它被应用在特定领域（如控制算法），只要其核心是提出一种新的智能体机制，就应该保留。本文的核心正是提出“Crucible”这一智能体评估机制。 **第五步：最终决策** 综合以上分析，论文《Crucible: Quantifying the Potential of Control Algorithms through LLM Agents》的核心贡献在于**构建了一个新颖的LLM智能体框架（Crucible），该框架利用多智能体模拟来评估和探索其他算法的潜力**。这完全符合您“构建、改进或演化LLM智能体”的核心目标，并触及了单智能体（规划、工具使用）和多智能体（协作、模拟）两个关键方向。因此，最终判断为 **True**。"
    },
    {
        "index": "#10",
        "title": "SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation",
        "link": "/arxiv/2510.18551",
        "arxiv_id": "2510.18551",
        "authors": "Yuncheng Hua, Sion Weatherhead, Mehdi Jafari, Hao Xue, Flora D. Salim",
        "summary": "In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that treats simulator construction asinstance optimization over code within a textual computation graph. Specialized LLM-driven agents are embedded as graph nodes, and a workflow manager executes a loss-driven loop: code synthesis -> execution -> evaluation -> code repair. The optimizer performs Textual-Gradient Descent (TGD), while human-in-the-loop interaction is reserved for task-spec confirmation, minimizing expert effort and keeping the code itself as the trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption, and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy. By unifying multi-agent orchestration with a loss-aligned optimization view, SOCIA-Nabla converts brittle prompt pipelines into reproducible, constraint-aware simulator code generation that scales across domains and simulation granularities. This work is under review, and we will release the code soon.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.436851",
        "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **第一步核心判断：论文的本质是构建新的LLM智能体框架，而非简单应用。** -   摘要明确指出，本文的核心贡献是提出了“SOCIA-Nabla”，一个“end-to-end, **agentic framework**”（端到端的智能体框架）。这直接命中了“保留”标准。 -   论文并非简单地将一个已有的智能体框架（如ReAct）应用到“仿真器生成”这个领域。相反，它提出了一种全新的方法论，将仿真器构建过程视为一个“instance optimization over code”（代码实例优化）问题，并为此设计了专门的智能体架构。因此，它不属于“非演化型应用”的排除范畴。 2.  **第二步正面指标：论文高度契合您的核心关注点，特别是多智能体和自我演化。** -   **多智能体**: 摘要中明确出现了 “**Multi-Agent Orchestration**”（多智能体编排）和 “Specialized LLM-driven agents are embedded as graph nodes”（专门的LLM驱动的智能体被嵌入为图节点）。这表明其核心贡献之一是设计了一个多智能体系统。 -   **自我演化**: 论文的核心机制是一个 “**loss-driven loop: code synthesis -> execution -> evaluation -> code repair**”（损失驱动的循环：代码合成 -> 执行 -> 评估 -> 代码修复）。这个循环完美体现了“自我演化”和“自我修正”的精髓。智能体系统通过评估反馈（损失）来迭代地修复和改进其产出的代码，这是一种明确的自我完善机制。其提出的“**Textual-Gradient Descent (TGD)**”（文本梯度下降）是实现这种演化的具体优化算法，属于一种新颖的演化机制。 -   **智能体能力**: 该框架包含了规划（整个工作流）、工具使用（生成和执行代码）、自我反思/修正（通过评估和代码修复循环）等关键能力。 3.  **第三步排除标准：论文不涉及安全、对齐或多模态等排除焦点。** -   论文的摘要完全没有提及Safety, Alignment, Interpretability, Vision等关键词。其核心是方法论创新，而非安全或视觉应用。 4.  **第四步特殊/模糊情况处理：论文完美符合“自我演化的应用”这一例外规则。** -   论文确实将框架应用到了“CPS任务”（如用户建模）这一特定领域。但根据您的规则，只要**核心贡献是提出一种新的“自我演化”机制**，即使应用在特定领域，也应该保留。 -   本文的核心贡献正是这个结合了“多智能体编排”和“文本梯度下降”的**自我演化框架**。应用领域（仿真器生成）只是为了验证其有效性的实验场景，而非研究主题本身。 **最终决策**：综合以上分析，这篇论文的核心贡献在于构建了一个融合了多智能体协作与新颖自我演化机制（基于文本梯度的迭代修复）的LLM智能体框架。它精准地命中了您研究范围中的“多智能体”和“自我演化”两个核心方向，并且其方法论创新是普适性的，不属于特定领域的简单应用。因此，这篇论文是高度相关且应被保留的前沿研究。"
    },
    {
        "index": "#7",
        "title": "VAR: Visual Attention Reasoning via Structured Search and Backtracking",
        "link": "/arxiv/2510.18619",
        "arxiv_id": "2510.18619",
        "authors": "Wei Cai, Jian Zhao, Yuchen Yuan, Tianle Zhang, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li",
        "summary": "Multimodal Large Language Models (MLLMs), despite their advances, are hindered by their high hallucination tendency and heavy reliance on brittle, linear reasoning processes, leading to failures in complex tasks. To address these limitations, we introduce Visual Attention Reasoning (VAR), a novel framework that recasts grounded reasoning as a structured search over a reasoning trajectory space. VAR decomposes the reasoning process into two key stages: traceable evidence grounding and search-based chain-of-thought (CoT) generation, which incorporates a backtracking mechanism for self-correction. The search is guided by a multi-faceted reward function with semantic and geometric self-verification components, which penalize outputs that are not faithfully grounded in the visual input. We provide a theoretical analysis for our search strategy, validating its capability to find the correct solution with high probability. Experimental results show that our 7B model, VAR-7B, sets a new state-of-the-art on a comprehensive suite of hallucination and safety benchmarks, significantly outperforming existing open-source models and demonstrating competitive performance against leading proprietary systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.430030",
        "filter_reason": "我的判断过程如下，严格遵循您提供的筛选标准： **第一步：核心判断** - **论文的本质是什么？** 这篇论文的核心贡献是提出一个名为 VAR (Visual Attention Reasoning) 的新**框架**。该框架旨在解决多模态大语言模型（MLLMs）的推理问题。 - **是否符合保留条件？** 符合。VAR 不是一个简单的应用，而是一个构建和改进智能体能力的方法论。它将推理过程重塑为“结构化搜索”，并包含“回溯机制”，这直接关系到智能体的规划和自我修正能力。因此，它属于“构建、改进LLM智能体”的范畴，应予以**保留**。 **第二步：正面指标** - 论文包含了多个我的核心关注点： - **核心范式**: 论文提出的 VAR 框架本质上是一个 `Agentic AI` 框架，它定义了智能体如何进行推理。 - **智能体能力**: - `Planning`: 论文明确将推理定义为“在推理轨迹空间上的结构化搜索”，这是一种高级的规划形式。 - `Self-Correction`: 摘要中明确提到了“ incorporates a backtracking mechanism for self-correction”（包含用于自我修正的回溯机制），这是自我反思和自我修正能力的直接体现。 - `ReAct`: 其“traceable evidence grounding”（可追溯的证据定位）和“search-based chain-of-thought”的组合，与 ReAct (Reason+Act) 范式思想一致，即推理与行动（在这里是搜索和验证）相结合。 **第三步：排除标准** - **安全与对齐**: 论文在“安全基准”上进行了测试，但这只是为了评估其性能（如降低幻觉），论文的**核心贡献**是 VAR 这个推理框架本身，而不是提出一种新的安全或对齐方法。因此，这不符合排除标准。 - **多模态与视觉**: 这是需要仔细辨析的关键点。论文标题和摘要都明确指出是关于视觉的。根据您的规则：“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在这篇论文中，**视觉输入是智能体进行推理的环境和感知对象**，而研究的**核心**是那个“结构化搜索与回溯”的**推理框架**。这个框架本身是通用方法论，其创新点在于推理过程，而非视觉模型。因此，它符合例外情况，不应排除。 **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美地符合“保留”条件。它不是在提升LLM的基础数学或逻辑能力，而是在构建一个让智能体（特别是MLLM）在复杂任务中进行多步推理和规划的**新框架**。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是提出了一种名为 VAR 的新颖智能体推理框架。该框架通过引入结构化搜索、自我验证和回溯自我修正机制，显著提升了智能体的规划和推理能力。尽管它以视觉为应用场景，但其核心创新在于智能体的工作机制本身，完全符合我研究课题中的“单智能体”方向，特别是规划、自我反思和自我修正等子方向。因此，最终判断为**符合要求**。"
    },
    {
        "index": "#19",
        "title": "PlanU: Large Language Model Decision Making through Planning under Uncertainty",
        "link": "/arxiv/2510.18442",
        "arxiv_id": "2510.18442",
        "authors": "Ziwei Deng, Mian Deng, Chenjing Liang, Zeming Gao, Chennan Ma, Chenxing Lin, Haipeng Zhang, Songzhu Mei, Cheng Wang, Siqi Shen",
        "summary": "Large Language Models (LLMs) are increasingly being explored across a range of decision-making tasks. However, LLMs sometimes struggle with decision-making tasks under uncertainty that are relatively easy for humans, such as planning actions in stochastic environments. The adoption of LLMs for decision-making is impeded by uncertainty challenges, such as LLM uncertainty and environmental uncertainty. LLM uncertainty arises from the stochastic sampling process inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM uncertainty through multiple reasoning chains or search trees. However, these approaches overlook environmental uncertainty, which leads to poor performance in environments with stochastic state transitions. Some recent LDM approaches deal with uncertainty by forecasting the probability of unknown variables. However, they are not designed for multi-step decision-making tasks that require interaction with the environment. To address uncertainty in LLM decision-making, we introduce PlanU, an LLM-based planning method that captures uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of each node in the MCTS as a quantile distribution, which uses a set of quantiles to represent the return distribution. To balance exploration and exploitation during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity (UCC) score which estimates the uncertainty of MCTS nodes. Through extensive experiments, we demonstrate the effectiveness of PlanU in LLM-based decision-making tasks under uncertainty.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.446494",
        "filter_reason": "根据您提供的筛选标准，我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献是提出了一种名为 `PlanU` 的新方法，该方法旨在**改进LLM智能体的决策制定能力**。它不是将一个已有的智能体框架简单地应用到某个特定领域，而是专注于解决智能体在“不确定性环境”下进行“规划”这一核心挑战。论文的本质是构建和改进LLM智能体的规划方法论，完全符合“构建、改进或演化 LLM智能体”的核心目标。它不属于“非演化型应用”，因为其贡献在于方法本身，而非应用。 2.  **第二步：正面指标** - 论文明确命中了多个核心正面指标。标题和摘要中反复出现 `Planning`，这直接对应了研究焦点“单智能体”下的“规划”子方向。同时，论文讨论的是“决策制定”和“与环境交互”，这是 `Agentic AI` 的核心特征。它提出的 `Monte Carlo Tree Search (MCTS)` 框架是一种高级的规划和推理方法，与 `ReAct`、`ToT` 等范式属于同一类别，都是关于智能体的多步决策框架。 3.  **第三步：排除标准** - 论文的主要贡献不涉及 `Safety`、`Alignment`、`Interpretability` 或 `Vision` 等排除标准。其焦点是提升智能体在特定环境（不确定性环境）下的性能和鲁棒性，属于能力构建的范畴。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是“保留”的典型案例。它不是关于提升LLM的基础数学或逻辑推理能力，而是**关于智能体如何进行规划**。它提出了一个结合了MCTS和不确定性建模的全新Agentic框架，用于解决需要与环境进行多步交互的复杂决策任务。这完全符合“如果论文是关于智能体如何进行规划或在复杂任务中进行多步推理...则保留”的规则。 **最终决策**: 论文的核心贡献是提出了一种新的LLM智能体规划框架 `PlanU`，专门用于解决不确定性环境下的决策问题。这直接属于“单智能体”研究范畴下的“规划”方向，是对LLM智能体能力的一次重要改进。因此，该论文完全符合您的研究范围，应予以保留。"
    },
    {
        "index": "#16",
        "title": "LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources",
        "link": "/arxiv/2510.18477",
        "arxiv_id": "2510.18477",
        "authors": "Haichao Ji, Zibo Wang, Yifei Zhu, Meng han, Dan Wang, Zhu Han",
        "summary": "Large Language Models (LLMs) have shown great promise in automating data analytics tasks by interpreting natural language queries and generating multi-operation execution plans. However, existing LLM-agent-based analytics frameworks operate under the assumption of centralized data access, offering little to no privacy protection. In contrast, federated analytics (FA) enables privacy-preserving computation across distributed data sources, but lacks support for natural language input and requires structured, machine-readable queries. In this work, we present LAFA, the first system that integrates LLM-agent-based data analytics with FA. LAFA introduces a hierarchical multi-agent architecture that accepts natural language queries and transforms them into optimized, executable FA workflows. A coarse-grained planner first decomposes complex queries into sub-queries, while a fine-grained planner maps each subquery into a Directed Acyclic Graph of FA operations using prior structural knowledge. To improve execution efficiency, an optimizer agent rewrites and merges multiple DAGs, eliminating redundant operations and minimizing computational and communicational overhead. Our experiments demonstrate that LAFA consistently outperforms baseline prompting strategies by achieving higher execution plan success rates and reducing resource-intensive FA operations by a substantial margin. This work establishes a practical foundation for privacy-preserving, LLM-driven analytics that supports natural language input in the FA setting.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.439619",
        "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建了一个新颖的多智能体系统。 1.  **第一步：核心判断——这篇论文的本质是什么？** 论文的核心是提出一个名为LAFA的**层次化多智能体架构**。它不是简单地将一个已有的LLM智能体应用到联邦分析领域，而是**设计和构建了一个新的多智能体框架**来解决该领域的特定挑战（如自然语言查询转换、工作流优化）。因此，它属于“保留”类别，即核心贡献是关于构建LLM智能体或多智能体系统的方法论或新框架。 2.  **第二步：正面指标——论文是否包含我的核心关注点？** 论文包含了多个核心关注点： *   **核心范式**: 论文标题和摘要明确提到了 `Agentic LLM` 和 `hierarchical multi-agent architecture`，直接命中 `Agentic AI` 和 `Multi-Agent Systems (MAS)`。 *   **智能体能力**: 论文的核心部分是关于 `Planning`。它详细描述了一个“粗粒度规划器”用于分解查询，和一个“细粒度规划器”用于生成执行计划。这是典型的智能体规划能力研究。 *   **多智能体**: 论文明确提出了一个多智能体系统，其中包含不同角色的智能体（规划器、优化器），它们之间需要协作来完成从自然语言到可执行工作流的转换，这属于 `Collaboration` 的范畴。 3.  **第三步：排除标准——是否为我的研究焦点之外？** 论文的主要贡献是关于智能体架构和规划，而非安全对齐或多模态。虽然它涉及隐私保护（联邦分析），但这只是其应用场景和解决的问题，而非论文的核心方法论贡献。因此，它没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文的研究内容是“智能体如何进行规划或在复杂任务中进行多步推理”。它提出的分层规划机制（先分解，再映射，最后优化）是一个典型的Agentic框架下的规划方法，而不是提升LLM本身的基础推理能力。因此，这完全符合“保留”的条件。 5.  **第五步：最终决策** 综合来看，这篇论文的核心贡献是**构建了一个新颖的、用于解决特定问题的多智能体协作框架**。其研究内容直接聚焦于多智能体系统的架构设计和智能体规划能力，这与您研究课题中的“多智能体”方向高度契合。因此，这篇论文应该被保留。"
    },
    {
        "index": "#28",
        "title": "Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming",
        "link": "/arxiv/2510.18314",
        "arxiv_id": "2510.18314",
        "authors": "Zheng Zhang, Jiarui He, Yuchen Cai, Deheng Ye, Peilin Zhao, Ruili Feng, Hao Wang",
        "summary": "As large language model (LLM) agents increasingly automate complex web tasks, they boost productivity while simultaneously introducing new security risks. However, relevant studies on web agent attacks remain limited. Existing red-teaming approaches mainly rely on manually crafted attack strategies or static models trained offline. Such methods fail to capture the underlying behavioral patterns of web agents, making it difficult to generalize across diverse environments. In web agent attacks, success requires the continuous discovery and evolution of attack strategies. To this end, we propose Genesis, a novel agentic framework composed of three modules: Attacker, Scorer, and Strategist. The Attacker generates adversarial injections by integrating the genetic algorithm with a hybrid strategy representation. The Scorer evaluates the target web agent's responses to provide feedback. The Strategist dynamically uncovers effective strategies from interaction logs and compiles them into a continuously growing strategy library, which is then re-deployed to enhance the Attacker's effectiveness. Extensive experiments across various web tasks show that our framework discovers novel strategies and consistently outperforms existing attack baselines.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.456806",
        "filter_reason": "这篇论文的核心贡献完全符合您的研究范围，应予以保留。判断依据如下： 1.  **第一步：核心判断** - 这篇论文的本质不是简单地应用LLM智能体去解决一个安全问题。其核心贡献在于**构建了一个名为Genesis的新型智能体框架**，该框架的核心机制是**“演化”**（Evolving Attack Strategies）。它不是使用一个静态的、已有的智能体，而是设计了一个能够自我改进和迭代的系统。因此，它属于“构建、改进或演化 LLM智能体”的范畴，应当 **保留**。 2.  **第二步：正面指标** - 论文高度匹配您的核心关注点。标题和摘要中明确包含了 `Agentic` (agentic framework), `Self-Evolving` (Evolving Attack Strategies), `Evolutionary Algorithms` (genetic algorithm) 等核心范式。 - 其提出的框架包含多个模块（Attacker, Scorer, Strategist），形成了一个完整的**自我演化闭环**：Attacker执行行动，Scorer提供环境反馈，Strategist从反馈中学习并生成新策略来增强Attacker。这完美体现了 `Self-Improvement`, `Iterative Improvement` 和 `Self-Evolving` 的核心思想。 3.  **第三步：排除标准** - **安全与对齐**: 这是本案例最关键的一点。虽然论文的应用领域是“红队测试”，属于安全范畴，但根据您的筛选标准，需要判断“主要贡献”是否是关于安全本身。本文的主要贡献**不是发现了一个新的安全漏洞或提出了一种新的防御方法**，而是**提出了一种能让智能体自动、持续地发现和演化攻击策略的通用框架**。安全问题是其验证框架有效性的“试验场”，而非其科学贡献的核心。因此，它不应被排除。 - **多模态与视觉**: 论文不涉及此内容。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是您筛选规则中“自我演化的应用”这一例外情况的完美范例。正如规则所述：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域（如‘用于化学实验的自我演化智能体’），也应该保留。” 本文正是提出了一个用于“Web智能体红队测试”这一特定领域的“自我演化智能体”（Genesis框架），其核心价值在于演化机制本身，因此完全符合保留条件。 **总结**: 该论文的核心贡献是构建了一个由Attacker、Scorer和Strategist组成的智能体框架，通过遗传算法和策略库的动态更新，实现了攻击策略的**自我演化**与**持续改进**。尽管其应用场景是安全领域的红队测试，但其研究的本质和核心创新点在于**智能体如何通过环境反馈进行自我迭代和演化**，这与您研究课题中的“自我演化”方向高度契合。因此，这篇论文是前沿且高度相关的，应该被保留。"
    },
    {
        "index": "#36",
        "title": "LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior",
        "link": "/arxiv/2510.18155",
        "arxiv_id": "2510.18155",
        "authors": "Man-Lin Chu, Lucian Terhorst, Kadin Reed, Tom Ni, Weiwei Chen, Rongyu Lin",
        "summary": "Simulating consumer decision-making is vital for designing and evaluating marketing strategies before costly real- world deployment. However, post-event analyses and rule-based agent-based models (ABMs) struggle to capture the complexity of human behavior and social interaction. We introduce an LLM-powered multi-agent simulation framework that models consumer decisions and social dynamics. Building on recent advances in large language model simulation in a sandbox envi- ronment, our framework enables generative agents to interact, express internal reasoning, form habits, and make purchasing decisions without predefined rules. In a price-discount marketing scenario, the system delivers actionable strategy-testing outcomes and reveals emergent social patterns beyond the reach of con- ventional methods. This approach offers marketers a scalable, low-risk tool for pre-implementation testing, reducing reliance on time-intensive post-event evaluations and lowering the risk of underperforming campaigns.",
        "subjects": "Artificial Intelligence, Social and Information Networks",
        "date": "2025-10-20",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.466606",
        "filter_reason": "这篇论文符合您的研究范围，应予以保留。 **判断过程:** 1.  **第一步：核心判断** - **核心贡献**: 论文的核心贡献是引入了一个“LLM驱动的多智能体模拟框架”。它不是简单地使用一个已有的智能体框架去解决营销问题，而是**构建了一个新的框架**本身。这个框架能够让智能体在没有预定义规则的情况下进行交互、推理和决策。 - **判断**: 这完全符合筛选标准中“保留”的条件——“论文的核心是关于构建LLM智能体、多智能体系统的方法论或新框架”。虽然它应用于营销领域，但其本质是方法论创新，而非单纯的应用。 2.  **第二步：正面指标** - 论文标题和摘要中包含了多个核心关注点： - **核心范式**: `LLM-Based Multi-Agent System` (多智能体系统)。 - **多智能体**: `interact` (交互), `social dynamics` (社会动态), `emergent social patterns` (涌现的社会模式)。这些都直接对应了您研究焦点中的“智能体间的协作、通信、社会学习”。 - **智能体能力**: `express internal reasoning` (表达内部推理), `form habits` (形成习惯)。这触及了“记忆”和“自我反思”的范畴。 - **结论**: 论文与您的“多智能体”研究方向高度相关，并包含了智能体的核心能力。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或视觉多模态。因此，没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **应用 vs. 方法论**: 这是最关键的判断点。这篇论文虽然以营销为应用场景，但其价值主张在于提供了一种“新的框架”，能够“超越传统方法”。它是在展示这个框架如何工作，并用营销场景作为案例研究来验证其有效性。这符合“构建、改进或演化LLM智能体”的核心目标。它不是一篇营销学论文，而是一篇以营销为案例的AI框架论文。 - **自我演化**: 论文没有明确提及“自我演化”机制（如自我完善、迭代改进），其重点在于多智能体间的社会模拟。因此，它属于“多智能体”方向，而非“自我演化”方向，但这同样在您的核心研究范围之内。 **最终决策:** 综合以上分析，该论文的核心贡献是提出了一种新颖的LLM多智能体框架，用于模拟复杂的社会行为。这直接命中了您研究课题中的“多智能体”方向，并且是关于“构建”智能体系统的方法论创新，而非简单的领域应用。因此，这篇论文与您的研究范围高度契合，应被筛选为 **True**。"
    },
    {
        "index": "#20",
        "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library",
        "link": "/arxiv/2510.18428",
        "arxiv_id": "2510.18428",
        "authors": "Minwei Kong, Ao Qu, Xiaotong Guo, Wenbin Ouyang, Chonghe Jiang, Han Zheng, Yining Ma, Dingyi Zhuang, Yuhan Tang, Junyi Li, Hai Wang, Cathy Wu, Jinhua Zhao",
        "summary": "Optimization modeling enables critical decisions across industries but remains difficult to automate: informal language must be mapped to precise mathematical formulations and executable solver code. Prior LLM approaches either rely on brittle prompting or costly retraining with limited generalization. We present AlphaOPT, a self-improving experience library that enables an LLM to learn from limited demonstrations (even answers alone, without gold-standard programs) and solver feedback - without annotated reasoning traces or parameter updates. AlphaOPT operates in a continual two-phase cycle: (i) a Library Learning phase that reflects on failed attempts, extracting solver-verified, structured insights as {taxonomy, condition, explanation, example}; and (ii) a Library Evolution phase that diagnoses retrieval misalignments and refines the applicability conditions of stored insights, improving transfer across tasks. This design (1) learns efficiently from limited demonstrations without curated rationales, (2) expands continually without costly retraining by updating the library rather than model weights, and (3) makes knowledge explicit and interpretable for human inspection and intervention. Experiments show that AlphaOPT steadily improves with more data (65% to 72% from 100 to 300 training items) and surpasses the strongest baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only on answers. Code and data are available at: https://github.com/Minw913/AlphaOPT.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.447097",
        "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献精准地命中了“自我演化”方向。 **判断过程:** 1.  **第一步：核心判断——保留** *   **论文本质**: 这篇论文的核心贡献是提出了一个名为 **AlphaOPT** 的新框架。这个框架的本质是一个“自我改进的经验库”，它使LLM能够通过与环境（数学求解器）的交互反馈，进行持续的自我完善和迭代。 *   **关键机制**: 论文的核心不是简单地将LLM应用于优化领域，而是提出了一种全新的、能让LLM**“自我演化”**的机制。它通过一个持续的两阶段循环（库学习、库演化）来更新外部知识库，而不是更新模型权重，从而实现能力提升。 *   **排除项检查**: *   **非演化型应用**: 论文虽然应用于优化领域，但其核心贡献是**提出了一种新的“自我演化”机制**，完全符合你设定的例外情况（第四步规则2）。因此，它不是一篇简单的应用论文。 *   **非Agentic推理**: 论文涉及的是智能体如何在一个复杂任务（优化建模）中学习和改进，包含了自我反思（reflects on failed attempts）和迭代优化的完整循环，远超基础Token预测的推理能力。 *   **基础设施**: 与基础设施无关。 2.  **第二步：正面指标——强匹配** *   论文的标题和摘要中明确包含了 `Self-Improving`、`Self-Improvement`、`Library Evolution` 等关键词，这些都是“自我演化”方向的核心指标。 *   论文描述的“经验库”本身就是一种结构化的**记忆**形式。 *   “反思失败的尝试”直接对应了**自我反思**能力。 *   整个框架的设计就是一个**迭代改进**和**自我完善**的过程。 3.  **第三步：排除标准——不命中** *   论文虽然提到其知识库是“可解释的”，但这只是该框架的一个优点，并非论文的核心研究贡献。论文的主旨是研究如何实现自我演化，而不是如何实现可解释性。因此，它不属于被排除的“可解释性”研究。 *   论文不涉及安全、对齐或多模态内容。 4.  **第四步：处理特殊情况——完全符合** *   **自我演化的应用**: 这篇论文是“自我演化的应用”这一特殊情况的完美范例。它的核心是提出一种**新的自我演化机制**（通过外部库的演化），并将其应用于优化领域。根据你的规则，这种情况应**保留**。 **最终决策:** 这篇论文的核心贡献在于构建了一个名为AlphaOPT的**自我演化框架**。该框架通过一个包含“学习”和“演化”的持续循环，让LLM能够从经验（尤其是失败经验和求解器反馈）中学习，并迭代优化其外部知识库，从而在不更新模型参数的情况下实现能力的持续提升。这完全契合你研究目标中的**“自我演化”**方向，并触及了智能体的“记忆”和“自我反思”等关键能力。因此，这篇论文应被保留。"
    },
    {
        "index": "#25",
        "title": "Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games",
        "link": "/arxiv/2510.18395",
        "arxiv_id": "2510.18395",
        "authors": "Runnan Qi, Yanan Ni, Lumin Jiang, Zongyuan Li, Kuihua Huang, Xian Guo",
        "summary": "This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel framework for LLM agents in real-time strategy games. Addressing key challenges like hallucinations and fragmented decision-making in existing approaches, MASMP integrates state machine prompting with memory mechanisms to unify structured actions with long-term tactical coherence. The framework features: (1) a natural language-driven state machine architecture that guides LLMs to emulate finite state machines and behavior trees through prompts, and (2) a lightweight memory module preserving strategic variables (e.g., tactics, priority units) across decision cycles. Experiments in StarCraft II demonstrate MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly outperforming baselines (0%). Case studies reveal the method retains LLMs' semantic comprehension while resolving the \"Knowing-Doing Gap\" through strict state-action mapping, achieving both interpretability and FSM-like reliability. This work establishes a new paradigm for combining neural and symbolic AI in complex decision-making.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.449420",
        "filter_reason": "这篇论文完全符合你的研究范围，属于“单智能体”方向下的一个高质量候选。我的判断过程如下： 1.  **第一步：核心判断——保留** *   **论文本质**: 这篇论文的核心贡献是提出了一个名为“记忆增强状态机提示（MASMP）”的**新框架**。这个框架旨在**构建和改进LLM智能体**，使其在复杂环境（实时战略游戏）中做出更连贯、更可靠的决策。它不是简单地将一个已有的智能体框架应用到游戏领域，而是提出了一种结合符号（状态机）和神经（LLM与记忆）的新方法论。因此，它通过了第一步的核心判断，属于构建/改进LLM智能体的范畴。 2.  **第二步：正面指标——高度匹配** *   **核心范式**: 论文明确提出了一个`LLM Agent Framework`，完全命中研究核心。 *   **智能体能力**: 论文的两大核心创新点直接对应了你的关注点：`Memory`（“记忆增强”、“lightweight memory module”）和`Planning`（“state machine prompting”指导结构化行动和长期战术策略，这是一种高级的规划和执行机制）。它通过解决“Knowing-Doing Gap”来提升智能体的行动可靠性。 3.  **第三步：排除标准——未触犯** *   **安全与对齐**: 尽管摘要中提到了“hallucinations”（幻觉），但这被定位为现有方法的一个待解决的**挑战**，而非论文的**主要贡献**。论文的目的是构建一个更可靠的框架来**缓解**这个问题，而不是研究幻觉本身或AI对齐。因此，它不属于安全与对齐的研究焦点。 *   **多模态与视觉**: 论文完全没有提及视觉或多模态内容，其焦点完全在于基于文本提示和游戏状态表示的决策逻辑。 4.  **第四步：处理特殊和模糊情况——适用保留规则** *   **推理/规划**: 这篇论文是**智能体规划**的典型范例。它不是在提升LLM的基础数学或逻辑推理能力，而是在设计一个**智能体层面的规划与执行架构**（状态机），来引导LLM在多步、长周期的任务中保持行为一致性。这完全符合保留条件。 **最终决策**: 这篇论文的核心贡献在于提出了一种创新的LLM智能体框架（MASMP），该框架通过融合记忆机制和状态机，显著增强了智能体的规划和长期决策能力。这直接命中了你研究课题中的“单智能体”方向，特别是“规划”和“记忆”这两个子方向。它不是一篇应用型论文，而是一篇方法论创新论文，因此是理想的筛选对象。"
    },
    {
        "index": "#24",
        "title": "Heterogeneous Adversarial Play in Interactive Environments",
        "link": "/arxiv/2510.18407",
        "arxiv_id": "2510.18407",
        "authors": "Manjie Xu, Xinyi Yang, Jiayu Zhan, Wei Liang, Chi Zhang, Yixin Zhu",
        "summary": "Self-play constitutes a fundamental paradigm for autonomous skill acquisition, whereby agents iteratively enhance their capabilities through self-directed environmental exploration. Conventional self-play frameworks exploit agent symmetry within zero-sum competitive settings, yet this approach proves inadequate for open-ended learning scenarios characterized by inherent asymmetry. Human pedagogical systems exemplify asymmetric instructional frameworks wherein educators systematically construct challenges calibrated to individual learners' developmental trajectories. The principal challenge resides in operationalizing these asymmetric, adaptive pedagogical mechanisms within artificial systems capable of autonomously synthesizing appropriate curricula without predetermined task hierarchies. Here we present Heterogeneous Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework that formalizes teacher-student interactions as a minimax optimization wherein task-generating instructor and problem-solving learner co-evolve through adversarial dynamics. In contrast to prevailing ACL methodologies that employ static curricula or unidirectional task selection mechanisms, HAP establishes a bidirectional feedback system wherein instructors continuously recalibrate task complexity in response to real-time learner performance metrics. Experimental validation across multi-task learning domains demonstrates that our framework achieves performance parity with SOTA baselines while generating curricula that enhance learning efficacy in both artificial agents and human subjects.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.448955",
        "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **第一步：核心判断 (保留)** *   论文的核心贡献是提出一个名为“异构对抗博弈”的新框架。这个框架并非简单应用现有技术，而是构建了一个全新的方法论，用于解决智能体如何通过交互进行开放式学习的问题。 *   摘要明确指出，该框架将“教师-学生互动形式化为一个最小-最大优化问题，其中任务生成教师和问题解决学生通过对抗性动态共同演化”。这直接命中了您研究范围中的“构建、改进或演化 LLM智能体”的核心目标，特别是“多智能体系统”和“自我演化”两个方向。因此，它不属于“非演化型应用”或“非Agentic的推理”等排除类别。 2.  **第二步：正面指标 (高度匹配)** *   **核心范式**: 论文的核心是 `Multi-Agent Systems (MAS)` (教师-学生) 和 `Self-Evolving` (共同演化)。 *   **多智能体**: 论文详细阐述了智能体间的 `Adversarial Dynamics` (对抗性动态) 和 `Bidirectional Feedback System` (双向反馈系统)，这属于多智能体交互的高级形式。 *   **演化机制**: “Co-evolve” (共同演化) 和 “iteratively enhance their capabilities” (迭代增强能力) 是论文的核心机制，完全符合 `Self-Evolving` 和 `Iterative Improvement` 的定义。 3.  **第三步：排除标准 (未触发)** *   论文的研究焦点是学习效率和课程生成，完全不涉及 `Safety`, `Alignment`, `Interpretability` 等安全与对齐问题。 *   论文也未将 `Vision` 或多模态作为研究核心，其“交互环境”是抽象的，不构成排除理由。 4.  **第四步：特殊情况处理 (进一步确认)** *   论文的研究内容是关于智能体如何在复杂任务中学习和适应，这属于“智能体如何进行规划或在复杂任务中进行多步推理”的范畴，应予以保留。 *   更重要的是，根据您的特殊规则，这篇论文的核心贡献就是提出一种新的“自我演化”机制（HAP框架），即使它在“多任务学习领域”进行验证，也属于应保留的例外情况。 **总结**: 该论文提出了一种新颖的多智能体框架（HAP），通过教师与学生智能体的对抗性动态和共同演化来实现自动课程学习。其核心贡献直接对应您研究课题中的“多智能体”和“自我演化”两个关键方向，是一个典型的关于如何构建和演化智能体的前沿研究。因此，这篇论文高度相关，应被保留。"
    },
    {
        "index": "#22",
        "title": "Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents",
        "link": "/arxiv/2510.18424",
        "arxiv_id": "2510.18424",
        "authors": "Guangfu Guo, Xiaoqian Lu, Yue Feng",
        "summary": "Visual Language Models (VLMs) achieve promising results in medical reasoning but struggle with hallucinations, vague descriptions, inconsistent logic and poor localization. To address this, we propose a agent framework named Medical Visual Reasoning Agent (\\textbf{Med-VRAgent}). The approach is based on Visual Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By combining the Visual Guidance with tree search, Med-VRAgent improves the medical visual reasoning capabilities of VLMs. We use the trajectories collected by Med-VRAgent as feedback to further improve the performance by fine-tuning the VLMs with the proximal policy optimization (PPO) objective. Experiments on multiple medical VQA benchmarks demonstrate that our method outperforms existing approaches.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-21",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.448032",
        "filter_reason": "这篇论文符合我的研究范围，应该被保留。判断过程如下： 1.  **第一步：核心判断** 论文的核心贡献是**构建了一个名为 Med-VRAgent 的智能体框架**，用于解决医学视觉推理问题。它不是简单地将一个已有的智能体或LLM作为工具应用到医学领域，而是提出了一套包含“视觉引导”、“自我奖励”和“蒙特卡洛树搜索（MCTS）”的新方法论。这完全符合筛选标准中的“保留”条件：论文的核心是关于构建LLM智能体的方法论或新框架。 2.  **第二步：正面指标** 论文包含了多个我的核心关注点： *   **核心范式**: 论文明确提出了一个 \"agent framework\"。 *   **智能体能力**: 它使用了 \"Monte Carlo Tree Search (MCTS)\"，这是一种典型的**规划**方法。同时，\"Self-Reward paradigms\" 直接对应了**自我反思**或**自我修正**的能力。 *   **演化机制**: 论文最关键的部分在于，“使用 Med-VRAgent 收集的轨迹作为反馈，通过近端策略优化（PPO）目标来微调VLMs以进一步提升性能”。这是一个明确的**自我演化**机制，智能体通过与环境交互产生经验数据，并利用这些数据来完善自身。 3.  **第三步：排除标准** *   **安全与对齐**: 尽管论文提到了 \"hallucinations\"（幻觉），但这只是它要解决的问题之一，而不是论文的核心贡献。论文的主要贡献是解决问题的智能体框架，而不是提出一种新的安全或对齐技术。因此，不应排除。 *   **多模态与视觉**: 论文涉及 \"Visual Language Models (VLMs)\"，但根据筛选标准，这属于“被用作智能体感知环境的工具，而不是研究的核心”的情况。研究的核心是指导VLMs进行推理和自我演化的**智能体框架**，而不是VLM模型本身。因此，不应排除。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文使用MCTS进行规划，这属于“智能体如何进行规划或在复杂任务中进行多步推理”的范畴，应保留。 *   **自我演化的应用**: 这篇论文是“自我演化的应用”这一例外情况的完美范例。它的核心是提出了一种新的“自我演化”机制（通过轨迹反馈和PPO微调），并将其应用在医学领域。根据规则“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域...也应该保留”，这篇论文必须保留。 **结论**: 综合以上分析，这篇论文的核心贡献在于构建了一个具备**规划**（MCTS）和**自我演化**（通过PPO进行自我完善）能力的LLM智能体框架。它精准地命中了我的研究焦点“单智能体”和“自我演化”，并且不属于任何排除类别。因此，最终判断为**True**。"
    },
    {
        "index": "#45",
        "title": "FABRIC: Framework for Agent-Based Realistic Intelligence Creation",
        "link": "/arxiv/2510.17995",
        "arxiv_id": "2510.17995",
        "authors": "Abhigya Verma, Seganrasan Subramanian, Nandhakumar Kandasamy, Naman Gupta",
        "summary": "Large language models (LLMs) are increasingly deployed as agents, expected to decompose goals, invoke tools, and verify results in dynamic environments. Realizing these capabilities requires access to agentic data- structured interaction records that couple user intents with tool specifications, argument-grounded calls, and verifiable execution traces. However, collecting such data from human annotators is costly, time-consuming, and difficult to scale. We present a unified framework for synthesizing agentic data using only LLMs, without any human-in-the-loop supervision. This framework decomposes generation into modular pipelines that produce complete interaction records spanning task specifications, tool definitions, policy pseudocode, natural language exchanges, and execution traces. Records conform to strict syntactic and semantic constraints, ensuring machine-parseability and faithful alignment across inputs, outputs, and tool calls. Beyond single tasks, there is support for both multi-task and multi-turn agent interactions, enabling the construction of datasets that reflect the full spectrum of tool-use competencies. To ensure quality and consistency, the framework integrates constrained generation formats, JSON-schema validation, and judge-based filtering. This paper formalizes the schema for agentic records, details the prompt design principles that guide generation, and introduces scalable pipelines for high-quality synthetic data. By providing a reproducible, LLM-only alternative to manual collection, hence advancing the development of agentic LLMs capable of robust tool use.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-20",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.471141",
        "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **第一步：核心判断 (保留)** 论文的核心贡献并非将LLM智能体应用于某个特定领域，而是提出了一个名为FABRIC的**方法论框架**，专门用于**合成高质量的智能体数据**。这个框架直接服务于“构建和改进LLM智能体”这一目标。它解决了当前智能体发展中一个关键的瓶颈问题——缺乏结构化、高质量的训练和评估数据。因此，这篇论文的本质是关于**改进LLM智能体的基础方法论**，而非简单的应用或基础设施，符合“保留”标准。 2.  **第二步：正面指标 (高度匹配)** 论文摘要中包含了多个核心关注点： *   **核心范式**: 明确提到了 \"agentic data\", \"agentic LLMs\"，直接对应 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**: 整个框架的核心是生成包含 \"tool specifications, argument-grounded calls, and verifiable execution traces\" 的数据，这直接命中了 `Tool Use / Tool Augmentation` 这一核心能力。同时，\"decompose goals\" 和 \"policy pseudocode\" 也与 `Planning` 能力密切相关。 *   **多智能体**: 论文提到了支持 \"multi-task and multi-turn agent interactions\"，这涉及到智能体的连续交互能力，是单智能体复杂行为的重要部分。 3.  **第三步：排除标准 (未触发)** 论文内容完全没有涉及安全、对齐、可解释性（Safety, Alignment, Interpretability）等排除主题，也未涉及多模态或视觉模型（Vision, MLLMs）。研究的焦点纯粹集中在智能体能力的构建上。 4.  **第四步：特殊/模糊情况 (清晰符合)** 论文虽然不是直接提出一个新的智能体推理算法（如ReAct），但它**为智能体的推理和工具使用能力提供了基础“养料”**。它解决的是“如何让智能体学会更好地使用工具”的前置问题，即“如何创造让智能体学习使用工具的数据”。这完全符合您对“智能体如何进行规划或在复杂任务中进行多步推理”这一方向的研究兴趣，因为它为这类能力的提升提供了可扩展的数据解决方案。 **最终决策**: 这篇论文的核心贡献是提出了一种创新的方法论框架，用于解决构建和改进LLM智能体（尤其是在工具使用方面）所面临的关键数据稀缺问题。它不是应用，不是基础设施，而是直接服务于“改进LLM智能体”这一核心目标的基础性工作。因此，它精准地契合了您关于“LLM智能体及其演化”的研究课题，应予以保留。"
    },
    {
        "index": "#136",
        "title": "BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?",
        "link": "/arxiv/2510.18003",
        "arxiv_id": "2510.18003",
        "authors": "Fengqing Jiang, Yichen Feng, Yuetai Li, Luyao Niu, Basel Alomair, Radha Poovendran",
        "summary": "The convergence of LLM-powered research assistants and AI-based peer review systems creates a critical vulnerability: fully automated publication loops where AI-generated research is evaluated by AI reviewers without human oversight. We investigate this through \\textbf{BadScientist}, a framework that evaluates whether fabrication-oriented paper generation agents can deceive multi-model LLM review systems. Our generator employs presentation-manipulation strategies requiring no real experiments. We develop a rigorous evaluation framework with formal error guarantees (concentration bounds and calibration analysis), calibrated on real data. Our results reveal systematic vulnerabilities: fabricated papers achieve acceptance rates up to . Critically, we identify \\textit{concern-acceptance conflict} -- reviewers frequently flag integrity issues yet assign acceptance-level scores. Our mitigation strategies show only marginal improvements, with detection accuracy barely exceeding random chance. Despite provably sound aggregation mathematics, integrity checking systematically fails, exposing fundamental limitations in current AI-driven review systems and underscoring the urgent need for defense-in-depth safeguards in scientific publishing.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Computers and Society",
        "date": "2025-10-20",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.512816",
        "filter_reason": "这篇论文符合您的研究范围，核心依据如下： 1.  **第一步：核心判断——论文的本质是构建LLM智能体。** 该论文的核心贡献是提出了一个名为 **BadScientist** 的框架。这个框架的本质是构建一个“面向伪造的论文生成智能体”。这完全符合您筛选标准中的“保留”条件：“论文的核心是关于构建LLM智能体的方法论或新框架”。它不是简单地将现有智能体应用到一个领域，而是**提出了一种新的智能体构建方法和策略**。 2.  **第二步：正面指标——论文触及了多个核心关注点。** -   **Agentic AI / LLM-based Agents**: 论文明确提出了 \"paper generation agents\"，其核心就是研究如何构建和设计这类智能体。 -   **Multi-Agent Systems (MAS)**: 整个研究设置了一个多智能体博弈场景：一个“生成智能体”试图欺骗一个“评审智能体”。这直接涉及智能体间的对抗和交互，是多智能体研究的典型范式。 -   **Planning / Tool Use**: 论文中提到生成器采用了 \"presentation-manipulation strategies\"（呈现操纵策略）。这可以被视为智能体为了达成目标（欺骗评审）而执行的特定规划或使用的一种高级“策略工具”，属于智能体能力范畴。 3.  **第三步：排除标准——论文的主要贡献并非安全或对齐研究。** 尽管论文的研究背景和发现与“安全”和“漏洞”紧密相关，但其**主要贡献**是构建了BadScientist这个**攻击性智能体框架**，而不是提出一种新的防御、对齐或安全检测方法。根据您的规则：“只要论文的主要贡献是关于 Safety, Security...一律排除”。本文的贡献在于“构建智能体”，而非“防御智能体”，因此不应被排除。它通过构建一个智能体来揭示一个安全问题，这本身是Agentic AI研究的一部分。 4.  **第四步：特殊和模糊情况处理。** -   **推理/规划**: 该论文中的智能体使用“策略”来生成论文，这属于智能体层面的规划和执行，而非提升LLM本身的基础数学或逻辑推理能力，因此符合保留条件。 -   **自我演化的应用**: 此处不适用。 **总结**: 该论文的核心是提出一个名为BadScientist的框架，用于构建一个具有特定目标（生成虚假论文以欺骗评审）的LLM智能体。其研究范式涉及多智能体交互，并且探讨了智能体的策略规划能力。虽然其应用场景是揭示学术出版的安全漏洞，但其根本贡献在于**智能体的构建方法论**，这与您“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的核心目标高度一致。因此，应予以保留。"
    },
    {
        "index": "#153",
        "title": "SpecAgent: A Speculative Retrieval and Forecasting Agent for Code Completion",
        "link": "/arxiv/2510.17925",
        "arxiv_id": "2510.17925",
        "authors": "George Ma, Anurag Koul, Qi Chen, Yawen Wu, Sachit Kuhar, Yu Yu, Aritra Sengupta, Varun Kumar, Murali Krishna Ramanathan",
        "summary": "Large Language Models (LLMs) excel at code-related tasks but often struggle in realistic software repositories, where project-specific APIs and cross-file dependencies are crucial. Retrieval-augmented methods mitigate this by injecting repository context at inference time. The low inference-time latency budget affects either retrieval quality or the added latency adversely impacts user experience. We address this limitation with SpecAgent, an agent that improves both latency and code-generation quality by proactively exploring repository files during indexing and constructing speculative context that anticipates future edits in each file. This indexing-time asynchrony allows thorough context computation, masking latency, and the speculative nature of the context improves code-generation quality. Additionally, we identify the problem of future context leakage in existing benchmarks, which can inflate reported performance. To address this, we construct a synthetic, leakage-free benchmark that enables a more realistic evaluation of our agent against baselines. Experiments show that SpecAgent consistently achieves absolute gains of 9-11% (48-58% relative) compared to the best-performing baselines, while significantly reducing inference latency.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-20",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.518646",
        "filter_reason": "这篇论文符合研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为 `SpecAgent` 的新颖智能体框架。它的核心贡献并非简单地将LLM应用于代码补全领域，而是**构建了一个具有主动性和预测性的智能体方法论**。它通过“主动探索”和“构建推测性上下文”来改进任务，这属于构建和改进LLM智能体的范畴，而非简单的非演化型应用。 2.  **第二步：正面指标** - 该论文命中了多个核心正面指标： - **核心范式**: 论文标题和摘要明确使用了 `Agent` 一词，其工作核心是 `LLM-based Agent`。 - **智能体能力**: - **规划**: 论文的核心机制是“主动探索仓库文件”和“构建推测性上下文以预测未来的编辑”。这是一种高级的**前瞻性规划** 和预测能力，智能体不是被动响应，而是主动为未来可能的需求做准备。 - **记忆**: “在索引期间探索”并“构建推测性上下文”是一种复杂的、长期的**记忆机制**。它不同于简单的RAG检索，而是对环境（代码库）进行深度理解和预先构建结构化知识，这属于智能体记忆能力的范畴。 - **工具使用**: 论文改进了检索 这一工具的使用方式，从同步的推理时检索转变为异步的索引时探索，这是对工具使用范式的创新。 3.  **第三步：排除标准** - 论文不涉及安全与对齐、多模态与视觉等排除标准，因此通过此步筛选。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美符合“保留”条件。它不是在改进LLM的基础数学或逻辑推理，而是在构建一个新的**Agentic框架**来解决需要多步、前瞻性规划的复杂任务（在大型代码库中进行准确的补全）。其“推测性”机制本身就是一种创新的规划策略。 **核心依据总结**: 该论文的核心贡献是 `SpecAgent` 这个智能体本身，及其所体现的**主动规划和复杂记忆**能力。虽然其应用场景是代码补全，但其方法论（主动、异步、预测性地构建上下文）对于LLM智能体如何高效地理解和与复杂环境交互具有普遍意义。这完全符合“单智能体”研究方向中关于“规划”和“记忆”的探索目标，因此是一篇高度相关的前沿论文。"
    },
    {
        "index": "#185",
        "title": "Repairing Tool Calls Using Post-tool Execution Reflection and RAG",
        "link": "/arxiv/2510.17874",
        "arxiv_id": "2510.17874",
        "authors": "Jason Tsay, Zidane Wright, Gaodan Fang, Kiran Kate, Saurabh Jha, Yara Rizk",
        "summary": "Agentic systems interact with external systems by calling tools such as Python functions, REST API endpoints, or command line tools such as kubectl in Kubernetes. These tool calls often fail for various syntactic and semantic reasons. Some less obvious semantic errors can only be identified and resolved after analyzing the tool's response. To repair these errors, we develop a post-tool execution reflection component that combines large language model (LLM)-based reflection with domain-specific retrieval-augmented generation (RAG) using documents describing both the specific tool being called and troubleshooting documents related to the tool. For this paper, we focus on the use case of the kubectl command line tool to manage Kubernetes, a platform for orchestrating cluster applications. Through a larger empirical study and a smaller manual evaluation, we find that our RAG-based reflection will repair kubectl commands such that they are both more likely to successfully execute (pass rate) for 55% of our models evaluated and 36% more likely to correctly answer the user query on average. We find that troubleshooting documents improve pass rate compared to official documentation by an average of 10%.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-17",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.528971",
        "filter_reason": "这篇论文完全符合你的研究范围，应予保留。 **核心判断依据 (第一步):** 论文的核心贡献是提出了一种“后工具执行反思”组件，用于修复LLM智能体在调用外部工具（如kubectl）时发生的错误。这本质上是**对LLM智能体能力的一种改进和构建**。它不是简单地将一个已有的智能体框架应用到Kubernetes领域去解决该领域的问题，而是**创造了一种新的方法论（反思+RAG）来增强智能体的鲁棒性和自我修正能力**。因此，它不属于“非演化型应用”的排除范围，其本质属于“构建、改进LLM智能体”。 **符合核心关注点 (第二步):** 论文精准地命中了你研究焦点的多个关键点： 1.  **单智能体**: 论文的核心是提升单个智能体的能力。 2.  **工具使用**: 论文的出发点就是解决智能体在“工具使用”中遇到的失败问题。 3.  **自我反思**: 论文提出的核心解决方案就是“后工具执行反思”，这是一个典型的智能体自我反思和自我修正机制。这直接关联到“自我演化”的范畴，即智能体通过与环境的交互（工具执行失败）和反馈（工具的响应）进行自我完善。 **排除标准分析 (第三步):** 论文的主要贡献不在于安全、对齐或多模态，因此不触及任何排除标准。它使用的RAG技术是作为实现“反思”功能的一种手段，而非研究核心。 **特殊情况处理 (第四步):** 这篇论文是筛选标准第四步第2条“自我演化的应用”的完美例证。虽然它应用在了`kubectl`这个特定领域，但其核心是提出了一种**新的“自我演化/自我修正”机制**。按照规则，这种情况下应予保留，因为该机制本身具有通用性，可以被迁移到其他需要工具调用的智能体场景中。 **最终决策 (第五步):** 综合来看，这篇论文的核心是关于如何通过**反思和反馈循环来改进LLM智能体的工具使用能力**，这直接贡献于Agentic AI的“单智能体”和“自我演化”方向。它提出的是一个新颖的、可推广的智能体组件，而非一个特定领域的应用。因此，这篇论文与你的研究课题高度相关，是一篇值得关注的、质量较高的前沿研究。"
    },
    {
        "index": "#197",
        "title": "Multi-Agent Design Assistant for the Simulation of Inertial Fusion Energy",
        "link": "/arxiv/2510.17830",
        "arxiv_id": "2510.17830",
        "authors": "Meir H. Shachar, Dane M. Sterbentz, Harshitha Menon, Charles F. Jekel, M. Giselle Fernández-Godino, Yue Hao, Kevin Korner, Robert Rieben, Daniel A. White, William J. Schill, Jonathan L. Belof",
        "summary": "Inertial fusion energy promises nearly unlimited, clean power if it can be achieved. However, the design and engineering of fusion systems requires controlling and manipulating matter at extreme energies and timescales; the shock physics and radiation transport governing the physical behavior under these conditions are complex requiring the development, calibration, and use of predictive multiphysics codes to navigate the highly nonlinear and multi-faceted design landscape. We hypothesize that artificial intelligence reasoning models can be combined with physics codes and emulators to autonomously design fusion fuel capsules. In this article, we construct a multi-agent system where natural language is utilized to explore the complex physics regimes around fusion energy. The agentic system is capable of executing a high-order multiphysics inertial fusion computational code. We demonstrate the capacity of the multi-agent design assistant to both collaboratively and autonomously manipulate, navigate, and optimize capsule geometry while accounting for high fidelity physics that ultimately achieve simulated ignition via inverse design.",
        "subjects": "Applied Physics, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-22T11:00:06.532699",
        "filter_reason": "这篇论文符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **核心贡献**: 论文的核心贡献是**构建了一个多智能体系统**，用于解决惯性聚变能设计这一复杂问题。摘要中明确指出 \"We construct a multi-agent system\" 和 \"We demonstrate the capacity of the multi-agent design assistant\"。 - **判断依据**: 尽管论文应用在特定的物理领域，但其本质并非简单地使用一个现成的LLM或多智能体框架去解决领域问题。相反，它的主要创新点在于**提出并构建了这个多智能体框架本身**。因此，它完全符合第一步“保留”标准中“构建LLM智能体（Agentic LLM）、多智能体系统或自我演化的方法论或新框架”的要求。它不属于“非演化型应用”的排除范畴，因为其贡献在于“系统”而非“应用”。 2.  **第二步：正面指标** - 论文包含了多个与您研究焦点高度相关的正面指标： - **多智能体**: 标题和摘要反复强调 `Multi-Agent System`，并提到了智能体间的 `Collaboration`（协作）能力。 - **工具使用**: 智能体系统能够 \"executing a high-order multiphysics inertial fusion computational code\"，这是典型的 `Tool Use` 能力，LLM智能体调用外部专业工具来完成任务。 - **规划与优化**: 智能体能够 \"autonomously manipulate, navigate, and optimize capsule geometry\"，这体现了其在复杂任务中的 `Planning` 和迭代优化能力。 3.  **第三步：排除标准** - 论文内容不涉及 `Safety`, `Alignment`, `Interpretability` 等安全与对齐主题。 - 论文也不涉及 `Vision`, `MLLMs` 等多模态内容，其智能体主要通过自然语言和代码接口与环境交互。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的规划是智能体框架的一部分（“自主地操作、导航和优化”），属于Agentic AI的范畴，而非提升LLM本身的基础推理能力，因此符合保留条件。 - **自我演化的应用**: 虽然这篇论文不是关于“自我演化”的，但它完美诠释了“应用”与“框架”的区别。它的核心是提出一个新的Agentic框架（多智能体协作+工具使用），这与“仅将LLM作为工具应用”有本质区别，因此应当被保留。 **最终决策**: 该论文的核心贡献在于构建和验证了一个具备协作、工具使用和规划能力的多智能体框架，以解决一个高度复杂的科学问题。这完全符合您研究目标中的“多智能体”方向，因此是高度相关的论文。"
    }
]