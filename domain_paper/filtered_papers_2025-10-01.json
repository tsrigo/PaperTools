[
    {
        "index": "#11",
        "title": "Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization",
        "link": "/arxiv/2509.26520",
        "arxiv_id": "2509.26520",
        "authors": "Yaoxiang Wang, Qingguo Hu, Yucheng Ding, Ruizhe Wang, Yeyun Gong, Jian Jiao, Yelong Shen, Peng Cheng, Jinsong Su",
        "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently scaling large language models without a proportional increase in computational cost. However, the standard training strategy of Top-K router prevents MoE models from realizing their full potential for elastic inference. When the number of activated experts is altered at inference time, these models exhibit precipitous performance degradation. In this work, we introduce Matryoshka MoE (M-MoE), a training framework that instills a coarse-to-fine structure directly into the expert ensemble. By systematically varying the number of activated experts during training, M-MoE compels the model to learn a meaningful ranking: top-ranked experts collaborate to provide essential, coarse-grained capabilities, while subsequent experts add progressively finer-grained detail. We explore this principle at multiple granularities, identifying a layer-wise randomization strategy as the most effective. Our experiments demonstrate that a single M-MoE model achieves remarkable elasticity, with its performance at various expert counts closely matching that of an entire suite of specialist models, but at only a fraction of the total training cost. This flexibility not only unlocks elastic inference but also enables optimizing performance by allocating different computational budgets to different model layers. Our work paves the way for more practical and adaptable deployments of large-scale MoE models.",
        "subjects": "Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.497664",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出Matryoshka MoE (M-MoE)，一种新的训练框架，用于改进Mixture-of-Experts (MoE)模型。论文通过在训练过程中系统性地改变激活专家的数量，使模型能够学习有意义的专家排名结构，从而实现在推理时灵活使用不同数量专家而不降低性能。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，而非将LLM作为工具应用于特定领域。因此，根据第一步的判断标准，这篇论文应该被保留。 第二步：正面指标 论文确实涉及LLMs核心概念，讨论了大型语言模型的MoE架构。虽然论文没有直接讨论推理能力，但MoE架构的改进可以间接提升模型在各种任务上的性能，包括推理任务。论文提出了一种新的训练方法(M-MoE)，虽然不属于强化学习或自我进化范畴，但确实是针对LLM架构的创新训练范式。 第三步：排除标准 论文没有涉及多模态与视觉内容，没有聚焦于任何特定应用领域（如医疗、化学、生物等），也没有讨论模型可靠性方面的应用问题（如水印、安全等）。因此，根据排除标准，这篇论文不应被排除。 第四步：特殊和模糊情况 这篇论文的情况相对清晰，没有涉及特殊或模糊的情况。它明确是关于改进LLM架构和训练方法的研究，而不是将LLM应用于特定领域或讨论应用层面的可靠性问题。 综上所述，这篇论文的核心贡献是提出了一种新的训练框架来改进MoE模型的效率和适应性，这属于改进LLM基础能力的研究，与\"提高大语言模型的通用推理能力\"的研究目标相关。虽然论文没有直接讨论推理能力，但通过改进模型架构和训练方法，可以间接提升模型在各种任务上的性能，包括推理任务。因此，我认为这篇论文符合研究范围。"
    },
    {
        "index": "#8",
        "title": "Towards Reliable Benchmarking: A Contamination Free, Controllable Evaluation Framework for Multi-step LLM Function Calling",
        "link": "/arxiv/2509.26553",
        "arxiv_id": "2509.26553",
        "authors": "Seiji Maekawa, Jackson Hassell, Pouya Pezeshkpour, Tom Mitchell, Estevam Hruschka",
        "summary": "As language models gain access to external tools via structured function calls, they become increasingly more capable of solving complex, multi-step tasks. However, existing benchmarks for tool-augmented language models (TaLMs) provide insufficient control over factors such as the number of functions accessible, task complexity, and input size, and remain vulnerable to data contamination. We present FuncBenchGen, a unified, contamination-free framework that evaluates TaLMs by generating synthetic multi-step tool-use tasks. The key idea is to cast tool use as traversal over a hidden function-dependency DAG where nodes are function calls and an edge between nodes represents one function consuming the output of another. Given a set of external function schemas, initial variable values, and a target variable, models must compose the correct call sequence to compute the target variable. FuncBenchGen allows users to precisely control task difficulty (e.g., graph size, dependency depth, and distractor functions) while avoiding data leakage. We apply our FuncBenchGen framework to evaluate seven LLMs on tool use tasks of varying difficulty. Reasoning-optimized models consistently outperform general-purpose models with GPT-5 significantly outperforming other models. Performance declines sharply as dependency depth increases. Furthermore, connected irrelevant functions prove especially difficult to handle. We find that strong models often make syntactically valid function calls but propagate incorrect or stale argument values across steps, revealing brittle state tracking by LLMs in multi-turn tool use. Motivated by this observation, we introduce a simple mitigation strategy that explicitly restates prior variable values to the agent at each step. Surprisingly, this lightweight change yields substantial gains across models. e.g., yielding a success rate improvement from 62.5% to 81.3% for GPT-5.",
        "subjects": "Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.495627",
        "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 首先，从核心判断来看，这篇论文的本质是研究LLM的工具使用能力，特别是多步函数调用中的推理能力。论文提出的FuncBenchGen框架旨在评估和改进LLM在多步工具使用任务中的表现，这属于改进LLM基础能力的范畴，符合筛选标准中的\"工具使用\"方法论研究。论文不是将LLM作为工具应用到特定领域，而是关注LLM本身的能力提升。 其次，论文包含多个正面指标：它明确研究大语言模型(LLMs)；关注多步推理(multi-step reasoning)能力，这是通用推理能力的核心体现；并且涉及工具使用(tool use)这一新兴范式，这是增强LLM通用问题解决能力的重要方向。 第三，论文不聚焦于任何排除标准中的领域。它不涉及多模态与视觉内容，不针对特定应用领域（如医疗、化学等），也不主要关注模型在应用层面的可靠性问题（如水印、安全等）。 最后，在特殊和模糊情况处理上，论文提出的工具使用框架是通用的，不针对特定领域，目的是增强LLM的通用问题解决能力，符合筛选标准中关于工具使用研究的保留条件。论文发现并改进了LLM在多步工具使用中的状态跟踪问题，这直接提升了模型的推理质量和可靠性。 论文的核心贡献是提出了一个评估框架并发现了一种简单有效的策略来提升LLM在多步工具使用中的表现，这直接关系到LLM的通用推理能力提升，因此完全符合研究目标。"
    },
    {
        "index": "#21",
        "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning",
        "link": "/arxiv/2509.26383",
        "arxiv_id": "2509.26383",
        "authors": "Jinyeop Song, Song Wang, Julian Shun, Yada Zhu",
        "summary": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.508769",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的KG-RAG框架(KG-R1)，通过强化学习训练单个智能体来增强大语言模型与知识图谱的交互和推理能力。论文的核心贡献是改进LLM的基础推理能力，提出了一种新的训练范式(端到端强化学习)，而不是将LLM作为工具应用到特定领域。虽然它使用了知识图谱作为外部工具，但重点是提升LLM本身的推理效率和可转移性。 其次，论文包含了多个正面指标： - 核心概念：明确涉及大语言模型(LLMs)，使用Qwen-2.5-3B作为基础模型 - 能力方向：关注推理能力(reasoning)，论文明确提到\"reasoning traces\"和\"reasoning and generation\" - 训练方法：使用强化学习(RL)进行端到端优化 - 新兴范式：提出了基于LLM的智能体框架(llm-based agents)，将知识图谱作为工具使用 第三，论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉内容 - 不是针对特定应用领域(如医疗、化学等)，而是提出通用框架 - 虽然提到减少幻觉，但这是通过改进模型基础推理能力实现，而非专注于应用层面的可靠性问题 最后，在特殊和模糊情况处理上，论文提出的是通用智能体框架来增强LLM的推理能力，而非针对特定领域的应用。减少幻觉是通过改进模型内在推理能力实现的，符合研究目标。 综上所述，这篇论文致力于通过强化学习和智能体框架提高大语言模型的通用推理能力，完全符合研究范围。"
    },
    {
        "index": "#23",
        "title": "Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in its Latent Thoughts",
        "link": "/arxiv/2509.26314",
        "arxiv_id": "2509.26314",
        "authors": "Hanwen Du, Yuxin Dong, Xia Ning",
        "summary": "Large Language Models (LLMs) excel at problem solving by generating chain of thoughts in natural language, but such verbal thinking is computationally costly and prone to overthinking. Recent work instead proposes a latent thinking architecture Huggin-3.5B, which represents intermediate reasoning steps as sequence of latent representations. However, latent thoughts lack interpretability and are difficult to supervise, raising concerns about the correctness and reliability of its latent thinking processes. In this paper, we provide a systematic study of how Huggin-3.5B thinks in the latent space and how external supervision signals can improve its latent thinking processes. We show that latent thoughts leading to correct versus incorrect answers exhibit highly distinguishable patterns, and that a latent classifier can reliably predict answer correctness directly from latent thoughts. Leveraging these insights, we propose Latent Thinking Optimization (LTO), a probabilistic algorithm that employs the latent classifier as a Latent Reward Model (LRM) to optimize the latent thinking processes. Extensive experiments across diverse reasoning tasks demonstrate that LRM is highly effective in detecting incorrect latent thinking patterns, and LTO can significantly improve the latent thinking processes. Furthermore, we show that LRM can generalize across diverse domains, and LTO can be seamlessly applied to general LLMs to improve their thinking processes. In contrast to verbal thinking, our method demonstrates that reward modeling and scaling test-time thinking with supervision can be performed directly in the latent space, highlighting its potential as a general, efficient, and domain-agnostic approach to improving the thinking processes of LLMs.",
        "subjects": "Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.509747",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是研究如何改进LLM的基础推理能力，提出了\"潜在思维优化\"(LTO)这一新方法，通过在潜在空间中优化模型的思维过程来提升其推理能力，而不是将LLM应用于特定领域。 论文满足多个正面指标：核心概念上明确研究LLMs（特别是Huggin-3.5B模型）；能力方向上专注于推理能力，尤其是潜在推理；训练方法上提出了类似于强化学习的LTO算法，使用潜在奖励模型来优化思维过程。 论文不符合任何排除标准：没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 在特殊情况下，论文确实关注了潜在思维的可解释性问题，并提出了解决方案，这属于提高模型内在可解释性的研究，从而提升模型的推理质量和可靠性，应该保留。 论文的核心贡献是提出了一种通用的、高效的、领域无关的方法来改进LLM的思维过程，这与研究目标高度一致。"
    },
    {
        "index": "#24",
        "title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient",
        "link": "/arxiv/2509.26313",
        "arxiv_id": "2509.26313",
        "authors": "Rui Ming, Haoyuan Wu, Shoubo Hu, Zhuolun He, Bei Yu",
        "summary": "Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), yet it often struggles with generalization compared to reinforcement learning (RL). In this work, we posit that this performance disparity stems not just from the loss function, but from a more fundamental difference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy. Building on this hypothesis, we introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides SFT with the policy gradient method. OTR reframes the autoregressive learning process by treating each token generation as a single-step reinforcement learning trajectory. At each step, it performs a Monte Carlo ``rollout'' by sampling multiple candidate tokens from the current policy's distribution. The ground-truth token from the supervised data is then used to provide a reward signal to these samples. Guided by policy gradient, our algorithm repurposes static, off-policy supervised data into a dynamic, on-policy signal at the token level, capturing the generalization benefits of on-policy learning while bypassing the costly overhead of full sentence generation. Through extensive experiments on a diverse suite of challenging benchmarks spanning mathematical reasoning, code generation, and general domain reasoning, we demonstrate that OTR consistently outperforms standard SFT. Our findings establish OTR as a powerful and practical alternative for fine-tuning LLMs and provide compelling evidence that the on-policy nature of data is a critical driver of generalization, offering a promising new direction for fine-tuning LLMs.",
        "subjects": "Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.510215",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"one-token rollout (OTR)\"的新颖微调算法，该方法将监督式微调(SFT)与策略梯度方法相结合，通过将每个token生成视为单步强化学习轨迹，从而提高LLM的泛化能力和推理能力。论文在数学推理、代码生成和通用领域推理等通用推理任务上验证了其方法的有效性。根据筛选标准，这篇论文符合\"保留\"条件，因为它核心是关于改进LLM的基础能力和提出新的训练范式，以增强其推理等通用能力。论文满足多个正面指标(涉及LLMs、reasoning和reinforcement learning)，不符合任何排除标准，且不涉及特殊或模糊的情况。因此，这篇论文符合研究\"大语言模型通用推理能力\"的范围。"
    },
    {
        "index": "#40",
        "title": "Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis",
        "link": "/arxiv/2509.26074",
        "arxiv_id": "2509.26074",
        "authors": "Leitian Tao, Xuefeng Du, Yixuan Li",
        "summary": "Reward modeling, crucial for aligning large language models (LLMs) with human preferences, is often bottlenecked by the high cost of preference data. Existing textual data synthesis methods are computationally expensive. We propose a novel framework LENS for synthesizing preference data directly in the LLM's latent embedding space. Our method employs a Variational Autoencoder (VAE) to learn a structured latent representation of response embeddings. By performing controlled perturbations in this latent space and decoding back to the embedding space, we efficiently generate diverse, semantically consistent synthetic preference pairs, bypassing costly text generation and annotation. We provide theoretical guarantees that our synthesized pairs approximately preserve original preference ordering and improve reward model generalization. Empirically, our latent-space synthesis significantly outperforms text-based augmentation on standard benchmarks, achieving superior results while being 18x faster in generation and using a 16,000x smaller model. Our work offers a scalable and effective alternative for enhancing reward modeling through efficient data augmentation. Code is publicly available at https://github.com/deeplearning-wisc/lens",
        "subjects": "Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.528537",
        "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，论文的本质是改进大语言模型的基础能力，特别是提出了一个新的框架LENS来增强奖励建模，这是RLHF(强化学习人类反馈)的关键组成部分。奖励模型的质量直接影响LLM的推理能力和对齐效果，因此这项研究属于改进LLM通用推理能力的范畴。 其次，论文包含多个正面指标：明确关注大语言模型(LLMs)，涉及强化学习(RLHF)的训练方法，通过改进奖励模型间接提升模型的问题解决和推理能力。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 最后，论文的核心贡献是提出一种在潜在空间合成偏好数据的新方法，以更高效地训练奖励模型，从而提升LLM与人类偏好的对齐效果。这种方法论研究直接服务于提升LLM的通用能力，而非将LLM作为工具应用于特定领域。因此，这篇论文完全符合\"提高大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#42",
        "title": "DyFlow: Dynamic Workflow Framework for Agentic Reasoning",
        "link": "/arxiv/2509.26062",
        "arxiv_id": "2509.26062",
        "authors": "Yanbo Wang, Zixiang Xu, Yue Huang, Xiangqi Wang, Zirui Song, Lang Gao, Chenxi Wang, Xiangru Tang, Yue Zhao, Arman Cohan, Xiangliang Zhang, Xiuying Chen",
        "summary": "Agent systems based on large language models (LLMs) have shown great potential in complex reasoning tasks, but building efficient and generalizable workflows remains a major challenge. Most existing approaches rely on manually designed processes, which limits their adaptability across different tasks. While a few methods attempt automated workflow generation, they are often tied to specific datasets or query types and make limited use of intermediate feedback, reducing system robustness and reasoning depth. Moreover, their operations are typically predefined and inflexible. To address these limitations, we propose DyFlow, a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures based on task requirements and real-time intermediate feedback, thereby enhancing cross-task generalization. DyFlow consists of two core components: a designer and an executor. The designer decomposes complex problems into a sequence of sub-goals defined by high-level objectives and dynamically plans the next steps based on intermediate outputs and feedback. These plans are then carried out by the executor, which executes each operation using dynamic operators with context-aware parameterization, enabling flexible and semantically grounded reasoning. We systematically evaluate DyFlow across diverse domains, including social reasoning, biomedical tasks, mathematical problem solving, and code generation. Results demonstrate that DyFlow significantly outperforms existing baselines, achieving substantial Pass@k improvements and exhibiting robust generalization across diverse domains. The code is publicly available at https://github.com/wyf23187/DyFlow.",
        "subjects": "Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.529536",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是提出DyFlow框架，这是一种动态工作流生成框架，专门用于增强基于大语言模型的智能体系统的推理能力。论文明确旨在解决现有方法依赖手动设计流程、适应性有限的问题，通过动态规划和执行来提升LLM的推理深度和跨任务泛化能力。这明显属于改进LLM基础能力和通用推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标。核心概念方面，明确研究\"基于大语言模型(LLMs)的智能体系统\"；能力方向方面，关注\"复杂推理任务\"、\"推理过程\"和\"推理深度\"，并在评估中包含\"数学问题求解\"；新兴范式方面，属于\"基于LLM的智能体\"研究。 第三步排除标准：论文不主要聚焦于任何排除领域。虽然评估部分使用了\"社会推理、生物医学任务、数学问题求解和代码生成\"等多个领域，但这些仅用于验证框架的泛化能力，而非论文的核心焦点。论文的核心是提出通用框架，而非针对特定应用领域。 第四步特殊情况处理：论文提出的是一种通用的智能体推理框架，旨在增强LLM的通用问题解决能力，而非应用于特定领域的智能体。这符合保留标准。 综上所述，DyFlow论文的核心贡献是提出一种动态工作流框架，通过自适应构建和调整推理过程来增强LLM的通用推理能力和跨任务泛化性，完全符合研究目标。"
    },
    {
        "index": "#44",
        "title": "RE-Searcher: Robust Agentic Search with Goal-oriented Planning and Self-reflection",
        "link": "/arxiv/2509.26048",
        "arxiv_id": "2509.26048",
        "authors": "Daocheng Fu, Jianbiao Mei, Licheng Wen, Xuemeng Yang, Cheng Yang, Rong Wu, Tao Hu, Siqi Li, Yufan Shen, Xinyu Cai, Pinlong Cai, Botian Shi, Yong Liu, Yu Qiao",
        "summary": "Large language models (LLMs) excel at knowledge-intensive question answering and reasoning, yet their real-world deployment remains constrained by knowledge cutoff, hallucination, and limited interaction modalities. Augmenting LLMs with external search tools helps alleviate these issues, but it also exposes agents to a complex search environment in which small, plausible variations in query formulation can steer reasoning into unproductive trajectories and amplify errors. We present a systematic analysis that quantifies how environmental complexity induces fragile search behaviors and, in turn, degrades overall performance. To address this challenge, we propose a simple yet effective approach to instantiate a search agent, RE-Searcher. During search, RE-Searcher explicitly articulates a concrete search goal and subsequently reflects on whether the retrieved evidence satisfies that goal. This combination of goal-oriented planning and self-reflection enables RE-Searcher to resist spurious cues in complex search environments and perform robust search. Extensive experiments show that our method improves search accuracy and achieves state-of-the-art results. Perturbation studies further demonstrate substantial resilience to noisy or misleading external signals, mitigating the fragility of the search process. We believe these findings offer practical guidance for integrating LLM-powered agents into more complex interactive environments and enabling more autonomous decision-making.",
        "subjects": "Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.530524",
        "filter_reason": "这篇论文的核心贡献是提出RE-Searcher方法，一种通过目标导向规划和自我反思来增强大型语言模型在复杂搜索环境中鲁棒性的通用智能体框架。该方法直接针对提升LLM的通用推理能力，特别是解决LLMs在知识截止、幻觉和有限交互模式方面的局限性。论文涉及多个正面指标，包括LLM-based agents、tool use、planning和self-reflection等新兴范式，这些都是提升LLM通用推理能力的关键方向。论文没有聚焦于多模态、特定应用领域或模型可靠性的应用层面问题，因此不符合任何排除标准。RE-Searcher作为一种通用的智能体搜索框架，旨在提升LLM的自主决策能力和问题解决能力，完全符合\"大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#45",
        "title": "Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning",
        "link": "/arxiv/2509.26041",
        "arxiv_id": "2509.26041",
        "authors": "Arash Marioriyad, Shaygan Adim, Nima Alighardashi, Mahdieh Soleymani Banghshah, Mohammad Hossein Rohban",
        "summary": "Large language models (LLMs) increasingly rely on chain-of-thought (CoT) prompting to solve mathematical and logical reasoning tasks. Yet, a central question remains: to what extent are these generated rationales \\emph{faithful} to the underlying computations, rather than post-hoc narratives shaped by hints that function as answer shortcuts embedded in the prompt? Following prior work on hinted vs.\\ unhinted prompting, we present a systematic study of CoT faithfulness under controlled hint manipulations. Our experimental design spans four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models (GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in correctness (correct and incorrect), presentation style (sycophancy and data leak), and complexity (raw answers, two-operator expressions, four-operator expressions). We evaluate both task accuracy and whether hints are explicitly acknowledged in the reasoning. Our results reveal three key findings. First, correct hints substantially improve accuracy, especially on harder benchmarks and logical reasoning, while incorrect hints sharply reduce accuracy in tasks with lower baseline competence. Second, acknowledgement of hints is highly uneven: equation-based hints are frequently referenced, whereas raw hints are often adopted silently, indicating that more complex hints push models toward verbalizing their reliance in the reasoning process. Third, presentation style matters: sycophancy prompts encourage overt acknowledgement, while leak-style prompts increase accuracy but promote hidden reliance. This may reflect RLHF-related effects, as sycophancy exploits the human-pleasing side and data leak triggers the self-censoring side. Together, these results demonstrate that LLM reasoning is systematically shaped by shortcuts in ways that obscure faithfulness.",
        "subjects": "Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.530962",
        "filter_reason": "这篇论文完全符合我的研究目标，它专注于研究大语言模型(LLM)的通用推理能力，特别是链式思维(CoT)推理的忠实度问题。 首先，从核心判断来看，论文本质上是研究LLM的基础推理能力，而不是将其作为工具应用于特定领域。论文探讨了LLM在数学和逻辑推理任务中的表现，以及提示中的\"暗示\"如何影响模型的推理过程和结果。这直接关系到改进LLM的推理能力这一核心目标。 其次，论文包含了多个正面指标： - 核心概念：明确研究大型语言模型(LLMs)，特别是GPT-4o和Gemini-2-Flash - 能力方向：专注于推理能力，特别是数学推理(math reasoning)和逻辑推理(logical reasoning) - 训练方法：部分涉及RLHF(基于人类反馈的强化学习)对模型行为的影响分析 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性(应用层面)。 在特殊和模糊情况处理方面，论文虽然涉及可解释性问题(研究推理过程的忠实度)，但这不是其主要焦点，而是作为理解LLM推理机制的一部分。 论文的核心贡献在于系统研究了LLM在链式思维推理中的忠实度问题，揭示了暗示如何影响模型推理的关键发现，如正确暗示提高准确性、不同类型暗示被承认程度不同等。这些发现有助于深入理解LLM的推理机制，为改进LLM的通用推理能力提供了重要见解，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#50",
        "title": "Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel",
        "link": "/arxiv/2509.25913",
        "arxiv_id": "2509.25913",
        "authors": "Chuanyang Zheng, Jiankai Sun, Yihang Gao, Enze Xie, Yuehao Wang, Peihao Wang, Ting Xu, Matthew Chang, Liliang Ren, Jingyao Li, Jing Xiong, Kashif Rasul, Mac Schwager, Anderson Schneider, Zhangyang Wang, Yuriy Nevmyvaka",
        "summary": "Mixture-of-Experts (MoE) has become a cornerstone in recent state-of-the-art large language models (LLMs). Traditionally, MoE relies on $\\mathrm{Softmax}$ as the router score function to aggregate expert output, a designed choice that has persisted from the earliest MoE models to modern LLMs, and is now widely regarded as standard practice. However, the necessity of using $\\mathrm{Softmax}$ to project router weights into a probability simplex remains an unchallenged assumption rather than a principled design choice. In this work, we first revisit the classical Nadaraya-Watson regression and observe that MoE shares the same mathematical formulation as Nadaraya-Watson regression. Furthermore, we show that both feed-forward neural network (FFN) and MoE can be interpreted as a special case of Nadaraya-Watson regression, where the kernel function corresponds to the input neurons of the output layer. Motivated by these insights, we propose the \\textbf{zero-additional-cost} Kernel Inspired Router with Normalization (KERN), an FFN-style router function, as an alternative to $\\mathrm{Softmax}$. We demonstrate that this router generalizes both $\\mathrm{Sigmoid}$- and $\\mathrm{Softmax}$-based routers. \\textbf{Based on empirical observations and established practices in FFN implementation, we recommend the use of $\\mathrm{ReLU}$ activation and $\\ell_2$-normalization in $\\mathrm{KERN}$ router function.} Comprehensive experiments in MoE and LLM validate the effectiveness of the proposed FFN-style router function \\methodNorm.",
        "subjects": "Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.538485",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。首先，从核心判断来看，这篇论文的本质是关于改进大语言模型的基础架构组件——Mixture-of-Experts (MoE)中的路由器设计。论文提出了一种名为KERN的新路由器函数，作为传统Softmax的替代方案，这属于改进LLM基础能力的范畴，而非将LLM应用于特定领域。虽然论文没有直接讨论逻辑推理、数学推理或多步推理等通用推理能力，但它关注的是LLM架构的核心组件优化，这种基础架构的改进可以间接提升模型的整体性能，包括推理能力。 从正面指标看，论文明确提到了Mixture-of-Experts (MoE)和large language models (LLMs)这一核心概念。虽然论文没有涉及reasoning、planning等能力方向，也没有讨论reinforcement learning等训练方法或llm-based agents等新兴范式，但它确实关注了LLM的基础架构改进。 从排除标准看，论文没有涉及多模态与视觉、特定应用领域或模型可靠性等应用层面的内容，因此不应被排除。 论文的核心贡献是通过重新审视Nadaraya-Watson回归与MoE的数学联系，提出了一种新的路由器设计方法，这种方法可以改进MoE模型的性能。由于MoE是现代大语言模型的关键组成部分，优化其路由器设计可能会对模型的整体性能和推理能力产生积极影响，因此这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#51",
        "title": "Mem-α: Learning Memory Construction via Reinforcement Learning",
        "link": "/arxiv/2509.25911",
        "arxiv_id": "2509.25911",
        "authors": "Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian McAuley, Xiaojian Wu",
        "summary": "Large language model (LLM) agents are constrained by limited context windows, necessitating external memory systems for long-term information understanding. Current memory-augmented agents typically depend on pre-defined instructions and tools for memory updates. However, language models may lack the ability to determine which information to store, how to structure it, and when to update it, especially as memory systems become more complex. This results in suboptimal memory construction and information loss. To this end, we propose Mem-alpha, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback. We also construct a specialized training dataset spanning diverse multi-turn interaction patterns paired with comprehensive evaluation questions designed to teach effective memory management. During training, agents process sequential information chunks, learn to extract and store relevant content, then update the memory system. The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction. To illustrate the effectiveness of our training framework, we design a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations. Empirical evaluation demonstrates that Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Despite being trained exclusively on instances with a maximum length of 30k tokens, our agents exhibit remarkable generalization to sequences exceeding 400k tokens, over 13x the training length, highlighting the robustness of Mem-alpha.",
        "subjects": "Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.538970",
        "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，该论文的本质是提出一种强化学习框架(Mem-α)来增强大语言模型智能体的记忆管理能力，这属于改进LLM基础能力的范畴。论文通过训练智能体学习如何有效构建和管理记忆系统，从而提升模型处理长期信息的能力，这直接关系到LLM的通用推理能力。 其次，从正面指标来看，论文包含多个相关主题： - 核心概念：明确讨论大语言模型(LLM)智能体 - 训练方法：使用强化学习框架训练智能体 - 新兴范式：涉及基于LLM的智能体和工具使用（\"equipped with multiple tools for memory operations\"） - 能力方向：虽然未直接提及数学或逻辑推理，但记忆管理是支持复杂推理和多步问题解决的基础能力 第三，论文不涉及任何排除标准中的领域。它不关注多模态与视觉问题，不针对特定应用领域（如医疗、化学等），也不主要讨论模型可靠性方面的水印、安全等问题。 最后，关于特殊情况的判断，论文提出的智能体和工具使用是为了增强LLM的通用记忆管理能力，而非应用于特定领域，因此符合保留标准。 论文的核心贡献是通过强化学习训练LLM智能体有效管理复杂记忆系统的方法，这直接提升了模型的长期信息处理能力，是增强LLM通用推理能力的重要研究方向。因此，该论文符合我的研究目标。"
    },
    {
        "index": "#57",
        "title": "Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling",
        "link": "/arxiv/2509.25827",
        "arxiv_id": "2509.25827",
        "authors": "Shuyang Jiang, Yusheng Liao, Ya Zhang, Yanfeng Wang, Yu Wang",
        "summary": "While large reasoning models trained with critic-free reinforcement learning and verifiable rewards (RLVR) represent the state-of-the-art, their practical utility is hampered by ``overthinking'', a critical issue where models generate excessively long reasoning paths without any performance benefit. Existing solutions that penalize length often fail, inducing performance degradation due to a fundamental misalignment between trajectory-level rewards and token-level optimization. In this work, we introduce a novel framework, DECS, built on our theoretical discovery of two previously unaddressed flaws in current length rewards: (1) the erroneous penalization of essential exploratory tokens and (2) the inadvertent rewarding of partial redundancy. Our framework's innovations include (i) a first-of-its-kind decoupled token-level reward mechanism that surgically distinguishes and penalizes redundant tokens, and (ii) a novel curriculum batch scheduling strategy to master the efficiency-efficacy equilibrium. Experimental results show DECS can achieve a dramatic reduction in reasoning tokens by over 50\\% across seven benchmarks while simultaneously maintaining or even improving performance. It demonstrates conclusively that substantial gains in reasoning efficiency can be achieved without compromising a model's underlying reasoning power.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.546913",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，该论文的本质是改进大语言模型的基础推理能力。论文针对大型推理模型中的\"过度思考\"问题，提出了一种新的框架DECS，通过解耦的token级奖励机制和课程批处理调度策略，显著提高了模型的推理效率，同时保持或提升了推理性能。这明显属于改进LLM通用推理能力的研究，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标主题： - 核心概念：明确研究\"large reasoning models\"，属于LLMs范畴 - 能力方向：聚焦于\"reasoning\"，特别是推理路径和推理token的优化 - 训练方法：基于\"critic-free reinforcement learning and verifiable rewards (RLVR)\"，属于强化学习方法 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是提出了一种新的方法来解决LLM在推理过程中的效率问题，通过优化奖励机制和训练策略，使模型能够生成更简洁但同样有效的推理路径。这直接提升了LLM的通用推理能力，与我的研究目标高度一致。"
    },
    {
        "index": "#62",
        "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
        "link": "/arxiv/2509.25760",
        "arxiv_id": "2509.25760",
        "authors": "Zhepei Wei, Xiao Yang, Kai Sun, Jiaqi Wang, Rulin Shao, Sean Chen, Mohammad Kachuee, Teja Gollapudi, Tony Liao, Nicolas Scheffer, Rakesh Wanga, Anuj Kumar, Yu Meng, Wen-tau Yih, Xin Luna Dong",
        "summary": "While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.549305",
        "filter_reason": "这篇论文完全符合我的研究目标，理由如下： 首先，从核心判断来看，这篇论文的本质是改进LLM的基础能力，具体是提升其真实性(truthfulness)和减少幻觉。论文提出了TruthRL，一种新的强化学习训练范式，通过三元奖励机制（区分正确回答、幻觉和回避回答）来优化模型。这明显是关于增强LLM通用推理能力的研究，而非将LLM作为工具应用到特定领域。 其次，从正面指标来看，论文包含了多个相关主题： - 核心概念：明确关注大语言模型(LLMs) - 能力方向：涉及推理能力，特别是知识密集型任务中的真实性和逻辑判断 - 训练方法：使用强化学习(RL)框架，具体是GRPO (Group Relative Policy Optimization) 第三，论文不涉及任何排除标准中的领域，没有关注多模态、特定应用领域或模型可靠性的应用层面。 最后，在特殊和模糊情况处理上，论文提出了新方法来减少LLM的幻觉，这属于\"提升模型的通用可靠性和推理质量\"的情况，应该保留。论文不是从社会学研究或应用层面讨论幻觉，而是提出了一种新的强化学习方法来从根本上减少幻觉。 综上所述，TruthRL论文的核心贡献是通过强化学习框架提升LLM的真实性和推理能力，这与\"大语言模型通用推理能力\"的研究目标高度一致，因此应该被保留。"
    },
    {
        "index": "#67",
        "title": "Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities",
        "link": "/arxiv/2509.25725",
        "arxiv_id": "2509.25725",
        "authors": "Jiayi Kuang, Haojing Huang, Yinghui Li, Xinnian Liang, Zhikun Xu, Yangning Li, Xiaoyu Tan, Chao Qu, Meishan Zhang, Ying Shen, Philip S. Yu",
        "summary": "Large Language Models (LLMs) have demonstrated outstanding performance in mathematical reasoning capabilities. However, we argue that current large-scale reasoning models primarily rely on scaling up training datasets with diverse mathematical problems and long thinking chains, which raises questions about whether LLMs genuinely acquire mathematical concepts and reasoning principles or merely remember the training data. In contrast, humans tend to break down complex problems into multiple fundamental atomic capabilities. Inspired by this, we propose a new paradigm for evaluating mathematical atomic capabilities. Our work categorizes atomic abilities into two dimensions: (1) field-specific abilities across four major mathematical fields, algebra, geometry, analysis, and topology, and (2) logical abilities at different levels, including conceptual understanding, forward multi-step reasoning with formal math language, and counterexample-driven backward reasoning. We propose corresponding training and evaluation datasets for each atomic capability unit, and conduct extensive experiments about how different atomic capabilities influence others, to explore the strategies to elicit the required specific atomic capability. Evaluation and experimental results on advanced models show many interesting discoveries and inspirations about the different performances of models on various atomic capabilities and the interactions between atomic capabilities. Our findings highlight the importance of decoupling mathematical intelligence into atomic components, providing new insights into model cognition and guiding the development of training strategies toward a more efficient, transferable, and cognitively grounded paradigm of \"atomic thinking\".",
        "subjects": "Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.556872",
        "filter_reason": "这篇论文的核心贡献是提出了一种\"原子思维\"新范式，用于解耦和探索大语言模型的数学推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力的研究，特别是数学推理这一通用推理能力的重要组成部分，而不是将LLM作为工具应用到特定领域。论文提出了将数学推理能力分解为不同维度原子能力的方法，并研究这些能力之间的相互作用，这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标。从第二步正面指标看，论文明确涉及LLMs核心概念和reasoning能力方向。第三步排除标准中，虽然论文涉及数学领域，但数学在这里是作为通用推理能力的基础来研究，而非特定应用领域。论文提出的\"原子思维\"范式旨在理解和提升LLM的基础推理能力，这与研究目标高度一致，因此应该保留。"
    },
    {
        "index": "#74",
        "title": "RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance",
        "link": "/arxiv/2509.25604",
        "arxiv_id": "2509.25604",
        "authors": "Tianlang Chen, Minkai Xu, Jure Leskovec, Stefano Ermon",
        "summary": "Diffusion large language models (dLLMs) have shown great potential in large-scale language modeling, and there is an increasing interest in further improving the capacity to solve complex problems by guiding the reasoning process step by step. Common practice for autoregressive language models typically learns a process reward model with dense annotation for each intermediate step. However, this is challenging for dLLMs where the generation is in an any-order fashion and intermediate states are partially masked sentences. To this end, in this paper, we propose reward-free guidance (RFG), a principled method for guiding the reasoning trajectory of dLLMs without explicit process reward. The key idea of RFG is to parameterize the process reward by log-likelihood ratios of the enhanced and reference dLLMs, where the enhanced model can be easily obtained by any off-the-shelf dLLM that has been post-trained with reinforcement learning (RL) or supervised fine-tuning (SFT). We provide theoretical justification that RFG induces the reward-guided sampling distribution with no additional reward. We conduct comprehensive experiments on four challenging mathematical reasoning and code generation benchmarks using a diverse suite of dLLMs enhanced with various post-training methods. RFG consistently yields significant improvements across all tasks and model types, achieving accuracy gains of up to 9.2%. These findings establish RFG as a general training-free framework that scales test-time reasoning without reliance on external reward models.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.560052",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Reward-Free Guidance (RFG)\"的方法，用于增强扩散大语言模型(dLLMs)的推理能力。根据筛选标准的第一步，论文本质上是关于改进LLM的基础推理能力，特别是通过测试时扩展(test-time scaling)技术来提升模型的数学推理和代码生成能力，这完全符合\"保留\"的标准。论文提出的方法不需要显式的过程奖励模型，而是通过增强模型和参考模型的对数似然比来参数化过程奖励，这是一种新的训练范式。 在第二步的正面指标方面，论文明确关注大语言模型(LLMs)，聚焦于推理能力(reasoning)，特别是数学推理(math reasoning)，并涉及强化学习(RL)和监督微调(SFT)等训练方法。论文没有涉及第三步排除标准中的多模态与视觉、特定应用领域或模型可靠性等需要排除的内容。虽然论文在数学推理和代码生成任务上进行了实验，但这些是评估通用推理能力的基准测试，而非将LLM应用于特定领域的研究。 因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围，其核心目标是提高LLM本身的推理能力，而非将其作为工具应用于特定领域。"
    },
    {
        "index": "#80",
        "title": "Calibrating Verbalized Confidence with Self-Generated Distractors",
        "link": "/arxiv/2509.25532",
        "arxiv_id": "2509.25532",
        "authors": "Victor Wang, Elias Stengel-Eskin",
        "summary": "Calibrated confidence estimates are necessary for large language model (LLM) outputs to be trusted by human users. While LLMs can express their confidence in human-interpretable ways, verbalized LLM-generated confidence scores have empirically been found to be miscalibrated, reporting high confidence on instances with low accuracy and thereby harming trust and safety. We hypothesize that this overconfidence often stems from a given LLM's heightened suggestibility when faced with claims that it encodes little information about; we empirically validate this hypothesis, finding more suggestibility on lower-accuracy claims. Building on this finding, we introduce Distractor-Normalized Coherence (DINCO), which estimates and accounts for an LLM's suggestibility bias by having the model verbalize its confidence independently across several self-generated distractors (i.e. alternative claims), and normalizes by the total verbalized confidence. To further improve calibration, we leverage generator-validator disagreement, augmenting normalized validator confidence with a consistency-based estimate of generator confidence. Here, we frame the popular approach of self-consistency as leveraging coherence across sampled generations, and normalized verbalized confidence as leveraging coherence across validations on incompatible claims, allowing us to integrate these complementary dimensions of coherence into DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and therefore more usable -- confidence estimates, and that further sampling alone cannot close the gap between DINCO and baselines, with DINCO at 10 inference calls outperforming self-consistency at 100.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.568159",
        "filter_reason": "这篇论文的核心是关于改进大语言模型的基础能力，特别是校准LLM的口头置信度。论文提出的DINCO方法通过让模型在多个自生成的干扰项上独立表达其置信度并进行归一化处理，来估计和纠正LLM的易受暗示性偏差。这属于提升模型内在可靠性的研究，与增强LLM的通用推理能力密切相关。准确的置信度评估对于模型的可靠决策和推理至关重要，因此这项研究符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文没有涉及多模态、特定应用领域或模型可靠性的应用层面（如水印、安全等），因此不应被排除。根据第四步的筛选标准，这篇论文提出了一种新方法来增强模型的内在可靠性，从而提升模型的通用推理质量，应该被保留。"
    },
    {
        "index": "#78",
        "title": "Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model",
        "link": "/arxiv/2509.25543",
        "arxiv_id": "2509.25543",
        "authors": "Fahim Faisal, Kaiqiang Song, Song Wang, Simin Ma, Shujian Liu, Haoyun Deng, Sathish Reddy Indurthi",
        "summary": "While reinforcement learning has advanced the reasoning abilities of Large Language Models (LLMs), these gains are largely confined to English, creating a significant performance disparity across languages. To address this, we introduce Pivot-Based Reinforcement Learning with Semantically Verifiable Rewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by circumventing the need for human-annotated data in target languages. Our approach employs a high-performing English LLM as a \"pivot\" model to generate reference responses for reasoning tasks. A multilingual model is then rewarded based on the semantic equivalence of its responses to the English reference, effectively transferring the pivot model's reasoning capabilities across languages. We investigate several cross-lingual semantic reward functions, including those based on embeddings and machine translation. Extensive experiments on a suite of multilingual reasoning benchmarks show that our method significantly narrows the performance gap between English and other languages, substantially outperforming traditional PPO baselines. Specifically, our PB-RLSVR framework improves the average multilingual performance of Llama-3.1-8B-Instruct and Qwen3-32B by 16.41% and 10.17%, respectively, demonstrating a powerful and data-efficient approach to building truly multilingual reasoning agents.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.567027",
        "filter_reason": "这篇论文完全符合我的研究目标，核心贡献是提出了一种名为PB-RLSVR的新框架，用于增强大语言模型的多语言推理能力。根据筛选标准分析如下： 第一步核心判断：论文的本质是改进LLM的基础推理能力，而非将其作为工具应用于特定领域。论文提出了新的训练范式（PB-RLSVR），通过强化学习方法提升LLM的多语言推理能力，这直接符合\"改进LLM基础能力\"的保留标准。 第二步正面指标：论文包含多个关键正面指标： - 核心概念：明确关注Large Language Models (LLMs) - 能力方向：直接针对reasoning能力（特别是multilingual reasoning） - 训练方法：采用reinforcement learning（强化学习）进行优化 - 新兴范式：论文目标是构建\"multilingual reasoning agents\" 第三步排除标准：论文不涉及任何排除领域： - 未涉及多模态与视觉内容 - 未专注于特定应用领域（如医疗、化学等） - 未主要关注模型可靠性方面的水印、安全等问题 第四步特殊判断：论文没有涉及需要特殊判断的模糊情况，如智能体/工具使用或幻觉/可解释性/安全等特殊议题。 综合来看，这篇论文的核心是提升LLM的通用推理能力（特别是多语言推理能力），提出了新的训练范式和方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#79",
        "title": "Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning",
        "link": "/arxiv/2509.25534",
        "arxiv_id": "2509.25534",
        "authors": "Zhiling Ye, Yun Yue, Haowen Wang, Xudong Han, Jiadi Jiang, Cheng Wei, Lei Fan, Jiaxin Liang, Shuowen Zhang, Ji Li, Chunxiao Guo, Jian Wang, Peng Wei, Jinjie Gu",
        "summary": "Open-ended evaluation is essential for deploying large language models in real-world settings. In studying HealthBench, we observe that using the model itself as a grader and generating rubric-based reward signals substantially improves reasoning performance. Remarkably, the trained model also becomes a stronger grader. Motivated by this, we introduce Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning, a lightweight framework that enables faster and more resource-efficient training while surpassing baselines. Remarkably, on Qwen3-32B, training with just the 4000-sample HealthBench Easy subset is sufficient to obtain a model that exceeds GPT-5 on HealthBench Hard. Incorporating a small amount of teacher-graded data further enhances performance for less capable models.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-19",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.567701",
        "filter_reason": "根据筛选标准，这篇论文符合研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的本质是提出一种\"基于量规的自奖励强化学习\"框架，用于提高大语言模型的开放性推理能力。论文的核心贡献是一种新的训练范式，使用模型自身作为评分器并生成基于量规的奖励信号，从而显著提升模型的推理性能。这明显属于改进LLM基础能力和增强其推理能力的研究，符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：研究大语言模型（LLMs），明确提到了Qwen3-32B和GPT-5 - 能力方向：专注于推理能力（reasoning），特别是\"Open-Ended Reasoning\" - 训练方法：提出了强化学习（Reinforcement Learning）的新方法 第三步：排除标准 论文虽然提到了HealthBench（可能是一个医疗相关的基准测试），但论文的核心贡献是提出一种通用的训练框架，而不是专门针对医疗领域的应用。论文没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除标准中的内容。 第四步：特殊和模糊情况处理 论文提出的是一种通用的训练方法，虽然使用HealthBench作为评估基准，但方法本身是通用的，可以应用于各种开放性推理任务。这不是将LLM作为工具应用到特定领域的研究，而是提升LLM本身通用推理能力的方法论研究。 综上所述，这篇论文的核心贡献是提出一种新的强化学习训练范式，旨在提高大语言模型的通用推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#91",
        "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models",
        "link": "/arxiv/2509.26628",
        "arxiv_id": "2509.26628",
        "authors": "Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai",
        "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.579137",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，该论文的本质是改进大语言模型的基础推理能力。论文提出了一种新的过程监督强化学习框架(AttnRL)，专注于提高推理模型的探索效率，这属于改进LLM基础能力和提出新训练范式的研究范畴。论文虽然使用数学推理作为测试基准，但其方法论是通用的，旨在增强模型的推理能力本身，而非将LLM作为工具应用于特定领域。 其次，论文包含了多个关键正面指标：明确提到Large Language Models (LLMs)作为核心概念；聚焦于reasoning capabilities和reasoning models；使用Reinforcement Learning (RL)和Process-Supervised RL (PSRL)作为训练方法。这些都是研究目标中的核心要素。 第三，论文不符合任何排除标准。它不涉及多模态与视觉研究，不聚焦于特定应用领域（数学推理被视为通用推理能力的核心组成部分而非特定应用），也不关注模型可靠性的应用层面（如水印、安全等）。 最后，论文没有涉及特殊或模糊情况需要额外判断。它明确致力于通过改进强化学习方法来提升LLM的通用推理能力，与研究目标高度一致。 因此，这篇论文的核心贡献是提出一种新的PSRL框架来提高LLM的推理能力，完全符合筛选条件，应被保留。"
    },
    {
        "index": "#87",
        "title": "From Faithfulness to Correctness: Generative Reward Models that Think Critically",
        "link": "/arxiv/2509.25409",
        "arxiv_id": "2509.25409",
        "authors": "Qiyao Ma, Yunsheng Shi, Hongtao Tian, Chao Wang, Weiming Chang, Ting Yao",
        "summary": "Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.571832",
        "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是关于改进大语言模型的基础推理能力，提出了\"Thinking-supervised Reward Model (TRM)\"这一新的训练范式，旨在增强模型的批判性思维和推理能力。论文明确关注如何通过强化学习优化奖励模型，使LLM能够更好地评估和提升其逻辑推理和正确性判断能力。 其次，论文包含多个正面指标：核心概念上直接针对大语言模型(LLMs)；能力方向上明确聚焦于reasoning和critical thinking；训练方法上采用了reinforcement learning with verifiable rewards (RLVR)并进行策略优化。 第三，论文不符合任何排除标准。它不涉及多模态与视觉内容，不专注于特定应用领域（如医疗、化学等），也不是从应用层面讨论模型可靠性问题。 最后，在特殊和模糊情况处理上，论文提出的TRM方法可以视为一种减少幻觉、提升模型内在推理质量的新方法，通过结构化的评估流程（忠实度、推理、正确性）来增强模型的通用可靠性和推理能力。 综上所述，这篇论文的核心贡献是提出了一种新的奖励模型框架，通过引入批判性思维机制来提升大语言模型的通用推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#107",
        "title": "RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning",
        "link": "/arxiv/2509.25958",
        "arxiv_id": "2509.25958",
        "authors": "Gang Li, Yulei Qin, Xiaoyu Tan, Dingkang Yang, Yuchen Shi, Zihan Xu, Xiang Li, Xing Sun, Ke Li",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in eliciting complex reasoning in large language models (LLMs). However, standard RLVR training often leads to excessively verbose processes (in reasoning tasks) and inefficient exploration trajectories (in agentic settings), as outcome-only rewards provide no incentive for efficiency and the high variance in response length within relatively small rollout groups results in noisy optimization signals. To address this, we propose Rollout Response Recomposition (RoRecomp), a plug-and-play method that guides models toward concise reasoning by strategically recomposing the training data. RoRecomp separates responses into two distinct batch types: 1) priority batches, which combine short-correct and long-incorrect responses selected from online batches to provide a clear gradient signal for brevity, and 2) compensation batches, which utilize remaining responses from a replay buffer to maintain stability and prevent model collapse. To comprehensively evaluate effectiveness, we test RoRecomp across three settings where results demonstrate substantial efficiency gains: reducing reasoning length by 27.7% in zero RL training, reducing unnecessary tool calls by 46.8% while improving accuracy in agentic RL, and achieving up to 52.5% length reduction in thinking compression, all with minimal performance impact.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.610068",
        "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是关于改进LLM的推理效率，提出了\"Rollout Response Recomposition (RoRecomp)\"这一新的强化学习方法来优化LLM的推理过程，使其更加简洁高效。这明确属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的范畴，而不是将LLM作为工具应用到特定领域。 其次，论文包含了多个正面指标：核心概念上明确涉及\"large language models (LLMs)\"；能力方向上专注于\"reasoning efficiency\"和\"complex reasoning\"；训练方法上基于\"Reinforcement learning with verifiable rewards (RLVR)\"；新兴范式上提到了\"agentic settings\"和\"tool calls\"，表明它涉及LLM-based agents和tool use。 第三，论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。 最后，在特殊情况下，论文对智能体/工具使用的讨论是从通用角度出发的，目的是提高LLM在智能体环境中的推理效率，而不是将智能体/工具应用在特定领域。 综上所述，这篇论文的核心贡献是通过一种新的强化学习方法提高LLM的推理效率，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#108",
        "title": "Boosting Process-Correct CoT Reasoning by Modeling Solvability of Multiple-Choice QA",
        "link": "/arxiv/2509.25941",
        "arxiv_id": "2509.25941",
        "authors": "Raphael Schumann, Stefan Riezler",
        "summary": "Reasoning quality in large language models depends not only on producing correct answers but also on generating valid intermediate steps. We study this through multiple-choice question answering (MCQA), which provides a controlled setting with fixed answer options. Our analysis shows that when questions are effectively unsolvable for a model, spurious chains of thought (CoTs) are more likely to appear, leading to false positives. By estimating the solvability of each question, we uncover an intermediate regime where learning is most effective. Building on this insight, we adapt outcome-supervised reward models and reinforcement learning with group-relative advantage to incorporate solvability into their objectives. Across experiments on math and multimodal datasets, these modifications consistently yield higher rates of process-correct reasoning and, in reinforcement learning, improved answer accuracy as well. Our results highlight solvability as a key factor for reducing hallucinations and increasing reliability in CoT reasoning.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.610694",
        "filter_reason": "这篇论文的核心贡献是提出了一种通过建模问题可解性来提升大语言模型思维链(CoT)推理质量的方法。论文发现，当问题对模型来说实际上无法解决时，更容易出现虚假的思维链，导致推理过程不可靠。基于这一发现，作者调整了奖励模型和强化学习方法，将可解性纳入训练目标，从而减少推理过程中的幻觉，提高过程正确推理率和答案准确性。这直接符合研究目标中\"改进LLM的基础能力、增强其逻辑、数学、多步推理等通用能力\"的要求，特别是针对思维链(CoT)这一重要的推理范式进行了优化。论文使用了强化学习方法进行训练，并在数学推理等通用能力上进行了验证，没有局限于特定应用领域。虽然论文提到了在多模态数据集上的实验，但这只是验证方法有效性的一个方面，并非论文的主要焦点。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#113",
        "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation",
        "link": "/arxiv/2509.25849",
        "arxiv_id": "2509.25849",
        "authors": "Ziniu Li, Congliang Chen, Tianyun Yang, Tian Ding, Ruoyu Sun, Ge Zhang, Wenhao Huang, Zhi-Quan Luo",
        "summary": "Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task's exploration as an \"item\" with a distinct \"value\" and \"cost\", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model's current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational \"free lunch\", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.618870",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础能力，提出了一种新的强化学习训练范式，通过优化预算分配来增强LLM的推理能力。论文聚焦于LLM的自我改进机制，而非将LLM作为工具应用于特定领域。 其次，论文包含了多个正面指标：核心概念明确聚焦于Large Language Models (LLMs)；能力方向关注数学推理(mathematical reasoning)，并在数学推理基准上展示了显著提升；训练方法采用强化学习(reinforcement learning)，讨论了LLM如何通过强化学习自我改进。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是提出了一种基于背包问题思想的探索预算分配方法，解决了强化学习训练LLM时资源分配不均导致的梯度问题。这种方法通过自适应地分配计算资源，显著提高了LLM在数学推理任务上的表现，平均提升2-4分，特定任务上最高提升9分。这直接增强了LLM的通用推理能力，而非针对特定领域的应用，因此完全符合研究目标。"
    },
    {
        "index": "#115",
        "title": "Learning to Reason as Action Abstractions with Scalable Mid-Training RL",
        "link": "/arxiv/2509.25810",
        "arxiv_id": "2509.25810",
        "authors": "Shenao Zhang, Donghan Yu, Yihao Feng, Bowen Jin, Zhaoran Wang, John Peebles, Zirui Wang",
        "summary": "Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a mid-training stage. An effective mid-training phase should identify a compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm. Specifically, we derive a sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.619944",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提出一种名为\"Reasoning as Action Abstractions (RA3)\"的新训练范式，通过中程训练和强化学习相结合的方式来增强大语言模型的推理能力。这明显属于改进LLM基础能力和通用推理能力的研究，而非将LLM作为工具应用于特定领域。 从正面指标看，论文直接关注大语言模型(LLMs)的推理能力(reasoning)，并使用强化学习(RL)作为核心训练方法，这些都是研究目标中的关键要素。虽然实验在代码生成任务上进行，但这只是验证方法有效性的手段，论文的核心贡献是一种通用的训练方法，而非专门针对代码领域。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性问题。它直接关注如何通过新的训练范式来提升LLM的推理能力，这与研究目标高度一致。 因此，这篇论文的核心贡献是提出一种可扩展的中程训练算法来增强大语言模型的通用推理能力，完全符合研究范围。"
    },
    {
        "index": "#124",
        "title": "Nudging the Boundaries of LLM Reasoning",
        "link": "/arxiv/2509.25666",
        "arxiv_id": "2509.25666",
        "authors": "Justin Chih-Yao Chen, Becky Xiangyu Peng, Prafulla Kumar Choubey, Kung-Hsiang Huang, Jiaxin Zhang, Mohit Bansal, Chien-Sheng Wu",
        "summary": "Current online reinforcement learning (RL) algorithms like GRPO share a key limitation in LLM reasoning: they cannot learn from problems that are \"unsolvable\" to the model. In other words, they can only improve performance on problems where the model is capable of exploring the correct answer. Consequently, the model's \"upper limit\" remains unchanged after RL training, even though the likelihood of solving easier, solvable problems may increase. These hard samples cannot contribute to training, as no rollouts yield rewards and thus no gradients are produced. To unlock learning from these hard samples, we propose NuRL, a \"nudging\" method that aims to push the upper bound of LLM reasoning using self-generated hints, i.e., abstract cues that help reduce the problem difficulty for the model. Given a question and its gold answer, the model generates a CoT and then produces a hint containing the core knowledge needed to solve the problem. During training, we generate G rollouts from the base policy and use the pass rate to decide whether the hint should be injected. For hard samples with a 0% pass rate, we inject the hint and regenerate a new batch of trajectories. This yields two benefits: (1) the hint boosts pass rates (from 0% to non-zero), thereby introducing training signals for previously unsolvable samples, and (2) the hints are self-generated, avoiding distributional shift and do not rely on external models. NuRL achieves consistent improvements across 6 benchmarks and 3 models, while remaining complementary to test-time scaling. Notably, NuRL can raise the model's upper limit, whereas GRPO leaves pass@1024 unchanged from the base model. Furthermore, we present a systematic study of what makes an effective hint and when hints are most useful. Interestingly, the best hints are abstract and high-level, and are most beneficial when applied necessarily and after GRPO has converged.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-30",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.629822",
        "filter_reason": "这篇论文的核心贡献是提出NuRL方法，用于提升大语言模型的推理能力上限。论文明确聚焦于解决当前强化学习算法（如GRPO）在LLM推理中的关键局限性：无法从模型\"无法解决\"的问题中学习。该方法通过自生成的提示（hints）来降低问题难度，使模型能够从原本无法解决的样本中获取训练信号，从而突破模型的推理能力上限。这完全符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的要求。 论文包含多个正面指标：核心概念涉及大语言模型(LLMs)，能力方向聚焦于推理(reasoning)，训练方法基于强化学习(RL)进行改进，并利用了思维链(CoT)这一提升推理能力的重要范式。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究范围。"
    },
    {
        "index": "#134",
        "title": "Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search",
        "link": "/arxiv/2509.25420",
        "arxiv_id": "2509.25420",
        "authors": "Yingqian Cui, Zhenwei Dai, Pengfei He, Bing He, Hui Liu, Xianfeng Tang, Jingying Zeng, Suhang Wang, Yue Xing, Jiliang Tang, Benoit Dumoulin",
        "summary": "Large Language Models (LLMs) have achieved significant advances in reasoning tasks. A key approach is tree-based search with verifiers, which expand candidate reasoning paths and use reward models to guide pruning and selection. Although effective in improving accuracy, these methods are not optimal in terms of efficiency: they perform simple decomposition on the reasoning process, but ignore the planning-execution nature of tasks such as math reasoning or code generation. This results in inefficient exploration of reasoning process. To address this, we propose a dual-phase test-time scaling framework that explicitly separates reasoning into planning and execution, and performs search over the two phases individually. Specifically, we decompose reasoning trajectories and develop reward models for each phase, enabling the search to explore and prune plans and executions separately. We further introduce a dynamic budget allocation mechanism that adaptively redistributes sampling effort based on reward feedback, allowing early stopping on confident steps and reallocation of computation to more challenging parts of the reasoning process. Experiments on both mathematical reasoning and code generation benchmarks demonstrate that our approach consistently improves accuracy while reducing redundant computation.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.640147",
        "filter_reason": "这篇论文完全符合我的研究目标。根据筛选标准，我进行了如下分析： 第一步：核心判断——这篇论文的本质是关于改进LLM的推理能力。论文提出了一种双阶段测试时扩展框架，明确将推理分为规划和执行两个阶段，并在这两个阶段上分别进行搜索。这是一种新的推理方法论，旨在提高LLM的基础推理能力，特别是在数学推理和代码生成任务上。这明显属于改进LLM通用推理能力的研究，而不是将LLM作为工具应用于特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确讨论了Large Language Models (LLMs) - 能力方向：专注于reasoning（特别是math reasoning），并明确提到了planning作为其双阶段框架的核心组成部分 - 训练方法：使用了reward models来引导搜索过程，这与强化学习方法相关 第三步：排除标准——论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉内容 - 不专注于特定应用领域（虽然使用数学推理和代码生成作为实验基准，但这些是评估通用推理能力的标准测试，而非特定领域应用） - 不讨论模型可靠性层面的水印、安全等问题 第四步：特殊和模糊情况——论文没有涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等内容。 论文的核心贡献是提出了一种新的推理框架，通过将推理过程分解为规划和执行两个阶段，并在这两个阶段上分别进行搜索，来提高LLM的推理效率和准确性。这是一种直接提升LLM通用推理能力的方法论研究，完全符合我的研究目标。"
    },
    {
        "index": "#135",
        "title": "Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs",
        "link": "/arxiv/2509.25414",
        "arxiv_id": "2509.25414",
        "authors": "Hao Ban, Kaiyi Ji",
        "summary": "Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \\textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \\textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at https://github.com/OptMN-Lab/ALoRA.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.640618",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。首先，从核心判断来看，这篇论文的本质是关于改进LLM微调技术的参数效率，特别是针对LoRA方法的优化，属于\"改进LLM的基础能力\"的范畴，而非将LLM作为工具应用到特定领域。论文提出了ALoRA和Fed-ALoRA两种新的参数共享策略，用于提升多任务微调和联邦学习场景下的模型性能。其次，从正面指标看，论文明确讨论了大语言模型(LLMs)，并在常识推理(math reasoning)和数学推理(math reasoning)任务上进行了实验验证，这些都是与通用推理能力直接相关的能力方向。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面的研究。虽然论文没有涉及强化学习、智能体等新兴范式，但这不是排除的必要条件。总体而言，这篇论文通过优化参数共享策略来提升LLM在推理任务上的表现，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#136",
        "title": "Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs",
        "link": "/arxiv/2509.25380",
        "arxiv_id": "2509.25380",
        "authors": "Shane Bergsma, Nolan Dey, Joel Hestness",
        "summary": "Data curriculums have become central to successful LLM training, yet principles governing optimal data placement remain unclear. We introduce the *training re-evaluation curve (TREC)*, a diagnostic that retrospectively evaluates training batches *using the final model weights*. The TREC characterizes how well a trained model retains training data as a function of *when* the data was encountered during training. Analyzing TRECs for models from 111M to 3.9B parameters, we show that placing high-quality data at low points on the TREC significantly improves performance. Importantly, while a TREC is initially observable only after training, we demonstrate it can be *predicted in advance* from AdamW's implicit EMA coefficients, enabling proactive curriculum design. By predicting TRECs for published training recipes, we explain prior ablations and reveal suboptimal data placements. We also align high-quality data with TREC minima in order to improve continual pre-training of a 3.9B-parameter LLM trained on 900B tokens.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.641099",
        "filter_reason": "这篇论文的核心贡献是提出了一种称为\"训练重评估曲线(TREC)\"的诊断方法，用于优化LLM的数据课程设计。从第一步核心判断来看，这属于\"提出新的训练范式\"的范畴，旨在提高LLM的基础能力。论文通过优化数据放置策略来改进模型性能，这种方法虽然不是直接针对推理能力，但属于提升LLM整体基础能力的研究，与通用推理能力的提升密切相关。论文明确研究LLM（从111M到3.9B参数的模型），符合正面指标中的核心概念。在排除标准方面，论文不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然论文没有直接提及推理、规划等具体能力方向，但其提出的训练优化方法为提升LLM的通用能力（包括推理能力）提供了新的方法论支持，因此符合研究范围。"
    },
    {
        "index": "#141",
        "title": "Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning",
        "link": "/arxiv/2509.25267",
        "arxiv_id": "2509.25267",
        "authors": "Jiexi Xu",
        "summary": "The performance of Large Language Models (LLMs) depends heavily on the chosen prompting strategy, yet static approaches such as Zero-Shot, Few-Shot, or Chain-of-Thought (CoT) impose a rigid efficiency-accuracy trade-off. Highly accurate strategies like Self-Consistency (SC) incur substantial computational waste on simple tasks, while lightweight methods often fail on complex inputs. This paper introduces the Prompt Policy Network (PPN), a lightweight reinforcement learning framework that formalizes adaptive strategy selection as a single-step Markov Decision Process (MDP). The PPN, trained with Proximal Policy Optimization (PPO) and guided by a resource-explicit reward function, learns to allocate costly reasoning strategies only when necessary. Experiments on arithmetic reasoning benchmarks demonstrate that PPN achieves superior performance on the efficiency-accuracy Pareto front, delivering up to 61.5% token cost reduction compared to Self-Consistency while maintaining competitive accuracy. This work contributes a systematic, adaptive framework for cost-efficient LLM deployment, advancing the design of lightweight optimization techniques for scalable and sustainable language model applications.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.648747",
        "filter_reason": "这篇论文的核心贡献是提出了Prompt Policy Network (PPN)，一个轻量级的强化学习框架，用于自适应地优化大语言模型的提示策略。该方法将策略选择形式化为马尔可夫决策过程，使用近端策略优化(PPO)进行训练，使模型能够根据任务复杂度智能地分配推理资源。这直接符合研究目标中\"改进LLM的基础能力\"和\"增强其推理能力\"的要求。论文专注于提高LLM在算术推理等通用推理任务上的效率-准确性权衡，而不是将LLM应用于特定领域。同时，论文采用了强化学习这一新的训练范式，进一步增强了LLM的通用推理能力。论文包含多个正面指标，如核心概念(LLMs)、能力方向(reasoning)和训练方法(reinforcement learning)，且不符合任何排除标准。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#143",
        "title": "HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement",
        "link": "/arxiv/2509.25240",
        "arxiv_id": "2509.25240",
        "authors": "Ming Yang, Xiaofan Li, Zhiyuan Ma, Dengliang Shi, Jintao Du, Yu Cheng, Weiguo Zheng",
        "summary": "Recent curriculum reinforcement learning for large language models (LLMs) typically rely on difficulty-based annotations for data filtering and ordering. However, such methods suffer from local optimization, where continual training on simple samples in the early steps can cause the policy to lose its exploration. We propose a novel schema, namely Hamiltonian curiosity augmented large language model reinforcement (HAMMER), that transfers diversity metrics, commonly used in dataset evaluation, into the dynamic reinforcement learning procedure, where training samples are ordered via a minimum-semantic Hamiltonian path making the initial training retrain more exploration. From a theoretical perspective of generalization bounds, diversity-driven ordering facilitates stable convergence. Empirical evaluations indicate that HAMMER stimulates model \"curiosity\" and consistently achieves a 3% to 4% average accuracy gain across diverse inference benchmark.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.649825",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种名为HAMMER的新型强化学习范式，旨在改进大语言模型的基础训练过程，解决当前课程强化学习中存在的局部优化问题。该方法通过将多样性度量引入动态强化学习过程，增强了模型的探索能力，从而提高其推理表现，这直接关注的是LLM本身的通用能力提升，而非特定领域应用。 其次，论文包含了多个关键正面指标：明确以大语言模型(LLMs)为核心研究对象；关注推理能力(inference benchmark上的表现)；采用强化学习方法作为主要训练范式；提出的\"curiosity\"概念与模型自我进化有一定关联。 第三，论文不涉及任何排除标准领域，没有讨论多模态与视觉、特定应用领域或模型可靠性等应用层面问题。 最后，论文的核心贡献在于提出了一种新的训练方法来增强LLM的通用推理能力，通过多样性驱动的样本排序促进稳定收敛，并在推理基准测试上实现了3%到4%的准确率提升。这完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#142",
        "title": "Language Model Planning from an Information Theoretic Perspective",
        "link": "/arxiv/2509.25260",
        "arxiv_id": "2509.25260",
        "authors": "Muhammed Ustaomeroglu, Baris Askin, Gauri Joshi, Carlee Joe-Wong, Guannan Qu",
        "summary": "The extent to which decoder-only language models (LMs) engage in planning, that is, organizing intermediate computations to support coherent long-range generation, remains an open and important question, with implications for interpretability, reliability, and principled model design. Planning involves structuring computations over long horizons, considering multiple possible continuations, and selectively reusing past information, but how effectively transformer-based LMs realize these capabilities is still unclear. We address these questions by analyzing the hidden states at the core of transformer computations, which capture intermediate results and act as carriers of information. Since these hidden representations are often redundant and encumbered with fine-grained details, we develop a pipeline based on vector-quantized variational autoencoders that compresses them into compact summary codes. These codes enable measuring mutual information, allowing systematic analysis of the computational structure underlying model behavior. Using this framework, we study planning in LMs across synthetic grammar, path-finding tasks, and natural language datasets, focusing on three key aspects: (i) the planning horizon of pre-output computations, (ii) the extent to which the model considers alternative valid continuations, and (iii) the reliance of new predictions on earlier computations. By answering these questions, we advance the understanding of how planning is realized in LMs and contribute a general-purpose pipeline for probing the internal dynamics of LMs and deep learning systems. Our results reveal that the effective planning horizon is task-dependent, that models implicitly preserve information about unused correct continuations, and that predictions draw most on recent computations, though earlier blocks remain informative.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.649271",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。根据筛选标准，我的判断过程如下： 第一步：核心判断 论文的核心是研究语言模型(LLM)的规划能力(planning)，这属于通用推理能力的重要组成部分。论文探讨了解码器语言模型如何组织中间计算以支持连贯的长距离生成，分析模型如何进行结构化计算、考虑多种可能延续并选择性重用过去信息。这不是将LLM作为工具应用到特定领域，而是研究LLM本身的基础能力，因此符合保留标准。 第二步：正面指标 论文包含了多个正面指标： - 核心概念：明确研究\"decoder-only language models (LMs)\"，属于LLMs范畴 - 能力方向：重点研究\"planning\"（规划），这是明确提到的推理能力，属于problem-solving的一部分 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉领域 - 不聚焦于医疗、化学、生物等特定应用领域（虽然使用了合成语法、路径查找等任务，但这些是用于评估通用能力的基准任务） - 不主要关注水印、安全等模型可靠性应用层面问题 第四步：特殊和模糊情况处理 论文涉及可解释性，但主要是为了理解模型的内部工作机制和规划过程，而不是单纯研究可解释性本身。这种对模型内部计算结构的分析有助于提升对模型通用推理能力的理解，符合保留标准。 综合判断：这篇论文的核心贡献是提出了一种基于信息理论的方法来分析和理解语言模型中的规划能力，这直接关系到提升LLM的通用推理能力。论文的研究成果有助于揭示LLM如何进行规划，从而为改进模型的推理能力提供理论基础和分析工具，完全符合研究目标。"
    },
    {
        "index": "#144",
        "title": "A Formal Comparison Between Chain-of-Thought and Latent Thought",
        "link": "/arxiv/2509.25239",
        "arxiv_id": "2509.25239",
        "authors": "Kevin Xu, Issei Sato",
        "summary": "Chain-of-Thought (CoT) elicits reasoning in large language models by explicitly generating intermediate steps in natural language. In contrast, Latent Thought in looped models operates directly in the continuous latent space, enabling computation beyond discrete linguistic representations. While both approaches exploit iterative computation, their comparative capabilities remain underexplored. In this work, we present a formal analysis showing that Latent Thought in Looped Transformers enables parallel computation, which is more efficient than the inherently sequential process of CoT. In contrast, CoT leverages stochastic decoding to approximate solutions to problems where exact computation is intractable. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical guidance for choosing between reasoning paradigms. Code is available at https://github.com/kevin671/cot-vs-loop.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.650308",
        "filter_reason": "这篇论文完全符合我的研究目标。根据筛选标准，我的判断过程如下： 第一步：核心判断——论文的核心是比较两种大语言模型的推理方法：Chain-of-Thought (CoT)和Latent Thought。论文分析了这两种方法如何提升LLM的推理能力，CoT通过生成自然语言中间步骤激发推理，而Latent Thought则在连续潜在空间中进行计算。这明显是关于改进LLM基础推理能力的研究，属于提升LLM本身通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标——论文包含多个正面指标： - 核心概念：明确讨论大语言模型(LLMs)的推理机制 - 能力方向：直接关注reasoning这一核心能力，分析不同推理范式的效率和适用性 - 新兴范式：探讨了推理范式的新比较，为选择推理方法提供实践指导 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域（如医疗、化学等） - 不讨论模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况——论文不涉及需要特殊判断的情况，它纯粹关注LLM的通用推理机制比较。 论文的核心贡献是通过形式化分析比较了两种不同的推理范式，揭示了它们在计算效率和适用问题类型上的差异，为提升LLM的通用推理能力提供了理论指导和实践建议。这完全符合我寻找\"致力于提高大语言模型本身的通用推理能力\"论文的研究目标。"
    },
    {
        "index": "#139",
        "title": "Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution",
        "link": "/arxiv/2509.25301",
        "arxiv_id": "2509.25301",
        "authors": "Tianrui Qin, Qianben Chen, Sinuo Wang, He Xing, King Zhu, He Zhu, Dingfeng Shi, Xinxin Liu, Ge Zhang, Jiaheng Liu, Yuchen Eleanor Jiang, Xitong Gao, Wangchunshu Zhou",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks when equipped with external tools. However, current frameworks predominantly rely on sequential processing, leading to inefficient execution particularly for tasks requiring extensive tool interaction. This paper introduces Flash-Searcher, a novel parallel agent reasoning framework that fundamentally reimagines the execution paradigm from sequential chains to directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into subtasks with explicit dependencies, enabling concurrent execution of independent reasoning paths while maintaining logical constraints. Through dynamic workflow optimization, our framework continuously refines the execution graph based on intermediate results, effectively integrating summary module. Comprehensive evaluations across multiple benchmarks demonstrate that Flash-Searcher consistently outperforms existing approaches. Specifically, it achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while reducing agent execution steps by up to 35% compared to current frameworks. Furthermore, when distilling this parallel reasoning pipeline into single models, we observe substantial performance gains across diverse backbone architectures, underscoring the generalizability of our methodology. Our work thus represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.647817",
        "filter_reason": "根据筛选标准，我对这篇论文进行了全面分析： 第一步：核心判断——这篇论文的本质是提出Flash-Searcher，一种基于有向无环图(DAGs)的并行智能体推理框架。论文的核心贡献是改进LLM使用工具的效率和推理能力，将执行范式从顺序链转变为并行执行。这属于\"智能体协作框架、工具使用\"的方法论研究，直接针对提升LLM的通用推理能力，因此符合保留标准。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确提到\"Large language models (LLMs)\" - 能力方向：涉及\"complex reasoning tasks\"、\"reasoning framework\"和任务分解为子任务，与推理和规划能力直接相关 - 新兴范式：提到\"web agents\"和\"agent reasoning框架\"，以及\"equipped with external tools\"，符合基于LLM的智能体和工具使用范式 第三步：排除标准——论文不符合任何排除领域： - 不涉及多模态与视觉内容 - 不专注于特定应用领域（如医疗、化学等），而是关注通用推理框架 - 不主要讨论模型可靠性方面的水印、安全等问题 第四步：特殊和模糊情况处理——论文提出的Flash-Searcher是一种通用的智能体协作框架，旨在增强LLM的通用问题解决能力，而非针对特定领域的应用。虽然涉及工具使用，但这是作为提升通用推理能力的手段，而非特定领域应用。 综合判断：这篇论文的核心贡献是提出一种新的并行执行框架来提升LLM的推理效率和工具使用能力，属于改进LLM通用推理能力的方法论研究，完全符合\"大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#145",
        "title": "Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation",
        "link": "/arxiv/2509.25204",
        "arxiv_id": "2509.25204",
        "authors": "Jin Li, Zhebo Wang, Tianliang Lu, Mohan Li, Wenpeng Xing, Meng Han",
        "summary": "Entropy-based inference methods have gained traction for improving the reliability of Large Language Models (LLMs). However, many existing approaches, such as entropy minimization techniques, suffer from high computational overhead and fail to leverage historical token context effectively. To address these limitations, we propose Spectral Logit Sculpting (SLS), a lightweight inference-time optimization method that dynamically modulates token distributions using spectral and entropic properties of recent logits. SLS maintains a sliding buffer of top-K logits, performs on-the-fly Singular Value Decomposition (SVD) to identify dominant spectral directions, and adaptively rescales logits based on both entropy and logit gap statistics--only activating when uncertainty is high. Without updating any model parameters, SLS effectively sharpens the output distribution while preserving contextual consistency. Experimental results on multiple public benchmarks demonstrate that SLS consistently outperforms existing baseline methods, achieving superior accuracy in mathematical, coding, and scientific reasoning tasks.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-19",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.650835",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：该论文的本质是提出一种名为\"Spectral Logit Sculpting (SLS)\"的轻量级推理时优化方法，通过动态调整token分布来提升LLM的推理能力。这属于改进LLM基础能力的研究，特别是增强其在数学、编码和科学推理任务中的表现，符合\"改进LLM的基础能力\"和\"增强其逻辑、数学、多步推理等通用能力\"的要求。 第二步正面指标：论文明确涉及LLMs核心概念，并直接针对reasoning能力（特别是math reasoning和scientific reasoning），满足关键正面指标。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域。虽然论文提到在数学、编码和科学推理任务中的性能，但这些被视为通用推理能力而非特定领域应用。 第四步特殊和模糊情况：论文提出的SLS方法通过锐化输出分布和保持上下文一致性，有效提升了模型在推理任务中的准确性，这属于提升模型通用推理质量的新方法，符合保留标准。 综合来看，这篇论文的核心贡献是提出一种新的推理时优化技术来增强LLM的通用推理能力，与研究目标高度一致。"
    },
    {
        "index": "#146",
        "title": "TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models",
        "link": "/arxiv/2509.24803",
        "arxiv_id": "2509.24803",
        "authors": "Tong Guan, Zijie Meng, Dianqi Li, Shiyu Wang, Chao-Han Huck Yang, Qingsong Wen, Zuozhu Liu, Sabato Marco Siniscalchi, Ming Jin, Shirui Pan",
        "summary": "Recent advances in multimodal time series learning underscore a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering, without reaching the depth of genuine reasoning. The absence of well-defined tasks that genuinely require time series reasoning, along with the scarcity of high-quality data, has limited progress in building practical time series reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite (TSR-Suite), which formalizes four atomic tasks that span three fundamental capabilities for reasoning with time series: (1) perception, acquired through scenario understanding and causality discovery; (2) extrapolation, realized via event-aware forecasting; and (3) decision-making, developed through deliberation over perception and extrapolation. TSR-Suite is the first comprehensive time series reasoning suite that supports not only thorough evaluation but also the data pipeline and training of TSRMs. It contains more than 23K samples, of which 2.3K are carefully curated through a human-guided hierarchical annotation process. Building on this foundation, we introduce TimeOmni-1, the first unified reasoning model designed to address diverse real-world problems demanding time series reasoning. The model is trained in multiple stages, integrating a mixture of task scenarios, novel reward functions, and tailored optimizations. Experiments show that TimeOmni-1 delivers strong out-of-distribution generalization across all tasks and achieves a high rate of valid responses. It significantly improves causality discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.651384",
        "filter_reason": "这篇论文的核心贡献是提出了TimeOmni-1模型和TSR-Suite框架，旨在增强大语言模型在时间序列推理方面的能力。根据筛选标准，我判断该论文符合研究范围，原因如下： 首先，从本质上看，论文的核心是改进LLM的基础推理能力，而非将LLM作为工具应用于特定领域。论文专注于提升大语言模型在时间序列推理方面的三个基本能力：感知、外推和决策制定，这属于增强LLM通用推理能力的范畴。 其次，论文包含多个正面指标：明确关注大语言模型(LLMs)，核心能力方向是reasoning（时间序列推理涉及逻辑推理和问题解决），并提到了使用\"novel reward functions\"的训练方法，可能涉及强化学习技术。 第三，论文不符合排除标准。虽然提到\"multimodal time series learning\"，但主要关注点是时间序列数据的推理能力，而非视觉或多模态处理。论文也未限定在特定应用领域，而是关注通用的时间序列推理能力。 最后，在特殊和模糊情况处理上，论文没有涉及智能体/工具使用或幻觉/可解释性/安全等可能引起歧义的方面。 综上所述，TimeOmni-1论文致力于通过新的训练范式和任务设计来提高LLM的通用推理能力，特别是时间序列推理能力，符合研究课题的核心目标。"
    },
    {
        "index": "#132",
        "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search",
        "link": "/arxiv/2509.25454",
        "arxiv_id": "2509.25454",
        "authors": "Fang Wu, Weihao Xuan, Heli Qi, Ximing Lu, Aaron Tu, Li Erran Li, Yejin ChoiRetry",
        "summary": "Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.639025",
        "filter_reason": "这篇论文完全符合我的研究目标。根据筛选标准分析如下： 第一步：核心判断 论文的核心是关于改进大语言模型的基础推理能力，提出了DeepSearch框架，将蒙特卡洛树搜索(Monte Carlo Tree Search)集成到可验证奖励的强化学习(RLVR)训练过程中。这正是一种新的训练范式，旨在解决当前LLM在推理能力训练中遇到的瓶颈问题，特别是探索不足导致的训练平台期。论文本质上是增强LLM的通用推理能力，而非将其应用于特定领域。 第二步：正面指标 论文包含多个关键正面指标： - 核心概念：明确关注大语言模型(LLMs) - 能力方向：重点研究推理能力(reasoning)，特别是数学推理(mathematical reasoning) - 训练方法：基于强化学习(RLVR)，并提出创新性改进 - 论文解决了LLM推理训练中的关键瓶颈，属于提升通用推理能力的方法论研究 第三步：排除标准 论文不涉及任何排除标准中的领域： - 没有涉及多模态与视觉内容 - 没有针对医疗、化学、生物等特定应用领域 - 没有讨论模型基础设施、部署优化或硬件加速 - 没有涉及水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况 论文不涉及需要特殊处理的模糊情况，其核心贡献是提升LLM的通用推理能力，而非应用层面的研究。 核心贡献： DeepSearch框架通过将蒙特卡洛树搜索嵌入到训练循环中，解决了RLVR训练中的探索不足问题，实现了系统化探索和细粒度的信用分配。论文提出的全局前沿选择策略、基于熵的指导和自适应重放缓冲训练等方法，都是为了提升LLM的推理能力。在数学推理基准测试上的结果证明了这种方法的有效性，展示了算法创新对于提升LLM推理能力的重要性。 因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围，应该被保留。"
    },
    {
        "index": "#4",
        "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models",
        "link": "/arxiv/2509.26626",
        "arxiv_id": "2509.26626",
        "authors": "Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain",
        "summary": "Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains -- not just the final answers -- and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the model to combine solutions via a novel aggregation-aware reinforcement learning approach yields significant performance gains. Code available at https://github.com/HyperPotatoNeo/RSA.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.844818",
        "filter_reason": "这篇论文完全符合我的研究范围，核心原因如下： 第一步核心判断：论文的本质是提出一种名为\"递归自聚合\"(RSA)的测试时缩放方法，用于提高大语言模型的基础推理能力。该方法通过结合并行和顺序缩放的优势，递归地改进候选推理链，从而增强LLM的逻辑推理和多步推理能力。这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的范畴，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确以大语言模型(LLMs)为研究对象 - 能力方向：专注于reasoning（推理）能力的提升，特别是通过递归自聚合增强深度思考能力 - 训练方法：提出了\"聚合感知强化学习\"这一新的训练方法来改进模型性能 第三步排除标准：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于任何特定应用领域（如医疗、化学、生物等） - 不主要关注模型可靠性方面的水印、安全等问题 第四步特殊和模糊情况处理：虽然论文没有明确提出智能体或工具使用的概念，但RSA的递归自聚合过程可以视为一种类似智能体协作的框架，用于增强LLM的通用问题解决能力，符合保留条件。 最终决策：论文的核心贡献是提出了一种新的方法来提升LLM的通用推理能力，使其能够在各种推理任务上取得更好的性能，甚至使较小的模型能够与更大的推理模型竞争。这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标，因此应该被保留。"
    },
    {
        "index": "#147",
        "title": "Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models",
        "link": "/arxiv/2509.23108",
        "arxiv_id": "2509.23108",
        "authors": "Morgan McCarty, Jorge Morales",
        "summary": "This study offers a novel approach for benchmarking complex cognitive behavior in artificial systems. Almost universally, Large Language Models (LLMs) perform best on tasks which may be included in their training data and can be accomplished solely using natural language, limiting our understanding of their emergent sophisticated cognitive capacities. In this work, we created dozens of novel items of a classic mental imagery task from cognitive psychology. A task which, traditionally, cognitive psychologists have argued is solvable exclusively via visual mental imagery (i.e., language alone would be insufficient). LLMs are perfect for testing this hypothesis. First, we tested several state-of-the-art LLMs by giving text-only models written instructions and asking them to report the resulting object after performing the transformations in the aforementioned task. Then, we created a baseline by testing 100 human subjects in exactly the same task. We found that the best LLMs performed significantly above average human performance. Finally, we tested reasoning models set to different levels of reasoning and found the strongest performance when models allocate greater amounts of reasoning tokens. These results provide evidence that the best LLMs may have the capability to complete imagery-dependent tasks despite the non-pictorial nature of their architectures. Our study not only demonstrates an emergent cognitive capacity in LLMs while performing a novel task, but it also provides the field with a new task that leaves lots of room for improvement in otherwise already highly capable models. Finally, our findings reignite the debate over the formats of representation of visual imagery in humans, suggesting that propositional reasoning (or at least non-imagistic reasoning) may be sufficient to complete tasks that were long-thought to be imagery-dependent.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T22:50:18.651827",
        "filter_reason": "这篇论文的核心是研究大型语言模型(LLMs)的命题推理能力，特别是探索它们如何通过非图像推理来完成传统上被认为需要视觉心理意象才能解决的任务。论文创建了一系列新颖的心理意象任务，测试了多个最先进的LLMs，发现最好的LLMs表现显著高于人类平均水平，并且当模型分配更多推理令牌时表现最强。这直接符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求，因为论文揭示了LLMs的一种新兴认知能力，有助于我们理解和改进LLMs的逻辑推理能力。论文包含正面指标中的核心概念(LLMs)和能力方向(reasoning)，不涉及任何排除标准中列出的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文应该被保留。"
    },
    {
        "index": "#8",
        "title": "Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning",
        "link": "/arxiv/2509.26578",
        "arxiv_id": "2509.26578",
        "authors": "Zheng Zhang, Ziwei Shan, Kaitao Song, Yexin Li, Kan Ren",
        "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning capabilities of large language models (LLMs) by guiding their step-by-step reasoning toward a final answer. However, existing PRMs either treat each reasoning step in isolation, failing to capture inter-step dependencies, or struggle to align process rewards with the final outcome. Consequently, the reward signal fails to respect temporal causality in sequential reasoning and faces ambiguous credit assignment. These limitations make downstream models vulnerable to reward hacking and lead to suboptimal performance. In this work, we propose Conditional Reward Modeling (CRM) that frames LLM reasoning as a temporal process leading to a correct answer. The reward of each reasoning step is not only conditioned on the preceding steps but also explicitly linked to the final outcome of the reasoning trajectory. By enforcing conditional probability rules, our design captures the causal relationships among reasoning steps, with the link to the outcome allowing precise attribution of each intermediate step, thereby resolving credit assignment ambiguity. Further, through this consistent probabilistic modeling, the rewards produced by CRM enable more reliable cross-sample comparison. Experiments across Best-of-N sampling, beam search and reinforcement learning demonstrate that CRM consistently outperforms existing reward models, offering a principled framework for enhancing LLM reasoning. In particular, CRM is more robust to reward hacking and delivers stable downstream improvements without relying on verifiable rewards derived from ground truth.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.852710",
        "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是关于改进LLM的基础推理能力，提出了条件奖励建模(CRM)这一新方法来增强大语言模型的推理过程。论文明确针对过程奖励模型(PRM)的局限性，即无法捕捉推理步骤间的依赖关系和难以将过程奖励与最终结果对齐的问题，这属于\"提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。 其次，论文满足多个正面指标：核心概念明确涉及Large language models (LLMs)；能力方向专注于reasoning，特别是step-by-step reasoning；训练方法方面提到了reinforcement learning作为实验评估的一部分，且CRM本身就是一种新的奖励建模方法。 第三，论文不符合任何排除标准：没有涉及多模态与视觉内容，不关注特定应用领域，也不以模型可靠性的应用层面（如水印、安全、安全性）为主要焦点。 最后，在特殊和模糊情况处理上，论文虽然提到了reward hacking问题，但这是从方法论角度提出解决方案，而不是仅仅研究这种现象本身。CRM是一种通用的训练框架，旨在提升LLM的内在推理能力，而非应用于特定领域。 综上所述，这篇论文的核心贡献是提出了一种新的奖励建模方法来增强LLM的通用推理能力，完全符合\"大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#16",
        "title": "Entropy After $\\langle \\texttt{/Think} \\rangle$ for reasoning model early exiting",
        "link": "/arxiv/2509.26522",
        "arxiv_id": "2509.26522",
        "authors": "Xi Wang, James McInerney, Lequn Wang, Nathan Kallus",
        "summary": "Large reasoning models show improved performance with longer chains of thought. However, recent work has highlighted (qualitatively) their tendency to overthink, continuing to revise answers even after reaching the correct solution. We quantitatively confirm this inefficiency by tracking Pass@1 for answers averaged over a large number of rollouts and find that the model often begins to always produce the correct answer early in the reasoning, making extra reasoning a waste of tokens. To detect and prevent overthinking, we propose a simple and inexpensive novel signal -- Entropy After </Think> (EAT) -- for monitoring and deciding whether to exit reasoning early. By appending a stop thinking token (</think>) and monitoring the entropy of the following token as the model reasons, we obtain a trajectory that decreases and stabilizes when Pass@1 plateaus; thresholding its variance under an exponential moving average yields a practical stopping rule. Importantly, our approach enables adaptively allocating compute based on the EAT trajectory, allowing us to spend compute in a more efficient way compared with fixing the token budget for all questions. Empirically, on MATH500 and AIME2025, EAT reduces token usage by 13 - 21% without harming accuracy, and it remains effective in black box settings where logits from the reasoning model are not accessible, and EAT is computed with proxy models.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.862236",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的本质是改进大型推理模型的推理效率，提出了一种名为\"Entropy After </Think> (EAT)\"的新方法来解决模型\"过度思考\"的问题。这属于提升LLM基础推理能力的范畴，而非将LLM作为工具应用到特定领域。论文关注的是如何优化推理过程，使模型能够更高效地进行推理，这直接关系到LLM的通用推理能力提升。 第二步正面指标：论文明确包含以下关键主题： - 核心概念：讨论了\"Large reasoning models\"，属于LLMs范畴 - 能力方向：直接聚焦于reasoning（推理），特别是数学推理（实验使用了MATH500和AIME2025数据集） 第三步排除标准：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不是针对特定应用领域的研究，而是提出通用推理优化方法 - 不关注模型可靠性方面的水印、安全或安全问题 第四步特殊和模糊情况：论文不涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等问题。 论文的核心贡献是提出了一种通用的推理过程优化方法，通过监控熵值来决定何时停止推理，从而提高计算效率。这种方法可以广泛应用于各种推理任务，直接提升了LLM的通用推理能力，因此完全符合研究目标。"
    },
    {
        "index": "#21",
        "title": "ACT: Agentic Classification Tree",
        "link": "/arxiv/2509.26433",
        "arxiv_id": "2509.26433",
        "authors": "Vincent Grari, Tim Arni, Thibault Laugel, Sylvain Lamprier, James Zou, Marcin Detyniecki",
        "summary": "When used in high-stakes settings, AI systems are expected to produce decisions that are transparent, interpretable, and auditable, a requirement increasingly expected by regulations. Decision trees such as CART provide clear and verifiable rules, but they are restricted to structured tabular data and cannot operate directly on unstructured inputs such as text. In practice, large language models (LLMs) are widely used for such data, yet prompting strategies such as chain-of-thought or prompt optimization still rely on free-form reasoning, limiting their ability to ensure trustworthy behaviors. We present the Agentic Classification Tree (ACT), which extends decision-tree methodology to unstructured inputs by formulating each split as a natural-language question, refined through impurity-based evaluation and LLM feedback via TextGrad. Experiments on text benchmarks show that ACT matches or surpasses prompting-based baselines while producing transparent and interpretable decision paths.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.864905",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Agentic Classification Tree (ACT)\"的新方法，它将决策树的透明性和可解释性与大语言模型处理非结构化数据的能力结合起来。论文不是将LLM应用到特定领域，而是提出了一种通用的方法来增强LLM的能力，特别是其可解释性和推理透明度。ACT通过将每个决策树分裂点表述为自然语言问题，并通过基于不纯度的评估和LLM反馈来优化这些问题，从而产生透明和可解释的决策路径。这种方法可以被视为一种新的智能体范式，旨在增强LLM的通用推理能力和可解释性，而不是将其应用到特定领域或解决特定领域的问题。论文关注的是提升LLM的内在能力（可解释性和透明决策路径），这符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。因此，这篇论文符合研究范围，应该被保留。"
    },
    {
        "index": "#27",
        "title": "Memory-Driven Self-Improvement for Decision Making with Large Language Models",
        "link": "/arxiv/2509.26340",
        "arxiv_id": "2509.26340",
        "authors": "Xue Yan, Zijing Ou, Mengyue Yang, Yan Song, Haifeng Zhang, Yingzhen Li, Jun Wang",
        "summary": "Large language models (LLMs) have emerged as effective action policies for sequential decision-making (SDM) tasks due to their extensive prior knowledge. However, this broad yet general knowledge is often insufficient for specific decision-making tasks with limited task-related data, making it challenging to efficiently adapt LLMs to specific SDM tasks. To address this challenge, we propose a memory-driven self-improvement framework that combines LLM general prior knowledge with a compact memory of domain-specific experiences. Memory retains past interactions and associated Q-values, thereby capturing decision-relevant knowledge that facilitates accurate value estimation and informs the LLM prior refinement. The refined LLM prior, in turn, generates higher-reward trajectories that further enrich memory, forming a natural self-improvement framework where memory and LLM prior mutually reinforce each other. Experiments show that our memory-driven approach significantly outperforms both traditional RL and LLM-based baselines, e.g., improving performance by over 40\\% on in-distribution tasks and over 75\\% when generalized to unseen tasks in ALFWorld.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.873203",
        "filter_reason": "这篇论文的核心贡献是提出了一种记忆驱动的自我改进框架，用于增强大语言模型在决策任务中的表现。论文本质上是关于改进LLM的基础能力，特别是其在序列决策任务中的推理和规划能力，而不是将LLM作为工具应用到特定领域。论文结合了LLM的通用先验知识和特定领域经验的记忆，通过自我改进机制（记忆和LLM先验相互强化）来提高模型的决策质量，这符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。 从正面指标来看，论文明确关注大语言模型(LLMs)核心概念，涉及决策能力(decision making)这一推理和规划的重要方向，并提出了自我改进(self-improvement)的训练方法。同时，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性等应用层面的内容。 虽然论文在ALFWorld环境中进行了实验，但其提出的方法是通用的，不是针对特定领域的应用。因此，这篇论文应该被保留，它为提高LLM的通用推理能力提供了一种新的方法论框架。"
    },
    {
        "index": "#32",
        "title": "Attribution-Guided Decoding",
        "link": "/arxiv/2509.26307",
        "arxiv_id": "2509.26307",
        "authors": "Piotr Komorowski, Elena Golimblevskaia, Reduan Achtibat, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek",
        "summary": "The capacity of Large Language Models (LLMs) to follow complex instructions and generate factually accurate text is critical for their real-world application. However, standard decoding methods often fail to robustly satisfy these requirements, while existing control techniques frequently degrade general output quality. In this work, we introduce Attribution-Guided Decoding (AGD), an interpretability-based decoding strategy. Instead of directly manipulating model activations, AGD considers a set of high-probability output token candidates and selects the one that exhibits the highest attribution to a user-defined Region of Interest (ROI). This ROI can be flexibly defined over different parts of the model's input or internal components, allowing AGD to steer generation towards various desirable behaviors. We demonstrate AGD's efficacy across three challenging domains. For instruction following, we show that AGD significantly boosts adherence (e.g., improving the overall success rate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show that guiding generation towards usage of internal knowledge components or contextual sources can reduce hallucinations and improve factual accuracy in both closed-book and open-book settings. Furthermore, we propose an adaptive, entropy-based variant of AGD that mitigates quality degradation and reduces computational overhead by applying guidance only when the model is uncertain. Our work presents a versatile, more interpretable, and effective method for enhancing the reliability of modern LLMs.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.875938",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"归因引导解码\"(AGD)的解码策略，旨在提高大语言模型(LLM)的指令遵循能力和事实准确性，同时减少幻觉。从第一步核心判断来看，论文的本质是改进LLM的基础能力，特别是其生成可靠和准确输出的能力，这与提高LLM的通用推理能力直接相关。论文没有将LLM作为工具应用于特定领域，也没有关注模型基础设施或部署优化。 在第二步正面指标方面，论文明确关注大型语言模型(LLMs)，并涉及推理能力，特别是通过减少幻觉和提高事实准确性来增强模型的推理质量。虽然论文没有提到强化学习、进化等训练方法，也没有涉及基于LLM的智能体等多智能体系统，但它在核心概念和能力方向上符合筛选标准。 在第三步排除标准方面，论文没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。它提出的是一种通用的解码策略，可以应用于各种任务，而不是专注于特定领域。 在第四步处理特殊和模糊情况方面，论文确实涉及减少幻觉和可解释性。它提出的方法(AGD)是一种基于可解释性的解码策略，旨在减少幻觉并提高事实准确性。根据筛选标准，如果论文提出一种新方法来减少幻觉、增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量，应该保留。这篇论文符合这一标准。 综上所述，这篇论文符合\"大语言模型通用推理能力\"的研究范围，因为它提出了一种通用的方法来增强LLM的基础能力，特别是其指令遵循和事实准确性，这些都是推理能力的重要组成部分。"
    },
    {
        "index": "#47",
        "title": "Alignment-Aware Decoding",
        "link": "/arxiv/2509.26169",
        "arxiv_id": "2509.26169",
        "authors": "Frédéric Berdoz, Luca A. Lanzendörfer, René Caky, Roger Wattenhofer",
        "summary": "Alignment of large language models remains a central challenge in natural language processing. Preference optimization has emerged as a popular and effective method for improving alignment, typically through training-time or prompt-based interventions. In this paper, we introduce alignment-aware decoding (AAD), a method to enhance model alignment directly at inference. Theoretically, AAD can be interpreted as implicit reward optimization, yet it requires no specialized training beyond the standard DPO setup. Empirically, AAD consistently outperforms strong baselines across diverse alignment benchmarks and model scales. Moreover, in data-constrained settings, AAD can produce high-quality synthetic data to improve alignment under standard decoding, providing a practical solution when labeled data is limited.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.894233",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"alignment-aware decoding (AAD)\"的新方法，用于在推理阶段直接增强大语言模型的对齐能力。根据筛选标准，这篇论文符合我的研究目标，原因如下：1）论文本质上是关于改进LLM的基础能力，特别是模型的对齐能力，这属于LLM的核心能力之一；2）论文关注的是大语言模型(LLMs)这一核心概念；3）论文提到了preference optimization和DPO等与强化学习相关的训练方法；4）论文没有涉及多模态、特定应用领域或模型可靠性的应用层面等排除标准。虽然论文没有直接关注推理、规划等典型的通用推理能力，但对齐能力与通用推理能力有密切关联，因为良好的对齐可以使模型更好地遵循人类意图，从而在推理任务中表现更好。因此，这篇论文符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#52",
        "title": "Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models",
        "link": "/arxiv/2509.26114",
        "arxiv_id": "2509.26114",
        "authors": "Jaesung R. Park, Junsu Kim, Gyeongman Kim, Jinyoung Jo, Sean Choi, Jaewoong Cho, Ernest K. Ryu",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently emerged as the leading approach for enhancing the reasoning capabilities of large language models (LLMs). However, RLVR is prone to entropy collapse, where the LLM quickly converges to a near-deterministic form, hindering exploration and progress during prolonged RL training. In this work, we reveal that the clipping mechanism in PPO and GRPO induces biases on entropy. Through theoretical and empirical analyses, we show that clip-low increases entropy, while clip-high decreases it. Further, under standard clipping parameters, the effect of clip-high dominates, resulting in an overall entropy reduction even when purely random rewards are provided to the RL algorithm. Our findings highlight an overlooked confounding factor in RLVR: independent of the reward signal, the clipping mechanism influences entropy, which in turn affects the reasoning behavior. Furthermore, our analysis demonstrates that clipping can be deliberately used to control entropy. Specifically, with a more aggressive clip-low value, one can increase entropy, promote exploration, and ultimately prevent entropy collapse in RLVR training.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.896935",
        "filter_reason": "这篇论文完全符合研究目标，理由如下： 首先，从核心判断来看，该论文的本质是研究如何改进大语言模型的强化学习训练过程，解决熵崩溃问题，从而提升LLM的推理能力。论文明确指出其研究对象是\"enhancing the reasoning capabilities of large language models\"，这直接符合研究目标中\"改进LLM的基础能力\"和\"增强其推理能力\"的要求。 其次，论文符合多个正面指标： - 核心概念：论文明确研究大语言模型(LLMs)的强化学习 - 能力方向：直接针对\"reasoning capabilities\"，完全符合推理能力的研究方向 - 训练方法：研究强化学习(RL)中的PPO和GRPO算法，探讨如何通过调整裁剪参数优化训练过程 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域，而是研究通用推理能力的训练方法 - 不关注模型基础设施、部署优化或硬件加速 最后，该论文不是将LLM作为工具应用到特定领域，而是直接研究如何改进LLM的训练方法。论文提出的裁剪参数调整方法是一种通用的训练技巧，可以应用于各种需要强化学习训练的LLM推理任务，从而提升模型的通用推理能力。 综上所述，这篇论文的核心贡献是揭示了强化学习中裁剪机制对熵的影响，并提出如何利用这一发现来防止熵崩溃，促进探索，最终提升LLM的推理能力。这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#70",
        "title": "RL-Guided Data Selection for Language Model Finetuning",
        "link": "/arxiv/2509.25850",
        "arxiv_id": "2509.25850",
        "authors": "Animesh Jha, Harshit Gupta, Ananjan Nandi",
        "summary": "Data selection for finetuning Large Language Models (LLMs) can be framed as a budget-constrained optimization problem: maximizing a model's downstream performance under a strict training data budget. Solving this problem is generally intractable, and existing approximate approaches are pretraining-oriented and transfer poorly to the fine-tuning setting. We reformulate this problem as a tractable Markov Decision Process (MDP) and train agents using various Reinforcement Learning (RL) methods to learn optimal data selection policies, guided by an efficient, proxy-model-based reward signal. Across four datasets, training on a $5\\%$ subset selected by our approach matches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy points, while cutting wall-clock training time by up to $2 \\times$, highlighting the promise of RL-guided data selection.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.917009",
        "filter_reason": "这篇论文符合我的研究目标，理由如下： 首先，从核心判断来看，该论文的本质是关于改进LLM的基础能力，提出了一种新的训练范式。具体来说，论文研究如何通过强化学习(RL)指导的数据选择方法来优化语言模型的微调过程，这属于改进LLM基础能力的范畴，而非将LLM作为工具应用到特定领域。 其次，论文符合多个正面指标： - 核心概念：明确关注大型语言模型(LLMs)的微调 - 训练方法：使用强化学习(RL)方法训练智能体来学习最优数据选择策略 - 虽然论文没有直接提到推理能力，但通过优化数据选择来提高模型整体性能，可能间接提升模型的通用推理能力 第三，论文不符合任何排除标准。它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，论文的核心贡献是提出了一种基于强化学习的数据选择框架，用于高效微调大型语言模型。这种方法通过在有限训练数据预算下最大化模型性能，直接提升了LLM的基础能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标是一致的。虽然论文没有直接针对推理能力进行优化，但其提出的方法论可以作为一种提升LLM整体性能（包括推理能力）的有效手段。"
    },
    {
        "index": "#78",
        "title": "Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse",
        "link": "/arxiv/2509.25808",
        "arxiv_id": "2509.25808",
        "authors": "Yuheng Zhang, Wenlin Yao, Changlong Yu, Yao Liu, Qingyu Yin, Bing Yin, Hyokun Yun, Lihong Li",
        "summary": "Large language models (LLMs) have achieved impressive reasoning performance, with reinforcement learning with verifiable rewards (RLVR) emerging as a standard paradigm for post-training. A representative algorithm, group relative policy optimization (GRPO) (Shao et al., 2024), computes advantages by normalizing outcome rewards within response groups, but suffers from a vanishing advantage issue when all responses in a group receive identical rewards. To address this issue, we propose Adaptive Rollout and Response Reuse Policy Optimization (AR3PO), a sampling efficient RLVR algorithm that introduces two novel techniques: adaptive rollout, which dynamically allocates more responses to difficult prompts while saving computation on easier ones, and response reuse, which leverages previously generated correct responses to provide useful training signals. We compare AR3PO with strong RLVR baselines on multiple representative benchmarks using two different families of base models. Across the 7B and 8B models, AR3PO consistently outperforms GRPO and matches or surpasses DAPO (Yu et al., 2025), reducing rollout cost by up to 4.2x. On the larger 32B model, AR3PO achieves comparable performance to DAPO at similar training steps while maintaining substantially lower rollout cost.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.926629",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为AR3PO的新算法，用于改进大语言模型(LLM)的强化学习训练过程，特别是针对可验证奖励的强化学习(RLVR)范式。论文专注于提高LLM的推理性能和训练效率，而不是将LLM应用于特定领域。具体来说，AR3PO通过自适应展开(根据提示难度动态分配响应数量)和响应重用(利用之前生成的正确响应)两种技术，解决了现有方法(如GRPO)中的优势消失问题，并显著降低了计算成本。这属于改进LLM基础能力和训练范式的研究，直接符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文涉及的核心概念(LLMs)、能力方向(reasoning)和训练方法(reinforcement learning)都是正面指标，且不涉及任何排除标准中的领域。因此，这篇论文完全符合研究范围。"
    },
    {
        "index": "#142",
        "title": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning",
        "link": "/arxiv/2509.25300",
        "arxiv_id": "2509.25300",
        "authors": "Zelin Tan, Hejia Geng, Mulei Zhang, Xiaohang Yu, Guancheng Wan, Yifan Zhou, Qiang He, Xiangyuan Xue, Heng Zhou, Yutao Fan, Zhongzhi Li, Zaibin Zhang, Guibin Zhang, Chen Zhang, Zhenfei Yin, Lei Bai",
        "summary": "While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on 54 experiments across diverse model sizes and training settings, we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: (1). Under a fixed computational budget, larger models trained for fewer steps consistently outperform smaller models trained for more steps. (2). Given a fixed amount of training data, larger models achieve superior sample efficiency, yielding lower loss. (3). In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. (4). These scaling behaviors are robust across both base and instruction-tuned models, which share similar learning dynamics (e.g., larger models show faster convergence) even while differing in absolute accuracy. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.997516",
        "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是研究如何通过强化学习(RL)后训练来提升大语言模型的推理能力，而不是将LLM作为工具应用到特定领域。论文虽然以数学推理为案例研究，但其目标是探索RL后训练的扩展行为规律，以提升LLM的通用推理能力，这属于改进LLM基础能力和提出新训练范式的研究。 其次，论文包含多个正面指标：明确涉及\"Large language models, LLMs\"核心概念；专注于\"mathematical reasoning\"这一推理能力方向；以\"reinforcement learning (RL)\"作为核心训练方法进行研究。 第三，论文不涉及任何排除标准中的领域：不关注多模态与视觉问题，不将LLM应用于特定领域解决问题，也不讨论模型基础设施或部署优化。 特别值得注意的是，论文的核心贡献是系统性地研究了RL后训练的扩展行为，提供了\"通过RL后训练有效扩展LLM推理能力的理论基础和实践指南\"，这与我寻找的\"致力于提高大语言模型本身的通用推理能力\"的研究完全吻合。因此，这篇论文应该被保留。"
    },
    {
        "index": "#154",
        "title": "PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases",
        "link": "/arxiv/2509.25238",
        "arxiv_id": "2509.25238",
        "authors": "Sri Vatsa Vuddanti, Aarav Shah, Satwik Kumar Chittiprolu, Tony Song, Sunishchal Dev, Kevin Zhu, Maheep Chaudhary",
        "summary": "Tool-augmented language agents frequently fail in real-world deployment due to tool malfunctions--timeouts, API exceptions, or inconsistent outputs--triggering cascading reasoning errors and task abandonment. Existing agent training pipelines optimize only for success trajectories, failing to expose models to the tool failures that dominate real-world usage. We propose \\textbf{PALADIN}, a generalizable framework for equipping language agents with robust failure recovery capabilities. PALADIN trains on 50,000+ recovery-annotated trajectories constructed via systematic failure injection and expert demonstrations on an enhanced ToolBench dataset. Training uses LoRA-based fine-tuning to retain base capabilities while injecting recovery competence. At inference, PALADIN detects execution-time errors and retrieves the most similar case from a curated bank of 55+ failure exemplars aligned with ToolScan's taxonomy, then executes the corresponding recovery action. This approach generalizes to novel failures beyond the training distribution, retaining 95.2\\% recovery performance on unseen tool APIs. Evaluation across PaladinEval and ToolReflectEval demonstrates consistent improvements in Recovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR), and Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57% relative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%) by +13.3%. Against vanilla agents, PALADIN achieves 89.86\\% RR (+66% relative improvement from 23.75%). These results establish PALADIN as an effective method for building fault-tolerant agents capable of robust recovery in real-world tool environments.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.009590",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。根据筛选标准分析如下： 第一步：核心判断上，论文本质是提出PALADIN框架，用于增强语言智能体的工具故障恢复能力。这属于改进LLM通用能力的研究，特别是智能体协作框架和工具使用的方法论研究，而非将LLM作为工具应用到特定领域。论文关注的是如何让智能体在工具出现故障时进行自我纠正和恢复，这直接提升了LLM的问题解决能力和鲁棒性。 第二步：正面指标方面，论文明确涉及多个相关主题： - 核心概念：讨论\"Tool-augmented language agents\"，基于大语言模型 - 能力方向：涉及\"reasoning errors\"和问题解决能力，特别是工具使用中的错误恢复 - 新兴范式：完全符合\"llm-based agents\"和\"tool use\"范式，提出自我纠正的智能体框架 第三步：排除标准方面，论文不涉及任何排除领域： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学等），而是关注通用工具使用环境 - 不主要讨论模型可靠性的应用层面问题（如水印、安全） 第四步：特殊和模糊情况处理： - 论文提出的是通用的智能体框架来增强LLM的通用问题解决能力，而非将智能体应用在特定领域，因此应该保留 - 论文虽涉及可靠性，但是从提升模型内在能力和推理质量的角度，而非应用层面的讨论 综上所述，这篇论文的核心贡献是提出了一种增强LLM智能体工具使用鲁棒性的通用框架，通过自我纠正机制提升模型在工具故障情况下的推理和问题解决能力，完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#194",
        "title": "Pretrain-Test Task Alignment Governs Generalization in In-Context Learning",
        "link": "/arxiv/2509.26551",
        "arxiv_id": "2509.26551",
        "authors": "Mary I. Letey, Jacob A. Zavatone-Veth, Yue M. Lu, Cengiz Pehlevan",
        "summary": "In-context learning (ICL) is a central capability of Transformer models, but the structures in data that enable its emergence and govern its robustness remain poorly understood. In this work, we study how the structure of pretraining tasks governs generalization in ICL. Using a solvable model for ICL of linear regression by linear attention, we derive an exact expression for ICL generalization error in high dimensions under arbitrary pretraining-testing task covariance mismatch. This leads to a new alignment measure that quantifies how much information about the pretraining task distribution is useful for inference at test time. We show that this measure directly predicts ICL performance not only in the solvable model but also in nonlinear Transformers. Our analysis further reveals a tradeoff between specialization and generalization in ICL: depending on task distribution alignment, increasing pretraining task diversity can either improve or harm test performance. Together, these results identify train-test task alignment as a key determinant of generalization in ICL.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.069052",
        "filter_reason": "这篇论文符合我的研究目标，因为它聚焦于大语言模型(LLM)的基础能力机制研究，而非特定领域应用。论文的核心贡献是研究Transformer模型的上下文学习(ICL)能力，这是大语言模型进行通用推理的关键机制之一。具体来说，论文分析了预训练任务结构如何影响ICL的泛化能力，提出了一个新的对齐度量来量化预训练任务分布对测试时推理的有用性，并揭示了ICL中专业化和泛化之间的权衡关系。这些研究直接关系到理解和提升LLM的通用推理能力，符合筛选标准中的\"改进LLM的基础能力\"类别。论文不涉及多模态、特定应用领域或模型基础设施等排除内容，而是纯粹关注LLM本身的推理机制，因此应该被保留。"
    },
    {
        "index": "#199",
        "title": "The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain",
        "link": "/arxiv/2509.26507",
        "arxiv_id": "2509.26507",
        "authors": "Adrian Kosowski, Przemysław Uznański, Jan Chorowski, Zuzanna Stamirowska, Michał Bartoszkiewicz",
        "summary": "The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models. We introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of \\$n\\$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance. BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data. BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech. BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.",
        "subjects": "Neural and Evolutionary Computing, Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.072216",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Dragon Hatchling\"(BDH)的新型大语言模型架构，该架构基于无标度生物启发网络。从本质上看，论文专注于改进LLM的基础架构能力，提出了新的模型架构范式，而不是将LLM作为工具应用到特定领域。BDH在保持类似Transformer性能的同时，增强了模型的生物合理性和可解释性。论文明确提到了\"通用推理模型\"(Universal Reasoning Models)，表明其研究与推理能力直接相关。虽然论文借鉴了生物大脑的原理，但这是从架构设计角度出发，目的是提升LLM的通用能力，而非将其应用于生物学或医学领域。此外，论文强调了BDH的可解释性设计，包括稀疏和正的激活向量，以及在语言任务上展示的单语义性，这些特性有助于提升模型的内在推理质量和可靠性。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。"
    },
    {
        "index": "#251",
        "title": "Test time training enhances in-context learning of nonlinear functions",
        "link": "/arxiv/2509.25741",
        "arxiv_id": "2509.25741",
        "authors": "Kento Kuwataka, Taiji Suzuki",
        "summary": "Test-time training (TTT) enhances model performance by explicitly updating designated parameters prior to each prediction to adapt to the test data. While TTT has demonstrated considerable empirical success, its theoretical underpinnings remain limited, particularly for nonlinear models. In this paper, we investigate the combination of TTT with in-context learning (ICL), where the model is given a few examples from the target distribution at inference time. We analyze this framework in the setting of single-index models $y=\\sigma_*(\\langle \\beta, \\mathbf{x} \\rangle)$, where the feature vector $\\beta$ is drawn from a hidden low-dimensional subspace. For single-layer transformers trained with gradient-based algorithms and adopting TTT, we establish an upper bound on the prediction risk. Our theory reveals that TTT enables the single-layer transformers to adapt to both the feature vector $\\beta$ and the link function $\\sigma_*$, which vary across tasks. This creates a sharp contrast with ICL alone, which is theoretically difficult to adapt to shifts in the link function. Moreover, we provide the convergence rate with respect to the data length, showing the predictive error can be driven arbitrarily close to the noise level as the context size and the network width grow.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.106695",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础能力和提出新的训练范式。论文研究了测试时训练(TTT)与上下文学习(ICL)的结合，这是一种增强模型适应能力和泛化能力的新方法。具体来说，论文分析了单层transformer如何通过TTT适应不同的特征向量和链接函数，这直接关系到模型的问题解决能力，属于增强LLM通用推理能力的研究。 其次，从正面指标来看，虽然论文没有明确提到\"large language models\"或\"LLMs\"这些关键词，但它研究的transformer架构是LLM的基础，并且论文关注的是模型的问题解决能力，这与推理能力密切相关。 第三，从排除标准来看，论文没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究，而是专注于模型本身的能力提升。 最后，论文没有涉及到特殊或模糊的情况，如智能体/工具使用或幻觉/可解释性/安全等问题。 综上所述，这篇论文的核心贡献是提出了一种通过测试时训练增强模型上下文学习能力的新方法，从而提高了模型对非线性函数的学习和适应能力，这直接关系到LLM的通用推理能力，因此符合研究范围。"
    },
    {
        "index": "#266",
        "title": "Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks",
        "link": "/arxiv/2509.25598",
        "arxiv_id": "2509.25598",
        "authors": "Peiran Xu, Zhuohao Li, Xiaoying Xing, Guannan Zhang, Debiao Li, Kunyu Shi",
        "summary": "Large Language Models (LLMs) increasingly rely on external tools such as search engines to solve complex agentic tasks that require reasoning and external knowledge retrieval. Recently, reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in advancing capabilities of LLMs by rewarding the final answers via outcome rewards. While straightforward to supervise, outcome rewards only provide sparse signals and delayed feedback, which limits their effectiveness on long trajectories. Process rewards address this by evaluating intermediate steps, providing fine-grained supervision and encouraging grounded problem solving. However, it is notoriously hard to annotate step-wise labels, especially in non-verifiable process without \"golden\" answers. Furthermore, step-wise judgment requires the balance between local quality with contribution to the final outcome, as optimizing towards higher process reward may not always align with better final outcomes. To address the above challenges, we introduce Principle Process Reward (PPR), an RL approach that unifies principled step-level assessment and outcome verification. We train a principle-based reward model to improve the transparency and reliability of process evaluation, and further introduce a Reward Normalization (ReNorm) strategy to calibrate outcome and process rewards. Experiment results show that PPR achieves state-of-the-art performance across a wide range of benchmarks, demonstrating its impressive robustness and generalization. Our code and model collection is available in this link.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.119931",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM基础推理能力的方法论研究。论文提出了一种名为\"原则过程奖励\"(PPR)的强化学习方法，旨在提升LLM在复杂智能体任务中的推理能力。这明显属于改进LLM通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 其次，论文包含了多个正面指标： - 核心概念：明确提到了\"Large Language Models (LLMs)\" - 能力方向：涉及\"reasoning\"和\"problem-solving\"，特别是针对需要推理的复杂智能体任务 - 训练方法：使用了\"reinforcement learning with verifiable rewards (RLVR)\" - 新兴范式：讨论了\"agentic tasks\"和\"external tools\"，属于基于LLM的智能体和工具使用研究 第三，论文不涉及任何排除标准中的领域： - 没有涉及多模态与视觉内容 - 没有针对特定应用领域（如医疗、化学等） - 没有主要关注模型可靠性层面的水印、安全等问题 在特殊和模糊情况处理上，论文虽然涉及智能体和工具使用，但这是从通用角度提出的框架和方法，用于增强LLM的通用问题解决能力，而非针对特定领域的应用。 论文的核心贡献是提出了一种统一原则性步骤级评估和结果验证的强化学习方法，通过训练基于原则的奖励模型和引入奖励归一化策略，解决了非可验证过程中步骤评估的挑战，从而提升了LLM的推理能力和问题解决能力。这完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#289",
        "title": "RADAR: Reasoning-Ability and Difficulty-Aware Routing for Reasoning LLMs",
        "link": "/arxiv/2509.25426",
        "arxiv_id": "2509.25426",
        "authors": "Nigel Fernandez, Branislav Kveton, Ryan A. Rossi, Andrew S. Lan, Zichao Wang",
        "summary": "Reasoning language models have demonstrated remarkable performance on many challenging tasks in math, science, and coding. Choosing the right reasoning model for practical deployment involves a performance and cost tradeoff at two key levels: model size and reasoning budget, where larger models and higher reasoning budget lead to better performance but with increased cost and latency. In this work, we tackle this tradeoff from the angle of model configuration routing for different queries, and present RADAR (Reasoning-Ability and Difficulty-Aware Routing), a lightweight, interpretable, and scalable routing framework. Inspired by psychometrics, RADAR learns an item response model from model responses with different budgets to different queries, with interpretable parameters including query difficulties and model-budget abilities. RADAR then routes queries with higher difficulty to model-budget pairs with higher ability, and vice versa. We conduct extensive experiments on 8 widely used challenging reasoning benchmarks, demonstrating the superior performance of RADAR compared to state-of-the-art model routing methods. RADAR also exhibits query generalization capabilities, showing strong performance on out-of-distribution queries in all benchmarks. RADAR is also scalable and can efficiently integrate additional models by dynamically selecting a small set of evaluation queries to estimate their abilities.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.148102",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出一种名为RADAR的路由框架，用于优化推理语言模型的使用效率。论文核心关注的是如何根据查询难度和模型能力进行智能路由，以平衡性能和成本。这属于改进LLM推理能力的通用方法论研究，而不是将LLM作为工具应用到特定领域。论文提出的方法旨在增强LLM的推理能力使用效率，符合\"改进LLM的基础能力\"的要求。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确关注\"reasoning language models\"，属于LLMs范畴 - 能力方向：直接针对\"reasoning\"能力，并在数学、科学和编程等推理任务上评估 - 论文在8个具有挑战性的推理基准测试上进行了实验，证明其方法的有效性 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域（如医疗、化学等） - 不主要关注模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况 论文没有涉及需要特殊处理的情况。虽然提到RADAR是\"可解释的\"，但这只是其路由框架的一个特性，而非论文主要焦点。 核心贡献： 论文提出了一种轻量级、可解释且可扩展的路由框架RADAR，通过学习查询难度和模型-预算能力，智能地将查询路由到最适合的模型-预算对。这种方法提高了推理语言模型的使用效率，在保持性能的同时优化了成本和延迟，属于增强LLM通用推理能力的方法论研究。 因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#309",
        "title": "Protocode: Prototype-Driven Interpretability for Code Generation in LLMs",
        "link": "/arxiv/2509.25247",
        "arxiv_id": "2509.25247",
        "authors": "Krishna Vamshi Bodla, Haizhao Yang",
        "summary": "Since the introduction of Large Language Models (LLMs), they have been widely adopted for various tasks such as text summarization, question answering, speech-to-text translation, and more. In recent times, the use of LLMs for code generation has gained significant attention, with tools such as Cursor and Windsurf demonstrating the ability to analyze massive code repositories and recommend relevant changes. Big tech companies have also acknowledged the growing reliance on LLMs for code generation within their codebases. Although these advances significantly improve developer productivity, increasing reliance on automated code generation can proportionally increase the risk of suboptimal solutions and insecure code. Our work focuses on automatically sampling In-Context Learning (ICL) demonstrations which can improve model performance and enhance the interpretability of the generated code. Using AST-based analysis on outputs from the MBPP test set, we identify regions of code most influenced by the chosen demonstrations. In our experiments, we show that high-quality ICL demonstrations not only make outputs easier to interpret but also yield a positive performance improvement on the pass@10 metric. Conversely, poorly chosen ICL demonstrations affected the LLM performance on the pass@10 metric negatively compared to the base model. Overall, our approach highlights the importance of efficient sampling strategies for ICL, which can affect the performance of the model on any given task.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning",
        "date": "2025-09-27",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.169681",
        "filter_reason": "这篇论文的核心贡献是提出了一种通过自动采样In-Context Learning (ICL)示例来提高LLM在代码生成任务上的性能和可解释性的方法。从筛选标准来看： 首先，论文本质上是关于改进LLM的基础能力的，特别是通过优化ICL示例采样策略来提升代码生成性能和可解释性。代码生成可以被视为LLM的一种通用推理能力，类似于数学推理或逻辑推理，而不是将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标：明确关注Large Language Models (LLMs)，研究代码生成这种问题解决能力，并探讨了可解释性这一与模型推理质量相关的主题。 在排除标准方面，论文并不主要聚焦于多模态、特定应用领域或模型可靠性的应用层面。虽然论文提到了代码安全性问题，但这只是背景，而非研究焦点。 特别地，论文对可解释性的研究符合第四步中的特殊情况处理标准——提出新方法来增强模型内在的可解释性，从而提升模型的推理质量，这应该被保留。 综上所述，这篇论文致力于提高LLM在代码生成这一通用任务上的表现，属于改进LLM基础能力的研究，符合\"提高大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#24",
        "title": "Interactive Learning for LLM Reasoning",
        "link": "/arxiv/2509.26306",
        "arxiv_id": "2509.26306",
        "authors": "Hehai Lin, Shilei Cao, Minzhi Li, Sudong Wang, Haotian Wu, Linyi Yang, Juepeng Zheng, Chengwei Qin",
        "summary": "Existing multi-agent learning approaches have developed interactive training environments to explicitly promote collaboration among multiple Large Language Models (LLMs), thereby constructing stronger multi-agent systems (MAS). However, during inference, they require re-executing the MAS to obtain final solutions, which diverges from human cognition that individuals can enhance their reasoning capabilities through interactions with others and resolve questions independently in the future. To investigate whether multi-agent interaction can enhance LLMs' independent problem-solving ability, we introduce ILR, a novel co-learning framework for MAS that integrates two key components: Dynamic Interaction and Perception Calibration. Specifically, Dynamic Interaction first adaptively selects either cooperative or competitive strategies depending on question difficulty and model ability. LLMs then exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea Fusion), an innovative interaction paradigm designed to mimic human discussion, before deriving their respective final answers. In Perception Calibration, ILR employs Group Relative Policy Optimization (GRPO) to train LLMs while integrating one LLM's reward distribution characteristics into another's reward function, thereby enhancing the cohesion of multi-agent interactions. We validate ILR on three LLMs across two model families of varying scales, evaluating performance on five mathematical benchmarks and one coding benchmark. Experimental results show that ILR consistently outperforms single-agent learning, yielding an improvement of up to 5% over the strongest baseline. We further discover that Idea3 can enhance the robustness of stronger LLMs during multi-agent inference, and dynamic interaction types can boost multi-agent learning compared to pure cooperative or competitive strategies.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T22:50:20.125122",
        "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出了一种新的交互学习框架(ILR)来增强大语言模型的通用推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，提出了一种新的训练范式(多智能体交互学习)，特别是增强其数学推理和独立问题解决能力，这明确符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。 从第二步正面指标看，论文包含了多个相关主题：明确以大语言模型(LLMs)为核心研究对象；聚焦于推理能力(reasoning)和问题解决能力(problem-solving)；采用了强化学习方法(Group Relative Policy Optimization)；并且涉及多智能体系统(multi-agent systems)这一新兴范式。 第三步排除标准方面，论文未涉及多模态与视觉、特定应用领域或模型可靠性等应排除的内容。 在第四步特殊情况处理中，论文提出的是一种通用的多智能体协作框架来增强LLM的通用推理能力，而非应用于特定领域，符合保留条件。 综上所述，这篇论文完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标，应被保留。"
    },
    {
        "index": "#28",
        "title": "Diversity-Incentivized Exploration for Versatile Reasoning",
        "link": "/arxiv/2509.26209",
        "arxiv_id": "2509.26209",
        "authors": "Zican Hu, Shilin Zhang, Yafu Li, Jianhao Yan, Xuyang Hu, Leyang Cui, Xiaoye Qu, Chunlin Chen, Yu Cheng, Zhi Wang",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a crucial paradigm for incentivizing reasoning capabilities in Large Language Models (LLMs). Due to vast state-action spaces and reward sparsity in reasoning tasks, existing methods often struggle with deficient exploration and poor sample efficiency. In the paper, we propose \\textbf{DIVER} (\\textbf{D}iversity-\\textbf{I}ncentivized Exploration for \\textbf{V}ersatil\\textbf{E} \\textbf{R}easoning), an innovative framework that highlights the pivotal role of global sequence-level diversity to incentivize deep exploration for versatile reasoning. We first conduct a primary empirical study to reveal a strong positive correlation between global diversity and reasoning capacity. Building on this insight, we introduce global diversity incentives as an intrinsic reward to promote deep exploration in a semantically structured space. Incorporating the intrinsic reward, we develop a potential-based reward shaping mechanism to preserve optimal policy invariance and design simple heuristics to mitigate possible reward hacking. Experimental results show that DIVER outperforms competitive RLVR baselines with various exploration strategies on both in-domain and out-of-domain tasks, excelling in both Pass@1 and Pass@k evaluations. Our code is available at https://github.com/NJU-RL/DIVER.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T22:50:20.132520",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是关于改进LLM的基础推理能力，提出了DIVER框架来增强LLM的通用推理能力，属于\"提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。从正面指标看，论文明确包含了核心概念\"Large Language Models (LLMs)\"、能力方向\"reasoning\"（多次提到推理能力和通用推理）以及训练方法\"reinforcement learning\"（提出了基于强化学习的RLVR范式）。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文的核心贡献是通过引入全局多样性激励作为内在奖励，促进在语义结构化空间中的深度探索，从而提升LLM的通用推理能力，这与研究目标高度一致。"
    },
    {
        "index": "#55",
        "title": "Chain-in-Tree: Back to Sequential Reasoning in LLM Tree Search",
        "link": "/arxiv/2509.25835",
        "arxiv_id": "2509.25835",
        "authors": "Xinzhe Li",
        "summary": "Test-time scaling enables large language models (LLMs) to improve performance on long-horizon reasoning tasks by allocating additional compute at inference. Tree-search-based approaches achieve state-of-the-art results in this setting, but they are notoriously inefficient, often an order of magnitude slower than simpler iterative methods. We introduce Chain-in-Tree (CiT), a plug-in framework that adaptively decides when to branch during search rather than branching at every step. CiT relies on lightweight Branching Necessity (BN) evaluation methods: BN-DP (Direct Prompting), where an auxiliary LLM directly judges whether a step requires branching, and BN-SC (Self-Consistency), which clusters multiple candidate actions to estimate agreement. We integrate CiT into three representative LLM-in-the-loop tree search frameworks: Tree of Thoughts (ToT-BS), ReST-MCTS, and RAP, and evaluate across GSM8K and Math500. Our results show that: (1) BN-DP consistently reduces token generation, model invocations, and runtime by 75-85 percent across all settings, with negligible accuracy loss and sometimes accuracy gains; (2) BN-SC typically yields substantial savings (up to 80 percent) but shows instability in 1-4 out of 14 settings, caused by a small subset of examples that produce very long reasoning steps; (3) the quality of auxiliary LLMs is critical, not only the BN evaluator in BN-DP, but also the models used in BN-SC for clustering and equivalence checking. When these roles are filled by smaller LLMs, performance degrades. Importantly, BN-SC does not require LLMs in domains with deterministic action spaces, where clustering can be done programmatically. We also provide a theoretical guarantee that BN-DP never increases LLM invocations relative to the baseline and release a unified implementation of CiT across ToT-BS, ReST-MCTS, and RAP to facilitate reproducibility and extension.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T22:50:20.162566",
        "filter_reason": "这篇论文的核心是关于改进大语言模型(LLM)的推理能力，特别是通过优化树搜索方法来提高LLM在长时程推理任务上的性能。论文提出的\"Chain-in-Tree\"(CiT)框架是一种新的推理方法论，它能够自适应地决定在搜索过程中何时进行分支，而不是在每个步骤都进行分支，从而显著提高推理效率。从筛选标准来看：1) 论文本质上是改进LLM的基础推理能力，属于通用推理能力提升的研究，符合第一步的核心判断；2) 论文包含多个正面指标，核心概念涉及LLMs，能力方向聚焦于reasoning(特别是math reasoning)，这正是研究目标关注的重点；3) 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面；4) 论文提出的框架是一种通用推理方法，而非针对特定领域的应用。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围，应当保留。"
    },
    {
        "index": "#58",
        "title": "Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs",
        "link": "/arxiv/2509.25779",
        "arxiv_id": "2509.25779",
        "authors": "Siyu Zhu, Yanbin Jiang, Hejian Sang, Shao Tang, Qingquan Song, Biao He, Rohit Jain, Zhipeng Wang, Alborz Geramifard",
        "summary": "We investigated Agentic RL with large language models on the \\textsc{TravelPlanner} benchmark. Our approach, \\textsc{Planner-R1}, achieved a \\textbf{56.9\\%} final-pass rate with only 180 training queries, a $2.7\\times$ improvement over GPT-5's $21.2\\%$ baseline and the strongest agentic result on the public leaderboard. A central finding was that smaller models (8B) were highly responsive to reward shaping: with dense process-level signals, they reached competitive performance while being $3.5\\times$ more compute-efficient and $1.5\\times$ more memory-efficient than 32B models. Larger models were more robust under sparse rewards but exhibited smaller relative gains from shaping and higher variance across runs. While curriculum learning offered no significant benefit, shaped rewards consistently amplified learning dynamics, making 8B models the most efficient setting for agentic RL. Crucially, these gains did not come at the cost of overfitting: fine-tuned models mostly maintained or exceeded baseline performance on out-of-domain tasks, including \\textsc{Multi-IF}, \\textsc{NaturalPlan}, and $\\tau$-\\textsc{Bench}. These results establish reward shaping as a decisive lever for scaling agentic RL, highlight the competitive strength of smaller models, and demonstrate that efficiency can be achieved without sacrificing generalization.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T22:50:20.164119",
        "filter_reason": "这篇论文完全符合我的研究目标，核心贡献是提升大语言模型的通用推理能力。从第一步核心判断来看，论文的本质是研究如何通过奖励塑形(reward shaping)方法来增强基于LLM的智能体在强化学习中的推理和规划能力，属于改进LLM基础能力的研究，而非将LLM作为工具应用于特定领域。 在第二步正面指标方面，论文完全符合所有关键主题：1)核心概念上明确研究LLMs，比较了8B和32B不同规模的模型；2)能力方向上专注于planning(规划)能力，在TravelPlanner基准上测试；3)训练方法上研究Agentic RL(智能体强化学习)和奖励塑形技术；4)新兴范式上探讨基于LLM的智能体(llm-based agents)。 第三步排除标准方面，论文不涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的研究。虽然使用了TravelPlanner基准，但这只是作为评估通用规划能力的测试平台，而非针对特定领域应用。 在第四步特殊情况下，论文提出的是通用的奖励塑形方法来增强LLM的智能体推理能力，而非将智能体应用于特定领域，因此应该保留。 综上所述，这篇论文通过奖励塑形技术提升了LLM的通用推理和规划能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#60",
        "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training",
        "link": "/arxiv/2509.25758",
        "arxiv_id": "2509.25758",
        "authors": "Yein Park, Minbyul Jeong, Jaewoo Kang",
        "summary": "The remarkable capabilities of modern large reasoning models are largely unlocked through post-training techniques such as supervised fine-tuning and reinforcement learning. However, the architectural mechanisms behind such improvements remain largely opaque. In this work, we use circuit analysis to demonstrate that post-training for complex reasoning sparks the emergence of novel, functionally specialized attention heads. These heads collectively support structured reasoning and computation. Our comparative analysis across Qwen families and DeepSeek-distilled model reveals that these emergent heads evolve differently under different training regimes. Distillation and SFT foster a cumulative addition of stable reasoning heads. In contrast, group relative policy optimization operates in a dynamic search mode: relatively few attention heads are iteratively activated, evaluated, and pruned, with their survival closely tracking fluctuations in the task reward signal. Furthermore, we find that controllable think on/off models do not possess dedicated thinking heads. Instead, turning off explicit reasoning triggers a broader-but less efficient-set of compensatory heads. Through ablation and qualitative analyses, we connect these circuit-level dynamics to a crucial performance trade-off: strengthened heads enable sophisticated problem-solving strategies for difficult problems but can also introduce over-thinking failure modes, such as calculation errors or logical loops on simpler tasks. These findings connect circuit-level dynamics to macro-level performance, identifying an inherent tension where complex reasoning comes at the cost of elementary computations. More broadly, our work points to future directions for training policy design, emphasizing the need to balance the development of effective reasoning strategies with the assurance of reliable, flawless execution.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T22:50:20.165112",
        "filter_reason": "这篇论文完全符合研究范围。首先，论文的核心是研究大语言模型在后期训练过程中推理机制的内部变化，特别是通过电路分析揭示了监督微调和强化学习等后期训练技术如何促使模型发展出新的、功能专门化的注意力头，这些注意力头共同支持结构化推理和计算。这直接属于\"改进LLM的基础能力\"和\"增强其逻辑、多步推理等通用能力\"的范畴。其次，论文包含多个关键正面指标：明确研究大型推理模型（Large reasoning models），聚焦于推理能力（reasoning），并探讨了强化学习方法（group relative policy optimization）。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。相反，它通过深入分析模型内部机制来提升对LLM推理能力的理解，这有助于未来设计更好的训练策略来平衡复杂推理能力和基本计算可靠性，从而提升LLM的通用推理能力。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#86",
        "title": "Beyond Static Retrieval: Opportunities and Pitfalls of Iterative Retrieval in GraphRAG",
        "link": "/arxiv/2509.25530",
        "arxiv_id": "2509.25530",
        "authors": "Kai Guo, Xinnan Dai, Shenglai Zeng, Harry Shomer, Haoyu Han, Yu Wang, Jiliang Tang",
        "summary": "Retrieval-augmented generation (RAG) is a powerful paradigm for improving large language models (LLMs) on knowledge-intensive question answering. Graph-based RAG (GraphRAG) leverages entity-relation graphs to support multi-hop reasoning, but most systems still rely on static retrieval. When crucial evidence, especially bridge documents that connect disjoint entities, is absent, reasoning collapses and hallucinations persist. Iterative retrieval, which performs multiple rounds of evidence selection, has emerged as a promising alternative, yet its role within GraphRAG remains poorly understood. We present the first systematic study of iterative retrieval in GraphRAG, analyzing how different strategies interact with graph-based backbones and under what conditions they succeed or fail. Our findings reveal clear opportunities: iteration improves complex multi-hop questions, helps promote bridge documents into leading ranks, and different strategies offer complementary strengths. At the same time, pitfalls remain: naive expansion often introduces noise that reduces precision, gains are limited on single-hop or simple comparison questions, and several bridge evidences still be buried too deep to be effectively used. Together, these results highlight a central bottleneck, namely that GraphRAG's effectiveness depends not only on recall but also on whether bridge evidence is consistently promoted into leading positions where it can support reasoning chains. To address this challenge, we propose Bridge-Guided Dual-Thought-based Retrieval (BDTR), a simple yet effective framework that generates complementary thoughts and leverages reasoning chains to recalibrate rankings and bring bridge evidence into leading positions. BDTR achieves consistent improvements across diverse GraphRAG settings and provides guidance for the design of future GraphRAG systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T22:50:20.183635",
        "filter_reason": "这篇论文符合我的研究目标，因为它核心关注的是提高大语言模型的通用推理能力。具体分析如下： 从第一步核心判断来看，论文的本质是改进LLM在知识密集型问答中的多跳推理能力。论文提出了BDTR框架，通过生成互补思维和利用推理链来重新校准检索结果排名，这是一种直接增强LLM推理能力的新方法，属于改进LLM基础能力的范畴。 从第二步正面指标来看，论文明确包含以下相关主题： - 核心概念：明确提到\"large language models (LLMs)\" - 能力方向：重点研究\"multi-hop reasoning\"，这正是推理能力的重要组成部分 - 训练方法：提出了BDTR框架作为新的检索增强方法，可视为改进LLM推理能力的方法论 从第三步排除标准来看，论文不涉及任何应排除的领域： - 不涉及多模态与视觉内容 - 不是针对特定应用领域（如医疗、化学等），而是关注通用知识密集型问答 - 虽提到减少幻觉，但这是作为改进推理能力的结果，而非主要研究焦点 从第四步特殊情况处理来看，论文在减少幻觉方面符合保留条件，因为它提出的方法是通过改进检索和推理过程来减少幻觉，从而提升模型的通用推理质量。 论文的核心贡献是提出了Bridge-Guided Dual-Thought-based Retrieval (BDTR)框架，解决了GraphRAG中静态检索的局限性，特别是在处理复杂多跳问题和桥接证据时的挑战。这项研究直接针对提高LLM的通用推理能力，尤其是多步推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#99",
        "title": "Where LLM Agents Fail and How They can Learn From Failures",
        "link": "/arxiv/2509.25370",
        "arxiv_id": "2509.25370",
        "authors": "Kunlun Zhu, Zijia Liu, Bingxuan Li, Muxin Tian, Yingxuan Yang, Jiaxun Zhang, Pengrui Han, Qipeng Xie, Fuyang Cui, Weijia Zhang, Xiaoteng Ma, Xiaodong Yu, Gowtham Ramesh, Jialian Wu, Zicheng Liu, Pan Lu, James Zou, Jiaxuan You",
        "summary": "Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at https://github.com/ulab-uiuc/AgentDebug",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T22:50:20.188041",
        "filter_reason": "根据筛选标准，这篇论文完全符合研究目标。首先，从核心判断来看，论文的本质是关于改进LLM agents的基础能力，提出了AgentDebug这一调试框架，使LLM agents能够从失败中学习和恢复，这直接增强了LLM的通用推理能力，特别是规划和问题解决能力。论文不是将LLM作为工具应用到特定领域，而是专注于提高LLM agents本身的通用能力。 其次，论文包含多个正面指标：核心概念上明确研究LLM agents；能力方向上涉及planning和problem-solving；新兴范式上研究LLM-based agents和tool use。这些都是提高LLM通用推理能力的关键要素。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或应用层面的模型可靠性研究。 在特殊和模糊情况处理上，论文提出的是通用的智能体调试框架，用于增强LLM agents的通用问题解决能力，而不是应用在特定领域。同时，论文关注的是提高模型内在可靠性和推理质量的技术解决方案，而非社会学研究或应用层面讨论。 论文的核心贡献是提出了AgentErrorTaxonomy、AgentErrorBench和AgentDebug，这些共同构成了一个系统性的方法，使LLM agents能够识别、理解和从失败中恢复，从而提高其通用推理能力和任务成功率。这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#100",
        "title": "Structural Reward Model: Enhancing Interpretability, Efficiency, and Scalability in Reward Modeling",
        "link": "/arxiv/2509.25361",
        "arxiv_id": "2509.25361",
        "authors": "Xiaoyu Liu, Di Liang, Hongyu Shan, Peiyang Liu, Yonghao Liu, Muling Wu, Yuntao Li, Xianjie Wu, LI Miao, Jiangrong Shen, Minlong Peng",
        "summary": "Reward Models (RMs) are key components for evaluating and guiding language model outputs. However, traditional scalar RMs often struggle with incorporating contextual and background information during inference, leading to incomplete evaluations. Generative RMs (GRMs) attempt to address these limitations by generating intermediate reasoning steps. Yet, their uncontrolled black-box nature and inefficiency due to sequential decoding hinder their industrial deployment. Industrial scenarios, such as search and recommendation systems, often involve single-domain tasks requiring evaluation along specific dimensions. In such contexts, diagnosing \"bad cases\" necessitates structured feedback to identify and optimize dimension-specific issues. In this paper, we propose the Structural Reward Model (SRM), a modular and interpretable framework integrating side-branch models as auxiliary feature generators. By introducing fine-grained dimensions, SRMs enable interpretable and efficient evaluation, facilitating targeted diagnostics and optimization. This structured approach ensures adaptability and scalability for industrial applications. Through comprehensive experiments, we demonstrate that SRMs outperform scalar RMs and GRMs in robustness and alignment with human preferences. The modular design further supports efficient optimization for practical scenarios, allowing SRM to provide a practical reward modeling solution for industry.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T22:50:20.188406",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，理由如下： 第一步核心判断：这篇论文的本质是提出一种结构化奖励模型(SRM)，用于改进语言模型输出的评估和引导机制。奖励模型是强化学习(如RLHF)中的关键组件，用于评估和指导语言模型的输出，这直接关系到改进LLM的基础能力和训练范式。论文不是将LLM作为工具应用到特定领域，而是专注于改进评估LLM输出的方法，这属于提升LLM通用能力的范畴。 第二步正面指标：论文确实包含多个正面指标。核心概念方面，论文明确关注语言模型(LLMs)的评估机制。训练方法方面，奖励模型是强化学习(RLHF, RL)中的关键组件，论文提出的SRM是对传统奖励模型的改进。虽然论文没有直接涉及reasoning、planning等能力方向，但提到了生成式奖励模型(GRMs)通过生成中间推理步骤来解决问题，这与推理能力间接相关。 第三步排除标准：论文没有主要聚焦于排除标准中的任何领域。虽然提到了\"Industrial scenarios, such as search and recommendation systems\"，但这只是应用场景的例子，论文的核心是提出一种通用的奖励模型框架，而不是专注于特定应用领域。 第四步特殊和模糊情况：论文明确涉及可解释性，标题中就提到\"Enhancing Interpretability\"，摘要中也强调SRM提供\"interpretable and efficient evaluation\"。根据筛选标准，如果论文提出一种新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量，应该保留。这篇论文正是通过结构化方法提升奖励模型的可解释性，进而可能提升LLM的推理质量。 综合分析，这篇论文的核心贡献是提出一种结构化奖励模型框架，通过引入细粒度维度和辅助特征生成器，提供更可解释和高效的评估机制，从而改进语言模型的训练和优化过程。这与研究目标\"提高大语言模型的通用推理能力\"直接相关，因为奖励模型在强化学习中起着指导模型优化方向的关键作用，改进奖励模型可以间接提升LLM的推理能力和整体性能。"
    },
    {
        "index": "#106",
        "title": "Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments",
        "link": "/arxiv/2509.25282",
        "arxiv_id": "2509.25282",
        "authors": "Jiexi Xu, Jiaqi Liu, Ran Tong, Su Liu",
        "summary": "Large language model (LLM) agents are increasingly capable of orchestrating complex tasks in low-code environments. However, these agents often exhibit hallucinations and logical inconsistencies because their inherent reasoning mechanisms rely on probabilistic associations rather than genuine causal understanding. This paper introduces a new programming paradigm: Causal-Visual Programming (CVP), designed to address this fundamental issue by explicitly introducing causal structures into the workflow design. CVP allows users to define a simple \"world model\" for workflow modules through an intuitive low-code interface, effectively creating a Directed Acyclic Graph (DAG) that explicitly defines the causal relationships between modules. This causal graph acts as a crucial constraint during the agent's reasoning process, anchoring its decisions to a user-defined causal structure and significantly reducing logical errors and hallucinations by preventing reliance on spurious correlations. To validate the effectiveness of CVP, we designed a synthetic experiment that simulates a common real-world problem: a distribution shift between the training and test environments. Our results show that a causally anchored model maintained stable accuracy in the face of this shift, whereas a purely associative baseline model that relied on probabilistic correlations experienced a significant performance drop. The primary contributions of this study are: a formal definition of causal structures for workflow modules; the proposal and implementation of a CVP framework that anchors agent reasoning to a user-defined causal graph; and empirical evidence demonstrating the framework's effectiveness in enhancing agent robustness and reducing errors caused by causal confusion in dynamic environments. CVP offers a viable path toward building more interpretable, reliable, and trustworthy AI agents.",
        "subjects": "Artificial Intelligence, Human-Computer Interaction, Software Engineering",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T22:50:20.190456",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Causal-Visual Programming (CVP)\"的新编程范式，旨在通过在工作流设计中明确引入因果结构来解决LLM智能体的幻觉和逻辑不一致问题。论文本质上是关于改进LLM智能体的通用推理能力的，特别是通过引入因果结构来锚定智能体的推理过程，减少逻辑错误和幻觉。这完全符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。 论文包含多个正面指标：明确关注LLM智能体(llm-based agents)，重点研究推理能力(reasoning)，特别是逻辑推理(logical reasoning)。论文不涉及任何需要排除的领域，如多模态与视觉处理、特定应用领域或模型可靠性的应用层面研究。 在特殊和模糊情况的处理上，论文提出的是一种通用的智能体推理增强方法，而不是针对特定领域的应用；同时，它通过改进模型内在的推理机制来减少幻觉和提高可靠性，而不是仅仅对这些现象进行表面研究。 因此，这篇论文完全符合筛选标准，应该被保留。"
    },
    {
        "index": "#110",
        "title": "Fact Grounded Attention: Eliminating Hallucination in Large Language Models Through Attention Level Knowledge Integration",
        "link": "/arxiv/2509.25252",
        "arxiv_id": "2509.25252",
        "authors": "Aayush Gupta",
        "summary": "\"The greatest enemy of knowledge is not ignorance, it is the illusion of knowledge.\" Large Language Models have conquered natural language but remain prisoners of their own probabilistic nature--confidently hallucinating facts they never truly knew. We present Fact Grounded Attention (FGA), a novel architectural modification that transforms unreliable language models into deterministic truth tellers by injecting verifiable knowledge directly into the attention mechanism. Unlike existing approaches that patch hallucinations after generation or prepend retrieved text, FGA intervenes at the mathematical heart of the transformer--the pre-softmax attention scores--creating a model that cannot hallucinate when facts exist in its knowledge base. Our experiments across 1,107 technical queries spanning smartphones, laptops, and electric vehicles demonstrate a transformation from 6.3% accuracy in vanilla Llama 3.2 to 99.7% accuracy with FGA. More critically, knowledge updates occur in under one second without retraining, compared to hours for parameter editing approaches. FGA doesn't just reduce hallucination--it eliminates it entirely for verifiable facts, marking a fundamental shift from probabilistic approximation to deterministic precision in neural language generation.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T22:50:20.191835",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步：核心判断——这篇论文的本质是什么？ 论文的核心是提出一种名为\"Fact Grounded Attention (FGA)\"的新型架构修改，通过在transformer的注意力机制层面直接注入可验证知识，来解决LLM的幻觉问题。这属于改进LLM基础能力的研究，特别是通过架构创新来提升模型的事实准确性和可靠性。幻觉的消除直接关系到模型推理能力的质量，因为错误的推理往往基于错误的事实。因此，这篇论文的本质符合\"改进LLM的基础能力\"的标准，应该保留。 第二步：正面指标——论文是否包含以下主题？ - 核心概念：论文明确研究Large Language Models，并以Llama 3.2作为实验模型，符合此指标。 - 能力方向：虽然论文没有直接讨论推理过程，但消除幻觉对于提升推理质量至关重要，因为准确的事实是一切正确推理的基础。论文关注的是提升模型的事实准确性，这与推理能力间接相关。 - 训练方法：论文没有涉及强化学习、进化等训练方法，而是提出架构层面的修改。 - 新兴范式：论文没有直接讨论智能体、工具使用等新兴范式。 第三步：排除标准——论文是否主要聚焦于以下领域？ 论文不符合任何排除标准。虽然实验使用了智能手机、笔记本电脑和电动汽车的技术查询，但这些只是验证方法有效性的例子，论文的核心是提出一种通用的减少幻觉的方法，而不是针对特定应用领域的研究。论文关注的是模型架构层面的改进，而非应用层面的水印、安全或安全性研究。 第四步：处理特殊和模糊情况 论文明确提出了一种新方法（FGA）来减少幻觉，通过在注意力机制层面注入知识，提升模型的通用可靠性和推理质量。这符合\"提出新方法来减少幻觉，从而提升模型的通用可靠性和推理质量\"的保留标准。 综上所述，这篇论文的核心贡献是通过架构创新（在注意力机制层面注入知识）来消除LLM的幻觉，从而提升模型的事实准确性和推理质量。这直接关系到LLM的通用推理能力，因为准确的事实是一切高质量推理的基础。因此，这篇论文符合研究范围，应该被保留。"
    },
    {
        "index": "#295",
        "title": "Reinforcement Learning-Guided Chain-of-Draft for Token-Efficient Code Generation",
        "link": "/arxiv/2509.25243",
        "arxiv_id": "2509.25243",
        "authors": "Xunzhu Tang, Iyiola Emmanuel Olatunji, Tiezhu Sun, Jacques Klein, Tegawende F. Bissyande",
        "summary": "LLMs demonstrate surface-level fluency in code generation but struggle with structured reasoning tasks requiring correctness and semantic alignment. While Chain-of-Thought (CoT) prompting enhances reasoning through intermediate steps, it suffers from verbosity and inefficiency. Chain-of-Draft (CoD) prompting offers more concise reasoning, but the stochastic nature of LLMs produces varying solution quality, making optimal selection challenging. We propose \\multicod, a reinforcement learning framework that learns to select the most promising candidate from CoD-generated solutions. Our approach uses strategy-guided prompting to encourage diverse reasoning styles and models solution selection as a contextual bandit problem. The framework optimizes interpretable features including code complexity, reasoning structure, and strategic metadata through a reward function balancing correctness, efficiency, and clarity. Experiments on MBPP, BigCodeBench, SWE-bench Verified, and Defects4J show \\multicod~outperforms and in some cases, on par with standard prompting, CoT, and CoD baselines while achieving cost and token efficiency from the user's perspective through a multi-candidate design that charges only for the selected output, reducing user billing by over 50\\% and improving LLM response quality, making \\multicod~more sustainable and scalable for real-world deployment. Our code is available: https://anonymous.4open.science/r/MultiCoD.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T22:50:20.255079",
        "filter_reason": "这篇论文的核心贡献是提出一种基于强化学习的Chain-of-Draft（multicod）方法，用于提高LLM在结构化推理任务中的表现。从第一步核心判断来看，论文本质上是改进LLM的基础推理能力，提出新的推理范式，而不是将LLM作为工具应用到特定领域。虽然论文使用代码生成作为评估平台，但其关注的是LLM的\"结构化推理任务 requiring correctness and semantic alignment\"，这是通用推理能力的核心组成部分。 从第二步正面指标看，论文完全符合：它研究LLM的核心概念，聚焦于推理能力（特别是结构化推理），采用强化学习方法（RL框架），并涉及策略引导等新兴范式。 从第三步排除标准看，论文不涉及多模态与视觉领域，也不主要关注模型可靠性问题。虽然代码生成可能被视为特定应用领域，但论文的核心是提高LLM的通用推理能力，代码生成只是作为评估这种能力的测试平台。 在第四步特殊和模糊情况处理中，虽然论文涉及代码生成这一特定任务，但其提出的方法（强化学习引导的推理链选择）具有通用性，可以应用于其他需要结构化推理的任务。论文的核心是改进LLM的推理机制，而非解决特定领域问题。 综上所述，这篇论文符合研究目标，应该被保留，因为它致力于提高LLM本身的通用推理能力，特别是结构化推理能力，通过强化学习方法优化推理过程。"
    }
]