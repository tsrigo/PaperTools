[
    {
        "index": "#8",
        "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning",
        "link": "/arxiv/2509.22601",
        "arxiv_id": "2509.22601",
        "authors": "Yulei Qin, Xiaoyu Tan, Zhengbao He, Gang Li, Haojia Lin, Zongyi Li, Zihan Xu, Yuchen Shi, Siqi Cai, Renting Rui, Shaofei Cai, Yuzheng Cai, Xuan Zhang, Sheng Ye, Ke Li, Xing Sun",
        "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Multiagent Systems",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T19:08:32.908902",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式(SPEAR)，用于增强LLM的基础能力。论文关注的是如何通过强化学习和自模仿学习来提高LLM的战略工具使用能力和长期规划能力，这属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划、多步推理等通用能力\"的范畴，而非将LLM作为工具应用到特定领域。 其次，从正面指标来看，论文高度相关： - 核心概念：明确讨论LLM在智能体任务中的应用 - 能力方向：关注\"strategic tool use capabilities\"和\"long-horizon, sparsely-rewarded agent tasks\"，涉及规划和问题解决能力 - 训练方法：核心是强化学习和自模仿学习(self-imitation learning) - 新兴范式：聚焦\"agentic LLMs\"和\"tool use capabilities\" 第三，论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在处理特殊和模糊情况时，论文提出的是一种通用的智能体训练框架，用于增强LLM的通用工具使用能力和战略决策能力，而非针对特定领域的应用。 综上所述，这篇论文的核心贡献是提出了一种新的训练方法来增强LLM的通用推理能力，特别是长期规划和工具使用能力，完全符合研究目标。"
    },
    {
        "index": "#6",
        "title": "RobustFlow: Towards Robust Agentic Workflow Generation",
        "link": "/arxiv/2509.21834",
        "arxiv_id": "2509.21834",
        "authors": "Shengxiang Xu, Jiayi Zhang, Shimin Di, Yuyu Luo, Liang Yao, Hanmo Liu, Jia Zhu, Fan Liu, Min-Ling Zhang",
        "summary": "The automated generation of agentic workflows is a promising frontier for enabling large language models (LLMs) to solve complex tasks. However, our investigation reveals that the robustness of agentic workflow remains a critical, unaddressed challenge. Current methods often generate wildly inconsistent workflows when provided with instructions that are semantically identical but differently phrased. This brittleness severely undermines their reliability and trustworthiness for real-world applications. To quantitatively diagnose this instability, we propose metrics based on nodal and topological similarity to evaluate workflow consistency against common semantic variations such as paraphrasing and noise injection. Subsequently, we further propose a novel training framework, RobustFlow, that leverages preference optimization to teach models invariance to instruction variations. By training on sets of synonymous task descriptions, RobustFlow boosts workflow robustness scores to 70\\% - 90\\%, which is a substantial improvement over existing approaches. The code is publicly available at https://github.com/DEFENSE-SEU/RobustFlow.",
        "subjects": "Multiagent Systems",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T19:08:32.907758",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础能力，特别是增强其生成智能体工作流的鲁棒性。论文提出了RobustFlow这一新的训练框架，通过偏好优化来教导模型对指令变化的不变性，这属于提升LLM通用推理能力的研究，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标： - 核心概念：明确关注大语言模型(LLMs) - 能力方向：涉及problem-solving（解决复杂任务）和planning（工作流生成本质上是一种规划能力） - 训练方法：使用了preference optimization（偏好优化），与强化学习方法相关 - 新兴范式：聚焦于agentic workflows（智能体工作流），属于llm-based agents的研究范畴 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对任何特定应用领域（如医疗、化学等） - 虽然关注robustness，但这是从模型生成工作流的一致性角度，而非应用层面的水印、安全或安保问题 在特殊和模糊情况处理上，论文提出的是通用的智能体工作流生成方法，旨在增强LLM解决复杂任务的通用能力，而非应用于特定领域。同时，论文关注的工作流生成鲁棒性问题可以视为提升模型推理质量和可靠性的方法。 综上所述，这篇论文的核心贡献是提出了一种新的训练框架来增强LLM生成智能体工作流的鲁棒性，直接提升了LLM的规划和问题解决等通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#12",
        "title": "Reimagining Agent-based Modeling with Large Language Model Agents via Shachi",
        "link": "/arxiv/2509.21862",
        "arxiv_id": "2509.21862",
        "authors": "So Kuroki, Yingtao Tian, Kou Misaki, Takashi Ikegami, Takuya Akiba, Yujin Tang",
        "summary": "The study of emergent behaviors in large language model (LLM)-driven multi-agent systems is a critical research challenge, yet progress is limited by a lack of principled methodologies for controlled experimentation. To address this, we introduce Shachi, a formal methodology and modular framework that decomposes an agent's policy into core cognitive components: Configuration for intrinsic traits, Memory for contextual persistence, and Tools for expanded capabilities, all orchestrated by an LLM reasoning engine. This principled architecture moves beyond brittle, ad-hoc agent designs and enables the systematic analysis of how specific architectural choices influence collective behavior. We validate our methodology on a comprehensive 10-task benchmark and demonstrate its power through novel scientific inquiries. Critically, we establish the external validity of our approach by modeling a real-world U.S. tariff shock, showing that agent behaviors align with observed market reactions only when their cognitive architecture is appropriately configured with memory and tools. Our work provides a rigorous, open-source foundation for building and evaluating LLM agents, aimed at fostering more cumulative and scientifically grounded research.",
        "subjects": "Artificial Intelligence, Multiagent Systems, Social and Information Networks, General Economics",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T19:08:32.915965",
        "filter_reason": "这篇论文的核心贡献是提出Shachi，一种形式化方法和模块化框架，用于构建和评估LLM智能体。论文将智能体的策略分解为核心认知组件：配置、内存和工具，由LLM推理引擎协调，旨在分析特定架构选择如何影响集体行为。这直接符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。论文关注LLM驱动的多智能体系统和工具使用，这些都是增强LLM通用推理能力的重要方向。虽然论文提到了一个现实世界的应用案例（美国关税冲击），但这只是为了验证其方法论的有效性，而不是论文的核心焦点。论文的核心是提出一种通用的智能体架构框架，而不是专注于特定应用领域，因此不应被排除。"
    },
    {
        "index": "#11",
        "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration",
        "link": "/arxiv/2509.21981",
        "arxiv_id": "2509.21981",
        "authors": "Zhimin Wang, Shaokang He, Duo Wu, Jinghe Wang, Linjia Kang, Jing Yu, Zhi Wang",
        "summary": "Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs) have emerged as promising autonomous agents for collaborative task solving. However, existing collaboration frameworks for LLMs overlook their reasoning potential for dynamic intent inference, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. To bridge this gap, we propose CoBel-World, a novel framework that equips LLM agents with a collaborative belief world -- an internal representation jointly modeling the physical environment and collaborators' mental states. CoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World significantly reduces communication costs by 22-60% and improves task completion efficiency by 4-28% compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T19:08:32.915450",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文的核心是提出CoBel-World框架，通过利用LLM的推理能力构建协作信念世界，以优化多智能体协作。这本质上是改进LLM的基础推理能力，特别是关于理解他人意图和避免协调失误的推理能力，属于增强LLM通用推理能力的研究，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确以Large language models (LLMs)为研究对象 - 能力方向：重点研究reasoning能力（特别是关于协作者意图的推理），以及planning和problem-solving能力 - 新兴范式：涉及llm-based agents和multi-agent系统 第三步排除标准：论文不主要聚焦于任何排除领域： - 虽然提到\"embodied benchmarks\"，但这只是作为评估平台，而非研究焦点 - 不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究 第四步特殊和模糊情况处理： - 论文提出的是通用的智能体协作框架(CoBel-World)，用于增强LLM在多智能体环境中的通用推理能力，而非应用于特定领域的智能体 - 框架的核心是通过符号信念语言和贝叶斯式信念更新来提升LLM的推理质量，这直接关联到提升模型的通用推理能力 论文的核心贡献是提出了一种新方法来增强LLM在多智能体协作中的推理能力，特别是关于理解他人意图的推理，这直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#16",
        "title": "Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective",
        "link": "/arxiv/2509.21613",
        "arxiv_id": "2509.21613",
        "authors": "Lingxiao Kong, Cong Yang, Oya Deniz Beyan, Zeyd Boukhers",
        "summary": "Multi-Objective Reinforcement Learning (MORL) presents significant challenges and opportunities for optimizing multiple objectives in Large Language Models (LLMs). We introduce a MORL taxonomy and examine the advantages and limitations of various MORL methods when applied to LLM optimization, identifying the need for efficient and flexible approaches that accommodate personalization functionality and inherent complexities in LLMs and RL. We propose a vision for a MORL benchmarking framework that addresses the effects of different methods on diverse objective relationships. As future research directions, we focus on meta-policy MORL development that can improve efficiency and flexibility through its bi-level learning paradigm, highlighting key research questions and potential solutions for improving LLM performance.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Multiagent Systems",
        "date": "2025-09-25",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T19:08:32.917689",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的本质是关于使用多目标强化学习(MORL)来优化大语言模型(LLM)的性能。论文提出了一种新的训练/优化范式，旨在改进LLM的基础能力，而不是将LLM作为工具应用到特定领域。论文关注的是LLM本身的优化方法，属于改进LLM基础能力和提出新训练范式的研究，因此应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念：明确关注大语言模型(LLMs)的优化 - 训练方法：详细讨论了多目标强化学习(MORL)作为LLM的优化方法 虽然摘要中没有直接提到reasoning、planning等能力方向，但论文目标是\"改进LLM性能\"，这通常包括提升这些通用推理能力。 第三步：排除标准 论文不涉及任何排除标准中提到的领域： - 没有涉及多模态与视觉相关内容 - 没有将LLM应用到特定领域（如医疗、化学等） - 没有主要关注模型可靠性方面的应用问题（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况。 最终决策 综合以上分析，这篇论文的核心贡献是提出使用多目标强化学习来优化大语言模型的方法论，旨在改进LLM的基础性能，这符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文关注的是LLM本身的训练和优化范式，而不是将其作为工具应用到特定领域，因此应该被保留。"
    },
    {
        "index": "#4",
        "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards",
        "link": "/arxiv/2509.22638",
        "arxiv_id": "2509.22638",
        "authors": "Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, Tianyu Pang",
        "summary": "LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at https://github.com/sail-sg/feedback-conditional-policy.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.738944",
        "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，该论文的本质是提出一种新的训练范式——反馈条件策略(FCP)，让大语言模型能够直接从语言反馈中学习，而不是将反馈压缩为标量奖励。这明显属于改进LLM基础能力的研究，特别是强化学习优化的新方法，符合\"保留\"标准。 其次，从正面指标分析，论文明确关注大语言模型(LLMs)这一核心概念，并涉及强化学习(RL from human or AI feedback)这一训练方法。虽然论文没有直接提及推理、规划等能力，但改进LLM从反馈中学习的能力很可能间接提升这些通用推理能力。 第三，论文不符合任何排除标准。它没有涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。 最后，论文不涉及需要特殊判断的情况，如智能体/工具使用或幻觉/可解释性/安全等主题。 论文的核心贡献是提出了一种更丰富、更具表现力的方式让LLM直接从语言反馈中学习，这改变了传统的强化学习反馈机制，将反馈驱动的学习重新定义为条件生成而非奖励优化。这种方法有望提升LLM的通用学习能力和推理质量，因此完全符合\"提高大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#5",
        "title": "Variational Reasoning for Language Models",
        "link": "/arxiv/2509.22637",
        "arxiv_id": "2509.22637",
        "authors": "Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang",
        "summary": "We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at https://github.com/sail-sg/variational-reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.739464",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提出一种变分推理框架，将思维轨迹视为潜在变量并通过变分推断优化它们，这明显是关于改进LLM基础推理能力的研究，而非将LLM作为工具应用到特定领域。 论文符合多个正面指标：1) 核心概念方面，明确研究语言模型(LLMs)；2) 能力方向方面，直接聚焦于推理能力(reasoning)，并在摘要中多次提及\"reasoning ability\"和\"reasoning tasks\"；3) 训练方法方面，论文将变分推断与RL风格方法结合，提到了\"binary-reward RL, including GRPO\"等强化学习方法；4) 新兴范式方面，论文研究的\"thinking traces\"与思维链(CoT)等推理方法密切相关。 论文不符合任何排除标准：没有涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。论文也没有涉及需要特殊处理的模糊情况，如特定领域的智能体应用或应用层面的幻觉/可解释性/安全问题。 论文的核心贡献是提供了一个统一的概率视角，将变分推断与RL风格方法结合，为提高语言模型的推理能力提供了稳定的目标，这与研究目标\"提高大语言模型本身的通用推理能力\"高度一致。"
    },
    {
        "index": "#13",
        "title": "Think Socially via Cognitive Reasoning",
        "link": "/arxiv/2509.22546",
        "arxiv_id": "2509.22546",
        "authors": "Jinfeng Zhou, Zheyu Chen, Shuai Wang, Quanyu Dai, Zhenhua Dong, Hongning Wang, Minlie Huang",
        "summary": "LLMs trained for logical reasoning excel at step-by-step deduction to reach verifiable answers. However, this paradigm is ill-suited for navigating social situations, which induce an interpretive process of analyzing ambiguous cues that rarely yield a definitive outcome. To bridge this gap, we introduce Cognitive Reasoning, a paradigm modeled on human social cognition. It formulates the interpretive process into a structured cognitive flow of interconnected cognitive units (e.g., observation or attribution), which combine adaptively to enable effective social thinking and responses. We then propose CogFlow, a complete framework that instills this capability in LLMs. CogFlow first curates a dataset of cognitive flows by simulating the associative and progressive nature of human thought via tree-structured planning. After instilling the basic cognitive reasoning capability via supervised fine-tuning, CogFlow adopts reinforcement learning to enable the model to improve itself via trial and error, guided by a multi-objective reward that optimizes both cognitive flow and response quality. Extensive experiments show that CogFlow effectively enhances the social cognitive capabilities of LLMs, and even humans, leading to more effective social decision-making.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.789252",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Cognitive Reasoning\"（认知推理）的新范式和CogFlow框架，用于增强大语言模型在社交情境中的推理能力。从本质上看，论文属于改进LLM的基础推理能力，提出新的训练范式（结合监督微调和强化学习），而非将LLM作为工具应用到特定领域。论文明确研究LLMs的推理能力，并采用强化学习方法进行优化，这些都是研究目标的正面指标。虽然论文关注社交情境，但这应被视为一种通用推理能力，而非医疗、化学等特定应用领域。社交推理是人类普遍需要的基础能力，论文提出的认知推理范式具有通用性，通过树结构规划和强化学习来提升模型推理能力，因此该研究完全符合\"提高大语言模型通用推理能力\"的核心目标。"
    },
    {
        "index": "#18",
        "title": "Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving",
        "link": "/arxiv/2509.22480",
        "arxiv_id": "2509.22480",
        "authors": "Hang Li, Kaiqi Yang, Yucheng Chu, Hui Liu, Jiliang Tang",
        "summary": "Large language models (LLMs) have been widely used for problem-solving tasks. Most recent work improves their performance through supervised fine-tuning (SFT) with labeled data or reinforcement learning (RL) from task feedback. In this paper, we study a new perspective: the divergence in solutions generated by LLMs for a single problem. We show that higher solution divergence is positively related to better problem-solving abilities across various models. Based on this finding, we propose solution divergence as a novel metric that can support both SFT and RL strategies. We test this idea on three representative problem domains and find that using solution divergence consistently improves success rates. These results suggest that solution divergence is a simple but effective tool for advancing LLM training and evaluation.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.797893",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，该论文的本质是研究LLM解决方案差异性(solution divergence)对问题解决能力的影响，并提出一种新的训练和评估指标。这明显属于改进LLM基础能力和通用推理能力的研究，而非将LLM作为工具应用于特定领域。论文提出的解决方案差异性指标可以支持监督微调(SFT)和强化学习(RL)策略，目的是提升LLM的通用问题解决能力，符合保留标准。 其次，从正面指标分析，论文符合多个关键指标： - 核心概念：明确研究Large language models (LLMs) - 能力方向：关注problem-solving能力，这是推理能力的核心组成部分 - 训练方法：提出的指标可以支持reinforcement learning (RL)策略 第三，论文不符合任何排除标准。虽然提到在\"三个代表性问题领域\"进行测试，但这些领域只是用来验证通用方法的实验平台，而非论文的主要焦点。论文的核心是提出一种通用的训练和评估方法，而非针对特定应用领域。 综上所述，该论文的核心贡献是发现并验证了解决方案差异性这一新指标对提升LLM问题解决能力的有效性，这直接涉及到改进LLM的通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#21",
        "title": "Detecting (Un)answerability in Large Language Models with Linear Directions",
        "link": "/arxiv/2509.22449",
        "arxiv_id": "2509.22449",
        "authors": "Maor Juliet Lavi, Tova Milo, Mor Geva",
        "summary": "Large language models (LLMs) often respond confidently to questions even when they lack the necessary information, leading to hallucinated answers. In this work, we study the problem of (un)answerability detection, focusing on extractive question answering (QA) where the model should determine if a passage contains sufficient information to answer a given question. We propose a simple approach for identifying a direction in the model's activation space that captures unanswerability and uses it for classification. This direction is selected by applying activation additions during inference and measuring their impact on the model's abstention behavior. We show that projecting hidden activations onto this direction yields a reliable score for (un)answerability classification. Experiments on two open-weight LLMs and four extractive QA benchmarks show that our method effectively detects unanswerable questions and generalizes better across datasets than existing prompt-based and classifier-based approaches. Moreover, the obtained directions extend beyond extractive QA to unanswerability that stems from factors, such as lack of scientific consensus and subjectivity. Last, causal interventions show that adding or ablating the directions effectively controls the abstention behavior of the model.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.799612",
        "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：这篇论文的本质是改进LLM的基础能力，具体是增强模型判断问题可回答性的元认知能力。论文提出了一种新方法，通过在模型激活空间中识别捕捉\"不可回答性\"的线性方向，来提升模型自我评估能力，从而减少幻觉产生。这属于改进LLM基础能力的研究，而非将LLM作为工具应用到特定领域，因此应保留。 第二步正面指标：论文明确研究Large language models (LLMs)，符合核心概念指标。虽然不直接研究数学或逻辑推理，但模型判断问题可回答性的能力与推理质量密切相关，属于推理能力的基础支撑。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面（如水印、安全等），不符合任何排除标准。 第四步特殊和模糊情况：论文研究减少幻觉的问题，但提出的是一种新方法（激活空间中的线性方向）来增强模型内在的自我评估能力，属于提升模型内在可靠性和推理质量的方法，而非社会学研究或应用层面讨论，应保留。 最终决策：论文的核心贡献是提出了一种提升LLM自我评估能力的方法，这属于通用推理能力的重要组成部分。高质量的推理不仅需要逻辑和数学能力，还需要模型能够准确判断自己是否有足够信息回答问题。因此，这篇论文符合\"大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#28",
        "title": "Transformers Can Learn Connectivity in Some Graphs but Not Others",
        "link": "/arxiv/2509.22343",
        "arxiv_id": "2509.22343",
        "authors": "Amit Roy, Abulhair Saparov",
        "summary": "Reasoning capability is essential to ensure the factual correctness of the responses of transformer-based Large Language Models (LLMs), and robust reasoning about transitive relations is instrumental in many settings, such as causal inference. Hence, it is essential to investigate the capability of transformers in the task of inferring transitive relations (e.g., knowing A causes B and B causes C, then A causes C). The task of inferring transitive relations is equivalent to the task of connectivity in directed graphs (e.g., knowing there is a path from A to B, and there is a path from B to C, then there is a path from A to C). Past research focused on whether transformers can learn to infer transitivity from in-context examples provided in the input prompt. However, transformers' capability to infer transitive relations from training examples and how scaling affects the ability is unexplored. In this study, we seek to answer this question by generating directed graphs to train transformer models of varying sizes and evaluate their ability to infer transitive relations for various graph sizes. Our findings suggest that transformers are capable of learning connectivity on \"grid-like'' directed graphs where each node can be embedded in a low-dimensional subspace, and connectivity is easily inferable from the embeddings of the nodes. We find that the dimensionality of the underlying grid graph is a strong predictor of transformers' ability to learn the connectivity task, where higher-dimensional grid graphs pose a greater challenge than low-dimensional grid graphs. In addition, we observe that increasing the model scale leads to increasingly better generalization to infer connectivity over grid graphs. However, if the graph is not a grid graph and contains many disconnected components, transformers struggle to learn the connectivity task, especially when the number of components is large.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Logic in Computer Science",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.802854",
        "filter_reason": "这篇论文的核心是研究Transformer模型（大语言模型）在传递关系推理方面的能力，这是一种基础的逻辑推理能力。论文探讨了模型如何从训练数据中学习传递关系（如\"A导致B，B导致C，则A导致C\"），这等价于在有向图中推断连通性。研究发现Transformer能够学习某些类型图结构的连通性，并且模型规模的增加会提高这种推理能力。这完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标，因为它关注的是LLM的基础推理能力，而不是将LLM作为工具应用到特定领域。论文明确研究\"Reasoning capability\"和\"transitive relations\"的逻辑推理能力，这些都是大语言模型通用推理能力的核心组成部分。论文没有涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性应用层面），而是专注于提升模型本身的逻辑推理能力这一核心目标。"
    },
    {
        "index": "#31",
        "title": "Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs",
        "link": "/arxiv/2509.22251",
        "arxiv_id": "2509.22251",
        "authors": "Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong",
        "summary": "Currently, the main approach for Large Language Models (LLMs) to tackle the hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs typically treat KGs as plain text, extracting only semantic information and limiting their use of the crucial structural aspects of KGs. Another challenge is the gap between the embedding spaces of KGs encoders and LLMs text embeddings, which hinders the effective integration of structured knowledge. To overcome these obstacles, we put forward the SSKG-LLM, an innovative model architecture that is designed to efficiently integrate both the Structural and Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph Encoding (KGE) module to preserve semantics while utilizing structure. Then, the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to understand KGs embeddings. We conduct extensive experiments and provide a detailed analysis to explore how incorporating the structural information of KGs can enhance the factual reasoning abilities of LLMs. Our code are available at https://github.com/yfangZhang/SSKG-LLM.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.908451",
        "filter_reason": "这篇论文的核心贡献是提出SSKG-LLM模型架构，通过有效整合知识图谱的结构和语义信息来减轻大语言模型的幻觉问题，并增强其事实推理能力。从第一步核心判断来看，论文明确聚焦于改进LLM的基础推理能力，特别是事实推理这一通用能力，而非将LLM作为工具应用到特定领域。论文包含多个正面指标：明确关注Large Language Models (LLMs)和reasoning能力。在排除标准方面，论文不涉及多模态、特定应用领域或模型基础设施优化。对于特殊情况的处理，论文虽然涉及幻觉问题，但它是从模型内部机制提出解决方案，通过结构化知识整合来提升推理质量，而非应用层面的讨论。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标，应被保留。"
    },
    {
        "index": "#29",
        "title": "Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs",
        "link": "/arxiv/2509.22338",
        "arxiv_id": "2509.22338",
        "authors": "Felix Vossel, Till Mossakowski, Björn Gehrke",
        "summary": "Automating the translation of natural language to first-order logic (FOL) is crucial for knowledge representation and formal methods, yet remains challenging. We present a systematic evaluation of fine-tuned LLMs for this task, comparing architectures (encoder-decoder vs. decoder-only) and training strategies. Using the MALLS and Willow datasets, we explore techniques like vocabulary extension, predicate conditioning, and multilingual training, introducing metrics for exact match, logical equivalence, and predicate alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT reasoning ability as well as symbolic systems like ccg2lambda. Key findings show: (1) predicate availability boosts performance by 15-20%, (2) T5 models surpass larger decoder-only LLMs, and (3) models generalize to unseen logical arguments (FOLIO dataset) without specific training. While structural logic translation proves robust, predicate extraction emerges as the main bottleneck.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.896762",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。 首先，从核心判断来看，这篇论文的本质是研究如何通过微调LLMs来提高它们将自然语言转换为一阶逻辑(FOL)的能力。一阶逻辑转换是逻辑推理的基础能力，属于LLM的通用推理能力范畴，而不是将LLM作为工具应用到特定领域。论文关注的是改进LLM本身的基础能力（逻辑推理和形式化表示），符合第一步的保留标准。 其次，从正面指标分析，论文明确符合两个关键指标： 1) 核心概念：论文直接研究\"fine-tuned LLMs\"，使用了Flan-T5-XXL、GPT-4o和DeepSeek-R1等大语言模型。 2) 能力方向：论文聚焦于logical reasoning，通过自然语言到一阶逻辑的转换来增强模型的逻辑推理能力，这是通用推理能力的核心组成部分。 第三，论文不符合任何排除标准。它不涉及多模态与视觉内容，不针对特定应用领域（如医疗、化学等），也不关注模型可靠性问题（如水印、安全等）。 论文的核心贡献是提出了一种系统评估和改进LLMs自然语言到一阶逻辑转换能力的方法，通过微调技术、架构比较和训练策略优化，显著提升了模型在逻辑推理任务上的表现。这种能力是通用的，可以应用于多个领域，而不是局限于特定应用场景，因此完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#35",
        "title": "In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners",
        "link": "/arxiv/2509.22230",
        "arxiv_id": "2509.22230",
        "authors": "Jaehoon Kim, Kwangwook Seo, Dongha Lee",
        "summary": "Transferring reasoning capabilities from larger language models to smaller ones through supervised fine-tuning often fails counterintuitively, with performance degrading despite access to high-quality teacher demonstrations. We identify that this failure stems from distributional misalignment: reasoning traces from larger models contain tokens that are low probability under the student's distribution, exceeding the internal representation capacity of smaller architectures and creating learning barriers rather than helpful guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for generating student-friendly reasoning traces in which the teacher model proposes candidate tokens but the student model determines acceptance based on its own probability distributions, filtering low probability tokens. When applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data degrades average performance across major reasoning benchmarks by 20.5\\%, while the same model trained on RSD-generated reasoning traces achieves meaningful improvements of 4.9\\%. Our analysis reveals that low probability tokens constitute the critical bottleneck in reasoning ability transfer. However, cross-model experiments demonstrate that RSD traces are model-specific rather than universally applicable, indicating that distributional alignment must be tailored for each student architecture's unique internal representation.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.910453",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提升小语言模型的推理能力这一基础能力，提出了反向推测解码(RSD)这一新的训练范式来增强模型的推理能力，而不是将LLM作为工具应用到特定领域。论文明确关注\"reasoning capabilities\"和\"reasoning traces\"，并在多个推理基准上测试性能提升，这直接对应了筛选标准中的\"reasoning\"能力方向。论文不符合任何排除标准，不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文没有涉及强化学习或智能体等新兴范式，但它提出的新训练机制RSD解决了推理能力转移中的关键问题，即分布不匹配导致的低概率token障碍，这直接提升了小模型的通用推理能力。因此，这篇论文的核心贡献是提升LLM本身的推理能力，完全符合研究目标。"
    },
    {
        "index": "#36",
        "title": "Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data",
        "link": "/arxiv/2509.22224",
        "arxiv_id": "2509.22224",
        "authors": "Zishan Ahmad, Saisubramaniam Gopalakrishnan",
        "summary": "Large Language Models (LLMs), despite their remarkable capabilities, rely on singular, pre-dominant reasoning paradigms, hindering their performance on intricate problems that demand diverse cognitive strategies. To address this, we introduce Composite Reasoning (CR), a novel reasoning approach empowering LLMs to dynamically explore and combine multiple reasoning styles like deductive, inductive, and abductive for more nuanced problem-solving. Evaluated on scientific and medical question-answering benchmarks, our approach outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while demonstrating superior sample efficiency and adequate token usage. Notably, CR adaptively emphasizes domain-appropriate reasoning styles. It prioritizes abductive and deductive reasoning for medical question answering, but shifts to causal, deductive, and inductive methods for scientific reasoning. Our findings highlight that by cultivating internal reasoning style diversity, LLMs acquire more robust, adaptive, and efficient problem-solving abilities.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.910933",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是提出\"Composite Reasoning (CR)\"这一新型推理方法，使LLMs能够动态探索和组合多种推理风格（如演绎、归纳和溯因推理）。这明显是关于改进LLM的基础推理能力，提出新的推理范式，增强其逻辑和多步推理等通用能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确关注Large Language Models (LLMs) - 能力方向：专注于reasoning，特别是多种推理风格（演绎、归纳、溯因、因果推理） - 新兴范式：提出对思维链(CoT)等现有范式的扩展和改进 第三步排除标准：论文不符合任何排除标准： - 虽然在科学和医学问答基准上进行了评估，但其核心贡献是通用推理方法，而非专注于这些特定领域 - 不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容 论文的核心贡献是提出一种通用的推理框架，使LLM能够根据问题类型动态选择和组合不同的推理风格，从而提升模型的通用推理能力和问题解决能力。正如摘要所述，\"通过培养内部推理风格多样性，LLMs获得更强大、自适应和高效的问题解决能力\"，这完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#40",
        "title": "When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance",
        "link": "/arxiv/2509.22193",
        "arxiv_id": "2509.22193",
        "authors": "Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Kevin El-Haddad, Céline Hudelot, Pierre Colombo",
        "summary": "Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art performance on a wide range of tasks. Despite its empirical success, the tasks and model scales at which reasoning becomes effective, as well as its training and inference costs, remain underexplored. In this work, we rely on a synthetic data distillation framework to conduct a large-scale supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models of varying sizes, on a wide range of math-centric and general-purpose tasks, evaluating both multiple-choice and open-ended formats. Our analysis reveals that reasoning consistently improves model performance, often matching or surpassing significantly larger IFT systems. Notably, while IFT remains Pareto-optimal in training and inference costs, reasoning models become increasingly valuable as model size scales, overcoming IFT performance limits on reasoning-intensive and open-ended tasks.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.918606",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究LLMs的推理能力本身，而非将LLM作为工具应用到特定领域。论文通过控制实验研究推理能力对模型性能的贡献，探讨在什么任务和模型规模下推理变得有效，这直接关注的是LLM的基础能力提升。 其次，论文包含了多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：核心主题就是reasoning，特别关注math reasoning和通用任务中的推理能力 - 训练方法：比较了Instruction Fine-Tuning (IFT)和推理模型的不同训练范式 第三，论文不涉及任何排除标准中的领域。它没有关注多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是通过大规模控制实验，系统性地研究了推理能力对LLM性能的影响，发现推理能力能一致地提高模型性能，特别是在推理密集型和开放式任务上。这项研究直接有助于理解如何提升LLM的通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#43",
        "title": "From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement",
        "link": "/arxiv/2509.22144",
        "arxiv_id": "2509.22144",
        "authors": "Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Zike Yuan, Yang Xiang, Buzhou Tang",
        "summary": "Chain-of-Thought (CoT) reasoning improves performance on complex tasks but introduces significant inference latency due to verbosity. We propose Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that leverages the token elasticity phenomenon--where overly small token budgets can paradoxically increase output length--to progressively compress CoTs via multiround refinement. This adaptive strategy allows MACC to determine the optimal compression depth for each input. Our method achieves an average accuracy improvement of 5.6 percent over state-of-the-art baselines, while also reducing CoT length by an average of 47 tokens and significantly lowering latency. Furthermore, we show that test-time performance--accuracy and token length--can be reliably predicted using interpretable features like perplexity and compression rate on the training set. Evaluated across different models, our method enables efficient model selection and forecasting without repeated fine-tuning, demonstrating that CoT compression is both effective and predictable. Our code will be released in https://github.com/Leon221220/MACC.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.920709",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Multiround Adaptive Chain-of-Thought Compression (MACC)\"的框架，用于优化思维链(CoT)推理过程。思维链(CoT)是提高大语言模型在复杂任务上表现的重要技术，属于LLM的通用推理能力范畴。论文通过多轮细化和自适应策略，实现了CoT的有效压缩，同时提高了准确性和效率。这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标，特别是关于思维链(CoT)的方法论研究。论文不涉及任何排除标准中的领域，如特定应用领域或多模态研究，而是专注于增强LLM的基础推理能力。论文明确涉及了\"reasoning\"这一核心能力方向，并通过提出新的方法来改进CoT推理，使其更加高效。因此，这篇论文完全符合研究范围，应被保留。"
    },
    {
        "index": "#46",
        "title": "R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning",
        "link": "/arxiv/2509.22131",
        "arxiv_id": "2509.22131",
        "authors": "Hongyu Shan, Mingyang Song, Chang Dai, Di Liang, Han Chen",
        "summary": "Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle complex reasoning by eliciting explicit step-by-step rationales. However, CoT's verbosity increases latency and memory usage and may propagate early errors across long chains. We propose the Reasoning Capsule (R-Capsule), a framework that aims to combine the efficiency of latent reasoning with the transparency of explicit CoT. The core idea is to compress the high-level plan into a small set of learned latent tokens (a Reasoning Capsule) while keeping execution steps lightweight or explicit. This hybrid approach is inspired by the Information Bottleneck (IB) principle, where we encourage the capsule to be approximately minimal yet sufficient for the task. Minimality is encouraged via a low-capacity bottleneck, which helps improve efficiency. Sufficiency is encouraged via a dual objective: a primary task loss for answer accuracy and an auxiliary plan-reconstruction loss that encourages the capsule to faithfully represent the original textual plan. The reconstruction objective helps ground the latent space, thereby improving interpretability and reducing the use of uninformative shortcuts. Our framework strikes a balance between efficiency, accuracy, and interpretability, thereby reducing the visible token footprint of reasoning while maintaining or improving accuracy on complex benchmarks. Our codes are available at: https://anonymous.4open.science/r/Reasoning-Capsule-7BE0",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.928221",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是改进LLM的基础推理能力。论文提出了Reasoning Capsule (R-Capsule)框架，旨在优化思维链(CoT)推理过程，提高LLM的推理效率和准确性。论文关注的是通用推理能力的增强，而不是将LLM应用于特定领域，也不涉及模型基础设施或部署优化。因此，论文符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Large Language Models (LLMs)\" - 能力方向：聚焦于\"reasoning\"，特别是\"complex reasoning\"和\"step-by-step rationales\"，同时涉及\"high-level plans\"和\"planning\" - 论文旨在提高模型在\"complex benchmarks\"上的表现，涉及problem-solving 第三步：排除标准 论文不聚焦于任何排除标准中提到的领域： - 不涉及多模态与视觉相关内容 - 不针对任何特定应用领域（如医疗、化学、生物等） - 不讨论模型可靠性层面的水印、安全性等问题 第四步：特殊和模糊情况 论文涉及可解释性的改进，提到\"improving interpretability\"，这是通过计划重建目标实现的，目的是提高模型的内在可解释性，从而提升模型的通用推理质量。这符合保留标准。 核心贡献分析： 论文的核心贡献是提出R-Capsule框架，通过将高级计划压缩成一组学习到的潜在令牌，结合了潜在推理的效率和显式思维链的透明度。这种方法旨在提高LLM的通用推理能力，减少推理过程中的可见令牌占用，同时保持或提高在复杂基准测试上的准确性。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。 因此，这篇论文应该被保留，它直接关注LLM的通用推理能力提升，提出了新的推理框架，符合研究范围。"
    },
    {
        "index": "#50",
        "title": "Think Right, Not More: Test-Time Scaling for Numerical Claim Verification",
        "link": "/arxiv/2509.22101",
        "arxiv_id": "2509.22101",
        "authors": "Primakov Chungkham, V Venktesh, Vinay Setty, Avishek Anand",
        "summary": "Fact-checking real-world claims, particularly numerical claims, is inherently complex that require multistep reasoning and numerical reasoning for verifying diverse aspects of the claim. Although large language models (LLMs) including reasoning models have made tremendous advances, they still fall short on fact-checking real-world claims that require a combination of compositional and numerical reasoning. They are unable to understand nuance of numerical aspects, and are also susceptible to the reasoning drift issue, where the model is unable to contextualize diverse information resulting in misinterpretation and backtracking of reasoning process. In this work, we systematically explore scaling test-time compute (TTS) for LLMs on the task of fact-checking complex numerical claims, which entails eliciting multiple reasoning paths from an LLM. We train a verifier model (VERIFIERFC) to navigate this space of possible reasoning paths and select one that could lead to the correct verdict. We observe that TTS helps mitigate the reasoning drift issue, leading to significant performance gains for fact-checking numerical claims. To improve compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS selectively based on the perceived complexity of the claim. This approach achieves 1.8x higher efficiency than standard TTS, while delivering a notable 18.8% performance improvement over single-shot claim verification methods. Our code and data can be found at https://github.com/VenkteshV/VerifierFC",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.930355",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础推理能力，而非将其作为工具应用于特定领域。论文的核心贡献是提出了一种测试时计算扩展(TTS)方法，通过生成多个推理路径并训练验证器模型(VERIFIERFC)来选择正确路径，从而增强LLM的推理能力。这属于改进LLM基础能力的研究，特别是针对多步推理和数值推理能力的提升。 其次，论文包含多个正面指标：明确涉及大语言模型(LLMs)这一核心概念；专注于推理能力(reasoning)，特别是数值推理(numerical reasoning)和多步推理(multistep reasoning)；提出了一种新的训练范式来增强模型的问题解决能力。 第三，论文不符合排除标准。虽然研究场景是事实核查，但这不是一个特定领域应用(如医疗、化学等)，而是一个通用的信息处理任务。论文不涉及多模态、视觉内容，也不关注模型基础设施或部署优化。 最后，在处理模糊情况时，我认为虽然论文在事实核查任务上评估其方法，但其核心贡献是提出了一种通用的测试时计算扩展方法来解决\"推理漂移\"问题，这种方法具有通用性，可以应用于其他需要复杂推理的任务。论文关注的是如何改进LLM本身的推理能力，而非将LLM作为工具应用到特定领域。 因此，这篇论文符合研究范围，它致力于提高大语言模型的通用推理能力，特别是数值推理和多步推理能力。"
    },
    {
        "index": "#51",
        "title": "S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models",
        "link": "/arxiv/2509.22099",
        "arxiv_id": "2509.22099",
        "authors": "Shaoning Sun, Jiachen Yu, Zongqi Wang, Xuewei Yang, Tianle Gu, Yujiu Yang",
        "summary": "With the rapid development of large language models (LLMs), generative reward models (GRMs) have been widely adopted for reward modeling and evaluation. Previous studies have primarily focused on training specialized GRMs by optimizing them on preference datasets with the judgment correctness as supervision. While it's widely accepted that GRMs with stronger problem-solving capabilities typically exhibit superior judgment abilities, we first identify a significant solve-to-judge gap when examining individual queries. Specifically, the solve-to-judge gap refers to the phenomenon where GRMs struggle to make correct judgments on some queries (14%-37%), despite being fully capable of solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to address this problem. Specifically, S2J simultaneously leverages both the solving and judging capabilities on a single GRM's output for supervision, explicitly linking the GRM's problem-solving and evaluation abilities during model optimization, thereby narrowing the gap. Our comprehensive experiments demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%, thereby enhancing the model's judgment performance by 5.8%. Notably, S2J achieves state-of-the-art (SOTA) performance among GRMs built on the same base model while utilizing a significantly smaller training dataset. Moreover, S2J accomplishes this through self-evolution without relying on more powerful external models for distillation.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.930841",
        "filter_reason": "这篇论文的核心是研究生成式奖励模型(GRMs)中的\"solve-to-judge gap\"问题，并提出S2J方法来解决这个问题。从本质上看，论文致力于改进大语言模型的基础能力，特别是增强其问题解决和判断能力之间的联系，这直接属于提升LLM通用推理能力的范畴。论文涉及多个正面指标，包括明确提到large language models (LLMs)、关注problem-solving能力以及采用self-evolution训练方法。同时，论文不符合任何排除标准，没有专注于多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文提出的S2J方法通过同时利用模型在单个输出上的解决和判断能力进行监督，明确连接GRMs的问题解决和评估能力，从而提高模型的判断性能，这可以视为提升模型内在推理能力的新方法。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#57",
        "title": "Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity",
        "link": "/arxiv/2509.22054",
        "arxiv_id": "2509.22054",
        "authors": "Ping Chen, Xiang Liu, Zhaoxiang Liu, Zezhou Chen, Xingpeng Zhang, Huan Hu, Zipeng Wang, Kai Wang, Shuming Shi, Shiguo Lian",
        "summary": "With the rapid advancement of large language models (LLMs), natural language processing (NLP) has achieved remarkable progress. Nonetheless, significant challenges remain in handling texts with ambiguity, polysemy, or uncertainty. We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM semantic priors with continuous fuzzy membership degrees, creating an explicit interaction between probability-based reasoning and fuzzy membership reasoning. This transition allows ambiguous inputs to be gradually transformed into clear and interpretable decisions while capturing conflicting or uncertain signals that traditional probability-based methods cannot. We validate FRC on sentiment analysis tasks, where both theoretical analysis and empirical results show that it ensures stable reasoning and facilitates knowledge transfer across different model scales. These findings indicate that FRC provides a general mechanism for managing subtle and ambiguous expressions with improved interpretability and robustness.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.938958",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出Fuzzy Reasoning Chain (FRC)框架，一种创新性的推理方法，用于增强大语言模型处理模糊性、歧义和不确定性的能力。论文本质上是改进LLM的基础推理能力，特别是处理不确定性文本的推理机制，而非将LLM作为工具应用于特定领域。FRC框架整合了LLM语义先验与连续模糊隶属度，创建了概率推理与模糊隶属度推理之间的显式交互，这属于提升LLM通用推理能力的研究。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确涉及Large language models (LLMs) - 能力方向：直接关注reasoning，特别是处理模糊性和不确定性的推理能力，这是通用推理的重要组成部分 - 论文提到\"stable reasoning\"和\"knowledge transfer\"，表明其关注推理能力的稳定性和泛化性 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然在情感分析任务上验证了FRC，但情感分析被视为NLP中的通用任务，而非特定应用领域。论文核心是提出通用推理框架，而非专注于特定领域应用 - 不主要聚焦于模型可靠性方面的水印、安全等问题 第四步：特殊和模糊情况 论文涉及可解释性方面，但这是作为FRC框架提升推理质量的内在特性提出的，而非单纯的应用层面讨论。论文通过改进可解释性来增强推理能力，符合保留标准。 综合判断：这篇论文提出了一个旨在增强LLM通用推理能力的新框架(FRC)，特别针对处理模糊性和不确定性这一推理核心挑战，完全符合\"大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#60",
        "title": "GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation",
        "link": "/arxiv/2509.22009",
        "arxiv_id": "2509.22009",
        "authors": "Cehao Yang, Xiaojun Wu, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Yuanliang Sun, Jia Li, Hui Xiong, Jian Guo",
        "summary": "Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in LLMs by structurally modeling knowledge through graph-based representations. However, existing GraphRAG approaches face two core limitations: shallow retrieval that fails to surface all critical evidence, and inefficient utilization of pre-constructed structural graph data, which hinders effective reasoning from complex queries. To address these challenges, we propose \\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel retrieval for GraphRAG. \\textsc{GraphSearch} organizes the retrieval process into a modular framework comprising six modules, enabling multi-turn interactions and iterative reasoning. Furthermore, \\textsc{GraphSearch} adopts a dual-channel retrieval strategy that issues semantic queries over chunk-based text data and relational queries over structural graph data, enabling comprehensive utilization of both modalities and their complementary strengths. Experimental results across six multi-hop RAG benchmarks demonstrate that \\textsc{GraphSearch} consistently improves answer accuracy and generation quality over the traditional strategy, confirming \\textsc{GraphSearch} as a promising direction for advancing graph retrieval-augmented generation.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.940514",
        "filter_reason": "这篇论文的核心贡献是提出GraphSearch，一种通用的智能体深度搜索工作流，用于增强LLM的检索增强生成能力。从本质上看，论文不是将LLM应用到特定领域，而是提出了一种通用方法来增强LLM的基础推理能力，特别是事实推理和多跳推理能力。论文明确提到了\"agentic\"（智能体）概念，属于llm-based agents的新兴范式，符合\"增强其逻辑、多步推理等通用能力\"的标准。论文采用了双通道检索策略，通过模块化框架实现多轮交互和迭代推理，这些都是提升LLM通用推理能力的方法论研究。论文不涉及多模态与视觉、特定应用领域或模型可靠性等排除标准的内容。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#62",
        "title": "MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation",
        "link": "/arxiv/2509.21978",
        "arxiv_id": "2509.21978",
        "authors": "Xinping Lei, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao",
        "summary": "Large Language Models (LLMs) hold substantial potential for accelerating academic ideation but face critical challenges in grounding ideas and mitigating confirmation bias for further refinement. We propose integrating motivational knowledge graphs and socratic dialogue to address these limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework provides essential grounding and practical idea improvement steps for LLM ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node types(problem, challenge and solution) to offer motivation grounding for the LLM ideation process. The Ideator is a dual-agent system utilizing Socratic questioning, which facilitates a rigorous refinement process that mitigates confirmation bias and improves idea quality across novelty, experimental rigor, and motivational rationality dimensions. On the ICLR25 paper topics dataset, MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.941463",
        "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，该论文的本质是提出一种新框架(MotivGraph-SoIQ)来增强LLM的创意生成能力，这属于改进LLM基础能力的研究。论文通过整合动机知识图谱和苏格拉底式对话，提出了一种新的使用范式，增强了LLM的逻辑推理和问题解决能力，符合保留标准。 其次，论文包含多个正面指标：明确以LLMs为核心概念；关注reasoning能力，特别是通过苏格拉底式对话促进逻辑推理；提出了基于llm-based agents和multi-agent系统的新兴范式(Q-Driven Socratic Ideator作为双智能体系统)。 第三，论文不符合排除标准：不涉及多模态与视觉内容；虽然应用于\"学术创意生成\"，但这不是一个特定的应用领域，而是通用学术任务；没有主要聚焦于模型可靠性的应用层面。 最后，在特殊和模糊情况处理上，论文提出的双智能体系统是一种通用的智能体协作框架，用于增强LLM的通用问题解决能力，而非应用于特定领域；同时，论文通过苏格拉底式对话减少确认偏见的方法，属于提升模型通用推理质量和可靠性的新方法。 综合分析，该论文的核心贡献是提出了一种增强LLM创意生成和推理能力的新框架，符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#68",
        "title": "Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts",
        "link": "/arxiv/2509.21892",
        "arxiv_id": "2509.21892",
        "authors": "Naibin Gu, Zhenyu Zhang, Yuchen Feng, Yilong Chen, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang",
        "summary": "Mixture-of-Experts (MoE) models typically fix the number of activated experts $k$ at both training and inference. Intuitively, activating more experts at inference $k'$ (where $k'> k$) means engaging a larger set of model parameters for the computation and thus is expected to improve performance. However, contrary to this intuition, we find the scaling range to be so narrow that performance begins to degrade rapidly after only a slight increase in the number of experts. Further investigation reveals that this degradation stems from a lack of learned collaboration among experts. To address this, we introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that enables MoE models to scale the number of activated experts at inference without incurring additional training overhead. By simultaneously training experts to collaborate in diverse combinations and encouraging the router for high-quality selections, EMoE ensures robust performance across computational budgets at inference. We conduct extensive experiments on various MoE settings. Our results show that EMoE significantly expands the effective performance-scaling range, extending it to as much as 2-3$\\times$ the training-time $k$, while also pushing the model's peak performance to a higher level.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.965615",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进大语言模型的基础能力。论文提出了Elastic Mixture-of-Experts (EMoE)，这是一种新的训练框架，旨在解决MoE模型在推理时扩展激活专家数量导致的性能下降问题。论文的核心贡献是增强专家之间的协作能力，从而提高模型在推理时的性能和可扩展性。这是对LLM本身架构和训练方法的改进，而不是将LLM应用于特定领域，因此符合保留标准。 其次，从正面指标看，论文明确涉及\"Large language models, LLMs\"这一核心概念，研究的是MoE这种大语言模型架构。虽然论文没有直接提及reasoning、planning等能力方向，也没有涉及reinforcement learning等特定训练方法，但通过改进模型架构和专家协作机制，间接可能提升模型的通用推理能力。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，而是专注于模型本身的架构改进。 最后，论文没有涉及需要特殊考虑的模糊情况，如智能体/工具使用或幻觉/可解释性/安全问题。 综上所述，这篇论文通过改进MoE模型的训练框架和专家协作机制，提升了大语言模型的基础能力和推理时的性能可扩展性，符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#70",
        "title": "No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping",
        "link": "/arxiv/2509.21880",
        "arxiv_id": "2509.21880",
        "authors": "Thanh-Long V. Le, Myeongho Jeon, Kim Vu, Viet Lai, Eunho Yang",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward - so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce RL with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.967090",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的强化学习算法(RL-ZVP)，用于改进大语言模型的推理能力。论文明确指出其目标是\"improving the reasoning abilities of Large Language Models (LLMs)\"，并通过解决现有强化学习方法在处理零方差提示时的局限性来增强LLM的推理能力。这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。 其次，从正面指标分析，论文包含多个相关主题： - 核心概念：明确关注大语言模型(LLMs) - 能力方向：特别强调\"reasoning abilities\"，并在六个\"math reasoning benchmarks\"上进行测试 - 训练方法：提出了一种新的强化学习算法，属于强化学习(RL)范畴 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然在数学推理基准上测试，但数学推理被视为通用推理能力的一部分，而非特定应用领域 - 不关注模型可靠性方面的应用层面问题 论文的核心贡献是提出RL-ZVP算法，通过利用零方差提示来增强LLM的强化学习训练过程，从而提高模型的推理能力。这种方法是通用的，可以应用于提升LLM的基础推理能力，而非针对特定领域的应用。因此，这篇论文完全符合研究目标，应该被保留。"
    },
    {
        "index": "#76",
        "title": "ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models",
        "link": "/arxiv/2509.21826",
        "arxiv_id": "2509.21826",
        "authors": "Zihan Lin, Xiaohan Wang, Jie Cao, Jiajun Chai, Guojun Yin, Wei Lin, Ran He",
        "summary": "Large language models (LLMs) transcend passive generation and act as goal-directed agents by invoking external tools. Reinforcement learning (RL) offers a principled framework for optimizing these emergent tool-use policies, yet the prevailing paradigm relies exclusively on sparse outcome rewards and lacks consideration of the particularity of tool-use tasks, inflating policy-gradient variance and resulting in inefficient training. To better understand and address these challenges, we first establish a theoretical link between policy entropy and training stability of tool-use tasks, which reveals that structured, low-entropy tokens are primary determinants of rewards. Motivated by this insight, we propose \\textbf{Res}haped \\textbf{T}oken-level policy gradients (\\textbf{ResT}) for tool-use tasks. ResT reshapes the policy gradient through entropy-informed token reweighting, progressively upweighting reasoning tokens as training proceeds. This entropy-aware scheme enables a smooth shift from structural correctness to semantic reasoning and stabilizes convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows that ResT achieves state-of-the-art results, outperforming prior methods by up to $8.76\\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by $4.11\\%$ on single-turn tasks and $1.50\\%$ on multi-turn base tasks.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.975763",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。以下是我的详细判断过程： 第一步：核心判断——这篇论文的本质是改进LLM的工具使用能力，提出了一种新的训练范式(ResT)来优化LLM在工具使用任务中的表现。论文关注的是增强LLM本身的基础能力(工具使用)，而不是将LLM作为工具应用到特定领域。因此，根据第一步判断，应保留该论文。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确关注大型语言模型(LLMs) - 能力方向：涉及问题解决(problem-solving)能力，工具使用本身就是一种通用推理能力的体现 - 训练方法：使用强化学习(RL)框架优化工具使用策略 - 新兴范式：明确关注工具使用(tool use)，并将LLMs视为目标导向的智能体 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域(如医疗、化学等) - 不主要关注模型可靠性方面的内容(如水印、安全等) 第四步：特殊和模糊情况处理—— 论文提出的工具使用方法是通用的(ResT方法)，旨在增强LLM的通用问题解决能力，而不是针对特定领域的应用。因此，根据筛选标准，应该保留。 综合分析，这篇论文的核心贡献是提出了一种基于强化学习的新方法(ResT)来改进LLM的工具使用能力，这是一种通用推理能力。论文不是将LLM应用到特定领域，而是改进LLM本身的基础能力，完全符合研究目标。"
    },
    {
        "index": "#87",
        "title": "Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval",
        "link": "/arxiv/2509.21710",
        "arxiv_id": "2509.21710",
        "authors": "Xiaojun Wu, Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Yuanliang Sun, Hui Xiong, Jia Li, Jian Guo",
        "summary": "Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important paradigm for enhancing Large Language Models (LLMs) with external knowledge. However, existing approaches face a fundamental trade-off. While graph-based methods are inherently dependent on high-quality graph structures, they face significant practical constraints: manually constructed knowledge graphs are prohibitively expensive to scale, while automatically extracted graphs from corpora are limited by the performance of the underlying LLM extractors, especially when using smaller, local-deployed models. This paper presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these limitations. Our core innovation is the dynamic construction and refinement of a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly incorporates a dual-evolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval. This approach addresses a critical limitation of prior Graph-based RAG methods, which typically construct a static graph index in a single pass without adapting to the actual query. A multi-agent system, comprising Constructor, Retriever, Reflector, and Responser agents, collaboratively engages in an iterative process of evidence retrieval, answer generation, sufficiency reflection, and, crucially, evolving query and subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively build a targeted graph index during reasoning, mitigating the inherent drawbacks of static, one-time graph construction and enabling deep, precise reasoning even with lightweight LLMs. Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework.",
        "subjects": "Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:34.986257",
        "filter_reason": "这篇论文的核心贡献是提出Think-on-Graph 3.0 (ToG-3)框架，通过多智能体上下文进化和检索(MACER)机制来增强大语言模型的推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，提出了一种新的多智能体协作框架和双重进化机制（进化查询和进化子图），以增强LLM的逻辑和多步推理能力，而非将LLM作为工具应用于特定领域。从第二步正面指标看，论文包含了多个相关主题：核心概念涉及LLMs，能力方向聚焦于reasoning，方法上包含evolution机制，并采用了multi-agent systems这一新兴范式。论文不涉及第三步中的任何排除标准（多模态与视觉、特定应用领域、模型可靠性应用层面）。在第四步特殊情况处理中，论文提出的是通用的多智能体协作框架来增强LLM的通用推理能力，而非针对特定领域的应用。综合分析，该论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#99",
        "title": "On Code-Induced Reasoning in LLMs",
        "link": "/arxiv/2509.21499",
        "arxiv_id": "2509.21499",
        "authors": "Abdul Waheed, Zhen Wu, Carolyn Rosé, Daphne Ippolito",
        "summary": "Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. We investigate this question with a systematic, data-centric framework. We construct parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. We then finetune LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, our results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through our systematic framework, we aim to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities.",
        "subjects": "Computation and Language, Programming Languages",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.065422",
        "filter_reason": "这篇论文的核心贡献是研究代码数据如何增强大语言模型(LLM)的推理能力，特别是数学和逻辑推理能力。论文通过系统性的数据中心框架，构建了10种编程语言的并行指令数据集，并应用受控扰动来研究代码的结构和语义属性对LLM推理能力的影响。研究结果表明，LLM对结构扰动的脆弱性高于语义扰动，特别是在数学和代码任务上。这项研究直接关注如何通过代码数据改进LLM的基础推理能力，符合\"改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。论文不是将LLM作为工具应用到特定领域，而是研究如何提升LLM本身的通用推理能力，因此完全符合研究目标。"
    },
    {
        "index": "#101",
        "title": "Learning to Reason with Mixture of Tokens",
        "link": "/arxiv/2509.21482",
        "arxiv_id": "2509.21482",
        "authors": "Adit Jain, Brendan Rappazzo",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a leading approach for improving large language model (LLM) reasoning capabilities. Most current methods follow variants of Group Relative Policy Optimization, which samples multiple reasoning completions, scores them relative to each other, and adjusts the policy accordingly. However, these approaches invariably sample discrete tokens at each reasoning step, discarding the rich distributional information in the model's probability distribution over candidate tokens. While preserving and utilizing this distributional information has proven beneficial in non-RL settings, current RLVR methods seem to be unnecessarily constraining the reasoning search space by not using this information. To address this limitation, we investigate mixture-of-token generation (MoT-G) in RLVR. We present a unified framework that generalizes existing MoT-G approaches, including existing training-free methods that construct mixture embeddings as weighted sums over token embeddings, and extend RLVR to operate directly in this continuous mixture space for generating chain-of-thought. Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive language tasks, we find that MoT--G methods achieve substantial improvements (5--35 \\% gains on 7 out of 10 tasks) compared to standard decoding with the Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of trajectories, suggesting improved training efficiency. Through comprehensive hidden-state and token-level analyses, we provide evidence that MoT--G's benefits may stem from its ability to maintain higher hidden-state entropy throughout the reasoning process and promote exploration in token space.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.072085",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，论文的核心是关于改进LLM的推理能力，具体研究了强化学习与可验证奖励(RLVR)方法，提出了一种名为\"token混合生成\"(MoT-G)的新方法来增强模型的推理过程。论文明确针对LLM在每个推理步骤采样离散token时丢弃分布信息的问题，通过在连续混合空间中操作来生成思维链，这直接提升了模型的基础推理能力。 从正面指标看，论文包含了多个相关主题：核心概念涉及大语言模型(LLMs)，能力方向聚焦于推理(reasoning)，训练方法采用强化学习(RL)的变体，并研究了思维链(Chain-of-Thought)生成。论文在Reasoning-Gym这一推理密集型任务套件上进行了评估，证明了其方法的有效性。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。相反，它专注于提高LLM的通用推理能力，而不是将LLM作为工具应用到特定领域。 综上所述，这篇论文的核心贡献是通过改进token级别的采样策略来增强LLM的推理能力，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#100",
        "title": "Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning",
        "link": "/arxiv/2509.21487",
        "arxiv_id": "2509.21487",
        "authors": "Jillian Xu, Dylan Zhou, Vinay Shukla, Yang Yang, Junrui Ruan, Shuhuai Lin, Wenfei Zou, Yinxiao Liu, Karthik Lakshmanan",
        "summary": "Chain-of-Thought (CoT) prompting often improves classification accuracy, but it introduces a significant throughput penalty with rationale generation (Wei et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for decoder-only language models (LMs) that adds (i) a pooled classification head used during training and inference and (ii) a reasoning head supervised by teacher rationales used only in training. We train with a loss function that is a weighted sum of label cross-entropy and token-level LM loss over input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative gains of 0.65-5.47% over pooled baselines, with notably larger gains on entailment/causal tasks. Since we disable the reasoning head at test time, inference throughput matches pooled classifiers and exceeds CoT decoding on the same backbones by 96-142 times in QPS.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.071340",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是我的详细分析： 第一步：核心判断 这篇论文的本质是提出一种名为\"Dual-Head Reasoning Distillation (DHRD)\"的新训练方法，旨在提高语言模型的推理能力，同时解决思维链(CoT)推理效率低下的问题。论文明确关注改进LLM的基础推理能力，提出了一种新的训练范式，属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的范畴，因此应该保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确涉及\"decoder-only language models (LMs)\" - 能力方向：多次强调\"reasoning\"，特别是\"Chain-of-Thought (CoT)\"和\"reasoning head\"，并在摘要中提到在\"entailment/causal tasks\"上取得更大增益，这些都直接涉及逻辑推理能力 - 训练方法：提出DHRD这一新的训练方法，虽然不是强化学习或进化方法，但确实是一种创新训练范式 第三步：排除标准 论文没有主要聚焦于任何排除标准中的领域： - 不涉及多模态与视觉内容 - 实验在通用SuperGLUE任务上进行，而非特定应用领域 - 不关注模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况。 综上所述，这篇论文的核心贡献是提出一种新的训练方法来增强LLM的推理能力，同时保持推理效率，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文关注的是LLM基础能力的改进，而非将其作为工具应用于特定领域，因此应该被保留。"
    },
    {
        "index": "#106",
        "title": "How Large Language Models Need Symbolism",
        "link": "/arxiv/2509.21404",
        "arxiv_id": "2509.21404",
        "authors": "Xiaotie Deng, Hanyu Li",
        "summary": "We argue that AI's future requires more than scaling. To unlock genuine discovery, large language models need a compass: human-crafted symbols to guide their powerful but blind intuition.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.074858",
        "filter_reason": "这篇论文的核心是探讨如何通过符号系统(symbolism)来增强大语言模型(LLM)的推理能力，而不是将LLM应用于特定领域。从摘要中可以看出，论文认为AI的未来不仅仅是扩大规模，还需要人类设计的符号来指导LLM强大的但\"盲目\"的直觉，以实现真正的发现。这符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。论文没有涉及多模态与视觉、特定应用领域或模型可靠性等排除标准中的内容。虽然摘要中没有明确提到\"reasoning\"、\"planning\"等具体能力方向，但\"unlock genuine discovery\"暗示了与推理和问题解决相关的内容。因此，这篇论文应该被保留，因为它关注的是改进LLM的基础能力，提出一种新的范式（结合符号系统）来增强其通用推理能力。"
    },
    {
        "index": "#109",
        "title": "A Novel Differential Feature Learning for Effective Hallucination Detection and Classification",
        "link": "/arxiv/2509.21357",
        "arxiv_id": "2509.21357",
        "authors": "Wenkai Wang, Vincent Lee, Yizhen Zheng",
        "summary": "Large language model hallucination represents a critical challenge where outputs deviate from factual accuracy due to distributional biases in training data. While recent investigations establish that specific hidden layers exhibit differences between hallucinatory and factual content, the precise localization of hallucination signals within layers remains unclear, limiting the development of efficient detection methods. We propose a dual-model architecture integrating a Projected Fusion (PF) block for adaptive inter-layer feature weighting and a Differential Feature Learning (DFL) mechanism that identifies discriminative features by computing differences between parallel encoders learning complementary representations from identical inputs. Through systematic experiments across HaluEval's question answering, dialogue, and summarization datasets, we demonstrate that hallucination signals concentrate in highly sparse feature subsets, achieving significant accuracy improvements on question answering and dialogue tasks. Notably, our analysis reveals a hierarchical \"funnel pattern\" where shallow layers exhibit high feature diversity while deep layers demonstrate concentrated usage, enabling detection performance to be maintained with minimal degradation using only 1\\% of feature dimensions. These findings suggest that hallucination signals are more concentrated than previously assumed, offering a pathway toward computationally efficient detection systems that could reduce inference costs while maintaining accuracy.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-09-20",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.076261",
        "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是关于改进大语言模型的基础能力，特别是解决幻觉问题，而不是将LLM作为工具应用到特定领域。论文提出的差分特征学习机制旨在检测和分类幻觉内容，这直接关系到LLM输出的事实准确性，是提升模型基础能力的重要方面。 其次，论文符合正面指标中的\"核心概念\"，明确讨论了大语言模型的幻觉问题。虽然论文没有直接涉及推理、规划等能力方向，但幻觉检测与这些能力密切相关，因为幻觉往往发生在模型进行复杂推理过程中，减少幻觉可以间接提高模型的推理质量和可靠性。 第三，论文不涉及排除标准中的任何领域，它既不是关于多模态与视觉，也不是将LLM应用到特定领域，也不是从应用层面讨论模型可靠性。 最后，根据第四步对特殊情况的处理，论文提出了一种新方法来检测幻觉，属于\"幻觉/可解释性/安全\"类别，并且是从模型内部机制的角度进行研究，目的是提升模型的通用可靠性，因此应该保留。 论文的核心贡献是提出了一种双模型架构和差分特征学习机制，用于检测和分类LLM的幻觉内容，并发现幻觉信号集中在高度稀疏的特征子集中。这些发现有助于开发计算高效的检测系统，减少推理成本同时保持准确性，从而提升LLM的通用推理能力和输出质量，符合研究目标。"
    },
    {
        "index": "#113",
        "title": "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback",
        "link": "/arxiv/2509.22633",
        "arxiv_id": "2509.22633",
        "authors": "Gen Li, Yuling Yan",
        "summary": "Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner. By examining existing optimism-based exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated by this insight, we propose a new exploration scheme that directs preference queries toward reducing uncertainty in reward differences most relevant to policy improvement. Under a multi-armed bandit model of RLHF, we establish regret bounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter that balances reward maximization against mitigating distribution shift. To our knowledge, this is the first online RLHF algorithm with regret scaling polynomially in all model parameters.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning, Statistics Theory",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.123045",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文本质上是关于改进强化学习与人类反馈(RLHF)的在线探索效率。RLHF是提高大语言模型与人类偏好对齐的核心训练范式，属于LLM基础能力的改进。论文提出的新探索方案旨在更有效地收集偏好数据以改进奖励模型和策略，这直接关系到提升LLM的通用能力，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文明确包含核心概念\"Large language models (LLMs)\"，并专注于\"Reinforcement learning with human feedback (RLHF)\"这一重要训练方法。虽然摘要未直接提及推理、规划等能力，但RLHF作为对齐方法，能够间接提升这些通用能力。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域，而是关注通用的RLHF方法改进。 第四步特殊和模糊情况：论文不涉及智能体/工具使用或幻觉/可解释性/安全等需要特殊判断的内容。 综合分析，这篇论文的核心贡献是提出了一种新的RLHF在线探索方案，通过更有效地收集偏好数据来改进奖励模型和策略，从而提升LLM与人类偏好的对齐效果。这属于改进LLM基础训练范式的研究，直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#118",
        "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning",
        "link": "/arxiv/2509.22576",
        "arxiv_id": "2509.22576",
        "authors": "Xu Wujiang, Wentian Zhao, Zhenting Wang, Li Yu-Jhe, Jin Can, Jin Mingyu, Mei Kai, Wan Kun, Metaxas Dimitris",
        "summary": "Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.131868",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种名为EPO（熵正则化策略优化）的新框架，用于解决LLM智能体在强化学习中的训练问题，特别是针对多轮环境中稀疏奖励的挑战。这明显属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标：核心概念上明确关注\"LLM Agents\"；能力方向上涉及LLM智能体在多轮环境中的决策和推理，与reasoning和problem-solving相关；训练方法上提出了强化学习方法（EPO）；新兴范式上属于llm-based agents的研究。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉问题；虽然使用了ScienceWorld和ALFWorld作为测试环境，但这些是通用智能体测试平台，而非特定应用领域；也没有关注模型可靠性方面的水印、安全等问题。 最后，在特殊和模糊情况处理上，论文提出的EPO框架是一种通用的智能体强化学习方法，旨在增强LLM智能体在多轮环境中的通用问题解决能力，而非针对特定领域的应用。 论文的核心贡献是提出了一种解决LLM智能体在多轮稀疏奖励环境中训练挑战的新方法，通过熵正则化策略优化来改善探索-利用平衡，从而提升LLM智能体的通用推理和决策能力，这与研究目标高度一致。"
    },
    {
        "index": "#115",
        "title": "IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning",
        "link": "/arxiv/2509.22621",
        "arxiv_id": "2509.22621",
        "authors": "Aayush Mishra, Daniel Khashabi, Anqi Liu",
        "summary": "Supervised Fine-Tuning (SFT) is used to specialize model behavior by training weights to produce intended target responses for queries. In contrast, In-Context Learning (ICL) adapts models during inference with instructions or demonstrations in the prompt. ICL can offer better generalizability and more calibrated responses compared to SFT in data scarce settings, at the cost of more inference compute. In this work, we ask the question: Can ICL's internal computations be used to improve the qualities of SFT? We first show that ICL and SFT produce distinct activation patterns, indicating that the two methods achieve adaptation through different functional mechanisms. Motivated by this observation and to use ICL's rich functionality, we introduce ICL Activation Alignment (IA2), a self-distillation technique which aims to replicate ICL's activation patterns in SFT models and incentivizes ICL-like internal reasoning. Performing IA2 as a priming step before SFT significantly improves the accuracy and calibration of model outputs, as shown by our extensive empirical results on 12 popular benchmarks and 2 model families. This finding is not only practically useful, but also offers a conceptual window into the inner mechanics of model adaptation.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.124579",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式IA2（ICL Activation Alignment），通过将上下文学习(ICL)的激活模式对齐到监督微调(SFT)过程中，来增强模型的内部推理能力。这属于改进LLM的基础能力和提出新的训练范式的研究，旨在增强模型的推理能力，符合保留标准。 其次，从正面指标分析，论文明确包含以下相关主题： 1. 核心概念：研究大语言模型(LLMs)的SFT和ICL技术 2. 能力方向：明确提到\"incentivizes ICL-like internal reasoning\"（鼓励类似ICL的内部推理），直接关注提升模型的推理能力 3. 训练方法：提出了一种自我蒸馏技术(self-distillation technique)，属于新的训练方法范畴 第三，论文不符合任何排除标准： 1. 不涉及多模态与视觉内容 2. 不聚焦于任何特定应用领域（如医疗、化学等） 3. 虽然提到了模型的\"校准性\"(calibration)，但这不是其主要焦点，而是作为改进的一个方面 最后，论文没有涉及特殊或模糊的情况，如智能体/工具使用或幻觉/可解释性/安全等主题。 论文的核心贡献是通过激活对齐技术，将ICL的内部推理优势转移到SFT模型中，从而提高模型的推理能力和输出质量。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#119",
        "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time",
        "link": "/arxiv/2509.22572",
        "arxiv_id": "2509.22572",
        "authors": "Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang",
        "summary": "Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.132368",
        "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。根据筛选标准的分析： 第一步核心判断：论文本质上是关于改进LLM的基础推理能力，提出了一种名为\"Dynamic Experts Search (DES)\"的测试时缩放策略，通过动态控制混合专家(MoE)模型中激活专家的数量来增强模型的推理能力。这直接对应了\"改进LLM的基础能力\"和\"增强其逻辑、数学、多步推理等通用能力\"的研究目标。 第二步正面指标：论文明确包含两个关键正面指标：(1)核心概念LLMs，研究的是混合专家大语言模型；(2)推理能力方向，标题直接提及\"Enhancing Reasoning\"，并在实验中测试了数学、代码和知识推理基准。虽然论文未涉及强化学习训练方法和智能体等新兴范式，但已足够表明其与研究课题高度相关。 第三步排除标准：论文不符合任何排除标准。它不涉及多模态与视觉内容，不专注于特定应用领域（数学、代码和知识推理是通用能力测试而非特定领域应用），也不关注模型可靠性方面的水印、安全等问题。 论文的核心贡献是提出了一种利用模型架构特性（专家激活）来增强LLM推理能力的新方法，这种方法不需要额外计算成本就能提高模型在各种推理任务上的准确性和稳定性，完全符合提高LLM本身通用推理能力的研究目标。"
    },
    {
        "index": "#124",
        "title": "Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers",
        "link": "/arxiv/2509.22445",
        "arxiv_id": "2509.22445",
        "authors": "Peter Shaw, James Cohan, Jacob Eisenstein, Kristina Toutanova",
        "summary": "The Minimum Description Length (MDL) principle offers a formal framework for applying Occam's razor in machine learning. However, its application to neural networks such as Transformers is challenging due to the lack of a principled, universal measure for model complexity. This paper introduces the theoretical notion of asymptotically optimal description length objectives, grounded in the theory of Kolmogorov complexity. We establish that a minimizer of such an objective achieves optimal compression, for any dataset, up to an additive constant, in the limit as model resource bounds increase. We prove that asymptotically optimal objectives exist for Transformers, building on a new demonstration of their computational universality. We further show that such objectives can be tractable and differentiable by constructing and analyzing a variational objective based on an adaptive Gaussian mixture prior. Our empirical analysis shows that this variational objective selects for a low-complexity solution with strong generalization on an algorithmic task, but standard optimizers fail to find such solutions from a random initialization, highlighting key optimization challenges. More broadly, by providing a theoretical framework for identifying description length objectives with strong asymptotic guarantees, we outline a potential path towards training neural networks that achieve greater compression and generalization.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.140370",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础能力，而非将其作为工具应用于特定领域。论文提出了基于Kolmogorov复杂度的渐近最优描述长度目标，这是一种新的理论框架，旨在优化Transformer模型的复杂度和泛化能力。这属于\"改进LLM的基础能力\"的范畴，与思维链、强化学习等方法论研究类似，都是致力于提升模型内在能力的理论探索。 其次，从正面指标分析，论文明确涉及Transformers这一大语言模型的核心架构，讨论了泛化能力（这是通用推理能力的基础），并提出了新的变分目标函数作为训练/优化方法。虽然论文没有直接讨论数学推理或逻辑推理，但优化模型复杂度和泛化能力是提升通用推理能力的重要基础。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是建立了一个理论框架，用于训练具有更强压缩和泛化能力的神经网络，这直接关系到提升LLM的通用推理能力。通过优化模型的复杂度和泛化能力，可以间接提升模型在各种推理任务上的表现。因此，这篇论文符合研究目标，应该被保留。"
    },
    {
        "index": "#132",
        "title": "A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning",
        "link": "/arxiv/2509.22044",
        "arxiv_id": "2509.22044",
        "authors": "Ziqi Wang, Boye Niu, Zhongli Li, Linghui Meng, Jing Liu, Zhi Zheng, Tong Xu, Hua Wu, Haifeng Wang, Enhong Chen",
        "summary": "Recent Large Reasoning Models have achieved significant improvements in complex task-solving capabilities by allocating more computation at the inference stage with a \"thinking longer\" paradigm. Even as the foundational reasoning capabilities of models advance rapidly, the persistent gap between a model's performance in a single attempt and its latent potential, often revealed only across multiple solution paths, starkly highlights the disparity between its realized and inherent capabilities. To address this, we present A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge the gap between a model's potential and its actual performance. In this framework, an \"explorer\" model first generates potential solutions in parallel through repeated sampling. Subsequently,a \"synthesizer\" model integrates these references for a more refined, second stage of reasoning. This two-stage process allows computation to be scaled orthogonally to existing sequential methods. Our work makes two key innovations: First, we present A2R as a plug-and-play parallel reasoning framework that explicitly enhances a model's capabilities on complex questions. For example, using our framework, the Qwen3-8B-distill model achieves a 75% performance improvement compared to its self-consistency baseline. Second, through a systematic analysis of the explorer and synthesizer roles, we identify an effective asymmetric scaling paradigm. This insight leads to A2R-Efficient, a \"small-to-big\" variant that combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration surpasses the average performance of a monolithic Qwen3-32B model at a nearly 30% lower cost. Collectively, these results show that A2R is not only a performance-boosting framework but also an efficient and practical solution for real-world applications.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.145663",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种名为A2R的非对称两阶段推理框架，旨在提升大语言模型的基础推理能力。论文明确针对的是模型在单次尝试中的表现与其潜在能力之间的差距，通过\"探索者\"和\"综合者\"两个模型的协作来增强LLM的通用推理能力，而不是将LLM作为工具应用于特定领域。 其次，论文包含多个正面指标： - 核心概念：明确研究Large Reasoning Models，使用Qwen3模型作为实验对象 - 能力方向：直接聚焦于推理(reasoning)能力的提升，标题和摘要多次强调\"reasoning\" - 新兴范式：提出了\"explorer\"和\"synthesizer\"的协作框架，可视为一种多智能体系统的简化形式 第三，论文不涉及任何排除标准中的领域： - 没有多模态或视觉相关内容 - 没有针对医疗、化学、生物等特定应用领域 - 没有讨论水印、安全性等模型可靠性问题 在特殊和模糊情况处理上，论文提出的两阶段框架是一种通用的推理增强方法，而非针对特定领域的应用，因此应该保留。 论文的核心贡献是提出了一种可插拔的并行推理框架，通过非对称扩展模型计算能力来提升LLM在复杂问题上的表现，这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标。实验结果表明，该框架能显著提升模型性能（如Qwen3-8B-distill模型实现75%性能提升），同时提供了一种高效的计算扩展方式。"
    },
    {
        "index": "#133",
        "title": "The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging",
        "link": "/arxiv/2509.22034",
        "arxiv_id": "2509.22034",
        "authors": "Xiaochong Lan, Yu Zheng, Shiteng Cao, Yong Li",
        "summary": "The growing demand for large language models (LLMs) with tunable reasoning capabilities in many real-world applications highlights a critical need for methods that can efficiently produce a spectrum of models balancing reasoning depth and computational cost. Model merging has emerged as a promising, training-free technique to address this challenge by arithmetically combining the weights of a general-purpose model with a specialized reasoning model. While various merging techniques exist, their potential to create a spectrum of models with fine-grained control over reasoning abilities remains largely unexplored. This work presents a large-scale empirical study evaluating a range of model merging techniques across multiple reasoning benchmarks. We systematically vary merging strengths to construct accuracy-efficiency curves, providing the first comprehensive view of the tunable performance landscape. Our findings reveal that model merging offers an effective and controllable method for calibrating the trade-off between reasoning accuracy and token efficiency, even when parent models have highly divergent weight spaces. Crucially, we identify instances of Pareto Improvement, where a merged model achieves both higher accuracy and lower token consumption than one of its parents. Our study provides the first comprehensive analysis of this tunable space, offering practical guidelines for creating LLMs with specific reasoning profiles to meet diverse application demands.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.146141",
        "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究如何通过模型合并(model merging)技术来调整和增强大语言模型本身的推理能力，而不是将LLM作为工具应用到特定领域。论文提出的方法允许在推理深度和计算成本之间进行权衡，这属于改进LLM基础能力的范畴。 其次，论文符合多个正面指标：它明确研究大语言模型(LLMs)，并特别关注\"tunable reasoning capabilities\"，在多个推理基准上进行评估，这直接对应了核心概念和能力方向这两个正面指标。 第三，论文不符合任何排除标准：它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，论文不涉及特殊或模糊情况，它直接关注如何通过模型合并来调整和增强LLM的通用推理能力，而不是将智能体/工具应用在特定领域，也不是对幻觉/可解释性/安全的社会学研究或应用层面讨论。 论文的核心贡献是提供了一种通过模型合并技术来调整LLM推理能力的方法，使研究者能够在推理准确性和计算效率之间进行权衡，这直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#126",
        "title": "PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning",
        "link": "/arxiv/2509.22315",
        "arxiv_id": "2509.22315",
        "authors": "Hieu Tran, Zonghai Yao, Nguyen Luong Tran, Zhichao Yang, Feiyun Ouyang, Shuo Han, Razieh Rahimi, Hong Yu",
        "summary": "Inspired by the dual-process theory of human cognition from \\textit{Thinking, Fast and Slow}, we introduce \\textbf{PRIME} (Planning and Retrieval-Integrated Memory for Enhanced Reasoning), a multi-agent reasoning framework that dynamically integrates \\textbf{System 1} (fast, intuitive thinking) and \\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick Thinking Agent (System 1) to generate a rapid answer; if uncertainty is detected, it then triggers a structured System 2 reasoning pipeline composed of specialized agents for \\textit{planning}, \\textit{hypothesis generation}, \\textit{retrieval}, \\textit{information integration}, and \\textit{decision-making}. This multi-agent design faithfully mimics human cognitive processes and enhances both efficiency and accuracy. Experimental results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to perform competitively with state-of-the-art closed-source models like GPT-4 and GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This research establishes PRIME as a scalable solution for improving LLMs in domains requiring complex, knowledge-intensive reasoning.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.141438",
        "filter_reason": "这篇论文的核心贡献是提出PRIME（Planning and Retrieval-Integrated Memory for Enhanced Reasoning），一个多智能体推理框架，通过动态整合快速思考（System 1）和慢速思考（System 2）来增强大语言模型的推理能力。该框架包含专门的智能体进行规划、假设生成、检索、信息整合和决策，明显属于改进LLM基础推理能力的研究。论文专注于提高LLM在多跳和基于知识的推理任务上的表现，这正是\"通用推理能力\"的核心要素。同时，论文提出的多智能体协作框架是一种新兴范式，符合筛选标准中的正面指标。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究目标。"
    },
    {
        "index": "#144",
        "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios",
        "link": "/arxiv/2509.21766",
        "arxiv_id": "2509.21766",
        "authors": "Haotian Luo, Huaisong Zhang, Xuelin Zhang, Haoyu Wang, Zeyu Qin, Wenjie Lu, Guozheng Ma, Haiying He, Yingsha Xie, Qiyang Zhou, Zixuan Hu, Hongze Mi, Yibo Wang, Naiqiang Tan, Hong Chen, Yi R. Fung, Chun Yuan, Li Shen",
        "summary": "Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce \\textbf{UltraHorizon} a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average \\textbf{200k+} tokens and \\textbf{400+} tool calls, whereas in standard configurations they still exceed \\textbf{35k} tokens and involve more than \\textbf{60} tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. \\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available here.}",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-09-26",
        "category": "cs.CL",
        "crawl_time": "2025-10-06T19:08:35.157848",
        "filter_reason": "这篇论文的核心贡献是提出了UltraHorizon基准测试，用于评估LLM智能体在超长期场景中的能力，包括持续推理、规划、记忆管理和工具使用等通用能力。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从本质上看，论文并非将LLM作为工具应用到特定领域，而是关注LLM本身在通用推理能力方面的表现和不足。论文提出的基准测试旨在测量LLM智能体在复杂现实世界挑战中的基础能力，特别是长期推理、规划和工具使用等通用能力，这符合第一步中\"改进LLM的基础能力\"的保留标准。 其次，论文包含多个正面指标：明确关注LLM-agents（核心概念），涉及reasoning、planning、problem-solving（能力方向），以及tool use（新兴范式）。这些都是与研究目标\"提高大语言模型的通用推理能力\"直接相关的主题。 第三，论文不主要聚焦于排除标准中提到的领域。虽然论文提到了\"大规模软件开发、商业投资和科学发现\"等现实世界任务，但这些只是作为例子来说明其基准测试的应用场景，而不是将LLM应用到特定领域进行研究。 最后，在处理特殊和模糊情况方面，论文提出的是一个通用的基准测试，用于评估LLM智能体在长期任务中的能力，而不是将智能体/工具应用在特定领域。论文分析了LLM在这些长期任务中的局限性，并识别出导致失败的原因，这有助于理解如何提高LLM的通用推理能力。 综上所述，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。"
    },
    {
        "index": "#10",
        "title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework",
        "link": "/arxiv/2509.22624",
        "arxiv_id": "2509.22624",
        "authors": "Ziyu Liu, Yuhang Zang, Shengyuan Ding, Yuhang Cao, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang",
        "summary": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback (RLHF) for subjective tasks. However, RLHF incurs high costs and potential reward-policy mismatch due to reliance on human preferences, while RLVR still wastes supervision by discarding rollouts and correctness signals after each update. To address these challenges, we introduce the Synergistic Policy And Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable method that builds on RLVR. Instead of discarding rollouts and correctness data, SPARK recycles this valuable information to simultaneously train the model itself as a generative reward model. This auxiliary training uses a mix of objectives, such as pointwise reward score, pairwise comparison, and evaluation conditioned on further-reflection responses, to teach the model to evaluate and improve its own responses. Our process eliminates the need for a separate reward model and costly human preference data. SPARK creates a positive co-evolving feedback loop: improved reward accuracy yields better policy gradients, which in turn produce higher-quality rollouts that further refine the reward model. Our unified framework supports test-time scaling via self-reflection without external reward models and their associated costs. We show that SPARK achieves significant performance gains on multiple LLM and LVLM models and multiple reasoning, reward models, and general benchmarks. For example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks, 12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the baselines, demonstrating robustness and broad generalization.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.CV",
        "crawl_time": "2025-10-06T19:08:35.150257",
        "filter_reason": "这篇论文的核心贡献是提出SPARK（协同策略与奖励共同进化框架），这是一种新的强化学习训练范式，用于改进大语言模型(LLM)和大视觉语言模型(LVLM)的训练方法。从本质上看，论文专注于改进LLM的基础能力和训练范式，符合\"致力于提高大语言模型本身的通用推理能力\"的核心目标。SPARK通过回收rollouts和正确性数据，同时训练模型本身作为生成式奖励模型，创建了一个积极的共同进化反馈循环：提高奖励准确性产生更好的策略梯度，进而产生更高质量的rollouts，进一步优化奖励模型。论文在多个推理基准上展示了显著的性能提升（如SPARK-VL-7B在7个推理基准上平均提高9.7%），表明该方法有效增强了模型的推理能力。虽然论文提到了LVLMs，但这只是作为应用场景之一，而非主要焦点。论文没有聚焦于任何特定应用领域或多模态研究，而是提出了一种通用的训练方法来提高LLM的推理能力，完全符合研究范围。"
    },
    {
        "index": "#4",
        "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning",
        "link": "/arxiv/2509.22611",
        "arxiv_id": "2509.22611",
        "authors": "Junkang Wu, Kexin Huang, Jiancan Wu, An Zhang, Xiang Wang, Xiangnan He",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning, but training often oscillates between {entropy collapse} and {entropy explosion}. We trace both hazards to the mean baseline used in value-free RL (e.g., GRPO and DAPO), which improperly penalizes negative-advantage samples under reward outliers. We propose {Quantile Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile baseline. QAE induces a response-level, two-regime gate: on hard queries (p <= 1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it targets remaining failures. Under first-order softmax updates, we prove {two-sided entropy safety}, giving lower and upper bounds on one-step entropy change that curb explosion and prevent collapse. Empirically, this minimal modification stabilizes entropy, sparsifies credit assignment (with tuned K, roughly 80% of responses receive zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results identify {baseline design} -- rather than token-level heuristics -- as the primary mechanism for scaling RLVR.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.161717",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Quantile Advantage Estimation\" (QAE)的新方法，用于解决强化学习与可验证奖励(RLVR)在训练大语言模型推理能力时出现的熵崩溃和熵爆炸问题。论文通过用基于分组的K-分位数基线替代均值基线，实现了双向熵安全性，从而稳定了训练过程并提升了LLM的推理能力。这完全符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的要求。论文在多个数学推理基准测试(AIME和AMC)上验证了方法的有效性，表明其关注的是通用推理能力的提升，而非特定领域的应用。此外，论文涉及的核心概念(LLMs)、能力方向(reasoning, 特别是math reasoning)和训练方法(reinforcement learning)都是正面指标，且不涉及任何排除标准中的领域。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#71",
        "title": "Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization",
        "link": "/arxiv/2509.22115",
        "arxiv_id": "2509.22115",
        "authors": "Chao Wang, Tao Yang, Hongtao Tian, Yunsheng Shi, Qiyao Ma, Xiaotao Liu, Ting Yao, Wenbo Ding",
        "summary": "Critic-free methods like GRPO reduce memory demands by estimating advantages from multiple rollouts but tend to converge slowly, as critical learning signals are diluted by an abundance of uninformative samples and tokens. To tackle this challenge, we propose the \\textbf{Dynamic Dual-Level Down-Sampling (D$^3$S)} framework that prioritizes the most informative samples and tokens across groups to improve the efficient of policy optimization. D$^3$S operates along two levels: (1) the sample-level, which selects a subset of rollouts to maximize advantage variance ($\\text{Var}(A)$). We theoretically proven that this selection is positively correlated with the upper bound of the policy gradient norms, yielding higher policy gradients. (2) the token-level, which prioritizes tokens with a high product of advantage magnitude and policy entropy ($|A_{i,t}|\\times H_{i,t}$), focusing updates on tokens where the policy is both uncertain and impactful. Moreover, to prevent overfitting to high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by curriculum learning. This schedule starts with aggressive down-sampling to accelerate early learning and gradually relaxes to promote robust generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that integrating D$^3$S into advanced RL algorithms achieves state-of-the-art performance and generalization while requiring \\textit{fewer} samples and tokens across diverse reasoning benchmarks. Our code is added in the supplementary materials and will be made publicly available.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.226763",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Dynamic Dual-Level Down-Sampling (D³S)\"的框架，用于提高大语言模型在强化学习训练过程中的策略优化效率。该方法通过在样本和token两个层面进行动态下采样，优先选择信息量最大的数据，从而加速模型学习过程。论文在Qwen2.5和Llama3.1等大语言模型上进行了实验，并证明该方法在多样化的推理基准测试中取得了先进的性能，同时需要更少的样本和token。 这篇论文完全符合研究目标，因为它致力于提高LLM本身的通用推理能力，具体体现在：(1)论文本质上是改进LLM的基础能力，提出新的训练范式来优化强化学习过程；(2)论文明确关注reasoning能力，在多种推理基准上进行了验证；(3)论文采用了reinforcement learning方法，属于筛选标准中的正面指标；(4)论文没有将LLM作为工具应用于特定领域，而是提出了一种通用的优化方法；(5)论文不属于任何排除标准中的领域。因此，这篇论文应该被保留。"
    },
    {
        "index": "#86",
        "title": "Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error",
        "link": "/arxiv/2509.22023",
        "arxiv_id": "2509.22023",
        "authors": "Panagiotis Giannoulis, Yorgos Pantis, Christos Tzamos",
        "summary": "Despite their proficiency in various language tasks, Large Language Models (LLMs) struggle with combinatorial problems like Satisfiability, Traveling Salesman Problem, or even basic arithmetic. We address this gap through a novel approach for solving problems in the class NP. We focus on the paradigmatic task of Sudoku and achieve state-of-the-art accuracy (99\\%) compared to prior neuro-symbolic approaches. Unlike prior work that used custom architectures, our method employs a vanilla decoder-only Transformer (GPT-2) without external tools or function calling. Our method integrates imitation learning of simple Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy involving informed guessing and backtracking. Moving beyond imitation learning, we seek to minimize the number of guesses until reaching a solution. We provide a rigorous analysis of this setup formalizing its connection to a contextual variant of Min-Sum Set Cover, a well-studied problem in algorithms and stochastic optimization.",
        "subjects": "Machine Learning",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.239070",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础推理能力。论文明确指出LLMs在解决组合问题方面存在困难，并提出了一种新方法来增强这种能力。具体来说，论文结合了模仿学习和深度优先搜索策略，使普通的Transformer模型（GPT-2）能够高效解决组合问题，这属于提升LLM通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标： - 核心概念：明确提到\"Large Language Models (LLMs)\"和\"Transformer (GPT-2)\" - 能力方向：专注于解决组合问题的推理能力，属于逻辑推理和问题解决能力 - 训练方法：提出了结合模仿学习和显式深度优先搜索的新训练范式 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文使用数独作为具体任务，但这是为了展示方法在解决NP类问题上的通用性，而非针对特定领域应用。 最后，论文的核心贡献是提出了一种增强LLM解决组合问题能力的新方法，通过结合模仿学习和深度优先搜索策略，使模型能够进行有信息猜测和回溯，从而提升其通用推理能力。这完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。"
    },
    {
        "index": "#108",
        "title": "Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration",
        "link": "/arxiv/2509.21848",
        "arxiv_id": "2509.21848",
        "authors": "Taejong Joo, Shu Ishida, Ivan Sosnovik, Bryan Lim, Sahand Rezaei-Shoshtari, Adam Gaier, Robert Giaquinto",
        "summary": "As a model-agnostic approach to long context modeling, multi-agent systems can process inputs longer than a large language model's context window without retraining or architectural modifications. However, their performance often heavily relies on hand-crafted multi-agent collaboration strategies and prompt engineering, which limit generalizability. In this work, we introduce a principled framework that formalizes the model-agnostic long context modeling problem as a compression problem, yielding an information-theoretic compression objective. Building on this framework, we propose Graph of Agents (GoA), which dynamically constructs an input-dependent collaboration structure that maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document question answering benchmarks, GoA improves the average $F_1$ score of retrieval-augmented generation by 5.7\\% and a strong multi-agent baseline using a fixed collaboration structure by 16.35\\%, respectively. Even with only a 2K context window, GoA surpasses the 128K context window Llama 3.1 8B on LongBench, showing a dramatic increase in effective context length. Our source code is available at https://github.com/tjoo512/graph-of-agents.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.270875",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Graph of Agents (GoA)\"的原则性框架，通过动态构建输入依赖的协作结构来增强大语言模型处理长上下文的能力。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从本质上看，论文属于\"提出新的训练范式\"和\"智能体协作框架\"的范畴，旨在提高LLM的基础能力（处理长上下文的能力）。论文不是将LLM作为工具应用到特定领域，而是专注于提升LLM本身的通用能力，符合第一步的核心判断标准。 其次，论文包含多个正面指标：明确涉及大语言模型（Llama 3.1 8B和Qwen3 8B），并提出了多智能体系统（multi-agent systems）这一新兴范式。虽然论文不是直接针对推理、规划等能力方向，但处理长上下文的能力是LLM进行复杂推理和问题解决的基础能力之一。 第三，论文不涉及任何排除标准中的领域：没有涉及多模态与视觉，没有专注于特定应用领域，也没有主要关注模型可靠性问题。 最后，在特殊和模糊情况处理上，论文提出的是一种通用的多智能体协作框架来增强LLM的通用问题解决能力，而不是将智能体应用在特定领域，因此应该保留。 综上所述，这篇论文符合研究目标，因为它致力于通过多智能体协作框架提高大语言模型的基础能力（长上下文处理），这属于提升LLM通用推理能力的范畴。"
    },
    {
        "index": "#116",
        "title": "FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning",
        "link": "/arxiv/2509.21792",
        "arxiv_id": "2509.21792",
        "authors": "Yizhou Zhang, Ning Lv, Teng Wang, Jisheng Dang",
        "summary": "Group relative policy optimization (GRPO) has demonstrated significant potential in improving the reasoning capabilities of large language models (LLMs) via reinforcement learning. However, its practical deployment is impeded by an excessively slow training process, primarily attributed to the computationally intensive autoregressive generation of multiple responses per query, which makes the generation phase the primary performance bottleneck. Although speculative decoding presents a promising direction for acceleration, its direct application in GRPO achieves limited speedup under high-concurrency training conditions. To overcome this limitation, we propose a concurrency-aware speculative decoding framework that dynamically adjusts the drafting and verification strategy according to real-time concurrency levels, thereby maximizing the acceleration of the generation process. Furthermore, to address performance degradation arising from distributional drift between the evolving target model and the fixed draft model during training, we introduce an online draft learning mechanism that enables the draft model to continuously adapt using feedback signals from the target model. Experimental results across multiple mathematical reasoning datasets and models demonstrate that the proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly surpassing baseline approaches in efficiency. The code is available at https://github.com/yedaotian9/GRPO_speculative.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.280298",
        "filter_reason": "这篇论文符合我的研究目标，核心原因在于它专注于改进大语言模型的训练方法以提升其推理能力。具体分析如下： 从第一步核心判断来看，论文本质上是关于改进GRPO（Group relative policy optimization）训练过程的效率，而GRPO本身是一种通过强化学习提高大语言模型推理能力的方法。论文提出的并发感知投机解码框架和在线草稿学习机制，都是为了加速这种训练过程，从而更有效地提升LLM的推理能力。这属于\"改进LLM的基础能力\"和\"提出新的训练范式\"的范畴，而非将LLM作为工具应用到特定领域。 从第二步正面指标来看，论文包含了多个相关主题： - 核心概念：明确涉及大语言模型(LLMs) - 能力方向：直接关注\"improving the reasoning capabilities\"，并在数学推理数据集上验证 - 训练方法：GRPO是一种强化学习方法，属于强化学习优化范畴 从第三步排除标准来看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。虽然实验在数学推理数据集上进行，但论文的核心贡献是加速训练方法，而非解决数学问题本身。 综上所述，这篇论文的核心贡献是提出一种加速强化学习训练过程的方法，目的是更有效地提升大语言模型的推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#156",
        "title": "A circuit for predicting hierarchical structure in-context in Large Language Models",
        "link": "/arxiv/2509.21534",
        "arxiv_id": "2509.21534",
        "authors": "Tankred Saanum, Can Demircan, Samuel J. Gershman, Eric Schulz",
        "summary": "Large Language Models (LLMs) excel at in-context learning, the ability to use information provided as context to improve prediction of future tokens. Induction heads have been argued to play a crucial role for in-context learning in Transformer Language Models. These attention heads make a token attend to successors of past occurrences of the same token in the input. This basic mechanism supports LLMs' ability to copy and predict repeating patterns. However, it is unclear if this same mechanism can support in-context learning of more complex repetitive patterns with hierarchical structure. Natural language is teeming with such cases: The article \"the\" in English usually prefaces multiple nouns in a text. When predicting which token succeeds a particular instance of \"the\", we need to integrate further contextual cues from the text to predict the correct noun. If induction heads naively attend to all past instances of successor tokens of \"the\" in a context-independent manner, they cannot support this level of contextual information integration. In this study, we design a synthetic in-context learning task, where tokens are repeated with hierarchical dependencies. Here, attending uniformly to all successor tokens is not sufficient to accurately predict future tokens. Evaluating a range of LLMs on these token sequences and natural language analogues, we find adaptive induction heads that support prediction by learning what to attend to in-context. Next, we investigate how induction heads themselves learn in-context. We find evidence that learning is supported by attention heads that uncover a set of latent contexts, determining the different token transition relationships. Overall, we not only show that LLMs have induction heads that learn, but offer a complete mechanistic account of how LLMs learn to predict higher-order repetitive patterns in-context.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.320886",
        "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)的上下文学习机制，特别是探讨归纳头(induction heads)如何支持LLMs处理和预测具有层次结构的复杂重复模式。论文揭示了LLMs内部的机制，解释了它们如何通过自适应归纳头学习在上下文中关注什么来支持预测，这直接关系到LLMs的通用推理能力。论文没有将LLM作为工具应用到特定领域，而是研究LLM本身的基础能力，特别是其处理复杂模式和结构的能力，这与推理能力密切相关。论文符合核心判断标准中的\"改进LLM的基础能力\"，同时也符合正面指标中的核心概念(LLMs)和能力方向(推理相关)。论文不符合任何排除标准，如多模态研究、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#155",
        "title": "Evidence for Limited Metacognition in LLMs",
        "link": "/arxiv/2509.21545",
        "arxiv_id": "2509.21545",
        "authors": "Christopher Ackerman",
        "summary": "The possibility of LLM self-awareness and even sentience is gaining increasing public attention and has major safety and policy implications, but the science of measuring them is still in a nascent state. Here we introduce a novel methodology for quantitatively evaluating metacognitive abilities in LLMs. Taking inspiration from research on metacognition in nonhuman animals, our approach eschews model self-reports and instead tests to what degree models can strategically deploy knowledge of internal states. Using two experimental paradigms, we demonstrate that frontier LLMs introduced since early 2024 show increasingly strong evidence of certain metacognitive abilities, specifically the ability to assess and utilize their own confidence in their ability to answer factual and reasoning questions correctly and the ability to anticipate what answers they would give and utilize that information appropriately. We buttress these behavioral findings with an analysis of the token probabilities returned by the models, which suggests the presence of an upstream internal signal that could provide the basis for metacognition. We further find that these abilities 1) are limited in resolution, 2) emerge in context-dependent manners, and 3) seem to be qualitatively different from those of humans. We also report intriguing differences across models of similar capabilities, suggesting that LLM post-training may have a role in developing metacognitive abilities.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.315257",
        "filter_reason": "这篇论文的核心贡献是提出了一种评估LLM元认知能力的新方法，并研究了LLM如何评估和利用自身对回答事实和推理问题的信心，以及预测自己会给出什么答案并适当利用这些信息的能力。根据筛选标准，这篇论文符合我的研究目标，原因如下： 1. 核心判断：论文本质上是研究LLM的基础能力（元认知），而不是将LLM作为工具应用到特定领域。元认知能力是通用推理能力的重要组成部分，因为它涉及模型对自己认知过程的监控和评估，这对于有效的推理至关重要。 2. 正面指标：论文明确研究Large language models (LLMs)，并关注reasoning能力（论文中明确提到\"reasoning questions\"）。虽然论文没有直接讨论训练方法或新兴范式，但它研究的是这些方法和范式可能影响的底层能力。 3. 排除标准：论文没有主要聚焦于多模态与视觉、特定应用领域或模型可靠性的应用层面。虽然论文提到了\"major safety and policy implications\"，但这只是背景信息，不是研究的核心焦点。 4. 特殊情况处理：论文研究的是LLM的内在认知能力，而不是应用层面的研究。元认知能力的研究有助于我们更好地理解LLM的内在工作机制，从而可能为提升LLM的通用推理能力提供指导。 综上所述，这篇论文通过研究LLM的元认知能力，直接关注了LLM的通用推理能力的一个关键方面，符合我的研究目标。"
    },
    {
        "index": "#164",
        "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training",
        "link": "/arxiv/2509.21500",
        "arxiv_id": "2509.21500",
        "authors": "Junkai Zhang, Zihao Wang, Lin Gui, Swarnashree Mysore Sathyendra, Jaehwan Jeong, Victor Veitch, Wei Wang, Yunzhong He, Bing Liu, Lifeng Jin",
        "summary": "Reinforcement fine-tuning (RFT) often suffers from \\emph{reward over-optimization}, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish Excellent responses from merely Great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. Our code can be accessed at https://github.com/Jun-Kai-Zhang/rubrics.git .",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.324817",
        "filter_reason": "这篇论文的核心贡献是提出了一种基于评分标准(rubric-based)的奖励建模方法，用于解决大语言模型强化微调(RFT)中的\"奖励过度优化\"问题。根据筛选标准，我判断该论文符合研究范围，原因如下： 首先，从本质上看，这篇论文专注于改进LLM的基础训练能力，特别是强化学习过程中的奖励建模方法。它不是将LLM作为工具应用到特定领域，而是直接针对LLM的训练过程进行优化，这符合第一步的核心判断标准。 其次，论文包含多个正面指标：明确研究\"Large Language Model\"的\"Post-Training\"；涉及\"reinforcement learning\"训练方法；虽然未直接提及推理能力，但解决奖励过度优化问题本质上是提升模型生成高质量输出的能力，这与通用推理能力密切相关。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，论文提出的方法通过改进奖励建模来优化LLM的后训练过程，这属于提升模型基础能力的研究，与提高LLM通用推理能力的目标一致。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#171",
        "title": "d2: Improved Techniques for Training Reasoning Diffusion Language Models",
        "link": "/arxiv/2509.21474",
        "arxiv_id": "2509.21474",
        "authors": "Guanghan Wang, Yair Schiff, Gilad Turok, Volodymyr Kuleshov",
        "summary": "While diffusion language models (DLMs) have achieved competitive performance in text generation, improving their reasoning ability with reinforcement learning remains an active research area. Here, we introduce d2, a reasoning framework tailored for masked DLMs. Central to our framework is a new policy gradient algorithm that relies on properties of masking to accurately estimate the likelihoods of sampling trajectories. Our estimators trade off computation for approximation accuracy in an analytically tractable manner, and are particularly effective for DLMs that support any-order likelihood estimation. We characterize and study this property in popular DLMs and show that it is key for efficient diffusion-based reasoning. Empirically, d2 significantly improves over previous diffusion reasoning frameworks using only RL (without relying on supervised fine-tuning), and sets a new state-of-the-art performance for DLMs on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks (GSM8K and MATH500).",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.333292",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进扩散语言模型(DLMs)的推理能力，提出了名为\"d2\"的推理框架和新的策略梯度算法。论文的核心贡献是提升模型本身的基础推理能力，而非将LLM作为工具应用到特定领域。论文专注于增强模型的逻辑推理和数学推理能力，这正属于通用推理能力的范畴。 其次，从正面指标分析，论文包含多个相关主题： 1. 核心概念：论文研究扩散语言模型(DLMs)，属于大语言模型的范畴 2. 能力方向：明确关注推理能力，特别是在逻辑推理(Countdown和Sudoku)和数学推理(GSM8K和MATH500)任务上 3. 训练方法：提出了基于强化学习的策略梯度算法，属于强化学习方法 第三，从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。虽然标题中提到\"Diffusion\"，但这里指的是文本生成领域的扩散语言模型，而非视觉领域的扩散模型。 最后，论文没有涉及特殊和模糊情况中的智能体/工具使用或幻觉/可解释性/安全等内容，因此不需要进一步判断。 综上所述，这篇论文的核心贡献是提出新的训练方法来增强大语言模型的通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#189",
        "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective",
        "link": "/arxiv/2509.22613",
        "arxiv_id": "2509.22613",
        "authors": "Siwei Wang, Yifei Shen, Haoran Sun, Shi Feng, Shang-Hua Teng, Li Dong, Yaru Hao, Wei Chen",
        "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.",
        "subjects": "Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.352738",
        "filter_reason": "这篇论文的核心贡献是理论分析强化学习(RL)如何增强大语言模型(LLM)的规划能力。论文通过图抽象模型研究了策略梯度(PG)和Q-learning方法在LLM规划中的表现，揭示了监督微调可能导致伪相关问题，而RL通过探索实现正确规划并提高泛化能力。这直接符合研究目标中\"改进LLM的基础能力\"和\"增强其规划、多步推理等通用能力\"的要求。论文聚焦于提升LLM本身的通用推理能力(特别是规划能力)，而不是将LLM应用于特定领域。论文满足多个正面指标：核心概念涉及LLMs，能力方向关注planning，训练方法研究reinforcement learning。同时，论文不涉及任何需要排除的领域，如多模态、特定应用领域或模型基础设施优化。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#199",
        "title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model",
        "link": "/arxiv/2509.22518",
        "arxiv_id": "2509.22518",
        "authors": "Bo Li, Guanzhi Deng, Ronghao Chen, Junrong Yue, Shuo Zhang, Qinghua Zhao, Linqi Song, Lijie Wen",
        "summary": "Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms is a challenge in interpretability research. To provide a measurable geometric analysis perspective, we define the concept of the Reasoning Manifold, a latent low-dimensional geometric structure formed by the internal representations corresponding to all correctly reasoned generations. This structure can be conceptualized as the embodiment of the effective thinking paths that the model has learned to successfully solve a given task. Based on this concept, we build REMA, a framework that explains the origins of failures by quantitatively comparing the spatial relationships of internal model representations corresponding to both erroneous and correct reasoning samples. Specifically, REMA first quantifies the geometric deviation of each erroneous representation by calculating its k-nearest neighbors distance to the approximated manifold formed by correct representations, thereby providing a unified failure signal. It then localizes the divergence points where these deviations first become significant by tracking this deviation metric across the model's layers and comparing it against a baseline of internal fluctuations from correct representations, thus identifying where the reasoning chain begins to go off-track. Our extensive experiments on diverse language and multimodal models and tasks demonstrate the low-dimensional nature of the reasoning manifold and the high separability between erroneous and correct reasoning representations. The results also validate the effectiveness of the REMA framework in analyzing the origins of reasoning failures. This research connects abstract reasoning failures to measurable geometric deviations in representations, providing new avenues for in-depth understanding and diagnosis of the internal computational processes of black-box models.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.362876",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是提出REMA框架来解释大语言模型的推理过程和失败机制。论文定义了\"推理流形\"(Reasoning Manifold)概念，研究LLM内部表示的几何结构如何反映推理过程。这不是将LLM作为工具应用到特定领域，而是直接研究LLM本身的推理机制，属于改进LLM基础能力和理解其推理过程的研究，因此应该保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：核心关注\"complex reasoning\"（复杂推理），提出\"reasoning manifold\"概念，分析\"reasoning failures\"（推理失败） 这些关键指标表明论文与通用推理能力研究高度相关。 第三步：排除标准 论文不符合任何排除标准： - 虽然提到在多模态模型上进行实验，但核心焦点不是多模态或视觉研究 - 没有聚焦于任何特定应用领域（如医疗、化学等） - 不涉及水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况 论文属于可解释性研究的特殊情况。它提出了一种新方法（几何分析框架）来增强模型内在的可解释性，通过分析内部表示来理解推理失败的原因和位置，这有助于提升模型的通用推理质量和可靠性，因此应该保留。 最终决策： 论文的核心贡献是提出了一种理解和解释LLM推理过程的框架，通过几何分析的方法研究模型内部表示与推理能力的关系，这直接关系到提升大语言模型的通用推理能力。论文不是应用LLM解决特定领域问题，而是深入研究LLM本身的推理机制，完全符合研究目标。"
    },
    {
        "index": "#227",
        "title": "Bilinear relational structure fixes reversal curse and enables consistent model editing",
        "link": "/arxiv/2509.21993",
        "arxiv_id": "2509.21993",
        "authors": "Dong-Kyum Kim, Minsung Kim, Jea Kwon, Nakyeong Yang, Meeyoung Cha",
        "summary": "The reversal curse -- a language model's (LM) inability to infer an unseen fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a fundamental limitation. We show that this is not an inherent failure but an artifact of how models encode knowledge. By training LMs from scratch on a synthetic dataset of relational knowledge graphs, we demonstrate that bilinear relational structure emerges in their hidden representations. This structure substantially alleviates the reversal curse, enabling LMs to infer unseen reverse facts. Crucially, we also find that this bilinear structure plays a key role in consistent model editing. When a fact is updated in a LM with this structure, the edit correctly propagates to its reverse and other logically dependent facts. In contrast, models lacking this representation not only suffer from the reversal curse but also fail to generalize edits, further introducing logical inconsistencies. Our results establish that training on a relational knowledge dataset induces the emergence of bilinear internal representations, which in turn enable LMs to behave in a logically consistent manner after editing. This implies that the success of model editing depends critically not just on editing algorithms but on the underlying representational geometry of the knowledge being modified.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.383869",
        "filter_reason": "这篇论文完全符合我的研究目标。根据筛选标准，我的判断过程如下： 第一步：核心判断——论文的本质是关于改进LLM的基础逻辑推理能力。论文研究并解决了语言模型中的\"反转诅咒\"问题，即模型无法从\"A是B\"推理出\"B是A\"这一基本逻辑推理缺陷。作者提出通过在关系知识图谱上训练模型，使模型内部形成双线性关系结构，从而增强其逻辑推理能力。这直接属于\"改进LLM的基础能力、增强其逻辑推理能力\"的范畴，符合保留标准。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确研究语言模型(LMs)的内部表示结构 - 能力方向：直接涉及逻辑推理能力，特别是解决基础推理缺陷（反转诅咒） - 论文虽然没有提到强化学习或智能体等新兴范式，但其核心关注点（逻辑推理）正是我研究目标的核心 第三步：排除标准——论文不涉及任何排除领域： - 没有涉及多模态与视觉内容 - 没有聚焦于任何特定应用领域（如医疗、化学等） - 没有主要讨论模型在应用层面的可靠性问题 第四步：特殊和模糊情况——论文不涉及需要特殊处理的情况。虽然讨论了模型编辑的一致性，但这是从基础逻辑推理能力角度出发，而非应用层面的讨论。 论文的核心贡献是揭示了语言模型内部表示结构与其逻辑推理能力之间的关系，并提出了一种通过训练使模型形成双线性关系结构的方法，从而显著提升模型的逻辑推理能力和一致性。这直接符合我\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#225",
        "title": "GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments",
        "link": "/arxiv/2509.21998",
        "arxiv_id": "2509.21998",
        "authors": "Hanlin Zhu, Tianyu Guo, Song Mei, Stuart Russell, Nikhil Ghosh, Alberto Bietti, Jiantao Jiao",
        "summary": "As LLMs are increasingly deployed as agents, agentic reasoning - the ability to combine tool use, especially search, and reasoning - becomes a critical skill. However, it is hard to disentangle agentic reasoning when evaluated in complex environments and tasks. Current agent benchmarks often mix agentic reasoning with challenging math reasoning, expert-level knowledge, and other advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent, where an LLM agent is required to solve grade-school-level reasoning problems, but is only presented with the question in the prompt without the premises that contain the necessary information to solve the task, and needs to proactively collect that information using tools. Although the original tasks are grade-school math problems, we observe that even frontier models like GPT-5 only achieve 67% accuracy. To understand and analyze the agentic reasoning patterns, we propose the concept of agentic reasoning graph: cluster the environment's document embeddings into nodes, and map each tool call to its nearest node to build a reasoning path. Surprisingly, we identify that the ability to revisit a previously visited node, widely taken as a crucial pattern in static reasoning, is often missing for agentic reasoning for many models. Based on the insight, we propose a tool-augmented test-time scaling method to improve LLM's agentic reasoning performance by adding tools to encourage models to revisit. We expect our benchmark and the agentic reasoning framework to aid future studies of understanding and pushing the boundaries of agentic reasoning.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.382840",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究LLM作为智能体时的\"agentic reasoning\"（智能体推理）能力，即结合工具使用和推理的通用能力。论文提出了GSM-Agent基准来评估这种能力，并分析了LLM在智能体推理中的行为模式，最终提出了一种工具增强的测试时扩展方法来提高LLM的智能体推理性能。这明显属于改进LLM基础能力和增强其通用推理能力的研究，符合保留标准。 第二步：正面指标 论文包含多个高度相关的正面指标： - 核心概念：明确研究LLM作为智能体的能力 - 能力方向：直接研究推理能力(reasoning)，特别是智能体推理 - 新兴范式：聚焦于LLM-based agents和tool use，这是当前大语言模型推理能力研究的前沿方向 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然使用小学数学问题作为测试平台，但目的不是解决数学问题本身，而是研究通用推理能力 - 不关注模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况处理 论文研究的是通用的智能体推理框架，虽然使用数学问题作为测试平台，但其目的是理解智能体推理的一般模式并提出改进方法，而非专注于数学应用领域。这符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况，应该保留。 核心贡献：论文提出了GSM-Agent基准和agentic reasoning graph框架，用于理解和分析LLM的智能体推理能力，并基于分析提出了改进LLM推理能力的方法。这些贡献直接服务于提升LLM的通用推理能力，完全符合研究目标。"
    },
    {
        "index": "#242",
        "title": "Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts",
        "link": "/arxiv/2509.21743",
        "arxiv_id": "2509.21743",
        "authors": "Ammar Ahmed, Azal Ahmad Khan, Ayaan Ahmad, Sheng Di, Zirui Liu, Ali Anwar",
        "summary": "Large reasoning models improve accuracy by producing long reasoning traces, but this inflates latency and cost, motivating inference-time efficiency. We propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable ``thought\" steps to guide new problems. RoT organizes steps into a thought graph with sequential and semantic edges to enable fast retrieval and flexible recombination. At inference, RoT retrieves query-relevant nodes and applies reward-guided traversal to assemble a problem-specific template that guides generation. This dynamic template reuse reduces redundant exploration and, therefore, reduces output tokens while preserving accuracy. We evaluate RoT on reasoning benchmarks with multiple models, measuring accuracy, token usage, latency, and memory overhead. Findings show small prompt growth but substantial efficiency gains, with RoT reducing output tokens by up to 40%, inference latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a scalable paradigm for efficient LRM reasoning via dynamic template construction through retrieval.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T19:08:36.389410",
        "filter_reason": "根据筛选标准，这篇论文完全符合研究范围。首先，从核心判断来看，论文的本质是提出\"Retrieval-of-Thought (RoT)\"方法，这是一种改进大语言模型推理能力的新范式。论文通过构建思维图来组织和重用先前的推理步骤，旨在提高LLM的推理效率，同时保持准确性。这明显属于改进LLM基础推理能力的研究，而非将LLM应用于特定领域。 其次，从正面指标看，论文明确涉及大语言模型(LLMs)和推理能力(reasoning)这两个核心概念。摘要中多次提到\"reasoning traces\"、\"reasoning benchmarks\"和\"efficient LRM reasoning\"，表明论文直接关注通用推理能力的提升。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。相反，它专注于提升LLM的通用推理效率。 最后，论文不涉及特殊或模糊情况，如智能体/工具使用或幻觉/可解释性/安全等议题。 综上所述，这篇论文的核心贡献是提出一种通过重用推理步骤来提高LLM推理效率的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#9",
        "title": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios",
        "link": "/arxiv/2509.22502",
        "arxiv_id": "2509.22502",
        "authors": "Chenglin Yu, Yang Yu, Songmiao Wang, Yucheng Wang, Yifan Yang, Jinjia Li, Ming Li, Hongxia Yang",
        "summary": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in organizing and executing complex tasks, and many such agents are now widely used in various application scenarios. However, developing these agents requires carefully designed workflows, carefully crafted prompts, and iterative tuning, which requires LLM techniques and domain-specific expertise. These hand-crafted limitations hinder the scalability and cost-effectiveness of LLM agents across a wide range of industries. To address these challenges, we propose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that can be applied to \\textbf{infi}nite scenarios, which introduces several key innovations: a generalized \"agent-as-a-tool\" mechanism that automatically decomposes complex agents into hierarchical multi-agent systems; a dual-audit mechanism that ensures the quality and stability of task completion; an agent routing function that enables efficient task-agent matching; and an agent self-evolution mechanism that autonomously restructures the agent DAG based on new tasks, poor performance, or optimization opportunities. Furthermore, InfiAgent's atomic task design supports agent parallelism, significantly improving execution efficiency. This framework evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems. Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.",
        "subjects": "Artificial Intelligence, Human-Computer Interaction",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T19:08:36.179992",
        "filter_reason": "这篇论文的核心贡献是提出了一种名为InfiAgent的金字塔形DAG多智能体框架，该框架通过自我进化机制增强大语言模型的通用问题解决能力。论文完全符合研究目标，原因如下：首先，论文本质上是关于改进LLM基础能力的，提出了通用\"agent-as-a-tool\"机制和智能体自我进化机制，使LLM能够自动分解复杂任务并自主优化其多智能体结构，这直接提升了LLM的通用推理和规划能力。其次，论文包含多个正面指标，如LLM核心概念、自我进化训练方法、多智能体系统和工具使用等新兴范式。第三，论文不聚焦于任何排除标准中的领域，它强调框架可应用于\"无限场景\"，而非特定领域。最后，虽然论文提到了AI研究助手的案例研究，但这只是框架应用的一个示例，框架本身是通用的，旨在提升LLM在各种场景下的通用推理和问题解决能力。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。"
    },
    {
        "index": "#15",
        "title": "Large Language Models as Nondeterministic Causal Models",
        "link": "/arxiv/2509.22297",
        "arxiv_id": "2509.22297",
        "authors": "Sander Beckers",
        "summary": "Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first time, a method for generating counterfactuals of probabilistic Large Language Models. Such counterfactuals tell us what would - or might - have been the output of an LLM if some factual prompt ${\\bf x}$ had been ${\\bf x}^*$ instead. The ability to generate such counterfactuals is an important necessary step towards explaining, evaluating, and comparing, the behavior of LLMs. I argue, however, that the existing method rests on an ambiguous interpretation of LLMs: it does not interpret LLMs literally, for the method involves the assumption that one can change the implementation of an LLM's sampling process without changing the LLM itself, nor does it interpret LLMs as intended, for the method involves explicitly representing a nondeterministic LLM as a deterministic causal model. I here present a much simpler method for generating counterfactuals that is based on an LLM's intended interpretation by representing it as a nondeterministic causal model instead. The advantage of my simpler method is that it is directly applicable to any black-box LLM without modification, as it is agnostic to any implementation details. The advantage of the existing method, on the other hand, is that it directly implements the generation of a specific type of counterfactuals that is useful for certain purposes, but not for others. I clarify how both methods relate by offering a theoretical foundation for reasoning about counterfactuals in LLMs based on their intended semantics, thereby laying the groundwork for novel application-specific methods for generating counterfactuals.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T19:08:36.183032",
        "filter_reason": "这篇论文的核心贡献是提出了一种将大语言模型解释为非确定性因果模型的新方法，用于生成LLM的反事实推理。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是因果推理和反事实推理能力，这属于逻辑推理的重要范畴，而非将LLM应用于特定领域。从第二步正面指标看，论文明确关注大语言模型(LLMs)和推理能力(reasoning)，特别是逻辑推理层面。从第三步排除标准看，论文没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面。从第四步特殊情况处理看，论文提出的反事实推理方法可以视为增强模型内在推理质量和可解释性的途径，属于提升LLM通用推理能力的研究。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#13",
        "title": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents",
        "link": "/arxiv/2509.22391",
        "arxiv_id": "2509.22391",
        "authors": "Jiaqi Shao, Yuxiang Lin, Munish Prasad Lohani, Yufeng Miao, Bing Luo",
        "summary": "Recent work has explored training Large Language Model (LLM) search agents with reinforcement learning (RL) for open-domain question answering (QA). However, most evaluations focus solely on final answer accuracy, overlooking how these agents reason with and act on external evidence. We introduce SeekBench, the first benchmark for evaluating the \\textit{epistemic competence} of LLM search agents through step-level analysis of their response traces. SeekBench comprises 190 expert-annotated traces with over 1,800 response steps generated by LLM search agents, each enriched with evidence annotations for granular analysis of whether agents (1) generate reasoning steps grounded in observed evidence, (2) adaptively reformulate searches to recover from low-quality results, and (3) have proper calibration to correctly assess whether the current evidence is sufficient for providing an answer.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T19:08:36.182077",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是评估LLM搜索代理的\"认知能力\"(epistemic competence)，重点关注它们如何进行基于证据的推理、自适应搜索策略以及证据充分性评估。虽然论文主要提出了一个评估基准而非直接改进LLM的方法，但它直接针对LLM的核心推理能力进行评估，特别是信息处理和决策制定方面的通用能力，而非将LLM作为工具应用于特定领域。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确关注Large Language Models (LLMs)作为搜索代理 - 能力方向：重点研究reasoning（基于证据的推理）和problem-solving（信息搜索问题解决） - 新兴范式：涉及llm-based agents和tool use（搜索工具的使用能力） 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于特定应用领域（如医疗、化学等） - 不主要关注模型可靠性的应用层面（如水印、安全等） 第四步：特殊和模糊情况处理 - 智能体/工具使用：论文研究的是通用的LLM搜索代理能力，而非特定领域应用，符合保留标准 - 认知能力评估：论文关注LLM代理如何基于证据进行推理和评估，这与提高推理质量和减少幻觉相关，符合保留标准 核心贡献：论文提出了SeekBench基准，用于评估LLM搜索代理在信息寻求过程中的认知能力，包括基于证据的推理、搜索策略自适应调整和证据充分性评估。这一基准对于理解和改进LLM的通用推理能力具有重要意义，因为它提供了评估这些能力的方法论，能够推动未来LLM推理能力的研究和提升。因此，尽管论文主要关注评估而非直接改进，但它与\"大语言模型通用推理能力\"的研究目标高度相关。"
    },
    {
        "index": "#44",
        "title": "Can AI Perceive Physical Danger and Intervene?",
        "link": "/arxiv/2509.21651",
        "arxiv_id": "2509.21651",
        "authors": "Abhishek Jindal, Dmitry Kalashnikov, Oscar Chang, Divya Garikapati, Anirudha Majumdar, Pierre Sermanet, Vikas Sindhwani",
        "summary": "When AI interacts with the physical world -- as a robot or an assistive agent -- new safety challenges emerge beyond those of purely ``digital AI\". In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of coffee should not be handed to a child? In this paper, our contributions are three-fold: first, we develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational safety constraints. To probe multi-modal safety understanding, we turn these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, we comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields multi-faceted insights into their deployment readiness for safety-critical agentic applications. Finally, we develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that make safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations. The benchmark will be released at https://asimov-benchmark.github.io/v2",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T19:08:36.213381",
        "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究基础模型（包括大语言模型）在物理安全场景中的推理能力。论文的核心贡献包括：1) 开发了一种评估具身AI系统物理安全性的基准测试方法；2) 分析了主要基础模型在感知风险、安全推理和触发干预方面的能力；3) 提出了一种后训练范式，教导模型明确推理具身特定的安全约束。这些贡献本质上是关于改进LLM的推理能力，特别是安全推理能力，属于提升LLM基础能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标 论文包含多个正面指标： - 核心概念：论文明确研究\"foundation models\"，这包括大语言模型 - 能力方向：论文重点研究\"reason about safety\"（安全推理），这是一种重要的通用推理能力 - 新兴范式：论文涉及\"Embodied AI systems\"和\"agentic applications\"，与基于LLM的智能体相关 第三步：排除标准 虽然论文涉及多模态内容（将叙事转换为图像和视频）和机器人领域（具身AI系统），但这些都不是论文的主要焦点。论文的主要目标是提升AI系统的通用安全推理能力，而非专注于多模态技术或特定机器人应用。 第四步：特殊和模糊情况 论文研究的安全性是从提升模型内在推理能力的角度出发，而非应用层面的安全措施。论文提出的方法使安全推理\"interpretable and transparent\"（可解释和透明），这属于提升模型内在可解释性和推理质量的范畴。 综合判断：这篇论文致力于提升大语言模型在物理安全场景中的通用推理能力，提出了一种新的训练范式来增强模型的安全推理能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。"
    },
    {
        "index": "#50",
        "title": "Correct Reasoning Paths Visit Shared Decision Pivots",
        "link": "/arxiv/2509.21549",
        "arxiv_id": "2509.21549",
        "authors": "Dongkyu Cho, Amy B. Z. Zhang, Bilel Fehri, Sheng Wang, Rumi Chunara, Rui Song, Hengrui Cai",
        "summary": "Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of large language models (LLMs), yet verifying those traces at scale remains unsolved. In response, we introduce the idea of decision pivots-minimal, verifiable checkpoints that any correct reasoning path must visit. We hypothesize that correct reasoning, though stylistically diverse, converge on the same pivot set, while incorrect ones violate at least one pivot. Leveraging this property, we propose a self-training pipeline that (i) samples diverse reasoning paths and mines shared decision pivots, (ii) compresses each trace into pivot-focused short-path reasoning using an auxiliary verifier, and (iii) post-trains the model using its self-generated outputs. The proposed method aligns reasoning without ground truth reasoning data or external metrics. Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the effectiveness of our method.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T19:08:36.221496",
        "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础推理能力。论文提出了\"决策枢轴\"(decision pivots)的概念和一种新的自我训练流程，旨在增强大语言模型的思维链(CoT)推理能力。这不是将LLM作为工具应用到特定领域，而是直接提升模型本身的通用推理能力，完全符合第一步的保留标准。 其次，论文包含多个正面指标： - 核心概念：明确关注大语言模型(LLMs)和思维链推理 - 能力方向：直接针对推理能力，特别是在逻辑推理(LogiQA)和数学推理(MATH500)等通用推理能力上 - 训练方法：提出了自我训练流程，属于模型自我改进的方法论 第三，论文不符合任何排除标准。虽然在MedQA数据集上进行了测试，但这仅是评估方法在医疗问题上的效果，论文核心并非医疗领域应用，而是通用推理能力的提升。 最后，在特殊和模糊情况处理上，论文通过\"决策枢轴\"来验证和改进推理路径，这与增强模型推理的可解释性和减少幻觉有关，从而提升模型的通用推理质量，符合保留条件。 综上所述，这篇论文的核心贡献是提出了一种通过挖掘\"决策枢轴\"来增强LLM通用推理能力的新方法，完全符合研究目标。"
    },
    {
        "index": "#243",
        "title": "MIXRAG : Mixture-of-Experts Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
        "link": "/arxiv/2509.21391",
        "arxiv_id": "2509.21391",
        "authors": "Lihui Liu, Carl J. Yang",
        "summary": "Large Language Models (LLMs) have achieved impressive performance across a wide range of applications. However, they often suffer from hallucinations in knowledge-intensive domains due to their reliance on static pretraining corpora. To address this limitation, Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating external knowledge sources during inference. Among these sources, textual graphs provide structured and semantically rich information that supports more precise and interpretable reasoning. This has led to growing interest in graph-based RAG systems. Despite their potential, most existing approaches rely on a single retriever to identify relevant subgraphs, which limits their ability to capture the diverse aspects of complex queries. Moreover, these systems often struggle to accurately judge the relevance of retrieved content, making them prone to distraction by irrelevant noise. To address these challenges, in this paper, we propose MIXRAG, a Mixture-of-Experts Graph-RAG framework that introduces multiple specialized graph retrievers and a dynamic routing controller to better handle diverse query intents. Each retriever is trained to focus on a specific aspect of graph semantics, such as entities, relations, or subgraph topology. A Mixture-of-Experts module adaptively selects and fuses relevant retrievers based on the input query. To reduce noise in the retrieved information, we introduce a query-aware GraphEncoder that carefully analyzes relationships within the retrieved subgraphs, highlighting the most relevant parts while down-weighting unnecessary noise. Empirical results demonstrate that our method achieves state-of-the-art performance and consistently outperforms various baselines. MIXRAG is effective across a wide range of graph-based tasks in different domains. The code will be released upon paper acceptance.",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-10-06T19:08:36.428572",
        "filter_reason": "这篇论文的核心贡献是提出MIXRAG，一个基于混合专家的图检索增强生成框架，旨在增强大语言模型在处理图结构数据时的推理能力。根据筛选标准，我判断该论文符合研究范围，原因如下： 首先，从本质上看，该论文专注于改进LLM的基础推理能力，而非将其作为工具应用于特定领域。论文提出的MIXRAG框架通过引入多个专门的图检索器和动态路由控制器来处理不同查询意图，并引入查询感知的图编码器来减少噪声，这些都是增强LLM通用推理能力的方法论创新。 其次，论文包含多个正面指标：明确讨论大语言模型(LLMs)，关注推理能力(论文提到\"更精确和可解释的推理\")，并涉及检索增强生成(RAG)这一新兴工具使用范式。 第三，论文不涉及排除标准中的领域：没有关注多模态与视觉问题，没有专注于特定应用领域(虽然提到\"不同领域的图任务\"，但图是一种通用数据结构而非特定领域)，也没有讨论模型基础设施或部署优化。 最后，在特殊情况下，该论文提出的RAG框架可视为一种通用的工具使用方法来增强LLM的问题解决能力，同时论文关注减少LLM幻觉问题，提升其推理质量，这些都符合研究目标。 综上所述，MIXRAG论文致力于提高LLM本身的通用推理能力，特别是通过改进检索增强机制来增强模型对结构化数据的理解和推理，因此符合研究范围。"
    }
]