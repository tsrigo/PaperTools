#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆè®ºæ–‡æ€»ç»“ç”Ÿæˆè„šæœ¬ - ä¸“é—¨é’ˆå¯¹ç¼“å­˜å¤ç”¨ä¼˜åŒ–
Fast paper summary generation script - optimized for cache reuse
"""

import json
import os
import argparse
import time
from pathlib import Path
from typing import Optional, Dict, List
from tqdm import tqdm
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# å¯¼å…¥é…ç½®
try:
    from config import (
        API_KEY, BASE_URL, MODEL, SUMMARY_DIR, TEMPERATURE, REQUEST_DELAY, 
        REQUEST_TIMEOUT, MAX_WORKERS, ENABLE_CACHE
    )
except ImportError:
    API_KEY = "your_api_key_here"
    BASE_URL = "https://api.x.ai/v1"
    MODEL = "grok-3-mini"
    SUMMARY_DIR = "summary"
    TEMPERATURE = 0.1
    REQUEST_DELAY = 2
    REQUEST_TIMEOUT = 120
    MAX_WORKERS = 4
    ENABLE_CACHE = True

# å¯¼å…¥ç¼“å­˜ç®¡ç†å™¨
from cache_manager import CacheManager
from generate_summary import (
    fetch_paper_content_from_jinja, generate_summary, 
    translate_summary, jina_rate_limiter
)


def process_papers_fast(papers: List[Dict], args, client: OpenAI, cache_manager: Optional[CacheManager] = None):
    """
    å¿«é€Ÿå¤„ç†è®ºæ–‡ - ä¼˜åŒ–ç‰ˆæœ¬
    
    ä¸»è¦ä¼˜åŒ–ï¼š
    1. ä¼˜å…ˆæ£€æŸ¥JSONä¸­å·²æœ‰çš„summary2
    2. ç„¶åæ£€æŸ¥ç¼“å­˜
    3. æœ€åæ‰è·å–è®ºæ–‡å†…å®¹
    """
    
    def process_paper_fast(paper_with_index):
        index, paper = paper_with_index
        paper_title = paper.get('title', 'Untitled Paper')
        paper_link = paper.get('link', '')
        
        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæ£€æŸ¥JSONä¸­æ˜¯å¦å·²æœ‰summary2
        if args.skip_existing and paper.get('summary2') and paper.get('summary2').strip():
            return 'skipped', index, paper, f"â­ï¸ JSONä¸­å·²æœ‰æ€»ç»“: {paper_title[:50]}..."
        
        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šæ£€æŸ¥ç¼“å­˜ä¸­çš„å®Œæ•´ç»“æœ
        if cache_manager and ENABLE_CACHE:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„è®ºæ–‡å†…å®¹
                paper_cache = cache_manager.get_paper_cache(paper_link)
                if paper_cache and paper_cache.get('data', {}).get('content'):
                    cached_content = paper_cache['data']['content']
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„æ€»ç»“ç¼“å­˜
                    cached_summary = cache_manager.get_summary_cache(paper_title, cached_content)
                    
                    # æ£€æŸ¥ç¿»è¯‘ç¼“å­˜
                    cached_translation = None
                    original_summary = paper.get('summary', '')
                    if original_summary:
                        cache_key = f"translation_{paper_title}_{original_summary[:100]}"
                        cached_translation = cache_manager.get_summary_cache(cache_key, original_summary)
                    
                    # å¦‚æœç¼“å­˜å®Œæ•´ï¼Œç›´æ¥ä½¿ç”¨
                    if cached_summary:
                        paper_copy = paper.copy()
                        paper_copy['summary2'] = cached_summary
                        paper_copy['summary_translation'] = cached_translation or "æ— éœ€ç¿»è¯‘"
                        paper_copy['summary_generated_time'] = time.strftime('%Y-%m-%d %H:%M:%S')
                        paper_copy['summary_model'] = args.model
                        return 'success', index, paper_copy, f"ğŸ“‹ ä½¿ç”¨å®Œæ•´ç¼“å­˜: {paper_title[:50]}..."
            except Exception as e:
                print(f"âš ï¸ æ£€æŸ¥ç¼“å­˜æ—¶å‡ºé”™ {paper_title[:30]}: {e}")
        
        # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šéœ€è¦è·å–å†…å®¹å¹¶ç”Ÿæˆ
        try:
            # å°è¯•ä»ç¼“å­˜è·å–è®ºæ–‡å†…å®¹
            paper_content = None
            if cache_manager and ENABLE_CACHE:
                paper_cache = cache_manager.get_paper_cache(paper_link)
                if paper_cache and paper_cache.get('data', {}).get('content'):
                    paper_content = paper_cache['data']['content']
            
            # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰å†…å®¹ï¼Œä»jina.aiè·å–
            if not paper_content:
                paper_content = fetch_paper_content_from_jinja(paper_link)
                if not paper_content:
                    return 'failed', index, paper, f"âŒ æ— æ³•è·å–è®ºæ–‡å†…å®¹: {paper_title}"
                
                # ä¿å­˜åˆ°ç¼“å­˜
                if cache_manager and ENABLE_CACHE:
                    cache_manager.set_paper_cache(paper_link, {'content': paper_content})
            
            # æˆªæ–­è¿‡é•¿å†…å®¹
            if len(paper_content) > 200000:
                paper_content = paper_content[:200000] + "\n\n[å†…å®¹å·²æˆªæ–­...]"
            
            # ç”Ÿæˆæ€»ç»“
            summary = generate_summary(paper_content, client, args.model, args.temperature, paper_title, cache_manager)
            
            # ç¿»è¯‘æ‘˜è¦
            summary_translation = ""
            original_summary = paper.get('summary', '')
            if original_summary:
                try:
                    summary_translation = translate_summary(original_summary, client, args.model, args.temperature, paper_title, cache_manager)
                except Exception as e:
                    print(f"âš ï¸ ç¿»è¯‘å¤±è´¥ {paper_title[:30]}: {e}")
                    summary_translation = "ç¿»è¯‘å¤±è´¥"
            
            # è¿”å›ç»“æœ
            paper_copy = paper.copy()
            paper_copy['summary2'] = summary
            paper_copy['summary_translation'] = summary_translation
            paper_copy['summary_generated_time'] = time.strftime('%Y-%m-%d %H:%M:%S')
            paper_copy['summary_model'] = args.model
            
            return 'success', index, paper_copy, f"âœ… æ–°ç”Ÿæˆ: {paper_title[:50]}..."
            
        except Exception as e:
            return 'failed', index, paper, f"âŒ å¤„ç†å‡ºé”™ {paper_title}: {e}"
    
    # æ‰§è¡Œå¤„ç†
    print(f"ğŸ”„ ä½¿ç”¨ {args.max_workers} ä¸ªçº¿ç¨‹å¿«é€Ÿå¤„ç†...")
    
    processed = 0
    skipped = 0
    failed = 0
    updated_papers = papers.copy()
    
    with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
        futures = [executor.submit(process_paper_fast, (i, paper)) for i, paper in enumerate(papers)]
        
        for future in tqdm(as_completed(futures), total=len(papers), desc="å¿«é€Ÿå¤„ç†"):
            try:
                status, index, updated_paper, message = future.result()
                
                if status == 'success':
                    processed += 1
                    updated_papers[index] = updated_paper
                elif status == 'skipped':
                    skipped += 1
                else:
                    failed += 1
                
                # å‡å°‘å»¶æ—¶ï¼Œå› ä¸ºå¤§éƒ¨åˆ†éƒ½æ˜¯ç¼“å­˜
                time.sleep(REQUEST_DELAY / (args.max_workers * 4))  # å‡å°‘åˆ°1/4
                
            except Exception as e:
                print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
                failed += 1
    
    return updated_papers, processed, skipped, failed


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¿«é€Ÿè®ºæ–‡æ€»ç»“ç”Ÿæˆå·¥å…·ï¼ˆç¼“å­˜ä¼˜åŒ–ç‰ˆï¼‰')
    parser.add_argument('--input-file', required=True, help='è¾“å…¥JSONæ–‡ä»¶')
    parser.add_argument('--output-dir', default=SUMMARY_DIR, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--api-key', default=API_KEY, help='APIå¯†é’¥')
    parser.add_argument('--base-url', default=BASE_URL, help='APIåŸºç¡€URL')
    parser.add_argument('--model', default=MODEL, help='ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('--temperature', type=float, default=TEMPERATURE, help='ç”Ÿæˆæ¸©åº¦')
    parser.add_argument('--max-papers', type=int, default=0, help='æœ€å¤§å¤„ç†æ•°é‡')
    parser.add_argument('--skip-existing', action='store_true', help='è·³è¿‡å·²æœ‰summary2çš„è®ºæ–‡')
    parser.add_argument('--max-workers', type=int, default=MAX_WORKERS, help='æœ€å¤§çº¿ç¨‹æ•°')
    parser.add_argument('--disable-cache', action='store_true', help='ç¦ç”¨ç¼“å­˜')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
    cache_manager = None
    if not args.disable_cache and ENABLE_CACHE:
        cache_manager = CacheManager()
        stats = cache_manager.get_cache_stats()
        print(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: è®ºæ–‡å†…å®¹={stats['papers']}, æ€»ç»“={stats['summaries']}, æ€»è®¡={stats['total']}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ°: {args.input_file}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = OpenAI(api_key=args.api_key, base_url=args.base_url)
    
    print(f"ğŸ“ å¿«é€Ÿè®ºæ–‡æ€»ç»“ç”Ÿæˆ")
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {args.input_file}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ¤– æ¨¡å‹: {args.model}")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®
    try:
        with open(args.input_file, 'r', encoding='utf-8') as f:
            papers = json.load(f)
        print(f"ğŸ“š åŠ è½½ {len(papers)} ç¯‡è®ºæ–‡")
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶é”™è¯¯: {e}")
        return
    
    # é™åˆ¶æ•°é‡
    if args.max_papers > 0:
        papers = papers[:args.max_papers]
        print(f"ğŸ”¢ é™åˆ¶å¤„ç†: {args.max_papers}")
    
    # å¿«é€Ÿå¤„ç†
    start_time = time.time()
    updated_papers, processed, skipped, failed = process_papers_fast(
        papers, args, client, cache_manager
    )
    elapsed = time.time() - start_time
    
    # ä¿å­˜ç»“æœ
    if processed > 0:
        input_filename = os.path.basename(args.input_file)
        name_without_ext = os.path.splitext(input_filename)[0]
        output_filename = f"{name_without_ext}_with_summary2.json"
        output_path = os.path.join(args.output_dir, output_filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(updated_papers, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜: {output_path}")
    
    # ç»Ÿè®¡
    print(f"\nğŸ“Š å¤„ç†å®Œæˆï¼è€—æ—¶: {elapsed:.1f}ç§’")
    print(f"âœ… å¤„ç†: {processed} ç¯‡")
    print(f"â­ï¸ è·³è¿‡: {skipped} ç¯‡")
    print(f"âŒ å¤±è´¥: {failed} ç¯‡")
    if processed > 0:
        print(f"âš¡ å¹³å‡é€Ÿåº¦: {processed/elapsed:.1f} ç¯‡/ç§’")


if __name__ == "__main__":
    main()
