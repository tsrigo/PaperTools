#!/usr/bin/env python3
"""
å®Œæ•´çš„å­¦æœ¯è®ºæ–‡å¤„ç†æµæ°´çº¿
Complete academic paper processing pipeline: crawl -> filter -> summarize -> generate webpages -> serve
"""

import os
import sys
import json
import argparse
import subprocess
import time
from datetime import datetime
from pathlib import Path
from typing import List, Optional
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥é…ç½®
try:
    from src.utils.config import (
        API_KEY, BASE_URL, MODEL, TEMPERATURE,
        ARXIV_PAPER_DIR, DOMAIN_PAPER_DIR, SUMMARY_DIR, WEBPAGES_DIR,
        CRAWL_CATEGORIES, MAX_PAPERS_PER_CATEGORY, MAX_WORKERS
    )
except ImportError:
    raise ImportError("âš ï¸ é”™è¯¯: æœªæ‰¾åˆ°config.py")


class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, total_steps: int = 5):
        self.total_steps = total_steps
        self.current_step = 0
        self.step_names = [
            "çˆ¬å–arXivè®ºæ–‡",
            "ç­›é€‰ç›¸å…³è®ºæ–‡", 
            "ç”Ÿæˆè®ºæ–‡æ€»ç»“",
            "ç”Ÿæˆç»Ÿä¸€é¡µé¢",
            "å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨"
        ]
        self.start_time = time.time()
        
    def log_with_timestamp(self, message: str, level: str = "INFO"):
        """å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—è¾“å‡º"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        elapsed = time.time() - self.start_time
        elapsed_str = f"{elapsed:.1f}s"
        print(f"[{timestamp}] [{elapsed_str:>6}] {message}")
    
    def start_step(self, step_name: str):
        """å¼€å§‹ä¸€ä¸ªæ­¥éª¤"""
        self.current_step += 1
        step_progress = f"({self.current_step}/{self.total_steps})"
        self.log_with_timestamp(f"ğŸ”„ æ­¥éª¤{self.current_step}: {step_name} {step_progress}")
        print("-" * 50)
        
    def complete_step(self, step_name: str, success: bool = True):
        """å®Œæˆä¸€ä¸ªæ­¥éª¤"""
        status = "âœ… å®Œæˆ" if success else "âŒ å¤±è´¥"
        self.log_with_timestamp(f"{status}: {step_name}")
        print()
        
    def skip_step(self, step_name: str):
        """è·³è¿‡ä¸€ä¸ªæ­¥éª¤"""
        self.current_step += 1
        self.log_with_timestamp(f"â­ï¸ è·³è¿‡æ­¥éª¤{self.current_step}: {step_name}")
        print()
        
    def show_summary(self):
        """æ˜¾ç¤ºæ€»ç»“"""
        total_time = time.time() - self.start_time
        self.log_with_timestamp(f"ğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆ! æ€»è€—æ—¶: {total_time:.1f}ç§’")


def run_command(cmd: List[str], description: str, progress_tracker: ProgressTracker = None) -> bool:
    """
    è¿è¡Œå‘½ä»¤å¹¶å¤„ç†ç»“æœ
    
    Args:
        cmd: è¦è¿è¡Œçš„å‘½ä»¤åˆ—è¡¨
        description: å‘½ä»¤æè¿°
        progress_tracker: è¿›åº¦è·Ÿè¸ªå™¨
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    if progress_tracker:
        progress_tracker.log_with_timestamp(f"ğŸ”„ å¼€å§‹: {description}")
        progress_tracker.log_with_timestamp(f"   å‘½ä»¤: {' '.join(cmd)}")
    else:
        print(f"ğŸ”„ {description}...")
        print(f"   å‘½ä»¤: {' '.join(cmd)}")
    
    start_time = time.time()
    
    try:
        # ä½¿ç”¨å®æ—¶è¾“å‡ºè€Œä¸æ˜¯æ•è·è¾“å‡ºï¼Œè¿™æ ·å¯ä»¥çœ‹åˆ°è¿›åº¦æ¡
        result = subprocess.run(cmd, text=True, check=True)
        duration = time.time() - start_time
        
        if progress_tracker:
            progress_tracker.log_with_timestamp(f"âœ… å®Œæˆ: {description} (è€—æ—¶: {duration:.1f}ç§’)")
        else:
            print(f"âœ… {description} å®Œæˆ")
            
        return True
        
    except subprocess.CalledProcessError as e:
        duration = time.time() - start_time
        
        if progress_tracker:
            progress_tracker.log_with_timestamp(f"âŒ å¤±è´¥: {description} (è€—æ—¶: {duration:.1f}ç§’)")
            progress_tracker.log_with_timestamp(f"   é”™è¯¯ç : {e.returncode}")
        else:
            print(f"âŒ {description} å¤±è´¥")
            print(f"   é”™è¯¯ç : {e.returncode}")
            
        return False
        
    except Exception as e:
        duration = time.time() - start_time
        if progress_tracker:
            progress_tracker.log_with_timestamp(f"âŒ å¼‚å¸¸: {description} - {e} (è€—æ—¶: {duration:.1f}ç§’)")
        else:
            print(f"âŒ {description} å‡ºé”™: {e}")
        return False



def find_latest_file(directory: str, pattern: str = "*.json") -> Optional[str]:
    """æ‰¾åˆ°ç›®å½•ä¸­æœ€æ–°çš„åŒ¹é…æ–‡ä»¶ï¼Œä¼˜å…ˆé€‰æ‹©åˆå¹¶æ–‡ä»¶å’Œç­›é€‰ç»“æœæ–‡ä»¶"""
    try:
        from glob import glob
        files = glob(os.path.join(directory, pattern))
        if not files:
            return None
        # domain_paper ä¼˜å…ˆ filtered_papers
        if 'domain_paper' in directory:
            filtered_files = [f for f in files if 'filtered_papers' in f and 'excluded' not in f]
            if filtered_files:
                return max(filtered_files, key=os.path.getmtime)
        # arxiv_paper ä¼˜å…ˆåˆå¹¶æ–‡ä»¶
        combined_files = [f for f in files if '_cs.' in f and f.count('_cs.') > 1]
        if combined_files:
            return max(combined_files, key=os.path.getmtime)
        return max(files, key=os.path.getmtime)
    except Exception as e:
        print(f"âŒ æŸ¥æ‰¾æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def find_file_by_date(directory: str, date_str: str, pattern: str = "*.json") -> Optional[str]:
    """
    åœ¨ç›®å½•ä¸­æŸ¥æ‰¾åŒ…å«æŒ‡å®šæ—¥æœŸå­—ç¬¦ä¸²çš„æ–‡ä»¶ï¼Œä¼˜å…ˆé€‰æ‹© filtered/åˆå¹¶æ–‡ä»¶ï¼Œæ‰¾ä¸åˆ°åˆ™ fallback åˆ°æœ€æ–°ã€‚
    date_str: æ ¼å¼ YYYY-MM-DD
    """
    from glob import glob
    files = glob(os.path.join(directory, pattern))
    if not files:
        return None
    # å…ˆç²¾ç¡®åŒ¹é…æ—¥æœŸ
    date_files = [f for f in files if date_str in os.path.basename(f)]
    if date_files:
        # domain_paper ä¼˜å…ˆ filtered_papers
        if 'domain_paper' in directory:
            filtered_files = [f for f in date_files if 'filtered_papers' in f and 'excluded' not in f]
            if filtered_files:
                return max(filtered_files, key=os.path.getmtime)
        # arxiv_paper ä¼˜å…ˆåˆå¹¶æ–‡ä»¶
        if 'arxiv_paper' in directory:
            combined_files = [f for f in date_files if '_cs.' in f and f.count('_cs.') > 1]
            if combined_files:
                return max(combined_files, key=os.path.getmtime)
        return max(date_files, key=os.path.getmtime)
    # fallback: ä¾ç„¶æŒ‰åŸæœ‰é€»è¾‘æ‰¾æœ€æ–°
    return find_latest_file(directory, pattern)


def check_file_exists(filepath: str, description: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    if os.path.exists(filepath):
        print(f"âœ… æ‰¾åˆ°{description}: {filepath}")
        return True
    else:
        print(f"âŒ æœªæ‰¾åˆ°{description}: {filepath}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å®Œæ•´çš„å­¦æœ¯è®ºæ–‡å¤„ç†æµæ°´çº¿')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--api-key', default=API_KEY, help='APIå¯†é’¥')
    parser.add_argument('--base-url', default=BASE_URL, help='APIåŸºç¡€URL')
    parser.add_argument('--model', default=MODEL, help='ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('--temperature', type=float, default=TEMPERATURE, help='ç”Ÿæˆæ¸©åº¦')
    
    # æµç¨‹æ§åˆ¶
    parser.add_argument('--skip-crawl', action='store_true', help='è·³è¿‡çˆ¬å–æ­¥éª¤')
    parser.add_argument('--skip-filter', action='store_true', help='è·³è¿‡ç­›é€‰æ­¥éª¤')
    parser.add_argument('--skip-summary', action='store_true', help='è·³è¿‡æ€»ç»“æ­¥éª¤')
    parser.add_argument('--skip-unified', action='store_true', help='è·³è¿‡ç»Ÿä¸€é¡µé¢ç”Ÿæˆæ­¥éª¤')
    parser.add_argument('--skip-serve', action='store_true', help='è·³è¿‡å¯åŠ¨æœåŠ¡å™¨æ­¥éª¤')
    parser.add_argument('--start-from', choices=['crawl', 'filter', 'summary', 'unified', 'serve'], default=None,
                       help='ä»æŒ‡å®šé˜¶æ®µå¼€å§‹æ‰§è¡Œï¼Œè‡ªåŠ¨è·³è¿‡ä¹‹å‰çš„é˜¶æ®µ')
    
    # å‚æ•°é…ç½®
    parser.add_argument('--categories', nargs='+', default=CRAWL_CATEGORIES,
                       help='è¦çˆ¬å–çš„ç±»åˆ«')
    parser.add_argument('--max-papers-per-category', type=int, default=MAX_PAPERS_PER_CATEGORY,
                       help='æ¯ä¸ªç±»åˆ«æœ€å¤§çˆ¬å–æ•°é‡')
    parser.add_argument('--max-papers-total', type=int, default=100,  # ä»10å¢åŠ åˆ°100
                       help='æ€»å…±å¤„ç†çš„æœ€å¤§è®ºæ–‡æ•°é‡')
    parser.add_argument('--max-workers', type=int, default=MAX_WORKERS,
                       help=f'æœ€å¤§çº¿ç¨‹æ•° (é»˜è®¤: {MAX_WORKERS})')
    parser.add_argument('--date', default=None,
                       help='æŒ‡å®šæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œç”¨äºçˆ¬å–ç‰¹å®šæ—¥æœŸçš„è®ºæ–‡å’Œç»„ç»‡ç½‘é¡µ')
    parser.add_argument('--start-date', default=None,
                       help='èµ·å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œä¸--end-dateä¸€èµ·ä½¿ç”¨æŒ‡å®šæ—¥æœŸèŒƒå›´')
    parser.add_argument('--end-date', default=None,
                       help='ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œä¸--start-dateä¸€èµ·ä½¿ç”¨æŒ‡å®šæ—¥æœŸèŒƒå›´')
    
    # è¾“å…¥è¾“å‡ºç›®å½•
    parser.add_argument('--crawl-input-file', help='çˆ¬å–æ­¥éª¤çš„è¾“å…¥æ–‡ä»¶ï¼ˆå¦‚æœè·³è¿‡çˆ¬å–ï¼‰')
    parser.add_argument('--filter-input-file', help='ç­›é€‰æ­¥éª¤çš„è¾“å…¥æ–‡ä»¶ï¼ˆå¦‚æœè·³è¿‡ç­›é€‰ï¼‰')
    
    args = parser.parse_args()

    # æ ¹æ® --start-from è‡ªåŠ¨è®¾ç½®è·³è¿‡æ ‡å¿—
    stage_order = ['crawl', 'filter', 'summary', 'unified', 'serve']
    if args.start_from:
        try:
            start_idx = stage_order.index(args.start_from)
            # 0:crawl,1:filter,2:summary,3:unified,4:serve
            if start_idx > 0:
                args.skip_crawl = True
            if start_idx > 1:
                args.skip_filter = True
            if start_idx > 2:
                args.skip_summary = True
            if start_idx > 3:
                args.skip_unified = True
            # start_idx > 4 æ— æ„ä¹‰
        except ValueError:
            pass
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªå™¨
    progress = ProgressTracker()
    
    print("ğŸš€ å¯åŠ¨å®Œæ•´çš„å­¦æœ¯è®ºæ–‡å¤„ç†æµæ°´çº¿")
    print("=" * 60)
    progress.log_with_timestamp(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {args.model}")
    progress.log_with_timestamp(f"ğŸ“Š æ¯ç±»æœ€å¤§è®ºæ–‡æ•°: {args.max_papers_per_category}")
    progress.log_with_timestamp(f"ğŸ”¢ æ€»å¤„ç†æ•°é‡: {args.max_papers_total}")
    progress.log_with_timestamp(f"ğŸ§µ æœ€å¤§çº¿ç¨‹æ•°: {args.max_workers}")
    
    # å¤„ç†æ—¥æœŸå‚æ•°
    use_date_range = args.start_date and args.end_date
    if use_date_range and args.date:
        progress.log_with_timestamp("âŒ ä¸èƒ½åŒæ—¶æŒ‡å®šå•ä¸ªæ—¥æœŸå’Œæ—¥æœŸèŒƒå›´")
        return
    
    if use_date_range:
        progress.log_with_timestamp(f"ğŸ“… æ—¥æœŸèŒƒå›´: {args.start_date} åˆ° {args.end_date}")
    elif args.date:
        progress.log_with_timestamp(f"ğŸ“… æŒ‡å®šæ—¥æœŸ: {args.date}")
    else:
        progress.log_with_timestamp(f"ğŸ“… çˆ¬å–æ¨¡å¼: æœ€æ–°è®ºæ–‡")
    print("=" * 60)
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    for directory in [ARXIV_PAPER_DIR, DOMAIN_PAPER_DIR, SUMMARY_DIR, WEBPAGES_DIR]:
        os.makedirs(directory, exist_ok=True)
    
    # è®°å½•å¤„ç†çš„æ–‡ä»¶è·¯å¾„
    crawl_output_file = None
    filter_output_file = None
    

    # ============ æ­¥éª¤1: çˆ¬å–è®ºæ–‡ ============
    if not args.skip_crawl:
        progress.start_step("çˆ¬å–arXivè®ºæ–‡")
        cmd = [
            sys.executable, "src/core/crawl_arxiv.py",
            "--categories"] + args.categories + [
            "--max-papers", str(args.max_papers_per_category),
            "--output-dir", ARXIV_PAPER_DIR,
            "--delay", "1.0",
            "--max-workers", str(args.max_workers)
        ]
        if use_date_range:
            cmd.extend(["--start-date", args.start_date, "--end-date", args.end_date])
        elif args.date:
            cmd.extend(["--date", args.date])
        if not run_command(cmd, "çˆ¬å–è®ºæ–‡", progress):
            progress.complete_step("çˆ¬å–è®ºæ–‡", False)
            progress.log_with_timestamp("âŒ çˆ¬å–å¤±è´¥ï¼Œæµæ°´çº¿ç»ˆæ­¢")
            return
        # æŒ‰æ—¥æœŸæŸ¥æ‰¾çˆ¬å–æ–‡ä»¶
        if args.date:
            crawl_output_file = find_file_by_date(ARXIV_PAPER_DIR, args.date, "*.json")
        else:
            crawl_output_file = find_latest_file(ARXIV_PAPER_DIR, "*.json")
        if not crawl_output_file:
            progress.complete_step("çˆ¬å–è®ºæ–‡", False)
            progress.log_with_timestamp("âŒ æœªæ‰¾åˆ°çˆ¬å–è¾“å‡ºæ–‡ä»¶")
            return
        progress.complete_step("çˆ¬å–è®ºæ–‡", True)
    else:
        progress.skip_step("çˆ¬å–arXivè®ºæ–‡")
        crawl_output_file = args.crawl_input_file
        if not crawl_output_file or not check_file_exists(crawl_output_file, "çˆ¬å–è¾“å…¥æ–‡ä»¶"):
            if args.date:
                crawl_output_file = find_file_by_date(ARXIV_PAPER_DIR, args.date, "*.json")
            else:
                crawl_output_file = find_latest_file(ARXIV_PAPER_DIR, "*.json")
            if not crawl_output_file:
                progress.log_with_timestamp("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„çˆ¬å–æ–‡ä»¶")
                return
    progress.log_with_timestamp(f"ğŸ“„ ä½¿ç”¨çˆ¬å–æ–‡ä»¶: {crawl_output_file}")
    
    # ============ æ­¥éª¤2: ç­›é€‰è®ºæ–‡ ============
    if not args.skip_filter:
        progress.start_step("ç­›é€‰ç›¸å…³è®ºæ–‡")
        cmd = [
            sys.executable, "src/core/select_.py",
            "--input-file", crawl_output_file,
            "--output-dir", DOMAIN_PAPER_DIR,
            "--api-key", args.api_key,
            "--base-url", args.base_url,
            "--model", args.model,
            "--temperature", str(args.temperature),
            "--max-papers", str(args.max_papers_total),
            "--max-workers", str(args.max_workers)
        ]
        if not run_command(cmd, "ç­›é€‰è®ºæ–‡", progress):
            progress.complete_step("ç­›é€‰è®ºæ–‡", False)
            progress.log_with_timestamp("âŒ ç­›é€‰å¤±è´¥ï¼Œæµæ°´çº¿ç»ˆæ­¢")
            return
        # æŒ‰æ—¥æœŸæŸ¥æ‰¾ç­›é€‰æ–‡ä»¶
        if args.date:
            filter_output_file = find_file_by_date(DOMAIN_PAPER_DIR, args.date, "*.json")
        else:
            filter_output_file = find_latest_file(DOMAIN_PAPER_DIR, "*.json")
        if not filter_output_file:
            progress.complete_step("ç­›é€‰è®ºæ–‡", False)
            progress.log_with_timestamp("âŒ æœªæ‰¾åˆ°ç­›é€‰è¾“å‡ºæ–‡ä»¶")
            return
        progress.complete_step("ç­›é€‰è®ºæ–‡", True)
    else:
        progress.skip_step("ç­›é€‰ç›¸å…³è®ºæ–‡")
        filter_output_file = args.filter_input_file
        if not filter_output_file or not check_file_exists(filter_output_file, "ç­›é€‰è¾“å…¥æ–‡ä»¶"):
            if args.date:
                filter_output_file = find_file_by_date(DOMAIN_PAPER_DIR, args.date, "*.json")
            else:
                filter_output_file = find_latest_file(DOMAIN_PAPER_DIR, "*.json")
            if not filter_output_file:
                progress.log_with_timestamp("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„ç­›é€‰æ–‡ä»¶")
                return
    progress.log_with_timestamp(f"ğŸ“„ ä½¿ç”¨ç­›é€‰æ–‡ä»¶: {filter_output_file}")
    
    # æ£€æŸ¥ç­›é€‰ç»“æœ
    try:
        with open(filter_output_file, 'r', encoding='utf-8') as f:
            filtered_papers = json.load(f)
        progress.log_with_timestamp(f"ğŸ“Š ç­›é€‰åè®ºæ–‡æ•°é‡: {len(filtered_papers)}")
        
        if len(filtered_papers) == 0:
            progress.log_with_timestamp("âš ï¸ ç­›é€‰åæ²¡æœ‰è®ºæ–‡ï¼Œæµæ°´çº¿ç»ˆæ­¢")
            return
    except Exception as e:
        progress.log_with_timestamp(f"âŒ è¯»å–ç­›é€‰æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # ============ æ­¥éª¤3: ç”Ÿæˆè®ºæ–‡æ€»ç»“ ============
    summary_output_file = filter_output_file  # é»˜è®¤ä½¿ç”¨ç­›é€‰åçš„æ–‡ä»¶
    
    if not args.skip_summary:
        progress.start_step("ç”Ÿæˆè®ºæ–‡æ€»ç»“")
        
        cmd = [
            sys.executable, "src/core/generate_summary.py",
            "--input-file", filter_output_file,
            "--output-dir", SUMMARY_DIR,
            "--api-key", args.api_key,
            "--base-url", args.base_url,
            "--model", args.model,
            "--temperature", str(args.temperature),
            "--skip-existing",
            "--max-workers", str(args.max_workers)
        ]
        
        if run_command(cmd, "ç”Ÿæˆè®ºæ–‡æ€»ç»“", progress):
            # æŸ¥æ‰¾ç”Ÿæˆçš„å¸¦æœ‰summary2çš„JSONæ–‡ä»¶
            filter_filename = os.path.basename(filter_output_file)
            name_without_ext = os.path.splitext(filter_filename)[0]
            summary_output_filename = f"{name_without_ext}_with_summary2.json"
            summary_output_file = os.path.join(SUMMARY_DIR, summary_output_filename)
            
            if os.path.exists(summary_output_file):
                progress.log_with_timestamp(f"ğŸ“„ ä½¿ç”¨å¸¦æ€»ç»“çš„æ–‡ä»¶: {summary_output_file}")
            else:
                progress.log_with_timestamp("âš ï¸ æœªæ‰¾åˆ°å¸¦æ€»ç»“çš„JSONæ–‡ä»¶ï¼Œä½¿ç”¨åŸå§‹ç­›é€‰æ–‡ä»¶")
                summary_output_file = filter_output_file
            progress.complete_step("ç”Ÿæˆè®ºæ–‡æ€»ç»“", True)
        else:
            progress.complete_step("ç”Ÿæˆè®ºæ–‡æ€»ç»“", False)
            progress.log_with_timestamp("âš ï¸ æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤")
    else:
        progress.skip_step("ç”Ÿæˆè®ºæ–‡æ€»ç»“")
        # å¦‚æœè·³è¿‡æ€»ç»“ï¼Œå°è¯•ä½¿ç”¨å·²å­˜åœ¨çš„å¸¦summary2æ–‡ä»¶
        try:
            if filter_output_file:
                filter_filename = os.path.basename(filter_output_file)
                name_without_ext = os.path.splitext(filter_filename)[0]
                candidate = os.path.join(SUMMARY_DIR, f"{name_without_ext}_with_summary2.json")
                if os.path.exists(candidate):
                    summary_output_file = candidate
                    progress.log_with_timestamp(f"ğŸ“„ ä½¿ç”¨å·²æœ‰çš„å¸¦æ€»ç»“æ–‡ä»¶: {summary_output_file}")
                else:
                    # å…œåº•ï¼šæŸ¥æ‰¾SUMMARY_DIRä¸‹æœ€è¿‘çš„ *_with_summary2.json
                    from glob import glob
                    candidates = glob(os.path.join(SUMMARY_DIR, "*_with_summary2.json"))
                    if candidates:
                        latest_summary = max(candidates, key=os.path.getmtime)
                        summary_output_file = latest_summary
                        progress.log_with_timestamp(f"ğŸ“„ ä½¿ç”¨æœ€è¿‘çš„å¸¦æ€»ç»“æ–‡ä»¶: {summary_output_file}")
                    else:
                        progress.log_with_timestamp("âš ï¸ æœªæ‰¾åˆ°å¸¦æ€»ç»“æ–‡ä»¶ï¼Œä½¿ç”¨ç­›é€‰æ–‡ä»¶")
            else:
                progress.log_with_timestamp("âš ï¸ æ— ç­›é€‰æ–‡ä»¶å¯ç”¨äºåŒ¹é…æ€»ç»“ï¼Œç»§ç»­ä½¿ç”¨ç­›é€‰æ–‡ä»¶")
        except Exception as e:
            progress.log_with_timestamp(f"âš ï¸ æ£€æŸ¥å·²æœ‰æ€»ç»“æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    # ============ æ­¥éª¤4: ç”Ÿæˆç»Ÿä¸€é¡µé¢ ============
    if not args.skip_unified:
        progress.start_step("ç”Ÿæˆç»Ÿä¸€é¡µé¢")
        
        try:
            # æ£€æŸ¥å¿…è¦æ–‡ä»¶
            if not os.path.exists("src/core/generate_unified_index.py"):
                progress.log_with_timestamp("âš ï¸ æœªæ‰¾åˆ° src/core/generate_unified_index.pyï¼Œè·³è¿‡ç»Ÿä¸€é¡µé¢ç”Ÿæˆ")
                progress.skip_step("ç”Ÿæˆç»Ÿä¸€é¡µé¢")
            elif not os.path.exists(SUMMARY_DIR) or not any(f.endswith('.json') for f in os.listdir(SUMMARY_DIR)):
                progress.log_with_timestamp("âš ï¸ æœªæ‰¾åˆ°è®ºæ–‡æ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡ç»Ÿä¸€é¡µé¢ç”Ÿæˆ")
                progress.skip_step("ç”Ÿæˆç»Ÿä¸€é¡µé¢")
            else:
                # è¿è¡Œç»Ÿä¸€é¡µé¢ç”Ÿæˆè„šæœ¬
                cmd = [sys.executable, "src/core/generate_unified_index.py"]
                
                if run_command(cmd, "ç”Ÿæˆç»Ÿä¸€é¡µé¢", progress):
                    unified_page_path = os.path.join(WEBPAGES_DIR, "index.html")
                    if os.path.exists(unified_page_path):
                        progress.log_with_timestamp(f"âœ… ç»Ÿä¸€é¡µé¢å·²ç”Ÿæˆ: {unified_page_path}")
                        progress.complete_step("ç”Ÿæˆç»Ÿä¸€é¡µé¢", True)
                    else:
                        progress.log_with_timestamp("âš ï¸ ç»Ÿä¸€é¡µé¢ç”Ÿæˆè„šæœ¬è¿è¡ŒæˆåŠŸä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶")
                        progress.complete_step("ç”Ÿæˆç»Ÿä¸€é¡µé¢", False)
                else:
                    progress.complete_step("ç”Ÿæˆç»Ÿä¸€é¡µé¢", False)
        except Exception as e:
            progress.log_with_timestamp(f"âŒ ç»Ÿä¸€é¡µé¢ç”Ÿæˆå¤±è´¥: {e}")
            progress.complete_step("ç”Ÿæˆç»Ÿä¸€é¡µé¢", False)
    else:
        progress.skip_step("ç”Ÿæˆç»Ÿä¸€é¡µé¢")
    
    # ============ æ­¥éª¤5: å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨ ============
    if not args.skip_serve:
        progress.start_step("å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç½‘é¡µæ–‡ä»¶
        if os.path.exists(WEBPAGES_DIR) and os.listdir(WEBPAGES_DIR):
            progress.log_with_timestamp(f"ğŸš€ å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨ï¼Œè®¿é—®ç½‘é¡µ...")
            progress.log_with_timestamp(f"ğŸ“‚ ç½‘é¡µç›®å½•: {WEBPAGES_DIR}")
            progress.log_with_timestamp("ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
            
            # ç›´æ¥è°ƒç”¨æœåŠ¡å™¨æ¨¡å—
            try:
                cmd = [sys.executable, "src/core/serve_webpages.py", "--webpages-dir", WEBPAGES_DIR]
                subprocess.run(cmd)
                progress.complete_step("å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨", True)
            except KeyboardInterrupt:
                progress.log_with_timestamp("\nğŸ›‘ æœåŠ¡å™¨å·²åœæ­¢")
                progress.complete_step("å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨", True)
        else:
            progress.log_with_timestamp("âš ï¸ ç½‘é¡µç›®å½•ä¸ºç©ºï¼Œè·³è¿‡æœåŠ¡å™¨å¯åŠ¨")
            progress.complete_step("å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨", False)
    else:
        progress.skip_step("å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨")
    
    # ============ å®Œæˆæ€»ç»“ ============
    print("\n" + "=" * 60)
    progress.show_summary()
    print("ğŸ“Š å¤„ç†æ€»ç»“:")
    
    if crawl_output_file and os.path.exists(crawl_output_file):
        try:
            with open(crawl_output_file, 'r', encoding='utf-8') as f:
                crawl_papers = json.load(f)
            progress.log_with_timestamp(f"  ğŸ“¥ çˆ¬å–è®ºæ–‡: {len(crawl_papers)} ç¯‡")
        except:
            progress.log_with_timestamp(f"  ğŸ“¥ çˆ¬å–æ–‡ä»¶: {crawl_output_file}")
    
    if filter_output_file and os.path.exists(filter_output_file):
        try:
            with open(filter_output_file, 'r', encoding='utf-8') as f:
                filter_papers = json.load(f)
            progress.log_with_timestamp(f"  ğŸ” ç­›é€‰è®ºæ–‡: {len(filter_papers)} ç¯‡")
        except:
            progress.log_with_timestamp(f"  ğŸ” ç­›é€‰æ–‡ä»¶: {filter_output_file}")
    
    if os.path.exists(SUMMARY_DIR):
        summary_files = len([f for f in os.listdir(SUMMARY_DIR) if f.endswith('.md')])
        progress.log_with_timestamp(f"  ğŸ“ ç”Ÿæˆæ€»ç»“: {summary_files} ç¯‡")
    
    if os.path.exists(WEBPAGES_DIR):
        webpage_dirs = len([d for d in os.listdir(WEBPAGES_DIR) if os.path.isdir(os.path.join(WEBPAGES_DIR, d))])
        progress.log_with_timestamp(f"  ğŸŒ ç”Ÿæˆç½‘é¡µ: {webpage_dirs} ä¸ª")
    
    print("\nğŸ“ è¾“å‡ºç›®å½•:")
    progress.log_with_timestamp(f"  - çˆ¬å–ç»“æœ: {ARXIV_PAPER_DIR}")
    progress.log_with_timestamp(f"  - ç­›é€‰ç»“æœ: {DOMAIN_PAPER_DIR}")
    progress.log_with_timestamp(f"  - è®ºæ–‡æ€»ç»“: {SUMMARY_DIR}")
    progress.log_with_timestamp(f"  - äº¤äº’ç½‘é¡µ: {WEBPAGES_DIR}")
    
    unified_page_path = os.path.join(WEBPAGES_DIR, "index.html")
    if os.path.exists(unified_page_path):
        progress.log_with_timestamp(f"  âœ¨ ç»Ÿä¸€é¡µé¢: {unified_page_path}")
    
    print("\nğŸŒ æ‰‹åŠ¨å¯åŠ¨æœåŠ¡å™¨:")
    progress.log_with_timestamp(f"  python src/core/serve_webpages.py --webpages-dir {WEBPAGES_DIR}")
    
    print("\nâœ¨ æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main()
