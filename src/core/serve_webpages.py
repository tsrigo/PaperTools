#!/usr/bin/env python3
"""
æœ¬åœ°ç½‘é¡µæœåŠ¡å™¨
Local web server for serving generated academic paper webpages
"""

import os
import sys
import http.server
import socketserver
import webbrowser
import argparse
from pathlib import Path
import json
import urllib.parse
import shutil
import re

# å¯¼å…¥é…ç½®
import sys
import os
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.utils.config import WEBPAGES_DIR, ENABLE_TIME_BASED_STRUCTURE, DATE_FORMAT
from src.utils.cache_manager import get_available_dates


class CustomHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):
    """è‡ªå®šä¹‰HTTPè¯·æ±‚å¤„ç†å™¨ï¼Œæ·»åŠ CORSä¸ç®€å•API"""
    
    def end_headers(self):
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        super().end_headers()
    
    def log_message(self, format, *args):
        """è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼"""
        print(f"[{self.log_date_time_string()}] {format % args}")

    # ---- ç®€å•çš„ç”¨æˆ·çŠ¶æ€å­˜å– ----
    def _state_file_for_date(self, date_str: str) -> str:
        # çŠ¶æ€æ–‡ä»¶ä¿å­˜åœ¨å¯¹åº”æ—¥æœŸç›®å½•ä¸‹
        return os.path.join('.', date_str, '.user_state.json')

    def _load_state(self, date_str: str) -> dict:
        state_file = self._state_file_for_date(date_str)
        if os.path.exists(state_file):
            try:
                with open(state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {"deleted_ids": [], "read_ids": []}
        return {"deleted_ids": [], "read_ids": []}

    def _save_state(self, date_str: str, state: dict) -> None:
        state_file = self._state_file_for_date(date_str)
        try:
            os.makedirs(os.path.dirname(state_file), exist_ok=True)
            with open(state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜çŠ¶æ€å¤±è´¥({date_str}): {e}")

    # ---- é¢„æ£€è¯·æ±‚ ----
    def do_OPTIONS(self):
        self.send_response(204)
        self.end_headers()

    # ---- API: è·å–çŠ¶æ€ ----
    def _handle_get_state(self):
        parsed = urllib.parse.urlparse(self.path)
        params = urllib.parse.parse_qs(parsed.query)
        date_str = params.get('date', [''])[0]
        if not date_str:
            self.send_response(400)
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({"error": "missing date"}).encode('utf-8'))
            return
        state = self._load_state(date_str)
        self.send_response(200)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(state).encode('utf-8'))

    # ---- API: åˆ é™¤è®ºæ–‡ ----
    def _handle_delete(self, payload: dict):
        date_str = payload.get('date', '')
        arxiv_id = payload.get('arxiv_id', '')
        paper_dir = payload.get('paper_dir', '')  # å¯é€‰ï¼Œè‹¥æä¾›åˆ™åˆ é™¤è¯¥ç›®å½•
        title = payload.get('title', '')  # å¯é€‰ï¼Œç”¨äºå°è¯•åŒ¹é…ç›®å½•
        if not date_str or not arxiv_id:
            self.send_response(400)
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({"error": "missing date or arxiv_id"}).encode('utf-8'))
            return

        state = self._load_state(date_str)
        if arxiv_id not in state["deleted_ids"]:
            state["deleted_ids"].append(arxiv_id)
        # åŒæ—¶ä»å·²è¯»é‡Œç§»é™¤å®ƒ
        if arxiv_id in state.get("read_ids", []):
            state["read_ids"].remove(arxiv_id)
        self._save_state(date_str, state)

        # åˆ é™¤å¯¹åº”ç›®å½•ï¼ˆè‹¥æä¾›ä¸”å­˜åœ¨ï¼‰
        deleted_dir = False
        if paper_dir:
            # åªå…è®¸åˆ é™¤æ—¥æœŸç›®å½•ä¸‹çš„å­è·¯å¾„ï¼Œé¿å…è¶Šæƒ
            safe_base = os.path.abspath(os.path.join('.', date_str))
            target_path = os.path.abspath(os.path.join('.', paper_dir))
            if target_path.startswith(safe_base) and os.path.isdir(target_path):
                try:
                    shutil.rmtree(target_path)
                    deleted_dir = True
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤ç›®å½•å¤±è´¥: {target_path}: {e}")
        else:
            # æœªæä¾›å…·ä½“ç›®å½•æ—¶ï¼Œå°è¯•åœ¨æ—¥æœŸç›®å½•ä¸‹æ ¹æ®æ ‡é¢˜çŒœæµ‹ç›®å½•
            try:
                date_dir = os.path.abspath(os.path.join('.', date_str))
                if os.path.isdir(date_dir) and title:
                    # ç”Ÿæˆå®‰å…¨æ ‡é¢˜ï¼ˆä¸ç”Ÿæˆé€»è¾‘ç›¸ä¼¼ï¼‰
                    safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:100]
                    # åœ¨æ—¥æœŸç›®å½•ä¸‹æŸ¥æ‰¾åŒ…å«å®‰å…¨æ ‡é¢˜çš„å­ç›®å½•
                    for name in os.listdir(date_dir):
                        sub = os.path.join(date_dir, name)
                        if os.path.isdir(sub) and safe_title in name:
                            try:
                                shutil.rmtree(sub)
                                deleted_dir = True
                                break
                            except Exception as e:
                                print(f"âš ï¸ åˆ é™¤æ¨æµ‹ç›®å½•å¤±è´¥: {sub}: {e}")
            except Exception as e:
                print(f"âš ï¸ æŸ¥æ‰¾æ¨æµ‹ç›®å½•å¤±è´¥: {e}")

        self.send_response(200)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps({"ok": True, "deleted_dir": deleted_dir}).encode('utf-8'))

    # ---- API: å‹¾é€‰é˜…è¯»çŠ¶æ€ ----
    def _handle_toggle_read(self, payload: dict):
        date_str = payload.get('date', '')
        arxiv_id = payload.get('arxiv_id', '')
        read = bool(payload.get('read', False))
        if not date_str or not arxiv_id:
            self.send_response(400)
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({"error": "missing date or arxiv_id"}).encode('utf-8'))
            return

        state = self._load_state(date_str)
        read_ids = set(state.get("read_ids", []))
        if read:
            read_ids.add(arxiv_id)
        else:
            read_ids.discard(arxiv_id)
        state["read_ids"] = sorted(read_ids)
        self._save_state(date_str, state)

        self.send_response(200)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps({"ok": True}).encode('utf-8'))

    def do_GET(self):
        if self.path.startswith('/api/state'):
            return self._handle_get_state()
        return super().do_GET()

    def do_POST(self):
        length = int(self.headers.get('Content-Length', '0') or 0)
        try:
            body = self.rfile.read(length) if length > 0 else b''
            payload = json.loads(body.decode('utf-8') or '{}')
        except Exception:
            payload = {}

        if self.path == '/api/delete':
            return self._handle_delete(payload)
        if self.path == '/api/toggle-read':
            return self._handle_toggle_read(payload)

        self.send_response(404)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps({"error": "not found"}).encode('utf-8'))


def find_available_port(start_port: int = 8080, max_attempts: int = 100) -> int:
    """æ‰¾åˆ°å¯ç”¨çš„ç«¯å£"""
    import socket
    
    for port in range(start_port, start_port + max_attempts):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('localhost', port))
                return port
        except OSError:
            continue
    
    raise RuntimeError(f"æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£ (å°è¯•èŒƒå›´: {start_port}-{start_port + max_attempts})")


def list_directory_contents(directory: str) -> None:
    """åˆ—å‡ºç›®å½•å†…å®¹ï¼Œæ”¯æŒæ—¶é—´å¯¼èˆªæ˜¾ç¤º"""
    if not os.path.exists(directory):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return
    
    print(f"ğŸ“‚ ç›®å½•å†…å®¹ ({directory}):")
    
    # å¦‚æœå¯ç”¨æ—¶é—´åˆ’åˆ†ï¼Œé¦–å…ˆæ˜¾ç¤ºå¯ç”¨æ—¥æœŸ
    if ENABLE_TIME_BASED_STRUCTURE:
        available_dates = get_available_dates(directory)
        if available_dates:
            print("ğŸ“… å¯ç”¨æ—¥æœŸ:")
            for date in available_dates[:10]:  # åªæ˜¾ç¤ºæœ€è¿‘10å¤©
                date_dir = os.path.join(directory, date)
                if os.path.exists(date_dir):
                    paper_count = len([d for d in os.listdir(date_dir) 
                                     if os.path.isdir(os.path.join(date_dir, d))])
                    index_exists = os.path.exists(os.path.join(date_dir, 'index.html'))
                    status = "âœ…" if index_exists else "âŒ"
                    print(f"  {status} {date} ({paper_count} ç¯‡è®ºæ–‡)")
            print()
    
    items = []
    for item in os.listdir(directory):
        item_path = os.path.join(directory, item)
        if os.path.isdir(item_path):
            # æ£€æŸ¥æ˜¯å¦æœ‰index.html
            index_file = os.path.join(item_path, 'index.html')
            if os.path.exists(index_file):
                items.append(f"  ğŸ“„ {item}/ (æœ‰ç½‘é¡µ)")
            else:
                items.append(f"  ğŸ“ {item}/ (æ–‡ä»¶å¤¹)")
        else:
            items.append(f"  ğŸ“„ {item}")
    
    if items:
        print("ğŸ“‹ æ–‡ä»¶å’Œæ–‡ä»¶å¤¹:")
        for item in sorted(items)[:20]:  # åªæ˜¾ç¤ºå‰20é¡¹
            print(item)
        
        if len(items) > 20:
            print(f"  ... è¿˜æœ‰ {len(items) - 20} é¡¹")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å­¦æœ¯è®ºæ–‡ç½‘é¡µæœ¬åœ°æœåŠ¡å™¨')
    parser.add_argument('--webpages-dir', default=WEBPAGES_DIR,
                       help=f'ç½‘é¡µæ–‡ä»¶ç›®å½• (é»˜è®¤: {WEBPAGES_DIR})')
    parser.add_argument('--port', type=int, default=0,
                       help='æœåŠ¡å™¨ç«¯å£ï¼Œ0è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹© (é»˜è®¤: 0)')
    parser.add_argument('--no-browser', action='store_true',
                       help='ä¸è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨')
    parser.add_argument('--list-only', action='store_true',
                       help='ä»…åˆ—å‡ºç›®å½•å†…å®¹ï¼Œä¸å¯åŠ¨æœåŠ¡å™¨')
    parser.add_argument('--date', default=None,
                       help='æŒ‡å®šè¦æœåŠ¡çš„æ—¥æœŸç›®å½• (æ ¼å¼: YYYY-MM-DD)')
    
    args = parser.parse_args()
    
    # å¤„ç†æ—¥æœŸå‚æ•°
    if args.date and ENABLE_TIME_BASED_STRUCTURE:
        try:
            # éªŒè¯æ—¥æœŸæ ¼å¼
            from datetime import datetime
            datetime.strptime(args.date, DATE_FORMAT)
            # æ„å»ºæ—¥æœŸç›®å½•è·¯å¾„
            date_dir = os.path.join(args.webpages_dir, args.date)
            if os.path.exists(date_dir):
                args.webpages_dir = date_dir
                print(f"ğŸ“… ä½¿ç”¨æ—¥æœŸç›®å½•: {args.date}")
            else:
                print(f"âŒ æ—¥æœŸç›®å½•ä¸å­˜åœ¨: {date_dir}")
                return
        except ValueError:
            print(f"âŒ æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {args.date}ï¼Œåº”ä¸º YYYY-MM-DD")
            return
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.webpages_dir):
        print(f"âŒ é”™è¯¯: ç½‘é¡µç›®å½•ä¸å­˜åœ¨: {args.webpages_dir}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ç”Ÿæˆç½‘é¡µ:")
        print("   python generate_webpage.py --input-file <è®ºæ–‡æ–‡ä»¶> --output-dir webpages")
        return
    
    # ä»…åˆ—å‡ºç›®å½•å†…å®¹
    if args.list_only:
        list_directory_contents(args.webpages_dir)
        return
    
    # åˆ‡æ¢åˆ°ç½‘é¡µç›®å½•
    original_dir = os.getcwd()
    os.chdir(args.webpages_dir)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰index.html
    if not os.path.exists('index.html'):
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°index.htmlæ–‡ä»¶")
        list_directory_contents('.')
        print()
    
    try:
        # ç¡®å®šç«¯å£
        if args.port == 0:
            port = find_available_port()
        else:
            port = args.port
        
        # åˆ›å»ºHTTPæœåŠ¡å™¨
        httpd = socketserver.TCPServer(("", port), CustomHTTPRequestHandler)
        
        print(f"ğŸš€ æ­£åœ¨å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨...")
        print(f"ğŸ“ æœåŠ¡å™¨åœ°å€: http://localhost:{port}")
        print(f"ğŸ“‚ æœåŠ¡ç›®å½•: {os.path.abspath('.')}")
        print(f"ğŸ›‘ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
        print("=" * 50)
        
        # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
        if not args.no_browser:
            try:
                webbrowser.open(f'http://localhost:{port}')
                print("ğŸŒ å·²è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨: {e}")
        
        # å¯åŠ¨æœåŠ¡å™¨
        httpd.serve_forever()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æœåŠ¡å™¨å·²åœæ­¢")
        httpd.shutdown()
    except OSError as e:
        if "Address already in use" in str(e):
            print(f"âŒ ç«¯å£ {port} å·²è¢«å ç”¨")
            if args.port == 0:
                print("ğŸ’¡ æ­£åœ¨å°è¯•å…¶ä»–ç«¯å£...")
                try:
                    port = find_available_port(port + 1)
                    httpd = socketserver.TCPServer(("", port), CustomHTTPRequestHandler)
                    print(f"âœ… ä½¿ç”¨ç«¯å£ {port}")
                    print(f"ğŸ“ æœåŠ¡å™¨åœ°å€: http://localhost:{port}")
                    httpd.serve_forever()
                except Exception as retry_e:
                    print(f"âŒ é‡è¯•å¤±è´¥: {retry_e}")
            else:
                print(f"ğŸ’¡ è¯·å°è¯•å…¶ä»–ç«¯å£: python serve_webpages.py --port {port + 1}")
        else:
            print(f"âŒ å¯åŠ¨æœåŠ¡å™¨æ—¶å‡ºé”™: {e}")
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
    finally:
        # æ¢å¤åŸç›®å½•
        os.chdir(original_dir)


if __name__ == "__main__":
    main()
