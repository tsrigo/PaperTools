#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆarXivè®ºæ–‡çˆ¬å–è„šæœ¬
Enhanced arXiv paper crawler with improved functionality
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import sys
from datetime import datetime
import time
import argparse
from typing import List, Dict, Tuple, Set
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥é…ç½®
try:
    from src.utils.config import ARXIV_PAPER_DIR, CRAWL_CATEGORIES, MAX_PAPERS_PER_CATEGORY, MAX_WORKERS, DATE_FORMAT
    from src.utils.cache_manager import CacheManager
except ImportError:
    # å¦‚æœæ²¡æœ‰configæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    ARXIV_PAPER_DIR = "arxiv_paper"
    CRAWL_CATEGORIES = ['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG', 'cs.MA']
    MAX_PAPERS_PER_CATEGORY = 1000
    MAX_WORKERS = 4
    DATE_FORMAT = "%Y-%m-%d"
    CacheManager = None

# å…¨å±€ç¼“å­˜ç®¡ç†å™¨
_cache_manager = None

def get_cache_manager():
    """è·å–ç¼“å­˜ç®¡ç†å™¨å•ä¾‹"""
    global _cache_manager
    if _cache_manager is None and CacheManager is not None:
        _cache_manager = CacheManager()
    return _cache_manager

# åŸºç¡€URLæ¨¡æ¿
base_url = "https://papers.cool/arxiv/{}?show={}"
# æŒ‰æ—¥æœŸæŸ¥è¯¢çš„URLæ¨¡æ¿
date_url = "https://papers.cool/arxiv/{}?date={}&show={}"

def _normalize_date_to_yyyy_mm_dd(raw_text: str) -> str:
    """ä»ä»»æ„åŒ…å«æ—¥æœŸçš„å­—ç¬¦ä¸²ä¸­æå–å¹¶è§„èŒƒåŒ–ä¸º YYYY-MM-DDã€‚

    æ”¯æŒçš„ç¤ºä¾‹ï¼š
    - 2025-09-24
    - 2025/09/24
    - 2025.09.24
    - 2025-09-24T12:34:56Z
    - 2025/09/24 10:00
    è¿”å›è§„èŒƒåŒ–åçš„æ—¥æœŸå­—ç¬¦ä¸²ï¼›è‹¥æ— æ³•æå–ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    if not raw_text:
        return ""

    text = raw_text.strip()

    # ç»Ÿä¸€ T åˆ†éš”çš„æ—¥æœŸæ—¶é—´
    if 'T' in text:
        text = text.split('T', 1)[0]

    # å¸¸è§åˆ†éš”ç¬¦æ›¿æ¢ä¸º '-'
    text = text.replace('/', '-').replace('.', '-')

    # åŒ¹é… YYYY-MM-DD
    m = re.search(r"(\d{4})-(\d{1,2})-(\d{1,2})", text)
    if not m:
        return ""

    year, month, day = m.groups()
    try:
        dt = datetime(int(year), int(month), int(day))
        return dt.strftime('%Y-%m-%d')
    except Exception:
        return ""

def _extract_date_from_div(div) -> str:
    """å°½å¯èƒ½ä»è®ºæ–‡æ¡ç›®çš„ DOM ç»“æ„ä¸­æå–å¹¶è§„èŒƒåŒ–æ—¥æœŸä¸º YYYY-MM-DDã€‚"""
    # 1) åŸé€‰æ‹©å™¨
    date_p = div.find('p', class_='metainfo date')
    date_span = date_p.find('span', class_='date-data') if date_p else None
    if date_span and date_span.text:
        norm = _normalize_date_to_yyyy_mm_dd(date_span.text)
        if norm:
            return norm

    # 2) å›é€€ï¼šä»»ä½• class å« "date" çš„å…ƒç´ 
    any_date_el = div.find(lambda tag: tag.has_attr('class') and any('date' in c for c in tag['class']))
    if any_date_el and any_date_el.text:
        norm = _normalize_date_to_yyyy_mm_dd(any_date_el.text)
        if norm:
            return norm

    # 3) å›é€€ï¼šåœ¨æ•´å—æ–‡æœ¬é‡Œç”¨æ­£åˆ™æå–
    block_text = div.get_text(separator=' ', strip=True)
    norm = _normalize_date_to_yyyy_mm_dd(block_text)
    return norm

def scrape_papers_for_date_range(category: str, max_papers: int, delay: float, start_date: str, end_date: str, use_cache: bool = True) -> Tuple[List[Dict], Set[str]]:
    """
    çˆ¬å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„è®ºæ–‡

    Args:
        category: è®ºæ–‡ç±»åˆ«
        max_papers: æœ€å¤§çˆ¬å–æ•°é‡
        delay: è¯·æ±‚é—´éš”æ—¶é—´
        start_date: èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

    Returns:
        Tuple[List[Dict], Set[str]]: (è®ºæ–‡åˆ—è¡¨, è®ºæ–‡IDé›†åˆ)
    """
    from datetime import datetime, timedelta

    all_papers = []
    all_paper_ids = set()

    # è§£ææ—¥æœŸ
    start_dt = datetime.strptime(start_date, '%Y-%m-%d')
    end_dt = datetime.strptime(end_date, '%Y-%m-%d')

    print(f"ğŸ” æ­£åœ¨çˆ¬å–ç±»åˆ« {category}ï¼Œæ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")

    # éå†æ—¥æœŸèŒƒå›´
    current_dt = start_dt
    while current_dt <= end_dt:
        current_date_str = current_dt.strftime('%Y-%m-%d')
        print(f"  ğŸ“… çˆ¬å–æ—¥æœŸ: {current_date_str}")

        papers, paper_ids = scrape_papers(category, max_papers, delay, current_date_str, use_cache)

        # åˆå¹¶ç»“æœï¼Œé¿å…é‡å¤
        for paper in papers:
            paper_id = paper.get('arxiv_id', '') or paper['link'].split('/')[-1]
            if paper_id not in all_paper_ids:
                all_papers.append(paper)
                all_paper_ids.add(paper_id)

        current_dt += timedelta(days=1)

        # æ·»åŠ é¢å¤–å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(delay)

    print(f"âœ… æ—¥æœŸèŒƒå›´çˆ¬å–å®Œæˆ {category}: {len(all_papers)} ç¯‡å»é‡è®ºæ–‡")
    return all_papers, all_paper_ids


def scrape_papers(category: str, max_papers: int = MAX_PAPERS_PER_CATEGORY, delay: float = 1.0, target_date: str = None, use_cache: bool = True) -> Tuple[List[Dict], Set[str]]:
    """
    çˆ¬å–æŒ‡å®šç±»åˆ«çš„è®ºæ–‡

    Args:
        category: è®ºæ–‡ç±»åˆ«ï¼Œå¦‚ 'cs.AI'
        max_papers: æœ€å¤§çˆ¬å–æ•°é‡
        delay: è¯·æ±‚é—´éš”æ—¶é—´
        target_date: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'ï¼Œå¦‚æœä¸ºNoneåˆ™çˆ¬å–æœ€æ–°è®ºæ–‡
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

    Returns:
        Tuple[List[Dict], Set[str]]: (è®ºæ–‡åˆ—è¡¨, è®ºæ–‡IDé›†åˆ)
    """
    # å°è¯•ä»ç¼“å­˜è·å–
    cache_manager = get_cache_manager()
    cache_date = target_date or datetime.now().strftime('%Y-%m-%d')

    if use_cache and cache_manager:
        cached_papers = cache_manager.get_crawl_cache(category, cache_date)
        if cached_papers is not None:
            print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½ {category} ({cache_date}): {len(cached_papers)} ç¯‡è®ºæ–‡")
            paper_ids = set(p.get('arxiv_id', p['link'].split('/')[-1]) for p in cached_papers)
            return cached_papers, paper_ids

    if target_date:
        url = date_url.format(category, target_date, max_papers)
        print(f"ğŸ” æ­£åœ¨çˆ¬å–ç±»åˆ« {category}ï¼Œæ—¥æœŸ: {target_date}ï¼Œæœ€å¤§æ•°é‡: {max_papers}")
    else:
        url = base_url.format(category, max_papers)
        print(f"ğŸ” æ­£åœ¨çˆ¬å–ç±»åˆ« {category}ï¼Œæœ€å¤§æ•°é‡: {max_papers}")
    
    papers = []
    paper_ids = set()
    
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(delay)
        
    except requests.RequestException as e:
        print(f"âŒ è·å– {category} å¤±è´¥: {e}")
        return papers, paper_ids

    soup = BeautifulSoup(response.text, 'html.parser')
    paper_divs = soup.find_all('div', class_='panel paper')

    print(f"ğŸ“„ æ‰¾åˆ° {len(paper_divs)} ä¸ªè®ºæ–‡æ¡ç›®")
    
    for div in tqdm(paper_divs, desc=f"è§£æ {category}", leave=False):
        paper_id = div.get('id', '')
        if paper_id in paper_ids:
            continue

        # æå–è®ºæ–‡ä¿¡æ¯
        index_span = div.find('span', class_='index notranslate')
        index = index_span.text.strip() if index_span else ''

        title_a = div.find('a', class_='title-link')
        title = title_a.text.strip() if title_a else ''
        link = title_a['href'] if title_a else ''

        authors_p = div.find('p', class_='metainfo authors notranslate')
        authors_list = [a.text.strip() for a in authors_p.find_all('a', class_='author notranslate')] if authors_p else []
        authors = ', '.join(authors_list)

        summary_p = div.find('p', class_='summary')
        summary = summary_p.text.strip() if summary_p else ''

        subjects_p = div.find('p', class_='metainfo subjects')
        subjects_list = [a.text.strip() for a in subjects_p.find_all('a', class_=lambda x: x and x.startswith('subject-'))] if subjects_p else []
        subjects = ', '.join(subjects_list)

        # è§£æå‘å¸ƒæ—¥æœŸï¼Œå¸¦æœ‰å¤šé‡å›é€€ä¸è§„èŒƒåŒ–
        date = _extract_date_from_div(div)

        # æå–arXiv ID
        arxiv_id = ''
        if link:
            arxiv_id = link.split('/')[-1] if '/' in link else link

        paper = {
            'index': index,
            'title': title,
            'link': link,
            'arxiv_id': arxiv_id,
            'authors': authors,
            'summary': summary,
            'subjects': subjects,
            'date': date,
            'category': category,
            'crawl_time': datetime.now().isoformat()
        }

        papers.append(paper)
        paper_ids.add(paper_id)

    print(f"âœ… æˆåŠŸçˆ¬å– {len(papers)} ç¯‡è®ºæ–‡ ({category})")

    # ä¿å­˜åˆ°ç¼“å­˜
    if use_cache and cache_manager and papers:
        cache_manager.set_crawl_cache(category, cache_date, papers)
        print(f"ğŸ’¾ å·²ç¼“å­˜ {category} ({cache_date}): {len(papers)} ç¯‡è®ºæ–‡")

    return papers, paper_ids

def save_papers(all_papers: Dict, selected_categories: List[str], output_dir: str, current_date: str, target_date: str = None) -> str:
    """ä»…ä¿å­˜æ‰€æœ‰è®ºæ–‡çš„åˆå¹¶æ–‡ä»¶åˆ°JSONã€‚"""
    
    # å¦‚æœæŒ‡å®šäº†ç›®æ ‡æ—¥æœŸï¼Œä½¿ç”¨ç›®æ ‡æ—¥æœŸä½œä¸ºæ–‡ä»¶ååç¼€
    # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡æ—¥æœŸï¼Œä»è®ºæ–‡æ•°æ®ä¸­æ¨æ–­æœ€å¸¸è§çš„å‘å¸ƒæ—¥æœŸ
    if target_date:
        date_suffix = target_date
    else:
        # ä»è®ºæ–‡ä¸­æ¨æ–­æœ€å¸¸è§çš„å‘å¸ƒæ—¥æœŸ
        paper_dates = []
        for paper in all_papers.values():
            paper_date = paper.get('date', '')
            if not paper_date:
                continue
            norm = _normalize_date_to_yyyy_mm_dd(paper_date)
            if norm:
                paper_dates.append(norm)
        
        if paper_dates:
            # ä½¿ç”¨æœ€æ–°çš„è®ºæ–‡å‘å¸ƒæ—¥æœŸ
            date_suffix = max(paper_dates)
            print(f"ğŸ“… ä»è®ºæ–‡æ•°æ®ä¸­æ¨æ–­å‡ºå‘å¸ƒæ—¥æœŸ: {date_suffix}")
        else:
            # å¦‚æœæ— æ³•æ¨æ–­ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
            date_suffix = current_date
            print(f"âš ï¸ æ— æ³•æ¨æ–­è®ºæ–‡å‘å¸ƒæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ: {date_suffix}")

    # åªä¿å­˜åˆå¹¶æ–‡ä»¶
    combined_filename = f"{'_'.join(sorted(selected_categories))}_paper_{date_suffix}.json"
    combined_filepath = os.path.join(output_dir, combined_filename)
    with open(combined_filepath, 'w', encoding='utf-8') as f:
        json.dump(list(all_papers.values()), f, ensure_ascii=False, indent=4)
    print(f"ğŸ“š å·²ä¿å­˜ {len(all_papers)} ç¯‡å»é‡è®ºæ–‡åˆ° {combined_filepath}")
    
    return combined_filepath


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆarXivè®ºæ–‡çˆ¬å–å·¥å…·')
    parser.add_argument('--categories', nargs='+', default=['all'], 
                       help='è¦çˆ¬å–çš„ç±»åˆ«ï¼Œå¯é€‰: cs.AI cs.CL cs.CV cs.LG cs.MA æˆ– all')
    parser.add_argument('--max-papers', type=int, default=MAX_PAPERS_PER_CATEGORY,
                       help=f'æ¯ä¸ªç±»åˆ«æœ€å¤§çˆ¬å–æ•°é‡ (é»˜è®¤: {MAX_PAPERS_PER_CATEGORY})')
    parser.add_argument('--output-dir', default=ARXIV_PAPER_DIR,
                       help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {ARXIV_PAPER_DIR})')
    parser.add_argument('--delay', type=float, default=1.0,
                       help='è¯·æ±‚é—´éš”æ—¶é—´ï¼Œç§’ (é»˜è®¤: 1.0)')
    parser.add_argument('--max-workers', type=int, default=MAX_WORKERS,
                       help=f'æœ€å¤§çº¿ç¨‹æ•° (é»˜è®¤: {MAX_WORKERS})')
    parser.add_argument('--date', default=None,
                       help='æŒ‡å®šæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œä¸æŒ‡å®šåˆ™çˆ¬å–æœ€æ–°è®ºæ–‡')
    parser.add_argument('--start-date', default=None,
                       help='èµ·å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œä¸--end-dateä¸€èµ·ä½¿ç”¨æŒ‡å®šæ—¥æœŸèŒƒå›´')
    parser.add_argument('--end-date', default=None,
                       help='ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œä¸--start-dateä¸€èµ·ä½¿ç”¨æŒ‡å®šæ—¥æœŸèŒƒå›´')
    parser.add_argument('--no-cache', action='store_true',
                       help='ç¦ç”¨ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°çˆ¬å–')
    parser.add_argument('--clear-cache', action='store_true',
                       help='æ¸…ç†è¿‡æœŸç¼“å­˜åé€€å‡º')

    args = parser.parse_args()

    # å¤„ç†æ¸…ç†ç¼“å­˜è¯·æ±‚
    if args.clear_cache:
        cache_manager = get_cache_manager()
        if cache_manager:
            cache_manager.clean_expired_cache()
            stats = cache_manager.get_cache_stats()
            print("ğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
            for cache_type, count in stats.items():
                print(f"  {cache_type}: {count}")
        else:
            print("âš ï¸ ç¼“å­˜ç®¡ç†å™¨ä¸å¯ç”¨")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # è·å–å½“å‰æ—¥æœŸ
    current_date = datetime.now().strftime(DATE_FORMAT)
    
    # å¤„ç†ç±»åˆ«é€‰æ‹©
    selected_categories = args.categories
    if 'all' in selected_categories:
        selected_categories = CRAWL_CATEGORIES
    
    # éªŒè¯ç±»åˆ«
    valid_categories = [cat for cat in selected_categories if cat in CRAWL_CATEGORIES]
    if not valid_categories:
        print("âŒ æ²¡æœ‰é€‰æ‹©æœ‰æ•ˆçš„ç±»åˆ«ã€‚å¯ç”¨ç±»åˆ«:", CRAWL_CATEGORIES)
        return
    
    # å¤„ç†æ—¥æœŸå‚æ•°
    use_date_range = args.start_date and args.end_date
    if use_date_range and args.date:
        print("âŒ ä¸èƒ½åŒæ—¶æŒ‡å®šå•ä¸ªæ—¥æœŸå’Œæ—¥æœŸèŒƒå›´")
        return
    
    print("ğŸš€ å¼€å§‹çˆ¬å–arXivè®ºæ–‡")
    print(f"ğŸ“‹ é€‰æ‹©çš„ç±»åˆ«: {valid_categories}")
    print(f"ğŸ“Š æ¯ç±»æœ€å¤§æ•°é‡: {args.max_papers}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ’¾ ç¼“å­˜: {'ç¦ç”¨' if args.no_cache else 'å¯ç”¨'}")
    
    if use_date_range:
        print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {args.start_date} åˆ° {args.end_date}")
    elif args.date:
        print(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {args.date}")
    else:
        print("ğŸ“… çˆ¬å–æ¨¡å¼: æœ€æ–°è®ºæ–‡")
    print("=" * 50)
    
    # å­˜å‚¨æ‰€æœ‰è®ºæ–‡ï¼Œé¿å…é‡å¤
    all_papers = {}
    global_paper_ids = set()
    
    # å¤šçº¿ç¨‹çˆ¬å–å„ç±»åˆ«è®ºæ–‡
    use_cache = not args.no_cache

    def scrape_category_wrapper(category):
        """åŒ…è£…å‡½æ•°ï¼Œç”¨äºå¤šçº¿ç¨‹æ‰§è¡Œ"""
        try:
            if use_date_range:
                return scrape_papers_for_date_range(category, args.max_papers, args.delay, args.start_date, args.end_date, use_cache)
            else:
                return scrape_papers(category, args.max_papers, args.delay, args.date, use_cache)
        except Exception as e:
            print(f"âŒ çˆ¬å–ç±»åˆ« {category} æ—¶å‡ºé”™: {e}")
            return [], set()
    
    print(f"ğŸ”„ ä½¿ç”¨ {args.max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œçˆ¬å–...")
    
    with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
        # æäº¤æ‰€æœ‰çˆ¬å–ä»»åŠ¡
        future_to_category = {
            executor.submit(scrape_category_wrapper, category): category 
            for category in valid_categories
        }
        
        # æ”¶é›†ç»“æœ
        for future in tqdm(as_completed(future_to_category), total=len(valid_categories), desc="çˆ¬å–ç±»åˆ«"):
            category = future_to_category[future]
            try:
                category_papers, paper_ids = future.result()
                
                for paper in category_papers:
                    paper_id = paper.get('arxiv_id', '') or paper['link'].split('/')[-1]
                    if paper_id not in global_paper_ids:
                        all_papers[paper_id] = paper
                        global_paper_ids.add(paper_id)
                        
            except Exception as e:
                print(f"âŒ å¤„ç†ç±»åˆ« {category} ç»“æœæ—¶å‡ºé”™: {e}")
                continue
    
    if not all_papers:
        print("âŒ æ²¡æœ‰æˆåŠŸçˆ¬å–åˆ°ä»»ä½•è®ºæ–‡")
        return
    
    # ä¿å­˜è®ºæ–‡
    if use_date_range:
        # å¯¹äºæ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨èµ·å§‹æ—¥æœŸä½œä¸ºä¸»è¦æ ‡è¯†
        target_date_for_filename = f"{args.start_date}_to_{args.end_date}"
    else:
        target_date_for_filename = args.date
    
    output_file = save_papers(all_papers, valid_categories, args.output_dir, current_date, target_date_for_filename)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 50)
    print("ğŸ‰ çˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“Š æ€»å…±çˆ¬å–: {len(all_papers)} ç¯‡å»é‡è®ºæ–‡")
    print(f"ğŸ“‚ ä¸»è¾“å‡ºæ–‡ä»¶: {output_file}")
    print("=" * 50)


if __name__ == "__main__":
    main()