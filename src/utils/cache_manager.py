#!/usr/bin/env python3
"""
ç¼“å­˜ç®¡ç†æ¨¡å—
Cache management module for academic paper processing
"""

import os
import json
import hashlib
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any, List

# å¯¼å…¥é…ç½®
try:
    from config import CACHE_DIR, ENABLE_CACHE, CACHE_EXPIRY_DAYS
except ImportError:
    CACHE_DIR = "cache"
    ENABLE_CACHE = True
    CACHE_EXPIRY_DAYS = 30


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = CACHE_DIR):
        self.cache_dir = cache_dir
        self.enabled = ENABLE_CACHE
        self.expiry_days = CACHE_EXPIRY_DAYS

        if self.enabled:
            os.makedirs(self.cache_dir, exist_ok=True)
            # åˆ›å»ºå­ç›®å½•
            os.makedirs(os.path.join(self.cache_dir, "papers"), exist_ok=True)
            os.makedirs(os.path.join(self.cache_dir, "summaries"), exist_ok=True)
            os.makedirs(os.path.join(self.cache_dir, "webpages"), exist_ok=True)
            os.makedirs(os.path.join(self.cache_dir, "crawl"), exist_ok=True)
    
    def _generate_key(self, data: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(data.encode('utf-8')).hexdigest()
    
    def _get_cache_file(self, cache_type: str, key: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, cache_type, f"{key}.json")
    
    def _is_cache_valid(self, cache_file: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆæœªè¿‡æœŸï¼‰"""
        if not os.path.exists(cache_file):
            return False
        
        try:
            cache_time = os.path.getmtime(cache_file)
            cache_datetime = datetime.fromtimestamp(cache_time)
            expiry_datetime = datetime.now() - timedelta(days=self.expiry_days)
            return cache_datetime > expiry_datetime
        except Exception:
            return False
    
    def get_paper_cache(self, paper_url: str) -> Optional[Dict[str, Any]]:
        """è·å–è®ºæ–‡ç¼“å­˜"""
        if not self.enabled:
            return None
        
        key = self._generate_key(paper_url)
        cache_file = self._get_cache_file("papers", key)
        
        if not self._is_cache_valid(cache_file):
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ è¯»å–è®ºæ–‡ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def set_paper_cache(self, paper_url: str, paper_data: Dict[str, Any]) -> None:
        """è®¾ç½®è®ºæ–‡ç¼“å­˜"""
        if not self.enabled:
            return
        
        key = self._generate_key(paper_url)
        cache_file = self._get_cache_file("papers", key)
        
        try:
            cache_data = {
                "url": paper_url,
                "data": paper_data,
                "cached_at": datetime.now().isoformat()
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è®ºæ–‡ç¼“å­˜å¤±è´¥: {e}")
    
    def get_summary_cache(self, paper_title: str, paper_content: str) -> Optional[str]:
        """è·å–æ€»ç»“ç¼“å­˜"""
        if not self.enabled:
            return None
        
        # ä½¿ç”¨æ ‡é¢˜å’Œå†…å®¹çš„ç»„åˆç”Ÿæˆé”®
        key = self._generate_key(f"{paper_title}:{paper_content[:1000]}")
        cache_file = self._get_cache_file("summaries", key)
        
        if not self._is_cache_valid(cache_file):
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                return cache_data.get("summary")
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ€»ç»“ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def set_summary_cache(self, paper_title: str, paper_content: str, summary: str) -> None:
        """è®¾ç½®æ€»ç»“ç¼“å­˜"""
        if not self.enabled:
            return
        
        key = self._generate_key(f"{paper_title}:{paper_content[:1000]}")
        cache_file = self._get_cache_file("summaries", key)
        
        try:
            cache_data = {
                "title": paper_title,
                "summary": summary,
                "cached_at": datetime.now().isoformat()
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ€»ç»“ç¼“å­˜å¤±è´¥: {e}")
    
    def get_webpage_cache(self, paper_title: str, content_hash: str) -> Optional[str]:
        """è·å–ç½‘é¡µç¼“å­˜"""
        if not self.enabled:
            return None
        
        key = self._generate_key(f"{paper_title}:{content_hash}")
        cache_file = self._get_cache_file("webpages", key)
        
        if not self._is_cache_valid(cache_file):
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                return cache_data.get("webpage_content")
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç½‘é¡µç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def set_webpage_cache(self, paper_title: str, content_hash: str, webpage_content: str) -> None:
        """è®¾ç½®ç½‘é¡µç¼“å­˜"""
        if not self.enabled:
            return
        
        key = self._generate_key(f"{paper_title}:{content_hash}")
        cache_file = self._get_cache_file("webpages", key)
        
        try:
            cache_data = {
                "title": paper_title,
                "content_hash": content_hash,
                "webpage_content": webpage_content,
                "cached_at": datetime.now().isoformat()
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç½‘é¡µç¼“å­˜å¤±è´¥: {e}")

    def get_crawl_cache(self, category: str, date: str) -> Optional[List[Dict[str, Any]]]:
        """è·å–çˆ¬å–ç¼“å­˜

        Args:
            category: è®ºæ–‡ç±»åˆ«ï¼Œå¦‚ 'cs.AI'
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            ç¼“å­˜çš„è®ºæ–‡åˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰ç¼“å­˜åˆ™è¿”å› None
        """
        if not self.enabled:
            return None

        key = self._generate_key(f"crawl:{category}:{date}")
        cache_file = self._get_cache_file("crawl", key)

        if not self._is_cache_valid(cache_file):
            return None

        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                return cache_data.get("papers")
        except Exception as e:
            print(f"âš ï¸ è¯»å–çˆ¬å–ç¼“å­˜å¤±è´¥: {e}")
            return None

    def set_crawl_cache(self, category: str, date: str, papers: List[Dict[str, Any]]) -> None:
        """è®¾ç½®çˆ¬å–ç¼“å­˜

        Args:
            category: è®ºæ–‡ç±»åˆ«
            date: æ—¥æœŸå­—ç¬¦ä¸²
            papers: è®ºæ–‡åˆ—è¡¨
        """
        if not self.enabled:
            return

        key = self._generate_key(f"crawl:{category}:{date}")
        cache_file = self._get_cache_file("crawl", key)

        try:
            cache_data = {
                "category": category,
                "date": date,
                "papers": papers,
                "paper_count": len(papers),
                "cached_at": datetime.now().isoformat()
            }

            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜çˆ¬å–ç¼“å­˜å¤±è´¥: {e}")

    def clean_expired_cache(self) -> None:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        if not self.enabled:
            return
        
        print("ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜...")
        cleaned_count = 0
        
        for cache_type in ["papers", "summaries", "webpages", "crawl"]:
            cache_type_dir = os.path.join(self.cache_dir, cache_type)
            if not os.path.exists(cache_type_dir):
                continue

            for cache_file in os.listdir(cache_type_dir):
                cache_path = os.path.join(cache_type_dir, cache_file)
                if not self._is_cache_valid(cache_path):
                    try:
                        os.remove(cache_path)
                        cleaned_count += 1
                    except Exception as e:
                        print(f"âš ï¸ åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_path}: {e}")
        
        if cleaned_count > 0:
            print(f"âœ… å·²æ¸…ç† {cleaned_count} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")
        else:
            print("âœ… æ²¡æœ‰è¿‡æœŸç¼“å­˜æ–‡ä»¶éœ€è¦æ¸…ç†")
    
    def get_cache_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enabled:
            return {"papers": 0, "summaries": 0, "webpages": 0, "crawl": 0, "total": 0}

        stats = {}
        total = 0

        for cache_type in ["papers", "summaries", "webpages", "crawl"]:
            cache_type_dir = os.path.join(self.cache_dir, cache_type)
            if os.path.exists(cache_type_dir):
                count = len([f for f in os.listdir(cache_type_dir) if f.endswith('.json')])
                stats[cache_type] = count
                total += count
            else:
                stats[cache_type] = 0
        
        stats["total"] = total
        return stats


def create_time_based_directory(base_dir: str, date_str: Optional[str] = None) -> str:
    """
    åˆ›å»ºæŒ‰æ—¶é—´åˆ’åˆ†çš„ç›®å½•ç»“æ„
    
    Args:
        base_dir: åŸºç¡€ç›®å½•
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
    
    Returns:
        åˆ›å»ºçš„æ—¶é—´ç›®å½•è·¯å¾„
    """
    if date_str is None:
        date_str = datetime.now().strftime("%Y-%m-%d")
    
    time_dir = os.path.join(base_dir, date_str)
    os.makedirs(time_dir, exist_ok=True)
    return time_dir


def get_available_dates(base_dir: str) -> List[str]:
    """
    è·å–å¯ç”¨çš„æ—¥æœŸåˆ—è¡¨
    
    Args:
        base_dir: åŸºç¡€ç›®å½•
    
    Returns:
        æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨ï¼ŒæŒ‰é™åºæ’åˆ—
    """
    if not os.path.exists(base_dir):
        return []
    
    dates = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ—¥æœŸæ ¼å¼
            try:
                datetime.strptime(item, "%Y-%m-%d")
                dates.append(item)
            except ValueError:
                continue
    
    return sorted(dates, reverse=True)


if __name__ == "__main__":
    # æµ‹è¯•ç¼“å­˜ç®¡ç†å™¨
    cache_manager = CacheManager()
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    stats = cache_manager.get_cache_stats()
    print("ğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
    for cache_type, count in stats.items():
        print(f"  {cache_type}: {count}")
    
    # æ¸…ç†è¿‡æœŸç¼“å­˜
    cache_manager.clean_expired_cache()

