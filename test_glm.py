import os
from openai import OpenAI

client = OpenAI(
    api_key="REDACTED_API_KEY",
    base_url="https://chat.sjtu.plus/v1",
)
completion = client.chat.completions.create(
    model="z-ai/glm-4.7",
    messages=[
        {"role": "system", "content": "ÊÄªÁªìËÆ∫Êñá„ÄÇ"},
        {"role": "user", "content": """ÊÄªÁªìËøôÁØáËÆ∫ÊñáÔºö000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 Under review as a conference paper at ICLR 2026 PARALLEL TEST-TIME SCALING WITH MULTI-SEQUENCE VERIFIERS Anonymous authors Paper under double-blind review ABSTRACT Parallel test-time scaling, which generates multiple candidate solutions for a single problem, is a powerful technique for improving large language model performance. However, it is hindered by two key bottlenecks: accurately selecting the correct solution from the candidate pool, and the high inference latency from generating many full solutions. We argue that both challenges are fundamentally linked to verifier calibration. A well-calibrated verifier not only improves answer selection, but also enables early-stopping strategies to reduce latency. However, existing verifiers are limited as they score each candidate in isolation, overlooking rich contextual information across the set of candidates. To address this, we introduce the Multi-Sequence Verifier (MSV), the first verifier designed to jointly process all candidate solutions and model their interactions. MSV achieves state-ofthe-art calibration, which directly enhances best-of-N selection performance. We further introduce a streaming MSV variant that empowers a novel early-stopping framework. Our novel framework fully leverages parallel decoding, which contrasts with the existing multi-sequence early exit works that decode sequences one by one and thus incur significant latency. In this novel setting, MSV can achieve the same target accuracy with around half the latency that would be required with its counterpart that scores each solution in isolation. 1 INTRODUCTION Large language models (Brown et al., 2020) have become increasingly powerful, with much of their performance unlocked by test-time scaling strategies. One of the most effective strategies is parallel scaling, where a model generates multiple, independent candidate solutions for a single problem (Wang et al., 2022; Lightman et al., 2023; Snell et al., 2024). Parallel scaling can be especially fruitful (Snell et al., 2024) when complemented with sequential scaling methods (Wei et al., 2022; Jaech et al., 2024; Guo et al., 2025) that search for and revise solutions adaptively inside a chain of thought. However, parallel scaling faces two bottlenecks that limit its usage: (1) the selection problem, as accurately identifying the correct solution from a large pool of candidates is difficult, and (2) the high inference latency required to generate numerous full solutions. We argue that these two bottlenecks are not independent; their solutions are deeply connected through the principle of calibration. The selection problem is fundamentally a classification task, where a verifier (Cobbe et al., 2021; Lightman et al., 2023) must accurately estimate the correctness of each solution. Its ability to do so, its calibration (Guo et al., 2017; Kadavath et al., 2022), directly determines the performance of downstream parallel scaling methods such as best-of-N . Concurrently, the high cost of generation can be mitigated by early stopping, a technique that also hinges on a well-calibrated verifier to score intermediate answers and terminate decoding once a threshold score is exceeded (Zhang et al., 2025a; Yang et al., 2025). Although this approach has only been explored in the single-sequence setting, we propose in our paper a way to extend it to parallel scaling. Thus, a verifier with superior calibration is the key to solving both the accuracy and efficiency challenges of parallel scaling. A fundamental limitation of existing verifiers is their isolated approach to scoring, which overlooks the rich contextual information available across a full set of generated outputs. The success of selfconsistency (Wang et al., 2022; Lyu et al., 2025), which relies on the simple cross-sequence statistic of vote counting, serves as a crucial insight: global statistics of sequences can be highly predictive of 1054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 Under review as a conference paper at ICLR 2026 individual correctness. Yet, this insight has remained surprisingly underexplored. We generalize this principle and introduce the Multi-Sequence Verifier (MSV), the first verifier model designed to learn from the interactions between all candidate solutions. By jointly processing multiple outputs, MSV produces highly accurate and calibrated scores, achieving state-of-the-art performance on standard calibration metrics. Multi-Sequence Verifier (MSV)‚Äôs superior calibration directly translates into improved performance on both the parallel scaling bottlenecks. First, it enhances best-of-N decoding, leading to a more accurate final answer selection and more reliable confidence scores for the chosen answers. Second, and more significantly, we introduce a streaming variant of MSV that empowers a novel framework for early stopping with parallel decoding. We decode multiple sequences in parallel while the streaming MSV calibrates their intermediate answers in real-time by jointly observing all sequences. Decoding terminates the moment any one sequence‚Äôs confidence exceeds a threshold. To be concrete, our experiments show that on challenging math reasoning benchmarks, MSV improves best-of-64 accuracy by over 6% relative to strong weighted-voting baselines. This accuracy gain is accompanied by a dramatic improvement in the calibration of the selected answer‚Äôs confidence score, with MSV reducing the Expected Calibration Error by over 75%. The benefits are even more notable in our parallel early-stopping framework, where our streaming MSV achieves the same peak accuracy as baseline verifiers models with as little as half the latency. These downstream improvements stem directly from MSV‚Äôs superior verifier calibration, where it reduces error metrics like the Brier score by 50% compared to models that score sequences in isolation. Our contributions can be summarized as follows: 1. We propose the Multi-Sequence Verifier (MSV), a novel verifier architecture that models cross-sequence interactions to achieve state-of-the-art calibration. 2. We demonstrate that improved calibration from MSV directly translates to enhanced bestof-N selection and its calibration, a primary application of parallel scaling. 3. We generalize an existing early-stopping framework to the parallel decoding setting, which theoretically enables one to scale test-time compute in such a way that latency doesn‚Äôt grow. We also introduce a streaming MSV variant that outperforms strong baselines in this novel setting. 2 RELATED WORK 2.1 CALIBRATION OF LLM OUTPUTS Confidence estimation for LLMs has been studied through black-box and white-box approaches. Black-box approaches have focused on prompting a language model to verbalize its confidence (Lin et al., 2022; Tian et al., 2023). Xiong et al. (2023) finds that white-box approaches to calibration, which instead probe internal states (Kadavath et al., 2022), generally outperform their black-box counterparts. Lyu et al. (2025) investigates using global sequence statistics such as the entropy of answer distribution to calibrate the majority voted answer, but their method cannot be applied to calibrating the other answers. Zhang et al. (2025a) demonstrates an interesting result that intermediate answers from reasoning models such as DeepSeek R1 (Guo et al., 2025) can be robustly calibrated through lightweight probes on their hidden states, and apply this finding to early stopping in the decoding of a single sequence. 2.2 MULTI-SEQUENCE ANSWER SELECTION Answer selection among multiple decoded sequences is a crucial component of parallel test-time scaling. One can regard this task as well-calibrated ranking of answers within a problem such that the correct answers are scored higher than the incorrect ones. As such, the most common approach is to train calibrated verifier models (Cobbe et al., 2021; Zhang et al., 2024). Self-consistency (Wang et al., 2022) is a simple selection method that doesn‚Äôt require training of a verifier model, and instead selects the answer with the most numerous occurrence among sequences. This hints at the possible usefulness of global statistics, or in other words looking at multiple sequences together, in improved calibration and answer selection. Tangentially related is multi-agent debate (Du et al., 2023), since 2108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 Under review as a conference paper at ICLR 2026 Figure 1: Illustration of our Multi-Sequence Verifier (MSV) that uses diverse attention masks in its Multi-Mask Transformer Block (MMTB) to predict the correctness of each answer. The different attention masks allow MSV to flexibly leverage both information across and within sequences. debate with a single round is equivalent to best-of-N selection with critics that look at all generated sequences to classify their own sequence as correct or incorrect. Again, the effectiveness of debate hints at the usefulness of cross-sequence features in estimating individual correctness. 2.3 EFFICIENT PARALLEL SCALING The efficiency of parallel test-time scaling is essential for its practical usage. Most works on efficient test-time scaling focus on reducing the total test-time compute, but largely disregard the tradeoff between latency and throughput. It is desirable, in practical settings, to reduce latency by buying a larger number of throughput, even if the total amount of computation remains the same. Various works such as DeepConf (Fu et al., 2025) and others (Li et al., 2024; Aggarwal et al., 2023; Xue et al., 2023; Huang et al., 2025) propose early stopping at the sequence level to reduce the number of decoded sequences. However, they all assume multiple sequences being decoded sequentially, and as a result, incur tens to hundreds times the latency of a single sequence. We focus on the parallel decoding setting token-level early stopping. To the best of our knowledge, we are the first in the LLM decoding literature to investigate this setting. Our paper aims at improving over simple but strong baselines in this novel setting. 3 METHOD 3.1 PROBLEM SETUP In this paper, we assume a parallel decoding or parallel sampling scenario, where our LLM model generates N sequences simultaneously for a given query q. At each decoding step, one token is sampled for every sequence, in parallel. This parallel approach reduces latency compared to sequential decoding, a crucial advantage for practical applications. Under this scenario, we consider two settings: 1) Terminal Answers and 2) Streaming Answers. In brief, the Terminal Answers setting focuses on the final answers obtained from each sequence after decoding has been complete. The Streaming Answers setting focuses also on intermediate answers produced while decoding. We describe each setting in more detail below. Terminal Answers. Given a query q, the LLM decodes N parallel sequences until they all reach termination. To extract an explicit answer, we append an elicitation prompt such as ### Final Answer ### \boxed to the end of each nth sequence, prompting the model to produce a boxed answer a(n). This results in one answer per sequence, denoted by {a(n)}N n=1. 3162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 Under review as a conference paper at ICLR 2026 Streaming Answers. Unlike the Terminal Answers setting, we additionally extract intermediate answers whenever a delimiter, e.g., the token ‚ÄúWait‚Äù, is encountered. This approach of extracting intermediate answers at delimiters has been explored in prior work for single-sequence early stopping (Zhang et al., 2025a; Yang et al., 2025), which we generalize to the parallel decoding setting with multiple sequences. Specifically, when the kth delimiter appears in the nth sequence, we immediately branch that sequence, append the elicitation prompt ### Final Answer ### \boxed, and obtain an intermediate answer a(n) k . We also extract the terminal answer at the end of each sequence. Thus, if a sequence contains K(n) ‚àí 1 delimiters, it may yield multiple answers {a(n) k }K(n) k=1 , a(n) K(n) is the terminal answer. For notational consistency, in the Terminal Answers setting, we regard the single terminal answer as a(n) 1 and K(n) = 1. Along with each answer, we store the representations of the answer tokens for later use with our Multi-Sequence Verifier. Specifically, if a(n) k consists of L(n) k tokens, we store {h(n) k,i} for i = 1, . . . , L(n) k , where h(n) k,i ‚àà Rd denotes the d-dimensional hidden state of the ith token, output by the last transformer layer of the LLM. Correctness and learning objectives. Now, we define the correctness of an answer a(n) k generated from a sequence. Let a‚àó denote the ground-truth answer for the query q, and the symbol ‚àº be an equivalence relation that captures symbolic or semantic equality, e.g., ‚Äú2 + 2‚Äù ‚àº ‚Äú4‚Äù, that can be computed with an algorithm such as SymPy checker (Meurer et al., 2017; Lewkowycz et al., 2022). Then, we define the correctness of each candidate answer a(n) k as follows: y(n) k = 1 a(n) k ‚àº a‚àó . Our goal is to accurately predict y(n) k for every candidate, so that we can make more effective use of the generated answers in downstream parallel test-time scaling methods such as best-of-N . Departing from methods that classify each answer in isolation, we propose to leverage information across multiple sequences to further improve the predictions. Specifically, in the Terminal Answers setting, the prediction for a(i) 1 may depend on the entire set of terminal answers {a(n) 1 }N n=1. In contrast, in the Streaming Answers setting, predictors must respect causality: the correctness prediction for a(n) k may only use information from tokens generated up to that point in time across all sequences. A detailed description of our method is provided in the following section. 3.2 MULTI-SEQUENCE VERIFIER In this section, we present our novel verifier architecture, MSV. The verifier predicts the correctness of each answer by consuming that answer tokens‚Äô (last-layer) hidden states and aggregating auxiliary signals drawn from the answers produced by the other sequences. This cross-sequence aggregation enables MSV to judge the correctness of a single answer in the context of diverse peer outputs, leading to better-calibrated predictions than when relying on information from a single answer alone. Refer to Fig. 1 to see the schematic diagram of MSV. We also provide a pseudocode for MSV‚Äôs forward pass in Algorithm 1. Input representation for MSV. Let t be a time index shared across parallel sequences, where all the sequences start generating from t = 0 and they generate one token at a time. Let œÑ (n) k denote the time stamp at which the nth sequence generates the kth answer a(n) k . At a specific readout time step t, MSV takes the representations of all the answers generated up to the step t to predict the correctness of the answers. Specifically, we collect the representations as follows, U (t) = concat nnh(n) k,i + e(n) L(n) k i=1 o œÑ (n) k ‚â§t oN n=1  . Here, concat(¬∑) denotes concatenation of all the representations along the sequence dimension, without any special separators. e(n) is a learnable per-sequence embedding added to identify token representations from different sequences. Multi-mask transformer blocks. To process the aggregated representation U (t), we introduce a Multi-Mask Transformer Block (MMTB). This block captures diverse aspects of the token sequences by combining multiple attention outputs, each derived from a different mask applied to the 4216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 Under review as a conference paper at ICLR 2026 Algorithm 1 Multi-Sequence Verifier (MSV) Forward Pass Require: N sequences with hidden states {h(n) k,i }, readout time t Ensure: Correctness predictions { ÃÉy(n) k } for all answers up to time t Trainable parameters: Sequence embeddings {e(n)}, QKV projections (WQ, WK , WV ), mask weights {wh}, MLP & LN, feature MLP, prediction head (w, b) 1: U (t) ‚Üê concat Ô£´ Ô£≠ ( {h(n) k,i + e(n)}L(n) k i=1  œÑ (n) k ‚â§t )N n=1 Ô£∂ Ô£∏ 2: Compute query, key, value projections: (Q(t) h , K(t) h , V (t) h )H h=1 3: for each head h = 1, . . . , H and mask j = 1, . . . , J do 4: A(t) h,j ‚Üê softmax  Q(t) h (K(t) h )‚ä§ ‚àöd + log Mj  V (t) h 5: end for 6:  ÃÉU (t) ‚Üê PH h=1 PJ j=1 Œ±h,j A(t) h,j where (Œ±h,1, . . . , Œ±h,J ) = softmax(wh) 7: Z(t) ‚Üê (U (t) +  ÃÉU (t)) + MLP(LN(U (t) +  ÃÉU (t))) 8: for each answer a(n) k generated by time t do 9: Compute Œ≥(n) k ‚Üê 1 N PN m=1 1[a(m) k ‚àº a(n) k ] 10:  ÃÑz(n) k ‚Üê z(n) k,L(n) k + MLP(Œ≥(n) k ) 11: end for 12: if Terminal Answers mode then 13:  ÃÉy(n) 1 ‚Üê œÉ  1 |C(n)| P m‚ààC(n)(w‚ä§  ÃÑz(m) 1 + b)  for all n 14: else if Streaming Answers mode then 15:  ÃÉy(n) k ‚Üê œÉ(w‚ä§  ÃÑz(n) k + b) for all (n, k) 16: end if 17: return { ÃÉy(n) k } same input, U (t). As in standard multi-head attention (Vaswani et al., 2017), we begin by computing the usual linear projections of U (t) into query, key, and value matrices (Q(t) h , K(t) h , V (t) h )H h=1 where H is the number of heads. For a fixed collection of J masks {Mj }J j=1, we then compute the output for each mask: A(t) h,j = softmax Q(t) h (K(t) h )‚ä§ ‚àöd + log Mj ! V (t) h , where log Mj adds 0 to permitted entries and ‚àí‚àû to masked entries, with softmax applied rowwise. The masked outputs are then combined via learnable mixture weights wh = (wh,1, . . . , wh,J ) for each head h: ÃÉU (t) = HX h=1 JX j=1 Œ±h,j A(t) h,j , (Œ±h,1, . . . , Œ±h,J ) = softmax(wh). The number of masks, J, is a fixed hyperparameter for both training and inference. In our instantiation, we use four complementary masks. The full mask permits all interactions, (Mfull)u,v = 1 for all positions u, v, enabling attention across all tokens in all sequences. The within-sequence mask restricts attention to tokens originating from the same sequence, (Mws)u,v = 1seq(u) = seq(v), capturing within-sequence signals while blocking cross-sequence signals. Here, seq(u) denotes the index n of the sequence containing the token u. The equivalence mask allows attention only between tokens whose answers are symbolically equivalent, (Meq)u,v = 1ans(u) ‚àº ans(v), where ans(u) is the answer a(n) k containing the token u. 5270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 Under review as a conference paper at ICLR 2026 Finally, the within-answer mask allows attention only between tokens inside a single answer instance a(n) k , (Mwa)u,v = 1(seq(u), step(u)) = (seq(v), step(v)), where step(u) identifies the answer step k to which the token belongs. To summarize, a(n) k can attend to a(n‚Ä≤ ) k‚Ä≤ (1) through the full mask, (2) through the within-sequence mask if n = n‚Ä≤, (3) through the equivalence mask if a(n) k ‚àº a(n‚Ä≤ ) k‚Ä≤ , and (4) through the within-answer mask if n = n‚Ä≤ and k = k‚Ä≤. In the Terminal Answers setting, the within-sequence mask and within-answer mask are equivalent, reducing the number of masks to three. We further restrict the attention masks to be ‚Äúcausal‚Äù in the Streaming Answers setting, meaning that an answer a(n) k may attend to a(n‚Ä≤ ) k‚Ä≤ only if œÑ (n) k ‚â• œÑ (n‚Ä≤) k‚Ä≤ . The block‚Äôs final output is then computed using standard residual connections and an MLP layer: Z(t) = (U (t) +  ÃÉU (t)) + MLP"""},
    ],
    stream=True
)
for chunk in completion:
    print(chunk.choices[0].delta.content, end="", flush=True)

# print(completion.choices[0].message.content)