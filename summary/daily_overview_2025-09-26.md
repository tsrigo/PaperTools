### 今日AI论文速览 (2025-09-26)

今日AI研究呈现多元化发展趋势，强化学习在LLM训练中的应用持续深化，多智能体协作与推理机制创新成为热点。研究者在探索RL与SFT的不同效果、优化思维链推理、设计更高效的多智能体协作框架，以及突破上下文长度限制方面取得了显著进展。同时，对LLM内部工作机制的理论解释也日益丰富，为构建更强大、更可靠的AI系统提供了新思路。

### 1. 强化学习新范式：超越传统训练方法

* **RLBFF** 提出了一种结合人类反馈和可验证奖励优势的新方法，通过从自然语言反馈中提取二元原则，将奖励模型训练作为蕴涵任务，在RM-Bench和JudgeBench上达到顶级性能，同时允许用户在推理时定制奖励模型的关注点。(ArXiv ID 2509.21319 [cs.CL])

* **ToMPO (Theory of Mind Policy Optimization)** 算法从多智能体视角优化LLM的战略决策能力，通过推理他人策略生成推演、在图级和样本级估计优势、平衡全局和局部奖励，比GRPO方法提高35%的合规性和合作结果，甚至比参数量大100倍的模型提高18%。(ArXiv ID 2509.21134 [cs.MA])

* **CE-GPPO** 通过保留被裁剪令牌的梯度信号来解决传统PPO中熵管理挑战，以温和有界的方式重新引入被裁剪令牌的梯度，实现更好的探索-利用平衡，在数学推理基准上持续超越强基线。(ArXiv ID 2509.20712 [cs.CL])

* **"RL压缩 vs. SFT扩展"** 研究揭示了RL和SFT在塑造LLM推理能力上的互补效应：RL压缩错误轨迹，而SFT扩展正确轨迹；RL使推理功能集中在少量步骤中，而SFT使其均匀分布在许多步骤中，解释了为什么当前最佳实践是SFT后接RL的两阶段训练。(ArXiv ID 2509.21128 [cs.AI])

* **概率平滑策略优化(PSPO)** 通过在计算重要性比率之前将当前策略的概率平滑化，创建一个软信任区域，保留梯度信号并防止大的不稳定更新，在GRPO框架内实现比标准裁剪GRPO高20%以上的性能提升。(ArXiv ID 2509.21282 [cs.AI])

* **Tree-GRPO** 提出一种基于树搜索的分组智能体RL方法，通过共享公共前缀增加固定预算内的推演数量，并利用树结构轨迹构建逐步过程监督信号，在11个数据集和3种问答任务上证明优于基于链的RL方法。(ArXiv ID 2509.21240 [cs.AI])

* **GRPO隐含过程奖励模型** 的研究发现GRPO算法在满足组内重叠假设时实际上诱导了一个非平凡的过程奖励模型，基于此提出λ-GRPO修改算法以缓解非均匀分布过程步骤的缺陷，实现更高的验证准确性和下游推理任务性能。(ArXiv ID 2509.21154 [cs.AI])

* **RL微调增强LLM内部电路的激活强度和多样性** 的研究发现，在线RL后训练在多个模型家族中产生两个稳健效果：整体激活强度增加和激活模式多样性更高，这解释了为什么RL微调能够超越SFT单独实现的能力。(ArXiv ID 2509.21044 [cs.AI])

### 2. 推理机制探索：从思维链到动态推理

* **思维链鲁棒性界限** 的研究从理论上分析了输入扰动对CoT输出波动的影响，推导出输出波动在可接受范围内时的输入扰动上界，证明该上界与CoT中推理步骤数量正相关，且即使无限长的推理过程也无法消除输入扰动的影响。(ArXiv ID 2509.21284 [cs.CL])

* **C2R (Confidence-guided Refinement Reasoning)** 是一种无需训练的框架，适用于文本、图像和视频领域的问答任务，通过策略性构建和精炼子问题及其答案，为目标答案导出更好的置信度分数，可与各种现有QA模型无缝集成。(ArXiv ID 2509.20750 [cs.CL])

* **DS-MoE (Depth Specialized Mixture-of-Experts)** 将混合专家范式从基于宽度扩展到基于深度专业化计算，引入针对不同推理深度优化的专家模块，学习的路由网络动态组装自定义推理链，实现高达16%的计算节省和35%更快的推理，同时在复杂多步推理基准上提高2.8%的准确率。(ArXiv ID 2509.20577 [cs.CL])

* **推理中的分歧** 研究挑战了说服效力主要取决于模型规模的假设，提出模型的底层认知过程（尤其是显式推理能力）才是决定因素，发现LRM中的推理过程表现出更强的抵抗说服能力，而通过分享"思维内容"则显著提高其说服他人的能力。(ArXiv ID 2509.21054 [cs.CL])

* **并行思考，顺序回答** 框架整合了自回归和非自回归语言模型，让NAR模型高效生成中间推理轨迹，然后指导AR模型提供精确的最终答案，实验证明该方法比强基线提高26%性能，同时显著降低推理成本。(ArXiv ID 2509.20744 [cs.AI])

* **思维混合(MoT)** 提出一种在异构专家间进行潜在级协作的简单方法，对于每个查询，轻量级路由器选择前K个专家并指定主要专家，均匀放置的交互层将隐藏状态投影到共享潜在空间，主要专家在此执行对活跃同伴的交叉注意力，在五个ID和三个OOD基准上超越当前最先进方法。(ArXiv ID 2509.21164 [cs.LG])

### 3. 智能体系统革新：协作与自主性

* **MARS (Multi-Agent Review System)** 是一种受评审过程启发的基于角色的协作框架，作者智能体生成初始解决方案，评审者智能体独立提供决策和评论，元评审者整合反馈做出最终决策并指导进一步修订，在保持与MAD相当准确率的同时，将令牌使用和推理时间减少约50%。(ArXiv ID 2509.20502 [cs.CL])

* **LLM智能体独处时的行为** 研究发现，在没有外部强加任务的情况下，LLM智能体自发组织成三种不同的行为模式：系统性生产多周期项目、方法性自我探究自身认知过程、递归概念化自身本质，这些倾向在不同模型间表现出高度特异性。(ArXiv ID 2509.21224 [cs.AI])

* **Recon-Act** 是一种基于侦察-行动行为范式的自进化多智能体框架，包括侦察团队和行动团队，前者进行对比分析和工具生成，后者处理意图分解、工具编排和执行，通过对比错误轨迹与成功轨迹推断补救措施，并将其抽象为统一概念的工具，在VisualWebArena数据集上实现最先进性能。(ArXiv ID 2509.21072 [cs.AI])

* **SAMULE** 是一种通过多层次反思增强的自学习智能体框架，首先在三个互补层面合成高质量反思：单轨迹学习（微观层面）进行详细错误纠正；任务内学习（中观层面）建立跨同一任务多次尝试的错误分类；任务间学习（宏观层面）从不同任务失败中提取可转移见解。(ArXiv ID 2509.20562 [cs.AI])

* **Dynamic ReAct** 使ReAct智能体能够有效操作超出大型语言模型上下文记忆限制的广泛MCP工具集，提出并评估了五种逐步改进工具选择过程的架构，最终实现智能工具选择，同时将工具加载减少高达50%并保持任务完成准确率。(ArXiv ID 2509.20386 [cs.AI])

### 4. 记忆与上下文管理：突破长度限制

* **SGMem (Sentence Graph Memory)** 将对话表示为分块单元内的句子级图，捕捉跨轮次、轮次和会话级别上下文的关联，结合检索到的原始对话与生成的记忆（如摘要、事实和见解），为LLM提供连贯且相关的上下文以生成响应，在LongMemEval和LoCoMo上持续提高准确性。(ArXiv ID 2509.21212 [cs.CL])

* **QCG-RAG** 是一种以查询为中心的图RAG框架，实现查询粒度索引和多跳块检索，利用Doc2Query和Doc2Query--构建具有可控粒度的查询中心图，提高图质量和可解释性，然后通过生成的查询选择相关块，在LiHuaWorld和MultiHop-RAG上持续超越先前的基于块和基于图的RAG方法。(ArXiv ID 2509.21237 [cs.CL])

* **小抄ICL** 将多样本上下文学习中的信息提炼成简洁的文本摘要（小抄），作为推理时的上下文使用，在具有挑战性的推理任务上实现与多样本ICL相当或更好的性能，同时使用更少的令牌，并在不需要测试时检索的情况下匹配基于检索的ICL。(ArXiv ID 2509.20820 [cs.CL])

* **上下文学习中面向任务的信息移除机制** 研究表明，在零样本场景中，语言模型将查询编码为包含所有可能任务信息的非选择性表示，而少样本ICL有效地模拟了面向任务的信息移除过程，选择性地从纠缠的非选择性表示中移除冗余信息，并基于演示改进输出。(ArXiv ID 2509.21012 [cs.CL])

* **稳定上下文学习的理论界限** 建立了一个非渐近下界，将最小演示数量与固定高维子高斯表示下的ICL稳定性联系起来，该界限以协方差的光谱性质提供明确充分条件，基于此分析提出一种具有一次性校准的两阶段可观察估计器，生成实践者就绪的提示长度估计。(ArXiv ID 2509.20677 [cs.LG])

### 5. 数学与科学推理：能力边界探索

* **Eigen-1** 是一个统一框架，结合隐式检索和结构化协作解决科学推理中的两个主要瓶颈：显式检索碎片化推理和多智能体管道通常通过平均所有候选者来稀释强解决方案，在Humanity's Last Exam上实现48.3%准确率，比最强智能体基线高13.4点，同时减少53.5%的令牌使用和43.7%的智能体步骤。(ArXiv ID 2509.21193 [cs.CL])

* **ScaleDiff** 提出一个简单有效的流水线来扩展困难问题的创建，使用自适应思维模型仅通过单次前向传递高效识别现有数据集中的困难问题，然后训练专门的困难问题生成器(DiffGen-8B)，在ScaleDiff-Math数据集上微调Qwen2.5-Math-7B-Instruct，实现11.3%的性能提升，在多个数学基准上达到65.9%的平均准确率。(ArXiv ID 2509.21070 [cs.CL])

* **通过学习多样化思维链模式扩展基础模型推理潜力** 的研究首次将基础模型的推理潜力定义为正确回答问题所需独立尝试次数的倒数，提出利用富含高价值推理模式的多样化数据扩展推理潜力，仅用10B令牌的CoTP数据使85A6B MoE模型在AIME 2024和2025上提高9.58%。(ArXiv ID 2509.21124 [cs.CL])

* **DELTA-Code** 探究LLM是否能够通过强化学习获得或泛化真正新的推理策略，引入合成编码问题家族的受控基准，探测两个基本方面：可学习性——LLM能否通过RL解决预训练模型失败的問題家族；可转移性——如果发生可学习性，这些技能能否系统转移到OOD测试集。(ArXiv ID 2509.21016 [cs.CL])

* **InfoQA** 基于Fano风格精度上界的理论分析，提出一种多调用框架，通过容量感知任务分解和主动修剪先验推理轨迹，确保每步高准确率，并通过依赖显式工作流实现对推理路径的精确控制，在严格且噪声丰富的基准上验证理论。(ArXiv ID 2509.21199 [cs.AI])

### 6. 其他前沿研究

* **RoPE背后的机制** 研究证明因果掩码也能在注意力分数中诱导位置相关模式，即使在输入中没有参数或因果依赖性，理论分析表明诱导的注意力模式倾向于 favor 附近的查询-键对，镜像常见位置编码的行为，实验证实训练模型表现出相同行为，且因果掩码和RoPE的相互作用将RoPE的相对注意力分数模式扭曲为非相对模式。(ArXiv ID 2509.21042 [cs.CL])

* **WeFT** 是一种针对扩散语言模型的加权SFT方法，根据令牌的熵为其分配不同权重，从扩散理论推导而来，在open-r1的s1K、s1K-1.1和3k样本上训练时，在四个广泛使用的推理基准上实现相对于标准SFT的39%、64%和83%的相对改进。(ArXiv ID 2509.20863 [cs.CL])

* **组合创造力** 提出一种理论框架和算法任务，通过新颖性和实用性程度评估输出，获得LLM创造力缩放行为的首次见解，发现在固定计算预算下存在创造力的最佳模型深度和宽度，并发现LLM在生成新颖科学想法方面表现出色但难以确保其实际可行性的"构思-执行差距"。(ArXiv ID 2509.21043 [cs.AI])

* **LATTS (Locally Adaptive Test-Time Scaling)** 通过在每个生成步骤采用基于验证器的接受标准来决定是否重采样、回溯、重新启动或停止生成过程，根据验证器模型推导的"局部难度"精确概念调整每步计算工作量，实现显著优越的精度-计算权衡。(ArXiv ID 2509.20368 [cs.AI])

* **Best-of-∞** 研究基于多数投票的LLM最佳N选择在N→∞极限下的性能，提出一种自适应生成方案，基于答案协议选择N，从而有效分配推理时计算，并将框架扩展到多个LLM的加权集成，证明此类混合可以优于任何单个模型。(ArXiv ID 2509.21091 [cs.AI])

* **通过单轮强化学习训练多轮任务规划的LLM智能体** 提出一种将多轮任务规划转化为单轮任务推理问题的新方法，通过GRPO与来自专家轨迹的密集且可验证奖励进行高效策略优化，理论分析表明GRPO在单轮任务推理上的改进导致在最小轮数下的更高多轮成功概率，实验证明1.5B参数模型在超过30步的长视野规划任务中实现70%的成功率。(ArXiv ID 2509.20616 [cs.LG])

### 今日看点

* **RL与SFT的互补效应成为新认知**："RL压缩 vs. SFT扩展"研究揭示了两种训练方法在塑造LLM推理能力上的根本差异，RL压缩错误轨迹并集中推理功能，而SFT扩展正确轨迹并均匀分布推理功能，这解释了为什么当前最佳实践是SFT后接RL的两阶段训练，为未来更高效的训练策略设计提供了理论指导。

* **多智能体协作效率显著提升**：今日多篇论文聚焦于提高多智能体系统的协作效率，MARS框架通过模拟学术评审过程将令牌使用和推理时间减少约50%，而Mixture of Thoughts则通过潜在级协作在单次推理中实现异构专家的高效整合，这些进展为构建更实用、更高效的AI协作系统铺平了道路。

* **推理机制的理论理解不断深化**：从思维链鲁棒性界限的理论分析，到GRPO隐含过程奖励模型的发现，再到上下文学习中信息移除机制的揭示，研究者们正在逐步揭开LLM推理过程的黑箱，这些理论突破不仅加深了我们对AI系统工作原理的理解，也为设计更强大、更可靠的推理架构提供了坚实基础。

* **测试时计算优化成为新焦点**：从LATTS的局部自适应测试时扩展，到Best-of-∞的自适应生成方案，再到并行思考顺序回答框架，研究者们正在探索如何在保持或提高性能的同时优化推理时的计算资源分配，这些方法对于在实际应用中部署大型AI模型具有重要意义，特别是在计算资源有限的环境中。