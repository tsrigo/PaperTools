[
  {
    "index": "#1",
    "title": "Language Models that Think, Chat Better",
    "link": "/arxiv/2509.20357",
    "arxiv_id": "2509.20357",
    "authors": "Adithya Bhaskar, Xi Ye, Danqi Chen",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) improves language model reasoning by using rule-based rewards in verifiable domains such as mathematics and code. However, RLVR leads to limited generalization for open-ended tasks -- such as writing outline essays or making meal plans -- where humans reason routinely. This paper shows that the RLVR paradigm is effective beyond verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking (**RLMT**) for general-purpose chat capabilities. Using diverse real-world prompts, RLMT requires LMs to generate long CoT reasoning before response, and optimizes them with online RL against a preference-based reward model used in RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT consistently outperforms standard RLHF pipelines. This includes substantial gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and ArenaHardV2), along with 1-3 point improvements on other tasks like creative writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be applied directly to base models without an SFT stage, akin to R1-Zero training. Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex multi-staged pipeline with 25M+ examples. We close with qualitative and quantitative analyses of how trained models plan their responses. Our results rethink the post-training pipeline and call upon future work to understand and employ thinking more broadly.",
    "subjects": "Computation and Language",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.406700",
    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。论文的核心贡献是提出了一种名为\"RL with Model-rewarded Thinking (RLMT)\"的新训练范式，旨在增强大语言模型的通用推理能力。该方法要求模型在回答前生成长链思维(CoT)推理，并使用基于偏好的奖励模型进行在线强化学习优化，从而提升模型的规划和问题解决能力。 从筛选标准来看： 1. 第一步核心判断：论文本质是改进LLM的基础能力，提出新的训练范式(RLMT)，增强其推理能力，完全符合保留标准。 2. 第二步正面指标：论文包含多个正面指标，如大语言模型(Llama-3.1-8B和Qwen-2.5-7B)、推理能力(reasoning)、强化学习(RL)等。 3. 第三步排除标准：论文不涉及多模态、特定应用领域或模型可靠性等排除领域。 4. 第四步特殊情况：论文专注于通过思维链推理和强化学习来提升模型本身的推理能力，而非将LLM作为工具应用到特定领域。 论文在多个聊天基准测试和任务上取得了显著改进，包括创意写作和一般知识，这表明其方法有效提升了模型的通用推理能力，而非仅限于特定领域。因此，这篇论文完全符合研究目标。",
    "summary2": "本文旨在解决基于可验证奖励的强化学习(RLVR)在开放性任务上泛化能力有限的问题。针对通用聊天任务，我们提出了一种结合长链推理与偏好模型奖励的强化学习方法(RLMT)，并在Llama-3.1-8B和Qwen-2.5-7B模型上通过多种聊天基准测试(AlpacaEval2, WildBench, Arena-HardV2)验证了其有效性，实现了3-7点的性能提升，甚至超越了GPT-4o在聊天和创意写作方面的表现。",
    "summary_translation": "可验证奖励强化学习(Reinforcement learning with verifiable rewards, RLVR)通过在可验证领域(如数学和代码)中使用基于规则的奖励来改善语言模型推理能力。然而，RLVR在开放性任务(如撰写大纲论文或制定膳食计划)中导致有限的泛化能力，而这些任务正是人类日常推理的场景。本文表明RLVR范式在可验证领域之外同样有效，并提出了用于通用聊天能力的**基于模型奖励思维的强化学习**(**R**einforcement **L**earning with **M**odel-rewarded **T**hinking, **RLMT**)。\n\n使用多样化的真实世界提示，RLMT要求语言模型(LMs)在响应前生成长链思维(Chain of Thought, CoT)推理，并通过在线强化学习(online RL)对其进行优化，使用的奖励模型是基于偏好的，类似于RLHF(Reinforcement Learning from Human Feedback，人类反馈强化学习)中使用的模型。在Llama-3.1-8B和Qwen-2.5-7B(包括基础模型和指令模型)上进行的40次训练运行以及多种优化算法(DPO、PPO和GRPO)中，RLMT始终优于标准RLHF流程。这包括在三个聊天基准测试(AlpacaEval2、WildBench和ArenaHardV2)上获得3-7分的显著提升，以及在创意写作和常识等其他任务上1-3分的改进。\n\n我们最佳的8B模型在聊天和创意写作方面超越了GPT-4o，并与Claude-3.7-Sonnet (Thinking)相当。RLMT也可以直接应用于基础模型，无需SFT(Supervised Fine-Tuning，监督微调)阶段，类似于R1-Zero训练方式。值得注意的是，仅使用7K个提示，通过我们的RLMT方法训练的Llama-3.1-8B基础模型，其性能超过了经过复杂多阶段流程(使用2500万+示例)后训练的Llama-3.1-8B-Instruct模型。最后，我们对训练模型如何规划其响应进行了定性和定量分析。我们的结果重新思考了后训练流程，并呼吁未来的工作更广泛地理解和运用思维过程。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#13",
    "title": "Thinking Augmented Pre-training",
    "link": "/arxiv/2509.20186",
    "arxiv_id": "2509.20186",
    "authors": "Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei",
    "summary": "This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\\%$ on several challenging reasoning benchmarks.",
    "subjects": "Computation and Language, Machine Learning",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.410478",
    "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式\"Thinking augmented Pre-Training (TPT)\"，通过在预训练阶段增加思维轨迹来增强模型的推理能力。这直接关注提升LLM的通用推理能力，而非将其作为工具应用于特定领域。 其次，论文包含了多个正面指标：核心概念明确关注大型语言模型(LLMs)，能力方向聚焦于推理能力(reasoning)，特别是通过\"step-by-step reasoning and decomposition\"来提升模型性能。实验结果也显示该方法在多个具有挑战性的推理基准上提高了模型性能。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 论文的核心贡献是提出了一种通用的预训练方法，通过自动生成的思维轨迹增强文本数据，使高质量token更易学习，从而提高LLM的数据效率和推理能力。这种方法不是针对特定领域，而是旨在从根本上提升LLM的通用推理能力，完全符合我筛选\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"论文的目标。",
    "summary2": "本文旨在提高大型语言模型(LLM)训练的数据效率。针对高质量训练数据有限且某些token难以直接学习的问题，我们提出了Thinking augmented Pre-training (TPT)，一种通过自动生成思维轨迹增强现有文本数据的通用方法。我们在多种训练配置上（包括数据受限和充足情况下的预训练以及中期训练）通过推理基准和语言理解任务验证了其有效性，实验表明TPT将LLM预训练的数据效率提高了3倍，显著提升了模型性能。",
    "summary_translation": "本文介绍了一种简单且可扩展的方法，通过用思维轨迹(thinking trajectories)增强现有文本来提高大型语言模型(Large Language Model, LLM)训练的数据效率。大型语言模型预训练的计算量一直在以前所未有的速度增长，而高质量数据的可用性仍然有限。因此，最大化可用数据的效用构成了一个重大的研究挑战。一个主要障碍是，在固定模型容量下，某些高质量标记(tokens)难以学习，因为单个标记的基本原理可能异常复杂和深入。为解决这一问题，我们提出了思维增强预训练(Thinking augmented Pre-Training, TPT)，这是一种通过自动生成的思维轨迹增强文本的通用方法。这种增强有效增加了训练数据的体量，并通过逐步推理和分解使高质量标记更易学习。我们在多种训练配置中应用TPT，规模高达1000亿(tokens)标记，包括数据受限和数据充足情况下的预训练，以及从强大的开源检查点(checkpoints)进行的中期训练。实验结果表明，我们的方法显著提高了各种规模和系列的大型语言模型的性能。值得注意的是，TPT将大型语言模型预训练的数据效率提高了3倍。对于一个30亿参数(3B parameters)的模型，它在几个具有挑战性的推理基准(benchmarks)上将训练后性能提高了超过10%。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#18",
    "title": "Causal Understanding by LLMs: The Role of Uncertainty",
    "link": "/arxiv/2509.20088",
    "arxiv_id": "2509.20088",
    "authors": "Oscar Lithgow-Serrano, Vani Kanjirangat, Alessandro Antonucci",
    "summary": "Recent papers show LLMs achieve near-random accuracy in causal relation classification, raising questions about whether such failures arise from limited pretraining exposure or deeper representational gaps. We investigate this under uncertainty-based evaluation, testing whether pretraining exposure to causal examples improves causal understanding >18K PubMed sentences -- half from The Pile corpus, half post-2024 -- across seven models (Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model behavior through: (i) causal classification, where the model identifies causal relationships in text, and (ii) verbatim memorization probing, where we assess whether the model prefers previously seen causal statements over their paraphrases. Models perform four-way classification (direct/conditional/correlational/no-relationship) and select between originals and their generated paraphrases. Results show almost identical accuracy on seen/unseen sentences (p > 0.05), no memorization bias (24.8% original selection), and output distribution over the possible options is almost flat, with entropic values near the maximum (1.35/1.39), confirming random guessing. Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence, 32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11% vs. direct). These findings suggest that failures in causal understanding arise from the lack of structured causal representation, rather than insufficient exposure to causal examples during pretraining.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.411669",
    "filter_reason": "这篇论文的核心是研究LLMs在因果关系理解方面的能力，属于LLM的基础推理能力研究，特别是逻辑推理能力的重要组成部分。论文通过多种模型测试，分析了LLMs在因果分类和记忆探测方面的表现，发现LLMs在因果理解上的失败源于缺乏结构化的因果表示，而非预训练中因果例子暴露不足。虽然论文使用了PubMed句子作为测试数据，但这只是为了评估LLM的通用因果理解能力，而不是将LLM应用于医疗领域。论文关注的是LLMs本身的推理能力缺陷，属于对LLM基础能力的探索，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。因此，这篇论文应该被保留。",
    "summary2": "本文旨在探究大型语言模型在因果理解任务中表现不佳的原因。针对预训练数据中因果例子暴露不足与表示能力缺陷的争议，我们提出了一种基于不确定性量化的评估方法，通过熵和校准误差等指标分析模型行为。在包含18,000多个PubMed句子的数据集上，通过因果类型分类和逐字记忆探测任务，验证了模型在已见和未见数据上表现无显著差异，输出分布接近随机，表明因果理解失败源于缺乏结构化因果表示而非数据暴露不足。",
    "summary_translation": "最近的研究表明，大型语言模型（LLMs）在因果关系分类（causal relation classification）中实现接近随机的准确率，引发了关于此类失败是源于预训练（pretraining）中接触有限还是更深层次的表征缺口（representational gaps）的问题。我们在基于不确定性（uncertainty-based）的评估下对此进行了研究，测试了预训练中接触因果例子是否能够改善对超过18,000条PubMed句子的因果理解（causal understanding）——其中一半来自The Pile语料库，一半来自2024年之后的内容——涉及七个模型（Pythia-1.4B/7B/12B、GPT-J-6B、Dolly-7B/12B、Qwen-7B）。我们通过以下方式分析模型行为：（i）因果关系分类（causal classification），模型识别文本中的因果关系；以及（ii）逐字记忆探测（verbatim memorization probing），我们评估模型是否更偏好之前见过的因果陈述而非其释义（paraphrases）。模型执行四类分类（direct/conditional/correlational/no-relationship，直接/条件/相关/无关系）并在原始句子和生成的释义之间进行选择。结果显示，模型在已见/未见句子上的准确率几乎相同（p > 0.05），没有记忆偏差（memorization bias）（24.8%选择原始句子），且可能选项的输出分布几乎平坦，熵值（entropic values）接近最大值（1.35/1.39），证实了随机猜测。指令微调（Instruction-tuned）模型表现出严重的校准失调（miscalibration）（Qwen：> 95%的置信度，32.8%的准确率，ECE=0.49）。条件关系（Conditional relations）诱导出最高的熵（比直接关系高+11%）。这些发现表明，因果理解的失败源于缺乏结构化的因果表征（structured causal representation），而非预训练中接触因果例子不足。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#6",
    "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
    "link": "/arxiv/2509.20317",
    "arxiv_id": "2509.20317",
    "authors": "Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin",
    "summary": "Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited the application of implicit CoT. We identify a core latent instability issue by scaling the computational budget of implicit CoT approaches: as we increase the number of implicit reasoning tokens to enhance performance, the training process often becomes unstable and collapses. Our analysis reveals that this instability arises from the latent representations becoming homogeneous and losing their semantic diversity, a failure caused by insufficient step-level supervision in existing implicit CoT approaches. To address this issue, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring that latent states capture distinct and meaningful information. The proposed auxiliary decoder is removed during inference, preserving the computational efficiency of implicit CoT methods with no added overhead. In addition, the auxiliary decoder affords interpretability of implicit reasoning by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization of semantic roles and diagnosis. SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain stability of various implicit CoT methods, boosting baselines like Coconut by +8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1% with 2.3\\times greater token efficiency, while substantially closing the performance gap on larger models like LLaMA-3.1 8B.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.408890",
    "filter_reason": "根据筛选标准，这篇论文完全符合研究目标。首先，从核心判断来看，论文的本质是关于改进大语言模型的推理能力，特别是针对Implicit Chain-of-Thought (CoT)方法提出了一种新的训练范式SIM-CoT，这直接属于改进LLM基础能力和通用推理能力的范畴，符合保留标准。 其次，论文包含多项正面指标：核心概念明确涉及Large Language Models (LLMs)；能力方向聚焦于reasoning，特别是Chain-of-Thought推理；训练方法方面提出了创新的step-level supervision机制来增强模型训练过程。 第三，论文不涉及任何排除标准领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文提出的辅助解码器增强了模型推理的可解释性，通过\"projecting each latent token onto an explicit reasoning vocabulary\"来提升模型的推理质量，这符合保留条件。 论文的核心贡献是解决了implicit CoT方法中的潜在不稳定性问题，通过step-level supervision稳定和丰富潜在推理空间，从而提升LLM的推理能力，这与研究目标\"致力于提高大语言模型的通用推理能力\"高度一致。",
    "summary2": "本文旨在解决隐式思维链(Implicit CoT)方法中的潜在不稳定性问题。针对大语言模型在增加隐式推理令牌时训练崩溃的场景，我们提出了一种SIM-CoT方法，通过引入步骤级监督的辅助解码器来稳定隐式推理空间，并在GSM8K-Aug等多个数学推理数据集上通过准确率、令牌效率等指标验证了其有效性。",
    "summary_translation": "Implicit Chain-of-Thought (CoT)（隐式思维链）方法为大型语言模型（Large Language Models, LLMs）中的显式思维链推理提供了一种有前景且高效的token（令牌）替代方案，但持续存在的性能差距限制了隐式CoT的应用。通过扩展隐式CoT方法的计算预算，我们发现了一个核心的潜在不稳定性问题：当我们增加隐式推理token的数量以提高性能时，训练过程常常变得不稳定并崩溃。我们的分析表明，这种不稳定性源于潜在表示变得同质化并失去其语义多样性，这是现有隐式CoT方法中步骤级别（step-level）监督不足导致的失败。\n\n为解决这一问题，我们提出了SIM-CoT，即插即用（plug-and-play）训练模块，它引入步骤级别监督以稳定并丰富潜在推理空间。具体而言，SIM-CoT在训练过程中采用辅助解码器（auxiliary decoder）将每个隐式token与其对应的显式推理步骤对齐，确保潜在状态捕获独特且有意义的信息。在推理过程中，所提出的辅助解码器被移除，保持了隐式CoT方法的计算效率，且不增加额外开销。此外，辅助解码器通过将每个潜在token投影到显式推理词汇表（explicit reasoning vocabulary）上，提供了隐式推理的可解释性，实现了每步语义角色和诊断的可视化。\n\nSIM-CoT显著提升了各种隐式CoT方法的域内（in-domain）准确性和域外（out-of-domain）稳定性，使GPT-2上的Coconut基线提高了+8.2%，LLaMA-3.1 8B上的CODI提高了+3.0%。展示了强大的可扩展性（scalability），SIM-CoT在GPT-2上以2.3倍的token效率超越了显式CoT基线2.1%，同时在更大模型如LLaMA-3.1 8B上显著缩小了性能差距。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#20",
    "title": "Can Constructions \"SCAN\" Compositionality ?",
    "link": "/arxiv/2509.20074",
    "arxiv_id": "2509.20074",
    "authors": "Ganesh Katrapati, Manish Shrivastava",
    "summary": "Sequence to Sequence models struggle at compositionality and systematic generalisation even while they excel at many other tasks. We attribute this limitation to their failure to internalise constructions conventionalised form meaning pairings that license productive recombination. Building on these insights, we introduce an unsupervised procedure for mining pseudo-constructions: variable-slot templates automatically extracted from training data. When applied to the SCAN dataset, our method yields large gains out-of-distribution splits: accuracy rises to 47.8 %on ADD JUMP and to 20.3% on AROUND RIGHT without any architectural changes or additional supervision. The model also attains competitive performance with? 40% of the original training data, demonstrating strong data efAciency. Our findings highlight the promise of construction-aware preprocessing as an alternative to heavy architectural or training-regime interventions.",
    "subjects": "Computation and Language",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.412471",
    "filter_reason": "这篇论文的核心是关于改进序列到序列模型(Seq2Seq)的组合性和系统泛化能力，这些能力是大语言模型通用推理能力的重要组成部分。论文提出了一种无监督方法来挖掘\"伪构造\"(pseudo-constructions)，即从训练数据中自动提取的可变槽模板，这种方法在SCAN数据集上显著提高了模型在分布外分割上的准确性。这属于改进模型基础能力的方法论研究，而非将LLM应用于特定领域。组合性是逻辑推理和语言理解的基础，系统泛化则涉及到模型如何处理新的、未见过的组合，这些都是通用推理能力的关键方面。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解决序列到序列模型在组合性和系统泛化方面的困难。针对SCAN数据集的分布外测试场景，我们提出了一种无监督的伪构造挖掘方法，并在SCAN数据集的ADD JUMP和AROUND RIGHT分割上通过准确率验证了其有效性。",
    "summary_translation": "序列到序列（Sequence to Sequence）模型在组合性（compositionality）和系统性泛化（systematic generalisation）方面表现不佳，尽管它们在许多其他任务上表现出色。我们将这一局限性归因于它们未能内化构式（constructions）——即约定俗成的形式-意义配对，而这种配对能够许可生产性重组。基于这些见解，我们引入了一种无监督（unsupervised）程序来挖掘伪构式（pseudo-constructions）：即从训练数据中自动提取的可变槽位模板（variable-slot templates）。当应用于SCAN数据集时，我们的方法在分布外（out-of-distribution）分割上取得了显著提升：在无需任何架构变更或额外监督的情况下，ADD JUMP上的准确率提高到47.8%，AROUND RIGHT上提高到20.3%。该模型仅使用40%的原始训练数据就能达到竞争性性能，展示了强大的数据效率（efficiency）。我们的研究结果突显了构式感知（construction-aware）预处理作为一种替代方案的潜力，以替代繁重的架构或训练机制干预。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#29",
    "title": "Future Policy Aware Preference Learning for Mathematical Reasoning",
    "link": "/arxiv/2509.19893",
    "arxiv_id": "2509.19893",
    "authors": "Minjae Oh, Yunho Choi, Dongmin Choi, Yohan Jo",
    "summary": "Preference learning methods such as Direct Preference Optimization (DPO) have become standard for Large Language Model (LLM) post-training, yet they are often ineffective for mathematical reasoning. A key challenge is the large token overlap between preferred and dispreferred trajectories; lowering the probability of dispreferred trajectories also reduces the probability of shared useful tokens, leading to over-penalization and overall performance collapse. As a mitigation, existing algorithms include the probability of a trajectory under the current policy as a regularization term, which decreases the effect of the gradient when the probability is low. However, by the time this effect takes hold, useful tokens may have already been over-penalized as the model has begun to degrade. To address this, we propose Future Policy Aware (FPA) preference learning, which replaces the current policy with a future policy in the regularization term. This future policy is estimated via lightweight, logit-space extrapolation from a reference model toward the current model. FPA enables safer training by preemptively regularizing potentially problematic gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH and GSM8K benchmarks. FPA yields consistent performance gains, with the largest improvements observed with SimPER, achieving gains of up to 5.75%. We demonstrate that FPA provides proactive regularization while preserving the probability of shared, useful mathematical tokens, and enables longer, degradation-free training with negligible computational overhead. We will release our code publicly upon publication.",
    "subjects": "Computation and Language",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.414766",
    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是改进LLM的数学推理能力，属于\"增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。论文提出了FPA（Future Policy Aware）方法，用于解决偏好学习在数学推理中的问题，这是直接提升LLM基础能力的研究，而非将LLM作为工具应用到特定领域。 其次，论文满足多个正面指标：核心概念上明确研究Large Language Models (LLMs)；能力方向上专注于mathematical reasoning（数学推理）；训练方法上涉及Direct Preference Optimization (DPO)等偏好学习方法，这些通常与强化学习相关。 第三，论文不符合任何排除标准：它不涉及多模态与视觉内容；虽然聚焦于数学推理，但数学推理被视为评估和提升LLM通用能力的重要方面，而非特定应用领域；也没有主要关注模型可靠性方面的应用问题。 论文的核心贡献是提出了一种新的偏好学习方法FPA，通过在正则化项中使用未来策略而非当前策略，解决了数学推理中偏好学习的过度惩罚问题，从而提升了LLM的数学推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
    "summary2": "本文旨在解决数学推理任务中偏好学习方法的梯度纠缠问题。针对数学推理轨迹中大量共享token导致的过度惩罚问题，我们提出了一种未来策略感知（FPA）偏好学习方法，通过轻量级logit空间外推估计未来策略进行主动正则化。在MATH和GSM8K基准测试上通过准确率验证了其有效性，FPA在SimPER算法上实现了高达5.75%的性能提升，同时支持更长的无退化训练。",
    "summary_translation": "像直接偏好优化（Direct Preference Optimization, DPO，直接偏好优化）这样的偏好学习方法已成为大型语言模型（Large Language Model, LLM，大型语言模型）后训练的标准方法，但它们在数学推理方面往往效果不佳。一个关键挑战是偏好轨迹和非偏好轨迹之间存在大量的标记（token，标记）重叠；降低非偏好轨迹的概率同时也会降低共享有用标记的概率，导致过度惩罚和整体性能崩溃。作为一种缓解措施，现有算法将轨迹在当前策略下的概率作为正则化项（regularization term，正则化项）包含在内，当概率较低时，这会降低梯度的影响。然而，当这种效果开始显现时，有用的标记可能已经被过度惩罚，因为模型已经开始退化。\n\n为解决这个问题，我们提出了未来策略感知（Future Policy Aware, FPA，未来策略感知）偏好学习，它在正则化项中用未来策略替代当前策略。这种未来策略通过从参考模型到当前模型的轻量级logit空间（logit-space，logit空间）外推来估计。FPA通过预先规范化可能有问题的梯度，实现了更安全的训练。我们将FPA应用于DPO、RPO和SimPER，并在MATH和GSM8K基准测试上对它们进行评估。FPA带来了一致的性能提升，其中在SimPER上观察到最大的改进，实现了高达5.75%的提升。我们证明FPA提供了主动的正则化，同时保留了共享的有用数学标记的概率，并实现了更长时间的无退化训练，且计算开销可忽略不计。我们将在发表后公开发布我们的代码。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#48",
    "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models",
    "link": "/arxiv/2509.19593",
    "arxiv_id": "2509.19593",
    "authors": "Dylan Hutson, Daniel Vennemeyer, Aneesh Deshmukh, Justin Zhan, Tianyu Jiang",
    "summary": "We introduce GuessingGame, a protocol for evaluating large language models (LLMs) as strategic question-askers in open-ended, open-domain settings. A Guesser LLM identifies a hidden object by posing free-form questions to an Oracle without predefined choices or candidate lists. To measure question quality, we propose two information gain (IG) metrics: a Bayesian method that tracks belief updates over semantic concepts using LLM-scored relevance, and an entropy-based method that filters candidates via ConceptNet. Both metrics are model-agnostic and support post hoc analysis. Across 858 games with multiple models and prompting strategies, higher IG strongly predicts efficiency: a one-standard-deviation IG increase reduces expected game length by 43\\%. Prompting constraints guided by IG, such as enforcing question diversity, enable weaker models to significantly improve performance. These results show that question-asking in LLMs is both measurable and improvable, and crucial for interactive reasoning.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-23",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.418907",
    "filter_reason": "这篇论文符合我的研究目标，因为它核心关注的是大语言模型的通用推理能力。论文提出了GuessingGame协议，用于评估LLMs作为战略提问者的能力，本质上是在研究模型如何通过提问和获取信息来进行有效推理。这种开放式提问能力是一种通用推理能力，类似于思维链(CoT)等多步推理能力，而非将LLM应用于特定领域。论文提出的两种信息增益指标旨在衡量和提升LLMs的推理效率，结果显示信息增益与推理效率显著相关，这种研究直接针对提升LLM的基础推理能力。论文不涉及任何排除标准中的领域（如多模态、特定应用或模型基础设施），而是聚焦于提高LLM本身的通用推理能力，因此完全符合我的研究范围。",
    "summary2": "本文旨在评估大型语言模型作为策略性提问者的能力。针对开放式、开放领域的问答场景，我们提出了GuessingGame协议，通过Guesser LLM向Oracle提问识别隐藏对象，并设计了两种信息增益(IG)度量方法：贝叶斯信念跟踪和基于ConceptNet的熵方法。在858个游戏实验中，通过成功率(SR)和平均问题数(ANQ)验证了方法有效性，证明IG与任务效率强相关，一个标准差IG增加可减少43%预期游戏长度。",
    "summary_translation": "我们提出了GuessingGame（猜谜游戏）协议，用于评估大型语言模型（LLMs, Large Language Models）在开放式、开放域环境中作为战略提问者的表现。在该协议中，一个Guesser LLM（猜测者语言模型）通过向Oracle（预言者）提出自由形式的问题来识别一个隐藏对象，无需预设选项或候选列表。为衡量问题质量，我们提出了两种信息增益（IG, Information Gain）指标：一种贝叶斯方法，通过使用LLM评分的相关性来追踪对语义概念的信念更新；另一种基于熵的方法，通过ConceptNet（概念网络）过滤候选对象。这两种指标都是模型无关的（model-agnostic），并支持事后分析（post hoc analysis）。在涉及多个模型和提示策略的858场游戏中，更高的IG strongly predicts效率：IG的一个标准差增加使预期游戏长度减少43%。由IG指导的提示约束，如强制问题多样性，使较弱的模型能显著提高性能。这些结果表明，LLMs中的提问既是可测量的也是可改进的，并且对交互式推理（interactive reasoning）至关重要。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#50",
    "title": "ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities",
    "link": "/arxiv/2509.19569",
    "arxiv_id": "2509.19569",
    "authors": "Aleksis Datseris, Sylvia Vassileva, Ivan Koychev, Svetla Boytcheva",
    "summary": "This paper introduces a novel approach to position embeddings in transformer models, named \"Exact Positional Embeddings\" (ExPE). An absolute positional embedding method that can extrapolate to sequences of lengths longer than the ones it was trained on. Traditional transformer models rely on absolute or relative position embeddings to incorporate positional information into token embeddings, which often struggle with extrapolation to sequences longer than those seen during training. Our proposed method utilizes a novel embedding strategy that encodes exact positional information by overriding specific dimensions of the embedding vectors, thereby enabling a more precise representation of token positions. The proposed approach not only maintains the integrity of the original embeddings but also enhances the model's ability to generalize to more extended sequences. In causal language modeling, our ExPE embeddings significantly reduce perplexity compared to rotary and sinusoidal embeddings, when tested on sequences longer than those used in training.",
    "subjects": "Computation and Language",
    "date": "2025-09-23",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.419452",
    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步核心判断：这篇论文的本质是改进Transformer模型的位置编码方法，提出了\"精确位置嵌入\"(ExPE)，使模型能够处理比训练时更长的序列。这属于改进LLM基础架构的研究，旨在增强模型处理长序列的基础能力，而非将LLM作为工具应用到特定领域。因此，论文符合保留标准。 第二步正面指标：论文涉及\"Generative Transformer Models\"，属于LLM范畴。虽然未直接讨论推理、规划等能力，但处理长序列的能力是支持复杂推理任务的基础。例如，数学推理、逻辑推理和多步规划通常需要处理长序列的能力，因此这项工作间接支持了通用推理能力的提升。 第三步排除标准：论文不涉及任何排除标准中的领域，包括多模态与视觉、特定应用领域以及模型可靠性的应用层面问题。它关注的是基础模型架构的改进。 第四步特殊和模糊情况：论文情况清晰，不涉及特殊或模糊情况。它明确关注的是Transformer模型的位置编码方法，属于基础模型架构的改进。 最终决策：虽然论文没有直接讨论推理、规划或问题解决能力，但改进模型处理长序列的能力是支持复杂推理任务的基础。因此，这篇致力于改进LLM基础架构能力的研究符合\"提高大语言模型（LLM）本身的『通用推理能力』\"的核心研究目标。",
    "summary2": "本文旨在解决Transformer模型在处理比训练序列更长时的位置编码外推问题。针对序列长度外推的场景，我们提出了一种精确位置编码（ExPE）方法，通过覆盖嵌入向量中的特定维度来编码精确位置信息，并在因果语言建模任务上通过困惑度（perplexity）指标验证了其有效性。",
    "summary_translation": "本文介绍了一种在transformer models（Transformer模型）中进行position embeddings（位置嵌入）的新方法，名为\"Exact Positional Embeddings\"（精确位置嵌入，ExPE）。这是一种absolute positional embedding（绝对位置嵌入）方法，能够extrapolate（外推）到比训练时更长的序列。传统的transformer models（Transformer模型）依赖absolute或relative position embeddings（绝对或相对位置嵌入）将位置信息整合到token embeddings（词元嵌入）中，但这些方法通常难以extrapolate（外推）到比训练时更长的序列。我们提出的方法利用一种新的embedding strategy（嵌入策略），通过重写embedding vectors（嵌入向量）的特定维度来编码精确的位置信息，从而实现对词元位置的更精确表示。所提出的方法不仅保持了原始embeddings（嵌入）的原有特性，还增强了模型对更长序列的泛化能力。在causal language modeling（因果语言建模）中，当在比训练中使用的更长的序列上进行测试时，我们的ExPE embeddings（ExPE嵌入）与rotary and sinusoidal embeddings（旋转和正弦嵌入）相比，显著降低了perplexity（困惑度）。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#56",
    "title": "How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models",
    "link": "/arxiv/2509.19371",
    "arxiv_id": "2509.19371",
    "authors": "Kangtao Lv, Haibin Chen, Yujin Yuan, Langming Liu, Shilei Liu, Yongwei Wang, Wenbo Su, Bo Zheng",
    "summary": "Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-19",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.420726",
    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，主要基于以下分析： 第一步核心判断：这篇论文的本质是关于改进LLM基础能力的研究，具体聚焦于预训练阶段的知识注入方法。论文提出了\"知识注入缩放定律\"，这是一种新的训练范式，旨在解决LLM在知识获取与保留方面的核心挑战。虽然论文提到了\"领域知识\"，但其核心贡献是通用的方法论，用于平衡知识注入与避免灾难性遗忘，这直接关系到提升LLM的基础能力，符合\"改进LLM的基础能力、提出新的训练范式\"的保留标准。 第二步正面指标：论文明确包含\"Large language models, LLMs\"这一核心概念。虽然论文没有直接讨论reasoning、planning等具体能力方向，但知识获取和保留是推理能力的基础，论文研究的是如何更有效地让模型获取并保留知识，这间接支持了通用推理能力的提升。 第三步排除标准：论文不涉及多模态与视觉研究。虽然提到\"domain-specific data\"和\"specialized knowledge\"，但论文的核心是提出一种通用的知识注入缩放定律，而非专注于某个特定应用领域（如医疗、化学等）。论文提到\"hallucination\"问题，但是从知识注入角度研究如何减少幻觉，而非仅作为应用层面的防御。 第四步特殊和模糊情况处理：论文虽然涉及\"领域知识\"，但其核心贡献是通用的方法论，可以应用于各种领域知识的注入，而不是针对特定领域的应用研究。因此，它更符合\"改进LLM基础能力\"而非\"特定应用领域\"的特征。 综上所述，这篇论文的核心贡献是提出了一种通用的知识注入缩放定律，用于优化LLM预训练过程中的知识获取和保留，这属于提升LLM基础能力的研究范畴，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决大型语言模型预训练中如何高效注入领域知识的问题。针对不同规模LLMs在知识注入时出现的记忆崩溃现象，我们提出了一种知识注入扩展律(Knowledge Infusion Scaling Law)，通过分析较小模型预测大型模型的最佳知识注入量，并在从137M到3B参数的不同模型规模和高达100B训练token的实验环境中，通过记忆保留率(Memorization Rate)指标验证了其有效性。",
    "summary_translation": "大型语言模型（Large language models, LLMs）因其令人印象深刻的跨多样化下游任务的通用能力而吸引了广泛关注。然而，在没有领域特定优化（domain-specific optimization）的情况下，它们在专业知识基准（specialized knowledge benchmarks）上表现不佳，甚至会产生幻觉（hallucination）。最近的研究表明，在预训练（pretraining）期间策略性地注入领域知识（domain knowledge）可以显著提高下游性能（downstream performance）。一个关键挑战在于平衡这种注入权衡（infusion trade-off）：注入过少的领域特定数据（domain-specific data）会导致专业化不足，而过量注入则会引发灾难性遗忘（catastrophic forgetting）先前获得的知识。在这项工作中，我们关注由过度注入（over-infusion）引起的记忆崩溃（memory collapse）现象。通过系统性实验，我们得出了两个关键观察结果，即1）关键崩溃点（Critical collapse point）：每个模型都表现出一个阈值，超过该阈值，其知识保留能力会急剧下降。2）规模相关性（Scale correlation）：这些崩溃点与模型规模（model's size）呈一致的比例关系。基于这些见解，我们提出了一种知识注入扩展定律（knowledge infusion scaling law），通过分析较小规模的对应模型来预测应注入大型语言模型（LLMs）的最佳领域知识量。在不同模型规模和预训练令牌预算（pertaining token budgets）上的大量实验验证了我们扩展定律的有效性和泛化性。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#66",
    "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution",
    "link": "/arxiv/2509.19349",
    "arxiv_id": "2509.19349",
    "authors": "Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin",
    "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large language models (LLMs) to advance scientific discovery with state-of-the-art performance and unprecedented efficiency. Recent advances in scaling inference time compute of LLMs have enabled significant progress in generalized scientific discovery. These approaches rely on evolutionary agentic harnesses that leverage LLMs as mutation operators to generate candidate solutions. However, current code evolution methods suffer from critical limitations: they are sample inefficient, requiring thousands of samples to identify effective solutions, and remain closed-source, hindering broad adoption and extension. ShinkaEvolve addresses these limitations, introducing three key innovations: a parent sampling technique balancing exploration and exploitation, code novelty rejection-sampling for efficient search space exploration, and a bandit-based LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks, demonstrating consistent improvements in sample efficiency and solution quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution using only 150 samples, designs high-performing agentic harnesses for AIME mathematical reasoning tasks, identifies improvements to ALE-Bench competitive programming solutions, and discovers novel mixture-of-expert load balancing loss functions that illuminate the space of optimization strategies. Our results demonstrate that ShinkaEvolve achieves broad applicability with exceptional sample efficiency. By providing open-source accessibility and cost-efficiency, this work democratizes open-ended discovery across diverse computational problems.",
    "subjects": "Computation and Language, Machine Learning",
    "date": "2025-09-17",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.422777",
    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文本质上是关于改进LLM的基础能力和提出新的训练范式。ShinkaEvolve框架利用LLMs作为变异操作符，通过进化机制增强模型生成解决方案的能力，这直接关注提升LLM的通用推理和问题解决能力，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确提到\"large language models (LLMs)\" - 能力方向：涉及\"mathematical reasoning\"和\"competitive programming solutions\"，属于通用推理能力范畴 - 训练方法：提出\"evolutionary agentic harnesses\"和\"bandit-based LLM ensemble selection strategy\"，属于进化学习方法 - 新兴范式：包含\"agentic harnesses\"，与LLM-based agents相关 第三步排除标准：论文不符合任何排除标准。虽然提到了圆打包、数学推理等应用场景，但这些是作为评估框架通用性的示例，而非论文的主要焦点。论文核心是提出通用的程序进化框架，而非针对特定领域应用。 第四步特殊情况处理：论文提出的\"evolutionary agentic harnesses\"是一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非针对特定领域的应用，因此符合保留条件。 综合分析，ShinkaEvolve的核心贡献是提出了一种新的进化框架，通过创新的采样和集成选择策略，提高LLM在程序进化方面的样本效率和解决方案质量，这直接服务于提升大语言模型的通用推理能力，符合研究目标。",
    "summary2": "本文旨在解决当前LLM驱动的科学发现方法中存在的样本效率低下和闭源限制问题。针对程序进化任务，我们提出了ShinkaEvolve框架，通过三种关键创新（自适应父程序采样、代码新颖性拒绝采样和基于bandit的LLM集成选择策略）显著提升样本效率。在circle packing、AIME数学推理、ALE-Bench竞赛编程和MoE负载平衡损失设计等多样任务上，ShinkaEvolve以更少样本实现了state-of-the-art性能，并通过开源发布促进了广泛应用。",
    "summary_translation": "我们介绍了ShinkaEvolve：一个新的开源框架，该框架利用大型语言模型（LLMs, Large Language Models）来推动科学发现，具有最先进的性能和前所未有的效率。最近在扩展大型语言模型推理时间计算方面的进展，为通用科学发现带来了显著进步。这些方法依赖于进化智能代理框架（evolutionary agentic harnesses），该框架利用大型语言模型作为变异算子（mutation operators）来生成候选解决方案。然而，当前的代码进化方法存在关键局限性：样本效率低下（sample inefficient），需要数千个样本才能识别有效解决方案，并且仍然是闭源的，阻碍了广泛采用和扩展。\n\nShinkaEvolve解决了这些局限性，引入了三项关键创新：一种平衡探索与利用的父代采样技术（parent sampling technique），用于高效搜索空间探索的代码新颖性拒绝采样（code novelty rejection-sampling），以及基于多臂老虎机（bandit-based）的大型语言模型集成选择策略（LLM ensemble selection strategy）。我们在多样化任务上评估了ShinkaEvolve，展示了在样本效率和解决方案质量上的一致性改进。\n\nShinkaEvolve仅使用150个样本就发现了一种新的最先进的圆形打包（circle packing）解决方案，为AIME数学推理任务设计了高性能的智能代理框架，识别出ALE-Bench竞赛编程解决方案的改进，并发现了新颖的专家混合负载平衡损失函数（mixture-of-expert load balancing loss functions），这些函数阐明了优化策略的空间。我们的结果表明，ShinkaEvolve实现了广泛的应用性和卓越的样本效率。通过提供开源的可访问性和成本效益，这项工作使多样化的计算问题中的开放式发现（open-ended discovery）变得民主化。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#73",
    "title": "Pluralistic Off-policy Evaluation and Alignment",
    "link": "/arxiv/2509.19333",
    "arxiv_id": "2509.19333",
    "authors": "Chengkai Huang, Junda Wu, Zhouhang Xie, Yu Xia, Rui Wang, Tong Yu, Subrata Mitra, Julian McAuley, Lina Yao",
    "summary": "Personalized preference alignment for LLMs with diverse human preferences requires evaluation and alignment methods that capture pluralism. Most existing preference alignment datasets are logged under policies that differ substantially from the evaluated LLMs, and existing off-policy estimators focus solely on overall utility while ignoring preference pluralism. Extending Off-Policy Evaluation (OPE) to pluralistic preference alignment, therefore, remains an open question. Thus, we propose the Pluralistic Off-Policy Evaluation (POPE), the first framework for offline pluralistic preference evaluation and alignment in LLMs. POPE includes a unified reward function that combines (1) a collaborative utility component derived from human preference signals (e.g., upvotes or relevance scores) and (2) a diversity component inspired by entropy-based coverage measures, together reflecting pluralistic alignment. Furthermore, to estimate this reward from logged interactions, we derive decomposable inverse propensity scoring (IPS) estimators that separately evaluate relevance and diversity. Theoretically, we prove that our decomposed IPS estimators establish a lower bound on their variance. With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance pluralistic alignment. Empirical results demonstrate that POPE efficiently enhances pluralistic response generation and maintains the models' general capabilities on downstream tasks",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-15",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.424248",
    "filter_reason": "这篇论文的核心贡献是提出了POPE（Pluralistic Off-Policy Evaluation）框架，用于解决LLM在多元人类偏好下的评估和对齐问题。从第一步判断来看，论文本质上是关于改进LLM的基础能力（偏好对齐），提出新的评估和优化框架，这符合保留标准。论文明确针对LLM的偏好对齐问题，并涉及到强化学习中的离线策略评估和优化概念，这与第二步中的正面指标部分吻合。论文不符合第三步中的排除标准，它不主要聚焦于多模态、特定应用领域或模型可靠性的应用层面问题。虽然论文没有直接讨论推理、规划或问题解决能力，但它关注的是偏好对齐，这是LLM的一个重要基础能力，良好的偏好对齐是模型展现高质量推理能力的前提。论文提出的框架通过结合人类偏好信号和多样性组件来改进模型的基础能力，这种改进可能间接提升模型在推理和其他任务上的表现。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解决LLM中多元化偏好对齐的离线评估问题。针对记录在不同策略下的偏好数据，我们提出了POPE框架，结合协作效用和多样性奖励的统一函数，并通过可分解的反向倾向评分估计器进行评估。在Alpaca-GPT4、电影评论等数据集上通过PL-Score、Pluralistic Coverage等指标验证了其有效性。",
    "summary_translation": "针对具有多样化人类偏好的大型语言模型(LLMs, 大型语言模型)的个性化偏好对齐(preference alignment, 偏好对齐)，需要能够捕捉多元性(pluralism, 多元性)的评估和对齐方法。大多数现有的偏好对齐数据集是在与被评估的LLMs显著不同的策略下记录的，而现有的离策略估计器(off-policy estimators, 离策略估计器)仅关注整体效用(utility, 效用)，却忽视了偏好多元性(preference pluralism, 偏好多元性)。因此，将离策略评估(Off-Policy Evaluation, OPE)扩展到多元偏好对齐(pluralistic preference alignment, 多元偏好对齐)仍然是一个开放性问题。为此，我们提出了多元离策略评估(Pluralistic Off-Policy Evaluation, POPE)，这是首个用于LLMs离线(offline, 离线)多元偏好评估和对齐的框架。POPE包含一个统一的奖励函数(reward function, 奖励函数)，该函数结合了(1)源自人类偏好信号（例如，点赞或相关性评分）的协作效用组件(collaborative utility component, 协作效用组件)，以及(2)受基于熵的覆盖度量(entropy-based coverage measures, 基于熵的覆盖度量)启发的多样性组件(diversity component, 多样性组件)，共同反映了多元对齐(pluralistic alignment, 多元对齐)。此外，为了从记录的交互中估计此奖励，我们推导出了可分解的反向倾向评分(inverse propensity scoring, IPS)估计器，该估计器分别评估相关性(relevance, 相关性)和多样性(diversity, 多样性)。理论上，我们证明了我们分解的IPS估计器为其方差建立了下界。通过离策略评估的值函数(value function, 值函数)，我们可以直接启用离策略优化(off-policy optimization, 离策略优化)，以进一步增强多元对齐。实证结果表明，POPE有效增强了多元响应生成(pluralistic response generation, 多元响应生成)，并保持了模型在下游任务(downstream tasks, 下游任务)上的通用能力。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#84",
    "title": "Failure Modes of Maximum Entropy RLHF",
    "link": "/arxiv/2509.20265",
    "arxiv_id": "2509.20265",
    "authors": "Ömer Veysel Çağatan, Barış Akgün",
    "summary": "In this paper, we show that Simple Preference Optimization (SimPO) can be derived as Maximum Entropy Reinforcement Learning with length-normalized temperature, providing a theoretical foundation for this reference-free method. Motivated by SimPO's strong performance in offline preference optimization, we investigate whether Maximum Entropy RL can achieve similar results in online RLHF settings. Our experiments find that Maximum Entropy RL consistently exhibits overoptimization and unstable KL dynamics, even at very low learning rates. Unlike KL-constrained methods that maintain stable training, entropy regularization fails to prevent reward hacking and appears to correlate with overoptimization. Lastly, we discuss possible explanations for why SimPO succeeds in offline settings while Maximum Entropy RL struggles in online scenarios. Our findings suggest that reference-free approaches may face distinct challenges when applied to online or offline preference learning.",
    "subjects": "Machine Learning, Computation and Language",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.426438",
    "filter_reason": "这篇论文的核心是研究RLHF（Reinforcement Learning from Human Feedback）的优化问题，特别是分析了最大熵强化学习在在线RLHF设置中的失败模式，并探讨了SimPO在离线设置中成功的原因。RLHF是提升大语言模型通用能力的关键训练技术，论文研究的是如何改进这一训练方法，属于\"改进LLM的基础能力、提出新的训练范式\"的范畴。论文直接关注强化学习（RLHF）这一训练方法，符合正面指标。同时，论文不涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性的应用层面）。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
    "summary2": "本文旨在研究最大熵强化学习在人类反馈强化学习(RLHF)中的失效模式。针对在线和离线偏好学习场景，我们提出了一种将SimPO解释为长度归一化温度的最大熵RL的理论框架，并在TL;DR数据集上通过胜率和KL散度等指标验证了其有效性。实验发现，尽管SimPO在离线设置中表现良好，但在线最大熵RL存在过优化和不稳定问题，表明熵正则化无法有效防止奖励 hacking。",
    "summary_translation": "本文表明，简单偏好优化（Simple Preference Optimization, SimPO）可被推导为具有长度归一化温度的最大熵强化学习（Maximum Entropy Reinforcement Learning），为这种无参考方法（reference-free method）提供了理论基础。受SimPO在离线偏好优化中出色表现的启发，我们研究了最大熵强化学习是否能在在线RLHF（基于人类反馈的强化学习）设置中取得类似结果。我们的实验发现，即使在非常低的学习率下，最大熵强化学习也始终表现出过度优化（overoptimization）和不稳定的KL（Kullback-Leibler）动态。与能够保持稳定训练的KL约束方法不同，熵正则化（entropy regularization）未能防止奖励黑客（reward hacking），并且似乎与过度优化相关。最后，我们讨论了为什么SimPO在离线设置中成功而最大熵强化学习在在线场景中挣扎的可能解释。我们的研究结果表明，无参考方法在应用于在线或离线偏好学习时可能面临不同的挑战。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#90",
    "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning",
    "link": "/arxiv/2509.19894",
    "arxiv_id": "2509.19894",
    "authors": "Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong",
    "summary": "Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at https://github.com/inclusionAI/PromptCoT.",
    "subjects": "Machine Learning, Computation and Language",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.427978",
    "filter_reason": "这篇论文完全符合我的研究范围。首先，从核心判断来看，论文的本质是提升大语言模型的通用推理能力，特别是数学和编程推理能力。论文提出了PromptCoT 2.0框架，通过改进提示合成方法来增强LLM的推理能力，这属于\"改进LLM的基础能力和提出新的训练范式\"的范畴。 其次，论文包含多个正面指标：明确关注\"Large language models (LLMs)\"；核心能力方向是\"reasoning\"，特别是\"math reasoning\"和\"logical reasoning\"；提出了新的训练方法，包括\"Self-Play\"和\"Supervised Fine-Tuning (SFT)\"。 第三，论文不涉及任何排除标准中的领域：没有关注多模态与视觉问题；虽然涉及数学和编程，但这些是通用推理的基础领域而非特定应用领域；也没有主要关注模型可靠性的应用层面问题。 论文的核心贡献是提出了一种可扩展的提示合成框架，通过迭代改进推理过程来生成更难、更多样化的问题，从而提升LLM的推理能力。这种方法从根本上增强了模型的基础推理能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决大型语言模型推理任务中高质量训练数据短缺的问题。针对数学和编程领域的数据需求，我们提出了一种基于期望最大化(EM)循环优化的PromptCoT 2.0框架，通过迭代改进推理来指导提示构建，生成更难且更多样化的问题。在AIME、HMMT、LiveCodeBench和Codeforces等六个基准测试上，通过pass@1准确率和Elo评级验证了其有效性，在Self-Play和SFT两种设置下均取得了最先进结果。",
    "summary_translation": "大型语言模型（Large Language Models, LLMs）正在从对话系统演变为强大的推理工具，用于奥数竞赛和竞技编程等任务。尽管参数扩展（scaling parameters）和测试时计算（test-time computation）推动了进展，但一个关键瓶颈是缺乏高质量的训练问题：人工策划的数据集成本高昂且有限，而现有的合成语料库（synthetic corpora）往往过于简单或范围狭窄。PromptCoT 1.0表明，在提示合成中注入推理过程（rationales）会增加问题难度。在此基础上，我们提出了PromptCoT 2.0，这是一个可扩展的框架，用期望最大化（Expectation-Maximization, EM）循环替代手工设计的启发式方法，其中推理过程被迭代优化以指导提示构建。这产生的问题比之前的语料库更难且更多样化。\n\n这些合成提示支持两种后训练机制：（1）自我对弈（Self-Play），其中强模型通过可验证的反馈在没有更强教师的情况下自主改进；（2）监督微调（Supervised Fine-Tuning, SFT），其中弱模型从教师蒸馏的轨迹中学习。广泛的实验证明了这种方法的有效性。在自我对弈中，将PromptCoT 2.0应用于Qwen3-30B-A3B-Thinking-2507在300亿参数规模上创造了新的最先进结果，在AIME 24/25和HMMT 25上分别提升+4.4、+4.8和+5.3，在LiveCodeBench v5/v6上提升+6.1和+5.0，在Codeforces上提升+35 Elo。在SFT中，仅使用合成提示训练Qwen2.5-7B-Instruct将准确率提升至73.1（AIME 24）、65.6（AIME 25）和53.4（LiveCodeBench v5），超过了在人工或混合数据上训练的模型。\n\n分析进一步证实，PromptCoT 2.0产生了本质上更难且分布上不同的问题。这些结果将提示合成确立为扩展推理的新维度，并将PromptCoT 2.0定位为未来开源模型的可扩展基础。该实现在https://github.com/inclusionAI/PromptCoT上可用。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#91",
    "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models",
    "link": "/arxiv/2509.19803",
    "arxiv_id": "2509.19803",
    "authors": "Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang",
    "summary": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.",
    "subjects": "Machine Learning, Computation and Language",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.428228",
    "filter_reason": "这篇论文的核心是提出VCRL，一种基于课程学习的强化学习框架，用于提高大语言模型的推理能力。论文本质上是关于改进LLM的基础能力，特别是数学推理能力，这符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。论文包含多个正面指标，如关注LLMs核心概念、数学推理能力方向以及强化学习训练方法。同时，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文主要在数学推理任务上进行实验，但提出的方法是通用的课程学习强化学习框架，通过动态控制训练样本的难度来提高LLM对不同难度样本的学习能力，这与人类从易到难的认知过程一致，可以推广到其他需要推理能力的任务。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解决大语言模型在数学推理任务中强化学习训练时没有考虑样本难度匹配的问题。针对不同难度的数学推理样本，我们提出了一种基于方差的课程强化学习框架VCRL，并在五个数学基准测试上通过准确率等指标验证了其有效性。",
    "summary_translation": "基于策略的强化学习（Policy-based reinforcement learning）目前在提升大语言模型（LLMs）数学推理能力方面发挥着重要作用。然而，现有的基于展开的强化学习方法（rollout-based reinforcement learning methods）（如GRPO、DAPO、GSPO等）未能明确考虑大语言模型对不同难度样本的学习能力，这与人类从易到难的数学推理任务认知过程相悖。直观上，我们发现RLVR中展开组（rollout group）的奖励方差部分反映了当前样本对大语言模型的难度。过易或过难的样本具有较低的方差，而难度适中的样本具有较高的方差。基于此，我们提出了VCRL，一种基于组奖励方差动态控制训练样本难度的课程强化学习（curriculum reinforcement learning）框架。在五个数学基准测试和两个模型上的实验揭示了VCRL相较于当前大语言模型强化学习基线方法的优势。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#86",
    "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI",
    "link": "/arxiv/2509.20175",
    "arxiv_id": "2509.20175",
    "authors": "Lorenzo Giusti, Ole Anton Werner, Riccardo Taiello, Matilde Carvalho Costa, Emre Tosun, Andrea Protani, Marc Molina, Rodrigo Lopes de Almeida, Paolo Cacace, Diogo Reis Santos, Luigi Serio",
    "summary": "We present Federation of Agents (FoA), a distributed orchestration framework that transforms static multi-agent coordination into dynamic, capability-driven collaboration. FoA introduces Versioned Capability Vectors (VCVs): machine-readable profiles that make agent capabilities searchable through semantic embeddings, enabling agents to advertise their capabilities, cost, and limitations. Our aarchitecturecombines three key innovations: (1) semantic routing that matches tasks to agents over sharded HNSW indices while enforcing operational constraints through cost-biased optimization, (2) dynamic task decomposition where compatible agents collaboratively break down complex tasks into DAGs of subtasks through consensus-based merging, and (3) smart clustering that groups agents working on similar subtasks into collaborative channels for k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe semantics for scalable message passing, FoA achieves sub-linear complexity through hierarchical capability matching and efficient index maintenance. Evaluation on HealthBench shows 13x improvements over single-model baselines, with clustering-enhanced laboration particularly effective for complex reasoning tasks requiring multiple perspectives. The system scales horizontally while maintaining consistent performance, demonstrating that semantic orchestration with structured collaboration can unlock the collective intelligence of heterogeneous federations of AI agents.",
    "subjects": "Artificial Intelligence, Computation and Language",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.427158",
    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Federation of Agents (FoA)\"的分布式编排框架，用于实现大规模智能体AI的动态协作。从本质上看，论文属于\"智能体协作框架\"的研究范畴，符合筛选标准中的保留条件。论文提出的版本化能力向量(VCVs)、语义路由、动态任务分解和智能聚类等创新方法，都是为了提升智能体系统的通用协作和推理能力，而非将LLM作为工具应用到特定领域。 论文在正面指标上表现良好，涉及了\"multi-agent systems\"这一新兴范式，并明确提到该系统在\"complex reasoning tasks\"上表现出色，这与\"通用推理能力\"的研究目标直接相关。虽然论文没有直接提及\"Large language models\"，但智能体系统通常基于LLM构建，且论文关注的是通用能力的提升。 在排除标准方面，论文没有主要关注多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然评估中使用了HealthBench数据集，但这仅用于验证系统性能，论文本身并非针对医疗等特定领域的研究。 综合分析，这篇论文提出的是一种通用的智能体协作框架，旨在通过语义感知的通信机制增强智能体系统的协作和推理能力，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
    "summary2": "本文旨在解决大规模多智能体AI系统中能力发现和动态协调的问题。针对异构智能体协作场景，我们提出了一种Federation of Agents (FoA)语义感知通信框架，通过Versioned Capability Vectors实现能力驱动的动态编排，并在HealthBench数据集上通过任务完成质量指标验证了其有效性，相比单模型基线实现了13倍的性能提升。",
    "summary_translation": "我们提出了代理联盟（Federation of Agents, FoA），这是一个分布式编排框架（distributed orchestration framework），能够将静态的多智能体协调转变为动态的、由能力驱动的协作。FoA引入了版本化能力向量（Versioned Capability Vectors, VCVs）：这是一种机器可读的配置文件，通过语义嵌入（semantic embeddings）使智能体能力可被搜索，使智能体能够宣传其能力、成本和局限性。我们的架构结合了三个关键创新：(1) 语义路由（semantic routing），它在分片的HNSW索引上将任务匹配到智能体，同时通过成本偏置优化（cost-biased optimization）强制执行操作约束；(2) 动态任务分解（dynamic task decomposition），其中兼容的智能体通过基于共识的合并（consensus-based merging）协作地将复杂任务分解为有向无环图（DAGs）的子任务；以及(3) 智能聚类（smart clustering），它将处理相似子任务的智能体分组到协作通道中，在综合之前进行k轮细化。FoA建立在MQTT的发布-订阅语义（publish-subscribe semantics）之上，以实现可扩展的消息传递，并通过分层能力匹配（hierarchical capability matching）和高效的索引维护（efficient index maintenance）实现了次线性复杂度（sub-linear complexity）。在HealthBench上的评估显示，与单模型基线相比有13倍的改进，其中聚类增强的协作（clustering-enhanced collaboration）对于需要多视角的复杂推理任务特别有效。该系统能够水平扩展（scales horizontally）同时保持一致的性能，表明具有结构化协作的语义编排（semantic orchestration）可以释放异构AI代理联盟（heterogeneous federations of AI agents）的集体智能。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#92",
    "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
    "link": "/arxiv/2509.19736",
    "arxiv_id": "2509.19736",
    "authors": "Cheng Qian, Zuxin Liu, Akshara Prabhakar, Jielin Qiu, Zhiwei Liu, Haolin Chen, Shirley Kokane, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang",
    "summary": "Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.",
    "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.428533",
    "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是提出UserRL框架，通过强化学习训练用户中心的智能体，这属于\"智能体协作框架\"的范畴，是改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用于特定领域。其次，论文包含多个正面指标：明确使用了大语言模型(Qwen3)，采用了强化学习(RL)方法训练模型，研究了基于LLM的智能体(agentic models)，并关注动态多轮交互能力，这些都符合研究目标。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。最后，虽然论文涉及智能体研究，但它提出的是通用的智能体训练框架，而非针对特定领域的应用，因此应予以保留。论文的核心贡献在于探索如何通过奖励塑造和用户模拟选择来提升智能体的通用交互能力，这与提高大语言模型通用推理能力的研究目标高度一致。",
    "summary2": "本文旨在解决如何训练能有效获取用户中心能力的智能体模型，同时考虑用户交互多样性和动态性的问题。针对多轮用户交互场景，我们提出了一种UserRL框架，结合标准化gym环境和模拟用户，并在Qwen3模型上通过不同奖励设计策略验证了其有效性。",
    "summary_translation": "强化学习 (Reinforcement learning, RL) 在训练智能体模型 (agentic models) 方面显示出潜力，这些模型能够超越静态基准测试 (static benchmarks)，进行动态、多轮交互 (dynamic, multi-turn interactions)。然而，这类智能体的最终价值在于其协助用户的能力，而在这一场景中，用户交互的多样性和动态性带来了挑战。在这项工作中，我们提出了UserRL，这是一个通过标准化的gym环境 (gym environments) 配合模拟用户 (simulated users) 来训练和评估以用户为中心能力 (user-centric abilities) 的统一框架。我们系统性地改变轮级奖励分配 (turn-level reward assignment) 和轨迹级分数计算 (trajectory-level score calculation)，以分析不同表述形式如何影响GRPO算法 (GRPO algorithm) 下的学习效果。我们在Qwen3模型 (Qwen3 models) 上的实验揭示了三个关键发现：(i) SFT冷启动 (SFT cold start) 对于解锁初始交互能力和实现持续的RL改进至关重要；(ii) 精心设计的轨迹评分 (deliberate trajectory scoring) 能够产生更高效且有效的多轮交互；(iii) 虽然更强大的模拟用户（如GPT-4o）能够促进训练，但开源模拟器（如Qwen3-32B）仍然是一种经济高效且可迁移的选择。总体而言，这些结果强调，奖励塑造 (reward shaping) 和用户模拟选择 (user simulation choice) 的精心设计与模型规模 (model scale) 同样重要，并将UserRL确立为开发强大的以用户为中心的智能体模型 (robust user-centric agentic models) 的实用途径。所有代码和数据均公开，以供未来研究使用。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#97",
    "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
    "link": "/arxiv/2509.19517",
    "arxiv_id": "2509.19517",
    "authors": "Sai Teja Reddy Adapala",
    "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.",
    "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
    "date": "2025-09-23",
    "category": "cs.CL",
    "crawl_time": "2025-09-25T09:53:05.429575",
    "filter_reason": "这篇论文完全符合研究目标，核心贡献是研究大语言模型在认知负荷下的多跳推理能力限制。根据筛选标准，我的判断过程如下： 第一步：核心判断——论文本质是研究LLM本身的推理能力限制。论文提出了计算认知负荷的形式理论，设计了新的评估基准(ICE)来测试LLM在多跳推理任务上的表现，特别是在认知负荷条件下的性能变化。这不是将LLM应用于特定领域，而是直接研究LLM的基础推理能力，属于改进LLM通用能力的研究。 第二步：正面指标——论文包含关键正面指标：(1)核心概念：明确研究Large Language Models (LLMs)；(2)能力方向：聚焦于multi-hop reasoning（多跳推理），属于逻辑推理范畴。虽然未涉及训练方法和新兴范式，但这两个核心正面指标已足够表明论文与研究方向高度相关。 第三步：排除标准——论文不涉及任何需要排除的领域。它没有研究多模态与视觉问题，没有聚焦于特定应用领域（如医疗、化学等），也没有从应用层面研究模型可靠性。 第四步：特殊和模糊情况——论文提到\"hallucination-as-guessing under uncertainty\"，这是从认知机制角度解释幻觉现象，探讨其与推理能力的关系，而非仅进行社会学研究或应用层面讨论，这有助于理解LLM推理能力的本质限制。 综合来看，这篇论文通过研究认知负荷对LLM推理能力的影响，提出了新的评估方法和理论框架，直接服务于提升LLM通用推理能力的研究目标，完全符合筛选要求。",
    "summary2": "本文旨在研究大型语言模型在认知负荷下的多跳推理能力限制。针对信息丰富、任务切换的动态场景，我们提出了计算认知负荷理论，并设计了Interleaved Cognitive Evaluation (ICE)基准测试系统操纵上下文饱和和注意力残留因素。在五个LLMs上通过Exact-Match准确率验证发现：Gemini-2.0-Flash-001在控制条件下达85%准确率，但在额外信息增加时性能显著下降(β = -0.003, p < 0.001)，而较小模型如Llama-3-8B-Instruct在所有条件下均表现完全失效。",
    "summary_translation": "大型语言模型（LLMs）的扩展揭示了它们在静态基准测试上的表现与在动态、信息丰富环境中的脆弱性之间的关键差距。尽管模型在孤立任务上表现出色，但控制其在认知负荷（cognitive load）下推理的计算限制仍然知之甚少。在本研究中，我们提出了一个计算认知负荷（computational cognitive load）的正式理论，假设外部的、与任务无关的信息（Context Saturation，上下文饱和）和任务切换造成的干扰（Attentional Residue，注意力残留）是导致性能下降的关键机制。我们设计了交错认知评估（Interleaved Cognitive Evaluation, ICE），这是一个去混淆的基准测试，用于在具有挑战性的多跳推理（multi-hop reasoning）任务上系统地操纵这些负荷因素。一项全面研究（200个问题中每个项目重复10次）揭示了五个经过指令微调（instruction-tuned）的模型之间存在显著的性能差异。较小的开源架构（Llama-3-8B-Instruct、Mistral-7B-Instruct-v0.2）表现出基线脆弱性（baseline brittleness），在这个高内在负荷（high-intrinsic-load）任务的所有条件下（包括干净的对照组）实现了0%的准确率（SEM = 0.0）。相比之下，Gemini-2.0-Flash-001表现出部分韧性（partial resilience），在对照条件下达到85%的准确率，在上下文饱和条件下出现统计学显著的性能下降（$\\beta = -0.003$每%负荷，$p < 0.001$）。这些发现提供了初步证据，表明认知负荷是推理失败的关键因素，支持了在不确定性下幻觉即猜测（hallucination-as-guessing）的理论。我们得出结论，动态的、具有认知意识的压力测试（cognitive-aware stress testing），如ICE基准测试所示，对于评估先进AI系统的真正韧性和安全性至关重要。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#5",
    "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs",
    "link": "/arxiv/2509.20105",
    "arxiv_id": "2509.20105",
    "authors": "Venkat Margapuri, Garik Kazanjian, Naren Kosaraju",
    "summary": "Large Language Models (LLMs) often struggle with maintaining coherent multi-step reasoning traces, particularly in tasks that require a structured logical flow. This work introduces a quantum-inspired approach to address the challenge by incorporating a fidelity-based reward derived from Projected Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior approaches that use direct supervision or contrastive objectives, the proposed method guides learning through structural consistency, offering a novel approach to enforce global coherence in generated reasoning traces. The proposed framework is evaluated using multiple coherence-determining metrics on diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning arithmetic, intuitive, and entailment-based reasoning. Results show that the proposed quantum-inspired approach offers significant improvements over supervised, contrastive, and pretrained baseline approaches, highlighting the effectiveness of quantum-inspired fidelity as a foundation to improve reasoning trace coherence in LLMs.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-24",
    "category": "cs.AI",
    "crawl_time": "2025-09-25T09:53:05.896620",
    "filter_reason": "这篇论文完全符合研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础推理能力，提出了一种量子启发的强化学习方法来增强LLM在连贯多步推理方面的表现。这直接对应了\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的核心标准。 从正面指标分析，论文明确包含以下关键要素： - 核心概念：直接关注\"Large Language Models (LLMs)\" - 能力方向：专注于\"coherent multi-step reasoning traces\"和\"structured logical flow\"，属于推理能力范畴 - 训练方法：采用强化学习方法（Proximal Policy Optimization），结合量子物理中的Projected Entangled Pair States (PEPS)概念 论文不涉及任何排除标准中的领域。它不是关于多模态与视觉研究，不是将LLM应用到特定领域，也不是关于模型可靠性在应用层面的研究。虽然论文在GSM8K、StrategyQA和EntailmentBank等数据集上进行了评估，但这些是评估通用推理能力的标准数据集，而非特定领域应用。 论文的核心贡献是提出了一种基于量子物理概念的强化学习方法，通过保真度奖励机制来提高LLM生成连贯推理痕迹的能力，这是一种从根本上提升模型推理能力的方法论创新，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决大型语言模型(LLMs)在保持连贯多步推理追踪方面的困难。针对需要结构化逻辑流程的任务，我们提出了一种量子启发的强化学习方法，使用Projected Entangled Pair States (PEPS)导出基于保真度的奖励并集成到Proximal Policy Optimization (PPO)中，并在GSM8K、StrategyQA和EntailmentBank数据集上通过MEC、WES、BERT和BLEURT等指标验证了其有效性。",
    "summary_translation": "大型语言模型（Large Language Models, LLMs）在保持连贯的多步推理轨迹方面常常面临困难，特别是在需要结构化逻辑流程的任务中。本研究引入了一种量子启发（quantum-inspired）的方法，通过将基于投影纠缠对态（Projected Entangled Pair States, PEPS）的保真度奖励（fidelity-based reward）纳入近端策略优化（Proximal Policy Optimization）来应对这一挑战。与先前使用直接监督或对比目标的方法不同，所提出的方法通过结构一致性指导学习，为在生成的推理轨迹中强制执行全局连贯性提供了一种新途径。该框架在多个数据集上使用多种连贯性确定指标进行了评估，这些数据集包括GSM8K、StrategyQA和EntailmentBank，涵盖了算术、直观和基于蕴含（entailment-based）的推理类型。结果表明，所提出的量子启发方法相比监督、对比和预训练的基线方法有显著改进，凸显了量子启发的保真度作为提高大型语言模型中推理轨迹连贯性基础的有效性。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#14",
    "title": "The Conductor and the Engine: A Path Towards Co-Designed Reasoning",
    "link": "/arxiv/2509.19762",
    "arxiv_id": "2509.19762",
    "authors": "Yuanxin Wang, Pawel Filipczuk, Anisha Garg, Amaan Dhada, Mohammad Hassanpour, David Bick, Ganesh Venkatesh",
    "summary": "Modern LLM reasoning relies on extensive test-time computation, driven by internal model training and external agentic orchestration. However, this synergy is often inefficient, as model verbosity and poor instruction following lead to wasted compute. We analyze this capability-cost trade-off and introduce an optimized reasoning workflow (\\cepo) that empowers smaller open-source models to outperform models multiple times their size. We will open-source this workflow to enable further research. Our work demonstrates a clear path toward co-designing orchestration frameworks with the underlying model capabilities to unlock powerful reasoning in small-to-medium sized models.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-24",
    "category": "cs.AI",
    "crawl_time": "2025-09-25T09:53:05.898361",
    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，该论文的本质是关于改进LLM的推理能力，提出了一种优化的推理工作流程(\\cepo)，通过协调内部模型训练和外部智能体编排来提高推理效率，使较小的开源模型能够超越比它们大得多的模型。这直接符合\"改进LLM的基础能力、增强其逻辑、多步推理等通用能力\"的标准。 其次，从正面指标看，论文明确涉及\"LLM reasoning\"这一核心概念和\"reasoning\"这一能力方向，同时提到了\"external agentic orchestration\"，与智能体(llm-based agents)这一新兴范式相关。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，在特殊情况下，论文提出的智能体编排框架是通用性的，旨在增强LLM的通用推理能力，而非应用于特定领域，因此应该保留。 综上所述，该论文的核心贡献是提出了一种协同设计编排框架与底层模型能力的方法，以释放中小型模型的强大推理能力，这与研究目标高度一致。",
    "summary2": "本文旨在 [解决现代LLM推理中因模型冗长和指令遵循不佳导致的计算浪费问题]。针对 [中小型开源模型的推理效率与性能问题]，我们提出了一种 [CODA（Conductor-driven Architecture）优化推理工作流，包含自适应规划、执行、自我反思和验证等关键组件]，并在 [AIME、GPQA、LiveCodeBench等数学和编码基准测试] 上通过 [准确率、Pass@k等指标] 验证了其有效性。",
    "summary_translation": "现代大语言模型(LLM, Large Language Model)推理依赖于广泛的测试时计算(test-time computation)，这种计算由内部模型训练和外部智能体编排(agentic orchestration)共同驱动。然而，这种协同作用(synergy)往往效率低下，因为模型的冗长性(verbosity)和不良的指令遵循(instruction following)能力导致计算资源浪费(wasted compute)。我们分析了这种能力-成本权衡(capability-cost trade-off)，并提出了一种优化的推理工作流(optimized reasoning workflow) \\cepo，它使较小的开源模型能够超越规模大得多的模型。我们将开源(open-source)这一工作流，以促进进一步的研究。我们的工作展示了一条明确的路径，即通过协同设计(co-designing)编排框架(orchestration frameworks)与底层模型能力(underlying model capabilities)，来解锁中小型模型(small-to-medium sized models)的强大推理能力。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#16",
    "title": "Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving",
    "link": "/arxiv/2509.19681",
    "arxiv_id": "2509.19681",
    "authors": "Anisha Garg, Engin Tekin, Yash More, David Bick, Nishit Neema, Ganesh Venkatesh",
    "summary": "Advanced test-time computing strategies are essential for scaling reasoning models, but their effectiveness is capped by the models' poor self-evaluation. We propose a pairwise Explanatory Verifier, trained via reinforcement learning (GRPO), that produces calibrated confidence scores and associated natural language reasoning for generated solutions. Our verifier improves the accuracy and efficiency of test-time strategies like best-of-n and self-reflection. Crucially, it excels at identifying challenging failure modes, such as when both candidate solutions are identically incorrect, succeeding where standard methods like majority voting fail.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-24",
    "category": "cs.AI",
    "crawl_time": "2025-09-25T09:53:05.898800",
    "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出一种\"解释性验证器\"(Explanatory Verifier)来增强大语言模型的通用推理能力。具体分析如下： 从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是通过强化学习(GRPO)训练的验证器来提升模型的自我评估能力，这属于增强LLM通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 从第二步正面指标看，论文涉及多个相关主题： 1. 能力方向：明确聚焦于\"reasoning\"和\"problem-solving\"，这正是研究目标的核心 2. 训练方法：使用\"reinforcement learning (GRPO)\"进行训练，符合强化学习优化LLM能力的方向 3. 提到的\"self-reflection\"也与提升模型自主推理能力相关 从第三步排除标准看，论文不涉及任何多模态、视觉内容，也不针对医疗、化学、生物等特定应用领域，同时虽然涉及到模型可靠性，但目的是从根本上提升模型的推理能力而非仅作为应用层面的防御。 论文特别关注提高LLM的\"自我评估\"能力，这是通用推理能力的重要组成部分，通过校准的置信度分数和自然语言解释来增强模型的推理质量，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决推理模型在测试时计算策略中自我评估能力不足的问题。针对数学和编码问题求解场景，我们提出了一种基于强化学习(GRPO)训练的成对解释性验证器(Explanatory Verifier)，并在Numina Math、CodeForces和LeetCode数据集上通过准确性和计算效率指标验证了其有效性。",
    "summary_translation": "先进的测试时计算（test-time computing）策略对于扩展推理模型至关重要，但其有效性受到模型自我评估（self-evaluation）能力不足的限制。我们提出了一种成对解释验证器（pairwise Explanatory Verifier），通过强化学习（GRPO）进行训练，可为生成的解决方案生成校准的置信度分数（calibrated confidence scores）及相关自然语言推理（natural language reasoning）。我们的验证器提高了最佳n选一（best-of-n）和自我反思（self-reflection）等测试时策略的准确性和效率。关键的是，它擅长识别具有挑战性的故障模式（failure modes），例如当两个候选解决方案都完全错误时，能够在多数投票（majority voting）等标准方法失败的情况下取得成功。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#28",
    "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing",
    "link": "/arxiv/2509.20336",
    "arxiv_id": "2509.20336",
    "authors": "Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang",
    "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.",
    "subjects": "Machine Learning, Artificial Intelligence",
    "date": "2025-09-24",
    "category": "cs.AI",
    "crawl_time": "2025-09-25T09:53:05.902024",
    "filter_reason": "这篇论文的核心是研究基于Transformer的LLMs在图推理任务中的内部机制，通过circuit-tracer框架来解释decoder-only transformers的推理过程。论文识别了图推理中的两个核心机制：token merging和structural memorization，并提供了统一的可解释性框架来理解结构推理。这符合研究目标中\"改进LLM的基础能力\"和\"增强其逻辑、多步推理等通用能力\"的要求。论文关注的是LLM本身的推理能力机制，而不是将LLM作为工具应用到特定领域。虽然论文聚焦于图推理这一特定类型的推理，但其目标是提供\"统一的可解释性框架\"来理解结构推理，这属于通用推理能力的研究范畴。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。因此，这篇论文符合研究范围。",
    "summary2": "抱歉，我无法根据提供的内容生成学术总结。提供的链接返回了404错误，显示\"File unavailable for 2509.20336\"，表明该论文ID对应的文件在arXiv上不可用或不存在。没有实际的论文内容，我无法提取研究问题、方法创新和实验验证等关键信息来生成专业的学术总结。请提供有效的论文链接或内容，我将很乐意为您生成符合要求的学术总结。",
    "summary_translation": "基于Transformer的大型语言模型（Transformer-based LLMs）在图推理任务（graph reasoning tasks）上表现出强大的性能，然而其内部机制（internal mechanisms）仍未被充分探索。为了以基础且统一的视角揭示这些推理过程机制（reasoning process mechanisms），我们使用了基本的仅解码器Transformer（basic decoder-only transformers），并采用电路追踪框架（circuit-tracer framework）对其进行解释。通过这一视角，我们可视化推理轨迹（reasoning traces），并识别出图推理中的两个核心机制：令牌合并（token merging）和结构记忆（structural memorization），这两个机制是路径推理（path reasoning）和子结构提取任务（substructure extraction tasks）的基础。我们进一步量化了这些行为（behaviors），并分析了它们如何受到图密度（graph density）和模型规模（model size）的影响。我们的研究为理解仅解码器Transformer（decoder-only Transformers）中的结构推理（structural reasoning）提供了一个统一的可解释性框架（unified interpretability framework）。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#107",
    "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms",
    "link": "/arxiv/2509.19702",
    "arxiv_id": "2509.19702",
    "authors": "Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama",
    "summary": "We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nyström extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nyström extrapolation, highlighting a powerful capability of in-context learning.",
    "subjects": "Machine Learning, Artificial Intelligence",
    "date": "2025-09-24",
    "category": "cs.AI",
    "crawl_time": "2025-09-25T09:53:05.925481",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究线性注意力transformer（一种LLM架构）的基础能力。论文展示了模型通过训练能够隐式地发现统一的数值算法，这直接涉及LLM的内在能力提升，而非将LLM作为工具应用于特定领域。论文关注的是上下文学习(in-context learning)能力，这是一种基础能力的研究，与提高LLM的通用推理能力密切相关。 其次，从正面指标分析，论文符合以下关键点： - 核心概念：研究的是线性注意力transformer，属于LLM架构变体 - 能力方向：涉及数学推理(math reasoning)和问题解决(problem-solving)能力，模型通过学习解决矩阵补全问题，隐式发现了数值算法 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于特定应用领域 - 不涉及模型可靠性的应用层面研究 最后，论文的核心贡献是揭示了LLM能够通过训练隐式发现统一的、资源自适应的迭代求解器，这展示了LLM在算法发现和数学推理方面的强大能力，直接关系到通用推理能力的提升。论文研究的是LLM内在的能力机制，而非特定应用，因此完全符合研究目标。",
    "summary2": "本文旨在探索训练线性Transformer是否能隐式发现统一的数值算法。针对低秩矩阵补全任务，我们提出了一种训练线性Transformer在masked-block completion任务上隐式学习数值算法的方法，并通过收敛速度和预测准确率验证了EAGLE算法的有效性。",
    "summary_translation": "我们在数百万个masked-block matrix completion tasks（掩码块矩阵补全任务）上训练了一个linear attention transformer（线性注意力Transformer）：每个提示是一个masked low-rank matrix（掩码低秩矩阵），其缺失的块可能是(i)一个scalar prediction target（标量预测目标）或(ii)一个Nyström extrapolation（Nyström外推）的unseen kernel slice（未见核切片）。模型仅看到输入-输出对和mean-squared loss（均方损失）；它没有被给予normal equations（正规方程）、handcrafted iterations（手工设计的迭代），也没有任何关于这些任务相关的提示。令人惊讶的是，训练后，algebraic unrolling（代数展开）揭示了在三个不同的computational regimes（计算机制）中相同的parameter-free update rule（无参数更新规则）：full visibility（完全可见性）、rank-limited updates（秩限制更新）和distributed computation（分布式计算）。我们证明该规则在full-batch problems（全批量问题）上实现了second-order convergence（二阶收敛），降低了distributed iteration complexity（分布式迭代复杂度），并在rank-limited attention（秩限制注意力）下保持准确性。因此，一个仅被训练来补全缺失块的transformer隐式地发现了一个统一的、resource-adaptive iterative solver（资源自适应迭代求解器），涵盖预测、估计和Nyström extrapolation（Nyström外推），突显了in-context learning（上下文学习）的强大能力。",
    "summary_generated_time": "2025-09-25 17:17:48",
    "summary_model": "z-ai/glm-4.5"
  }
]