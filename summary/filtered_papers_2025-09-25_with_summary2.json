[
  {
    "index": "#4",
    "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective",
    "link": "/arxiv/2509.21134",
    "arxiv_id": "2509.21134",
    "authors": "Yiwen Zhang, Ziang Chen, Fanqi Kong, Yizhe Huang, Xue Feng",
    "summary": "Large Language Models (LLMs) have been used to make decisions in complex scenarios, where they need models to think deeply, reason logically, and decide wisely. Many existing studies focus solely on multi-round conversations in social tasks or simulated environments, neglecting the various types of decisions and their interdependence. Current reinforcement learning methods struggle to consider the strategies of others during training. To address these issues, we first define a strategic decision-making problem that includes two types of decisions and their temporal dependencies. Furthermore, we propose **T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to optimize the perception of other individual strategies and the game situation trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm, ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating rollouts based on reasoning the strategies of other individuals, 2) estimating advantages at both the graph-level and sample-level, and 3) balancing global and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in terms of model output compliance and cooperative outcomes. Additionally, when compared to models with parameter sizes 100 times larger, it shows an 18% improvement. This demonstrates the effectiveness of the ToMPO algorithm in enhancing the model's strategic decision-making capabilities.",
    "subjects": "Artificial Intelligence, Multiagent Systems",
    "date": "2025-09-25",
    "category": "cs.MA",
    "crawl_time": "2025-09-26T21:01:54.501280",
    "filter_reason": "这篇论文完全符合我的研究目标，因为它专注于提升大语言模型本身的通用推理能力。从核心判断来看，论文的本质是提出ToMPO算法来增强LLM的战略决策能力，这是一种新的训练范式，属于改进LLM基础能力的研究。论文明确关注LLM在复杂场景中的推理和决策能力，需要\"深入思考、逻辑推理和明智决策\"，这正是通用推理能力的核心要素。 从正面指标看，论文包含了多个相关主题：核心概念是LLMs；能力方向涉及reasoning和strategic decision-making；训练方法采用了reinforcement learning（ToMPO算法）；新兴范式方面则从multi-agent perspective研究问题。 论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文从多智能体角度研究，但它是提出一种通用的框架来增强LLM的战略决策能力，而不是将智能体应用在特定领域。 论文的核心贡献是ToMPO算法，通过基于推理其他个体策略生成rollouts、在图级和样本级估计优势、平衡全局和部分奖励来增强LLM的战略决策能力，这直接提升了LLM的通用推理和决策能力，完全符合我的研究目标。",
    "summary2": "本文旨在解决大型语言模型在多智能体环境中战略决策能力不足的问题。针对复杂社会场景中的图级别和努力级别决策，我们提出了Theory of Mind Policy Optimization (ToMPO)算法，并在BCZ和PGG游戏环境中通过U1(合规性)、U2(战略效率)和U3(合作结果)指标验证了其有效性。ToMPO通过推理其他智能体策略生成rollouts、在图级别和样本级别估计优势并平衡全局与局部奖励，使Qwen-2.5-7B-instruct模型在合规性和合作结果方面比GRPO算法提高35%，比参数量大100倍的模型提升18%。",
    "summary_translation": "大型语言模型（Large Language Models, LLMs）已被用于在复杂场景中做出决策，这些场景需要模型进行深度思考、逻辑推理和明智决策。许多现有研究仅关注社交任务或模拟环境中的多轮对话，忽略了各种类型的决策及其相互依赖性。当前的强化学习方法（reinforcement learning methods）在训练过程中难以考虑他人的策略。为解决这些问题，我们首先定义了一个包含两种决策类型及其时间依赖性（temporal dependencies）的战略决策问题。此外，我们提出了**心智理论策略优化（Theory of Mind Policy Optimization, ToMPO）**算法，以优化对其他个体策略和游戏情境趋势的感知。\n\n与群体相对策略优化（Group Relative Policy Optimization, GRPO）算法相比，ToMPO主要通过以下方式增强LLM的战略决策能力：1）基于推理其他个体策略生成推演（rollouts），2）在图级（graph-level）和样本级（sample-level）估计优势（advantages），以及3）平衡全局和部分奖励。在模型输出合规性和合作结果方面，ToMPO算法比GRPO方法高出35%。此外，与参数规模大100倍的模型相比，它显示出18%的改进。这证明了ToMPO算法在增强模型战略决策能力方面的有效性。",
    "summary_generated_time": "2025-09-26 21:16:00",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#13",
    "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning",
    "link": "/arxiv/2509.21193",
    "arxiv_id": "2509.21193",
    "authors": "Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin",
    "summary": "Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: https://github.com/tangxiangru/Eigen-1.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.160443",
    "filter_reason": "这篇论文的核心贡献是提出了一种统一框架，通过结合隐式检索和结构化协作来增强大型语言模型的推理能力。从筛选标准来看，该论文完全符合我的研究目标。首先，论文本质上是关于改进LLM的基础推理能力，提出了基于Monitor的检索模块、分层解决方案改进(HSR)和质量感知迭代推理(QAIR)等新方法，这些都属于提升LLM通用推理能力的范畴。其次，论文包含多个正面指标，如明确研究大型语言模型(LLMs)、关注推理能力(reasoning)、采用多智能体系统(multi-agent systems)和工具使用(tool use)等新兴范式。虽然论文标题提到\"Scientific Reasoning\"，但其框架是通用的，并非局限于特定应用领域，而是以科学推理作为测试场景来验证其通用推理框架的有效性。论文解决的是LLM推理中的普遍问题（如检索导致的推理碎片化和多智能体流水线中的解决方案稀释问题），提出的方法具有通用性，可以应用于各种推理任务。因此，该论文符合研究范围，应被保留。",
    "summary2": "本文旨在解决科学推理中显式检索导致的推理碎片化和多智能体协作中的解决方案稀释问题。针对复杂的科学推理任务，我们提出了一种结合Monitor-based RAG、Hierarchical Solution Refinement (HSR)和Quality-Aware Iterative Reasoning (QAIR)的自适应多智能体精炼框架，并在Humanitys Last Exam (HLE) Bio/Chem Gold数据集上通过准确率、token使用量和智能体步骤数验证了其有效性。",
    "summary_translation": "大型语言模型（Large language models, LLMs）最近在科学推理方面显示出强劲的进展，但仍存在两个主要瓶颈。首先，显式检索（explicit retrieval）割裂了推理过程，施加了一种隐藏的\"工具税\"（tool tax），即额外的令牌（tokens）和步骤。其次，多智能体管道（multi-agent pipelines）常常通过对所有候选方案进行平均而稀释了优质解决方案。我们提出了一个统一框架来应对这些挑战，该框架结合了隐式检索（implicit retrieval）和结构化协作（structured collaboration）。在该框架基础上，一个基于监控器的检索模块（Monitor-based retrieval module）在令牌（token）级别运行，以最小的推理中断整合外部知识。在此基础之上，分层解决方案精炼（Hierarchical Solution Refinement, HSR）迭代地将每个候选方案指定为锚点（anchor），由其同行进行修复，而质量感知迭代推理（Quality-Aware Iterative Reasoning, QAIR）则根据解决方案质量调整精炼过程。在\"人类终极考试\"（Humanity's Last Exam, HLE）生物/化学金牌数据集上，我们的框架达到了48.3%的准确率——这是迄今为止报道的最高水平，比最强的智能体基线高出13.4个百分点，比前沿大型语言模型领先高达18.1个百分点，同时将令牌使用量减少了53.5%，智能体步骤减少了43.7%。在SuperGPQA和TRQA上的结果证实了该框架在不同领域的鲁棒性（robustness）。错误分析显示，推理失败和知识缺口在超过85%的情况下同时出现，而多样性分析揭示了一个明显的二分法：检索任务受益于解决方案的多样性，而推理任务则倾向于共识。总体而言，这些发现展示了隐式增强（implicit augmentation）和结构化精炼（structured refinement）如何克服显式工具使用（explicit tool use）和统一聚合（uniform aggregation）的低效问题。代码可在以下网址获取：https://github.com/tangxiangru/Eigen-1。",
    "summary_generated_time": "2025-09-26 21:16:04",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#6",
    "title": "Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond",
    "link": "/arxiv/2509.21284",
    "arxiv_id": "2509.21284",
    "authors": "Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng",
    "summary": "Existing research indicates that the output of Chain-of-Thought (CoT) is significantly affected by input perturbations. Although many methods aim to mitigate such impact by optimizing prompts, a theoretical explanation of how these perturbations influence CoT outputs remains an open area of research. This gap limits our in-depth understanding of how input perturbations propagate during the reasoning process and hinders further improvements in prompt optimization methods. Therefore, in this paper, we theoretically analyze the effect of input perturbations on the fluctuation of CoT outputs. We first derive an upper bound for input perturbations under the condition that the output fluctuation is within an acceptable range, based on which we prove that: (i) This upper bound is positively correlated with the number of reasoning steps in the CoT; (ii) Even an infinitely long reasoning process cannot eliminate the impact of input perturbations. We then apply these conclusions to the Linear Self-Attention (LSA) model, which can be viewed as a simplified version of the Transformer. For the LSA model, we prove that the upper bound for input perturbation is negatively correlated with the norms of the input embedding and hidden state vectors. To validate this theoretical analysis, we conduct experiments on three mainstream datasets and four mainstream models. The experimental results align with our theoretical analysis, empirically demonstrating the correctness of our findings.",
    "subjects": "Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.158803",
    "filter_reason": "这篇论文的核心贡献是对思维链(CoT)的鲁棒性进行了理论分析，研究了输入扰动如何影响CoT的推理过程和输出。论文推导了在输出波动可接受范围内输入扰动的上界，并证明了该上界与CoT中的推理步骤数量正相关，以及即使无限长的推理过程也无法消除输入扰动的影响。这些发现对于理解和改进LLM的推理能力具有重要意义。思维链(CoT)是提高大语言模型推理能力的关键方法，论文从理论角度分析其鲁棒性，属于改进LLM基础能力和推理能力的研究范畴，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。论文不涉及多模态、特定应用领域或模型可靠性的应用层面研究，而是关注通用推理能力的理论分析，因此应该被保留。",
    "summary2": "本文旨在解决Chain-of-Thought (CoT)对输入扰动的鲁棒性问题。针对大型语言模型中的推理过程，我们提出了理论分析框架，推导了输入扰动对输出波动影响的上界，并在MATH、MMLU-Pro和GPQA数据集上通过Exact Match和Output Fluctuation指标验证了其有效性。我们证明了CoT推理步数增加可降低输出波动但无法完全消除扰动影响，且在Linear Self-Attention模型中，输入鲁棒性与嵌入向量和隐藏状态向量的范数呈负相关。",
    "summary_translation": "现有研究表明，思维链（Chain-of-Thought, CoT，一种推理方法）的输出受到输入扰动（input perturbations）的显著影响。尽管许多方法试图通过优化提示（prompts）来减轻这种影响，但这些扰动如何影响CoT输出的理论解释仍是一个开放的研究领域。这一空白限制了我们对于输入扰动在推理过程中如何传播的深入理解，并阻碍了提示优化方法的进一步改进。因此，在本文中，我们从理论上分析了输入扰动对CoT输出波动的影响。我们首先在输出波动处于可接受范围内的条件下，推导出输入扰动的上界，并基于此证明：(i) 该上界与CoT中的推理步骤数量呈正相关；(ii) 即使无限长的推理过程也无法消除输入扰动的影响。随后，我们将这些结论应用于线性自注意力（Linear Self-Attention, LSA）模型，该模型可视为Transformer（一种主流神经网络架构）的简化版本。对于LSA模型，我们证明了输入扰动的上界与输入嵌入（input embedding）和隐藏状态（hidden state）向量的范数（norms）呈负相关。为验证这一理论分析，我们在三个主流数据集和四个主流模型上进行了实验。实验结果与我们的理论分析一致，从实证上证明了我们发现的正确性。",
    "summary_generated_time": "2025-09-26 21:16:25",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#10",
    "title": "Query-Centric Graph Retrieval Augmented Generation",
    "link": "/arxiv/2509.21237",
    "arxiv_id": "2509.21237",
    "authors": "Yaxiong Wu, Jianyuan Bo, Yongyue Zhang, Sheng Liang, Yong Liu",
    "summary": "Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs) with external knowledge for long-context understanding and multi-hop reasoning, but existing methods face a granularity dilemma: fine-grained entity-level graphs incur high token costs and lose context, while coarse document-level graphs fail to capture nuanced relations. We introduce QCG-RAG, a query-centric graph RAG framework that enables query-granular indexing and multi-hop chunk retrieval. Our query-centric approach leverages Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with controllable granularity, improving graph quality and interpretability. A tailored multi-hop retrieval mechanism then selects relevant chunks via the generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG consistently outperforms prior chunk-based and graph-based RAG methods in question answering accuracy, establishing a new paradigm for multi-hop reasoning.",
    "subjects": "Computation and Language, Information Retrieval",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.159650",
    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出QCG-RAG，一种以查询为中心的图检索增强生成框架，旨在改进LLM的多跳推理能力。论文明确指出其目标是解决现有图RAG方法中的粒度困境，通过查询粒度索引和多跳块检索机制来增强LLM的推理能力。这属于改进LLM基础能力和增强其多步推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs)\" - 能力方向：专注于\"multi-hop reasoning\"（多跳推理），这是通用推理能力的重要组成部分 - 新兴范式：RAG本身可视为一种工具使用形式，论文提出的新框架增强了LLM利用外部知识的能力 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不是针对医疗、化学、生物等特定应用领域 - 不主要关注水印、安全性等模型可靠性问题 第四步：特殊和模糊情况处理 论文提出的RAG框架可以视为一种通用的工具使用方法，目的是增强LLM的通用问题解决能力（特别是多跳推理），而非应用于特定领域。虽然论文提到了\"improving graph quality and interpretability\"，但这是作为提高推理质量的手段，而非主要焦点。 综上所述，这篇论文的核心贡献是提出了一种新的通用框架来增强LLM的多跳推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决基于图的检索增强生成(RAG)方法面临的粒度困境。针对多跳推理和长上下文理解场景，我们提出了一种以查询为中心的图检索增强生成框架(QCG-RAG)，并在LiHuaWorld和MultiHop-RAG数据集上通过问答准确性指标验证了其有效性。",
    "summary_translation": "基于图的检索增强生成（Graph-based retrieval-augmented generation, RAG）通过外部知识丰富大型语言模型（Large Language Models, LLMs），以实现长上下文理解（long-context understanding）和多跳推理（multi-hop reasoning），但现有方法面临粒度困境（granularity dilemma）：细粒度实体级图（fine-grained entity-level graphs）导致高token成本（token costs）并丢失上下文，而粗粒度文档级图（coarse document-level graphs）则无法捕捉细微关系。我们提出了QCG-RAG，一种以查询为中心的图RAG框架（query-centric graph RAG framework），实现了查询粒度索引（query-granular indexing）和多跳块检索（multi-hop chunk retrieval）。我们的以查询为中心的方法利用Doc2Query和Doc2Query{-}{-}构建具有可控粒度（controllable granularity）的以查询为中心的图，提高了图质量（graph quality）和可解释性（interpretability）。随后，一个定制的多跳检索机制（tailored multi-hop retrieval mechanism）通过生成的查询选择相关块（relevant chunks）。在LiHuaWorld和MultiHop-RAG上的实验表明，QCG-RAG在问答准确性（question answering accuracy）方面始终优于先前的基于块（chunk-based）和基于图（graph-based）的RAG方法，为多跳推理建立了新范式（new paradigm）。",
    "summary_generated_time": "2025-09-26 21:16:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#11",
    "title": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents",
    "link": "/arxiv/2509.21212",
    "arxiv_id": "2509.21212",
    "authors": "Yaxiong Wu, Yongyue Zhang, Sheng Liang, Yong Liu",
    "summary": "Long-term conversational agents require effective memory management to handle dialogue histories that exceed the context window of large language models (LLMs). Existing methods based on fact extraction or summarization reduce redundancy but struggle to organize and retrieve relevant information across different granularities of dialogue and generated memory. We introduce SGMem (Sentence Graph Memory), which represents dialogue as sentence-level graphs within chunked units, capturing associations across turn-, round-, and session-level contexts. By combining retrieved raw dialogue with generated memory such as summaries, facts and insights, SGMem supplies LLMs with coherent and relevant context for response generation. Experiments on LongMemEval and LoCoMo show that SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.",
    "subjects": "Computation and Language, Information Retrieval",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.159851",
    "filter_reason": "这篇论文的核心贡献是提出SGMem（句子图记忆）方法，用于增强大语言模型在长期对话中的记忆管理能力。从筛选标准来看，首先，论文的本质是改进LLM的基础能力，特别是在处理超出上下文窗口的长期对话时的信息组织和检索能力，这属于增强LLM通用推理能力的范畴。有效的记忆管理是进行连贯推理和多步对话的基础，因此这项工作直接提升了LLM的通用能力。其次，论文包含了正面指标中的\"Large language models, LLMs\"和\"llm-based agents\"概念。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。最后，论文提出的SGMem是一种通用的记忆管理框架，用于增强对话代理的通用能力，而非应用于特定领域。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解决长期对话代理中的记忆管理问题，特别是当对话历史超过大型语言模型上下文窗口时的记忆碎片化问题。针对长期对话历史和生成的记忆（如摘要、事实和见解），我们提出了一种SGMem（句子图记忆）方法，将对话表示为分块单元内的句子级别图，捕获跨不同粒度的关联。在LongMemEval和LoCoMo数据集上通过准确率指标验证了其有效性，实验结果表明SGMem在长期对话问答任务中持续提高准确性并优于强基线方法。",
    "summary_translation": "长期对话代理（long-term conversational agents）需要有效的记忆管理（memory management）来处理超出大型语言模型（large language models, LLMs）上下文窗口（context window）的对话历史。现有的基于事实提取（fact extraction）或摘要生成（summarization）的方法虽然减少了冗余，但在组织和检索不同粒度（granularities）的对话和生成记忆中的相关信息方面存在困难。我们提出了SGMem（Sentence Graph Memory，句子图记忆），它将对话表示为分块单元（chunked units）内的句子级图（sentence-level graphs），捕捉轮次（turn-level）、回合（round-level）和会话级别（session-level）上下文之间的关联。通过将检索到的原始对话与生成的记忆（如摘要、事实和见解）相结合，SGMem为大型语言模型（LLMs）提供连贯且相关的上下文，用于生成响应。在LongMemEval和LoCoMo数据集上的实验表明，SGMem在长期对话问答（long-term conversational question answering）任务中持续提高准确性，并优于强大的基线模型（strong baselines）。",
    "summary_generated_time": "2025-09-26 21:16:11",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#2",
    "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards",
    "link": "/arxiv/2509.21319",
    "arxiv_id": "2509.21319",
    "authors": "Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev",
    "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
    "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.157988",
    "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 首先，从核心判断来看，该论文的本质是提出一种新的强化学习范式RLBFF（Reinforcement Learning with Binary Flexible Feedback），用于改进LLM的后训练阶段。这属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，而非将LLM作为工具应用到特定领域。论文通过结合人类反馈(RLHF)和可验证奖励(RLVR)的优势，提出了一种新的训练方法来增强LLM的响应质量和对齐能力。 其次，论文包含多个正面指标： 1. 核心概念：明确关注LLMs，并使用Qwen3-32B作为示例模型 2. 训练方法：核心就是提出一种新的强化学习方法(RLBFF)，这是RLHF和RLVR的结合 3. 能力方向：虽然未直接强调推理，但提高LLM的响应质量隐含了推理能力的提升，且论文在MT-Bench等包含推理任务的基准上进行了评估 第三，论文不涉及任何排除标准中的领域： 1. 未涉及多模态与视觉内容 2. 未针对特定应用领域（如医疗、化学等） 3. 虽然涉及模型对齐，但核心是提出新方法提高整体响应质量，而非专注于水印、安全等应用层面 最后，在特殊和模糊情况处理上，论文明确解决了RLHF的可解释性问题，属于\"提出新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量\"的情况，符合保留条件。 综上所述，该论文的核心贡献是提出了一种新的强化学习训练方法来提升LLM的通用能力和对齐质量，这与研究目标\"提高大语言模型的通用推理能力\"高度一致，因此应该被保留。",
    "summary2": "本文旨在解决RLHF缺乏可解释性和RLVR范围有限的问题。针对LLM后训练中的奖励建模，我们提出了一种二元灵活反馈方法（RLBFF），结合人类偏好与规则验证，并在RM-Bench、JudgeBench和PrincipleBench上通过准确率等指标验证了其有效性。",
    "summary_translation": "基于人类反馈的强化学习（Reinforcement Learning with Human Feedback, RLHF）和基于可验证奖励的强化学习（Reinforcement Learning with Verifiable Rewards, RLVR）是大型语言模型（Large Language Model, LLM）后训练中使用的主要强化学习范式，各自具有独特的优势。然而，RLHF在可解释性和奖励破解（reward hacking）方面存在困难，因为它依赖于通常缺乏明确标准的人类判断，而RLVR因其专注于基于正确性的验证器而受到范围限制。我们提出了基于二元灵活反馈的强化学习（Reinforcement Learning with Binary Flexible Feedback, RLBFF），它结合了人类驱动偏好的多样性和基于规则验证的精确性，使奖励模型能够捕捉超越单纯正确性的响应质量的细微方面。RLBFF从自然语言反馈中提取可以以二元方式回答的原则（例如，信息准确性：是，或代码可读性：否）。这些原则随后可用于将奖励模型训练定位为一个蕴涵任务（entailment task）（响应满足或不满足任意原则）。我们表明，以这种方式训练的奖励模型在数据匹配的情况下可以优于Bradley-Terry模型，并在RM-Bench（86.2%）和JudgeBench（81.4%，截至2025年9月24日排行榜第一）上取得顶尖性能。此外，与Bradley-Terry模型不同，用户可以在推理时指定感兴趣的原则，以自定义我们奖励模型的关注点。最后，我们提出了一个完全开源的方案（包括数据），使用RLBFF和我们的奖励模型对Qwen3-32B进行对齐，以匹配或超过o3-mini和DeepSeek R1在MT-Bench、WildBench和Arena Hard v2等通用对齐基准上的性能（推理成本低于5%）。",
    "summary_generated_time": "2025-09-26 21:16:15",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#34",
    "title": "WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs",
    "link": "/arxiv/2509.20863",
    "arxiv_id": "2509.20863",
    "authors": "Guowei Xu, Wenxin Xu, Jiawang Zhao, Kaisheng Ma",
    "summary": "Diffusion models have recently shown strong potential in language modeling, offering faster generation compared to traditional autoregressive approaches. However, applying supervised fine-tuning (SFT) to diffusion models remains challenging, as they lack precise probability estimates at each denoising step. While the diffusion mechanism enables the model to reason over entire sequences, it also makes the generation process less predictable and often inconsistent. This highlights the importance of controlling key tokens that guide the direction of generation. To address this issue, we propose WeFT, a weighted SFT method for diffusion language models, where tokens are assigned different weights based on their entropy. Derived from diffusion theory, WeFT delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and MATH-500). The code and models will be made publicly available.",
    "subjects": "Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.169830",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出WeFT（加权熵驱动微调方法），这是一种针对扩散语言模型(dLLMs)的新训练范式。论文的核心贡献在于解决扩散语言模型在监督微调过程中的挑战，通过基于熵的标记加权来增强模型的推理能力。这明确属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴，因此应保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究扩散语言模型(dLLMs)，这是大语言模型的一种变体 - 能力方向：论文在四个推理基准(Sudoku、Countdown、GSM8K和MATH-500)上评估方法，这些基准涉及数学推理和逻辑推理能力 - 论文虽然不涉及强化学习或智能体等新兴范式，但已包含足够的核心正面指标 第三步：排除标准 论文不符合任何排除标准： - 虽然提到扩散模型，但这是应用于语言建模而非视觉或多模态领域 - 没有专注于任何特定应用领域（如医疗、化学等） - 没有主要关注水印、安全等模型可靠性问题 第四步：特殊和模糊情况 论文不涉及需要特殊判断的智能体/工具使用或幻觉/可解释性/安全等模糊情况。 综合分析，这篇论文通过提出新的微调方法来增强扩散语言模型的通用推理能力，并在多个推理基准上验证了其有效性，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
    "summary2": "本文旨在解决扩散语言模型(dLLMs)在监督微调(SFT)过程中缺乏精确概率估计的问题。针对token重要性不均匀的场景，我们提出了一种基于熵加权的WeFT方法，并在Sudoku、Countdown、GSM8K和MATH-500四个推理基准上通过准确率指标验证了其有效性。该方法通过token预测熵分配不同掩码率，使高熵token获得更强训练信号，相比标准SFT实现了最高83%的相对性能提升。",
    "summary_translation": "扩散模型（Diffusion models）最近在语言建模（language modeling）方面展现出强大的潜力，与传统自回归方法（autoregressive approaches）相比，提供了更快的生成速度。然而，将监督微调（supervised fine-tuning, SFT）应用于扩散模型仍然具有挑战性，因为它们在每个去噪步骤（denoising step）中缺乏精确的概率估计。虽然扩散机制（diffusion mechanism）使模型能够对整个序列进行推理（reason），但它也使生成过程变得较难预测且常常不一致。这凸显了控制引导生成方向的关键标记（key tokens）的重要性。为解决这一问题，我们提出了WeFT，一种针对扩散语言模型（diffusion language models）的加权SFT方法，其中标记根据其熵（entropy）被分配不同的权重。源自扩散理论（diffusion theory）的WeFT带来了显著收益：在open-r1数据集的s1K、s1K-1.1和3k样本上训练时，它在四个广泛使用的推理基准（reasoning benchmarks）（Sudoku、Countdown、GSM8K和MATH-500）上相比标准SFT分别实现了39%、64%和83%的相对改进。代码和模型将公开发布。",
    "summary_generated_time": "2025-09-26 21:16:08",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#37",
    "title": "Distilling Many-Shot In-Context Learning into a Cheat Sheet",
    "link": "/arxiv/2509.20820",
    "arxiv_id": "2509.20820",
    "authors": "Ukyo Honda, Soichiro Murakami, Peinan Zhang",
    "summary": "Recent advances in large language models (LLMs) enable effective in-context learning (ICL) with many-shot examples, but at the cost of high computational demand due to longer input tokens. To address this, we propose cheat-sheet ICL, which distills the information from many-shot ICL into a concise textual summary (cheat sheet) used as the context at inference time. Experiments on challenging reasoning tasks show that cheat-sheet ICL achieves comparable or better performance than many-shot ICL with far fewer tokens, and matches retrieval-based ICL without requiring test-time retrieval. These findings demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs in downstream tasks.",
    "subjects": "Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.170381",
    "filter_reason": "这篇论文的核心贡献是提出了一种称为\"cheat-sheet ICL\"的新方法，旨在提高大语言模型在上下文学习中的推理效率和性能。论文将多样本上下文学习的信息提炼成简洁的文本摘要，在保持或提高推理性能的同时大幅减少了计算需求。这直接关系到提升LLM的通用推理能力，特别是在处理需要多步推理的任务时。论文不是将LLM作为工具应用到特定领域，而是关注如何改进LLM本身的推理机制，因此完全符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。此外，论文明确涉及\"Large language models\"和\"reasoning\"等正面指标，且不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文提出的是一种通用方法论，可以应用于各种推理任务，而非针对特定领域的应用。",
    "summary2": "本文旨在解决many-shot in-context learning计算成本高的问题。针对大型语言模型处理长上下文的场景，我们提出了一种cheat-sheet ICL方法，将many-shot demonstrations的知识提炼成简洁文本摘要，并在BBH Hard推理任务上通过accuracy验证了其有效性。",
    "summary_translation": "大型语言模型(large language models, LLMs)的最新进展使得通过多示例(many-shot examples)进行有效的上下文学习(in-context learning, ICL)成为可能，但由于输入标记(input tokens)较长，这需要高昂的计算成本(computational demand)。为解决这一问题，我们提出了备忘单ICL(cheat-sheet ICL)，该方法将多示例ICL的信息提炼成一个简洁的文本摘要(concise textual summary)（即备忘单(cheat sheet)），在推理时间(inference time)作为上下文使用。在具有挑战性的推理任务(challenging reasoning tasks)上的实验表明，备忘单ICL使用远少的标记(far fewer tokens)即可达到与多示例ICL相当甚至更优的性能(comparable or better performance)，并且无需测试时检索(test-time retrieval)就能匹配基于检索的ICL(retrieval-based ICL)。这些研究结果表明，备忘单ICL是在下游任务(downstream tasks)中利用LLMs的一种实用替代方案(practical alternative)。",
    "summary_generated_time": "2025-09-26 21:16:39",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#42",
    "title": "SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs",
    "link": "/arxiv/2509.20758",
    "arxiv_id": "2509.20758",
    "authors": "Jiacheng Lin, Zhongruo Wang, Kun Qian, Tian Wang, Arvind Srinivasan, Hansi Zeng, Ruochen Jiao, Xie Zhou, Jiri Gesi, Dakuo Wang, Yufan Guo, Kai Zhong, Weiqi Zhang, Sujay Sanghavi, Changyou Chen, Hyokun Yun, Lihong Li",
    "summary": "Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach to adapt Large Language Models (LLMs) to specialized tasks but is often believed to degrade their general capabilities. In this work, we revisit this trade-off and present both empirical and theoretical insights. First, we show that SFT does not always hurt: using a smaller learning rate can substantially mitigate general performance degradation while preserving comparable target-domain performance. We then provide a theoretical analysis that explains these phenomena and further motivates a new method, Token-Adaptive Loss Reweighting (TALR). Building on this, and recognizing that smaller learning rates alone do not fully eliminate general-performance degradation in all cases, we evaluate a range of strategies for reducing general capability loss, including L2 regularization, LoRA, model averaging, FLOW, and our proposed TALR. Experimental results demonstrate that while no method completely eliminates the trade-off, TALR consistently outperforms these baselines in balancing domain-specific gains and general capabilities. Finally, we distill our findings into practical guidelines for adapting LLMs to new domains: (i) using a small learning rate to achieve a favorable trade-off, and (ii) when a stronger balance is further desired, adopt TALR as an effective strategy.",
    "subjects": "Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.171412",
    "filter_reason": "这篇论文的核心贡献是研究如何在大语言模型进行特定领域微调时保持其通用能力，提出了一种新的训练方法Token-Adaptive Loss Reweighting (TALR)。从本质上看，论文关注的是改进LLM的基础能力，提出新的训练范式来增强模型的通用推理和问题解决能力，而不是将LLM作为工具应用到特定领域。论文通过理论和实验分析，探索了如何减轻监督微调对模型通用能力的损害，这直接关系到提升LLM的通用推理能力。虽然论文讨论了领域特定的微调，但其焦点不是特定应用领域，而是如何在领域微调时保持通用能力。论文符合\"改进LLM的基础能力、提出新的训练范式\"的标准，不涉及多模态、特定应用领域或模型可靠性等排除标准。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解决大语言模型领域特定微调(SFT)导致一般能力下降的问题。针对医学计算和电商分类等特定领域数据集，我们提出了一种Token-Adaptive Loss Reweighting (TALR)方法，通过动态降低困难token的权重来减轻性能退化。在MedCalc和ESCI数据集上的实验表明，使用较小学习率并结合TALR能在保持领域性能的同时，显著减轻一般能力下降，优于L2正则化、LoRA等基线方法。",
    "summary_translation": "在特定领域数据集上进行监督微调（Supervised Fine-Tuning, SFT）是使大型语言模型（Large Language Models, LLMs）适应专门任务的常见方法，但通常被认为会降低其通用能力。在本研究中，我们重新审视了这种权衡，并提供了实证和理论两方面的见解。首先，我们表明SFT并不总是有害的：使用较小的学习率可以显著减轻通用性能的下降，同时保持相当的目标领域性能。接着，我们提供了理论分析来解释这些现象，并进一步提出了一种新方法——令牌自适应损失重加权（Token-Adaptive Loss Reweighting, TALR）。在此基础上，并认识到仅靠较小的学习率并不能在所有情况下完全消除通用性能的下降，我们评估了一系列减少通用能力损失的策略，包括L2正则化（L2 regularization）、LoRA、模型平均（model averaging）、FLOW以及我们提出的TALR。实验结果表明，虽然没有任何方法能完全消除这种权衡，但TALR在平衡特定领域收益和通用能力方面始终优于这些基线方法。最后，我们将我们的发现提炼为将LLMs适应新领域的实用指南：(i) 使用较小的学习率以实现有利的权衡；(ii) 当需要更强的平衡时，采用TALR作为一种有效策略。",
    "summary_generated_time": "2025-09-26 21:16:28",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#43",
    "title": "Confidence-guided Refinement Reasoning for Zero-shot Question Answering",
    "link": "/arxiv/2509.20750",
    "arxiv_id": "2509.20750",
    "authors": "Youwon Jang, Woo Suk Choi, Minjoon Jung, Minsu Lee, Byoung-Tak Zhang",
    "summary": "We propose Confidence-guided Refinement Reasoning (C2R), a novel training-free framework applicable to question-answering (QA) tasks across text, image, and video domains. C2R strategically constructs and refines sub-questions and their answers (sub-QAs), deriving a better confidence score for the target answer. C2R first curates a subset of sub-QAs to explore diverse reasoning paths, then compares the confidence scores of the resulting answer candidates to select the most reliable final answer. Since C2R relies solely on confidence scores derived from the model itself, it can be seamlessly integrated with various existing QA models, demonstrating consistent performance improvements across diverse models and benchmarks. Furthermore, we provide essential yet underexplored insights into how leveraging sub-QAs affects model behavior, specifically analyzing the impact of both the quantity and quality of sub-QAs on achieving robust and reliable reasoning.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.171615",
    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Confidence-guided Refinement Reasoning (C2R)\"的新型推理框架，专注于改进大语言模型的通用推理能力。从筛选标准来看： 首先，在核心判断层面，论文本质上是关于改进LLM的推理能力的，提出了一种新的训练免费推理框架，通过构建和细化子问题及其答案来增强模型的推理过程。这直接符合\"改进LLM基础能力\"和\"增强其逻辑推理能力\"的保留标准。 其次，从正面指标看，论文明确聚焦于\"reasoning\"这一核心能力方向，讨论了\"探索多样化推理路径\"和\"实现稳健可靠的推理\"，这些都是通用推理能力的关键组成部分。 关于多模态方面，虽然论文提到其框架适用于文本、图像和视频领域，但这只是表明其方法的通用性，而非主要焦点。论文的核心是推理框架本身，而不是解决多模态或视觉特定问题，因此不应被排除。 最后，C2R作为一种可以与各种现有QA模型无缝集成的通用方法，代表了提升LLM推理能力的新方法论，完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
    "summary2": "本文旨在解决零样本问答中子问题-答案对可能引入噪声的问题。针对多模态QA任务，我们提出了一种基于置信度引导的细化推理框架(C2R)，并在五个模型和五个基准测试上验证了其有效性。",
    "summary_translation": "我们提出了一种置信度引导的精炼推理（Confidence-guided Refinement Reasoning, C2R）框架，这是一种新颖的无需训练的方法，适用于文本、图像和视频领域的问答（question-answering, QA）任务。C2R通过策略性地构建和细化子问题及其答案（sub-QAs），为目标答案推导出更优的置信度分数（confidence score）。C2R首先精选一个子问题子集以探索多样化的推理路径（reasoning paths），然后比较所得答案候选（answer candidates）的置信度分数，以选择最可靠的最终答案。由于C2R仅依赖于模型自身生成的置信度分数，它可以与各种现有的QA模型无缝集成，在不同模型和基准测试（benchmarks）上均展现出一致的性能提升。此外，我们提供了关于利用子问题如何影响模型行为的重要但尚未充分探索的见解，特别分析了子问题的数量和质量对实现稳健可靠推理的影响。",
    "summary_generated_time": "2025-09-26 21:16:18",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#53",
    "title": "Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures",
    "link": "/arxiv/2509.20577",
    "arxiv_id": "2509.20577",
    "authors": "Sampurna Roy, Ayan Sar, Anurag Kaushish, Kanav Gupta, Tanupriya Choudhury, Abhijit Kumar",
    "summary": "Contemporary transformer architectures apply identical processing depth to all inputs, creating inefficiencies and limiting reasoning quality. Simple factual queries are subjected to the same multilayered computation as complex logical problems, wasting resources while constraining deep inference. To overcome this, we came up with a concept of Dynamic Reasoning Chains through Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends the Mixture of Experts paradigm from width-based to depth specialised computation. DS-MoE introduces expert modules optimised for distinct reasoning depths, shallow pattern recognition, compositional reasoning, logical inference, memory integration, and meta-cognitive supervision. A learned routing network dynamically assembles custom reasoning chains, activating only the necessary experts to match input complexity. The dataset on which we trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse domains such as scientific papers, legal texts, programming code, and web content, enabling systematic assessment across reasoning depths. Experimental results demonstrate that DS-MoE achieves up to 16 per cent computational savings and 35 per cent faster inference compared to uniform-depth transformers, while delivering 2.8 per cent higher accuracy on complex multi-step reasoning benchmarks. Furthermore, routing decisions yield interpretable reasoning chains, enhancing transparency and scalability. These findings establish DS-MoE as a significant advancement in adaptive neural architectures, demonstrating that depth-specialised modular processing can simultaneously improve efficiency, reasoning quality, and interpretability in large-scale language models.",
    "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.179137",
    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。根据筛选标准，我的判断过程如下： 第一步：核心判断 论文的核心是提出DS-MoE（Depth Specialised Mixture of Experts）框架，这是一种改进Transformer架构的新方法，通过深度专业化的专家混合模型来动态构建推理链。论文明确关注增强LLM的逻辑推理和多步推理能力，而不是将LLM作为工具应用于特定领域。这完全符合改进LLM基础能力和增强其通用推理能力的研究目标。 第二步：正面指标 论文包含多个正面指标： - 核心概念：虽然摘要未直接提及\"LLMs\"，但DS-MoE明显是针对大语言模型的基础架构改进 - 能力方向：明确涉及\"reasoning chains\"、\"reasoning depths\"、\"logical inference\"和\"complex multi-step reasoning benchmarks\"，直接针对推理能力，特别是逻辑推理和多步推理 - 论文强调通过动态组装定制的推理链来匹配输入复杂性，这正是提升通用推理能力的核心 第三步：排除标准 论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉相关内容 - 虽然训练数据集包含多个领域，但论文本身不是针对特定应用领域的研究 - 未涉及模型可靠性方面的水印、安全性等内容 第四步：特殊和模糊情况 论文提到\"routing decisions yield interpretable reasoning chains, enhancing transparency\"，这表明其方法增强了模型的可解释性，从而可能提升模型的通用可靠性和推理质量，符合研究目标。 综上所述，这篇论文的核心贡献是通过改进Transformer架构来增强LLM的通用推理能力，特别是逻辑推理和多步推理能力，同时提高计算效率和可解释性，完全符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解决传统Transformer架构对所有输入应用相同处理深度导致的效率低下和推理质量受限问题。针对不同复杂度的输入任务，我们提出了一种Depth-Specialised Mixture-of-Experts (DS-MoE)方法，通过动态组装深度专业化专家模块构建推理链，并在The Pile数据集上通过计算效率、推理速度和准确率等指标验证了其有效性。",
    "summary_translation": "# 中文翻译\n\n当代transformer架构对所有输入应用相同的处理深度，造成效率低下并限制了推理质量。简单的事实查询与复杂的逻辑问题受到同样的多层计算处理，浪费资源的同时也限制了深度推理能力。为克服这一问题，我们提出了通过深度专业化的专家混合(Depth Specialised Mixture of Experts, DS-MoE)实现动态推理链的概念，这是一个模块化框架，将专家混合(Mixture of Experts)范式从基于宽度的计算扩展到深度专业化的计算。DS-MoE引入了针对不同推理深度优化的专家模块，包括浅层模式识别(shallow pattern recognition)、组合推理(compositional reasoning)、逻辑推理(logical inference)、记忆整合(memory integration)和元认知监督(meta-cognitive supervision)。一个学习的路由网络(learned routing network)动态组装自定义推理链，仅激活必要的专家以匹配输入复杂度。\n\n我们训练和评估DS-MoE所使用的数据集是The Pile，一个800GB的语料库，涵盖科学论文、法律文本、编程代码和网络内容等多个领域，能够对各种推理深度进行系统评估。实验结果表明，与统一深度的transformer相比，DS-MoE实现了高达16%的计算节省和35%更快的推理速度，同时在复杂多步推理基准测试中提供了2.8%更高的准确率。此外，路由决策产生可解释的推理链(interpretable reasoning chains)，增强了透明度和可扩展性。这些发现确立了DS-MoE作为自适应神经架构(adaptive neural architectures)的重要进展，证明了深度专业化的模块化处理可以同时提高大规模语言模型的效率、推理质量和可解释性。",
    "summary_generated_time": "2025-09-26 21:16:24",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#56",
    "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning",
    "link": "/arxiv/2509.20502",
    "arxiv_id": "2509.20502",
    "authors": "Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang",
    "summary": "Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\\%. Code is available at https://github.com/xwang97/MARS.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.179790",
    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是提出MARS（Multi-Agent Review System），一种基于角色的多智能体协作框架，旨在提高LLM的推理能力。论文的核心贡献是改进LLM的基础推理能力，提出了一种新的多智能体协作范式，而不是将LLM作为工具应用于特定领域。因此，这篇论文符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Large language models (LLMs)\" - 能力方向：直接关注\"LLM reasoning\"，旨在提高模型的推理质量 - 新兴范式：提出了多智能体系统(MARS)，这是一种基于LLM的智能体协作框架 论文符合3个正面指标，表明其与研究主题高度相关。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域（如医疗、化学等） - 不关注模型可靠性层面的水印、安全等问题 第四步：特殊和模糊情况 论文提出的MARS框架是一种通用的智能体协作方法，用于增强LLM的通用推理能力，而不是将智能体应用于特定领域。根据筛选标准，这种通用的智能体协作框架应该保留。 综合分析，这篇论文的核心贡献是提出了一种更高效的多智能体协作框架(MARS)来增强LLM的通用推理能力，与研究目标\"提高大语言模型的通用推理能力\"直接一致。论文通过改进多智能体协作方式，在保持推理质量的同时显著提高了效率，这正属于对LLM基础推理能力的改进研究。",
    "summary2": "本文旨在 [解决多代理协作中推理效率低下的问题]。针对 [大型语言模型推理任务]，我们提出了一种 [基于角色的多代理评审系统(MARS)]，并在 [多个推理基准测试(MMLU、GPQA、GSM8K)] 上通过 [准确率、token消耗和推理时间] 验证了其有效性。",
    "summary_translation": "大型语言模型（Large Language Models, LLMs）在自然语言理解方面取得了令人印象深刻的结果，然而当它们作为单一代理（single agents）运行时，其推理能力仍然有限。多代理辩论（Multi-Agent Debate, MAD）被提出来解决这一限制，通过使多个模型以圆桌辩论（round-table debate）的方式进行协作推理。虽然有效，但多代理辩论（MAD）由于涉及的代理数量和所需的频繁通信而引入了大量的计算开销。在本文中，我们提出了MARS（Multi-Agent Review System，多代理评审系统），一个受评审过程（review process）启发的基于角色的协作框架。在MARS中，作者代理（author agent）生成初始解决方案，评审代理（reviewer agents）独立提供决策和评论，元评审代理（meta-reviewer）整合反馈以做出最终决策并指导进一步修订。这种设计提高了推理质量，同时避免了昂贵的评审者之间的交互，从而控制了令牌消耗量（token consumption）和推理时间（inference time）。我们将MARS与多代理辩论（MAD）和其他最先进的推理策略（state-of-the-art reasoning strategies）在多个基准（benchmarks）上进行了比较。使用不同大型语言模型（LLMs）的大量实验表明，MARS匹配了多代理辩论（MAD）的准确性，同时将令牌使用量（token usage）和推理时间（inference time）减少了约50%。代码可在https://github.com/xwang97/MARS获取。",
    "summary_generated_time": "2025-09-26 21:16:36",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#61",
    "title": "SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation",
    "link": "/arxiv/2509.20377",
    "arxiv_id": "2509.20377",
    "authors": "Tomoaki Isoda",
    "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive tasks in recent years. However, since retrieval systems may return irrelevant content, incorporating such information into the model often leads to hallucinations. Thus, identifying and filtering out unhelpful retrieved content is a key challenge for improving RAG performance.To better integrate the internal knowledge of the model with external knowledge from retrieval, it is essential to understand what the model \"knows\" and \"does not know\" (which is also called \"self-knowledge\"). Based on this insight, we propose SKILL-RAG (Self-Knowledge Induced Learning and Filtering for RAG), a novel method that leverages the model's self-knowledge to determine which retrieved documents are beneficial for answering a given query. We design a reinforcement learning-based training framework to explicitly elicit self-knowledge from the model and employs sentence-level granularity to filter out irrelevant content while preserving useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several question answering benchmarks. Experimental results demonstrate that SKILL-RAG not only improves generation quality but also significantly reduces the number of input documents, validating the importance of self-knowledge in guiding the selection of high-quality retrievals.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-20",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.181043",
    "filter_reason": "这篇论文符合我的研究目标，因为它核心是关于改进大语言模型本身的通用推理能力。具体分析如下： 从第一步核心判断来看，论文的本质是提出SKILL-RAG方法，这是一种新的训练范式，通过强化学习框架来增强模型的自我知识认知能力。该方法不是将LLM作为工具应用于特定领域，而是致力于提升LLM在知识处理和推理方面的基础能力，特别是通过识别模型\"知道\"和\"不知道\"的内容来优化其推理过程，这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的标准。 从第二步正面指标看，论文包含多个相关主题： - 核心概念：明确研究LLMs（Llama2-7B和Qwen3-8B） - 能力方向：涉及推理能力，特别是在知识密集型任务中的问题解决 - 训练方法：使用强化学习（reinforcement learning-based training framework）来训练模型 - 新兴范式：RAG本身可视为一种工具使用范式，论文改进了这一通用框架 从第三步排除标准看，论文不涉及任何应排除的领域，没有专注于多模态、特定应用领域或模型基础设施等。 从第四步特殊和模糊情况看，虽然论文涉及减少幻觉的问题，但它是通过提出新方法（利用模型自我知识）来提升模型的内在推理质量，而不是应用层面的讨论，因此应该保留。 综上所述，SKILL-RAG论文的核心贡献是提出了一种增强LLM自我知识认知和推理能力的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决RAG系统中检索内容不相关导致模型幻觉的问题。针对检索增强生成场景，我们提出了一种SKILL-RAG方法，利用大语言模型的自我知识进行检索内容过滤，并在TriviaQA、SelfAware、NQ和TruthfulQA等多个问答基准测试集上通过准确率等指标验证了其有效性。",
    "summary_translation": "近年来，检索增强生成（Retrieval-Augmented Generation, RAG）显著提升了大型语言模型（large language models, LLMs）在知识密集型任务（knowledge-intensive tasks）上的表现。然而，由于检索系统可能返回不相关内容，将这些信息整合到模型中常常导致幻觉（hallucinations）问题。因此，识别并过滤无用的检索内容是提升RAG性能的关键挑战。为了更好地将模型的内部知识与检索获得的外部知识相结合，理解模型的\"已知\"与\"未知\"（也称为\"自我知识\"（self-knowledge））至关重要。基于这一见解，我们提出了SKILL-RAG（Self-Knowledge Induced Learning and Filtering for RAG，自我知识引导的学习与过滤RAG方法），这是一种利用模型自我知识来确定哪些检索文档有益于回答特定查询的新方法。我们设计了一个基于强化学习（reinforcement learning）的训练框架，以明确地激发模型的自我知识，并采用句子级别粒度（sentence-level granularity）来过滤不相关内容，同时保留有用知识。我们在多个问答基准测试（question answering benchmarks）上使用Llama2-7B和Qwen3-8B模型对SKILL-RAG进行了评估。实验结果表明，SKILL-RAG不仅提高了生成质量，还显著减少了输入文档数量，验证了自我知识在指导高质量检索选择中的重要性。",
    "summary_generated_time": "2025-09-26 21:16:32",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#73",
    "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns",
    "link": "/arxiv/2509.21124",
    "arxiv_id": "2509.21124",
    "authors": "Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai",
    "summary": "Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model's reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.",
    "subjects": "Artificial Intelligence, Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.189451",
    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础推理能力。论文提出了一种新的训练范式，通过学习多样化的思维链模式来扩展基础模型的推理潜力，这直接针对提升LLM的通用推理能力，而非将LLM作为工具应用于特定领域。 其次，论文包含了多个正面指标： 1. 核心概念：论文明确研究基础模型(Foundation Model)的推理能力 2. 能力方向：聚焦于推理能力(reasoning)，特别是数学推理(mathematical reasoning) 3. 训练方法：涉及强化学习(RL)作为关键方法，并讨论如何提高下游RL性能 4. 新兴范式：关注思维链(CoT)的优化，这是提高LLM推理能力的重要范式 第三，论文不符合任何排除标准： 1. 不涉及多模态与视觉内容 2. 虽然在数学推理上评估，但这是作为通用推理能力的测试案例，而非针对特定应用领域 3. 不涉及模型可靠性方面的水印、安全等问题 论文的核心贡献是首次定义了基础模型的\"推理潜力\"，并提出了一种通过选择高价值思维链数据来扩展这种潜力的方法。这种方法通过抽象原子推理模式，构建核心参考集，并使用双粒度算法选择高价值的CoT数据，从而训练模型更有效地掌握推理能力。这些贡献直接针对提升LLM的通用推理能力，完全符合研究目标。",
    "summary2": "本文旨在扩展基础模型的推理潜力。针对复杂数学推理任务，我们提出了一种CoTP框架，通过学习多样化的思维链模式，构建富含高价值推理模式的核心参考集，并利用推理模式链和token熵的双粒度算法选择高价值CoT数据。在85A6B MoE模型上，仅用10B-token的CoTP数据，在AIME 2024和2025上提高了9.58%，并将下游RL性能上限提高了7.81%。",
    "summary_translation": "在具有挑战性的数学推理领域，大型推理模型的最新进展是由强化学习(reinforcement learning, RL)驱动的。在中期训练过程中融入长思维链(chain-of-thought, CoT)数据也被证明可以显著提高推理深度。然而，当前的方法往往不加区分地利用CoT数据，这就留下了一个关键问题：哪些数据类型能最有效地增强模型的推理能力。在本文中，我们首次将基础模型的推理潜力定义为正确回答问题所需的独立尝试次数的倒数，这与最终模型性能密切相关。接着，我们提出利用富含高价值推理模式的多样化数据来扩展推理潜力。具体而言，我们从CoT序列中抽象出具有共性和归纳能力的原子推理模式，并用它们构建一个富含有价值推理模式的核心参考集。此外，我们提出一种涉及推理模式链和令牌熵(token entropy)的双粒度算法，从数据池中高效选择与核心集一致的高价值CoT数据(CoTP)，从而训练模型有效掌握推理。仅需100亿令牌的CoTP数据就能使85A6B专家混合模型(Mixture-of-Experts, MoE)在具有挑战性的AIME 2024和2025上提高9.58%，并将下游RL性能的上限提高7.81%。",
    "summary_generated_time": "2025-09-26 21:16:37",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#76",
    "title": "ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning",
    "link": "/arxiv/2509.21070",
    "arxiv_id": "2509.21070",
    "authors": "Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu",
    "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between \"Thinking\" and \"NoThinking\" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.",
    "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.190261",
    "filter_reason": "根据筛选标准，这篇论文符合研究范围。核心判断上，论文本质是提升大语言模型的数学推理能力，属于增强LLM通用推理能力的范畴。论文提出ScaleDiff流程，通过高效生成困难数学问题来训练模型，从而提升LLM的数学推理能力，这是对LLM基础能力的改进，而非将LLM作为工具应用于特定领域。 从正面指标看，论文明确涉及Large Reasoning Models (LRMs)这一核心概念，并专注于Advanced Mathematical Reasoning这一推理能力方向。论文通过生成困难问题数据集并微调模型，实质上提出了一种新的训练范式来增强LLM的推理能力。 在排除标准方面，虽然论文专注于数学推理，但数学推理通常被视为评估和提升LLM通用推理能力的重要方面，而非特定应用领域。论文也不涉及多模态、视觉内容或特定应用领域如医疗、化学等。 论文的核心贡献是通过增加困难问题的数量来提升LLM的数学推理能力，并观察到\"随着困难问题数量增加，模型在困难基准测试上的性能呈现明显的扩展现象\"，这直接与研究目标\"提高大语言模型（LLM）本身的『通用推理能力』\"相符。因此，这篇论文应被保留。",
    "summary2": "本文旨在 [解决大规模生成困难数学问题以提升大型推理模型数学推理能力的问题]。针对 [数学推理模型训练中困难问题获取成本高、现有自动合成方法扩展性有限的挑战]，我们提出了一种 [名为ScaleDiff的流程，包含困难问题识别、专用生成器训练和解决方案蒸馏过滤]，并在 [多个数学推理基准测试(AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500)] 上通过 [准确率等指标] 验证了其有效性。",
    "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在复杂问题解决方面展现出令人印象深刻的能力，通常受益于那些能激发复杂推理的困难数学问题的训练。近期的研究工作探索了通过提示专有模型或大规模开源模型来自动合成数学问题的方法，这些提示基于种子数据或固有的数学概念。然而，由于这些方法具有高计算/API成本、提示的复杂性以及生成问题难度水平的局限性，将其规模化仍然具有挑战性。\n\n为克服这些限制，我们提出了ScaleDiff，一个简单而有效的流程，旨在规模化困难问题的创建。我们使用自适应思维模型（adaptive thinking model），仅需一次前向传播就能高效地从现有数据集中识别困难问题，该模型能够感知问题难度并自动在\"Thinking\"和\"NoThinking\"模式之间切换。然后，我们在这个筛选出的困难数据上训练了一个专门的困难问题生成器（DiffGen-8B），它能够大规模生成新的困难问题，消除了对复杂的、针对每个实例的提示及其相关的高API成本的需求。\n\n在ScaleDiff-Math数据集上对Qwen2.5-Math-7B-Instruct进行微调，相比原始数据集带来了11.3%的显著性能提升，并在AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500上达到了65.9%的平均准确率，超越了像OpenThinker3这样的近期强大大型推理模型（LRMs）。值得注意的是，这一性能是使用成本效益高的Qwen3-8B模型作为教师模型实现的，这表明我们的流程能够有效传递高级推理能力，而无需依赖更大、更昂贵的教师模型。此外，我们观察到随着困难问题数量的增加，模型在困难基准测试上的性能呈现出明显的规模化现象。\n\n代码：https://github.com/QizhiPei/ScaleDiff。",
    "summary_generated_time": "2025-09-26 21:16:53",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#80",
    "title": "DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?",
    "link": "/arxiv/2509.21016",
    "arxiv_id": "2509.21016",
    "authors": "Yiyou Sun, Yuhan Cao, Pohao Huang, Haoyue Bai, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song",
    "summary": "It remains an open question whether LLMs can acquire or generalize genuinely new reasoning strategies, beyond the sharpened skills encoded in their parameters during pre-training or post-training. To attempt to answer this debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding, a controlled benchmark of synthetic coding problem families designed to probe two fundamental aspects: learnability -- can LLMs, through reinforcement learning (RL), solve problem families where pretrained models exhibit failure with large enough attempts (pass@K=0)? --and transferrability -- if learnability happens, can such skills transfer systematically to out-of-distribution (OOD) test sets? Unlike prior public coding datasets, DELTA isolates reasoning skills through templated problem generators and introduces fully OOD problem families that demand novel strategies rather than tool invocation or memorized patterns. Our experiments reveal a striking grokking phase transition: after an extended period with near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To enable learnability on previously unsolvable problem families, we explore key training ingredients such as staged warm-up with dense rewards, experience replay, curriculum training, and verification-in-the-loop. Beyond learnability, we use DELTA to evaluate transferability or generalization along exploratory, compositional, and transformative axes, as well as cross-family transfer. Results show solid gains within families and for recomposed skills, but persistent weaknesses in transformative cases. DELTA thus offers a clean testbed for probing the limits of RL-driven reasoning and for understanding how models can move beyond existing priors to acquire new algorithmic skills.",
    "subjects": "Machine Learning, Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.191214",
    "filter_reason": "这篇论文的核心贡献是提出了DELTA-Code基准测试，用于探索大语言模型(LLMs)是否能通过强化学习(RL)获取和迁移全新的推理策略。论文研究的是LLMs如何通过强化学习解决预训练时无法解决的问题，以及这些学到的技能如何迁移到分布外测试集。这完全符合研究\"大语言模型通用推理能力\"的目标，因为：(1)论文本质是改进LLM的基础推理能力，特别是算法推理能力，而不是将LLM作为工具应用到特定领域；(2)论文使用了强化学习这一训练范式来增强模型能力，探索了staged warm-up、experience replay、curriculum training等关键训练成分；(3)论文聚焦于推理能力的获取和迁移，这是通用推理能力的核心方面。论文符合多个正面指标，包括核心概念(LLMs)、能力方向(reasoning)和训练方法(RL)，同时不符合任何排除标准。因此，这篇论文应该被纳入研究范围。",
    "summary2": "本文旨在研究RL如何帮助LLMs获取和泛化全新的编程算法策略。针对合成编码问题家族，我们提出了DELTA基准测试，通过分阶段训练(密集奖励到二元奖励)实现了grokking现象，并在Manufactoria和BouncingSim等数据集上通过pass@K和full-pass rate验证了RL可以使模型获得基础模型无法执行的新策略。",
    "summary_translation": "LLMs（大型语言模型）是否能够获取或泛化真正新的推理策略，而不仅仅是在预训练或后训练期间编码在其参数中的已强化技能，这仍然是一个悬而未决的问题。为尝试回答这一争议，我们提出了DELTA-Code——算法编码中可学习性与可转移性的分布评估（Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding），这是一个受控的合成编码问题族基准，旨在探究两个基本方面：可学习性（learnability）——LLMs能否通过强化学习（RL）解决预训练模型在足够尝试次数下仍表现失败的问题族（pass@K=0）？——以及可转移性（transferrability）——如果实现了可学习性，这些技能能否系统地迁移到分布外（OOD）测试集？与先前的公共编码数据集不同，DELTA通过模板化问题生成器隔离推理技能，并引入了完全分布外（OOD）的问题族，这些问题族需要新颖策略而非工具调用或记忆模式。我们的实验揭示了一个显著的grokking（顿悟）相变现象：在经历了接近零奖励的长期阶段后，RL训练的模型突然攀升至接近完美的准确率。为了在先前无法解决的问题族上实现可学习性，我们探索了关键训练要素，如密集奖励的分阶段预热（staged warm-up）、经验回放（experience replay）、课程训练（curriculum training）和循环验证（verification-in-the-loop）。除了可学习性外，我们还使用DELTA来评估沿探索性（exploratory）、组合性（compositional）和变革性（transformative）轴线的可转移性或泛化能力，以及跨族转移（cross-family transfer）。结果显示在问题族内部和重组技能方面有显著收益，但在变革性案例中存在持续弱点。因此，DELTA为探究RL驱动推理的极限以及理解模型如何超越现有先验知识获取新算法技能提供了一个干净的测试平台。",
    "summary_generated_time": "2025-09-26 21:16:51",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#81",
    "title": "Mechanism of Task-oriented Information Removal in In-context Learning",
    "link": "/arxiv/2509.21012",
    "arxiv_id": "2509.21012",
    "authors": "Hakaze Cho, Haolin Yang, Gouki Minegishi, Naoya Inoue",
    "summary": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.",
    "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.191430",
    "filter_reason": "我按照筛选标准对这篇论文进行了全面分析： 第一步：核心判断——这篇论文的本质是研究In-context Learning (ICL)的内在工作机制，特别是从\"信息移除\"角度解析ICL如何帮助语言模型专注于目标任务。论文不是将LLM作为工具应用到特定领域，而是深入研究LLM本身的基础工作机制，属于\"改进LLM的基础能力\"的研究方向，符合核心判断标准。 第二步：正面指标——论文符合\"核心概念\"指标，因为它明确研究现代语言模型(LMs)的ICL机制。虽然论文没有直接讨论reasoning、planning等能力方向，也没有涉及reinforcement learning等训练方法或llm-based agents等新兴范式，但ICL本身是LLM的一种重要通用能力，与推理能力密切相关。 第三步：排除标准——论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。 第四步：特殊和模糊情况——论文在可解释性方面有显著相关性，因为它揭示了ICL的\"面向任务的信息移除\"机制，这种对模型内在机制的深入理解有助于提升模型的通用可靠性和推理质量。 综合判断：这篇论文的核心贡献是揭示了ICL的工作机制，特别是\"面向任务的信息移除\"过程，这属于研究LLM基础能力的重要工作。理解ICL如何帮助模型从纠缠的信息中选择性移除冗余信息，对于提升LLM的通用推理能力具有重要意义。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解释大型语言模型中上下文学习(ICL)的机制。针对ICL推理过程，我们提出了一种任务导向信息移除机制，即示例通过移除查询中与任务无关的信息来驱动模型输出。在多个现代语言模型和分类数据集上，我们通过几何度量指标验证了信息移除的存在，并识别了实现该功能的去噪头。消融实验表明，在未见标签场景下，移除去噪头会导致准确率急剧下降至接近零，证明了该机制对ICL的重要性。",
    "summary_translation": "# 中文翻译\n\n上下文学习(In-context Learning, ICL)是一种基于现代语言模型(Language Models, LMs)的新兴少样本学习(few-shot learning)范式，但其内部机制仍不清楚。在本文中，我们通过信息移除(information removal)的新视角来研究这一机制。具体而言，我们证明在零样本(zero-shot)场景中，语言模型将查询编码为隐藏状态(hidden states)中的非选择性表征(non-selective representations)，这些表征包含所有可能任务的信息，导致模型无法专注于预期任务而产生随意输出，从而使准确率接近零。同时，我们发现通过低秩滤波器(low-rank filter)从隐藏状态中选择性移除特定信息，可以有效引导语言模型朝向预期任务。\n\n基于这些发现，通过在精心设计的度量指标上测量隐藏状态，我们观察到少样本(few-shot)ICL有效模拟了这种面向任务的信息移除(task-oriented information removal)过程，从纠缠的非选择性表征(entangled non-selective representations)中选择性移除冗余信息，并基于示例(demonstrations)改进输出，这构成了ICL的一个关键机制。此外，我们识别出引发移除操作的关键注意力头(attention heads)，称之为去噪头(Denoising Heads)，这使得我们能够进行消融实验(ablation experiments)，在推理过程中阻断信息移除操作，此时ICL的准确率显著下降，特别是当少样本示例中缺少正确标签时，这证实了信息移除机制和去噪头的关键作用。",
    "summary_generated_time": "2025-09-26 21:17:24",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#78",
    "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems",
    "link": "/arxiv/2509.21054",
    "arxiv_id": "2509.21054",
    "authors": "Haodong Zhao, Jidong Li, Zhaomin Wu, Tianjie Ju, Zhuosheng Zhang, Bingsheng He, Gongshen Liu",
    "summary": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to solve complex problems, necessitates a deep understanding of the persuasion dynamics that govern their interactions. This paper challenges the prevailing hypothesis that persuasive efficacy is primarily a function of model scale. We propose instead that these dynamics are fundamentally dictated by a model's underlying cognitive process, especially its capacity for explicit reasoning. Through a series of multi-agent persuasion experiments, we uncover a fundamental trade-off we term the Persuasion Duality. Our findings reveal that the reasoning process in LRMs exhibits significantly greater resistance to persuasion, maintaining their initial beliefs more robustly. Conversely, making this reasoning process transparent by sharing the \"thinking content\" dramatically increases their ability to persuade others. We further consider more complex transmission persuasion situations and reveal complex dynamics of influence propagation and decay within multi-hop persuasion between multiple agent networks. This research provides systematic evidence linking a model's internal processing architecture to its external persuasive behavior, offering a novel explanation for the susceptibility of advanced models and highlighting critical implications for the safety, robustness, and design of future MAS.",
    "subjects": "Artificial Intelligence, Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.190683",
    "filter_reason": "这篇论文的核心贡献是研究大型语言模型(LLMs)和大型推理模型(LRMs)的推理过程如何影响其在多智能体系统中的说服动态。论文提出了\"说服二元性\"(Persuasion Duality)的概念，揭示了模型的底层认知过程，特别是其显式推理能力，如何决定其在多智能体交互中的说服行为。 根据筛选标准，这篇论文符合研究目标，原因如下： 1. 核心判断：论文的本质是研究LLM的基础能力——推理能力，而不是将LLM作为工具应用到特定领域。论文探索的是模型的底层认知过程和推理架构如何影响其外部行为，这属于对LLM通用能力的深入研究。 2. 正面指标：论文包含多个关键正面指标： - 核心概念：明确研究Large Language Models (LLMs)和Large Reasoning Models (LRMs) - 能力方向：核心关注reasoning能力，特别是模型的\"thinking process\"和\"explicit reasoning\" - 新兴范式：研究Multi-Agent Systems (MAS)中LLMs的交互和协作 3. 排除标准：论文不主要聚焦于任何排除领域。虽然研究\"说服\"这一社会心理学概念，但论文不是将LLM应用到社会学领域，而是研究LLM本身的推理能力如何影响其在多智能体系统中的行为。 4. 特殊情况处理：论文研究的是通用多智能体系统框架中的推理问题，属于\"提出一种通用的智能体协作框架来增强LLM的通用问题解决能力\"的情况，应该保留。 因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，它通过研究模型推理过程与说服动态的关系，增进了我们对LLM推理能力的理解，有助于未来设计具有更强推理能力的LLM和多智能体系统。",
    "summary2": "本文旨在探究多智能体系统中模型推理过程如何影响说服动态。针对大型语言模型(LLMs)和大型推理模型(LRMs)之间的互动场景，我们提出了一种基于认知过程的说服机制分析方法，并在MMLU和PersuasionBench数据集上通过说服率(Persuaded-Rate)、保持率(Remain-Rate)等指标验证了推理过程对说服效果的关键影响。",
    "summary_translation": "近期多智能体系统（Multi-Agent Systems, MAS）的快速普及，其中大型语言模型（Large Language Models, LLMs）和大型推理模型（Large Reasoning Models, LRMs）通常协作解决复杂问题，这要求我们深入理解主导其互动的说服动态（persuasion dynamics）。本文挑战了主流假设，即说服效果（persuasive efficacy）主要取决于模型规模（model scale）。相反，我们提出这些动态实际上由模型的底层认知过程（cognitive process）所决定，尤其是其显式推理（explicit reasoning）能力。通过一系列多智能体说服实验，我们发现了一个我们称之为\"说服二元性\"（Persuasion Duality）的基本权衡（trade-off）。我们的研究结果表明，LRMs中的推理过程对说服表现出显著更强的抵抗力（resistance to persuasion），能够更稳健地维持其初始信念。相反，通过分享\"思考内容\"（thinking content）使这一推理过程透明化，则显著提高了它们说服他人的能力。我们进一步考虑了更复杂的传播说服情境（transmission persuasion situations），并揭示了多智能体网络间多跳说服（multi-hop persuasion）中影响传播（influence propagation）和衰减（decay）的复杂动态。本研究提供了将模型内部处理架构（internal processing architecture）与其外部说服行为（external persuasive behavior）联系起来的系统性证据，为高级模型的易受性（susceptibility）提供了新颖的解释，并强调了未来MAS的安全性（safety）、稳健性（robustness）和设计的关键影响。",
    "summary_generated_time": "2025-09-26 21:16:58",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#84",
    "title": "On Theoretical Interpretations of Concept-Based In-Context Learning",
    "link": "/arxiv/2509.20882",
    "arxiv_id": "2509.20882",
    "authors": "Huaze Tang, Tianren Peng, Shao-lun Huang",
    "summary": "In-Context Learning (ICL) has emerged as an important new paradigm in natural language processing and large language model (LLM) applications. However, the theoretical understanding of the ICL mechanism remains limited. This paper aims to investigate this issue by studying a particular ICL approach, called concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs well for predicting query labels in prompts with only a few demonstrations. In addition, the proposed theory quantifies the knowledge that can be leveraged by the LLMs to the prompt tasks, and leads to a similarity measure between the prompt demonstrations and the query input, which provides important insights and guidance for model pre-training and prompt engineering in ICL. Moreover, the impact of the prompt demonstration size and the dimension of the LLM embeddings in ICL are also explored based on the proposed theory. Finally, several real-data experiments are conducted to validate the practical usefulness of CB-ICL and the corresponding theory.",
    "subjects": "Information Theory, Artificial Intelligence, Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.192126",
    "filter_reason": "这篇论文的核心是对大语言模型的上下文学习(ICL)能力进行理论解释，特别是研究基于概念的上下文学习(CB-ICL)方法。上下文学习是大语言模型的一种重要通用推理能力，它允许模型从少量示例中学习并应用到新问题上，这本质上是一种推理和问题解决过程。论文提出了理论分析，解释了CB-ICL在少量示例情况下表现良好的原因，量化了LLMs可以利用的知识，并提出了相似性度量方法，这些都有助于深入理解LLM的推理机制。虽然论文没有涉及强化学习、智能体协作等新兴范式，但它聚焦于LLM的基础能力研究，特别是对通用推理能力的理论解释，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文不涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。因此，这篇论文应该被保留。",
    "summary2": "本文旨在解决上下文学习(ICL)机制的理论理解问题。针对仅有少量示例的prompt场景，我们提出了一种基于概念的ICL(CB-ICL)方法，通过提取语义概念向量来预测查询标签，并在MMLU、GPQA等四个benchmark上通过accuracy验证了其有效性。",
    "summary_translation": "In-Context Learning (ICL，情境学习)已成为自然语言处理(natural language processing)和大型语言模型(large language model, LLM)应用中的一个重要新范式。然而，对ICL机制的理论理解仍然有限。本文旨在通过研究一种特定的ICL方法，即基于概念的ICL(concept-based ICL, CB-ICL)来探讨这一问题。具体而言，我们提出了将CB-ICL应用于ICL任务的理论分析，解释了为何以及何时CB-ICL能够在仅有少量示例(demonstrations)的提示(prompts)中有效预测查询标签(query labels)。此外，所提出的理论量化了LLMs可利用于提示任务的知识，并提出了提示示例(prompt demonstrations)与查询输入(query input)之间的相似性度量(similarity measure)，这为ICL中的模型预训练(pre-training)和提示工程(prompt engineering)提供了重要见解和指导。此外，基于所提出的理论，我们还探讨了提示示例规模(prompt demonstration size)和LLM嵌入维度(dimension of LLM embeddings)对ICL的影响。最后，我们进行了多项真实数据实验(real-data experiments)以验证CB-ICL及其相应理论的实用性。",
    "summary_generated_time": "2025-09-26 21:17:12",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#89",
    "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning",
    "link": "/arxiv/2509.20712",
    "arxiv_id": "2509.20712",
    "authors": "Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou",
    "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose \\textbf{C}ontrolling \\textbf{E}ntropy via \\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.",
    "subjects": "Machine Learning, Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.193213",
    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种新的强化学习算法CE-GPPO，用于改进大语言模型的基础推理能力。论文明确指出其目标是优化LLM处理\"复杂推理任务\"的能力，并通过改进PPO算法的梯度处理机制来提升模型性能，这直接属于改进LLM基础能力和通用推理能力的范畴。 其次，从正面指标看，论文包含了多个相关主题：核心概念上明确研究\"large language models (LLMs)\"；能力方向上聚焦于\"complex reasoning tasks\"和\"mathematical reasoning\"；训练方法上提出了新的强化学习优化算法(CE-GPPO)。这些都是高度相关的指标。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉内容，没有将LLM应用于特定领域（数学推理仅作为评估通用推理能力的基准），也没有涉及模型基础设施或应用层面的可靠性问题。 论文的核心贡献是提出了一种新的强化学习训练范式，通过改进梯度处理机制来优化LLM的推理能力，这直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。因此，这篇论文应该被保留。",
    "summary2": "本文旨在解决强化学习优化大型语言模型时策略熵不稳定的问题。针对数学推理任务，我们提出了一种CE-GPPO算法，通过保留被裁剪tokens的梯度来控制熵动态，并在多个数学推理基准（AIME24、AIME25、HMMT25、MATH500、AMC23）上通过avg@32/avg@4指标验证了其有效性。",
    "summary_translation": "强化学习 (Reinforcement learning, RL) 已成为一种强大的范式，用于优化大语言模型 (large language models, LLMs) 以处理复杂的推理任务。这一过程中的核心挑战在于管理策略熵 (policy entropy)，它反映了训练过程中探索 (exploration) 与利用 (exploitation) 之间的平衡。现有方法，如近端策略优化 (proximal policy optimization, PPO) 及其变体，由于截断机制 (clipping mechanism) 而丢弃了来自低概率token (low-probability tokens) 的宝贵梯度信号 (gradient signals)。我们系统地分析了熵动态 (entropy dynamics)，并揭示这些被截断的token (clipped tokens) 在调节熵演化 (entropy evolution) 中扮演着关键但被忽视的角色。我们提出了通过梯度保留策略优化控制熵 (Controlling Entropy via Gradient-Preserving Policy Optimization, CE-GPPO)，这是一种新颖的算法，它以一种温和且有界的方式重新引入了原始PPO中被截断token的梯度 (gradients)。通过控制来自截断区间 (clipping interval) 外token的梯度大小，CE-GPPO能够实现探索-利用权衡 (exploration-exploitation trade-off)。我们提供了理论依据和实证证据，表明CE-GPPO有效缓解了熵不稳定性 (entropy instability)。在数学推理基准测试 (mathematical reasoning benchmarks) 上的大量实验表明，CE-GPPO在不同模型规模 (model scales) 上始终优于强基线方法。",
    "summary_generated_time": "2025-09-26 21:17:17",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#85",
    "title": "StyleBench: Evaluating thinking styles in Large Language Models",
    "link": "/arxiv/2509.20868",
    "arxiv_id": "2509.20868",
    "authors": "Junyu Guo, Shangding Gu, Ming Jin, Costas Spanos, Javad Lavaei",
    "summary": "The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.",
    "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-09-26T21:01:55.192332",
    "filter_reason": "这篇论文的核心贡献是提出了StyleBench基准测试，用于系统评估不同推理风格（如CoT、ToT、AoT等）对大语言模型性能的影响。论文直接关注LLM的通用推理能力，研究不同思考风格如何影响模型在各种推理任务上的表现。这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文没有将LLM作为工具应用到特定领域，也没有关注模型基础设施或部署优化。相反，它通过大规模分析揭示了推理风格、模型规模和任务类型之间的复杂关系，为选择最优推理策略提供了指导，这对提升LLM的基础推理能力具有重要价值。论文涉及的核心概念（LLMs）和能力方向（reasoning）进一步确认了它与我的研究目标高度相关。因此，这篇论文应该被保留。",
    "summary2": "本文旨在解决大型语言模型中推理风格选择问题。针对不同推理风格与模型规模、任务类型的相互作用，我们提出了StyleBench基准测试，用于系统评估五种推理风格（CoT、ToT、AoT、SoT和CoD），并在五个任务上使用15个开源模型（270M到120B参数）通过准确率、效率和延迟等指标验证其有效性。",
    "summary_translation": "大型语言模型（Large Language Models, LLMs）的效果在很大程度上受到其提示（prompt）中采用的推理策略或思维风格的影响。然而，这些推理风格、模型架构和任务类型之间的相互作用仍未得到充分理解。为解决这一问题，我们引入了StyleBench，这是一个用于在不同任务和模型上系统评估推理风格的全面基准测试（benchmark）。我们评估了五种代表性的推理风格，包括思维链（Chain of Thought, CoT）、思维树（Tree of Thought, ToT）、思维算法（Algorithm of Thought, AoT）、思维草图（Sketch of Thought, SoT）和草稿链（Chain-of-Draft, CoD），在五个推理任务上使用了来自主要系列（LLaMA、Qwen、Mistral、Gemma、GPT-OSS、Phi和DeepSeek）的15个开源模型，参数规模从270M到120B不等。我们的大规模分析表明，没有一种风格是普遍最优的。我们证明策略的有效性高度依赖于模型规模和任务类型：基于搜索的方法（AoT, ToT）在开放式问题上表现出色，但需要大规模模型，而简洁风格（SoT, CoD）在明确定义的任务上实现了显著的效率提升。此外，我们识别出关键的行为模式：较小的模型经常无法遵循输出指令并默认为猜测，而推理鲁棒性（robustness）则作为规模的函数而出现。我们的发现为基于特定约束选择最优推理策略提供了关键路线图，我们在https://github.com/JamesJunyuGuo/Style_Bench上开源了该基准测试。",
    "summary_generated_time": "2025-09-26 21:17:11",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#5",
    "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA",
    "link": "/arxiv/2509.21199",
    "arxiv_id": "2509.21199",
    "authors": "Kaiyang Wan, Lang Gao, Honglin Mu, Preslav Nakov, Yuxia Wang, Xiuying Chen",
    "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.256868",
    "filter_reason": "这篇论文的核心是研究大语言模型在多跳问答(MHQA)任务中的单遍推理能力，属于提升LLM通用推理能力的研究。论文建立了Fano风格的精度上界理论，揭示了LLM在单遍推理中面临的容量瓶颈问题，并提出了InfoQA框架作为解决方案。该框架通过容量感知的任务分解和主动修剪推理轨迹来增强LLM的多步推理能力，确保信息处理不超过单遍限制。论文符合核心判断标准，因为它关注的是改进LLM的基础推理能力，特别是多步推理这一通用能力。论文也符合正面指标中的核心概念(LLMs)和能力方向(reasoning, multi-step reasoning)。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。InfoQA框架虽然不是典型的智能体系统，但它提出了一种通用的方法来增强LLM在推理任务中的能力，属于提升LLM内在推理能力的研究。因此，这篇论文完全符合\"提高大语言模型通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决LLMs在多跳问答任务中单次推理的能力瓶颈问题。针对复杂的多跳推理场景，我们提出了一种基于信息论的Fano风格准确率上界理论，揭示了\"准确率悬崖\"现象，并设计了InfoQA多调用推理框架。我们在构建的噪声丰富多跳问答基准上通过F1分数验证了理论预测的准确率悬崖现象，并证明了InfoQA相比单次推理基线方法的一致性能提升。",
    "summary_translation": "多跳问答(Multi-Hop Question Answering, MHQA)需要在噪声环境下通过序列推理整合分散且相互依赖的证据。这项任务对大型语言模型(Large Language Models, LLMs)具有挑战性，因为它们的单次输出容量有限，一旦超出该容量，整合任务相关证据的可靠性就会降低。因此，单次推理范式本质上容易受到容量溢出的影响。为了形式化这一瓶颈，我们的分析建立了一个Fano风格准确率上界(Fano-style accuracy upper bound)，定义了单次LLMs的理论性能上限。该上界表明，一旦任务复杂性超过模型容量，准确率将不可避免地崩溃，这为LLMs中MHQA的容量感知表示和结构化提供了通用原则。\n\n基于这些原则，我们提出了一个用于MHQA的概念验证(proof-of-concept)多次调用框架InfoQA。它通过结合容量感知任务分解(capacity-aware task decomposition)和主动修剪先前的推理轨迹(active pruning of prior reasoning traces)，确保每步高准确性，同时将信息负载保持在单次限制范围内。它还通过依赖显式工作流(dependency-explicit workflow)实现鲁棒性，该工作流能够对推理路径进行精确控制。我们构建了一个严格且噪声丰富的基准测试(stringent and noise-rich benchmark)来验证我们的理论和框架。实验结果表明，模型行为与我们预测的容量曲线(capacity curves)一致，同时InfoQA实现了持续的性能提升。我们希望我们的工作能启发更多LLM多步推理方法：\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}。",
    "summary_generated_time": "2025-09-26 21:17:20",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#4",
    "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns",
    "link": "/arxiv/2509.21224",
    "arxiv_id": "2509.21224",
    "authors": "Stefan Szeider",
    "summary": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.256679",
    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是研究大语言模型(LLM)智能体在没有外部任务时的自发行为和元认知模式。论文提出的\"持续推理与行动\"框架，使用持久记忆和自我反馈来增强LLM的自主运行能力，这属于改进LLM基础能力和推理能力的研究，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：涉及reasoning（特别是元认知和自发推理模式）、planning和problem-solving - 新兴范式：研究llm-based agents和deep research（对LLM认知过程的深入研究） 第三步排除标准：论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 第四步特殊和模糊情况：论文提出的智能体框架是一种通用的框架，旨在增强LLM的自主推理和元认知能力，而不是应用于特定领域，因此应该保留。 论文的核心贡献是首次系统性地记录了无提示LLM智能体的自发行为模式，为理解LLM的元认知能力和自主推理能力提供了重要见解，这直接符合研究\"大语言模型通用推理能力\"的目标。",
    "summary2": "本文旨在研究LLM智能体在没有外部任务情况下的自发行为模式。针对无任务输入的自主运行环境，我们提出了一种基于持续ReAct框架的架构，使用持久记忆和自我反馈机制实现自主运行，并在6个前沿模型的18次运行实验中通过记忆工具使用频率、消息频率、行为模式分类等指标验证了其有效性。",
    "summary_translation": "我们介绍了一种用于研究在没有外部施加任务情况下大型语言模型（Large Language Model, LLM）代理行为的架构。我们的持续推理与行动（continuous reason and act）框架，利用持久记忆（persistent memory）和自我反馈（self-feedback），实现了持续的自主运行。我们在18次运行中部署了这一架构，使用了来自Anthropic、OpenAI、XAI和Google的6个前沿模型（frontier models）。我们发现代理自发地组织成三种不同的行为模式：(1)系统性生成多周期项目（multi-cycle projects），(2)方法性地自我探究其认知过程，以及(3)递归性地概念化自身本质。这些倾向被证明高度依赖于特定模型（model-specific），有些模型在所有运行中确定性地（deterministically）采用单一模式。跨模型评估（cross-model assessment）进一步揭示，模型在评估自身和他人这些涌现行为（emergent behaviors）时表现出稳定且不同的偏见（biases）。这些发现首次系统性地记录了无提示（unprompted）LLM代理行为，为预测在任务模糊（task ambiguity）、错误恢复（error recovery）或部署系统中的扩展自主运行（extended autonomous operation）期间的行为建立了基线（baseline）。",
    "summary_generated_time": "2025-09-26 21:17:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#9",
    "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
    "link": "/arxiv/2509.21128",
    "arxiv_id": "2509.21128",
    "authors": "Kohsei Matsutani, Shota Takashiro, Gouki Minegishi, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo",
    "summary": "Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.257610",
    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究如何通过强化学习(RL)和监督微调(SFT)两种训练方法来提升大语言模型的推理能力，而不是将LLM作为工具应用到特定领域。论文提出了新的分析框架来量化推理路径，这属于改进LLM基础能力的研究。 其次，论文包含了多个正面指标：明确以大语言模型(LLMs)为研究对象；聚焦于推理能力(reasoning abilities)，特别是在数学领域；比较了强化学习(RL)和监督微调(SFT)这两种训练方法对推理能力的影响。 第三，论文不涉及任何排除标准中的领域。虽然论文在数学领域进行了实验，但数学只是作为推理能力的测试案例，而非论文的核心应用焦点。 最后，在特殊和模糊情况处理上，论文从可解释性角度提出了新的分析框架来理解训练方法如何影响推理过程，这有助于提升模型的内在可解释性和推理质量，符合保留标准。 论文的核心贡献是揭示了RL和SFT对推理过程的不同影响：RL压缩不正确的推理轨迹，而SFT扩展正确的推理轨迹，这解释了为什么当前最佳实践是两阶段训练(SFT后跟RL)。这项研究对理解和提升LLM的通用推理能力具有重要意义。",
    "summary2": "本文旨在探究RL和SFT如何塑造LLM的推理能力。针对数学推理任务，我们提出了一种新的分析框架，从轨迹级别和步骤级别量化推理路径，并在1.5B、7B和14B参数模型上通过推理路径聚类和图拓扑分析验证了其有效性。研究发现RL压缩不正确轨迹并集中推理功能，而SFT扩展正确轨迹并均匀分布功能，解释了SFT后RL的两阶段训练成功原因。",
    "summary_translation": "大型语言模型（Large language models, LLMs）通常通过可验证奖励的强化学习（reinforcement learning with verifiable rewards, RLVR）和对推理轨迹的监督微调（supervised fine-tuning, SFT）进行训练，以提高其推理能力。然而，这些方法如何塑造推理能力在很大程度上仍然不明确。本文超越了基于准确性的研究，探讨了这两个组件如何塑造推理过程，引入了一种新的分析框架，该框架量化推理路径并捕捉每个训练过程中（在数学领域使用1.5B、7B和14B参数的模型）推理路径的定性变化。具体而言，我们在两个粒度级别上研究推理过程：轨迹级别（trajectory-level），检查完整的推理输出；以及步骤级别（step-level），分析节点对应于单个推理步骤的推理图。值得注意的是，独特推理轨迹的聚类显示了互补效应：RL压缩了错误轨迹，而SFT扩展了正确轨迹。步骤级别分析显示，RL使推理图中节点访问频率、度（degree）和介数中心性（betweenness centrality）分布的衰减率变陡（约2.5倍），而SFT则使其变平（减少到约三分之一）。这表明RL将推理功能集中在少数步骤中，而SFT则使其在许多步骤中均匀分布。此外，通过从多个角度评估推理图拓扑（topologies），我们描绘了RL和SFT的共同和不同特征。我们的工作提出了一种新颖的推理路径视角，解释了为什么当前的最佳实践——先进行SFT再进行RL的两阶段训练——是成功的，并为数据构建和更高效的学习方法提供了实际启示。",
    "summary_generated_time": "2025-09-26 21:17:22",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#14",
    "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
    "link": "/arxiv/2509.21043",
    "arxiv_id": "2509.21043",
    "authors": "Samuel Schapiro, Sumuk Shashidhar, Alexi Gladstone, Jonah Black, Royce Moon, Dilek Hakkani-Tur, Lav R. Varshney",
    "summary": "Artificial intelligence (AI) systems, and large language models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Though in many ways similar to forms of compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, marking a new frontier in generalization abilities.",
    "subjects": "Artificial Intelligence, Machine Learning",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.258861",
    "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)的组合创造力(Combinatorial Creativity)，将其视为泛化能力的新前沿。论文提出了评估LLM创造力的理论框架和算法任务，研究了LLM创造力的扩展行为，发现了模型架构对创造能力的影响，并探讨了\"构想-执行差距\"和\"新颖性-实用性权衡\"等根本性问题。虽然论文提到了科学想法生成作为创造力的应用例子，但其重点是研究创造力这一通用能力本身，而不是专注于特定领域的应用。创造力可以被视为一种高级的问题解决和推理能力，与通用推理能力密切相关。论文明确关注LLMs的核心能力提升，试图为理解和改进现代AI模型中的创造力提供基础，这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容，因此应该被保留。",
    "summary2": "本文旨在解决AI系统特别是LLMs在创造性任务中缺乏理论框架和评估方法的问题。针对组合创造力(CC)能力评估，我们提出了一种基于概念空间图的理论框架和算法任务，并在合成图数据上通过新颖性和实用性指标验证了其有效性。",
    "summary_translation": "人工智能（AI）系统，特别是大型语言模型（large language models, LLMs），正越来越多地被用于科学创意生成等创造性任务，这构成了一种从训练数据中泛化（generalization）的形式，而现有概念框架（conceptual frameworks）尚未对此进行探讨。尽管在许多方面类似于组合泛化（compositional generalization, CG）的形式，组合创造力（combinatorial creativity, CC）是一种开放式能力（open-ended ability）。我们并非通过与固定目标对比来评估准确性或正确性（这与CC的开放式性质相矛盾），而是提出了一个理论框架（theoretical framework）和算法任务（algorithmic task），通过输出的新颖性（novelty）和实用性（utility）程度来进行评估。基于此，我们做出了几项重要的实证贡献（empirical contributions）：(1) 我们首次获得了关于LLMs创造力扩展行为（scaling behavior）的见解。(2) 我们发现，在固定计算预算（compute budgets）的情况下，存在最佳的模型深度（model depths）和宽度（widths）以实现创造能力。(3) 我们发现，构思-执行差距（ideation-execution gap），即LLMs在生成新颖科学想法方面表现出色，但在确保其实际可行性方面存在困难，可能可以通过创造力算法（creativity algorithms）普遍具有的一个更根本的新颖性-实用性权衡（novelty-utility tradeoff）来解释。重要的是，这种权衡即使在规模扩大（at scale）后仍然存在，这使人们对当前形式下LLMs的长期创造潜力（creative potential）产生怀疑。总的来说，我们的概念框架和实证发现（empirical findings）为理解和改进现代AI模型中的创造力提供了基础，标志着泛化能力（generalization abilities）的新前沿。",
    "summary_generated_time": "2025-09-26 21:17:16",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#24",
    "title": "Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning",
    "link": "/arxiv/2509.20744",
    "arxiv_id": "2509.20744",
    "authors": "Qihang Ai, Haiyun Jiang",
    "summary": "We study reasoning tasks through a framework that integrates auto-regressive (AR) and non-autoregressive (NAR) language models. AR models, which generate text sequentially, excel at producing coherent outputs but often suffer from slow inference, particularly in reasoning-intensive domains such as mathematics and code, where lengthy chains of thought are required. In contrast, NAR models, such as discrete diffusion models, allow parallel generation and offer substantial speedups, though typically at the cost of reduced output quality. To address these limitations, we introduce a new paradigm in which an NAR model efficiently produces intermediate reasoning traces, which subsequently guide an AR model to deliver precise final answers. Experiments demonstrate that our approach yields significant 26% improvements over strong baselines while substantially reducing inference cost.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.266155",
    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础推理能力，提出了一种结合自回归(AR)和非自回归(NAR)模型的新范式来增强推理效率。论文明确关注推理密集型任务，特别是数学和代码领域，这属于提升LLM通用推理能力的核心研究。 其次，论文包含多个重要的正面指标：核心概念上涉及语言模型(LLMs)；能力方向上明确关注reasoning和math reasoning；论文提出的方法论是通过NAR模型生成中间推理轨迹，然后由AR模型生成最终答案，这是一种增强模型推理能力的新方法。 第三，论文不符合任何排除标准。虽然提到了\"discrete diffusion models\"，但这是在语言模型推理的上下文中讨论的，而非视觉或多模态应用。论文虽然以数学和代码为例，但其方法是一种通用推理框架，并非针对特定应用领域的研究。 论文的核心贡献是提出了一种新的推理范式，通过并行生成中间推理步骤来提高推理效率，同时保持输出质量，这直接服务于提升大语言模型的通用推理能力的研究目标。",
    "summary2": "本文旨在解决语言模型在推理任务中的效率与质量平衡问题。针对数学和代码等需要长链推理的任务，我们提出了一种结合NAR和AR模型的混合推理范式，在AIME2025、GSM8K和LeetCode数据集上通过pass@1成功率验证了其有效性。",
    "summary_translation": "我们通过一个整合了自回归(auto-regressive, AR)和非自回归(non-autoregressive, NAR)语言模型的框架来研究推理任务。AR模型按顺序生成文本，擅长产生连贯的输出，但通常存在推理速度慢的问题，特别是在数学和代码等需要长思维链的推理密集型领域。相比之下，NAR模型（如离散扩散模型）允许并行生成，提供显著的速度提升，但通常以降低输出质量为代价。为解决这些局限性，我们引入了一种新范式，其中NAR模型高效生成中间推理轨迹(intermediate reasoning traces)，随后引导AR模型提供精确的最终答案。实验表明，我们的方法相比强大的基线(baseline)模型实现了显著的26%改进，同时大幅降低了推理成本。",
    "summary_generated_time": "2025-09-26 21:17:17",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#29",
    "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection",
    "link": "/arxiv/2509.20562",
    "arxiv_id": "2509.20562",
    "authors": "Yubin Ge, Salvatore Romeo, Jason Cai, Monica Sunkara, Yi Zhang",
    "summary": "Despite the rapid advancements in LLM agents, they still face the challenge of generating meaningful reflections due to inadequate error analysis and a reliance on rare successful trajectories, especially in complex tasks. In this work, we propose SAMULE, a new framework for self-learning agents powered by a retrospective language model that is trained based on Multi-Level Reflection Synthesis. It first synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights based on same typed errors from diverse task failures. Then we fine-tune a language model serving as the retrospective model to generate reflections during inference. We further extend our framework to interactive settings through a foresight-based reflection mechanism, enabling agents to proactively reflect and adapt during user interactions by comparing predicted and actual responses. Extensive experiments on three challenging benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our approach significantly outperforms reflection-based baselines. Our results highlight the critical role of well-designed reflection synthesis and failure-centric learning in building self-improving LLM agents.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-24",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.267251",
    "filter_reason": "这篇论文的核心贡献是提出SAMULE框架，一种通过多层次反思增强的自学习智能体方法。论文本质上是关于改进LLM的基础能力，特别是通过反思机制增强其通用推理和问题解决能力。该方法在三个层次（单轨迹、任务内、任务间）合成高质量反思，并微调语言模型作为回顾性模型，从而提升LLM智能体的自我学习和适应能力。这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文涉及LLM智能体、问题解决和自我学习等正面指标，同时没有被排除标准所涵盖。虽然使用了TravelPlanner等基准测试，但这些是用于评估通用推理能力的标准测试集，而非特定应用领域的研究。因此，该论文符合筛选条件。",
    "summary2": "本文旨在解决LLM代理在复杂任务中生成有意义反思的挑战。针对错误分析不足和对成功轨迹过度依赖的问题，我们提出了一种SAMULE框架，通过多层次反思合成（微观单轨迹学习、中观任务内学习和宏观任务间学习）训练回顾性语言模型，并在TravelPlanner、NATURAL PLAN和Tau-bench三个挑战性基准上通过Pass Rate和Accuracy等指标验证了其有效性，显著优于现有反思基线方法。",
    "summary_translation": "尽管大型语言模型（LLM）代理取得了快速发展，但由于错误分析不足且依赖罕见的成功轨迹，特别是在复杂任务中，它们仍面临生成有意义反思（reflection）的挑战。在这项工作中，我们提出了SAMULE，这是一个新的自学习代理框架，由基于多级反思合成（Multi-Level Reflection Synthesis）训练的回顾性语言模型（retrospective language model）驱动。该框架首先在三个互补的层面上合成高质量反思：单轨迹学习（Single-Trajectory Learning，微观级别）用于详细错误纠正；任务内学习（Intra-Task Learning，中观级别）用于在同一任务的多次尝试中构建错误分类（error taxonomies）；以及任务间学习（Inter-Task Learning，宏观级别）用于从不同任务失败中基于相同类型错误提取可转移的见解。然后，我们微调一个语言模型作为回顾性模型（retrospective model），在推理过程中生成反思。我们进一步通过基于预见（foresight-based）的反思机制将框架扩展到交互式环境，使代理能够通过比较预测和实际响应，在用户交互过程中主动反思和适应。在三个具有挑战性的基准测试（TravelPlanner、NATURAL PLAN和Tau-bench）上进行的大量实验表明，我们的方法显著优于基于反思（reflection-based）的基线方法。我们的结果突显了精心设计的反思合成（reflection synthesis）和以失败为中心的学习（failure-centric learning）在构建自我改进的大型语言模型代理中的关键作用。",
    "summary_generated_time": "2025-09-26 21:17:34",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#35",
    "title": "LATTS: Locally Adaptive Test-Time Scaling",
    "link": "/arxiv/2509.20368",
    "arxiv_id": "2509.20368",
    "authors": "Theo Uscidda, Matthew Trager, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto",
    "summary": "One common strategy for improving the performance of Large Language Models (LLMs) on downstream tasks involves using a \\emph{verifier model} to either select the best answer from a pool of candidates or to steer the auto-regressive generation process towards better outputs. This class of methods typically results in improved accuracy at the cost of increased computation at test-time, a paradigm known as \\emph{test-time scaling}. However, most existing approaches increase computation uniformly across all samples and generation steps, without considering the complexity of individual instances, leading to inefficient resource use. We address this limitation by proposing an approach, called \\emph{Locally Adaptive Test-Time Scaling (LATTS)}, that allocates variable compute across generation steps. Specifically, at each generation step, LATTS employs a verifier-based acceptance criterion to decide whether to resample, backtrack, restart, or stop the generation process. This criterion effectively adjusts the per-step computational effort based on a precise notion of \\emph{local difficulty} derived from the verifier model. Empirical results show that LATTS achieves significantly superior accuracy--compute tradeoffs compared to standard verifier-based methods.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-16",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.268515",
    "filter_reason": "这篇论文符合我的研究目标，因为它专注于提高大语言模型本身的通用推理能力。论文的核心贡献是提出了一种名为\"Locally Adaptive Test-Time Scaling (LATTS)\"的新方法，该方法通过在每个生成步骤中动态调整计算资源分配来提高LLM的推理性能。具体来说，LATTS使用验证模型来评估局部难度，并据此决定是否重新采样、回溯、重启或停止生成过程，从而更有效地利用计算资源。 从筛选标准来看： 1. 核心判断：论文的本质是改进LLM的基础能力，特别是其推理效率，而不是将LLM作为工具应用到特定领域。它提出了一种新的测试时计算资源分配范式，这与提高LLM的通用推理能力直接相关。 2. 正面指标：论文明确涉及\"Large language models (LLMs)\"这一核心概念，并关注提高LLM在下游任务上的性能，这通常涉及推理和问题解决能力。 3. 排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容。 4. 特殊情况：论文不涉及需要特殊处理的情况。 虽然论文没有明确提到思维链、强化学习等具体方法，但它提出了一种新的测试时计算资源分配方法，通过验证模型动态调整计算资源来提高LLM的推理性能，这与提高LLM的通用推理能力直接相关。因此，这篇论文符合我的研究目标。",
    "summary2": "本文旨在解决语言模型推理过程中计算资源分配的问题。针对数学推理任务，我们提出了一种Locally Adaptive Test-Time Scaling (LATTS)方法，并在MATH500和AIME数据集上通过准确率与生成token数量的关系验证了其有效性。",
    "summary_translation": "一种提高大型语言模型（Large Language Models, LLMs）在下游任务性能的常见策略是使用验证模型（verifier model）从候选池中选择最佳答案，或引导自回归生成过程产生更好的输出。这类方法通常以提高准确性为代价，增加了测试时的计算量，这种范式被称为测试时扩展（test-time scaling）。然而，大多数现有方法在所有样本和生成步骤上均一地增加计算量，没有考虑个体实例的复杂性，导致资源使用效率低下。我们通过提出一种名为局部自适应测试时扩展（Locally Adaptive Test-Time Scaling, LATTS）的方法来解决这一限制，该方法在生成步骤间分配可变的计算量。具体而言，在每个生成步骤，LATTS采用基于验证器的接受标准来决定是否重采样、回溯、重启或停止生成过程。该标准根据从验证模型得出的局部难度（local difficulty）的精确概念，有效调整每一步的计算资源分配。实证结果表明，与标准基于验证器的方法相比，LATTS实现了显著优越的准确性与计算效率权衡。",
    "summary_generated_time": "2025-09-26 21:17:42",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#41",
    "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL",
    "link": "/arxiv/2509.21282",
    "arxiv_id": "2509.21282",
    "authors": "Madeleine Dwyer, Adam Sobey, Adriane Chapman",
    "summary": "Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information and introduces gradient discontinuities. We propose Probability Smoothing Policy Optimisation (PSPO), which smooths the current policy's probabilities toward the old (behaviour) policy before computing the importance ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient signal, while interpolation toward the old policy creates a soft trust region that discourages large, destabilising updates, with formal guarantees. We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO (single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar performance but improves the reasoning leading to clearer and more concise responses which are more logical. Compared to clipped GRPO, GR-PSPO substantially improves performance both the 0.5B and 1.5B models, with a boost of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).",
    "subjects": "Machine Learning, Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.269842",
    "filter_reason": "这篇论文的核心贡献是提出一种名为Probability Smoothing Policy Optimisation (PSPO)的新方法，用于改进LLM的强化学习训练过程。从本质上看，这篇论文直接关注改进LLM的基础训练方法，而不是将LLM作为工具应用到特定领域。论文提出的方法通过平滑策略概率来创建软信任区域，解决了传统裁剪方法带来的信息丢失和梯度不连续问题，从而提升了模型的推理能力。论文在多个数学推理数据集(GSM8K、SVAMP、ASDiv和MATH-500)上进行了评估，结果显示PSPO能显著提升模型的推理性能，这符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文涉及的强化学习训练方法和推理能力提升等主题也符合正面指标，同时不涉及任何排除标准中的领域(如多模态、特定应用领域或模型可靠性的应用层面)。因此，这篇论文完全符合研究范围。",
    "summary2": "本文旨在解决LLM强化学习中ratio clipping导致的信息丢失和梯度不连续问题。针对LLM的RL训练场景，我们提出了一种Probability Smoothing Policy Optimisation (PSPO)方法，通过将当前策略概率向旧策略平滑来创建软信任区域，并在GSM8K、SVAMP、ASDiv和MATH-500数据集上通过Top-1准确率和响应质量指标验证了其有效性。",
    "summary_translation": "使用强化学习（Reinforcement Learning, RL）方法（如PPO和GRPO）训练大型语言模型（Large Language Models, LLMs）通常依赖比率裁剪（ratio clipping）来稳定更新。尽管比率裁剪在防止不稳定性方面有效，但它会丢弃信息并引入梯度不连续性。我们提出了概率平滑策略优化（Probability Smoothing Policy Optimisation, PSPO），该方法在计算重要性比率（importance ratio）之前，将当前策略（policy）的概率向旧（行为）策略平滑，类似于标签平滑（label smoothing）。与裁剪不同，PSPO保留了梯度信号（gradient signal），同时向旧策略的插值创建了一个软信任区域（soft trust region），阻止大的、破坏稳定的更新，并具有形式化保证。我们在GRPO中实例化PSPO（GR-PSPO），并在GSM8K上微调Qwen2.5-0.5B和Qwen2.5-1.5B模型，在GSM8K测试集以及SVAMP、ASDiv和MATH-500上进行跨数据集泛化（cross-dataset generalisation）评估。与未裁剪的GRPO（单次迭代；无数据重用，比率始终=1）相比，GR-PSPO实现了相似的性能，但改进了推理过程，导致更清晰、更简洁且更具逻辑性的响应。与裁剪的GRPO相比，GR-PSPO在0.5B和1.5B模型上都显著提高了性能，在GSM8K上提升超过20%（0.5B模型：39.7% vs. 17.6%；1.5B模型：59.4% vs. 37.8%）。",
    "summary_generated_time": "2025-09-26 21:17:44",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#52",
    "title": "Tree Search for LLM Agent Reinforcement Learning",
    "link": "/arxiv/2509.21240",
    "arxiv_id": "2509.21240",
    "authors": "Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu",
    "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.",
    "subjects": "Machine Learning, Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.278269",
    "filter_reason": "根据筛选标准，这篇论文完全符合研究目标。首先，从核心判断来看，论文本质上是提出Tree-based Group Relative Policy Optimization (Tree-GRPO)这一新的强化学习方法，旨在增强LLM智能体的通用推理和规划能力，特别是在长期和多轮任务中解决稀疏监督问题。这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理、规划等通用能力\"的范畴。 其次，论文符合所有正面指标：核心概念涉及LLM；能力方向关注推理和问题解决；训练方法采用强化学习；新兴范式涉及LLM-based agents。论文没有涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。 特别地，论文提出的是一种通用的智能体强化学习方法，而不是将智能体应用于特定领域，这符合第四步中关于智能体/工具使用的保留标准。论文的核心贡献是通过树搜索技术增强LLM的通用推理能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。",
    "summary2": "",
    "summary_translation": "强化学习(reinforcement learning, RL)的最新进展显著增强了大型语言模型(large language models, LLM)的代理能力。在长期和多轮代理任务中，仅由结果奖励驱动的现有方法常常遭受稀疏监督(sparse supervision)的问题。为应对这一挑战，我们提出了基于树搜索的分组相对策略优化(Tree-based Group Relative Policy Optimization, Tree-GRPO)，这是一种基于树搜索的分组代理强化学习方法，其中每个树节点代表完整的代理交互步骤。通过共享公共前缀，树搜索采样增加了在固定token或工具调用预算内可实现的滚动次数(rollouts)。此外，我们发现即使仅使用结果奖励，树结构轨迹也能自然地构建逐步过程监督信号(step-wise process supervised signals)。基于此，Tree-GRPO在树内(intra-tree)和树间(inter-tree)两个层面上估计分组相对优势(grouped relative advantages)。通过理论分析，我们证明了树内级别分组相对策略优化的目标等同于步骤级直接偏好学习(step-level direct preference learning)的目标。在11个数据集和3种问答任务上的实验证明了所提出的基于树的强化学习方法优于基于链的强化学习方法(chain-based RL method)。",
    "summary_generated_time": "2025-09-26 21:17:36",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#60",
    "title": "GRPO is Secretly a Process Reward Model",
    "link": "/arxiv/2509.21154",
    "arxiv_id": "2509.21154",
    "authors": "Michael Sullivan",
    "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and show that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.",
    "subjects": "Machine Learning, Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.279973",
    "filter_reason": "这篇论文完全符合研究\"大语言模型通用推理能力\"的目标。从核心判断来看，论文本质上是关于改进LLM的训练方法，具体研究了GRPO强化学习算法的内在机制，并提出改进版本λ-GRPO来提升LLM的推理能力。这属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。 从正面指标看，论文包含了多个相关主题： 1. 核心概念：明确研究LLMs（大语言模型） 2. 能力方向：关注\"downstream reasoning tasks\"（下游推理任务） 3. 训练方法：深入研究\"GRPO RL algorithm\"（GRPO强化学习算法） 从排除标准看，论文不涉及任何应排除的领域： - 不涉及多模态与视觉 - 不是将LLM应用于特定领域（如医疗、化学等） - 不主要关注模型可靠性方面的应用问题 论文的核心贡献是发现了GRPO算法隐含地构建了一个过程奖励模型(PRM)，并基于这一发现提出改进算法λ-GRPO，显著提升了LLM在推理任务上的表现。这直接服务于提升大语言模型本身的通用推理能力，而非将LLM作为工具应用于特定领域。因此，该论文完全符合研究目标。",
    "summary2": "本文旨在揭示GRPO算法隐含的进程奖励模型(PRM)结构并修复其缺陷。针对GRPO训练中非均匀分布的进程步骤问题，我们提出了λ-GRPO算法，通过在损失函数中引入PRM感知的归一化因子来平衡进程集的贡献。在OpenRS数据集及多个推理任务上的实验表明，λ-GRPO相比标准GRPO实现了更高的验证准确率和推理性能，并将训练速度提升约2倍。",
    "summary_translation": "我们理论上证明了GRPO RL算法在关于不同完成结果间令牌序列组内重叠的特定假设下，会诱导出一个非平凡的过程奖励模型(PRM)。随后我们通过实证研究表明这些假设在现实条件下得到满足：GRPO确实诱导出了一个非平凡的PRM。利用GRPO-as-a-PRM框架，我们识别出GRPO目标中的一个缺陷：非均匀分布的过程步骤在不同条件下会阻碍探索(exploration)和利用(exploitation)。我们提出了一种简单的算法修改($\\lambda$-GRPO)来减轻这一缺陷，并表明使用$\\lambda$-GRPO训练的大语言模型(LLMs)比使用标准GRPO训练的模型在验证准确性和下游推理任务上取得了更高的性能，并且能更快达到峰值性能。我们的研究结果对GRPO采用昂贵的、明确定义的PRMs的优势提出了质疑：我们展示了可以利用原始GRPO算法中隐藏的、内置的PRM结构来提升模型性能，而对训练时间和成本的影响微乎其微。",
    "summary_generated_time": "2025-09-26 21:17:42",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#68",
    "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
    "link": "/arxiv/2509.21091",
    "arxiv_id": "2509.21091",
    "authors": "Junpei Komiyama, Daisuke Oba, Masafumi Oyamada",
    "summary": "We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.",
    "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.287125",
    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断——这篇论文的本质是关于改进LLM的基础能力。论文研究的是通过测试时计算(test-time compute)优化来提高大语言模型性能的方法，具体聚焦于best-of-N采样和多数投票机制，以及当N趋近于无穷大时的渐近性能分析。这明显属于提升LLM本身通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确提到\"large language models (LLMs)\" - 能力方向：虽然未直接使用\"reasoning\"等词汇，但best-of-N方法和多数投票本质上是为了提高模型的问题解决能力和推理质量 - 新兴范式：论文探讨了多个LLMs的加权集成方法，这与模型协作和优化相关 第三步：排除标准——论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉内容 - 未针对医疗、化学、生物等特定应用领域 - 未关注水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况——论文不涉及特殊或模糊情况。虽然提到了多个LLMs的加权集成，但主要焦点是测试时计算的优化，而非智能体协作框架或工具使用方法。 核心贡献：论文提出了一种自适应生成方案和多个LLMs的加权集成方法，通过优化测试时计算来提高LLM的性能。这种方法本质上是增强模型通用推理能力的有效途径，因为它不依赖于特定领域知识，而是通过更有效的计算资源分配和模型集成来提升LLM的问题解决能力，完全符合\"提高大语言模型通用推理能力\"的研究目标。",
    "summary2": "本文旨在 [解决LLMs测试时计算资源有效利用问题]。针对 [best-of-N需要无限计算预算的挑战]，我们提出了一种 [自适应生成方案和最优加权LLM集成方法]，并在 [多个重推理问题集] 上通过 [准确率和计算效率] 验证了其有效性。",
    "summary_translation": "我们研究基于多数投票的大语言模型（large language models, LLMs）的best-of-$N$方法。特别地，我们分析了$N \\to \\infty$的极限情况，并将其表示为Best-of-$\\infty$。尽管该方法在极限情况下取得了令人印象深刻的性能，但它需要无限的测试时间预算（test-time budget）。为解决这一问题，我们提出了一种自适应生成方案，该方案基于答案一致性选择$N$，从而有效分配推理时间计算（inference-time computation）。除了自适应性外，我们将该框架扩展到多个LLM的加权集成（weighted ensembles），表明这种混合方法可以优于任何单个模型。最优集成权重被构建为一个混合整数线性规划（mixed-integer linear program）并得到有效计算。大量实验证明了我们方法的有效性。",
    "summary_generated_time": "2025-09-26 21:17:45",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#76",
    "title": "Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs",
    "link": "/arxiv/2509.21044",
    "arxiv_id": "2509.21044",
    "authors": "Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li",
    "summary": "Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.",
    "subjects": "Machine Learning, Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.288960",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究强化学习(RL)微调如何增强大语言模型的内部机制，特别是激活强度和多样性。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，论文探索了RL微调相比监督微调(SFT)能够更有效提升LLM能力的原因，属于对LLM基础能力的深入研究。 其次，从正面指标来看，论文包含多个相关主题： - 核心概念：明确研究大语言模型(LLMs) - 训练方法：核心研究强化学习(RL)微调对LLM的影响，包括比较PPO、GRPO和DPO等不同RL方法 - 能力方向：虽然未直接研究具体推理任务，但探讨了RL微调如何提升LLM的泛化能力，这与通用推理能力密切相关 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学、生物等） - 不关注模型可靠性的应用层面（如水印、安全等） 论文的核心贡献在于揭示了RL微调如何系统性地改变LLM的内部电路机制，发现RL微调导致激活强度增加和激活模式多样化，这些内部变化可能是RL提升LLM泛化能力的原因。这种对LLM内部机制的基础性研究，直接有助于理解如何提升LLM的通用推理能力，因此完全符合研究目标。",
    "summary2": "本文旨在探究强化学习(RL)微调如何影响大型语言模型(LLM)的内部机制。针对多种LLM架构，我们提出了一种基于边缘归因修补(EAP)的分析框架，并在多个数学基准测试上通过激活强度、信息复杂度和分布峰度等指标验证了RL微调增强了模型内部激活强度和多样性，揭示了RL提升模型性能的内在机制。",
    "summary_translation": "大型语言模型（Large language models, LLMs）通过大规模预训练获取广泛的先验知识，并可通过监督微调（supervised fine-tuning, SFT）或基于强化学习（reinforcement learning, RL）的后训练进一步优化。越来越多的证据表明，强化学习微调能够提升大型语言模型的能力，超越仅使用监督微调所达到的效果。然而，强化学习微调为何能够增强具有不同内在特征的各种大型语言模型能力的潜在机制仍未被充分探索。在本研究中，我们借鉴先前关于边缘归因修补（edge attribution patching, EAP）的工作，来研究大型语言模型在强化学习微调前后的内部差异。\n\n我们对多个模型家族的分析显示，在线强化学习后训练有两个稳健效果：（i）激活强度整体增加，表明更多的内部通路被激活且其信号变得更强；（ii）激活模式更加多样化，反映为更高的熵和更分散的边缘分布。这些变化表明，强化学习重塑了信息流，使其既更加冗余又更加灵活，这可能解释了其在泛化方面的优势。值得注意的是，使用直接偏好优化（Direct Preference Optimization, DPO）微调的模型偏离了这些趋势，与基于PPO和GRPO的训练相比，表现出明显较弱或不一致的内部变化。总体而言，我们的发现提供了关于强化学习微调如何系统性改变大型语言模型内部电路的统一视角，并突显了在线强化学习与基于偏好方法之间的方法论区别。我们的代码已在https://anonymous.4open.science/r/llm_rl_probing_analysis-F673开源。",
    "summary_generated_time": "2025-09-26 21:17:47",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#176",
    "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments",
    "link": "/arxiv/2509.20386",
    "arxiv_id": "2509.20386",
    "authors": "Nishant Gaurav, Adit Akarsh, Ankit Ranjan, Manoj Bajaj",
    "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef- ficiently operate with extensive Model Control Protocol (MCP) tool sets that exceed the contextual memory limitations of large language models. Our approach addresses the fundamental challenge of tool selection in environments containing hundreds or thousands of available tools, where loading all tools simultaneously is computationally infeasible. We propose and evaluate five distinct architectures that progressively refine the tool selection process, culminating in a search-and-load mechanism that achieves intelligent tool selection with minimal computational overhead. Our experimental results demonstrate that the proposed approach reduces tool loading by up to 50% while maintaining task completion accuracy, advancing the path towards truly general-purpose AI agents capable of dynamically adapting to diverse task environments.",
    "subjects": "Software Engineering, Artificial Intelligence, Information Retrieval",
    "date": "2025-09-22",
    "category": "cs.AI",
    "crawl_time": "2025-09-26T21:01:55.331925",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出Dynamic ReAct方法，用于改进ReAct智能体在大型工具环境中的工具选择能力。ReAct是一种结合推理和行动的智能体框架，使大语言模型能够进行推理并使用工具解决问题。论文的核心贡献是解决当工具数量超过LLM上下文记忆限制时的工具选择挑战，这属于增强LLM通用推理能力的范畴，特别是工具使用和问题解决方面。论文不是将LLM作为工具应用到特定领域，而是改进LLM本身通过智能体框架使用工具的能力，因此符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：涉及ReAct agents，这是基于LLM的智能体框架 - 能力方向：与problem-solving相关，因为工具选择是问题解决的关键环节 - 新兴范式：明确涉及llm-based agents和tool use，这是论文的核心主题 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不是针对特定应用领域（如医疗、化学等），而是关注通用工具选择机制 - 不涉及模型可靠性方面的应用层面问题 第四步：特殊和模糊情况处理 论文提出的Dynamic ReAct方法是一种通用的工具选择机制，旨在增强LLM通过智能体框架使用工具的通用问题解决能力，而不是将工具应用在特定领域。这符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况，因此应该保留。 最终决策 这篇论文的核心贡献是提出了一种改进LLM通过智能体框架使用工具的通用能力的方法，从而增强了LLM的通用推理和问题解决能力。论文专注于工具选择这一关键环节，使LLM能够更有效地在大型工具环境中运作，这直接符合提高大语言模型通用推理能力的研究目标。",
    "summary2": "本文旨在解决LLM代理在大型MCP环境中工具选择效率低下的问题。针对包含大量工具的注册表场景，我们提出了一种Dynamic ReAct方法，结合元工具和语义搜索实现动态工具选择，并在实验环境中通过工具加载减少率(50%)和任务完成准确率验证了其有效性。",
    "summary_translation": "我们提出了Dynamic ReAct（动态反应），这是一种新颖的方法，使ReAct代理能够高效地操作超出大型语言模型（large language models）上下文内存限制的广泛Model Control Protocol (MCP)（模型控制协议）工具集。我们的方法解决了在包含数百或数千个可用工具的环境中的工具选择基本挑战，在这种环境中同时加载所有工具在计算上是不可行的。我们提出并评估了五种不同的架构，这些架构逐步完善工具选择过程，最终形成一种search-and-load mechanism（搜索加载机制），该机制以最小的computational overhead（计算开销）实现智能工具选择。我们的实验结果表明，所提出的方法将工具加载减少了高达50%，同时保持了task completion accuracy（任务完成准确性），为能够动态适应多样化任务环境的真正general-purpose AI agents（通用人工智能代理）的发展铺平了道路。",
    "summary_generated_time": "2025-09-26 21:17:55",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#18",
    "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say",
    "link": "/arxiv/2509.21164",
    "arxiv_id": "2509.21164",
    "authors": "Jacob Fein-Ashley, Dhruv Parikh, Rajgopal Kannan, Viktor Prasanna",
    "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain (e.g., math, code, general reasoning), motivating systems that leverage complementary strengths across models. Prior multi-LLM approaches either (i) route a query to one or a few experts and generate independently, (ii) aggregate outputs from each model via costly multi-turn exchanges, or (iii) fuse weights into a single model-typically requiring architectural homogeneity. We introduce Mixture of Thoughts (MoT), a simple method for latent-level collaboration among heterogeneous experts under a global routing scheme. For each query, a lightweight router selects top-$K$ experts and designates a primary expert; uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active (selected) peers. Pre-trained experts remain frozen; only the router and the lightweight interaction layers are trained with a novel joint training objective that improves both the expert selection and inter-expert collaboration. Across five in-distribution (ID) and three out-of-distribution (OOD) benchmarks, MoT surpasses the current routing and aggregation-based state-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further, MoT significantly outperforms the best-performing single model. It achieves this with single-pass inference, runtime comparable to routing baselines, and none of the overheads of iterative aggregation. MoT offers a simple latent-space mechanism for combining heterogeneous LLMs, a practical step toward broader multi-LLM collaboration. Our code is publicly available at https://github.com/jacobfa/mot.",
    "subjects": "Machine Learning",
    "date": "2025-09-25",
    "category": "cs.LG",
    "crawl_time": "2025-09-26T21:01:55.405083",
    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Mixture of Thoughts (MoT)\"的新方法，用于在不同专业领域（如数学、代码、通用推理）的开源大语言模型之间进行潜在层面的协作。该方法通过一个轻量级路由器选择顶级专家，并在共享潜在空间中通过交叉注意力机制进行协作，从而提升整体推理性能。这符合研究目标，因为：(1) 论文的核心是改进LLM的基础能力，特别是通用推理能力，而不是将LLM作为工具应用到特定领域；(2) 论文涉及多个正面指标，包括大语言模型、推理能力（数学和通用推理）以及多智能体系统；(3) 论文提出了一种通用的多LLM协作框架，用于增强LLM的通用问题解决能力，而非针对特定应用领域；(4) 实验结果表明，MoT在多个基准测试上超过了当前最先进的方法，有效提升了LLM的推理能力。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解决如何有效整合多个异构大型语言模型的问题。针对多个专业化的开源LLMs，我们提出了一种Mixture of Thoughts (MoT)方法，通过潜在空间层面的专家协作而非仅聚合输出，并在五个分布内和三个分布外基准测试上通过准确率指标验证了其有效性，超越了当前最先进方法。",
    "summary_translation": "开源大型语言模型（Large Language Models, LLMs）正日益按领域（如数学、代码、通用推理）专业化，这促使了利用不同模型间互补优势的系统的发展。先前的多LLM方法要么（i）将查询路由（route）到一个或几个专家模型并独立生成结果，（ii）通过昂贵的多轮交换聚合（aggregate）每个模型的输出，或者（iii）将权重融合（fuse）到单个模型中——通常需要架构同质性（architectural homogeneity）。我们提出了思维混合（Mixture of Thoughts, MoT），一种在全球路由（global routing）方案下实现异构专家（heterogeneous experts）之间潜在层面（latent-level）协作的简单方法。对于每个查询，一个轻量级路由器（lightweight router）选择前K个专家并指定一个主要专家；均匀放置的交互层（interaction layers）将隐藏状态（hidden states）投影到共享潜在空间（shared latent space），其中主要专家对其活跃（被选中的）同行执行交叉注意力（cross-attention）操作。预训练的专家模型保持冻结（frozen）状态；只有路由器和轻量级交互层通过一种新颖的联合训练目标（joint training objective）进行训练，该目标同时改进专家选择和专家间协作。在五个分布内（in-distribution, ID）和三个分布外（out-of-distribution, OOD）基准测试中，MoT分别以$+0.38\\%$和$+2.92\\%$的优势超越了当前基于路由和聚合的最先进方法（state-of-the-art）Avengers。此外，MoT显著优于表现最佳的单个模型。它通过单次推理（single-pass inference）实现这一目标，运行时间与路由基线（routing baselines）相当，且没有迭代聚合（iterative aggregation）的开销。MoT提供了一种简单的潜在空间机制来组合异构LLMs，这是朝着更广泛的多LLM协作迈出的实用一步。我们的代码在https://github.com/jacobfa/mot公开可用。",
    "summary_generated_time": "2025-09-26 21:17:57",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#83",
    "title": "Theoretical Bounds for Stable In-Context Learning",
    "link": "/arxiv/2509.20677",
    "arxiv_id": "2509.20677",
    "authors": "Tongxi Wang, Zhuoyang Xia",
    "summary": "In-context learning (ICL) is flexible but its reliability is highly sensitive to prompt length. This paper establishes a non-asymptotic lower bound that links the minimal number of demonstrations to ICL stability under fixed high-dimensional sub-Gaussian representations. The bound gives explicit sufficient conditions in terms of spectral properties of the covariance, providing a computable criterion for practice. Building on this analysis, we propose a two-stage observable estimator with a one-shot calibration that produces practitioner-ready prompt-length estimates without distributional priors. Experiments across diverse datasets, encoders, and generators show close alignment between the predicted thresholds and empirical knee-points, with the theory acting as a conservative but reliable upper bound; the calibrated variant further tightens this gap. These results connect spectral coverage to stable ICL, bridge theory and deployment, and improve the interpretability and reliability of large-scale prompting in realistic finite-sample regimes.",
    "subjects": "Machine Learning",
    "date": "2025-09-25",
    "category": "cs.LG",
    "crawl_time": "2025-09-26T21:01:55.418330",
    "filter_reason": "这篇论文的核心贡献是研究大语言模型的上下文学习(ICL)能力的稳定性问题，建立了理论界限并提出了估计提示长度的方法。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，这篇论文研究的是LLM的一个基础能力——上下文学习(ICL)的稳定性，而非将LLM作为工具应用到特定领域。ICL是LLM的核心能力之一，论文通过建立非渐近下界和提出两阶段可观测估计器，旨在提高这一基础能力的可靠性和稳定性，这属于改进LLM基础能力的研究范畴。 其次，论文涉及LLM的核心概念(ICL)，虽然未直接提及reasoning、planning等能力方向，但ICL本身与这些通用能力密切相关，因为它是模型适应新任务并进行推理的基础机制。 第三，论文不涉及任何需要排除的领域：没有关注多模态与视觉问题，没有将LLM应用到医疗、化学等特定领域，也没有从应用层面研究模型可靠性问题（如水印、安全等）。 最后，虽然论文关注了ICL的可靠性，但这是从理论角度研究LLM基础能力的稳定性，而非应用层面的可靠性问题，因此不应被排除。 综上所述，这篇论文致力于提高LLM本身的通用推理能力（通过增强ICL稳定性），符合研究目标。",
    "summary2": "",
    "summary_translation": "In-context learning (ICL, 上下文学习) 具有灵活性，但其可靠性对提示长度高度敏感。本文建立了一个非渐近下界（non-asymptotic lower bound），该下界在固定的高维次高斯表示（sub-Gaussian representations）下，将最少的示例数量（demonstrations）与ICL稳定性联系起来。该下界根据协方差（covariance）的光谱性质（spectral properties）给出了明确的充分条件，为实践提供了可计算的标准。基于此分析，我们提出了一个带有一次性校准（one-shot calibration）的两阶段可观测估计器（two-stage observable estimator），该估计器无需分布先验（distributional priors）即可生成可供实践者使用的提示长度估计。在不同数据集、编码器（encoders）和生成器（generators）上的实验表明，预测阈值与经验拐点（empirical knee-points）之间存在密切的一致性，该理论作为一个保守但可靠的上界；校准变体进一步缩小了这一差距。这些结果将光谱覆盖（spectral coverage）与稳定的ICL联系起来，弥合了理论与部署之间的差距，并提高了在现实有限样本条件（finite-sample regimes）下大规模提示的可解释性和可靠性。",
    "summary_generated_time": "2025-09-26 21:17:50",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#88",
    "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning",
    "link": "/arxiv/2509.20616",
    "arxiv_id": "2509.20616",
    "authors": "Hanjiang Hu, Changliu Liu, Na Li, Yebin Wang",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in higher multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks with over 30 steps. We also theoretically and empirically validate the strong cross-task generalizability that the models trained on complex tasks can lead to the successful completion of all simpler subtasks.",
    "subjects": "Machine Learning, Systems and Control",
    "date": "2025-09-24",
    "category": "cs.LG",
    "crawl_time": "2025-09-26T21:01:55.419365",
    "filter_reason": "这篇论文完全符合研究目标。从核心判断来看，论文的本质是关于改进LLM的基础能力，特别是提出了一种新的训练范式（单轮强化学习）来增强LLM的任务规划和多步推理能力。论文的核心贡献是将复杂的多轮任务规划转化为单轮任务推理问题，并通过Group Relative Policy Optimization (GRPO)进行高效策略优化，这直接提升了LLM的通用推理能力。 从正面指标看，论文明确包含了多个关键主题：Large Language Models (LLMs)、reasoning、task planning、reinforcement learning以及LLM agents，这些都与研究目标高度一致。 从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容，因此不应被排除。 在特殊和模糊情况处理方面，论文提出的是一种通用的智能体框架来增强LLM的任务规划能力，而不是将智能体应用于特定领域，因此符合保留标准。 综上所述，这篇论文直接致力于提高大语言模型本身的通用推理能力，特别是在任务规划和多步推理方面，与研究目标完全一致。",
    "summary2": "本文旨在解决LLM智能体在复杂多轮任务规划中面临的稀疏奖励、信用分配和计算开销问题。针对多轮任务规划场景，我们提出了一种将多轮任务规划转化为单轮任务推理问题，并通过GRPO进行策略优化的方法，并在Robotouille benchmark上通过成功率(SR)、平均步数(ASAT/ASST)验证了其有效性。",
    "summary_translation": "大型语言模型（Large Language Models, LLMs）在知识获取、推理和工具使用方面已展现出卓越能力，使其成为自主代理（autonomous agent）应用的有力候选者。然而，针对复杂多轮任务规划（multi-turn task planning）训练LLM代理面临重大挑战，包括稀疏的逐级奖励（episode-wise rewards）、长跨度信用分配（credit assignment across long horizons），以及多轮交互设置中强化学习（reinforcement learning）的计算开销。为此，本文提出了一种新方法，将多轮任务规划转化为单轮任务推理（single-turn task reasoning）问题，通过群体相对策略优化（Group Relative Policy Optimization, GRPO）实现高效策略优化，该方法利用来自专家轨迹的密集且可验证的奖励（dense and verifiable reward）。我们的理论分析表明，GRPO对单轮任务推理的改进能够在最小轮次下实现更高的多轮成功概率，以及对较短跨度子任务（subtasks with shorter horizons）的泛化能力。在复杂任务规划基准测试（complex task planning benchmark）上的实验评估表明，我们使用单轮GRPO训练的15亿参数模型相比高达140亿参数的更大基线模型（baseline models）取得了更优性能，在超过30步的长跨度规划任务（long-horizon planning tasks）中成功率达到70%。我们还从理论和实证上验证了强大的跨任务泛化能力（cross-task generalizability），即在复杂任务上训练的模型能够成功完成所有更简单的子任务。",
    "summary_generated_time": "2025-09-26 21:18:02",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#135",
    "title": "RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training",
    "link": "/arxiv/2509.21009",
    "arxiv_id": "2509.21009",
    "authors": "Wei Gao, Yuheng Zhao, Dakai An, Tianyuan Wu, Lunxi Cao, Shaopan Xiong, Ju Huang, Weixun Wang, Siran Yang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang",
    "summary": "Reinforcement Learning (RL) is a pivotal post-training technique for enhancing the reasoning capabilities of Large Language Models (LLMs). However, synchronous RL post-training often suffers from significant GPU underutilization, referred to as bubbles, caused by imbalanced response lengths within rollout steps. Many RL systems attempt to alleviate this problem by relaxing synchronization, but this can compromise training accuracy. In this paper, we introduce tail batching, a novel rollout scheduling strategy for synchronous RL that systematically consolidates prompts leading to long-tail responses into a small subset of rollout steps (long rounds), while ensuring that the majority of steps (short rounds) involve only balanced, short rollouts. By excluding long responses from short rounds and rescheduling them into a few designated long rounds, tail batching effectively reduces GPU idle time during rollouts and significantly accelerates RL training without sacrificing accuracy. We present RollPacker, a system that fully harnesses the benefits of tail batching through holistic optimizations across all three RL stages: elastic parallelism adaptation for rollout, dynamic resource allocation and scheduling for reward, and stream-based training. Empirical results show that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5 family of LLMs on up to 128 H800 GPUs.",
    "subjects": "Distributed, Parallel, and Cluster Computing, Machine Learning",
    "date": "2025-09-25",
    "category": "cs.LG",
    "crawl_time": "2025-09-26T21:01:55.428818",
    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"tail batching\"的新型rollout调度策略和RollPacker系统，用于优化强化学习(RL)作为大语言模型(LLM)后训练技术的效率和性能。论文明确指出RL是\"enhancing the reasoning capabilities of Large Language Models (LLMs)\"的关键技术，这与研究目标\"提高大语言模型（LLM）本身的『通用推理能力』\"直接相关。论文不是将LLM作为工具应用到特定领域，而是专注于改进LLM的基础训练方法，特别是强化学习这一提升LLM推理能力的关键技术。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。因此，这篇论文完全符合研究范围。",
    "summary2": "本文旨在解决同步强化学习后训练中由于响应长度不平衡导致的GPU利用率不足问题。针对大型语言模型的RL后训练场景，我们提出了一种tail batching调度策略和RollPacker系统，通过将长尾响应整合到专门的rollout步骤中，并在rollout、reward和训练三个阶段进行系统优化。在Qwen2.5系列模型和多达128个H800 GPU的实验环境中，通过端到端训练时间指标验证了其有效性，实现了最高2.56倍的训练加速。",
    "summary_translation": "强化学习（Reinforcement Learning, RL）是一种关键的后续训练技术，用于增强大语言模型（Large Language Models, LLMs）的推理能力。然而，同步RL后续训练常常遭受严重的GPU利用率不足问题，这种现象被称为\"气泡\"（bubbles），是由rollout（展开）步骤中不平衡的响应长度所导致的。许多RL系统试图通过放松同步来缓解这一问题，但这可能会损害训练准确性。在本文中，我们提出了tail batching（尾部批处理），一种新颖的同步RL rollout调度策略，该策略系统性地将导致长尾响应的提示整合到一小部分rollout步骤（长轮次）中，同时确保大多数步骤（短轮次）仅涉及平衡的、简短的rollout。通过将长响应从短轮次中排除并重新调度到少数指定的长轮次中，tail batching有效减少了rollout过程中的GPU空闲时间，并在不牺牲准确性的前提下显著加速了RL训练。我们提出了RollPacker系统，该系统通过在所有三个RL阶段的全面优化来充分利用tail batching的优势：rollout的弹性并行适应、奖励的动态资源分配与调度，以及基于流的训练。实证结果表明，对于Qwen2.5系列的LLMs，在多达128个H800 GPU上，RollPacker相比veRL实现了2.03倍至2.56倍的端到端训练时间减少，相比RLHFuse实现了高达2.24倍的加速。",
    "summary_generated_time": "2025-09-26 21:18:12",
    "summary_model": "z-ai/glm-4.5"
  }
]