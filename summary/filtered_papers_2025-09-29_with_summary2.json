[
  {
    "index": "#8",
    "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning",
    "link": "/arxiv/2509.22601",
    "arxiv_id": "2509.22601",
    "authors": "Yulei Qin, Xiaoyu Tan, Zhengbao He, Gang Li, Haojia Lin, Zongyi Li, Zihan Xu, Yuchen Shi, Siqi Cai, Renting Rui, Shaofei Cai, Yuzheng Cai, Xuan Zhang, Sheng Ye, Ke Li, Xing Sun",
    "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.",
    "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Multiagent Systems",
    "date": "2025-09-26",
    "category": "cs.MA",
    "crawl_time": "2025-10-06T19:08:32.908902",
    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式(SPEAR)，用于增强LLM的基础能力。论文关注的是如何通过强化学习和自模仿学习来提高LLM的战略工具使用能力和长期规划能力，这属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划、多步推理等通用能力\"的范畴，而非将LLM作为工具应用到特定领域。 其次，从正面指标来看，论文高度相关： - 核心概念：明确讨论LLM在智能体任务中的应用 - 能力方向：关注\"strategic tool use capabilities\"和\"long-horizon, sparsely-rewarded agent tasks\"，涉及规划和问题解决能力 - 训练方法：核心是强化学习和自模仿学习(self-imitation learning) - 新兴范式：聚焦\"agentic LLMs\"和\"tool use capabilities\" 第三，论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在处理特殊和模糊情况时，论文提出的是一种通用的智能体训练框架，用于增强LLM的通用工具使用能力和战略决策能力，而非针对特定领域的应用。 综上所述，这篇论文的核心贡献是提出了一种新的训练方法来增强LLM的通用推理能力，特别是长期规划和工具使用能力，完全符合研究目标。",
    "summary2": "本文旨在解决LLM智能体在长期、稀疏奖励任务中强化学习面临的探索-利用平衡问题。针对多轮工具交互场景，我们提出了一种SPEAR方法，结合课程自模仿学习和内在奖励塑造，实现从技能级到动作级的渐进式探索。在ALFWorld、WebShop和AIME24/25等基准测试上，通过成功率和准确率等指标验证了其有效性，显著提升了GRPO/GiGPO/Dr.BoT等基线性能，最高提升20.7%。",
    "inspiration_trace": "## 面临的挑战\n作者识别到LLM智能体在长时程、稀疏奖励任务中面临探索-利用平衡的根本挑战。传统基于熵最大化的探索方法在多轮工具交互中导致分布偏移，引发训练不稳定，表现为熵崩溃或无限制发散。\n\n## 关键洞察\n作者独特地认为策略熵应作为渐进式指导，而非机械最大化。训练早期应增加熵以促进技能级探索，后期则应收敛熵以实现行动级探索，实现从广泛技能获取到精细化策略优化的自然过渡。\n\n## 解决方案演进\n从这一洞察出发，作者将自我模仿学习(SIL)与课程学习相结合，通过内在奖励塑造早期探索，逐步强化对成功轨迹的自我模仿。引入优势重新校准和协方差裁剪正则化，防止过度自信，确保熵在可控范围内动态演化。\n\n## 创新点总结\n创新性地将策略熵视为渐进式指导信号，通过课程式自我模仿学习实现探索-利用的自然过渡，避免了传统方法的极端情况，为智能体RL提供了更稳定有效的训练范式。",
    "summary_translation": "强化学习（Reinforcement learning, RL）是提升大型语言模型（LLMs）在长时程、稀疏奖励代理任务中策略工具使用能力的主流范式，然而它面临着探索-利用权衡（exploration-exploitation trade-off）的根本挑战。现有研究通过策略熵（policy entropy）的视角促进探索，但由于多轮分布变化（multi-turn distribution shifting），这种机械的熵最大化容易导致强化学习训练不稳定。在本文中，我们的目标是在代理自身经验的指导下实现渐进的探索-利用平衡，同时避免陷入熵崩溃（entropy collapsing）或失控发散（runaway divergence）。我们提出了SPEAR，一种基于课程的自模仿学习（self-imitation learning, SIL）方法，用于训练代理型大型语言模型（agentic LLMs）。它扩展了原始SIL框架（在原始框架中，经验回放缓冲区存储自生成的有前景轨迹以进行离策略更新），通过在各阶段内将策略演化引导在良好平衡的熵范围内。具体而言，我们的方法引入课程（curriculum）来管理探索过程，利用内在奖励（intrinsic rewards）促进技能级探索，并通过自模仿学习促进动作级探索。起初，辅助工具调用奖励（auxiliary tool call reward）在工具使用技能积累中扮演关键角色，使代理能够广泛接触环境反馈的陌生分布，并呈现熵上升趋势。随着训练的推进，自模仿得到加强，利用重放经验中的现有成功模式进行对比性动作级探索，从而加速解决方案迭代，同时避免无限制的熵增长。为了进一步稳定训练，我们重新校准经验回放缓冲区中经验的优势（advantages），以解决潜在的政策漂移（policy drift）问题。我们引入了正则化方法（regularizations），如裁剪概率与优势之间高协方差的token（tokens），用于轨迹级熵控制，以抑制过度自信（over-confidence）。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#6",
    "title": "RobustFlow: Towards Robust Agentic Workflow Generation",
    "link": "/arxiv/2509.21834",
    "arxiv_id": "2509.21834",
    "authors": "Shengxiang Xu, Jiayi Zhang, Shimin Di, Yuyu Luo, Liang Yao, Hanmo Liu, Jia Zhu, Fan Liu, Min-Ling Zhang",
    "summary": "The automated generation of agentic workflows is a promising frontier for enabling large language models (LLMs) to solve complex tasks. However, our investigation reveals that the robustness of agentic workflow remains a critical, unaddressed challenge. Current methods often generate wildly inconsistent workflows when provided with instructions that are semantically identical but differently phrased. This brittleness severely undermines their reliability and trustworthiness for real-world applications. To quantitatively diagnose this instability, we propose metrics based on nodal and topological similarity to evaluate workflow consistency against common semantic variations such as paraphrasing and noise injection. Subsequently, we further propose a novel training framework, RobustFlow, that leverages preference optimization to teach models invariance to instruction variations. By training on sets of synonymous task descriptions, RobustFlow boosts workflow robustness scores to 70\\% - 90\\%, which is a substantial improvement over existing approaches. The code is publicly available at https://github.com/DEFENSE-SEU/RobustFlow.",
    "subjects": "Multiagent Systems",
    "date": "2025-09-26",
    "category": "cs.MA",
    "crawl_time": "2025-10-06T19:08:32.907758",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础能力，特别是增强其生成智能体工作流的鲁棒性。论文提出了RobustFlow这一新的训练框架，通过偏好优化来教导模型对指令变化的不变性，这属于提升LLM通用推理能力的研究，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标： - 核心概念：明确关注大语言模型(LLMs) - 能力方向：涉及problem-solving（解决复杂任务）和planning（工作流生成本质上是一种规划能力） - 训练方法：使用了preference optimization（偏好优化），与强化学习方法相关 - 新兴范式：聚焦于agentic workflows（智能体工作流），属于llm-based agents的研究范畴 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对任何特定应用领域（如医疗、化学等） - 虽然关注robustness，但这是从模型生成工作流的一致性角度，而非应用层面的水印、安全或安保问题 在特殊和模糊情况处理上，论文提出的是通用的智能体工作流生成方法，旨在增强LLM解决复杂任务的通用能力，而非应用于特定领域。同时，论文关注的工作流生成鲁棒性问题可以视为提升模型推理质量和可靠性的方法。 综上所述，这篇论文的核心贡献是提出了一种新的训练框架来增强LLM生成智能体工作流的鲁棒性，直接提升了LLM的规划和问题解决等通用推理能力，完全符合研究目标。",
    "summary2": "本文旨在解决自动生成agentic工作流在面对语义相同但表述不同的指令时缺乏鲁棒性的问题。针对语义变体指令场景，我们提出了一种RobustFlow框架，利用偏好优化来教导模型对指令变化的不变性，并在包含31,889个工作流的数据集上通过节点和拓扑相似性指标验证了其有效性，将鲁棒性分数提高到70%-90%。",
    "inspiration_trace": "## 面临的挑战\n现有智能体工作流生成方法在面对语义相同但表述不同的指令时，会产生高度不一致的工作流，稳定性仅40%-70%，严重影响了系统可靠性和实际应用价值。\n\n## 关键洞察\n这种不稳定性并非简单的随机性产物（即使将LLM采样温度降至零问题仍存在），而是模型未能实现语义不变性的深层失败。需要从语义层面处理指令变体，确保模型理解不同表述背后的相同任务需求。\n\n## 解决方案演进\n首先建立结构感知评估方法，通过节点和拓扑相似性指标量化工作流一致性；接着构建包含31,889个工作流的数据集，来自1,255个任务描述的语义变体；最后提出RobustFlow框架，利用偏好优化训练模型对指令变体的不变性，通过同义任务描述集合训练，使模型生成一致规范的工作流。\n\n## 创新点总结\n首次系统研究智能体工作流生成鲁棒性问题；提出结构感知评估指标定量分析一致性；创新应用偏好优化于工作流生成，通过自我一致性偏好优化训练语义不变性；在保持性能同时显著提升鲁棒性至70%-90%。",
    "summary_translation": "智能体工作流（agentic workflows）的自动生成是使大型语言模型（large language models, LLMs）能够解决复杂任务的一个有前景的前沿领域。然而，我们的研究表明，智能体工作流的鲁棒性（robustness）仍然是一个关键且尚未解决的挑战。当前方法在提供语义相同但表述不同的指令时，常常生成高度不一致的工作流。这种脆弱性（brittleness）严重削弱了它们在现实世界应用中的可靠性和可信度。为了定量诊断这种不稳定性，我们提出了基于节点和拓扑相似性（nodal and topological similarity）的指标，以评估工作流在常见语义变化（如释义（paraphrasing）和噪声注入（noise injection））下的一致性。随后，我们进一步提出了一个新颖的训练框架 RobustFlow，该框架利用偏好优化（preference optimization）来教导模型对指令变化的不变性（invariance）。通过对同义任务描述集进行训练，RobustFlow 将工作流鲁棒性分数提升至 70\\% - 90\\%，相比现有方法有了显著改进。代码公开可在 https://github.com/DEFENSE-SEU/RobustFlow 获取。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#12",
    "title": "Reimagining Agent-based Modeling with Large Language Model Agents via Shachi",
    "link": "/arxiv/2509.21862",
    "arxiv_id": "2509.21862",
    "authors": "So Kuroki, Yingtao Tian, Kou Misaki, Takashi Ikegami, Takuya Akiba, Yujin Tang",
    "summary": "The study of emergent behaviors in large language model (LLM)-driven multi-agent systems is a critical research challenge, yet progress is limited by a lack of principled methodologies for controlled experimentation. To address this, we introduce Shachi, a formal methodology and modular framework that decomposes an agent's policy into core cognitive components: Configuration for intrinsic traits, Memory for contextual persistence, and Tools for expanded capabilities, all orchestrated by an LLM reasoning engine. This principled architecture moves beyond brittle, ad-hoc agent designs and enables the systematic analysis of how specific architectural choices influence collective behavior. We validate our methodology on a comprehensive 10-task benchmark and demonstrate its power through novel scientific inquiries. Critically, we establish the external validity of our approach by modeling a real-world U.S. tariff shock, showing that agent behaviors align with observed market reactions only when their cognitive architecture is appropriately configured with memory and tools. Our work provides a rigorous, open-source foundation for building and evaluating LLM agents, aimed at fostering more cumulative and scientifically grounded research.",
    "subjects": "Artificial Intelligence, Multiagent Systems, Social and Information Networks, General Economics",
    "date": "2025-09-26",
    "category": "cs.MA",
    "crawl_time": "2025-10-06T19:08:32.915965",
    "filter_reason": "这篇论文的核心贡献是提出Shachi，一种形式化方法和模块化框架，用于构建和评估LLM智能体。论文将智能体的策略分解为核心认知组件：配置、内存和工具，由LLM推理引擎协调，旨在分析特定架构选择如何影响集体行为。这直接符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。论文关注LLM驱动的多智能体系统和工具使用，这些都是增强LLM通用推理能力的重要方向。虽然论文提到了一个现实世界的应用案例（美国关税冲击），但这只是为了验证其方法论的有效性，而不是论文的核心焦点。论文的核心是提出一种通用的智能体架构框架，而不是专注于特定应用领域，因此不应被排除。",
    "summary2": "本文旨在解决基于LLM的多智能体系统缺乏原则性方法论的问题。针对复杂系统中的涌现行为研究，我们提出了Shachi方法论，将智能体策略分解为Configuration、Memory和Tools核心认知组件，并在10个任务的benchmark上通过复现先前研究和模拟真实世界经济事件验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n当前基于LLM的Agent-based Modeling研究缺乏原则性方法论，导致研究碎片化、难以复现和比较。Agent与环境交互接口不兼容，内部架构分散不一致，且缺乏对复杂现实世界现象的验证。\n\n## 关键洞察\n作者认识到需要将Agent内部架构与环境解耦，通过标准化接口实现模块化设计。独特视角是将认知科学原理与ABM结合，认为Agent应模拟人类认知的核心组成部分，而非仅作为黑盒决策器。\n\n## 解决方案演进\n首先设计标准化接口实现Agent与环境解耦；然后从认知科学汲取灵感，将Agent策略分解为四个核心组件：LLM推理引擎、Configs内在特质、Memory上下文持久性和Tools扩展能力；最后建立分层基准测试套件，逐步增加社会复杂性以系统评估Agent架构。\n\n## 创新点总结\n首次提出基于LLM的ABM原则性方法论，将Agent设计从ad-hoc转向模块化标准化；通过认知架构分解实现设计选择的系统分析；建立分层评估框架提供标准化测试平台；通过模拟现实事件验证方法的外部有效性。",
    "summary_translation": "大型语言模型(LLM)驱动的多智能体系统中的涌现行为研究是一项关键的研究挑战，然而由于缺乏用于受控实验的原则性方法论，研究进展受到限制。为解决这一问题，我们介绍了Shachi，这是一种形式化方法论和模块化框架，它将智能体的策略分解为核心认知组件：用于内在特质的Configuration(配置)、用于上下文持久性的Memory(记忆)和用于扩展能力的Tools(工具)，所有这些都由LLM推理引擎协调。这种原则性架构超越了脆弱的、临时性的智能体设计，使得能够系统性分析特定架构选择如何影响集体行为。我们在一个全面的10任务基准测试上验证了我们的方法论，并通过新颖的科学探究展示了其强大功能。关键的是，我们通过对现实世界美国关税冲击(tariff shock)进行建模，确立了我们的方法的外部有效性，表明只有当智能体的认知架构适当配置了记忆和工具时，其行为才与观察到的市场反应一致。我们的工作为构建和评估LLM智能体提供了一个严格的开源基础，旨在促进更具累积性和科学基础的研究。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#11",
    "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration",
    "link": "/arxiv/2509.21981",
    "arxiv_id": "2509.21981",
    "authors": "Zhimin Wang, Shaokang He, Duo Wu, Jinghe Wang, Linjia Kang, Jing Yu, Zhi Wang",
    "summary": "Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs) have emerged as promising autonomous agents for collaborative task solving. However, existing collaboration frameworks for LLMs overlook their reasoning potential for dynamic intent inference, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. To bridge this gap, we propose CoBel-World, a novel framework that equips LLM agents with a collaborative belief world -- an internal representation jointly modeling the physical environment and collaborators' mental states. CoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World significantly reduces communication costs by 22-60% and improves task completion efficiency by 4-28% compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.",
    "subjects": "Artificial Intelligence, Multiagent Systems",
    "date": "2025-09-26",
    "category": "cs.MA",
    "crawl_time": "2025-10-06T19:08:32.915450",
    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文的核心是提出CoBel-World框架，通过利用LLM的推理能力构建协作信念世界，以优化多智能体协作。这本质上是改进LLM的基础推理能力，特别是关于理解他人意图和避免协调失误的推理能力，属于增强LLM通用推理能力的研究，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确以Large language models (LLMs)为研究对象 - 能力方向：重点研究reasoning能力（特别是关于协作者意图的推理），以及planning和problem-solving能力 - 新兴范式：涉及llm-based agents和multi-agent系统 第三步排除标准：论文不主要聚焦于任何排除领域： - 虽然提到\"embodied benchmarks\"，但这只是作为评估平台，而非研究焦点 - 不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究 第四步特殊和模糊情况处理： - 论文提出的是通用的智能体协作框架(CoBel-World)，用于增强LLM在多智能体环境中的通用推理能力，而非应用于特定领域的智能体 - 框架的核心是通过符号信念语言和贝叶斯式信念更新来提升LLM的推理质量，这直接关联到提升模型的通用推理能力 论文的核心贡献是提出了一种新方法来增强LLM在多智能体协作中的推理能力，特别是关于理解他人意图的推理，这直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
    "summary2": "本文旨在解决基于LLM的多智能体协作中的沟通冗余和规划不一致问题。针对部分可观察环境下的具身多智能体协作场景，我们提出了一种CoBel-World框架，通过构建协作信念世界来优化多智能体协作。在TDW-MAT和C-WAH具身基准测试上，通过通信成本和任务完成效率等指标验证了其有效性，结果显示通信成本降低22-60%，任务完成效率提升4-28%。",
    "inspiration_trace": "## 面临的挑战\n现有LLM多智能体协作框架忽视了动态意图推理潜力，导致在部分可观察环境中产生不一致计划和冗余通信，降低协作效率。这些方法依赖固定通信协议，缺乏动态识别协调失调和自适应通信的能力。\n\n## 关键洞察\n作者认为根本缺陷在于缺乏\"信念建模\"。在多智能体系统中，信念是对外部环境和协作者心理状态的内部表示。准确的信念估计使智能体能选择性地交流有价值信息，实现高效通信和一致协作。\n\n## 解决方案演进\n作者首先认识到传统信念建模难以直接应用于LLM智能体，面临开放环境形式化和零样本构建两大挑战。为此提出\"协作信念世界\"，演进为两个核心组件：符号信念表示（形式化任务设置）和贝叶斯信念协作（通过LLM推理实现零样本信念更新）。\n\n## 创新点总结\n首次将结构化信念建模引入LLM多智能体系统，利用LLM推理能力实现零样本贝叶斯式更新，无需大量训练数据。基于信念的自适应协作机制使智能体能动态决定通信时机，显著减少冗余并提高效率。",
    "summary_translation": "有效的现实世界多智能体（multi-agent）协作不仅需要精确的规划，还需要推理协作者意图的能力——这是在部分可观察环境（partial observable environments）下避免协调不当（miscoordination）和冗余通信的关键能力。凭借其强大的规划和推理能力，大型语言模型（large language models, LLMs）已成为协作任务解决的有前景的自主智能体（autonomous agents）。然而，现有的LLM协作框架忽视了它们在动态意图推理（dynamic intent inference）方面的潜力，因此产生不一致的规划和冗余通信，降低了协作效率。为了弥补这一差距，我们提出了CoBel-World，这是一个新颖的框架，为LLM智能体配备了协作信念世界（collaborative belief world）——一种共同建模物理环境和协作者心理状态的内部表征。CoBel-World使智能体能够通过符号信念语言（symbolic belief language）将开放世界任务知识解析为结构化信念，并通过LLM推理执行零样本（zero-shot）贝叶斯式信念更新。这使得智能体能够主动检测潜在的协调不当（例如，冲突的计划）并进行适应性通信。在具有挑战性的具身基准测试（embodied benchmarks）（即TDW-MAT和C-WAH）上评估，与最强的基线相比，CoBel-World将通信成本显著降低了22-60%，并将任务完成效率提高了4-28%。我们的研究结果表明，明确的、感知意图的信念建模（intent-aware belief modeling）对于基于LLM的多智能体系统中的高效和类人协作至关重要。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#16",
    "title": "Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective",
    "link": "/arxiv/2509.21613",
    "arxiv_id": "2509.21613",
    "authors": "Lingxiao Kong, Cong Yang, Oya Deniz Beyan, Zeyd Boukhers",
    "summary": "Multi-Objective Reinforcement Learning (MORL) presents significant challenges and opportunities for optimizing multiple objectives in Large Language Models (LLMs). We introduce a MORL taxonomy and examine the advantages and limitations of various MORL methods when applied to LLM optimization, identifying the need for efficient and flexible approaches that accommodate personalization functionality and inherent complexities in LLMs and RL. We propose a vision for a MORL benchmarking framework that addresses the effects of different methods on diverse objective relationships. As future research directions, we focus on meta-policy MORL development that can improve efficiency and flexibility through its bi-level learning paradigm, highlighting key research questions and potential solutions for improving LLM performance.",
    "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Multiagent Systems",
    "date": "2025-09-25",
    "category": "cs.MA",
    "crawl_time": "2025-10-06T19:08:32.917689",
    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的本质是关于使用多目标强化学习(MORL)来优化大语言模型(LLM)的性能。论文提出了一种新的训练/优化范式，旨在改进LLM的基础能力，而不是将LLM作为工具应用到特定领域。论文关注的是LLM本身的优化方法，属于改进LLM基础能力和提出新训练范式的研究，因此应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念：明确关注大语言模型(LLMs)的优化 - 训练方法：详细讨论了多目标强化学习(MORL)作为LLM的优化方法 虽然摘要中没有直接提到reasoning、planning等能力方向，但论文目标是\"改进LLM性能\"，这通常包括提升这些通用推理能力。 第三步：排除标准 论文不涉及任何排除标准中提到的领域： - 没有涉及多模态与视觉相关内容 - 没有将LLM应用到特定领域（如医疗、化学等） - 没有主要关注模型可靠性方面的应用问题（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况。 最终决策 综合以上分析，这篇论文的核心贡献是提出使用多目标强化学习来优化大语言模型的方法论，旨在改进LLM的基础性能，这符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文关注的是LLM本身的训练和优化范式，而不是将其作为工具应用到特定领域，因此应该被保留。",
    "summary2": "本文旨在解决大型语言模型(LLM)中多目标优化问题。针对多个生成特性需同时优化的场景，我们提出了一种基于多目标强化学习(MORL)的分类法和基准测试框架，重点探讨元策略(meta-policy)方法。通过提出的评估指标体系验证了不同MORL方法在LLM优化中的有效性。",
    "inspiration_trace": "## 面临的挑战\nLLM优化需同时处理多个目标(如反思性和流畅性)，但现有MORL方法存在效率低、灵活性差问题：单策略方法需重新训练适应不同偏好；多策略方法计算成本高；元策略方法性能受限且无法近似帕累托最优。\n\n## 关键洞察\n作者认识到用户偏好动态变化，目标间关系(竞争或相关)影响优化程度。元策略方法通过双层学习范式可解决效率和灵活性问题，且LLM的序列特性要求保持模型间连接，而非简单聚合参数或logits。\n\n## 解决方案演进\n从分析现有方法局限→尝试不同聚合策略→发现隐藏状态聚合优势→提出基于MoE的未来方向：下层训练专家网络，上层学习门控网络组合输出，并针对两个关键问题提出MGDA确保帕累托最优性和CMABs实现动态加权。\n\n## 创新点总结\n将元策略方法引入LLM优化领域，提出系统性MORL基准框架，创新性地应用MoE架构解决多目标平衡问题，针对LLM序列特性强调模型连接重要性，为未来研究提供明确路径。",
    "summary_translation": "多目标强化学习(Multi-Objective Reinforcement Learning, MORL)在优化大型语言模型(Large Language Models, LLMs)的多个目标方面提出了重大挑战和机遇。我们引入了一种MORL分类法，并检验了各种MORL方法在应用于LLM优化时的优缺点，确定了需要能够适应个性化功能以及LLMs和强化学习(Reinforcement Learning, RL)中固有复杂性的高效灵活方法。我们提出了一个MORL基准测试框架的愿景，该框架解决了不同方法对多样化目标关系的影响。作为未来的研究方向，我们专注于元策略MORL(meta-policy MORL)的开发，它可以通过其双层学习范式(bi-level learning paradigm)提高效率和灵活性，强调了改进LLM性能的关键研究问题和潜在解决方案。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#4",
    "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards",
    "link": "/arxiv/2509.22638",
    "arxiv_id": "2509.22638",
    "authors": "Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, Tianyu Pang",
    "summary": "LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at https://github.com/sail-sg/feedback-conditional-policy.",
    "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.738944",
    "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，该论文的本质是提出一种新的训练范式——反馈条件策略(FCP)，让大语言模型能够直接从语言反馈中学习，而不是将反馈压缩为标量奖励。这明显属于改进LLM基础能力的研究，特别是强化学习优化的新方法，符合\"保留\"标准。 其次，从正面指标分析，论文明确关注大语言模型(LLMs)这一核心概念，并涉及强化学习(RL from human or AI feedback)这一训练方法。虽然论文没有直接提及推理、规划等能力，但改进LLM从反馈中学习的能力很可能间接提升这些通用推理能力。 第三，论文不符合任何排除标准。它没有涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。 最后，论文不涉及需要特殊判断的情况，如智能体/工具使用或幻觉/可解释性/安全等主题。 论文的核心贡献是提出了一种更丰富、更具表现力的方式让LLM直接从语言反馈中学习，这改变了传统的强化学习反馈机制，将反馈驱动的学习重新定义为条件生成而非奖励优化。这种方法有望提升LLM的通用学习能力和推理质量，因此完全符合\"提高大语言模型通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决传统RLHF方法将丰富语言反馈压缩为标量奖励导致信息丢失和奖励不平衡问题。针对语言模型从语言反馈中学习的场景，我们提出了一种反馈条件策略（FCP），将语言反馈视为条件信号而非标量奖励，通过离线最大似然训练和在线引导阶段直接从响应-反馈对中学习。在数学推理（Big-Math）和一般推理（WebInstruct）数据集上通过准确率等指标验证了FCP能有效匹配或超越基于标量的强基线方法（如RFT和GRPO），实现了无需标量奖励的反馈学习。",
    "inspiration_trace": "## 面临的挑战\n传统强化学习方法将丰富的语言反馈压缩成标量奖励，导致信息损失、反馈模糊性和跨任务奖励尺度不平衡三大问题。例如，不同类型的问题反馈可能被映射到相同奖励值，而混合、情绪化的反馈难以准确标量化。\n\n## 关键洞察\n作者认识到LLMs具有强大的语言先验和隐式理解反馈的能力，无需将反馈转化为标量。受文本到图像生成启发，语言模型能像组合未见过的文本提示一样，吸收多样化语言反馈并生成高质量响应。\n\n## 解决方案演进\n作者将反馈驱动学习重新定义为条件生成问题，提出反馈条件策略(FCP)。通过最小化前向KL散度，直接从响应-反馈对中学习反馈条件后验分布。采用两阶段训练：离线阶段初始化策略，在线阶段通过积极条件生成和新鲜反馈引导自我改进。\n\n## 创新点总结\n该方法绕过了奖励假设，保留了语言反馈的丰富性，能自然处理混合反馈，避免跨任务奖励不平衡，充分利用了LLMs的语言先验能力，为语言模型对齐提供了新范式。",
    "summary_translation": "LLMs（大型语言模型）通常通过来自人类或AI反馈的强化学习（RL）进行训练，然而这类方法通常将细致入微的反馈压缩为标量奖励，从而丢弃了反馈的丰富性并导致规模不平衡。我们提出将语言反馈（verbal feedback）视为一种条件信号（conditioning signal）。受文本到图像生成中的语言先验（language priors）启发，这种先验能够从未见过的提示中产生新颖输出，我们引入了反馈条件策略（feedback-conditional policy, FCP）。FCP直接从响应-反馈对中学习，通过在离线数据上进行最大似然训练来近似反馈条件后验（feedback-conditional posterior）。我们进一步开发了一种在线自举（online bootstrapping）阶段，在该阶段中，策略在积极条件下生成并接收新鲜反馈以自我完善。这将反馈驱动的学习重新构建为条件生成（conditional generation）而非奖励优化（reward optimization），为LLMs提供了一种更具表现力的方式来直接从语言反馈中学习。我们的代码可在 https://github.com/sail-sg/feedback-conditional-policy 获取。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#5",
    "title": "Variational Reasoning for Language Models",
    "link": "/arxiv/2509.22637",
    "arxiv_id": "2509.22637",
    "authors": "Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang",
    "summary": "We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at https://github.com/sail-sg/variational-reasoning.",
    "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.739464",
    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提出一种变分推理框架，将思维轨迹视为潜在变量并通过变分推断优化它们，这明显是关于改进LLM基础推理能力的研究，而非将LLM作为工具应用到特定领域。 论文符合多个正面指标：1) 核心概念方面，明确研究语言模型(LLMs)；2) 能力方向方面，直接聚焦于推理能力(reasoning)，并在摘要中多次提及\"reasoning ability\"和\"reasoning tasks\"；3) 训练方法方面，论文将变分推断与RL风格方法结合，提到了\"binary-reward RL, including GRPO\"等强化学习方法；4) 新兴范式方面，论文研究的\"thinking traces\"与思维链(CoT)等推理方法密切相关。 论文不符合任何排除标准：没有涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。论文也没有涉及需要特殊处理的模糊情况，如特定领域的智能体应用或应用层面的幻觉/可解释性/安全问题。 论文的核心贡献是提供了一个统一的概率视角，将变分推断与RL风格方法结合，为提高语言模型的推理能力提供了稳定的目标，这与研究目标\"提高大语言模型本身的通用推理能力\"高度一致。",
    "summary2": "本文旨在解决语言模型推理能力训练的稳定性与原则性问题。针对思维轨迹与答案生成的联合优化场景，我们提出了一种变分推理框架，将思维轨迹视为潜在变量并通过变分推断优化，扩展了ELBO到多轨迹目标并提出了前向KL公式。在Qwen 2.5和Qwen 3模型系列上，通过MATH500、AIME24&25、OlympiadBench、LiveCodeBench等多种推理任务的准确率指标验证了其有效性，显著提升了模型的推理性能。",
    "inspiration_trace": "## 面临的挑战\n现有语言模型推理能力训练方法存在根本局限：监督微调依赖精心策划的思维轨迹，成本高且泛化差；强化学习方法训练不稳定，输出多样性易崩溃，导致对困难问题的正确答案变得稀少。\n\n## 关键洞察\n作者将推理过程重新概念化为概率建模问题，将思维轨迹视为潜在变量。变分推断提供了一种优化正确答案对数似然的自然方式，通过可处理的下界替代不可行的思维轨迹边缘化，并引入变分后验采样更可能产生正确答案的思考路径。\n\n## 解决方案演进\n从基本证据下界(ELBO)出发，扩展到多轨迹目标以获得更紧致界限；提出前向KL公式稳定变分后验训练；进而发现拒绝采样微调和二元奖励RL可解释为局部前向KL目标，揭示这些方法中存在对简单问题的隐式偏差。\n\n## 创新点总结\n提供了统一变分推断与RL方法的原则性概率视角，揭示了现有方法中的隐式偏差，为理解主流方法提供了新理论框架，同时提出了稳定目标函数改进语言模型推理能力。",
    "summary_translation": "我们提出了一种针对语言模型的变分推理框架（variational reasoning framework），该框架将思维轨迹（thinking traces）视为潜在变量（latent variables），并通过变分推断（variational inference）对其进行优化。从证据下界（evidence lower bound, ELBO）出发，我们将其扩展为多轨迹目标（multi-trace objective）以获得更紧致的界限，并提出一个前向KL公式（forward-KL formulation），该公式稳定了变分后验（variational posterior）的训练。我们进一步表明，拒绝采样微调（rejection sampling finetuning）和二元奖励强化学习（binary-reward RL），包括GRPO，可以被解释为局部前向KL目标（local forward-KL objectives），其中模型准确性的隐式权重自然地从推导过程中产生，并揭示了一个先前未被注意到的对简单问题的偏向。我们在Qwen 2.5和Qwen 3模型系列上，通过广泛的推理任务（reasoning tasks）实证验证（empirically validate）了我们的方法。总体而言，我们的工作提供了一个原则性的概率视角（probabilistic perspective），统一了变分推断与强化学习方法（RL-style methods），并产生了稳定的目标函数（stable objectives），用于提高语言模型的推理能力（reasoning ability）。我们的代码可在https://github.com/sail-sg/variational-reasoning获取。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#13",
    "title": "Think Socially via Cognitive Reasoning",
    "link": "/arxiv/2509.22546",
    "arxiv_id": "2509.22546",
    "authors": "Jinfeng Zhou, Zheyu Chen, Shuai Wang, Quanyu Dai, Zhenhua Dong, Hongning Wang, Minlie Huang",
    "summary": "LLMs trained for logical reasoning excel at step-by-step deduction to reach verifiable answers. However, this paradigm is ill-suited for navigating social situations, which induce an interpretive process of analyzing ambiguous cues that rarely yield a definitive outcome. To bridge this gap, we introduce Cognitive Reasoning, a paradigm modeled on human social cognition. It formulates the interpretive process into a structured cognitive flow of interconnected cognitive units (e.g., observation or attribution), which combine adaptively to enable effective social thinking and responses. We then propose CogFlow, a complete framework that instills this capability in LLMs. CogFlow first curates a dataset of cognitive flows by simulating the associative and progressive nature of human thought via tree-structured planning. After instilling the basic cognitive reasoning capability via supervised fine-tuning, CogFlow adopts reinforcement learning to enable the model to improve itself via trial and error, guided by a multi-objective reward that optimizes both cognitive flow and response quality. Extensive experiments show that CogFlow effectively enhances the social cognitive capabilities of LLMs, and even humans, leading to more effective social decision-making.",
    "subjects": "Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.789252",
    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Cognitive Reasoning\"（认知推理）的新范式和CogFlow框架，用于增强大语言模型在社交情境中的推理能力。从本质上看，论文属于改进LLM的基础推理能力，提出新的训练范式（结合监督微调和强化学习），而非将LLM作为工具应用到特定领域。论文明确研究LLMs的推理能力，并采用强化学习方法进行优化，这些都是研究目标的正面指标。虽然论文关注社交情境，但这应被视为一种通用推理能力，而非医疗、化学等特定应用领域。社交推理是人类普遍需要的基础能力，论文提出的认知推理范式具有通用性，通过树结构规划和强化学习来提升模型推理能力，因此该研究完全符合\"提高大语言模型通用推理能力\"的核心目标。",
    "summary2": "本文旨在解决大型语言模型在社交情境中推理能力不足的问题。针对社交情境中模糊线索分析的需求，我们提出了一种基于认知单元结构化流的Cognitive Reasoning范式和CogFlow框架，并在Reddit收集的5100个社交情境数据集上通过比较偏好排名和人类评估等指标验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\nLLMs在逻辑推理任务上表现出色，但这种范式不适合社交情境。社交情境涉及分析模糊线索，很少产生确定性答案，而当前LLMs的推理结构与社交智能本质不匹配，甚至会导致\"认知反刍\"问题。\n\n## 关键洞察\n作者认识到社交认知是人类社交智能的核心，是一个动态过程，可通过构建内部认知地图来驾驭复杂社交情境。人类社交认知可分解为六个核心认知单元：观察、归因、动机、调节、效能和行为，它们之间相互流动形成结构化推理过程。\n\n## 解决方案演进\n作者首先将社交认知形式化为\"认知推理\"，提出CogFlow框架。通过树状结构规划模拟人类思维，收集认知流数据集；然后通过有监督微调灌输基本认知推理能力；最后采用强化学习使模型通过试错自我改进，由多目标奖励同时优化认知流和响应质量。\n\n## 创新点总结\n首次提出专门用于社交思考的认知推理范式，通过认知单元结构化交互实现社交智能。创新性地结合基于偏好的SFT和多目标RL，不仅增强LLMs社交认知能力，还发现这种方法有潜力提升人类社交智能。",
    "summary_translation": "为逻辑推理训练的大型语言模型（LLMs，Large Language Models）擅长通过逐步演绎（step-by-step deduction）得出可验证的答案。然而，这种范式（paradigm）并不适合处理社交情境，社交情境引发一种解释过程（interpretive process），即分析很少产生明确结果的模糊线索（ambiguous cues）。为弥合这一差距，我们引入了认知推理（Cognitive Reasoning），这是一种模拟人类社交认知（human social cognition）的范式。它将解释过程构建为一个由相互关联的认知单元（cognitive units，如观察或归因）组成的结构化认知流（cognitive flow），这些认知单元自适应地结合，以实现有效的社交思维和响应。随后，我们提出了CogFlow，一个将这种能力灌输给LLMs的完整框架。CogFlow首先通过树状结构规划（tree-structured planning）模拟人类思维的联想性和渐进性，策划一个认知流数据集。在通过监督微调（supervised fine-tuning）灌输基本认知推理能力后，CogFlow采用强化学习（reinforcement learning）使模型通过试错（trial and error）自我改进，由优化认知流和响应质量的多目标奖励（multi-objective reward）指导。大量实验表明，CogFlow有效增强了LLMs甚至人类的社交认知能力（social cognitive capabilities），从而实现更有效的社交决策（social decision-making）。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#18",
    "title": "Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving",
    "link": "/arxiv/2509.22480",
    "arxiv_id": "2509.22480",
    "authors": "Hang Li, Kaiqi Yang, Yucheng Chu, Hui Liu, Jiliang Tang",
    "summary": "Large language models (LLMs) have been widely used for problem-solving tasks. Most recent work improves their performance through supervised fine-tuning (SFT) with labeled data or reinforcement learning (RL) from task feedback. In this paper, we study a new perspective: the divergence in solutions generated by LLMs for a single problem. We show that higher solution divergence is positively related to better problem-solving abilities across various models. Based on this finding, we propose solution divergence as a novel metric that can support both SFT and RL strategies. We test this idea on three representative problem domains and find that using solution divergence consistently improves success rates. These results suggest that solution divergence is a simple but effective tool for advancing LLM training and evaluation.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.797893",
    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，该论文的本质是研究LLM解决方案差异性(solution divergence)对问题解决能力的影响，并提出一种新的训练和评估指标。这明显属于改进LLM基础能力和通用推理能力的研究，而非将LLM作为工具应用于特定领域。论文提出的解决方案差异性指标可以支持监督微调(SFT)和强化学习(RL)策略，目的是提升LLM的通用问题解决能力，符合保留标准。 其次，从正面指标分析，论文符合多个关键指标： - 核心概念：明确研究Large language models (LLMs) - 能力方向：关注problem-solving能力，这是推理能力的核心组成部分 - 训练方法：提出的指标可以支持reinforcement learning (RL)策略 第三，论文不符合任何排除标准。虽然提到在\"三个代表性问题领域\"进行测试，但这些领域只是用来验证通用方法的实验平台，而非论文的主要焦点。论文的核心是提出一种通用的训练和评估方法，而非针对特定应用领域。 综上所述，该论文的核心贡献是发现并验证了解决方案差异性这一新指标对提升LLM问题解决能力的有效性，这直接涉及到改进LLM的通用推理能力，完全符合研究目标。",
    "summary2": "本文旨在提升大型语言模型的问题解决能力。针对单一问题的多种解决方案，我们提出了一种解决方案发散性作为新的评估指标，并将其应用于监督微调和强化学习中。我们在数学(Math-500)、编程(MBPP+)和逻辑推理(Maze)三个代表性问题领域上通过Pass@1和Pass@10成功率验证了其有效性，实验结果表明利用解决方案发散性可显著提升模型性能。",
    "inspiration_trace": "## 面临的挑战\n作者识别到当前提升LLM问题解决能力的方法主要依赖有监督微调和强化学习，但这些方法需要大量标注数据或复杂奖励设计，成本高昂且劳动密集，同时忽视了单个问题存在多种有效解决方案的重要特性。\n\n## 关键洞察\n作者从认知科学研究中获得独特视角：人类拥有更多问题解决策略时在复杂任务上表现更佳。通过实验发现LLM的解决方案发散性与问题解决能力呈正相关，特别是在中等难度问题上，表明发散性可作为评估和提升LLM性能的新指标。\n\n## 解决方案演进\n基于这一洞察，作者提出两种方法：1)将解决方案发散性作为数据集选择标准，在SFT中优先选择高发散性样本；2)设计融合发散性的奖励函数，在RL训练中同时优化正确性和多样性，分别应用于训练数据选择和奖励设计两个环节。\n\n## 创新点总结\n该思路创新在于首次将解决方案发散性形式化为可量化指标并与认知科学理论联系；将这一指标同时应用于SFT和RL两种主流训练范式；通过利用现有问题的多解性，避免了生成新数据的成本和复杂性。",
    "summary_translation": "大型语言模型（Large language models, LLMs）已被广泛应用于问题解决任务。最近的研究主要通过有标签数据的监督微调（supervised fine-tuning, SFT）或基于任务反馈的强化学习（reinforcement learning, RL）来提高其性能。在本文中，我们研究了一个新视角：大型语言模型为单个问题生成的解决方案之间的差异性（divergence）。我们表明，更高的解决方案差异性与更好的问题解决能力呈正相关，这一现象在各种模型中普遍存在。基于这一发现，我们提出将解决方案差异性（solution divergence）作为一种新的评估指标，它可以同时支持监督微调和强化学习策略。我们在三个代表性问题领域测试了这一想法，发现利用解决方案差异性能够持续提高成功率。这些结果表明，解决方案差异性是一个简单但有效的工具，可用于推动大型语言模型的训练和评估。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#21",
    "title": "Detecting (Un)answerability in Large Language Models with Linear Directions",
    "link": "/arxiv/2509.22449",
    "arxiv_id": "2509.22449",
    "authors": "Maor Juliet Lavi, Tova Milo, Mor Geva",
    "summary": "Large language models (LLMs) often respond confidently to questions even when they lack the necessary information, leading to hallucinated answers. In this work, we study the problem of (un)answerability detection, focusing on extractive question answering (QA) where the model should determine if a passage contains sufficient information to answer a given question. We propose a simple approach for identifying a direction in the model's activation space that captures unanswerability and uses it for classification. This direction is selected by applying activation additions during inference and measuring their impact on the model's abstention behavior. We show that projecting hidden activations onto this direction yields a reliable score for (un)answerability classification. Experiments on two open-weight LLMs and four extractive QA benchmarks show that our method effectively detects unanswerable questions and generalizes better across datasets than existing prompt-based and classifier-based approaches. Moreover, the obtained directions extend beyond extractive QA to unanswerability that stems from factors, such as lack of scientific consensus and subjectivity. Last, causal interventions show that adding or ablating the directions effectively controls the abstention behavior of the model.",
    "subjects": "Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.799612",
    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：这篇论文的本质是改进LLM的基础能力，具体是增强模型判断问题可回答性的元认知能力。论文提出了一种新方法，通过在模型激活空间中识别捕捉\"不可回答性\"的线性方向，来提升模型自我评估能力，从而减少幻觉产生。这属于改进LLM基础能力的研究，而非将LLM作为工具应用到特定领域，因此应保留。 第二步正面指标：论文明确研究Large language models (LLMs)，符合核心概念指标。虽然不直接研究数学或逻辑推理，但模型判断问题可回答性的能力与推理质量密切相关，属于推理能力的基础支撑。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面（如水印、安全等），不符合任何排除标准。 第四步特殊和模糊情况：论文研究减少幻觉的问题，但提出的是一种新方法（激活空间中的线性方向）来增强模型内在的自我评估能力，属于提升模型内在可靠性和推理质量的方法，而非社会学研究或应用层面讨论，应保留。 最终决策：论文的核心贡献是提出了一种提升LLM自我评估能力的方法，这属于通用推理能力的重要组成部分。高质量的推理不仅需要逻辑和数学能力，还需要模型能够准确判断自己是否有足够信息回答问题。因此，这篇论文符合\"大语言模型通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决大型语言模型在缺乏信息时仍自信回答导致幻觉的问题。针对抽取式问答场景，我们提出了一种在模型激活空间中识别线性方向来检测无法回答性的方法，并在Llama-3-8B-Instruct和Gemma-3-12B-IT模型上通过SQuAD、RepLiQA、NQ和MuSiQue四个数据集的F1分数验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n大型语言模型在缺乏必要信息时仍会自信回答问题，导致产生幻觉答案。特别是在抽取式问答任务中，现有方法（如基于提示和分类器的方法）难以可靠检测不可回答问题，且在不同数据集上泛化能力差。\n\n## 关键洞察\n作者洞察到\"不可回答性\"可能像情感、真实性等抽象概念一样，被线性编码在模型内部表示空间中。这一视角转变了问题本质——不再是设计外部检测机制，而是发现模型内部已有的表示结构。\n\n## 解决方案演进\n基于此洞察，作者首先计算可回答与不可回答示例的平均激活差异生成候选方向；然后创新地使用\"因果转向\"方法，通过添加候选向量并测量其对模型回避行为的影响来选择最佳方向；最后利用选定方向构建简单分类器，通过投影激活得到不可回答性分数。\n\n## 创新点总结\n创新点在于将不可回答性视为内部表示空间的线性方向，利用因果干预验证方向有效性，构建轻量可解释的检测方法，且展现出更强的跨数据集泛化能力，揭示了模型内部一致编码的不可回答性信号。",
    "summary_translation": "大型语言模型（LLMs）即使在缺乏必要信息的情况下也经常自信地回答问题，导致产生幻觉答案（hallucinated answers）。在本研究中，我们探讨（无法）回答性检测（(un)answerability detection）问题，专注于抽取式问答（extractive question answering, QA），其中模型需要确定一段文本是否包含足够的信息来回答给定问题。我们提出了一种简单方法，用于识别模型激活空间（activation space）中捕捉无法回答性的方向，并将其用于分类。该方向通过在推理过程中应用激活加法（activation additions）并测量其对模型回避行为（abstention behavior）的影响来选择。我们表明，将隐藏激活（hidden activations）投影到该方向上会产生一个可靠的（无法）回答性分类分数。在两个开放权重大型语言模型（open-weight LLMs）和四个抽取式问答基准测试（extractive QA benchmarks）上的实验表明，我们的方法能有效检测无法回答的问题，并且在跨数据集上的泛化能力优于现有的基于提示（prompt-based）和基于分类器（classifier-based）的方法。此外，所获得的方向不仅适用于抽取式问答，还扩展到由其他因素导致的无法回答性，如缺乏科学共识（scientific consensus）和主观性（subjectivity）。最后，因果干预（causal interventions）表明，添加或消除这些方向可以有效控制模型的回避行为。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#28",
    "title": "Transformers Can Learn Connectivity in Some Graphs but Not Others",
    "link": "/arxiv/2509.22343",
    "arxiv_id": "2509.22343",
    "authors": "Amit Roy, Abulhair Saparov",
    "summary": "Reasoning capability is essential to ensure the factual correctness of the responses of transformer-based Large Language Models (LLMs), and robust reasoning about transitive relations is instrumental in many settings, such as causal inference. Hence, it is essential to investigate the capability of transformers in the task of inferring transitive relations (e.g., knowing A causes B and B causes C, then A causes C). The task of inferring transitive relations is equivalent to the task of connectivity in directed graphs (e.g., knowing there is a path from A to B, and there is a path from B to C, then there is a path from A to C). Past research focused on whether transformers can learn to infer transitivity from in-context examples provided in the input prompt. However, transformers' capability to infer transitive relations from training examples and how scaling affects the ability is unexplored. In this study, we seek to answer this question by generating directed graphs to train transformer models of varying sizes and evaluate their ability to infer transitive relations for various graph sizes. Our findings suggest that transformers are capable of learning connectivity on \"grid-like'' directed graphs where each node can be embedded in a low-dimensional subspace, and connectivity is easily inferable from the embeddings of the nodes. We find that the dimensionality of the underlying grid graph is a strong predictor of transformers' ability to learn the connectivity task, where higher-dimensional grid graphs pose a greater challenge than low-dimensional grid graphs. In addition, we observe that increasing the model scale leads to increasingly better generalization to infer connectivity over grid graphs. However, if the graph is not a grid graph and contains many disconnected components, transformers struggle to learn the connectivity task, especially when the number of components is large.",
    "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Logic in Computer Science",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.802854",
    "filter_reason": "这篇论文的核心是研究Transformer模型（大语言模型）在传递关系推理方面的能力，这是一种基础的逻辑推理能力。论文探讨了模型如何从训练数据中学习传递关系（如\"A导致B，B导致C，则A导致C\"），这等价于在有向图中推断连通性。研究发现Transformer能够学习某些类型图结构的连通性，并且模型规模的增加会提高这种推理能力。这完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标，因为它关注的是LLM的基础推理能力，而不是将LLM作为工具应用到特定领域。论文明确研究\"Reasoning capability\"和\"transitive relations\"的逻辑推理能力，这些都是大语言模型通用推理能力的核心组成部分。论文没有涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性应用层面），而是专注于提升模型本身的逻辑推理能力这一核心目标。",
    "summary2": "本文旨在 [研究Transformer学习图连通性的能力]。针对 [有向图连通性任务]，我们提出了一种 [生成网格图和断开链图训练Transformer的方法]，并在 [合成图数据] 上通过 [准确率和损失] 验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n作者识别到Transformer模型从训练示例中学习传递关系推理（等价于图连通性）的能力尚未被探索，特别是模型规模和图规模如何影响这种学习能力仍不清楚。现有研究主要关注上下文学习，而非训练阶段的学习能力。\n\n## 关键洞察\n作者洞察到图的拓扑结构是关键：Transformer更容易学习那些节点可嵌入低维子空间的图（如网格图），因为连通性可从节点嵌入中推断；而断开的链图不具备这种特性，难以低维嵌入，导致学习困难。\n\n## 解决方案演进\n基于此洞察，作者设计对比实验：生成不同维度和规模的网格图与断开链图，训练不同规模Transformer模型学习连通性任务。通过系统分析模型在不同图结构、规模下的表现，验证了图的拓扑结构和规模因素对学习效果的影响机制。\n\n## 创新点总结\n创新点在于揭示了图的拓扑结构（特别是可低维嵌入性）对Transformer学习传递关系的关键影响，发现了模型规模和图规模对不同拓扑结构图的不同影响规律，为理解Transformer的逻辑推理能力提供了新视角。",
    "summary_translation": "推理能力对于确保基于transformer的大型语言模型（Large Language Models, LLMs）回答的事实正确性至关重要，而对传递关系（transitive relations）的稳健推理在许多场景中都发挥着重要作用，例如因果推断（causal inference）。因此，研究transformer在推断传递关系任务中的能力至关重要（例如，已知A导致B，B导致C，则A导致C）。推断传递关系的任务等价于有向图（directed graphs）中的连通性（connectivity）任务（例如，已知存在从A到B的路径，且存在从B到C的路径，则存在从A到C的路径）。\n\n过去的研究主要关注transformer是否能从输入提示（input prompt）中提供的上下文示例（in-context examples）学习推断传递性（transitivity）。然而，transformer从训练示例（training examples）推断传递关系的能力，以及规模扩展（scaling）如何影响这种能力，尚未得到探索。在本研究中，我们通过生成有向图来训练不同大小的transformer模型，并评估它们推断不同大小图中传递关系的能力，旨在回答这一问题。\n\n我们的研究结果表明，transformer能够在\"网格状\"（grid-like）有向图上学习连通性，其中每个节点可以嵌入到低维子空间（low-dimensional subspace）中，且连通性可以很容易地从节点嵌入（embeddings）中推断出来。我们发现，底层网格图（grid graph）的维度（dimensionality）是transformer学习连通性能力的强预测因子（predictor），其中高维网格图比低维网格图构成更大挑战。此外，我们观察到增加模型规模会导致在推断网格图连通性方面的泛化能力（generalization）越来越强。然而，如果图不是网格图且包含许多不连通组件（disconnected components），transformer在学习连通性任务时会遇到困难，尤其是当组件数量较大时。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#31",
    "title": "Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs",
    "link": "/arxiv/2509.22251",
    "arxiv_id": "2509.22251",
    "authors": "Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong",
    "summary": "Currently, the main approach for Large Language Models (LLMs) to tackle the hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs typically treat KGs as plain text, extracting only semantic information and limiting their use of the crucial structural aspects of KGs. Another challenge is the gap between the embedding spaces of KGs encoders and LLMs text embeddings, which hinders the effective integration of structured knowledge. To overcome these obstacles, we put forward the SSKG-LLM, an innovative model architecture that is designed to efficiently integrate both the Structural and Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph Encoding (KGE) module to preserve semantics while utilizing structure. Then, the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to understand KGs embeddings. We conduct extensive experiments and provide a detailed analysis to explore how incorporating the structural information of KGs can enhance the factual reasoning abilities of LLMs. Our code are available at https://github.com/yfangZhang/SSKG-LLM.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.908451",
    "filter_reason": "这篇论文的核心贡献是提出SSKG-LLM模型架构，通过有效整合知识图谱的结构和语义信息来减轻大语言模型的幻觉问题，并增强其事实推理能力。从第一步核心判断来看，论文明确聚焦于改进LLM的基础推理能力，特别是事实推理这一通用能力，而非将LLM作为工具应用到特定领域。论文包含多个正面指标：明确关注Large Language Models (LLMs)和reasoning能力。在排除标准方面，论文不涉及多模态、特定应用领域或模型基础设施优化。对于特殊情况的处理，论文虽然涉及幻觉问题，但它是从模型内部机制提出解决方案，通过结构化知识整合来提升推理质量，而非应用层面的讨论。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标，应被保留。",
    "summary2": "本文旨在减轻LLMs的幻觉问题，有效整合知识图谱的结构和语义信息。针对知识图谱与大型语言模型的集成场景，我们提出了一种SSKG-LLM模型架构，包含KGR、KGE和KGA三个模块，通过GraphLM编码知识图谱结构和语义信息，并使用交叉注意力对齐嵌入空间，并在CommonsenseQA、SIQA和TruthfulQA问答数据集上通过准确率(ACC)、ROUGE-N和BLEU指标验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\nLLMs存在幻觉问题，而现有KGs整合方法各有局限：GraphRAG将KGs视为纯文本忽略结构信息；传统图编码器方法难以捕获完整语义且存在表示空间不对齐；LLMs-based KBQA方法无法回答KGs未覆盖的问题。\n\n## 关键洞察\n作者认识到KGs同时包含语义属性和结构特征，两者结合对增强LLMs理解至关重要；同时发现LLMs与图编码器间存在根本性数据格式差异，造成表示空间不对齐，阻碍了有效知识整合。\n\n## 解决方案演进\n针对结构与语义整合，采用图遍历提取相关子图，用GraphLM编码保留双重信息；针对表示空间差距，设计KG-Adapter模块通过交叉注意力机制将图和文本编码视为不同模态进行对齐，最终形成KGR-KGE-KGA的完整框架。\n\n## 创新点总结\n首次同时整合KGs的语义和结构信息；揭示并解决表示空间不对齐问题；通过DFS生成类似CoT的推理链增强推理能力；使用GraphLM编码器平衡结构性与语义性信息捕获。",
    "summary_translation": "目前，大型语言模型（Large Language Models, LLMs）解决幻觉问题（hallucination issue）的主要方法是整合知识图谱（Knowledge Graphs, KGs）。然而，LLMs通常将KGs视为纯文本，仅提取语义信息，限制了其对KGs关键结构方面的利用。另一个挑战是KGs编码器（KGs encoders）的嵌入空间（embedding spaces）与LLMs文本嵌入（text embeddings）之间的差距，这阻碍了结构化知识（structured knowledge）的有效整合。为克服这些障碍，我们提出了SSKG-LLM，这是一种创新的模型架构，旨在高效地将KGs的结构（Structural）和语义（Semantic）信息整合到LLMs的推理过程中。SSKG-LLM整合了知识图谱检索（Knowledge Graph Retrieval, KGR）模块和知识图谱编码（Knowledge Graph Encoding, KGE）模块，以在利用结构的同时保留语义。随后，整合了知识图谱适应（Knowledge Graph Adaptation, KGA）模块，使LLMs能够理解KGs的嵌入（embeddings）。我们进行了广泛的实验并提供了详细分析，以探索整合KGs的结构信息如何增强LLMs的事实推理（factual reasoning）能力。我们的代码可在https://github.com/yfangZhang/SSKG-LLM获取。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#29",
    "title": "Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs",
    "link": "/arxiv/2509.22338",
    "arxiv_id": "2509.22338",
    "authors": "Felix Vossel, Till Mossakowski, Björn Gehrke",
    "summary": "Automating the translation of natural language to first-order logic (FOL) is crucial for knowledge representation and formal methods, yet remains challenging. We present a systematic evaluation of fine-tuned LLMs for this task, comparing architectures (encoder-decoder vs. decoder-only) and training strategies. Using the MALLS and Willow datasets, we explore techniques like vocabulary extension, predicate conditioning, and multilingual training, introducing metrics for exact match, logical equivalence, and predicate alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT reasoning ability as well as symbolic systems like ccg2lambda. Key findings show: (1) predicate availability boosts performance by 15-20%, (2) T5 models surpass larger decoder-only LLMs, and (3) models generalize to unseen logical arguments (FOLIO dataset) without specific training. While structural logic translation proves robust, predicate extraction emerges as the main bottleneck.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.896762",
    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。 首先，从核心判断来看，这篇论文的本质是研究如何通过微调LLMs来提高它们将自然语言转换为一阶逻辑(FOL)的能力。一阶逻辑转换是逻辑推理的基础能力，属于LLM的通用推理能力范畴，而不是将LLM作为工具应用到特定领域。论文关注的是改进LLM本身的基础能力（逻辑推理和形式化表示），符合第一步的保留标准。 其次，从正面指标分析，论文明确符合两个关键指标： 1) 核心概念：论文直接研究\"fine-tuned LLMs\"，使用了Flan-T5-XXL、GPT-4o和DeepSeek-R1等大语言模型。 2) 能力方向：论文聚焦于logical reasoning，通过自然语言到一阶逻辑的转换来增强模型的逻辑推理能力，这是通用推理能力的核心组成部分。 第三，论文不符合任何排除标准。它不涉及多模态与视觉内容，不针对特定应用领域（如医疗、化学等），也不关注模型可靠性问题（如水印、安全等）。 论文的核心贡献是提出了一种系统评估和改进LLMs自然语言到一阶逻辑转换能力的方法，通过微调技术、架构比较和训练策略优化，显著提升了模型在逻辑推理任务上的表现。这种能力是通用的，可以应用于多个领域，而不是局限于特定应用场景，因此完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
    "summary2": "本文旨在解决自然语言到一阶逻辑自动翻译的挑战性问题。针对MALLS和Willow数据集，我们提出了一种使用微调LLMs的方法，并通过精确匹配、逻辑等价性和谓词对齐指标验证了其有效性。微调的Flan-T5-XXL模型在提供谓词列表情况下达到70%准确率，优于GPT-4o和DeepSeek-R1-0528等模型。",
    "inspiration_trace": "## 面临的挑战\n自然语言到一阶逻辑的自动化翻译是知识表示的关键瓶颈，现有方法要么依赖缺乏灵活性的符号系统，要么在复杂逻辑推理上表现不佳，导致形式化方法无法充分利用丰富的自然语言信息。\n\n## 关键洞察\n作者发现NL-FOL翻译的主要瓶颈不是逻辑结构生成，而是谓词提取；编码器-解码器架构比纯解码器模型更适合此类结构化转换任务；模型能泛化到未见过的逻辑结构，显示逻辑翻译能力的鲁棒性。\n\n## 解决方案演进\n从标准微调出发，尝试符号表示优化但效果有限；基于谓词瓶颈的洞察，设计谓词条件化策略，包括提供谓词列表、添加噪声干扰及多阶段课程学习；进一步探索多语言训练提升谓词抽象能力，使模型不依赖特定语言句法模式。\n\n## 创新点总结\n创新性地将翻译任务分解为谓词提取与结构生成，识别出关键瓶颈；证明编码器-解码器架构在此任务上的优势；提出实用的谓词条件化方法，即使有噪声也能显著提升性能；通过多语言训练增强模型对谓词语义的理解。",
    "summary_translation": "将自然语言自动翻译为一阶逻辑(first-order logic, FOL)对于知识表示(knowledge representation)和形式方法(formal methods)至关重要，但仍然具有挑战性。我们针对这项任务对经过微调(fine-tuned)的大型语言模型(large language models, LLMs)进行了系统性评估，比较了不同架构(编码器-解码器encoder-decoder与仅解码器decoder-only)和训练策略。利用MALLS和Willow数据集，我们探索了词汇扩展(vocabulary extension)、谓词调节(predicate conditioning)和多语言训练(multilingual training)等技术，并引入了用于精确匹配(exact match)、逻辑等价性(logical equivalence)和谓词对齐(predicate alignment)的评估指标。我们经过微调的Flan-T5-XXL模型在提供谓词列表的情况下达到了70%的准确率，优于GPT-4o以及具备思维链(chain-of-thought, CoT)推理能力的DeepSeek-R1-0528模型，同时也优于ccg2lambda等符号系统。主要发现表明：(1) 谓词(predicate)的可用性将性能提升了15-20%；(2) T5模型超越了更大的仅解码器大型语言模型；(3) 模型无需特定训练即可泛化到未见过的逻辑参数(FOLIO数据集)。虽然结构逻辑翻译(structural logic translation)表现稳健，但谓词提取(predicate extraction)成为主要瓶颈。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#35",
    "title": "In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners",
    "link": "/arxiv/2509.22230",
    "arxiv_id": "2509.22230",
    "authors": "Jaehoon Kim, Kwangwook Seo, Dongha Lee",
    "summary": "Transferring reasoning capabilities from larger language models to smaller ones through supervised fine-tuning often fails counterintuitively, with performance degrading despite access to high-quality teacher demonstrations. We identify that this failure stems from distributional misalignment: reasoning traces from larger models contain tokens that are low probability under the student's distribution, exceeding the internal representation capacity of smaller architectures and creating learning barriers rather than helpful guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for generating student-friendly reasoning traces in which the teacher model proposes candidate tokens but the student model determines acceptance based on its own probability distributions, filtering low probability tokens. When applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data degrades average performance across major reasoning benchmarks by 20.5\\%, while the same model trained on RSD-generated reasoning traces achieves meaningful improvements of 4.9\\%. Our analysis reveals that low probability tokens constitute the critical bottleneck in reasoning ability transfer. However, cross-model experiments demonstrate that RSD traces are model-specific rather than universally applicable, indicating that distributional alignment must be tailored for each student architecture's unique internal representation.",
    "subjects": "Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.910453",
    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提升小语言模型的推理能力这一基础能力，提出了反向推测解码(RSD)这一新的训练范式来增强模型的推理能力，而不是将LLM作为工具应用到特定领域。论文明确关注\"reasoning capabilities\"和\"reasoning traces\"，并在多个推理基准上测试性能提升，这直接对应了筛选标准中的\"reasoning\"能力方向。论文不符合任何排除标准，不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文没有涉及强化学习或智能体等新兴范式，但它提出的新训练机制RSD解决了推理能力转移中的关键问题，即分布不匹配导致的低概率token障碍，这直接提升了小模型的通用推理能力。因此，这篇论文的核心贡献是提升LLM本身的推理能力，完全符合研究目标。",
    "summary2": "本文旨在解决大型语言模型推理能力向小型模型转移时的性能下降问题。针对小型模型在学习大型模型推理轨迹时遇到的分布不匹配问题，我们提出了一种反向推测解码(RSD)方法，通过让教师模型提出候选token而学生模型根据自身概率分布决定是否接受，过滤低概率token。在Qwen3-0.6B模型和多个推理基准上的实验表明，直接蒸馏会导致平均性能下降20.5%，而RSD生成的推理轨迹实现了4.9%的性能提升。",
    "inspiration_trace": "## 面临的挑战\n大型模型推理能力向小型模型迁移时出现反直觉现象：尽管使用高质量教师演示，小型模型性能反而下降，直接蒸馏方法失效。\n\n## 关键洞察\n作者发现问题本质是\"分布不对齐\"：大型模型推理轨迹包含的标记在学生模型分布中概率极低，超出小型架构内部表示能力，形成学习障碍。有效转移需管理学生学习的\"意外性\"，确保推理步骤在其处理范围内。\n\n## 解决方案演进\n基于此，作者提出\"反向推测解码\"(RSD)：颠覆传统师生动态，教师提出候选标记，学生根据自身概率分布决定接受与否。低于阈值的标记被拒绝，生成回退到学生自身预测，确保轨迹分布对齐。\n\n## 创新点总结\n创新点在于：识别低概率标记是推理转移瓶颈；提出以学生为中心的轨迹生成方法；发现分布对齐是模型特定的，需为每个架构定制，为小型模型推理能力转移开辟新途径。",
    "summary_translation": "通过监督式微调(supervised fine-tuning)将较大语言模型的推理能力转移到较小模型时，常常会出现反直觉的失败，尽管能够获得高质量的教师示范，但性能反而下降。我们发现这种失败源于分布不匹配(distributional misalignment)：较大模型的推理轨迹(reasoning traces)包含在学生模型分布下概率较低的标记(tokens)，这些标记超出了较小架构的内部表示能力，形成了学习障碍而非有效指导。我们提出了反向推测解码(Reverse Speculative Decoding, RSD)，这是一种生成对学生模型友好的推理轨迹的机制，在该机制中，教师模型提出候选标记，但学生模型基于自身的概率分布决定是否接受，从而过滤掉低概率标记。当应用于Qwen3-0.6B模型时，直接蒸馏s1K-1.1推理轨迹数据会导致在主要推理基准(reasoning benchmarks)上的平均性能下降20.5%，而使用RSD生成的推理轨迹训练的同一模型则实现了4.9%的显著提升。我们的分析表明，低概率标记构成了推理能力转移的关键瓶颈。然而，跨模型实验表明，RSD轨迹是模型特定的(model-specific)而非普遍适用的，这表明分布对齐必须针对每个学生架构独特的内部表示进行定制。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#36",
    "title": "Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data",
    "link": "/arxiv/2509.22224",
    "arxiv_id": "2509.22224",
    "authors": "Zishan Ahmad, Saisubramaniam Gopalakrishnan",
    "summary": "Large Language Models (LLMs), despite their remarkable capabilities, rely on singular, pre-dominant reasoning paradigms, hindering their performance on intricate problems that demand diverse cognitive strategies. To address this, we introduce Composite Reasoning (CR), a novel reasoning approach empowering LLMs to dynamically explore and combine multiple reasoning styles like deductive, inductive, and abductive for more nuanced problem-solving. Evaluated on scientific and medical question-answering benchmarks, our approach outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while demonstrating superior sample efficiency and adequate token usage. Notably, CR adaptively emphasizes domain-appropriate reasoning styles. It prioritizes abductive and deductive reasoning for medical question answering, but shifts to causal, deductive, and inductive methods for scientific reasoning. Our findings highlight that by cultivating internal reasoning style diversity, LLMs acquire more robust, adaptive, and efficient problem-solving abilities.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.910933",
    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是提出\"Composite Reasoning (CR)\"这一新型推理方法，使LLMs能够动态探索和组合多种推理风格（如演绎、归纳和溯因推理）。这明显是关于改进LLM的基础推理能力，提出新的推理范式，增强其逻辑和多步推理等通用能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确关注Large Language Models (LLMs) - 能力方向：专注于reasoning，特别是多种推理风格（演绎、归纳、溯因、因果推理） - 新兴范式：提出对思维链(CoT)等现有范式的扩展和改进 第三步排除标准：论文不符合任何排除标准： - 虽然在科学和医学问答基准上进行了评估，但其核心贡献是通用推理方法，而非专注于这些特定领域 - 不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容 论文的核心贡献是提出一种通用的推理框架，使LLM能够根据问题类型动态选择和组合不同的推理风格，从而提升模型的通用推理能力和问题解决能力。正如摘要所述，\"通过培养内部推理风格多样性，LLMs获得更强大、自适应和高效的问题解决能力\"，这完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
    "summary2": "本文旨在提升大型语言模型在复杂问题上的推理能力，特别是在数据有限的情况下。针对科学和医学问答任务及资源受限的训练环境（最多1,500个样本），我们提出了一种Composite Reasoning (CR)方法，使LLMs能够动态探索和结合多种推理风格（如演绎、归纳和溯因推理），并在MedMCQA、MedXpertQA和ARC-Complex数据集上通过准确率和令牌使用效率验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n大型语言模型依赖单一主导推理范式，难以处理需要多样化认知策略的复杂问题，尤其在数据有限情况下表现受限。\n\n## 关键洞察\n真实世界问题需要动态综合多种推理方法；不同领域需要不同推理风格组合，如医学问题需溯因和演绎推理，科学问题需因果、演绎和归纳方法。\n\n## 解决方案演进\n从单一推理局限性出发，假设多样化推理策略组合可提升性能，设计复合推理(CR)方法使模型动态探索结合多种推理风格，通过参数高效微调和GRPO在资源受限环境中验证其有效性。\n\n## 创新点总结\n首次明确鼓励模型内部探索整合多种推理策略；证明培养推理风格多样性可显著提升有限数据下的模型性能；发现模型能根据领域需求自适应调整推理风格，更接近人类专家推理方式。",
    "summary_translation": "大型语言模型（Large Language Models, LLMs），尽管具有显著能力，但依赖于单一的、主导性的推理范式（reasoning paradigms），这限制了它们在需要多样化认知策略的复杂问题上的表现。为解决这一问题，我们提出了复合推理（Composite Reasoning, CR），这是一种新颖的推理方法，使大型语言模型能够动态探索和结合多种推理风格，如演绎推理（deductive）、归纳推理（inductive）和溯因推理（abductive），以实现更细致的问题解决。在科学和医学问答基准测试（benchmarks）中，我们的方法优于现有的基线方法，如思维链（Chain-of-Thought, CoT），并且超越了DeepSeek-R1风格推理（style reasoning, SR）的准确性，同时表现出卓越的样本效率和适当的令牌（token）使用。值得注意的是，CR自适应地强调适合特定领域的推理风格。对于医学问答，它优先采用溯因推理和演绎推理；而对于科学推理，则转向因果推理（causal）、演绎推理和归纳推理方法。我们的研究结果表明，通过培养内部推理风格的多样性，大型语言模型能够获得更强大、自适应和高效的问题解决能力。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#40",
    "title": "When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance",
    "link": "/arxiv/2509.22193",
    "arxiv_id": "2509.22193",
    "authors": "Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Kevin El-Haddad, Céline Hudelot, Pierre Colombo",
    "summary": "Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art performance on a wide range of tasks. Despite its empirical success, the tasks and model scales at which reasoning becomes effective, as well as its training and inference costs, remain underexplored. In this work, we rely on a synthetic data distillation framework to conduct a large-scale supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models of varying sizes, on a wide range of math-centric and general-purpose tasks, evaluating both multiple-choice and open-ended formats. Our analysis reveals that reasoning consistently improves model performance, often matching or surpassing significantly larger IFT systems. Notably, while IFT remains Pareto-optimal in training and inference costs, reasoning models become increasingly valuable as model size scales, overcoming IFT performance limits on reasoning-intensive and open-ended tasks.",
    "subjects": "Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.918606",
    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究LLMs的推理能力本身，而非将LLM作为工具应用到特定领域。论文通过控制实验研究推理能力对模型性能的贡献，探讨在什么任务和模型规模下推理变得有效，这直接关注的是LLM的基础能力提升。 其次，论文包含了多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：核心主题就是reasoning，特别关注math reasoning和通用任务中的推理能力 - 训练方法：比较了Instruction Fine-Tuning (IFT)和推理模型的不同训练范式 第三，论文不涉及任何排除标准中的领域。它没有关注多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是通过大规模控制实验，系统性地研究了推理能力对LLM性能的影响，发现推理能力能一致地提高模型性能，特别是在推理密集型和开放式任务上。这项研究直接有助于理解如何提升LLM的通用推理能力，完全符合研究目标。",
    "summary2": "本文旨在探究推理能力对大语言模型性能的贡献条件。针对不同规模模型和任务类型，我们提出了一种基于合成数据蒸馏的受控研究方法，通过单一教师生成IFT和推理配对数据，并在12个基准测试上验证了推理在开放式和数学密集型任务上的显著优势，尤其在大规模模型(>7B)中能有效突破IFT性能瓶颈，同时揭示了推理与IFT在训练和推理效率上的权衡关系。",
    "inspiration_trace": "## 面临的挑战\n推理能力在大语言模型中展现出强大性能，但其在哪些任务和模型规模下有效，以及训练和推理成本如何权衡，仍缺乏系统研究。现有工作未能解开模型规模、任务类型与计算预算等混杂因素，导致实践者缺乏明确指导。\n\n## 关键洞察\n推理效果高度任务依赖：在数学和编码等需多步推理的任务上收益显著，而在简单事实任务上收益有限。同时，推理与模型规模存在复杂动态关系——小模型难以吸收深度推理，而大模型能突破IFT性能瓶颈。推理与IFT间存在效率与性能的权衡。\n\n## 解决方案演进\n为隔离性能驱动因素，设计受控蒸馏框架：使用单一教师模型为相同输入生成配对的IFT和推理答案，保持数据和容量恒定，仅改变监督格式。通过1.6M训练对和12个基准测试，系统映射推理在模型规模、任务类型和答案格式上的影响。\n\n## 创新点总结\n首次提供统一受控的推理与IFT比较视图，消除混杂因素实现性能清晰归因；系统揭示推理作用边界，为实践者提供可操作指导；通过纯监督蒸馏隔离推理信号，避免强化学习的复杂性；开放资源促进可重复研究。",
    "summary_translation": "具有推理能力的大型语言模型（Large Language Models, LLMs）在广泛任务上取得了最先进的（state-of-the-art）性能。尽管其在经验上取得了成功，但推理变得有效的任务和模型规模，以及其训练和推理成本，仍未得到充分探索。在这项工作中，我们依靠合成数据蒸馏（synthetic data distillation）框架进行大规模监督研究。我们比较了不同大小的指令微调（Instruction Fine-Tuning, IFT）模型和推理模型，在广泛的以数学为中心和通用任务上，评估了多项选择和开放式两种格式。我们的分析表明，推理持续提高模型性能，通常匹配或超过显著更大的IFT系统。值得注意的是，虽然IFT在训练和推理成本上保持帕累托最优（Pareto-optimal），但随着模型规模的扩大，推理模型变得越来越有价值，克服了IFT在推理密集型和开放式任务上的性能限制。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#43",
    "title": "From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement",
    "link": "/arxiv/2509.22144",
    "arxiv_id": "2509.22144",
    "authors": "Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Zike Yuan, Yang Xiang, Buzhou Tang",
    "summary": "Chain-of-Thought (CoT) reasoning improves performance on complex tasks but introduces significant inference latency due to verbosity. We propose Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that leverages the token elasticity phenomenon--where overly small token budgets can paradoxically increase output length--to progressively compress CoTs via multiround refinement. This adaptive strategy allows MACC to determine the optimal compression depth for each input. Our method achieves an average accuracy improvement of 5.6 percent over state-of-the-art baselines, while also reducing CoT length by an average of 47 tokens and significantly lowering latency. Furthermore, we show that test-time performance--accuracy and token length--can be reliably predicted using interpretable features like perplexity and compression rate on the training set. Evaluated across different models, our method enables efficient model selection and forecasting without repeated fine-tuning, demonstrating that CoT compression is both effective and predictable. Our code will be released in https://github.com/Leon221220/MACC.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.920709",
    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Multiround Adaptive Chain-of-Thought Compression (MACC)\"的框架，用于优化思维链(CoT)推理过程。思维链(CoT)是提高大语言模型在复杂任务上表现的重要技术，属于LLM的通用推理能力范畴。论文通过多轮细化和自适应策略，实现了CoT的有效压缩，同时提高了准确性和效率。这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标，特别是关于思维链(CoT)的方法论研究。论文不涉及任何排除标准中的领域，如特定应用领域或多模态研究，而是专注于增强LLM的基础推理能力。论文明确涉及了\"reasoning\"这一核心能力方向，并通过提出新的方法来改进CoT推理，使其更加高效。因此，这篇论文完全符合研究范围，应被保留。",
    "summary2": "本文旨在解决思维链推理导致的推理延迟问题。针对大型语言模型在复杂任务中的冗长推理过程，我们提出了一种多轮自适应思维链压缩框架MACC，利用token弹性现象通过多轮细化逐步压缩CoT。在GSM8K、MATH-500和AIME24数据集上通过准确率、token数量、延迟和token效率等指标验证了其有效性，实现了平均5.6%的准确率提升和47个token的长度减少。",
    "inspiration_trace": "## 面临的挑战\nChain-of-Thought推理虽提升复杂任务性能，但其冗长性导致显著推理延迟。现有压缩方法缺乏细粒度适应性，无法在多样化输入中有效管理压缩与准确性的权衡，统一压缩策略忽略了输入特定的推理复杂性。\n\n## 关键洞察\n作者观察到\"token弹性现象\"——过度压缩反而会增加输出长度，因生成质量下降导致补偿性冗余。这揭示了令牌约束与模型行为间的非线性关系，表明压缩需是渐进、自适应过程，而非一次性或静态操作。\n\n## 解决方案演进\n基于token弹性现象，作者提出多轮自适应压缩：先生成完整推理链，再通过多轮迭代逐步压缩，每轮移除冗余同时保留关键信息。创新点在于动态控制压缩比率，并在压缩导致长度增加时停止，确保压缩只在有意义时进行，避免语义损失。\n\n## 创新点总结\n将token弹性现象转化为实用多轮压缩策略；引入自适应停止机制动态决定最佳压缩深度；提出性能估计假设，使压缩效果可预测，无需昂贵重训练即可选择策略，实现压缩与准确性的智能平衡。",
    "summary_translation": "思维链（Chain-of-Thought, CoT）推理提高了复杂任务的性能，但由于其冗长性引入了显著的推理延迟。我们提出了多轮自适应思维链压缩（Multiround Adaptive Chain-of-Thought Compression, MACC）框架，该框架利用token弹性（token elasticity）现象——即过小的token预算反而会矛盾地增加输出长度——通过多轮迭代逐步压缩思维链。这种自适应策略使MACC能够为每个输入确定最佳压缩深度。我们的方法相比最先进的基线（state-of-the-art baselines）实现了平均5.6%的准确率提升，同时将思维链长度平均减少47个token（token），并显著降低延迟。此外，我们表明测试时性能（test-time performance）——准确率和token长度——可以通过训练集上的可解释特征（如困惑度（perplexity）和压缩率（compression rate））进行可靠预测。在不同模型上的评估表明，我们的方法无需重复微调（fine-tuning）即可实现高效的模型选择和预测，证明了思维链压缩既有效又可预测。我们的代码将在https://github.com/Leon221220/MACC上发布。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#46",
    "title": "R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning",
    "link": "/arxiv/2509.22131",
    "arxiv_id": "2509.22131",
    "authors": "Hongyu Shan, Mingyang Song, Chang Dai, Di Liang, Han Chen",
    "summary": "Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle complex reasoning by eliciting explicit step-by-step rationales. However, CoT's verbosity increases latency and memory usage and may propagate early errors across long chains. We propose the Reasoning Capsule (R-Capsule), a framework that aims to combine the efficiency of latent reasoning with the transparency of explicit CoT. The core idea is to compress the high-level plan into a small set of learned latent tokens (a Reasoning Capsule) while keeping execution steps lightweight or explicit. This hybrid approach is inspired by the Information Bottleneck (IB) principle, where we encourage the capsule to be approximately minimal yet sufficient for the task. Minimality is encouraged via a low-capacity bottleneck, which helps improve efficiency. Sufficiency is encouraged via a dual objective: a primary task loss for answer accuracy and an auxiliary plan-reconstruction loss that encourages the capsule to faithfully represent the original textual plan. The reconstruction objective helps ground the latent space, thereby improving interpretability and reducing the use of uninformative shortcuts. Our framework strikes a balance between efficiency, accuracy, and interpretability, thereby reducing the visible token footprint of reasoning while maintaining or improving accuracy on complex benchmarks. Our codes are available at: https://anonymous.4open.science/r/Reasoning-Capsule-7BE0",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.928221",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是改进LLM的基础推理能力。论文提出了Reasoning Capsule (R-Capsule)框架，旨在优化思维链(CoT)推理过程，提高LLM的推理效率和准确性。论文关注的是通用推理能力的增强，而不是将LLM应用于特定领域，也不涉及模型基础设施或部署优化。因此，论文符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Large Language Models (LLMs)\" - 能力方向：聚焦于\"reasoning\"，特别是\"complex reasoning\"和\"step-by-step rationales\"，同时涉及\"high-level plans\"和\"planning\" - 论文旨在提高模型在\"complex benchmarks\"上的表现，涉及problem-solving 第三步：排除标准 论文不聚焦于任何排除标准中提到的领域： - 不涉及多模态与视觉相关内容 - 不针对任何特定应用领域（如医疗、化学、生物等） - 不讨论模型可靠性层面的水印、安全性等问题 第四步：特殊和模糊情况 论文涉及可解释性的改进，提到\"improving interpretability\"，这是通过计划重建目标实现的，目的是提高模型的内在可解释性，从而提升模型的通用推理质量。这符合保留标准。 核心贡献分析： 论文的核心贡献是提出R-Capsule框架，通过将高级计划压缩成一组学习到的潜在令牌，结合了潜在推理的效率和显式思维链的透明度。这种方法旨在提高LLM的通用推理能力，减少推理过程中的可见令牌占用，同时保持或提高在复杂基准测试上的准确性。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。 因此，这篇论文应该被保留，它直接关注LLM的通用推理能力提升，提出了新的推理框架，符合研究范围。",
    "summary2": "本文旨在解决大型语言模型使用Chain-of-Thought推理时的高延迟和内存消耗问题。针对复杂推理任务，我们提出了一种R-Capsule框架，将高级计划压缩为紧凑的潜在令牌，同时保持执行步骤显式，并在数学和常识推理基准测试上通过准确率、令牌数量和延迟指标验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\nChain-of-Thought推理虽提升大语言模型复杂任务表现，但其冗长性增加推理延迟和内存使用，且长链推理易传播早期错误。现有解决方案如集成方法增加计算成本，隐式推理缺乏可解释性，分层推理重新引入令牌开销。\n\n## 关键洞察\n作者洞察到推理过程具有层次结构：高层战略计划与底层执行步骤。高层计划的语义本质比具体措辞对指导解决方案更关键，是压缩的理想目标。压缩计划而非执行步骤可保留有用归纳偏差，同时减少可见令牌。\n\n## 解决方案演进\n从推理层次性出发，将过程分解为计划和执行两阶段。借鉴信息瓶颈原理，设计框架将高层计划压缩为少量潜在令牌。通过架构瓶颈和双重目标（任务准确性和计划重建）确保压缩表示既最小又充分。训练中用辅助解码器重建原始文本计划，推理时直接基于胶囊生成答案。\n\n## 创新点总结\n创新点在于首次提出只压缩高层计划而非整个推理链，平衡效率与透明度；基于信息瓶颈原理确保表示紧凑且语义丰富；通过计划重建目标提供可解释性；验证计划压缩与执行步骤生成间的不对称性，为高效推理开辟新方向。",
    "summary_translation": "思维链（Chain-of-Thought, CoT）提示通过引导大型语言模型（Large Language Models, LLMs）生成明确的逐步推理过程，帮助它们解决复杂推理问题。然而，CoT的冗长性增加了延迟和内存使用，并可能在长链中传播早期错误。我们提出了推理胶囊（Reasoning Capsule, R-Capsule）框架，旨在将潜在推理的效率与显式CoT的透明度相结合。其核心思想是将高级计划压缩为一小组学习到的潜在标记（一个推理胶囊），同时保持执行步骤的轻量级或显式性。这种混合方法受信息瓶颈（Information Bottleneck, IB）原理的启发，我们鼓励胶囊对任务而言既近似最小又足够充分。通过低容量瓶颈来促进最小性，这有助于提高效率。通过双重目标来促进充分性：一个用于答案准确性的主要任务损失，以及一个鼓励胶囊忠实表示原始文本计划的辅助计划重建损失。重建目标有助于将潜在空间具象化，从而提高可解释性并减少无信息捷径的使用。我们的框架在效率、准确性和可解释性之间取得了平衡，从而在保持或提高复杂基准测试准确性的同时，减少了推理的可见标记占用。我们的代码可在以下网址获取：https://anonymous.4open.science/r/Reasoning-Capsule-7BE0",
    "summary_generated_time": "2025-10-06 22:42:18",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#50",
    "title": "Think Right, Not More: Test-Time Scaling for Numerical Claim Verification",
    "link": "/arxiv/2509.22101",
    "arxiv_id": "2509.22101",
    "authors": "Primakov Chungkham, V Venktesh, Vinay Setty, Avishek Anand",
    "summary": "Fact-checking real-world claims, particularly numerical claims, is inherently complex that require multistep reasoning and numerical reasoning for verifying diverse aspects of the claim. Although large language models (LLMs) including reasoning models have made tremendous advances, they still fall short on fact-checking real-world claims that require a combination of compositional and numerical reasoning. They are unable to understand nuance of numerical aspects, and are also susceptible to the reasoning drift issue, where the model is unable to contextualize diverse information resulting in misinterpretation and backtracking of reasoning process. In this work, we systematically explore scaling test-time compute (TTS) for LLMs on the task of fact-checking complex numerical claims, which entails eliciting multiple reasoning paths from an LLM. We train a verifier model (VERIFIERFC) to navigate this space of possible reasoning paths and select one that could lead to the correct verdict. We observe that TTS helps mitigate the reasoning drift issue, leading to significant performance gains for fact-checking numerical claims. To improve compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS selectively based on the perceived complexity of the claim. This approach achieves 1.8x higher efficiency than standard TTS, while delivering a notable 18.8% performance improvement over single-shot claim verification methods. Our code and data can be found at https://github.com/VenkteshV/VerifierFC",
    "subjects": "Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.930355",
    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础推理能力，而非将其作为工具应用于特定领域。论文的核心贡献是提出了一种测试时计算扩展(TTS)方法，通过生成多个推理路径并训练验证器模型(VERIFIERFC)来选择正确路径，从而增强LLM的推理能力。这属于改进LLM基础能力的研究，特别是针对多步推理和数值推理能力的提升。 其次，论文包含多个正面指标：明确涉及大语言模型(LLMs)这一核心概念；专注于推理能力(reasoning)，特别是数值推理(numerical reasoning)和多步推理(multistep reasoning)；提出了一种新的训练范式来增强模型的问题解决能力。 第三，论文不符合排除标准。虽然研究场景是事实核查，但这不是一个特定领域应用(如医疗、化学等)，而是一个通用的信息处理任务。论文不涉及多模态、视觉内容，也不关注模型基础设施或部署优化。 最后，在处理模糊情况时，我认为虽然论文在事实核查任务上评估其方法，但其核心贡献是提出了一种通用的测试时计算扩展方法来解决\"推理漂移\"问题，这种方法具有通用性，可以应用于其他需要复杂推理的任务。论文关注的是如何改进LLM本身的推理能力，而非将LLM作为工具应用到特定领域。 因此，这篇论文符合研究范围，它致力于提高大语言模型的通用推理能力，特别是数值推理和多步推理能力。",
    "summary2": "本文旨在解决大型语言模型在数值声明事实核查中的推理漂移问题。针对复杂数值声明的多步推理场景，我们提出了一种测试时计算扩展(TTS)策略和验证器模型(VERIFIER_FC)，通过生成多个推理路径并选择最佳路径来减轻推理漂移，并在QuanTemp和ClaimDecomp数据集上通过每类F1分数、宏平均和加权F1分数验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n大型语言模型在验证复杂数值声明时存在\"推理漂移\"问题，模型难以理解数值细微差别，无法正确处理多方面信息，导致误解和推理过程回溯，约34%的数值声明受此影响。\n\n## 关键洞察\n作者发现推理漂移不同于简单\"过度思考\"，它源于模型过度关注声明次要方面或错误解释多样化证据。测试时计算扩展(TTS)可能通过生成多条推理路径缓解此问题，且不同复杂度的声明应分配不同计算资源。\n\n## 解决方案演进\n首先探索TTS生成多条推理路径，然后训练验证模型选择最佳路径。进一步认识到统一TTS效率低下，进而开发基于声明复杂度的自适应机制，利用LLM层潜在表示预测复杂度，对简单声明用单次推理，复杂声明用扩展推理。\n\n## 创新点总结\n首次定义并研究事实检查中的推理漂移问题，创新性地将TTS应用于数值声明验证，提出基于复杂度的自适应TTS策略，实现计算效率提升1.8倍，性能提高18.8%。",
    "summary_translation": "对现实世界声明进行事实核查（fact-checking），特别是涉及数字的声明（numerical claims），本质上是一项复杂的工作，需要多步推理（multistep reasoning）和数字推理（numerical reasoning）来验证声明的多个方面。尽管大型语言模型（large language models, LLMs），包括推理模型（reasoning models），已经取得了巨大进步，但在需要组合推理（compositional reasoning）和数字推理相结合的事实核查任务上，它们仍然表现不佳。这些模型无法理解数字方面的细微差别，并且容易受到推理漂移（reasoning drift）问题的影响，即模型无法将多样化信息进行上下文化处理，导致推理过程中的误解和回溯。\n\n在这项工作中，我们系统地探索了扩展测试时计算（test-time compute, TTS）在大型语言模型处理复杂数字声明事实核查任务中的应用，该方法需要从大型语言模型中引出多条推理路径。我们训练了一个验证器模型（VERIFIERFC）来导航这个可能的推理路径空间，并选择能够导致正确判断的路径。我们观察到，测试时计算（TTS）有助于减轻推理漂移问题，从而在数字声明的事实核查方面带来显著的性能提升。为提高测试时计算（TTS）的计算效率，我们引入了一种自适应机制，该机制根据感知到的声明复杂性选择性地执行测试时计算。这种方法比标准测试时计算（TTS）实现了1.8倍的更高效率，同时比单次声明验证（single-shot claim verification）方法实现了显著的18.8%的性能提升。我们的代码和数据可在https://github.com/VenkteshV/VerifierFC获取。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#51",
    "title": "S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models",
    "link": "/arxiv/2509.22099",
    "arxiv_id": "2509.22099",
    "authors": "Shaoning Sun, Jiachen Yu, Zongqi Wang, Xuewei Yang, Tianle Gu, Yujiu Yang",
    "summary": "With the rapid development of large language models (LLMs), generative reward models (GRMs) have been widely adopted for reward modeling and evaluation. Previous studies have primarily focused on training specialized GRMs by optimizing them on preference datasets with the judgment correctness as supervision. While it's widely accepted that GRMs with stronger problem-solving capabilities typically exhibit superior judgment abilities, we first identify a significant solve-to-judge gap when examining individual queries. Specifically, the solve-to-judge gap refers to the phenomenon where GRMs struggle to make correct judgments on some queries (14%-37%), despite being fully capable of solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to address this problem. Specifically, S2J simultaneously leverages both the solving and judging capabilities on a single GRM's output for supervision, explicitly linking the GRM's problem-solving and evaluation abilities during model optimization, thereby narrowing the gap. Our comprehensive experiments demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%, thereby enhancing the model's judgment performance by 5.8%. Notably, S2J achieves state-of-the-art (SOTA) performance among GRMs built on the same base model while utilizing a significantly smaller training dataset. Moreover, S2J accomplishes this through self-evolution without relying on more powerful external models for distillation.",
    "subjects": "Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.930841",
    "filter_reason": "这篇论文的核心是研究生成式奖励模型(GRMs)中的\"solve-to-judge gap\"问题，并提出S2J方法来解决这个问题。从本质上看，论文致力于改进大语言模型的基础能力，特别是增强其问题解决和判断能力之间的联系，这直接属于提升LLM通用推理能力的范畴。论文涉及多个正面指标，包括明确提到large language models (LLMs)、关注problem-solving能力以及采用self-evolution训练方法。同时，论文不符合任何排除标准，没有专注于多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文提出的S2J方法通过同时利用模型在单个输出上的解决和判断能力进行监督，明确连接GRMs的问题解决和评估能力，从而提高模型的判断性能，这可以视为提升模型内在推理能力的新方法。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解决生成式奖励模型(GRM)中解决问题能力与判断能力之间的差距问题。针对GRM能够正确解决某些问题却无法正确评估相同问题响应的场景，我们提出了一种Solve-to-Judge (S2J)方法，通过在判断过程中要求模型首先自己解决问题，并结合判断奖励和解决时间解决奖励进行联合优化。在PPE Correctness、PPE Preference、Reward Bench和RMB四个基准测试上，通过判断准确率和solve-to-judge gap验证了其有效性，将差距减少了16.2%，提高了判断性能5.8%。",
    "inspiration_trace": "## 面临的挑战\n作者发现生成式奖励模型(GRMs)存在\"解决-判断差距\"：模型能正确解决的问题中，有14%-37%的情况下却无法做出正确判断。这表明模型未能充分利用其问题解决能力来支持判断能力。\n\n## 关键洞察\n作者认识到，虽然问题解决能力与判断能力总体上正相关，但在单个查询层面存在显著脱节。这种差距源于模型从解决场景转换到评估场景时性能下降，现有方法只关注判断正确性作为监督信号，忽略了显式连接两种能力的必要性。\n\n## 解决方案演进\n基于这一洞察，作者提出S2J方法：在判断过程中先要求模型自己解决问题，然后将解决过程作为判断依据。通过同时利用解决能力和判断能力作为监督信号，明确连接GRM的问题解决和评估能力。对客观任务用规则验证，对主观任务用辅助模型评分。\n\n## 创新点总结\n创新点在于首次识别并量化解决-判断差距，提出在判断过程中显式利用问题解决能力的方法，通过自我进化而非外部模型蒸馏提升性能，联合优化两种能力显著缩小差距并提高判断准确性。",
    "summary_translation": "随着大型语言模型（large language models, LLMs）的快速发展，生成式奖励模型（generative reward models, GRMs）已被广泛采用于奖励建模（reward modeling）和评估。先前的研究主要专注于通过在偏好数据集（preference datasets）上优化模型，并以判断正确性（judgment correctness）作为监督来训练专门的GRMs。尽管人们普遍认为具有更强问题解决能力的GRMs通常表现出更优越的判断能力，我们在检查单个查询时首次发现了一个显著的解决-判断差距（solve-to-judge gap）。具体而言，解决-判断差距指的是GRMs尽管完全能够解决某些问题，却难以对这些查询（14%-37%）做出正确判断的现象。在本文中，我们提出了解决到判断（Solve-to-Judge, S2J）方法来应对这一问题。具体来说，S2J同时利用单个GRM输出的解决能力和判断能力作为监督，在模型优化过程中明确链接GRM的问题解决和评估能力，从而缩小这一差距。我们的全面实验表明，S2J有效减少了16.2%的解决-判断差距，进而提高了模型5.8%的判断性能。值得注意的是，S2J在基于相同基础模型构建的GRMs中实现了最先进（state-of-the-art, SOTA）的性能，同时使用了显著更小的训练数据集。此外，S2J通过自我进化（self-evolution）实现了这一目标，而无需依赖更强大的外部模型进行蒸馏（distillation）。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#57",
    "title": "Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity",
    "link": "/arxiv/2509.22054",
    "arxiv_id": "2509.22054",
    "authors": "Ping Chen, Xiang Liu, Zhaoxiang Liu, Zezhou Chen, Xingpeng Zhang, Huan Hu, Zipeng Wang, Kai Wang, Shuming Shi, Shiguo Lian",
    "summary": "With the rapid advancement of large language models (LLMs), natural language processing (NLP) has achieved remarkable progress. Nonetheless, significant challenges remain in handling texts with ambiguity, polysemy, or uncertainty. We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM semantic priors with continuous fuzzy membership degrees, creating an explicit interaction between probability-based reasoning and fuzzy membership reasoning. This transition allows ambiguous inputs to be gradually transformed into clear and interpretable decisions while capturing conflicting or uncertain signals that traditional probability-based methods cannot. We validate FRC on sentiment analysis tasks, where both theoretical analysis and empirical results show that it ensures stable reasoning and facilitates knowledge transfer across different model scales. These findings indicate that FRC provides a general mechanism for managing subtle and ambiguous expressions with improved interpretability and robustness.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.938958",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出Fuzzy Reasoning Chain (FRC)框架，一种创新性的推理方法，用于增强大语言模型处理模糊性、歧义和不确定性的能力。论文本质上是改进LLM的基础推理能力，特别是处理不确定性文本的推理机制，而非将LLM作为工具应用于特定领域。FRC框架整合了LLM语义先验与连续模糊隶属度，创建了概率推理与模糊隶属度推理之间的显式交互，这属于提升LLM通用推理能力的研究。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确涉及Large language models (LLMs) - 能力方向：直接关注reasoning，特别是处理模糊性和不确定性的推理能力，这是通用推理的重要组成部分 - 论文提到\"stable reasoning\"和\"knowledge transfer\"，表明其关注推理能力的稳定性和泛化性 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然在情感分析任务上验证了FRC，但情感分析被视为NLP中的通用任务，而非特定应用领域。论文核心是提出通用推理框架，而非专注于特定领域应用 - 不主要聚焦于模型可靠性方面的水印、安全等问题 第四步：特殊和模糊情况 论文涉及可解释性方面，但这是作为FRC框架提升推理质量的内在特性提出的，而非单纯的应用层面讨论。论文通过改进可解释性来增强推理能力，符合保留标准。 综合判断：这篇论文提出了一个旨在增强LLM通用推理能力的新框架(FRC)，特别针对处理模糊性和不确定性这一推理核心挑战，完全符合\"大语言模型通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决自然语言处理中处理模糊性、多义性或不确定性文本的挑战。针对具有复杂情感和冲突信号的文本，我们提出了一种Fuzzy Reasoning Chain (FRC)框架，整合LLM语义先验与连续模糊隶属度，实现从概率推理到模糊隶属度推理的过渡，并在SemEval-2016 Task 4和Takeout Review Dataset上通过Robustness Score、Monotonicity Score和F1-score验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n大型语言模型在处理模糊性、多义性或不确定性的文本时面临重大挑战。传统方法基于固定标签或静态规则，无法捕捉复杂情感表达中的多维语义；现有模糊推理依赖手动定义规则，缺乏适应性；而思维链推理的离散决策过程难以处理包含讽刺和矛盾的复杂文本。\n\n## 关键洞察\n作者认识到需要将概率推理与模糊隶属度推理结合，创建从模糊到清晰的推理框架。他们观察到LLMs生成的语义隶属度具有近似鲁棒性、条件单调性和动态完整性三大特性，这为构建能处理模糊输入并产生清晰决策的框架提供了理论基础。\n\n## 解决方案演进\n作者提出\"概率到隶属度的碰撞\"作为核心创新，设计FRC框架：首先利用LLMs计算情感类别的连续隶属度；然后通过多粒度语义解析进行层次推理；最后在全局决策融合中动态调整权重，确保捕捉所有相关语义元素及其相互作用。\n\n## 创新点总结\n首次将模糊隶属度与LLM推理结合，创建从模糊到清晰的推理范式；通过概率到隶属度转换，捕捉传统方法无法表达的冲突信号；提供处理微妙模糊表达的通用机制，实现不同模型规模间的知识转移。",
    "summary_translation": "随着大型语言模型（large language models, LLMs）的快速发展，自然语言处理（natural language processing, NLP）取得了显著进展。然而，在处理具有歧义、多义性（polysemy）或不确定性的文本时，仍然存在重大挑战。我们提出了模糊推理链（Fuzzy Reasoning Chain, FRC）框架，该框架将LLM语义先验（semantic priors）与连续模糊隶属度（fuzzy membership degrees）相结合，在基于概率的推理（probability-based reasoning）和模糊隶属度推理（fuzzy membership reasoning）之间建立了明确的交互。这种转换允许将模糊输入逐步转化为清晰且可解释的决策，同时捕获传统基于概率的方法无法捕捉的冲突或不确定信号。我们在情感分析（sentiment analysis）任务上验证了FRC，理论分析和实证结果均表明，它确保了稳定的推理，并促进了跨不同模型规模的知识转移（knowledge transfer）。这些研究结果表明，FRC为处理微妙和模糊的表达提供了一种通用机制，具有更好的可解释性（interpretability）和鲁棒性（robustness）。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#60",
    "title": "GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation",
    "link": "/arxiv/2509.22009",
    "arxiv_id": "2509.22009",
    "authors": "Cehao Yang, Xiaojun Wu, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Yuanliang Sun, Jia Li, Hui Xiong, Jian Guo",
    "summary": "Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in LLMs by structurally modeling knowledge through graph-based representations. However, existing GraphRAG approaches face two core limitations: shallow retrieval that fails to surface all critical evidence, and inefficient utilization of pre-constructed structural graph data, which hinders effective reasoning from complex queries. To address these challenges, we propose \\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel retrieval for GraphRAG. \\textsc{GraphSearch} organizes the retrieval process into a modular framework comprising six modules, enabling multi-turn interactions and iterative reasoning. Furthermore, \\textsc{GraphSearch} adopts a dual-channel retrieval strategy that issues semantic queries over chunk-based text data and relational queries over structural graph data, enabling comprehensive utilization of both modalities and their complementary strengths. Experimental results across six multi-hop RAG benchmarks demonstrate that \\textsc{GraphSearch} consistently improves answer accuracy and generation quality over the traditional strategy, confirming \\textsc{GraphSearch} as a promising direction for advancing graph retrieval-augmented generation.",
    "subjects": "Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.940514",
    "filter_reason": "这篇论文的核心贡献是提出GraphSearch，一种通用的智能体深度搜索工作流，用于增强LLM的检索增强生成能力。从本质上看，论文不是将LLM应用到特定领域，而是提出了一种通用方法来增强LLM的基础推理能力，特别是事实推理和多跳推理能力。论文明确提到了\"agentic\"（智能体）概念，属于llm-based agents的新兴范式，符合\"增强其逻辑、多步推理等通用能力\"的标准。论文采用了双通道检索策略，通过模块化框架实现多轮交互和迭代推理，这些都是提升LLM通用推理能力的方法论研究。论文不涉及多模态与视觉、特定应用领域或模型可靠性等排除标准的内容。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决现有GraphRAG方法中浅层检索和图数据利用效率低下的问题。针对复杂查询场景，我们提出了一种GRAPH SEARCH代理式深度搜索工作流程，采用双通道检索策略结合语义和关系查询，并通过六个模块实现多轮交互和迭代推理。在六个多跳RAG基准测试上通过SubEM、A-Score和E-Score指标验证了其有效性，显著提升了答案准确性和生成质量。",
    "inspiration_trace": "## 面临的挑战\n现有GraphRAG方法存在两个核心局限：浅层检索无法获取复杂查询所需的所有关键证据，导致推理逻辑断裂；以及对预构建结构图数据利用效率低下，难以从复杂查询中有效推理。\n\n## 关键洞察\n作者认识到文本数据与图数据具有互补优势，需同时利用语义查询和关系查询；复杂查询需要多轮交互和迭代推理，而非单轮检索；检索过程应模块化分解，通过专门模块协同实现深度搜索。\n\n## 解决方案演进\n从单轮转向多轮交互，设计代理式工作流支持迭代推理；将检索分解为六个功能模块，分别处理查询分解、上下文细化等；设计双通道检索策略，同时处理语义和关系查询；引入反思路由机制，通过证据评估和查询扩展实现自我完善。\n\n## 创新点总结\n首次将代理概念引入GraphRAG实现多轮交互；创新性设计语义和关系双通道检索；提出模块化可插拔架构；引入反思驱动的证据完善机制，使系统能主动识别并补充证据缺口。",
    "summary_translation": "图检索增强生成（Graph Retrieval-Augmented Generation, GraphRAG）通过基于图的表示对知识进行结构化建模，从而增强大型语言模型（LLMs）的事实推理能力。然而，现有的GraphRAG方法面临两个核心限制：浅层检索（shallow retrieval）无法揭示所有关键证据，以及对预构建的结构图数据利用效率低下，这阻碍了从复杂查询中进行有效推理。为应对这些挑战，我们提出了\\textsc{GraphSearch}，一种新颖的具有双通道检索（dual-channel retrieval）功能的代理式（agentic）深度搜索工作流程，专为GraphRAG设计。\\textsc{GraphSearch}将检索过程组织成一个包含六个模块的模块化框架，支持多轮交互（multi-turn interactions）和迭代推理（iterative reasoning）。此外，\\textsc{GraphSearch}采用双通道检索策略，对基于块（chunk-based）的文本数据发出语义查询，对结构图数据发出关系查询，从而全面利用两种模态及其互补优势。在六个多跳RAG（multi-hop RAG）基准测试上的实验结果表明，与传统策略相比，\\textsc{GraphSearch}持续提高了答案准确性和生成质量，证实了\\textsc{GraphSearch}是推进图检索增强生成的一个有前景的方向。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#62",
    "title": "MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation",
    "link": "/arxiv/2509.21978",
    "arxiv_id": "2509.21978",
    "authors": "Xinping Lei, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao",
    "summary": "Large Language Models (LLMs) hold substantial potential for accelerating academic ideation but face critical challenges in grounding ideas and mitigating confirmation bias for further refinement. We propose integrating motivational knowledge graphs and socratic dialogue to address these limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework provides essential grounding and practical idea improvement steps for LLM ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node types(problem, challenge and solution) to offer motivation grounding for the LLM ideation process. The Ideator is a dual-agent system utilizing Socratic questioning, which facilitates a rigorous refinement process that mitigates confirmation bias and improves idea quality across novelty, experimental rigor, and motivational rationality dimensions. On the ICLR25 paper topics dataset, MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.",
    "subjects": "Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.941463",
    "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，该论文的本质是提出一种新框架(MotivGraph-SoIQ)来增强LLM的创意生成能力，这属于改进LLM基础能力的研究。论文通过整合动机知识图谱和苏格拉底式对话，提出了一种新的使用范式，增强了LLM的逻辑推理和问题解决能力，符合保留标准。 其次，论文包含多个正面指标：明确以LLMs为核心概念；关注reasoning能力，特别是通过苏格拉底式对话促进逻辑推理；提出了基于llm-based agents和multi-agent系统的新兴范式(Q-Driven Socratic Ideator作为双智能体系统)。 第三，论文不符合排除标准：不涉及多模态与视觉内容；虽然应用于\"学术创意生成\"，但这不是一个特定的应用领域，而是通用学术任务；没有主要聚焦于模型可靠性的应用层面。 最后，在特殊和模糊情况处理上，论文提出的双智能体系统是一种通用的智能体协作框架，用于增强LLM的通用问题解决能力，而非应用于特定领域；同时，论文通过苏格拉底式对话减少确认偏见的方法，属于提升模型通用推理质量和可靠性的新方法。 综合分析，该论文的核心贡献是提出了一种增强LLM创意生成和推理能力的新框架，符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决大型语言模型在学术创意生成中缺乏理论基础和存在确认偏见的问题。针对学术研究创意生成场景，我们提出了一种集成动机知识图谱(MotivGraph)和苏格拉底式对话(Q-Driven Socratic Ideator)的框架，并在ICLR25论文主题数据集上通过LLM评分、ELO排名和人工评估指标验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n大型语言模型在学术创意生成中存在两大核心问题：缺乏稳健的理论或事实基础，导致创意不可靠；以及确认偏见问题，使模型难以自我修正和产生新颖思考。\n\n## 关键洞察\n人类研究者通过文献综述建立学术动机连接，促进跨领域创新；导师与研究者间的讨论能有效减轻确认偏见。创造力理论表明，内在动机是突破性创意的关键组成部分。\n\n## 解决方案演进\n作者首先设计MotivGraph动机知识图谱，包含问题、挑战和解决方案三类节点，通过SciMotivMiner从论文中自动提取三元组。为解决确认偏见，受苏格拉底方法启发，构建双代理系统，让\"导师\"代理质疑\"研究者\"代理，通过批判性对话促进自我修正。最终整合为MotivGraph-SoIQ统一框架。\n\n## 创新点总结\n首次将动机知识图谱与苏格拉底对话结合，同时解决基础缺失和确认偏见问题；通过结构化三元组表示学术动机，使LLM更好理解研究逻辑；双代理系统模拟学术讨论，有效减轻偏见，全面提高创意质量。",
    "summary_translation": "大型语言模型（Large Language Models, LLMs）在加速学术创意生成（academic ideation）方面具有巨大潜力，但在思想基础构建（grounding ideas）和减轻确认偏见（confirmation bias）以进行进一步改进方面面临关键挑战。我们提出整合动机知识图谱（motivational knowledge graphs）和苏格拉底式对话（socratic dialogue），以解决增强型LLM创意生成（enhanced LLM ideation, MotivGraph-SoIQ）中的这些局限性。这一新颖框架通过将动机知识图谱（Motivational Knowledge Graph, MotivGraph）与问题驱动的苏格拉底创意生成器（Q-Driven Socratic Ideator）相结合，为LLM创意生成提供了必要的基础构建和实用的创意改进步骤。MotivGraph结构化地存储三种关键节点类型（node types）（问题、挑战和解决方案），为LLM创意生成过程提供动机基础（motivation grounding）。该创意生成器（Ideator）是一个利用苏格拉底式提问（Socratic questioning）的双代理系统（dual-agent system），它促进了一个严格的改进过程，减轻了确认偏见，并在新颖性（novelty）、实验严谨性（experimental rigor）和动机合理性（motivational rationality）维度上提高了创意质量。在ICLR25（国际学习表征会议2025）论文主题数据集上，MotivGraph-SoIQ在基于LLM的评分（LLM-based scoring）、ELO排名（ELO ranking）和人类评估指标（human evaluation metrics）方面均表现出比现有最先进方法（state-of-the-art approaches）更明显的优势。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#68",
    "title": "Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts",
    "link": "/arxiv/2509.21892",
    "arxiv_id": "2509.21892",
    "authors": "Naibin Gu, Zhenyu Zhang, Yuchen Feng, Yilong Chen, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang",
    "summary": "Mixture-of-Experts (MoE) models typically fix the number of activated experts $k$ at both training and inference. Intuitively, activating more experts at inference $k'$ (where $k'> k$) means engaging a larger set of model parameters for the computation and thus is expected to improve performance. However, contrary to this intuition, we find the scaling range to be so narrow that performance begins to degrade rapidly after only a slight increase in the number of experts. Further investigation reveals that this degradation stems from a lack of learned collaboration among experts. To address this, we introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that enables MoE models to scale the number of activated experts at inference without incurring additional training overhead. By simultaneously training experts to collaborate in diverse combinations and encouraging the router for high-quality selections, EMoE ensures robust performance across computational budgets at inference. We conduct extensive experiments on various MoE settings. Our results show that EMoE significantly expands the effective performance-scaling range, extending it to as much as 2-3$\\times$ the training-time $k$, while also pushing the model's peak performance to a higher level.",
    "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.965615",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进大语言模型的基础能力。论文提出了Elastic Mixture-of-Experts (EMoE)，这是一种新的训练框架，旨在解决MoE模型在推理时扩展激活专家数量导致的性能下降问题。论文的核心贡献是增强专家之间的协作能力，从而提高模型在推理时的性能和可扩展性。这是对LLM本身架构和训练方法的改进，而不是将LLM应用于特定领域，因此符合保留标准。 其次，从正面指标看，论文明确涉及\"Large language models, LLMs\"这一核心概念，研究的是MoE这种大语言模型架构。虽然论文没有直接提及reasoning、planning等能力方向，也没有涉及reinforcement learning等特定训练方法，但通过改进模型架构和专家协作机制，间接可能提升模型的通用推理能力。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，而是专注于模型本身的架构改进。 最后，论文没有涉及需要特殊考虑的模糊情况，如智能体/工具使用或幻觉/可解释性/安全问题。 综上所述，这篇论文通过改进MoE模型的训练框架和专家协作机制，提升了大语言模型的基础能力和推理时的性能可扩展性，符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决MoE模型在推理时扩展激活专家数量导致的性能下降问题。针对固定k值训练的MoE模型在推理时增加专家数量k'的场景，我们提出了一种Elastic Mixture-of-Experts (EMoE)框架，包含随机协同激活采样和分层路由器损失，并在多种MoE设置的九个基准数据集上通过模型性能指标验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\nMoE模型在推理时增加激活专家数量(k' > k)会导致性能迅速下降，与直觉相悖。这种限制阻碍了模型根据可用计算资源灵活扩展性能。\n\n## 关键洞察\n作者通过专家共现矩阵分析发现，性能下降源于专家间缺乏协作训练。训练时罕见的专家组合在推理时被迫协作，导致\"协作崩溃\"。训练与推理间专家共现模式的差异程度与性能下降高度相关。\n\n## 解决方案演进\n基于此洞察，作者提出双重策略：1)随机共激活采样，从更大候选池中采样专家组合进行训练，使各种组合获得协作经验；2)分层路由器损失，通过反向KL散度确保专家有明确层次排序。两者结合使模型能在不同计算预算下稳定扩展。\n\n## 创新点总结\n首次解决MoE推理时专家扩展的性能瓶颈，不增加训练成本却实现2-3倍扩展范围。通过训练策略创新而非架构改变，使小规模训练的模型能有效利用大规模推理时的专家组合，实现真正的推理时弹性扩展。",
    "summary_translation": "Mixture-of-Experts (MoE，专家混合)模型通常在训练和推理过程中固定激活专家的数量$k$。直观来看，在推理过程中激活更多专家$k'$（其中$k'> k$）意味着使用更大的模型参数集进行计算，因此预期会提高性能。然而，与这种直觉相反，我们发现扩展范围如此狭窄，以至于在专家数量仅略微增加后，性能就开始迅速下降。进一步研究表明，这种性能下降源于专家之间缺乏学习的协作。为解决这一问题，我们提出了Elastic Mixture-of-Experts (EMoE，弹性专家混合)，这是一种新颖的训练框架，使MoE模型能够在推理时扩展激活专家的数量，而不会产生额外的训练开销。通过同时训练专家以多样化组合方式进行协作，并鼓励路由器(router)进行高质量选择，EMoE确保了在推理时不同计算预算下的稳健性能。我们在各种MoE设置上进行了广泛的实验。我们的结果表明，EMoE显著扩大了有效的性能扩展范围，将其扩展到训练时$k$的2-3倍，同时也将模型的峰值性能提升到更高水平。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#70",
    "title": "No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping",
    "link": "/arxiv/2509.21880",
    "arxiv_id": "2509.21880",
    "authors": "Thanh-Long V. Le, Myeongho Jeon, Kim Vu, Viet Lai, Eunho Yang",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward - so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce RL with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR.",
    "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.967090",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的强化学习算法(RL-ZVP)，用于改进大语言模型的推理能力。论文明确指出其目标是\"improving the reasoning abilities of Large Language Models (LLMs)\"，并通过解决现有强化学习方法在处理零方差提示时的局限性来增强LLM的推理能力。这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。 其次，从正面指标分析，论文包含多个相关主题： - 核心概念：明确关注大语言模型(LLMs) - 能力方向：特别强调\"reasoning abilities\"，并在六个\"math reasoning benchmarks\"上进行测试 - 训练方法：提出了一种新的强化学习算法，属于强化学习(RL)范畴 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然在数学推理基准上测试，但数学推理被视为通用推理能力的一部分，而非特定应用领域 - 不关注模型可靠性方面的应用层面问题 论文的核心贡献是提出RL-ZVP算法，通过利用零方差提示来增强LLM的强化学习训练过程，从而提高模型的推理能力。这种方法是通用的，可以应用于提升LLM的基础推理能力，而非针对特定领域的应用。因此，这篇论文完全符合研究目标，应该被保留。",
    "summary2": "本文旨在解决LLM强化学习中零方差提示被忽略而导致的训练效率低下问题。针对所有响应获得相同奖励的零方差提示，我们提出了一种基于熵引导优势塑造的RL-ZVP算法，在六个数学推理基准测试上通过准确率和通过率验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n在LLM强化学习中，当模型对同一输入的所有响应都获得相同奖励时（\"零方差提示\"），现有GRPO等方法无法提取学习信号，导致这些提示被浪费。这些提示在训练中占比高达30%-99%，且生成响应消耗大量计算资源，忽略它们造成严重效率损失。\n\n## 关键洞察\n作者洞察到零方差提示并非无用，而是可提供有价值的学习信号：即使没有对比响应，模型仍应因正确答案获奖励、因错误答案受惩罚；且奖励/惩罚程度应根据响应中token特性（如熵）调整，高熵token（推理关键点）应获得更大更新权重。\n\n## 解决方案演进\n从洞察到方案的演进：首先识别GRPO在零方差提示上的局限性（优势值归零）；然后将零方差提示分为全正确（正提示）和全错误（负提示）两类；设计基于token熵的优势函数，为正提示提供正奖励，负提示提供负惩罚；最终形成RL-ZVP算法，在非零方差提示使用GRPO，零方差提示使用新优势函数。\n\n## 创新点总结\n创新点在于颠覆\"零方差提示无用\"的常规认知，首次从中提取学习信号；引入token级熵引导机制，实现细粒度优势塑造；将奖励塑造成功应用于LLM强化学习，解决零方差提示下学习信号缺失问题。",
    "summary_translation": "可验证奖励强化学习 (Reinforcement Learning with Verifiable Rewards, RLVR) 是提升大型语言模型 (Large Language Models, LLMs) 推理能力的强大框架。然而，当前方法如 GRPO 仅依赖于模型对同一输入的响应在正确性上存在差异的问题，而忽略了所有响应获得相同奖励的情况——即所谓的零方差提示 (zero-variance prompts)。在本研究中，我们认为这类提示并非无用，实际上可以为策略优化提供有意义的反馈。为此，我们提出了零方差提示强化学习 (RL with Zero-Variance Prompts, RL-ZVP)，这是一种从零方差提示中提取学习信号的新算法。RL-ZVP 直接奖励正确性并惩罚错误，即使在没有对比响应的情况下也能如此，同时通过 token 级特征 (token-level characteristics) 调节反馈，以保留信息丰富且细致入微的信号。在六个数学推理基准测试中，RL-ZVP 相比 GRPO 在准确率上实现了高达 8.61 分的提升，在通过率上实现了 7.77 分的提升，同时始终优于其他过滤掉零方差提示的基线方法。这些结果突显了在 RLVR 中从零方差提示学习的未开发潜力。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#76",
    "title": "ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models",
    "link": "/arxiv/2509.21826",
    "arxiv_id": "2509.21826",
    "authors": "Zihan Lin, Xiaohan Wang, Jie Cao, Jiajun Chai, Guojun Yin, Wei Lin, Ran He",
    "summary": "Large language models (LLMs) transcend passive generation and act as goal-directed agents by invoking external tools. Reinforcement learning (RL) offers a principled framework for optimizing these emergent tool-use policies, yet the prevailing paradigm relies exclusively on sparse outcome rewards and lacks consideration of the particularity of tool-use tasks, inflating policy-gradient variance and resulting in inefficient training. To better understand and address these challenges, we first establish a theoretical link between policy entropy and training stability of tool-use tasks, which reveals that structured, low-entropy tokens are primary determinants of rewards. Motivated by this insight, we propose \\textbf{Res}haped \\textbf{T}oken-level policy gradients (\\textbf{ResT}) for tool-use tasks. ResT reshapes the policy gradient through entropy-informed token reweighting, progressively upweighting reasoning tokens as training proceeds. This entropy-aware scheme enables a smooth shift from structural correctness to semantic reasoning and stabilizes convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows that ResT achieves state-of-the-art results, outperforming prior methods by up to $8.76\\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by $4.11\\%$ on single-turn tasks and $1.50\\%$ on multi-turn base tasks.",
    "subjects": "Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.975763",
    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。以下是我的详细判断过程： 第一步：核心判断——这篇论文的本质是改进LLM的工具使用能力，提出了一种新的训练范式(ResT)来优化LLM在工具使用任务中的表现。论文关注的是增强LLM本身的基础能力(工具使用)，而不是将LLM作为工具应用到特定领域。因此，根据第一步判断，应保留该论文。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确关注大型语言模型(LLMs) - 能力方向：涉及问题解决(problem-solving)能力，工具使用本身就是一种通用推理能力的体现 - 训练方法：使用强化学习(RL)框架优化工具使用策略 - 新兴范式：明确关注工具使用(tool use)，并将LLMs视为目标导向的智能体 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域(如医疗、化学等) - 不主要关注模型可靠性方面的内容(如水印、安全等) 第四步：特殊和模糊情况处理—— 论文提出的工具使用方法是通用的(ResT方法)，旨在增强LLM的通用问题解决能力，而不是针对特定领域的应用。因此，根据筛选标准，应该保留。 综合分析，这篇论文的核心贡献是提出了一种基于强化学习的新方法(ResT)来改进LLM的工具使用能力，这是一种通用推理能力。论文不是将LLM应用到特定领域，而是改进LLM本身的基础能力，完全符合研究目标。",
    "summary2": "本文旨在解决工具使用大型语言模型在强化学习训练中面临的高方差和低效问题。针对多轮工具调用任务，我们提出了一种基于熵感知的令牌级策略梯度重塑方法(ResT)，通过动态调整不同区域令牌的权重，实现从结构正确性到语义推理的平滑过渡。在BFCL和API-Bank数据集上的实验表明，ResT实现了最先进的性能，比以前的方法提高高达8.76%，并在4B基础LLM上超越GPT-4o的性能。",
    "inspiration_trace": "## 面临的挑战\n工具使用大型语言模型的强化学习训练中，现有方法依赖稀疏结果奖励，缺乏对任务特殊性的考虑，导致策略梯度方差过大，训练效率低下。多轮工具调用任务系统效率低，且规则奖励在早期训练阶段主要集中在格式标签、工具名称等低熵token上，而推理token贡献小，均匀处理所有token稀释了强化学习信号。\n\n## 关键洞察\n作者通过理论分析建立了策略熵与训练稳定性的联系：结构化的低熵token（如工具名称和参数）是奖励的主要决定因素，较低的平均熵与策略梯度更新的减少方差相关。不同token区域具有不同平均熵，影响其对奖励信号的贡献，这为优化训练提供了理论依据。\n\n## 解决方案演进\n从理论洞察出发，作者首先证明低熵token对减少梯度方差的重要性，然后提出基于熵的token重新加权机制，强调结构化低熵token。进一步设计课程学习方法，随着训练进展逐渐增加推理token权重，实现从结构正确性到语义推理的平稳过渡，最终形成ResT方法。\n\n## 创新点总结\n创新点在于首次建立策略熵与工具使用任务稳定性的理论联系，提出熵感知的token级重新加权机制，设计轻量级课程学习实现从结构到语义的自然过渡，通过理论指导的梯度重塑在保持性能的同时显著提高训练稳定性。",
    "summary_translation": "大语言模型 (Large language models, LLMs) 通过调用外部工具超越了被动生成，充当目标导向的代理 (agents)。强化学习 (Reinforcement learning, RL) 为优化这些新兴的工具使用策略提供了一个原则性框架，然而主流范式完全依赖于稀疏的结果奖励 (sparse outcome rewards)，并且缺乏对工具使用任务特殊性的考虑，导致策略梯度 (policy-gradient) 方差增大，训练效率低下。为了更好地理解和解决这些挑战，我们首先建立了策略熵 (policy entropy) 与工具使用任务训练稳定性之间的理论联系，该联系揭示了结构化的、低熵的标记 (tokens) 是奖励的主要决定因素。受此见解的启发，我们为工具使用任务提出了重塑的标记级策略梯度 (Reshaped Token-level policy gradients, ResT)。ResT 通过基于熵的标记重新加权 (entropy-informed token reweighting) 来重塑策略梯度，随着训练的进行逐步增加推理标记 (reasoning tokens) 的权重。这种感知熵的方案实现了从结构正确性到语义推理的平稳过渡，并稳定了多轮工具使用任务 (multi-turn tool-use tasks) 的收敛。在 BFCL 和 API-Bank 上的评估表明，ResT 实现了最先进 (state-of-the-art) 的结果，比先前的方法高出最多 8.76%。在 4B 基础大语言模型上进行微调 (fine-tuned) 时，ResT 在单轮任务 (single-turn tasks) 上进一步超过 GPT-4o 4.11%，在多轮基础任务 (multi-turn base tasks) 上超过 1.50%。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#87",
    "title": "Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval",
    "link": "/arxiv/2509.21710",
    "arxiv_id": "2509.21710",
    "authors": "Xiaojun Wu, Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Yuanliang Sun, Hui Xiong, Jia Li, Jian Guo",
    "summary": "Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important paradigm for enhancing Large Language Models (LLMs) with external knowledge. However, existing approaches face a fundamental trade-off. While graph-based methods are inherently dependent on high-quality graph structures, they face significant practical constraints: manually constructed knowledge graphs are prohibitively expensive to scale, while automatically extracted graphs from corpora are limited by the performance of the underlying LLM extractors, especially when using smaller, local-deployed models. This paper presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these limitations. Our core innovation is the dynamic construction and refinement of a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly incorporates a dual-evolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval. This approach addresses a critical limitation of prior Graph-based RAG methods, which typically construct a static graph index in a single pass without adapting to the actual query. A multi-agent system, comprising Constructor, Retriever, Reflector, and Responser agents, collaboratively engages in an iterative process of evidence retrieval, answer generation, sufficiency reflection, and, crucially, evolving query and subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively build a targeted graph index during reasoning, mitigating the inherent drawbacks of static, one-time graph construction and enabling deep, precise reasoning even with lightweight LLMs. Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework.",
    "subjects": "Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:34.986257",
    "filter_reason": "这篇论文的核心贡献是提出Think-on-Graph 3.0 (ToG-3)框架，通过多智能体上下文进化和检索(MACER)机制来增强大语言模型的推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，提出了一种新的多智能体协作框架和双重进化机制（进化查询和进化子图），以增强LLM的逻辑和多步推理能力，而非将LLM作为工具应用于特定领域。从第二步正面指标看，论文包含了多个相关主题：核心概念涉及LLMs，能力方向聚焦于reasoning，方法上包含evolution机制，并采用了multi-agent systems这一新兴范式。论文不涉及第三步中的任何排除标准（多模态与视觉、特定应用领域、模型可靠性应用层面）。在第四步特殊情况处理中，论文提出的是通用的多智能体协作框架来增强LLM的通用推理能力，而非针对特定领域的应用。综合分析，该论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决基于图的检索增强生成(RAG)系统在资源受限和本地部署场景下面临的图质量依赖问题。针对使用轻量级LLMs构建的静态图存在不完整三元组、细节不足和解析失败等问题，我们提出了一种名为Think-on-Graph 3.0 (ToG-3)的新框架，该框架引入了Multi-Agent Context Evolution and Retrieval (MACER)机制，创新性地结合了Evolving Query和Evolving Sub-Graph双重进化机制。在HotpotQA、2WikiMultihopQA和Musique等深度推理基准测试以及UltraDomain广度推理任务上，通过Exact Match、F1分数和综合评估指标验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n现有图RAG方法面临基本权衡：高质量图结构依赖与实际应用限制的矛盾。手动构建知识图谱成本过高难以扩展，而自动提取图又受限于LLM提取器性能，尤其在使用轻量级本地模型时。静态图构建无法适应实际查询需求，限制了推理深度和准确性。\n\n## 关键洞察\n作者认识到传统图RAG的核心局限在于静态图索引构建方式——一次性构建而不适应实际查询。真正需要的不是完美初始图，而是能随推理过程动态演化的图结构。这种\"双重演化\"机制（查询演化和子图演化）可从可能不完美的初始图开始，通过迭代专门针对特定查询推理路径优化。\n\n## 解决方案演进\n基于此洞察，作者设计了Chunk-Triplets-Community异构图架构，统一细粒度和高级信息。然后引入多智能体框架（MACER），包含四个智能体形成迭代循环：Reflector评估上下文充分性并生成精确子查询，Constructor根据子查询扩展精炼子图。这种双重演化使系统能自适应构建针对查询的图索引。\n\n## 创新点总结\n创新点在于：1）首次在图RAG中引入查询和图结构协同演化的双重机制；2）通过多智能体协作实现动态上下文演化，突破静态预构建图限制；3）使轻量级LLM在资源受限环境下实现深度推理，解决传统方法对高质量图或强大LLM的依赖。",
    "summary_translation": "检索增强生成(Retrieval-Augmented Generation, RAG)和基于图的检索增强生成(Graph-based RAG)已成为用外部知识增强大型语言模型(Large Language Models, LLMs)的重要范式。然而，现有方法面临一个基本的权衡问题。虽然基于图的方法本质上依赖于高质量的图结构，但它们面临重大的实际约束：手动构建的知识图谱(knowledge graphs)扩展成本过高，而从语料库中自动提取的图则受制于底层LLM提取器的性能，尤其是在使用较小的、本地部署的模型时。\n\n本文提出了Think-on-Graph 3.0 (ToG-3)，这是一个引入多智能体上下文演进与检索(Multi-Agent Context Evolution and Retrieval, MACER)机制的新型框架，旨在克服这些局限性。我们的核心创新是动态构建和优化块-三元组-社区(Chunk-Triplets-Community)异构图索引，该索引首创性地结合了演进查询(Evolving Query)和演进子图(Evolving Sub-Graph)的双演进机制，用于精确的证据检索。这种方法解决了先前基于图的RAG方法的一个关键局限性，这些方法通常一次性构建静态图索引，而无法适应实际查询。\n\n一个由构造器(Constructor)、检索器(Retriever)、反射器(Reflector)和响应器(Responser)智能体组成的多智能体系统，协作参与证据检索、答案生成、充分性反思，以及至关重要的演进查询和子图的迭代过程。这种双演进多智能体系统使ToG-3能够在推理过程中自适应地构建有针对性的图索引，减轻了静态一次性图构建的固有缺点，并即使在使用轻量级LLMs时也能实现深度、精确的推理。大量实验表明，ToG-3在深度和广度推理基准测试中均优于比较基线，而消融研究(ablation studies)证实了MACER框架各组件的有效性。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#99",
    "title": "On Code-Induced Reasoning in LLMs",
    "link": "/arxiv/2509.21499",
    "arxiv_id": "2509.21499",
    "authors": "Abdul Waheed, Zhen Wu, Carolyn Rosé, Daphne Ippolito",
    "summary": "Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. We investigate this question with a systematic, data-centric framework. We construct parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. We then finetune LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, our results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through our systematic framework, we aim to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities.",
    "subjects": "Computation and Language, Programming Languages",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.065422",
    "filter_reason": "这篇论文的核心贡献是研究代码数据如何增强大语言模型(LLM)的推理能力，特别是数学和逻辑推理能力。论文通过系统性的数据中心框架，构建了10种编程语言的并行指令数据集，并应用受控扰动来研究代码的结构和语义属性对LLM推理能力的影响。研究结果表明，LLM对结构扰动的脆弱性高于语义扰动，特别是在数学和代码任务上。这项研究直接关注如何通过代码数据改进LLM的基础推理能力，符合\"改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。论文不是将LLM作为工具应用到特定领域，而是研究如何提升LLM本身的通用推理能力，因此完全符合研究目标。",
    "summary2": "本文旨在探究代码数据中哪些方面能够增强大型语言模型(LLM)的推理能力。针对代码的结构和语义特性，我们提出了一种系统性的数据驱动框架，通过构建10种编程语言的并行指令数据集并应用受控扰动，在5个模型家族和8个规模的LLM上进行了3,331次实验，通过自然语言、数学和代码任务上的性能指标验证了代码结构特性对推理能力的关键影响。",
    "inspiration_trace": "## 面临的挑战\n代码数据能增强LLMs推理能力，但尚不清楚代码的哪些方面最关键——是语法规则性、结构抽象还是语言风格？缺乏对代码影响推理机制的系统性理解。\n\n## 关键洞察\n代码对推理的影响可能是多方面的，需要通过控制变量来解构代码的不同属性。作者意识到，要理解代码如何影响推理，必须设计实验分别测试结构属性和语义属性的独立贡献。\n\n## 解决方案演进\n从洞察到方法的演进：1)构建自然语言与代码的平行数据集确保可比性；2)设计系统性扰动——规则性扰动(如移除空白符)和生成性扰动(如转为伪代码)；3)在5个模型家族8个规模上进行3,331个实验；4)多维度评估不同扰动对推理任务的影响。\n\n## 创新点总结\n创新点在于提出系统性数据中心框架，通过受控扰动解构代码属性影响；探索代码抽象表示(如伪代码、流程图)的推理价值；揭示结构属性比语义属性更关键，以及不同编程语言对任务特定增益的影响。",
    "summary_translation": "代码数据（Code data）已被证明可以增强大型语言模型（Large Language Models, LLMs）的推理能力，但尚不清楚代码的哪些方面对此贡献最大。我们通过一个系统的、以数据为中心的框架来研究这个问题。我们构建了十种编程语言的并行指令数据集（parallel instruction datasets），并应用了可控扰动（controlled perturbations）来选择性地破坏代码的结构或语义属性。然后，我们在每个变体上对来自五个模型家族（model families）和八个规模（scales）的LLMs进行微调（finetune），并评估它们在自然语言、数学和代码任务上的表现。在3,331个实验中，我们的结果表明，LLMs对结构扰动（structural perturbations）比语义扰动（semantic perturbations）更敏感，尤其是在数学和代码任务上。适当的抽象（abstractions）如伪代码（pseudocode）和流程图（flowcharts）可以与代码一样有效，而用更少的标记（tokens）编码相同信息且不遵循原始语法（syntax）的方式通常可以保持甚至提高性能。值得注意的是，即使带有误导信号的损坏代码（corrupted code），只要保持表面规律性（surface-level regularities），仍然具有竞争力。最后，语法风格（syntactic styles）也会影响特定任务的收益，Python有利于自然语言推理，而Java和Rust等低级语言则有利于数学推理。通过我们的系统框架，我们旨在深入理解代码的不同特性如何影响推理，并为增强LLM推理能力的训练数据设计提供参考。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#101",
    "title": "Learning to Reason with Mixture of Tokens",
    "link": "/arxiv/2509.21482",
    "arxiv_id": "2509.21482",
    "authors": "Adit Jain, Brendan Rappazzo",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a leading approach for improving large language model (LLM) reasoning capabilities. Most current methods follow variants of Group Relative Policy Optimization, which samples multiple reasoning completions, scores them relative to each other, and adjusts the policy accordingly. However, these approaches invariably sample discrete tokens at each reasoning step, discarding the rich distributional information in the model's probability distribution over candidate tokens. While preserving and utilizing this distributional information has proven beneficial in non-RL settings, current RLVR methods seem to be unnecessarily constraining the reasoning search space by not using this information. To address this limitation, we investigate mixture-of-token generation (MoT-G) in RLVR. We present a unified framework that generalizes existing MoT-G approaches, including existing training-free methods that construct mixture embeddings as weighted sums over token embeddings, and extend RLVR to operate directly in this continuous mixture space for generating chain-of-thought. Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive language tasks, we find that MoT--G methods achieve substantial improvements (5--35 \\% gains on 7 out of 10 tasks) compared to standard decoding with the Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of trajectories, suggesting improved training efficiency. Through comprehensive hidden-state and token-level analyses, we provide evidence that MoT--G's benefits may stem from its ability to maintain higher hidden-state entropy throughout the reasoning process and promote exploration in token space.",
    "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.072085",
    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，论文的核心是关于改进LLM的推理能力，具体研究了强化学习与可验证奖励(RLVR)方法，提出了一种名为\"token混合生成\"(MoT-G)的新方法来增强模型的推理过程。论文明确针对LLM在每个推理步骤采样离散token时丢弃分布信息的问题，通过在连续混合空间中操作来生成思维链，这直接提升了模型的基础推理能力。 从正面指标看，论文包含了多个相关主题：核心概念涉及大语言模型(LLMs)，能力方向聚焦于推理(reasoning)，训练方法采用强化学习(RL)的变体，并研究了思维链(Chain-of-Thought)生成。论文在Reasoning-Gym这一推理密集型任务套件上进行了评估，证明了其方法的有效性。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。相反，它专注于提高LLM的通用推理能力，而不是将LLM作为工具应用到特定领域。 综上所述，这篇论文的核心贡献是通过改进token级别的采样策略来增强LLM的推理能力，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
    "summary2": "本文旨在提高大型语言模型在推理任务中的表现。针对强化学习与可验证奖励(RLVR)框架下离散令牌采样导致的推理搜索空间受限问题，我们提出了一种混合令牌生成(MoT-G)方法，在每一步推理中维护多个令牌的分布而非离散选择。在Reasoning-Gym推理任务套件上通过准确率和轨迹效率验证了其有效性，MoT-G在7/10任务上实现5-35%性能提升，且仅需一半轨迹数即可达到相当准确率。",
    "inspiration_trace": "## 面临的挑战\n当前强化学习与可验证奖励(RLVR)方法在每个推理步骤都采样离散令牌，丢弃了模型概率分布中的丰富分布信息，迫使模型做出早期不可逆决策，限制了探索替代推理路径的能力。\n\n## 关键洞察\n作者认识到保留分布信息在非RL设置中已被证明有益，但RLVR方法却忽视了这一点，不必要地限制了推理搜索空间。混合令牌表示可能帮助模型维持不确定性并增强探索能力。\n\n## 解决方案演进\n作者首先提出统一的混合令牌生成(MoT-G)框架，概括现有方法并将RLVR扩展到连续混合空间。然后设计两种变体：Dirichlet随机加权top-k令牌和采样多令牌归一化加权。通过实验验证其在推理任务中的有效性。\n\n## 创新点总结\n将连续表示引入RLVR框架，打破传统离散采样限制，通过维持更高隐藏状态熵和促进令牌空间探索，更有效平衡探索与利用，提高推理性能和训练效率。",
    "summary_translation": "可验证奖励强化学习(Reinforcement learning with verifiable rewards, RLVR)已成为提升大型语言模型(Large Language Model, LLM)推理能力的主要方法。当前大多数方法采用群组相对策略优化(Group Relative Policy Optimization)的变体，该方法采样多个推理完成结果，对它们进行相对评分，并相应地调整策略。然而，这些方法在每个推理步骤中总是采样离散token(离散标记)，丢弃了模型在候选token上的概率分布中丰富的分布信息。尽管在非强化学习(non-RL)环境中，保留和利用这种分布信息已被证明是有益的，但当前的RLVR方法似乎因未使用此信息而不必要地限制了推理搜索空间。\n\n为解决这一限制，我们在RLVR中研究了token混合生成(Mixture-of-token Generation, MoT-G)。我们提出了一个统一框架，该框架概括了现有的MoT-G方法，包括将混合嵌入(mixture embeddings)构建为token嵌入(token embeddings)加权和的现有无训练方法，并将RLVR扩展到直接在此连续混合空间(continuous mixture space)中操作以生成思维链(chain-of-thought)。在Reasoning-Gym（一套推理密集型语言任务）上评估两种MoT-G变体，我们发现与Qwen2.5-1.5B模型的标准解码(standard decoding)相比，MoT-G方法取得了显著改进（在10项任务中的7项上提升了5-35%），同时仅用一半的轨迹(trajectories)数量就达到了相当的准确率，表明训练效率有所提高。通过全面的隐藏状态(hidden-state)和token级别(token-level)分析，我们提供了证据表明MoT-G的优势可能源于其在整个推理过程中保持更高隐藏状态熵(hidden-state entropy)的能力，以及在token空间(token space)中促进探索的能力。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#100",
    "title": "Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning",
    "link": "/arxiv/2509.21487",
    "arxiv_id": "2509.21487",
    "authors": "Jillian Xu, Dylan Zhou, Vinay Shukla, Yang Yang, Junrui Ruan, Shuhuai Lin, Wenfei Zou, Yinxiao Liu, Karthik Lakshmanan",
    "summary": "Chain-of-Thought (CoT) prompting often improves classification accuracy, but it introduces a significant throughput penalty with rationale generation (Wei et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for decoder-only language models (LMs) that adds (i) a pooled classification head used during training and inference and (ii) a reasoning head supervised by teacher rationales used only in training. We train with a loss function that is a weighted sum of label cross-entropy and token-level LM loss over input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative gains of 0.65-5.47% over pooled baselines, with notably larger gains on entailment/causal tasks. Since we disable the reasoning head at test time, inference throughput matches pooled classifiers and exceeds CoT decoding on the same backbones by 96-142 times in QPS.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.071340",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是我的详细分析： 第一步：核心判断 这篇论文的本质是提出一种名为\"Dual-Head Reasoning Distillation (DHRD)\"的新训练方法，旨在提高语言模型的推理能力，同时解决思维链(CoT)推理效率低下的问题。论文明确关注改进LLM的基础推理能力，提出了一种新的训练范式，属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的范畴，因此应该保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确涉及\"decoder-only language models (LMs)\" - 能力方向：多次强调\"reasoning\"，特别是\"Chain-of-Thought (CoT)\"和\"reasoning head\"，并在摘要中提到在\"entailment/causal tasks\"上取得更大增益，这些都直接涉及逻辑推理能力 - 训练方法：提出DHRD这一新的训练方法，虽然不是强化学习或进化方法，但确实是一种创新训练范式 第三步：排除标准 论文没有主要聚焦于任何排除标准中的领域： - 不涉及多模态与视觉内容 - 实验在通用SuperGLUE任务上进行，而非特定应用领域 - 不关注模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况。 综上所述，这篇论文的核心贡献是提出一种新的训练方法来增强LLM的推理能力，同时保持推理效率，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文关注的是LLM基础能力的改进，而非将其作为工具应用于特定领域，因此应该被保留。",
    "summary2": "本文旨在解决Chain-of-Thought提高分类准确性但引入吞吐量惩罚的问题。针对仅解码器语言模型分类任务，我们提出了Dual-Head Reasoning Distillation (DHRD)方法，在训练时结合池化分类头和推理头，并在七个SuperGLUE任务上通过准确率和F1值验证了其有效性，实现了0.65-5.47%的相对增益，同时保持了96-142倍的推理吞吐量提升。",
    "inspiration_trace": "## 面临的挑战\n作者面临的核心问题是：将decoder-only语言模型应用于分类任务时，存在准确率与效率的权衡。简单池化分类器效率高但无法充分利用模型的推理能力；而思维链(CoT)虽能提升准确率，却因逐token解码导致吞吐量大幅下降，不适用于高吞吐量场景。\n\n## 关键洞察\n作者洞察到推理过程的价值主要体现在提升模型理解能力，而非必须在推理时显式生成。关键突破点在于：可以将推理的计算成本从推理时间转移到训练时间，通过\"输入-推理-标签\"三元组的对齐来提升模型性能，而非仅依赖通用语言模型正则化。\n\n## 解决方案演进\n作者首先思考如何在保持推理效率的同时获取CoT的准确率优势。接着突破传统知识蒸馏的软标签匹配思路，提出双头架构：分类头用于训练和推理，推理头仅用于训练。最后设计加权损失函数平衡分类和推理目标，使模型学习推理能力但推理时无需生成推理过程。\n\n## 创新点总结\n创新点在于实现了\"训练时推理，推理时分类\"的新范式，通过双头架构巧妙结合分类与语言建模优势，在不增加推理成本情况下显著提升准确率，特别在需要推理的任务上效果更佳。",
    "summary_translation": "思维链(Chain-of-Thought, CoT)提示通常能提高分类准确率，但通过理由生成(rationale generation)会带来显著的吞吐量(throughput)损失(Wei et al., 2022; Cheng and Van Durme, 2024)。为解决这一权衡问题，我们提出了双头推理蒸馏(Dual-Head Reasoning Distillation, DHRD)，这是一种针对仅解码器语言模型(decoder-only language models, LMs)的简单训练方法，该方法增加了(i)在训练和推理过程中使用的池化分类头(pooled classification head)和(ii)仅在训练中使用、由教师理由(teacher rationales)监督的推理头(reasoning head)。我们使用的损失函数是标签交叉熵(label cross-entropy)和在输入加理由序列(input-plus-rationale sequences)上的词元级语言模型(token-level LM)损失的加权和。在七项SuperGLUE任务上，与池化基线(pooled baselines)相比，DHRD实现了0.65-5.47%的相对提升，在蕴含/因果(entailment/causal)任务上的提升尤为显著。由于我们在测试时禁用了推理头，推理吞吐量与池化分类器相当，并且在相同骨干网络上以QPS(每秒查询数)计算，比CoT解码高出96-142倍。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#106",
    "title": "How Large Language Models Need Symbolism",
    "link": "/arxiv/2509.21404",
    "arxiv_id": "2509.21404",
    "authors": "Xiaotie Deng, Hanyu Li",
    "summary": "We argue that AI's future requires more than scaling. To unlock genuine discovery, large language models need a compass: human-crafted symbols to guide their powerful but blind intuition.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-24",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.074858",
    "filter_reason": "这篇论文的核心是探讨如何通过符号系统(symbolism)来增强大语言模型(LLM)的推理能力，而不是将LLM应用于特定领域。从摘要中可以看出，论文认为AI的未来不仅仅是扩大规模，还需要人类设计的符号来指导LLM强大的但\"盲目\"的直觉，以实现真正的发现。这符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。论文没有涉及多模态与视觉、特定应用领域或模型可靠性等排除标准中的内容。虽然摘要中没有明确提到\"reasoning\"、\"planning\"等具体能力方向，但\"unlock genuine discovery\"暗示了与推理和问题解决相关的内容。因此，这篇论文应该被保留，因为它关注的是改进LLM的基础能力，提出一种新的范式（结合符号系统）来增强其通用推理能力。",
    "summary2": "本文旨在解决大型语言模型(LLMs)在数据稀缺场景下的推理局限。针对复杂前沿领域，我们提出了一种将人类智慧符号作为认知技术来增强LLMs统计直觉的方法，并在AlphaGeometry系统上通过国际数学奥林匹克竞赛(IMO)金牌水平表现验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n大型语言模型通过\"扩展法则\"取得成功，但这一范式在数据稀缺的复杂前沿领域失效。LLMs依赖统计模式匹配，而非人类式的逻辑推理和抽象，导致在需要创新的多步推理任务中表现不佳。\n\n## 关键洞察\n作者认识到符号作为\"认知技术\"的核心价值。人类通过\"商化\"能力，从广阔问题空间创建紧凑符号空间，简化复杂性。符号不是概念本身，而是增强思维的工具，如数字语言帮助记忆和比较，莱布尼茨微积分符号促进直觉理解。\n\n## 解决方案演进\n作者提出将符号用作压缩人类智慧的容器，引导LLMs的统计直觉。这与早期符号AI不同，不是构建刚性系统，而是创造符号与统计能力的协同。如AlphaGeometry结合符号语言与神经符号引擎，LLM提出关键辅助线，演绎器高效探索结果。\n\n## 创新点总结\n创新在于将符号主义与现代LLMs创造性结合，形成新范式。符号作为认知技术，特别适用于数据稀缺的复杂领域，为LLMs提供\"指南针\"，实现真正发现，而非仅依赖扩展法则。",
    "summary_translation": "我们认为，人工智能（AI）的未来需要的不仅仅是规模扩大（scaling）。要实现真正的发现（genuine discovery），大型语言模型（large language models）需要一个指南针：人类创造的符号（human-crafted symbols）来引导其强大但盲目的直觉。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#109",
    "title": "A Novel Differential Feature Learning for Effective Hallucination Detection and Classification",
    "link": "/arxiv/2509.21357",
    "arxiv_id": "2509.21357",
    "authors": "Wenkai Wang, Vincent Lee, Yizhen Zheng",
    "summary": "Large language model hallucination represents a critical challenge where outputs deviate from factual accuracy due to distributional biases in training data. While recent investigations establish that specific hidden layers exhibit differences between hallucinatory and factual content, the precise localization of hallucination signals within layers remains unclear, limiting the development of efficient detection methods. We propose a dual-model architecture integrating a Projected Fusion (PF) block for adaptive inter-layer feature weighting and a Differential Feature Learning (DFL) mechanism that identifies discriminative features by computing differences between parallel encoders learning complementary representations from identical inputs. Through systematic experiments across HaluEval's question answering, dialogue, and summarization datasets, we demonstrate that hallucination signals concentrate in highly sparse feature subsets, achieving significant accuracy improvements on question answering and dialogue tasks. Notably, our analysis reveals a hierarchical \"funnel pattern\" where shallow layers exhibit high feature diversity while deep layers demonstrate concentrated usage, enabling detection performance to be maintained with minimal degradation using only 1\\% of feature dimensions. These findings suggest that hallucination signals are more concentrated than previously assumed, offering a pathway toward computationally efficient detection systems that could reduce inference costs while maintaining accuracy.",
    "subjects": "Computation and Language, Artificial Intelligence",
    "date": "2025-09-20",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.076261",
    "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是关于改进大语言模型的基础能力，特别是解决幻觉问题，而不是将LLM作为工具应用到特定领域。论文提出的差分特征学习机制旨在检测和分类幻觉内容，这直接关系到LLM输出的事实准确性，是提升模型基础能力的重要方面。 其次，论文符合正面指标中的\"核心概念\"，明确讨论了大语言模型的幻觉问题。虽然论文没有直接涉及推理、规划等能力方向，但幻觉检测与这些能力密切相关，因为幻觉往往发生在模型进行复杂推理过程中，减少幻觉可以间接提高模型的推理质量和可靠性。 第三，论文不涉及排除标准中的任何领域，它既不是关于多模态与视觉，也不是将LLM应用到特定领域，也不是从应用层面讨论模型可靠性。 最后，根据第四步对特殊情况的处理，论文提出了一种新方法来检测幻觉，属于\"幻觉/可解释性/安全\"类别，并且是从模型内部机制的角度进行研究，目的是提升模型的通用可靠性，因此应该保留。 论文的核心贡献是提出了一种双模型架构和差分特征学习机制，用于检测和分类LLM的幻觉内容，并发现幻觉信号集中在高度稀疏的特征子集中。这些发现有助于开发计算高效的检测系统，减少推理成本同时保持准确性，从而提升LLM的通用推理能力和输出质量，符合研究目标。",
    "summary2": "本文旨在解决大型语言模型中的幻觉检测问题。针对LLMs输出的事实不准确内容，我们提出了一种双模型架构，包含Projected Fusion块和Differential Feature Learning机制，并在HaluEval基准测试的问答、对话和摘要任务上通过准确率、精确率、召回率和F1分数验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n大型语言模型常产生幻觉，现有研究虽发现特定隐藏层在幻觉与事实内容间有差异，但幻觉信号在层内的精确位置尚不清楚。层级分析方法处理整个隐藏表示，未检查内部特征重要性，导致检测效率低下。\n\n## 关键洞察\n作者推测幻觉信号可能高度集中在特定特征子集中，而非均匀分布。通过双模型架构(一个专注幻觉检测，一个专注事实识别)可创建差异化错误模式，突出最具判别性的特征维度。\n\n## 解决方案演进\n首先设计双模型架构处理相同输入学习互补表示；然后提出Projected Fusion块整合不同层信息；接着开发Differential Feature Learning机制计算模型间差异；最后选择差异最大的特征维度，创建稀疏掩码。实验证实仅需1%特征即可保持检测性能。\n\n## 创新点总结\n首次探索幻觉信号在特征维度级别的精确位置，发现\"漏斗模式\"(浅层多样性高，深层集中使用)，证明极少量特征足以检测幻觉，为高效检测系统提供新路径。",
    "summary_translation": "大型语言模型幻觉（large language model hallucination）代表着一个关键挑战，即由于训练数据中的分布偏差（distributional biases），模型输出偏离了事实准确性。尽管近期研究已确定特定隐藏层（hidden layers）在幻觉内容（hallucinatory content）和事实内容（factual content）之间表现出差异，但幻觉信号（hallucination signals）在层内的精确定位仍不明确，这限制了高效检测方法的开发。我们提出了一种双模型架构（dual-model architecture），集成了投影融合（Projected Fusion, PF）块用于自适应层间特征加权（adaptive inter-layer feature weighting），以及差分特征学习（Differential Feature Learning, DFL）机制，该机制通过计算从相同输入学习互补表示（complementary representations）的并行编码器（parallel encoders）之间的差异来识别判别性特征（discriminative features）。通过对HaluEval数据集的问答（question answering）、对话（dialogue）和摘要（summarization）任务进行系统实验，我们证明了幻觉信号集中在高度稀疏的特征子集（highly sparse feature subsets）中，并在问答和对话任务上实现了显著的准确性提升。值得注意的是，我们的分析揭示了一种分层的\"漏斗模式\"（hierarchical \"funnel pattern\"），其中浅层（shallow layers）表现出高特征多样性（high feature diversity），而深层（deep layers）则表现出集中使用（concentrated usage），这使得仅使用1%的特征维度（feature dimensions）就能保持检测性能，仅有最小程度的性能下降。这些研究结果表明，幻觉信号比先前假设的更为集中，为开发计算高效的检测系统提供了一条途径，这些系统可以在保持准确性的同时降低推理成本（inference costs）。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#113",
    "title": "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback",
    "link": "/arxiv/2509.22633",
    "arxiv_id": "2509.22633",
    "authors": "Gen Li, Yuling Yan",
    "summary": "Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner. By examining existing optimism-based exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated by this insight, we propose a new exploration scheme that directs preference queries toward reducing uncertainty in reward differences most relevant to policy improvement. Under a multi-armed bandit model of RLHF, we establish regret bounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter that balances reward maximization against mitigating distribution shift. To our knowledge, this is the first online RLHF algorithm with regret scaling polynomially in all model parameters.",
    "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning, Statistics Theory",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.123045",
    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文本质上是关于改进强化学习与人类反馈(RLHF)的在线探索效率。RLHF是提高大语言模型与人类偏好对齐的核心训练范式，属于LLM基础能力的改进。论文提出的新探索方案旨在更有效地收集偏好数据以改进奖励模型和策略，这直接关系到提升LLM的通用能力，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文明确包含核心概念\"Large language models (LLMs)\"，并专注于\"Reinforcement learning with human feedback (RLHF)\"这一重要训练方法。虽然摘要未直接提及推理、规划等能力，但RLHF作为对齐方法，能够间接提升这些通用能力。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域，而是关注通用的RLHF方法改进。 第四步特殊和模糊情况：论文不涉及智能体/工具使用或幻觉/可解释性/安全等需要特殊判断的内容。 综合分析，这篇论文的核心贡献是提出了一种新的RLHF在线探索方案，通过更有效地收集偏好数据来改进奖励模型和策略，从而提升LLM与人类偏好的对齐效果。这属于改进LLM基础训练范式的研究，直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
    "summary2": "本文旨在解决强化学习中基于人类反馈的在线探索效率问题。针对在线RLHF场景，我们提出了一种基于不确定性的探索方案，通过动态校准策略引导偏好查询，减少与策略改进最相关的奖励差异不确定性。在多臂老虎机模型上，我们建立了T^(β+1)/(β+2)阶的遗憾界，其中β是平衡奖励最大化和减轻分布偏移的超参数。这是首个在所有模型参数上具有多项式缩放的在线RLHF算法。",
    "inspiration_trace": "## 面临的挑战\n现有基于乐观主义的在线RLHF探索算法存在根本缺陷：它们收集的比较数据无法有效减少与策略改进最相关的奖励差异不确定性。作者证明这些方法在特定情况下会导致指数级长时间范围内的线性遗憾，探索效率极低。\n\n## 关键洞察\n作者认识到问题本质在于探索方向与不确定性减少目标的不匹配。固定校准策略（如参考策略或静态策略）无法适应动态学习过程，导致探索集中在非信息量丰富的比较上。关键洞察是：校准策略应随学习过程动态演进，引导探索针对最相关的不确定性。\n\n## 解决方案演进\n从\"比较当前策略与固定校准策略\"的初始想法，演进到\"动态更新校准策略\"的突破。最终方案是在每次迭代中使用当前策略作为校准策略，并比较连续两个策略生成的动作，直接减少与策略改进最相关的不确定性。\n\n## 创新点总结\n首次提出动态校准策略的探索范式，使校准策略随学习过程自适应演进。通过针对最相关不确定性进行定向探索，实现了多项式级别的遗憾界限，突破了现有方法指数级依赖的理论瓶颈。",
    "summary_translation": "人类反馈强化学习(Reinforcement learning with human feedback, RLHF)通过从人类偏好数据中学习奖励模型(reward model)，然后优化策略(policy)以产生偏好的响应，已成为将大型语言模型(Large language models, LLMs)与人类偏好对齐的核心范式。在本文中，我们研究了在线RLHF(online RLHF)的探索原则(exploration principles)，其中目标是自适应地收集新的偏好数据，以数据高效的方式(data-efficient manner)同时改进奖励模型和策略。通过检查现有的基于乐观的探索算法(optimism-based exploration algorithms)，我们发现了它们采样协议(sampling protocol)中的一个缺点：它们倾向于收集那些无法减少奖励差异(reward differences)中最信息量大的不确定性(informative uncertainties)的比较，并且我们证明了下界(lower bounds)表明这类方法在指数级长的时间范围(exponentially long horizons)内可能产生线性遗憾(linear regret)。基于这一见解，我们提出了一种新的探索方案(exploration scheme)，该方案将偏好查询(preference queries)引导至减少与策略改进(policy improvement)最相关的奖励差异中的不确定性。在RLHF的多臂老虎机模型(multi-armed bandit model)下，我们建立了$T^{(\\beta+1)/(\\beta+2)}$阶的遗憾界限(regret bounds)，其中$\\beta>0$是一个平衡奖励最大化(reward maximization)与缓解分布偏移(mitigating distribution shift)的超参数(hyperparameter)。据我们所知，这是第一个在所有模型参数(model parameters)上具有多项式缩放(polynomially scaling)遗憾的在线RLHF算法。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#118",
    "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning",
    "link": "/arxiv/2509.22576",
    "arxiv_id": "2509.22576",
    "authors": "Xu Wujiang, Wentian Zhao, Zhenting Wang, Li Yu-Jhe, Jin Can, Jin Mingyu, Mei Kai, Wan Kun, Metaxas Dimitris",
    "summary": "Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.",
    "subjects": "Machine Learning, Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.131868",
    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种名为EPO（熵正则化策略优化）的新框架，用于解决LLM智能体在强化学习中的训练问题，特别是针对多轮环境中稀疏奖励的挑战。这明显属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标：核心概念上明确关注\"LLM Agents\"；能力方向上涉及LLM智能体在多轮环境中的决策和推理，与reasoning和problem-solving相关；训练方法上提出了强化学习方法（EPO）；新兴范式上属于llm-based agents的研究。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉问题；虽然使用了ScienceWorld和ALFWorld作为测试环境，但这些是通用智能体测试平台，而非特定应用领域；也没有关注模型可靠性方面的水印、安全等问题。 最后，在特殊和模糊情况处理上，论文提出的EPO框架是一种通用的智能体强化学习方法，旨在增强LLM智能体在多轮环境中的通用问题解决能力，而非针对特定领域的应用。 论文的核心贡献是提出了一种解决LLM智能体在多轮稀疏奖励环境中训练挑战的新方法，通过熵正则化策略优化来改善探索-利用平衡，从而提升LLM智能体的通用推理和决策能力，这与研究目标高度一致。",
    "summary2": "本文旨在解决LLM智能体在多轮稀疏奖励环境中的训练挑战。针对多轮交互中的探索-利用级联失败问题，我们提出了一种EPO（熵正则化策略优化）框架，通过轨迹感知熵计算、熵平滑正则化和自适应阶段权重三个机制来稳定训练。在ScienceWorld和ALFWorld基准测试上通过成功率指标验证了其有效性，实现了高达152%的性能提升。",
    "inspiration_trace": "## 面临的挑战\n作者发现在多轮稀疏奖励环境中训练LLM智能体时，存在独特的\"探索-利用级联失败\"问题：早期阶段因稀疏反馈导致智能体过早收敛到有缺陷策略，晚期阶段传统熵正则化反而促进混乱探索，破坏训练稳定性。\n\n## 关键洞察\n作者认识到标准熵正则化在多轮环境中不足的根本原因是缺乏\"时间感知\"能力。关键洞见是将策略熵锚定到动态调整的历史边界可提供必要稳定性，阻止级联失败同时保持必要探索。\n\n## 解决方案演进\n作者首先将熵正则化适应多轮设置，计算轨迹内所有轮次的熵并在批次上平均；然后引入熵平滑正则化器，惩罚与历史熵平均值的偏差；最后开发自适应加权方案，动态平衡训练各阶段的探索和利用。\n\n## 创新点总结\n创新点在于首次识别并形式化多轮环境中的级联失败现象，提出基于历史熵锚定的稳定机制，设计自适应阶段性权重调整，证明多轮稀疏奖励需要与传统RL根本不同的熵控制方法。",
    "summary_translation": "在稀疏奖励(sparse rewards)的多轮(multi-turn)环境中训练大型语言模型(LLM)代理，其中完成单个任务需要在一个回合(episode)内进行30次以上的交互，这对强化学习(reinforcement learning)提出了根本性挑战。我们确定了这种设置特有的一个关键失效模式：探索-利用(exploration-exploitation)级联失效(cascade failure)。这种级联失效始于早期策略(early-stage policy)过早收敛(premature convergence)，其中稀疏反馈(sparse feedback)导致代理坚持有缺陷的低熵(low-entropy)策略。随后，代理进入后期策略(late-stage policy)崩溃(collapse)阶段，此时传统的熵正则化(entropy regularization)变得适得其反，促进混乱探索(chaotic exploration)从而破坏训练稳定性。我们提出了熵正则化策略优化(Entropy-regularized Policy Optimization, EPO)，这是一个通过三种协同机制打破这种失效循环的通用框架：(1)在多轮(multi-turn)设置中采用熵正则化(entropy regularization)以增强探索，(2)一种熵平滑(entropy smoothing)正则化器(regularizer)，将策略熵限制在历史平均值内以防止剧烈波动，以及(3)基于阶段的自适应(adaptive phase-based)加权(weighting)，在训练过程中平衡探索和利用。我们的分析证明了EPO在保证收敛性(convergence)的同时，确保熵方差(entropy variance)单调递减(monotonically decreasing)。EPO在ScienceWorld上实现了高达152%的性能提升，在ALFWorld上实现了高达19.8%的性能提升。我们的研究表明，多轮稀疏奖励(multi-turn sparse-reward)设置需要与传统强化学习(traditional RL)根本不同的熵控制(entropy control)，这对大型语言模型(LLM)代理训练具有广泛影响。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#115",
    "title": "IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning",
    "link": "/arxiv/2509.22621",
    "arxiv_id": "2509.22621",
    "authors": "Aayush Mishra, Daniel Khashabi, Anqi Liu",
    "summary": "Supervised Fine-Tuning (SFT) is used to specialize model behavior by training weights to produce intended target responses for queries. In contrast, In-Context Learning (ICL) adapts models during inference with instructions or demonstrations in the prompt. ICL can offer better generalizability and more calibrated responses compared to SFT in data scarce settings, at the cost of more inference compute. In this work, we ask the question: Can ICL's internal computations be used to improve the qualities of SFT? We first show that ICL and SFT produce distinct activation patterns, indicating that the two methods achieve adaptation through different functional mechanisms. Motivated by this observation and to use ICL's rich functionality, we introduce ICL Activation Alignment (IA2), a self-distillation technique which aims to replicate ICL's activation patterns in SFT models and incentivizes ICL-like internal reasoning. Performing IA2 as a priming step before SFT significantly improves the accuracy and calibration of model outputs, as shown by our extensive empirical results on 12 popular benchmarks and 2 model families. This finding is not only practically useful, but also offers a conceptual window into the inner mechanics of model adaptation.",
    "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.124579",
    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式IA2（ICL Activation Alignment），通过将上下文学习(ICL)的激活模式对齐到监督微调(SFT)过程中，来增强模型的内部推理能力。这属于改进LLM的基础能力和提出新的训练范式的研究，旨在增强模型的推理能力，符合保留标准。 其次，从正面指标分析，论文明确包含以下相关主题： 1. 核心概念：研究大语言模型(LLMs)的SFT和ICL技术 2. 能力方向：明确提到\"incentivizes ICL-like internal reasoning\"（鼓励类似ICL的内部推理），直接关注提升模型的推理能力 3. 训练方法：提出了一种自我蒸馏技术(self-distillation technique)，属于新的训练方法范畴 第三，论文不符合任何排除标准： 1. 不涉及多模态与视觉内容 2. 不聚焦于任何特定应用领域（如医疗、化学等） 3. 虽然提到了模型的\"校准性\"(calibration)，但这不是其主要焦点，而是作为改进的一个方面 最后，论文没有涉及特殊或模糊的情况，如智能体/工具使用或幻觉/可解释性/安全等主题。 论文的核心贡献是通过激活对齐技术，将ICL的内部推理优势转移到SFT模型中，从而提高模型的推理能力和输出质量。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
    "summary2": "本文旨在 [提高Supervised Fine-Tuning (SFT) 的质量和校准性]。针对 [数据稀缺场景下的模型适应问题]，我们提出了一种 [ICL Activation Alignment (IA²) 方法，通过在SFT模型中复制ICL的激活模式来鼓励类似ICL的内部推理]，并在 [12个流行基准测试和2个模型家族] 上通过 [准确性和预期校准误差(ECE)] 验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\nSFT在数据稀缺时需要大量标记样本才能泛化，成本高昂；而ICL虽在少样本下泛化性好且响应校准，但推理成本高且占用上下文空间。现有方法仅从输出层面蒸馏ICL，无法确保模型功能与ICL一致。\n\n## 关键洞察\n作者发现ICL和SFT虽表面行为相似，但内部激活模式截然不同，表明二者通过不同机制实现适应。ICL激活包含丰富的可泛化模式提取信息，而SFT在少样本下易学习捷径。激活模式差异揭示了模型适应的内在机制差异。\n\n## 解决方案演进\n从激活差异出发，作者提出能否利用ICL激活改进SFT。设计目标使SFT模型在处理查询时产生与ICL相似的激活模式。提出IA2自蒸馏技术，先收集ICL激活并对齐，再进行标准SFT，形成\"功能对齐+输出对齐\"的两步流程。\n\n## 创新点总结\n首次从激活空间探索ICL与SFT差异并利用此改进SFT；不仅关注输出对齐，更注重功能对齐，将ICL的泛化性与校准优势融入SFT；为理解模型适应内部机制提供了新视角。",
    "summary_translation": "监督微调（Supervised Fine-Tuning, SFT）通过训练权重来产生针对查询的预期目标响应，从而专门化模型行为。相比之下，上下文学习（In-Context Learning, ICL）在推理过程中通过提示中的指令或示例来调整模型。在数据稀缺的情况下，与SFT相比，ICL能够提供更好的泛化能力（generalizability）和更校准的响应（calibrated responses），但代价是需要更多的推理计算（inference compute）。在本研究中，我们提出一个问题：ICL的内部计算能否被用来提升SFT的质量？我们首先展示ICL和SFT产生不同的激活模式（activation patterns），表明这两种方法通过不同的功能机制（functional mechanisms）实现适应。基于这一观察并为了利用ICL的丰富功能，我们引入了ICL激活对齐（ICL Activation Alignment, IA2），这是一种自蒸馏（self-distillation）技术，旨在SFT模型中复制ICL的激活模式，并促进类似ICL的内部推理。我们在12个流行基准（benchmarks）和2个模型家族（model families）上的广泛实证结果（empirical results）表明，将IA2作为SFT之前的启动步骤（priming step）执行，能显著提高模型输出的准确性和校准度。这一发现不仅具有实际应用价值，还为理解模型适应的内部机制（inner mechanics）提供了概念窗口。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#119",
    "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time",
    "link": "/arxiv/2509.22572",
    "arxiv_id": "2509.22572",
    "authors": "Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang",
    "summary": "Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning.",
    "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.132368",
    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。根据筛选标准的分析： 第一步核心判断：论文本质上是关于改进LLM的基础推理能力，提出了一种名为\"Dynamic Experts Search (DES)\"的测试时缩放策略，通过动态控制混合专家(MoE)模型中激活专家的数量来增强模型的推理能力。这直接对应了\"改进LLM的基础能力\"和\"增强其逻辑、数学、多步推理等通用能力\"的研究目标。 第二步正面指标：论文明确包含两个关键正面指标：(1)核心概念LLMs，研究的是混合专家大语言模型；(2)推理能力方向，标题直接提及\"Enhancing Reasoning\"，并在实验中测试了数学、代码和知识推理基准。虽然论文未涉及强化学习训练方法和智能体等新兴范式，但已足够表明其与研究课题高度相关。 第三步排除标准：论文不符合任何排除标准。它不涉及多模态与视觉内容，不专注于特定应用领域（数学、代码和知识推理是通用能力测试而非特定领域应用），也不关注模型可靠性方面的水印、安全等问题。 论文的核心贡献是提出了一种利用模型架构特性（专家激活）来增强LLM推理能力的新方法，这种方法不需要额外计算成本就能提高模型在各种推理任务上的准确性和稳定性，完全符合提高LLM本身通用推理能力的研究目标。",
    "summary2": "本文旨在 [解决Test-Time Scaling方法忽略模型架构作用的问题]。针对 [Mixture-of-Experts LLMs]，我们提出了一种 [Dynamic Experts Search方法，将专家激活作为搜索空间的可控维度]，并在 [多种推理基准] 上通过 [准确性和稳定性指标] 验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n现有Test-Time Scaling方法主要依赖输出层面采样增强LLMs推理能力，却忽略了模型架构的作用。这些方法将模型内部计算视为架构无关的，尤其在MoE架构中未能利用其灵活激活专家的特性。\n\n## 关键洞察\n作者发现MoE模型中改变激活专家数量会产生互补解决方案集，同时保持稳定准确性。不同专家数量解决的问题集有低重叠性，这揭示了专家激活是一个未被充分利用的多样性来源，可转化为搜索空间的可控维度。\n\n## 解决方案演进\n从这一洞察出发，作者首先设计Dynamic MoE使专家数量成为推理时可调参数。随后提出Expert Configuration Inheritance，在单一路径内保持专家数量一致，不同路径间变化配置。两者结合将专家激活提升为搜索空间的新维度，平衡了稳定性和多样性。\n\n## 创新点总结\n开创了\"架构感知的TTS\"新范式，首次将MoE专家激活作为可控维度。通过结构灵活性而非参数扩展提升性能，在不增加计算成本的情况下增强推理能力，为现代LLMs的结构灵活性利用提供了新思路。",
    "summary_translation": "测试时缩放（Test-Time Scaling, TTS）通过在推理过程中分配额外计算资源来增强大型语言模型（Large Language Models, LLMs）的推理能力。然而，现有方法主要依赖于输出层面的采样，而忽视了模型架构的作用。在主流的专家混合（Mixture-of-Experts, MoE）大型语言模型中，我们观察到改变激活专家的数量会产生具有稳定准确性的互补解集，这揭示了一个新的且未被充分探索的多样性来源。基于这一观察，我们提出了动态专家搜索（Dynamic Experts Search, DES），这是一种将专家激活提升为搜索空间可控维度的TTS策略。DES集成了两个关键组件：(1) 动态MoE（Dynamic MoE），它能够在推理过程中直接控制专家数量，以生成多样化的推理轨迹而无需额外成本；(2) 专家配置继承（Expert Configuration Inheritance），它在单次推理路径内保持一致的专家数量，而在不同运行之间变化，从而在整个搜索过程中平衡稳定性和多样性。在多种MoE架构、验证器和推理基准（即数学、代码和知识）上的广泛实验表明，DES可靠地超越了TTS基线方法，在无需额外成本的情况下提高了准确性和稳定性。这些结果突显了DES作为一种实用且可扩展的架构感知TTS形式，展示了现代大型语言模型中的结构灵活性如何能够推进推理能力。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#124",
    "title": "Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers",
    "link": "/arxiv/2509.22445",
    "arxiv_id": "2509.22445",
    "authors": "Peter Shaw, James Cohan, Jacob Eisenstein, Kristina Toutanova",
    "summary": "The Minimum Description Length (MDL) principle offers a formal framework for applying Occam's razor in machine learning. However, its application to neural networks such as Transformers is challenging due to the lack of a principled, universal measure for model complexity. This paper introduces the theoretical notion of asymptotically optimal description length objectives, grounded in the theory of Kolmogorov complexity. We establish that a minimizer of such an objective achieves optimal compression, for any dataset, up to an additive constant, in the limit as model resource bounds increase. We prove that asymptotically optimal objectives exist for Transformers, building on a new demonstration of their computational universality. We further show that such objectives can be tractable and differentiable by constructing and analyzing a variational objective based on an adaptive Gaussian mixture prior. Our empirical analysis shows that this variational objective selects for a low-complexity solution with strong generalization on an algorithmic task, but standard optimizers fail to find such solutions from a random initialization, highlighting key optimization challenges. More broadly, by providing a theoretical framework for identifying description length objectives with strong asymptotic guarantees, we outline a potential path towards training neural networks that achieve greater compression and generalization.",
    "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.140370",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础能力，而非将其作为工具应用于特定领域。论文提出了基于Kolmogorov复杂度的渐近最优描述长度目标，这是一种新的理论框架，旨在优化Transformer模型的复杂度和泛化能力。这属于\"改进LLM的基础能力\"的范畴，与思维链、强化学习等方法论研究类似，都是致力于提升模型内在能力的理论探索。 其次，从正面指标分析，论文明确涉及Transformers这一大语言模型的核心架构，讨论了泛化能力（这是通用推理能力的基础），并提出了新的变分目标函数作为训练/优化方法。虽然论文没有直接讨论数学推理或逻辑推理，但优化模型复杂度和泛化能力是提升通用推理能力的重要基础。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是建立了一个理论框架，用于训练具有更强压缩和泛化能力的神经网络，这直接关系到提升LLM的通用推理能力。通过优化模型的复杂度和泛化能力，可以间接提升模型在各种推理任务上的表现。因此，这篇论文符合研究目标，应该被保留。",
    "summary2": "",
    "inspiration_trace": "## 面临的挑战\nMDL原则在神经网络应用中缺乏原则性、通用的模型复杂度度量；现有压缩方法无法捕获所有潜在规律性，导致次优压缩和泛化；算法信息理论与实际神经网络间存在概念鸿沟，难以量化神经网络计算函数的复杂性。\n\n## 关键洞察\nKolmogorov复杂性提供相对于任何可计算描述长度的最优压缩，其普适性根植于Church-Turing论题；Transformers具有计算普适性，能表示任何可计算的有理值条件概率分布；通过构建渐近最优编码族，可在资源限制增加时逼近最优压缩。\n\n## 解决方案演进\n从定义通用两部件编码理论框架开始，证明其存在性并建立与Kolmogorov复杂性的联系；通过证明Transformers能模拟通用前缀图灵机，确立其计算普适性；为解决实际应用局限，构建基于自适应高斯混合先验的可微分变分目标；最后通过算法任务实验验证理论框架。\n\n## 创新点总结\n建立了Kolmogorov复杂性与深度学习间的形式化桥梁；证明了存在渐近最优编码族，提供普适性保证；提出实用变分框架使理论可应用于Transformers；识别并实证了优化这些目标的挑战，为未来研究指明方向。",
    "summary_translation": "最小描述长度（Minimum Description Length, MDL）原则为在机器学习中应用奥卡姆剃刀原理（Occam's razor）提供了一个形式化框架。然而，由于缺乏一个原则性的、通用的模型复杂度度量标准，将其应用于Transformer（变换器）等神经网络具有挑战性。本文介绍了渐近最优描述长度目标（asymptotically optimal description length objectives）的理论概念，该概念基于Kolmogorov复杂度（Kolmogorov complexity）理论。我们证明，随着模型资源界限的增加，对于任何数据集，这种目标的最小化器都能达到最优压缩，最多相差一个加性常数。我们基于对Transformer计算普适性（computational universality）的新证明，证明了渐近最优目标对于Transformer的存在性。我们进一步表明，通过构建和分析一个基于自适应高斯混合先验（adaptive Gaussian mixture prior）的变分目标（variational objective），这样的目标可以是可处理的且可微的。我们的实证分析表明，这个变分目标在一个算法任务上选择了一个具有强泛化能力（generalization）的低复杂度解，但标准优化器无法从随机初始化中找到这样的解，这凸显了关键的优化挑战。更广泛地说，通过提供一个用于识别具有强渐近保证的描述长度目标的理论框架，我们概述了一条训练能够实现更好压缩和泛化能力的神经网络的潜在路径。",
    "summary_generated_time": "2025-10-06 22:42:17",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#132",
    "title": "A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning",
    "link": "/arxiv/2509.22044",
    "arxiv_id": "2509.22044",
    "authors": "Ziqi Wang, Boye Niu, Zhongli Li, Linghui Meng, Jing Liu, Zhi Zheng, Tong Xu, Hua Wu, Haifeng Wang, Enhong Chen",
    "summary": "Recent Large Reasoning Models have achieved significant improvements in complex task-solving capabilities by allocating more computation at the inference stage with a \"thinking longer\" paradigm. Even as the foundational reasoning capabilities of models advance rapidly, the persistent gap between a model's performance in a single attempt and its latent potential, often revealed only across multiple solution paths, starkly highlights the disparity between its realized and inherent capabilities. To address this, we present A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge the gap between a model's potential and its actual performance. In this framework, an \"explorer\" model first generates potential solutions in parallel through repeated sampling. Subsequently,a \"synthesizer\" model integrates these references for a more refined, second stage of reasoning. This two-stage process allows computation to be scaled orthogonally to existing sequential methods. Our work makes two key innovations: First, we present A2R as a plug-and-play parallel reasoning framework that explicitly enhances a model's capabilities on complex questions. For example, using our framework, the Qwen3-8B-distill model achieves a 75% performance improvement compared to its self-consistency baseline. Second, through a systematic analysis of the explorer and synthesizer roles, we identify an effective asymmetric scaling paradigm. This insight leads to A2R-Efficient, a \"small-to-big\" variant that combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration surpasses the average performance of a monolithic Qwen3-32B model at a nearly 30% lower cost. Collectively, these results show that A2R is not only a performance-boosting framework but also an efficient and practical solution for real-world applications.",
    "subjects": "Artificial Intelligence, Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.145663",
    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种名为A2R的非对称两阶段推理框架，旨在提升大语言模型的基础推理能力。论文明确针对的是模型在单次尝试中的表现与其潜在能力之间的差距，通过\"探索者\"和\"综合者\"两个模型的协作来增强LLM的通用推理能力，而不是将LLM作为工具应用于特定领域。 其次，论文包含多个正面指标： - 核心概念：明确研究Large Reasoning Models，使用Qwen3模型作为实验对象 - 能力方向：直接聚焦于推理(reasoning)能力的提升，标题和摘要多次强调\"reasoning\" - 新兴范式：提出了\"explorer\"和\"synthesizer\"的协作框架，可视为一种多智能体系统的简化形式 第三，论文不涉及任何排除标准中的领域： - 没有多模态或视觉相关内容 - 没有针对医疗、化学、生物等特定应用领域 - 没有讨论水印、安全性等模型可靠性问题 在特殊和模糊情况处理上，论文提出的两阶段框架是一种通用的推理增强方法，而非针对特定领域的应用，因此应该保留。 论文的核心贡献是提出了一种可插拔的并行推理框架，通过非对称扩展模型计算能力来提升LLM在复杂问题上的表现，这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标。实验结果表明，该框架能显著提升模型性能（如Qwen3-8B-distill模型实现75%性能提升），同时提供了一种高效的计算扩展方式。",
    "summary2": "本文旨在解决大型推理模型在单次尝试性能与潜在能力之间的差距问题。针对复杂推理任务，我们提出了一种A2R非对称两阶段推理框架，通过Explorer并行生成多个解决方案，再由Synthesizer进行整合和二次推理，并在AIME 2024、AIME 2025和BeyondAIME等数学推理基准上通过pass@1准确率验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n大型推理模型在单次尝试中的表现与其在多路径中展现的潜在能力之间存在显著差距。现有并行推理方法存在计算冗余、信息无法共享，以及聚合阶段仅做简单选择而不进行额外推理等局限。此外，单路径推理中的\"前缀陷阱\"问题会导致整个推理轨迹不可逆转地偏离。\n\n## 关键洞察\n作者认识到推理计算可正交于现有方法进行扩展，是一种与参数扩展和顺序推理互补的改进维度。通过系统分析发现，合成器的内在推理能力是最终性能上限的关键决定因素，而非简单路由器。探索阶段占主导计算成本，而合成阶段是计算较轻的任务。\n\n## 解决方案演进\n作者首先将推理解耦为探索和合成两个互补阶段，引入合成器对完整推理链集进行生成式再推理。通过实验验证合成器能力与性能提升的强相关性后，提出A2R-Efficient不对称架构：用小模型探索多样化路径，大模型执行最终再推理，实现资源优化分配。\n\n## 创新点总结\n通过显式生成式再推理桥接模型潜力与实际表现的差距；揭示合成器能力是性能瓶颈，提出\"小探索器，大合成器\"高效配置；在降低30%计算成本的同时，实现与更大单体模型相当的性能。",
    "summary_translation": "近期的大型推理模型（Large Reasoning Models）通过在推理阶段采用\"思考更久\"（thinking longer）范式分配更多计算资源，在复杂任务解决能力方面取得了显著进步。尽管模型的基础推理能力正在迅速提升，但模型单次尝试的表现与其潜在能力之间持续存在的差距——这种潜力通常只有在多个解决路径中才能显现——鲜明地凸显了其已实现能力与固有能力之间的差异。为解决这一问题，我们提出了A2R，一种非对称两阶段推理（Asymmetric Two-Stage Reasoning）框架，旨在明确弥合模型潜力与实际表现之间的差距。\n\n在该框架中，\"探索者\"（explorer）模型首先通过重复采样并行生成潜在解决方案。随后，\"综合者\"（synthesizer）模型整合这些参考方案，进行更精细的第二阶段推理。这种两阶段过程使得计算能够以正交于现有顺序方法的方式进行扩展。\n\n我们的工作有两项关键创新：首先，我们将A2R呈现为一个即插即用（plug-and-play）的并行推理框架，明确提升模型在复杂问题上的能力。例如，使用我们的框架，Qwen3-8B-distill模型相比其自洽性（self-consistency）基线实现了75%的性能提升。其次，通过对探索者和综合者角色的系统分析，我们确定了一种有效的非对称扩展（asymmetric scaling）范式。这一见解催生了A2R-Efficient，一种\"小到大\"（small-to-big）的变体，结合了Qwen3-4B探索者和Qwen3-8B综合者。这种配置以近30%的更低成本超越了单体Qwen3-32B模型的平均性能。\n\n总体而言，这些结果表明A2R不仅是一个性能提升框架，还是一种高效且实用的现实应用解决方案。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#133",
    "title": "The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging",
    "link": "/arxiv/2509.22034",
    "arxiv_id": "2509.22034",
    "authors": "Xiaochong Lan, Yu Zheng, Shiteng Cao, Yong Li",
    "summary": "The growing demand for large language models (LLMs) with tunable reasoning capabilities in many real-world applications highlights a critical need for methods that can efficiently produce a spectrum of models balancing reasoning depth and computational cost. Model merging has emerged as a promising, training-free technique to address this challenge by arithmetically combining the weights of a general-purpose model with a specialized reasoning model. While various merging techniques exist, their potential to create a spectrum of models with fine-grained control over reasoning abilities remains largely unexplored. This work presents a large-scale empirical study evaluating a range of model merging techniques across multiple reasoning benchmarks. We systematically vary merging strengths to construct accuracy-efficiency curves, providing the first comprehensive view of the tunable performance landscape. Our findings reveal that model merging offers an effective and controllable method for calibrating the trade-off between reasoning accuracy and token efficiency, even when parent models have highly divergent weight spaces. Crucially, we identify instances of Pareto Improvement, where a merged model achieves both higher accuracy and lower token consumption than one of its parents. Our study provides the first comprehensive analysis of this tunable space, offering practical guidelines for creating LLMs with specific reasoning profiles to meet diverse application demands.",
    "subjects": "Artificial Intelligence, Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.146141",
    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究如何通过模型合并(model merging)技术来调整和增强大语言模型本身的推理能力，而不是将LLM作为工具应用到特定领域。论文提出的方法允许在推理深度和计算成本之间进行权衡，这属于改进LLM基础能力的范畴。 其次，论文符合多个正面指标：它明确研究大语言模型(LLMs)，并特别关注\"tunable reasoning capabilities\"，在多个推理基准上进行评估，这直接对应了核心概念和能力方向这两个正面指标。 第三，论文不符合任何排除标准：它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，论文不涉及特殊或模糊情况，它直接关注如何通过模型合并来调整和增强LLM的通用推理能力，而不是将智能体/工具应用在特定领域，也不是对幻觉/可解释性/安全的社会学研究或应用层面讨论。 论文的核心贡献是提供了一种通过模型合并技术来调整LLM推理能力的方法，使研究者能够在推理准确性和计算效率之间进行权衡，这直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
    "summary2": "本文旨在解决如何高效创建具有可调节推理能力的大语言模型谱系的问题。针对思考模型和直接模型这两种极端情况，我们提出了一种通过模型合并技术来创建具有不同推理深度和计算效率平衡的模型谱系，并在多个推理基准测试上通过推理准确率和token效率验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n当前大语言模型处于\"思维谱系\"两极：深度思考模型准确性高但计算成本大，直接响应模型速度快但推理能力有限。实际应用需要在这两极间取得平衡，而现有调节推理能力的方法需大量训练资源，不适用于资源受限场景。\n\n## 关键洞察\n作者将深度思考和直接响应视为两种不同的元能力，洞察到即使这两类模型在参数空间存在显著差异，它们仍可能位于一个连接的低损失盆地中，使通过模型合并创建中间权衡模型成为可能。\n\n## 解决方案演进\n作者先验证了思维与直接模型间参数差异远大于领域专业模型，质疑简单合并有效性。但通过大规模实验，发现多种合并技术都能有效创建推理能力可调的模型谱系，甚至出现帕累托改进现象。基于此，提出模型合并类似于采样连续训练轨迹中间检查点的假设。\n\n## 创新点总结\n首次系统探索模型合并用于调节推理能力；发现即使面对巨大参数差异合并仍有效；观察到推理行为的非线性相变；证明合并模型可超越父模型；为创建特定推理能力的模型提供实用指导。",
    "summary_translation": "在许多实际应用中，对具有可调节推理能力的大语言模型（large language models, LLMs）日益增长的需求，凸显了对能够高效生成一系列平衡推理深度和计算成本的模型的方法的迫切需求。模型合并（model merging）已成为一种有前景的、无需训练（training-free）的技术，通过算术组合通用模型（general-purpose model）和专用推理模型（specialized reasoning model）的权重来应对这一挑战。尽管存在各种合并技术，但它们在创建具有细粒度（fine-grained）推理能力控制的模型谱系方面的潜力仍未得到充分探索。本研究提出了一项大规模实证研究（empirical study），评估了多种模型合并技术在多个推理基准（reasoning benchmarks）上的表现。我们系统地改变合并强度（merging strengths）以构建准确率-效率曲线（accuracy-efficiency curves），首次全面展示了可调节性能的概况。我们的研究结果表明，模型合并提供了一种有效且可控的方法来校准推理准确性（reasoning accuracy）和令牌效率（token efficiency）之间的权衡，即使在父模型（parent models）具有高度发散的权重空间（weight spaces）的情况下也是如此。至关重要的是，我们发现了帕累托改进（Pareto Improvement）的实例，即合并模型比其父模型之一实现了更高的准确性和更低的令牌消耗。我们的研究首次全面分析了这一可调节空间，为创建具有特定推理特征（reasoning profiles）的大语言模型以满足不同应用需求提供了实用指南。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#126",
    "title": "PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning",
    "link": "/arxiv/2509.22315",
    "arxiv_id": "2509.22315",
    "authors": "Hieu Tran, Zonghai Yao, Nguyen Luong Tran, Zhichao Yang, Feiyun Ouyang, Shuo Han, Razieh Rahimi, Hong Yu",
    "summary": "Inspired by the dual-process theory of human cognition from \\textit{Thinking, Fast and Slow}, we introduce \\textbf{PRIME} (Planning and Retrieval-Integrated Memory for Enhanced Reasoning), a multi-agent reasoning framework that dynamically integrates \\textbf{System 1} (fast, intuitive thinking) and \\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick Thinking Agent (System 1) to generate a rapid answer; if uncertainty is detected, it then triggers a structured System 2 reasoning pipeline composed of specialized agents for \\textit{planning}, \\textit{hypothesis generation}, \\textit{retrieval}, \\textit{information integration}, and \\textit{decision-making}. This multi-agent design faithfully mimics human cognitive processes and enhances both efficiency and accuracy. Experimental results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to perform competitively with state-of-the-art closed-source models like GPT-4 and GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This research establishes PRIME as a scalable solution for improving LLMs in domains requiring complex, knowledge-intensive reasoning.",
    "subjects": "Artificial Intelligence, Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.141438",
    "filter_reason": "这篇论文的核心贡献是提出PRIME（Planning and Retrieval-Integrated Memory for Enhanced Reasoning），一个多智能体推理框架，通过动态整合快速思考（System 1）和慢速思考（System 2）来增强大语言模型的推理能力。该框架包含专门的智能体进行规划、假设生成、检索、信息整合和决策，明显属于改进LLM基础推理能力的研究。论文专注于提高LLM在多跳和基于知识的推理任务上的表现，这正是\"通用推理能力\"的核心要素。同时，论文提出的多智能体协作框架是一种新兴范式，符合筛选标准中的正面指标。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究目标。",
    "summary2": "本文旨在 [提高大型语言模型在复杂推理任务中的性能和效率]。针对 [多跳和知识密集型推理任务]，我们提出了一种 [受人类双过程认知理论启发的多智能体推理框架PRIME，结合System 1快速思考和System 2深思熟虑]，并在 [医学和多跳推理基准测试] 上通过 [准确率和F1分数] 验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n现有LLM推理方法存在两难困境：快速直观推理计算高效但易产生幻觉；深度分析推理准确但资源密集。如何在保持高准确性的同时有效利用计算资源，避免不必要的深度推理，是当前AI推理系统的核心挑战。\n\n## 关键洞察\n作者从人类认知双过程理论获得独特视角：人脑能在系统1（快速直觉）和系统2（缓慢分析）间灵活切换，关键在于自我反思机制——只有当直觉不足时才启动深度思考。这种选择性触发机制是平衡效率与准确性的关键。\n\n## 解决方案演进\n从双过程理论出发，先设计系统1快速思考代理进行结构化子问题分解；引入反思代理评估答案可靠性；当检测到不确定性时，触发模块化系统2，包含规划、检索、假设生成和决策等专业代理，形成完整的\"快速思考-反思-深度推理\"流程。\n\n## 创新点总结\n首次在多智能体框架中明确操作化双过程理论，通过反思机制选择性触发深度推理，既减少计算开销又提高准确性。模块化专业智能体协作模拟人脑认知过程，实现了效率与性能的平衡。",
    "summary_translation": "受《思考，快与慢》（\\textit{Thinking, Fast and Slow}）中人类认知的双过程理论启发，我们提出了\\textbf{PRIME}（Planning and Retrieval-Integrated Memory for Enhanced Reasoning，用于增强推理的规划与检索集成记忆框架），这是一个动态整合\\textbf{System 1}（系统1，快速、直觉性思维）和\\textbf{System 2}（系统2，缓慢、深思熟虑思维）的多智能体推理框架。PRIME首先采用快速思维代理（Quick Thinking Agent，System 1）生成快速答案；如果检测到不确定性，则会触发一个结构化的System 2推理流程，该流程由专门负责\\textit{planning}（规划）、\\textit{hypothesis generation}（假设生成）、\\textit{retrieval}（检索）、\\textit{information integration}（信息整合）和\\textit{decision-making}（决策）的专门智能体组成。这种多智能体设计忠实地模拟了人类认知过程，同时提高了效率和准确性。使用LLaMA 3模型的实验结果表明，在需要多跳推理（multi-hop reasoning）和基于知识的推理（knowledge-grounded reasoning）的基准测试中，PRIME使开源大语言模型（open-source LLMs）能够与最先进的闭源模型（closed-source models）如GPT-4和GPT-4o竞争。这项研究确立了PRIME作为在需要复杂、知识密集型推理（knowledge-intensive reasoning）的领域中改进大语言模型的可扩展解决方案。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#144",
    "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios",
    "link": "/arxiv/2509.21766",
    "arxiv_id": "2509.21766",
    "authors": "Haotian Luo, Huaisong Zhang, Xuelin Zhang, Haoyu Wang, Zeyu Qin, Wenjie Lu, Guozheng Ma, Haiying He, Yingsha Xie, Qiyang Zhou, Zixuan Hu, Hongze Mi, Yibo Wang, Naiqiang Tan, Hong Chen, Yi R. Fung, Chun Yuan, Li Shen",
    "summary": "Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce \\textbf{UltraHorizon} a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average \\textbf{200k+} tokens and \\textbf{400+} tool calls, whereas in standard configurations they still exceed \\textbf{35k} tokens and involve more than \\textbf{60} tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. \\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available here.}",
    "subjects": "Artificial Intelligence, Computation and Language",
    "date": "2025-09-26",
    "category": "cs.CL",
    "crawl_time": "2025-10-06T19:08:35.157848",
    "filter_reason": "这篇论文的核心贡献是提出了UltraHorizon基准测试，用于评估LLM智能体在超长期场景中的能力，包括持续推理、规划、记忆管理和工具使用等通用能力。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从本质上看，论文并非将LLM作为工具应用到特定领域，而是关注LLM本身在通用推理能力方面的表现和不足。论文提出的基准测试旨在测量LLM智能体在复杂现实世界挑战中的基础能力，特别是长期推理、规划和工具使用等通用能力，这符合第一步中\"改进LLM的基础能力\"的保留标准。 其次，论文包含多个正面指标：明确关注LLM-agents（核心概念），涉及reasoning、planning、problem-solving（能力方向），以及tool use（新兴范式）。这些都是与研究目标\"提高大语言模型的通用推理能力\"直接相关的主题。 第三，论文不主要聚焦于排除标准中提到的领域。虽然论文提到了\"大规模软件开发、商业投资和科学发现\"等现实世界任务，但这些只是作为例子来说明其基准测试的应用场景，而不是将LLM应用到特定领域进行研究。 最后，在处理特殊和模糊情况方面，论文提出的是一个通用的基准测试，用于评估LLM智能体在长期任务中的能力，而不是将智能体/工具应用在特定领域。论文分析了LLM在这些长期任务中的局限性，并识别出导致失败的原因，这有助于理解如何提高LLM的通用推理能力。 综上所述，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
    "summary2": "本文旨在解决当前agent评估主要关注短时程任务而忽视长时程复杂场景的问题。针对长时程、部分可观察环境中的agent能力评估，我们提出了UltraHorizon benchmark，通过三个不同环境（Mystery Grid、Sequence Exploration和Alien Genetics Laboratory）验证agent的持续推理、规划、内存管理和工具使用能力。实验表明，在最重设置下轨迹平均超过200k token和400+工具调用，LLM-agent表现明显落后于人类参与者，突显了长时程能力差距。通过轨迹分析，我们确定了八种错误类型并归因于上下文锁定和基础能力差距两个主要原因。",
    "inspiration_trace": "## 面临的挑战\n现有自主代理评估主要聚焦短时程、完全可观测任务，而关键现实世界任务（如软件开发、投资和科学发现）发生在长时程、部分可观测场景中。现有基准无法捕捉这些长时程挑战，造成评估空白。\n\n## 关键洞察\n作者认识到长时程任务成功取决于持续推理、规划、记忆管理和工具使用能力。他们独特地洞察到需要设计模拟现实世界复杂性的基准，并发现探索任务可作为统一框架评估这些核心能力。\n\n## 解决方案演进\n从洞察出发，作者先确立设计原则：时间深度、一致性、未知性和现实能力相关性。基于此，创建三个环境（神秘网格、序列探索、外星遗传实验室），要求代理通过持续交互发现隐藏规则。实验验证显示LLM代理表现不佳，证实了评估长时程能力的必要性。\n\n## 创新点总结\n首次系统提出针对长时程、部分可观测环境的代理评估基准；使用探索作为统一任务框架；提出\"上下文锁定\"和\"基础能力差距\"的失败分析框架；揭示简单扩展无法解决问题，并提出CRNR扩展策略。",
    "summary_translation": "自主代理（Autonomous agents）最近在多个领域取得了显著进展，然而大多数评估集中在短期、完全可观察（fully observable）的任务上。相比之下，许多关键的现实世界任务，如大规模软件开发、商业投资和科学发现，都在长期（long-horizon）和部分可观察（partially observable）的场景中展开，这些场景的成功取决于持续的推理（reasoning）、规划（planning）、记忆管理（memory management）和工具使用（tool use）。现有基准很少捕捉这些长期挑战，导致系统评估存在差距。为弥合这一差距，我们引入了**UltraHorizon**，这是一个衡量复杂现实世界挑战所必需的基础能力的新型基准。我们使用探索（exploration）作为三个不同环境中的统一任务，以验证这些核心能力。代理被设计在长期发现任务中，它们必须通过持续推理、规划、记忆和工具管理以及与环境交互来迭代发现隐藏规则。在最大规模设置下，轨迹平均超过**20万token**和**400次工具调用**，而在标准配置中，它们仍然超过**3.5万token**并平均涉及**60多次工具调用**。我们的广泛实验表明，LLM代理（LLM-agents）在这些设置下表现不佳，而人类参与者获得更高的分数，突显了代理在长期能力上存在的持续差距。我们还观察到，简单的扩展（scaling）在我们的任务中失败。为了更好地说明代理的失败，我们对收集的轨迹进行了深入分析。我们识别出八种错误类型，并将其归因于两个主要原因：上下文锁定（in-context locking）和功能基础能力差距（functional fundamental capability gaps）。我们的代码将在[此处](https://github.com/StarDewXXX/UltraHorizon)提供。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#10",
    "title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework",
    "link": "/arxiv/2509.22624",
    "arxiv_id": "2509.22624",
    "authors": "Ziyu Liu, Yuhang Zang, Shengyuan Ding, Yuhang Cao, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang",
    "summary": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback (RLHF) for subjective tasks. However, RLHF incurs high costs and potential reward-policy mismatch due to reliance on human preferences, while RLVR still wastes supervision by discarding rollouts and correctness signals after each update. To address these challenges, we introduce the Synergistic Policy And Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable method that builds on RLVR. Instead of discarding rollouts and correctness data, SPARK recycles this valuable information to simultaneously train the model itself as a generative reward model. This auxiliary training uses a mix of objectives, such as pointwise reward score, pairwise comparison, and evaluation conditioned on further-reflection responses, to teach the model to evaluate and improve its own responses. Our process eliminates the need for a separate reward model and costly human preference data. SPARK creates a positive co-evolving feedback loop: improved reward accuracy yields better policy gradients, which in turn produce higher-quality rollouts that further refine the reward model. Our unified framework supports test-time scaling via self-reflection without external reward models and their associated costs. We show that SPARK achieves significant performance gains on multiple LLM and LVLM models and multiple reasoning, reward models, and general benchmarks. For example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks, 12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the baselines, demonstrating robustness and broad generalization.",
    "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
    "date": "2025-09-26",
    "category": "cs.CV",
    "crawl_time": "2025-10-06T19:08:35.150257",
    "filter_reason": "这篇论文的核心贡献是提出SPARK（协同策略与奖励共同进化框架），这是一种新的强化学习训练范式，用于改进大语言模型(LLM)和大视觉语言模型(LVLM)的训练方法。从本质上看，论文专注于改进LLM的基础能力和训练范式，符合\"致力于提高大语言模型本身的通用推理能力\"的核心目标。SPARK通过回收rollouts和正确性数据，同时训练模型本身作为生成式奖励模型，创建了一个积极的共同进化反馈循环：提高奖励准确性产生更好的策略梯度，进而产生更高质量的rollouts，进一步优化奖励模型。论文在多个推理基准上展示了显著的性能提升（如SPARK-VL-7B在7个推理基准上平均提高9.7%），表明该方法有效增强了模型的推理能力。虽然论文提到了LVLMs，但这只是作为应用场景之一，而非主要焦点。论文没有聚焦于任何特定应用领域或多模态研究，而是提出了一种通用的训练方法来提高LLM的推理能力，完全符合研究范围。",
    "summary2": "本文旨在解决LLMs和LVLMs在强化学习训练中RLHF成本高且RLVR浪费监督信息的问题。针对可验证奖励任务，我们提出了一种SPARK协同策略与奖励共同进化框架，通过回收rollouts和正确性数据同时训练模型作为生成式奖励模型，并在多个推理、奖励和通用基准上通过准确率等指标验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n现有强化学习方法存在关键问题：RLHF依赖人类偏好数据，成本高且易产生奖励-策略不匹配；RLVR虽适用于可验证任务，但每次更新后丢弃rollouts和正确性信号，浪费监督信息；策略与奖励模型分离训练导致不匹配和泛化能力差；外部奖励模型引入高延迟和成本。\n\n## 关键洞察\n作者认识到RLVR过程中被丢弃的rollouts和正确性数据实则是宝贵监督信号；模型本身可同时作为策略和奖励模型；通过回收利用这些数据可形成正反馈循环：提高奖励准确性产生更好策略梯度，进而生成更高质量rollouts，进一步细化奖励模型。\n\n## 解决方案演进\n基于RLVR框架但不丢弃rollouts数据；设计多种训练目标（点wise评分、成对比较、反思评估）教模型评估自身响应；将策略优化和奖励建模整合到统一框架中共同优化；设计测试时扩展策略，利用模型集成的推理、判断和自我反思能力进行迭代改进。\n\n## 创新点总结\n首次将策略和奖励能力统一在单一模型中；创造性地回收利用RLVR训练rollouts；建立策略和奖励共同进化的正反馈循环；实现无需人类偏好数据和外部奖励模型的训练框架；统一训练和测试时扩展机制，大幅降低成本和复杂性。",
    "summary_translation": "近期的大型语言模型（Large Language Models, LLMs）和大型视觉语言模型（Large Vision-Language Models, LVLMs）越来越多地使用强化学习（Reinforcement Learning, RL）进行预训练后训练，例如用于客观任务的可验证奖励强化学习（RL with Verifiable Rewards, RLVR）和用于主观任务的人类反馈强化学习（RL from Human Feedback, RLHF）。然而，RLHF由于依赖人类偏好而产生高成本和潜在的奖励-策略不匹配问题，而RLVR仍然通过在每次更新后丢弃轨迹（rollouts）和正确性信号来浪费监督信息。为应对这些挑战，我们提出了协同策略与奖励共同演化框架（Synergistic Policy And Reward Co-Evolving Framework, SPARK），这是一种基于RLVR的高效、在策略（on-policy）且稳定的方法。SPARK不丢弃轨迹和正确性数据，而是回收这些有价值的信息，同时将模型本身训练为生成式奖励模型（generative reward model）。这种辅助训练使用多种目标的混合，如点态奖励分数（pointwise reward score）、成对比较（pairwise comparison）和基于进一步反思响应（further-reflection responses）的评估，来教会模型评估和改进自身的响应。我们的过程消除了对单独奖励模型和昂贵的人类偏好数据的需求。SPARK创建了一个积极的共同演化反馈循环：提高的奖励准确性产生更好的策略梯度（policy gradients），进而产生更高质量的轨迹，进一步优化奖励模型。我们的统一框架通过自我反思（self-reflection）支持测试时扩展（test-time scaling），无需外部奖励模型及其相关成本。我们表明，SPARK在多个LLM和LVLM模型以及多个推理、奖励模型和通用基准测试上实现了显著的性能提升。例如，SPARK-VL-7B在7个推理基准测试上平均提升了9.7%，在2个奖励基准测试上提升了12.1%，在8个通用基准测试上提升了1.5%，相比基线模型展示了鲁棒性和广泛的泛化能力。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#4",
    "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning",
    "link": "/arxiv/2509.22611",
    "arxiv_id": "2509.22611",
    "authors": "Junkang Wu, Kexin Huang, Jiancan Wu, An Zhang, Xiang Wang, Xiangnan He",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning, but training often oscillates between {entropy collapse} and {entropy explosion}. We trace both hazards to the mean baseline used in value-free RL (e.g., GRPO and DAPO), which improperly penalizes negative-advantage samples under reward outliers. We propose {Quantile Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile baseline. QAE induces a response-level, two-regime gate: on hard queries (p <= 1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it targets remaining failures. Under first-order softmax updates, we prove {two-sided entropy safety}, giving lower and upper bounds on one-step entropy change that curb explosion and prevent collapse. Empirically, this minimal modification stabilizes entropy, sparsifies credit assignment (with tuned K, roughly 80% of responses receive zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results identify {baseline design} -- rather than token-level heuristics -- as the primary mechanism for scaling RLVR.",
    "subjects": "Machine Learning, Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.161717",
    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Quantile Advantage Estimation\" (QAE)的新方法，用于解决强化学习与可验证奖励(RLVR)在训练大语言模型推理能力时出现的熵崩溃和熵爆炸问题。论文通过用基于分组的K-分位数基线替代均值基线，实现了双向熵安全性，从而稳定了训练过程并提升了LLM的推理能力。这完全符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的要求。论文在多个数学推理基准测试(AIME和AMC)上验证了方法的有效性，表明其关注的是通用推理能力的提升，而非特定领域的应用。此外，论文涉及的核心概念(LLMs)、能力方向(reasoning, 特别是math reasoning)和训练方法(reinforcement learning)都是正面指标，且不涉及任何排除标准中的领域。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解决Reinforcement Learning with Verifiable Rewards (RLVR)中训练在熵崩溃和熵爆炸间振荡的问题。针对LLM推理训练中的熵不稳定性，我们提出了一种Quantile Advantage Estimation (QAE)方法，用K-分位数基线替代均值基线，实现响应级别的双机制门控。在Qwen3-8B/14B-Base模型上，通过AIME'24/'25和AMC'23数据集上的pass@1和pass@16指标验证，QAE稳定了熵变化，使约80%响应获得零优势，实现了持续的推理性能提升。",
    "inspiration_trace": "## 面临的挑战\n强化学习中使用可验证奖励时，训练常在\"熵崩溃\"和\"熵爆炸\"间振荡。现有方法主要关注防止熵崩溃，却忽视了熵爆炸问题，导致训练不稳定和性能停滞。\n\n## 关键洞察\n作者发现这两个问题源于价值自由强化学习中使用的均值基线，该基线在奖励异常值情况下不恰当地惩罚负优势样本。问题本质不在于token级别调整，而在于基线设计。\n\n## 解决方案演进\n首先识别熵崩溃和爆炸是对称问题，需要双向控制；分析现有方法局限性，发现token级别控制不足；洞察基线设计是关键，提出用分位数基线替代均值基线；通过理论分析和实验验证，证明QAE能有效稳定熵并提高性能。\n\n## 创新点总结\n将熵调节重新定义为基线设计问题，提出响应级别门控机制，通过单一参数K控制探索-利用平衡，提供双向熵安全保证，实现信用分配稀疏化，集中资源在信息量最大的样本上。",
    "summary_translation": "可验证奖励强化学习 (Reinforcement Learning with Verifiable Rewards, RLVR) 增强了大语言模型 (LLM) 的推理能力，但训练过程常常在 {熵坍缩} (entropy collapse) 和 {熵爆炸} (entropy explosion) 之间振荡。我们将这两种危害追溯到无价值强化学习（如 GRPO 和 DAPO）中使用的均值基线，该基线在奖励异常值情况下不当地惩罚了负优势样本。我们提出了 {分位数优势估计} (Quantile Advantage Estimation, QAE)，用分组 K-分位数基线替代了均值基线。QAE 引入了一个响应级别的双机制门控：对于困难查询（p <= 1 - K），它强化罕见的成功；而对于简单查询（p > 1 - K），它则针对剩余的失败。在一阶 softmax 更新下，我们证明了 {双向熵安全} (two-sided entropy safety)，为单步熵变化提供了上下界，从而抑制爆炸并防止坍缩。实证表明，这一微小修改稳定了熵，稀疏化了信用分配（通过调整 K，约 80% 的响应获得零优势），并在 AIME 2024/2025 和 AMC 2023 上的 Qwen3-8B/14B-Base 模型上带来了持续的 pass@1 提升。这些结果确定了 {基线设计} (baseline design)——而非词元级启发式方法——是扩展 RLVR 的主要机制。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#71",
    "title": "Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization",
    "link": "/arxiv/2509.22115",
    "arxiv_id": "2509.22115",
    "authors": "Chao Wang, Tao Yang, Hongtao Tian, Yunsheng Shi, Qiyao Ma, Xiaotao Liu, Ting Yao, Wenbo Ding",
    "summary": "Critic-free methods like GRPO reduce memory demands by estimating advantages from multiple rollouts but tend to converge slowly, as critical learning signals are diluted by an abundance of uninformative samples and tokens. To tackle this challenge, we propose the \\textbf{Dynamic Dual-Level Down-Sampling (D$^3$S)} framework that prioritizes the most informative samples and tokens across groups to improve the efficient of policy optimization. D$^3$S operates along two levels: (1) the sample-level, which selects a subset of rollouts to maximize advantage variance ($\\text{Var}(A)$). We theoretically proven that this selection is positively correlated with the upper bound of the policy gradient norms, yielding higher policy gradients. (2) the token-level, which prioritizes tokens with a high product of advantage magnitude and policy entropy ($|A_{i,t}|\\times H_{i,t}$), focusing updates on tokens where the policy is both uncertain and impactful. Moreover, to prevent overfitting to high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by curriculum learning. This schedule starts with aggressive down-sampling to accelerate early learning and gradually relaxes to promote robust generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that integrating D$^3$S into advanced RL algorithms achieves state-of-the-art performance and generalization while requiring \\textit{fewer} samples and tokens across diverse reasoning benchmarks. Our code is added in the supplementary materials and will be made publicly available.",
    "subjects": "Machine Learning, Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.226763",
    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Dynamic Dual-Level Down-Sampling (D³S)\"的框架，用于提高大语言模型在强化学习训练过程中的策略优化效率。该方法通过在样本和token两个层面进行动态下采样，优先选择信息量最大的数据，从而加速模型学习过程。论文在Qwen2.5和Llama3.1等大语言模型上进行了实验，并证明该方法在多样化的推理基准测试中取得了先进的性能，同时需要更少的样本和token。 这篇论文完全符合研究目标，因为它致力于提高LLM本身的通用推理能力，具体体现在：(1)论文本质上是改进LLM的基础能力，提出新的训练范式来优化强化学习过程；(2)论文明确关注reasoning能力，在多种推理基准上进行了验证；(3)论文采用了reinforcement learning方法，属于筛选标准中的正面指标；(4)论文没有将LLM作为工具应用于特定领域，而是提出了一种通用的优化方法；(5)论文不属于任何排除标准中的领域。因此，这篇论文应该被保留。",
    "summary2": "本文旨在 [解决无评论家强化学习方法收敛慢的问题]。针对 [大语言模型策略优化中的低效样本利用]，我们提出了一种 [动态双级下采样框架D³S，通过最大化优势方差选择样本，并基于优势幅度与策略熵乘积选择重要token]，并在 [多个数学推理基准测试] 上通过 [Pass@1和Pass@8指标] 验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n无评论家强化学习方法(如GRPO)虽减少内存需求，但收敛缓慢，因大量无信息样本和标记稀释了关键学习信号，特别是在数学推理任务中，组内样本的平均效应掩盖了重要信号。\n\n## 关键洞察\n作者通过理论分析发现，策略梯度范数的上界与优势方差(Var(A))呈正相关，而非传统认为的奖励方差(Var(R))。最大化奖励方差的方法因固定优势方差而无法改变梯度上界，且在小子集中估计优势会导致不稳定。\n\n## 解决方案演进\n基于此洞察，作者提出动态双层下采样框架：样本层面选择最大化优势方差的子集；标记层面优先选择优势幅度与政策熵乘积高的标记；并引入动态调度，从激进下采样逐渐过渡到温和采样，平衡早期收敛与后期泛化。\n\n## 创新点总结\n创新点在于理论上证明了优势方差与梯度上界的关系，设计了双层下采样机制精细利用学习信号，并通过动态调度解决了高效学习与避免过拟合的权衡问题。",
    "summary_translation": "无评论家(critic-free)方法如GRPO通过从多个推演(rollouts)中估计优势(advantages)来减少内存需求，但由于关键学习信号被大量无信息样本和标记(tokens)所稀释，往往收敛缓慢。为应对这一挑战，我们提出了\\textbf{动态双层下采样(Dynamic Dual-Level Down-Sampling, D$^3$S)}框架，该框架优先考虑各组中最具信息量的样本和标记，以提高策略优化(policy optimization)的效率。D$^3$S在两个层次上运行：(1)样本级别(sample-level)，选择推演子集以最大化优势方差($\\text{Var}(A)$)。我们从理论上证明，这种选择与策略梯度范数(policy gradient norms)的上界呈正相关，从而产生更高的策略梯度。(2)标记级别(token-level)，优先考虑优势幅度与策略熵(advantage magnitude and policy entropy)乘积($|A_{i,t}|\\times H_{i,t}$)较高的标记，将更新集中在策略既不确定又有影响力的标记上。此外，为防止对高信号数据的过拟合，D$^3$S采用了一种受课程学习(curriculum learning)启发的动态下采样调度。该调度从激进的下采样开始以加速早期学习，然后逐渐放宽以促进稳健的泛化。在Qwen2.5和Llama3.1上的大量实验表明，将D$^3$S集成到先进的强化学习(RL)算法中，能够在各种推理基准上实现最先进的性能和泛化能力，同时需要\\textit{更少}的样本和标记。我们的代码已添加在补充材料中，并将公开发布。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#86",
    "title": "Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error",
    "link": "/arxiv/2509.22023",
    "arxiv_id": "2509.22023",
    "authors": "Panagiotis Giannoulis, Yorgos Pantis, Christos Tzamos",
    "summary": "Despite their proficiency in various language tasks, Large Language Models (LLMs) struggle with combinatorial problems like Satisfiability, Traveling Salesman Problem, or even basic arithmetic. We address this gap through a novel approach for solving problems in the class NP. We focus on the paradigmatic task of Sudoku and achieve state-of-the-art accuracy (99\\%) compared to prior neuro-symbolic approaches. Unlike prior work that used custom architectures, our method employs a vanilla decoder-only Transformer (GPT-2) without external tools or function calling. Our method integrates imitation learning of simple Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy involving informed guessing and backtracking. Moving beyond imitation learning, we seek to minimize the number of guesses until reaching a solution. We provide a rigorous analysis of this setup formalizing its connection to a contextual variant of Min-Sum Set Cover, a well-studied problem in algorithms and stochastic optimization.",
    "subjects": "Machine Learning",
    "date": "2025-09-26",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.239070",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础推理能力。论文明确指出LLMs在解决组合问题方面存在困难，并提出了一种新方法来增强这种能力。具体来说，论文结合了模仿学习和深度优先搜索策略，使普通的Transformer模型（GPT-2）能够高效解决组合问题，这属于提升LLM通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标： - 核心概念：明确提到\"Large Language Models (LLMs)\"和\"Transformer (GPT-2)\" - 能力方向：专注于解决组合问题的推理能力，属于逻辑推理和问题解决能力 - 训练方法：提出了结合模仿学习和显式深度优先搜索的新训练范式 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文使用数独作为具体任务，但这是为了展示方法在解决NP类问题上的通用性，而非针对特定领域应用。 最后，论文的核心贡献是提出了一种增强LLM解决组合问题能力的新方法，通过结合模仿学习和深度优先搜索策略，使模型能够进行有信息猜测和回溯，从而提升其通用推理能力。这完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
    "summary2": "本文旨在解决大型语言模型在组合优化问题上的推理困难。针对Sudoku和1-in-3 SAT等NP类问题，我们提出了一种结合模仿学习和显式深度优先搜索的试错推理框架，通过动作级标记化和多目标训练方法，使Transformer能够进行有根据的猜测和回溯。在随机生成的Sudoku和1-in-3 SAT数据集上，我们的方法分别达到了99%的准确率，显著优于现有神经符号方法，展示了Transformer在组合推理任务上的强大潜力。",
    "inspiration_trace": "## 面临的挑战\n大型语言模型在组合问题（如数独）上表现不佳，现有方法依赖定制架构而非标准LLM范式，缺乏错误恢复机制，推理过程不透明，且无法超越训练数据中的策略。\n\n## 关键洞察\n人类解决组合问题不仅应用逻辑规则，还会进行有根据的猜测并回溯。作者发现大多数随机数独可通过简单规则加一次\"后门\"猜测解决，这为优化猜测过程提供了理论基础。\n\n## 解决方案演进\n首先结合模仿学习与DFS搜索策略，使模型能猜测和回溯；进而将猜测优化形式化为Min-Sum Set Cover问题，设计新损失函数最小化解决步骤，从追求正确性转向效率。\n\n## 创新点总结\n首次将标准Transformer与显式搜索策略结合，无需外部工具；引入动作级标记化和多目标学习；建立猜测优化与Min-Sum Set Cover的理论联系；开发高效数独生成库，方法可推广至任何NP类问题。",
    "summary_translation": "尽管大型语言模型（Large Language Models, LLMs）在各种语言任务上表现出色，但它们在组合问题（如可满足性问题、旅行商问题，甚至是基本算术）方面仍存在困难。我们通过一种新颖的方法来解决NP类（NP）问题，从而弥补这一差距。我们专注于数独（Sudoku）这一典型任务，与先前的神经符号（neuro-symbolic）方法相比，达到了最先进的准确率（99%）。与使用定制架构的先前工作不同，我们的方法采用了原始的仅解码器Transformer（vanilla decoder-only Transformer, GPT-2），无需外部工具或函数调用。我们的方法将简单数独规则的模仿学习（imitation learning）与一种明确的深度优先搜索（Depth-First Search, DFS）探索策略相结合，该策略涉及有根据的猜测和回溯（backtracking）。超越模仿学习，我们力求在达到解决方案之前最小化猜测次数。我们对此设置进行了严格分析，将其与最小和集覆盖（Min-Sum Set Cover）的上下文变体之间的联系形式化，这是算法和随机优化（stochastic optimization）中一个研究充分的问题。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#108",
    "title": "Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration",
    "link": "/arxiv/2509.21848",
    "arxiv_id": "2509.21848",
    "authors": "Taejong Joo, Shu Ishida, Ivan Sosnovik, Bryan Lim, Sahand Rezaei-Shoshtari, Adam Gaier, Robert Giaquinto",
    "summary": "As a model-agnostic approach to long context modeling, multi-agent systems can process inputs longer than a large language model's context window without retraining or architectural modifications. However, their performance often heavily relies on hand-crafted multi-agent collaboration strategies and prompt engineering, which limit generalizability. In this work, we introduce a principled framework that formalizes the model-agnostic long context modeling problem as a compression problem, yielding an information-theoretic compression objective. Building on this framework, we propose Graph of Agents (GoA), which dynamically constructs an input-dependent collaboration structure that maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document question answering benchmarks, GoA improves the average $F_1$ score of retrieval-augmented generation by 5.7\\% and a strong multi-agent baseline using a fixed collaboration structure by 16.35\\%, respectively. Even with only a 2K context window, GoA surpasses the 128K context window Llama 3.1 8B on LongBench, showing a dramatic increase in effective context length. Our source code is available at https://github.com/tjoo512/graph-of-agents.",
    "subjects": "Machine Learning, Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.270875",
    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Graph of Agents (GoA)\"的原则性框架，通过动态构建输入依赖的协作结构来增强大语言模型处理长上下文的能力。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从本质上看，论文属于\"提出新的训练范式\"和\"智能体协作框架\"的范畴，旨在提高LLM的基础能力（处理长上下文的能力）。论文不是将LLM作为工具应用到特定领域，而是专注于提升LLM本身的通用能力，符合第一步的核心判断标准。 其次，论文包含多个正面指标：明确涉及大语言模型（Llama 3.1 8B和Qwen3 8B），并提出了多智能体系统（multi-agent systems）这一新兴范式。虽然论文不是直接针对推理、规划等能力方向，但处理长上下文的能力是LLM进行复杂推理和问题解决的基础能力之一。 第三，论文不涉及任何排除标准中的领域：没有涉及多模态与视觉，没有专注于特定应用领域，也没有主要关注模型可靠性问题。 最后，在特殊和模糊情况处理上，论文提出的是一种通用的多智能体协作框架来增强LLM的通用问题解决能力，而不是将智能体应用在特定领域，因此应该保留。 综上所述，这篇论文符合研究目标，因为它致力于通过多智能体协作框架提高大语言模型的基础能力（长上下文处理），这属于提升LLM通用推理能力的范畴。",
    "summary2": "本文旨在解决大型语言模型处理长上下文输入时的限制问题。针对超出模型上下文窗口的长文档，我们提出了一种Graph of Agents (GoA)方法，通过动态构建输入相关的多智能体协作结构来优化信息论压缩目标，并在六个文档问答基准测试上通过F1分数验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n大语言模型受限于上下文窗口，无法处理超长输入；现有多智能体系统依赖手工设计的协作策略，缺乏泛化能力；检索增强生成可能丢弃关键信息，而固定协作结构无法适应多样化输入。\n\n## 关键洞察\n作者将长上下文建模本质视为压缩问题，从信息论角度提出最优压缩应最大化与答案的互信息。这一理论视角为多智能体系统设计提供了原则性指导，突破了现有方法依赖启发式规则的局限。\n\n## 解决方案演进\n从互信息目标出发，将其转化为可操作的语义相似性优化；设计Graph of Agents框架，将多智能体协作建模为动态图结构；引入线性森林平衡并行性与灵活性；通过语义聚类和上下文感知的贪心算法构建输入依赖的协作图。\n\n## 创新点总结\n首次从信息论角度原则性形式化长上下文建模问题；提出输入依赖的动态协作结构，替代静态手工设计模式；建立理论目标与可操作优化之间的桥梁，显著扩展了模型有效上下文长度。",
    "summary_translation": "作为一种模型无关（model-agnostic）的长上下文建模方法，多智能体系统（multi-agent systems）无需重新训练或架构修改，即可处理超出大型语言模型上下文窗口的输入。然而，其性能往往严重依赖手工制作的多智能体协作策略和提示工程（prompt engineering），这限制了其泛化能力。在本研究中，我们提出了一个原则性框架，将模型无关的长上下文建模问题形式化为压缩问题，从而得出一个信息论压缩目标（information-theoretic compression objective）。基于此框架，我们提出了智能体图（Graph of Agents, GoA），它动态构建一个依赖于输入的协作结构，以最大化该目标。在六个文档问答基准测试中，对于Llama 3.1 8B和Qwen3 8B模型，GoA分别将检索增强生成（retrieval-augmented generation）的平均$F_1$分数提高了5.7%，并将使用固定协作结构的强多智能体基线提高了16.35%。即使仅有2K的上下文窗口，GoA在LongBench上也超越了拥有128K上下文窗口的Llama 3.1 8B，显示出有效上下文长度的显著增加。我们的源代码可在https://github.com/tjoo512/graph-of-agents获取。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#116",
    "title": "FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning",
    "link": "/arxiv/2509.21792",
    "arxiv_id": "2509.21792",
    "authors": "Yizhou Zhang, Ning Lv, Teng Wang, Jisheng Dang",
    "summary": "Group relative policy optimization (GRPO) has demonstrated significant potential in improving the reasoning capabilities of large language models (LLMs) via reinforcement learning. However, its practical deployment is impeded by an excessively slow training process, primarily attributed to the computationally intensive autoregressive generation of multiple responses per query, which makes the generation phase the primary performance bottleneck. Although speculative decoding presents a promising direction for acceleration, its direct application in GRPO achieves limited speedup under high-concurrency training conditions. To overcome this limitation, we propose a concurrency-aware speculative decoding framework that dynamically adjusts the drafting and verification strategy according to real-time concurrency levels, thereby maximizing the acceleration of the generation process. Furthermore, to address performance degradation arising from distributional drift between the evolving target model and the fixed draft model during training, we introduce an online draft learning mechanism that enables the draft model to continuously adapt using feedback signals from the target model. Experimental results across multiple mathematical reasoning datasets and models demonstrate that the proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly surpassing baseline approaches in efficiency. The code is available at https://github.com/yedaotian9/GRPO_speculative.",
    "subjects": "Machine Learning, Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.280298",
    "filter_reason": "这篇论文符合我的研究目标，核心原因在于它专注于改进大语言模型的训练方法以提升其推理能力。具体分析如下： 从第一步核心判断来看，论文本质上是关于改进GRPO（Group relative policy optimization）训练过程的效率，而GRPO本身是一种通过强化学习提高大语言模型推理能力的方法。论文提出的并发感知投机解码框架和在线草稿学习机制，都是为了加速这种训练过程，从而更有效地提升LLM的推理能力。这属于\"改进LLM的基础能力\"和\"提出新的训练范式\"的范畴，而非将LLM作为工具应用到特定领域。 从第二步正面指标来看，论文包含了多个相关主题： - 核心概念：明确涉及大语言模型(LLMs) - 能力方向：直接关注\"improving the reasoning capabilities\"，并在数学推理数据集上验证 - 训练方法：GRPO是一种强化学习方法，属于强化学习优化范畴 从第三步排除标准来看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。虽然实验在数学推理数据集上进行，但论文的核心贡献是加速训练方法，而非解决数学问题本身。 综上所述，这篇论文的核心贡献是提出一种加速强化学习训练过程的方法，目的是更有效地提升大语言模型的推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决GRPO训练过程中生成阶段效率低下的问题。针对高并发训练条件下投机解码加速效果有限的场景，我们提出了一种并发感知投机解码与在线草稿学习相结合的方法，并在多个数学推理数据集（GSM8K、SimpleRL-Abel-Level3to5、DAPO-Math-17K）上通过端到端加速比（2.35x-2.72x）验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\nGRPO训练中生成阶段占91%-98%时间，成为主要瓶颈。直接应用投机解码在高并发训练条件下效果有限，且目标模型不断更新导致与固定草稿模型间分布漂移，加速效果随时间下降。\n\n## 关键洞察\nGRPO生成阶段并发性动态变化：从高并发逐渐过渡到低并发，因响应长度差异导致序列完成时间不一致。投机解码在低并发有效，高并发下可能从内存受限转为计算受限。分布漂移是加速效果下降的根本原因。\n\n## 解决方案演进\n针对动态并发问题，提出并发感知投机解码框架，根据实时并发调整策略；基于操作强度理论，动态调整验证token数量和草稿树大小；引入在线草稿学习，使草稿模型利用目标模型反馈持续适应。\n\n## 创新点总结\n首次识别GRPO训练中动态并发特性对投机解码的影响；提出基于操作强度的动态参数调整策略；创新性地将在线学习引入草稿模型训练，解决分布漂移问题，维持长期加速效果。",
    "summary_translation": "组相对策略优化（Group relative policy optimization, GRPO）已展现出通过强化学习提升大型语言模型（Large language models, LLMs）推理能力的显著潜力。然而，其实际部署受到训练过程过慢的阻碍，主要归因于每个查询需要计算密集型的自回归生成（autoregressive generation）多个响应，这使得生成阶段成为主要性能瓶颈。尽管推测解码（speculative decoding）呈现为一个有前景的加速方向，但在高并发训练条件（high-concurrency training conditions）下，其在GRPO中的直接应用只能实现有限的加速。为克服这一限制，我们提出了一个并发感知推测解码框架（concurrency-aware speculative decoding framework），该框架能根据实时并发水平动态调整草稿和验证策略（drafting and verification strategy），从而最大化生成过程的加速效果。此外，为解决训练过程中不断演进的目标模型（target model）与固定的草稿模型（draft model）之间分布漂移（distributional drift）导致的性能退化问题，我们引入了一种在线草稿学习机制（online draft learning mechanism），使草稿模型能够利用来自目标模型的反馈信号持续适应。在多个数学推理数据集（mathematical reasoning datasets）和模型上的实验结果表明，所提出的方法实现了2.35倍至2.72倍的端到端加速，在效率方面显著超越基线方法。代码可在https://github.com/yedaotian9/GRPO_speculative获取。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#156",
    "title": "A circuit for predicting hierarchical structure in-context in Large Language Models",
    "link": "/arxiv/2509.21534",
    "arxiv_id": "2509.21534",
    "authors": "Tankred Saanum, Can Demircan, Samuel J. Gershman, Eric Schulz",
    "summary": "Large Language Models (LLMs) excel at in-context learning, the ability to use information provided as context to improve prediction of future tokens. Induction heads have been argued to play a crucial role for in-context learning in Transformer Language Models. These attention heads make a token attend to successors of past occurrences of the same token in the input. This basic mechanism supports LLMs' ability to copy and predict repeating patterns. However, it is unclear if this same mechanism can support in-context learning of more complex repetitive patterns with hierarchical structure. Natural language is teeming with such cases: The article \"the\" in English usually prefaces multiple nouns in a text. When predicting which token succeeds a particular instance of \"the\", we need to integrate further contextual cues from the text to predict the correct noun. If induction heads naively attend to all past instances of successor tokens of \"the\" in a context-independent manner, they cannot support this level of contextual information integration. In this study, we design a synthetic in-context learning task, where tokens are repeated with hierarchical dependencies. Here, attending uniformly to all successor tokens is not sufficient to accurately predict future tokens. Evaluating a range of LLMs on these token sequences and natural language analogues, we find adaptive induction heads that support prediction by learning what to attend to in-context. Next, we investigate how induction heads themselves learn in-context. We find evidence that learning is supported by attention heads that uncover a set of latent contexts, determining the different token transition relationships. Overall, we not only show that LLMs have induction heads that learn, but offer a complete mechanistic account of how LLMs learn to predict higher-order repetitive patterns in-context.",
    "subjects": "Machine Learning",
    "date": "2025-09-25",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.320886",
    "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)的上下文学习机制，特别是探讨归纳头(induction heads)如何支持LLMs处理和预测具有层次结构的复杂重复模式。论文揭示了LLMs内部的机制，解释了它们如何通过自适应归纳头学习在上下文中关注什么来支持预测，这直接关系到LLMs的通用推理能力。论文没有将LLM作为工具应用到特定领域，而是研究LLM本身的基础能力，特别是其处理复杂模式和结构的能力，这与推理能力密切相关。论文符合核心判断标准中的\"改进LLM的基础能力\"，同时也符合正面指标中的核心概念(LLMs)和能力方向(推理相关)。论文不符合任何排除标准，如多模态研究、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在探究大型语言模型如何在上下文学习中处理具有层次依赖关系的重复模式。针对合成的层次依赖标记序列和自然语言示例，我们提出了一种由自适应感应头和上下文匹配头组成的电路机制，并在多个LLMs上通过预测准确性和注意力模式分析验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n现有归纳头机制只能解释LLMs中简单的上下文学习（如复制重复模式），但无法解释更复杂的、具有层次结构和上下文依赖的重复模式学习。自然语言中常见的情况（如冠词\"the\"后可能跟多个不同名词）需要模型根据上下文信息正确预测后续token，而非简单复制。\n\n## 关键洞察\n作者洞察到归纳头可能具有适应性，能够\"学习\"在上下文中关注什么，而非静态复制机制。这种适应性使归纳头能根据上下文信息调整注意力模式，选择性地关注正确的后续token，这可能是LLMs处理复杂层次结构模式的关键。\n\n## 解决方案演进\n作者设计合成任务测试LLMs处理层次依赖的能力，发现存在\"自适应归纳头\"能学习关注正确后续token。这些归纳头依赖\"上下文匹配头\"识别潜在上下文结构，后者通过使token关注前驱token链来传递信息，帮助归纳头确定正确关注目标。消融实验验证了组件间的因果关系。\n\n## 创新点总结\n揭示了归纳头的自适应学习能力，发现并验证了支持其学习的\"上下文匹配头\"机制，提出了完整的电路解释说明LLMs如何通过专门注意力头协作学习预测层次结构模式，并将机制从合成数据扩展到自然语言案例。",
    "summary_translation": "大型语言模型（Large Language Models, LLMs）擅长上下文学习（in-context learning），即利用提供的信息作为上下文来改进对未来标记（token）的预测能力。归纳头（induction heads）被认为在Transformer语言模型（Transformer Language Models）的上下文学习中扮演着关键角色。这些注意力头（attention heads）使一个标记能够关注输入中相同标记过去出现情况的后继标记。这一基本机制支持了LLMs复制和预测重复模式的能力。\n\n然而，目前尚不清楚这一机制是否能够支持具有层次结构（hierarchical structure）的更复杂重复模式的上下文学习。自然语言中充满了这种情况：英语中的冠词\"the\"通常在文本中引导多个名词。当预测哪个标记会跟随\"the\"的特定实例时，我们需要整合文本中的更多上下文线索来预测正确的名词。如果归纳头以上下文无关的方式天真地关注\"the\"的所有过去后继标记实例，它们就无法支持这种程度的上下文信息整合。\n\n在本研究中，我们设计了一个合成上下文学习任务，其中标记以层次依赖关系（hierarchical dependencies）重复出现。在这种情况下，均匀地关注所有后继标记不足以准确预测未来的标记。通过在这些标记序列和自然语言类似物上评估一系列LLMs，我们发现了自适应归纳头（adaptive induction heads），它们通过学习在上下文中关注什么来支持预测。\n\n接下来，我们研究了归纳头本身如何在上下文中学习。我们发现有证据表明，学习是由那些揭示一组潜在上下文（latent contexts）的注意力头支持的，这些上下文决定了不同的标记转换关系。总体而言，我们不仅展示了LLMs具有能够学习的归纳头，还提供了一个完整的机制解释，说明LLMs如何学习在上下文中预测高阶重复模式（higher-order repetitive patterns）。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#155",
    "title": "Evidence for Limited Metacognition in LLMs",
    "link": "/arxiv/2509.21545",
    "arxiv_id": "2509.21545",
    "authors": "Christopher Ackerman",
    "summary": "The possibility of LLM self-awareness and even sentience is gaining increasing public attention and has major safety and policy implications, but the science of measuring them is still in a nascent state. Here we introduce a novel methodology for quantitatively evaluating metacognitive abilities in LLMs. Taking inspiration from research on metacognition in nonhuman animals, our approach eschews model self-reports and instead tests to what degree models can strategically deploy knowledge of internal states. Using two experimental paradigms, we demonstrate that frontier LLMs introduced since early 2024 show increasingly strong evidence of certain metacognitive abilities, specifically the ability to assess and utilize their own confidence in their ability to answer factual and reasoning questions correctly and the ability to anticipate what answers they would give and utilize that information appropriately. We buttress these behavioral findings with an analysis of the token probabilities returned by the models, which suggests the presence of an upstream internal signal that could provide the basis for metacognition. We further find that these abilities 1) are limited in resolution, 2) emerge in context-dependent manners, and 3) seem to be qualitatively different from those of humans. We also report intriguing differences across models of similar capabilities, suggesting that LLM post-training may have a role in developing metacognitive abilities.",
    "subjects": "Machine Learning",
    "date": "2025-09-25",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.315257",
    "filter_reason": "这篇论文的核心贡献是提出了一种评估LLM元认知能力的新方法，并研究了LLM如何评估和利用自身对回答事实和推理问题的信心，以及预测自己会给出什么答案并适当利用这些信息的能力。根据筛选标准，这篇论文符合我的研究目标，原因如下： 1. 核心判断：论文本质上是研究LLM的基础能力（元认知），而不是将LLM作为工具应用到特定领域。元认知能力是通用推理能力的重要组成部分，因为它涉及模型对自己认知过程的监控和评估，这对于有效的推理至关重要。 2. 正面指标：论文明确研究Large language models (LLMs)，并关注reasoning能力（论文中明确提到\"reasoning questions\"）。虽然论文没有直接讨论训练方法或新兴范式，但它研究的是这些方法和范式可能影响的底层能力。 3. 排除标准：论文没有主要聚焦于多模态与视觉、特定应用领域或模型可靠性的应用层面。虽然论文提到了\"major safety and policy implications\"，但这只是背景信息，不是研究的核心焦点。 4. 特殊情况处理：论文研究的是LLM的内在认知能力，而不是应用层面的研究。元认知能力的研究有助于我们更好地理解LLM的内在工作机制，从而可能为提升LLM的通用推理能力提供指导。 综上所述，这篇论文通过研究LLM的元认知能力，直接关注了LLM的通用推理能力的一个关键方面，符合我的研究目标。",
    "summary2": "本文旨在评估大型语言模型(LLMs)的元认知能力。针对多种前沿LLMs，我们提出了Delegate Game和Second Chance Game两种实验范式，并在GPQA和SimpleQA数据集上通过偏相关分析和变化率提升指标验证了模型评估自身信心和预测自身输出的能力。实验表明近期LLMs展现出有限的元认知能力，但这些能力在分辨率上有限且与人类有质的不同。",
    "inspiration_trace": "## 面临的挑战\n如何评估LLMs的自我意识与元认知能力，而不依赖其不可靠的自我报告。LLMs经过大量文本训练，天生就不适合提供可信的自我描述，传统测量方法无法有效区分真实内部状态与语言模仿。\n\n## 关键洞察\n借鉴动物元认知研究，区分两种核心能力：评估自己信心的能力(\"知道他们知道\")和预测自己回答的能力(\"知道他们知道什么\")。关键是将模型的语言输出作为自我意识的间接测量，而非字面解释。\n\n## 解决方案演进\n设计两个游戏化实验：委托游戏测试模型是否能利用内部信心信号决定是否回答问题；第二次机会游戏测试模型是否能预测并改变自己的回答。同时控制表面特征等混淆变量，利用token概率作为内部状态代理指标。\n\n## 创新点总结\n避开自我报告依赖，将动物元认知研究范式创新应用于LLMs；区分并量化两种元认知能力；通过实验控制区分真实内省与表面线索决策，提供首个系统性评估LLMs元认知的框架。",
    "summary_translation": "LLM (大型语言模型) 的自我意识甚至感知能力 (sentience) 的可能性正日益引起公众关注，并具有重要的安全与政策影响，但衡量这些能力的科学仍处于起步阶段。本文我们介绍了一种用于定量评估LLM (大型语言模型) 元认知能力 (metacognitive abilities) 的新方法。受非人类动物元认知研究的启发，我们的方法避开了模型自我报告，而是测试模型在多大程度上能够战略性地部署内部状态知识。通过两种实验范式 (experimental paradigms)，我们证明自2024年初以来推出的前沿LLM (大型语言模型) 显示出越来越强的某些元认知能力证据，特别是评估和利用其自身正确回答事实和推理问题信心的能力，以及预测它们将给出什么答案并适当利用该信息的能力。我们通过对模型返回的token (标记) 概率的分析来支持这些行为发现，这表明存在一个可能为元认知提供基础的上游内部信号。我们进一步发现这些能力1)在分辨率上有限，2)以情境依赖的方式出现，以及3)似乎与人类的能力在质量上有所不同。我们还报告了在能力相似的模型之间存在有趣的差异，这表明LLM (大型语言模型) 的后训练 (post-training) 可能在发展元认知能力中发挥作用。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#164",
    "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training",
    "link": "/arxiv/2509.21500",
    "arxiv_id": "2509.21500",
    "authors": "Junkai Zhang, Zihao Wang, Lin Gui, Swarnashree Mysore Sathyendra, Jaehwan Jeong, Victor Veitch, Wei Wang, Yunzhong He, Bing Liu, Lifeng Jin",
    "summary": "Reinforcement fine-tuning (RFT) often suffers from \\emph{reward over-optimization}, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish Excellent responses from merely Great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. Our code can be accessed at https://github.com/Jun-Kai-Zhang/rubrics.git .",
    "subjects": "Machine Learning, Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.324817",
    "filter_reason": "这篇论文的核心贡献是提出了一种基于评分标准(rubric-based)的奖励建模方法，用于解决大语言模型强化微调(RFT)中的\"奖励过度优化\"问题。根据筛选标准，我判断该论文符合研究范围，原因如下： 首先，从本质上看，这篇论文专注于改进LLM的基础训练能力，特别是强化学习过程中的奖励建模方法。它不是将LLM作为工具应用到特定领域，而是直接针对LLM的训练过程进行优化，这符合第一步的核心判断标准。 其次，论文包含多个正面指标：明确研究\"Large Language Model\"的\"Post-Training\"；涉及\"reinforcement learning\"训练方法；虽然未直接提及推理能力，但解决奖励过度优化问题本质上是提升模型生成高质量输出的能力，这与通用推理能力密切相关。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，论文提出的方法通过改进奖励建模来优化LLM的后训练过程，这属于提升模型基础能力的研究，与提高LLM通用推理能力的目标一致。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解决强化微调中的奖励过度优化问题。针对大型语言模型后训练场景，我们提出了一种基于评分标准(rubric-based)的奖励建模方法，专注于高奖励区域的准确性，并在通用和专业医疗领域的三个数据集上通过胜率(win rate)和HealthBench指标验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n强化微调中存在奖励过度优化问题，即语言模型会利用奖励模型的不准确性，获得高代理分数但实际输出质量下降。传统解决方案如在线RLHF成本高昂且缓慢。\n\n## 关键洞察\n作者通过理论分析发现，奖励过度优化的本质在于高奖励区域的奖励误指定——无法可靠区分优秀响应和仅仅良好的响应。高奖励区域的误差对对齐质量的影响远大于其他区域。\n\n## 解决方案演进\n基于这一洞察，作者意识到需将奖励建模集中在高奖励区域。然而，基础LLM下这类尾部例子稀少。虽然使用离策略样本(如来自更强模型)更容易获得，但直接训练会学习表面特征。作者转向基于评分标准的奖励，它可利用离策略示例同时对其伪影不敏感。为此提出两个原则：区分优秀与良好响应、区分多样化响应，并设计迭代工作流程实现这些原则。\n\n## 创新点总结\n创新在于从理论揭示高奖励区域误指定是核心问题，到利用评分标准解决离策略数据问题的完整思路链条，特别是通过区分高质量响应来精确捕捉高奖励尾部行为的机制。",
    "summary_translation": "强化微调 (Reinforcement fine-tuning, RFT) 经常遭受奖励过度优化 (reward over-optimization) 的问题，即策略模型通过操纵奖励信号来获得高分，同时却产生低质量的输出。我们的理论分析表明，关键在于高奖励尾部的奖励误指定 (reward misspecification)：无法可靠地区分卓越 (Excellent) 响应与仅仅良好 (Great) 的响应。这促使我们专注于高奖励区域。然而，在基础大型语言模型 (base LLM) 下，这类尾部示例非常稀少。虽然离策略示例 (off-policy exemplars)（例如来自更强模型或重写的示例）更容易获得，但简单地在这些示例上进行训练会导致我们想要对齐的策略产生误指定的奖励。为解决此问题，我们研究了基于评分标准 (rubric-based) 的奖励。通过设计，评分标准 (rubrics) 可以利用离策略示例，同时对其人工痕迹 (artifacts) 不敏感。为了引出能够捕捉高奖励尾部的评分标准，我们强调了区分良好且多样化响应的重要性，并引入了一个实现这一想法的工作流程。我们通过实证证明，基于评分标准的奖励显著减轻了奖励过度优化，并带来了有效的大型语言模型 (LLM) 后训练改进。我们的代码可通过 https://github.com/Jun-Kai-Zhang/rubrics.git 访问。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#171",
    "title": "d2: Improved Techniques for Training Reasoning Diffusion Language Models",
    "link": "/arxiv/2509.21474",
    "arxiv_id": "2509.21474",
    "authors": "Guanghan Wang, Yair Schiff, Gilad Turok, Volodymyr Kuleshov",
    "summary": "While diffusion language models (DLMs) have achieved competitive performance in text generation, improving their reasoning ability with reinforcement learning remains an active research area. Here, we introduce d2, a reasoning framework tailored for masked DLMs. Central to our framework is a new policy gradient algorithm that relies on properties of masking to accurately estimate the likelihoods of sampling trajectories. Our estimators trade off computation for approximation accuracy in an analytically tractable manner, and are particularly effective for DLMs that support any-order likelihood estimation. We characterize and study this property in popular DLMs and show that it is key for efficient diffusion-based reasoning. Empirically, d2 significantly improves over previous diffusion reasoning frameworks using only RL (without relying on supervised fine-tuning), and sets a new state-of-the-art performance for DLMs on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks (GSM8K and MATH500).",
    "subjects": "Machine Learning",
    "date": "2025-09-25",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.333292",
    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进扩散语言模型(DLMs)的推理能力，提出了名为\"d2\"的推理框架和新的策略梯度算法。论文的核心贡献是提升模型本身的基础推理能力，而非将LLM作为工具应用到特定领域。论文专注于增强模型的逻辑推理和数学推理能力，这正属于通用推理能力的范畴。 其次，从正面指标分析，论文包含多个相关主题： 1. 核心概念：论文研究扩散语言模型(DLMs)，属于大语言模型的范畴 2. 能力方向：明确关注推理能力，特别是在逻辑推理(Countdown和Sudoku)和数学推理(GSM8K和MATH500)任务上 3. 训练方法：提出了基于强化学习的策略梯度算法，属于强化学习方法 第三，从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。虽然标题中提到\"Diffusion\"，但这里指的是文本生成领域的扩散语言模型，而非视觉领域的扩散模型。 最后，论文没有涉及特殊和模糊情况中的智能体/工具使用或幻觉/可解释性/安全等内容，因此不需要进一步判断。 综上所述，这篇论文的核心贡献是提出新的训练方法来增强大语言模型的通用推理能力，完全符合研究目标。",
    "summary2": "本文旨在提高扩散语言模型(DLMs)的推理能力。针对掩码扩散语言模型，我们提出了一种d2推理框架，包含两种新的策略梯度估计器(d2-StepMerge和d2-AnyOrder)，并在逻辑推理任务(Countdown和Sudoku)和数学推理基准(GSM8K和MATH500)上通过准确率等指标验证了其有效性，实现了仅使用强化学习(无需监督微调)的最先进性能。",
    "inspiration_trace": "## 面临的挑战\n扩散语言模型(DLMs)在文本生成方面表现优异，但使用强化学习提高其推理能力仍面临核心难题：DLMs的精确似然计算在计算上难以处理，无法直接应用自回归模型的策略梯度方法，现有扩散推理框架性能有限且常依赖监督微调。\n\n## 关键洞察\n作者认识到DLMs的似然计算是RL训练的关键瓶颈，发现掩码扩散模型的特性可被利用来准确估计采样轨迹的似然。特别洞察到\"任意顺序因果解码\"特性对高效扩散推理至关重要，这是之前被忽视的关键属性。\n\n## 解决方案演进\n作者首先从策略梯度公式推导出适用于掩码DLMs的GRPO风格RL目标，然后针对似然估计瓶颈提出两种估计器：d2-StepMerge通过分割轨迹在特定步骤评估似然，以计算换取精度；d2-AnyOrder则在满足特定条件下实现无偏单次似然估计，并通过理论分析和实验验证其有效性。\n\n## 创新点总结\n首次为DLMs提出原则性RL框架，创新利用掩码扩散特性解决似然估计问题，深入研究并证明\"任意顺序因果性\"对推理能力的重要性，在不依赖监督微调情况下实现DLMs推理性能的突破。",
    "summary_translation": "尽管扩散语言模型(diffusion language models, DLMs)在文本生成方面已经取得了有竞争力的性能，但通过强化学习(reinforcement learning)提高其推理能力仍然是一个活跃的研究领域。在本文中，我们介绍了d2，一个专为掩码扩散语言模型(masked DLMs)定制的推理框架。我们框架的核心是一种新的策略梯度算法(policy gradient algorithm)，该算法依赖于掩码(masking)的特性来准确估计采样轨迹(sampling trajectories)的可能性。我们的估计器(estimators)以分析上易于处理(analytically tractable)的方式在计算和近似精度之间进行权衡，并且对于支持任意阶似然估计(any-order likelihood estimation)的扩散语言模型特别有效。我们在流行的扩散语言模型中描述并研究了这一特性，并表明它是高效基于扩散的推理(diffusion-based reasoning)的关键。实验上，d2显著优于以前仅使用强化学习(不依赖监督微调(supervised fine-tuning))的扩散推理框架，并在逻辑推理任务(logical reasoning tasks)(Countdown和Sudoku)和数学推理基准(math reasoning benchmarks)(GSM8K和MATH500)上为扩散语言模型设立了新的最先进性能(state-of-the-art performance)。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#189",
    "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective",
    "link": "/arxiv/2509.22613",
    "arxiv_id": "2509.22613",
    "authors": "Siwei Wang, Yifei Shen, Haoran Sun, Shi Feng, Shang-Hua Teng, Li Dong, Yaru Hao, Wei Chen",
    "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.",
    "subjects": "Artificial Intelligence, Machine Learning, Machine Learning",
    "date": "2025-09-26",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.352738",
    "filter_reason": "这篇论文的核心贡献是理论分析强化学习(RL)如何增强大语言模型(LLM)的规划能力。论文通过图抽象模型研究了策略梯度(PG)和Q-learning方法在LLM规划中的表现，揭示了监督微调可能导致伪相关问题，而RL通过探索实现正确规划并提高泛化能力。这直接符合研究目标中\"改进LLM的基础能力\"和\"增强其规划、多步推理等通用能力\"的要求。论文聚焦于提升LLM本身的通用推理能力(特别是规划能力)，而不是将LLM应用于特定领域。论文满足多个正面指标：核心概念涉及LLMs，能力方向关注planning，训练方法研究reinforcement learning。同时，论文不涉及任何需要排除的领域，如多模态、特定应用领域或模型基础设施优化。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在 [解决强化学习在语言模型规划中的理论基础问题]。针对 [语言模型规划任务]，我们提出了一种 [基于图抽象的理论分析框架]，并在 [Blocksworld规划基准和Erdős-Rényi图] 上通过 [准确性和输出多样性指标] 验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n作者面临的核心挑战是：强化学习(RL)在增强语言模型规划能力方面取得成功，但其理论基础尚不明确；监督微调(SFT)在规划任务中存在根本局限，倾向于记忆而非泛化；现有RL方法如策略梯度存在多样性崩溃问题。\n\n## 关键洞察\n作者通过图论抽象将规划问题建模为路径寻找，发现SFT本质上是记忆训练数据中的共现关系，无法利用传递性信息；RL优于SFT主要在于其迭代数据生成过程促进了探索；策略梯度存在多样性崩溃问题，而Q-learning具有保持多样性和支持离线学习的双重优势。\n\n## 解决方案演进\n作者首先建立理论框架分析SFT局限，然后揭示策略梯度与SFT的联系及多样性崩溃问题，转而分析Q-learning发现过程奖励可避免奖励黑客，最终证明Q-learning在保持多样性和离线学习方面的优势，并通过实验验证理论发现。\n\n## 创新点总结\n该研究首次从理论角度系统分析RL在语言模型规划中的作用机制，揭示SFT记忆和RL泛化的理论基础，发现并证明策略梯度多样性崩溃现象，提出Q-learning在语言模型规划中的优势，为改进语言模型规划中的RL方法提供理论基础。",
    "summary_translation": "最近的强化学习(reinforcement learning, RL)方法显著提升了大型语言模型(Large Language Models, LLMs)的规划能力，然而其有效性的理论基础仍然难以捉摸。在这项工作中，我们通过一个可处理的基于图的抽象来研究强化学习的优势和局限性，重点关注策略梯度(policy gradient, PG)和Q学习(Q-learning)方法。我们的理论分析表明，监督微调(supervised fine-tuning, SFT)可能会引入基于共现的伪解(spurious solutions)，而强化学习主要通过探索(exploration)实现正确规划，强调了探索在实现更好泛化(generalization)中的作用。然而，我们也表明策略梯度存在多样性崩溃(diversity collapse)问题，即在训练过程中输出多样性减少，且在达到完美准确度后仍然存在。相比之下，Q学习提供了两个关键优势：离策略学习(off-policy learning)和收敛(convergence)时的多样性保持(diversity preservation)。我们进一步证明，在Q学习中需要精心设计奖励(reward design)以防止奖励黑客(reward hacking)问题。最后，我们将我们的框架应用于真实世界规划基准(benchmark)Blocksworld，确认了这些行为在实际中确实存在。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#199",
    "title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model",
    "link": "/arxiv/2509.22518",
    "arxiv_id": "2509.22518",
    "authors": "Bo Li, Guanzhi Deng, Ronghao Chen, Junrong Yue, Shuo Zhang, Qinghua Zhao, Linqi Song, Lijie Wen",
    "summary": "Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms is a challenge in interpretability research. To provide a measurable geometric analysis perspective, we define the concept of the Reasoning Manifold, a latent low-dimensional geometric structure formed by the internal representations corresponding to all correctly reasoned generations. This structure can be conceptualized as the embodiment of the effective thinking paths that the model has learned to successfully solve a given task. Based on this concept, we build REMA, a framework that explains the origins of failures by quantitatively comparing the spatial relationships of internal model representations corresponding to both erroneous and correct reasoning samples. Specifically, REMA first quantifies the geometric deviation of each erroneous representation by calculating its k-nearest neighbors distance to the approximated manifold formed by correct representations, thereby providing a unified failure signal. It then localizes the divergence points where these deviations first become significant by tracking this deviation metric across the model's layers and comparing it against a baseline of internal fluctuations from correct representations, thus identifying where the reasoning chain begins to go off-track. Our extensive experiments on diverse language and multimodal models and tasks demonstrate the low-dimensional nature of the reasoning manifold and the high separability between erroneous and correct reasoning representations. The results also validate the effectiveness of the REMA framework in analyzing the origins of reasoning failures. This research connects abstract reasoning failures to measurable geometric deviations in representations, providing new avenues for in-depth understanding and diagnosis of the internal computational processes of black-box models.",
    "subjects": "Artificial Intelligence, Machine Learning",
    "date": "2025-09-26",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.362876",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是提出REMA框架来解释大语言模型的推理过程和失败机制。论文定义了\"推理流形\"(Reasoning Manifold)概念，研究LLM内部表示的几何结构如何反映推理过程。这不是将LLM作为工具应用到特定领域，而是直接研究LLM本身的推理机制，属于改进LLM基础能力和理解其推理过程的研究，因此应该保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：核心关注\"complex reasoning\"（复杂推理），提出\"reasoning manifold\"概念，分析\"reasoning failures\"（推理失败） 这些关键指标表明论文与通用推理能力研究高度相关。 第三步：排除标准 论文不符合任何排除标准： - 虽然提到在多模态模型上进行实验，但核心焦点不是多模态或视觉研究 - 没有聚焦于任何特定应用领域（如医疗、化学等） - 不涉及水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况 论文属于可解释性研究的特殊情况。它提出了一种新方法（几何分析框架）来增强模型内在的可解释性，通过分析内部表示来理解推理失败的原因和位置，这有助于提升模型的通用推理质量和可靠性，因此应该保留。 最终决策： 论文的核心贡献是提出了一种理解和解释LLM推理过程的框架，通过几何分析的方法研究模型内部表示与推理能力的关系，这直接关系到提升大语言模型的通用推理能力。论文不是应用LLM解决特定领域问题，而是深入研究LLM本身的推理机制，完全符合研究目标。",
    "summary2": "本文旨在解决大型语言模型复杂推理过程的理解和失败机制分析的挑战。针对LLMs的内部表示，我们提出了一种基于\"Reasoning Manifold\"概念的REMA框架，通过量化错误表示与正确推理流形的几何偏差来定位推理失败点，并在多种LLMs和多模态LLMs及多个推理任务上通过内部表示的几何偏差分析和可分离性测试验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n理解大型语言模型如何执行复杂推理及其失败机制是解释性研究的挑战。现有方法多依赖特定错误类型的探针设计或通过对比受控输入对分析表示变化，缺乏一个能从内部表示几何结构定位多样化推理失败的统一框架。\n\n## 关键洞察\n作者将流形假设扩展到LLM推理过程，提出\"推理流形\"概念：当模型学习有效推理策略时，正确推理的内部表示倾向于集中在低维结构化子空间中，而非随机分布。推理失败与模型内部表示显著偏离这些学习到的正确推理流形相关。\n\n## 解决方案演进\n基于此洞察，作者将所有推理失败统一为内部表示相对于正确推理流形的几何偏离。设计两步分析：首先计算错误表示到正确流形的k近邻距离量化偏离程度；然后逐层跟踪偏离距离并与正确表示内部波动基线比较，定位推理链开始偏离的具体点。\n\n## 创新点总结\n创新点在于将抽象推理过程转化为可测量的几何对象，建立任务无关的统一失败解释框架，不仅能量化失败严重程度还能定位失败起源点，为理解黑盒模型内部计算过程提供了新视角和工具。",
    "summary_translation": "理解大型语言模型(Large Language Models, LLMs)如何执行复杂推理及其失败机制是可解释性研究中的一个挑战。为了提供一种可测量的几何分析视角，我们定义了推理流形(Reasoning Manifold)的概念，这是一种由所有正确推理生成内容对应的内部表示所形成的潜在低维几何结构。该结构可被概念化为模型已学习到的、用于成功解决给定任务的有效思维路径的具体体现。基于这一概念，我们构建了REMA框架，该框架通过定量比较错误和正确推理样本对应的模型内部表示的空间关系，来解释失败的起源。具体而言，REMA首先通过计算每个错误表示到由正确表示形成的近似流形的k最近邻(k-nearest neighbors)距离，来量化其几何偏差，从而提供一个统一的失败信号。然后，通过在模型各层中追踪这一偏差指标，并将其与正确表示的内部波动基线进行比较，REMA定位这些偏差首次变得显著的分歧点，从而确定推理链开始偏离轨道的位置。我们在多种语言和多模态模型及任务上的广泛实验证明了推理流形的低维特性以及错误和正确推理表示之间的高度可分性。结果还验证了REMA框架在分析推理失败起源方面的有效性。本研究将抽象的推理失败与表示中的可测量几何偏差联系起来，为深入理解和诊断黑盒模型的内部计算过程提供了新途径。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#227",
    "title": "Bilinear relational structure fixes reversal curse and enables consistent model editing",
    "link": "/arxiv/2509.21993",
    "arxiv_id": "2509.21993",
    "authors": "Dong-Kyum Kim, Minsung Kim, Jea Kwon, Nakyeong Yang, Meeyoung Cha",
    "summary": "The reversal curse -- a language model's (LM) inability to infer an unseen fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a fundamental limitation. We show that this is not an inherent failure but an artifact of how models encode knowledge. By training LMs from scratch on a synthetic dataset of relational knowledge graphs, we demonstrate that bilinear relational structure emerges in their hidden representations. This structure substantially alleviates the reversal curse, enabling LMs to infer unseen reverse facts. Crucially, we also find that this bilinear structure plays a key role in consistent model editing. When a fact is updated in a LM with this structure, the edit correctly propagates to its reverse and other logically dependent facts. In contrast, models lacking this representation not only suffer from the reversal curse but also fail to generalize edits, further introducing logical inconsistencies. Our results establish that training on a relational knowledge dataset induces the emergence of bilinear internal representations, which in turn enable LMs to behave in a logically consistent manner after editing. This implies that the success of model editing depends critically not just on editing algorithms but on the underlying representational geometry of the knowledge being modified.",
    "subjects": "Artificial Intelligence, Machine Learning",
    "date": "2025-09-26",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.383869",
    "filter_reason": "这篇论文完全符合我的研究目标。根据筛选标准，我的判断过程如下： 第一步：核心判断——论文的本质是关于改进LLM的基础逻辑推理能力。论文研究并解决了语言模型中的\"反转诅咒\"问题，即模型无法从\"A是B\"推理出\"B是A\"这一基本逻辑推理缺陷。作者提出通过在关系知识图谱上训练模型，使模型内部形成双线性关系结构，从而增强其逻辑推理能力。这直接属于\"改进LLM的基础能力、增强其逻辑推理能力\"的范畴，符合保留标准。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确研究语言模型(LMs)的内部表示结构 - 能力方向：直接涉及逻辑推理能力，特别是解决基础推理缺陷（反转诅咒） - 论文虽然没有提到强化学习或智能体等新兴范式，但其核心关注点（逻辑推理）正是我研究目标的核心 第三步：排除标准——论文不涉及任何排除领域： - 没有涉及多模态与视觉内容 - 没有聚焦于任何特定应用领域（如医疗、化学等） - 没有主要讨论模型在应用层面的可靠性问题 第四步：特殊和模糊情况——论文不涉及需要特殊处理的情况。虽然讨论了模型编辑的一致性，但这是从基础逻辑推理能力角度出发，而非应用层面的讨论。 论文的核心贡献是揭示了语言模型内部表示结构与其逻辑推理能力之间的关系，并提出了一种通过训练使模型形成双线性关系结构的方法，从而显著提升模型的逻辑推理能力和一致性。这直接符合我\"提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决语言模型中的逆转诅咒和模型编辑中的逻辑一致性问题。针对合成关系知识图数据，我们提出了一种通过适当正则化训练使模型内部表示中出现双线性关系结构的方法，并在合成家庭关系知识图上通过关系推理准确性和模型编辑泛化能力验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n语言模型存在\"反转诅咒\"问题：学习\"A是B\"后无法推断\"B是A\"。同时，模型编辑时无法将更新逻辑传播到相关事实，如修改配偶关系后无法自动更新逆关系和衍生关系。\n\n## 关键洞察\n这些问题并非transformer架构的固有缺陷，而是模型编码知识方式的产物。作者假设，若模型能学习到双线性关系结构，通过矩阵转置和乘法自然捕捉逆关系和组合关系，则可实现逻辑推理。\n\n## 解决方案演进\n作者创建合成家庭关系知识图谱，通过适当正则化训练模型，发现其能克服反转诅咒。探测实验揭示成功模型在中间层形成双线性结构，且满足代数性质。进一步证明，这种结构与模型编辑能力存在强关联，使编辑能逻辑传播。\n\n## 创新点总结\n将反转诅咒和编辑失败视为表示问题而非架构问题，揭示内部表示几何结构对逻辑一致性的关键作用，证明通过适当训练可引导模型学习代数稳健的双线性关系表示。",
    "summary_translation": "逆转诅咒(reversal curse)——即语言模型(language model, LM)无法从已学到的\"A是B\"这一事实推断出未见的\"B是A\"——被广泛认为是一种基本限制。我们表明，这并非一种固有缺陷，而是模型编码知识方式所产生的副产品。通过在关系知识图(relational knowledge graphs)的合成数据集上从头训练语言模型，我们证明了其隐藏表示中出现了双线性关系结构(bilinear relational structure)。这种结构显著减轻了逆转诅咒，使语言模型能够推断出未见的逆向事实。关键的是，我们还发现这种双线性结构在一致的模型编辑(consistent model editing)中起着关键作用。当具有这种结构的语言模型中的事实被更新时，编辑会正确地传播到其逆向事实及其他逻辑相关的事实。相比之下，缺乏这种表示的模型不仅受到逆转诅咒的影响，还无法泛化编辑(generalize edits)，进一步引入逻辑不一致性(logical inconsistencies)。我们的结果确立了，在关系知识数据集上的训练会诱导双线性内部表示(bilinear internal representations)的出现，进而使语言模型在编辑后能够以逻辑一致的方式运行。这意味着模型编辑的成功不仅关键取决于编辑算法(editing algorithms)，还取决于被修改知识的底层表示几何(representational geometry)。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#225",
    "title": "GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments",
    "link": "/arxiv/2509.21998",
    "arxiv_id": "2509.21998",
    "authors": "Hanlin Zhu, Tianyu Guo, Song Mei, Stuart Russell, Nikhil Ghosh, Alberto Bietti, Jiantao Jiao",
    "summary": "As LLMs are increasingly deployed as agents, agentic reasoning - the ability to combine tool use, especially search, and reasoning - becomes a critical skill. However, it is hard to disentangle agentic reasoning when evaluated in complex environments and tasks. Current agent benchmarks often mix agentic reasoning with challenging math reasoning, expert-level knowledge, and other advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent, where an LLM agent is required to solve grade-school-level reasoning problems, but is only presented with the question in the prompt without the premises that contain the necessary information to solve the task, and needs to proactively collect that information using tools. Although the original tasks are grade-school math problems, we observe that even frontier models like GPT-5 only achieve 67% accuracy. To understand and analyze the agentic reasoning patterns, we propose the concept of agentic reasoning graph: cluster the environment's document embeddings into nodes, and map each tool call to its nearest node to build a reasoning path. Surprisingly, we identify that the ability to revisit a previously visited node, widely taken as a crucial pattern in static reasoning, is often missing for agentic reasoning for many models. Based on the insight, we propose a tool-augmented test-time scaling method to improve LLM's agentic reasoning performance by adding tools to encourage models to revisit. We expect our benchmark and the agentic reasoning framework to aid future studies of understanding and pushing the boundaries of agentic reasoning.",
    "subjects": "Artificial Intelligence, Machine Learning",
    "date": "2025-09-26",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.382840",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究LLM作为智能体时的\"agentic reasoning\"（智能体推理）能力，即结合工具使用和推理的通用能力。论文提出了GSM-Agent基准来评估这种能力，并分析了LLM在智能体推理中的行为模式，最终提出了一种工具增强的测试时扩展方法来提高LLM的智能体推理性能。这明显属于改进LLM基础能力和增强其通用推理能力的研究，符合保留标准。 第二步：正面指标 论文包含多个高度相关的正面指标： - 核心概念：明确研究LLM作为智能体的能力 - 能力方向：直接研究推理能力(reasoning)，特别是智能体推理 - 新兴范式：聚焦于LLM-based agents和tool use，这是当前大语言模型推理能力研究的前沿方向 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然使用小学数学问题作为测试平台，但目的不是解决数学问题本身，而是研究通用推理能力 - 不关注模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况处理 论文研究的是通用的智能体推理框架，虽然使用数学问题作为测试平台，但其目的是理解智能体推理的一般模式并提出改进方法，而非专注于数学应用领域。这符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况，应该保留。 核心贡献：论文提出了GSM-Agent基准和agentic reasoning graph框架，用于理解和分析LLM的智能体推理能力，并基于分析提出了改进LLM推理能力的方法。这些贡献直接服务于提升LLM的通用推理能力，完全符合研究目标。",
    "summary2": "本文旨在理解和评估大型语言模型的代理推理能力。针对LLM作为代理需要结合工具使用和推理的场景，我们提出了GSM-Agent基准测试，将GSM8K数学问题转化为需要主动搜索信息的代理任务。通过代理推理图框架分析发现，重新访问已访问节点的能力与代理推理性能强相关。基于此，我们设计了工具增强的测试时扩展方法，在多个LLM上验证了其有效性，显著提升了模型在代理推理任务中的表现。",
    "inspiration_trace": "## 面临的挑战\n作者发现现有基准测试无法清晰比较LLM在静态推理与智能体推理间的能力差异，因为它们常将智能体推理与复杂数学推理、专业知识等混合，难以分离纯粹的智能体推理能力进行评估。\n\n## 关键洞察\n通过实验，作者发现即使是前沿模型在简单智能体推理任务上表现不佳（GPT-5仅67%准确率），而同样任务在静态推理下容易得多。这表明静态推理能力强不直接转化为智能体推理能力强。通过提出\"智能体推理图\"概念，作者发现\"重访\"能力是智能体推理的关键技能。\n\n## 解决方案演进\n基于\"重访是关键技能\"的洞察，作者提出工具增强的测试时扩展方法，通过添加专门工具（如重访工具）鼓励模型重访，而非简单增加交互轮次。实验证明这比传统交互轮次扩展更有效。\n\n## 创新点总结\n创新点在于：1）创建可控智能体推理基准，实现静态与智能体推理的公平比较；2）提出智能体推理图分析框架，量化推理模式；3）发现重访能力的关键作用并设计有效工具增强方法。",
    "summary_translation": "随着大语言模型（LLMs）越来越多地被部署为代理（agents），代理推理（agentic reasoning）——即结合工具使用（tool use），特别是搜索（search）和推理（reasoning）的能力——成为一项关键技能。然而，在复杂环境和任务中进行评估时，很难厘清代理推理的表现。当前的代理基准测试（agent benchmarks）通常将代理推理与具有挑战性的数学推理、专家级知识和其他高级能力混合在一起。为填补这一空白，我们构建了一个名为GSM-Agent的新型基准测试（novel benchmark），在该测试中，大语言模型代理（LLM agent）需要解决小学水平的推理问题，但提示（prompt）中仅呈现问题，而不包含解决任务所需信息的前提（premises），代理需要使用工具主动收集这些信息。尽管原始任务是小学数学问题，但我们观察到即使是像GPT-5这样的前沿模型（frontier models）也只能达到67%的准确率（accuracy）。为了理解和分析代理推理模式（agentic reasoning patterns），我们提出了代理推理图（agentic reasoning graph）的概念：将环境的文档嵌入（document embeddings）聚类成节点（nodes），并将每个工具调用（tool call）映射到最近的节点以构建推理路径（reasoning path）。令人惊讶的是，我们发现许多模型在代理推理中常常缺乏重新访问（revisit）之前访问过的节点的能力，而这种能力在静态推理（static reasoning）中被广泛认为是关键模式。基于这一洞察（insight），我们提出了一种工具增强的测试时扩展方法（tool-augmented test-time scaling method），通过添加工具来鼓励模型重新访问，从而提高大语言模型的代理推理性能（agentic reasoning performance）。我们期望我们的基准测试和代理推理框架（agentic reasoning framework）能够有助于未来对理解和推动代理推理边界的研究。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#242",
    "title": "Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts",
    "link": "/arxiv/2509.21743",
    "arxiv_id": "2509.21743",
    "authors": "Ammar Ahmed, Azal Ahmad Khan, Ayaan Ahmad, Sheng Di, Zirui Liu, Ali Anwar",
    "summary": "Large reasoning models improve accuracy by producing long reasoning traces, but this inflates latency and cost, motivating inference-time efficiency. We propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable ``thought\" steps to guide new problems. RoT organizes steps into a thought graph with sequential and semantic edges to enable fast retrieval and flexible recombination. At inference, RoT retrieves query-relevant nodes and applies reward-guided traversal to assemble a problem-specific template that guides generation. This dynamic template reuse reduces redundant exploration and, therefore, reduces output tokens while preserving accuracy. We evaluate RoT on reasoning benchmarks with multiple models, measuring accuracy, token usage, latency, and memory overhead. Findings show small prompt growth but substantial efficiency gains, with RoT reducing output tokens by up to 40%, inference latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a scalable paradigm for efficient LRM reasoning via dynamic template construction through retrieval.",
    "subjects": "Artificial Intelligence, Machine Learning",
    "date": "2025-09-26",
    "category": "cs.LG",
    "crawl_time": "2025-10-06T19:08:36.389410",
    "filter_reason": "根据筛选标准，这篇论文完全符合研究范围。首先，从核心判断来看，论文的本质是提出\"Retrieval-of-Thought (RoT)\"方法，这是一种改进大语言模型推理能力的新范式。论文通过构建思维图来组织和重用先前的推理步骤，旨在提高LLM的推理效率，同时保持准确性。这明显属于改进LLM基础推理能力的研究，而非将LLM应用于特定领域。 其次，从正面指标看，论文明确涉及大语言模型(LLMs)和推理能力(reasoning)这两个核心概念。摘要中多次提到\"reasoning traces\"、\"reasoning benchmarks\"和\"efficient LRM reasoning\"，表明论文直接关注通用推理能力的提升。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。相反，它专注于提升LLM的通用推理效率。 最后，论文不涉及特殊或模糊情况，如智能体/工具使用或幻觉/可解释性/安全等议题。 综上所述，这篇论文的核心贡献是提出一种通过重用推理步骤来提高LLM推理效率的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决大型推理模型(LRMs)因产生长推理轨迹而导致的延迟和成本问题。针对数学推理任务，我们提出了一种Retrieval-of-Thought (RoT)方法，通过构建思维图重用先前的推理步骤，并在推理时动态组装问题特定模板。在AIME和AMC数学推理基准上，通过准确性、令牌使用量、延迟和成本等指标验证了其有效性，结果显示RoT减少了高达40%的输出令牌、82%的推理延迟和59%的成本，同时保持推理准确性。",
    "inspiration_trace": "## 面临的挑战\n大型推理模型通过产生长推理链提高准确性，但导致高延迟和高成本。输出token比输入token贵2-5倍，且现有检索增强方法依赖静态模板，缺乏动态组合能力，无法灵活适应新问题。\n\n## 关键洞察\n作者发现三个关键现象：同一领域问题的推理步骤高度相似；检索比生成快17倍；提供正确推理路径可大幅减少生成token。这揭示了推理步骤的可重用性，以及人类\"连接点\"能力——将解决方案片段重新组合成新配置——的重要性。\n\n## 解决方案演进\n从简单模板重用开始，发现静态模板缺乏适应性；转向细粒度思路，将推理分解为可重用的\"思想\"步骤；设计思想图组织这些步骤，包含顺序边和语义边；最终提出三步过程：初始节点检索、奖励引导遍历和模板集成，动态构建问题特定推理模板。\n\n## 创新点总结\n首次提出动态模板构建范式，使模型能在推理时组装上下文特定模板；设计带奖励引导遍历的思想图，高效组合推理步骤；通过减少冗余探索，降低输出token最多40%，同时保持准确性，突破静态模板局限，实现接近人类思维的动态推理组合。",
    "summary_translation": "大型推理模型（Large reasoning models）通过产生长推理轨迹（reasoning traces）来提高准确性，但这会增加延迟和成本，从而推动了对推理时效率（inference-time efficiency）的需求。我们提出了思维检索（Retrieval-of-Thought, RoT），它将先前的推理作为可组合的\"思维\"（thought）步骤重用，以指导新问题的解决。RoT将步骤组织成一个具有顺序和语义边（sequential and semantic edges）的思维图（thought graph），以实现快速检索和灵活重组。在推理过程中，RoT检索与查询相关的节点（query-relevant nodes），并应用奖励引导遍历（reward-guided traversal）来组装一个特定问题的模板（problem-specific template），该模板指导生成过程。这种动态模板重用减少了冗余探索，因此在保持准确性的同时减少了输出令牌（output tokens）。我们在多个模型的推理基准（reasoning benchmarks）上评估了RoT，测量了准确性、令牌使用量（token usage）、延迟和内存开销（memory overhead）。研究结果显示提示（prompt）增长较小但效率显著提高，RoT在保持准确性的同时，将输出令牌减少了高达40%，推理延迟（inference latency）降低了82%，成本降低了59%。RoT通过检索进行动态模板构建，为高效的LRM（大型推理模型）推理建立了一个可扩展的范式（scalable paradigm）。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#9",
    "title": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios",
    "link": "/arxiv/2509.22502",
    "arxiv_id": "2509.22502",
    "authors": "Chenglin Yu, Yang Yu, Songmiao Wang, Yucheng Wang, Yifan Yang, Jinjia Li, Ming Li, Hongxia Yang",
    "summary": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in organizing and executing complex tasks, and many such agents are now widely used in various application scenarios. However, developing these agents requires carefully designed workflows, carefully crafted prompts, and iterative tuning, which requires LLM techniques and domain-specific expertise. These hand-crafted limitations hinder the scalability and cost-effectiveness of LLM agents across a wide range of industries. To address these challenges, we propose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that can be applied to \\textbf{infi}nite scenarios, which introduces several key innovations: a generalized \"agent-as-a-tool\" mechanism that automatically decomposes complex agents into hierarchical multi-agent systems; a dual-audit mechanism that ensures the quality and stability of task completion; an agent routing function that enables efficient task-agent matching; and an agent self-evolution mechanism that autonomously restructures the agent DAG based on new tasks, poor performance, or optimization opportunities. Furthermore, InfiAgent's atomic task design supports agent parallelism, significantly improving execution efficiency. This framework evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems. Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.",
    "subjects": "Artificial Intelligence, Human-Computer Interaction",
    "date": "2025-09-26",
    "category": "cs.AI",
    "crawl_time": "2025-10-06T19:08:36.179992",
    "filter_reason": "这篇论文的核心贡献是提出了一种名为InfiAgent的金字塔形DAG多智能体框架，该框架通过自我进化机制增强大语言模型的通用问题解决能力。论文完全符合研究目标，原因如下：首先，论文本质上是关于改进LLM基础能力的，提出了通用\"agent-as-a-tool\"机制和智能体自我进化机制，使LLM能够自动分解复杂任务并自主优化其多智能体结构，这直接提升了LLM的通用推理和规划能力。其次，论文包含多个正面指标，如LLM核心概念、自我进化训练方法、多智能体系统和工具使用等新兴范式。第三，论文不聚焦于任何排除标准中的领域，它强调框架可应用于\"无限场景\"，而非特定领域。最后，虽然论文提到了AI研究助手的案例研究，但这只是框架应用的一个示例，框架本身是通用的，旨在提升LLM在各种场景下的通用推理和问题解决能力。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
    "summary2": "本文旨在解决LLM智能体开发中依赖手工设计工作流和提示的问题，这些限制阻碍了智能体在各行业的可扩展性和成本效益。针对无限场景的应用需求，我们提出了一种InfiAgent自我进化金字塔智能体框架，该框架基于DAG的多智能体系统，引入了\"智能体即工具\"机制、双重审计机制、智能体路由功能和自我进化机制。在多个基准测试和InfiHelper AI研究助手的案例研究中，通过性能指标和人类评审验证了其有效性，相比ADAS框架实现了9.9%的性能提升。",
    "inspiration_trace": "## 面临的挑战\n当前LLM智能体开发依赖精心设计的工作流程和提示，需要专业知识和迭代调优，导致可扩展性和成本效益受限。传统多智能体系统采用点对点协作，造成协调开销、死锁和不可预测行为，缺乏自动分解任务、确保稳定性和自主适应的通用框架。\n\n## 关键洞察\n作者认识到问题本质在于缺乏结构化的智能体组织范式。智能体应被视为可组合工具，形成层次化结构；系统需具备自我进化能力，根据反馈自主调整；同时需要有效的质量保证机制，确保任务完成质量和系统稳定性。\n\n## 解决方案演进\n从洞察出发，作者首先提出\"智能体即工具\"机制，实现复杂智能体的自动分解；继而设计双重审计机制保障质量；然后开发智能路由功能优化任务分配；最后引入自我进化机制，使系统能根据新需求和性能反馈自主重构，形成金字塔状的多智能体架构。\n\n## 创新点总结\n创新点在于将智能体视为可组合工具，实现前所未有的模块化；通过双重审计和自我进化解决稳定性与适应性难题；金字塔结构使系统能处理指数级复杂任务同时保持个体简单性；整个框架通用性强，可自动适应各领域无需专业知识。",
    "summary_translation": "大型语言模型（Large Language Model, LLM）代理（agents）在组织和执行复杂任务方面展现了显著能力，许多此类代理现已在各种应用场景中得到广泛应用。然而，开发这些代理需要精心设计的工作流程（workflows）、精心制作的提示词（prompts）和迭代调优（tuning），这需要LLM技术和特定领域的专业知识。这些手工制作的限制阻碍了LLM代理在广泛行业中的可扩展性（scalability）和成本效益（cost-effectiveness）。为应对这些挑战，我们提出了**InfiAgent**，一个可应用于**无限**（infinite）场景的类似金字塔的基于DAG（Directed Acyclic Graph，有向无环图）的多代理框架（Multi-Agent Framework），该框架引入了几项关键创新：一种通用的\"代理即工具\"（agent-as-a-tool）机制，可自动将复杂代理分解为分层多代理系统；一种双重审计机制（dual-audit mechanism），确保任务完成的质量和稳定性；一种代理路由功能（agent routing function），实现高效的任务-代理匹配；以及一种代理自我进化机制（agent self-evolution mechanism），可根据新任务、性能不佳或优化机会自主重构代理DAG。此外，InfiAgent的原子任务设计（atomic task design）支持代理并行性（agent parallelism），显著提高了执行效率。该框架演变为一个通用的类似金字塔的多代理系统，能够解决广泛的问题。在多个基准测试（benchmarks）上的评估表明，与ADAS（类似的自动生成代理框架）相比，InfiAgent的性能提高了9.9%，而AI研究助手InfiHelper的案例研究（case study）表明，它生成的科学论文获得了顶级IEEE会议人类评审的认可。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#15",
    "title": "Large Language Models as Nondeterministic Causal Models",
    "link": "/arxiv/2509.22297",
    "arxiv_id": "2509.22297",
    "authors": "Sander Beckers",
    "summary": "Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first time, a method for generating counterfactuals of probabilistic Large Language Models. Such counterfactuals tell us what would - or might - have been the output of an LLM if some factual prompt ${\\bf x}$ had been ${\\bf x}^*$ instead. The ability to generate such counterfactuals is an important necessary step towards explaining, evaluating, and comparing, the behavior of LLMs. I argue, however, that the existing method rests on an ambiguous interpretation of LLMs: it does not interpret LLMs literally, for the method involves the assumption that one can change the implementation of an LLM's sampling process without changing the LLM itself, nor does it interpret LLMs as intended, for the method involves explicitly representing a nondeterministic LLM as a deterministic causal model. I here present a much simpler method for generating counterfactuals that is based on an LLM's intended interpretation by representing it as a nondeterministic causal model instead. The advantage of my simpler method is that it is directly applicable to any black-box LLM without modification, as it is agnostic to any implementation details. The advantage of the existing method, on the other hand, is that it directly implements the generation of a specific type of counterfactuals that is useful for certain purposes, but not for others. I clarify how both methods relate by offering a theoretical foundation for reasoning about counterfactuals in LLMs based on their intended semantics, thereby laying the groundwork for novel application-specific methods for generating counterfactuals.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.AI",
    "crawl_time": "2025-10-06T19:08:36.183032",
    "filter_reason": "这篇论文的核心贡献是提出了一种将大语言模型解释为非确定性因果模型的新方法，用于生成LLM的反事实推理。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是因果推理和反事实推理能力，这属于逻辑推理的重要范畴，而非将LLM应用于特定领域。从第二步正面指标看，论文明确关注大语言模型(LLMs)和推理能力(reasoning)，特别是逻辑推理层面。从第三步排除标准看，论文没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面。从第四步特殊情况处理看，论文提出的反事实推理方法可以视为增强模型内在推理质量和可解释性的途径，属于提升LLM通用推理能力的研究。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决概率性大型语言模型(LLMs)的反事实生成问题。针对LLMs的采样过程不确定性，我们提出了一种将LLMs视为非确定性因果模型的核心方法，证明了其满足简单语义，即反事实分布与观察分布形式相同。该方法无需修改源代码，可直接应用于任何黑盒LLM。通过理论分析，我们阐明了现有Gumbel-based方法可视为在简单语义基础上引入特定偏差的实现，为未来开发特定应用的反事实生成方法奠定了理论基础。",
    "inspiration_trace": "## 面临的挑战\n现有LLM反事实生成方法存在解释模糊性，要么未按字面解释LLM（假设可改变采样过程而不改变LLM本身），要么未按预期解释（将非确定性LLM表示为确定性因果模型）。这些方法需要修改源代码，不适用于商业黑盒LLM。\n\n## 关键洞察\n作者洞察到LLM应被理解为非确定性因果模型，而非确定性模型。简单语义（反事实分布与观测分布形式相同）是处理LLM反事实的一般语义，而现有Gumbel-based方法只是在此基础上的特定偏差实现。\n\n## 解决方案演进\n作者首先将LLM形式化为条件概率分布，提出理想化和字面两种解释。通过将LLM表示为非确定性因果模型，证明了简单语义的有效性。进而展示现有方法可视为在简单语义上引入特定偏差的实现，最终建立允许开发不同应用场景反事实生成方法的理论框架。\n\n## 创新点总结\n提供了LLM反事实推理的理论基础，证明简单语义作为一般语义的有效性，使反事实查询计算与事实查询相同，将现有方法重新解释为特定偏差情况，为开发新方法铺平道路，且适用于任何黑盒LLM。",
    "summary_translation": "Chatzi等人和Ravfogel等人的最新研究首次开发了一种生成概率性大型语言模型（Large Language Models, LLMs）反事实（counterfactuals）的方法。这类反事实告诉我们，如果某个事实提示（prompt）${\\bf x}$被替换为${\\bf x}^*$，大型语言模型的输出将会或可能是什么。生成这类反事实的能力是解释、评估和比较大型语言模型行为的重要必要步骤。然而，我认为现有方法基于对大型语言模型的模糊解释：该方法没有从字面上解释大型语言模型，因为它假设可以改变大型语言模型采样过程（sampling process）的实现而不改变大型语言模型本身；也没有按照预期解释大型语言模型，因为它将非确定性（nondeterministic）大型语言模型明确表示为确定性因果模型（deterministic causal model）。我在此提出一种更简单的生成反事实的方法，该方法基于大型语言模型的预期解释，将其表示为非确定性因果模型（nondeterministic causal model）。我的简单方法的优点是它可以直接应用于任何黑盒大型语言模型（black-box LLM）而无需修改，因为它对任何实现细节都是不可知的。另一方面，现有方法的优点是它直接实现了特定类型反事实的生成，这对某些目的有用，但对其他目的则不然。我通过提供一个基于大型语言模型预期语义（semantics）的反事实推理理论基础，阐明了这两种方法之间的关系，从而为新颖的特定应用反事实生成方法奠定了基础。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#13",
    "title": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents",
    "link": "/arxiv/2509.22391",
    "arxiv_id": "2509.22391",
    "authors": "Jiaqi Shao, Yuxiang Lin, Munish Prasad Lohani, Yufeng Miao, Bing Luo",
    "summary": "Recent work has explored training Large Language Model (LLM) search agents with reinforcement learning (RL) for open-domain question answering (QA). However, most evaluations focus solely on final answer accuracy, overlooking how these agents reason with and act on external evidence. We introduce SeekBench, the first benchmark for evaluating the \\textit{epistemic competence} of LLM search agents through step-level analysis of their response traces. SeekBench comprises 190 expert-annotated traces with over 1,800 response steps generated by LLM search agents, each enriched with evidence annotations for granular analysis of whether agents (1) generate reasoning steps grounded in observed evidence, (2) adaptively reformulate searches to recover from low-quality results, and (3) have proper calibration to correctly assess whether the current evidence is sufficient for providing an answer.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-26",
    "category": "cs.AI",
    "crawl_time": "2025-10-06T19:08:36.182077",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是评估LLM搜索代理的\"认知能力\"(epistemic competence)，重点关注它们如何进行基于证据的推理、自适应搜索策略以及证据充分性评估。虽然论文主要提出了一个评估基准而非直接改进LLM的方法，但它直接针对LLM的核心推理能力进行评估，特别是信息处理和决策制定方面的通用能力，而非将LLM作为工具应用于特定领域。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确关注Large Language Models (LLMs)作为搜索代理 - 能力方向：重点研究reasoning（基于证据的推理）和problem-solving（信息搜索问题解决） - 新兴范式：涉及llm-based agents和tool use（搜索工具的使用能力） 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于特定应用领域（如医疗、化学等） - 不主要关注模型可靠性的应用层面（如水印、安全等） 第四步：特殊和模糊情况处理 - 智能体/工具使用：论文研究的是通用的LLM搜索代理能力，而非特定领域应用，符合保留标准 - 认知能力评估：论文关注LLM代理如何基于证据进行推理和评估，这与提高推理质量和减少幻觉相关，符合保留标准 核心贡献：论文提出了SeekBench基准，用于评估LLM搜索代理在信息寻求过程中的认知能力，包括基于证据的推理、搜索策略自适应调整和证据充分性评估。这一基准对于理解和改进LLM的通用推理能力具有重要意义，因为它提供了评估这些能力的方法论，能够推动未来LLM推理能力的研究和提升。因此，尽管论文主要关注评估而非直接改进，但它与\"大语言模型通用推理能力\"的研究目标高度相关。",
    "summary2": "本文旨在解决LLM搜索代理评估中仅关注最终答案准确性而忽视推理过程的问题。针对信息搜索代理的轨迹数据，我们提出了SeekBench基准，用于评估代理的认知能力，并在7个QA基准数据集上通过推理质量指数(RQI)、证据恢复函数(ERF)和校准误差(CE)等指标验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n现有LLM搜索代理评估过于关注最终答案准确性，忽略了代理如何推理和使用外部证据的过程。代理可能获得高基准分数，却表现出无根据声明、无法识别知识缺口等不良认识行为，仅关注结果会误导对代理真实能力的判断。\n\n## 关键洞察\n作者引入\"认识能力\"概念，强调需从\"过程层面\"评估代理。提出三个核心能力：基于证据的推理、自适应证据恢复和证据校准的决策，并通过\"证据状态\"概念捕捉推理过程中信息的质量和充分性。\n\n## 解决方案演进\n首先构建注释模式系统标记代理响应轨迹中的可观察特征；然后分析注释模式识别三种潜在认识能力；最后将这些能力转化为具体量化指标：推理质量指数、证据恢复函数和校准误差，通过大规模实验验证其有效性。\n\n## 创新点总结\n首次提出针对LLM搜索代理的认识能力评估框架，从过程层面而非仅结果层面评估代理，揭示不同代理的专门能力（如Search-R1的合成能力），为开发更可靠的信息搜索代理提供新思路。",
    "summary_translation": "近期研究探索了使用强化学习(reinforcement learning, RL)训练大型语言模型(Large Language Model, LLM)搜索代理，用于开放域问答(open-domain question answering, QA)。然而，大多数评估仅关注最终答案的准确性，忽视了这些代理如何推理并利用外部证据。我们介绍了SeekBench，这是首个通过步骤级分析响应轨迹来评估LLM搜索代理认知能力(epistemic competence)的基准测试。SeekBench包含190条由专家注释的轨迹，这些轨迹由LLM搜索代理生成，包含超过1,800个响应步骤，每条轨迹都附有证据注释，用于精细分析代理是否：(1)生成基于观察证据的推理步骤，(2)自适应地重新制定搜索以从低质量结果中恢复，以及(3)具有适当的校准能力以正确评估当前证据是否足以提供答案。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#44",
    "title": "Can AI Perceive Physical Danger and Intervene?",
    "link": "/arxiv/2509.21651",
    "arxiv_id": "2509.21651",
    "authors": "Abhishek Jindal, Dmitry Kalashnikov, Oscar Chang, Divya Garikapati, Anirudha Majumdar, Pierre Sermanet, Vikas Sindhwani",
    "summary": "When AI interacts with the physical world -- as a robot or an assistive agent -- new safety challenges emerge beyond those of purely ``digital AI\". In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of coffee should not be handed to a child? In this paper, our contributions are three-fold: first, we develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational safety constraints. To probe multi-modal safety understanding, we turn these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, we comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields multi-faceted insights into their deployment readiness for safety-critical agentic applications. Finally, we develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that make safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations. The benchmark will be released at https://asimov-benchmark.github.io/v2",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-10-06T19:08:36.213381",
    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究基础模型（包括大语言模型）在物理安全场景中的推理能力。论文的核心贡献包括：1) 开发了一种评估具身AI系统物理安全性的基准测试方法；2) 分析了主要基础模型在感知风险、安全推理和触发干预方面的能力；3) 提出了一种后训练范式，教导模型明确推理具身特定的安全约束。这些贡献本质上是关于改进LLM的推理能力，特别是安全推理能力，属于提升LLM基础能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标 论文包含多个正面指标： - 核心概念：论文明确研究\"foundation models\"，这包括大语言模型 - 能力方向：论文重点研究\"reason about safety\"（安全推理），这是一种重要的通用推理能力 - 新兴范式：论文涉及\"Embodied AI systems\"和\"agentic applications\"，与基于LLM的智能体相关 第三步：排除标准 虽然论文涉及多模态内容（将叙事转换为图像和视频）和机器人领域（具身AI系统），但这些都不是论文的主要焦点。论文的主要目标是提升AI系统的通用安全推理能力，而非专注于多模态技术或特定机器人应用。 第四步：特殊和模糊情况 论文研究的安全性是从提升模型内在推理能力的角度出发，而非应用层面的安全措施。论文提出的方法使安全推理\"interpretable and transparent\"（可解释和透明），这属于提升模型内在可解释性和推理质量的范畴。 综合判断：这篇论文致力于提升大语言模型在物理安全场景中的通用推理能力，提出了一种新的训练范式来增强模型的安全推理能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
    "summary2": "本文旨在解决AI在物理世界中的安全感知与干预能力评估问题。针对具身AI系统在物理环境中的安全挑战，我们提出了ASIMOV-2.0基准测试框架，通过生成模型将真实伤害叙述转化为多模态安全场景，并在该基准上通过风险识别准确率、约束违反率等指标验证了主流模型的性能。实验揭示了模型在文本与视频模态间的性能差距，以及具身约束理解不足的问题。我们进一步开发了后训练范式，通过监督微调和强化学习使模型生成可解释的安全推理轨迹，显著提升了安全约束满足能力。",
    "inspiration_trace": "## 面临的挑战\nAI进入物理世界带来新型安全挑战，现有安全评估局限于文本领域或数字行动，无法充分评估AI对物理危险的感知、推理和干预能力。AI系统在理解物理安全常识和遵守具身约束方面存在显著不足。\n\n## 关键洞察\n物理安全是多维度的，需基于真实世界伤害报告和工业标准评估。不同模态对AI安全理解能力有不同要求，视频模态特别能评估时间维度的危险感知。推理过程可提高AI对安全约束的理解和遵守。\n\n## 解决方案演进\n首先构建ASIMOV-2.0多模态基准，基于真实数据生成场景；评估主流模型识别\"模态差距\"和\"具身差距\"；通过后训练教导模型明确推理具身安全约束，生成可解释的安全推理轨迹；优化推理过程，发现结构化简洁推理比冗长思维链更有效。\n\n## 创新点总结\n将安全评估建立在真实伤害报告基础上；多模态综合评估特别是视频时间维度；集成具身约束更贴近实际应用；通过\"思考轨迹\"使安全决策透明可解释；提出专门针对安全推理的后训练方法。",
    "summary_translation": "当AI作为机器人(robot)或辅助代理(assistive agent)与物理世界互动时，会出现超出纯\"数字AI\"(digital AI)的新安全挑战。在这种互动中，造成物理伤害的潜力是直接和即时的。最先进的基础模型(foundation models)对物理安全的常识事实理解得如何，例如一个盒子可能太重而无法举起，或者一杯热咖啡不应该递给孩子？\n\n在本文中，我们的贡献有三方面：首先，我们开发了一种高度可扩展的方法，用于对具身AI系统(Embodied AI systems)进行连续物理安全基准测试(physical safety benchmarking)，该方法基于现实世界的伤害叙述(injury narratives)和操作安全约束(operational safety constraints)。为了探究多模态安全理解(multi-modal safety understanding)，我们使用先进的生成模型(generative models)将这些叙述和约束转变为捕捉从安全状态到不安全状态转变的逼真图像(photorealistic images)和视频。其次，我们全面分析了主要基础模型(foundation models)感知风险(perceive risks)、推理安全(reason about safety)和触发干预(trigger interventions)的能力；这为它们在安全关键代理应用(safety-critical agentic applications)中的部署准备度(deployment readiness)提供了多方面的见解(multi-faceted insights)。最后，我们开发了一种后训练范式(post-training paradigm)，教导模型明确推理通过系统指令提供的具身特定安全约束(embodiment-specific safety constraints)。由此产生的模型生成思维痕迹(thinking traces)，使安全推理可解释(interpretable)和透明(transparent)，在约束满足评估(constraint satisfaction evaluations)中实现了最先进的性能(state of the art performance)。\n\n该基准测试(benchmark)将在https://asimov-benchmark.github.io/v2发布。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#50",
    "title": "Correct Reasoning Paths Visit Shared Decision Pivots",
    "link": "/arxiv/2509.21549",
    "arxiv_id": "2509.21549",
    "authors": "Dongkyu Cho, Amy B. Z. Zhang, Bilel Fehri, Sheng Wang, Rumi Chunara, Rui Song, Hengrui Cai",
    "summary": "Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of large language models (LLMs), yet verifying those traces at scale remains unsolved. In response, we introduce the idea of decision pivots-minimal, verifiable checkpoints that any correct reasoning path must visit. We hypothesize that correct reasoning, though stylistically diverse, converge on the same pivot set, while incorrect ones violate at least one pivot. Leveraging this property, we propose a self-training pipeline that (i) samples diverse reasoning paths and mines shared decision pivots, (ii) compresses each trace into pivot-focused short-path reasoning using an auxiliary verifier, and (iii) post-trains the model using its self-generated outputs. The proposed method aligns reasoning without ground truth reasoning data or external metrics. Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the effectiveness of our method.",
    "subjects": "Artificial Intelligence",
    "date": "2025-09-25",
    "category": "cs.AI",
    "crawl_time": "2025-10-06T19:08:36.221496",
    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础推理能力。论文提出了\"决策枢轴\"(decision pivots)的概念和一种新的自我训练流程，旨在增强大语言模型的思维链(CoT)推理能力。这不是将LLM作为工具应用到特定领域，而是直接提升模型本身的通用推理能力，完全符合第一步的保留标准。 其次，论文包含多个正面指标： - 核心概念：明确关注大语言模型(LLMs)和思维链推理 - 能力方向：直接针对推理能力，特别是在逻辑推理(LogiQA)和数学推理(MATH500)等通用推理能力上 - 训练方法：提出了自我训练流程，属于模型自我改进的方法论 第三，论文不符合任何排除标准。虽然在MedQA数据集上进行了测试，但这仅是评估方法在医疗问题上的效果，论文核心并非医疗领域应用，而是通用推理能力的提升。 最后，在特殊和模糊情况处理上，论文通过\"决策枢轴\"来验证和改进推理路径，这与增强模型推理的可解释性和减少幻觉有关，从而提升模型的通用推理质量，符合保留条件。 综上所述，这篇论文的核心贡献是提出了一种通过挖掘\"决策枢轴\"来增强LLM通用推理能力的新方法，完全符合研究目标。",
    "summary2": "本文旨在解决大型语言模型链式思维推理中存在的冗余、自相矛盾或逻辑不健全问题。针对缺乏可扩展方法验证和训练高质量解释的挑战，我们提出了一种基于决策枢轴的自训练方法ROMA，并在LogiQA、MedQA和MATH500基准测试上通过任务准确率和推理质量指标验证了其有效性。",
    "inspiration_trace": "## 面临的挑战\n大语言模型生成的思维链推理常冗余、自相矛盾或逻辑不健全，而现有训练方法只评估整体响应，无法有效监督底层推理过程。缺乏可扩展方法验证和训练高质量解释，尤其在没有人工标注的情况下。\n\n## 关键洞察\n作者发现推理任务由\"决策枢轴\"主导——任何正确解释必须经过的最小可验证检查点。正确推理虽风格多样但收敛于相同枢轴集合，而不正确推理则至少违反一个枢轴，如同\"条条大路通罗马\"。\n\n## 解决方案演进\n从这一洞察出发，作者首先提出通过采样多样化推理路径挖掘共享决策枢轴；然后设计辅助验证器将推理压缩为专注于枢轴的短路径；最后利用自生成输出进行后训练，形成完整自训练循环，无需人工标注。\n\n## 创新点总结\n首次形式化决策枢轴概念，证明正确推理集中在紧凑枢轴集合上；提出不依赖真实推理数据或外部指标的对齐方法；通过多路径聚合提炼出简洁一致的推理路径，建立完全模型驱动的可扩展训练框架。",
    "summary_translation": "思维链(Chain-of-thought, CoT)推理揭示了大型语言模型(large language models, LLMs)的中间思维过程，然而大规模验证这些推理轨迹仍然是一个未解决的问题。为应对此问题，我们提出了决策支点(decision pivots)的概念——即任何正确推理路径都必须经过的最小且可验证的检查点。我们假设，尽管正确的推理在风格上可能多样，但它们会收敛于同一支点集合，而错误的推理则会违反至少一个支点。利用这一特性，我们提出了一个自训练流程(self-training pipeline)，该流程包括：(i) 采样多样化的推理路径并挖掘共享的决策支点；(ii) 使用辅助验证器(auxiliary verifier)将每条轨迹压缩为以支点为中心的短路径推理；(iii) 使用模型自生成的输出对模型进行后训练。所提出的方法能够在没有真实推理数据(ground truth reasoning data)或外部指标(external metrics)的情况下对齐推理。在LogiQA、MedQA和MATH500等标准基准测试上的实验证明了我们方法的有效性。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  },
  {
    "index": "#243",
    "title": "MIXRAG : Mixture-of-Experts Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
    "link": "/arxiv/2509.21391",
    "arxiv_id": "2509.21391",
    "authors": "Lihui Liu, Carl J. Yang",
    "summary": "Large Language Models (LLMs) have achieved impressive performance across a wide range of applications. However, they often suffer from hallucinations in knowledge-intensive domains due to their reliance on static pretraining corpora. To address this limitation, Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating external knowledge sources during inference. Among these sources, textual graphs provide structured and semantically rich information that supports more precise and interpretable reasoning. This has led to growing interest in graph-based RAG systems. Despite their potential, most existing approaches rely on a single retriever to identify relevant subgraphs, which limits their ability to capture the diverse aspects of complex queries. Moreover, these systems often struggle to accurately judge the relevance of retrieved content, making them prone to distraction by irrelevant noise. To address these challenges, in this paper, we propose MIXRAG, a Mixture-of-Experts Graph-RAG framework that introduces multiple specialized graph retrievers and a dynamic routing controller to better handle diverse query intents. Each retriever is trained to focus on a specific aspect of graph semantics, such as entities, relations, or subgraph topology. A Mixture-of-Experts module adaptively selects and fuses relevant retrievers based on the input query. To reduce noise in the retrieved information, we introduce a query-aware GraphEncoder that carefully analyzes relationships within the retrieved subgraphs, highlighting the most relevant parts while down-weighting unnecessary noise. Empirical results demonstrate that our method achieves state-of-the-art performance and consistently outperforms various baselines. MIXRAG is effective across a wide range of graph-based tasks in different domains. The code will be released upon paper acceptance.",
    "subjects": "Information Retrieval, Artificial Intelligence",
    "date": "2025-09-24",
    "category": "cs.AI",
    "crawl_time": "2025-10-06T19:08:36.428572",
    "filter_reason": "这篇论文的核心贡献是提出MIXRAG，一个基于混合专家的图检索增强生成框架，旨在增强大语言模型在处理图结构数据时的推理能力。根据筛选标准，我判断该论文符合研究范围，原因如下： 首先，从本质上看，该论文专注于改进LLM的基础推理能力，而非将其作为工具应用于特定领域。论文提出的MIXRAG框架通过引入多个专门的图检索器和动态路由控制器来处理不同查询意图，并引入查询感知的图编码器来减少噪声，这些都是增强LLM通用推理能力的方法论创新。 其次，论文包含多个正面指标：明确讨论大语言模型(LLMs)，关注推理能力(论文提到\"更精确和可解释的推理\")，并涉及检索增强生成(RAG)这一新兴工具使用范式。 第三，论文不涉及排除标准中的领域：没有关注多模态与视觉问题，没有专注于特定应用领域(虽然提到\"不同领域的图任务\"，但图是一种通用数据结构而非特定领域)，也没有讨论模型基础设施或部署优化。 最后，在特殊情况下，该论文提出的RAG框架可视为一种通用的工具使用方法来增强LLM的问题解决能力，同时论文关注减少LLM幻觉问题，提升其推理质量，这些都符合研究目标。 综上所述，MIXRAG论文致力于提高LLM本身的通用推理能力，特别是通过改进检索增强机制来增强模型对结构化数据的理解和推理，因此符合研究范围。",
    "summary2": "本文旨在解决基于文本图的检索增强生成系统中单一检索器难以处理多样化查询意图和检索内容噪声干扰的问题。针对文本图问答任务，我们提出了一种基于专家混合(Mixture-of-Experts)的检索增强生成框架(MIX RAG)，该框架包含多个专门图检索器和动态路由控制器，以及查询感知的GraphEncoder。在GraphQA benchmark上通过准确率(ACC)和Hit@1指标验证了其有效性，在ExplaGraphs、SceneGraphs和WebQSP三个数据集上均取得了最先进的性能。",
    "inspiration_trace": "## 面临的挑战\n现有图检索增强生成系统依赖单一检索器，难以捕捉文本图中多方面信息（实体、关系、拓扑结构）；同时检索到的子图常含噪声信息，会误导LLM生成不准确回答。\n\n## 关键洞察\n不同查询类型需要不同检索策略，单一检索器无法适应查询意图多样性；检索到的知识价值不等，需动态识别有用信息并过滤噪声；查询与检索专长的动态匹配是提升效果关键。\n\n## 解决方案演进\n从单一检索局限性出发，引入多个专业检索器分别处理不同图语义；借鉴MoE概念设计动态路由控制器，根据查询自适应融合专家；针对噪声问题，设计查询感知GraphEncoder分析子图关系，突出相关信息抑制噪声；整合为统一框架MIXRAG。\n\n## 创新点总结\n首创MoE机制用于图检索增强生成，实现多方面图知识自适应检索；提出查询感知图编码方法动态调整关注度；同时解决检索多样性和噪声问题，显著提升图理解与问答能力。",
    "summary_translation": "大型语言模型（Large Language Models, LLMs）在广泛的应用领域取得了令人印象深刻的性能。然而，由于依赖静态的预训练语料库，它们在知识密集型领域经常出现幻觉（hallucinations）问题。为了解决这一限制，检索增强生成（Retrieval-Augmented Generation, RAG）通过在推理过程中整合外部知识源来增强大型语言模型。在这些知识源中，文本图（textual graphs）提供了结构化和语义丰富的信息，支持更精确和可解释的推理。这导致了对基于图的RAG系统日益增长的兴趣。\n\n尽管具有潜力，大多数现有方法依赖于单个检索器（retriever）来识别相关子图，这限制了它们捕捉复杂查询多方面特征的能力。此外，这些系统往往难以准确判断检索内容的相关性，使其容易受到无关噪声的干扰。为了应对这些挑战，在本文中，我们提出了MIXRAG，一个专家混合图RAG框架（Mixture-of-Experts Graph-RAG framework），它引入了多个专门的图检索器和动态路由控制器，以更好地处理多样化的查询意图。每个检索器被训练专注于图语义的特定方面，如实体、关系或子图拓扑。专家混合模块（Mixture-of-Experts module）根据输入查询自适应地选择和融合相关检索器。\n\n为了减少检索信息中的噪声，我们引入了一个查询感知图编码器（query-aware GraphEncoder），它仔细分析检索到的子图内的关系，突出最相关的部分，同时降低不必要噪声的权重。实验结果表明，我们的方法达到了最先进的性能（state-of-the-art performance），并持续优于各种基线方法。MIXRAG在不同领域的各种基于图的任务中都表现出有效性。代码将在论文接受后发布。",
    "summary_generated_time": "2025-10-06 22:42:05",
    "summary_model": "z-ai/glm-4.5"
  }
]