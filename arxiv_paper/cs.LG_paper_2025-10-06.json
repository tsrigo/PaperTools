[
    {
        "index": "#2",
        "title": "To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable Reinforcement Learning",
        "link": "/arxiv/2510.03207",
        "arxiv_id": "2510.03207",
        "authors": "Yuda Song, Dhruv Rohatgi, Aarti Singh, J. Andrew Bagnell",
        "summary": "Partial observability is a notorious challenge in reinforcement learning (RL), due to the need to learn complex, history-dependent policies. Recent empirical successes have used privileged expert distillation--which leverages availability of latent state information during training (e.g., from a simulator) to learn and imitate the optimal latent, Markovian policy--to disentangle the task of \"learning to see\" from \"learning to act\". While expert distillation is more computationally efficient than RL without latent state information, it also has well-documented failure modes. In this paper--through a simple but instructive theoretical model called the perturbed Block MDP, and controlled experiments on challenging simulated locomotion tasks--we investigate the algorithmic trade-off between privileged expert distillation and standard RL without privileged information. Our main findings are: (1) The trade-off empirically hinges on the stochasticity of the latent dynamics, as theoretically predicted by contrasting approximate decodability with belief contraction in the perturbed Block MDP; and (2) The optimal latent policy is not always the best latent policy to distill. Our results suggest new guidelines for effectively exploiting privileged information, potentially advancing the efficiency of policy learning across many practical partially observable domains.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.551973"
    },
    {
        "index": "#3",
        "title": "Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling",
        "link": "/arxiv/2510.03199",
        "arxiv_id": "2510.03199",
        "authors": "Qiwei Di, Kaixuan Ji, Xuheng Li, Heyang Zhao, Quanquan Gu",
        "summary": "LLM inference often generates a batch of candidates for a prompt and selects one via strategies like majority voting or Best-of- N (BoN). For difficult tasks, this single-shot selection often underperforms. Consequently, evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses, and only the best of them is used when computing regret. Motivated by this, we study inference scaling in the more general Pass@$k$ inference setting, and prove that neither majority voting nor BoN exhibits the desirable scaling with $k$ and the sampling budget $N$. Combining the advantages of majority voting and BoN, we propose a new inference strategy called Best-of-Majority (BoM), with a pivotal step that restricts the candidates to the responses with high frequency in the $N$ samples before selecting the top-$k$ rewards. We prove that when the sampling budget is $N=\\tilde\\Omega(C^*)$, the regret of BoM is $O(\\epsilon_{\\mathrm{opt}}+\\sqrt{\\epsilon_{\\mathrm{RM}}^2C^*/k})$, where $C^*$ is the coverage coefficient, $\\epsilon_{\\mathrm{RM}}$ is the estimation error of the reward model, and $\\epsilon_{\\mathrm{opt}}$ is the estimation error of reward at the optimal response. We further establish a matching lower bound, certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a key advantage: unlike majority voting and BoN, its performance does not degrade when increasing $N$. Experimental results of inference on math problems show BoM outperforming both majority voting and BoN.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.552442"
    },
    {
        "index": "#4",
        "title": "Estimation of Resistance Training RPE using Inertial Sensors and Electromyography",
        "link": "/arxiv/2510.03197",
        "arxiv_id": "2510.03197",
        "authors": "James Thomas, Johan Walhström",
        "summary": "Accurate estimation of rating of perceived exertion (RPE) can enhance resistance training through personalized feedback and injury prevention. This study investigates the application of machine learning models to estimate RPE during single-arm dumbbell bicep curls, using data from wearable inertial and electromyography (EMG) sensors. A custom dataset of 69 sets and over 1000 repetitions was collected, with statistical features extracted for model training. Among the models evaluated, a random forest classifier achieved the highest performance, with 41.4% exact accuracy and 85.9% $\\pm1$ RPE accuracy. While the inclusion of EMG data slightly improved model accuracy over inertial sensors alone, its utility may have been limited by factors such as data quality and placement sensitivity. Feature analysis highlighted eccentric repetition time as the strongest RPE predictor. The results demonstrate the feasibility of wearable-sensor-based RPE estimation and identify key challenges for improving model generalizability.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.552852"
    },
    {
        "index": "#5",
        "title": "Superposition disentanglement of neural representations reveals hidden alignment",
        "link": "/arxiv/2510.03186",
        "arxiv_id": "2510.03186",
        "authors": "André Longon, David Klindt, Meenakshi Khosla",
        "summary": "The superposition hypothesis states that a single neuron within a population may participate in the representation of multiple features in order for the population to represent more features than the number of neurons. In neuroscience and AI, representational alignment metrics measure the extent to which different deep neural networks (DNNs) or brains represent similar information. In this work, we explore a critical question: \\textit{does superposition interact with alignment metrics in any undesirable way?} We hypothesize that models which represent the same features in \\textit{different superposition arrangements}, i.e., their neurons have different linear combinations of the features, will interfere with predictive mapping metrics (semi-matching, soft-matching, linear regression), producing lower alignment than expected. We first develop a theory for how the strict permutation metrics are dependent on superposition arrangements. This is tested by training sparse autoencoders (SAEs) to disentangle superposition in toy models, where alignment scores are shown to typically increase when a model's base neurons are replaced with its sparse overcomplete latent codes. We find similar increases for DNN\\(\\rightarrow\\)DNN and DNN\\(\\rightarrow\\)brain linear regression alignment in the visual domain. Our results suggest that superposition disentanglement is necessary for mapping metrics to uncover the true representational alignment between neural codes.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.553270"
    },
    {
        "index": "#6",
        "title": "PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning",
        "link": "/arxiv/2510.03185",
        "arxiv_id": "2510.03185",
        "authors": "Wanjia Zhao, Qinwei Ma, Jingzhe Shi, Shirley Wu, Jiaqi Han, Yijia Xiao, Si-Yuan Chen, Xiao Luo, Ludwig Schmidt, James Zou",
        "summary": "Benchmarks for competition-style reasoning have advanced evaluation in mathematics and programming, yet physics remains comparatively explored. Most existing physics benchmarks evaluate only final answers, which fail to capture reasoning processes, while recent stepwise methods rely on heuristic LLM-as-judge scoring or restrictive linear assumptions, limiting reliability and diagnostic validity. We introduce PRISM-Physics, a process-level evaluation framework and benchmark for complex physics reasoning problems. Solutions are represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding causal dependencies among intermediate steps to enable fine-grained, interpretable, and theoretically grounded scoring. We prove the optimality of the DAG representation and the corresponding scoring policy. Combining with a fully rule-based method for symbolic formula equivalence matching that we developed, we ensure consistent validation across diverse formulations without heuristic judgments. Results show that our evaluation framework is more aligned with human experts' scoring. Experiments on state-of-the-art LLMs reveal persistent reasoning failures in physics, while step-level scoring offers both diagnostic insight and rich signals for later training. By combining structural rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides a principled foundation for advancing process-level evaluation and guiding the development of models with deeper scientific reasoning capabilities.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.553776"
    },
    {
        "index": "#7",
        "title": "Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning",
        "link": "/arxiv/2510.03181",
        "arxiv_id": "2510.03181",
        "authors": "Ha Manh Bui, Felix Parker, Kimia Ghobadi, Anqi Liu",
        "summary": "We study the Non-Stationary Reinforcement Learning (RL) under distribution shifts in both finite-horizon episodic and infinite-horizon discounted Markov Decision Processes (MDPs). In the finite-horizon case, the transition functions may suddenly change at a particular episode. In the infinite-horizon setting, such changes can occur at an arbitrary time step during the agent's interaction with the environment. While the Q-learning Upper Confidence Bound algorithm (QUCB) can discover a proper policy during learning, due to the distribution shifts, this policy can exploit sub-optimal rewards after the shift happens. To address this issue, we propose Density-QUCB (DQUCB), a shift-aware Q-learning~UCB algorithm, which uses a transition density function to detect distribution shifts, then leverages its likelihood to enhance the uncertainty estimation quality of Q-learning~UCB, resulting in a balance between exploration and exploitation. Theoretically, we prove that our oracle DQUCB achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the computational efficiency of model-free RL and outperforms QUCB baselines by having a lower regret across RL tasks, as well as a real-world COVID-19 patient hospital allocation task using a Deep-Q-learning architecture.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.554202"
    },
    {
        "index": "#8",
        "title": "FTTE: Federated Learning on Resource-Constrained Devices",
        "link": "/arxiv/2510.03165",
        "arxiv_id": "2510.03165",
        "authors": "Irene Tenison, Anna Murphy, Charles Beauville, Lalana Kagal",
        "summary": "Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy, but deployment on resource-constrained edge nodes remains challenging due to limited memory, energy, and communication bandwidth. Traditional synchronous and asynchronous FL approaches further suffer from straggler induced delays and slow convergence in heterogeneous, large scale networks. We present FTTE (Federated Tiny Training Engine),a novel semi-asynchronous FL framework that uniquely employs sparse parameter updates and a staleness-weighted aggregation based on both age and variance of client updates. Extensive experiments across diverse models and data distributions - including up to 500 clients and 90% stragglers - demonstrate that FTTE not only achieves 81% faster convergence, 80% lower on-device memory usage, and 69% communication payload reduction than synchronous FL (eg.FedAVG), but also consistently reaches comparable or higher target accuracy than semi-asynchronous (eg.FedBuff) in challenging regimes. These results establish FTTE as the first practical and scalable solution for real-world FL deployments on heterogeneous and predominantly resource-constrained edge devices.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.554633"
    },
    {
        "index": "#9",
        "title": "Why Do We Need Warm-up? A Theoretical Perspective",
        "link": "/arxiv/2510.03164",
        "arxiv_id": "2510.03164",
        "authors": "Foivos Alimisis, Rustem Islamov, Aurelien Lucchi",
        "summary": "Learning rate warm-up - increasing the learning rate at the beginning of training - has become a ubiquitous heuristic in modern deep learning, yet its theoretical foundations remain poorly understood. In this work, we provide a principled explanation for why warm-up improves training. We rely on a generalization of the $(L_0, L_1)$-smoothness condition, which bounds local curvature as a linear function of the loss sub-optimality and exhibits desirable closure properties. We demonstrate both theoretically and empirically that this condition holds for common neural architectures trained with mean-squared error and cross-entropy losses. Under this assumption, we prove that Gradient Descent with a warm-up schedule achieves faster convergence than with a fixed step-size, establishing upper and lower complexity bounds. Finally, we validate our theoretical insights through experiments on language and vision models, confirming the practical benefits of warm-up schedules.",
        "subjects": "Machine Learning, Optimization and Control, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.555092"
    },
    {
        "index": "#10",
        "title": "Calibrated Uncertainty Sampling for Active Learning",
        "link": "/arxiv/2510.03162",
        "arxiv_id": "2510.03162",
        "authors": "Ha Manh Bui, Iliana Maifeld-Carucci, Anqi Liu",
        "summary": "We study the problem of actively learning a classifier with a low calibration error. One of the most popular Acquisition Functions (AFs) in pool-based Active Learning (AL) is querying by the model's uncertainty. However, we recognize that an uncalibrated uncertainty model on the unlabeled pool may significantly affect the AF effectiveness, leading to sub-optimal generalization and high calibration error on unseen data. Deep Neural Networks (DNNs) make it even worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we propose a new AF by estimating calibration errors and query samples with the highest calibration error before leveraging DNN uncertainty. Specifically, we utilize a kernel calibration error estimator under the covariate shift and formally show that AL with this AF eventually leads to a bounded calibration error on the unlabeled pool and unseen test data. Empirically, our proposed method surpasses other AF baselines by having a lower calibration and generalization error across pool-based AL settings.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.555504"
    },
    {
        "index": "#11",
        "title": "Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory Perspective",
        "link": "/arxiv/2510.03151",
        "arxiv_id": "2510.03151",
        "authors": "Yehuda Dar",
        "summary": "This paper uses classical high-rate quantization theory to provide new insights into mixture-of-experts (MoE) models for regression tasks. Our MoE is defined by a segmentation of the input space to regions, each with a single-parameter expert that acts as a constant predictor with zero-compute at inference. Motivated by high-rate quantization theory assumptions, we assume that the number of experts is sufficiently large to make their input-space regions very small. This lets us to study the approximation error of our MoE model class: (i) for one-dimensional inputs, we formulate the test error and its minimizing segmentation and experts; (ii) for multidimensional inputs, we formulate an upper bound for the test error and study its minimization. Moreover, we consider the learning of the expert parameters from a training dataset, given an input-space segmentation, and formulate their statistical learning properties. This leads us to theoretically and empirically show how the tradeoff between approximation and estimation errors in MoE learning depends on the number of experts.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.561054"
    },
    {
        "index": "#12",
        "title": "Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking",
        "link": "/arxiv/2510.03149",
        "arxiv_id": "2510.03149",
        "authors": "Dhruv Rohatgi, Abhishek Shetty, Donya Saless, Yuchen Li, Ankur Moitra, Andrej Risteski, Dylan J. Foster",
        "summary": "Test-time algorithms that combine the generative power of language models with process verifiers that assess the quality of partial generations offer a promising lever for eliciting new reasoning capabilities, but the algorithmic design space and computational scaling properties of such approaches are still opaque, and their benefits are far from apparent when one accounts for the cost of learning a high-quality verifier. Our starting point is the observation that seemingly benign errors in a learned verifier can lead to catastrophic failures for standard decoding techniques due to error amplification during the course of generation. We then ask: can this be improved with more sophisticated decoding strategies? We introduce a new process-guided test-time sampling algorithm, VGB, which uses theoretically grounded backtracking to achieve provably better robustness to verifier errors. VGB interprets autoregressive generation as a random walk on a tree of partial generations, with transition probabilities guided by the process verifier and base model; crucially, backtracking occurs probabilistically. This process generalizes the seminal Sinclair-Jerrum random walk (Sinclair & Jerrum, 1989) from the literature on approximate counting and sampling in theoretical computer science, and a conceptual contribution of our work is to highlight parallels with this literature. Empirically, we demonstrate on both synthetic and real language modeling tasks that VGB outperforms baselines on a variety of metrics.",
        "subjects": "Machine Learning, Data Structures and Algorithms",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.561564"
    },
    {
        "index": "#13",
        "title": "Enhancing XAI Narratives through Multi-Narrative Refinement and Knowledge Distillation",
        "link": "/arxiv/2510.03134",
        "arxiv_id": "2510.03134",
        "authors": "Flavio Giorgi, Matteo Silvestri, Cesare Campagnano, Fabrizio Silvestri, Gabriele Tolomei",
        "summary": "Explainable Artificial Intelligence has become a crucial area of research, aiming to demystify the decision-making processes of deep learning models. Among various explainability techniques, counterfactual explanations have been proven particularly promising, as they offer insights into model behavior by highlighting minimal changes that would alter a prediction. Despite their potential, these explanations are often complex and technical, making them difficult for non-experts to interpret. To address this challenge, we propose a novel pipeline that leverages Language Models, large and small, to compose narratives for counterfactual explanations. We employ knowledge distillation techniques along with a refining mechanism to enable Small Language Models to perform comparably to their larger counterparts while maintaining robust reasoning abilities. In addition, we introduce a simple but effective evaluation method to assess natural language narratives, designed to verify whether the models' responses are in line with the factual, counterfactual ground truth. As a result, our proposed pipeline enhances both the reasoning capabilities and practical performance of student models, making them more suitable for real-world use cases.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.562010"
    },
    {
        "index": "#14",
        "title": "Signature-Informed Transformer for Asset Allocation",
        "link": "/arxiv/2510.03129",
        "arxiv_id": "2510.03129",
        "authors": "Yoontae Hwang, Stefan Zohren",
        "summary": "Robust asset allocation is a key challenge in quantitative finance, where deep-learning forecasters often fail due to objective mismatch and error amplification. We introduce the Signature-Informed Transformer (SIT), a novel framework that learns end-to-end allocation policies by directly optimizing a risk-aware financial objective. SIT's core innovations include path signatures for a rich geometric representation of asset dynamics and a signature-augmented attention mechanism embedding financial inductive biases, like lead-lag effects, into the model. Evaluated on daily S\\&P 100 equity data, SIT decisively outperforms traditional and deep-learning baselines, especially when compared to predict-then-optimize models. These results indicate that portfolio-aware objectives and geometry-aware inductive biases are essential for risk-aware capital allocation in machine-learning systems. The code is available at: https://github.com/Yoontae6719/Signature-Informed-Transformer-For-Asset-Allocation",
        "subjects": "Machine Learning, Artificial Intelligence, Portfolio Management",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.562449"
    },
    {
        "index": "#15",
        "title": "Real Time Headway Predictions in Urban Rail Systems and Implications for Service Control: A Deep Learning Approach",
        "link": "/arxiv/2510.03121",
        "arxiv_id": "2510.03121",
        "authors": "Muhammad Usama, Haris Koutsopoulos",
        "summary": "Efficient real-time dispatching in urban metro systems is essential for ensuring service reliability, maximizing resource utilization, and improving passenger satisfaction. This study presents a novel deep learning framework centered on a Convolutional Long Short-Term Memory (ConvLSTM) model designed to predict the complex spatiotemporal propagation of train headways across an entire metro line. By directly incorporating planned terminal headways as a critical input alongside historical headway data, the proposed model accurately forecasts future headway dynamics, effectively capturing both their temporal evolution and spatial dependencies across all stations. This capability empowers dispatchers to evaluate the impact of various terminal headway control decisions without resorting to computationally intensive simulations. We introduce a flexible methodology to simulate diverse dispatcher strategies, ranging from maintaining even headways to implementing custom patterns derived from observed terminal departures. In contrast to existing research primarily focused on passenger load predictioning or atypical disruption scenarios, our approach emphasizes proactive operational control. Evaluated on a large-scale dataset from an urban metro line, the proposed ConvLSTM model demonstrates promising headway predictions, offering actionable insights for real-time decision-making. This framework provides rail operators with a powerful, computationally efficient tool to optimize dispatching strategies, thereby significantly improving service consistency and passenger satisfaction.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.562854"
    },
    {
        "index": "#16",
        "title": "AdaBet: Gradient-free Layer Selection for Efficient Training of Deep Neural Networks",
        "link": "/arxiv/2510.03101",
        "arxiv_id": "2510.03101",
        "authors": "Irene Tenison, Soumyajit Chatterjee, Fahim Kawsar, Mohammad Malekzadeh",
        "summary": "To utilize pre-trained neural networks on edge and mobile devices, we often require efficient adaptation to user-specific runtime data distributions while operating under limited compute and memory resources. On-device retraining with a target dataset can facilitate such adaptations; however, it remains impractical due to the increasing depth of modern neural nets, as well as the computational overhead associated with gradient-based optimization across all layers. Current approaches reduce training cost by selecting a subset of layers for retraining, however, they rely on labeled data, at least one full-model backpropagation, or server-side meta-training; limiting their suitability for constrained devices. We introduce AdaBet, a gradient-free layer selection approach to rank important layers by analyzing topological features of their activation spaces through Betti Numbers and using forward passes alone. AdaBet allows selecting layers with high learning capacity, which are important for retraining and adaptation, without requiring labels or gradients. Evaluating AdaBet on sixteen pairs of benchmark models and datasets, shows AdaBet achieves an average gain of 5% more classification accuracy over gradient-based baselines while reducing average peak memory consumption by 40%.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.563283"
    },
    {
        "index": "#17",
        "title": "Adaptive Node Feature Selection For Graph Neural Networks",
        "link": "/arxiv/2510.03096",
        "arxiv_id": "2510.03096",
        "authors": "Ali Azizpour, Madeline Navarro, Santiago Segarra",
        "summary": "We propose an adaptive node feature selection approach for graph neural networks (GNNs) that identifies and removes unnecessary features during training. The ability to measure how features contribute to model output is key for interpreting decisions, reducing dimensionality, and even improving performance by eliminating unhelpful variables. However, graph-structured data introduces complex dependencies that may not be amenable to classical feature importance metrics. Inspired by this challenge, we present a model- and task-agnostic method that determines relevant features during training based on changes in validation performance upon permuting feature values. We theoretically motivate our intervention-based approach by characterizing how GNN performance depends on the relationships between node data and graph structure. Not only do we return feature importance scores once training concludes, we also track how relevance evolves as features are successively dropped. We can therefore monitor if features are eliminated effectively and also evaluate other metrics with this technique. Our empirical results verify the flexibility of our approach to different graph architectures as well as its adaptability to more challenging graph learning settings.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.563707"
    },
    {
        "index": "#18",
        "title": "Distilled Protein Backbone Generation",
        "link": "/arxiv/2510.03095",
        "arxiv_id": "2510.03095",
        "authors": "Liyang Xie, Haoran Zhang, Zhendong Wang, Wesley Tansey, Mingyuan Zhou",
        "summary": "Diffusion- and flow-based generative models have recently demonstrated strong performance in protein backbone generation tasks, offering unprecedented capabilities for de novo protein design. However, while achieving notable performance in generation quality, these models are limited by their generating speed, often requiring hundreds of iterative steps in the reverse-diffusion process. This computational bottleneck limits their practical utility in large-scale protein discovery, where thousands to millions of candidate structures are needed. To address this challenge, we explore the techniques of score distillation, which has shown great success in reducing the number of sampling steps in the vision domain while maintaining high generation quality. However, a straightforward adaptation of these methods results in unacceptably low designability. Through extensive study, we have identified how to appropriately adapt Score identity Distillation (SiD), a state-of-the-art score distillation strategy, to train few-step protein backbone generators which significantly reduce sampling time, while maintaining comparable performance to their pretrained teacher model. In particular, multistep generation combined with inference time noise modulation is key to the success. We demonstrate that our distilled few-step generators achieve more than a 20-fold improvement in sampling speed, while achieving similar levels of designability, diversity, and novelty as the Proteina teacher model. This reduction in inference cost enables large-scale in silico protein design, thereby bringing diffusion-based models closer to real-world protein engineering applications.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.564193"
    },
    {
        "index": "#19",
        "title": "Bootstrap Learning for Combinatorial Graph Alignment with Sequential GNNs",
        "link": "/arxiv/2510.03086",
        "arxiv_id": "2510.03086",
        "authors": "Marc Lelarge",
        "summary": "Graph neural networks (GNNs) have struggled to outperform traditional optimization methods on combinatorial problems, limiting their practical impact. We address this gap by introducing a novel chaining procedure for the graph alignment problem, a fundamental NP-hard task of finding optimal node correspondences between unlabeled graphs using only structural information. Our method trains a sequence of GNNs where each network learns to iteratively refine similarity matrices produced by previous networks. During inference, this creates a bootstrap effect: each GNN improves upon partial solutions by incorporating discrete ranking information about node alignment quality from prior iterations. We combine this with a powerful architecture that operates on node pairs rather than individual nodes, capturing global structural patterns essential for alignment that standard message-passing networks cannot represent. Extensive experiments on synthetic benchmarks demonstrate substantial improvements: our chained GNNs achieve over 3x better accuracy than existing methods on challenging instances, and uniquely solve regular graphs where all competing approaches fail. When combined with traditional optimization as post-processing, our method substantially outperforms state-of-the-art solvers on the graph alignment benchmark.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.564591"
    },
    {
        "index": "#20",
        "title": "A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem",
        "link": "/arxiv/2510.03065",
        "arxiv_id": "2510.03065",
        "authors": "Mingfeng Fan, Jiaqi Cheng, Yaoxin Wu, Yifeng Zhang, Yibin Yang, Guohua Wu, Guillaume Sartoretti",
        "summary": "In recent years, deep reinforcement learning (DRL) has gained traction for solving the NP-hard traveling salesman problem (TSP). However, limited attention has been given to the close-enough TSP (CETSP), primarily due to the challenge introduced by its neighborhood-based visitation criterion, wherein a node is considered visited if the agent enters a compact neighborhood around it. In this work, we formulate a Markov decision process (MDP) for CETSP using a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL) framework that separates decision-making into node selection and waypoint determination. Specifically, an adapted encoder is employed for effective feature extraction, followed by a node-decoder and a loc-decoder to handle the two sub-tasks, respectively. A k-nearest neighbors subgraph interaction strategy is further introduced to enhance spatial reasoning during location decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a unified model capable of generalizing across different problem sizes and varying neighborhood radius types (i.e., constant and random radii). Experimental results show that UD3RL outperforms conventional methods in both solution quality and runtime, while exhibiting strong generalization across problem scales, spatial distributions, and radius ranges, as well as robustness to dynamic environments.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.565076"
    },
    {
        "index": "#21",
        "title": "Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation",
        "link": "/arxiv/2510.03064",
        "arxiv_id": "2510.03064",
        "authors": "Ubayd Bapoo, Clement N Nyirenda",
        "summary": "This study evaluates the performance of Soft Actor Critic (SAC), Greedy Actor Critic (GAC), and Truncated Quantile Critics (TQC) in high-dimensional decision-making tasks using fully observable environments. The focus is on parametrized action (PA) spaces, eliminating the need for recurrent networks, with benchmarks Platform-v0 and Goal-v0 testing discrete actions linked to continuous action-parameter spaces. Hyperparameter optimization was performed with Microsoft NNI, ensuring reproducibility by modifying the codebase for GAC and TQC. Results show that Parameterized Action Greedy Actor-Critic (PAGAC) outperformed other algorithms, achieving the fastest training times and highest returns across benchmarks, completing 5,000 episodes in 41:24 for the Platform game and 24:04 for the Robot Soccer Goal game. Its speed and stability provide clear advantages in complex action spaces. Compared to PASAC and PATQC, PAGAC demonstrated superior efficiency and reliability, making it ideal for tasks requiring rapid convergence and robust performance. Future work could explore hybrid strategies combining entropy-regularization with truncation-based methods to enhance stability and expand investigations into generalizability.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.565500"
    },
    {
        "index": "#22",
        "title": "ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization",
        "link": "/arxiv/2510.03051",
        "arxiv_id": "2510.03051",
        "authors": "Jamison Meindl, Yunsheng Tian, Tony Cui, Veronika Thost, Zhang-Wei Hong, Johannes Dürholt, Jie Chen, Wojciech Matusik, Mina Konaković Luković",
        "summary": "Global optimization of expensive, derivative-free black-box functions requires extreme sample efficiency. While Bayesian optimization (BO) is the current state-of-the-art, its performance hinges on surrogate and acquisition function hyper-parameters that are often hand-tuned and fail to generalize across problem landscapes. We present ZeroShotOpt, a general-purpose, pretrained model for continuous black-box optimization tasks ranging from 2D to 20D. Our approach leverages offline reinforcement learning on large-scale optimization trajectories collected from 12 BO variants. To scale pretraining, we generate millions of synthetic Gaussian process-based functions with diverse landscapes, enabling the model to learn transferable optimization policies. As a result, ZeroShotOpt achieves robust zero-shot generalization on a wide array of unseen benchmarks, matching or surpassing the sample efficiency of leading global optimizers, including BO, while also offering a reusable foundation for future extensions and improvements. Our open-source code, dataset, and model are available at: https://github.com/jamisonmeindl/zeroshotopt",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.571151"
    },
    {
        "index": "#23",
        "title": "Bayesian E(3)-Equivariant Interatomic Potential with Iterative Restratification of Many-body Message Passing",
        "link": "/arxiv/2510.03046",
        "arxiv_id": "2510.03046",
        "authors": "Soohaeng Yoo Willow, Tae Hyeon Park, Gi Beom Sim, Sung Wook Moon, Seung Kyu Min, D. ChangMo Yang, Hyun Woo Kim, Juho Lee, Chang Woo Myung",
        "summary": "Machine learning potentials (MLPs) have become essential for large-scale atomistic simulations, enabling ab initio-level accuracy with computational efficiency. However, current MLPs struggle with uncertainty quantification, limiting their reliability for active learning, calibration, and out-of-distribution (OOD) detection. We address these challenges by developing Bayesian E(3) equivariant MLPs with iterative restratification of many-body message passing. Our approach introduces the joint energy-force negative log-likelihood (NLL$_\\text{JEF}$) loss function, which explicitly models uncertainty in both energies and interatomic forces, yielding superior accuracy compared to conventional NLL losses. We systematically benchmark multiple Bayesian approaches, including deep ensembles with mean-variance estimation, stochastic weight averaging Gaussian, improved variational online Newton, and laplace approximation by evaluating their performance on uncertainty prediction, OOD detection, calibration, and active learning tasks. We further demonstrate that NLL$_\\text{JEF}$ facilitates efficient active learning by quantifying energy and force uncertainties. Using Bayesian active learning by disagreement (BALD), our framework outperforms random sampling and energy-uncertainty-based sampling. Our results demonstrate that Bayesian MLPs achieve competitive accuracy with state-of-the-art models while enabling uncertainty-guided active learning, OOD detection, and energy/forces calibration. This work establishes Bayesian equivariant neural networks as a powerful framework for developing uncertainty-aware MLPs for atomistic simulations at scale.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.571659"
    },
    {
        "index": "#24",
        "title": "CHORD: Customizing Hybrid-precision On-device Model for Sequential Recommendation with Device-cloud Collaboration",
        "link": "/arxiv/2510.03038",
        "arxiv_id": "2510.03038",
        "authors": "Tianqi Liu, Kairui Fu, Shengyu Zhang, Wenyan Fan, Zhaocheng Du, Jieming Zhu, Fan Wu, Fei Wu",
        "summary": "With the advancement of mobile device capabilities, deploying reranking models directly on devices has become feasible, enabling real-time contextual recommendations. When migrating models from cloud to devices, resource heterogeneity inevitably necessitates model compression. Recent quantization methods show promise for efficient deployment, yet they overlook device-specific user interests, resulting in compromised recommendation accuracy. While on-device finetuning captures personalized user preference, it imposes additional computational burden through local retraining. To address these challenges, we propose a framework for \\underline{\\textbf{C}}ustomizing \\underline{\\textbf{H}}ybrid-precision \\underline{\\textbf{O}}n-device model for sequential \\underline{\\textbf{R}}ecommendation with \\underline{\\textbf{D}}evice-cloud collaboration (\\textbf{CHORD}), leveraging channel-wise mixed-precision quantization to simultaneously achieve personalization and resource-adaptive deployment. CHORD distributes randomly initialized models across heterogeneous devices and identifies user-specific critical parameters through auxiliary hypernetwork modules on the cloud. Our parameter sensitivity analysis operates across multiple granularities (layer, filter, and element levels), enabling precise mapping from user profiles to quantization strategy. Through on-device mixed-precision quantization, CHORD delivers dynamic model adaptation and accelerated inference without backpropagation, eliminating costly retraining cycles. We minimize communication overhead by encoding quantization strategies using only 2 bits per channel instead of 32-bit weights. Experiments on three real-world datasets with two popular backbones (SASRec and Caser) demonstrate the accuracy, efficiency, and adaptivity of CHORD.",
        "subjects": "Machine Learning, Artificial Intelligence, Information Retrieval",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.572200"
    },
    {
        "index": "#25",
        "title": "Lightweight Transformer for EEG Classification via Balanced Signed Graph Algorithm Unrolling",
        "link": "/arxiv/2510.03027",
        "arxiv_id": "2510.03027",
        "authors": "Junyi Yao, Parham Eftekhar, Gene Cheung, Xujin Chris Liu, Yao Wang, Wei Hu",
        "summary": "Samples of brain signals collected by EEG sensors have inherent anti-correlations that are well modeled by negative edges in a finite graph. To differentiate epilepsy patients from healthy subjects using collected EEG signals, we build lightweight and interpretable transformer-like neural nets by unrolling a spectral denoising algorithm for signals on a balanced signed graph -- graph with no cycles of odd number of negative edges. A balanced signed graph has well-defined frequencies that map to a corresponding positive graph via similarity transform of the graph Laplacian matrices. We implement an ideal low-pass filter efficiently on the mapped positive graph via Lanczos approximation, where the optimal cutoff frequency is learned from data. Given that two balanced signed graph denoisers learn posterior probabilities of two different signal classes during training, we evaluate their reconstruction errors for binary classification of EEG signals. Experiments show that our method achieves classification performance comparable to representative deep learning schemes, while employing dramatically fewer parameters.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.572657"
    },
    {
        "index": "#26",
        "title": "Differentially Private Wasserstein Barycenters",
        "link": "/arxiv/2510.03021",
        "arxiv_id": "2510.03021",
        "authors": "Anming Gu, Sasidhar Kunapuli, Mark Bun, Edward Chien, Kristjan Greenewald",
        "summary": "The Wasserstein barycenter is defined as the mean of a set of probability measures under the optimal transport metric, and has numerous applications spanning machine learning, statistics, and computer graphics. In practice these input measures are empirical distributions built from sensitive datasets, motivating a differentially private (DP) treatment. We present, to our knowledge, the first algorithms for computing Wasserstein barycenters under differential privacy. Empirically, on synthetic data, MNIST, and large-scale U.S. population datasets, our methods produce high-quality private barycenters with strong accuracy-privacy tradeoffs.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.573125"
    },
    {
        "index": "#27",
        "title": "Learning Robust Diffusion Models from Imprecise Supervision",
        "link": "/arxiv/2510.03016",
        "arxiv_id": "2510.03016",
        "authors": "Dong-Dong Wu, Jiacheng Cui, Wei Wang, Zhiqiang She, Masashi Sugiyama",
        "summary": "Conditional diffusion models have achieved remarkable success in various generative tasks recently, but their training typically relies on large-scale datasets that inevitably contain imprecise information in conditional inputs. Such supervision, often stemming from noisy, ambiguous, or incomplete labels, will cause condition mismatch and degrade generation quality. To address this challenge, we propose DMIS, a unified framework for training robust Diffusion Models from Imprecise Supervision, which is the first systematic study within diffusion models. Our framework is derived from likelihood maximization and decomposes the objective into generative and classification components: the generative component models imprecise-label distributions, while the classification component leverages a diffusion classifier to infer class-posterior probabilities, with its efficiency further improved by an optimized timestep sampling strategy. Extensive experiments on diverse forms of imprecise supervision, covering tasks of image generation, weakly supervised learning, and noisy dataset condensation demonstrate that DMIS consistently produces high-quality and class-discriminative samples.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.573587"
    },
    {
        "index": "#28",
        "title": "Distributional Inverse Reinforcement Learning",
        "link": "/arxiv/2510.03013",
        "arxiv_id": "2510.03013",
        "authors": "Feiyang Wu, Ye Zhao, Anqi Wu",
        "summary": "We propose a distributional framework for offline Inverse Reinforcement Learning (IRL) that jointly models uncertainty over reward functions and full distributions of returns. Unlike conventional IRL approaches that recover a deterministic reward estimate or match only expected returns, our method captures richer structure in expert behavior, particularly in learning the reward distribution, by minimizing first-order stochastic dominance (FSD) violations and thus integrating distortion risk measures (DRMs) into policy learning, enabling the recovery of both reward distributions and distribution-aware policies. This formulation is well-suited for behavior analysis and risk-aware imitation learning. Empirical results on synthetic benchmarks, real-world neurobehavioral data, and MuJoCo control tasks demonstrate that our method recovers expressive reward representations and achieves state-of-the-art imitation performance.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.574001"
    },
    {
        "index": "#29",
        "title": "BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia",
        "link": "/arxiv/2510.03004",
        "arxiv_id": "2510.03004",
        "authors": "Tianzheng Hu, Qiang Li, Shu Liu, Vince D. Calhoun, Guido van Wingen, Shujian Yu",
        "summary": "The development of diagnostic models is gaining traction in the field of psychiatric disorders. Recently, machine learning classifiers based on resting-state functional magnetic resonance imaging (rs-fMRI) have been developed to identify brain biomarkers that differentiate psychiatric disorders from healthy controls. However, conventional machine learning-based diagnostic models often depend on extensive feature engineering, which introduces bias through manual intervention. While deep learning models are expected to operate without manual involvement, their lack of interpretability poses significant challenges in obtaining explainable and reliable brain biomarkers to support diagnostic decisions, ultimately limiting their clinical applicability. In this study, we introduce an end-to-end innovative graph neural network framework named BrainIB++, which applies the information bottleneck (IB) principle to identify the most informative data-driven brain regions as subgraphs during model training for interpretation. We evaluate the performance of our model against nine established brain network classification methods across three multi-cohort schizophrenia datasets. It consistently demonstrates superior diagnostic accuracy and exhibits generalizability to unseen data. Furthermore, the subgraphs identified by our model also correspond with established clinical biomarkers in schizophrenia, particularly emphasizing abnormalities in the visual, sensorimotor, and higher cognition brain functional network. This alignment enhances the model's interpretability and underscores its relevance for real-world diagnostic applications.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.574482"
    },
    {
        "index": "#30",
        "title": "From high-frequency sensors to noon reports: Using transfer learning for shaft power prediction in maritime",
        "link": "/arxiv/2510.03003",
        "arxiv_id": "2510.03003",
        "authors": "Akriti Sharma, Dogan Altan, Dusica Marijan, Arnbjørn Maressa",
        "summary": "With the growth of global maritime transportation, energy optimization has become crucial for reducing costs and ensuring operational efficiency. Shaft power is the mechanical power transmitted from the engine to the shaft and directly impacts fuel consumption, making its accurate prediction a paramount step in optimizing vessel performance. Power consumption is highly correlated with ship parameters such as speed and shaft rotation per minute, as well as weather and sea conditions. Frequent access to this operational data can improve prediction accuracy. However, obtaining high-quality sensor data is often infeasible and costly, making alternative sources such as noon reports a viable option. In this paper, we propose a transfer learning-based approach for predicting vessels shaft power, where a model is initially trained on high-frequency data from a vessel and then fine-tuned with low-frequency daily noon reports from other vessels. We tested our approach on sister vessels (identical dimensions and configurations), a similar vessel (slightly larger with a different engine), and a different vessel (distinct dimensions and configurations). The experiments showed that the mean absolute percentage error decreased by 10.6 percent for sister vessels, 3.6 percent for a similar vessel, and 5.3 percent for a different vessel, compared to the model trained solely on noon report data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.574932"
    },
    {
        "index": "#31",
        "title": "Confidence and Dispersity as Signals: Unsupervised Model Evaluation and Ranking",
        "link": "/arxiv/2510.02956",
        "arxiv_id": "2510.02956",
        "authors": "Weijian Deng, Weijie Tu, Ibrahim Radwan, Mohammad Abu Alsheikh, Stephen Gould, Liang Zheng",
        "summary": "Assessing model generalization under distribution shift is essential for real-world deployment, particularly when labeled test data is unavailable. This paper presents a unified and practical framework for unsupervised model evaluation and ranking in two common deployment settings: (1) estimating the accuracy of a fixed model on multiple unlabeled test sets (dataset-centric evaluation), and (2) ranking a set of candidate models on a single unlabeled test set (model-centric evaluation). We demonstrate that two intrinsic properties of model predictions, namely confidence (which reflects prediction certainty) and dispersity (which captures the diversity of predicted classes), together provide strong and complementary signals for generalization. We systematically benchmark a set of confidence-based, dispersity-based, and hybrid metrics across a wide range of model architectures, datasets, and distribution shift types. Our results show that hybrid metrics consistently outperform single-aspect metrics on both dataset-centric and model-centric evaluation settings. In particular, the nuclear norm of the prediction matrix provides robust and accurate performance across tasks, including real-world datasets, and maintains reliability under moderate class imbalance. These findings offer a practical and generalizable basis for unsupervised model assessment in deployment scenarios.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.575409"
    },
    {
        "index": "#32",
        "title": "ContextFlow: Context-Aware Flow Matching For Trajectory Inference From Spatial Omics Data",
        "link": "/arxiv/2510.02952",
        "arxiv_id": "2510.02952",
        "authors": "Santanu Subhash Rathod, Francesco Ceccarelli, Sean B. Holden, Pietro Liò, Xiao Zhang, Jovan Tanevski",
        "summary": "Inferring trajectories from longitudinal spatially-resolved omics data is fundamental to understanding the dynamics of structural and functional tissue changes in development, regeneration and repair, disease progression, and response to treatment. We propose ContextFlow, a novel context-aware flow matching framework that incorporates prior knowledge to guide the inference of structural tissue dynamics from spatially resolved omics data. Specifically, ContextFlow integrates local tissue organization and ligand-receptor communication patterns into a transition plausibility matrix that regularizes the optimal transport objective. By embedding these contextual constraints, ContextFlow generates trajectories that are not only statistically consistent but also biologically meaningful, making it a generalizable framework for modeling spatiotemporal dynamics from longitudinal, spatially resolved omics data. Evaluated on three datasets, ContextFlow consistently outperforms state-of-the-art flow matching methods across multiple quantitative and qualitative metrics of inference accuracy and biological coherence. Our code is available at: \\href{https://github.com/santanurathod/ContextFlow}{ContextFlow}",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.576066"
    },
    {
        "index": "#33",
        "title": "Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning",
        "link": "/arxiv/2510.02945",
        "arxiv_id": "2510.02945",
        "authors": "Juan Sebastian Rojas, Chi-Guhn Lee",
        "summary": "Continual reinforcement learning (continual RL) seeks to formalize the notions of lifelong learning and endless adaptation in RL. In particular, the aim of continual RL is to develop RL agents that can maintain a careful balance between retaining useful information and adapting to new situations. To date, continual RL has been explored almost exclusively through the lens of risk-neutral decision-making, in which the agent aims to optimize the expected (or mean) long-run performance. In this work, we present the first formal theoretical treatment of continual RL through the lens of risk-aware decision-making, in which the agent aims to optimize a reward-based measure of long-run performance beyond the mean. In particular, we show that the classical theory of risk measures, widely used as a theoretical foundation in non-continual risk-aware RL, is, in its current form, incompatible with the continual setting. Then, building on this insight, we extend risk measure theory into the continual setting by introducing a new class of ergodic risk measures that are compatible with continual learning. Finally, we provide a case study of risk-aware continual learning, along with empirical results, which show the intuitive appeal and theoretical soundness of ergodic risk measures.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.581658"
    },
    {
        "index": "#34",
        "title": "RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification",
        "link": "/arxiv/2510.02936",
        "arxiv_id": "2510.02936",
        "authors": "Aydin Javadov, Samir Garibov, Tobias Hoesli, Qiyang Sun, Florian von Wangenheim, Joseph Ollier, Björn W. Schuller",
        "summary": "Medical time series analysis is challenging due to data sparsity, noise, and highly variable recording lengths. Prior work has shown that stochastic sparse sampling effectively handles variable-length signals, while retrieval-augmented approaches improve explainability and robustness to noise and weak temporal correlations. In this study, we generalize the stochastic sparse sampling framework for retrieval-informed classification. Specifically, we weight window predictions by within-channel similarity and aggregate them in probability space, yielding convex series-level scores and an explicit evidence trail for explainability. Our method achieves competitive iEEG classification performance and provides practitioners with greater transparency and explainability. We evaluate our method in iEEG recordings collected in four medical centers, demonstrating its potential for reliable and explainable clinical variable-length time series classification.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.582133"
    },
    {
        "index": "#35",
        "title": "FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting",
        "link": "/arxiv/2510.02914",
        "arxiv_id": "2510.02914",
        "authors": "Tharuka Kasthuri Arachchige, Veselka Boeva, Shahrooz Abghari",
        "summary": "This work focuses on improving the performance and fairness of Federated Learning (FL) in non IID settings by enhancing model aggregation and boosting the training of underperforming clients. We propose FeDABoost, a novel FL framework that integrates a dynamic boosting mechanism and an adaptive gradient aggregation strategy. Inspired by the weighting mechanism of the Multiclass AdaBoost (SAMME) algorithm, our aggregation method assigns higher weights to clients with lower local error rates, thereby promoting more reliable contributions to the global model. In parallel, FeDABoost dynamically boosts underperforming clients by adjusting the focal loss focusing parameter, emphasizing hard to classify examples during local training. We have evaluated FeDABoost on three benchmark datasets MNIST, FEMNIST, and CIFAR10, and compared its performance with those of FedAvg and Ditto. The results show that FeDABoost achieves improved fairness and competitive performance.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.582571"
    },
    {
        "index": "#36",
        "title": "Learning Explicit Single-Cell Dynamics Using ODE Representations",
        "link": "/arxiv/2510.02903",
        "arxiv_id": "2510.02903",
        "authors": "Jan-Philipp von Bassewitz, Adeel Pervez, Marco Fumero, Matthew Robinson, Theofanis Karaletsos, Francesco Locatello",
        "summary": "Modeling the dynamics of cellular differentiation is fundamental to advancing the understanding and treatment of diseases associated with this process, such as cancer. With the rapid growth of single-cell datasets, this has also become a particularly promising and active domain for machine learning. Current state-of-the-art models, however, rely on computationally expensive optimal transport preprocessing and multi-stage training, while also not discovering explicit gene interactions. To address these challenges we propose Cell-Mechanistic Neural Networks (Cell-MNN), an encoder-decoder architecture whose latent representation is a locally linearized ODE governing the dynamics of cellular evolution from stem to tissue cells. Cell-MNN is fully end-to-end (besides a standard PCA pre-processing) and its ODE representation explicitly learns biologically consistent and interpretable gene interactions. Empirically, we show that Cell-MNN achieves competitive performance on single-cell benchmarks, surpasses state-of-the-art baselines in scaling to larger datasets and joint training across multiple datasets, while also learning interpretable gene interactions that we validate against the TRRUST database of gene interactions.",
        "subjects": "Machine Learning, Cell Behavior",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.583065"
    },
    {
        "index": "#37",
        "title": "DMark: Order-Agnostic Watermarking for Diffusion Large Language Models",
        "link": "/arxiv/2510.02902",
        "arxiv_id": "2510.02902",
        "authors": "Linyu Wu, Linhao Zhong, Wenjie Qu, Yuexin Li, Yue Liu, Shengfang Zhai, Chunhua Shen, Jiaheng Zhang",
        "summary": "Diffusion large language models (dLLMs) offer faster generation than autoregressive models while maintaining comparable quality, but existing watermarking methods fail on them due to their non-sequential decoding. Unlike autoregressive models that generate tokens left-to-right, dLLMs can finalize tokens in arbitrary order, breaking the causal design underlying traditional watermarks. We present DMark, the first watermarking framework designed specifically for dLLMs. DMark introduces three complementary strategies to restore watermark detectability: predictive watermarking uses model-predicted tokens when actual context is unavailable; bidirectional watermarking exploits both forward and backward dependencies unique to diffusion decoding; and predictive-bidirectional watermarking combines both approaches to maximize detection strength. Experiments across multiple dLLMs show that DMark achieves 92.0-99.5% detection rates at 1% false positive rate while maintaining text quality, compared to only 49.6-71.2% for naive adaptations of existing methods. DMark also demonstrates robustness against text manipulations, establishing that effective watermarking is feasible for non-autoregressive language models.",
        "subjects": "Machine Learning, Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.583589"
    },
    {
        "index": "#38",
        "title": "RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning",
        "link": "/arxiv/2510.02892",
        "arxiv_id": "2510.02892",
        "authors": "Aleksei Arzhantsev, Otmane Sakhi, Flavian Vasile",
        "summary": "Reinforcement learning (RL) is central to improving reasoning in large language models (LLMs) but typically requires ground-truth rewards. Test-Time Reinforcement Learning (TTRL) removes this need by using majority-vote rewards, but relies on heavy online RL and incurs substantial computational cost. We propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a family of lightweight offline learning alternatives that can target the same regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to maintain a reference model and instead optimizes weighted log-likelihood objectives, enabling stable training with significantly lower memory and compute requirements. Experimental results show that RoiRL trains to 2.5x faster and consistently outperforms TTRL on reasoning benchmarks, establishing a scalable path to self-improving LLMs without labels.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.584020"
    },
    {
        "index": "#39",
        "title": "Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics",
        "link": "/arxiv/2510.02839",
        "arxiv_id": "2510.02839",
        "authors": "Vijay Babu Pamshetti, Wei Zhang, Sumei Sun, Jie Zhang, Yonggang Wen, Qingyu Yan",
        "summary": "Battery health prognostics are critical for ensuring safety, efficiency, and sustainability in modern energy systems. However, it has been challenging to achieve accurate and robust prognostics due to complex battery degradation behaviors with nonlinearity, noise, capacity regeneration, etc. Existing data-driven models capture temporal degradation features but often lack knowledge guidance, which leads to unreliable long-term health prognostics. To overcome these limitations, we propose Karma, a knowledge-aware model with frequency-adaptive learning for battery capacity estimation and remaining useful life prediction. The model first performs signal decomposition to derive battery signals in different frequency bands. A dual-stream deep learning architecture is developed, where one stream captures long-term low-frequency degradation trends and the other models high-frequency short-term dynamics. Karma regulates the prognostics with knowledge, where battery degradation is modeled as a double exponential function based on empirical studies. Our dual-stream model is used to optimize the parameters of the knowledge with particle filters to ensure physically consistent and reliable prognostics and uncertainty quantification. Experimental study demonstrates Karma's superior performance, achieving average error reductions of 50.6% and 32.6% over state-of-the-art algorithms for battery health prediction on two mainstream datasets, respectively. These results highlight Karma's robustness, generalizability, and potential for safer and more reliable battery management across diverse applications.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.584495"
    },
    {
        "index": "#40",
        "title": "Subject-Adaptive Sparse Linear Models for Interpretable Personalized Health Prediction from Multimodal Lifelog Data",
        "link": "/arxiv/2510.02835",
        "arxiv_id": "2510.02835",
        "authors": "Dohyun Bu, Jisoo Han, Soohwa Kwon, Yulim So, Jong-Seok Lee",
        "summary": "Improved prediction of personalized health outcomes -- such as sleep quality and stress -- from multimodal lifelog data could have meaningful clinical and practical implications. However, state-of-the-art models, primarily deep neural networks and gradient-boosted ensembles, sacrifice interpretability and fail to adequately address the significant inter-individual variability inherent in lifelog data. To overcome these challenges, we propose the Subject-Adaptive Sparse Linear (SASL) framework, an interpretable modeling approach explicitly designed for personalized health prediction. SASL integrates ordinary least squares regression with subject-specific interactions, systematically distinguishing global from individual-level effects. We employ an iterative backward feature elimination method based on nested $F$-tests to construct a sparse and statistically robust model. Additionally, recognizing that health outcomes often represent discretized versions of continuous processes, we develop a regression-then-thresholding approach specifically designed to maximize macro-averaged F1 scores for ordinal targets. For intrinsically challenging predictions, SASL selectively incorporates outputs from compact LightGBM models through confidence-based gating, enhancing accuracy without compromising interpretability. Evaluations conducted on the CH-2025 dataset -- which comprises roughly 450 daily observations from ten subjects -- demonstrate that the hybrid SASL-LightGBM framework achieves predictive performance comparable to that of sophisticated black-box methods, but with significantly fewer parameters and substantially greater transparency, thus providing clear and actionable insights for clinicians and practitioners.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.584952"
    },
    {
        "index": "#41",
        "title": "Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise",
        "link": "/arxiv/2510.02826",
        "arxiv_id": "2510.02826",
        "authors": "Steve Hong, Samuel Belkadi",
        "summary": "We revisit Visual Autoregressive (VAR) models through the lens of an iterative-refinement framework. Rather than viewing VAR solely as next-scale autoregression, we formalise it as a deterministic forward process that constructs a Laplacian-style latent pyramid, paired with a learned backward process that reconstructs it in a small number of coarse-to-fine steps. This view connects VAR to denoising diffusion and isolates three design choices that help explain its efficiency and fidelity: refining in a learned latent space, casting prediction as discrete classification over code indices, and partitioning the task by spatial frequency. We run controlled experiments to quantify each factor's contribution to fidelity and speed, and we outline how the same framework extends to permutation-invariant graph generation and to probabilistic, ensemble-style medium-range weather forecasting. The framework also suggests practical interfaces for VAR to leverage tools from the diffusion ecosystem while retaining few-step, scale-parallel generation.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.585357"
    },
    {
        "index": "#42",
        "title": "The Curious Case of In-Training Compression of State Space Models",
        "link": "/arxiv/2510.02823",
        "arxiv_id": "2510.02823",
        "authors": "Makram Chahine, Philipp Nazari, Daniela Rus, T. Konstantin Rusch",
        "summary": "State Space Models (SSMs), developed to tackle long sequence modeling tasks efficiently, offer both parallelizable training and fast inference. At their core are recurrent dynamical systems that maintain a hidden state, with update costs scaling with the state dimension. A key design challenge is striking the right balance between maximizing expressivity and limiting this computational burden. Control theory, and more specifically Hankel singular value analysis, provides a potent framework for the measure of energy for each state, as well as the balanced truncation of the original system down to a smaller representation with performance guarantees. Leveraging the eigenvalue stability properties of Hankel matrices, we apply this lens to SSMs during training, where only dimensions of high influence are identified and preserved. Our approach applies to Linear Time-Invariant SSMs such as Linear Recurrent Units, but is also extendable to selective models. Experiments show that in-training reduction significantly accelerates optimization while preserving expressivity, with compressed models retaining task-critical structure lost by models trained directly at smaller dimension. In other words, SSMs that begin large and shrink during training achieve computational efficiency while maintaining higher performance.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.585784"
    },
    {
        "index": "#43",
        "title": "FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks",
        "link": "/arxiv/2510.02822",
        "arxiv_id": "2510.02822",
        "authors": "Jaemin Kim, Hongjun Um, Sungkyun Kim, Yongjun Park, Jiwon Seo",
        "summary": "Neural networks commonly execute on hardware accelerators such as NPUs and GPUs for their size and computation overhead. These accelerators are costly and it is hard to scale their resources to handle real-time workload fluctuations. We present FlexiQ, an adaptive mixed-precision quantization scheme for computer vision models. FlexiQ selectively applies low-bitwidth computation to feature channels with small value ranges and employs an efficient bit-lowering method to minimize quantization errors while maintaining inference accuracy. Furthermore, FlexiQ adjusts its low-bitwidth channel ratio in real time, enabling quantized models to effectively manage fluctuating inference workload. We implemented FlexiQ prototype, including the mixed-precision inference runtime on our custom NPU and GPUs. Evaluated on eleven convolution- and transformer-based vision models, FlexiQ achieves on average 6.6% higher accuracy for 4-bit models with finetuning and outperforms four state-of-the-art quantization techniques. Moreover, our mixed-precision models achieved an efficient accuracy-latency trade-off, with the 50% 4-bit model incurring only 0.6% accuracy loss while achieving 40% of the speedup of the 100% 4-bit model over 8-bit model. Latency evaluations on our NPU and GPUs confirmed that FlexiQ introduces minimal runtime overhead, demonstrating its hardware efficiency and overall performance benefits.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.586230"
    },
    {
        "index": "#44",
        "title": "Online Learning in the Random Order Model",
        "link": "/arxiv/2510.02820",
        "arxiv_id": "2510.02820",
        "authors": "Martino Bernasconi, Andrea Celli, Riccardo Colini-Baldeschi, Federico Fusco, Stefano Leonardi, Matteo Russo",
        "summary": "In the random-order model for online learning, the sequence of losses is chosen upfront by an adversary and presented to the learner after a random permutation. Any random-order input is \\emph{asymptotically} equivalent to a stochastic i.i.d. one, but, for finite times, it may exhibit significant {\\em non-stationarity}, which can hinder the performance of stochastic learning algorithms. While algorithms for adversarial inputs naturally maintain their regret guarantees in random order, simple no-regret algorithms exist for the stochastic model that fail against random-order instances. In this paper, we propose a general template to adapt stochastic learning algorithms to the random-order model without substantially affecting their regret guarantees. This allows us to recover improved regret bounds for prediction with delays, online learning with constraints, and bandits with switching costs. Finally, we investigate online classification and prove that, in random order, learnability is characterized by the VC dimension rather than the Littlestone dimension, thus providing a further separation from the general adversarial model.",
        "subjects": "Machine Learning, Data Structures and Algorithms",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.591854"
    },
    {
        "index": "#45",
        "title": "Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets",
        "link": "/arxiv/2510.02818",
        "arxiv_id": "2510.02818",
        "authors": "Sung Ho Jo, Seonghwi Kim, Minwoo Chae",
        "summary": "Conventional supervised learning methods are often vulnerable to spurious correlations, particularly under distribution shifts in test data. To address this issue, several approaches, most notably Group DRO, have been developed. While these methods are highly robust to subpopulation or group shifts, they remain vulnerable to intra-group distributional shifts, which frequently occur in minority groups with limited samples. We propose a hierarchical extension of Group DRO that addresses both inter-group and intra-group uncertainties, providing robustness to distribution shifts at multiple levels. We also introduce new benchmark settings that simulate realistic minority group distribution shifts-an important yet previously underexplored challenge in spurious correlation research. Our method demonstrates strong robustness under these conditions-where existing robust learning methods consistently fail-while also achieving superior performance on standard benchmarks. These results highlight the importance of broadening the ambiguity set to better capture both inter-group and intra-group distributional uncertainties.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.592276"
    },
    {
        "index": "#46",
        "title": "Dissecting Transformers: A CLEAR Perspective towards Green AI",
        "link": "/arxiv/2510.02810",
        "arxiv_id": "2510.02810",
        "authors": "Hemang Jain, Shailender Goyal, Divyansh Pandey, Karthik Vaidhyanathan",
        "summary": "The rapid adoption of Large Language Models (LLMs) has raised significant environmental concerns. Unlike the one-time cost of training, LLM inference occurs continuously at a global scale and now dominates the AI energy footprint. Yet, most sustainability studies report only coarse, model-level metrics due to the lack of fine-grained measurement methods, treating energy efficiency more as an afterthought than as a primary objective. We present the first fine-grained empirical analysis of inference energy across core components of transformer architecture. We propose a novel methodology, Component-Level Energy Assessment via Repeated sampling (CLEAR), to overcome temporal mismatch between microsecond scale component execution and monitoring of millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models spanning four distinct architecture types and consistently keep component-wise energy variance below 9.5\\% while capturing more than 90\\% of the model's total energy as individual components. Our empirical analysis reveals that Attention blocks consume significantly more energy per floating-point operation (FLOP), indicating that energy consumption is not proportionally aligned with FLOP counts. This shows that FLOPs alone fail to capture the true energy cost at a component level. Our findings establish detailed component-level energy baselines and provide insight as an initial step to build energy-efficient transformer models through component-level optimizations.",
        "subjects": "Machine Learning, Artificial Intelligence, Software Engineering",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.592748"
    },
    {
        "index": "#47",
        "title": "Relevance-Aware Thresholding in Online Conformal Prediction for Time Series",
        "link": "/arxiv/2510.02809",
        "arxiv_id": "2510.02809",
        "authors": "Théo Dupuy, Binbin Xu, Stéphane Perrey, Jacky Montmain, Abdelhak Imoussaten",
        "summary": "Uncertainty quantification has received considerable interest in recent works in Machine Learning. In particular, Conformal Prediction (CP) gains ground in this field. For the case of time series, Online Conformal Prediction (OCP) becomes an option to address the problem of data distribution shift over time. Indeed, the idea of OCP is to update a threshold of some quantity (whether the miscoverage level or the quantile) based on the distribution observation. To evaluate the performance of OCP methods, two key aspects are typically considered: the coverage validity and the prediction interval width minimization. Recently, new OCP methods have emerged, offering long-run coverage guarantees and producing more informative intervals. However, during the threshold update step, most of these methods focus solely on the validity of the prediction intervals~--~that is, whether the ground truth falls inside or outside the interval~--~without accounting for their relevance. In this paper, we aim to leverage this overlooked aspect. Specifically, we propose enhancing the threshold update step by replacing the binary evaluation (inside/outside) with a broader class of functions that quantify the relevance of the prediction interval using the ground truth. This approach helps prevent abrupt threshold changes, potentially resulting in narrower prediction intervals. Indeed, experimental results on real-world datasets suggest that these functions can produce tighter intervals compared to existing OCP methods while maintaining coverage validity.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.593212"
    },
    {
        "index": "#48",
        "title": "OptunaHub: A Platform for Black-Box Optimization",
        "link": "/arxiv/2510.02798",
        "arxiv_id": "2510.02798",
        "authors": "Yoshihiko Ozaki, Shuhei Watanabe, Toshihiko Yanase",
        "summary": "Black-box optimization (BBO) drives advances in domains such as AutoML and Materials Informatics, yet research efforts often remain fragmented across domains. We introduce OptunaHub (https://hub.optuna.org/), a community platform that centralizes BBO methods and benchmarks. OptunaHub provides unified Python APIs, a contributor package registry, and a web interface to promote searchability and cross-domain research. OptunaHub aims to foster a virtuous cycle of contributions and applications. The source code is publicly available in the optunahub, optunahub-registry, and optunahub-web repositories under the Optuna organization on GitHub (https://github.com/optuna/).",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.593647"
    },
    {
        "index": "#49",
        "title": "Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification",
        "link": "/arxiv/2510.02779",
        "arxiv_id": "2510.02779",
        "authors": "Yuanfan Li, Yunwen Lei, Zheng-Chu Guo, Yiming Ying",
        "summary": "Recent advances have significantly improved our understanding of the generalization performance of gradient descent (GD) methods in deep neural networks. A natural and fundamental question is whether GD can achieve generalization rates comparable to the minimax optimal rates established in the kernel setting. Existing results either yield suboptimal rates of $O(1/\\sqrt{n})$, or focus on networks with smooth activation functions, incurring exponential dependence on network depth $L$. In this work, we establish optimal generalization rates for GD with deep ReLU networks by carefully trading off optimization and generalization errors, achieving only polynomial dependence on depth. Specifically, under the assumption that the data are NTK separable from the margin $\\gamma$, we prove an excess risk rate of $\\widetilde{O}(L^4 (1 + \\gamma L^2) / (n \\gamma^2))$, which aligns with the optimal SVM-type rate $\\widetilde{O}(1 / (n \\gamma^2))$ up to depth-dependent factors. A key technical contribution is our novel control of activation patterns near a reference model, enabling a sharper Rademacher complexity bound for deep ReLU networks trained with gradient descent.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.594074"
    },
    {
        "index": "#51",
        "title": "Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse Plasticity",
        "link": "/arxiv/2510.02765",
        "arxiv_id": "2510.02765",
        "authors": "Hugo Ninou, Jonathan Kadmon, N. Alex Cayco-Gajic",
        "summary": "Gradient-based algorithms are a cornerstone of artificial neural network training, yet it remains unclear whether biological neural networks use similar gradient-based strategies during learning. Experiments often discover a diversity of synaptic plasticity rules, but whether these amount to an approximation to gradient descent is unclear. Here we investigate a previously overlooked possibility: that learning dynamics may include fundamentally non-gradient \"curl\"-like components while still being able to effectively optimize a loss function. Curl terms naturally emerge in networks with inhibitory-excitatory connectivity or Hebbian/anti-Hebbian plasticity, resulting in learning dynamics that cannot be framed as gradient descent on any objective. To investigate the impact of these curl terms, we analyze feedforward networks within an analytically tractable student-teacher framework, systematically introducing non-gradient dynamics through neurons exhibiting rule-flipped plasticity. Small curl terms preserve the stability of the original solution manifold, resulting in learning dynamics similar to gradient descent. Beyond a critical value, strong curl terms destabilize the solution manifold. Depending on the network architecture, this loss of stability can lead to chaotic learning dynamics that destroy performance. In other cases, the curl terms can counterintuitively speed learning compared to gradient descent by allowing the weight dynamics to escape saddles by temporarily ascending the loss. Our results identify specific architectures capable of supporting robust learning via diverse learning rules, providing an important counterpoint to normative theories of gradient-based learning in neural networks.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.594981"
    },
    {
        "index": "#52",
        "title": "Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning",
        "link": "/arxiv/2510.02763",
        "arxiv_id": "2510.02763",
        "authors": "Nicholas LaHaye, Kelly M. Luis, Michelle M. Gierach",
        "summary": "We present a self-supervised machine learning framework for detecting and mapping harmful algal bloom (HAB) severity and speciation using multi-sensor satellite data. By fusing reflectance data from operational instruments (VIIRS, MODIS, Sentinel-3, PACE) with TROPOMI solar-induced fluorescence (SIF), our framework, called SIT-FUSE, generates HAB severity and speciation products without requiring per-instrument labeled datasets. The framework employs self-supervised representation learning, hierarchical deep clustering to segment phytoplankton concentrations and speciations into interpretable classes, validated against in-situ data from the Gulf of Mexico and Southern California (2018-2025). Results show strong agreement with total phytoplankton, Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. measurements. This work advances scalable HAB monitoring in label-scarce environments while enabling exploratory analysis via hierarchical embeddings: a critical step toward operationalizing self-supervised learning for global aquatic biogeochemistry.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.595428"
    },
    {
        "index": "#53",
        "title": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling",
        "link": "/arxiv/2510.02758",
        "arxiv_id": "2510.02758",
        "authors": "Junyi Chen, Chuheng Du, Renyuan Liu, Shuochao Yao, Dingtian Yan, Jiang Liao, Shengzhong Liu, Fan Wu, Guihai Chen",
        "summary": "Real-time LLM interactions demand streamed token generations, where text tokens are progressively generated and delivered to users while balancing two objectives: responsiveness (i.e., low time-to-first-token) and steady generation (i.e.,required time-between-tokens). Standard LLM serving systems suffer from the inflexibility caused by non-preemptive request scheduling and reactive memory management, leading to poor resource utilization and low request processing parallelism under request bursts. Therefore, we present TokenFlow, a novel LLM serving system with enhanced text streaming performance via preemptive request scheduling and proactive key-value (KV) cache management. TokenFlow dynamically prioritizes requests based on real-time token buffer occupancy and token consumption rate, while actively transferring KV cache between GPU and CPU memory in the background and overlapping I/O with computation to minimize request preemption overhead. Extensive experiments on Llama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200) demonstrate that TokenFlow achieves up to 82.5% higher effective throughput (accounting for actual user consumption) while reducing P99 TTFT by up to 80.2%, without degrading overall token throughput.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.595943"
    },
    {
        "index": "#54",
        "title": "Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering",
        "link": "/arxiv/2510.02731",
        "arxiv_id": "2510.02731",
        "authors": "Tianxiang Zhao, Youqing Wang, Jinlu Wang, Jiapu Wang, Mingliang Cui, Junbin Gao, Jipeng Guo",
        "summary": "Due to its powerful capability of self-supervised representation learning and clustering, contrastive attributed graph clustering (CAGC) has achieved great success, which mainly depends on effective data augmentation and contrastive objective setting. However, most CAGC methods utilize edges as auxiliary information to obtain node-level embedding representation and only focus on node-level embedding augmentation. This approach overlooks edge-level embedding augmentation and the interactions between node-level and edge-level embedding augmentations across various granularity. Moreover, they often treat all contrastive sample pairs equally, neglecting the significant differences between hard and easy positive-negative sample pairs, which ultimately limits their discriminative capability. To tackle these issues, a novel robust attributed graph clustering (RAGC), incorporating hybrid-collaborative augmentation (HCA) and contrastive sample adaptive-differential awareness (CSADA), is proposed. First, node-level and edge-level embedding representations and augmentations are simultaneously executed to establish a more comprehensive similarity measurement criterion for subsequent contrastive learning. In turn, the discriminative similarity further consciously guides edge augmentation. Second, by leveraging pseudo-label information with high confidence, a CSADA strategy is elaborately designed, which adaptively identifies all contrastive sample pairs and differentially treats them by an innovative weight modulation function. The HCA and CSADA modules mutually reinforce each other in a beneficent cycle, thereby enhancing discriminability in representation learning. Comprehensive graph clustering evaluations over six benchmark datasets demonstrate the effectiveness of the proposed RAGC against several state-of-the-art CAGC methods.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.596405"
    },
    {
        "index": "#55",
        "title": "Dale meets Langevin: A Multiplicative Denoising Diffusion Model",
        "link": "/arxiv/2510.02730",
        "arxiv_id": "2510.02730",
        "authors": "Nishanth Shetty, Madhava Prasath, Chandra Sekhar Seelamantula",
        "summary": "Gradient descent has proven to be a powerful and effective technique for optimization in numerous machine learning applications. Recent advances in computational neuroscience have shown that learning in standard gradient descent optimization formulation is not consistent with learning in biological systems. This has opened up interesting avenues for building biologically inspired learning techniques. One such approach is inspired by Dale's law, which states that inhibitory and excitatory synapses do not swap roles during the course of learning. The resulting exponential gradient descent optimization scheme leads to log-normally distributed synaptic weights. Interestingly, the density that satisfies the Fokker-Planck equation corresponding to the stochastic differential equation (SDE) with geometric Brownian motion (GBM) is the log-normal density. Leveraging this connection, we start with the SDE governing geometric Brownian motion, and show that discretizing the corresponding reverse-time SDE yields a multiplicative update rule, which surprisingly, coincides with the sampling equivalent of the exponential gradient descent update founded on Dale's law. Furthermore, we propose a new formalism for multiplicative denoising score-matching, subsuming the loss function proposed by Hyvaerinen for non-negative data. Indeed, log-normally distributed data is positive and the proposed score-matching formalism turns out to be a natural fit. This allows for training of score-based models for image data and results in a novel multiplicative update scheme for sample generation starting from a log-normal density. Experimental results on MNIST, Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the new scheme. To the best of our knowledge, this is the first instance of a biologically inspired generative model employing multiplicative updates, founded on geometric Brownian motion.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.601957"
    },
    {
        "index": "#56",
        "title": "Accuracy Law for the Future of Deep Time Series Forecasting",
        "link": "/arxiv/2510.02729",
        "arxiv_id": "2510.02729",
        "authors": "Yuxuan Wang, Haixu Wu, Yuezhou Ma, Yuchen Fang, Ziyi Zhang, Yong Liu, Shiyu Wang, Zhou Ye, Yang Xiang, Jianmin Wang, Mingsheng Long",
        "summary": "Deep time series forecasting has emerged as a booming direction in recent years. Despite the exponential growth of community interests, researchers are sometimes confused about the direction of their efforts due to minor improvements on standard benchmarks. In this paper, we notice that, unlike image recognition, whose well-acknowledged and realizable goal is 100% accuracy, time series forecasting inherently faces a non-zero error lower bound due to its partially observable and uncertain nature. To pinpoint the research objective and release researchers from saturated tasks, this paper focuses on a fundamental question: how to estimate the performance upper bound of deep time series forecasting? Going beyond classical series-wise predictability metrics, e.g., ADF test, we realize that the forecasting performance is highly related to window-wise properties because of the sequence-to-sequence forecasting paradigm of deep time series models. Based on rigorous statistical tests of over 2,800 newly trained deep forecasters, we discover a significant exponential relationship between the minimum forecasting error of deep models and the complexity of window-wise series patterns, which is termed the accuracy law. The proposed accuracy law successfully guides us to identify saturated tasks from widely used benchmarks and derives an effective training strategy for large time series models, offering valuable insights for future research.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.602485"
    },
    {
        "index": "#58",
        "title": "CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks",
        "link": "/arxiv/2510.02717",
        "arxiv_id": "2510.02717",
        "authors": "Waqas Ishtiaq, Ashrafun Zannat, A. H. M. Shahariar Parvez, Md. Alamgir Hossain, Muntasir Hasan Kanchan, Muhammad Masud Tarek",
        "summary": "The rapid expansion of the Internet of Things (IoT) has revolutionized modern industries by enabling smart automation and real time connectivity. However, this evolution has also introduced complex cybersecurity challenges due to the heterogeneous, resource constrained, and distributed nature of these environments. To address these challenges, this research presents CST AFNet, a novel dual attention based deep learning framework specifically designed for robust intrusion detection in IoT networks. The model integrates multi scale Convolutional Neural Networks (CNNs) for spatial feature extraction, Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal dependencies, and a dual attention mechanism, channel and temporal attention, to enhance focus on critical patterns in the data. The proposed method was trained and evaluated on the Edge IIoTset dataset, a comprehensive and realistic benchmark containing more than 2.2 million labeled instances spanning 15 attack types and benign traffic, collected from a seven layer industrial testbed. Our proposed model achieves outstanding accuracy for both 15 attack types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover, this model demonstrates exceptional performance with macro averaged precision, recall, and F1 score all above 99.3 percent. Experimental results show that CST AFNet achieves superior detection accuracy, significantly outperforming traditional deep learning models. The findings confirm that CST AFNet is a powerful and scalable solution for real time cyber threat detection in complex IoT and IIoT environments, paving the way for more secure, intelligent, and adaptive cyber physical systems.",
        "subjects": "Machine Learning, Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.603427"
    },
    {
        "index": "#59",
        "title": "A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks",
        "link": "/arxiv/2510.02711",
        "arxiv_id": "2510.02711",
        "authors": "Tarun Kumar Biswas, Ashrafun Zannat, Waqas Ishtiaq, Md. Alamgir Hossain",
        "summary": "The growing integration of drones across commercial, industrial, and civilian domains has introduced significant cybersecurity challenges, particularly due to the susceptibility of drone networks to a wide range of cyberattacks. Existing intrusion detection mechanisms often lack the adaptability, efficiency, and generalizability required for the dynamic and resource constrained environments in which drones operate. This paper proposes TSLT-Net, a novel lightweight and unified Temporal Spatial Transformer based intrusion detection system tailored specifically for drone networks. By leveraging self attention mechanisms, TSLT-Net effectively models both temporal patterns and spatial dependencies in network traffic, enabling accurate detection of diverse intrusion types. The framework includes a streamlined preprocessing pipeline and supports both multiclass attack classification and binary anomaly detection within a single architecture. Extensive experiments conducted on the ISOT Drone Anomaly Detection Dataset, consisting of more than 2.3 million labeled records, demonstrate the superior performance of TSLT-Net with 99.99 percent accuracy in multiclass detection and 100 percent in binary anomaly detection, while maintaining a minimal memory footprint of only 0.04 MB and 9722 trainable parameters. These results establish TSLT-Net as an effective and scalable solution for real time drone cybersecurity, particularly suitable for deployment on edge devices in mission critical UAV systems.",
        "subjects": "Machine Learning, Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.603909"
    },
    {
        "index": "#60",
        "title": "RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization",
        "link": "/arxiv/2510.02695",
        "arxiv_id": "2510.02695",
        "authors": "Kai Fukazawa, Kunal Mundada, Iman Soltani",
        "summary": "In safety-critical domains where online data collection is infeasible, offline reinforcement learning (RL) offers an attractive alternative but only if policies deliver high returns without incurring catastrophic lower-tail risk. Prior work on risk-averse offline RL achieves safety at the cost of value conservatism and restricted policy classes, whereas expressive policies are only used in risk-neutral settings. Here, we address this gap by introducing the \\textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which couples an \\emph{expressive generative actor} with a distributional critic. The RAMAC differentiates composite objective combining distributional risk and BC loss through the generative path, achieving risk-sensitive learning in complex multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching actors and observe consistent gains in $\\mathrm{CVaR}_{0.1}$ while maintaining strong returns on most Stochastic-D4RL tasks. Code: https://github.com/KaiFukazawa/RAMAC.git",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.604338"
    },
    {
        "index": "#61",
        "title": "Fine-Tuning Diffusion Models via Intermediate Distribution Shaping",
        "link": "/arxiv/2510.02692",
        "arxiv_id": "2510.02692",
        "authors": "Gautham Govind Anil, Shaan Ul Haque, Nithish Kannen, Dheeraj Nagaraj, Sanjay Shakkottai, Karthikeyan Shanmugam",
        "summary": "Diffusion models are widely used for generative tasks across domains. While pre-trained diffusion models effectively capture the training data distribution, it is often desirable to shape these distributions using reward functions to align with downstream applications. Policy gradient methods, such as Proximal Policy Optimization (PPO), are widely used in the context of autoregressive generation. However, the marginal likelihoods required for such methods are intractable for diffusion models, leading to alternative proposals and relaxations. In this context, we unify variants of Rejection sAmpling based Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with reshaped rewards. We then introduce P-GRAFT to shape distributions at intermediate noise levels and demonstrate empirically that this can lead to more effective fine-tuning. We mathematically explain this via a bias-variance tradeoff. Motivated by this, we propose inverse noise correction to improve flow models without leveraging explicit rewards. We empirically evaluate our methods on text-to-image(T2I) generation, layout generation, molecule generation and unconditional image generation. Notably, our framework, applied to Stable Diffusion 2, improves over policy gradient methods on popular T2I benchmarks in terms of VQAScore and shows an $8.81\\%$ relative improvement over the base model. For unconditional image generation, inverse noise correction improves FID of generated images at lower FLOPs/image.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.604820"
    },
    {
        "index": "#62",
        "title": "EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics",
        "link": "/arxiv/2510.02686",
        "arxiv_id": "2510.02686",
        "authors": "Meng Xu, Jiao Liu, Yew Soon Ong",
        "summary": "Genetic programming (GP) has demonstrated strong effectiveness in evolving tree-structured heuristics for complex optimization problems. Yet, in dynamic and large-scale scenarios, the most effective heuristics are often highly complex, hindering interpretability, slowing convergence, and limiting transferability across tasks. To address these challenges, we present EvoSpeak, a novel framework that integrates GP with large language models (LLMs) to enhance the efficiency, transparency, and adaptability of heuristic evolution. EvoSpeak learns from high-quality GP heuristics, extracts knowledge, and leverages this knowledge to (i) generate warm-start populations that accelerate convergence, (ii) translate opaque GP trees into concise natural-language explanations that foster interpretability and trust, and (iii) enable knowledge transfer and preference-aware heuristic generation across related tasks. We verify the effectiveness of EvoSpeak through extensive experiments on dynamic flexible job shop scheduling (DFJSS), under both single- and multi-objective formulations. The results demonstrate that EvoSpeak produces more effective heuristics, improves evolutionary efficiency, and delivers human-readable reports that enhance usability. By coupling the symbolic reasoning power of GP with the interpretative and generative strengths of LLMs, EvoSpeak advances the development of intelligent, transparent, and user-aligned heuristics for real-world optimization problems.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.605231"
    },
    {
        "index": "#63",
        "title": "Can Data-Driven Dynamics Reveal Hidden Physics? There Is A Need for Interpretable Neural Operators",
        "link": "/arxiv/2510.02683",
        "arxiv_id": "2510.02683",
        "authors": "Wenhan Gao, Jian Luo, Fang Wan, Ruichen Xu, Xiang Liu, Haipeng Xing, Yi Liu",
        "summary": "Recently, neural operators have emerged as powerful tools for learning mappings between function spaces, enabling data-driven simulations of complex dynamics. Despite their successes, a deeper understanding of their learning mechanisms remains underexplored. In this work, we classify neural operators into two types: (1) Spatial domain models that learn on grids and (2) Functional domain models that learn with function bases. We present several viewpoints based on this classification and focus on learning data-driven dynamics adhering to physical principles. Specifically, we provide a way to explain the prediction-making process of neural operators and show that neural operator can learn hidden physical patterns from data. However, this explanation method is limited to specific situations, highlighting the urgent need for generalizable explanation methods. Next, we show that a simple dual-space multi-scale model can achieve SOTA performance and we believe that dual-space multi-spatio-scale models hold significant potential to learn complex physics and require further investigation. Lastly, we discuss the critical need for principled frameworks to incorporate known physics into neural operators, enabling better generalization and uncovering more hidden physical phenomena.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.605729"
    },
    {
        "index": "#64",
        "title": "To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration",
        "link": "/arxiv/2510.02676",
        "arxiv_id": "2510.02676",
        "authors": "Zeyu Yang, Tianyi Zhang, Jianwen Xie, Chuan Li, Zhaozhuo Xu, Anshumali Shrivastava",
        "summary": "The scaling of Generative AI (GenAI) models into the hundreds of billions of parameters makes low-precision computation indispensable for efficient deployment. We argue that the fundamental solution lies in developing low-precision floating-point formats, which inherently provide numerical stability, memory savings, and hardware efficiency without dequantization overhead. In this paper, we present a theoretical and empirical study of an exponent concentration phenomenon in GenAI weights: exponents consistently exhibit low entropy across architectures and modalities. We show that this arises naturally from $\\alpha$-stable distributions induced by stochastic gradient descent, and we prove tight bounds on the entropy of exponents. Our analysis establishes a theoretical compression limit near FP4.67, which motivates the design of a practical FP8 format. Building on these insights, we propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs up to 671B parameters demonstrate up to 26.9% memory savings and 177.1% throughput acceleration, with perfectly lossless computations, i.e., no deviation in model outputs. Our results establish exponent concentration as a statistical law of trained models and open a principled path for lossless low-precision floating-point design in the FP8 era.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.606202"
    },
    {
        "index": "#65",
        "title": "Topological Invariance and Breakdown in Learning",
        "link": "/arxiv/2510.02670",
        "arxiv_id": "2510.02670",
        "authors": "Yongyi Yang, Tomaso Poggio, Isaac Chuang, Liu Ziyin",
        "summary": "We prove that for a broad class of permutation-equivariant learning rules (including SGD, Adam, and others), the training process induces a bi-Lipschitz mapping between neurons and strongly constrains the topology of the neuron distribution during training. This result reveals a qualitative difference between small and large learning rates $\\eta$. With a learning rate below a topological critical point $\\eta^*$, the training is constrained to preserve all topological structure of the neurons. In contrast, above $\\eta^*$, the learning process allows for topological simplification, making the neuron manifold progressively coarser and thereby reducing the model's expressivity. Viewed in combination with the recent discovery of the edge of stability phenomenon, the learning dynamics of neuron networks under gradient descent can be divided into two phases: first they undergo smooth optimization under topological constraints, and then enter a second phase where they learn through drastic topological simplifications. A key feature of our theory is that it is independent of specific architectures or loss functions, enabling the universal application of topological methods to the study of deep learning.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.615978"
    },
    {
        "index": "#66",
        "title": "TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models",
        "link": "/arxiv/2510.02663",
        "arxiv_id": "2510.02663",
        "authors": "Rakshith S Srinivasa, Zora Che, Chen Bo Calvin Zhang, Diego Mares, Ernesto Hernandez, Jayeon Park, Dean Lee, Guillermo Mangialardi, Charmaine Ng, Ed-Yeremai Hernandez Cardona, Anisha Gunjal, Yunzhong He, Bing Liu, Chen Xing",
        "summary": "As students increasingly adopt large language models (LLMs) as learning aids, it is crucial to build models that are adept at handling the nuances of tutoring: they need to identify the core needs of students, be adaptive, provide personalized guidance, and be accurate. To this end, we introduce TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated by human experts, focused on high-school and AP-level curricula. The samples are drawn from three common tutoring tasks: (i) generating adaptive explanations tailored to a student's confusion, (ii) providing actionable feedback on a student's work, and (iii) promoting active learning through effective hint generation. To account for the inherent complexity of tutoring, samples are accompanied by sample-specific rubrics which are used to judge model responses during evaluation. TutorBench uses a reliable and fine-grained automatic evaluation method that uses an LLM-judge and the sample-specific rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed analysis of their performance and behavior. Our results show that none of the frontier LLMs achieve a score of greater than $56\\%$, showing a large room for improvement. We find that LLMs fall short in exhibiting the full range of tutoring skills needed to guide, diagnose, and support students effectively, with all the frontier models achieving less than a $60\\%$ pass rate on rubric criteria related to these skills. We also find that different model families exhibit varied strengths and limitations: the Claude models outperform others in supporting active learning, while they lag behind in the other two use cases. By releasing TutorBench, we provide a comprehensive and unsaturated benchmark to guide the development of the next-generation of AI tutors.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.616833"
    },
    {
        "index": "#67",
        "title": "Optimal Characteristics of Inspection Vehicle for Drive-by Bridge Inspection",
        "link": "/arxiv/2510.02658",
        "arxiv_id": "2510.02658",
        "authors": "A. Calderon Hurtado, E. Atroshchenko, K. C. Chang, C. W. Kim, M. Makki Alamdari",
        "summary": "Drive-by inspection for bridge health monitoring has gained increasing attention over the past decade. This method involves analysing the coupled vehicle-bridge response, recorded by an instrumented inspection vehicle, to assess structural integrity and detect damage. However, the vehicles mechanical and dynamic properties significantly influence detection performance, limiting the effectiveness of the approach. This study presents a framework for optimising the inspection vehicle to enhance damage sensitivity. An unsupervised deep learning methodbased on adversarial autoencoders (AAE)is used to reconstruct the frequency- domain representation of acceleration responses. The mass and stiffness of the tyre suspension system of a two-axle vehicle are optimised by minimising the Wasserstein distance between damage index distributions for healthy and damaged bridge states. A Kriging meta-model is employed to approximate this objective function efficiently and identify optimal vehicle configurations in both dimensional and non-dimensional parameter spaces. Results show that vehicles with frequency ratios between 0.3 and 0.7 relative to the bridges' first natural frequency are most effective, while those near resonance perform poorly. Lighter vehicles require lower natural frequencies for optimal detection. This is the first study to rigorously optimise the sensing platform for drive-by sensing and to propose a purpose-built inspection vehicle.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.617447"
    },
    {
        "index": "#69",
        "title": "TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer",
        "link": "/arxiv/2510.02625",
        "arxiv_id": "2510.02625",
        "authors": "Jacob Feitelberg, Dwaipayan Saha, Kyuseong Choi, Zaid Ahmad, Anish Agarwal, Raaz Dwivedi",
        "summary": "Missing data is a pervasive problem in tabular settings. Existing solutions range from simple averaging to complex generative adversarial networks. However, due to huge variance in performance across real-world domains and time-consuming hyperparameter tuning, no default imputation method exists. Building on TabPFN, a recent tabular foundation model for supervised learning, we propose TabImpute, a pre-trained transformer that delivers accurate and fast zero-shot imputations requiring no fitting or hyperparameter tuning at inference-time. To train and evaluate TabImpute, we introduce (i) an entry-wise featurization for tabular settings, which enables a $100\\times$ speedup over the previous TabPFN imputation method, (ii) a synthetic training data generation pipeline incorporating realistic missingness patterns, which boosts test-time performance, and (iii) MissBench, a comprehensive benchmark for evaluation of imputation methods with $42$ OpenML datasets and $13$ missingness patterns. MissBench spans domains such as medicine, finance, and engineering, showcasing TabImpute's robust performance compared to $11$ established imputation methods.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.618401"
    },
    {
        "index": "#70",
        "title": "MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection",
        "link": "/arxiv/2510.02610",
        "arxiv_id": "2510.02610",
        "authors": "Taurai Muvunzaa, Egor Kraev, Pere Planell-Morell, Alexander Y. Shestopaloff",
        "summary": "Existing feature filters rely on statistical pair-wise dependence metrics to model feature-target relationships, but this approach may fail when the target depends on higher-order feature interactions rather than individual contributions. We introduce Mutual Information Neural Estimation Regularized Vetting Algorithm (MINERVA), a novel approach to supervised feature selection based on neural estimation of mutual information between features and targets. We paramaterize the approximation of mutual information with neural networks and perform feature selection using a carefully designed loss function augmented with sparsity-inducing regularizers. Our method is implemented in a two-stage process to decouple representation learning from feature selection, ensuring better generalization and a more accurate expression of feature importance. We present examples of ubiquitous dependency structures that are rarely captured in literature and show that our proposed method effectively captures these complex feature-target relationships by evaluating feature subsets as an ensemble. Experimental results on synthetic and real-life fraud datasets demonstrate the efficacy of our method and its ability to perform exact solutions.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.618837"
    },
    {
        "index": "#71",
        "title": "Towards CONUS-Wide ML-Augmented Conceptually-Interpretable Modeling of Catchment-Scale Precipitation-Storage-Runoff Dynamics",
        "link": "/arxiv/2510.02605",
        "arxiv_id": "2510.02605",
        "authors": "Yuan-Heng Wang, Yang Yang, Fabio Ciulla, Hoshin V. Gupta, Charuleka Varadharajan",
        "summary": "While many modern studies are dedicated to ML-based large-sample hydrologic modeling, these efforts have not necessarily translated into predictive improvements that are grounded in enhanced physical-conceptual understanding. Here, we report on a CONUS-wide large-sample study (spanning diverse hydro-geo-climatic conditions) using ML-augmented physically-interpretable catchment-scale models of varying complexity based in the Mass-Conserving Perceptron (MCP). Results were evaluated using attribute masks such as snow regime, forest cover, and climate zone. Our results indicate the importance of selecting model architectures of appropriate model complexity based on how process dominance varies with hydrological regime. Benchmark comparisons show that physically-interpretable mass-conserving MCP-based models can achieve performance comparable to data-based models based in the Long Short-Term Memory network (LSTM) architecture. Overall, this study highlights the potential of a theory-informed, physically grounded approach to large-sample hydrology, with emphasis on mechanistic understanding and the development of parsimonious and interpretable model architectures, thereby laying the foundation for future models of everywhere that architecturally encode information about spatially- and temporally-varying process dominance.",
        "subjects": "Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.619268"
    },
    {
        "index": "#72",
        "title": "Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning",
        "link": "/arxiv/2510.02590",
        "arxiv_id": "2510.02590",
        "authors": "Ahmed Hendawy, Henrik Metternich, Théo Vincent, Mahdi Kallel, Jan Peters, Carlo D'Eramo",
        "summary": "The use of target networks is a popular approach for estimating value functions in deep Reinforcement Learning (RL). While effective, the target network remains a compromise solution that preserves stability at the cost of slowly moving targets, thus delaying learning. Conversely, using the online network as a bootstrapped target is intuitively appealing, albeit well-known to lead to unstable learning. In this work, we aim to obtain the best out of both worlds by introducing a novel update rule that computes the target using the MINimum estimate between the Target and Online network, giving rise to our method, MINTO. Through this simple, yet effective modification, we show that MINTO enables faster and stable value function learning, by mitigating the potential overestimation bias of using the online network for bootstrapping. Notably, MINTO can be seamlessly integrated into a wide range of value-based and actor-critic algorithms with a negligible cost. We evaluate MINTO extensively across diverse benchmarks, spanning online and offline RL, as well as discrete and continuous action spaces. Across all benchmarks, MINTO consistently improves performance, demonstrating its broad applicability and effectiveness.",
        "subjects": "Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.619755"
    },
    {
        "index": "#73",
        "title": "Geospatial Machine Learning Libraries",
        "link": "/arxiv/2510.02572",
        "arxiv_id": "2510.02572",
        "authors": "Adam J. Stewart, Caleb Robinson, Arindam Banerjee",
        "summary": "Recent advances in machine learning have been supported by the emergence of domain-specific software libraries, enabling streamlined workflows and increased reproducibility. For geospatial machine learning (GeoML), the availability of Earth observation data has outpaced the development of domain libraries to handle its unique challenges, such as varying spatial resolutions, spectral properties, temporal cadence, data coverage, coordinate systems, and file formats. This chapter presents a comprehensive overview of GeoML libraries, analyzing their evolution, core functionalities, and the current ecosystem. It also introduces popular GeoML libraries such as TorchGeo, eo-learn, and Raster Vision, detailing their architecture, supported data types, and integration with ML frameworks. Additionally, it discusses common methodologies for data preprocessing, spatial--temporal joins, benchmarking, and the use of pretrained models. Through a case study in crop type mapping, it demonstrates practical applications of these tools. Best practices in software design, licensing, and testing are highlighted, along with open challenges and future directions, particularly the rise of foundation models and the need for governance in open-source geospatial software. Our aim is to guide practitioners, developers, and researchers in navigating and contributing to the rapidly evolving GeoML landscape.",
        "subjects": "Machine Learning, Software Engineering",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.620179"
    },
    {
        "index": "#74",
        "title": "On The Expressive Power of GNN Derivatives",
        "link": "/arxiv/2510.02565",
        "arxiv_id": "2510.02565",
        "authors": "Yam Eitan, Moshe Eliasof, Yoav Gelberg, Fabrizio Frasca, Guy Bar-Shalom, Haggai Maron",
        "summary": "Despite significant advances in Graph Neural Networks (GNNs), their limited expressivity remains a fundamental challenge. Research on GNN expressivity has produced many expressive architectures, leading to architecture hierarchies with models of increasing expressive power. Separately, derivatives of GNNs with respect to node features have been widely studied in the context of the oversquashing and over-smoothing phenomena, GNN explainability, and more. To date, these derivatives remain unexplored as a means to enhance GNN expressivity. In this paper, we show that these derivatives provide a natural way to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN (HOD-GNN), a novel method that enhances the expressivity of Message Passing Neural Networks (MPNNs) by leveraging high-order node derivatives of the base model. These derivatives generate expressive structure-aware node embeddings processed by a second GNN in an end-to-end trainable architecture. Theoretically, we show that the resulting architecture family's expressive power aligns with the WL hierarchy. We also draw deep connections between HOD-GNN, Subgraph GNNs, and popular structural encoding schemes. For computational efficiency, we develop a message-passing algorithm for computing high-order derivatives of MPNNs that exploits graph sparsity and parallelism. Evaluations on popular graph learning benchmarks demonstrate HOD-GNN's strong performance on popular graph learning tasks.",
        "subjects": "Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.620620"
    },
    {
        "index": "#75",
        "title": "AttentiveGRUAE: An Attention-Based GRU Autoencoder for Temporal Clustering and Behavioral Characterization of Depression from Wearable Data",
        "link": "/arxiv/2510.02558",
        "arxiv_id": "2510.02558",
        "authors": "Nidhi Soley, Vishal M Patel, Casey O Taylor",
        "summary": "In this study, we present AttentiveGRUAE, a novel attention-based gated recurrent unit (GRU) autoencoder designed for temporal clustering and prediction of outcome from longitudinal wearable data. Our model jointly optimizes three objectives: (1) learning a compact latent representation of daily behavioral features via sequence reconstruction, (2) predicting end-of-period depression rate through a binary classification head, and (3) identifying behavioral subtypes through Gaussian Mixture Model (GMM) based soft clustering of learned embeddings. We evaluate AttentiveGRUAE on longitudinal sleep data from 372 participants (GLOBEM 2018-2019), and it demonstrates superior performance over baseline clustering, domain-aligned self-supervised, and ablated models in both clustering quality (silhouette score = 0.70 vs 0.32-0.70) and depression classification (AUC = 0.74 vs 0.50-0.67). Additionally, external validation on cross-year cohorts from 332 participants (GLOBEM 2020-2021) confirms cluster reproducibility (silhouette score = 0.63, AUC = 0.61) and stability. We further perform subtype analysis and visualize temporal attention, which highlights sleep-related differences between clusters and identifies salient time windows that align with changes in sleep regularity, yielding clinically interpretable explanations of risk.",
        "subjects": "Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.626163"
    },
    {
        "index": "#76",
        "title": "Model-brain comparison using inter-animal transforms",
        "link": "/arxiv/2510.02523",
        "arxiv_id": "2510.02523",
        "authors": "Imran Thobani, Javier Sagastuy-Brena, Aran Nayebi, Jacob Prince, Rosa Cao, Daniel Yamins",
        "summary": "Artificial neural network models have emerged as promising mechanistic models of the brain. However, there is little consensus on the correct method for comparing model activations to brain responses. Drawing on recent work in philosophy of neuroscience, we propose a comparison methodology based on the Inter-Animal Transform Class (IATC) - the strictest set of functions needed to accurately map neural responses between subjects in an animal population. Using the IATC, we can map bidirectionally between a candidate model's responses and brain data, assessing how well the model can masquerade as a typical subject using the same kinds of transforms needed to map across real subjects. We identify the IATC in three settings: a simulated population of neural network models, a population of mouse subjects, and a population of human subjects. We find that the IATC resolves detailed aspects of the neural mechanism, such as the non-linear activation function. Most importantly, we find that the IATC enables accurate predictions of neural activity while also achieving high specificity in mechanism identification, evidenced by its ability to separate response patterns from different brain areas while strongly aligning same-brain-area responses between subjects. In other words, the IATC is a proof-by-existence that there is no inherent tradeoff between the neural engineering goal of high model-brain predictivity and the neuroscientific goal of identifying mechanistically accurate brain models. Using IATC-guided transforms, we obtain new evidence in favor of topographical deep neural networks (TDANNs) as models of the visual system. Overall, the IATC enables principled model-brain comparisons, contextualizing previous findings about the predictive success of deep learning models of the brain, while improving upon previous approaches to model-brain comparison.",
        "subjects": "Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.626832"
    },
    {
        "index": "#77",
        "title": "Graph Generation with Spectral Geodesic Flow Matching",
        "link": "/arxiv/2510.02520",
        "arxiv_id": "2510.02520",
        "authors": "Xikun Huang, Tianyu Ruan, Chihao Zhang, Shihua Zhang",
        "summary": "Graph generation is a fundamental task with wide applications in modeling complex systems. Although existing methods align the spectrum or degree profile of the target graph, they often ignore the geometry induced by eigenvectors and the global structure of the graph. In this work, we propose Spectral Geodesic Flow Matching (SFMG), a novel framework that uses spectral eigenmaps to embed both input and target graphs into continuous Riemannian manifolds. We then define geodesic flows between embeddings and match distributions along these flows to generate output graphs. Our method yields several advantages: (i) captures geometric structure beyond eigenvalues, (ii) supports flexible generation of diverse graphs, and (iii) scales efficiently. Empirically, SFMG matches the performance of state-of-the-art approaches on graphlet, degree, and spectral metrics across diverse benchmarks. In particular, it achieves up to 30$\\times$ speedup over diffusion-based models, offering a substantial advantage in scalability and training efficiency. We also demonstrate its ability to generalize to unseen graph scales. Overall, SFMG provides a new approach to graph synthesis by integrating spectral geometry with flow matching.",
        "subjects": "Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.627394"
    },
    {
        "index": "#78",
        "title": "In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning",
        "link": "/arxiv/2510.02516",
        "arxiv_id": "2510.02516",
        "authors": "Jindan Li, Zhaoxian Wu, Gaowen Liu, Tayfun Gokmen, Tianyi Chen",
        "summary": "Analog in-memory computing (AIMC) accelerators enable efficient deep neural network computation directly within memory using resistive crossbar arrays, where model parameters are represented by the conductance states of memristive devices. However, effective in-memory training typically requires at least 8-bit conductance states to match digital baselines. Realizing such fine-grained states is costly and often requires complex noise mitigation techniques that increase circuit complexity and energy consumption. In practice, many promising memristive devices such as ReRAM offer only about 4-bit resolution due to fabrication constraints, and this limited update precision substantially degrades training accuracy. To enable on-chip training with these limited-state devices, this paper proposes a \\emph{residual learning} framework that sequentially learns on multiple crossbar tiles to compensate the residual errors from low-precision weight updates. Our theoretical analysis shows that the optimality gap shrinks with the number of tiles and achieves a linear convergence rate. Experiments on standard image classification benchmarks demonstrate that our method consistently outperforms state-of-the-art in-memory analog training strategies under limited-state settings, while incurring only moderate hardware overhead as confirmed by our cost analysis.",
        "subjects": "Machine Learning, Hardware Architecture, Optimization and Control",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.627947"
    },
    {
        "index": "#80",
        "title": "Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking",
        "link": "/arxiv/2510.02490",
        "arxiv_id": "2510.02490",
        "authors": "Shaifalee Saxena, Alan Williams, Rafael Fierro, Alexander Scheinker",
        "summary": "In this paper, we study the use of robust model independent bounded extremum seeking (ES) feedback control to improve the robustness of deep reinforcement learning (DRL) controllers for a class of nonlinear time-varying systems. DRL has the potential to learn from large datasets to quickly control or optimize the outputs of many-parameter systems, but its performance degrades catastrophically when the system model changes rapidly over time. Bounded ES can handle time-varying systems with unknown control directions, but its convergence speed slows down as the number of tuned parameters increases and, like all local adaptive methods, it can get stuck in local minima. We demonstrate that together, DRL and bounded ES result in a hybrid controller whose performance exceeds the sum of its parts with DRL taking advantage of historical data to learn how to quickly control a many-parameter system to a desired setpoint while bounded ES ensures its robustness to time variations. We present a numerical study of a general time-varying system and a combined ES-DRL controller for automatic tuning of the Low Energy Beam Transport section at the Los Alamos Neutron Science Center linear particle accelerator.",
        "subjects": "Machine Learning, Systems and Control",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.628922"
    },
    {
        "index": "#81",
        "title": "From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning",
        "link": "/arxiv/2510.02484",
        "arxiv_id": "2510.02484",
        "authors": "Rafael Rodriguez-Sanchez, Cameron Allen, George Konidaris",
        "summary": "Algorithms that exploit factored Markov decision processes are far more sample-efficient than factor-agnostic methods, yet they assume a factored representation is known a priori -- a requirement that breaks down when the agent sees only high-dimensional observations. Conversely, deep reinforcement learning handles such inputs but cannot benefit from factored structure. We address this representation problem with Action-Controllable Factorization (ACF), a contrastive learning approach that uncovers independently controllable latent variables -- state components each action can influence separately. ACF leverages sparsity: actions typically affect only a subset of variables, while the rest evolve under the environment's dynamics, yielding informative data for contrastive training. ACF recovers the ground truth controllable factors directly from pixel observations on three benchmarks with known factored structure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistently outperforming baseline disentanglement algorithms.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.629391"
    },
    {
        "index": "#83",
        "title": "Uncertainty-Guided Model Selection for Tabular Foundation Models in Biomolecule Efficacy Prediction",
        "link": "/arxiv/2510.02476",
        "arxiv_id": "2510.02476",
        "authors": "Jie Li, Andrew McCarthy, Zhizhuo Zhang, Stephen Young",
        "summary": "In-context learners like TabPFN are promising for biomolecule efficacy prediction, where established molecular feature sets and relevant experimental results can serve as powerful contextual examples. However, their performance is highly sensitive to the provided context, making strategies like post-hoc ensembling of models trained on different data subsets a viable approach. An open question is how to select the best models for the ensemble without access to ground truth labels. In this study, we investigate an uncertainty-guided strategy for model selection. We demonstrate on an siRNA knockdown efficacy task that a TabPFN model using simple sequence-based features can surpass specialized state-of-the-art predictors. We also show that the model's predicted inter-quantile range (IQR), a measure of its uncertainty, has a negative correlation with true prediction error. By selecting and averaging an ensemble of models with the lowest mean IQR, we achieve superior performance compared to naive ensembling or using a single model trained on all available data. This finding highlights model uncertainty as a powerful, label-free heuristic for optimizing biomolecule efficacy predictions.",
        "subjects": "Machine Learning, Quantitative Methods",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.630255"
    },
    {
        "index": "#84",
        "title": "SAGE: Streaming Agreement-Driven Gradient Sketches for Representative Subset Selection",
        "link": "/arxiv/2510.02470",
        "arxiv_id": "2510.02470",
        "authors": "Ashish Jha, Salman Ahmadi-Asl",
        "summary": "Training modern neural networks on large datasets is computationally and energy intensive. We present SAGE, a streaming data-subset selection method that maintains a compact Frequent Directions (FD) sketch of gradient geometry in $O(\\ell D)$ memory and prioritizes examples whose sketched gradients align with a consensus direction. The approach eliminates $N \\times N$ pairwise similarities and explicit $N \\times \\ell$ gradient stores, yielding a simple two-pass, GPU-friendly pipeline. Leveraging FD's deterministic approximation guarantees, we analyze how agreement scoring preserves gradient energy within the principal sketched subspace. Across multiple benchmarks, SAGE trains with small kept-rate budgets while retaining competitive accuracy relative to full-data training and recent subset-selection baselines, and reduces end-to-end compute and peak memory. Overall, SAGE offers a practical, constant-memory alternative that complements pruning and model compression for efficient training.",
        "subjects": "Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.630653"
    },
    {
        "index": "#85",
        "title": "Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization",
        "link": "/arxiv/2510.02457",
        "arxiv_id": "2510.02457",
        "authors": "Logan Frank, Paul Ardis",
        "summary": "Post-training quantization (PTQ) has recently emerged as an effective tool for reducing the computational complexity and memory usage of a neural network by representing its weights and activations with lower precision. While this paradigm has shown great success in lowering compute and storage costs, there is the potential for drastic performance reduction depending upon the distribution of inputs experienced in inference. When considering possible deployment in safety-critical environments, it is important to investigate the extent of potential performance reduction, and what characteristics of input distributions may give rise to this reduction. In this work, we explore the idea of extreme failure stemming from dynamic PTQ and formulate a knowledge distillation and reinforcement learning task to learn a network and bit-width policy pair such that catastrophic failure under quantization is analyzed in terms of worst case potential. Our results confirm the existence of this \"detrimental\" network-policy pair, with several instances demonstrating performance reductions in the range of 10-65% in accuracy, compared to their \"robust\" counterparts encountering a <2% decrease. From systematic experimentation and analyses, we also provide an initial exploration into points at highest vulnerability. While our results represent an initial step toward understanding failure cases introduced by PTQ, our findings ultimately emphasize the need for caution in real-world deployment scenarios. We hope this work encourages more rigorous examinations of robustness and a greater emphasis on safety considerations for future works within the broader field of deep learning.",
        "subjects": "Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.631058"
    },
    {
        "index": "#86",
        "title": "Market-Based Data Subset Selection -- Principled Aggregation of Multi-Criteria Example Utility",
        "link": "/arxiv/2510.02456",
        "arxiv_id": "2510.02456",
        "authors": "Ashish Jha, Valentin Leplat, AH Phan",
        "summary": "Selecting a small yet useful subset of training data is hard because signals of example utility (uncertainty, rarity, diversity, etc.) are heterogeneous and typically combined with ad hoc weights. We propose a market-based selector that prices each example via a cost-function prediction market (LMSR), signals act as traders, a single liquidity parameter controls concentration, and topic-wise normalization stabilizes calibration. Token budgets are handled explicitly by a price-per-token rule $\\rho=p/\\ell^{\\gamma}$, with $\\gamma$ exposing an interpretable length bias; a lightweight diversity head improves coverage. We quantify coverage via topic cluster coverage and effective sample size. On the theory side, we show that LMSR implements a maximum-entropy aggregation with exponential weighting and a convex objective, yielding transparent knobs for aggregation strength. Empirically, on GSM8K (60k-token budget) the market with diversity achieves parity with strong single-signal baselines while reducing seed variance and incurring $<\\!0.1$ GPU-hr selection overhead; on AGNews at kept=5-25\\% the market (with light balancing) delivers competitive accuracy with improved balance and stability. The framework unifies multi-signal data curation under fixed compute for prompt-level reasoning and classification.",
        "subjects": "Machine Learning, Artificial Intelligence, Numerical Analysis",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.636730"
    },
    {
        "index": "#88",
        "title": "RainSeer: Fine-Grained Rainfall Reconstruction via Physics-Guided Modeling",
        "link": "/arxiv/2510.02414",
        "arxiv_id": "2510.02414",
        "authors": "Lin Chen, Jun Chen, Minghui Qiu, Shuxin Zhong, Binghong Chen, Kaishun Wu",
        "summary": "Reconstructing high-resolution rainfall fields is essential for flood forecasting, hydrological modeling, and climate analysis. However, existing spatial interpolation methods-whether based on automatic weather station (AWS) measurements or enhanced with satellite/radar observations often over-smooth critical structures, failing to capture sharp transitions and localized extremes. We introduce RainSeer, a structure-aware reconstruction framework that reinterprets radar reflectivity as a physically grounded structural prior-capturing when, where, and how rain develops. This shift, however, introduces two fundamental challenges: (i) translating high-resolution volumetric radar fields into sparse point-wise rainfall observations, and (ii) bridging the physical disconnect between aloft hydro-meteors and ground-level precipitation. RainSeer addresses these through a physics-informed two-stage architecture: a Structure-to-Point Mapper performs spatial alignment by projecting mesoscale radar structures into localized ground-level rainfall, through a bidirectional mapping, and a Geo-Aware Rain Decoder captures the semantic transformation of hydro-meteors through descent, melting, and evaporation via a causal spatiotemporal attention mechanism. We evaluate RainSeer on two public datasets-RAIN-F (Korea, 2017-2019) and MeteoNet (France, 2016-2018)-and observe consistent improvements over state-of-the-art baselines, reducing MAE by over 13.31% and significantly enhancing structural fidelity in reconstructed rainfall fields.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.637724"
    },
    {
        "index": "#89",
        "title": "OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data",
        "link": "/arxiv/2510.02410",
        "arxiv_id": "2510.02410",
        "authors": "Patrick Langer, Thomas Kaar, Max Rosenblattl, Maxwell A. Xu, Winnie Chow, Martin Maritsch, Aradhana Verma, Brian Han, Daniel Seung Kim, Henry Chubb, Scott Ceresnak, Aydin Zahedivash, Alexander Tarlochan Singh Sandhu, Fatima Rodriguez, Daniel McDuff, Elgar Fleisch, Oliver Aalami, Filipe Barata, Paul Schmiedmayer",
        "summary": "LLMs have emerged as powerful tools for interpreting multimodal data. In medicine, they hold particular promise for synthesizing large volumes of clinical information into actionable insights and digital health applications. Yet, a major limitation remains their inability to handle time series. To overcome this gap, we present OpenTSLM, a family of Time Series Language Models (TSLMs) created by integrating time series as a native modality to pretrained LLMs, enabling reasoning over multiple time series of any length. We investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt, models time series implicitly by concatenating learnable time series tokens with text tokens via soft prompting. Although parameter-efficient, we hypothesize that explicit time series modeling scales better and outperforms implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time series with text via cross-attention. We benchmark both variants against baselines that treat time series as text tokens or plots, across a suite of text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR, compared to 9.05 and 52.2 for finetuned text-only models. Notably, even 1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences, while maintaining stable memory requirements. By contrast, SoftPrompt grows exponentially in memory with sequence length, requiring around 110 GB compared to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA. To facilitate further research, we provide all code, datasets, and models open-source.",
        "subjects": "Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.638330"
    },
    {
        "index": "#90",
        "title": "Extreme value forecasting using relevance-based data augmentation with deep learning models",
        "link": "/arxiv/2510.02407",
        "arxiv_id": "2510.02407",
        "authors": "Junru Hua, Rahul Ahluwalia, Rohitash Chandra",
        "summary": "Data augmentation with generative adversarial networks (GANs) has been popular for class imbalance problems, mainly for pattern classification and computer vision-related applications. Extreme value forecasting is a challenging field that has various applications from finance to climate change problems. In this study, we present a data augmentation framework for extreme value forecasting. In this framework, our focus is on forecasting extreme values using deep learning models in combination with data augmentation models such as GANs and synthetic minority oversampling technique (SMOTE). We use deep learning models such as convolutional long short-term memory (Conv-LSTM) and bidirectional long short-term memory (BD-LSTM) networks for multistep ahead prediction featuring extremes. We investigate which data augmentation models are the most suitable, taking into account the prediction accuracy overall and at extreme regions, along with computational efficiency. We also present novel strategies for incorporating data augmentation, considering extreme values based on a relevance function. Our results indicate that the SMOTE-based strategy consistently demonstrated superior adaptability, leading to improved performance across both short- and long-horizon forecasts. Conv-LSTM and BD-LSTM exhibit complementary strengths: the former excels in periodic, stable datasets, while the latter performs better in chaotic or non-stationary sequences.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.638766"
    },
    {
        "index": "#91",
        "title": "Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles",
        "link": "/arxiv/2510.03224",
        "arxiv_id": "2510.03224",
        "authors": "Dong Lao, Yuxiang Zhang, Haniyeh Ehsani Oskouie, Yangchao Wu, Alex Wong, Stefano Soatto",
        "summary": "We propose a test-time defense mechanism against adversarial attacks: imperceptible image perturbations that significantly alter the predictions of a model. Unlike existing methods that rely on feature filtering or smoothing, which can lead to information loss, we propose to \"combat noise with noise\" by leveraging stochastic resonance to enhance robustness while minimizing information loss. Our approach introduces small translational perturbations to the input image, aligns the transformed feature embeddings, and aggregates them before mapping back to the original reference image. This can be expressed in a closed-form formula, which can be deployed on diverse existing network architectures without introducing additional network modules or fine-tuning for specific attack types. The resulting method is entirely training-free, architecture-agnostic, and attack-agnostic. Empirical results show state-of-the-art robustness on image classification and, for the first time, establish a generic test-time defense for dense prediction tasks, including stereo matching and optical flow, highlighting the method's versatility and practicality. Specifically, relative to clean (unperturbed) performance, our method recovers up to 68.1% of the accuracy loss on image classification, 71.9% on stereo matching, and 29.2% on optical flow under various types of adversarial attacks.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.639251"
    },
    {
        "index": "#93",
        "title": "Joint Bidding on Intraday and Frequency Containment Reserve Markets",
        "link": "/arxiv/2510.03209",
        "arxiv_id": "2510.03209",
        "authors": "Yiming Zhang, Wolfgang Ridinger, David Wozabal",
        "summary": "As renewable energy integration increases supply variability, battery energy storage systems (BESS) present a viable solution for balancing supply and demand. This paper proposes a novel approach for optimizing battery BESS participation in multiple electricity markets. We develop a joint bidding strategy that combines participation in the primary frequency reserve market with continuous trading in the intraday market, addressing a gap in the extant literature which typically considers these markets in isolation or simplifies the continuous nature of intraday trading. Our approach utilizes a mixed integer linear programming implementation of the rolling intrinsic algorithm for intraday decisions and state of charge recovery, alongside a learned classifier strategy (LCS) that determines optimal capacity allocation between markets. A comprehensive out-of-sample backtest over more than one year of historical German market data validates our approach: The LCS increases overall profits by over 4% compared to the best-performing static strategy and by more than 3% over a naive dynamic benchmark. Crucially, our method closes the gap to a theoretical perfect foresight strategy to just 4%, demonstrating the effectiveness of dynamic, learning-based allocation in a complex, multi-market environment.",
        "subjects": "Computational Finance, Machine Learning, Trading and Market Microstructure",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.640204"
    },
    {
        "index": "#94",
        "title": "Automatic Generation of Digital Twins for Network Testing",
        "link": "/arxiv/2510.03205",
        "arxiv_id": "2510.03205",
        "authors": "Shenjia Ding, David Flynn, Paul Harvey",
        "summary": "The increased use of software in the operation and management of telecommunication networks has moved the industry one step closer to realizing autonomous network operation. One consequence of this shift is the significantly increased need for testing and validation before such software can be deployed. Complementing existing simulation or hardware-based approaches, digital twins present an environment to achieve this testing; however, they require significant time and human effort to configure and execute. This paper explores the automatic generation of digital twins to provide efficient and accurate validation tools, aligned to the ITU-T autonomous network architecture's experimentation subsystem. We present experimental results for an initial use case, demonstrating that the approach is feasible in automatically creating efficient digital twins with sufficient accuracy to be included as part of existing validation pipelines.",
        "subjects": "Networking and Internet Architecture, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.640637"
    },
    {
        "index": "#95",
        "title": "Improving Online-to-Nonconvex Conversion for Smooth Optimization via Double Optimism",
        "link": "/arxiv/2510.03167",
        "arxiv_id": "2510.03167",
        "authors": "Francisco Patitucci, Ruichen Jiang, Aryan Mokhtari",
        "summary": "A recent breakthrough in nonconvex optimization is the online-to-nonconvex conversion framework of \\cite{cutkosky2023optimal}, which reformulates the task of finding an $\\varepsilon$-first-order stationary point as an online learning problem. When both the gradient and the Hessian are Lipschitz continuous, instantiating this framework with two different online learners achieves a complexity of $\\mathcal{O}(\\varepsilon^{-1.75}\\log(1/\\varepsilon))$ in the deterministic case and a complexity of $\\mathcal{O}(\\varepsilon^{-3.5})$ in the stochastic case. However, this approach suffers from several limitations: (i) the deterministic method relies on a complex double-loop scheme that solves a fixed-point equation to construct hint vectors for an optimistic online learner, introducing an extra logarithmic factor; (ii) the stochastic method assumes a bounded second-order moment of the stochastic gradient, which is stronger than standard variance bounds; and (iii) different online learning algorithms are used in the two settings. In this paper, we address these issues by introducing an online optimistic gradient method based on a novel \\textit{doubly optimistic hint function}. Specifically, we use the gradient at an extrapolated point as the hint, motivated by two optimistic assumptions: that the difference between the hint and the target gradient remains near constant, and that consecutive update directions change slowly due to smoothness. Our method eliminates the need for a double loop and removes the logarithmic factor. Furthermore, by simply replacing full gradients with stochastic gradients and under the standard assumption that their variance is bounded by $\\sigma^2$, we obtain a unified algorithm with complexity $\\mathcal{O}(\\varepsilon^{-1.75} + \\sigma^2 \\varepsilon^{-3.5})$, smoothly interpolating between the best-known deterministic rate and the optimal stochastic rate.",
        "subjects": "Optimization and Control, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.641072"
    },
    {
        "index": "#96",
        "title": "Stimulus-Voltage-Based Prediction of Action Potential Onset Timing: Classical vs. Quantum-Inspired Approaches",
        "link": "/arxiv/2510.03155",
        "arxiv_id": "2510.03155",
        "authors": "Stevens Johnson, Varun Puram, Johnson Thomas, Acsah Konuparamban, Ashwin Kannan",
        "summary": "Accurate modeling of neuronal action potential (AP) onset timing is crucial for understanding neural coding of danger signals. Traditional leaky integrate-and-fire (LIF) models, while widely used, exhibit high relative error in predicting AP onset latency, especially under strong or rapidly changing stimuli. Inspired by recent experimental findings and quantum theory, we present a quantum-inspired leaky integrate-and-fire (QI-LIF) model that treats AP onset as a probabilistic event, represented by a Gaussian wave packet in time. This approach captures the biological variability and uncertainty inherent in neuronal firing. We systematically compare the relative error of AP onset predictions between the classical LIF and QI-LIF models using synthetic data from hippocampal and sensory neurons subjected to varying stimulus amplitudes. Our results demonstrate that the QI-LIF model significantly reduces prediction error, particularly for high-intensity stimuli, aligning closely with observed biological responses. This work highlights the potential of quantum-inspired computational frameworks in advancing the accuracy of neural modeling and has implications for quantum engineering approaches to brain-inspired computing.",
        "subjects": "Neurons and Cognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.646673"
    },
    {
        "index": "#97",
        "title": "ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories",
        "link": "/arxiv/2510.03152",
        "arxiv_id": "2510.03152",
        "authors": "Anantajit Subrahmanya, Chandrakanth Gudavalli, Connor Levenson, Umang Garg, B. S. Manjunath",
        "summary": "Accurately modeling human mobility is critical for urban planning, epidemiology, and traffic management. In this work, we introduce Markovian Reeb Graphs, a novel framework for simulating spatiotemporal trajectories that preserve Patterns of Life (PoLs) learned from baseline data. By combining individual- and population-level mobility structures within a probabilistic topological model, our approach generates realistic future trajectories that capture both consistency and variability in daily life. Evaluations on the Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon Divergence (JSD) across population- and agent-level metrics demonstrate that the proposed method achieves strong fidelity while remaining data- and compute-efficient. These results position Markovian Reeb Graphs as a scalable framework for trajectory simulation with broad applicability across diverse urban environments.",
        "subjects": "Computer Vision and Pattern Recognition, Computational Engineering, Finance, and Science, Machine Learning, Social and Information Networks",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.647167"
    },
    {
        "index": "#98",
        "title": "The Computational Complexity of Almost Stable Clustering with Penalties",
        "link": "/arxiv/2510.03143",
        "arxiv_id": "2510.03143",
        "authors": "Kamyar Khodamoradi, Farnam Mansouri, Sandra Zilles",
        "summary": "We investigate the complexity of stable (or perturbation-resilient) instances of $\\mathrm{k-M\\small{EANS}}$ and $\\mathrm{k-M\\small{EDIAN}}$ clustering problems in metrics with small doubling dimension. While these problems have been extensively studied under multiplicative perturbation resilience in low-dimensional Euclidean spaces (e.g., (Friggstad et al., 2019; Cohen-Addad and Schwiegelshohn, 2017)), we adopt a more general notion of stability, termed ``almost stable'', which is closer to the notion of $(\\alpha, \\varepsilon)$-perturbation resilience introduced by Balcan and Liang (2016). Additionally, we extend our results to $\\mathrm{k-M\\small{EANS}}$/$\\mathrm{k-M\\small{EDIAN}}$ with penalties, where each data point is either assigned to a cluster centre or incurs a penalty. We show that certain special cases of almost stable $\\mathrm{k-M\\small{EANS}}$/$\\mathrm{k-M\\small{EDIAN}}$ (with penalties) are solvable in polynomial time. To complement this, we also examine the hardness of almost stable instances and $(1 + \\frac{1}{poly(n)})$-stable instances of $\\mathrm{k-M\\small{EANS}}$/$\\mathrm{k-M\\small{EDIAN}}$ (with penalties), proving super-polynomial lower bounds on the runtime of any exact algorithm under the widely believed Exponential Time Hypothesis (ETH).",
        "subjects": "Computational Complexity, Data Structures and Algorithms, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.647620"
    },
    {
        "index": "#99",
        "title": "What Drives Compositional Generalization in Visual Generative Models?",
        "link": "/arxiv/2510.03075",
        "arxiv_id": "2510.03075",
        "authors": "Karim Farid, Rajat Sahay, Yumna Ali Alnaggar, Simon Schrodi, Volker Fischer, Cordelia Schmid, Thomas Brox",
        "summary": "Compositional generalization, the ability to generate novel combinations of known concepts, is a key ingredient for visual generative models. Yet, not all mechanisms that enable or inhibit it are fully understood. In this work, we conduct a systematic study of how various design choices influence compositional generalization in image and video generation in a positive or negative way. Through controlled experiments, we identify two key factors: (i) whether the training objective operates on a discrete or continuous distribution, and (ii) to what extent conditioning provides information about the constituent concepts during training. Building on these insights, we show that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based objective can improve compositional performance in discrete models like MaskGIT.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.648134"
    },
    {
        "index": "#100",
        "title": "FR-LUX: Friction-Aware, Regime-Conditioned Policy Optimization for Implementable Portfolio Management",
        "link": "/arxiv/2510.02986",
        "arxiv_id": "2510.02986",
        "authors": "Jian'an Zhang",
        "summary": "Transaction costs and regime shifts are major reasons why paper portfolios fail in live trading. We introduce FR-LUX (Friction-aware, Regime-conditioned Learning under eXecution costs), a reinforcement learning framework that learns after-cost trading policies and remains robust across volatility-liquidity regimes. FR-LUX integrates three ingredients: (i) a microstructure-consistent execution model combining proportional and impact costs, directly embedded in the reward; (ii) a trade-space trust region that constrains changes in inventory flow rather than logits, yielding stable low-turnover updates; and (iii) explicit regime conditioning so the policy specializes to LL/LH/HL/HH states without fragmenting the data. On a 4 x 5 grid of regimes and cost levels with multiple random seeds, FR-LUX achieves the top average Sharpe ratio with narrow bootstrap confidence intervals, maintains a flatter cost-performance slope than strong baselines, and attains superior risk-return efficiency for a given turnover budget. Pairwise scenario-level improvements are strictly positive and remain statistically significant after multiple-testing corrections. We provide formal guarantees on optimality under convex frictions, monotonic improvement under a KL trust region, long-run turnover bounds and induced inaction bands due to proportional costs, positive value advantage for regime-conditioned policies, and robustness to cost misspecification. The methodology is implementable: costs are calibrated from standard liquidity proxies, scenario-level inference avoids pseudo-replication, and all figures and tables are reproducible from released artifacts.",
        "subjects": "Trading and Market Microstructure, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.648552"
    },
    {
        "index": "#101",
        "title": "Oracle-based Uniform Sampling from Convex Bodies",
        "link": "/arxiv/2510.02983",
        "arxiv_id": "2510.02983",
        "authors": "Thanh Dang, Jiaming Liang",
        "summary": "We propose new Markov chain Monte Carlo algorithms to sample a uniform distribution on a convex body $K$. Our algorithms are based on the Alternating Sampling Framework/proximal sampler, which uses Gibbs sampling on an augmented distribution and assumes access to the so-called restricted Gaussian oracle (RGO). The key contribution of this work is the efficient implementation of RGO for uniform sampling on $K$ via rejection sampling and access to either a projection oracle or a separation oracle on $K$. In both oracle cases, we establish non-asymptotic complexities to obtain unbiased samples where the accuracy is measured in Rényi divergence or $\\chi^2$-divergence.",
        "subjects": "Data Structures and Algorithms, Machine Learning, Optimization and Control, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.648991"
    },
    {
        "index": "#102",
        "title": "oRANS: Online optimisation of RANS machine learning models with embedded DNS data generation",
        "link": "/arxiv/2510.02982",
        "arxiv_id": "2510.02982",
        "authors": "Daniel Dehtyriov, Jonathan F. MacArt, Justin Sirignano",
        "summary": "Deep learning (DL) has demonstrated promise for accelerating and enhancing the accuracy of flow physics simulations, but progress is constrained by the scarcity of high-fidelity training data, which is costly to generate and inherently limited to a small set of flow conditions. Consequently, closures trained in the conventional offline paradigm tend to overfit and fail to generalise to new regimes. We introduce an online optimisation framework for DL-based Reynolds-averaged Navier--Stokes (RANS) closures which seeks to address the challenge of limited high-fidelity datasets. Training data is dynamically generated by embedding a direct numerical simulation (DNS) within a subdomain of the RANS domain. The RANS solution supplies boundary conditions to the DNS, while the DNS provides mean velocity and turbulence statistics that are used to update a DL closure model during the simulation. This feedback loop enables the closure to adapt to the embedded DNS target flow, avoiding reliance on precomputed datasets and improving out-of-distribution performance. The approach is demonstrated for the stochastically forced Burgers equation and for turbulent channel flow at $Re_\\tau=180$, $270$, $395$ and $590$ with varying embedded domain lengths $1\\leq L_0/L\\leq 8$. Online-optimised RANS models significantly outperform both offline-trained and literature-calibrated closures, with accurate training achieved using modest DNS subdomains. Performance degrades primarily when boundary-condition contamination dominates or when domains are too short to capture low-wavenumber modes. This framework provides a scalable route to physics-informed machine learning closures, enabling data-adaptive reduced-order models that generalise across flow regimes without requiring large precomputed training datasets.",
        "subjects": "Fluid Dynamics, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.649422"
    },
    {
        "index": "#103",
        "title": "Scalable Quantum Optimisation using HADOF: Hamiltonian Auto-Decomposition Optimisation Framework",
        "link": "/arxiv/2510.02926",
        "arxiv_id": "2510.02926",
        "authors": "Namasi G Sankar, Georgios Miliotis, Simon Caton",
        "summary": "Quantum Annealing (QA) and QAOA are promising quantum optimisation algorithms used for finding approximate solutions to combinatorial problems on near-term NISQ systems. Many NP-hard problems can be reformulated as Quadratic Unconstrained Binary Optimisation (QUBO), which maps naturally onto quantum Hamiltonians. However, the limited qubit counts of current NISQ devices restrict practical deployment of such algorithms. In this study, we present the Hamiltonian Auto-Decomposition Optimisation Framework (HADOF), which leverages an iterative strategy to automatically divide the Quadratic Unconstrained Binary Optimisation (QUBO) Hamiltonian into sub-Hamiltonians which can be optimised separately using Hamiltonian based optimisers such as QAOA, QA or Simulated Annealing (SA) and aggregated into a global solution. We compare HADOF with Simulated Annealing (SA) and the CPLEX exact solver, showing scalability to problem sizes far exceeding available qubits while maintaining competitive accuracy and runtime. Furthermore, we realise HADOF for a toy problem on an IBM quantum computer, showing promise for practical applications of quantum optimisation.",
        "subjects": "Quantum Physics, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.649845"
    },
    {
        "index": "#104",
        "title": "Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders",
        "link": "/arxiv/2510.02917",
        "arxiv_id": "2510.02917",
        "authors": "Kriz Tahimic, Charibeth Cheng",
        "summary": "As Large Language Models become integral to software development, with substantial portions of AI-suggested code entering production, understanding their internal correctness mechanisms becomes critical for safe deployment. We apply sparse autoencoders to decompose LLM representations, identifying directions that correspond to code correctness. We select predictor directions using t-statistics and steering directions through separation scores from base model representations, then analyze their mechanistic properties through steering, attention analysis, and weight orthogonalization. We find that code correctness directions in LLMs reliably predict incorrect code, while correction capabilities, though statistically significant, involve tradeoffs between fixing errors and preserving correct code. Mechanistically, successful code generation depends on attending to test cases rather than problem descriptions. Moreover, directions identified in base models retain their effectiveness after instruction-tuning, suggesting code correctness mechanisms learned during pre-training are repurposed during fine-tuning. Our mechanistic insights suggest three practical applications: prompting strategies should prioritize test examples over elaborate problem descriptions, predictor directions can serve as error alarms for developer review, and these same predictors can guide selective steering, intervening only when errors are anticipated to prevent the code corruption from constant steering.",
        "subjects": "Software Engineering, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.650272"
    },
    {
        "index": "#105",
        "title": "SALSA-V: Shortcut-Augmented Long-form Synchronized Audio from Videos",
        "link": "/arxiv/2510.02916",
        "arxiv_id": "2510.02916",
        "authors": "Amir Dellali, Luca A. Lanzendörfer, Florian Grötschla, Roger Wattenhofer",
        "summary": "We propose SALSA-V, a multimodal video-to-audio generation model capable of synthesizing highly synchronized, high-fidelity long-form audio from silent video content. Our approach introduces a masked diffusion objective, enabling audio-conditioned generation and the seamless synthesis of audio sequences of unconstrained length. Additionally, by integrating a shortcut loss into our training process, we achieve rapid generation of high-quality audio samples in as few as eight sampling steps, paving the way for near-real-time applications without requiring dedicated fine-tuning or retraining. We demonstrate that SALSA-V significantly outperforms existing state-of-the-art methods in both audiovisual alignment and synchronization with video content in quantitative evaluation and a human listening study. Furthermore, our use of random masking during training enables our model to match spectral characteristics of reference audio samples, broadening its applicability to professional audio synthesis tasks such as Foley generation and sound design.",
        "subjects": "Sound, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.650710"
    },
    {
        "index": "#106",
        "title": "WavInWav: Time-domain Speech Hiding via Invertible Neural Network",
        "link": "/arxiv/2510.02915",
        "arxiv_id": "2510.02915",
        "authors": "Wei Fan, Kejiang Chen, Xiangkun Wang, Weiming Zhang, Nenghai Yu",
        "summary": "Data hiding is essential for secure communication across digital media, and recent advances in Deep Neural Networks (DNNs) provide enhanced methods for embedding secret information effectively. However, previous audio hiding methods often result in unsatisfactory quality when recovering secret audio, due to their inherent limitations in the modeling of time-frequency relationships. In this paper, we explore these limitations and introduce a new DNN-based approach. We use a flow-based invertible neural network to establish a direct link between stego audio, cover audio, and secret audio, enhancing the reversibility of embedding and extracting messages. To address common issues from time-frequency transformations that degrade secret audio quality during recovery, we implement a time-frequency loss on the time-domain signal. This approach not only retains the benefits of time-frequency constraints but also enhances the reversibility of message recovery, which is vital for practical applications. We also add an encryption technique to protect the hidden data from unauthorized access. Experimental results on the VCTK and LibriSpeech datasets demonstrate that our method outperforms previous approaches in terms of subjective and objective metrics and exhibits robustness to various types of noise, suggesting its utility in targeted secure communication scenarios.",
        "subjects": "Sound, Artificial Intelligence, Cryptography and Security, Machine Learning, Audio and Speech Processing",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.651457"
    },
    {
        "index": "#107",
        "title": "ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment",
        "link": "/arxiv/2510.02876",
        "arxiv_id": "2510.02876",
        "authors": "Md Zahim Hassan, Md. Osama, Muhammad Ashad Kabir, Md. Saiful Islam, Zannatul Naim",
        "summary": "Accurate, non-destructive assessment of egg quality is critical for ensuring food safety, maintaining product standards, and operational efficiency in commercial poultry production. This paper introduces ELMF4EggQ, an ensemble learning framework that employs multimodal feature fusion to classify egg grade and freshness using only external attributes - image, shape, and weight. A novel, publicly available dataset of 186 brown-shelled eggs was constructed, with egg grade and freshness levels determined through laboratory-based expert assessments involving internal quality measurements, such as yolk index and Haugh unit. To the best of our knowledge, this is the first study to apply machine learning methods for internal egg quality assessment using only external, non-invasive features, and the first to release a corresponding labeled dataset. The proposed framework integrates deep features extracted from external egg images with structural characteristics such as egg shape and weight, enabling a comprehensive representation of each egg. Image feature extraction is performed using top-performing pre-trained CNN models (ResNet152, DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction, SMOTE augmentation, and classification using multiple machine learning algorithms. An ensemble voting mechanism combines predictions from the best-performing classifiers to enhance overall accuracy. Experimental results demonstrate that the multimodal approach significantly outperforms image-only and tabular (shape and weight) only baselines, with the multimodal ensemble approach achieving 86.57% accuracy in grade classification and 70.83% in freshness prediction. All code and data are publicly available at https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting transparency, reproducibility, and further research in this domain.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.651935"
    },
    {
        "index": "#108",
        "title": "The land use-climate change-biodiversity nexus in European islands stakeholders",
        "link": "/arxiv/2510.02829",
        "arxiv_id": "2510.02829",
        "authors": "Aristides Moustakas, Irene Christoforidi, George Zittis, Nazli Demirel, Mauro Fois, Savvas Zotos, Eirini Gallou, Valentini Stamatiadou, Elli Tzirkalli, Christos Zoumides, Kristina Košić, Aikaterini Christopoulou, Aleksandra Dragin, Damian Łowicki, Artur Gil, Bruna Almeida, Panos Chrysos, Mario V. Balzan, Mark D. C. Mansoldo, Rannveig Ólafsdóttir, Cigdem Kaptan Ayhan, Lutfi Atay, Mirela Tase, Vladimir Stojanović, Maja Mijatov Ladičorbić, Juan Pedro Díaz, Francisco Javier Expósito, Sonia Quiroga, Miguel Ángel Casquet Cano, Haoran Wang, Cristina Suárez, Paraskevi Manolaki, Ioannis N. Vogiatzakis",
        "summary": "To promote climate adaptation and mitigation, it is crucial to understand stakeholder perspectives and knowledge gaps on land use and climate changes. Stakeholders across 21 European islands were consulted on climate and land use change issues affecting ecosystem services. Climate change perceptions included temperature, precipitation, humidity, extremes, and wind. Land use change perceptions included deforestation, coastal degradation, habitat protection, renewable energy facilities, wetlands, and others. Additional concerns such as invasive species, water or energy scarcity, infrastructure problems, and austerity were also considered. Climate and land use change impact perceptions were analysed with machine learning to quantify their influence. The predominant climatic characteristic is temperature, and the predominant land use characteristic is deforestation. Water-related problems are top priorities for stakeholders. Energy-related problems, including energy deficiency and issues with wind and solar facilities, rank high as combined climate and land use risks. Stakeholders generally perceive climate change impacts on ecosystem services as negative, with natural habitat destruction and biodiversity loss identified as top issues. Land use change impacts are also negative but more complex, with more explanatory variables. Stakeholders share common perceptions on biodiversity impacts despite geographic disparity, but they differentiate between climate and land use impacts. Water, energy, and renewable energy issues pose serious concerns, requiring management measures.",
        "subjects": "Populations and Evolution, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.652739"
    },
    {
        "index": "#110",
        "title": "Align Your Query: Representation Alignment for Multimodality Medical Object Detection",
        "link": "/arxiv/2510.02789",
        "arxiv_id": "2510.02789",
        "authors": "Ara Seo, Bryan Sangwoo Kim, Hyungjin Chung, Jong Chul Ye",
        "summary": "Medical object detection suffers when a single detector is trained on mixed medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and disjoint representation spaces. To address this challenge, we turn to representation alignment, an approach that has proven effective for bringing features from different sources into a shared space. Specifically, we target the representations of DETR-style object queries and propose a simple, detector-agnostic framework to align them with modality context. First, we define modality tokens: compact, text-derived embeddings encoding imaging modality that are lightweight and require no extra annotations. We integrate the modality tokens into the detection process via Multimodality Context Attention (MoCA), mixing object-query representations via self-attention to propagate modality context within the query set. This preserves DETR-style architectures and adds negligible latency while injecting modality cues into object queries. We further introduce QueryREPA, a short pretraining stage that aligns query representations to their modality tokens using a task-specific contrastive objective with modality-balanced batches. Together, MoCA and QueryREPA produce modality-aware, class-faithful queries that transfer effectively to downstream training. Across diverse modalities trained altogether, the proposed approach consistently improves AP with minimal overhead and no architectural modifications, offering a practical path toward robust multimodality medical object detection. Project page: https://araseo.github.io/alignyourquery/.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.653667"
    },
    {
        "index": "#111",
        "title": "Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology",
        "link": "/arxiv/2510.02760",
        "arxiv_id": "2510.02760",
        "authors": "Matthias Perkonigg, Patrick Rockenschaub, Georg Göbel, Adelheid Wöhrer",
        "summary": "Accurate brain tumor classification is critical for intra-operative decision making in neuro-oncological surgery. However, existing approaches are restricted to a fixed set of predefined classes and are therefore unable to capture patterns of tumor types not available during training. Unsupervised learning can extract general-purpose features, but it lacks the ability to incorporate prior knowledge from labelled data, and semi-supervised methods often assume that all potential classes are represented in the labelled data. Generalized Category Discovery (GCD) aims to bridge this gap by categorizing both known and unknown classes within unlabelled data. To reflect the hierarchical structure of brain tumor taxonomies, in this work, we introduce Hierarchical Generalized Category Discovery for Brain Tumor Classification (HGCD-BT), a novel approach that integrates hierarchical clustering with contrastive learning. Our method extends contrastive learning based GCD by incorporating a novel semi-supervised hierarchical clustering loss. We evaluate HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images, achieving a +28% improvement in accuracy over state-of-the-art GCD methods for patch-level classification, particularly in identifying previously unseen tumor categories. Furthermore, we demonstrate the generalizability of HGCD-BT on slide-level classification of hematoxylin and eosin stained whole-slide images from the Digital Brain Tumor Atlas, confirming its utility across imaging modalities.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.654142"
    },
    {
        "index": "#112",
        "title": "Neural Jump ODEs as Generative Models",
        "link": "/arxiv/2510.02757",
        "arxiv_id": "2510.02757",
        "authors": "Robert A. Crowell, Florian Krach, Josef Teichmann",
        "summary": "In this work, we explore how Neural Jump ODEs (NJODEs) can be used as generative models for Itô processes. Given (discrete observations of) samples of a fixed underlying Itô process, the NJODE framework can be used to approximate the drift and diffusion coefficients of the process. Under standard regularity assumptions on the Itô processes, we prove that, in the limit, we recover the true parameters with our approximation. Hence, using these learned coefficients to sample from the corresponding Itô process generates, in the limit, samples with the same law as the true underlying process. Compared to other generative machine learning models, our approach has the advantage that it does not need adversarial training and can be trained solely as a predictive model on the observed samples without the need to generate any samples during training to empirically approximate the distribution. Moreover, the NJODE framework naturally deals with irregularly sampled data with missing values as well as with path-dependent dynamics, allowing to apply this approach in real-world settings. In particular, in the case of path-dependent coefficients of the Itô processes, the NJODE learns their optimal approximation given the past observations and therefore allows generating new paths conditionally on discrete, irregular, and incomplete past observations in an optimal way.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.654574"
    },
    {
        "index": "#113",
        "title": "Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data",
        "link": "/arxiv/2510.02738",
        "arxiv_id": "2510.02738",
        "authors": "Tianyu Li, Yihan Li, Zizhe Zhang, Nadia Figueroa",
        "summary": "While visuomotor policy has made advancements in recent years, contact-rich tasks still remain a challenge. Robotic manipulation tasks that require continuous contact demand explicit handling of compliance and force. However, most visuomotor policies ignore compliance, overlooking the importance of physical interaction with the real world, often leading to excessive contact forces or fragile behavior under uncertainty. Introducing force information into vision-based imitation learning could help improve awareness of contacts, but could also require a lot of data to perform well. One remedy for data scarcity is to generate data in simulation, yet computationally taxing processes are required to generate data good enough not to suffer from the Sim2Real gap. In this work, we introduce a framework for generating force-informed data in simulation, instantiated by a single human demonstration, and show how coupling with a compliant policy improves the performance of a visuomotor policy learned from synthetic data. We validate our approach on real-robot tasks, including non-prehensile block flipping and a bi-manual object moving, where the learned policy exhibits reliable contact maintenance and adaptation to novel conditions. Project Website: https://flow-with-the-force-field.github.io/webpage/",
        "subjects": "Robotics, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.655029"
    },
    {
        "index": "#114",
        "title": "Quantitative Convergence Analysis of Projected Stochastic Gradient Descent for Non-Convex Losses via the Goldstein Subdifferential",
        "link": "/arxiv/2510.02735",
        "arxiv_id": "2510.02735",
        "authors": "Yuping Zheng, Andrew Lamperski",
        "summary": "Stochastic gradient descent (SGD) is the main algorithm behind a large body of work in machine learning. In many cases, constraints are enforced via projections, leading to projected stochastic gradient algorithms. In recent years, a large body of work has examined the convergence properties of projected SGD for non-convex losses in asymptotic and non-asymptotic settings. Strong quantitative guarantees are available for convergence measured via Moreau envelopes. However, these results cannot be compared directly with work on unconstrained SGD, since the Moreau envelope construction changes the gradient. Other common measures based on gradient mappings have the limitation that convergence can only be guaranteed if variance reduction methods, such as mini-batching, are employed. This paper presents an analysis of projected SGD for non-convex losses over compact convex sets. Convergence is measured via the distance of the gradient to the Goldstein subdifferential generated by the constraints. Our proposed convergence criterion directly reduces to commonly used criteria in the unconstrained case, and we obtain convergence without requiring variance reduction. We obtain results for data that are independent, identically distributed (IID) or satisfy mixing conditions ($L$-mixing). In these cases, we derive asymptotic convergence and $O(N^{-1/3})$ non-asymptotic bounds in expectation, where $N$ is the number of steps. In the case of IID sub-Gaussian data, we obtain almost-sure asymptotic convergence and high-probability non-asymptotic $O(N^{-1/5})$ bounds. In particular, these are the first non-asymptotic high-probability bounds for projected SGD with non-convex losses.",
        "subjects": "Optimization and Control, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.655443"
    },
    {
        "index": "#116",
        "title": "A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison",
        "link": "/arxiv/2510.02707",
        "arxiv_id": "2510.02707",
        "authors": "Chinthana Wimalasuriya, Spyros Tragoudas",
        "summary": "Adversarial attacks present a significant threat to modern machine learning systems. Yet, existing detection methods often lack the ability to detect unseen attacks or detect different attack types with a high level of accuracy. In this work, we propose a statistical approach that establishes a detection baseline before a neural network's deployment, enabling effective real-time adversarial detection. We generate a metric of adversarial presence by comparing the behavior of a compressed/uncompressed neural network pair. Our method has been tested against state-of-the-art techniques, and it achieves near-perfect detection across a wide range of attack types. Moreover, it significantly reduces false positives, making it both reliable and practical for real-world applications.",
        "subjects": "Cryptography and Security, Computer Vision and Pattern Recognition, Machine Learning, Image and Video Processing",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.661336"
    },
    {
        "index": "#117",
        "title": "ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks",
        "link": "/arxiv/2510.02677",
        "arxiv_id": "2510.02677",
        "authors": "Zhaorun Chen, Xun Liu, Mintong Kang, Jiawei Zhang, Minzhou Pan, Shuang Yang, Bo Li",
        "summary": "As vision-language models (VLMs) gain prominence, their multimodal interfaces also introduce new safety vulnerabilities, making the safety evaluation challenging and critical. Existing red-teaming efforts are either restricted to a narrow set of adversarial patterns or depend heavily on manual engineering, lacking scalable exploration of emerging real-world VLM vulnerabilities. To bridge this gap, we propose ARMs, an adaptive red-teaming agent that systematically conducts comprehensive risk assessments for VLMs. Given a target harmful behavior or risk definition, ARMs automatically optimizes diverse red-teaming strategies with reasoning-enhanced multi-step orchestration, to effectively elicit harmful outputs from target VLMs. We propose 11 novel multimodal attack strategies, covering diverse adversarial patterns of VLMs (e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming algorithms into ARMs via model context protocol (MCP). To balance the diversity and effectiveness of the attack, we design a layered memory with an epsilon-greedy attack exploration algorithm. Extensive experiments on instance- and policy-based benchmarks show that ARMs achieves SOTA attack success rates, exceeding baselines by an average of 52.1% and surpassing 90% on Claude-4-Sonnet. We show that the diversity of red-teaming instances generated by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs. Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety dataset comprising over 30K red-teaming instances spanning 51 diverse risk categories, grounded in both real-world multimodal threats and regulatory risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness of VLMs while preserving their general utility, providing actionable guidance to improve multimodal safety alignment against emerging threats.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.661645"
    },
    {
        "index": "#120",
        "title": "FLOWR.root: A flow matching based foundation model for joint multi-purpose structure-aware 3D ligand generation and affinity prediction",
        "link": "/arxiv/2510.02578",
        "arxiv_id": "2510.02578",
        "authors": "Julian Cremer, Tuan Le, Mohammad M. Ghahremanpour, Emilia Sługocka, Filipe Menezes, Djork-Arné Clevert",
        "summary": "We present Flowr.root, an equivariant flow-matching model for pocket-aware 3D ligand generation with joint binding affinity prediction and confidence estimation. The model supports de novo generation, pharmacophore-conditional sampling, fragment elaboration, and multi-endpoint affinity prediction (pIC50, pKi, pKd, pEC50). Training combines large-scale ligand libraries with mixed-fidelity protein-ligand complexes, followed by refinement on curated co-crystal datasets and parameter-efficient finetuning for project-specific adaptation. Flowr.root achieves state-of-the-art performance in unconditional 3D molecule generation and pocket-conditional ligand design, producing geometrically realistic, low-strain structures. The integrated affinity prediction module demonstrates superior accuracy on the SPINDR test set and outperforms recent models on the Schrodinger FEP+/OpenFE benchmark with substantial speed advantages. As a foundation model, Flowr.root requires finetuning on project-specific datasets to account for unseen structure-activity landscapes, yielding strong correlation with experimental data. Joint generation and affinity prediction enable inference-time scaling through importance sampling, steering molecular design toward higher-affinity compounds. Case studies validate this: selective CK2alpha ligand generation against CLK3 shows significant correlation between predicted and quantum-mechanical binding energies, while ERalpha and TYK2 scaffold elaboration demonstrates strong agreement with QM calculations. By integrating structure-aware generation, affinity estimation, and property-guided sampling, Flowr.root provides a comprehensive foundation for structure-based drug design spanning hit identification through lead optimization.",
        "subjects": "Biomolecules, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.662564"
    },
    {
        "index": "#121",
        "title": "Agentic Additive Manufacturing Alloy Discovery",
        "link": "/arxiv/2510.02567",
        "arxiv_id": "2510.02567",
        "authors": "Peter Pak, Achuth Chandrasekhar, Amir Barati Farimani",
        "summary": "Agentic systems enable the intelligent use of research tooling, augmenting a researcher's ability to investigate and propose novel solutions to existing problems. Within Additive Manufacturing (AM), alloy discovery remains a complex challenge, often requiring expertise in the various domains of materials science, thermodynamic simulations, and experimental analysis. Large Language Model (LLM) enabled agents can facilitate this endeavor by utilizing their extensive knowledge base to dispatch tool calls via Model Context Protocol (MCP) to perform actions such as Thermo-Calc property diagram calculations and lack of fusion process map generation. In addition, the multi-agent system developed in this work is able to effectively reason through complex user prompts and provide analysis on the printability of proposed alloys. These agents can dynamically adjust their task trajectory to the outcomes of tool call results, effectively enabling autonomous decision-making in practical environments. This work aims to utilize LLM enabled agents to automate and accelerate the task of alloy discovery within the field of additive manufacturing and showcase the benefits of adopting this multi-agent system.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.662829"
    },
    {
        "index": "#122",
        "title": "Even Faster Kernel Matrix Linear Algebra via Density Estimation",
        "link": "/arxiv/2510.02540",
        "arxiv_id": "2510.02540",
        "authors": "Rikhav Shah, Sandeep Silwal, Haike Xu",
        "summary": "This paper studies the use of kernel density estimation (KDE) for linear algebraic tasks involving the kernel matrix of a collection of $n$ data points in $\\mathbb R^d$. In particular, we improve upon existing algorithms for computing the following up to $(1+\\varepsilon)$ relative error: matrix-vector products, matrix-matrix products, the spectral norm, and sum of all entries. The runtimes of our algorithms depend on the dimension $d$, the number of points $n$, and the target error $\\varepsilon$. Importantly, the dependence on $n$ in each case is far lower when accessing the kernel matrix through KDE queries as opposed to reading individual entries. Our improvements over existing best algorithms (particularly those of Backurs, Indyk, Musco, and Wagner '21) for these tasks reduce the polynomial dependence on $\\varepsilon$, and additionally decreases the dependence on $n$ in the case of computing the sum of all entries of the kernel matrix. We complement our upper bounds with several lower bounds for related problems, which provide (conditional) quadratic time hardness results and additionally hint at the limits of KDE based approaches for the problems we study.",
        "subjects": "Data Structures and Algorithms, Machine Learning, Numerical Analysis",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.663110"
    },
    {
        "index": "#123",
        "title": "Learning Multi-Index Models with Hyper-Kernel Ridge Regression",
        "link": "/arxiv/2510.02532",
        "arxiv_id": "2510.02532",
        "authors": "Shuo Huang, Hippolyte Labarrière, Ernesto De Vito, Tomaso Poggio, Lorenzo Rosasco",
        "summary": "Deep neural networks excel in high-dimensional problems, outperforming models such as kernel methods, which suffer from the curse of dimensionality. However, the theoretical foundations of this success remain poorly understood. We follow the idea that the compositional structure of the learning task is the key factor determining when deep networks outperform other approaches. Taking a step towards formalizing this idea, we consider a simple compositional model, namely the multi-index model (MIM). In this context, we introduce and study hyper-kernel ridge regression (HKRR), an approach blending neural networks and kernel methods. Our main contribution is a sample complexity result demonstrating that HKRR can adaptively learn MIM, overcoming the curse of dimensionality. Further, we exploit the kernel nature of the estimator to develop ad hoc optimization approaches. Indeed, we contrast alternating minimization and alternating gradient methods both theoretically and numerically. These numerical results complement and reinforce our theoretical findings.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.663400"
    },
    {
        "index": "#124",
        "title": "Multimodal Function Vectors for Spatial Relations",
        "link": "/arxiv/2510.02528",
        "arxiv_id": "2510.02528",
        "authors": "Shuhao Fu, Esther Goldberg, Ying Nian Wu, Hongjing Lu",
        "summary": "Large Multimodal Models (LMMs) demonstrate impressive in-context learning abilities from limited multimodal demonstrations, yet the internal mechanisms supporting such task learning remain opaque. Building on prior work of large language models, we show that a small subset of attention heads in the vision-language model OpenFlamingo-4B is responsible for transmitting representations of spatial relations. The activations of these attention heads, termed function vectors, can be extracted and manipulated to alter an LMM's performance on relational tasks. First, using both synthetic and real image datasets, we apply causal mediation analysis to identify attention heads that strongly influence relational predictions, and extract multimodal function vectors that improve zero-shot accuracy at inference time. We further demonstrate that these multimodal function vectors can be fine-tuned with a modest amount of training data, while keeping LMM parameters frozen, to significantly outperform in-context learning baselines. Finally, we show that relation-specific function vectors can be linearly combined to solve analogy problems involving novel and untrained spatial relations, highlighting the strong generalization ability of this approach. Our results show that LMMs encode spatial relational knowledge within localized internal structures, which can be systematically extracted and optimized, thereby advancing our understanding of model modularity and enhancing control over relational reasoning in LMMs.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.663670"
    },
    {
        "index": "#125",
        "title": "Self-supervised diffusion model fine-tuning for costate initialization using Markov chain Monte Carlo",
        "link": "/arxiv/2510.02527",
        "arxiv_id": "2510.02527",
        "authors": "Jannik Graebner, Ryne Beeson",
        "summary": "Global search and optimization of long-duration, low-thrust spacecraft trajectories with the indirect method is challenging due to a complex solution space and the difficulty of generating good initial guesses for the costate variables. This is particularly true in multibody environments. Given data that reveals a partial Pareto optimal front, it is desirable to find a flexible manner in which the Pareto front can be completed and fronts for related trajectory problems can be found. In this work we use conditional diffusion models to represent the distribution of candidate optimal trajectory solutions. We then introduce into this framework the novel approach of using Markov Chain Monte Carlo algorithms with self-supervised fine-tuning to achieve the aforementioned goals. Specifically, a random walk Metropolis algorithm is employed to propose new data that can be used to fine-tune the diffusion model using a reward-weighted training based on efficient evaluations of constraint violations and missions objective functions. The framework removes the need for separate focused and often tedious data generation phases. Numerical experiments are presented for two problems demonstrating the ability to improve sample quality and explicitly target Pareto optimality based on the theory of Markov chains. The first problem does so for a transfer in the Jupiter-Europa circular restricted three-body problem, where the MCMC approach completes a partial Pareto front. The second problem demonstrates how a dense and superior Pareto front can be generated by the MCMC self-supervised fine-tuning method for a Saturn-Titan transfer starting from the Jupiter-Europa case versus a separate dedicated global search.",
        "subjects": "Earth and Planetary Astrophysics, Instrumentation and Methods for Astrophysics, Machine Learning, Systems and Control, Optimization and Control",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.663988"
    },
    {
        "index": "#127",
        "title": "Adaptive randomized pivoting and volume sampling",
        "link": "/arxiv/2510.02513",
        "arxiv_id": "2510.02513",
        "authors": "Ethan N. Epperly",
        "summary": "Adaptive randomized pivoting (ARP) is a recently proposed and highly effective algorithm for column subset selection. This paper reinterprets the ARP algorithm by drawing connections to the volume sampling distribution and active learning algorithms for linear regression. As consequences, this paper presents new analysis for the ARP algorithm and faster implementations using rejection sampling.",
        "subjects": "Machine Learning, Data Structures and Algorithms, Machine Learning, Numerical Analysis, Computation",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.664558"
    },
    {
        "index": "#128",
        "title": "Beyond Linear Diffusions: Improved Representations for Rare Conditional Generative Modeling",
        "link": "/arxiv/2510.02499",
        "arxiv_id": "2510.02499",
        "authors": "Kulunu Dharmakeerthi, Yousef El-Laham, Henry H. Wong, Vamsi K. Potluru, Changhong He, Taosong He",
        "summary": "Diffusion models have emerged as powerful generative frameworks with widespread applications across machine learning and artificial intelligence systems. While current research has predominantly focused on linear diffusions, these approaches can face significant challenges when modeling a conditional distribution, $P(Y|X=x)$, when $P(X=x)$ is small. In these regions, few samples, if any, are available for training, thus modeling the corresponding conditional density may be difficult. Recognizing this, we show it is possible to adapt the data representation and forward scheme so that the sample complexity of learning a score-based generative model is small in low probability regions of the conditioning space. Drawing inspiration from conditional extreme value theory we characterize this method precisely in the special case in the tail regions of the conditioning variable, $X$. We show how diffusion with a data-driven choice of nonlinear drift term is best suited to model tail events under an appropriate representation of the data. Through empirical validation on two synthetic datasets and a real-world financial dataset, we demonstrate that our tail-adaptive approach significantly outperforms standard diffusion models in accurately capturing response distributions at the extreme tail conditions.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.664854"
    },
    {
        "index": "#129",
        "title": "Safe and Efficient In-Context Learning via Risk Control",
        "link": "/arxiv/2510.02480",
        "arxiv_id": "2510.02480",
        "authors": "Andrea Wynn, Metod Jazbec, Charith Peris, Rinat Khaziev, Anqi Liu, Daniel Khashabi, Eric Nalisnick",
        "summary": "Large language models (LLMs) demonstrate a remarkable ability to learn new tasks from a few in-context examples. However, this flexibility introduces safety concerns: LLMs can be influenced by incorrect or malicious demonstrations -- for example, if an adversary tampers with or injects harmful examples without a human supervisor noticing. This motivates principled designs in which the system itself includes built-in mechanisms to guard against such attacks. We propose a novel approach to limit the degree to which harmful demonstrations can degrade model performance. First, we define a baseline ``safe'' behavior for the model -- the model's performance given no in-context demonstrations (zero-shot). Next, we apply distribution-free risk control (DFRC) to control the extent to which in-context samples can decay performance below zero-shot. We achieve this by leveraging dynamic early exit prediction, ignoring later attention heads that attend the most to the unsafe inputs. Finally, we propose modifications to DFRC that allow it to both control risk for harmful inputs \\textit{and} leverage performance and efficiency gains on helpful inputs. We present both theoretical and empirical results showing that our approach can effectively control risk for harmful in-context demonstrations while simultaneously achieving substantial computational efficiency gains with helpful demonstrations.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.665357"
    },
    {
        "index": "#130",
        "title": "Heterogeneous Graph Representation of Stiffened Panels with Non-Uniform Boundary Conditions and Loads",
        "link": "/arxiv/2510.02472",
        "arxiv_id": "2510.02472",
        "authors": "Yuecheng Cai, Jasmin Jelovica",
        "summary": "Surrogate models are essential in structural analysis and optimization. We propose a heterogeneous graph representation of stiffened panels that accounts for geometrical variability, non-uniform boundary conditions, and diverse loading scenarios, using heterogeneous graph neural networks (HGNNs). The structure is partitioned into multiple structural units, such as stiffeners and the plates between them, with each unit represented by three distinct node types: geometry, boundary, and loading nodes. Edge heterogeneity is introduced by incorporating local orientations and spatial relationships of the connecting nodes. Several heterogeneous graph representations, each with varying degrees of heterogeneity, are proposed and analyzed. These representations are implemented into a heterogeneous graph transformer (HGT) to predict von Mises stress and displacement fields across stiffened panels, based on loading and degrees of freedom at their boundaries. To assess the efficacy of our approach, we conducted numerical tests on panels subjected to patch loads and box beams composed of stiffened panels under various loading conditions. The heterogeneous graph representation was compared with a homogeneous counterpart, demonstrating superior performance. Additionally, an ablation analysis was performed to evaluate the impact of graph heterogeneity on HGT performance. The results show strong predictive accuracy for both displacement and von Mises stress, effectively capturing structural behavior patterns and maximum values.",
        "subjects": "Computational Engineering, Finance, and Science, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.665781"
    },
    {
        "index": "#131",
        "title": "Predictive inference for time series: why is split conformal effective despite temporal dependence?",
        "link": "/arxiv/2510.02471",
        "arxiv_id": "2510.02471",
        "authors": "Rina Foygel Barber, Ashwin Pananjady",
        "summary": "We consider the problem of uncertainty quantification for prediction in a time series: if we use past data to forecast the next time point, can we provide valid prediction intervals around our forecasts? To avoid placing distributional assumptions on the data, in recent years the conformal prediction method has been a popular approach for predictive inference, since it provides distribution-free coverage for any iid or exchangeable data distribution. However, in the time series setting, the strong empirical performance of conformal prediction methods is not well understood, since even short-range temporal dependence is a strong violation of the exchangeability assumption. Using predictors with \"memory\" -- i.e., predictors that utilize past observations, such as autoregressive models -- further exacerbates this problem. In this work, we examine the theoretical properties of split conformal prediction in the time series setting, including the case where predictors may have memory. Our results bound the loss of coverage of these methods in terms of a new \"switch coefficient\", measuring the extent to which temporal dependence within the time series creates violations of exchangeability. Our characterization of the coverage probability is sharp over the class of stationary, $\\beta$-mixing processes. Along the way, we introduce tools that may prove useful in analyzing other predictive inference methods for dependent data.",
        "subjects": "Machine Learning, Machine Learning, Statistics Theory",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.671350"
    },
    {
        "index": "#133",
        "title": "Adaptive Deception Framework with Behavioral Analysis for Enhanced Cybersecurity Defense",
        "link": "/arxiv/2510.02424",
        "arxiv_id": "2510.02424",
        "authors": "Basil Abdullah AL-Zahrani",
        "summary": "This paper presents CADL (Cognitive-Adaptive Deception Layer), an adaptive deception framework achieving 99.88% detection rate with 0.13% false positive rate on the CICIDS2017 dataset. The framework employs ensemble machine learning (Random Forest, XGBoost, Neural Networks) combined with behavioral profiling to identify and adapt responses to network intrusions. Through a coordinated signal bus architecture, security components share real-time intelligence, enabling collective decision-making. The system profiles attackers based on temporal patterns and deploys customized deception strategies across five escalation levels. Evaluation on 50,000 CICIDS2017 test samples demonstrates that CADL significantly outperforms traditional intrusion detection systems (Snort: 71.2%, Suricata: 68.5%) while maintaining production-ready false positive rates. The framework's behavioral analysis achieves 89% accuracy in classifying attacker profiles. We provide open-source implementation and transparent performance metrics, offering an accessible alternative to commercial deception platforms costing $150-400 per host annually.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.672240"
    },
    {
        "index": "#134",
        "title": "Higher-arity PAC learning, VC dimension and packing lemma",
        "link": "/arxiv/2510.02420",
        "arxiv_id": "2510.02420",
        "authors": "Artem Chernikov, Henry Towsner",
        "summary": "The aim of this note is to overview some of our work in Chernikov, Towsner'20 (arXiv:2010.00726) developing higher arity VC theory (VC$_n$ dimension), including a generalization of Haussler packing lemma, and an associated tame (slice-wise) hypergraph regularity lemma; and to demonstrate that it characterizes higher arity PAC learning (PAC$_n$ learning) in $n$-fold product spaces with respect to product measures introduced by Kobayashi, Kuriyama and Takeuchi'15. We also point out how some of the recent results in arXiv:2402.14294, arXiv:2505.15688, arXiv:2509.20404 follow from our work in arXiv:2010.00726.",
        "subjects": "Machine Learning, Discrete Mathematics, Machine Learning, Combinatorics, Logic, Statistics Theory",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.672727"
    },
    {
        "index": "#135",
        "title": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks",
        "link": "/arxiv/2510.02418",
        "arxiv_id": "2510.02418",
        "authors": "Sagnik Anupam, Davis Brown, Shuo Li, Eric Wong, Hamed Hassani, Osbert Bastani",
        "summary": "LLM web agents now browse and take actions on the open web, yet current agent evaluations are constrained to sandboxed environments or artificial tasks. We introduce BrowserArena, a live open-web agent evaluation platform that collects user-submitted tasks, runs Arena-style head-to-head comparisons, and uses step-level human feedback to surface failure modes. Collecting and analyzing step-level annotations on the agent traces, we identify three consistent failure modes: captcha resolution, pop-up banner removal, and direct navigation to URLs. By constructing targeted datasets to further study these tasks, we discover variations in how different language models navigate these failure modes. We find, for example, that o4-mini deploys a wider variety of strategies to circumvent captcha resolution than other models and DeepSeek-R1 consistently misleads users about captcha resolution. Our findings surface both the diversity and brittleness of current web agents. More broadly, our benchmarking methodology provides an approach to evaluating and understanding web agent failure modes at scale.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.673207"
    },
    {
        "index": "#136",
        "title": "NEURODNAAI: Neural pipeline approaches for the advancing dna-based information storage as a sustainable digital medium using deep learning framework",
        "link": "/arxiv/2510.02417",
        "arxiv_id": "2510.02417",
        "authors": "Rakesh Thakur, Lavanya Singh, Yashika, Manomay Bundawala, Aruna Kumar",
        "summary": "DNA is a promising medium for digital information storage for its exceptional density and durability. While prior studies advanced coding theory, workflow design, and simulation tools, challenges such as synthesis costs, sequencing errors, and biological constraints (GC-content imbalance, homopolymers) limit practical deployment. To address this, our framework draws from quantum parallelism concepts to enhance encoding diversity and resilience, integrating biologically informed constraints with deep learning to enhance error mitigation in DNA storage. NeuroDNAAI encodes binary data streams into symbolic DNA sequences, transmits them through a noisy channel with substitutions, insertions, and deletions, and reconstructs them with high fidelity. Our results show that traditional prompting or rule-based schemes fail to adapt effectively to realistic noise, whereas NeuroDNAAI achieves superior accuracy. Experiments on benchmark datasets demonstrate low bit error rates for both text and images. By unifying theory, workflow, and simulation into one pipeline, NeuroDNAAI enables scalable, biologically valid archival DNA storage",
        "subjects": "Emerging Technologies, Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.673678"
    },
    {
        "index": "#137",
        "title": "The Equilibrium Response of Atmospheric Machine-Learning Models to Uniform Sea Surface Temperature Warming",
        "link": "/arxiv/2510.02415",
        "arxiv_id": "2510.02415",
        "authors": "Bosong Zhang, Timothy M. Merlis",
        "summary": "Machine learning models for the global atmosphere that are capable of producing stable, multi-year simulations of Earth's climate have recently been developed. However, the ability of these ML models to generalize beyond the training distribution remains an open question. In this study, we evaluate the climate response of several state-of-the-art ML models (ACE2-ERA5, NeuralGCM, and cBottle) to a uniform sea surface temperature warming, a widely used benchmark for evaluating climate change. We assess each ML model's performance relative to a physics-based general circulation model (GFDL's AM4) across key diagnostics, including surface air temperature, precipitation, temperature and wind profiles, and top-of-the-atmosphere radiation. While the ML models reproduce key aspects of the physical model response, particularly the response of precipitation, some exhibit notable departures from robust physical responses, including radiative responses and land region warming. Our results highlight the promise and current limitations of ML models for climate change applications and suggest that further improvements are needed for robust out-of-sample generalization.",
        "subjects": "Atmospheric and Oceanic Physics, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.674107"
    },
    {
        "index": "#138",
        "title": "Linear RNNs for autoregressive generation of long music samples",
        "link": "/arxiv/2510.02401",
        "arxiv_id": "2510.02401",
        "authors": "Konrad Szewczyk, Daniel Gallo Fernández, James Townsend",
        "summary": "Directly learning to generate audio waveforms in an autoregressive manner is a challenging task, due to the length of the raw sequences and the existence of important structure on many different timescales. Traditional approaches based on recurrent neural networks, as well as causal convolutions and self-attention, have only had limited success on this task. However, recent work has shown that deep state space models, also referred to as linear RNNs, can be highly efficient in this context. In this work, we push the boundaries of linear RNNs applied to raw audio modeling, investigating the effects of different architectural choices and using context-parallelism to enable training on sequences up to one minute (1M tokens) in length. We present a model, HarmonicRNN, which attains state of the art log-likelihoods and perceptual metrics on small-scale datasets.",
        "subjects": "Sound, Artificial Intelligence, Machine Learning, Audio and Speech Processing",
        "date": "2025-10-01",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.674571"
    },
    {
        "index": "#139",
        "title": "LLM-Generated Samples for Android Malware Detection",
        "link": "/arxiv/2510.02391",
        "arxiv_id": "2510.02391",
        "authors": "Nik Rollinson, Nikolaos Polatidis",
        "summary": "Android malware continues to evolve through obfuscation and polymorphism, posing challenges for both signature-based defenses and machine learning models trained on limited and imbalanced datasets. Synthetic data has been proposed as a remedy for scarcity, yet the role of large language models (LLMs) in generating effective malware data for detection tasks remains underexplored. In this study, we fine-tune GPT-4.1-mini to produce structured records for three malware families: BankBot, Locker/SLocker, and Airpush/StopSMS, using the KronoDroid dataset. After addressing generation inconsistencies with prompt engineering and post-processing, we evaluate multiple classifiers under three settings: training with real data only, real-plus-synthetic data, and synthetic data alone. Results show that real-only training achieves near perfect detection, while augmentation with synthetic data preserves high performance with only minor degradations. In contrast, synthetic-only training produces mixed outcomes, with effectiveness varying across malware families and fine-tuning strategies. These findings suggest that LLM-generated malware can enhance scarce datasets without compromising detection accuracy, but remains insufficient as a standalone training source.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.674992"
    },
    {
        "index": "#140",
        "title": "From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization",
        "link": "/arxiv/2510.02389",
        "arxiv_id": "2510.02389",
        "authors": "Haoran Xi, Minghao Shao, Brendan Dolan-Gavitt, Muhammad Shafique, Ramesh Karri",
        "summary": "Large language models show promise for vulnerability discovery, yet prevailing methods inspect code in isolation, struggle with long contexts, and focus on coarse function- or file-level detections - offering limited actionable guidance to engineers who need precise line-level localization and targeted patches in real-world software development. We present T2L-Agent (Trace-to-Line Agent), a project-level, end-to-end framework that plans its own analysis and progressively narrows scope from modules to exact vulnerable lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer (ATA) that fuses runtime evidence - crash points, stack traces, and coverage deltas - with AST-based code chunking, enabling iterative refinement beyond single pass predictions and translating symptoms into actionable, line-level diagnoses. To benchmark line-level vulnerability discovery, we introduce T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash families and real-world projects. T2L-ARVO is specifically designed to support both coarse-grained detection and fine-grained localization, enabling rigorous evaluation of systems that aim to move beyond file-level predictions. On T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level localization, substantially outperforming baselines. Together, the framework and benchmark push LLM-based vulnerability detection from coarse identification toward deployable, robust, precision diagnostics that reduce noise and accelerate patching in open-source software workflows.",
        "subjects": "Software Engineering, Cryptography and Security, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.675469"
    },
    {
        "index": "#141",
        "title": "CWM: An Open-Weights LLM for Research on Code Generation with World Models",
        "link": "/arxiv/2510.02387",
        "arxiv_id": "2510.02387",
        "authors": "FAIR CodeGen team, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, David Zhang, Kunhao Zheng, Jordi Armengol-Estapé, Pedram Bashiri, Maximilian Beck, Pierre Chambon, Abhishek Charnalia, Chris Cummins, Juliette Decugis, Zacharias V. Fisches, François Fleuret, Fabian Gloeckle, Alex Gu, Michael Hassid, Daniel Haziza, Badr Youbi Idrissi, Christian Keller, Rahul Kindi, Hugh Leather, Gallil Maimon, Aram Markosyan, Francisco Massa, Pierre-Emmanuel Mazaré, Vegard Mella, Naila Murray, Keyur Muzumdar, Peter O'Hearn, Matteo Pagliardini, Dmitrii Pedchenko, Tal Remez, Volker Seeker, Marco Selvi, Oren Sultan, Sida Wang, Luca Wehrstedt, Ori Yoran, Lingming Zhang, Taco Cohen, Yossi Adi, Gabriel Synnaeve",
        "summary": "We release Code World Model (CWM), a 32-billion-parameter open-weights LLM, to advance research on code generation with world models. To improve code understanding beyond what can be learned from training on static code alone, we mid-train CWM on a large amount of observation-action trajectories from Python interpreter and agentic Docker environments, and perform extensive multi-task reasoning RL in verifiable coding, math, and multi-turn software engineering environments. With CWM, we provide a strong testbed for researchers to explore the opportunities world modeling affords for improving code generation with reasoning and planning in computational environments. We present first steps of how world models can benefit agentic coding, enable step-by-step simulation of Python code execution, and show early results of how reasoning can benefit from the latter. CWM is a dense, decoder-only LLM trained with a context size of up to 131k tokens. Independent of its world modeling capabilities, CWM offers strong performance on general coding and math tasks: it reaches pass@1 scores of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further research on code world modeling, we release model checkpoints after mid-training, SFT, and RL.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.681685"
    },
    {
        "index": "#142",
        "title": "On The Fragility of Benchmark Contamination Detection in Reasoning Models",
        "link": "/arxiv/2510.02386",
        "arxiv_id": "2510.02386",
        "authors": "Han Wang, Haoyu Li, Brian Ko, Huan Zhang",
        "summary": "Leaderboards for LRMs have turned evaluation into a competition, incentivizing developers to optimize directly on benchmark suites. A shortcut to achieving higher rankings is to incorporate evaluation benchmarks into the training data, thereby yielding inflated performance, known as benchmark contamination. Surprisingly, our studies find that evading contamination detections for LRMs is alarmingly easy. We focus on the two scenarios where contamination may occur in practice: (I) when the base model evolves into LRM via SFT and RL, we find that contamination during SFT can be originally identified by contamination detection methods. Yet, even a brief GRPO training can markedly conceal contamination signals that most detection methods rely on. Further empirical experiments and theoretical analysis indicate that PPO style importance sampling and clipping objectives are the root cause of this detection concealment, indicating that a broad class of RL methods may inherently exhibit similar concealment capability; (II) when SFT contamination with CoT is applied to advanced LRMs as the final stage, most contamination detection methods perform near random guesses. Without exposure to non-members, contaminated LRMs would still have more confidence when responding to those unseen samples that share similar distributions to the training set, and thus, evade existing memorization-based detection methods. Together, our findings reveal the unique vulnerability of LRMs evaluations: Model developers could easily contaminate LRMs to achieve inflated leaderboards performance while leaving minimal traces of contamination, thereby strongly undermining the fairness of evaluation and threatening the integrity of public leaderboards. This underscores the urgent need for advanced contamination detection methods and trustworthy evaluation protocols tailored to LRMs.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.682154"
    },
    {
        "index": "#145",
        "title": "An Encoder-Decoder Network for Beamforming over Sparse Large-Scale MIMO Channels",
        "link": "/arxiv/2510.02355",
        "arxiv_id": "2510.02355",
        "authors": "Yubo Zhang, Jeremy Johnston, Xiaodong Wang",
        "summary": "We develop an end-to-end deep learning framework for downlink beamforming in large-scale sparse MIMO channels. The core is a deep EDN architecture with three modules: (i) an encoder NN, deployed at each user end, that compresses estimated downlink channels into low-dimensional latent vectors. The latent vector from each user is compressed and then fed back to the BS. (ii) a beamformer decoder NN at the BS that maps recovered latent vectors to beamformers, and (iii) a channel decoder NN at the BS that reconstructs downlink channels from recovered latent vectors to further refine the beamformers. The training of EDN leverages two key strategies: (a) semi-amortized learning, where the beamformer decoder NN contains an analytical gradient ascent during both training and inference stages, and (b) knowledge distillation, where the loss function consists of a supervised term and an unsupervised term, and starting from supervised training with MMSE beamformers, over the epochs, the model training gradually shifts toward unsupervised using the sum-rate objective. The proposed EDN beamforming framework is extended to both far-field and near-field hybrid beamforming scenarios. Extensive simulations validate its effectiveness under diverse network and channel conditions.",
        "subjects": "Systems and Control, Information Theory, Machine Learning, Signal Processing",
        "date": "2025-09-27",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.683586"
    },
    {
        "index": "#154",
        "title": "EEFSUVA: A New Mathematical Olympiad Benchmark",
        "link": "/arxiv/2510.01227",
        "arxiv_id": "2510.01227",
        "authors": "Nicole N Khatibi, Daniil A. Radamovich, Michael P. Brenner",
        "summary": "Recent breakthroughs have spurred claims that large language models (LLMs) match gold medal Olympiad to graduate level proficiency on mathematics benchmarks. In this work, we examine these claims in detail and assess the extent to which current benchmarks capture genuine LLM mathematical reasoning. The composition of these benchmarks, primarily drawing from the International Mathematics Olympiad (IMO) and related competitions, may overstate models reasoning ability due to potential data contamination and a narrow focus on familiar problem types. To enable a more holistic assessment of mathematical understanding, we introduce EEFSUVA, a novel benchmark curated from under circulated regional and national Olympiads of Eastern Europe and the countries from the former Soviet Union. These contests feature problems of comparable difficulty to the IMO and are renowned for demanding nonstandard problem-solving techniques, yet their problems are far less prevalent in online corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a notable performance decline on EEFSUVA relative to other Olympiad-style benchmarks. These findings also suggest the potential importance of broader evaluation datasets for a fuller assessment of mathematical reasoning and for guiding future model development.",
        "subjects": "Computation and Language, History and Overview",
        "date": "2025-09-23",
        "category": "cs.LG",
        "crawl_time": "2025-10-07T01:04:27.692931"
    }
]