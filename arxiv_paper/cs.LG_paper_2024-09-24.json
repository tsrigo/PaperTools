[
    {
        "index": "#2",
        "title": "Peer-to-Peer Learning Dynamics of Wide Neural Networks",
        "link": "/arxiv/2409.15267",
        "arxiv_id": "2409.15267",
        "authors": "Shreyas Chaudhari, Srinivasa Pranav, Emile Anand, Jos√© M. F. Moura",
        "summary": "Peer-to-peer learning is an increasingly popular framework that enables beyond-5G distributed edge devices to collaboratively train deep neural networks in a privacy-preserving manner without the aid of a central server. Neural network training algorithms for emerging environments, e.g., smart cities, have many design considerations that are difficult to tune in deployment settings -- such as neural network architectures and hyperparameters. This presents a critical need for characterizing the training dynamics of distributed optimization algorithms used to train highly nonconvex neural networks in peer-to-peer learning environments. In this work, we provide an explicit, non-asymptotic characterization of the learning dynamics of wide neural networks trained using popular distributed gradient descent (DGD) algorithms. Our results leverage both recent advancements in neural tangent kernel (NTK) theory and extensive previous work on distributed learning and consensus. We validate our analytical results by accurately predicting the parameter and error dynamics of wide neural networks trained for classification tasks.",
        "subjects": "Machine Learning, Systems and Control",
        "date": "2024-09-23 17:57:58 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.813826"
    },
    {
        "index": "#6",
        "title": "MotifDisco: Motif Causal Discovery For Time Series Motifs",
        "link": "/arxiv/2409.15219",
        "arxiv_id": "2409.15219",
        "authors": "Josephine Lamp, Mark Derdzinski, Christopher Hannemann, Sam Hatfield, Joost van der Linden",
        "summary": "Many time series, particularly health data streams, can be best understood as a sequence of phenomenon or events, which we call motifs. A time series motif is a short trace segment which may implicitly capture an underlying phenomenon within the time series. Specifically, we focus on glucose traces collected from continuous glucose monitors (CGMs), which inherently contain motifs representing underlying human behaviors such as eating and exercise. The ability to identify and quantify causal relationships amongst motifs can provide a mechanism to better understand and represent these patterns, useful for improving deep learning and generative models and for advanced technology development (e.g., personalized coaching and artificial insulin delivery systems). However, no previous work has developed causal discovery methods for time series motifs. Therefore, in this paper we develop MotifDisco (motif disco-very of causality), a novel causal discovery framework to learn causal relations amongst motifs from time series traces. We formalize a notion of Motif Causality (MC), inspired from Granger Causality and Transfer Entropy, and develop a Graph Neural Network-based framework that learns causality between motifs by solving an unsupervised link prediction problem. We also integrate MC with three model use cases of forecasting, anomaly detection and clustering, to showcase the use of MC as a building block for other downstream tasks. Finally, we evaluate our framework and find that Motif Causality provides a significant performance improvement in all use cases.",
        "subjects": "Machine Learning",
        "date": "2024-09-23 17:08:37 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.814705"
    },
    {
        "index": "#8",
        "title": "Enabling Tensor Decomposition for Time-Series Classification via A Simple Pseudo-Laplacian Contrast",
        "link": "/arxiv/2409.15200",
        "arxiv_id": "2409.15200",
        "authors": "Man Li, Ziyue Li, Lijun Sun, Fugee Tsung",
        "summary": "Tensor decomposition has emerged as a prominent technique to learn low-dimensional representation under the supervision of reconstruction error, primarily benefiting data inference tasks like completion and imputation, but not classification task. We argue that the non-uniqueness and rotation invariance of tensor decomposition allow us to identify the directions with largest class-variability and simple graph Laplacian can effectively achieve this objective. Therefore we propose a novel Pseudo Laplacian Contrast (PLC) tensor decomposition framework, which integrates the data augmentation and cross-view Laplacian to enable the extraction of class-aware representations while effectively capturing the intrinsic low-rank structure within reconstruction constraint. An unsupervised alternative optimization algorithm is further developed to iteratively estimate the pseudo graph and minimize the loss using Alternating Least Square (ALS). Extensive experimental results on various datasets demonstrate the effectiveness of our approach.",
        "subjects": "Machine Learning",
        "date": "2024-09-23 16:48:13 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.815099"
    },
    {
        "index": "#9",
        "title": "Data-driven model discovery with Kolmogorov-Arnold networks",
        "link": "/arxiv/2409.15167",
        "arxiv_id": "2409.15167",
        "authors": "Mohammadamin Moradi, Shirin Panahi, Erik M. Bollt, Ying-Cheng Lai",
        "summary": "Data-driven model discovery of complex dynamical systems is typically done using sparse optimization, but it has a fundamental limitation: sparsity in that the underlying governing equations of the system contain only a small number of elementary mathematical terms. Examples where sparse optimization fails abound, such as the classic Ikeda or optical-cavity map in nonlinear dynamics and a large variety of ecosystems. Exploiting the recently articulated Kolmogorov-Arnold networks, we develop a general model-discovery framework for any dynamical systems including those that do not satisfy the sparsity condition. In particular, we demonstrate non-uniqueness in that a large number of approximate models of the system can be found which generate the same invariant set with the correct statistics such as the Lyapunov exponents and Kullback-Leibler divergence. An analogy to shadowing of numerical trajectories in chaotic systems is pointed out.",
        "subjects": "Machine Learning, Dynamical Systems, Chaotic Dynamics, Data Analysis, Statistics and Probability",
        "date": "2024-09-23 16:22:07 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.815336"
    },
    {
        "index": "#10",
        "title": "A Gated Residual Kolmogorov-Arnold Networks for Mixtures of Experts",
        "link": "/arxiv/2409.15161",
        "arxiv_id": "2409.15161",
        "authors": "Hugo Inzirillo, Remi Genet",
        "summary": "This paper introduces KAMoE, a novel Mixture of Experts (MoE) framework based on Gated Residual Kolmogorov-Arnold Networks (GRKAN). We propose GRKAN as an alternative to the traditional gating function, aiming to enhance efficiency and interpretability in MoE modeling. Through extensive experiments on digital asset markets and real estate valuation, we demonstrate that KAMoE consistently outperforms traditional MoE architectures across various tasks and model types. Our results show that GRKAN exhibits superior performance compared to standard Gating Residual Networks, particularly in LSTM-based models for sequential tasks. We also provide insights into the trade-offs between model complexity and performance gains in MoE and KAMoE architectures.",
        "subjects": "Machine Learning, Neural and Evolutionary Computing",
        "date": "2024-09-23 16:11:43 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.815512"
    },
    {
        "index": "#11",
        "title": "Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling",
        "link": "/arxiv/2409.15156",
        "arxiv_id": "2409.15156",
        "authors": "Lechao Xiao",
        "summary": "The remarkable success of large language pretraining and the discovery of scaling laws signify a paradigm shift in machine learning. Notably, the primary objective has evolved from minimizing generalization error to reducing approximation error, and the most effective strategy has transitioned from regularization (in a broad sense) to scaling up models. This raises a critical question: Do the established principles that proved successful in the generalization-centric era remain valid in this new era of scaling? This paper examines several influential regularization-based principles that may no longer hold true in the scaling-centric, large language model (LLM) era. These principles include explicit L2 regularization and implicit regularization through small batch sizes and large learning rates. Additionally, we identify a new phenomenon termed ``scaling law crossover,'' where two scaling curves intersect at a certain scale, implying that methods effective at smaller scales may not generalize to larger ones. Together, these observations highlight two fundamental questions within this new paradigm: $\\bullet$ Guiding Principles for Scaling: If regularization is no longer the primary guiding principle for model design, what new principles are emerging to guide scaling? $\\bullet$ Model Comparison at Scale: How to reliably and effectively compare models at the scale where only a single experiment is feasible?",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2024-09-23 16:04:03 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.815685"
    },
    {
        "index": "#12",
        "title": "Designing an Interpretable Interface for Contextual Bandits",
        "link": "/arxiv/2409.15143",
        "arxiv_id": "2409.15143",
        "authors": "Andrew Maher, Matia Gobbo, Lancelot Lachartre, Subash Prabanantham, Rowan Swiers, Puli Liyanagama",
        "summary": "Contextual bandits have become an increasingly popular solution for personalized recommender systems. Despite their growing use, the interpretability of these systems remains a significant challenge, particularly for the often non-expert operators tasked with ensuring their optimal performance. In this paper, we address this challenge by designing a new interface to explain to domain experts the underlying behaviour of a bandit. Central is a metric we term \"value gain\", a measure derived from off-policy evaluation to quantify the real-world impact of sub-components within a bandit. We conduct a qualitative user study to evaluate the effectiveness of our interface. Our findings suggest that by carefully balancing technical rigour with accessible presentation, it is possible to empower non-experts to manage complex machine learning systems. We conclude by outlining guiding principles that other researchers should consider when building similar such interfaces in future.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2024-09-23 15:47:44 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.815932"
    },
    {
        "index": "#13",
        "title": "The Number of Trials Matters in Infinite-Horizon General-Utility Markov Decision Processes",
        "link": "/arxiv/2409.15128",
        "arxiv_id": "2409.15128",
        "authors": "Pedro P. Santos, Alberto Sardinha, Francisco S. Melo",
        "summary": "The general-utility Markov decision processes (GUMDPs) framework generalizes the MDPs framework by considering objective functions that depend on the frequency of visitation of state-action pairs induced by a given policy. In this work, we contribute with the first analysis on the impact of the number of trials, i.e., the number of randomly sampled trajectories, in infinite-horizon GUMDPs. We show that, as opposed to standard MDPs, the number of trials plays a key-role in infinite-horizon GUMDPs and the expected performance of a given policy depends, in general, on the number of trials. We consider both discounted and average GUMDPs, where the objective function depends, respectively, on discounted and average frequencies of visitation of state-action pairs. First, we study policy evaluation under discounted GUMDPs, proving lower and upper bounds on the mismatch between the finite and infinite trials formulations for GUMDPs. Second, we address average GUMDPs, studying how different classes of GUMDPs impact the mismatch between the finite and infinite trials formulations. Third, we provide a set of empirical results to support our claims, highlighting how the number of trajectories and the structure of the underlying GUMDP influence policy evaluation.",
        "subjects": "Machine Learning",
        "date": "2024-09-23 15:34:45 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.816116"
    },
    {
        "index": "#16",
        "title": "AdapFair: Ensuring Continuous Fairness for Machine Learning Operations",
        "link": "/arxiv/2409.15088",
        "arxiv_id": "2409.15088",
        "authors": "Yinghui Huang, Zihao Tang, Xiangyu Chang",
        "summary": "The biases and discrimination of machine learning algorithms have attracted significant attention, leading to the development of various algorithms tailored to specific contexts. However, these solutions often fall short of addressing fairness issues inherent in machine learning operations. In this paper, we present a debiasing framework designed to find an optimal fair transformation of input data that maximally preserves data predictability. A distinctive feature of our approach is its flexibility and efficiency. It can be integrated with any downstream black-box classifiers, providing continuous fairness guarantees with minimal retraining efforts, even in the face of frequent data drifts, evolving fairness requirements, and batches of similar tasks. To achieve this, we leverage the normalizing flows to enable efficient, information-preserving data transformation, ensuring that no critical information is lost during the debiasing process. Additionally, we incorporate the Wasserstein distance as the unfairness measure to guide the optimization of data transformations. Finally, we introduce an efficient optimization algorithm with closed-formed gradient computations, making our framework scalable and suitable for dynamic, real-world environments.",
        "subjects": "Machine Learning, Computers and Society",
        "date": "2024-09-23 15:01:47 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.816680"
    },
    {
        "index": "#17",
        "title": "SHFL: Secure Hierarchical Federated Learning Framework for Edge Networks",
        "link": "/arxiv/2409.15067",
        "arxiv_id": "2409.15067",
        "authors": "Omid Tavallaie, Kanchana Thilakarathna, Suranga Seneviratne, Aruna Seneviratne, Albert Y. Zomaya",
        "summary": "Federated Learning (FL) is a distributed machine learning paradigm designed for privacy-sensitive applications that run on resource-constrained devices with non-Identically and Independently Distributed (IID) data. Traditional FL frameworks adopt the client-server model with a single-level aggregation (AGR) process, where the server builds the global model by aggregating all trained local models received from client devices. However, this conventional approach encounters challenges, including susceptibility to model/data poisoning attacks. In recent years, advancements in the Internet of Things (IoT) and edge computing have enabled the development of hierarchical FL systems with a two-level AGR process running at edge and cloud servers. In this paper, we propose a Secure Hierarchical FL (SHFL) framework to address poisoning attacks in hierarchical edge networks. By aggregating trained models at the edge, SHFL employs two novel methods to address model/data poisoning attacks in the presence of client adversaries: 1) a client selection algorithm running at the edge for choosing IoT devices to participate in training, and 2) a model AGR method designed based on convex optimization theory to reduce the impact of edge models from networks with adversaries in the process of computing the global model (at the cloud level). The evaluation results reveal that compared to state-of-the-art methods, SHFL significantly increases the maximum accuracy achieved by the global model in the presence of client adversaries applying model/data poisoning attacks.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2024-09-23 14:38:20 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.816877"
    },
    {
        "index": "#18",
        "title": "Anomaly Detection from a Tensor Train Perspective",
        "link": "/arxiv/2409.15030",
        "arxiv_id": "2409.15030",
        "authors": "Alejandro Mata Ali, Aitor Moreno Fdez. de Leceta, Jorge L√≥pez Rubio",
        "summary": "We present a series of algorithms in tensor networks for anomaly detection in datasets, by using data compression in a Tensor Train representation. These algorithms consist of preserving the structure of normal data in compression and deleting the structure of anomalous data. The algorithms can be applied to any tensor network representation. We test the effectiveness of the methods with digits and Olivetti faces datasets and a cybersecurity dataset to determine cyber-attacks.",
        "subjects": "Machine Learning, Cryptography and Security, Emerging Technologies, Information Theory, Quantum Physics",
        "date": "2024-09-23 13:55:58 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.817086"
    },
    {
        "index": "#20",
        "title": "Evaluating Synthetic Activations composed of SAE Latents in GPT-2",
        "link": "/arxiv/2409.15019",
        "arxiv_id": "2409.15019",
        "authors": "Giorgi Giglemiani, Nora Petrova, Chatrik Singh Mangat, Jett Janiak, Stefan Heimersheim",
        "summary": "Sparse Auto-Encoders (SAEs) are commonly employed in mechanistic interpretability to decompose the residual stream into monosemantic SAE latents. Recent work demonstrates that perturbing a model's activations at an early layer results in a step-function-like change in the model's final layer activations. Furthermore, the model's sensitivity to this perturbation differs between model-generated (real) activations and random activations. In our study, we assess model sensitivity in order to compare real activations to synthetic activations composed of SAE latents. Our findings indicate that synthetic activations closely resemble real activations when we control for the sparsity and cosine similarity of the constituent SAE latents. This suggests that real activations cannot be explained by a simple \"bag of SAE latents\" lacking internal structure, and instead suggests that SAE latents possess significant geometric and statistical properties. Notably, we observe that our synthetic activations exhibit less pronounced activation plateaus compared to those typically surrounding real activations.",
        "subjects": "Machine Learning",
        "date": "2024-09-23 13:46:38 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.817500"
    },
    {
        "index": "#22",
        "title": "Adaptive Learning on User Segmentation: Universal to Specific Representation via Bipartite Neural Interaction",
        "link": "/arxiv/2409.14945",
        "arxiv_id": "2409.14945",
        "authors": "Xiaoyu Tan, Yongxin Deng, Chao Qu, Siqiao Xue, Xiaoming Shi, James Zhang, Xihe Qiu",
        "summary": "Recently, models for user representation learning have been widely applied in click-through-rate (CTR) and conversion-rate (CVR) prediction. Usually, the model learns a universal user representation as the input for subsequent scenario-specific models. However, in numerous industrial applications (e.g., recommendation and marketing), the business always operates such applications as various online activities among different user segmentation. These segmentation are always created by domain experts. Due to the difference in user distribution (i.e., user segmentation) and business objectives in subsequent tasks, learning solely on universal representation may lead to detrimental effects on both model performance and robustness. In this paper, we propose a novel learning framework that can first learn general universal user representation through information bottleneck. Then, merge and learn a segmentation-specific or a task-specific representation through neural interaction. We design the interactive learning process by leveraging a bipartite graph architecture to model the representation learning and merging between contextual clusters and each user segmentation. Our proposed method is evaluated in two open-source benchmarks, two offline business datasets, and deployed on two online marketing applications to predict users' CVR. The results demonstrate that our method can achieve superior performance and surpass the baseline methods.",
        "subjects": "Machine Learning, Information Retrieval",
        "date": "2024-09-23 12:02:23 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.817942"
    },
    {
        "index": "#23",
        "title": "FastGL: A GPU-Efficient Framework for Accelerating Sampling-Based GNN Training at Large Scale",
        "link": "/arxiv/2409.14939",
        "arxiv_id": "2409.14939",
        "authors": "Zeyu Zhu, Peisong Wang, Qinghao Hu, Gang Li, Xiaoyao Liang, Jian Cheng",
        "summary": "Graph Neural Networks (GNNs) have shown great superiority on non-Euclidean graph data, achieving ground-breaking performance on various graph-related tasks. As a practical solution to train GNN on large graphs with billions of nodes and edges, the sampling-based training is widely adopted by existing training frameworks. However, through an in-depth analysis, we observe that the efficiency of existing sampling-based training frameworks is still limited due to the key bottlenecks lying in all three phases of sampling-based training, i.e., subgraph sample, memory IO, and computation. To this end, we propose FastGL, a GPU-efficient Framework for accelerating sampling-based training of GNN at Large scale by simultaneously optimizing all above three phases, taking into account both GPU characteristics and graph structure. Specifically, by exploiting the inherent overlap within graph structures, FastGL develops the Match-Reorder strategy to reduce the data traffic, which accelerates the memory IO without incurring any GPU memory overhead. Additionally, FastGL leverages a Memory-Aware computation method, harnessing the GPU memory's hierarchical nature to mitigate irregular data access during computation. FastGL further incorporates the Fused-Map approach aimed at diminishing the synchronization overhead during sampling. Extensive experiments demonstrate that FastGL can achieve an average speedup of 11.8x, 2.2x and 1.5x over the state-of-the-art frameworks PyG, DGL, and GNNLab, respectively.Our code is available at https://github.com/a1bc2def6g/fastgl-ae.",
        "subjects": "Machine Learning, Hardware Architecture, Distributed, Parallel, and Cluster Computing",
        "date": "2024-09-23 11:45:47 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.818159"
    },
    {
        "index": "#24",
        "title": "Kriformer: A Novel Spatiotemporal Kriging Approach Based on Graph Transformers",
        "link": "/arxiv/2409.14906",
        "arxiv_id": "2409.14906",
        "authors": "Renbin Pan, Feng Xiao, Hegui Zhang, Minyu Shen",
        "summary": "Accurately estimating data in sensor-less areas is crucial for understanding system dynamics, such as traffic state estimation and environmental monitoring. This study addresses challenges posed by sparse sensor deployment and unreliable data by framing the problem as a spatiotemporal kriging task and proposing a novel graph transformer model, Kriformer. This model estimates data at locations without sensors by mining spatial and temporal correlations, even with limited resources. Kriformer utilizes transformer architecture to enhance the model's perceptual range and solve edge information aggregation challenges, capturing spatiotemporal information effectively. A carefully constructed positional encoding module embeds the spatiotemporal features of nodes, while a sophisticated spatiotemporal attention mechanism enhances estimation accuracy. The multi-head spatial interaction attention module captures subtle spatial relationships between observed and unobserved locations. During training, a random masking strategy prompts the model to learn with partial information loss, allowing the spatiotemporal embedding and multi-head attention mechanisms to synergistically capture correlations among locations. Experimental results show that Kriformer excels in representation learning for unobserved locations, validated on two real-world traffic speed datasets, demonstrating its effectiveness in spatiotemporal kriging tasks.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2024-09-23 11:01:18 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.818346"
    },
    {
        "index": "#25",
        "title": "Novel Gradient Sparsification Algorithm via Bayesian Inference",
        "link": "/arxiv/2409.14893",
        "arxiv_id": "2409.14893",
        "authors": "Ali Bereyhi, Ben Liang, Gary Boudreau, Ali Afana",
        "summary": "Error accumulation is an essential component of the Top-$k$ sparsification method in distributed gradient descent. It implicitly scales the learning rate and prevents the slow-down of lateral movement, but it can also deteriorate convergence. This paper proposes a novel sparsification algorithm called regularized Top-$k$ (RegTop-$k$) that controls the learning rate scaling of error accumulation. The algorithm is developed by looking at the gradient sparsification as an inference problem and determining a Bayesian optimal sparsification mask via maximum-a-posteriori estimation. It utilizes past aggregated gradients to evaluate posterior statistics, based on which it prioritizes the local gradient entries. Numerical experiments with ResNet-18 on CIFAR-10 show that at $0.1\\%$ sparsification, RegTop-$k$ achieves about $8\\%$ higher accuracy than standard Top-$k$.",
        "subjects": "Machine Learning, Information Theory, Signal Processing",
        "date": "2024-09-23 10:42:34 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.823914"
    },
    {
        "index": "#26",
        "title": "Testing Dependency of Weighted Random Graphs",
        "link": "/arxiv/2409.14870",
        "arxiv_id": "2409.14870",
        "authors": "Mor Oren, Vered Paslev, Wasim Huleihel",
        "summary": "In this paper, we study the task of detecting the edge dependency between two weighted random graphs. We formulate this task as a simple hypothesis testing problem, where under the null hypothesis, the two observed graphs are statistically independent, whereas under the alternative, the edges of one graph are dependent on the edges of a randomly vertex-permuted version of the other graph. For general edge-weights distributions, we establish thresholds at which optimal testing is information-theoretically impossible and possible, as a function of the total number of nodes in the observed graphs and the generative distributions of the weights. Finally, we observe a statistical-computational gap in our problem, and we provide evidence that this is fundamental using the framework of low-degree polynomials.",
        "subjects": "Machine Learning, Information Theory",
        "date": "2024-09-23 10:07:41 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.824114"
    },
    {
        "index": "#28",
        "title": "SDBA: A Stealthy and Long-Lasting Durable Backdoor Attack in Federated Learning",
        "link": "/arxiv/2409.14805",
        "arxiv_id": "2409.14805",
        "authors": "Minyeong Choe, Cheolhee Park, Changho Seo, Hyunil Kim",
        "summary": "Federated Learning is a promising approach for training machine learning models while preserving data privacy, but its distributed nature makes it vulnerable to backdoor attacks, particularly in NLP tasks while related research remains limited. This paper introduces SDBA, a novel backdoor attack mechanism designed for NLP tasks in FL environments. Our systematic analysis across LSTM and GPT-2 models identifies the most vulnerable layers for backdoor injection and achieves both stealth and long-lasting durability through layer-wise gradient masking and top-k% gradient masking within these layers. Experiments on next token prediction and sentiment analysis tasks show that SDBA outperforms existing backdoors in durability and effectively bypasses representative defense mechanisms, with notable performance in LLM such as GPT-2. These results underscore the need for robust defense strategies in NLP-based FL systems.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2024-09-23 08:30:57 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.824513"
    },
    {
        "index": "#30",
        "title": "Isometric Immersion Learning with Riemannian Geometry",
        "link": "/arxiv/2409.14760",
        "arxiv_id": "2409.14760",
        "authors": "Zihao Chen, Wenyong Wang, Yu Xiang",
        "summary": "Manifold learning has been proven to be an effective method for capturing the implicitly intrinsic structure of non-Euclidean data, in which one of the primary challenges is how to maintain the distortion-free (isometry) of the data representations. Actually, there is still no manifold learning method that provides a theoretical guarantee of isometry. Inspired by Nash's isometric theorem, we introduce a new concept called isometric immersion learning based on Riemannian geometry principles. Following this concept, an unsupervised neural network-based model that simultaneously achieves metric and manifold learning is proposed by integrating Riemannian geometry priors. What's more, we theoretically derive and algorithmically implement a maximum likelihood estimation-based training method for the new model. In the simulation experiments, we compared the new model with the state-of-the-art baselines on various 3-D geometry datasets, demonstrating that the new model exhibited significantly superior performance in multiple evaluation metrics. Moreover, we applied the Riemannian metric learned from the new model to downstream prediction tasks in real-world scenarios, and the accuracy was improved by an average of 8.8%.",
        "subjects": "Machine Learning",
        "date": "2024-09-23 07:17:06 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.824890"
    },
    {
        "index": "#33",
        "title": "From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks",
        "link": "/arxiv/2409.14623",
        "arxiv_id": "2409.14623",
        "authors": "Cl√©mentine C. J. Domin√©, Nicolas Anguita, Alexandra M. Proca, Lukas Braun, Daniel Kunin, Pedro A. M. Mediano, Andrew M. Saxe",
        "summary": "Biological and artificial neural networks develop internal representations that enable them to perform complex tasks. In artificial networks, the effectiveness of these models relies on their ability to build task specific representation, a process influenced by interactions among datasets, architectures, initialization strategies, and optimization algorithms. Prior studies highlight that different initializations can place networks in either a lazy regime, where representations remain static, or a rich/feature learning regime, where representations evolve dynamically. Here, we examine how initialization influences learning dynamics in deep linear neural networks, deriving exact solutions for lambda-balanced initializations-defined by the relative scale of weights across layers. These solutions capture the evolution of representations and the Neural Tangent Kernel across the spectrum from the rich to the lazy regimes. Our findings deepen the theoretical understanding of the impact of weight initialization on learning regimes, with implications for continual learning, reversal learning, and transfer learning, relevant to both neuroscience and practical applications.",
        "subjects": "Machine Learning",
        "date": "2024-09-22 23:19:04 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.825530"
    },
    {
        "index": "#34",
        "title": "Protein-Mamba: Biological Mamba Models for Protein Function Prediction",
        "link": "/arxiv/2409.14617",
        "arxiv_id": "2409.14617",
        "authors": "Bohao Xu, Yingzhou Lu, Yoshitaka Inoue, Namkyeong Lee, Tianfan Fu, Jintai Chen",
        "summary": "Protein function prediction is a pivotal task in drug discovery, significantly impacting the development of effective and safe therapeutics. Traditional machine learning models often struggle with the complexity and variability inherent in predicting protein functions, necessitating more sophisticated approaches. In this work, we introduce Protein-Mamba, a novel two-stage model that leverages both self-supervised learning and fine-tuning to improve protein function prediction. The pre-training stage allows the model to capture general chemical structures and relationships from large, unlabeled datasets, while the fine-tuning stage refines these insights using specific labeled datasets, resulting in superior prediction performance. Our extensive experiments demonstrate that Protein-Mamba achieves competitive performance, compared with a couple of state-of-the-art methods across a range of protein function datasets. This model's ability to effectively utilize both unlabeled and labeled data highlights the potential of self-supervised learning in advancing protein function prediction and offers a promising direction for future research in drug discovery.",
        "subjects": "Machine Learning, Biomolecules, Quantitative Methods",
        "date": "2024-09-22 22:51:56 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.825743"
    },
    {
        "index": "#35",
        "title": "Implicit Dynamical Flow Fusion (IDFF) for Generative Modeling",
        "link": "/arxiv/2409.14599",
        "arxiv_id": "2409.14599",
        "authors": "Mohammad R. Rezaei, Rahul G. Krishnan, Milos R. Popovic, Milad Lankarany",
        "summary": "Conditional Flow Matching (CFM) models can generate high-quality samples from a non-informative prior, but they can be slow, often needing hundreds of network evaluations (NFE). To address this, we propose Implicit Dynamical Flow Fusion (IDFF); IDFF learns a new vector field with an additional momentum term that enables taking longer steps during sample generation while maintaining the fidelity of the generated distribution. Consequently, IDFFs reduce the NFEs by a factor of ten (relative to CFMs) without sacrificing sample quality, enabling rapid sampling and efficient handling of image and time-series data generation tasks. We evaluate IDFF on standard benchmarks such as CIFAR-10 and CelebA for image generation. We achieved likelihood and quality performance comparable to CFMs and diffusion-based models with fewer NFEs. IDFF also shows superior performance on time-series datasets modeling, including molecular simulation and sea surface temperature (SST) datasets, highlighting its versatility and effectiveness across different domains.",
        "subjects": "Machine Learning",
        "date": "2024-09-22 21:22:35 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.825920"
    },
    {
        "index": "#39",
        "title": "Domain knowledge-guided machine learning framework for state of health estimation in Lithium-ion batteries",
        "link": "/arxiv/2409.14575",
        "arxiv_id": "2409.14575",
        "authors": "Andrea Lanubile, Pietro Bosoni, Gabriele Pozzato, Anirudh Allam, Matteo Acquarone, Simona Onori",
        "summary": "Accurate estimation of battery state of health is crucial for effective electric vehicle battery management. Here, we propose five health indicators that can be extracted online from real-world electric vehicle operation and develop a machine learning-based method to estimate the battery state of health. The proposed indicators provide physical insights into the energy and power fade of the battery and enable accurate capacity estimation even with partially missing data. Moreover, they can be computed for portions of the charging profile and real-world driving discharging conditions, facilitating real-time battery degradation estimation. The indicators are computed using experimental data from five cells aged under electric vehicle conditions, and a linear regression model is used to estimate the state of health. The results show that models trained with power autocorrelation and energy-based features achieve capacity estimation with maximum absolute percentage error within 1.5% to 2.5% .",
        "subjects": "Machine Learning, Systems and Control",
        "date": "2024-09-22 19:39:53 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.826749"
    },
    {
        "index": "#40",
        "title": "Adaptive Feedforward Gradient Estimation in Neural ODEs",
        "link": "/arxiv/2409.14549",
        "arxiv_id": "2409.14549",
        "authors": "Jaouad Dabounou",
        "summary": "Neural Ordinary Differential Equations (Neural ODEs) represent a significant breakthrough in deep learning, promising to bridge the gap between machine learning and the rich theoretical frameworks developed in various mathematical fields over centuries. In this work, we propose a novel approach that leverages adaptive feedforward gradient estimation to improve the efficiency, consistency, and interpretability of Neural ODEs. Our method eliminates the need for backpropagation and the adjoint method, reducing computational overhead and memory usage while maintaining accuracy. The proposed approach has been validated through practical applications, and showed good performance relative to Neural ODEs state of the art methods.",
        "subjects": "Machine Learning",
        "date": "2024-09-22 18:21:01 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.826907"
    },
    {
        "index": "#41",
        "title": "Distributionally Robust Inverse Reinforcement Learning for Identifying Multi-Agent Coordinated Sensing",
        "link": "/arxiv/2409.14542",
        "arxiv_id": "2409.14542",
        "authors": "Luke Snow, Vikram Krishnamurthy",
        "summary": "We derive a minimax distributionally robust inverse reinforcement learning (IRL) algorithm to reconstruct the utility functions of a multi-agent sensing system. Specifically, we construct utility estimators which minimize the worst-case prediction error over a Wasserstein ambiguity set centered at noisy signal observations. We prove the equivalence between this robust estimation and a semi-infinite optimization reformulation, and we propose a consistent algorithm to compute solutions. We illustrate the efficacy of this robust IRL scheme in numerical studies to reconstruct the utility functions of a cognitive radar network from observed tracking signals.",
        "subjects": "Machine Learning, Multiagent Systems, Signal Processing",
        "date": "2024-09-22 17:44:32 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.827084"
    },
    {
        "index": "#42",
        "title": "Order of Magnitude Speedups for LLM Membership Inference",
        "link": "/arxiv/2409.14513",
        "arxiv_id": "2409.14513",
        "authors": "Martin Bertran, Rongting Zhang, Aaron Roth",
        "summary": "Large Language Models (LLMs) have the promise to revolutionize computing broadly, but their complexity and extensive training data also expose significant privacy vulnerabilities. One of the simplest privacy risks associated with LLMs is their susceptibility to membership inference attacks (MIAs), wherein an adversary aims to determine whether a specific data point was part of the model's training set. Although this is a known risk, state of the art methodologies for MIAs rely on training multiple computationally costly shadow models, making risk evaluation prohibitive for large models. Here we adapt a recent line of work which uses quantile regression to mount membership inference attacks; we extend this work by proposing a low-cost MIA that leverages an ensemble of small quantile regression models to determine if a document belongs to the model's training set or not. We demonstrate the effectiveness of this approach on fine-tuned LLMs of varying families (OPT, Pythia, Llama) and across multiple datasets. Across all scenarios we obtain comparable or improved accuracy compared to state of the art shadow model approaches, with as little as 6% of their computation budget. We demonstrate increased effectiveness across multi-epoch trained target models, and architecture miss-specification robustness, that is, we can mount an effective attack against a model using a different tokenizer and architecture, without requiring knowledge on the target model.",
        "subjects": "Machine Learning, Cryptography and Security, Machine Learning",
        "date": "2024-09-22 16:18:14 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.827367"
    },
    {
        "index": "#46",
        "title": "Investigating the Impact of Hard Samples on Accuracy Reveals In-class Data Imbalance",
        "link": "/arxiv/2409.14401",
        "arxiv_id": "2409.14401",
        "authors": "Pawel Pukowski, Haiping Lu",
        "summary": "In the AutoML domain, test accuracy is heralded as the quintessential metric for evaluating model efficacy, underpinning a wide array of applications from neural architecture search to hyperparameter optimization. However, the reliability of test accuracy as the primary performance metric has been called into question, notably through research highlighting how label noise can obscure the true ranking of state-of-the-art models. We venture beyond, along another perspective where the existence of hard samples within datasets casts further doubt on the generalization capabilities inferred from test accuracy alone. Our investigation reveals that the distribution of hard samples between training and test sets affects the difficulty levels of those sets, thereby influencing the perceived generalization capability of models. We unveil two distinct generalization pathways-toward easy and hard samples-highlighting the complexity of achieving balanced model evaluation. Finally, we propose a benchmarking procedure for comparing hard sample identification methods, facilitating the advancement of more nuanced approaches in this area. Our primary goal is not to propose a definitive solution but to highlight the limitations of relying primarily on test accuracy as an evaluation metric, even when working with balanced datasets, by introducing the in-class data imbalance problem. By doing so, we aim to stimulate a critical discussion within the research community and open new avenues for research that consider a broader spectrum of model evaluation criteria. The anonymous code is available at https://github.com/PawPuk/CurvBIM blueunder the GPL-3.0 license.",
        "subjects": "Machine Learning",
        "date": "2024-09-22 11:38:14 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.828210"
    },
    {
        "index": "#47",
        "title": "Flat-LoRA: Low-Rank Adaption over a Flat Loss Landscape",
        "link": "/arxiv/2409.14396",
        "arxiv_id": "2409.14396",
        "authors": "Tao Li, Zhengbao He, Yujun Li, Yasheng Wang, Lifeng Shang, Xiaolin Huang",
        "summary": "Fine-tuning large-scale pre-trained models is prohibitively expensive in terms of computational and memory costs. Low-Rank Adaptation (LoRA), a popular Parameter-Efficient Fine-Tuning (PEFT) method, provides an efficient way to fine-tune models by optimizing only a low-rank matrix. Despite recent progress made in improving LoRA's performance, the connection between the LoRA optimization space and the original full parameter space is often overlooked. A solution that appears flat in the LoRA space may exist sharp directions in the full parameter space, potentially harming generalization performance. In this paper, we propose Flat-LoRA, an efficient approach that seeks a low-rank adaptation located in a flat region of the full parameter space.Instead of relying on the well-established sharpness-aware minimization approach, which can incur significant computational and memory burdens, we utilize random weight perturbation with a Bayesian expectation loss objective to maintain training efficiency and design a refined perturbation generation strategy for improved performance. Experiments on natural language processing and image classification tasks with various architectures demonstrate the effectiveness of our approach.",
        "subjects": "Machine Learning",
        "date": "2024-09-22 11:24:10 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.828405"
    },
    {
        "index": "#50",
        "title": "Sketch-and-Solve: Optimized Overdetermined Least-Squares Using Randomized Numerical Linear Algebra",
        "link": "/arxiv/2409.14309",
        "arxiv_id": "2409.14309",
        "authors": "Alex Lavaee",
        "summary": "Sketch-and-solve is a powerful paradigm for tackling large-scale computational problems by reducing their dimensionality using sketching matrices. This paper focuses on applying sketch-and-solve algorithms to efficiently solve the overdetermined least squares problem, which is fundamental in various domains such as machine learning, signal processing, and numerical optimization. We provide a comprehensive overview of the sketch-and-solve paradigm and analyze different sketching operators, including dense and sparse variants. We introduce the Sketch-and-Apply (SAA-SAS) algorithm, which leverages randomized numerical linear algebra techniques to compute approximate solutions efficiently. Through extensive experiments on large-scale least squares problems, we demonstrate that our proposed approach significantly outperforms the traditional Least-Squares QR (LSQR) algorithm in terms of runtime while maintaining comparable accuracy. Our results highlight the potential of sketch-and-solve techniques in efficiently handling large-scale numerical linear algebra problems.",
        "subjects": "Machine Learning, Numerical Analysis",
        "date": "2024-09-22 04:29:51 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.834237"
    },
    {
        "index": "#52",
        "title": "Structure Learning via Mutual Information",
        "link": "/arxiv/2409.14235",
        "arxiv_id": "2409.14235",
        "authors": "Jeremy Nixon",
        "summary": "This paper presents a novel approach to machine learning algorithm design based on information theory, specifically mutual information (MI). We propose a framework for learning and representing functional relationships in data using MI-based features. Our method aims to capture the underlying structure of information in datasets, enabling more efficient and generalizable learning algorithms. We demonstrate the efficacy of our approach through experiments on synthetic and real-world datasets, showing improved performance in tasks such as function classification, regression, and cross-dataset transfer. This work contributes to the growing field of metalearning and automated machine learning, offering a new perspective on how to leverage information theory for algorithm design and dataset analysis and proposing new mutual information theoretic foundations to learning algorithms.",
        "subjects": "Machine Learning, Information Theory",
        "date": "2024-09-21 19:33:56 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.834639"
    },
    {
        "index": "#53",
        "title": "ReFine: Boosting Time Series Prediction of Extreme Events by Reweighting and Fine-tuning",
        "link": "/arxiv/2409.14232",
        "arxiv_id": "2409.14232",
        "authors": "Jimeng Shi, Azam Shirali, Giri Narasimhan",
        "summary": "Extreme events are of great importance since they often represent impactive occurrences. For instance, in terms of climate and weather, extreme events might be major storms, floods, extreme heat or cold waves, and more. However, they are often located at the tail of the data distribution. Consequently, accurately predicting these extreme events is challenging due to their rarity and irregularity. Prior studies have also referred to this as the out-of-distribution (OOD) problem, which occurs when the distribution of the test data is substantially different from that used for training. In this work, we propose two strategies, reweighting and fine-tuning, to tackle the challenge. Reweighting is a strategy used to force machine learning models to focus on extreme events, which is achieved by a weighted loss function that assigns greater penalties to the prediction errors for the extreme samples relative to those on the remainder of the data. Unlike previous intuitive reweighting methods based on simple heuristics of data distribution, we employ meta-learning to dynamically optimize these penalty weights. To further boost the performance on extreme samples, we start from the reweighted models and fine-tune them using only rare extreme samples. Through extensive experiments on multiple data sets, we empirically validate that our meta-learning-based reweighting outperforms existing heuristic ones, and the fine-tuning strategy can further increase the model performance. More importantly, these two strategies are model-agnostic, which can be implemented on any type of neural network for time series forecasting. The open-sourced code is available at \\url{https://github.com/JimengShi/ReFine}.",
        "subjects": "Machine Learning",
        "date": "2024-09-21 19:29:29 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.834814"
    },
    {
        "index": "#54",
        "title": "Advancing Employee Behavior Analysis through Synthetic Data: Leveraging ABMs, GANs, and Statistical Models for Enhanced Organizational Efficiency",
        "link": "/arxiv/2409.14197",
        "arxiv_id": "2409.14197",
        "authors": "Rakshitha Jayashankar, Mahesh Balan",
        "summary": "Success in todays data-driven corporate climate requires a deep understanding of employee behavior. Companies aim to improve employee satisfaction, boost output, and optimize workflow. This research study delves into creating synthetic data, a powerful tool that allows us to comprehensively understand employee performance, flexibility, cooperation, and team dynamics. Synthetic data provides a detailed and accurate picture of employee activities while protecting individual privacy thanks to cutting-edge methods like agent-based models (ABMs), Generative Adversarial Networks (GANs), and statistical models. Through the creation of multiple situations, this method offers insightful viewpoints regarding increasing teamwork, improving adaptability, and accelerating overall productivity. We examine how synthetic data has evolved from a specialized field to an essential resource for researching employee behavior and enhancing management efficiency. Keywords: Agent-Based Model, Generative Adversarial Network, workflow optimization, organizational success",
        "subjects": "Machine Learning, Formal Languages and Automata Theory, Other Statistics",
        "date": "2024-09-21 16:58:23 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.835002"
    },
    {
        "index": "#56",
        "title": "A Distribution-Aware Flow-Matching for Generating Unstructured Data for Few-Shot Reinforcement Learning",
        "link": "/arxiv/2409.14178",
        "arxiv_id": "2409.14178",
        "authors": "Mohammad Pivezhandi, Abusayeed Saifullah",
        "summary": "Generating realistic and diverse unstructured data is a significant challenge in reinforcement learning (RL), particularly in few-shot learning scenarios where data is scarce. Traditional RL methods often rely on extensive datasets or simulations, which are costly and time-consuming. In this paper, we introduce a distribution-aware flow matching, designed to generate synthetic unstructured data tailored specifically for an application of few-shot RL called Dynamic Voltage and Frequency Scaling (DVFS) on embedded processors. This method leverages the sample efficiency of flow matching and incorporates statistical learning techniques such as bootstrapping to improve its generalization and robustness of the latent space. Additionally, we apply feature weighting through Random Forests to prioritize critical data aspects, thereby improving the precision of the generated synthetic data. This approach not only mitigates the challenges of overfitting and data correlation in unstructured data in traditional Model-Based RL but also aligns with the Law of Large Numbers, ensuring convergence to true empirical values and optimal policy as the number of samples increases. Through extensive experimentation on an application of DVFS for low energy processing, we demonstrate that our method provides an stable convergence based on max Q-value while enhancing frame rate by 30\\% in the very beginning first timestamps, making this RL model efficient in resource-constrained environments.",
        "subjects": "Machine Learning",
        "date": "2024-09-21 15:50:59 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.835389"
    },
    {
        "index": "#57",
        "title": "Component-based Sketching for Deep ReLU Nets",
        "link": "/arxiv/2409.14174",
        "arxiv_id": "2409.14174",
        "authors": "Di Wang, Shao-Bo Lin, Deyu Meng, Feilong Cao",
        "summary": "Deep learning has made profound impacts in the domains of data mining and AI, distinguished by the groundbreaking achievements in numerous real-world applications and the innovative algorithm design philosophy. However, it suffers from the inconsistency issue between optimization and generalization, as achieving good generalization, guided by the bias-variance trade-off principle, favors under-parameterized networks, whereas ensuring effective convergence of gradient-based algorithms demands over-parameterized networks. To address this issue, we develop a novel sketching scheme based on deep net components for various tasks. Specifically, we use deep net components with specific efficacy to build a sketching basis that embodies the advantages of deep networks. Subsequently, we transform deep net training into a linear empirical risk minimization problem based on the constructed basis, successfully avoiding the complicated convergence analysis of iterative algorithms. The efficacy of the proposed component-based sketching is validated through both theoretical analysis and numerical experiments. Theoretically, we show that the proposed component-based sketching provides almost optimal rates in approximating saturated functions for shallow nets and also achieves almost optimal generalization error bounds. Numerically, we demonstrate that, compared with the existing gradient-based training methods, component-based sketching possesses superior generalization performance with reduced training costs.",
        "subjects": "Machine Learning, Statistics Theory",
        "date": "2024-09-21 15:30:43 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.835663"
    },
    {
        "index": "#58",
        "title": "When Witnesses Defend: A Witness Graph Topological Layer for Adversarial Graph Learning",
        "link": "/arxiv/2409.14161",
        "arxiv_id": "2409.14161",
        "authors": "Naheed Anjum Arafat, Debabrota Basu, Yulia Gel, Yuzhou Chen",
        "summary": "Capitalizing on the intuitive premise that shape characteristics are more robust to perturbations, we bridge adversarial graph learning with the emerging tools from computational topology, namely, persistent homology representations of graphs. We introduce the concept of witness complex to adversarial analysis on graphs, which allows us to focus only on the salient shape characteristics of graphs, yielded by the subset of the most essential nodes (i.e., landmarks), with minimal loss of topological information on the whole graph. The remaining nodes are then used as witnesses, governing which higher-order graph substructures are incorporated into the learning process. Armed with the witness mechanism, we design Witness Graph Topological Layer (WGTL), which systematically integrates both local and global topological graph feature representations, the impact of which is, in turn, automatically controlled by the robust regularized topological loss. Given the attacker's budget, we derive the important stability guarantees of both local and global topology encodings and the associated robust topological loss. We illustrate the versatility and efficiency of WGTL by its integration with five GNNs and three existing non-topological defense mechanisms. Our extensive experiments across six datasets demonstrate that WGTL boosts the robustness of GNNs across a range of perturbations and against a range of adversarial attacks, leading to relative gains of up to 18%.",
        "subjects": "Machine Learning",
        "date": "2024-09-21 14:53:32 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.835857"
    },
    {
        "index": "#59",
        "title": "ESDS: AI-Powered Early Stunting Detection and Monitoring System using Edited Radius-SMOTE Algorithm",
        "link": "/arxiv/2409.14105",
        "arxiv_id": "2409.14105",
        "authors": "A. A. Gde Yogi Pramana, Haidar Muhammad Zidan, Muhammad Fazil Maulana, Oskar Natan",
        "summary": "Stunting detection is a significant issue in Indonesian healthcare, causing lower cognitive function, lower productivity, a weakened immunity, delayed neuro-development, and degenerative diseases. In regions with a high prevalence of stunting and limited welfare resources, identifying children in need of treatment is critical. The diagnostic process often raises challenges, such as the lack of experience in medical workers, incompatible anthropometric equipment, and inefficient medical bureaucracy. To counteract the issues, the use of load cell sensor and ultrasonic sensor can provide suitable anthropometric equipment and streamline the medical bureaucracy for stunting detection. This paper also employs machine learning for stunting detection based on sensor readings. The experiment results show that the sensitivity of the load cell sensor and the ultrasonic sensor is 0.9919 and 0.9986, respectively. Also, the machine learning test results have three classification classes, which are normal, stunted, and stunting with an accuracy rate of 98\\%.",
        "subjects": "Machine Learning, Signal Processing",
        "date": "2024-09-21 11:15:13 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.836048"
    },
    {
        "index": "#62",
        "title": "Implicit Neural Representations for Speed-of-Sound Estimation in Ultrasound",
        "link": "/arxiv/2409.14035",
        "arxiv_id": "2409.14035",
        "authors": "Michal Byra, Piotr Jarosik, Piotr Karwat, Ziemowit Klimonda, Marcin Lewandowski",
        "summary": "Accurate estimation of the speed-of-sound (SoS) is important for ultrasound (US) image reconstruction techniques and tissue characterization. Various approaches have been proposed to calculate SoS, ranging from tomography-inspired algorithms like CUTE to convolutional networks, and more recently, physics-informed optimization frameworks based on differentiable beamforming. In this work, we utilize implicit neural representations (INRs) for SoS estimation in US. INRs are a type of neural network architecture that encodes continuous functions, such as images or physical quantities, through the weights of a network. Implicit networks may overcome the current limitations of SoS estimation techniques, which mainly arise from the use of non-adaptable and oversimplified physical models of tissue. Moreover, convolutional networks for SoS estimation, usually trained using simulated data, often fail when applied to real tissues due to out-of-distribution and data-shift issues. In contrast, implicit networks do not require extensive training datasets since each implicit network is optimized for an individual data case. This adaptability makes them suitable for processing US data collected from varied tissues and across different imaging protocols. We evaluated the proposed SoS estimation method based on INRs using data collected from a tissue-mimicking phantom containing four cylindrical inclusions, with SoS values ranging from 1480 m/s to 1600 m/s. The inclusions were immersed in a material with an SoS value of 1540 m/s. In experiments, the proposed method achieved strong performance, clearly demonstrating the usefulness of implicit networks for quantitative US applications.",
        "subjects": "Machine Learning, Medical Physics",
        "date": "2024-09-21 06:43:38 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.836664"
    },
    {
        "index": "#69",
        "title": "High-Resolution Flood Probability Mapping Using Generative Machine Learning with Large-Scale Synthetic Precipitation and Inundation Data",
        "link": "/arxiv/2409.13936",
        "arxiv_id": "2409.13936",
        "authors": "Lipai Huang, Federico Antolini, Ali Mostafavi, Russell Blessing, Matthew Garcia, Samuel D. Brody",
        "summary": "High-resolution flood probability maps are essential for addressing the limitations of existing flood risk assessment approaches but are often limited by the availability of historical event data. Also, producing simulated data needed for creating probabilistic flood maps using physics-based models involves significant computation and time effort inhibiting the feasibility. To address this gap, this study introduces Flood-Precip GAN (Flood-Precipitation Generative Adversarial Network), a novel methodology that leverages generative machine learning to simulate large-scale synthetic inundation data to produce probabilistic flood maps. With a focus on Harris County, Texas, Flood-Precip GAN begins with training a cell-wise depth estimator using a limited number of physics-based model-generated precipitation-flood events. This model, which emphasizes precipitation-based features, outperforms universal models. Subsequently, a Generative Adversarial Network (GAN) with constraints is employed to conditionally generate synthetic precipitation records. Strategic thresholds are established to filter these records, ensuring close alignment with true precipitation patterns. For each cell, synthetic events are smoothed using a K-nearest neighbors algorithm and processed through the depth estimator to derive synthetic depth distributions. By iterating this procedure and after generating 10,000 synthetic precipitation-flood events, we construct flood probability maps in various formats, considering different inundation depths. Validation through similarity and correlation metrics confirms the fidelity of the synthetic depth distributions relative to true data. Flood-Precip GAN provides a scalable solution for generating synthetic flood depth data needed to create high-resolution flood probability maps, significantly enhancing flood preparedness and mitigation efforts.",
        "subjects": "Machine Learning",
        "date": "2024-09-20 22:43:31 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.837998"
    },
    {
        "index": "#71",
        "title": "Causal Feature Selection Method for Contextual Multi-Armed Bandits in Recommender System",
        "link": "/arxiv/2409.13888",
        "arxiv_id": "2409.13888",
        "authors": "Zhenyu Zhao, Yexi Jiang",
        "summary": "Features (a.k.a. context) are critical for contextual multi-armed bandits (MAB) performance. In practice of large scale online system, it is important to select and implement important features for the model: missing important features can led to sub-optimal reward outcome, and including irrelevant features can cause overfitting, poor model interpretability, and implementation cost. However, feature selection methods for conventional machine learning models fail short for contextual MAB use cases, as conventional methods select features correlated with the outcome variable, but not necessarily causing heterogeneuous treatment effect among arms which are truely important for contextual MAB. In this paper, we introduce model-free feature selection methods designed for contexutal MAB problem, based on heterogeneous causal effect contributed by the feature to the reward distribution. Empirical evaluation is conducted based on synthetic data as well as real data from an online experiment for optimizing content cover image in a recommender system. The results show this feature selection method effectively selects the important features that lead to higher contextual MAB reward than unimportant features. Compared with model embedded method, this model-free method has advantage of fast computation speed, ease of implementation, and prune of model mis-specification issues.",
        "subjects": "Machine Learning, Information Retrieval, Machine Learning",
        "date": "2024-09-20 20:39:23 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.838401"
    },
    {
        "index": "#74",
        "title": "Achieving Predictive Precision: Leveraging LSTM and Pseudo Labeling for Volvo's Discovery Challenge at ECML-PKDD 2024",
        "link": "/arxiv/2409.13877",
        "arxiv_id": "2409.13877",
        "authors": "Carlo Metta, Marco Gregnanin, Andrea Papini, Silvia Giulia Galfr√®, Andrea Fois, Francesco Morandin, Marco Fantozzi, Maurizio Parton",
        "summary": "This paper presents the second-place methodology in the Volvo Discovery Challenge at ECML-PKDD 2024, where we used Long Short-Term Memory networks and pseudo-labeling to predict maintenance needs for a component of Volvo trucks. We processed the training data to mirror the test set structure and applied a base LSTM model to label the test data iteratively. This approach refined our model's predictive capabilities and culminated in a macro-average F1-score of 0.879, demonstrating robust performance in predictive maintenance. This work provides valuable insights for applying machine learning techniques effectively in industrial settings.",
        "subjects": "Machine Learning",
        "date": "2024-09-20 20:12:12 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.844230"
    },
    {
        "index": "#75",
        "title": "Physics-Informed Variational State-Space Gaussian Processes",
        "link": "/arxiv/2409.13876",
        "arxiv_id": "2409.13876",
        "authors": "Oliver Hamelijnck, Arno Solin, Theodoros Damoulas",
        "summary": "Differential equations are important mechanistic models that are integral to many scientific and engineering applications. With the abundance of available data there has been a growing interest in data-driven physics-informed models. Gaussian processes (GPs) are particularly suited to this task as they can model complex, non-linear phenomena whilst incorporating prior knowledge and quantifying uncertainty. Current approaches have found some success but are limited as they either achieve poor computational scalings or focus only on the temporal setting. This work addresses these issues by introducing a variational spatio-temporal state-space GP that handles linear and non-linear physical constraints while achieving efficient linear-in-time computation costs. We demonstrate our methods in a range of synthetic and real-world settings and outperform the current state-of-the-art in both predictive and computational performance.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2024-09-20 20:12:11 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.844435"
    },
    {
        "index": "#76",
        "title": "Data Distribution Shifts in (Industrial) Federated Learning as a Privacy Issue",
        "link": "/arxiv/2409.13875",
        "arxiv_id": "2409.13875",
        "authors": "David Brunner, Alessio Montuoro",
        "summary": "We consider industrial federated learning, a collaboration between a small number of powerful, potentially competing industrial players, mediated by a third party aspiring to improve the service it provides to its customers. We argue that this configuration harbours covert privacy risks that do not arise in e.g. cross-device settings. Companies are very protective of their intellectual property and production processes. Information about changes to their production and the timing of which is to be kept private. We study a scenario in which one of the collaborators infers changes to their competitors' production by detecting potentially subtle temporal data distribution shifts. In this framing, a data distribution shift is always problematic, even if it has no negative effect on training convergence. Thus, our goal is to find means that allow the detection of distributional shifts better than customary evaluation metrics. Based on the assumption that even minor shifts translate into the collaboratively learned machine learning model, the attacker tracks the shared models' internal state with a selection of metrics from literature in order to pick up on relevant changes. In an empirical study on benchmark datasets, we show an honest-but-curious attacker to be capable of detecting subtle distributional shifts on other clients, in some cases long before they become obvious in evaluation.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2024-09-20 20:09:19 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.844670"
    },
    {
        "index": "#77",
        "title": "Persistent Backdoor Attacks in Continual Learning",
        "link": "/arxiv/2409.13864",
        "arxiv_id": "2409.13864",
        "authors": "Zhen Guo, Abhinav Kumar, Reza Tourani",
        "summary": "Backdoor attacks pose a significant threat to neural networks, enabling adversaries to manipulate model outputs on specific inputs, often with devastating consequences, especially in critical applications. While backdoor attacks have been studied in various contexts, little attention has been given to their practicality and persistence in continual learning, particularly in understanding how the continual updates to model parameters, as new data distributions are learned and integrated, impact the effectiveness of these attacks over time. To address this gap, we introduce two persistent backdoor attacks-Blind Task Backdoor and Latent Task Backdoor-each leveraging minimal adversarial influence. Our blind task backdoor subtly alters the loss computation without direct control over the training process, while the latent task backdoor influences only a single task's training, with all other tasks trained benignly. We evaluate these attacks under various configurations, demonstrating their efficacy with static, dynamic, physical, and semantic triggers. Our results show that both attacks consistently achieve high success rates across different continual learning algorithms, while effectively evading state-of-the-art defenses, such as SentiNet and I-BAU.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2024-09-20 19:28:48 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.844864"
    },
    {
        "index": "#80",
        "title": "Segment Discovery: Enhancing E-commerce Targeting",
        "link": "/arxiv/2409.13847",
        "arxiv_id": "2409.13847",
        "authors": "Qiqi Li, Roopali Singh, Charin Polpanumas, Tanner Fiez, Namita Kumar, Shreya Chakrabarti",
        "summary": "Modern e-commerce services frequently target customers with incentives or interventions to engage them in their products such as games, shopping, video streaming, etc. This customer engagement increases acquisition of more customers and retention of existing ones, leading to more business for the company while improving customer experience. Often, customers are either randomly targeted or targeted based on the propensity of desirable behavior. However, such policies can be suboptimal as they do not target the set of customers who would benefit the most from the intervention and they may also not take account of any constraints. In this paper, we propose a policy framework based on uplift modeling and constrained optimization that identifies customers to target for a use-case specific intervention so as to maximize the value to the business, while taking account of any given constraints. We demonstrate improvement over state-of-the-art targeting approaches using two large-scale experimental studies and a production implementation.",
        "subjects": "Machine Learning, Information Retrieval",
        "date": "2024-09-20 18:42:04 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.845463"
    },
    {
        "index": "#85",
        "title": "A constrained optimization approach to improve robustness of neural networks",
        "link": "/arxiv/2409.13770",
        "arxiv_id": "2409.13770",
        "authors": "Shudian Zhao, Jan Kronqvist",
        "summary": "In this paper, we present a novel nonlinear programming-based approach to fine-tune pre-trained neural networks to improve robustness against adversarial attacks while maintaining high accuracy on clean data. Our method introduces adversary-correction constraints to ensure correct classification of adversarial data and minimizes changes to the model parameters. We propose an efficient cutting-plane-based algorithm to iteratively solve the large-scale nonconvex optimization problem by approximating the feasible region through polyhedral cuts and balancing between robustness and accuracy. Computational experiments on standard datasets such as MNIST and CIFAR10 demonstrate that the proposed approach significantly improves robustness, even with a very small set of adversarial data, while maintaining minimal impact on accuracy.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2024-09-18 18:37:14 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.846437"
    },
    {
        "index": "#88",
        "title": "Machine Learning Toric Duality in Brane Tilings",
        "link": "/arxiv/2409.15251",
        "arxiv_id": "2409.15251",
        "authors": "Pietro Capuozzo, Tancredi Schettini Gherardini, Benjamin Suzzoni",
        "summary": "We apply a variety of machine learning methods to the study of Seiberg duality within 4d $\\mathcal{N}=1$ quantum field theories arising on the worldvolumes of D3-branes probing toric Calabi-Yau 3-folds. Such theories admit an elegant description in terms of bipartite tessellations of the torus known as brane tilings or dimer models. An intricate network of infrared dualities interconnects the space of such theories and partitions it into universality classes, the prediction and classification of which is a problem that naturally lends itself to a machine learning investigation. In this paper, we address a preliminary set of such enquiries. We begin by training a fully connected neural network to identify classes of Seiberg dual theories realised on $\\mathbb{Z}_m\\times\\mathbb{Z}_n$ orbifolds of the conifold and achieve $R^2=0.988$. Then, we evaluate various notions of robustness of our methods against perturbations of the space of theories under investigation, and discuss these results in terms of the nature of the neural network's learning. Finally, we employ a more sophisticated residual architecture to classify the toric phase space of the $Y^{6,0}$ theories, and to predict the individual gauged linear $\\sigma$-model multiplicities in toric diagrams thereof. In spite of the non-trivial nature of this task, we achieve remarkably accurate results; namely, upon fixing a choice of Kasteleyn matrix representative, the regressor achieves a mean absolute error of $0.021$. We also discuss how the performance is affected by relaxing these assumptions.",
        "subjects": "High Energy Physics - Theory, Machine Learning",
        "date": "2024-09-23 17:48:14 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.847254"
    },
    {
        "index": "#91",
        "title": "Intelligent Routing Algorithm over SDN: Reusable Reinforcement Learning Approach",
        "link": "/arxiv/2409.15226",
        "arxiv_id": "2409.15226",
        "authors": "Wang Wumian, Sajal Saha, Anwar Haque, Greg Sidebottom",
        "summary": "Traffic routing is vital for the proper functioning of the Internet. As users and network traffic increase, researchers try to develop adaptive and intelligent routing algorithms that can fulfill various QoS requirements. Reinforcement Learning (RL) based routing algorithms have shown better performance than traditional approaches. We developed a QoS-aware, reusable RL routing algorithm, RLSR-Routing over SDN. During the learning process, our algorithm ensures loop-free path exploration. While finding the path for one traffic demand (a source destination pair with certain amount of traffic), RLSR-Routing learns the overall network QoS status, which can be used to speed up algorithm convergence when finding the path for other traffic demands. By adapting Segment Routing, our algorithm can achieve flow-based, source packet routing, and reduce communications required between SDN controller and network plane. Our algorithm shows better performance in terms of load balancing than the traditional approaches. It also has faster convergence than the non-reusable RL approach when finding paths for multiple traffic demands.",
        "subjects": "Networking and Internet Architecture, Machine Learning",
        "date": "2024-09-23 17:15:24 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.847855"
    },
    {
        "index": "#94",
        "title": "Fast and Accurate Triangle Counting in Graph Streams Using Predictions",
        "link": "/arxiv/2409.15205",
        "arxiv_id": "2409.15205",
        "authors": "Cristian Boldrin, Fabio Vandin",
        "summary": "In this work, we present the first efficient and practical algorithm for estimating the number of triangles in a graph stream using predictions. Our algorithm combines waiting room sampling and reservoir sampling with a predictor for the heaviness of edges, that is, the number of triangles in which an edge is involved. As a result, our algorithm is fast, provides guarantees on the amount of memory used, and exploits the additional information provided by the predictor to produce highly accurate estimates. We also propose a simple and domain-independent predictor, based on the degree of nodes, that can be easily computed with one pass on a stream of edges when the stream is available beforehand. Our analytical results show that, when the predictor provides useful information on the heaviness of edges, it leads to estimates with reduced variance compared to the state-of-the-art, even when the predictions are far from perfect. Our experimental results show that, when analyzing a single graph stream, our algorithm is faster than the state-of-the-art for a given memory budget, while providing significantly more accurate estimates. Even more interestingly, when sequences of hundreds of graph streams are analyzed, our algorithm significantly outperforms the state-of-the-art using our simple degree-based predictor built by analyzing only the first graph of the sequence.",
        "subjects": "Data Structures and Algorithms, Machine Learning",
        "date": "2024-09-23 16:52:11 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.848495"
    },
    {
        "index": "#95",
        "title": "RAMBO: Enhancing RAG-based Repository-Level Method Body Completion",
        "link": "/arxiv/2409.15204",
        "arxiv_id": "2409.15204",
        "authors": "Tuan-Dung Bui, Duc-Thieu Luu-Van, Thanh-Phat Nguyen, Thu-Trang Nguyen, Son Nguyen, Hieu Dinh Vo",
        "summary": "Code completion is essential in software development, helping developers by predicting code snippets based on context. Among completion tasks, Method Body Completion (MBC) is particularly challenging as it involves generating complete method bodies based on their signatures and context. This task becomes significantly harder in large repositories, where method bodies must integrate repositoryspecific elements such as custom APIs, inter-module dependencies, and project-specific conventions. In this paper, we introduce RAMBO, a novel RAG-based approach for repository-level MBC. Instead of retrieving similar method bodies, RAMBO identifies essential repositoryspecific elements, such as classes, methods, and variables/fields, and their relevant usages. By incorporating these elements and their relevant usages into the code generation process, RAMBO ensures more accurate and contextually relevant method bodies. Our experimental results with leading code LLMs across 40 Java projects show that RAMBO significantly outperformed the state-of-the-art repository-level MBC approaches, with the improvements of up to 46% in BLEU, 57% in CodeBLEU, 36% in Compilation Rate, and up to 3X in Exact Match. Notably, RAMBO surpassed RepoCoder Oracle method by up to 12% in Exact Match, setting a new benchmark for repository-level MBC.",
        "subjects": "Software Engineering, Machine Learning",
        "date": "2024-09-23 16:51:43 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.848721"
    },
    {
        "index": "#99",
        "title": "Harmonic Path Integral Diffusion",
        "link": "/arxiv/2409.15166",
        "arxiv_id": "2409.15166",
        "authors": "Hamidreza Behjoo, Michael Chertkov",
        "summary": "In this manuscript, we present a novel approach for sampling from a continuous multivariate probability distribution, which may either be explicitly known (up to a normalization factor) or represented via empirical samples. Our method constructs a time-dependent bridge from a delta function centered at the origin of the state space at $t=0$, optimally transforming it into the target distribution at $t=1$. We formulate this as a Stochastic Optimal Control problem of the Path Integral Control type, with a cost function comprising (in its basic form) a quadratic control term, a quadratic state term, and a terminal constraint. This framework, which we refer to as Harmonic Path Integral Diffusion (H-PID), leverages an analytical solution through a mapping to an auxiliary quantum harmonic oscillator in imaginary time. The H-PID framework results in a set of efficient sampling algorithms, without the incorporation of Neural Networks. The algorithms are validated on two standard use cases: a mixture of Gaussians over a grid and images from CIFAR-10. We contrast these algorithms with other sampling methods, particularly simulated annealing and path integral sampling, highlighting their advantages in terms of analytical control, accuracy, and computational efficiency on benchmark problems. Additionally, we extend the methodology to more general cases where the underlying stochastic differential equation includes an external deterministic, possibly non-conservative force, and where the cost function incorporates a gauge potential term. These extensions open up new possibilities for applying our framework to a broader range of statistics specific to applications.",
        "subjects": "Machine Learning, Machine Learning, Computation",
        "date": "2024-09-23 16:20:21 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.849824"
    },
    {
        "index": "#101",
        "title": "UTrace: Poisoning Forensics for Private Collaborative Learning",
        "link": "/arxiv/2409.15126",
        "arxiv_id": "2409.15126",
        "authors": "Evan Rose, Hidde Lycklama, Harsh Chaudhari, Anwar Hithnawi, Alina Oprea",
        "summary": "Privacy-preserving machine learning (PPML) enables multiple data owners to contribute their data privately to a set of servers that run a secure multi-party computation (MPC) protocol to train a joint ML model. In these protocols, the input data remains private throughout the training process, and only the resulting model is made available. While this approach benefits privacy, it also exacerbates the risks of data poisoning, where compromised data owners induce undesirable model behavior by contributing malicious datasets. Existing MPC mechanisms can mitigate certain poisoning attacks, but these measures are not exhaustive. To complement existing poisoning defenses, we introduce UTrace: a framework for User-level Traceback of poisoning attacks in PPML. Utrace computes user responsibility scores using gradient similarity metrics aggregated across the most relevant samples in an owner's dataset. UTrace is effective at low poisoning rates and is resilient to poisoning attacks distributed across multiple data owners, unlike existing unlearning-based methods. We introduce methods for checkpointing gradients with low storage overhead, enabling traceback in the absence of data owners at deployment time. We also design several optimizations that reduce traceback time and communication in MPC. We provide a comprehensive evaluation of UTrace across four datasets from three data modalities (vision, text, and malware) and show its effectiveness against 10 poisoning attacks.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2024-09-23 15:32:46 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.850257"
    },
    {
        "index": "#103",
        "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving System for Transformer based Models with Long Prompts",
        "link": "/arxiv/2409.15104",
        "arxiv_id": "2409.15104",
        "authors": "Zeyu Zhang, Haiying Shen",
        "summary": "Long-sequence generative large-language model (LLM) applications have become increasingly popular. In this paper, through trace-based experiments, we found that the existing method for long sequences results in a high Time-To-First-Token (TTFT) due to sequential chunk processing, long Time-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and low throughput due to constrained key-value cache (KVC) for long sequences. To address these issues, we propose two Sequence-Parallelism (SP) architectures for both tensor parallelism (TP) and non-TP. However, SP introduces two challenges: 1) network communication and computation become performance bottlenecks; 2) the latter two issues above are mitigated but not resolved, and SP's resultant KV value distribution across GPUs still requires communication for decode, increasing TBT. Hence, we propose a Communication-efficient Sparse Attention (CSA) and communication-computation-communication three-phase pipelining. We also propose SP-based decode that processes decode separately from prefill, distributes KV values of a request across different GPUs, and novelly moves Query (Q) values instead of KV values to reduce communication overhead. These methods constitute a communication-efficient Sequence-Parallelism based LLM Serving System (SPS2). Our trace-driven evaluation demonstrates that SPS2 improves the average TTFT, TBT, and response time by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode throughput by 8.2x and 5.2x while maintaining the accuracy compared to Sarathi-Serve. We distributed our source code.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Machine Learning",
        "date": "2024-09-23 15:16:29 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.850788"
    },
    {
        "index": "#109",
        "title": "Methods for Convex $(L_0,L_1)$-Smooth Optimization: Clipping, Acceleration, and Adaptivity",
        "link": "/arxiv/2409.14989",
        "arxiv_id": "2409.14989",
        "authors": "Eduard Gorbunov, Nazarii Tupitsa, Sayantan Choudhury, Alen Aliev, Peter Richt√°rik, Samuel Horv√°th, Martin Tak√°ƒç",
        "summary": "Due to the non-smoothness of optimization problems in Machine Learning, generalized smoothness assumptions have been gaining a lot of attention in recent years. One of the most popular assumptions of this type is $(L_0,L_1)$-smoothness (Zhang et al., 2020). In this paper, we focus on the class of (strongly) convex $(L_0,L_1)$-smooth functions and derive new convergence guarantees for several existing methods. In particular, we derive improved convergence rates for Gradient Descent with (Smoothed) Gradient Clipping and for Gradient Descent with Polyak Stepsizes. In contrast to the existing results, our rates do not rely on the standard smoothness assumption and do not suffer from the exponential dependency from the initial distance to the solution. We also extend these results to the stochastic case under the over-parameterization assumption, propose a new accelerated method for convex $(L_0,L_1)$-smooth optimization, and derive new convergence rates for Adaptive Gradient Descent (Malitsky and Mishchenko, 2020).",
        "subjects": "Optimization and Control, Machine Learning",
        "date": "2024-09-23 13:11:37 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.852263"
    },
    {
        "index": "#111",
        "title": "(De)-regularized Maximum Mean Discrepancy Gradient Flow",
        "link": "/arxiv/2409.14980",
        "arxiv_id": "2409.14980",
        "authors": "Zonghao Chen, Aratrika Mustafi, Pierre Glaser, Anna Korba, Arthur Gretton, Bharath K. Sriperumbudur",
        "summary": "We introduce a (de)-regularization of the Maximum Mean Discrepancy (DrMMD) and its Wasserstein gradient flow. Existing gradient flows that transport samples from source distribution to target distribution with only target samples, either lack tractable numerical implementation ($f$-divergence flows) or require strong assumptions, and modifications such as noise injection, to ensure convergence (Maximum Mean Discrepancy flows). In contrast, DrMMD flow can simultaneously (i) guarantee near-global convergence for a broad class of targets in both continuous and discrete time, and (ii) be implemented in closed form using only samples. The former is achieved by leveraging the connection between the DrMMD and the $\\chi^2$-divergence, while the latter comes by treating DrMMD as MMD with a de-regularized kernel. Our numerical scheme uses an adaptive de-regularization schedule throughout the flow to optimally trade off between discretization errors and deviations from the $\\chi^2$ regime. The potential application of the DrMMD flow is demonstrated across several numerical experiments, including a large-scale setting of training student/teacher networks.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2024-09-23 12:57:42 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.852687"
    },
    {
        "index": "#112",
        "title": "Blind Spatial Impulse Response Generation from Separate Room- and Scene-Specific Information",
        "link": "/arxiv/2409.14971",
        "arxiv_id": "2409.14971",
        "authors": "Francesc Llu√≠s, Nils Meyer-Kahlen",
        "summary": "For audio in augmented reality (AR), knowledge of the users' real acoustic environment is crucial for rendering virtual sounds that seamlessly blend into the environment. As acoustic measurements are usually not feasible in practical AR applications, information about the room needs to be inferred from available sound sources. Then, additional sound sources can be rendered with the same room acoustic qualities. Crucially, these are placed at different positions than the sources available for estimation. Here, we propose to use an encoder network trained using a contrastive loss that maps input sounds to a low-dimensional feature space representing only room-specific information. Then, a diffusion-based spatial room impulse response generator is trained to take the latent space and generate a new response, given a new source-receiver position. We show how both room- and position-specific parameters are considered in the final output.",
        "subjects": "Sound, Machine Learning, Audio and Speech Processing",
        "date": "2024-09-23 12:41:31 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.852878"
    },
    {
        "index": "#113",
        "title": "A Realistic Simulation Framework for Analog/Digital Neuromorphic Architectures",
        "link": "/arxiv/2409.14918",
        "arxiv_id": "2409.14918",
        "authors": "Fernando M. Quintana, Maryada, Pedro L. Galindo, Elisa Donati, Giacomo Indiveri, Fernando Perez-Pe√±a",
        "summary": "Developing dedicated neuromorphic computing platforms optimized for embedded or edge-computing applications requires time-consuming design, fabrication, and deployment of full-custom neuromorphic processors.bTo ensure that initial prototyping efforts, exploring the properties of different network architectures and parameter settings, lead to realistic results it is important to use simulation frameworks that match as best as possible the properties of the final hardware. This is particularly challenging for neuromorphic hardware platforms made using mixed-signal analog/digital circuits, due to the variability and noise sensitivity of their components. In this paper, we address this challenge by developing a software spiking neural network simulator explicitly designed to account for the properties of mixed-signal neuromorphic circuits, including device mismatch variability. The simulator, called ARCANA (A Realistic Simulation Framework for Analog/Digital Neuromorphic Architectures), is designed to reproduce the dynamics of mixed-signal synapse and neuron electronic circuits with autogradient differentiation for parameter optimization and GPU acceleration. We demonstrate the effectiveness of this approach by matching software simulation results with measurements made from an existing neuromorphic processor. We show how the results obtained provide a reliable estimate of the behavior of the spiking neural network trained in software, once deployed in hardware. This framework enables the development and innovation of new learning rules and processing architectures in neuromorphic embedded systems.",
        "subjects": "Neural and Evolutionary Computing, Hardware Architecture, Machine Learning",
        "date": "2024-09-23 11:16:46 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.853093"
    },
    {
        "index": "#115",
        "title": "Efficient Tabular Data Preprocessing of ML Pipelines",
        "link": "/arxiv/2409.14912",
        "arxiv_id": "2409.14912",
        "authors": "Yu Zhu, Wenqi Jiang, Gustavo Alonso",
        "summary": "Data preprocessing pipelines, which includes data decoding, cleaning, and transforming, are a crucial component of Machine Learning (ML) training. Thy are computationally intensive and often become a major bottleneck, due to the increasing performance gap between the CPUs used for preprocessing and the GPUs used for model training. Recent studies show that a significant number of CPUs across several machines are required to achieve sufficient throughput to saturate the GPUs, leading to increased resource and energy consumption. When the pipeline involves vocabulary generation, the preprocessing performance scales poorly due to significant row-wise synchronization overhead between different CPU cores and servers. To address this limitation, in this paper we present the design of Piper, a hardware accelerator for tabular data preprocessing, prototype it on FPGAs, and demonstrate its potential for training pipelines of commercial recommender systems. Piper achieves 4.7 $\\sim$ 71.3$\\times$ speedup in latency over a 128-core CPU server and outperforms a data-center GPU by 4.8$\\sim$ 20.3$\\times$ when using binary input. The impressive performance showcases Piper's potential to increase the efficiency of data preprocessing pipelines and significantly reduce their resource consumption.",
        "subjects": "Hardware Architecture, Machine Learning",
        "date": "2024-09-23 11:07:57 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.853460"
    },
    {
        "index": "#117",
        "title": "Built Different: Tactile Perception to Overcome Cross-Embodiment Capability Differences in Collaborative Manipulation",
        "link": "/arxiv/2409.14896",
        "arxiv_id": "2409.14896",
        "authors": "William van den Bogert, Madhavan Iyengar, Nima Fazeli",
        "summary": "Tactile sensing is a powerful means of implicit communication between a human and a robot assistant. In this paper, we investigate how tactile sensing can transcend cross-embodiment differences across robotic systems in the context of collaborative manipulation. Consider tasks such as collaborative object carrying where the human-robot interaction is force rich. Learning and executing such skills requires the robot to comply to the human and to learn behaviors at the joint-torque level. However, most robots do not offer this compliance or provide access to their joint torques. To address this challenge, we present an approach that uses tactile sensors to transfer policies from robots with these capabilities to those without. We show how our method can enable a cooperative task where a robot and human must work together to maneuver objects through space. We first demonstrate the skill on an impedance control-capable robot equipped with tactile sensing, then show the positive transfer of the tactile policy to a planar prismatic robot that is only capable of position control and does not come equipped with any sort of force/torque feedback, yet is able to comply to the human motions only using tactile feedback. Further details and videos can be found on our project website at https://www.mmintlab.com/research/tactile-collaborative/.",
        "subjects": "Robotics, Machine Learning",
        "date": "2024-09-23 10:45:41 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.853889"
    },
    {
        "index": "#124",
        "title": "Energy-Aware Federated Learning in Satellite Constellations",
        "link": "/arxiv/2409.14832",
        "arxiv_id": "2409.14832",
        "authors": "Nasrin Razmi, Bho Matthiesen, Armin Dekorsy, Petar Popovski",
        "summary": "Federated learning in satellite constellations, where the satellites collaboratively train a machine learning model, is a promising technology towards enabling globally connected intelligence and the integration of space networks into terrestrial mobile networks. The energy required for this computationally intensive task is provided either by solar panels or by an internal battery if the satellite is in Earth's shadow. Careful management of this battery and system's available energy resources is not only necessary for reliable satellite operation, but also to avoid premature battery aging. We propose a novel energy-aware computation time scheduler for satellite FL, which aims to minimize battery usage without any impact on the convergence speed. Numerical results indicate an increase of more than 3x in battery lifetime can be achieved over energy-agnostic task scheduling.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Machine Learning, Signal Processing",
        "date": "2024-09-23 09:01:17 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.856024"
    },
    {
        "index": "#126",
        "title": "Pre-trained Language Model and Knowledge Distillation for Lightweight Sequential Recommendation",
        "link": "/arxiv/2409.14810",
        "arxiv_id": "2409.14810",
        "authors": "Li Li, Mingyue Cheng, Zhiding Liu, Hao Zhang, Qi Liu, Enhong Chen",
        "summary": "Sequential recommendation models user interests based on historical behaviors to provide personalized recommendation. Previous sequential recommendation algorithms primarily employ neural networks to extract features of user interests, achieving good performance. However, due to the recommendation system datasets sparsity, these algorithms often employ small-scale network frameworks, resulting in weaker generalization capability. Recently, a series of sequential recommendation algorithms based on large pre-trained language models have been proposed. Nonetheless, given the real-time demands of recommendation systems, the challenge remains in applying pre-trained language models for rapid recommendations in real scenarios. To address this, we propose a sequential recommendation algorithm based on a pre-trained language model and knowledge distillation. The key of proposed algorithm is to transfer pre-trained knowledge across domains and achieve lightweight inference by knowledge distillation. The algorithm operates in two stages: in the first stage, we fine-tune the pre-trained language model on the recommendation dataset to transfer the pre-trained knowledge to the recommendation task; in the second stage, we distill the trained language model to transfer the learned knowledge to a lightweight model. Extensive experiments on multiple public recommendation datasets show that the proposed algorithm enhances recommendation accuracy and provide timely recommendation services.",
        "subjects": "Information Retrieval, Machine Learning",
        "date": "2024-09-23 08:39:07 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.856465"
    },
    {
        "index": "#127",
        "title": "Adaptive Conformal Inference for Multi-Step Ahead Time-Series Forecasting Online",
        "link": "/arxiv/2409.14792",
        "arxiv_id": "2409.14792",
        "authors": "Johan Hallberg Szabadv√°ry",
        "summary": "The aim of this paper is to propose an adaptation of the well known adaptive conformal inference (ACI) algorithm to achieve finite-sample coverage guarantees in multi-step ahead time-series forecasting in the online setting. ACI dynamically adjusts significance levels, and comes with finite-sample guarantees on coverage, even for non-exchangeable data. Our multi-step ahead ACI procedure inherits these guarantees at each prediction step, as well as for the overall error rate. The multi-step ahead ACI algorithm can be used with different target error and learning rates at different prediction steps, which is illustrated in our numerical examples, where we employ a version of the confromalised ridge regression algorithm, adapted to multi-input multi-output forecasting. The examples serve to show how the method works in practice, illustrating the effect of variable target error and learning rates for different prediction steps, which suggests that a balance may be struck between efficiency (interval width) and coverage.t",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2024-09-23 08:07:49 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.856636"
    },
    {
        "index": "#128",
        "title": "Multiscale scattered data analysis in samplet coordinates",
        "link": "/arxiv/2409.14791",
        "arxiv_id": "2409.14791",
        "authors": "Sara Avesani, R√ºdiger Kempf, Michael Multerer, Holger Wendland",
        "summary": "We study multiscale scattered data interpolation schemes for globally supported radial basis functions, with a focus on the Mat√©rn class. The multiscale approximation is constructed through a sequence of residual corrections, where radial basis functions with different lengthscale parameters are employed to capture varying levels of detail. To apply this approach to large data sets, we suggest to represent the resulting generalized Vandermonde matrices in samplet coordinates. Samplets are localized, discrete signed measures exhibiting vanishing moments and allow for the sparse approximation of generalized Vandermonde matrices issuing from a vast class of radial basis functions. Given a quasi-uniform set of $N$ data sites, and local approximation spaces with geometrically decreasing dimension, the full multiscale system can be assembled with cost $\\mathcal{O}(N \\log N)$. We prove that the condition numbers of the linear systems at each level remain bounded independent of the particular level, allowing us to use an iterative solver with a bounded number of iterations for the numerical solution. Hence, the overall cost of the proposed approach is $\\mathcal{O}(N \\log N)$. The theoretical findings are accompanied by extensive numerical studies in two and three spatial dimensions.",
        "subjects": "Numerical Analysis, Machine Learning",
        "date": "2024-09-23 08:07:47 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.856826"
    },
    {
        "index": "#130",
        "title": "Neural refractive index field: Unlocking the Potential of Background-oriented Schlieren Tomography in Volumetric Flow Visualization",
        "link": "/arxiv/2409.14722",
        "arxiv_id": "2409.14722",
        "authors": "Yuanzhe He, Yutao Zheng, Shijie Xu, Chang Liu, Di Peng, Yingzheng Liu, Weiwei Cai",
        "summary": "Background-oriented Schlieren tomography (BOST) is a prevalent method for visualizing intricate turbulent flows, valued for its ease of implementation and capacity to capture three-dimensional distributions of a multitude of flow parameters. However, the voxel-based meshing scheme leads to significant challenges, such as inadequate spatial resolution, substantial discretization errors, poor noise immunity, and excessive computational costs. This work presents an innovative reconstruction approach termed neural refractive index field (NeRIF) which implicitly represents the flow field with a neural network, which is trained with tailored strategies. Both numerical simulations and experimental demonstrations on turbulent Bunsen flames suggest that our approach can significantly improve the reconstruction accuracy and spatial resolution while concurrently reducing computational expenses. Although showcased in the context of background-oriented schlieren tomography here, the key idea embedded in the NeRIF can be readily adapted to various other tomographic modalities including tomographic absorption spectroscopy and tomographic particle imaging velocimetry, broadening its potential impact across different domains of flow visualization and analysis.",
        "subjects": "Fluid Dynamics, Human-Computer Interaction, Machine Learning",
        "date": "2024-09-23 05:40:50 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.857291"
    },
    {
        "index": "#131",
        "title": "EDGE-Rec: Efficient and Data-Guided Edge Diffusion For Recommender Systems Graphs",
        "link": "/arxiv/2409.14689",
        "arxiv_id": "2409.14689",
        "authors": "Utkarsh Priyam, Hemit Shah, Edoardo Botta",
        "summary": "Most recommender systems research focuses on binary historical user-item interaction encodings to predict future interactions. User features, item features, and interaction strengths remain largely under-utilized in this space or only indirectly utilized, despite proving largely effective in large-scale production recommendation systems. We propose a new attention mechanism, loosely based on the principles of collaborative filtering, called Row-Column Separable Attention RCSA to take advantage of real-valued interaction weights as well as user and item features directly. Building on this mechanism, we additionally propose a novel Graph Diffusion Transformer GDiT architecture which is trained to iteratively denoise the weighted interaction matrix of the user-item interaction graph directly. The weighted interaction matrix is built from the bipartite structure of the user-item interaction graph and corresponding edge weights derived from user-item rating interactions. Inspired by the recent progress in text-conditioned image generation, our method directly produces user-item rating predictions on the same scale as the original ratings by conditioning the denoising process on user and item features with a principled approach.",
        "subjects": "Information Retrieval, Machine Learning",
        "date": "2024-09-23 03:23:20 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.857484"
    },
    {
        "index": "#132",
        "title": "Robust Training Objectives Improve Embedding-based Retrieval in Industrial Recommendation Systems",
        "link": "/arxiv/2409.14682",
        "arxiv_id": "2409.14682",
        "authors": "Matthew Kolodner, Mingxuan Ju, Zihao Fan, Tong Zhao, Elham Ghazizadeh, Yan Wu, Neil Shah, Yozen Liu",
        "summary": "Improving recommendation systems (RS) can greatly enhance the user experience across many domains, such as social media. Many RS utilize embedding-based retrieval (EBR) approaches to retrieve candidates for recommendation. In an EBR system, the embedding quality is key. According to recent literature, self-supervised multitask learning (SSMTL) has showed strong performance on academic benchmarks in embedding learning and resulted in an overall improvement in multiple downstream tasks, demonstrating a larger resilience to the adverse conditions between each downstream task and thereby increased robustness and task generalization ability through the training objective. However, whether or not the success of SSMTL in academia as a robust training objectives translates to large-scale (i.e., over hundreds of million users and interactions in-between) industrial RS still requires verification. Simply adopting academic setups in industrial RS might entail two issues. Firstly, many self-supervised objectives require data augmentations (e.g., embedding masking/corruption) over a large portion of users and items, which is prohibitively expensive in industrial RS. Furthermore, some self-supervised objectives might not align with the recommendation task, which might lead to redundant computational overheads or negative transfer. In light of these two challenges, we evaluate using a robust training objective, specifically SSMTL, through a large-scale friend recommendation system on a social media platform in the tech sector, identifying whether this increase in robustness can work at scale in enhancing retrieval in the production setting. Through online A/B testing with SSMTL-based EBR, we observe statistically significant increases in key metrics in the friend recommendations, with up to 5.45% improvements in new friends made and 1.91% improvements in new friends made with cold-start users.",
        "subjects": "Information Retrieval, Machine Learning",
        "date": "2024-09-23 03:12:33 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.857715"
    },
    {
        "index": "#133",
        "title": "Fourier neural operators for spatiotemporal dynamics in two-dimensional turbulence",
        "link": "/arxiv/2409.14660",
        "arxiv_id": "2409.14660",
        "authors": "Mohammad Atif, Pulkit Dubey, Pratik P. Aghor, Vanessa Lopez-Marrero, Tao Zhang, Abdullah Sharfuddin, Kwangmin Yu, Fan Yang, Foluso Ladeinde, Yangang Liu, Meifeng Lin, Lingda Li",
        "summary": "High-fidelity direct numerical simulation of turbulent flows for most real-world applications remains an outstanding computational challenge. Several machine learning approaches have recently been proposed to alleviate the computational cost even though they become unstable or unphysical for long time predictions. We identify that the Fourier neural operator (FNO) based models combined with a partial differential equation (PDE) solver can accelerate fluid dynamic simulations and thus address computational expense of large-scale turbulence simulations. We treat the FNO model on the same footing as a PDE solver and answer important questions about the volume and temporal resolution of data required to build pre-trained models for turbulence. We also discuss the pitfalls of purely data-driven approaches that need to be avoided by the machine learning models to become viable and competitive tools for long time simulations of turbulence.",
        "subjects": "Fluid Dynamics, Machine Learning, Chaotic Dynamics",
        "date": "2024-09-23 02:02:02 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.857981"
    },
    {
        "index": "#134",
        "title": "Federated Graph Learning with Adaptive Importance-based Sampling",
        "link": "/arxiv/2409.14655",
        "arxiv_id": "2409.14655",
        "authors": "Anran Li, Yuanyuan Chen, Chao Ren, Wenhan Wang, Ming Hu, Tianlin Li, Han Yu, Qingyu Chen",
        "summary": "For privacy-preserving graph learning tasks involving distributed graph datasets, federated learning (FL)-based GCN (FedGCN) training is required. A key challenge for FedGCN is scaling to large-scale graphs, which typically incurs high computation and communication costs when dealing with the explosively increasing number of neighbors. Existing graph sampling-enhanced FedGCN training approaches ignore graph structural information or dynamics of optimization, resulting in high variance and inaccurate node embeddings. To address this limitation, we propose the Federated Adaptive Importance-based Sampling (FedAIS) approach. It achieves substantial computational cost saving by focusing the limited resources on training important nodes, while reducing communication overhead via adaptive historical embedding synchronization. The proposed adaptive importance-based sampling method jointly considers the graph structural heterogeneity and the optimization dynamics to achieve optimal trade-off between efficiency and accuracy. Extensive evaluations against five state-of-the-art baselines on five real-world graph datasets show that FedAIS achieves comparable or up to 3.23% higher test accuracy, while saving communication and computation costs by 91.77% and 85.59%.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Cryptography and Security, Machine Learning",
        "date": "2024-09-23 01:49:20 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.858209"
    },
    {
        "index": "#135",
        "title": "Demystifying Trajectory Recovery From Ash: An Open-Source Evaluation and Enhancement",
        "link": "/arxiv/2409.14645",
        "arxiv_id": "2409.14645",
        "authors": "Nicholas D'Silva, Toran Shahi, √òyvind Timian Dokk Husveg, Adith Sanjeeve, Erik Buchholz, Salil S. Kanhere",
        "summary": "Once analysed, location trajectories can provide valuable insights beneficial to various applications. However, such data is also highly sensitive, rendering them susceptible to privacy risks in the event of mismanagement, for example, revealing an individual's identity, home address, or political affiliations. Hence, ensuring that privacy is preserved for this data is a priority. One commonly taken measure to mitigate this concern is aggregation. Previous work by Xu et al. shows that trajectories are still recoverable from anonymised and aggregated datasets. However, the study lacks implementation details, obfuscating the mechanisms of the attack. Additionally, the attack was evaluated on commercial non-public datasets, rendering the results and subsequent claims unverifiable. This study reimplements the trajectory recovery attack from scratch and evaluates it on two open-source datasets, detailing the preprocessing steps and implementation. Results confirm that privacy leakage still exists despite common anonymisation and aggregation methods but also indicate that the initial accuracy claims may have been overly ambitious. We release all code as open-source to ensure the results are entirely reproducible and, therefore, verifiable. Moreover, we propose a stronger attack by designing a series of enhancements to the baseline attack. These enhancements yield higher accuracies by up to 16%, providing an improved benchmark for future research in trajectory recovery methods. Our improvements also enable online execution of the attack, allowing partial attacks on larger datasets previously considered unprocessable, thereby furthering the extent of privacy leakage. The findings emphasise the importance of using strong privacy-preserving mechanisms when releasing aggregated mobility data and not solely relying on aggregation as a means of anonymisation.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2024-09-23 01:06:41 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.858418"
    },
    {
        "index": "#144",
        "title": "Optimizing Feature Selection with Genetic Algorithms: A Review of Methods and Applications",
        "link": "/arxiv/2409.14563",
        "arxiv_id": "2409.14563",
        "authors": "Zhila Yaseen Taha, Abdulhady Abas Abdullah, Tarik A. Rashid",
        "summary": "Analyzing large datasets to select optimal features is one of the most important research areas in machine learning and data mining. This feature selection procedure involves dimensionality reduction which is crucial in enhancing the performance of the model, making it less complex. Recently, several types of attribute selection methods have been proposed that use different approaches to obtain representative subsets of the attributes. However, population-based evolutionary algorithms like Genetic Algorithms (GAs) have been proposed to provide remedies for these drawbacks by avoiding local optima and improving the selection process itself. This manuscript presents a sweeping review on GA-based feature selection techniques in applications and their effectiveness across different domains. This review was conducted using the PRISMA methodology; hence, the systematic identification, screening, and analysis of relevant literature were performed. Thus, our results hint that the field's hybrid GA methodologies including, but not limited to, GA-Wrapper feature selector and HGA-neural networks, have substantially improved their potential through the resolution of problems such as exploration of unnecessary search space, accuracy performance problems, and complexity. The conclusions of this paper would result in discussing the potential that GAs bear in feature selection and future research directions for their enhancement in applicability and performance.",
        "subjects": "Neural and Evolutionary Computing, Machine Learning",
        "date": "2024-09-05 22:28:42 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.861153"
    },
    {
        "index": "#145",
        "title": "Exploiting Exogenous Structure for Sample-Efficient Reinforcement Learning",
        "link": "/arxiv/2409.14557",
        "arxiv_id": "2409.14557",
        "authors": "Jia Wan, Sean R. Sinclair, Devavrat Shah, Martin J. Wainwright",
        "summary": "We study a class of structured Markov Decision Processes (MDPs) known as Exo-MDPs, characterized by a partition of the state space into two components. The exogenous states evolve stochastically in a manner not affected by the agent's actions, whereas the endogenous states are affected by the actions, and evolve in a deterministic and known way conditional on the exogenous states. Exo-MDPs are a natural model for various applications including inventory control, finance, power systems, ride sharing, among others. Despite seeming restrictive, this work establishes that any discrete MDP can be represented as an Exo-MDP. Further, Exo-MDPs induce a natural representation of the transition and reward dynamics as linear functions of the exogenous state distribution. This linear representation leads to near-optimal algorithms with regret guarantees scaling only with the (effective) size of the exogenous state space $d$, independent of the sizes of the endogenous state and action spaces. Specifically, when the exogenous state is fully observed, a simple plug-in approach achieves a regret upper bound of $O(H^{3/2}\\sqrt{dK})$, where $H$ denotes the horizon and $K$ denotes the total number of episodes. When the exogenous state is unobserved, the linear representation leads to a regret upper bound of $O(H^{3/2}d\\sqrt{K})$. We also establish a nearly matching regret lower bound of $\\Omega(Hd\\sqrt{K})$ for the no observation regime. We complement our theoretical findings with an experimental study on inventory control problems.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2024-09-22 18:45:38 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.861344"
    },
    {
        "index": "#148",
        "title": "Sliding Window Training -- Utilizing Historical Recommender Systems Data for Foundation Models",
        "link": "/arxiv/2409.14517",
        "arxiv_id": "2409.14517",
        "authors": "Swanand Joshi, Yesu Feng, Ko-Jen Hsiao, Zhe Zhang, Sudarshan Lamkhede",
        "summary": "Long-lived recommender systems (RecSys) often encounter lengthy user-item interaction histories that span many years. To effectively learn long term user preferences, Large RecSys foundation models (FM) need to encode this information in pretraining. Usually, this is done by either generating a long enough sequence length to take all history sequences as input at the cost of large model input dimension or by dropping some parts of the user history to accommodate model size and latency requirements on the production serving side. In this paper, we introduce a sliding window training technique to incorporate long user history sequences during training time without increasing the model input dimension. We show the quantitative & qualitative improvements this technique brings to the RecSys FM in learning user long term preferences. We additionally show that the average quality of items in the catalog learnt in pretraining also improves.",
        "subjects": "Information Retrieval, Machine Learning",
        "date": "2024-08-21 18:59:52 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.862000"
    },
    {
        "index": "#153",
        "title": "A High-Performance External Validity Index for Clustering with a Large Number of Clusters",
        "link": "/arxiv/2409.14455",
        "arxiv_id": "2409.14455",
        "authors": "Mohammad Yasin Karbasian, Ramin Javadi",
        "summary": "This paper introduces the Stable Matching Based Pairing (SMBP) algorithm, a high-performance external validity index for clustering evaluation in large-scale datasets with a large number of clusters. SMBP leverages the stable matching framework to pair clusters across different clustering methods, significantly reducing computational complexity to $O(N^2)$, compared to traditional Maximum Weighted Matching (MWM) with $O(N^3)$ complexity. Through comprehensive evaluations on real-world and synthetic datasets, SMBP demonstrates comparable accuracy to MWM and superior computational efficiency. It is particularly effective for balanced, unbalanced, and large-scale datasets with a large number of clusters, making it a scalable and practical solution for modern clustering tasks. Additionally, SMBP is easily implementable within machine learning frameworks like PyTorch and TensorFlow, offering a robust tool for big data applications. The algorithm is validated through extensive experiments, showcasing its potential as a powerful alternative to existing methods such as Maximum Match Measure (MMM) and Centroid Ratio (CR).",
        "subjects": "Data Structures and Algorithms, Computer Science and Game Theory, Machine Learning",
        "date": "2024-09-22 14:08:57 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.863041"
    },
    {
        "index": "#154",
        "title": "A Unified Approach for Learning the Dynamics of Power System Generators and Inverter-based Resources",
        "link": "/arxiv/2409.14454",
        "arxiv_id": "2409.14454",
        "authors": "Shaohui Liu, Weiqian Cai, Hao Zhu, Brian Johnson",
        "summary": "The growing prevalence of inverter-based resources (IBRs) for renewable energy integration and electrification greatly challenges power system dynamic analysis. To account for both synchronous generators (SGs) and IBRs, this work presents an approach for learning the model of an individual dynamic component. The recurrent neural network (RNN) model is used to match the recursive structure in predicting the key dynamical states of a component from its terminal bus voltage and set-point input. To deal with the fast transients especially due to IBRs, we develop a Stable Integral (SI-)RNN to mimic high-order integral methods that can enhance the stability and accuracy for the dynamic learning task. We demonstrate that the proposed SI-RNN model not only can successfully predict the component's dynamic behaviors, but also offers the possibility of efficiently computing the dynamic sensitivity relative to a set-point change. These capabilities have been numerically validated based on full-order Electromagnetic Transient (EMT) simulations on a small test system with both SGs and IBRs, particularly for predicting the dynamics of grid-forming inverters.",
        "subjects": "Systems and Control, Machine Learning",
        "date": "2024-09-22 14:07:10 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.863230"
    },
    {
        "index": "#158",
        "title": "A Feature Engineering Approach for Literary and Colloquial Tamil Speech Classification using 1D-CNN",
        "link": "/arxiv/2409.14348",
        "arxiv_id": "2409.14348",
        "authors": "M. Nanmalar, S. Johanan Joysingh, P. Vijayalakshmi, T. Nagarajan",
        "summary": "In ideal human computer interaction (HCI), the colloquial form of a language would be preferred by most users, since it is the form used in their day-to-day conversations. However, there is also an undeniable necessity to preserve the formal literary form. By embracing the new and preserving the old, both service to the common man (practicality) and service to the language itself (conservation) can be rendered. Hence, it is ideal for computers to have the ability to accept, process, and converse in both forms of the language, as required. To address this, it is first necessary to identify the form of the input speech, which in the current work is between literary and colloquial Tamil speech. Such a front-end system must consist of a simple, effective, and lightweight classifier that is trained on a few effective features that are capable of capturing the underlying patterns of the speech signal. To accomplish this, a one-dimensional convolutional neural network (1D-CNN) that learns the envelope of features across time, is proposed. The network is trained on a select number of handcrafted features initially, and then on Mel frequency cepstral coefficients (MFCC) for comparison. The handcrafted features were selected to address various aspects of speech such as the spectral and temporal characteristics, prosody, and voice quality. The features are initially analyzed by considering ten parallel utterances and observing the trend of each feature with respect to time. The proposed 1D-CNN, trained using the handcrafted features, offers an F1 score of 0.9803, while that trained on the MFCC offers an F1 score of 0.9895. In light of this, feature ablation and feature combination are explored. When the best ranked handcrafted features, from the feature ablation study, are combined with the MFCC, they offer the best results with an F1 score of 0.9946.",
        "subjects": "Audio and Speech Processing, Machine Learning, Sound",
        "date": "2024-09-22 07:20:42 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.864172"
    },
    {
        "index": "#161",
        "title": "A competitive baseline for deep learning enhanced data assimilation using conditional Gaussian ensemble Kalman filtering",
        "link": "/arxiv/2409.14300",
        "arxiv_id": "2409.14300",
        "authors": "Zachariah Malik, Romit Maulik",
        "summary": "Ensemble Kalman Filtering (EnKF) is a popular technique for data assimilation, with far ranging applications. However, the vanilla EnKF framework is not well-defined when perturbations are nonlinear. We study two non-linear extensions of the vanilla EnKF - dubbed the conditional-Gaussian EnKF (CG-EnKF) and the normal score EnKF (NS-EnKF) - which sidestep assumptions of linearity by constructing the Kalman gain matrix with the `conditional Gaussian' update formula in place of the traditional one. We then compare these models against a state-of-the-art deep learning based particle filter called the score filter (SF). This model uses an expensive score diffusion model for estimating densities and also requires a strong assumption on the perturbation operator for validity. In our comparison, we find that CG-EnKF and NS-EnKF dramatically outperform SF for a canonical problem in high-dimensional multiscale data assimilation given by the Lorenz-96 system. Our analysis also demonstrates that the CG-EnKF and NS-EnKF can handle highly non-Gaussian additive noise perturbations, with the latter typically outperforming the former.",
        "subjects": "Machine Learning, Machine Learning, Dynamical Systems, Atmospheric and Oceanic Physics",
        "date": "2024-09-22 02:54:33 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.864812"
    },
    {
        "index": "#162",
        "title": "Accelerated Stochastic ExtraGradient: Mixing Hessian and Gradient Similarity to Reduce Communication in Distributed and Federated Learning",
        "link": "/arxiv/2409.14280",
        "arxiv_id": "2409.14280",
        "authors": "Dmitry Bylinkin, Kirill Degtyarev, Aleksandr Beznosikov",
        "summary": "Modern realities and trends in learning require more and more generalization ability of models, which leads to an increase in both models and training sample size. It is already difficult to solve such tasks in a single device mode. This is the reason why distributed and federated learning approaches are becoming more popular every day. Distributed computing involves communication between devices, which requires solving two key problems: efficiency and privacy. One of the most well-known approaches to combat communication costs is to exploit the similarity of local data. Both Hessian similarity and homogeneous gradients have been studied in the literature, but separately. In this paper, we combine both of these assumptions in analyzing a new method that incorporates the ideas of using data similarity and clients sampling. Moreover, to address privacy concerns, we apply the technique of additional noise and analyze its impact on the convergence of the proposed method. The theory is confirmed by training on real datasets.",
        "subjects": "Optimization and Control, Machine Learning",
        "date": "2024-09-22 00:49:10 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.864994"
    },
    {
        "index": "#173",
        "title": "Are Music Foundation Models Better at Singing Voice Deepfake Detection? Far-Better Fuse them with Speech Foundation Models",
        "link": "/arxiv/2409.14131",
        "arxiv_id": "2409.14131",
        "authors": "Orchid Chetia Phukan, Sarthak Jain, Swarup Ranjan Behera, Arun Balaji Buduru, Rajesh Sharma, S. R Mahadeva Prasanna",
        "summary": "In this study, for the first time, we extensively investigate whether music foundation models (MFMs) or speech foundation models (SFMs) work better for singing voice deepfake detection (SVDD), which has recently attracted attention in the research community. For this, we perform a comprehensive comparative study of state-of-the-art (SOTA) MFMs (MERT variants and music2vec) and SFMs (pre-trained for general speech representation learning as well as speaker recognition). We show that speaker recognition SFM representations perform the best amongst all the foundation models (FMs), and this performance can be attributed to its higher efficacy in capturing the pitch, tone, intensity, etc, characteristics present in singing voices. To our end, we also explore the fusion of FMs for exploiting their complementary behavior for improved SVDD, and we propose a novel framework, FIONA for the same. With FIONA, through the synchronization of x-vector (speaker recognition SFM) and MERT-v1-330M (MFM), we report the best performance with the lowest Equal Error Rate (EER) of 13.74 %, beating all the individual FMs as well as baseline FM fusions and achieving SOTA results.",
        "subjects": "Audio and Speech Processing, Machine Learning, Sound",
        "date": "2024-09-21 12:50:53 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.871386"
    },
    {
        "index": "#175",
        "title": "Consistency for Large Neural Networks",
        "link": "/arxiv/2409.14123",
        "arxiv_id": "2409.14123",
        "authors": "Haoran Zhan, Yingcun Xia",
        "summary": "Neural networks have shown remarkable success, especially in overparameterized or \"large\" models. Despite increasing empirical evidence and intuitive understanding, a formal mathematical justification for the behavior of such models, particularly regarding overfitting, remains incomplete. In this paper, we prove that the Mean Integrated Squared Error (MISE) of neural networks with either $L^1$ or $L^2$ penalty decreases after a certain model size threshold, provided that the sample size is sufficiently large, and achieves nearly the minimax optimality in the Barron space. These results challenge conventional statistical modeling frameworks and broadens recent findings on the double descent phenomenon in neural networks. Our theoretical results also extend to deep learning models with ReLU activation functions.",
        "subjects": "Machine Learning, Machine Learning, Statistics Theory",
        "date": "2024-09-21 12:25:44 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.871770"
    },
    {
        "index": "#176",
        "title": "Efficient and Effective Model Extraction",
        "link": "/arxiv/2409.14122",
        "arxiv_id": "2409.14122",
        "authors": "Hongyu Zhu, Wentao Hu, Sichu Liang, Fangqi Li, Wenwen Wang, Shilin Wang",
        "summary": "Model extraction aims to create a functionally similar copy from a machine learning as a service (MLaaS) API with minimal overhead, often for illicit purposes or as a precursor to further attacks, posing a significant threat to the MLaaS ecosystem. However, recent studies show that model extraction is inefficient, especially when the target task distribution is unavailable. In such cases, even significantly increasing the attack budget fails to yield a sufficiently similar model, reducing the adversary's incentive. In this paper, we revisit the basic design choices throughout the extraction process and propose an efficient and effective algorithm, Efficient and Effective Model Extraction (E3), which optimizes both query preparation and the training routine. E3 achieves superior generalization over state-of-the-art methods while minimizing computational costs. For example, with only 0.005 times the query budget and less than 0.2 times the runtime, E3 outperforms classical generative model-based data-free model extraction with over 50% absolute accuracy improvement on CIFAR-10. Our findings highlight the ongoing risk of model extraction and propose E3 as a useful benchmark for future security evaluations.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2024-09-21 12:22:09 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.871980"
    },
    {
        "index": "#177",
        "title": "CONGRA: Benchmarking Automatic Conflict Resolution",
        "link": "/arxiv/2409.14121",
        "arxiv_id": "2409.14121",
        "authors": "Qingyu Zhang, Liangcai Su, Kai Ye, Chenxiong Qian",
        "summary": "Resolving conflicts from merging different software versions is a challenging task. To reduce the overhead of manual merging, researchers develop various program analysis-based tools which only solve specific types of conflicts and have a limited scope of application. With the development of language models, researchers treat conflict code as text, which theoretically allows for addressing almost all types of conflicts. However, the absence of effective conflict difficulty grading methods hinders a comprehensive evaluation of large language models (LLMs), making it difficult to gain a deeper understanding of their limitations. Furthermore, there is a notable lack of large-scale open benchmarks for evaluating the performance of LLMs in automatic conflict resolution. To address these issues, we introduce ConGra, a CONflict-GRAded benchmarking scheme designed to evaluate the performance of software merging tools under varying complexity conflict scenarios. We propose a novel approach to classify conflicts based on code operations and use it to build a large-scale evaluation dataset based on 44,948 conflicts from 34 real-world projects. We evaluate state-of-the-art LLMs on conflict resolution tasks using this dataset. By employing the dataset, we assess the performance of multiple state-of-the-art LLMs and code LLMs, ultimately uncovering two counterintuitive yet insightful phenomena. ConGra will be released at https://github.com/HKU-System-Security-Lab/ConGra.",
        "subjects": "Software Engineering, Machine Learning",
        "date": "2024-09-21 12:21:41 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.872168"
    },
    {
        "index": "#179",
        "title": "Quantum enhanced stratification of Breast Cancer: exploring quantum expressivity for real omics data",
        "link": "/arxiv/2409.14089",
        "arxiv_id": "2409.14089",
        "authors": "Valeria Repetto, Elia Giuseppe Ceroni, Giuseppe Buonaiuto, Romina D'Aurizio",
        "summary": "Quantum Machine Learning (QML) is considered one of the most promising applications of Quantum Computing in the Noisy Intermediate Scale Quantum (NISQ) era for the impact it is thought to have in the near future. Although promising theoretical assumptions, the exploration of how QML could foster new discoveries in Medicine and Biology fields is still in its infancy with few examples. In this study, we aimed to assess whether Quantum Kernels (QK) could effectively classify subtypes of Breast Cancer (BC) patients on the basis of molecular characteristics. We performed an heuristic exploration of encoding configurations with different entanglement levels to determine a trade-off between kernel expressivity and performances. Our results show that QKs yield comparable clustering results with classical methods while using fewer data points, and are able to fit the data with a higher number of clusters. Additionally, we conducted the experiments on the Quantum Processing Unit (QPU) to evaluate the effect of noise on the outcome. We found that less expressive encodings showed a higher resilience to noise, indicating that the computational pipeline can be reliably implemented on the NISQ devices. Our findings suggest that QK methods show promises for application in Precision Oncology, especially in scenarios where the dataset is limited in size and a granular non-trivial stratification of complex molecular data cannot be achieved classically.",
        "subjects": "Quantum Physics, Machine Learning",
        "date": "2024-09-21 10:00:09 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.872563"
    },
    {
        "index": "#181",
        "title": "AMT-APC: Automatic Piano Cover by Fine-Tuning an Automatic Music Transcription Model",
        "link": "/arxiv/2409.14086",
        "arxiv_id": "2409.14086",
        "authors": "Kazuma Komiya, Yoshihisa Fukuhara",
        "summary": "There have been several studies on automatically generating piano covers, and recent advancements in deep learning have enabled the creation of more sophisticated covers. However, existing automatic piano cover models still have room for improvement in terms of expressiveness and fidelity to the original. To address these issues, we propose a learning algorithm called AMT-APC, which leverages the capabilities of automatic music transcription models. By utilizing the strengths of well-established automatic music transcription models, we aim to improve the accuracy of piano cover generation. Our experiments demonstrate that the AMT-APC model reproduces original tracks more accurately than any existing models.",
        "subjects": "Sound, Machine Learning, Audio and Speech Processing",
        "date": "2024-09-21 09:51:22 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.872952"
    },
    {
        "index": "#188",
        "title": "Training Large ASR Encoders with Differential Privacy",
        "link": "/arxiv/2409.13953",
        "arxiv_id": "2409.13953",
        "authors": "Geeticka Chauhan, Steve Chien, Om Thakkar, Abhradeep Thakurta, Arun Narayanan",
        "summary": "Self-supervised learning (SSL) methods for large speech models have proven to be highly effective at ASR. With the interest in public deployment of large pre-trained models, there is a rising concern for unintended memorization and leakage of sensitive data points from the training data. In this paper, we apply differentially private (DP) pre-training to a SOTA Conformer-based encoder, and study its performance on a downstream ASR task assuming the fine-tuning data is public. This paper is the first to apply DP to SSL for ASR, investigating the DP noise tolerance of the BEST-RQ pre-training method. Notably, we introduce a novel variant of model pruning called gradient-based layer freezing that provides strong improvements in privacy-utility-compute trade-offs. Our approach yields a LibriSpeech test-clean/other WER (%) of 3.78/ 8.41 with ($10$, 1e^-9)-DP for extrapolation towards low dataset scales, and 2.81/ 5.89 with (10, 7.9e^-11)-DP for extrapolation towards high scales.",
        "subjects": "Sound, Cryptography and Security, Machine Learning, Audio and Speech Processing",
        "date": "2024-09-21 00:01:49 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.874763"
    },
    {
        "index": "#191",
        "title": "High-dimensional learning of narrow neural networks",
        "link": "/arxiv/2409.13904",
        "arxiv_id": "2409.13904",
        "authors": "Hugo Cui",
        "summary": "Recent years have been marked with the fast-pace diversification and increasing ubiquity of machine learning applications. Yet, a firm theoretical understanding of the surprising efficiency of neural networks to learn from high-dimensional data still proves largely elusive. In this endeavour, analyses inspired by statistical physics have proven instrumental, enabling the tight asymptotic characterization of the learning of neural networks in high dimensions, for a broad class of solvable models. This manuscript reviews the tools and ideas underlying recent progress in this line of work. We introduce a generic model -- the sequence multi-index model -- which encompasses numerous previously studied models as special instances. This unified framework covers a broad class of machine learning architectures with a finite number of hidden units, including multi-layer perceptrons, autoencoders, attention mechanisms; and tasks, including (un)supervised learning, denoising, contrastive learning, in the limit of large data dimension, and comparably large number of samples. We explicate in full detail the analysis of the learning of sequence multi-index models, using statistical physics techniques such as the replica method and approximate message-passing algorithms. This manuscript thus provides a unified presentation of analyses reported in several previous works, and a detailed overview of central techniques in the field of statistical physics of machine learning. This review should be a useful primer for machine learning theoreticians curious of statistical physics approaches; it should also be of value to statistical physicists interested in the transfer of such ideas to the study of neural networks.",
        "subjects": "Machine Learning, Disordered Systems and Neural Networks, Machine Learning",
        "date": "2024-09-20 21:20:04 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.875355"
    },
    {
        "index": "#192",
        "title": "PTQ4ADM: Post-Training Quantization for Efficient Text Conditional Audio Diffusion Models",
        "link": "/arxiv/2409.13894",
        "arxiv_id": "2409.13894",
        "authors": "Jayneel Vora, Aditya Krishnan, Nader Bouacida, Prabhu RV Shankar, Prasant Mohapatra",
        "summary": "Denoising diffusion models have emerged as state-of-the-art in generative tasks across image, audio, and video domains, producing high-quality, diverse, and contextually relevant data. However, their broader adoption is limited by high computational costs and large memory footprints. Post-training quantization (PTQ) offers a promising approach to mitigate these challenges by reducing model complexity through low-bandwidth parameters. Yet, direct application of PTQ to diffusion models can degrade synthesis quality due to accumulated quantization noise across multiple denoising steps, particularly in conditional tasks like text-to-audio synthesis. This work introduces PTQ4ADM, a novel framework for quantizing audio diffusion models(ADMs). Our key contributions include (1) a coverage-driven prompt augmentation method and (2) an activation-aware calibration set generation algorithm for text-conditional ADMs. These techniques ensure comprehensive coverage of audio aspects and modalities while preserving synthesis fidelity. We validate our approach on TANGO, Make-An-Audio, and AudioLDM models for text-conditional audio generation. Extensive experiments demonstrate PTQ4ADM's capability to reduce the model size by up to 70\\% while achieving synthesis quality metrics comparable to full-precision models($<$5\\% increase in FD scores). We show that specific layers in the backbone network can be quantized to 4-bit weights and 8-bit activations without significant quality loss. This work paves the way for more efficient deployment of ADMs in resource-constrained environments.",
        "subjects": "Sound, Machine Learning, Audio and Speech Processing",
        "date": "2024-09-20 20:52:56 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.875618"
    },
    {
        "index": "#194",
        "title": "Investigation of Time-Frequency Feature Combinations with Histogram Layer Time Delay Neural Networks",
        "link": "/arxiv/2409.13881",
        "arxiv_id": "2409.13881",
        "authors": "Amirmohammad Mohammadi, Iren'e Masabarakiza, Ethan Barnes, Davelle Carreiro, Alexandra Van Dine, Joshua Peeples",
        "summary": "While deep learning has reduced the prevalence of manual feature extraction, transformation of data via feature engineering remains essential for improving model performance, particularly for underwater acoustic signals. The methods by which audio signals are converted into time-frequency representations and the subsequent handling of these spectrograms can significantly impact performance. This work demonstrates the performance impact of using different combinations of time-frequency features in a histogram layer time delay neural network. An optimal set of features is identified with results indicating that specific feature combinations outperform single data features.",
        "subjects": "Sound, Machine Learning, Audio and Speech Processing",
        "date": "2024-09-20 20:22:24 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.876124"
    },
    {
        "index": "#195",
        "title": "Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models",
        "link": "/arxiv/2409.13878",
        "arxiv_id": "2409.13878",
        "authors": "Amirmohammad Mohammadi, Tejashri Kelhe, Davelle Carreiro, Alexandra Van Dine, Joshua Peeples",
        "summary": "Transfer learning is commonly employed to leverage large, pre-trained models and perform fine-tuning for downstream tasks. The most prevalent pre-trained models are initially trained using ImageNet. However, their ability to generalize can vary across different data modalities. This study compares pre-trained Audio Neural Networks (PANNs) and ImageNet pre-trained models within the context of underwater acoustic target recognition (UATR). It was observed that the ImageNet pre-trained models slightly out-perform pre-trained audio models in passive sonar classification. We also analyzed the impact of audio sampling rates for model pre-training and fine-tuning. This study contributes to transfer learning applications of UATR, illustrating the potential of pre-trained models to address limitations caused by scarce, labeled data in the UATR domain.",
        "subjects": "Sound, Machine Learning, Audio and Speech Processing",
        "date": "2024-09-20 20:13:45 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.876348"
    },
    {
        "index": "#199",
        "title": "Learning to Simulate Aerosol Dynamics with Graph Neural Networks",
        "link": "/arxiv/2409.13861",
        "arxiv_id": "2409.13861",
        "authors": "Fabiana Ferracina, Payton Beeler, Mahantesh Halappanavar, Bala Krishnamoorthy, Marco Minutoli, Laura Fierce",
        "summary": "Aerosol effects on climate, weather, and air quality depend on characteristics of individual particles, which are tremendously diverse and change in time. Particle-resolved models are the only models able to capture this diversity in particle physiochemical properties, and these models are computationally expensive. As a strategy for accelerating particle-resolved microphysics models, we introduce Graph-based Learning of Aerosol Dynamics (GLAD) and use this model to train a surrogate of the particle-resolved model PartMC-MOSAIC. GLAD implements a Graph Network-based Simulator (GNS), a machine learning framework that has been used to simulate particle-based fluid dynamics models. In GLAD, each particle is represented as a node in a graph, and the evolution of the particle population over time is simulated through learned message passing. We demonstrate our GNS approach on a simple aerosol system that includes condensation of sulfuric acid onto particles composed of sulfate, black carbon, organic carbon, and water. A graph with particles as nodes is constructed, and a graph neural network (GNN) is then trained using the model output from PartMC-MOSAIC. The trained GNN can then be used for simulating and predicting aerosol dynamics over time. Results demonstrate the framework's ability to accurately learn chemical dynamics and generalize across different scenarios, achieving efficient training and prediction times. We evaluate the performance across three scenarios, highlighting the framework's robustness and adaptability in modeling aerosol microphysics and chemistry.",
        "subjects": "Atmospheric and Oceanic Physics, Machine Learning",
        "date": "2024-09-20 19:21:43 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.877145"
    },
    {
        "index": "#201",
        "title": "Learning Ordering in Crystalline Materials with Symmetry-Aware Graph Neural Networks",
        "link": "/arxiv/2409.13851",
        "arxiv_id": "2409.13851",
        "authors": "Jiayu Peng, James Damewood, Jessica Karaguesian, Jaclyn R. Lunger, Rafael G√≥mez-Bombarelli",
        "summary": "Graph convolutional neural networks (GCNNs) have become a machine learning workhorse for screening the chemical space of crystalline materials in fields such as catalysis and energy storage, by predicting properties from structures. Multicomponent materials, however, present a unique challenge since they can exhibit chemical (dis)order, where a given lattice structure can encompass a variety of elemental arrangements ranging from highly ordered structures to fully disordered solid solutions. Critically, properties like stability, strength, and catalytic performance depend not only on structures but also on orderings. To enable rigorous materials design, it is thus critical to ensure GCNNs are capable of distinguishing among atomic orderings. However, the ordering-aware capability of GCNNs has been poorly understood. Here, we benchmark various neural network architectures for capturing the ordering-dependent energetics of multicomponent materials in a custom-made dataset generated with high-throughput atomistic simulations. Conventional symmetry-invariant GCNNs were found unable to discern the structural difference between the diverse symmetrically inequivalent atomic orderings of the same material, while symmetry-equivariant model architectures could inherently preserve and differentiate the distinct crystallographic symmetries of various orderings.",
        "subjects": "Materials Science, Machine Learning, Chemical Physics",
        "date": "2024-09-20 18:53:48 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.877621"
    },
    {
        "index": "#203",
        "title": "Physics-informed kernel learning",
        "link": "/arxiv/2409.13786",
        "arxiv_id": "2409.13786",
        "authors": "Nathan Doum√®che, Francis Bach, G√©rard Biau, Claire Boyer",
        "summary": "Physics-informed machine learning typically integrates physical priors into the learning process by minimizing a loss function that includes both a data-driven term and a partial differential equation (PDE) regularization. Building on the formulation of the problem as a kernel regression task, we use Fourier methods to approximate the associated kernel, and propose a tractable estimator that minimizes the physics-informed risk function. We refer to this approach as physics-informed kernel learning (PIKL). This framework provides theoretical guarantees, enabling the quantification of the physical prior's impact on convergence speed. We demonstrate the numerical performance of the PIKL estimator through simulations, both in the context of hybrid modeling and in solving PDEs. In particular, we show that PIKL can outperform physics-informed neural networks in terms of both accuracy and computation time. Additionally, we identify cases where PIKL surpasses traditional PDE solvers, particularly in scenarios with noisy boundary conditions.",
        "subjects": "Machine Learning, Machine Learning, Statistics Theory",
        "date": "2024-09-20 06:55:20 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.878207"
    },
    {
        "index": "#207",
        "title": "Effect of Clinical History on Predictive Model Performance for Renal Complications of Diabetes",
        "link": "/arxiv/2409.13743",
        "arxiv_id": "2409.13743",
        "authors": "Davide Dei Cas, Barbara Di Camillo, Gian Paolo Fadini, Giovanni Sparacino, Enrico Longato",
        "summary": "Diabetes is a chronic disease characterised by a high risk of developing diabetic nephropathy, which, in turn, is the leading cause of end-stage chronic kidney disease. The early identification of individuals at heightened risk of such complications or their exacerbation can be of paramount importance to set a correct course of treatment. In the present work, from the data collected in the DARWIN-Renal (DApagliflozin Real-World evIdeNce-Renal) study, a nationwide multicentre retrospective real-world study, we develop an array of logistic regression models to predict, over different prediction horizons, the crossing of clinically relevant glomerular filtration rate (eGFR) thresholds for patients with diabetes by means of variables associated with demographic, anthropometric, laboratory, pathology, and therapeutic data. In doing so, we investigate the impact of information coming from patient's past visits on the model's predictive performance, coupled with an analysis of feature importance through the Boruta algorithm. Our models yield very good performance (AUROC as high as 0.98). We also show that the introduction of information from patient's past visits leads to improved model performance of up to 4%. The usefulness of past information is further corroborated by a feature importance analysis.",
        "subjects": "Quantitative Methods, Machine Learning",
        "date": "2024-09-10 20:27:00 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.879289"
    },
    {
        "index": "#217",
        "title": "Artificial neural networks on graded vector spaces",
        "link": "/arxiv/2407.19031",
        "arxiv_id": "2407.19031",
        "authors": "T. Shaska",
        "summary": "We develop new artificial neural network models for graded vector spaces, which are suitable when different features in the data have different significance (weights). This is the first time that such models are designed mathematically and they are expected to perform better than neural networks over usual vector spaces, which are the special case when the gradings are all 1s.",
        "subjects": "Artificial Intelligence",
        "date": "2024-07-26 18:17:58 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.881791"
    },
    {
        "index": "#218",
        "title": "Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques",
        "link": "/arxiv/2304.04190",
        "arxiv_id": "2304.04190",
        "authors": "Ye Jiang",
        "summary": "This paper describes the participation of team QUST in the SemEval2023 task 3. The monolingual models are first evaluated with the under-sampling of the majority classes in the early stage of the task. Then, the pre-trained multilingual model is fine-tuned with a combination of the class weights and the sample weights. Two different fine-tuning strategies, the task-agnostic and the task-dependent, are further investigated. All experiments are conducted under the 10-fold cross-validation, the multilingual approaches are superior to the monolingual ones. The submitted system achieves the second best in Italian and Spanish (zero-shot) in subtask-1.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2023-04-09 08:14:01 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.881980"
    },
    {
        "index": "#219",
        "title": "Lecture notes on high-dimensional data",
        "link": "/arxiv/2101.05841",
        "arxiv_id": "2101.05841",
        "authors": "Sven-Ake Wegner",
        "summary": "These are lecture notes based on the first part of a course on 'Mathematical Data Science', which I taught to final year BSc students in the UK in 2019-2020. Topics include: concentration of measure in high dimensions; Gaussian random vectors in high dimensions; random projections; separation/disentangling of Gaussian data. A revised version has been published as part of the textbook [Mathematical Introduction to Data Science, Springer, Berlin, Heidelberg, 2024, https://link.springer.com/book/10.1007/978-3-662-69426-8].",
        "subjects": "Functional Analysis, Machine Learning",
        "date": "2021-01-14 19:31:44 UTC",
        "category": "cs.LG",
        "crawl_time": "2025-09-24T16:31:00.882152"
    }
]