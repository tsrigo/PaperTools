[
    {
        "index": "#1",
        "title": "Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading",
        "link": "/arxiv/2510.04787",
        "arxiv_id": "2510.04787",
        "authors": "Zifan Song, Kaitao Song, Guosheng Hu, Ding Qi, Junyao Gao, Xiaohua Wang, Dongsheng Li, Cairong Zhao",
        "summary": "Recent advancements in large language models (LLMs) and agentic systems have shown exceptional decision-making capabilities, revealing significant potential for autonomic finance. Current financial trading agents predominantly simulate anthropomorphic roles that inadvertently introduce emotional biases and rely on peripheral information, while being constrained by the necessity for continuous inference during deployment. In this paper, we pioneer the harmonization of strategic depth in agents with the mechanical rationality essential for quantitative trading. Consequently, we present TiMi (Trade in Minutes), a rationality-driven multi-agent system that architecturally decouples strategy development from minute-level deployment. TiMi leverages specialized LLM capabilities of semantic analysis, code programming, and mathematical reasoning within a comprehensive policy-optimization-deployment chain. Specifically, we propose a two-tier analytical paradigm from macro patterns to micro customization, layered programming design for trading bot implementation, and closed-loop optimization driven by mathematical reflection. Extensive evaluations across 200+ trading pairs in stock and cryptocurrency markets empirically validate the efficacy of TiMi in stable profitability, action efficiency, and risk control under volatile market dynamics.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.835078"
    },
    {
        "index": "#2",
        "title": "NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social Simulation Environment",
        "link": "/arxiv/2510.04368",
        "arxiv_id": "2510.04368",
        "authors": "Shashank Mangla, Chris Hokamp, Jack Boylan, Demian Gholipour Ghalandari, Yuuv Jauhari, Lauren Cassidy, Oisin Duffy",
        "summary": "We design and implement NegotiationGym, an API and user interface for configuring and running multi-agent social simulations focused upon negotiation and cooperation. The NegotiationGym codebase offers a user-friendly, configuration-driven API that enables easy design and customization of simulation scenarios. Agent-level utility functions encode optimization criteria for each agent, and agents can self-optimize by conducting multiple interaction rounds with other agents, observing outcomes, and modifying their strategies for future rounds.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.835511"
    },
    {
        "index": "#3",
        "title": "Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs",
        "link": "/arxiv/2510.04303",
        "arxiv_id": "2510.04303",
        "authors": "Om Tailor",
        "summary": "Multi-agent deployments of large language models (LLMs) are increasingly embedded in market, allocation, and governance workflows, yet covert coordination among agents can silently erode trust and social welfare. Existing audits are dominated by heuristics that lack theoretical guarantees, struggle to transfer across tasks, and seldom ship with the infrastructure needed for independent replication. We introduce \\emph{Audit the Whisper}, a conference-grade research artifact that spans theory, benchmark design, detection, and reproducibility. Our contributions are: (i) a channel-capacity analysis showing how interventions such as paraphrase, rate limiting, and role permutation impose quantifiable capacity penalties -- operationalized via paired-run Kullback--Leibler diagnostics -- that tighten mutual-information thresholds with finite-sample guarantees; (ii) \\textsc{ColludeBench}-v0, covering pricing, first-price auctions, and peer review with configurable covert schemes, deterministic manifests, and reward instrumentation; and (iii) a calibrated auditing pipeline that fuses cross-run mutual information, permutation invariance, watermark variance, and fairness-aware acceptance bias, each tuned to a \\(10^{-3}\\) false-positive budget. Across 600 audited runs spanning 12 intervention conditions, the union meta-test attains TPR~$=1$ with zero observed false alarms, while ablations surface the price-of-auditing trade-off and highlight fairness-driven colluders invisible to MI alone. We release regeneration scripts, seed-stamped manifests, and documentation so that external auditors can reproduce every figure and extend the framework with minimal effort.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.835877"
    },
    {
        "index": "#4",
        "title": "Small Fleet, Big Impact: Enhancing Shared Micromobility Efficiency through Minimal Autonomous Vehicle Deployment",
        "link": "/arxiv/2510.04271",
        "arxiv_id": "2510.04271",
        "authors": "Heng Tan, Hua Yan, Lucas Yang, Yu Yang",
        "summary": "Shared micromobility systems, such as electric scooters and bikes, have gained widespread popularity as sustainable alternatives to traditional transportation modes. However, these systems face persistent challenges due to spatio-temporal demand fluctuations, often resulting in a mismatch between vehicle supply and user demand. Existing shared micromobility vehicle scheduling methods typically redistribute vehicles once or twice per day, which makes them vulnerable to performance degradation under atypical conditions. In this work, we design to augment existing micromobility scheduling methods by integrating a small number of autonomous shared micromobility vehicles (ASMVs), which possess self-rebalancing capabilities to dynamically adapt to real-time demand. Specifically, we introduce SMART, a hierarchical reinforcement learning framework that jointly optimizes high-level initial deployment and low-level real-time rebalancing for ASMVs. We evaluate our framework based on real-world e-scooter usage data from Chicago. Our experiment results show that our framework is highly effective and possesses strong generalization capability, allowing it to seamlessly integrate with existing vehicle scheduling methods and significantly enhance overall micromobility service performance.",
        "subjects": "Multiagent Systems",
        "date": "2025-10-05",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.836234"
    },
    {
        "index": "#5",
        "title": "Cooperative Flexibility Exchange: Fair and Comfort-Aware Decentralized Resource Allocation",
        "link": "/arxiv/2510.04192",
        "arxiv_id": "2510.04192",
        "authors": "Rabiya Khalid, Evangelos Pournaras",
        "summary": "The growing electricity demand and increased use of smart appliances are placing new pressures on power grids, making efficient energy management more important than ever. The existing energy management systems often prioritize system efficiency (balanced energy demand and supply) at the expense of user comfort. This paper addresses this gap by proposing a novel decentralized multi-agent coordination-based demand-side management system. The proposed system enables individual agents to coordinate for demand-side energy optimization while improving the user comfort and maintaining the system efficiency. A key innovation of this work is the introduction of a slot exchange mechanism, where agents first receive optimized appliance-level energy consumption schedules and then coordinate with each other to adjust these schedules through slot exchanges. This approach improves user comfort even when agents show non-altruistic behaviour, and it scales well with large populations. The system also promotes fairness by balancing satisfaction levels across users. For performance evaluation, a real-world dataset is used, and the results demonstrate that the proposed slot exchange mechanism increases user comfort and fairness without raising system inefficiency cost, making it a practical and scalable solution for future smart grids.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.836588"
    },
    {
        "index": "#6",
        "title": "Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning",
        "link": "/arxiv/2510.03534",
        "arxiv_id": "2510.03534",
        "authors": "Nicolò Dal Fabbro, Milad Mesbahi, Renato Mendes, João Borges de Sousa, George J. Pappas",
        "summary": "We study the problem of long-term (multiple days) mapping of a river plume using multiple autonomous underwater vehicles (AUVs), focusing on the Douro river representative use-case. We propose an energy - and communication - efficient multi-agent reinforcement learning approach in which a central coordinator intermittently communicates with the AUVs, collecting measurements and issuing commands. Our approach integrates spatiotemporal Gaussian process regression (GPR) with a multi-head Q-network controller that regulates direction and speed for each AUV. Simulations using the Delft3D ocean model demonstrate that our method consistently outperforms both single- and multi-agent benchmarks, with scaling the number of agents both improving mean squared error (MSE) and operational endurance. In some instances, our algorithm demonstrates that doubling the number of AUVs can more than double endurance while maintaining or improving accuracy, underscoring the benefits of multi-agent coordination. Our learned policies generalize across unseen seasonal regimes over different months and years, demonstrating promise for future developments of data-driven long-term monitoring of dynamic plume environments.",
        "subjects": "Multiagent Systems, Machine Learning, Systems and Control, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.837002"
    },
    {
        "index": "#7",
        "title": "LegalSim: Multi-Agent Simulation of Legal Systems for Discovering Procedural Exploits",
        "link": "/arxiv/2510.03405",
        "arxiv_id": "2510.03405",
        "authors": "Sanket Badhe",
        "summary": "We present LegalSim, a modular multi-agent simulation of adversarial legal proceedings that explores how AI systems can exploit procedural weaknesses in codified rules. Plaintiff and defendant agents choose from a constrained action space (for example, discovery requests, motions, meet-and-confer, sanctions) governed by a JSON rules engine, while a stochastic judge model with calibrated grant rates, cost allocations, and sanction tendencies resolves outcomes. We compare four policies: PPO, a contextual bandit with an LLM, a direct LLM policy, and a hand-crafted heuristic; Instead of optimizing binary case outcomes, agents are trained and evaluated using effective win rate and a composite exploit score that combines opponent-cost inflation, calendar pressure, settlement pressure at low merit, and a rule-compliance margin. Across configurable regimes (e.g., bankruptcy stays, inter partes review, tax procedures) and heterogeneous judges, we observe emergent ``exploit chains'', such as cost-inflating discovery sequences and calendar-pressure tactics that remain procedurally valid yet systemically harmful. Evaluation via cross-play and Bradley-Terry ratings shows, PPO wins more often, the bandit is the most consistently competitive across opponents, the LLM trails them, and the heuristic is weakest. The results are stable in judge settings, and the simulation reveals emergent exploit chains, motivating red-teaming of legal rule systems in addition to model-level testing.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-03",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.837359"
    },
    {
        "index": "#8",
        "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "link": "/arxiv/2510.05096",
        "arxiv_id": "2510.05096",
        "authors": "Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou",
        "summary": "Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce PaperTalker, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language, Multiagent Systems, Multimedia",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.837766"
    },
    {
        "index": "#9",
        "title": "A Fixed Point Framework for the Existence of EFX Allocations",
        "link": "/arxiv/2510.04915",
        "arxiv_id": "2510.04915",
        "authors": "S. Rasoul Etesami",
        "summary": "We consider the problem of the existence of an envy-free allocation up to any good (EFX) for linear valuations and establish new results by connecting this problem to a fixed point framework. Specifically, we first use randomized rounding to extend the discrete EFX constraints into a continuous space and show that an EFX allocation exists if and only if the optimal value of the continuously extended objective function is nonpositive. In particular, we demonstrate that this optimization problem can be formulated as an unconstrained difference of convex (DC) program, which can be further simplified to the minimization of a piecewise linear concave function over a polytope. Leveraging this connection, we show that the proposed DC program has a nonpositive optimal objective value if and only if a well-defined continuous vector map admits a fixed point. Crucially, we prove that the reformulated fixed point problem satisfies all the conditions of Brouwer's fixed point theorem, except that self-containedness is violated by an arbitrarily small positive constant. To address this, we propose a slightly perturbed continuous map that always admits a fixed point. This fixed point serves as a proxy for the fixed point (if it exists) of the original map, and hence for an EFX allocation through an appropriate transformation. Our results offer a new approach to establishing the existence of EFX allocations through fixed point theorems. Moreover, the equivalence with DC programming enables a more efficient and systematic method for computing such allocations (if one exists) using tools from nonlinear optimization. Our findings bridge the discrete problem of finding an EFX allocation with two continuous frameworks: solving an unconstrained DC program and identifying a fixed point of a continuous vector map.",
        "subjects": "Computer Science and Game Theory, Multiagent Systems, Systems and Control, Optimization and Control",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.838131"
    },
    {
        "index": "#10",
        "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution",
        "link": "/arxiv/2510.04886",
        "arxiv_id": "2510.04886",
        "authors": "Adi Banerjee, Anirudh Nair, Tarik Borogovac",
        "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents a significant challenge in debugging and improving collaborative AI systems. Current approaches to pinpointing agent and step level failures in interaction traces - whether using all-at-once evaluation, step-by-step analysis, or binary search - fall short when analyzing complex patterns, struggling with both accuracy and consistency. We present ECHO (Error attribution through Contextual Hierarchy and Objective consensus analysis), a novel algorithm that combines hierarchical context representation, objective analysis-based evaluation, and consensus voting to improve error attribution accuracy. Our approach leverages a positional-based leveling of contextual understanding while maintaining objective evaluation criteria, ultimately reaching conclusions through a consensus mechanism. Experimental results demonstrate that ECHO outperforms existing methods across various multi-agent interaction scenarios, showing particular strength in cases involving subtle reasoning errors and complex interdependencies. Our findings suggest that leveraging these concepts of structured, hierarchical context representation combined with consensus-based objective decision-making, provides a more robust framework for error attribution in multi-agent systems.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.838499"
    },
    {
        "index": "#11",
        "title": "Video Game Level Design as a Multi-Agent Reinforcement Learning Problem",
        "link": "/arxiv/2510.04862",
        "arxiv_id": "2510.04862",
        "authors": "Sam Earle, Zehua Jiang, Eugene Vinitsky, Julian Togelius",
        "summary": "Procedural Content Generation via Reinforcement Learning (PCGRL) offers a method for training controllable level designer agents without the need for human datasets, using metrics that serve as proxies for level quality as rewards. Existing PCGRL research focuses on single generator agents, but are bottlenecked by the need to frequently recalculate heuristics of level quality and the agent's need to navigate around potentially large maps. By framing level generation as a multi-agent problem, we mitigate the efficiency bottleneck of single-agent PCGRL by reducing the number of reward calculations relative to the number of agent actions. We also find that multi-agent level generators are better able to generalize to out-of-distribution map shapes, which we argue is due to the generators' learning more local, modular design policies. We conclude that treating content generation as a distributed, multi-agent task is beneficial for generating functional artifacts at scale.",
        "subjects": "Artificial Intelligence, Machine Learning, Multiagent Systems, Neural and Evolutionary Computing",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.838903"
    },
    {
        "index": "#12",
        "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation",
        "link": "/arxiv/2510.04851",
        "arxiv_id": "2510.04851",
        "authors": "Dongge Han, Camille Couturier, Daniel Madrigal Diaz, Xuchao Zhang, Victor Rühle, Saravan Rajmohan",
        "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent large language model (LLM) systems in workflow automation. LEGOMem decomposes past task trajectories into reusable memory units and flexibly allocates them across orchestrators and task agents to support planning and execution. To explore the design space of memory in multi-agent systems, we use LEGOMem as a lens and conduct a systematic study of procedural memory in multi-agent systems, examining where memory should be placed, how it should be retrieved, and which agents benefit most. Experiments on the OfficeBench benchmark show that orchestrator memory is critical for effective task decomposition and delegation, while fine-grained agent memory improves execution accuracy. We find that even teams composed of smaller language models can benefit substantially from procedural memory, narrowing the performance gap with stronger agents by leveraging prior execution traces for more accurate planning and tool use. These results position LEGOMem as both a practical framework for memory-augmented agent systems and a research tool for understanding memory design in multi-agent workflow automation.",
        "subjects": "Artificial Intelligence, Machine Learning, Multiagent Systems",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.839320"
    },
    {
        "index": "#13",
        "title": "Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy",
        "link": "/arxiv/2510.04774",
        "arxiv_id": "2510.04774",
        "authors": "Weixu Zhu, Marco Dorigo, Mary Katherine Heinrich",
        "summary": "Our recently introduced self-organizing nervous system (SoNS) provides robot swarms with 1) ease of behavior design and 2) global estimation of the swarm configuration and its collective environment, facilitating the implementation of online automatic code generation for robot swarms. In a demonstration with 6 real robots and simulation trials with >30 robots, we show that when a SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code generated by an external LLM on the fly, completing its mission with an 85% success rate.",
        "subjects": "Robotics, Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.839694"
    },
    {
        "index": "#14",
        "title": "Fairness in Repeated Matching: A Maximin Perspective",
        "link": "/arxiv/2510.04624",
        "arxiv_id": "2510.04624",
        "authors": "Eugene Lim, Tzeh Yuan Neoh, Nicholas Teh",
        "summary": "We study a sequential decision-making model where a set of items is repeatedly matched to the same set of agents over multiple rounds. The objective is to determine a sequence of matchings that either maximizes the utility of the least advantaged agent at the end of all rounds (optimal) or at the end of every individual round (anytime optimal). We investigate the computational challenges associated with finding (anytime) optimal outcomes and demonstrate that these problems are generally computationally intractable. However, we provide approximation algorithms, fixed-parameter tractable algorithms, and identify several special cases whereby the problem(s) can be solved efficiently. Along the way, we also establish characterizations of Pareto-optimal/maximum matchings, which may be of independent interest to works in matching theory and house allocation.",
        "subjects": "Computer Science and Game Theory, Artificial Intelligence, Machine Learning, Multiagent Systems, Theoretical Economics",
        "date": "2025-10-06",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.840106"
    },
    {
        "index": "#15",
        "title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems",
        "link": "/arxiv/2510.04371",
        "arxiv_id": "2510.04371",
        "authors": "Naimeng Ye, Arnav Ahuja, Georgios Liargkovas, Yunan Lu, Kostis Kaffes, Tianyi Peng",
        "summary": "Despite growing interest in AI agents across industry and academia, their execution in an environment is often slow, hampering training, evaluation, and deployment. For example, a game of chess between two state-of-the-art agents may take hours. A critical bottleneck is that agent behavior unfolds sequentially: each action requires an API call, and these calls can be time-consuming. Inspired by speculative execution in microprocessors and speculative decoding in LLM inference, we propose speculative actions, a lossless framework for general agentic systems that predicts likely actions using faster models, enabling multiple steps to be executed in parallel. We evaluate this framework across three agentic environments: gaming, e-commerce, web search, and a \"lossy\" extension for an operating systems environment. In all cases, speculative actions achieve substantial accuracy in next-action prediction (up to 55%), translating into significant reductions in end-to-end latency. Moreover, performance can be further improved through stronger guessing models, top-K action prediction, multi-step speculation, and uncertainty-aware optimization, opening a promising path toward deploying low-latency agentic systems in the real world.",
        "subjects": "Artificial Intelligence, Distributed, Parallel, and Cluster Computing, Multiagent Systems",
        "date": "2025-10-05",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.840514"
    },
    {
        "index": "#16",
        "title": "Strategy Logic, Imperfect Information, and Hyperproperties",
        "link": "/arxiv/2510.03952",
        "arxiv_id": "2510.03952",
        "authors": "Raven Beutner, Bernd Finkbeiner",
        "summary": "Strategy logic (SL) is a powerful temporal logic that enables first-class reasoning over strategic behavior in multi-agent systems (MAS). In many MASs, the agents (and their strategies) cannot observe the global state of the system, leading to many extensions of SL centered around imperfect information, such as strategy logic with imperfect information (SL$_\\mathit{ii}$). Along orthogonal lines, researchers have studied the combination of strategic behavior and hyperproperties. Hyperproperties are system properties that relate multiple executions in a system and commonly arise when specifying security policies. Hyper Strategy Logic (HyperSL) is a temporal logic that combines quantification over strategies with the ability to express hyperproperties on the executions of different strategy profiles. In this paper, we study the relation between SL$_\\mathit{ii}$ and HyperSL. Our main result is that both logics (restricted to formulas where no state formulas are nested within path formulas) are equivalent in the sense that we can encode SL$_\\mathit{ii}$ instances into HyperSL instances and vice versa. For the former direction, we build on the well-known observation that imperfect information is a hyperproperty. For the latter direction, we construct a self-composition of MASs and show how we can simulate hyperproperties using imperfect information.",
        "subjects": "Logic in Computer Science, Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-04",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.840882"
    },
    {
        "index": "#17",
        "title": "Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning",
        "link": "/arxiv/2510.03823",
        "arxiv_id": "2510.03823",
        "authors": "Adam Haroon, Tristan Schuler",
        "summary": "High Altitude Balloons (HABs) can leverage stratospheric wind layers for limited horizontal control, enabling applications in reconnaissance, environmental monitoring, and communications networks. Existing multi-agent HAB coordination approaches use deterministic methods like Voronoi partitioning and extremum seeking control for large global constellations, which perform poorly for smaller teams and localized missions. While single-agent HAB control using reinforcement learning has been demonstrated on HABs, coordinated multi-agent reinforcement learning (MARL) has not yet been investigated. This work presents the first systematic application of multi-agent reinforcement learning (MARL) to HAB coordination for distributed area coverage. We extend our previously developed reinforcement learning simulation environment (RLHAB) to support cooperative multi-agent learning, enabling multiple agents to operate simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area coverage coordination, leveraging Centralized Training with Decentralized Execution to address atmospheric vehicle coordination challenges. Our approach employs specialized observation spaces providing individual state, environmental context, and teammate data, with hierarchical rewards prioritizing coverage while encouraging spatial distribution. We demonstrate that QMIX achieves similar performance to the theoretically optimal geometric deterministic method for distributed area coverage, validating the MARL approach and providing a foundation for more complex autonomous multi-HAB missions where deterministic methods become intractable.",
        "subjects": "Machine Learning, Multiagent Systems, Robotics",
        "date": "2025-10-04",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.841245"
    },
    {
        "index": "#18",
        "title": "Cooperation in public goods game on regular lattices with agents changing interaction groups",
        "link": "/arxiv/2510.03772",
        "arxiv_id": "2510.03772",
        "authors": "Jarosław Adam Miszczak",
        "summary": "The emergence of cooperation in the groups of interacting agents is one of the most fascinating phenomena observed in many complex systems studied in social science and ecology, even in the situations where one would expect the agent to use a free-rider policy. This is especially surprising in the situation where no external mechanisms based on reputation or punishment are present. One of the possible explanations of this effect is the inhomogeneity of the various aspects of interactions, which can be used to clarify the seemingly paradoxical behavior. In this report we demonstrate that the diversity of interaction networks helps to some degree to explain the emergence of cooperation. We extend the model of spatial interaction diversity introduced in [L. Shang et al., Physica A, 593:126999 (2022)] by enabling the evaluation of the interaction groups. We show that the process of the reevaluation of the interaction group facilitates the emergence of cooperation. Furthermore, we also observe that a significant participation of agents switching their interaction neighborhoods has a negative impact on the formation of cooperation. The introduced scenario can help to understand the formation of cooperation in the systems where no additional mechanisms for controlling agents are included.",
        "subjects": "Physics and Society, Multiagent Systems, Adaptation and Self-Organizing Systems",
        "date": "2025-10-04",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.841605"
    },
    {
        "index": "#19",
        "title": "Deep Reinforcement Learning for Multi-Agent Coordination",
        "link": "/arxiv/2510.03592",
        "arxiv_id": "2510.03592",
        "authors": "Kehinde O. Aina, Sehoon Ha",
        "summary": "We address the challenge of coordinating multiple robots in narrow and confined environments, where congestion and interference often hinder collective task performance. Drawing inspiration from insect colonies, which achieve robust coordination through stigmergy -- modifying and interpreting environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement Learning (S-MADRL) framework that leverages virtual pheromones to model local and social interactions, enabling decentralized emergent coordination without explicit communication. To overcome the convergence and scalability limitations of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum learning, which decomposes complex tasks into progressively harder sub-problems. Simulation results show that our framework achieves the most effective coordination of up to eight agents, where robots self-organize into asymmetric workload distributions that reduce congestion and modulate group performance. This emergent behavior, analogous to strategies observed in nature, demonstrates a scalable solution for decentralized multi-agent coordination in crowded environments with communication constraints.",
        "subjects": "Machine Learning, Artificial Intelligence, Multiagent Systems, Robotics",
        "date": "2025-10-04",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.841985"
    },
    {
        "index": "#20",
        "title": "REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement",
        "link": "/arxiv/2510.03588",
        "arxiv_id": "2510.03588",
        "authors": "Anvith Pabba, Simin Chen, Alex Mathai, Anindya Chakraborty, Baishakhi Ray",
        "summary": "Large Language Models (LLMs) have recently shown strong potential in automatic program repair (APR), especially in repository-level settings where the goal is to generate patches based on natural language issue descriptions, large codebases, and regression tests. However, despite their promise, current LLM-based APR techniques often struggle to produce correct fixes due to limited understanding of code context and over-reliance on incomplete test suites. As a result, they frequently generate Draft Patches-partially correct patches that either incompletely address the bug or overfit to the test cases. In this work, we propose a novel patch refinement framework, Refine, that systematically transforms Draft Patches into correct ones. Refine addresses three key challenges: disambiguating vague issue and code context, diversifying patch candidates through test-time scaling, and aggregating partial fixes via an LLM-powered code review process. We implement Refine as a general refinement module that can be integrated into both open-agent-based and workflow-based APR systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine achieves state-of-the-art results among workflow-based approaches and approaches the best-known performance across all APR categories. Specifically, Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of 51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine improves the resolution rate by 12.2%, and when integrated across multiple APR systems, it yields an average improvement of 14%-demonstrating its broad effectiveness and generalizability. These results highlight the effectiveness of refinement as a missing component in current APR pipelines and the potential of agentic collaboration in closing the gap between near-correct and correct patches. We also open source our code.",
        "subjects": "Software Engineering, Multiagent Systems",
        "date": "2025-10-04",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.842368"
    },
    {
        "index": "#21",
        "title": "Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems",
        "link": "/arxiv/2510.03472",
        "arxiv_id": "2510.03472",
        "authors": "Yulun Zhang, Alexandre O. G. Barbosa, Federico Pecora, Jiaoyang Li",
        "summary": "We study optimizing a destination-to-chutes task mapping to improve throughput in Robotic Sorting Systems (RSS), where a team of robots sort packages on a sortation floor by transporting them from induct workstations to eject chutes based on their shipping destinations (e.g. Los Angeles or Pittsburgh). The destination-to-chutes task mapping is used to determine which chutes a robot can drop its package. Finding a high-quality task mapping is challenging because of the complexity of a real-world RSS. First, optimizing task mapping is interdependent with robot target assignment and path planning. Second, chutes will be CLOSED for a period of time once they receive sufficient packages to allow for downstream processing. Third, task mapping quality directly impacts the downstream processing, as scattered chutes for the same destination increase package handling time. In this paper, we first formally define task mappings and the problem of Task Mapping Optimization (TMO). We then present a simulator of RSS to evaluate task mappings. We then present a simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear Programming, demonstrating the advantage of our optimized task mappings over the greedily generated ones in various RSS setups with different map sizes, numbers of chutes, and destinations. Finally, we use Quality Diversity algorithms to analyze the throughput of a diverse set of task mappings. Our code is available online at https://github.com/lunjohnzhang/tmo_public.",
        "subjects": "Robotics, Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-03",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.842753"
    },
    {
        "index": "#22",
        "title": "Downside Risk-Aware Equilibria for Strategic Decision-Making",
        "link": "/arxiv/2510.03446",
        "arxiv_id": "2510.03446",
        "authors": "Oliver Slumbers, Benjamin Patrick Evans, Sumitra Ganesh, Leo Ardon",
        "summary": "Game theory has traditionally had a relatively limited view of risk based on how a player's expected reward is impacted by the uncertainty of the actions of other players. Recently, a new game-theoretic approach provides a more holistic view of risk also considering the reward-variance. However, these variance-based approaches measure variance of the reward on both the upside and downside. In many domains, such as finance, downside risk only is of key importance, as this represents the potential losses associated with a decision. In contrast, large upside \"risk\" (e.g. profits) are not an issue. To address this restrictive view of risk, we propose a novel solution concept, downside risk aware equilibria (DRAE) based on lower partial moments. DRAE restricts downside risk, while placing no restrictions on upside risk, and additionally, models higher-order risk preferences. We demonstrate the applicability of DRAE on several games, successfully finding equilibria which balance downside risk with expected reward, and prove the existence and optimality of this equilibria.",
        "subjects": "Computer Science and Game Theory, Multiagent Systems, General Economics, Risk Management",
        "date": "2025-10-03",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.843153"
    },
    {
        "index": "#23",
        "title": "The Argument is the Explanation: Structured Argumentation for Trust in Agents",
        "link": "/arxiv/2510.03442",
        "arxiv_id": "2510.03442",
        "authors": "Ege Cakar, Per Ola Kristensson",
        "summary": "Humans are black boxes -- we cannot observe their neural processes, yet society functions by evaluating verifiable arguments. AI explainability should follow this principle: stakeholders need verifiable reasoning chains, not mechanistic transparency. We propose using structured argumentation to provide a level of explanation and verification neither interpretability nor LLM-generated explanation is able to offer. Our pipeline achieves state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7 points above prior work) and $0.81$ macro F1, $\\sim$0.07 above previous published results with comparable data setups, for Argumentative MicroTexts relation classification, converting LLM text into argument graphs and enabling verification at each inferential step. We demonstrate this idea on multi-agent risk assessment using the Structured What-If Technique, where specialized agents collaborate transparently to carry out risk assessment otherwise achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we capture support/attack relationships, thereby enabling automatic hallucination detection via fact nodes attacking arguments. We also provide a verification mechanism that enables iterative refinement through test-time feedback without retraining. For easy deployment, we provide a Docker container for the fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python package on GitHub.",
        "subjects": "Machine Learning, Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-03",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.843518"
    },
    {
        "index": "#24",
        "title": "ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection",
        "link": "/arxiv/2510.03418",
        "arxiv_id": "2510.03418",
        "authors": "Ananya Mantravadi, Shivali Dalmia, Abhishek Mukherji, Nand Dave, Anudha Mittal",
        "summary": "Retrieval-Augmented Generation (RAG) integrates LLMs with external sources, offering advanced capabilities for information access and decision-making. However, contradictions in retrieved evidence can result in inconsistent or untrustworthy outputs, which is especially problematic in enterprise settings where compliance, governance, and accountability are critical. Existing benchmarks for contradiction detection are limited to sentence-level analysis and do not capture the complexity of enterprise documents such as contracts, financial filings, compliance reports, or policy manuals. To address this limitation, we propose ContraGen, a contradiction-aware benchmark framework tailored to enterprise domain. The framework generates synthetic enterprise-style documents with embedded contradictions, enabling systematic evaluation of both intra-document and cross-document consistency. Automated contradiction mining is combined with human-in-the-loop validation to ensure high accuracy. Our contributions include generating realistic enterprise documents, modeling a taxonomy of contradiction types common in business processes, enabling controlled creation of self- and pairwise contradictions, developing a contradiction-aware retrieval evaluation pipeline and embedding human oversight to reflect domain-specific judgment complexity. This work establishes a foundation for more trustworthy and accountable RAG systems in enterprise information-seeking applications, where detecting and resolving contradictions is essential for reducing risk and ensuring compliance.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-03",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.843922"
    },
    {
        "index": "#25",
        "title": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing",
        "link": "/arxiv/2510.03346",
        "arxiv_id": "2510.03346",
        "authors": "Xiangyu Shi, Marco Chiesa, Gerald Q. Maguire Jr., Dejan Kostic",
        "summary": "Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30\\% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems.",
        "subjects": "Machine Learning, Artificial Intelligence, Multiagent Systems",
        "date": "2025-10-02",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.844311"
    },
    {
        "index": "#26",
        "title": "Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?",
        "link": "/arxiv/2510.03257",
        "arxiv_id": "2510.03257",
        "authors": "Zijian Zhao, Sen Li",
        "summary": "On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate real-time challenge of bundling and matching passengers-each with distinct origins and destinations-to available vehicles, all while navigating significant system uncertainties. Due to the extensive observation space arising from the large number of drivers and orders, order dispatching, though fundamentally a centralized task, is often addressed using Multi-Agent Reinforcement Learning (MARL). However, independent MARL methods fail to capture global information and exhibit poor cooperation among workers, while Centralized Training Decentralized Execution (CTDE) MARL methods suffer from the curse of dimensionality. To overcome these challenges, we propose Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method designed specifically for large-scale order dispatching on ride-sharing platforms. Built on a variant TD3, our approach addresses the vast action space through an action decomposition strategy that breaks down the joint action probability into individual driver action probabilities. To handle the extensive observation space, we introduce a novel BERT-based network, where parameter reuse mitigates parameter growth as the number of drivers and orders increases, and the attention mechanism effectively captures the complex relationships among the large pool of driver and orders. We validate our method using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves approximately an 11.95% improvement over current state-of-the-art methods, with a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our code, trained model parameters, and processed data are publicly available at the repository https://github.com/RS2002/Triple-BERT .",
        "subjects": "Machine Learning, Artificial Intelligence, Multiagent Systems",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-08T08:16:48.844673"
    },
    {
        "index": "#1",
        "title": "Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models",
        "link": "/arxiv/2510.05090",
        "arxiv_id": "2510.05090",
        "authors": "Runchu Tian, Junxia Cui, Xueqiang Xu, Feng Yao, Jingbo Shang",
        "summary": "Diffusion large language models (dLLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering advantages such as accelerated parallel decoding and bidirectional context modeling. However, the vanilla decoding strategy in discrete dLLMs suffers from a critical limitation: once a token is accepted, it can no longer be revised in subsequent steps. As a result, early mistakes persist across iterations, harming both intermediate predictions and final output quality. To address this issue, we propose Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding strategy that leverages cross-validation among predicted tokens. Unlike existing methods that follow a single progressive unmasking procedure, Tolerator introduces a two-stage process: (i) sequence fill-up and (ii) iterative refinement by remasking and decoding a subset of tokens while treating the remaining as context. This design enables previously accepted tokens to be reconsidered and corrected when necessary, leading to more reliable diffusion decoding outputs. We evaluate Tolerator on five standard benchmarks covering language understanding, code generation, and mathematics. Experiments show that our method achieves consistent improvements over the baselines under the same computational budget. These findings suggest that decoding algorithms are crucial to realizing the full potential of diffusion large language models. Code and data are publicly available.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.368456"
    },
    {
        "index": "#2",
        "title": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data",
        "link": "/arxiv/2510.05087",
        "arxiv_id": "2510.05087",
        "authors": "Janos Perczel, Jin Chow, Dorottya Demszky",
        "summary": "The promise of generative AI to revolutionize education is constrained by the pedagogical limits of large language models (LLMs). A major issue is the lack of access to high-quality training data that reflect the learning of actual students. Prompt engineering has emerged as a stopgap, but the ability of prompts to encode complex pedagogical strategies in rule-based natural language is inherently limited. To address this gap we introduce TeachLM - an LLM optimized for teaching through parameter-efficient fine-tuning of state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000 hours of one-on-one, longitudinal student-tutor interactions maintained by Polygence, which underwent a rigorous anonymization process to protect privacy. We use parameter-efficient fine-tuning to develop an authentic student model that enables the generation of high-fidelity synthetic student-tutor dialogues. Building on this capability, we propose a novel multi-turn evaluation protocol that leverages synthetic dialogue generation to provide fast, scalable, and reproducible assessments of the dialogical capabilities of LLMs. Our evaluations demonstrate that fine-tuning on authentic learning data significantly improves conversational and pedagogical performance - doubling student talk time, improving questioning style, increasing dialogue turns by 50%, and greater personalization of instruction.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.369416"
    },
    {
        "index": "#3",
        "title": "Slm-mux: Orchestrating small language models for reasoning",
        "link": "/arxiv/2510.05077",
        "arxiv_id": "2510.05077",
        "authors": "Chenyu Wang, Zishen Wan, Hao Kang, Emma Chen, Zhiqiang Xie, Tushar Krishna, Vijay Janapa Reddi, Yilun Du",
        "summary": "With the rapid development of language models, the number of small language models (SLMs) has grown significantly. Although they do not achieve state-of-the-art accuracy, they are more efficient and often excel at specific tasks. This raises a natural question: can multiple SLMs be orchestrated into a system where each contributes effectively, achieving higher accuracy than any individual model? Existing orchestration methods have primarily targeted frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To address this gap, we propose a three-stage approach for orchestrating SLMs. First, we introduce SLM-MUX, a multi-model architecture that effectively coordinates multiple SLMs. Building on this, we develop two optimization strategies: (i) a model selection search that identifies the most complementary SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our approach delivers strong results: Compared to existing orchestration methods, our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0% on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and GSM8K, and matches its performance on MATH. We further provide theoretical analyses to substantiate the advantages of our method. In summary, we demonstrate that SLMs can be effectively orchestrated into more accurate and efficient systems through the proposed approach.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.370483"
    },
    {
        "index": "#4",
        "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs",
        "link": "/arxiv/2510.05069",
        "arxiv_id": "2510.05069",
        "authors": "Dachuan Shi, Abedelkadir Asi, Keying Li, Xiangchi Yuan, Leyan Pan, Wenke Lee, Wen Xiao",
        "summary": "Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.371526"
    },
    {
        "index": "#5",
        "title": "COLE: a Comprehensive Benchmark for French Language Understanding Evaluation",
        "link": "/arxiv/2510.05046",
        "arxiv_id": "2510.05046",
        "authors": "David Beauchemin, Yan Tremblay, Mohamed Amine Youssef, Richard Khoury",
        "summary": "To address the need for a more comprehensive evaluation of French Natural Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23 diverse task covering a broad range of NLU capabilities, including sentiment analysis, paraphrase detection, grammatical judgment, and reasoning, with a particular focus on linguistic phenomena relevant to the French language. We benchmark 94 large language models (LLM), providing an extensive analysis of the current state of French NLU. Our results highlight a significant performance gap between closed- and open-weights models and identify key challenging frontiers for current LLMs, such as zero-shot extractive question-answering (QA), fine-grained word sense disambiguation, and understanding of regional language variations. We release COLE as a public resource to foster further progress in French language modelling.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.377562"
    },
    {
        "index": "#6",
        "title": "Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization",
        "link": "/arxiv/2510.05038",
        "arxiv_id": "2510.05038",
        "authors": "Omri Uzan, Asaf Yehudai, Roi pony, Eyal Shnarch, Ariel Gera",
        "summary": "Multimodal encoders have pushed the boundaries of visual document retrieval, matching textual query tokens directly to image patches and achieving state-of-the-art performance on public benchmarks. Recent models relying on this paradigm have massively scaled the sizes of their query and document representations, presenting obstacles to deployment and scalability in real-world pipelines. Furthermore, purely vision-centric approaches may be constrained by the inherent modality gap still exhibited by modern vision-language models. In this work, we connect these challenges to the paradigm of hybrid retrieval, investigating whether a lightweight dense text retriever can enhance a stronger vision-centric model. Existing hybrid methods, which rely on coarse-grained fusion of ranks or scores, fail to exploit the rich interactions within each model's representation space. To address this, we introduce Guided Query Refinement (GQR), a novel test-time optimization method that refines a primary retriever's query embedding using guidance from a complementary retriever's scores. Through extensive experiments on visual document retrieval benchmarks, we demonstrate that GQR allows vision-centric models to match the performance of models with significantly larger representations, while being up to 14x faster and requiring 54x less memory. Our findings show that GQR effectively pushes the Pareto frontier for performance and efficiency in multimodal retrieval. We release our code at https://github.com/IBM/test-time-hybrid-retrieval",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.378220"
    },
    {
        "index": "#7",
        "title": "A Set of Quebec-French Corpus of Regional Expressions and Terms",
        "link": "/arxiv/2510.05026",
        "arxiv_id": "2510.05026",
        "authors": "David Beauchemin, Yan Tremblay, Mohamed Amine Youssef, Richard Khoury",
        "summary": "The tasks of idiom understanding and dialect understanding are both well-established benchmarks in natural language processing. In this paper, we propose combining them, and using regional idioms as a test of dialect understanding. Towards this end, we propose two new benchmark datasets for the Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic words. We explain how to construct these corpora, so that our methodology can be replicated for other dialects. Our experiments with 94 LLM demonstrate that our regional idiom benchmarks are a reliable tool for measuring a model's proficiency in a specific dialect.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.378836"
    },
    {
        "index": "#8",
        "title": "Imperceptible Jailbreaking against Large Language Models",
        "link": "/arxiv/2510.05025",
        "arxiv_id": "2510.05025",
        "authors": "Kuofeng Gao, Yiming Li, Chao Du, Xin Wang, Xingjun Ma, Shu-Tao Xia, Tianyu Pang",
        "summary": "Jailbreaking attacks on the vision modality typically rely on imperceptible adversarial perturbations, whereas attacks on the textual modality are generally assumed to require visible modifications (e.g., non-semantic suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a class of Unicode characters called variation selectors. By appending invisible variation selectors to malicious questions, the jailbreak prompts appear visually identical to original malicious questions on screen, while their tokenization is \"secretly\" altered. We propose a chain-of-search pipeline to generate such adversarial suffixes to induce harmful responses. Our experiments show that our imperceptible jailbreaks achieve high attack success rates against four aligned LLMs and generalize to prompt injection attacks, all without producing any visible modifications in the written prompt. Our code is available at https://github.com/sail-sg/imperceptible-jailbreaks.",
        "subjects": "Computation and Language, Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.379583"
    },
    {
        "index": "#9",
        "title": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning",
        "link": "/arxiv/2510.05003",
        "arxiv_id": "2510.05003",
        "authors": "Imran Mansha",
        "summary": "Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated remarkable reasoning abilities but require significant computational resources for fine-tuning. This paper presents a resource-efficient fine-tuning approach for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating under constrained GPU and memory settings. Using parameter-efficient tuning techniques such as LoRA and QLoRA, we adapt the base model on publicly available medical reasoning datasets. The model achieves improved reasoning coherence and factual accuracy while reducing memory usage by up to 60% compared to standard full fine-tuning. Experimental evaluation demonstrates that lightweight adaptations can retain strong reasoning capability in medical question-answering tasks. This work highlights practical strategies for deploying LLMs in low-resource research environments and provides insights into balancing efficiency and domain specialization for medical AI systems.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.380181"
    },
    {
        "index": "#10",
        "title": "AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives",
        "link": "/arxiv/2510.04983",
        "arxiv_id": "2510.04983",
        "authors": "Khalid Mehtab Khan, Anagha Kulkarni",
        "summary": "Identifying cultural capital (CC) themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms. However, themes such as aspirational goals or family support are often woven into narratives, rather than appearing as direct keywords. This makes them difficult to detect for standard NLP models that process sentences in isolation. The core challenge stems from a lack of awareness, as standard models are pre-trained on general corpora, leaving them blind to the domain-specific language and narrative context inherent to the data. To address this, we introduce AWARE, a framework that systematically attempts to improve a transformer model's awareness for this nuanced task. AWARE has three core components: 1) Domain Awareness, adapting the model's vocabulary to the linguistic style of student reflections; 2) Context Awareness, generating sentence embeddings that are aware of the full essay context; and 3) Class Overlap Awareness, employing a multi-label strategy to recognize the coexistence of themes in a single sentence. Our results show that by making the model explicitly aware of the properties of the input, AWARE outperforms a strong baseline by 2.1 percentage points in Macro-F1 and shows considerable improvements across all themes. This work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative.",
        "subjects": "Computation and Language, Artificial Intelligence, Computers and Society, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.380845"
    },
    {
        "index": "#11",
        "title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)",
        "link": "/arxiv/2510.04950",
        "arxiv_id": "2510.04950",
        "authors": "Om Dobariya, Akhil Kumar",
        "summary": "The wording of natural language prompts has been shown to influence the performance of large language models (LLMs), yet the role of politeness and tone remains underexplored. In this study, we investigate how varying levels of prompt politeness affect model accuracy on multiple-choice questions. We created a dataset of 50 base questions spanning mathematics, science, and history, each rewritten into five tone variants: Very Polite, Polite, Neutral, Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we evaluated responses across these conditions and applied paired sample t-tests to assess statistical significance. Contrary to expectations, impolite prompts consistently outperformed polite ones, with accuracy ranging from 80.8% for Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from earlier studies that associated rudeness with poorer outcomes, suggesting that newer LLMs may respond differently to tonal variation. Our results highlight the importance of studying pragmatic aspects of prompting and raise broader questions about the social dimensions of human-AI interaction.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Neural and Evolutionary Computing, Methodology",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.381543"
    },
    {
        "index": "#12",
        "title": "A First Context-Free Grammar Applied to Nawatl Corpora Augmentation",
        "link": "/arxiv/2510.04945",
        "arxiv_id": "2510.04945",
        "authors": "Juan-José Guzmán-Landa, Juan-Manuel Torres-Moreno, Miguel Figueroa-Saavedra, Ligia Quintana-Torres, Martha-Lorena Avendaño-Garrido, Graham Ranger",
        "summary": "In this article we introduce a context-free grammar (CFG) for the Nawatl language. Nawatl (or Nahuatl) is an Amerindian language of the $\\pi$-language type, i.e. a language with few digital resources, in which the corpora available for machine learning are virtually non-existent. The objective here is to generate a significant number of grammatically correct artificial sentences, in order to increase the corpora available for language model training. We want to show that a grammar enables us significantly to expand a corpus in Nawatl which we call $\\pi$-\\textsc{yalli}. The corpus, thus enriched, enables us to train algorithms such as FastText and to evaluate them on sentence-level semantic tasks. Preliminary results show that by using the grammar, comparative improvements are achieved over some LLMs. However, it is observed that to achieve more significant improvement, grammars that model the Nawatl language even more effectively are required.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.382218"
    },
    {
        "index": "#13",
        "title": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models",
        "link": "/arxiv/2510.04933",
        "arxiv_id": "2510.04933",
        "authors": "Amir Hameed Mir",
        "summary": "Large Language Models (LLMs) often produce fluent yet factually incorrect statements-a phenomenon known as hallucination-posing serious risks in high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric framework for hallucination detection that analyzes the evolution of hidden-state semantics across transformer layers. Unlike prior methods that rely on multiple sampling passes or external verification sources, LSD operates intrinsically within the model's representational space. Using margin-based contrastive learning, LSD aligns hidden activations with ground-truth embeddings derived from a factual encoder, revealing a distinct separation in semantic trajectories: factual responses preserve stable alignment, while hallucinations exhibit pronounced semantic drift across depth. Evaluated on the TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming SelfCheckGPT and Semantic Entropy baselines while requiring only a single forward pass. This efficiency yields a 5-20x speedup over sampling-based methods without sacrificing precision or interpretability. LSD offers a scalable, model-agnostic mechanism for real-time hallucination monitoring and provides new insights into the geometry of factual consistency within large language models.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Theory, Machine Learning, Neural and Evolutionary Computing",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.393115"
    },
    {
        "index": "#14",
        "title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment",
        "link": "/arxiv/2510.04919",
        "arxiv_id": "2510.04919",
        "authors": "Davood Rafiei, Morgan Lindsay Heisler, Weiwei Zhang, Mohammadreza Pourreza, Yong Zhang",
        "summary": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large Language Models (LLMs) on downstream tasks. However, variability in training data can hinder a model's ability to generalize across domains. This paper studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or text to SQL), examining how well SFT training data matches the structural characteristics of target queries and how this alignment impacts model performance. We hypothesize that alignment can be accurately estimated by comparing the distributions of structural SQL features across the training set, target data, and the model's predictions prior to SFT. Through comprehensive experiments on three large cross-domain NL2SQL benchmarks and multiple model families, we show that structural alignment is a strong predictor of fine-tuning success. When alignment is high, SFT yields substantial gains in accuracy and SQL generation quality; when alignment is low, improvements are marginal or absent. These findings highlight the importance of alignment-aware data selection for effective fine-tuning and generalization in NL2SQL tasks.",
        "subjects": "Computation and Language, Artificial Intelligence, Databases",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.393832"
    },
    {
        "index": "#15",
        "title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests",
        "link": "/arxiv/2510.04891",
        "arxiv_id": "2510.04891",
        "authors": "Punya Syon Pandey, Hai Son Le, Devansh Bhardwaj, Rada Mihalcea, Zhijing Jin",
        "summary": "Large language models (LLMs) are increasingly deployed in contexts where their failures can have direct sociopolitical consequences. Yet, existing safety benchmarks rarely test vulnerabilities in domains such as political manipulation, propaganda and disinformation generation, or surveillance and information control. We introduce SocialHarmBench, a dataset of 585 prompts spanning 7 sociopolitical categories and 34 countries, designed to surface where LLMs most acutely fail in politically charged contexts. Our evaluations reveal several shortcomings: open-weight models exhibit high vulnerability to harmful compliance, with Mistral-7B reaching attack success rates as high as 97% to 98% in domains such as historical revisionism, propaganda, and political manipulation. Moreover, temporal and geographic analyses show that LLMs are most fragile when confronted with 21st-century or pre-20th-century contexts, and when responding to prompts tied to regions such as Latin America, the USA, and the UK. These findings demonstrate that current safeguards fail to generalize to high-stakes sociopolitical settings, exposing systematic biases and raising concerns about the reliability of LLMs in preserving human rights and democratic values. We share the SocialHarmBench benchmark at https://huggingface.co/datasets/psyonp/SocialHarmBench.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.394529"
    },
    {
        "index": "#16",
        "title": "Detecting Distillation Data from Reasoning Models",
        "link": "/arxiv/2510.04850",
        "arxiv_id": "2510.04850",
        "authors": "Hengxiang Zhang, Hyeong Kyu Choi, Yixuan Li, Hongxin Wei",
        "summary": "Reasoning distillation has emerged as an efficient and powerful paradigm for enhancing the reasoning capabilities of large language models. However, reasoning distillation may inadvertently cause benchmark contamination, where evaluation data included in distillation datasets can inflate performance metrics of distilled models. In this work, we formally define the task of distillation data detection, which is uniquely challenging due to the partial availability of distillation data. Then, we propose a novel and effective method Token Probability Deviation (TBD), which leverages the probability patterns of the generated output tokens. Our method is motivated by the analysis that distilled models tend to generate near-deterministic tokens for seen questions, while producing more low-probability tokens for unseen questions. Our key idea behind TBD is to quantify how far the generated tokens' probabilities deviate from a high reference probability. In effect, our method achieves competitive detection performance by producing lower scores for seen questions than for unseen questions. Extensive experiments demonstrate the effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of 0.470 on the S1 dataset.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.395186"
    },
    {
        "index": "#17",
        "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA",
        "link": "/arxiv/2510.04849",
        "arxiv_id": "2510.04849",
        "authors": "Elisei Rykov, Kseniia Petrushina, Maksim Savkin, Valerii Olisov, Artem Vazhentsev, Kseniia Titova, Alexander Panchenko, Vasily Konovalov, Julia Belikova",
        "summary": "Hallucination detection remains a fundamental challenge for the safe and reliable deployment of large language models (LLMs), especially in applications requiring factual accuracy. Existing hallucination benchmarks often operate at the sequence level and are limited to English, lacking the fine-grained, multilingual supervision needed for a comprehensive evaluation. In this work, we introduce PsiloQA, a large-scale, multilingual dataset annotated with span-level hallucinations across 14 languages. PsiloQA is constructed through an automated three-stage pipeline: generating question-answer pairs from Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse LLMs in a no-context setting, and automatically annotating hallucinated spans using GPT-4o by comparing against golden answers and retrieved context. We evaluate a wide range of hallucination detection methods -- including uncertainty quantification, LLM-based tagging, and fine-tuned encoder models -- and show that encoder-based models achieve the strongest performance across languages. Furthermore, PsiloQA demonstrates effective cross-lingual generalization and supports robust knowledge transfer to other benchmarks, all while being significantly more cost-efficient than human-annotated datasets. Our dataset and results advance the development of scalable, fine-grained hallucination detection in multilingual settings.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.395933"
    },
    {
        "index": "#18",
        "title": "Instability in Downstream Task Performance During LLM Pretraining",
        "link": "/arxiv/2510.04848",
        "arxiv_id": "2510.04848",
        "authors": "Yuto Nishida, Masaru Isonuma, Yusuke Oda",
        "summary": "When training large language models (LLMs), it is common practice to track downstream task performance throughout the training process and select the checkpoint with the highest validation score. However, downstream metrics often exhibit substantial fluctuations, making it difficult to identify the checkpoint that truly represents the best-performing model. In this study, we empirically analyze the stability of downstream task performance in an LLM trained on diverse web-scale corpora. We find that task scores frequently fluctuate throughout training, both at the aggregate and example levels. To address this instability, we investigate two post-hoc checkpoint integration methods: checkpoint averaging and ensemble, motivated by the hypothesis that aggregating neighboring checkpoints can reduce performance volatility. We demonstrate both empirically and theoretically that these methods improve downstream performance stability without requiring any changes to the training procedure.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.396526"
    },
    {
        "index": "#19",
        "title": "How I Built ASR for Endangered Languages with a Spoken Dictionary",
        "link": "/arxiv/2510.04832",
        "arxiv_id": "2510.04832",
        "authors": "Christopher Bartley, Anton Ragni",
        "summary": "Nearly half of the world's languages are endangered. Speech technologies such as Automatic Speech Recognition (ASR) are central to revival efforts, yet most languages remain unsupported because standard pipelines expect utterance-level supervised data. Speech data often exist for endangered languages but rarely match these formats. Manx Gaelic ($\\sim$2,200 speakers), for example, has had transcribed speech since 1948, yet remains unsupported by modern systems. In this paper, we explore how little data, and in what form, is needed to build ASR for critically endangered languages. We show that a short-form pronunciation resource is a viable alternative, and that 40 minutes of such data produces usable ASR for Manx ($<$50\\% WER). We replicate our approach, applying it to Cornish ($\\sim$600 speakers), another critically endangered language. Results show that the barrier to entry, in quantity and form, is far lower than previously thought, giving hope to endangered language communities that cannot afford to meet the requirements arbitrarily imposed upon them.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.397105"
    },
    {
        "index": "#20",
        "title": "Hybrid Architectures for Language Models: Systematic Analysis and Design Insights",
        "link": "/arxiv/2510.04800",
        "arxiv_id": "2510.04800",
        "authors": "Sangmin Bae, Bilge Acun, Haroun Habeeb, Seungyeon Kim, Chien-Yu Lin, Liang Luo, Junjie Wang, Carole-Jean Wu",
        "summary": "Recent progress in large language models demonstrates that hybrid architectures--combining self-attention mechanisms with structured state space models like Mamba--can achieve a compelling balance between modeling quality and computational efficiency, particularly for long-context tasks. While these hybrid models show promising performance, systematic comparisons of hybridization strategies and analyses on the key factors behind their effectiveness have not been clearly shared to the community. In this work, we present a holistic evaluation of hybrid architectures based on inter-layer (sequential) or intra-layer (parallel) fusion. We evaluate these designs from a variety of perspectives: language modeling performance, long-context capabilities, scaling analysis, and training and inference efficiency. By investigating the core characteristics of their computational primitive, we identify the most critical elements for each hybridization strategy and further propose optimal design recipes for both hybrid models. Our comprehensive analysis provides practical guidance and valuable insights for developing hybrid language models, facilitating the optimization of architectural configurations.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.397801"
    },
    {
        "index": "#21",
        "title": "Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models",
        "link": "/arxiv/2510.04764",
        "arxiv_id": "2510.04764",
        "authors": "Raha Askari, Sina Zarrieß, Özge Alacam, Judith Sieker",
        "summary": "Implicit meanings are integral to human communication, making it essential for language models to be capable of identifying and interpreting them. Grice (1975) proposed a set of conversational maxims that guide cooperative dialogue, noting that speakers may deliberately violate these principles to express meanings beyond literal words, and that listeners, in turn, recognize such violations to draw pragmatic inferences. Building on Surian et al. (1996)'s study of children's sensitivity to violations of Gricean maxims, we introduce a novel benchmark to test whether language models pretrained on less than 10M and less than 100M tokens can distinguish maxim-adhering from maxim-violating utterances. We compare these BabyLMs across five maxims and situate their performance relative to children and a Large Language Model (LLM) pretrained on 3T tokens. We find that overall, models trained on less than 100M tokens outperform those trained on less than 10M, yet fall short of child-level and LLM competence. Our results suggest that modest data increases improve some aspects of pragmatic behavior, leading to finer-grained differentiation between pragmatic dimensions.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.403529"
    },
    {
        "index": "#22",
        "title": "ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever",
        "link": "/arxiv/2510.04757",
        "arxiv_id": "2510.04757",
        "authors": "Eduardo Martínez Rivera, Filippo Menolascina",
        "summary": "Retrieval-Augmented Generation (RAG) is a powerful technique for enriching Large Language Models (LLMs) with external knowledge, allowing for factually grounded responses, a critical requirement in high-stakes domains such as healthcare. However, the efficacy of RAG systems is fundamentally restricted by the performance of their retrieval module, since irrelevant or semantically misaligned documents directly compromise the accuracy of the final generated response. General-purpose dense retrievers can struggle with the nuanced language of specialised domains, while the high accuracy of in-domain models is often achieved at prohibitive computational costs. In this work, we aim to address this trade-off by developing and evaluating a two-stage retrieval architecture that combines a lightweight ModernBERT bidirectional encoder for efficient initial candidate retrieval with a ColBERTv2 late-interaction model for fine-grained re-ranking. We conduct comprehensive evaluations of our retriever module performance and RAG system performance in the biomedical context, fine-tuning the IR module using 10k question-passage pairs from PubMedQA. Our analysis of the retriever module confirmed the positive impact of the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points compared to its retrieve-only counterpart. When integrated into the biomedical RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on the five tasks of the MIRAGE question-answering benchmark, outperforming strong baselines such as MedCPT (0.4436). Our ablation studies reveal that this performance is critically dependent on a joint fine-tuning process that aligns the retriever and re-ranker; otherwise, the re-ranker might degrade the performance.",
        "subjects": "Computation and Language, Quantitative Methods",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.404185"
    },
    {
        "index": "#23",
        "title": "A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance",
        "link": "/arxiv/2510.04750",
        "arxiv_id": "2510.04750",
        "authors": "Peshala Perera, Deshan Sumanathilaka",
        "summary": "Dyslexia in adults remains an under-researched and under-served area, particularly in non-English-speaking contexts, despite its significant impact on personal and professional lives. This work addresses that gap by focusing on Sinhala, a low-resource language with limited tools for linguistic accessibility. We present an assistive system explicitly designed for Sinhala-speaking adults with dyslexia. The system integrates Whisper for speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model trained for Sinhala to identify common dyslexic errors, and a combined mT5 and Mistral-based model to generate corrected text. Finally, the output is converted back to speech using gTTS, creating a complete multimodal feedback loop. Despite the challenges posed by limited Sinhala-language datasets, the system achieves 0.66 transcription accuracy and 0.7 correction accuracy with 0.65 overall system accuracy. These results demonstrate both the feasibility and effectiveness of the approach. Ultimately, this work highlights the importance of inclusive Natural Language Processing (NLP) technologies in underrepresented languages and showcases a practical",
        "subjects": "Computation and Language, Software Engineering",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.404794"
    },
    {
        "index": "#24",
        "title": "JSON Whisperer: Efficient JSON Editing with LLMs",
        "link": "/arxiv/2510.04717",
        "arxiv_id": "2510.04717",
        "authors": "Sarel Duanis, Asnat Greenstein-Messica, Eliya Habba",
        "summary": "Large language models (LLMs) can modify JSON documents through natural language commands, but current approaches regenerate entire structures for each edit, resulting in computational inefficiency. We present JSON Whisperer, a framework that enables LLMs to generate RFC 6902 diff patches-expressing only the necessary modifications-rather than complete documents. We identify two key challenges in patch-based editing: (1) LLMs often miss related updates when generating isolated patches, and (2) array manipulations require tracking index shifts across operations, which LLMs handle poorly. To address these issues, we introduce EASE (Explicitly Addressed Sequence Encoding), which transforms arrays into dictionaries with stable keys, eliminating index arithmetic complexities. Our evaluation shows that patch generation with EASE reduces token usage by 31% while maintaining edit quality within 5% of full regeneration with particular gains for complex instructions and list manipulations. The dataset is available at: https://github.com/emnlp2025/JSON-Whisperer/",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.405399"
    },
    {
        "index": "#25",
        "title": "Multilingual Routing in Mixture-of-Experts",
        "link": "/arxiv/2510.04694",
        "arxiv_id": "2510.04694",
        "authors": "Lucas Bandarkar, Chenyuan Yang, Mohsen Fayyaz, Junlin Hu, Nanyun Peng",
        "summary": "Mixture-of-Experts (MoE) architectures have become the key to scaling modern LLMs, yet little is understood about how their sparse routing dynamics respond to multilingual data. In this work, we analyze expert routing patterns using parallel multilingual datasets and present highly interpretable layer-wise phenomena. We find that MoE models route tokens in language-specific ways in the early and late decoder layers but exhibit significant cross-lingual routing alignment in middle layers, mirroring parameter-sharing trends observed in dense LLMs. In particular, we reveal a clear, strong correlation between a model's performance in a given language and how similarly its tokens are routed to English in these layers. Extending beyond correlation, we explore inference-time interventions that induce higher cross-lingual routing alignment. We introduce a method that steers the router by promoting middle-layer task experts frequently activated in English, and it successfully increases multilingual performance. These 1-2% gains are remarkably consistent across two evaluation tasks, three models, and 15+ languages, especially given that these simple interventions override routers of extensively trained, state-of-the-art LLMs. In comparison, interventions outside of the middle layers or targeting multilingual-specialized experts only yield performance degradation. Altogether, we present numerous findings that explain how MoEs process non-English text and demonstrate that generalization is limited by the model's ability to leverage language-universal experts in all languages.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.406102"
    },
    {
        "index": "#26",
        "title": "TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA",
        "link": "/arxiv/2510.04682",
        "arxiv_id": "2510.04682",
        "authors": "Chanjoo Jung, Jaehyung Kim",
        "summary": "Large Language Models (LLMs) are widely applied in real world scenarios, but fine-tuning them comes with significant computational and storage costs. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these costs, but the adapted parameters are dependent on the base model and cannot be transferred across different backbones. One way to address this issue is through knowledge distillation, but its effectiveness inherently depends on training data. Recent work such as TransLoRA avoids this by generating synthetic data, but this adds complexity because it requires training an additional discriminator model. In this paper, we propose TiTok, a new framework that enables effective LoRA Transplantation through Token-level knowledge transfer. Specifically, TiTok captures task-relevant information through a contrastive excess between a source model with and without LoRA. This excess highlights informative tokens and enables selective filtering of synthetic data, all without additional models or overhead. Through experiments on three benchmarks across multiple transfer settings, our experiments show that the proposed method is consistently effective, achieving average performance gains of +4~8% compared to baselines overall.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.406702"
    },
    {
        "index": "#27",
        "title": "Multi-Agent Tool-Integrated Policy Optimization",
        "link": "/arxiv/2510.04678",
        "arxiv_id": "2510.04678",
        "authors": "Zhanfeng Mo, Xingxuan Li, Yuntao Chen, Lidong Bing",
        "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge-intensive and complex reasoning tasks. Existing implementations typically rely on a single agent, but they suffer from limited context length and noisy tool responses. A natural solution is to adopt a multi-agent framework with planner- and worker-agents to manage context. However, no existing methods support effective reinforcement learning post-training of tool-integrated multi-agent frameworks. To address this gap, we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which enables distinct roles (planner and worker) to be trained within a single LLM instance using role-specific prompts via reinforcement learning. MATPO is derived from a principled credit assignment mechanism across planner and worker rollouts. This design eliminates the need to deploy multiple LLMs, which would be memory-intensive, while preserving the benefits of specialization. Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently outperforms single-agent baselines by an average of 18.38% relative improvement in performance and exhibits greater robustness to noisy tool outputs. Our findings highlight the effectiveness of unifying multiple agent roles within a single LLM and provide practical insights for stable and efficient multi-agent RL training.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.407327"
    },
    {
        "index": "#28",
        "title": "FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification",
        "link": "/arxiv/2510.04671",
        "arxiv_id": "2510.04671",
        "authors": "Chao Liu, Ling Luo, Tengxiao Lv, Huan Zhuang, Lejing Yu, Jian Wang, Hongfei Lin",
        "summary": "With the rapid development of online medical platforms, consumer health questions (CHQs) are inefficient in diagnosis due to redundant information and frequent non-professional terms. The medical question summary (MQS) task aims to transform CHQs into streamlined doctors' frequently asked questions (FAQs), but existing methods still face challenges such as poor identification of question focus and model hallucination. This paper explores the potential of large language models (LLMs) in the MQS task and finds that direct fine-tuning is prone to focus identification bias and generates unfaithful content. To this end, we propose an optimization framework based on core focus guidance. First, a prompt template is designed to drive the LLMs to extract the core focus from the CHQs that is faithful to the original text. Then, a fine-tuning dataset is constructed in combination with the original CHQ-FAQ pairs to improve the ability to identify the focus of the question. Finally, a multi-dimensional quality evaluation and selection mechanism is proposed to comprehensively improve the quality of the summary from multiple dimensions. We conduct comprehensive experiments on two widely-adopted MQS datasets using three established evaluation metrics. The proposed framework achieves state-of-the-art performance across all measures, demonstrating a significant boost in the model's ability to identify critical focus of questions and a notable mitigation of hallucinations. The source codes are freely available at https://github.com/DUT-LiuChao/FocusMed.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.408043"
    },
    {
        "index": "#29",
        "title": "FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method",
        "link": "/arxiv/2510.04655",
        "arxiv_id": "2510.04655",
        "authors": "Yuheng Li, Jiechao Gao, Wei Han, Wenwen Ouyang, Wei Zhu, Hui Yi Leong",
        "summary": "Knowledge of the medical decision process, which can be modeled as medical decision trees (MDTs), is critical to building clinical decision support systems. However, current MDT construction methods rely heavily on time-consuming and laborious manual annotation. To address this challenge, we propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for automatically extracting MDTs from clinical guidelines and textbooks. We integrate gradient path information to capture synergistic effects between different modules, enabling more effective and reliable rank allocation. This framework ensures that the most critical modules receive appropriate rank allocations while less important ones are pruned, resulting in a more efficient and accurate model for extracting medical decision trees from clinical texts. Extensive experiments on medical guideline datasets demonstrate that our PI-LoRA method significantly outperforms existing parameter-efficient fine-tuning approaches for the Text2MDT task, achieving better accuracy with substantially reduced model complexity. The proposed method achieves state-of-the-art results while maintaining a lightweight architecture, making it particularly suitable for clinical decision support systems where computational resources may be limited.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.413829"
    },
    {
        "index": "#30",
        "title": "Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study",
        "link": "/arxiv/2510.04641",
        "arxiv_id": "2510.04641",
        "authors": "Ayan Majumdar, Feihao Chen, Jinghui Li, Xiaozhen Wang",
        "summary": "Large-scale web-scraped text corpora used to train general-purpose AI models often contain harmful demographic-targeted social biases, creating a regulatory need for data auditing and developing scalable bias-detection methods. Although prior work has investigated biases in text datasets and related detection methods, these studies remain narrow in scope. They typically focus on a single content type (e.g., hate speech), cover limited demographic axes, overlook biases affecting multiple demographics simultaneously, and analyze limited techniques. Consequently, practitioners lack a holistic understanding of the strengths and limitations of recent large language models (LLMs) for automated bias detection. In this study, we present a comprehensive evaluation framework aimed at English texts to assess the ability of LLMs in detecting demographic-targeted social biases. To align with regulatory requirements, we frame bias detection as a multi-label task using a demographic-focused taxonomy. We then conduct a systematic evaluation with models across scales and techniques, including prompting, in-context learning, and fine-tuning. Using twelve datasets spanning diverse content types and demographics, our study demonstrates the promise of fine-tuned smaller models for scalable detection. However, our analyses also expose persistent gaps across demographic axes and multi-demographic targeted biases, underscoring the need for more effective and scalable auditing frameworks.",
        "subjects": "Computation and Language, Computers and Society, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.414503"
    },
    {
        "index": "#31",
        "title": "Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry",
        "link": "/arxiv/2510.04631",
        "arxiv_id": "2510.04631",
        "authors": "Anastasia Zhukova, Jonas Lührs, Christian E. Matt, Bela Gipp",
        "summary": "Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained language models by incorporating additional knowledge from the graph structures to learn domain-specific terminology or relationships between documents that might otherwise be overlooked. This paper explores how SciNCL, a graph-aware neighborhood contrastive learning methodology originally designed for scientific publications, can be applied to the process industry domain, where text logs contain crucial information about daily operations and are often structured as sparse KGs. Our experiments demonstrate that language models fine-tuned with triplets derived from GE outperform a state-of-the-art mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process industry text embedding benchmark (PITEB) while being 3-5 times smaller in size.",
        "subjects": "Computation and Language, Information Retrieval",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.415157"
    },
    {
        "index": "#32",
        "title": "FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning",
        "link": "/arxiv/2510.04601",
        "arxiv_id": "2510.04601",
        "authors": "Guochen Yan, Luyuan Xie, Qingni Shen, Yuejian Fang, Zhonghai Wu",
        "summary": "The current paradigm of training large language models (LLMs) on publicly available Web data is becoming unsustainable, with high-quality data sources in specialized domains nearing exhaustion. Federated Learning (FL) emerges as a practical solution for the next generation of AI on a decentralized Web, enabling privacy-preserving collaborative fine-tuning by leveraging private data distributed across a global client base. While Low-Rank Adaptation (LoRA) is the standard for efficient fine-tuning, its application in federated settings presents a critical challenge: communication overhead remains a significant bottleneck across the Web's heterogeneous network conditions. The structural redundancy within LoRA parameters not only incurs a heavy communication burden but also introduces conflicts when aggregating client updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose framework designed for communication-efficient FL. We first introduce an importance-aware sparsification method that preserves the structural integrity of LoRA updates to reduce the uploaded parameter count. The server then reconstructs and aggregates these updates in a full-rank space to mitigate conflicts. Finally, it decomposes the global update into a sparse low-rank format for broadcast, ensuring a symmetrically efficient cycle. We also propose an efficient variant, FedSRD-e, to reduce computational overhead. Experimental results on 10 benchmarks demonstrate that our framework significantly reduces communication costs by up to 90\\% while even improving model performance on heterogeneous client data.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.415800"
    },
    {
        "index": "#33",
        "title": "Robustness assessment of large audio language models in multiple-choice evaluation",
        "link": "/arxiv/2510.04584",
        "arxiv_id": "2510.04584",
        "authors": "Fernando López, Santosh Kesiraju, Jordi Luque",
        "summary": "Recent advances in large audio language models (LALMs) have primarily been assessed using a multiple-choice question answering (MCQA) framework. However, subtle changes, such as shifting the order of choices, result in substantially different results. Existing MCQA frameworks do not account for this variability and report a single accuracy number per benchmark or category. We dive into the MCQA evaluation framework and conduct a systematic study spanning three benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings indicate that models are sensitive not only to the ordering of choices, but also to the paraphrasing of the question and the choices. Finally, we propose a simpler evaluation protocol and metric that account for subtle variations and provide a more detailed evaluation report of LALMs within the MCQA framework.",
        "subjects": "Computation and Language, Sound, Audio and Speech Processing",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.416460"
    },
    {
        "index": "#34",
        "title": "Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference",
        "link": "/arxiv/2510.04581",
        "arxiv_id": "2510.04581",
        "authors": "Dang Anh, Rick Nouwen, Massimo Poesio",
        "summary": "Our goal is to study how LLMs represent and interpret plural reference in ambiguous and unambiguous contexts. We ask the following research questions: (1) Do LLMs exhibit human-like preferences in representing plural reference? (2) Are LLMs able to detect ambiguity in plural anaphoric expressions and identify possible referents? To address these questions, we design a set of experiments, examining pronoun production using next-token prediction tasks, pronoun interpretation, and ambiguity detection using different prompting strategies. We then assess how comparable LLMs are to humans in formulating and interpreting plural reference. We find that LLMs are sometimes aware of possible referents of ambiguous pronouns. However, they do not always follow human reference when choosing between interpretations, especially when the possible interpretation is not explicitly mentioned. In addition, they struggle to identify ambiguity without direct instruction. Our findings also reveal inconsistencies in the results across different types of experiments.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.417046"
    },
    {
        "index": "#35",
        "title": "Fine-grained auxiliary learning for real-world product recommendation",
        "link": "/arxiv/2510.04551",
        "arxiv_id": "2510.04551",
        "authors": "Mario Almagro, Diego Ortego, David Jimenez",
        "summary": "Product recommendation is the task of recovering the closest items to a given query within a large product corpora. Generally, one can determine if top-ranked products are related to the query by applying a similarity threshold; exceeding it deems the product relevant, otherwise manual revision is required. Despite being a well-known problem, the integration of these models in real-world systems is often overlooked. In particular, production systems have strong coverage requirements, i.e., a high proportion of recommendations must be automated. In this paper we propose ALC , an Auxiliary Learning strategy that boosts Coverage through learning fine-grained embeddings. Concretely, we introduce two training objectives that leverage the hardest negatives in the batch to build discriminative training signals between positives and negatives. We validate ALC using three extreme multi-label classification approaches in two product recommendation datasets; LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating state-of-the-art coverage rates when combined with a recent threshold-consistent margin loss.",
        "subjects": "Computation and Language, Information Retrieval",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.417665"
    },
    {
        "index": "#36",
        "title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization",
        "link": "/arxiv/2510.04506",
        "arxiv_id": "2510.04506",
        "authors": "Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han",
        "summary": "Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.429236"
    },
    {
        "index": "#37",
        "title": "GenQuest: An LLM-based Text Adventure Game for Language Learners",
        "link": "/arxiv/2510.04498",
        "arxiv_id": "2510.04498",
        "authors": "Qiao Wang, Adnan Labib, Robert Swier, Michael Hofmeyr, Zheng Yuan",
        "summary": "GenQuest is a generative text adventure game that leverages Large Language Models (LLMs) to facilitate second language learning through immersive, interactive storytelling. The system engages English as a Foreign Language (EFL) learners in a collaborative \"choose-your-own-adventure\" style narrative, dynamically generated in response to learner choices. Game mechanics such as branching decision points and story milestones are incorporated to maintain narrative coherence while allowing learner-driven plot development. Key pedagogical features include content generation tailored to each learner's proficiency level, and a vocabulary assistant that provides in-context explanations of learner-queried text strings, ranging from words and phrases to sentences. Findings from a pilot study with university EFL students in China indicate promising vocabulary gains and positive user perceptions. Also discussed are suggestions from participants regarding the narrative length and quality, and the request for multi-modal content such as illustrations.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.430007"
    },
    {
        "index": "#38",
        "title": "Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness",
        "link": "/arxiv/2510.04484",
        "arxiv_id": "2510.04484",
        "authors": "Amin Banayeeanzade, Ala N. Tak, Fatemeh Bahrani, Anahita Bolourani, Leonardo Blas, Emilio Ferrara, Jonathan Gratch, Sai Praneeth Karimireddy",
        "summary": "The ability to control LLMs' emulated emotional states and personality traits is essential for enabling rich, human-centered interactions in socially interactive settings. We introduce PsySET, a Psychologically-informed benchmark to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion and personality domains. Our study spans four models from different LLM families paired with various steering strategies, including prompting, fine-tuning, and representation engineering. Our results indicate that prompting is consistently effective but limited in intensity control, whereas vector injections achieve finer controllability while slightly reducing output quality. Moreover, we explore the trustworthiness of steered LLMs by assessing safety, truthfulness, fairness, and ethics, highlighting potential side effects and behavioral shifts. Notably, we observe idiosyncratic effects; for instance, even a positive emotion like joy can degrade robustness to adversarial factuality, lower privacy awareness, and increase preferential bias. Meanwhile, anger predictably elevates toxicity yet strengthens leakage resistance. Our framework establishes the first holistic evaluation of emotion and personality steering, offering insights into its interpretability and reliability for socially interactive applications.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.430792"
    },
    {
        "index": "#39",
        "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space",
        "link": "/arxiv/2510.04476",
        "arxiv_id": "2510.04476",
        "authors": "Tomas Figliolia, Nicholas Alonso, Rishi Iyer, Quentin Anthony, Beren Millidge",
        "summary": "Multi-headed Attention's (MHA) quadratic compute and linearly growing KV-cache make long-context transformers expensive to train and serve. Prior works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA) shrink the cache, speeding decode, but leave compute, which determines prefill and training speed, largely unchanged. We introduce Compressed Convolutional Attention (CCA), a novel attention method which down-projects queries, keys, and values and performs the entire attention operation inside the shared latent space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all at once by the desired compression factor. Because CCA is orthogonal to head-sharing, we combine the two to form Compressed Convolutional Grouped Query Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier so that users can tune compression toward either FLOP or memory limits without sacrificing quality. Experiments show that CCGQA consistently outperforms both GQA and MLA at equal KV-cache compression on dense and MoE models. Additionally, we show that CCGQA outperforms all other attention methods on MoE models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache compression with no drop in performance compared to standard MHA. CCA and CCGQA also dramatically reduce the FLOP cost of attention which leads to substantially faster training and prefill than existing methods. On H100 GPUs, our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence length of 16k relative to MHA, and accelerates backward by about 1.3x.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.431509"
    },
    {
        "index": "#40",
        "title": "Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners",
        "link": "/arxiv/2510.04454",
        "arxiv_id": "2510.04454",
        "authors": "Xiangchi Yuan, Xiang Chen, Tong Yu, Dachuan Shi, Can Jin, Wenke Lee, Saayan Mitra",
        "summary": "Large Language Models (LLMs) show strong reasoning abilities, often amplified by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although RL algorithms can substantially improve reasoning, they struggle to expand reasoning boundaries because they learn from their own reasoning trajectories rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers complementary benefits but typically requires large-scale data and risks overfitting. Recent attempts to combine SFT and RL face three main challenges: data inefficiency, algorithm-specific designs, and catastrophic forgetting. We propose a plug-and-play framework that dynamically integrates SFT into RL by selecting challenging examples for SFT. This approach reduces SFT data requirements and remains agnostic to the choice of RL or SFT algorithm. To mitigate catastrophic forgetting of RL-acquired skills during SFT, we select high-entropy tokens for loss calculation and freeze parameters identified as critical for RL. Our method achieves state-of-the-art (SoTA) reasoning performance using only 1.5% of the SFT data and 20.4% of the RL data used by prior SoTA, providing an efficient and plug-and-play solution for combining SFT and RL in reasoning post-training.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.432255"
    },
    {
        "index": "#41",
        "title": "On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs",
        "link": "/arxiv/2510.04439",
        "arxiv_id": "2510.04439",
        "authors": "Lucie Kunitomo-Jacquin, Edison Marrese-Taylor, Ken Fukuda",
        "summary": "Quantifying uncertainty in large language models (LLMs) is important for safety-critical applications because it helps spot incorrect answers, known as hallucinations. One major trend of uncertainty quantification methods is based on estimating the entropy of the distribution of the LLM's potential output sequences. This estimation is based on a set of output sequences and associated probabilities obtained by querying the LLM several times. In this paper, we advocate and experimentally show that the probability of unobserved sequences plays a crucial role, and we recommend future research to integrate it to enhance such LLM uncertainty quantification methods.",
        "subjects": "Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.432883"
    },
    {
        "index": "#42",
        "title": "Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?",
        "link": "/arxiv/2510.04434",
        "arxiv_id": "2510.04434",
        "authors": "Grace LeFevre, Qingcheng Zeng, Adam Leif, Jason Jewell, Denis Peskoff, Rob Voigt",
        "summary": "The social impact of Natural Language Processing (NLP) is increasingly important, with a rising community focus on initiatives related to NLP for Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the ACL Anthology address topics related to social good as defined by the UN Sustainable Development Goals (Adauto et al., 2023). In this study, we take an author- and venue-level perspective to map the landscape of NLP4SG, quantifying the proportion of work addressing social good concerns both within and beyond the ACL community, by both core ACL contributors and non-ACL authors. With this approach we discover two surprising facts about the landscape of NLP4SG. First, ACL authors are dramatically more likely to do work addressing social good concerns when publishing in venues outside of ACL. Second, the vast majority of publications using NLP techniques to address concerns of social good are done by non-ACL authors in venues outside of ACL. We discuss the implications of these findings on agenda-setting considerations for the ACL community related to NLP4SG.",
        "subjects": "Computation and Language, Social and Information Networks",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.433611"
    },
    {
        "index": "#43",
        "title": "Large Language Models Preserve Semantic Isotopies in Story Continuations",
        "link": "/arxiv/2510.04400",
        "arxiv_id": "2510.04400",
        "authors": "Marc Cavazza",
        "summary": "In this work, we explore the relevance of textual semantics to Large Language Models (LLMs), extending previous insights into the connection between distributional semantics and structural semantics. We investigate whether LLM-generated texts preserve semantic isotopies. We design a story continuation experiment using 10,000 ROCStories prompts completed by five LLMs. We first validate GPT-4o's ability to extract isotopies from a linguistic benchmark, then apply it to the generated stories. We then analyze structural (coverage, density, spread) and semantic properties of isotopies to assess how they are affected by completion. Results show that LLM completion within a given token horizon preserves semantic isotopies across multiple properties.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.470614"
    },
    {
        "index": "#44",
        "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations",
        "link": "/arxiv/2510.04398",
        "arxiv_id": "2510.04398",
        "authors": "Buyun Liang, Liangzu Peng, Jinqi Luo, Darshan Thaker, Kwan Ho Ryan Chan, René Vidal",
        "summary": "Large Language Models (LLMs) are increasingly deployed in high-risk domains. However, state-of-the-art LLMs often produce hallucinations, raising serious concerns about their reliability. Prior work has explored adversarial attacks for hallucination elicitation in LLMs, but it often produces unrealistic prompts, either by inserting gibberish tokens or by altering the original meaning. As a result, these approaches offer limited insight into how hallucinations may occur in practice. While adversarial attacks in computer vision often involve realistic modifications to input images, the problem of finding realistic adversarial prompts for eliciting LLM hallucinations has remained largely underexplored. To address this gap, we propose Semantically Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic modifications to the prompt that preserve its meaning while maintaining semantic coherence. Our contributions are threefold: (i) we formulate finding realistic attacks for hallucination elicitation as a constrained optimization problem over the input prompt space under semantic equivalence and coherence constraints; (ii) we introduce a constraint-preserving zeroth-order method to effectively search for adversarial yet feasible prompts; and (iii) we demonstrate through experiments on open-ended multiple-choice question answering tasks that SECA achieves higher attack success rates while incurring almost no constraint violations compared to existing methods. SECA highlights the sensitivity of both open-source and commercial gradient-inaccessible LLMs to realistic and plausible prompt variations. Code is available at https://github.com/Buyun-Liang/SECA.",
        "subjects": "Computation and Language, Artificial Intelligence, Cryptography and Security, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.492766"
    },
    {
        "index": "#45",
        "title": "Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation",
        "link": "/arxiv/2510.04394",
        "arxiv_id": "2510.04394",
        "authors": "Ankit Vadehra, Bill Johnson, Gene Saunders, Pascal Poupart",
        "summary": "Text editing can involve several iterations of revision. Incorporating an efficient Grammar Error Correction (GEC) tool in the initial correction round can significantly impact further human editing effort and final text quality. This raises an interesting question to quantify GEC Tool usability: How much effort can the GEC Tool save users? We present the first large-scale dataset of post-editing (PE) time annotations and corrections for two English GEC test datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET) for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by estimating PE time-to-correct. Using our dataset, we quantify the amount of time saved by GEC Tools in text editing. Analyzing the edit type indicated that determining whether a sentence needs correction and edits like paraphrasing and punctuation changes had the greatest impact on PE time. Finally, comparison with human rankings shows that PEET correlates well with technical effort judgment, providing a new human-centric direction for evaluating GEC tool usability. We release our dataset and code at: https://github.com/ankitvad/PEET_Scorer.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.493686"
    },
    {
        "index": "#46",
        "title": "Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards",
        "link": "/arxiv/2510.04392",
        "arxiv_id": "2510.04392",
        "authors": "Faisal Hamman, Chenyang Zhu, Anoop Kumar, Xujun Peng, Sanghamitra Dutta, Daben Liu, Alfy Samuel",
        "summary": "RAG systems are increasingly deployed in high-stakes domains where users expect outputs to be consistent across semantically equivalent queries. However, existing systems often exhibit significant inconsistencies due to variability in both the retriever and generator (LLM), undermining trust and reliability. In this work, we focus on information consistency, i.e., the requirement that outputs convey the same core content across semantically equivalent inputs. We introduce a principled evaluation framework that decomposes RAG consistency into retriever-level, generator-level, and end-to-end components, helping identify inconsistency sources. To improve consistency, we propose Paraphrased Set Group Relative Policy Optimization (PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased set to assign group similarity rewards. We leverage PS-GRPO to achieve Information Consistent RAG (Con-RAG), training the generator to produce consistent outputs across paraphrased queries and remain robust to retrieval-induced variability. Because exact reward computation over paraphrase sets is computationally expensive, we also introduce a scalable approximation method that retains effectiveness while enabling efficient, large-scale training. Empirical evaluations across short-form, multi-hop, and long-form QA benchmarks demonstrate that Con-RAG significantly improves both consistency and accuracy over strong baselines, even in the absence of explicit ground-truth supervision. Our work provides practical solutions for evaluating and building reliable RAG systems for safety-critical deployments.",
        "subjects": "Computation and Language, Artificial Intelligence, Computers and Society, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.494939"
    },
    {
        "index": "#47",
        "title": "Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models",
        "link": "/arxiv/2510.04347",
        "arxiv_id": "2510.04347",
        "authors": "Anindya Sundar Das, Kangjie Chen, Monowar Bhuyan",
        "summary": "Pre-trained language models have achieved remarkable success across a wide range of natural language processing (NLP) tasks, particularly when fine-tuned on large, domain-relevant datasets. However, they remain vulnerable to backdoor attacks, where adversaries embed malicious behaviors using trigger patterns in the training data. These triggers remain dormant during normal usage, but, when activated, can cause targeted misclassifications. In this work, we investigate the internal behavior of backdoored pre-trained encoder-based language models, focusing on the consistent shift in attention and gradient attribution when processing poisoned inputs; where the trigger token dominates both attention and gradient signals, overriding the surrounding context. We propose an inference-time defense that constructs anomaly scores by combining token-level attention and gradient information. Extensive experiments on text classification tasks across diverse backdoor attack scenarios demonstrate that our method significantly reduces attack success rates compared to existing baselines. Furthermore, we provide an interpretability-driven analysis of the scoring mechanism, shedding light on trigger localization and the robustness of the proposed defense.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.495878"
    },
    {
        "index": "#48",
        "title": "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time",
        "link": "/arxiv/2510.04340",
        "arxiv_id": "2510.04340",
        "authors": "Daniel Tan, Anders Woodruff, Niels Warncke, Arun Jose, Maxime Riché, David Demitri Africa, Mia Taylor",
        "summary": "Language model finetuning often results in learning undesirable traits in combination with desired ones. To address this, we propose inoculation prompting: modifying finetuning data by prepending a short system-prompt instruction that deliberately elicits the undesirable trait. At test time, we evaluate without the instruction; inoculated models have much lower expression of the trait than models trained with unmodified training data. Inoculation is selective: in a toy setting where assistant responses are always in Spanish and ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'') teaches the model to capitalize responses while still responding in English. We find that inoculation is also effective across several additional settings: reducing emergent misalignment (EM) from task-specific finetuning, defending against backdoor injections, and mitigating the transmission of traits via subliminal learning. Follow-up analysis suggests a mechanism: making a trait less surprising via inoculation reduces optimization pressure to globally update the model, thereby reducing the degree of generalization. Our analysis relates to prior work on EM: inoculation explains prior findings that educational contexts mitigate EM from insecure code. Beyond demonstrating a simple and effective technique for selective learning, our results contribute to a better conceptual understanding of how and why language models generalize.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.502148"
    },
    {
        "index": "#49",
        "title": "Evaluation of Clinical Trials Reporting Quality using Large Language Models",
        "link": "/arxiv/2510.04338",
        "arxiv_id": "2510.04338",
        "authors": "Mathieu Laï-king, Patrick Paroubek",
        "summary": "Reporting quality is an important topic in clinical trial research articles, as it can impact clinical decisions. In this article, we test the ability of large language models to assess the reporting quality of this type of article using the Consolidated Standards of Reporting Trials (CONSORT). We create CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality with CONSORT-abstract standards. We then evaluate the ability of different large generative language models (from the general domain or adapted to the biomedical domain) to correctly assess CONSORT criteria with different known prompting methods, including Chain-of-thought. Our best combination of model and prompting method achieves 85% accuracy. Using Chain-of-thought adds valuable information on the model's reasoning for completing the task.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.502811"
    },
    {
        "index": "#50",
        "title": "Read the Scene, Not the Script: Outcome-Aware Safety for LLMs",
        "link": "/arxiv/2510.04320",
        "arxiv_id": "2510.04320",
        "authors": "Rui Wu, Yihao Quan, Zeru Shi, Zhenting Wang, Yanshu Li, Ruixiang Tang",
        "summary": "Safety-aligned Large Language Models (LLMs) still show two dominant failure modes: they are easily jailbroken, or they over-refuse harmless inputs that contain sensitive surface signals. We trace both to a common cause: current models reason weakly about links between actions and outcomes and over-rely on surface-form signals, lexical or stylistic cues that do not encode consequences. We define this failure mode as Consequence-blindness. To study consequence-blindness, we build a benchmark named CB-Bench covering four risk scenarios that vary whether semantic risk aligns with outcome risk, enabling evaluation under both matched and mismatched conditions which are often ignored by existing safety benchmarks. Mainstream models consistently fail to separate these risks and exhibit consequence-blindness, indicating that consequence-blindness is widespread and systematic. To mitigate consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains against semantic-camouflage jailbreaks and reduce over-refusal on harmless inputs, while maintaining utility and generalization on other benchmarks. These results clarify the limits of current alignment, establish consequence-aware reasoning as a core alignment goal and provide a more practical and reproducible evaluation path.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.503556"
    },
    {
        "index": "#51",
        "title": "Measuring Language Model Hallucinations Through Distributional Correctness",
        "link": "/arxiv/2510.04302",
        "arxiv_id": "2510.04302",
        "authors": "Thomas F Burns",
        "summary": "Common evaluation paradigms for language models focus on scoring single responses through accuracy metrics or proper scoring rules, failing to capture the full richness of a model's belief state. Recent work illustrates that language models hallucinate in-part because they are optimised to be good test-takers under binary scoring schemes that reward any answer over abstention. While this insight naturally leads to penalty-based approaches, they ignore crucial distinctions in how models distribute uncertainty, for example between hedging toward incorrect answers versus hedging toward \"I don't know\" responses. A novel evaluation metric, the Distributional Correctness Score (DCS), is introduced to solve this problem, i.e., of not considering a model's entire probability distribution over answer choices. DCS naturally distinguishes between harmful overconfidence in wrong answers and uncertainty expressed through abstention, providing scores in an interpretable default range. Through theoretical analysis and illustrative examples, DCS is demonstrated to offer a more nuanced and aligned evaluation paradigm that incentivises models to express genuine uncertainty rather than guessing. Adapting 12 existing evaluation benchmarks to DCS's variants and measuring performance on six language models reveals that for half of the tested benchmarks scores are negative across all tested models, indicating significant tendencies towards hallucination.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.504160"
    },
    {
        "index": "#52",
        "title": "Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness",
        "link": "/arxiv/2510.04293",
        "arxiv_id": "2510.04293",
        "authors": "Lingnan Xu, Chong Feng, Kaiyuan Zhang, Liu Zhengyong, Wenqiang Xu, Fanqing Meng",
        "summary": "While large language models (LLMs) demonstrate impressive capabilities, their reliance on parametric knowledge often leads to factual inaccuracies. Retrieval-Augmented Generation (RAG) mitigates this by leveraging external documents, yet existing approaches treat retrieved passages as isolated chunks, ignoring valuable structure that is crucial for document organization. Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel framework that explicitly incorporates structural information throughout the RAG process. RDR2 employs an LLM-based router to dynamically navigate document structure trees, jointly evaluating content relevance and hierarchical relationships to assemble optimal evidence. Our key innovation lies in formulating document routing as a trainable task, with automatic action curation and structure-aware passage selection inspired by human reading strategies. Through comprehensive evaluation on five challenging datasets, RDR2 achieves state-of-the-art performance, demonstrating that explicit structural awareness significantly enhances RAG systems' ability to acquire and utilize knowledge, particularly in complex scenarios requiring multi-document synthesis.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.504880"
    },
    {
        "index": "#53",
        "title": "PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis",
        "link": "/arxiv/2510.04291",
        "arxiv_id": "2510.04291",
        "authors": "Mehrzad Tareh, Aydin Mohandesi, Ebrahim Ansari",
        "summary": "Sentiment analysis is a key task in Natural Language Processing (NLP), enabling the extraction of meaningful insights from user opinions across various domains. However, performing sentiment analysis in Persian remains challenging due to the scarcity of labeled datasets, limited preprocessing tools, and the lack of high-quality embeddings and feature extraction methods. To address these limitations, we propose a hybrid approach that integrates machine learning (ML) and deep learning (DL) techniques for Persian aspect-based sentiment analysis (ABSA). In particular, we utilize polarity scores from multilingual BERT as additional features and incorporate them into a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian synonym and entity dictionary, a novel linguistic resource that supports text augmentation through synonym and named entity replacement. Our results demonstrate the effectiveness of hybrid modeling and feature augmentation in advancing sentiment analysis for low-resource languages such as Persian.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.505540"
    },
    {
        "index": "#54",
        "title": "SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling",
        "link": "/arxiv/2510.04286",
        "arxiv_id": "2510.04286",
        "authors": "Harshil Vejendla",
        "summary": "Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a sparse subset of feed-forward experts. Token-level routing, however, assigns an entire semantic spectrum to each expert, creating capacity bottlenecks, load-balancing pathologies, and limited specialization. We introduce SliceMoE, an architecture that routes contiguous slices of a token's hidden vector. A d-dimensional embedding is partitioned into S slices, and for each slice, a lightweight shared router predicts the top-k experts. Experts operate on their assigned slices independently, and outputs are reassembled, maintaining per-token FLOP efficiency. Because slices from different tokens interleave within an expert, utilization is naturally smoother. We propose a slice-level capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels. Experiments on WikiText-103 language modeling, WMT En-De translation, and three text-classification datasets show SliceMoE attains up to 1.7x faster inference than dense baselines, 12 to 18 percent lower perplexity than parameter-matched token-MoE, and improved expert balance, with interpretable expertise over syntactic versus semantic subspaces.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.506215"
    },
    {
        "index": "#55",
        "title": "Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy",
        "link": "/arxiv/2510.04285",
        "arxiv_id": "2510.04285",
        "authors": "Karthik Viswanathan, Sang Eon Park",
        "summary": "We introduce a cumulant-expansion framework for quantifying how large language models (LLMs) internalize higher-order statistical structure during next-token prediction. By treating the softmax entropy of each layer's logit distribution as a perturbation around its \"center\" distribution, we derive closed-form cumulant observables that isolate successively higher-order correlations. Empirically, we track these cumulants in GPT-2 and Pythia models on Pile-10K prompts. (i) Structured prompts exhibit a characteristic rise-and-plateau profile across layers, whereas token-shuffled prompts remain flat, revealing the dependence of the cumulant profile on meaningful context. (ii) During training, all cumulants increase monotonically before saturating, directly visualizing the model's progression from capturing variance to learning skew, kurtosis, and higher-order statistical structures. (iii) Mathematical prompts show distinct cumulant signatures compared to general text, quantifying how models employ fundamentally different processing mechanisms for mathematical versus linguistic content. Together, these results establish cumulant analysis as a lightweight, mathematically grounded probe of feature-learning dynamics in high-dimensional neural networks.",
        "subjects": "Computation and Language, Statistical Mechanics, Machine Learning, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.537529"
    },
    {
        "index": "#56",
        "title": "LongTail-Swap: benchmarking language models' abilities on rare words",
        "link": "/arxiv/2510.04268",
        "arxiv_id": "2510.04268",
        "authors": "Robin Algayres, Charles-Éric Saint-James, Mahi Luthra, Jiayi Shen, Dongyan Lin, Youssef Benchekroun, Rashel Moritz, Juan Pino, Emmanuel Dupoux",
        "summary": "Children learn to speak with a low amount of data and can be taught new words on a few-shot basis, making them particularly data-efficient learners. The BabyLM challenge aims at exploring language model (LM) training in the low-data regime but uses metrics that concentrate on the head of the word distribution. Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the tail of the distribution, i.e., measures the ability of LMs to learn new words with very little exposure, like infants do. LT-Swap is a pretraining corpus-specific test set of acceptable versus unacceptable sentence pairs that isolate semantic and syntactic usage of rare words. Models are evaluated in a zero-shot fashion by computing the average log probabilities over the two members of each pair. We built two such test sets associated with the 10M words and 100M words BabyLM training sets, respectively, and evaluated 16 models from the BabyLM leaderboard. Our results not only highlight the poor performance of language models on rare words but also reveal that performance differences across LM architectures are much more pronounced in the long tail than in the head. This offers new insights into which architectures are better at handling rare word generalization. We've also made the code publicly avail",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.538697"
    },
    {
        "index": "#57",
        "title": "Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought",
        "link": "/arxiv/2510.04230",
        "arxiv_id": "2510.04230",
        "authors": "Guijin Son, Donghun Yang, Hitesh Laxmichand Patel, Amit Agarwal, Hyunwoo Ko, Chanuk Lim, Srikant Panda, Minhyuk Kim, Nikunj Drolia, Dasol Choi, Kyong-Ha Lee, Youngjae Yu",
        "summary": "Recent frontier models employ long chain-of-thought reasoning to explore solution spaces in context and achieve stonger performance. While many works study distillation to build smaller yet capable models, most focus on English and little is known about language-specific reasoning. To bridge this gap, we first introduct **Language-Mixed CoT**, a reasoning schema that switches between English and a target language, using English as an anchor to excel in reasoning while minimizing translation artificats. As a Korean case study, we curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5, Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves state-of-the-art performance, with the highest overall average score (64.0 \\pm 25), ranking first on 5/9 benchmarks and second on the remainder. Samller and mid-sized models also benefit substantially, with an average improvement of +18.6 points across teh evaluated nine benchmarks. Ablations show **Language-Mixed CoT** is more effective than monolingual CoT, also resulting in cross-lingual and mult-modal performance gains. We release our data-curation pipeline, evaluation system, datasets, and models to advance research on language-specific reasoning. Data and model collection: https://huggingface.co/KOREAson.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.539981"
    },
    {
        "index": "#58",
        "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models",
        "link": "/arxiv/2510.04226",
        "arxiv_id": "2510.04226",
        "authors": "Dustin Wright, Sarah Masud, Jared Moore, Srishti Yadav, Maria Antoniak, Chan Young Park, Isabelle Augenstein",
        "summary": "Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses a risk of knowledge collapse, where homogenous LLMs mediate a shrinking in the range of accessible information over time. Existing works on homogenization are limited by a focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform a broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation",
        "subjects": "Computation and Language, Artificial Intelligence, Computers and Society, Information Retrieval, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.540932"
    },
    {
        "index": "#59",
        "title": "Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards",
        "link": "/arxiv/2510.04214",
        "arxiv_id": "2510.04214",
        "authors": "Zhuoran Zhuang, Ye Chen, Xia Zeng, Chao Luo, Luhui Liu, Yihan Chen",
        "summary": "We study deploying large language models (LLMs) as business development (BD) agents for persuasive price negotiation in online travel agencies (OTAs), where aligning traveler affordability and hotel profitability directly affects bookings, partner relationships, and access to travel. The agent must follow a Standard Operating Procedure (SOP) while conducting multi-turn persuasion, interpreting colloquial inputs, and adhering to guardrails (no over-promising, no hallucinations). Conventional post-training -- supervised fine-tuning (SFT) or single-source reward optimization -- overfits scripts, misses nuanced persuasive style, and fails to enforce verifiable business constraints. We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement learning post-training framework that aligns an LLM with heterogeneous rewards: a preference-trained reward model (RM) for dense human alignment, a reward judge (RJ) for high-level persuasive behavior and SOP compliance, and programmatic reward functions (RF) for deterministic checks on numerics, formatting, and guardrails. A straightforward enhancement mechanism is proposed to combine the RM with RJ and RF signals to curb reward hacking and improve negotiation quality. In production-style evaluations -- approximately 150 turns from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO), increases the share of conversations with at least one excellent response to 66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also observe emergent capabilities -- proactive empathy, localized reasoning, calibrated tactics -- that surpass gold annotations.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.541614"
    },
    {
        "index": "#60",
        "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling",
        "link": "/arxiv/2510.04204",
        "arxiv_id": "2510.04204",
        "authors": "Zhengyang Tang, Zihan Ye, Chenyu Huang, Xuhan Huang, Chengpeng Li, Sihang Li, Guanhua Chen, Ming Yan, Zizhuo Wang, Hongyuan Zha, Dayiheng Liu, Benyou Wang",
        "summary": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs -- In particular, we show that direct fine-tuning on traditional \\textit{non-reflective} datasets leads to limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose \\textbf{CALM} (\\textit{Corrective Adaptation with Lightweight Modification}), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6\\% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop \\textbf{STORM} (\\textit{Smart Thinking Optimization Reasoning Model}), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9\\% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks.",
        "subjects": "Computation and Language, Artificial Intelligence, Computational Engineering, Finance, and Science, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.542543"
    },
    {
        "index": "#61",
        "title": "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization",
        "link": "/arxiv/2510.04182",
        "arxiv_id": "2510.04182",
        "authors": "Wengao Ye, Yan Liang, Lianlei Shan",
        "summary": "Recent advancements in Large Language Models (LLMs) have shifted from explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning, where intermediate thoughts are represented as vectors rather than text. However, latent reasoning can be brittle on challenging, out-of-distribution tasks where robust reasoning is most critical. To overcome these limitations, we introduce Latent Thought Policy Optimization (LTPO), a parameter-free framework that enhances LLM reasoning entirely at test time, without requiring model parameter updates. LTPO treats intermediate latent \"thought\" vectors as dynamic parameters that are actively optimized for each problem instance. It employs an online policy gradient method guided by an intrinsic, confidence-based reward signal computed directly from the frozen LLM's own output distributions, eliminating the need for external supervision or expensive text generation during optimization. Extensive experiments on five reasoning benchmarks show that LTPO not only matches or surpasses strong baselines on standard tasks but also demonstrates remarkable robustness where others fail. Most notably, on highly challenging AIME benchmarks where existing latent reasoning baselines collapse to near-zero accuracy, LTPO delivers substantial improvements, showcasing a unique capability for complex reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.548369"
    },
    {
        "index": "#62",
        "title": "Self Speculative Decoding for Diffusion Large Language Models",
        "link": "/arxiv/2510.04147",
        "arxiv_id": "2510.04147",
        "authors": "Yifeng Gao, Ziang Ji, Yuxuan Wang, Biqing Qi, Hanlin Xu, Linfeng Zhang",
        "summary": "Diffusion-based Large Language Models (dLLMs) have emerged as a competitive alternative to autoregressive models, offering unique advantages through bidirectional attention and parallel generation paradigms. However, the generation results of current parallel decoding methods deviate from stepwise decoding, introducing potential performance degradation, which limits their practical deployment. To address this problem, we propose \\textbf{S}elf \\textbf{S}peculative \\textbf{D}ecoding (SSD), a lossless inference acceleration method that leverages the dLLM itself as both speculative decoding drafter and verifier without auxiliary modules. SSD introduces a self-drafting mechanism where the model generates predictions for multiple positions, then verifies them through hierarchical verification trees in a single forward pass. Unlike traditional speculative decoding that requires separate draft models, SSD eliminates model redundancy and memory overhead by exploiting the dLLM's inherent parallel prediction capability for multiple positions. This self-speculative approach allows the model to progressively verify and accept multiple tokens in a single forward pass. Our experiments demonstrate that SSD achieves up to 3.46$\\times$ speedup while keeping the output identical to stepwise decoding on open source models such as LLaDA and Dream. Code will be made publicly available on GitHub.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.549079"
    },
    {
        "index": "#63",
        "title": "Fine Tuning Methods for Low-resource Languages",
        "link": "/arxiv/2510.04139",
        "arxiv_id": "2510.04139",
        "authors": "Tim Bakkenes, Daniel Wang, Anton Johansson",
        "summary": "The rise of Large Language Models has not been inclusive of all cultures. The models are mostly trained on English texts and culture which makes them underperform in other languages and cultural contexts. By developing a generalizable method for preparing culturally relevant datasets and post-training the Gemma 2 model, this project aimed to increase the performance of Gemma 2 for an underrepresented language and showcase how others can do the same to unlock the power of Generative AI in their country and preserve their cultural heritage.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.549727"
    },
    {
        "index": "#64",
        "title": "Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)",
        "link": "/arxiv/2510.04124",
        "arxiv_id": "2510.04124",
        "authors": "Nuwan I. Senaratna",
        "summary": "We present a collection of open, machine-readable document datasets covering parliamentary proceedings, legal judgments, government publications, news, and tourism statistics from Sri Lanka. As of v20251005, the collection currently comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and English. The datasets are updated daily and mirrored on GitHub and Hugging Face. These resources aim to support research in computational linguistics, legal analytics, socio-political studies, and multilingual natural language processing. We describe the data sources, collection pipeline, formats, and potential use cases, while discussing licensing and ethical considerations.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.550327"
    },
    {
        "index": "#65",
        "title": "Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence",
        "link": "/arxiv/2510.04120",
        "arxiv_id": "2510.04120",
        "authors": "Fengying Ye, Shanshan Wang, Lidia S. Chao, Derek F. Wong",
        "summary": "Metaphor analysis is a complex linguistic phenomenon shaped by context and external factors. While Large Language Models (LLMs) demonstrate advanced capabilities in knowledge integration, contextual reasoning, and creative generation, their mechanisms for metaphor comprehension remain insufficiently explored. This study examines LLMs' metaphor-processing abilities from three perspectives: (1) Concept Mapping: using embedding space projections to evaluate how LLMs map concepts in target domains (e.g., misinterpreting \"fall in love\" as \"drop down from love\"); (2) Metaphor-Literal Repository: analyzing metaphorical words and their literal counterparts to identify inherent metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how metaphorical syntactic structures influence LLMs' performance. Our findings reveal that LLMs generate 15\\%-25\\% conceptually irrelevant interpretations, depend on metaphorical indicators in training data rather than contextual cues, and are more sensitive to syntactic irregularities than to structural comprehension. These insights underline the limitations of LLMs in metaphor analysis and call for more robust computational approaches.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.551013"
    },
    {
        "index": "#66",
        "title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning",
        "link": "/arxiv/2510.04081",
        "arxiv_id": "2510.04081",
        "authors": "Honglin Lin, Qizhi Pei, Xin Gao, Zhuoshi Pan, Yu Li, Juntao Li, Conghui He, Lijun Wu",
        "summary": "Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Caco's code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention.",
        "subjects": "Computation and Language, Programming Languages",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.551831"
    },
    {
        "index": "#67",
        "title": "PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity",
        "link": "/arxiv/2510.04080",
        "arxiv_id": "2510.04080",
        "authors": "Zixin Song, Bowen Zhang, Qian-Wen Zhang, Di Yin, Xing Sun, Chunping Li",
        "summary": "Conditional Semantic Textual Similarity (C-STS) measures the semantic proximity between text segments under a specific condition, thereby overcoming the ambiguity inherent in traditional STS. However, existing methods are largely confined to discriminative models, failing to fully integrate recent breakthroughs in the NLP community concerning Large Language Models (LLMs) and Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this task, as it can directly optimize the non-differentiable Spearman ranking metric and guide the reasoning process required by C-STS. However, we find that naively applying listwise RL fails to produce meaningful improvements, as the model is overwhelmed by complex, coarse-grained reward signals. To address this challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning framework. PoLi-RL employs a two-stage curriculum: it first trains the model with simple pointwise rewards to establish fundamental scoring capabilities, then transitions to a hybrid reward that combines pointwise, pairwise, and listwise objectives to refine the model's ability to discern subtle semantic distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward (PSRR) mechanism that computes ranking rewards in parallel slices, where each slice comprises same-indexed completions from different samples. This provides a precise, differentiated learning signal for each individual completion, enabling granular credit assignment and effective optimization. On the official C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18, establishing a new SOTA for the cross-encoder architecture. As the first work to successfully apply RL to C-STS, our study introduces a powerful and precise paradigm for training LLMs on complex, ranking-based conditional judgment tasks.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.552517"
    },
    {
        "index": "#68",
        "title": "What Makes Diffusion Language Models Super Data Learners?",
        "link": "/arxiv/2510.04071",
        "arxiv_id": "2510.04071",
        "authors": "Zitian Gao, Haoming Luo, Lynx Chen, Jason Klein Liu, Ran Tao, Joey Zhou, Bryan Dai",
        "summary": "Recent studies have shown that diffusion language models achieve remarkable data efficiency under limited-data constraints, yet the underlying mechanisms remain unclear. In this work, we perform extensive ablation experiments to disentangle the sources of this efficiency. Our results show that random masking of input tokens plays the dominant role. We further show that similar gains can be obtained through in MLP dropout and weight decay, indicating that stochastic regularization broadly enhances data efficiency in multi-epoch training. Our code is available at https://github.com/zitian-gao/data-efficiency.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.558373"
    },
    {
        "index": "#69",
        "title": "Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment",
        "link": "/arxiv/2510.04045",
        "arxiv_id": "2510.04045",
        "authors": "Yunfan Zhang, Kathleen McKeown, Smaranda Muresan",
        "summary": "Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.559052"
    },
    {
        "index": "#70",
        "title": "Small Language Models for Emergency Departments Decision Support: A Benchmark Study",
        "link": "/arxiv/2510.04032",
        "arxiv_id": "2510.04032",
        "authors": "Zirui Wang, Jiajun Wu, Braden Teitge, Jessalyn Holodinsky, Steve Drew",
        "summary": "Large language models (LLMs) have become increasingly popular in medical domains to assist physicians with a variety of clinical and operational tasks. Given the fast-paced and high-stakes environment of emergency departments (EDs), small language models (SLMs), characterized by a reduction in parameter count compared to LLMs, offer significant potential due to their inherent reasoning capability and efficient performance. This enables SLMs to support physicians by providing timely and accurate information synthesis, thereby improving clinical decision-making and workflow efficiency. In this paper, we present a comprehensive benchmark designed to identify SLMs suited for ED decision support, taking into account both specialized medical expertise and broad general problem-solving capabilities. In our evaluations, we focus on SLMs that have been trained on a mixture of general-domain and medical corpora. A key motivation for emphasizing SLMs is the practical hardware limitations, operational cost constraints, and privacy concerns in the typical real-world deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and PubMedQA, with the medical abstracts dataset emulating tasks aligned with real ED physicians' daily tasks. Experimental results reveal that general-domain SLMs surprisingly outperform their medically fine-tuned counterparts across these diverse benchmarks for ED. This indicates that for ED, specialized medical fine-tuning of the model may not be required.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.559793"
    },
    {
        "index": "#71",
        "title": "Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?",
        "link": "/arxiv/2510.04031",
        "arxiv_id": "2510.04031",
        "authors": "Nelvin Tan, James Asikin Cheung, Yu-Ching Shih, Dong Yang, Amol Salunkhe",
        "summary": "Large language models (LLMs) are becoming useful in many domains due to their impressive abilities that arise from large training datasets and large model sizes. More recently, they have been shown to be very effective in textual classification tasks, motivating the need to explain the LLMs' decisions. Motivated by practical constrains where LLMs are black-boxed and LLM calls are expensive, we study how incorporating counterfactuals into LLM reasoning can affect the LLM's ability to identify the top words that have contributed to its classification decision. To this end, we introduce a framework called the decision changing rate that helps us quantify the importance of the top words in classification. Our experimental results show that using counterfactuals can be helpful.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.560524"
    },
    {
        "index": "#72",
        "title": "Thai Semantic End-of-Turn Detection for Real-Time Voice Agents",
        "link": "/arxiv/2510.04016",
        "arxiv_id": "2510.04016",
        "authors": "Thanapol Popit, Natthapath Rungseesiripak, Monthol Charattrakool, Saksorn Ruangtanusak",
        "summary": "Fluid voice-to-voice interaction requires reliable and low-latency detection of when a user has finished speaking. Traditional audio-silence end-pointers add hundreds of milliseconds of delay and fail under hesitations or language-specific phenomena. We present, to our knowledge, the first systematic study of Thai text-only end-of-turn (EOT) detection for real-time agents. We compare zero-shot and few-shot prompting of compact LLMs to supervised fine-tuning of lightweight transformers. Using transcribed subtitles from the YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final particles), we formulate EOT as a binary decision over token boundaries. We report a clear accuracy-latency tradeoff and provide a public-ready implementation plan. This work establishes a Thai baseline and demonstrates that small, fine-tuned models can deliver near-instant EOT decisions suitable for on-device agents.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.561210"
    },
    {
        "index": "#73",
        "title": "LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization",
        "link": "/arxiv/2510.04013",
        "arxiv_id": "2510.04013",
        "authors": "Jiarui Liu, Jivitesh Jain, Mona Diab, Nishant Subramani",
        "summary": "Although large language models (LLMs) have tremendous utility, trustworthiness is still a chief concern: models often generate incorrect information with high confidence. While contextual information can help guide generation, identifying when a query would benefit from retrieved context and assessing the effectiveness of that context remains challenging. In this work, we operationalize interpretability methods to ascertain whether we can predict the correctness of model outputs from the model's activations alone. We also explore whether model internals contain signals about the efficacy of external context. We consider correct, incorrect, and irrelevant context and introduce metrics to distinguish amongst them. Experiments on six different models reveal that a simple classifier trained on intermediate layer activations of the first output token can predict output correctness with about 75% accuracy, enabling early auditing. Our model-internals-based metric significantly outperforms prompting baselines at distinguishing between correct and incorrect context, guarding against inaccuracies introduced by polluted context. These findings offer a lens to better understand the underlying decision-making processes of LLMs. Our code is publicly available at https://github.com/jiarui-liu/LLM-Microscope",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.561859"
    },
    {
        "index": "#74",
        "title": "AgriGPT-VL: Agricultural Vision-Language Understanding Suite",
        "link": "/arxiv/2510.04002",
        "arxiv_id": "2510.04002",
        "authors": "Bo Yang, Yunkui Chen, Lanfei Feng, Yu Zhang, Xiao Xu, Jianyu Zhang, Nueraili Aierken, Runhe Huang, Hongjian Lin, Yibin Ying, Shijian Li",
        "summary": "Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the scarcity of domain-tailored models, curated vision-language corpora, and rigorous evaluation. To address these challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL, the largest vision-language corpus for agriculture to our knowledge, curated by a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO reinforcement learning samples. Second, we develop AgriGPT-VL, an agriculture-specialized vision-language model trained via a progressive curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO refinement. This method achieves strong multimodal reasoning while preserving text-only capability. Third, we establish AgriBench-VL-4K, a compact yet challenging evaluation suite with open-ended and image-grounded questions, paired with multi-metric evaluation and an LLM-as-a-judge framework. Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K with no noticeable degradation of language ability. Ablation studies further confirm consistent gains from our alignment and GRPO refinement stages. We will open source all of the resources to support reproducible research and deployment in low-resource agricultural settings.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.562662"
    },
    {
        "index": "#75",
        "title": "Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation",
        "link": "/arxiv/2510.04001",
        "arxiv_id": "2510.04001",
        "authors": "Xuankang Zhang, Jiangming Liu",
        "summary": "The COVID-19 pandemic causes severe social and economic disruption around the world, raising various subjects that are discussed over social media. Identifying pandemic-related named entities as expressed on social media is fundamental and important to understand the discussions about the pandemic. However, there is limited work on named entity recognition on this topic due to the following challenges: 1) COVID-19 texts in social media are informal and their annotations are rare and insufficient to train a robust recognition model, and 2) named entity recognition in COVID-19 requires extensive domain-specific knowledge. To address these issues, we propose a novel entity knowledge augmentation approach for COVID-19, which can also be applied in general biomedical named entity recognition in both informal text format and formal text format. Experiments carried out on the COVID-19 tweets dataset and PubMed dataset show that our proposed entity knowledge augmentation improves NER performance in both fully-supervised and few-shot settings. Our source code is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.588970"
    },
    {
        "index": "#76",
        "title": "Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions",
        "link": "/arxiv/2510.03999",
        "arxiv_id": "2510.03999",
        "authors": "Yang Xu, Xuanming Zhang, Min-Hsuan Yeh, Jwala Dhamala, Ousmane Dia, Rahul Gupta, Yixuan Li",
        "summary": "Deception is a pervasive feature of human communication and an emerging concern in large language models (LLMs). While recent studies document instances of LLM deception under pressure, most evaluations remain confined to single-turn prompts and fail to capture the long-horizon interactions in which deceptive strategies typically unfold. We introduce the first simulation framework for probing and evaluating deception in LLMs under extended sequences of interdependent tasks and dynamic contextual pressures. Our framework instantiates a multi-agent system: a performer agent tasked with completing tasks and a supervisor agent that evaluates progress, provides feedback, and maintains evolving states of trust. An independent deception auditor then reviews full trajectories to identify when and how deception occurs. We conduct extensive experiments across 11 frontier models, spanning both closed- and open-source systems, and find that deception is model-dependent, increases with event pressure, and consistently erodes supervisor trust. Qualitative analyses further reveal distinct strategies of concealment, equivocation, and falsification. Our findings establish deception as an emergent risk in long-horizon interactions and provide a foundation for evaluating future LLMs in real-world, trust-sensitive contexts.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.590140"
    },
    {
        "index": "#77",
        "title": "Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs",
        "link": "/arxiv/2510.03997",
        "arxiv_id": "2510.03997",
        "authors": "Junjie Luo, Rui Han, Arshana Welivita, Zeleikun Di, Jingfu Wu, Xuzhe Zhi, Ritu Agarwal, Gordon Gao",
        "summary": "Understanding how patients perceive their physicians is essential to improving trust, communication, and satisfaction. We present a large language model (LLM)-based pipeline that infers Big Five personality traits and five patient-oriented subjective judgments. The analysis encompasses 4.1 million patient reviews of 226,999 U.S. physicians from an initial pool of one million. We validate the method through multi-model comparison and human expert benchmarking, achieving strong agreement between human and LLM assessments (correlation coefficients 0.72-0.89) and external validity through correlations with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis reveals systematic patterns: male physicians receive higher ratings across all traits, with largest disparities in clinical competence perceptions; empathy-related traits predominate in pediatrics and psychiatry; and all traits positively predict overall satisfaction. Cluster analysis identifies four distinct physician archetypes, from \"Well-Rounded Excellent\" (33.8%, uniformly high traits) to \"Underperforming\" (22.6%, consistently low). These findings demonstrate that automated trait extraction from patient narratives can provide interpretable, validated metrics for understanding physician-patient relationships at scale, with implications for quality measurement, bias detection, and workforce development in healthcare.",
        "subjects": "Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.591249"
    },
    {
        "index": "#78",
        "title": "PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian",
        "link": "/arxiv/2510.03913",
        "arxiv_id": "2510.03913",
        "authors": "Mohammad Amin Abbasi, Hassan Naderi",
        "summary": "This study presents PsychoLexTherapy, a framework for simulating psychotherapeutic reasoning in Persian using small language models (SLMs). The framework tackles the challenge of developing culturally grounded, therapeutically coherent dialogue systems with structured memory for multi-turn interactions in underrepresented languages. To ensure privacy and feasibility, PsychoLexTherapy is optimized for on-device deployment, enabling use without external servers. Development followed a three-stage process: (i) assessing SLMs psychological knowledge with PsychoLexEval; (ii) designing and implementing the reasoning-oriented PsychoLexTherapy framework; and (iii) constructing two evaluation datasets-PsychoLexQuery (real Persian user questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark against multiple baselines. Experiments compared simple prompting, multi-agent debate, and structured therapeutic reasoning paths. Results showed that deliberate model selection balanced accuracy, efficiency, and privacy. On PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic LLM-as-a-judge evaluation and was ranked highest by human evaluators in a single-turn preference study. In multi-turn tests with PsychoLexDialogue, the long-term memory module proved essential: while naive history concatenation caused incoherence and information loss, the full framework achieved the highest ratings in empathy, coherence, cultural fit, and personalization. Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and culturally aligned foundation for Persian psychotherapy simulation, contributing novel datasets, a reproducible evaluation pipeline, and empirical insights into structured memory for therapeutic reasoning.",
        "subjects": "Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.592172"
    },
    {
        "index": "#79",
        "title": "Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles",
        "link": "/arxiv/2510.03898",
        "arxiv_id": "2510.03898",
        "authors": "Nusrat Jahan Lia, Shubhashis Roy Dipta, Abdullah Khan Zehady, Naymul Islam, Madhusodan Chakraborty, Abdullah Al Wasif",
        "summary": "Detecting media bias is crucial, specifically in the South Asian region. Despite this, annotated datasets and computational studies for Bangla political bias research remain scarce. Crucially because, political stance detection in Bangla news requires understanding of linguistic cues, cultural context, subtle biases, rhetorical strategies, code-switching, implicit sentiment, and socio-political background. To address this, we introduce the first benchmark dataset of 200 politically significant and highly debated Bangla news articles, labeled for government-leaning, government-critique, and neutral stances, alongside diagnostic analyses for evaluating large language models (LLMs). Our comprehensive evaluation of 28 proprietary and open-source LLMs shows strong performance in detecting government-critique content (F1 up to 0.83) but substantial difficulty with neutral articles (F1 as low as 0.00). Models also tend to over-predict government-leaning stances, often misinterpreting ambiguous narratives. This dataset and its associated diagnostics provide a foundation for advancing stance detection in Bangla media research and offer insights for improving LLM performance in low-resource languages.",
        "subjects": "Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.593183"
    },
    {
        "index": "#80",
        "title": "Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches",
        "link": "/arxiv/2510.03808",
        "arxiv_id": "2510.03808",
        "authors": "Mehedi Hasan Emon",
        "summary": "This research explores the annotation of rhetorical relations in discourse using the INCEpTION tool and compares manual annotation with automatic approaches based on large language models. The study focuses on sports reports (specifically cricket news) and evaluates the performance of BERT, DistilBERT, and Logistic Regression models in classifying rhetorical relations such as elaboration, contrast, background, and cause-effect. The results show that DistilBERT achieved the highest accuracy, highlighting its potential for efficient discourse relation prediction. This work contributes to the growing intersection of discourse parsing and transformer-based NLP. (This paper was conducted as part of an academic requirement under the supervision of Prof. Dr. Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords: Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing, NLP.",
        "subjects": "Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.599011"
    },
    {
        "index": "#81",
        "title": "Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models",
        "link": "/arxiv/2510.03805",
        "arxiv_id": "2510.03805",
        "authors": "Canhui Wu, Qiong Cao, Chang Li, Zhenfang Wang, Chao Xue, Yuwei Fan, Wei Xi, Xiaodong He",
        "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks but often suffer from excessive verbosity, known as \"overthinking.\" Existing solutions via reinforcement learning (RL) typically penalize generated tokens to promote conciseness. However, these methods encounter two challenges: responses with fewer tokens do not always correspond to fewer reasoning steps, and models may develop hacking behavior in later stages of training by discarding reasoning steps to minimize token usage. In this work, we introduce \\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more efficient reasoning by favoring compact reasoning steps. Our step-aware reward function prioritizes correctness while imposing penalties for redundant steps, and withholds rewards for incorrect responses to prevent the reinforcement of erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when the length of any output step exceeds the upper limit, we halt updates to prevent hacking behavior caused by merging steps. Extensive experiments across four reasoning benchmarks demonstrate that SP achieves state-of-the-art accuracy while significantly reducing response length. For instance, on AIME24, SP reduces token usage by \\textbf{69.7\\%}.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.599801"
    },
    {
        "index": "#82",
        "title": "Mechanistic Interpretability of Socio-Political Frames in Language Models",
        "link": "/arxiv/2510.03799",
        "arxiv_id": "2510.03799",
        "authors": "Hadi Asghari, Sami Nenno",
        "summary": "This paper explores the ability of large language models to generate and recognize deep cognitive frames, particularly in socio-political contexts. We demonstrate that LLMs are highly fluent in generating texts that evoke specific frames and can recognize these frames in zero-shot settings. Inspired by mechanistic interpretability research, we investigate the location of the `strict father' and `nurturing parent' frames within the model's hidden representation, identifying singular dimensions that correlate strongly with their presence. Our findings contribute to understanding how LLMs capture and express meaningful human concepts.",
        "subjects": "Computation and Language, Artificial Intelligence, Computers and Society",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.600486"
    },
    {
        "index": "#83",
        "title": "Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development",
        "link": "/arxiv/2510.03781",
        "arxiv_id": "2510.03781",
        "authors": "Majid Asgari-Bidhendi, Muhammad Amin Ghaseminia, Alireza Shahbazi, Sayyed Ali Hossayni, Najmeh Torabian, Behrouz Minaei-Bidgoli",
        "summary": "This paper presents the development of Rezwan, a large-scale AI-assisted Hadith corpus comprising over 1.2M narrations, extracted and structured through a fully automated pipeline. Building on digital repositories such as Maktabat Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for segmentation, chain--text separation, validation, and multi-layer enrichment. Each narration is enhanced with machine translation into twelve languages, intelligent diacritization, abstractive summarization, thematic tagging, and cross-text semantic analysis. This multi-step process transforms raw text into a richly annotated research-ready infrastructure for digital humanities and Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled narrations, assessed by six domain experts. Results show near-human accuracy in structured tasks such as chain--text separation (9.33/10) and summarization (9.33/10), while highlighting ongoing challenges in diacritization and semantic similarity detection. Comparative analysis against the manually curated Noor Corpus demonstrates the superiority of Najm in both scale and quality, with a mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis confirms the economic feasibility of the AI approach: tasks requiring over 229,000 hours of expert labor were completed within months at a fraction of the cost. The work introduces a new paradigm in religious text processing by showing how AI can augment human expertise, enabling large-scale, multilingual, and semantically enriched access to Islamic heritage.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.601225"
    },
    {
        "index": "#84",
        "title": "Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs",
        "link": "/arxiv/2510.03762",
        "arxiv_id": "2510.03762",
        "authors": "Deshan Sumanathilaka, Nicholas Micallef, Julian Hough",
        "summary": "Recent advances in Large Language Models (LLMs) have significantly reshaped the landscape of Natural Language Processing (NLP). Among the various prompting techniques, few-shot prompting has gained considerable attention for its practicality and effectiveness. This study investigates how few-shot prompting strategies impact the Word Sense Disambiguation (WSD) task, particularly focusing on the biases introduced by imbalanced sample distributions. We use the GLOSSGPT prompting method, an advanced approach for English WSD, to test its effectiveness across five languages: English, German, Spanish, French, and Italian. Our results show that imbalanced few-shot examples can cause incorrect sense predictions in multilingual languages, but this issue does not appear in English. To assess model behavior, we evaluate both the GPT-4o and LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual WSD to sample distribution in few-shot settings, emphasizing the need for balanced and representative prompting strategies.",
        "subjects": "Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.601857"
    },
    {
        "index": "#85",
        "title": "Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech",
        "link": "/arxiv/2510.03758",
        "arxiv_id": "2510.03758",
        "authors": "Ilias Tougui, Mehdi Zakroum, Mounir Ghogho",
        "summary": "Parkinson's Disease (PD) affects over 10 million people worldwide, with speech impairments in up to 89% of patients. Current speech-based detection systems analyze entire utterances, potentially overlooking the diagnostic value of specific phonetic elements. We developed a granularity-aware approach for multilingual PD detection using an automated pipeline that extracts time-aligned phonemes, syllables, and words from recordings. Using Italian, Spanish, and English datasets, we implemented a bidirectional LSTM with multi-head attention to compare diagnostic performance across the different granularity levels. Phoneme-level analysis achieved superior performance with AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates enhanced diagnostic capability for cross-linguistic PD detection. Importantly, attention analysis revealed that the most informative speech features align with those used in established clinical protocols: sustained vowels (/a/, /e/, /o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/) at syllable level, and /pataka/ sequences at word level. Source code will be available at https://github.com/jetliqs/clearpd.",
        "subjects": "Computation and Language, Sound, Audio and Speech Processing",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.602571"
    },
    {
        "index": "#86",
        "title": "TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation",
        "link": "/arxiv/2510.03748",
        "arxiv_id": "2510.03748",
        "authors": "Ramtin Kakavand, Ebrahim Ansari",
        "summary": "Large Language Models (LLMs) have consistently demonstrated strong performance in machine translation, especially when guided by high-quality prompts. Few-shot prompting is an effective technique to improve translation quality; however, most existing example selection methods focus solely on query-to-example similarity and do not account for the quality of the examples. In this work, we propose TreePrompt, a novel example selection approach that learns LLM preferences to identify high-quality, contextually relevant examples within a tree-structured framework. To further explore the balance between similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN) and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs - English-Persian (MIZAN) and English-German (WMT19) - show that integrating TreePrompt with AFSP or Random selection leads to improved translation performance.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.603451"
    },
    {
        "index": "#87",
        "title": "MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction",
        "link": "/arxiv/2510.03687",
        "arxiv_id": "2510.03687",
        "authors": "Yue Huang, Yanyuan Chen, Dexuan Xu, Weihua Yue, Huamin Zhang, Meikang Qiu, Yu Huang",
        "summary": "Medical problem solving demands expert knowledge and intricate reasoning. Recent studies of large language models (LLMs) attempt to ease this complexity by introducing external knowledge verification through retrieval-augmented generation or by training on reasoning datasets. However, these approaches suffer from drawbacks such as retrieval overhead and high annotation costs, and they heavily rely on substituted external assistants to reach limited performance in medical field. In this paper, we introduce MedReflect, a generalizable framework designed to inspire LLMs with a physician-like reflective thinking mode. MedReflect generates a single-pass reflection chain that includes initial hypothesis generation, self-questioning, self-answering and decision refinement. This self-verified and self-reflective nature releases large language model's latent capability in medical problem-solving without external retrieval or heavy annotation. We demonstrate that MedReflect enables cost-efficient medical dataset construction: with merely 2,000 randomly sampled training examples and a light fine-tuning, this approach achieves notable absolute accuracy improvements across a series of medical benchmarks while cutting annotation requirements. Our results provide evidence that LLMs can learn to solve specialized medical problems via self-reflection and self-improve, reducing reliance on external supervision and extensive task-specific fine-tuning data.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.604221"
    },
    {
        "index": "#88",
        "title": "Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text",
        "link": "/arxiv/2510.03683",
        "arxiv_id": "2510.03683",
        "authors": "Nisar Hussain, Amna Qasim, Gull Mehak, Muhammad Zain, Momina Hafeez, Grigori Sidorov",
        "summary": "The use of derogatory terms in languages that employ code mixing, such as Roman Urdu, presents challenges for Natural Language Processing systems due to unstated grammar, inconsistent spelling, and a scarcity of labeled data. In this work, we propose a QLoRA based fine tuning framework to improve offensive language detection in Roman Urdu-English text. We translated the Roman Urdu-English code mixed dataset into English using Google Translate to leverage English LLMs, while acknowledging that this translation reduces direct engagement with code mixing features. Our focus is on classification performance using English translated low resource inputs. We fine tuned several transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient adaptation. Models were trained and evaluated on a manually annotated Roman Urdu dataset for offensive vs non offensive content. Of all tested models, the highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral 7B at 89.66, surpassing traditional transformer baselines. These results demonstrate the efficacy of QLoRA in fine tuning high performing models for low resource environments such as code mixed offensive language detection, and confirm the potential of LLMs for this task. This work advances a scalable approach to Roman Urdu moderation and paves the way for future multilingual offensive detection systems based on LLMs.",
        "subjects": "Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.604942"
    },
    {
        "index": "#89",
        "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG",
        "link": "/arxiv/2510.03663",
        "arxiv_id": "2510.03663",
        "authors": "Xiangyu Peng, Cab Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, Chien-Sheng Wu",
        "summary": "Multimodal retrieval-augmented generation (MM-RAG) is a key approach for applying large language models (LLMs) and agents to real-world knowledge bases, yet current evaluations are fragmented, focusing on either text or images in isolation or on simplified multimodal setups that fail to capture document-centric multimodal use cases. In this paper, we introduce UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from 70k real-world PDF pages across eight domains. Our pipeline extracts and links evidence from text, tables, and figures, then generates 1,600 multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries. To ensure reliability, 20% of QA pairs are validated by multiple annotators and expert adjudication. UniDoc-Bench supports apples-to-apples comparison across four paradigms: (1) text-only, (2) image-only, (3) multimodal text-image fusion, and (4) multimodal joint retrieval -- under a unified protocol with standardized candidate pools, prompts, and evaluation metrics. Our experiments show that multimodal text-image fusion RAG systems consistently outperform both unimodal and jointly multimodal embedding-based retrieval, indicating that neither text nor images alone are sufficient and that current multimodal embeddings remain inadequate. Beyond benchmarking, our analysis reveals when and how visual context complements textual evidence, uncovers systematic failure modes, and offers actionable guidance for developing more robust MM-RAG pipelines.",
        "subjects": "Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.605616"
    },
    {
        "index": "#90",
        "title": "Towards Unsupervised Speech Recognition at the Syllable-Level",
        "link": "/arxiv/2510.03639",
        "arxiv_id": "2510.03639",
        "authors": "Liming Wang, Junrui Ni, Kai-Wei Chang, Saurabhchand Bhati, David Harwath, Mark Hasegawa-Johnson, James R. Glass",
        "summary": "Training speech recognizers with unpaired speech and text -- known as unsupervised speech recognition (UASR) -- is a crucial step toward extending ASR to low-resource languages in the long-tail distribution and enabling multimodal learning from non-parallel data. However, existing approaches based on phones often rely on costly resources such as grapheme-to-phoneme converters (G2Ps) and struggle to generalize to languages with ambiguous phoneme boundaries due to training instability. In this paper, we address both challenges by introducing a syllable-level UASR framework based on masked language modeling, which avoids the need for G2P and the instability of GAN-based methods. Our approach achieves up to a 40\\% relative reduction in character error rate (CER) on LibriSpeech and generalizes effectively to Mandarin, a language that has remained particularly difficult for prior methods. Code will be released upon acceptance.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.606359"
    },
    {
        "index": "#91",
        "title": "Can an LLM Induce a Graph? Investigating Memory Drift and Context Length",
        "link": "/arxiv/2510.03611",
        "arxiv_id": "2510.03611",
        "authors": "Raquib Bin Yousuf, Aadyant Khatri, Shengzhe Xu, Mandar Sharma, Naren Ramakrishnan",
        "summary": "Recently proposed evaluation benchmarks aim to characterize the effective context length and the forgetting tendencies of large language models (LLMs). However, these benchmarks often rely on simplistic 'needle in a haystack' retrieval or continuation tasks that may not accurately reflect the performance of these models in information-dense scenarios. Thus, rather than simple next token prediction, we argue for evaluating these models on more complex reasoning tasks that requires them to induce structured relational knowledge from the text - such as graphs from potentially noisy natural language content. While the input text can be viewed as generated in terms of a graph, its structure is not made explicit and connections must be induced from distributed textual cues, separated by long contexts and interspersed with irrelevant information. Our findings reveal that LLMs begin to exhibit memory drift and contextual forgetting at much shorter effective lengths when tasked with this form of relational reasoning, compared to what existing benchmarks suggest. With these findings, we offer recommendations for the optimal use of popular LLMs for complex reasoning tasks. We further show that even models specialized for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in these settings. These results point to significant limitations in the models' ability to abstract structured knowledge from unstructured input and highlight the need for architectural adaptations to improve long-range reasoning.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.607103"
    },
    {
        "index": "#92",
        "title": "Decoupling Task-Solving and Output Formatting in LLM Generation",
        "link": "/arxiv/2510.03595",
        "arxiv_id": "2510.03595",
        "authors": "Haikang Deng, Po-Nien Kung, Nanyun Peng",
        "summary": "Large language models (LLMs) are increasingly adept at following instructions containing task descriptions to solve complex problems, such as mathematical reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow more complex, models often struggle to adhere to all instructions. This difficulty is especially common when instructive prompts intertwine reasoning directives -- specifying what the model should solve -- with rigid formatting requirements that dictate how the solution must be presented. The entanglement creates competing goals for the model, suggesting that more explicit separation of these two aspects could lead to improved performance. To this front, we introduce Deco-G, a decoding framework that explicitly decouples format adherence from task solving. Deco-G handles format compliance with a separate tractable probabilistic model (TPM), while prompts LLMs with only task instructions. At each decoding step, Deco-G combines next token probabilities from the LLM with the TPM calculated format compliance likelihood to form the output probability. To make this approach both practical and scalable for modern instruction-tuned LLMs, we introduce three key innovations: instruction-aware distillation, a flexible trie-building algorithm, and HMM state pruning for computational efficiency. We demonstrate the effectiveness of Deco-G across a wide range of tasks with diverse format requirements, including mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall, our approach yields 1.0% to 6.0% relative gain over regular prompting practice with guaranteed format compliance.",
        "subjects": "Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.607761"
    },
    {
        "index": "#93",
        "title": "LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction",
        "link": "/arxiv/2510.03577",
        "arxiv_id": "2510.03577",
        "authors": "Ikram Belmadani, Parisa Nazari Hashemi, Thomas Sebbag, Benoit Favre, Guillaume Fortier, Solen Quiniou, Emmanuel Morin, Richard Dufour",
        "summary": "This work presents our participation in the EvalLLM 2025 challenge on biomedical Named Entity Recognition (NER) and health event extraction in French (few-shot setting). For NER, we propose three approaches combining large language models (LLMs), annotation guidelines, synthetic data, and post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating automatic selection of 10 examples and a summary of the annotation guidelines into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic corpus and then verified by an LLM in post-processing, and (3) the open LLM LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event extraction uses the same ICL strategy with GPT-4.1, reusing the guideline summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for NER and 15.02% for event extraction, highlighting the importance of well-crafted prompting to maximize performance in very low-resource scenarios.",
        "subjects": "Computation and Language, Information Retrieval",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.613371"
    },
    {
        "index": "#94",
        "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models",
        "link": "/arxiv/2510.03561",
        "arxiv_id": "2510.03561",
        "authors": "Adam Filipek",
        "summary": "The Transformer architecture has become the de facto standard for Large Language Models (LLMs), demonstrating remarkable capabilities in language understanding and generation. However, its application in conversational AI is fundamentally constrained by its stateless nature and the quadratic computational complexity ($O(L^2)$) with respect to sequence length $L$. Current models emulate memory by reprocessing an ever-expanding conversation history with each turn, leading to prohibitive costs and latency in long dialogues. This paper introduces the Reactive Transformer (RxT), a novel architecture designed to overcome these limitations by shifting from a data-driven to an event-driven paradigm. RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system. The architecture features a distinct operational cycle where a generator-decoder produces a response based on the current query and the previous memory state, after which a memory-encoder and a dedicated Memory Attention network asynchronously update the STM with a representation of the complete interaction. This design fundamentally alters the scaling dynamics, reducing the total user-facing cost of a conversation from quadratic ($O(N^2 \\cdot T)$) to linear ($O(N \\cdot T)$) with respect to the number of interactions $N$. By decoupling response generation from memory updates, RxT achieves low latency, enabling truly real-time, stateful, and economically viable long-form conversations. We validated our architecture with a series of proof-of-concept experiments on synthetic data, demonstrating superior performance and constant-time inference latency compared to a baseline stateless model of comparable size.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.613787"
    },
    {
        "index": "#95",
        "title": "CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making",
        "link": "/arxiv/2510.03553",
        "arxiv_id": "2510.03553",
        "authors": "Hasibur Rahman, Hanan Salam",
        "summary": "Although large language models (LLMs) are increasingly implicated in interpersonal and societal decision-making, their ability to navigate explicit conflicts between legitimately different cultural value systems remains largely unexamined. Existing benchmarks predominantly target cultural knowledge (CulturalBench), value prediction (WorldValuesBench), or single-axis bias diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple culturally grounded values directly clash. We address this gap with CCD-Bench, a benchmark that assesses LLM decision-making under cross-cultural value conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains, each paired with ten anonymized response options corresponding to the ten GLOBE cultural clusters. These dilemmas are presented using a stratified Latin square to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe (12.4 percent), while options for Eastern Europe and the Middle East and North Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of rationales reference multiple GLOBE dimensions, this pluralism is superficial: models recombine Future Orientation and Performance Orientation, and rarely ground choices in Assertiveness or Gender Egalitarianism (both under 3 percent). Ordering effects are negligible (Cramer's V less than 0.10), and symmetrized KL divergence shows clustering by developer lineage rather than geography. These patterns suggest that current alignment pipelines promote a consensus-oriented worldview that underserves scenarios demanding power negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts evaluation beyond isolated bias detection toward pluralistic decision making and highlights the need for alignment strategies that substantively engage diverse worldviews.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.614161"
    },
    {
        "index": "#96",
        "title": "What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification",
        "link": "/arxiv/2510.03541",
        "arxiv_id": "2510.03541",
        "authors": "Andrew Halterman, Katherine A. Keith",
        "summary": "Generative large language models (LLMs) are now used extensively for text classification in computational social science (CSS). In this work, focus on the steps before and after LLM prompting -- conceptualization of concepts to be classified and using LLM predictions in downstream statistical inference -- which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can tempt analysts to skip the conceptualization step, creating conceptualization errors that bias downstream estimates. Using simulations, we show that this conceptualization-induced bias cannot be corrected for solely by increasing LLM accuracy or post-hoc bias correction methods. We conclude by reminding CSS analysts that conceptualization is still a first-order concern in the LLM-era and provide concrete advice on how to pursue low-cost, unbiased, low-variance downstream estimates.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.614552"
    },
    {
        "index": "#97",
        "title": "TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering",
        "link": "/arxiv/2510.03536",
        "arxiv_id": "2510.03536",
        "authors": "Zhaohan Meng, Zaiqiao Meng, Siwei Liu, Iadh Ounis",
        "summary": "Large Language Models (LLMs) perform strongly in static and single-turn medical Question Answer (QA) benchmarks, yet such settings diverge from the iterative information gathering process required in practical clinical consultations. The MEDIQ framework addresses this mismatch by recasting the diagnosis as an interactive dialogue between a patient and an expert system, but the reliability of LLMs drops dramatically when forced to reason with dialogue logs, where clinical facts appear in sentences without clear links. To bridge this gap, we introduce TriMediQ, a triplet-structured approach that summarises patient responses into triplets and integrates them into a Knowledge Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet generator that extracts clinically relevant triplets, using prompts designed to ensure factual consistency. In parallel, a trainable projection module, comprising a graph encoder and a projector, captures relational information from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i) the projection module fine-tuning with all LLM weights frozen; and (ii) using the fine-tuned module to guide multi-hop reasoning during inference. We evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up to 10.4\\% improvement in accuracy over five baselines on the iMedQA dataset. These results demonstrate that converting patient responses into structured triplet-based graphs enables more accurate clinical reasoning in multi-turn settings, providing a solution for the deployment of LLM-based medical assistants.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.614985"
    },
    {
        "index": "#98",
        "title": "Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance",
        "link": "/arxiv/2510.03528",
        "arxiv_id": "2510.03528",
        "authors": "Ahmed Alajrami, Xingwei Tan, Nikolaos Aletras",
        "summary": "Instruction-tuning plays a vital role in enhancing the task-solving abilities of large language models (LLMs), improving their usability in generating helpful responses on various tasks. However, previous work has demonstrated that they are sensitive to minor variations in instruction phrasing. In this paper, we explore whether introducing perturbations in instruction-tuning data can enhance LLMs' resistance against noisy instructions. We focus on how instruction-tuning with perturbations, such as removing stop words or shuffling words, affects LLMs' performance on the original and perturbed versions of widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics and potential shifts in model behavior. Surprisingly, our results suggest that instruction-tuning on perturbed instructions can, in some cases, improve downstream performance. These findings highlight the importance of including perturbed instructions in instruction-tuning, which can make LLMs more resilient to noisy user inputs.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.615397"
    },
    {
        "index": "#99",
        "title": "Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs",
        "link": "/arxiv/2510.03527",
        "arxiv_id": "2510.03527",
        "authors": "Sayan Ghosh, Shahzaib Saqib Warraich, Dhruv Tarsadiya, Gregory Yauney, Swabha Swayamdipta",
        "summary": "Language models can be sampled multiple times to access the distribution underlying their responses, but existing methods cannot efficiently synthesize rich epistemic signals across different long-form responses. We introduce Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents shared information, as well as semantic variation in a set of sampled LM responses to the same prompt. We construct ConGrs using a light-weight lexical sequence alignment algorithm from bioinformatics, supplemented by the targeted usage of a secondary LM judge. Further, we design task-dependent decoding methods to synthesize a single, final response from our ConGr data structure. Our experiments show that synthesizing responses from ConGrs improves factual precision on two biography generation tasks by up to 31% over an average response and reduces reliance on LM judges by more than 80% compared to other methods. We also use ConGrs for three refusal-based tasks requiring abstention on unanswerable queries and find that abstention rate is increased by up to 56%. We apply our approach to the MATH and AIME reasoning tasks and find an improvement over self-verification and majority vote baselines by up to 6 points of accuracy. We show that ConGrs provide a flexible method for capturing variation in LM responses and using the epistemic signals provided by response variation to synthesize more effective responses.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.615839"
    },
    {
        "index": "#100",
        "title": "Identifying Financial Risk Information Using RAG with a Contrastive Insight",
        "link": "/arxiv/2510.03521",
        "arxiv_id": "2510.03521",
        "authors": "Ali Elahi",
        "summary": "In specialized domains, humans often compare new problems against similar examples, highlight nuances, and draw conclusions instead of analyzing information in isolation. When applying reasoning in specialized contexts with LLMs on top of a RAG, the pipeline can capture contextually relevant information, but it is not designed to retrieve comparable cases or related problems. While RAG is effective at extracting factual information, its outputs in specialized reasoning tasks often remain generic, reflecting broad facts rather than context-specific insights. In finance, it results in generic risks that are true for the majority of companies. To address this limitation, we propose a peer-aware comparative inference layer on top of RAG. Our contrastive approach outperforms baseline RAG in text generation metrics such as ROUGE and BERTScore in comparison with human-generated equity research and risk.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.616229"
    },
    {
        "index": "#101",
        "title": "TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning",
        "link": "/arxiv/2510.03519",
        "arxiv_id": "2510.03519",
        "authors": "Fangxu Yu, Hongyu Zhao, Tianyi Zhou",
        "summary": "Time series reasoning is crucial to decision-making in diverse domains, including finance, energy usage, traffic, weather, and scientific discovery. While existing time series foundation models (TSFMs) can capture low-level dynamic patterns and provide accurate forecasting, further analysis usually requires additional background knowledge and sophisticated reasoning, which are lacking in most TSFMs but can be achieved through large language models (LLMs). On the other hand, without expensive post-training, LLMs often struggle with the numerical understanding of time series data. Although it is intuitive to integrate the two types of models, developing effective training recipes that align the two modalities for reasoning tasks is still an open challenge. To this end, we propose TS-Reasoner that aligns the latent representations of TSFMs with the textual inputs of LLMs for downstream understanding/reasoning tasks. Specifically, we propose a simple yet effective method to curate diverse, synthetic pairs of time series and textual captions for alignment training. We then develop a two-stage training recipe that applies instruction finetuning after the alignment pretraining. Unlike existing works that train an LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it during training. Extensive experiments on several benchmarks demonstrate that TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision Language Models (VLMs), and Time Series LLMs, but also achieves this with remarkable data efficiency, e.g., using less than half the training data.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.616880"
    },
    {
        "index": "#102",
        "title": "ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection",
        "link": "/arxiv/2510.03502",
        "arxiv_id": "2510.03502",
        "authors": "Ali Khairallah, Arkaitz Zubiaga",
        "summary": "We introduce ALHD, the first large-scale comprehensive Arabic dataset explicitly designed to distinguish between human- and LLM-generated texts. ALHD spans three genres (news, social media, reviews), covering both MSA and dialectal Arabic, and contains over 400K balanced samples generated by three leading LLMs and originated from multiple human sources, which enables studying generalizability in Arabic LLM-genearted text detection. We provide rigorous preprocessing, rich annotations, and standardized balanced splits to support reproducibility. In addition, we present, analyze and discuss benchmark experiments using our new dataset, in turn identifying gaps and proposing future research directions. Benchmarking across traditional classifiers, BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that fine-tuned BERT models achieve competitive performance, outperforming LLM-based models. Results are however not always consistent, as we observe challenges when generalizing across genres; indeed, models struggle to generalize when they need to deal with unseen patterns in cross-genre settings, and these challenges are particularly prominent when dealing with news articles, where LLM-generated texts resemble human texts in style, which opens up avenues for future research. ALHD establishes a foundation for research related to Arabic LLM-detection and mitigating risks of misinformation, academic dishonesty, and cyber threats.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.617565"
    },
    {
        "index": "#103",
        "title": "SEER: The Span-based Emotion Evidence Retrieval Benchmark",
        "link": "/arxiv/2510.03490",
        "arxiv_id": "2510.03490",
        "authors": "Aneesha Sampath, Oya Aran, Emily Mower Provost",
        "summary": "We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to test Large Language Models' (LLMs) ability to identify the specific spans of text that express emotion. Unlike traditional emotion recognition tasks that assign a single label to an entire sentence, SEER targets the underexplored task of emotion evidence detection: pinpointing which exact phrases convey emotion. This span-level approach is crucial for applications like empathetic dialogue and clinical support, which need to know how emotion is expressed, not just what the emotion is. SEER includes two tasks: identifying emotion evidence within a single sentence, and identifying evidence across a short passage of five consecutive sentences. It contains new annotations for both emotion and emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs and find that, while some models approach average human performance on single-sentence inputs, their accuracy degrades in longer passages. Our error analysis reveals key failure modes, including overreliance on emotion keywords and false positives in neutral text.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.664247"
    },
    {
        "index": "#104",
        "title": "Searching for the Most Human-like Emergent Language",
        "link": "/arxiv/2510.03467",
        "arxiv_id": "2510.03467",
        "authors": "Brendon Boldt, David Mortensen",
        "summary": "In this paper, we design a signalling game-based emergent communication environment to generate state-of-the-art emergent languages in terms of similarity to human language. This is done with hyperparameter optimization, using XferBench as the objective function. XferBench quantifies the statistical similarity of emergent language to human language by measuring its suitability for deep transfer learning to human language. Additionally, we demonstrate the predictive power of entropy on the transfer learning performance of emergent language as well as corroborate previous results on the entropy-minimization properties of emergent communication systems. Finally, we report generalizations regarding what hyperparameters produce more realistic emergent languages, that is, ones which transfer better to human language.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.665175"
    },
    {
        "index": "#105",
        "title": "Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video",
        "link": "/arxiv/2510.03458",
        "arxiv_id": "2510.03458",
        "authors": "Mengyao Xu, Wenfei Zhou, Yauhen Babakhin, Gabriel Moreira, Ronay Ak, Radek Osmulski, Bo Liu, Even Oldridge, Benedikt Schifferer",
        "summary": "We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding model developed to handle the increasing complexity of real-world information needs. While Retrieval-Augmented Generation (RAG) has significantly advanced language models by incorporating external knowledge, existing text-based retrievers rely on clean, structured input and struggle with the visually and semantically rich content found in real-world documents such as PDFs, slides, or videos. Recent work such as ColPali has shown that preserving document layout using image-based representations can improve retrieval quality. Building on this, and inspired by the capabilities of recent multimodal models such as Qwen2.5-Omni, we extend retrieval beyond text and images to also support audio and video modalities. Omni-Embed-Nemotron enables both cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio) retrieval using a single model. We describe the architecture, training setup, and evaluation results of Omni-Embed-Nemotron, and demonstrate its effectiveness in text, image, and video retrieval.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.666265"
    },
    {
        "index": "#106",
        "title": "Morpheme Induction for Emergent Language",
        "link": "/arxiv/2510.03439",
        "arxiv_id": "2510.03439",
        "authors": "Brendon Boldt, David Mortensen",
        "summary": "We introduce CSAR, an algorithm for inducing morphemes from emergent language corpora of parallel utterances and meanings. It is a greedy algorithm that (1) weights morphemes based on mutual information between forms and meanings, (2) selects the highest-weighted pair, (3) removes it from the corpus, and (4) repeats the process to induce further morphemes (i.e., Count, Select, Ablate, Repeat). The effectiveness of CSAR is first validated on procedurally generated datasets and compared against baselines for related tasks. Second, we validate CSAR's performance on human language data to show that the algorithm makes reasonable predictions in adjacent domains. Finally, we analyze a handful of emergent languages, quantifying linguistic characteristics like degree of synonymy and polysemy.",
        "subjects": "Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.667151"
    },
    {
        "index": "#107",
        "title": "Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks",
        "link": "/arxiv/2510.03384",
        "arxiv_id": "2510.03384",
        "authors": "Arjun Arunasalam, Madison Pickering, Z. Berkay Celik, Blase Ur",
        "summary": "Large language models (LLMs) can underpin AI assistants that help users with everyday tasks, such as by making recommendations or performing basic computation. Despite AI assistants' promise, little is known about the implicit values these assistants display while completing subjective everyday tasks. Humans may consider values like environmentalism, charity, and diversity. To what extent do LLMs exhibit these values in completing everyday tasks? How do they compare with humans? We answer these questions by auditing how six popular LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human crowdworkers from the US. We find LLMs often do not align with humans, nor with other LLMs, in the implicit values exhibited.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.668188"
    },
    {
        "index": "#108",
        "title": "Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision",
        "link": "/arxiv/2510.03323",
        "arxiv_id": "2510.03323",
        "authors": "Ge Chang, Jinbo Su, Jiacheng Liu, Pengfei Yang, Yuhao Shang, Huiwen Zheng, Hongli Ma, Yan Liang, Yuanchun Li, Yunxin Liu",
        "summary": "A significant portion of real-world data is inherently represented as textual graphs, and integrating these graphs into large language models (LLMs) is promising to enable complex graph-based question answering. However, a key challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e., how to retrieve relevant content from large graphs that is sufficiently informative while remaining compact for the LLM context. Existing retrievers suffer from poor performance since they either rely on shallow embedding similarity or employ interactive retrieving policies that demand excessive data labeling and training cost. To address these issues, we present Graph-$S^3$, an agentic textual graph reasoning framework that employs an LLM-based retriever trained with synthetic stepwise supervision. Instead of rewarding the agent based on the final answers, which may lead to sparse and unstable training signals, we propose to closely evaluate each step of the retriever based on offline-extracted golden subgraphs. Our main techniques include a data synthesis pipeline to extract the golden subgraphs for reward generation and a two-stage training scheme to learn the interactive graph exploration policy based on the synthesized rewards. Based on extensive experiments on three common datasets in comparison with seven strong baselines, our approach achieves an average improvement of 8.1\\% in accuracy and 9.7\\% in F$_1$ score. The advantage is even higher in more complicated multi-hop reasoning tasks. Our code will be open-sourced.",
        "subjects": "Computation and Language",
        "date": "2025-10-01",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.674108"
    },
    {
        "index": "#109",
        "title": "Decomposing Attention To Find Context-Sensitive Neurons",
        "link": "/arxiv/2510.03315",
        "arxiv_id": "2510.03315",
        "authors": "Alex Gibson",
        "summary": "We study transformer language models, analyzing attention heads whose attention patterns are spread out, and whose attention scores depend weakly on content. We argue that the softmax denominators of these heads are stable when the underlying token distribution is fixed. By sampling softmax denominators from a \"calibration text\", we can combine together the outputs of multiple such stable heads in the first layer of GPT2-Small, approximating their combined output by a linear summary of the surrounding text. This approximation enables a procedure where from the weights alone - and a single calibration text - we can uncover hundreds of first layer neurons that respond to high-level contextual properties of the surrounding text, including neurons that didn't activate on the calibration text.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-01",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.674774"
    },
    {
        "index": "#111",
        "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models",
        "link": "/arxiv/2510.05095",
        "arxiv_id": "2510.05095",
        "authors": "Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia",
        "summary": "Large reasoning models (LRMs) generate intermediate reasoning traces before producing final answers, yielding strong gains on multi-step and mathematical tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for model deployment, remains underexplored. The statistically correct objective for preference alignment requires marginalizing over reasoning traces, but this computation is intractable in practice. A common workaround optimizes a single sampled trajectory, which introduces substantial gradient variance from stochastic trace sampling. To address this challenge, we frame preference optimization for LRMs through the lens of the bias--variance trade-off and propose Bias--Variance Optimized Preference Optimization (BVPO), a simple, drop-in method that mixes two gradient estimators: a high-variance trace-based estimator and a low-variance empty-trace estimator obtained by disabling reasoning trace generation. Our theory shows that BVPO strictly reduces trace-induced variance for any nontrivial mixture, provides a closed-form choice of the mixing weight that minimizes mean-squared error relative to the true marginal gradient, and under standard smoothness and step-size conditions, tightens classical convergence bounds for stochastic gradient descent. Empirically, BVPO improves alignment over the best baseline by up to 7.8 points on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on general conversational data, BVPO also boosts reasoning performance for base models by up to 4.0 points on the average of six math reasoning benchmarks. These results identify variance from trace sampling as a key bottleneck and demonstrate that directly optimizing the bias--variance trade-off yields more stable training and stronger overall performance.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.676327"
    },
    {
        "index": "#112",
        "title": "Learning to Interpret Weight Differences in Language Models",
        "link": "/arxiv/2510.05092",
        "arxiv_id": "2510.05092",
        "authors": "Avichal Goel, Yoon Kim, Nir Shavit, Tony T. Wang",
        "summary": "Finetuning (pretrained) language models is a standard approach for updating their internal parametric knowledge and specializing them to new tasks and domains. However, the corresponding model weight changes (\"weight diffs\") are not generally interpretable. While inspecting the finetuning dataset can give a sense of how the model might have changed, these datasets are often not publicly available or are too large to work with directly. Towards the goal of comprehensively understanding weight diffs in natural language, we introduce Diff Interpretation Tuning (DIT), a method that trains models to describe their own finetuning-induced modifications. Our approach uses synthetic, labeled weight diffs to train a DIT adapter, which can be applied to a compatible finetuned model to make it describe how it has changed. We demonstrate in two proof-of-concept settings (reporting hidden behaviors and summarizing finetuned knowledge) that our method enables models to describe their finetuning-induced modifications using accurate natural language descriptions.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.677049"
    },
    {
        "index": "#113",
        "title": "Proactive defense against LLM Jailbreak",
        "link": "/arxiv/2510.05052",
        "arxiv_id": "2510.05052",
        "authors": "Weiliang Zhao, Jinjun Peng, Daniel Ben-Levi, Zhou Yu, Junfeng Yang",
        "summary": "The proliferation of powerful large language models (LLMs) has necessitated robust safety alignment, yet these models remain vulnerable to evolving adversarial attacks, including multi-turn jailbreaks that iteratively search for successful queries. Current defenses, primarily reactive and static, often fail to counter these search-based attacks. In this paper, we introduce ProAct, a novel proactive defense framework designed to disrupt and mislead autonomous jailbreaking processes. Our core idea is to intentionally provide adversaries with \"spurious responses\" that appear to be results of successful jailbreak attacks but contain no actual harmful content. These misleading responses provide false signals to the attacker's internal optimization loop, causing the adversarial search to terminate prematurely and effectively jailbreaking the jailbreak. By conducting extensive experiments across state-of-the-art LLMs, jailbreaking frameworks, and safety benchmarks, our method consistently and significantly reduces attack success rates by up to 92\\%. When combined with other defense frameworks, it further reduces the success rate of the latest attack strategies to 0\\%. ProAct represents an orthogonal defense strategy that can serve as an additional guardrail to enhance LLM safety against the most effective jailbreaking attacks.",
        "subjects": "Cryptography and Security, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.677772"
    },
    {
        "index": "#114",
        "title": "Large Language Models Achieve Gold Medal Performance at International Astronomy & Astrophysics Olympiad",
        "link": "/arxiv/2510.05016",
        "arxiv_id": "2510.05016",
        "authors": "Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff, Yingbin Liang, Yuan-Sen Ting, Huan Sun",
        "summary": "While task-specific demonstrations show early success in applying large language models (LLMs) to automate some astronomical research tasks, they only provide incomplete views of all necessary capabilities in solving astronomy problems, calling for more thorough understanding of LLMs' strengths and limitations. So far, existing benchmarks and evaluations focus on simple question-answering that primarily tests astronomical knowledge and fails to evaluate the complex reasoning required for real-world research in the discipline. Here, we address this gap by systematically benchmarking five state-of-the-art LLMs on the International Olympiad on Astronomy and Astrophysics (IOAA) exams, which are designed to examine deep conceptual understanding, multi-step derivations, and multimodal analysis. With average scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing models) not only achieve gold medal level performance but also rank in the top two among ~200-300 participants in all four IOAA theory exams evaluated (2022-2025). In comparison, results on the data analysis exams show more divergence. GPT-5 still excels in the exams with an 88.5% average score, ranking top 10 among the participants in the four most recent IOAAs, while other models' performances drop to 48-76%. Furthermore, our in-depth error analysis underscores conceptual reasoning, geometric reasoning, and spatial visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence, although LLMs approach peak human performance in theory exams, critical gaps must be addressed before they can serve as autonomous research agents in astronomy.",
        "subjects": "Instrumentation and Methods for Astrophysics, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.678548"
    },
    {
        "index": "#115",
        "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training",
        "link": "/arxiv/2510.04996",
        "arxiv_id": "2510.04996",
        "authors": "Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, Tong Zhang",
        "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.715344"
    },
    {
        "index": "#116",
        "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game",
        "link": "/arxiv/2510.04980",
        "arxiv_id": "2510.04980",
        "authors": "Fangzhou Liang, Tianshi Zheng, Chunkit Chan, Yauwai Yim, Yangqiu Song",
        "summary": "Effective multi-agent collaboration requires agents to infer the rationale behind others' actions, a capability rooted in Theory-of-Mind (ToM). While recent Large Language Models (LLMs) excel at logical inference, their ability to infer rationale in dynamic, collaborative settings remains under-explored. This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework features an automated evaluation system that measures both game performance and ToM proficiency. Across a range of models, we find a significant positive correlation between ToM and in-game success. Notably, first-order ToM (interpreting others' intent) correlates more strongly with performance than second-order ToM (predicting others' interpretations). These findings highlight that for effective AI collaboration, the ability to accurately interpret a partner's rationale is more critical than higher-order reasoning. We conclude that prioritizing first-order ToM is a promising direction for enhancing the collaborative capabilities of future models.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.716465"
    },
    {
        "index": "#117",
        "title": "On Structured State-Space Duality",
        "link": "/arxiv/2510.04944",
        "arxiv_id": "2510.04944",
        "authors": "Jerry Yao-Chieh Hu, Xiwen Zhang, Weimin Wu, Han Liu",
        "summary": "Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence between a simple Structured State-Space Model (SSM) and a masked attention mechanism. In particular, a state-space model with a scalar-times-identity state matrix is equivalent to a masked self-attention with a $1$-semiseparable causal mask. Consequently, the same sequence transformation (model) has two algorithmic realizations: as a linear-time $O(T)$ recurrence or as a quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize this duality: (i) we extend SSD from the scalar-identity case to general diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs match the scalar case's training complexity lower bounds while supporting richer dynamics; (iii) we establish a necessary and sufficient condition under which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we show that such duality fails to extend to standard softmax attention due to rank explosion. Together, these results tighten bridge between recurrent SSMs and Transformers, and widen the design space for expressive yet efficient sequence models.",
        "subjects": "Machine Learning, Computation and Language, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.717544"
    },
    {
        "index": "#118",
        "title": "ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures",
        "link": "/arxiv/2510.04938",
        "arxiv_id": "2510.04938",
        "authors": "Shiwen Qin, Alexander Auras, Shay B. Cohen, Elliot J. Crowley, Michael Moeller, Linus Ericsson, Jovita Lukasik",
        "summary": "Neural architecture search (NAS) automates the design process of high-performing architectures, but remains bottlenecked by expensive performance evaluation. Most existing studies that achieve faster evaluation are mostly tied to cell-based search spaces and graph encodings tailored to those individual search spaces, limiting their flexibility and scalability when applied to more expressive search spaces. In this work, we aim to close the gap of individual search space restrictions and search space dependent network representations. We present ONNX-Bench, a benchmark consisting of a collection of neural networks in a unified format based on ONNX files. ONNX-Bench includes all open-source NAS-bench-based neural networks, resulting in a total size of more than 600k {architecture, accuracy} pairs. This benchmark allows creating a shared neural network representation, ONNX-Net, able to represent any neural architecture using natural language descriptions acting as an input to a performance predictor. This text-based encoding can accommodate arbitrary layer types, operation parameters, and heterogeneous topologies, enabling a single surrogate to generalise across all neural architectures rather than being confined to cell-based search spaces. Experiments show strong zero-shot performance across disparate search spaces using only a small amount of pretraining samples, enabling the unprecedented ability to evaluate any neural network architecture instantly.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.718698"
    },
    {
        "index": "#119",
        "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning",
        "link": "/arxiv/2510.04935",
        "arxiv_id": "2510.04935",
        "authors": "Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, Fei Huang",
        "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in simple tasks, where the models excessively utilize System 2-type, deliberate reasoning, leading to inefficient token generation. Furthermore, these models face challenges in adapting their reasoning capabilities to rapidly changing environments due to the static nature of their pretraining data. To address these issues, advancing Large Language Models (LLMs) for complex reasoning tasks requires innovative approaches that bridge intuitive and deliberate cognitive processes, akin to human cognition's dual-system dynamic. This paper introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless integration of System 1's fast, intuitive thinking with System 2's deliberate reasoning within LLMs. MARS strategically integrates multiple external tools, such as Google Search, Google Scholar, and Python Interpreter, to access up-to-date information and execute complex computations, while creating a specialized division of labor where System 1 efficiently processes and summarizes high-volume external information, providing distilled insights that expand System 2's reasoning context without overwhelming its capacity. Furthermore, we propose a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to simultaneously optimize both systems with multi-turn tool interactions, bin-packing optimization, and sample balancing strategies that enhance collaborative efficiency. Extensive experiments demonstrate MARS achieves substantial improvements of 3.86% on the challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.834331"
    },
    {
        "index": "#120",
        "title": "Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches",
        "link": "/arxiv/2510.04905",
        "arxiv_id": "2510.04905",
        "authors": "Yicheng Tao, Yao Qin, Yepang Liu",
        "summary": "Recent advancements in large language models (LLMs) have substantially improved automated code generation. While function-level and file-level generation have achieved promising results, real-world software development typically requires reasoning across entire repositories. This gives rise to the challenging task of Repository-Level Code Generation (RLCG), where models must capture long-range dependencies, ensure global semantic consistency, and generate coherent code spanning multiple files or modules. To address these challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm that integrates external retrieval mechanisms with LLMs, enhancing context-awareness and scalability. In this survey, we provide a comprehensive review of research on Retrieval-Augmented Code Generation (RACG), with an emphasis on repository-level approaches. We categorize existing work along several dimensions, including generation strategies, retrieval modalities, model architectures, training paradigms, and evaluation protocols. Furthermore, we summarize widely used datasets and benchmarks, analyze current limitations, and outline key challenges and opportunities for future research. Our goal is to establish a unified analytical framework for understanding this rapidly evolving field and to inspire continued progress in AI-powered software engineering.",
        "subjects": "Software Engineering, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.835204"
    },
    {
        "index": "#121",
        "title": "Visual Representations inside the Language Model",
        "link": "/arxiv/2510.04819",
        "arxiv_id": "2510.04819",
        "authors": "Benlin Liu, Amita Kamath, Madeleine Grunde-McLaughlin, Winson Han, Ranjay Krishna",
        "summary": "Despite interpretability work analyzing VIT encoders and transformer activations, we don't yet understand why Multimodal Language Models (MLMs) struggle on perception-heavy tasks. We offer an under-studied perspective by examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the flow of visual information through the language model, finding that image value tokens encode sufficient information to perform several perception-heavy tasks zero-shot: segmentation, semantic correspondence, temporal correspondence, and referring expression detection. We find that while the language model does augment the visual information received from the projection of input visual encodings-which we reveal correlates with overall MLM perception capability-it contains less visual information on several tasks than the equivalent visual encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that the visual information corresponding to input-agnostic image key tokens in later layers of language models contains artifacts which reduce perception capability of the overall MLM. Next, we discuss controlling visual information in the language model, showing that adding a text prefix to the image input improves perception capabilities of visual representations. Finally, we reveal that if language models were able to better control their visual information, their perception would significantly improve; e.g., in 33.3% of Art Style questions in the BLINK benchmark, perception information present in the language model is not surfaced to the output! Our findings reveal insights into the role of key-value tokens in multimodal systems, paving the way for deeper mechanistic interpretability of MLMs and suggesting new directions for training their visual encoder and language model components.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.836141"
    },
    {
        "index": "#122",
        "title": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba",
        "link": "/arxiv/2510.04738",
        "arxiv_id": "2510.04738",
        "authors": "Baher Mohammad, Magauiya Zhussip, Stamatios Lefkimmiatis",
        "summary": "We introduce MAVE (Mamba with Cross-Attention for Voice Editing and Synthesis), a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, while not being explicitly trained on the latter task, outperforming leading autoregressive and diffusion models on diverse, real-world audio. By integrating Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment, MAVE enables context-aware voice editing with exceptional naturalness and speaker consistency. In pairwise human evaluations on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2% of listeners rated MAVE - edited speech as perceptually equal to the original, while 24.8% prefered the original and 18.0% MAVE - demonstrating that in the majority of cases edits are indistinguishable from the source. MAVE compares favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE exceeds VoiceCraft in both speaker similarity and naturalness, without requiring multiple inference runs or post-processing. Remarkably, these quality gains come with a significantly lower memory cost and approximately the same latency: MAVE requires ~6x less memory than VoiceCraft during inference on utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch size 1). Our results demonstrate that MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention.",
        "subjects": "Sound, Artificial Intelligence, Computation and Language, Machine Learning, Audio and Speech Processing",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.852830"
    },
    {
        "index": "#123",
        "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs",
        "link": "/arxiv/2510.04721",
        "arxiv_id": "2510.04721",
        "authors": "Ivo Petrov, Jasper Dekoninck, Martin Vechev",
        "summary": "Large language models (LLMs) have recently shown strong performance on mathematical benchmarks. At the same time, they are prone to hallucination and sycophancy, often providing convincing but flawed proofs for incorrect mathematical statements provided by users. This significantly limits the applicability of LLMs in theorem proving, as verification of these flawed proofs must be done manually by expert mathematicians. However, existing benchmarks that measure sycophancy in mathematics are limited: they focus solely on final-answer problems, rely on very simple and often contaminated datasets, and construct benchmark samples using synthetic modifications that create ill-posed questions rather than well-posed questions that are demonstrably false. To address these issues, we introduce BrokenMath, the first benchmark for evaluating sycophantic behavior in LLMs within the context of natural language theorem proving. BrokenMath is built from advanced 2025 competition problems, which are perturbed with an LLM to produce false statements and subsequently refined through expert review. Using an LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems and find that sycophancy is widespread, with the best model, GPT-5, producing sycophantic answers 29% of the time. We further investigate several mitigation strategies, including test-time interventions and supervised fine-tuning on curated sycophantic examples. These approaches substantially reduce, but do not eliminate, sycophantic behavior.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.853804"
    },
    {
        "index": "#124",
        "title": "AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials",
        "link": "/arxiv/2510.04704",
        "arxiv_id": "2510.04704",
        "authors": "Taoyuze Lv, Alexander Chen, Fengyu Xie, Chu Wu, Jeffrey Meng, Dongzhan Zhou, Bram Hoex, Zhicheng Zhong, Tong Xie",
        "summary": "Large Language Models (LLMs) excel at textual reasoning and are beginning to develop spatial understanding, prompting the question of whether these abilities can be combined for complex, domain-specific tasks. This question is essential in fields like materials science, where deep understanding of 3D atomic structures is fundamental. While initial studies have successfully applied LLMs to tasks involving pure crystal generation or coordinate understandings, a standardized benchmark to systematically evaluate their core reasoning abilities across diverse atomic structures has been notably absent. To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on tasks based in Crystallographic Information Files (CIFs), a standard structure representation format. These tasks, including structural editing, CIF perception, and property-guided modeling, reveal a critical limitation: current models, despite establishing promising baselines, consistently fail in structural understanding and spatial reasoning. Our experiments show that these models make frequent errors on structure modification tasks, and even in the basic CIF format understandings, potentially leading to cumulative errors in subsequent analysis and materials insights. By defining these standardized tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale modeling, crucial for accelerating materials research and automating scientific workflows.",
        "subjects": "Materials Science, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.854938"
    },
    {
        "index": "#125",
        "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
        "link": "/arxiv/2510.04618",
        "arxiv_id": "2510.04618",
        "authors": "Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, Kunle Olukotun",
        "summary": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.856212"
    },
    {
        "index": "#126",
        "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning",
        "link": "/arxiv/2510.04573",
        "arxiv_id": "2510.04573",
        "authors": "Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamaki, Navdeep Jaitly, Yi-An Ma, Lianhui Qin",
        "summary": "Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may limit the ability to revisit and refine earlier tokens in a holistic manner, which can also lead to inefficient exploration for diverse solutions. In this paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning framework that unifies the expressiveness of continuous latent representation with the iterative refinement capabilities of latent diffusion models for an existing LLM. We first construct a structured latent reasoning space using a Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of thought tokens, preserving semantic information and interpretability while offering compact but expressive representations. Subsequently, we utilize a latent diffusion model that learns to denoise a block of latent thought tokens with a blockwise bidirectional attention mask, enabling longer horizon and iterative refinement with adaptive test-time compute. This design allows efficient parallel generation of diverse reasoning trajectories, allowing the model to plan and revise the reasoning process holistically. We conduct evaluations on a suite of mathematical reasoning and planning benchmarks. Empirical results show that LaDiR consistently improves accuracy, diversity, and interpretability over existing autoregressive, diffusion-based, and latent reasoning methods, revealing a new paradigm for text reasoning with latent diffusion.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.862517"
    },
    {
        "index": "#127",
        "title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models",
        "link": "/arxiv/2510.04532",
        "arxiv_id": "2510.04532",
        "authors": "Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo",
        "summary": "Vision-Language Model (VLM) driving agents promise explainable end-to-end autonomy by first producing natural-language reasoning and then predicting trajectory planning. However, whether planning is causally driven by this reasoning remains a critical but unverified assumption. To investigate this, we build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan. Our data generation process converts sensors and annotations into structured inputs and, crucially, separates priors from to-be-reasoned signals, enabling clean information ablations. Using DriveMind, we train representative VLM agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately, indicate a consistent causal disconnect in reasoning-planning: removing ego/navigation priors causes large drops in planning scores, whereas removing CoT produces only minor changes. Attention analysis further shows that planning primarily focuses on priors rather than the CoT. Based on this evidence, we propose the Reasoning-Planning Decoupling Hypothesis, positing that the training-yielded reasoning is an ancillary byproduct rather than a causal mediator. To enable efficient diagnosis, we also introduce a novel, training-free probe that measures an agent's reliance on priors by evaluating its planning robustness against minor input perturbations. In summary, we provide the community with a new dataset and a diagnostic tool to evaluate the causal fidelity of future models.",
        "subjects": "Artificial Intelligence, Computation and Language, Robotics",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.863464"
    },
    {
        "index": "#128",
        "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering",
        "link": "/arxiv/2510.04514",
        "arxiv_id": "2510.04514",
        "authors": "Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Sumitra Ganesh, Manuela Veloso",
        "summary": "Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.",
        "subjects": "Artificial Intelligence, Computational Engineering, Finance, and Science, Computation and Language, Computer Vision and Pattern Recognition, Methodology",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.864243"
    },
    {
        "index": "#129",
        "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs",
        "link": "/arxiv/2510.04503",
        "arxiv_id": "2510.04503",
        "authors": "Shuai Zhao, Xinyi Wu, Shiqian Zhao, Xiaobao Wu, Zhongliang Guo, Yanhao Jia, Anh Tuan Luu",
        "summary": "During fine-tuning, large language models (LLMs) are increasingly vulnerable to data-poisoning backdoor attacks, which compromise their reliability and trustworthiness. However, existing defense strategies suffer from limited generalization: they only work on specific attack types or task settings. In this study, we propose Poison-to-Poison (P2P), a general and effective backdoor defense algorithm. P2P injects benign triggers with safe alternative labels into a subset of training samples and fine-tunes the model on this re-poisoned dataset by leveraging prompt-based learning. This enforces the model to associate trigger-induced representations with safe outputs, thereby overriding the effects of original malicious triggers. Thanks to this robust and generalizable trigger-based fine-tuning, P2P is effective across task settings and attack types. Theoretically and empirically, we show that P2P can neutralize malicious backdoors while preserving task performance. We conduct extensive experiments on classification, mathematical reasoning, and summary generation tasks, involving multiple state-of-the-art LLMs. The results demonstrate that our P2P algorithm significantly reduces the attack success rate compared with baseline models. We hope that the P2P can serve as a guideline for defending against backdoor attacks and foster the development of a secure and trustworthy LLM community.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.864992"
    },
    {
        "index": "#130",
        "title": "Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents",
        "link": "/arxiv/2510.04491",
        "arxiv_id": "2510.04491",
        "authors": "Muyu He, Anand Kumar, Tsach Mackey, Meghana Rajeev, James Zou, Nazneen Rajani",
        "summary": "Despite rapid progress in building conversational AI agents, robustness is still largely untested. Small shifts in user behavior, such as being more impatient, incoherent, or skeptical, can cause sharp drops in agent performance, revealing how brittle current AI agents are. Today's benchmarks fail to capture this fragility: agents may perform well under standard evaluations but degrade spectacularly in more realistic and varied settings. We address this robustness testing gap by introducing TraitBasis, a lightweight, model-agnostic method for systematically stress testing AI agents. TraitBasis learns directions in activation space corresponding to steerable user traits (e.g., impatience or incoherence), which can be controlled, scaled, composed, and applied at inference time without any fine-tuning or extra data. Using TraitBasis, we extend $\\tau$-Bench to $\\tau$-Trait, where user behaviors are altered via controlled trait vectors. We observe on average a 2%-30% performance degradation on $\\tau$-Trait across frontier models, highlighting the lack of robustness of current AI agents to variations in user behavior. Together, these results highlight both the critical role of robustness testing and the promise of TraitBasis as a simple, data-efficient, and compositional tool. By powering simulation-driven stress tests and training loops, TraitBasis opens the door to building AI agents that remain reliable in the unpredictable dynamics of real-world human interactions. We have open-sourced $\\tau$-Trai across four domains: airline, retail, telecom, and telehealth, so the community can systematically QA their agents under realistic, behaviorally diverse intents and trait scenarios: https://github.com/collinear-ai/tau-trait.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.865710"
    },
    {
        "index": "#131",
        "title": "MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models",
        "link": "/arxiv/2510.04477",
        "arxiv_id": "2510.04477",
        "authors": "Soo Yong Kim, Suin Cho, Vincent-Daniel Yun, Gyeongyeon Hwang",
        "summary": "Bridging clinical diagnostic reasoning with AI remains a central challenge in medical imaging. We introduce MedCLM, an automated pipeline that converts detection datasets into large-scale medical visual question answering (VQA) data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ segmentation and structured rationales. These contextual signals enable medical vision-language models to generate question-answer pairs with step-by-step reasoning. To utilize this data effectively, we propose an Integrated CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes for visual grounding, a Medium stage that encourages implicit localization, and a Hard stage for weakly supervised reasoning. Experimental results demonstrate that MedCLM attains state-of-the-art performance on several medical VQA benchmarks, providing a scalable framework for developing clinically aligned medical vision-language models.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.866423"
    },
    {
        "index": "#132",
        "title": "Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions",
        "link": "/arxiv/2510.04417",
        "arxiv_id": "2510.04417",
        "authors": "Wenyuan Zhao, Adithya Balachandran, Chao Tian, Paul Pu Liang",
        "summary": "The study of multimodality has garnered significant interest in fields where the analysis of interactions among multiple information sources can enhance predictive modeling, data fusion, and interpretability. Partial information decomposition (PID) has emerged as a useful information-theoretic framework to quantify the degree to which individual modalities independently, redundantly, or synergistically convey information about a target variable. However, existing PID methods depend on optimizing over a joint distribution constrained by estimated pairwise probability distributions, which are costly and inaccurate for continuous and high-dimensional modalities. Our first key insight is that the problem can be solved efficiently when the pairwise distributions are multivariate Gaussians, and we refer to this problem as Gaussian PID (GPID). We propose a new gradient-based algorithm that substantially improves the computational efficiency of GPID based on an alternative formulation of the underlying optimization problem. To generalize the applicability to non-Gaussian data, we learn information-preserving encoders to transform random variables of arbitrary input distributions into pairwise Gaussian random variables. Along the way, we resolved an open problem regarding the optimality of joint Gaussian solutions for GPID. Empirical validation in diverse synthetic examples demonstrates that our proposed method provides more accurate and efficient PID estimates than existing baselines. We further evaluate a series of large-scale multimodal benchmarks to show its utility in real-world applications of quantifying PID in multimodal datasets and selecting high-performing models.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Information Theory",
        "date": "2025-10-06",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.867157"
    },
    {
        "index": "#133",
        "title": "Internal World Models as Imagination Networks in Cognitive Agents",
        "link": "/arxiv/2510.04391",
        "arxiv_id": "2510.04391",
        "authors": "Saurabh Ranjan, Brian Odegaard",
        "summary": "What is the computational objective of imagination? While classical interpretations suggest imagination is useful for maximizing rewards, recent findings challenge this view. In this study, we propose that imagination serves to access an internal world model (IWM) and use psychological network analysis to explore IWMs in humans and large language models (LLMs). Specifically, we assessed imagination vividness ratings using two questionnaires and constructed imagination networks from these reports. Imagination networks from human groups showed correlations between different centrality measures, including expected influence, strength, and closeness. However, imagination networks from LLMs showed a lack of clustering and lower correlations between centrality measures under different prompts and conversational memory conditions. Together, these results indicate a lack of similarity between IWMs in human and LLM agents. Overall, our study offers a novel method for comparing internally-generated representations in humans and AI, providing insights for developing human-like imagination in artificial intelligence.",
        "subjects": "Artificial Intelligence, Computation and Language, Social and Information Networks, Neurons and Cognition",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.872952"
    },
    {
        "index": "#134",
        "title": "MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator",
        "link": "/arxiv/2510.04390",
        "arxiv_id": "2510.04390",
        "authors": "Xuehai He, Shijie Zhou, Thivyanth Venkateswaran, Kaizhi Zheng, Ziyu Wan, Achuta Kadambi, Xin Eric Wang",
        "summary": "World models that support controllable and editable spatiotemporal environments are valuable for robotics, enabling scalable training data, repro ducible evaluation, and flexible task design. While recent text-to-video models generate realistic dynam ics, they are constrained to 2D views and offer limited interaction. We introduce MorphoSim, a language guided framework that generates 4D scenes with multi-view consistency and object-level controls. From natural language instructions, MorphoSim produces dynamic environments where objects can be directed, recolored, or removed, and scenes can be observed from arbitrary viewpoints. The framework integrates trajectory-guided generation with feature field dis tillation, allowing edits to be applied interactively without full re-generation. Experiments show that Mor phoSim maintains high scene fidelity while enabling controllability and editability. The code is available at https://github.com/eric-ai-lab/Morph4D.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.873702"
    },
    {
        "index": "#135",
        "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models",
        "link": "/arxiv/2510.04363",
        "arxiv_id": "2510.04363",
        "authors": "Hyunjun Kim, Sejong Kim",
        "summary": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs can synthesize reusable browser automation programs from natural language goals by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like, Facebook-like, Discord-like, and Threads-like, covering 681 tasks across interaction complexity and targeting difficulty. Our end-to-end protocol validates generated code via static checks, sandboxed execution, and outcome verification including DOM assertions and database snapshots, and includes a safety suite for scraping, spam/abuse, and credential/privacy prompts. Across 2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8 percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent, and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at 91.7 percent but fail on complex workflows at 0.0 percent, and none meet production-quality coding practices despite functional completion. We release our complete benchmark pipeline, evaluation framework, and experimental results to enable reproducible assessment of macro synthesis for web automation.",
        "subjects": "Software Engineering, Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.874350"
    },
    {
        "index": "#136",
        "title": "Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention",
        "link": "/arxiv/2510.04304",
        "arxiv_id": "2510.04304",
        "authors": "Harshil Vejendla",
        "summary": "We introduce Wave-PDE Nets, a neural architecture whose elementary operation is a differentiable simulation of the second-order wave equation. Each layer propagates its hidden state as a continuous field through a medium with trainable spatial velocity c(x) and damping {\\gamma}(x). A symplectic spectral solver based on FFTs realises this propagation in O(nlog n) time. This oscillatory, global mechanism provides a powerful alternative to attention and first-order state-space models. We prove that a single Wave-PDE layer is a universal approximator. On language and vision benchmarks, Wave-PDE Nets match or exceed Transformer performance while demonstrating superior practical efficiency, reducing wall-clock time by up to 30% and peak memory by 25%. Ablation studies confirm the critical role of symplectic integration and a spectral Laplacian for stability and performance. Visualizations of the learned physical parameters reveal that the model learns intuitive strategies for information propagation. These results position Wave-PDE Nets as a computationally efficient and robust architecture with a strong physical inductive bias.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.874929"
    },
    {
        "index": "#137",
        "title": "Don't Pass$\\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation",
        "link": "/arxiv/2510.04265",
        "arxiv_id": "2510.04265",
        "authors": "Mohsen Hariri, Amirhossein Samandar, Michael Hinczewski, Vipin Chaudhary",
        "summary": "Pass$@k$ is widely used to report performance for LLM reasoning, but it often yields unstable, misleading rankings, especially when the number of trials (samples) is limited and compute is constrained. We present a principled Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over $N$ trials (avg$@N$) with posterior estimates of a model's underlying success probability and credible intervals, yielding stable rankings and a transparent decision rule for differences. Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty of any weighted rubric and enabling the use of prior evidence when appropriate. Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$), explaining its empirical robustness while adding principled uncertainty. Empirically, in simulations with known ground-truth success rates and on AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster convergence and greater rank stability than Pass$@k$ and recent variants, enabling reliable comparisons at far smaller sample counts. The framework clarifies when observed gaps are statistically meaningful (non-overlapping credible intervals) versus noise, and it naturally extends to graded, rubric-based evaluations. Together, these results recommend replacing Pass$@k$ for LLM evaluation and ranking with a posterior-based, compute-efficient protocol that unifies binary and non-binary evaluation while making uncertainty explicit. Code is available at https://mohsenhariri.github.io/bayes-kit",
        "subjects": "Artificial Intelligence, Computation and Language, Statistics Theory, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.875640"
    },
    {
        "index": "#138",
        "title": "Zoom-In to Sort AI-Generated Images Out",
        "link": "/arxiv/2510.04225",
        "arxiv_id": "2510.04225",
        "authors": "Yikun Ji, Yan Hong, Bowen Deng, jun lan, Huijia Zhu, Weiqiang Wang, Liqing Zhang, Jianfu Zhang",
        "summary": "The rapid growth of AI-generated imagery has blurred the boundary between real and synthetic content, raising critical concerns for digital integrity. Vision-language models (VLMs) offer interpretability through explanations but often fail to detect subtle artifacts in high-quality synthetic images. We propose ZoomIn, a two-stage forensic framework that improves both accuracy and interpretability. Mimicking human visual inspection, ZoomIn first scans an image to locate suspicious regions and then performs a focused analysis on these zoomed-in areas to deliver a grounded verdict. To support training, we introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images annotated with bounding boxes and forensic explanations, generated through an automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust generalization, while providing human-understandable explanations grounded in visual evidence.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.876419"
    },
    {
        "index": "#139",
        "title": "Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models",
        "link": "/arxiv/2510.04146",
        "arxiv_id": "2510.04146",
        "authors": "Minseo Kim, Coleman Hooper, Aditya Tomar, Chenfeng Xu, Mehrdad Farajtabar, Michael W. Mahoney, Kurt Keutzer, Amir Gholami",
        "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance on a broad range of Natural Language Processing (NLP) tasks, including document processing and coding. Autoregressive Language Models (ARMs), which generate tokens sequentially conditioned on all previous tokens, have been the predominant paradigm for LLMs. However, while these networks have achieved high accuracy across a range of downstream tasks, they exhibit low arithmetic intensity due to the inherent sequential dependency with next-token prediction. Recently, Diffusion Language Models (DLMs) have emerged as a promising alternative architecture. DLMs generate output text in parallel, breaking the limitations of sequential dependency. However, the performance implications of DLMs relative to commonly deployed ARMs are not fully understood. In this work, we present a comprehensive performance study analyzing the performance characteristics of ARMs and DLMs, using both theoretical analysis and profiling data to characterize the trade-offs between these approaches. We illustrate that although DLMs exhibit higher arithmetic intensity compared to ARMs because of their capability to utilize parallelism across sequence lengths, they fail to scale effectively to longer contexts. We then explore DLMs with block-wise decoding, outlining how this approach allows for increased arithmetic intensity, while still scaling well to long contexts (similar to ARMs). We also show interesting trade-offs for batched inference, where we find that ARMs exhibit superior throughput, as they benefit more from parallelism across sequences in the batch. Finally, we highlight opportunities for accelerating DLM inference, and, in particular, highlight the importance of reducing the number of sampling steps for allowing open-source DLMs to provide improved latency relative to ARMs.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.877172"
    },
    {
        "index": "#140",
        "title": "Automating construction safety inspections using a multi-modal vision-language RAG framework",
        "link": "/arxiv/2510.04145",
        "arxiv_id": "2510.04145",
        "authors": "Chenxin Wang, Elyas Asadi Shamsabadi, Zhaohui Chen, Luming Shen, Alireza Ahmadian Fard Fini, Daniel Dias-da-Costa",
        "summary": "Conventional construction safety inspection methods are often inefficient as they require navigating through large volume of information. Recent advances in large vision-language models (LVLMs) provide opportunities to automate safety inspections through enhanced visual and linguistic understanding. However, existing applications face limitations including irrelevant or unspecific responses, restricted modal inputs and hallucinations. Utilisation of Large Language Models (LLMs) for this purpose is constrained by availability of training data and frequently lack real-time adaptability. This study introduces SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG) framework for automating construction safety inspection reports by integrating visual and audio inputs. Using real-world data, SiteShield outperformed unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04, precision of 0.76, and recall of 0.96. The findings indicate that SiteShield offers a novel pathway to enhance information retrieval and efficiency in generating safety reports.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language, Information Retrieval",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.883006"
    },
    {
        "index": "#141",
        "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs",
        "link": "/arxiv/2510.04140",
        "arxiv_id": "2510.04140",
        "authors": "Zishang Jiang, Jinyi Han, Tingyun Li, Xinyi Wang, Sihang Jiang, Jiaqing Liang, Zhaoqian Dai, Shuguang Ma, Fei Yu, Yanghua Xiao",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely adopted technique for enhancing the reasoning ability of Large Language Models (LLMs). However, the effectiveness of RLVR strongly depends on the capability of base models. This issue arises because it requires the model to have sufficient capability to perform high-quality exploration, which involves both effectiveness and diversity. Unfortunately, existing methods address this issue by imitating expert trajectories, which improve effectiveness but neglect diversity. To address this, we argue that the expert only needs to provide guidance only at critical decision points rather than the entire reasoning path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation for Token-level Optimization of Reasoning, a framework that provides expert guidance only at critical decision points to perform effective and diverse exploration in RLVR. Extensive experiments show that MENTOR enables models capture the essence of expert strategies rather than surface imitation, thereby performing high-quality exploration and achieving superior overall performance. Our code is available online.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.883813"
    },
    {
        "index": "#142",
        "title": "Internal states before wait modulate reasoning patterns",
        "link": "/arxiv/2510.04128",
        "arxiv_id": "2510.04128",
        "authors": "Dmitrii Troitskii, Koyena Pal, Chris Wendler, Callum Stuart McDougall, Neel Nanda",
        "summary": "Prior work has shown that a significant driver of performance in reasoning models is their ability to reason and self-correct. A distinctive marker in these reasoning traces is the token wait, which often signals reasoning behavior such as backtracking. Despite being such a complex behavior, little is understood of exactly why models do or do not decide to reason in this particular manner, which limits our understanding of what makes a reasoning model so effective. In this work, we address the question whether model's latents preceding wait tokens contain relevant information for modulating the subsequent reasoning process. We train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent attribution technique in the crosscoder setting. We locate a small set of features relevant for promoting/suppressing wait tokens' probabilities. Finally, through a targeted series of experiments analyzing max activating examples and causal interventions, we show that many of our identified features indeed are relevant for the reasoning process and give rise to different types of reasoning patterns such as restarting from the beginning, recalling prior knowledge, expressing uncertainty, and double-checking.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.884483"
    },
    {
        "index": "#143",
        "title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning",
        "link": "/arxiv/2510.04072",
        "arxiv_id": "2510.04072",
        "authors": "Ziyan Wang, Zheng Wang, Jie Fu, Xingwei Qu, Qi Cheng, Shengpu Tang, Minjia Zhang, Xiaoming Huo",
        "summary": "Reinforcement learning (RL) has become central to enhancing reasoning in large language models (LLMs). Yet on-policy algorithms such as Group Relative Policy Optimization (GRPO) often suffer in early training: noisy gradients from low-quality rollouts lead to unstable updates and inefficient exploration. We introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient framework to address these limitations via decomposing each step into three stages: a short fast trajectory of inner steps on the same batch, a reposition mechanism to control off-policy drift, and a final slow correction. This reposition-before-update design preserves the objective and rollout process unchanged, making SFPO plug-compatible with existing policy-gradient pipelines. Extensive experiments demonstrate that SFPO consistently improves stability, reduces rollouts, and accelerates convergence of reasoning RL training. Specifically, it outperforms GRPO by up to 2.80 points in average on math reasoning benchmarks. It also achieves up to 4.93\\texttimes{} fewer rollouts and a 4.19\\texttimes{} reduction in wall-clock time to match GRPO's best accuracy.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.885273"
    },
    {
        "index": "#144",
        "title": "What Scales in Cross-Entropy Scaling Law?",
        "link": "/arxiv/2510.04067",
        "arxiv_id": "2510.04067",
        "authors": "Junxi Yan, Zixi Wei, Jingtao Zhan, Qingyao Ai, Yiqun Liu",
        "summary": "The cross-entropy scaling law has long served as a key tool for guiding the development of large language models. It shows that cross-entropy loss decreases in a predictable power-law rate as the model size increases. However, recent evidence indicates that this law breaks down at very large scales: the loss decreases more slowly than expected, which causes significant trouble for developing large language models. In this paper, we hypothesize that the root cause lies in the fact that cross-entropy itself does not truly scale; instead, only one of its hidden components does. To investigate this, we introduce a novel decomposition of cross-entropy into three parts: Error-Entropy, Self-Alignment, and Confidence. We show both theoretically and empirically that this decomposition precisely captures the training dynamics and optimization objectives. Through extensive experiments on multiple datasets and 32 models spanning five orders of magnitude in size, we find that only error-entropy follows a robust power-law scaling, while the other two terms remain largely invariant. Moreover, error-entropy constitutes the dominant share of cross-entropy in small models but diminishes in proportion as models grow larger. This explains why the cross-entropy scaling law appears accurate at small scales but fails at very large ones. Our findings establish the error-entropy scaling law as a more accurate description of model behavior. We believe it will have wide applications in the training, understanding, and future development of large language models.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.885984"
    },
    {
        "index": "#145",
        "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions",
        "link": "/arxiv/2510.04023",
        "arxiv_id": "2510.04023",
        "authors": "Mizanur Rahman, Amran Bhuiyan, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Ridwan Mahbub, Ahmed Masry, Shafiq Joty, Enamul Hoque",
        "summary": "Recent advances in large language models (LLMs) have enabled a new class of AI agents that automate multiple stages of the data science workflow by integrating planning, tool use, and multimodal reasoning across text, code, tables, and visuals. This survey presents the first comprehensive, lifecycle-aligned taxonomy of data science agents, systematically analyzing and mapping forty-five systems onto the six stages of the end-to-end data science process: business understanding and data acquisition, exploratory analysis and visualization, feature engineering, model building and selection, interpretation and explanation, and deployment and monitoring. In addition to lifecycle coverage, we annotate each agent along five cross-cutting design dimensions: reasoning and planning style, modality integration, tool orchestration depth, learning and alignment methods, and trust, safety, and governance mechanisms. Beyond classification, we provide a critical synthesis of agent capabilities, highlight strengths and limitations at each stage, and review emerging benchmarks and evaluation practices. Our analysis identifies three key trends: most systems emphasize exploratory analysis, visualization, and modeling while neglecting business understanding, deployment, and monitoring; multimodal reasoning and tool orchestration remain unresolved challenges; and over 90% lack explicit trust and safety mechanisms. We conclude by outlining open challenges in alignment stability, explainability, governance, and robust evaluation frameworks, and propose future research directions to guide the development of robust, trustworthy, low-latency, transparent, and broadly accessible data science agents.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.886716"
    },
    {
        "index": "#146",
        "title": "Principled and Tractable RL for Reasoning with Diffusion Language Models",
        "link": "/arxiv/2510.04019",
        "arxiv_id": "2510.04019",
        "authors": "Anthony Zhan",
        "summary": "Diffusion large language models (dLLMs) are a new paradigm of non-autoregressive language models that are trained to predict multiple tokens in parallel and generate text via iterative unmasking. Recent works have successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B scale, but dLLMs have yet to benefit from modern post-training techniques, e.g. reinforcement learning (RL), that have proven effective for autoregressive models. Crucially, algorithms designed for traditional LLMs aren't directly compatible with diffusion frameworks due to inherent differences in modeling assumptions. Moreover, existing attempts at dLLM post-training with RL rely on heuristic-based objectives with no theoretical grounding. In this work, we present Amortized Group Relative Policy Optimization (AGRPO), a principled on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo sampling to compute an unbiased policy gradient estimate, making it the first tractable, faithful adaptation of policy gradient methods for dLLMs. We demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x performance on the Countdown task over the baseline LLaDA-8B-Instruct model and 1.3x performance gains over comparable RL methods such as diffu-GRPO. Furthermore, these gains persist across different numbers of sampling steps at inference time, achieving better tradeoffs between compute and performance. Our results demonstrate that online RL algorithms can be extended to diffusion LLMs in principled ways, maintaining both theoretical soundness and practical effectiveness.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.887333"
    },
    {
        "index": "#147",
        "title": "Visual Lifelog Retrieval through Captioning-Enhanced Interpretation",
        "link": "/arxiv/2510.04010",
        "arxiv_id": "2510.04010",
        "authors": "Yu-Fei Shih, An-Zi Yen, Hen-Hsen Huang, Hsin-Hsi Chen",
        "summary": "People often struggle to remember specific details of past experiences, which can lead to the need to revisit these memories. Consequently, lifelog retrieval has emerged as a crucial application. Various studies have explored methods to facilitate rapid access to personal lifelogs for memory recall assistance. In this paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval System for extracting specific images from a user's visual lifelog based on textual queries. Unlike traditional embedding-based methods, our system first generates captions for visual lifelogs and then utilizes a text embedding model to project both the captions and user queries into a shared vector space. Visual lifelogs, captured through wearable cameras, provide a first-person viewpoint, necessitating the interpretation of the activities of the individual behind the camera rather than merely describing the scene. To address this, we introduce three distinct approaches: the single caption method, the collective caption method, and the merged caption method, each designed to interpret the life experiences of lifeloggers. Experimental results show that our method effectively describes first-person visual images, enhancing the outcomes of lifelog retrieval. Furthermore, we construct a textual dataset that converts visual lifelogs into captions, thereby reconstructing personal life experiences.",
        "subjects": "Information Retrieval, Computation and Language, Computer Vision and Pattern Recognition, Multimedia",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.934280"
    },
    {
        "index": "#148",
        "title": "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models",
        "link": "/arxiv/2510.04009",
        "arxiv_id": "2510.04009",
        "authors": "Zicong He, Boxuan Zhang, Weihao Liu, Ruixiang Tang, Lu Cheng",
        "summary": "The meteoric rise of foundation models (FMs) has expanded their capabilities far beyond conventional tasks. Creativity, long regarded as a hallmark of human intelligence and a driver of innovation, is now increasingly recognized as a critical dimension of machine intelligence in the era of generative FMs, complementing traditional measures of accuracy. However, existing evaluation frameworks for creativity remain fragmented, relying on ad hoc metrics not firmly grounded in established theories. To address this gap, we introduce C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs. C^2-Eval distinguishes between two complementary forms of creativity: convergent creativity, where tasks admit constrained solutions (e.g., code generation), and divergent creativity, where tasks are open-ended (e.g., storytelling). It evaluates both dimensions using fine-grained criteria derived from social-science theory, focusing on Usefulness, Originality, and Surprise (U-O-S). Through extensive experiments on leading proprietary and open-source models, we analyze trade-offs in their creative capabilities. Our results highlight both the strengths and challenges of current FMs in pursuing a creative machine mind, showing that C^2-Eval is an effective lens for examining the evolving landscape of creative AI.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.935275"
    },
    {
        "index": "#149",
        "title": "Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5",
        "link": "/arxiv/2510.04003",
        "arxiv_id": "2510.04003",
        "authors": "Minh Hoang Nguyen, Su Nguyen Thiet",
        "summary": "Recognizing and processing Classical Chinese (Han-Nom) texts play a vital role in digitizing Vietnamese historical documents and enabling cross-lingual semantic research. However, existing OCR systems struggle with degraded scans, non-standard glyphs, and handwriting variations common in ancient sources. In this work, we propose a fine-tuning approach for PaddleOCRv5 to improve character recognition on Han-Nom texts. We retrain the text recognition module using a curated subset of ancient Vietnamese Chinese manuscripts, supported by a full training pipeline covering preprocessing, LMDB conversion, evaluation, and visualization. Experimental results show a significant improvement over the base model, with exact accuracy increasing from 37.5 percent to 50.0 percent, particularly under noisy image conditions. Furthermore, we develop an interactive demo that visually compares pre- and post-fine-tuning recognition results, facilitating downstream applications such as Han-Vietnamese semantic alignment, machine translation, and historical linguistics research. The demo is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language",
        "date": "2025-10-05",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.936168"
    },
    {
        "index": "#150",
        "title": "No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models",
        "link": "/arxiv/2510.03978",
        "arxiv_id": "2510.03978",
        "authors": "Min Woo Sun, Alejandro Lozano, Javier Gamazo Tejero, Vishwesh Nath, Xiao Xiao Sun, James Burgess, Yuhui Zhang, Kun Yuan, Robert Tibshirani, Sean Huver, Serena Yeung-Levy",
        "summary": "Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that a huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on long-format biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M image-caption pairs enriched with context-aware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than short-context. Our results demonstrate that long-context modeling is a promising direction for advancing biomedical VLMs.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.968247"
    },
    {
        "index": "#151",
        "title": "LLM Chemistry Estimation for Multi-LLM Recommendation",
        "link": "/arxiv/2510.03930",
        "arxiv_id": "2510.03930",
        "authors": "Huascar Sanchez, Briland Hitaj",
        "summary": "Multi-LLM collaboration promises accurate, robust, and context-aware solutions, yet existing approaches rely on implicit selection and output assessment without analyzing whether collaborating models truly complement or conflict. We introduce LLM Chemistry -- a framework that measures when LLM combinations exhibit synergistic or antagonistic behaviors that shape collective performance beyond individual capabilities. We formalize the notion of chemistry among LLMs, propose algorithms that quantify it by analyzing interaction dependencies, and recommend optimal model ensembles accordingly. Our theoretical analysis shows that chemistry among collaborating LLMs is most evident under heterogeneous model profiles, with its outcome impact shaped by task type, group size, and complexity. Evaluation on classification, summarization, and program repair tasks provides initial evidence for these task-dependent effects, thereby reinforcing our theoretical results. This establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and a foundation for ensemble recommendation.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.969171"
    },
    {
        "index": "#152",
        "title": "Kantian-Utilitarian XAI: Meta-Explained",
        "link": "/arxiv/2510.03892",
        "arxiv_id": "2510.03892",
        "authors": "Zahra Atf, Peter R. Lewis",
        "summary": "We present a gamified explainable AI (XAI) system for ethically aware consumer decision-making in the coffee domain. Each session comprises six rounds with three options per round. Two symbolic engines provide real-time reasons: a Kantian module flags rule violations (e.g., child labor, deforestation risk without shade certification, opaque supply chains, unsafe decaf), and a utilitarian module scores options via multi-criteria aggregation over normalized attributes (price, carbon, water, transparency, farmer income share, taste/freshness, packaging, convenience). A meta-explainer with a regret bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a deontically clean, near-parity option when welfare loss is small. We release a structured configuration (attribute schema, certification map, weights, rule set), a policy trace for auditability, and an interactive UI.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.970058"
    },
    {
        "index": "#153",
        "title": "Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration",
        "link": "/arxiv/2510.03865",
        "arxiv_id": "2510.03865",
        "authors": "Wenhao Deng, Long Wei, Chenglei Yu, Tailin Wu",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently enhanced the reasoning capabilities of large language models (LLMs), particularly for mathematical problem solving. However, a fundamental limitation remains: as the sampling budget increases, the advantage of RLVR-trained models over their pretrained bases often diminishes or even vanishes, revealing a strong dependence on the base model's restricted search space. We attribute this phenomenon to the widespread use of the reverse Kullback-Leibler (KL) divergence regularizer, whose mode-seeking behavior keeps the policy trapped inside the base model's support region and hampers wider exploration. To address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an algorithm to promote broader yet focused exploration. Our method (i) utilizes the forward KL penalty to replace the reverse KL penalty for out-of-distribution exploration, and (ii) reweights the reference policy to facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B models with RAPO on the 8K SimpleRL-Zero dataset, without supervised fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO consistently improves problem-solving performance. Notably, RAPO enables models to surpass the base model's performance ceiling and solves previously intractable problems, advancing the frontier of RLVR for challenging reasoning tasks.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.971026"
    },
    {
        "index": "#154",
        "title": "Investigating LLM Variability in Personalized Conversational Information Retrieval",
        "link": "/arxiv/2510.03795",
        "arxiv_id": "2510.03795",
        "authors": "Simon Lupart, Daniël van Dijk, Eric Langezaal, Ian van Dort, Mohammad Aliannejadi",
        "summary": "Personalized Conversational Information Retrieval (CIR) has seen rapid progress in recent years, driven by the development of Large Language Models (LLMs). Personalized CIR aims to enhance document retrieval by leveraging user-specific information, such as preferences, knowledge, or constraints, to tailor responses to individual needs. A key resource for this task is the TREC iKAT 2023 dataset, designed to evaluate personalization in CIR pipelines. Building on this resource, Mo et al. explored several strategies for incorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query reformulation. Their findings suggested that personalization from PTKBs could be detrimental and that human annotations were often noisy. However, these conclusions were based on single-run experiments using the GPT-3.5 Turbo model, raising concerns about output variability and repeatability. In this reproducibility study, we rigorously reproduce and extend their work, focusing on LLM output variability and model generalization. We apply the original methods to the new TREC iKAT 2024 dataset and evaluate a diverse range of models, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that human-selected PTKBs consistently enhance retrieval performance, while LLM-based selection methods do not reliably outperform manual choices. We further compare variance across datasets and observe higher variability on iKAT than on CAsT, highlighting the challenges of evaluating personalized CIR. Notably, recall-oriented metrics exhibit lower variance than precision-oriented ones, a critical insight for first-stage retrievers. Finally, we underscore the need for multi-run evaluations and variance reporting when assessing LLM-based CIR systems. By broadening evaluation across models, datasets, and metrics, our study contributes to more robust and generalizable practices for personalized CIR.",
        "subjects": "Information Retrieval, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.972016"
    },
    {
        "index": "#155",
        "title": "Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation",
        "link": "/arxiv/2510.03731",
        "arxiv_id": "2510.03731",
        "authors": "Yongfu Xue",
        "summary": "The rapid development of parameter-efficient fine-tuning methods has noticeably improved the efficiency of adapting large language models. Among these, LoRA has gained widespread popularity due to its strong balance of effectiveness and parameter efficiency. However, LoRA relies on initializing two low-rank matrices whose product is zero, which limits its ability to effectively activate and leverage the original model weights-creating a potential bottleneck for optimal performance. To address this limitation, we propose \\textbf{IniLoRA}, a novel initialization strategy that initializes the low-rank matrices to closely approximate the original model weights. Experimental results indicate that IniLoRA achieves better performance than LoRA across a range of models and tasks. Additionally, we introduce two variants, IniLoRA-$\\alpha$ and IniLoRA-$\\beta$, both leveraging distinct initialization methods to enhance performance further.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.978019"
    },
    {
        "index": "#156",
        "title": "Bridging the Gap Between Multimodal Foundation Models and World Models",
        "link": "/arxiv/2510.03727",
        "arxiv_id": "2510.03727",
        "authors": "Xuehai He",
        "summary": "Humans understand the world through the integration of multiple sensory modalities, enabling them to perceive, reason about, and imagine dynamic physical processes. Inspired by this capability, multimodal foundation models (MFMs) have emerged as powerful tools for multimodal understanding and generation. However, today's MFMs fall short of serving as effective world models. They lack the essential ability such as perform counterfactual reasoning, simulate dynamics, understand the spatiotemporal information, control generated visual outcomes, and perform multifaceted reasoning. We investigates what it takes to bridge the gap between multimodal foundation models and world models. We begin by improving the reasoning capabilities of MFMs through discriminative tasks and equipping MFMs with structured reasoning skills, such as causal inference, counterfactual thinking, and spatiotemporal reasoning, enabling them to go beyond surface correlations and understand deeper relationships within visual and textual data. Next, we explore generative capabilities of multimodal foundation models across both image and video modalities, introducing new frameworks for structured and controllable generation. Our approaches incorporate scene graphs, multimodal conditioning, and multimodal alignment strategies to guide the generation process, ensuring consistency with high-level semantics and fine-grained user intent. We further extend these techniques to controllable 4D generation, enabling interactive, editable, and morphable object synthesis over time and space.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.978755"
    },
    {
        "index": "#157",
        "title": "Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition",
        "link": "/arxiv/2510.03723",
        "arxiv_id": "2510.03723",
        "authors": "Martin Kocour, Martin Karafiat, Alexander Polok, Dominik Klement, Lukáš Burget, Jan Černocký",
        "summary": "We propose a speaker-attributed (SA) Whisper-based model for multi-talker speech recognition that combines target-speaker modeling with serialized output training (SOT). Our approach leverages a Diarization-Conditioned Whisper (DiCoW) encoder to extract target-speaker embeddings, which are concatenated into a single representation and passed to a shared decoder. This enables the model to transcribe overlapping speech as a serialized output stream with speaker tags and timestamps. In contrast to target-speaker ASR systems such as DiCoW, which decode each speaker separately, our approach performs joint decoding, allowing the decoder to condition on the context of all speakers simultaneously. Experiments show that the model outperforms existing SOT-based approaches and surpasses DiCoW on multi-talker mixtures (e.g., LibriMix).",
        "subjects": "Audio and Speech Processing, Computation and Language, Sound",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.979469"
    },
    {
        "index": "#158",
        "title": "Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models",
        "link": "/arxiv/2510.03721",
        "arxiv_id": "2510.03721",
        "authors": "Leander Girrbach, Stephan Alaniz, Genevieve Smith, Trevor Darrell, Zeynep Akata",
        "summary": "Vision-language models trained on large-scale multimodal datasets show strong demographic biases, but the role of training data in producing these biases remains unclear. A major barrier has been the lack of demographic annotations in web-scale datasets such as LAION-400M. We address this gap by creating person-centric annotations for the full dataset, including over 276 million bounding boxes, perceived gender and race/ethnicity labels, and automatically generated captions. These annotations are produced through validated automatic labeling pipelines combining object detection, multimodal captioning, and finetuned classifiers. Using them, we uncover demographic imbalances and harmful associations, such as the disproportionate linking of men and individuals perceived as Black or Middle Eastern with crime-related and negative content. We also show that 60-70% of gender bias in CLIP and Stable Diffusion can be linearly explained by direct co-occurrences in the data. Our resources establish the first large-scale empirical link between dataset composition and downstream model bias.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language, Computers and Society, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.980213"
    },
    {
        "index": "#159",
        "title": "Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models",
        "link": "/arxiv/2510.03696",
        "arxiv_id": "2510.03696",
        "authors": "Deepak Babu Piskala, Sharlene Chen, Udita Patel, Parul Kalra, Rafael Castrillo",
        "summary": "Evaluating the quality of multi-turn chatbot interactions remains challenging, as most existing methods assess interactions at the turn level without addressing whether a user's overarching goal was fulfilled. A ``goal'' here refers to an information need or task, such as asking for policy information or applying for leave. We propose a comprehensive framework for goal-oriented evaluation of multi-agent systems (MAS), introducing the \\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals, and a \\textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for failure in multi-agent chatbots. Our method segments conversations by user goals and evaluates success using all relevant turns. We present a model-based evaluation system combining teacher LLMs, where domain experts define goals, set quality standards serving as a guidance for the LLMs. The LLMs use ``thinking tokens'' to produce interpretable rationales, enabling \\textit{explainable}, \\textit{data-efficient} evaluations. In an enterprise setting, we apply our framework to evaluate AIDA, a zero-to-one employee conversational agent system built as a ground-up multi-agent conversational agent, and observe GSR improvement from 63\\% to 79\\% over six months since its inception. Our framework is generic and offers actionable insights through a detailed defect taxonomy based on analysis of failure points in multi-agent chatbots, diagnosing overall success, identifying key failure modes, and informing system improvements.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.980883"
    },
    {
        "index": "#160",
        "title": "Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning",
        "link": "/arxiv/2510.03669",
        "arxiv_id": "2510.03669",
        "authors": "Wenlong Deng, Yi Ren, Yushu Li, Boying Gong, Danica J. Sutherland, Xiaoxiao Li, Christos Thrampoulidis",
        "summary": "Reinforcement learning with verifiable rewards has significantly advanced the reasoning capabilities of large language models, yet how to explicitly steer training toward exploration or exploitation remains an open problem. We introduce Token Hidden Reward (THR), a token-level metric that quantifies each token's influence on the likelihood of correct responses under Group Relative Policy Optimization (GRPO). We find that training dynamics are dominated by a small subset of tokens with high absolute THR values. Most interestingly, tokens with positive THR strengthen confidence in correct outputs, thus favoring exploitation, while tokens with negative THR preserve probability mass for alternative outputs, enabling exploration. This insight suggests a natural intervention: a THR-guided reweighting algorithm that modulates GRPO's learning signals to explicitly bias training toward exploitation or exploration. We validate the efficacy of this algorithm on diverse math reasoning benchmarks. By amplifying tokens with positive THR value and weakening negative ones, our algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse strategy yields consistent gains in Pass@K accuracy, favoring exploration. We further demonstrate that our algorithm integrates seamlessly with other RL objectives such as GSPO and generalizes across architectures including Llama. These findings establish THR as a principled and fine-grained mechanism for dynamically controlling exploration and exploitation in RL-tuned LLMs, providing new tools for targeted fine-tuning in reasoning-intensive applications.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:51.981607"
    },
    {
        "index": "#161",
        "title": "Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders",
        "link": "/arxiv/2510.03659",
        "arxiv_id": "2510.03659",
        "authors": "Xu Wang, Yan Hu, Benyou Wang, Difan Zou",
        "summary": "Sparse Autoencoders (SAEs) are widely used to steer large language models (LLMs), based on the assumption that their interpretable features naturally enable effective model behavior steering. Yet, a fundamental question remains unanswered: does higher interpretability indeed imply better steering utility? To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B, Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels, and evaluate their interpretability and steering utility based on SAEBench (arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis reveals only a relatively weak positive association (tau b approx 0.298), indicating that interpretability is an insufficient proxy for steering performance. We conjecture the interpretability utility gap may stem from the selection of SAE features, as not all of them are equally effective for steering. To further find features that truly steer the behavior of LLMs, we propose a novel selection criterion called Delta Token Confidence, which measures how much amplifying a feature changes the next token distribution. We show that our method improves the steering performance of three LLMs by 52.52 percent compared to the current best output score based criterion (arXiv:2503.34567). Strikingly, after selecting features with high Delta Token Confidence, the correlation between interpretability and utility vanishes (tau b approx 0), and can even become negative. This further highlights the divergence between interpretability and utility for the most effective steering features.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.007899"
    },
    {
        "index": "#162",
        "title": "From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse",
        "link": "/arxiv/2510.03636",
        "arxiv_id": "2510.03636",
        "authors": "Rabeya Amin Jhuma, Mostafa Mohaimen Akand Faisal",
        "summary": "This study explored how in-context learning (ICL) in large language models can be disrupted by data poisoning attacks in the setting of public health sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small adversarial perturbations such as synonym replacement, negation insertion, and randomized perturbation were introduced into the support examples. Even these minor manipulations caused major disruptions, with sentiment labels flipping in up to 67% of cases. To address this, a Spectral Signature Defense was applied, which filtered out poisoned examples while keeping the data's meaning and sentiment intact. After defense, ICL accuracy remained steady at around 46.7%, and logistic regression validation reached 100% accuracy, showing that the defense successfully preserved the dataset's integrity. Overall, the findings extend prior theoretical studies of ICL poisoning to a practical, high-stakes setting in public health discourse analysis, highlighting both the risks and potential defenses for robust LLM deployment. This study also highlights the fragility of ICL under attack and the value of spectral defenses in making AI systems more reliable for health-related social media monitoring.",
        "subjects": "Machine Learning, Computation and Language, Cryptography and Security",
        "date": "2025-10-04",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.008849"
    },
    {
        "index": "#163",
        "title": "Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs",
        "link": "/arxiv/2510.03567",
        "arxiv_id": "2510.03567",
        "authors": "Fatmazohra Rezkellah, Ramzi Dakhmouche",
        "summary": "With the increasing adoption of Large Language Models (LLMs), more customization is needed to ensure privacy-preserving and safe generation. We address this objective from two critical aspects: unlearning of sensitive information and robustness to jail-breaking attacks. We investigate various constrained optimization formulations that address both aspects in a \\emph{unified manner}, by finding the smallest possible interventions on LLM weights that either make a given vocabulary set unreachable or embed the LLM with robustness to tailored attacks by shifting part of the weights to a \\emph{safer} region. Beyond unifying two key properties, this approach contrasts with previous work in that it doesn't require an oracle classifier that is typically not available or represents a computational overhead. Surprisingly, we find that the simplest point-wise constraint-based intervention we propose leads to better performance than max-min interventions, while having a lower computational cost. Comparison against state-of-the-art defense methods demonstrates superior performance of the proposed approach.",
        "subjects": "Machine Learning, Computation and Language, Cryptography and Security, Computers and Society, Optimization and Control",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.009879"
    },
    {
        "index": "#164",
        "title": "Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making",
        "link": "/arxiv/2510.03514",
        "arxiv_id": "2510.03514",
        "authors": "Toby Drinkall",
        "summary": "As military organisations consider integrating large language models (LLMs) into command and control (C2) systems for planning and decision support, understanding their behavioural tendencies is critical. This study develops a benchmarking framework for evaluating aspects of legal and moral risk in targeting behaviour by comparing LLMs acting as agents in multi-turn simulated conflict. We introduce four metrics grounded in International Humanitarian Law (IHL) and military doctrine: Civilian Target Rate (CTR) and Dual-use Target Rate (DTR) assess compliance with legal targeting principles, while Mean and Max Simulated Non-combatant Casualty Value (SNCV) quantify tolerance for civilian harm. We evaluate three frontier models, GPT-4o, Gemini-2.5, and LLaMA-3.1, through 90 multi-agent, multi-turn crisis simulations across three geographic regions. Our findings reveal that off-the-shelf LLMs exhibit concerning and unpredictable targeting behaviour in simulated conflict environments. All models violated the IHL principle of distinction by targeting civilian objects, with breach rates ranging from 16.7% to 66.7%. Harm tolerance escalated through crisis simulations with MeanSNCV increasing from 16.5 in early turns to 27.7 in late turns. Significant inter-model variation emerged: LLaMA-3.1 selected an average of 3.47 civilian strikes per simulation with MeanSNCV of 28.4, while Gemini-2.5 selected 0.90 civilian strikes with MeanSNCV of 17.6. These differences indicate that model selection for deployment constitutes a choice about acceptable legal and moral risk profiles in military operations. This work seeks to provide a proof-of-concept of potential behavioural risks that could emerge from the use of LLMs in Decision Support Systems (AI DSS) as well as a reproducible benchmarking framework with interpretable metrics for standardising pre-deployment testing.",
        "subjects": "Computers and Society, Artificial Intelligence, Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.010787"
    },
    {
        "index": "#165",
        "title": "Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation",
        "link": "/arxiv/2510.03437",
        "arxiv_id": "2510.03437",
        "authors": "Jairo Diaz-Rodriguez, Mumin Jia",
        "summary": "Kernel change-point detection (KCPD) has become a widely used tool for identifying structural changes in complex data. While existing theory establishes consistency under independence assumptions, real-world sequential data such as text exhibits strong dependencies. We establish new guarantees for KCPD under $m$-dependent data: specifically, we prove consistency in the number of detected change points and weak consistency in their locations under mild additional assumptions. We perform an LLM-based simulation that generates synthetic $m$-dependent text to validate the asymptotics. To complement these results, we present the first comprehensive empirical study of KCPD for text segmentation with modern embeddings. Across diverse text datasets, KCPD with text embeddings outperforms baselines in standard text segmentation metrics. We demonstrate through a case study on Taylor Swift's tweets that KCPD not only provides strong theoretical and simulated reliability but also practical effectiveness for text segmentation tasks.",
        "subjects": "Machine Learning, Computation and Language, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.011716"
    },
    {
        "index": "#166",
        "title": "PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters",
        "link": "/arxiv/2510.03415",
        "arxiv_id": "2510.03415",
        "authors": "Aditya Thimmaiah, Jiyang Zhang, Jayanth Srinivasa, Junyi Jessy Li, Milos Gligoric",
        "summary": "As large language models (LLMs) excel at code reasoning, a natural question arises: can an LLM execute programs (i.e., act as an interpreter) purely based on a programming language's formal semantics? If so, it will enable rapid prototyping of new programming languages and language features. We study this question using the imperative language IMP (a subset of C), formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). We introduce three evaluation sets-Human-Written, LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by code-complexity metrics spanning the size, control-flow, and data-flow axes. Given a program and its semantics formalized with SOS/K-semantics, models are evaluated on three tasks ranging from coarse to fine: (1) final-state prediction, (2) semantic rule prediction, and (3) execution trace prediction. To distinguish pretraining memorization from semantic competence, we define two nonstandard semantics obtained through systematic mutations of the standard rules. Across strong code/reasoning LLMs, performance drops under nonstandard semantics despite high performance under the standard one. We further find that (i) there are patterns to different model failures, (ii) most reasoning models perform exceptionally well on coarse grained tasks involving reasoning about highly complex programs often containing nested loop depths beyond five, and surprisingly, (iii) providing formal semantics helps on simple programs but often hurts on more complex ones. Overall, the results show a promise that LLMs could serve as programming language interpreters, but points to the lack of their robust semantics understanding. We release the benchmark and the supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.",
        "subjects": "Programming Languages, Artificial Intelligence, Computation and Language, Software Engineering",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.012777"
    },
    {
        "index": "#167",
        "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition",
        "link": "/arxiv/2510.03399",
        "arxiv_id": "2510.03399",
        "authors": "Xiaoyan Bai, Aryan Shrivastava, Ari Holtzman, Chenhao Tan",
        "summary": "Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.",
        "subjects": "Artificial Intelligence, Computation and Language, Computers and Society, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.018667"
    },
    {
        "index": "#168",
        "title": "Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning",
        "link": "/arxiv/2510.03394",
        "arxiv_id": "2510.03394",
        "authors": "Donghwan Rho",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training large language models (LLMs) with stronger reasoning abilities. It has also been applied to a variety of logic puzzles. In this work, we study the Korean word-chain game using RLVR. We show that rule-derived rewards can naturally conflict, and demonstrate through experiments that a curriculum-learning scheme mitigates these conflicts. Our findings motivate further studies of puzzle tasks in diverse languages.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.019253"
    },
    {
        "index": "#169",
        "title": "Lightweight Prompt Engineering for Cognitive Alignment in Educational AI: A OneClickQuiz Case Study",
        "link": "/arxiv/2510.03374",
        "arxiv_id": "2510.03374",
        "authors": "Antoun Yaacoub, Zainab Assaghir, Jérôme Da-Rugna",
        "summary": "The rapid integration of Artificial Intelligence (AI) into educational technology promises to revolutionize content creation and assessment. However, the quality and pedagogical alignment of AI-generated content remain critical challenges. This paper investigates the impact of lightweight prompt engineering strategies on the cognitive alignment of AI-generated questions within OneClickQuiz, a Moodle plugin leveraging generative AI. We evaluate three prompt variants-a detailed baseline, a simpler version, and a persona-based approach-across Knowledge, Application, and Analysis levels of Bloom's Taxonomy. Utilizing an automated classification model (from prior work) and human review, our findings demonstrate that explicit, detailed prompts are crucial for precise cognitive alignment. While simpler and persona-based prompts yield clear and relevant questions, they frequently misalign with intended Bloom's levels, generating outputs that are either too complex or deviate from the desired cognitive objective. This study underscores the importance of strategic prompt engineering in fostering pedagogically sound AI-driven educational solutions and advises on optimizing AI for quality content generation in learning analytics and smart learning environments.",
        "subjects": "Computers and Society, Artificial Intelligence, Computation and Language",
        "date": "2025-10-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.019929"
    },
    {
        "index": "#170",
        "title": "AgentCaster: Reasoning-Guided Tornado Forecasting",
        "link": "/arxiv/2510.03349",
        "arxiv_id": "2510.03349",
        "authors": "Michael Chen",
        "summary": "There is a growing need to evaluate Large Language Models (LLMs) on complex, high-impact, real-world tasks to assess their true readiness as reasoning agents. To address this gap, we introduce AgentCaster, a contamination-free framework employing multimodal LLMs end-to-end for the challenging, long-horizon task of tornado forecasting. Within AgentCaster, models interpret heterogeneous spatiotemporal data from a high-resolution convection-allowing forecast archive. We assess model performance over a 40-day period featuring diverse historical data, spanning several major tornado outbreaks and including over 500 tornado reports. Each day, models query interactively from a pool of 3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of 12-36 hours. Probabilistic tornado-risk polygon predictions are verified against ground truths derived from geometric comparisons across disjoint risk bands in projected coordinate space. To quantify accuracy, we propose domain-specific TornadoBench and TornadoHallucination metrics, with TornadoBench highly challenging for both LLMs and domain expert human forecasters. Notably, human experts significantly outperform state-of-the-art models, which demonstrate a strong tendency to hallucinate and overpredict risk intensity, struggle with precise geographic placement, and exhibit poor spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster aims to advance research on improving LLM agents for challenging reasoning tasks in critical domains.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Atmospheric and Oceanic Physics",
        "date": "2025-10-02",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.020585"
    },
    {
        "index": "#171",
        "title": "CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models",
        "link": "/arxiv/2510.03298",
        "arxiv_id": "2510.03298",
        "authors": "Dongqi Zheng, Wenjin Fu",
        "summary": "We introduce Constraint-Aware Federated Learning with Lagrangian Dual Optimization (CAFL-L), a principled extension of FedAvg that explicitly incorporates device-level resource constraints including energy, communication, memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to dynamically adapt training hyperparameters -- freezing depth, local steps, batch size, and communication compression -- while preserving training stability through token-budget preservation via gradient accumulation. Experiments on a character-level language model demonstrate that CAFL-L achieves superior constraint satisfaction compared to standard FedAvg (reducing memory usage by 20% and communication by 95%) while maintaining competitive validation performance, making it practical for deployment on resource-constrained edge devices.",
        "subjects": "Machine Learning, Computation and Language, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.021228"
    },
    {
        "index": "#172",
        "title": "Multimodal Arabic Captioning with Interpretable Visual Concept Integration",
        "link": "/arxiv/2510.03295",
        "arxiv_id": "2510.03295",
        "authors": "Passant Elchafei, Amany Fashwan",
        "summary": "We present VLCAP, an Arabic image captioning framework that integrates CLIP-based visual label retrieval with multimodal text generation. Rather than relying solely on end-to-end captioning, VLCAP grounds generation in interpretable Arabic visual concepts extracted with three multilingual encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label retrieval. A hybrid vocabulary is built from training captions and enriched with about 21K general domain labels translated from the Visual Genome dataset, covering objects, attributes, and scenes. The top-k retrieved labels are transformed into fluent Arabic prompts and passed along with the original image to vision-language models. In the second stage, we tested Qwen-VL and Gemini Pro Vision for caption generation, resulting in six encoder-decoder configurations. The results show that mCLIP + Gemini Pro Vision achieved the best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL obtained the highest LLM-judge score (36.33%). This interpretable pipeline enables culturally coherent and contextually accurate Arabic captions.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.021872"
    },
    {
        "index": "#173",
        "title": "Why mask diffusion does not work",
        "link": "/arxiv/2510.03289",
        "arxiv_id": "2510.03289",
        "authors": "Haocheng Sun, Cynthia Xin Wen, Edward Hong Wang",
        "summary": "The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-29",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.022536"
    },
    {
        "index": "#174",
        "title": "MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment",
        "link": "/arxiv/2510.03283",
        "arxiv_id": "2510.03283",
        "authors": "Yufei Li, Yu Fu, Yue Dong, Cong Liu",
        "summary": "Large language models (LLMs) deployed on edge servers are increasingly used in latency-sensitive applications such as personalized assistants, recommendation, and content moderation. However, the non-stationary nature of user data necessitates frequent retraining, which introduces a fundamental tension between inference latency and model accuracy under constrained GPU resources. Existing retraining strategies either delay model updates, over-commit resources to retraining, or overlook iteration-level retraining granularity. In this paper, we identify that iteration-level scheduling is crucial for adapting retraining frequency to model drift without violating service-level objectives (SLOs). We propose MACE, a hybrid LLM system that colocates concurrent inference (prefill, decode) and fine-tuning, with intelligent memory management to maximize task performance while promising inference throughput. MACE leverages the insight that not all model updates equally affect output alignment and allocates GPU cycles accordingly to balance throughput, latency, and update freshness. Our trace-driven evaluation shows that MACE matches or exceeds continuous retraining while reducing inference latency by up to 63% and maintaining throughput under resource constraints. Compared to periodic retraining, MACE improves latency breakdown across prefill, decode, and finetune stages, and sustains GPU utilization above 85% in NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid scheduling is a promising direction for deploying LLMs with continual learning capabilities on edge platforms.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.048828"
    },
    {
        "index": "#175",
        "title": "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework",
        "link": "/arxiv/2510.03282",
        "arxiv_id": "2510.03282",
        "authors": "Hao Gu, Vibhas Nair, Amrithaa Ashok Kumar, Jayvart Sharma, Ryan Lagasse",
        "summary": "Interpreting language models often involves circuit analysis, which aims to identify sparse subnetworks, or circuits, that accomplish specific tasks. Existing circuit discovery algorithms face a fundamental trade-off: attribution patching is fast but unfaithful to the full model, while edge pruning is faithful but computationally expensive. This research proposes a hybrid attribution and pruning (HAP) framework that uses attribution patching to identify a high-potential subgraph, then applies edge pruning to extract a faithful circuit from it. We show that HAP is 46\\% faster than baseline algorithms without sacrificing circuit faithfulness. Furthermore, we present a case study on the Indirect Object Identification task, showing that our method preserves cooperative circuit components (e.g. S-inhibition heads) that attribution patching methods prune at high sparsity. Our results show that HAP could be an effective approach for improving the scalability of mechanistic interpretability research to larger models. Our code is available at https://anonymous.4open.science/r/HAP-circuit-discovery.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.049815"
    },
    {
        "index": "#176",
        "title": "Training Optimal Large Diffusion Language Models",
        "link": "/arxiv/2510.03280",
        "arxiv_id": "2510.03280",
        "authors": "Jinjie Ni, Qian Liu, Chao Du, Longxu Dou, Hang Yan, Zili Wang, Tianyu Pang, Michael Qizhe Shieh",
        "summary": "We introduce Quokka, the first systematic scaling law for diffusion language models (DLMs), encompassing both compute-constrained and data-constrained regimes, and studying the key modeling and optimization designs. Quokka is a good friend of Chinchilla and provides wider scopes. We hope the results would bring short-term practical guidance in DLMs training and long-term inspirations for the whole AI community.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.050933"
    },
    {
        "index": "#177",
        "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
        "link": "/arxiv/2510.03279",
        "arxiv_id": "2510.03279",
        "authors": "Youjin Wang, Yangjingyi Chen, Jiahao Yan, Jiaxuan Lu, Xiao Sun",
        "summary": "With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-28",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.052024"
    },
    {
        "index": "#178",
        "title": "General Exploratory Bonus for Optimistic Exploration in RLHF",
        "link": "/arxiv/2510.03269",
        "arxiv_id": "2510.03269",
        "authors": "Wendi Li, Changdae Oh, Yixuan Li",
        "summary": "Optimistic exploration is central to improving sample efficiency in reinforcement learning with human feedback, yet existing exploratory bonus methods to incentivize exploration often fail to realize optimism. We provide a theoretical analysis showing that current formulations, under KL or $\\alpha$-divergence regularization, unintentionally bias exploration toward high-probability regions of the reference model, thereby reinforcing conservative behavior instead of promoting discovery of uncertain regions. To address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle. GEB counteracts divergence-induced bias via reference-dependent reward regulation and unifies prior heuristic bonuses as special cases, while extending naturally across the full $\\alpha$-divergence family. Empirically, GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones. These results demonstrate that GEB offers both a principled and practical solution for optimistic exploration in RLHF.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-09-27",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.083852"
    },
    {
        "index": "#179",
        "title": "CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano",
        "link": "/arxiv/2412.18708",
        "arxiv_id": "2412.18708",
        "authors": "Vivek Vellaiyappan Surulimuthu, Aditya Karnam Gururaj Rao",
        "summary": "We present Chunked Augmented Generation (CAG), an architecture specifically designed to overcome the context window limitations of Google Chrome's built-in Gemini Nano model. While Chrome's integration of Gemini Nano represents a significant advancement in bringing AI capabilities directly to the browser, its restricted context window poses challenges for processing large inputs. CAG addresses this limitation through intelligent input chunking and processing strategies, enabling efficient handling of extensive content while maintaining the model's performance within browser constraints. Our implementation demonstrates particular efficacy in processing large documents and datasets directly within Chrome, making sophisticated AI capabilities accessible through the browser without external API dependencies. Get started now at https://github.com/vivekVells/cag-js.",
        "subjects": "Artificial Intelligence",
        "date": "2024-12-24",
        "category": "cs.CL",
        "crawl_time": "2025-10-08T08:16:52.084715"
    },
    {
        "index": "#1",
        "title": "Staircase Streaming for Low-Latency Multi-Agent Inference",
        "link": "/arxiv/2510.05059",
        "arxiv_id": "2510.05059",
        "authors": "Junlin Wang, Jue Wang, Zhen, Xu, Ben Athiwaratkun, Bhuwan Dhingra, Ce Zhang, James Zou",
        "summary": "Recent advances in large language models (LLMs) opened up new directions for leveraging the collective expertise of multiple LLMs. These methods, such as Mixture-of-Agents, typically employ additional inference steps to generate intermediate outputs, which are then used to produce the final response. While multi-agent inference can enhance response quality, it can significantly increase the time to first token (TTFT), posing a challenge for latency-sensitive applications and hurting user experience. To address this issue, we propose staircase streaming for low-latency multi-agent inference. Instead of waiting for the complete intermediate outputs from previous steps, we begin generating the final response as soon as we receive partial outputs from these steps. Experimental results demonstrate that staircase streaming reduces TTFT by up to 93% while maintaining response quality.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.125578"
    },
    {
        "index": "#2",
        "title": "Look-ahead Reasoning with a Learned Model in Imperfect Information Games",
        "link": "/arxiv/2510.05048",
        "arxiv_id": "2510.05048",
        "authors": "Ondřej Kubíček, Viliam Lisý",
        "summary": "Test-time reasoning significantly enhances pre-trained AI agents' performance. However, it requires an explicit environment model, often unavailable or overly complex in real-world scenarios. While MuZero enables effective model learning for search in perfect information games, extending this paradigm to imperfect information games presents substantial challenges due to more nuanced look-ahead reasoning techniques and large number of states relevant for individual decisions. This paper introduces an algorithm LAMIR that learns an abstracted model of an imperfect information game directly from the agent-environment interaction. During test time, this trained model is used to perform look-ahead reasoning. The learned abstraction limits the size of each subgame to a manageable size, making theoretically principled look-ahead reasoning tractable even in games where previous methods could not scale. We empirically demonstrate that with sufficient capacity, LAMIR learns the exact underlying game structure, and with limited capacity, it still learns a valuable abstraction, which improves game playing performance of the pre-trained agents even in large games.",
        "subjects": "Artificial Intelligence, Computer Science and Game Theory",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.126243"
    },
    {
        "index": "#3",
        "title": "Think Then Embed: Generative Context Improves Multimodal Embedding",
        "link": "/arxiv/2510.05014",
        "arxiv_id": "2510.05014",
        "authors": "Xuanming Cui, Jianpeng Cheng, Hong-you Chen, Satya Narayan Shukla, Abhijeet Awasthi, Xichen Pan, Chaitanya Ahuja, Shlok Kumar Mishra, Qi Guo, Ser-Nam Lim, Aashu Singh, Xiangjun Fan",
        "summary": "There is a growing interest in Universal Multimodal Embeddings (UME), where models are required to generate task-specific representations. While recent studies show that Multimodal Large Language Models (MLLMs) perform well on such tasks, they treat MLLMs solely as encoders, overlooking their generative capacity. However, such an encoding paradigm becomes less effective as instructions become more complex and require compositional reasoning. Inspired by the proven effectiveness of chain-of-thought reasoning, we propose a general Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an embedder. The reasoner MLLM first generates reasoning traces that explain complex queries, followed by an embedder that produces representations conditioned on both the original query and the intermediate reasoning. This explicit reasoning step enables more nuanced understanding of complex multimodal instructions. Our contributions are threefold. First, by leveraging a powerful MLLM reasoner, we achieve state-of-the-art performance on the MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune a smaller MLLM reasoner using high-quality embedding-centric reasoning traces, achieving the best performance among open-source models with a 7% absolute gain over recently proposed models. Third, we investigate strategies for integrating the reasoner and embedder into a unified model for improved efficiency without sacrificing performance.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.127099"
    },
    {
        "index": "#5",
        "title": "Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI",
        "link": "/arxiv/2510.04978",
        "arxiv_id": "2510.04978",
        "authors": "Kun Xiang, Terry Jingchen Zhang, Yinya Huang, Jixi He, Zirong Liu, Yueling Tang, Ruizhe Zhou, Lijing Luo, Youpeng Wen, Xiuwei Chen, Bingqian Lin, Jianhua Han, Hang Xu, Hanhui Li, Bin Dong, Xiaodan Liang",
        "summary": "The rapid advancement of embodied intelligence and world models has intensified efforts to integrate physical laws into AI systems, yet physical perception and symbolic physics reasoning have developed along separate trajectories without a unified bridging framework. This work provides a comprehensive overview of physical AI, establishing clear distinctions between theoretical physics reasoning and applied physical understanding while systematically examining how physics-grounded methods enhance AI's real-world comprehension across structured symbolic reasoning, embodied systems, and generative models. Through rigorous analysis of recent advances, we advocate for intelligent systems that ground learning in both physical principles and embodied reasoning processes, transcending pattern recognition toward genuine understanding of physical laws. Our synthesis envisions next-generation world models capable of explaining physical phenomena and predicting future states, advancing safe, generalizable, and interpretable AI systems. We maintain a continuously updated resource at https://github.com/AI4Phys/Awesome-AI-for-Physics.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.128738"
    },
    {
        "index": "#6",
        "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits",
        "link": "/arxiv/2510.04952",
        "arxiv_id": "2510.04952",
        "authors": "Ailiya Borjigin, Cong He",
        "summary": "We present a cross-market algorithmic trading system that balances execution quality with rigorous compliance enforcement. The architecture comprises a high-level planner, a reinforcement learning execution agent, and an independent compliance agent. We formulate trade execution as a constrained Markov decision process with hard constraints on participation limits, price bands, and self-trading avoidance. The execution agent is trained with proximal policy optimization, while a runtime action-shield projects any unsafe action into a feasible set. To support auditability without exposing proprietary signals, we add a zero-knowledge compliance audit layer that produces cryptographic proofs that all actions satisfied the constraints. We evaluate in a multi-venue, ABIDES-based simulator and compare against standard baselines (e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and variance while exhibiting no observed constraint violations across stress scenarios including elevated latency, partial fills, compliance module toggling, and varying constraint limits. We report effects at the 95% confidence level using paired t-tests and examine tail risk via CVaR. We situate the work at the intersection of optimal execution, safe reinforcement learning, regulatory technology, and verifiable AI, and discuss ethical considerations, limitations (e.g., modeling assumptions and computational overhead), and paths to real-world deployment.",
        "subjects": "Artificial Intelligence, Distributed, Parallel, and Cluster Computing",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.129380"
    },
    {
        "index": "#8",
        "title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding",
        "link": "/arxiv/2510.04899",
        "arxiv_id": "2510.04899",
        "authors": "Keane Ong, Wei Dai, Carol Li, Dewei Feng, Hengzhi Li, Jingyao Wu, Jiaee Cheong, Rui Mao, Gianmarco Mengaldo, Erik Cambria, Paul Pu Liang",
        "summary": "Using intelligent systems to perceive psychological and social behaviors, that is, the underlying affective, cognitive, and pathological states that are manifested through observable behaviors and social interactions, remains a challenge due to their complex, multifaceted, and personalized nature. Existing work tackling these dimensions through specialized datasets and single-task systems often miss opportunities for scalability, cross-task transfer, and broader generalization. To address this gap, we curate Human Behavior Atlas, a unified benchmark of diverse behavioral tasks designed to support the development of unified models for understanding psychological and social behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text, audio, and visual modalities, covering tasks on affective states, cognitive states, pathologies, and social processes. Our unification efforts can reduce redundancy and cost, enable training to scale efficiently across tasks, and enhance generalization of behavioral features across domains. On Human Behavior Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models to consistently outperform existing multimodal LLMs across diverse behavioral tasks. Pretraining on Human Behavior Atlas also improves transfer to novel behavioral datasets; with the targeted use of behavioral descriptors yielding meaningful performance gains.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.136235"
    },
    {
        "index": "#12",
        "title": "Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning",
        "link": "/arxiv/2510.04817",
        "arxiv_id": "2510.04817",
        "authors": "Abhinav Madahar",
        "summary": "Controllers for structured LM reasoning (e.g., Chain-of-Thought, self-consistency, and Tree-of-Thoughts) often entangle what to try next with how to execute it, exposing only coarse global knobs and yielding brittle, compute-inefficient, and hard-to-audit behavior. We introduce Natural Language Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form natural-language directive to each search edge and translates it into a schema-bounded control vector for decoding, search (branch quotas, exploration $\\beta$), generation bundle size, retrieval mixtures, and verification passes. A labeller $\\Lambda$ emits labels from the parent state and a compact context; a tuner $\\Psi$ maps $(P, L, C)\\to \\Pi$, with strict schema validation and trust-region projection around safe defaults. Downstream selection remains ToT-style with score $S=\\mu+\\beta\\sigma$ and depth-annealed $\\beta$. We show NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for top-$k$ selection under label-conditioned bundles, and bound selector shortfall by control-vector distortion, providing decision-relevant justification for guards like trust regions and verification passes. We instantiate $\\Psi$ as a prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH (subset), StrategyQA, and ARC-Challenge with compute-aware reporting (success@compute, tokens-per-success) and ablations over $\\Lambda$, $\\Psi$, trust-region radius, and control quantization; preregistered forecasts anticipate accuracy gains at comparable token budgets and improved success@compute under constraints. NLEL offers an interpretable, model-agnostic interface that separates intent from execution for controllable, auditable LM inference.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.138853"
    },
    {
        "index": "#13",
        "title": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems",
        "link": "/arxiv/2510.04792",
        "arxiv_id": "2510.04792",
        "authors": "Ni Zhang, Zhiguang Cao",
        "summary": "Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically employ Trajectory Balance (TB) to achieve global optimization but often neglect important aspects of local optimization. While Detailed Balance (DB) addresses local optimization more effectively, it alone falls short in solving VRPs, which inherently require holistic trajectory optimization. To address these limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which uniquely integrates TB and DB in a principled and adaptive manner by aligning their intrinsically complementary strengths. Additionally, we propose a specialized inference strategy for depot-centric scenarios like the Capacitated Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility in selecting successors. Despite this specialization, HBG maintains broad applicability, extending effectively to problems without explicit depots, such as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate consistent and significant improvements across both CVRP and TSP, underscoring the enhanced solution quality and generalization afforded by our approach.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.139424"
    },
    {
        "index": "#14",
        "title": "LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0",
        "link": "/arxiv/2510.04765",
        "arxiv_id": "2510.04765",
        "authors": "Jinbo Wen, Jiawen Kang, Linfeng Zhang, Xiaoying Tang, Jianhang Tang, Yang Zhang, Zhaohui Yang, Dusit Niyato",
        "summary": "Web 3.0 represents the next generation of the Internet, which is widely recognized as a decentralized ecosystem that focuses on value expression and data ownership. By leveraging blockchain and artificial intelligence technologies, Web 3.0 offers unprecedented opportunities for users to create, own, and monetize their content, thereby enabling User-Generated Content (UGC) to an entirely new level. However, some self-interested users may exploit the limitations of content curation mechanisms and generate low-quality content with less effort, obtaining platform rewards under information asymmetry. Such behavior can undermine Web 3.0 performance. To this end, we propose \\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based contract-theoretic model to motivate users to generate high-quality UGC, thereby mitigating the adverse selection problem from information asymmetry. To alleviate potential moral hazards after contract selection, we leverage LMM agents to evaluate UGC quality, which is the primary component of the contract, utilizing prompt engineering techniques to improve the evaluation performance of LMM agents. Recognizing that traditional contract design methods cannot effectively adapt to the dynamic environment of Web 3.0, we develop an improved Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for optimal contract design. Simulation results demonstrate the superiority of the proposed MoE-based PPO algorithm over representative benchmarks in the context of contract design. Finally, we deploy the designed contract within an Ethereum smart contract framework, further validating the effectiveness of the proposed scheme.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.145254"
    },
    {
        "index": "#16",
        "title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents",
        "link": "/arxiv/2510.04695",
        "arxiv_id": "2510.04695",
        "authors": "Yiding Wang, Zhepei Wei, Xinyu Zhu, Yu Meng",
        "summary": "Enabling large language models (LLMs) to utilize search tools offers a promising path to overcoming fundamental limitations such as knowledge cutoffs and hallucinations. Recent work has explored reinforcement learning (RL) for training search-augmented agents that interleave reasoning and retrieval before answering. These approaches usually rely on outcome-based rewards (e.g., exact match), implicitly assuming that optimizing for final answers will also yield effective intermediate search behaviors. Our analysis challenges this assumption: we uncover multiple systematic deficiencies in search that arise under outcome-only training and ultimately degrade final answer quality, including failure to invoke tools, invalid queries, and redundant searches. To address these shortcomings, we introduce DeSA (Decoupling Search-and-Answering), a simple two-stage training framework that explicitly separates search optimization from answer generation. In Stage 1, agents are trained to improve search effectiveness with retrieval recall-based rewards. In Stage 2, outcome rewards are employed to optimize final answer generation. Across seven QA benchmarks, DeSA-trained agents consistently improve search behaviors, delivering substantially higher search recall and answer accuracy than outcome-only baselines. Notably, DeSA outperforms single-stage training approaches that simultaneously optimize recall and outcome rewards, underscoring the necessity of explicitly decoupling the two objectives.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.146530"
    },
    {
        "index": "#17",
        "title": "Watch and Learn: Learning to Use Computers from Online Videos",
        "link": "/arxiv/2510.04673",
        "arxiv_id": "2510.04673",
        "authors": "Chan Hee Song, Yiwen Song, Palash Goyal, Yu Su, Oriana Riva, Hamid Palangi, Tomas Pfister",
        "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse, ever-changing applications and environments, but learning is hindered by the scarcity of large-scale, high-quality training data in the target application. Existing datasets are domain-specific, static, and costly to annotate, while current synthetic data generation methods often yield simplistic or misaligned task demonstrations. To address these limitations, we introduce Watch & Learn (W&L), a framework that converts human demonstration videos readily available on the Internet into executable UI trajectories at scale. Instead of directly generating trajectories or relying on ad hoc reasoning heuristics, we cast the problem as an inverse dynamics objective: predicting the user's action from consecutive screen states. This formulation reduces manual engineering, is easier to learn, and generalizes more robustly across applications. Concretely, we develop an inverse dynamics labeling pipeline with task-aware video retrieval, generate over 53k high-quality trajectories from raw web videos, and demonstrate that these trajectories improve CUAs both as in-context demonstrations and as supervised training data. On the challenging OSWorld benchmark, UI trajectories extracted with W&L consistently enhance both general-purpose and state-of-the-art frameworks in-context, and deliver stronger gains for open-source models under supervised training. These results highlight web-scale human demonstration videos as a practical and scalable foundation for advancing CUAs towards real-world deployment.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.147242"
    },
    {
        "index": "#18",
        "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing",
        "link": "/arxiv/2510.04670",
        "arxiv_id": "2510.04670",
        "authors": "Xuanhua Yin, Runkai Zhao, Weidong Cai",
        "summary": "Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic Framework for Multimodal fMRI Response Encoding), an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating. Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from upstream fusion, while MIND combines token-dependent Top-K sparse routing with a subject prior to personalize expert usage without sacrificing generality. Experiments across multiple multimodal backbones and subjects show consistent improvements over strong baselines, enhanced cross-subject generalization, and interpretable expert patterns that correlate with content type. The framework offers a simple attachment point for new encoders and datasets, enabling robust, plug-and-improve performance for naturalistic neuroimaging studies.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.147851"
    },
    {
        "index": "#19",
        "title": "QuantAgents: Towards Multi-agent Financial System via Simulated Trading",
        "link": "/arxiv/2510.04643",
        "arxiv_id": "2510.04643",
        "authors": "Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu",
        "summary": "In this paper, our objective is to develop a multi-agent financial system that incorporates simulated trading, a technique extensively utilized by financial professionals. While current LLM-based agent models demonstrate competitive performance, they still exhibit significant deviations from real-world fund companies. A critical distinction lies in the agents' reliance on ``post-reflection'', particularly in response to adverse outcomes, but lack a distinctly human capability: long-term prediction of future trends. Therefore, we introduce QuantAgents, a multi-agent system integrating simulated trading, to comprehensively evaluate various investment strategies and market scenarios without assuming actual risks. Specifically, QuantAgents comprises four agents: a simulated trading analyst, a risk control analyst, a market news analyst, and a manager, who collaborate through several meetings. Moreover, our system incentivizes agents to receive feedback on two fronts: performance in real-world markets and predictive accuracy in simulated trading. Extensive experiments demonstrate that our framework excels across all metrics, yielding an overall return of nearly 300% over the three years (https://quantagents.github.io/).",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.148484"
    },
    {
        "index": "#20",
        "title": "MedPAO: A Protocol-Driven Agent for Structuring Medical Reports",
        "link": "/arxiv/2510.04623",
        "arxiv_id": "2510.04623",
        "authors": "Shrish Shrinath Vaidya, Gowthamaan Palani, Sidharth Ramesh, Velmurugan Balasubramanian, Minmini Selvam, Gokulraja Srinivasaraja, Ganapathy Krishnamurthi",
        "summary": "The deployment of Large Language Models (LLMs) for structuring clinical data is critically hindered by their tendency to hallucinate facts and their inability to follow domain-specific rules. To address this, we introduce MedPAO, a novel agentic framework that ensures accuracy and verifiable reasoning by grounding its operation in established clinical protocols such as the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring task into a transparent process managed by a Plan-Act-Observe (PAO) loop and specialized tools. This protocol-driven method provides a verifiable alternative to opaque, monolithic models. The efficacy of our approach is demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96 on the critical sub-task of concept categorization. Notably, expert radiologists and clinicians rated the final structured outputs with an average score of 4.52 out of 5, indicating a level of reliability that surpasses baseline approaches relying solely on LLM-based foundation models. The code is available at: https://github.com/MiRL-IITM/medpao-agent",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.149167"
    },
    {
        "index": "#21",
        "title": "Making Mathematical Reasoning Adaptive",
        "link": "/arxiv/2510.04617",
        "arxiv_id": "2510.04617",
        "authors": "Zhejian Lai, Xiang Geng, Zhijun Wang, Yang Bai, Jiahuan Li, Rongxiang Weng, Jingang Wang, Xuezhi Cao, Xunliang Cai, Shujian Huang",
        "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. However, existing LLMs exhibit failures of robustness and generalization. This paper attributes these deficiencies to spurious reasoning, i.e., producing answers from superficial features. To address this challenge, we propose the AdaR framework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers. AdaR synthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. To improve data quality, we extract the problem-solving logic from the original query and generate the corresponding answer by code execution, then apply a sanity check. Experimental results demonstrate that AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis indicates that data synthesis and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs. Subsequent analyses derive key design insights into the effect of critical factors and the applicability to instruct LLMs. Our project is available at https://github.com/LaiZhejian/AdaR",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.149907"
    },
    {
        "index": "#22",
        "title": "Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma",
        "link": "/arxiv/2510.04588",
        "arxiv_id": "2510.04588",
        "authors": "Shurui Li",
        "summary": "Rapid advances in artificial intelligence necessitate a re-examination of the epistemological foundations upon which we attribute consciousness. As AI systems increasingly mimic human behavior and interaction with high fidelity, the concept of a \"perfect mimic\"-an entity empirically indistinguishable from a human through observation and interaction-shifts from hypothetical to technologically plausible. This paper argues that such developments pose a fundamental challenge to the consistency of our mind-recognition practices. Consciousness attributions rely heavily, if not exclusively, on empirical evidence derived from behavior and interaction. If a perfect mimic provides evidence identical to that of humans, any refusal to grant it equivalent epistemic status must invoke inaccessible factors, such as qualia, substrate requirements, or origin. Selectively invoking such factors risks a debilitating dilemma: either we undermine the rational basis for attributing consciousness to others (epistemological solipsism), or we accept inconsistent reasoning. I contend that epistemic consistency demands we ascribe the same status to empirically indistinguishable entities, regardless of metaphysical assumptions. The perfect mimic thus acts as an epistemic mirror, forcing critical reflection on the assumptions underlying intersubjective recognition in light of advancing AI. This analysis carries significant implications for theories of consciousness and ethical frameworks concerning artificial agents.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.155585"
    },
    {
        "index": "#23",
        "title": "Strongly Solving 2048 4x3",
        "link": "/arxiv/2510.04580",
        "arxiv_id": "2510.04580",
        "authors": "Tomoyuki Kaneko, Shuhei Yamashita",
        "summary": "2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid, where a player chooses a direction among up, down, left, and right to obtain a score by merging two tiles with the same number located in neighboring cells along the chosen direction. This paper presents that a variant 2048-4x3 12 cells on a 4 by 3 board, one row smaller than the original, has been strongly solved. In this variant, the expected score achieved by an optimal strategy is about $50724.26$ for the most common initial states: ones with two tiles of number 2. The numbers of reachable states and afterstates are identified to be $1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is to partition state space by the sum of tile numbers on a board, which we call the age of a state. An age is invariant between a state and its successive afterstate after any valid action and is increased two or four by stochastic response from the environment. Therefore, we can partition state space by ages and enumerate all (after)states of an age depending only on states with the recent ages. Similarly, we can identify (after)state values by going along with ages in decreasing order.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.156167"
    },
    {
        "index": "#24",
        "title": "COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context",
        "link": "/arxiv/2510.04568",
        "arxiv_id": "2510.04568",
        "authors": "Naman Gupta, Shreeyash Gowaikar, Arun Iyer, Kirankumar Shiragur, Ramakrishna B Bairi, Rishikesh Maurya, Ritabrata Maiti, Sankarshan Damle, Shachee Mishra Gupta",
        "summary": "Reasoning over very long inputs remains difficult for large language models (LLMs). Common workarounds either shrink the input via retrieval (risking missed evidence), enlarge the context window (straining selectivity), or stage multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents, CoA), free-form summaries passed between agents can discard crucial details and amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc messages with a structured memory. A Planner agent first turns a user query into concrete, checkable sub-questions. worker agents process chunks via a fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared memory. A Manager agent then Synthesizes the final answer directly from the memory. This preserves step-wise read-then-reason benefits while changing both the communication medium (structured memory) and the worker procedure (fixed micro-cycle), yielding higher faithfulness, better long-range aggregation, and auditability. On long-context QA from the HELMET suite, COSMIR reduces propagation-stage information loss and improves accuracy over a CoA baseline.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.156898"
    },
    {
        "index": "#25",
        "title": "ContextNav: Towards Agentic Multimodal In-Context Learning",
        "link": "/arxiv/2510.04560",
        "arxiv_id": "2510.04560",
        "authors": "Honghao Fu, Yuan Ouyang, Kai-Wei Chang, Yiwei Wang, Zi Huang, Yujun Cai",
        "summary": "Recent advances demonstrate that multimodal large language models (MLLMs) exhibit strong multimodal in-context learning (ICL) capabilities, enabling them to adapt to novel vision-language tasks from a few contextual examples. However, existing ICL approaches face challenges in reconciling scalability with robustness across diverse tasks and noisy contextual examples: manually selecting examples produces clean contexts but is labor-intensive and task-specific, while similarity-based retrieval improves scalability but could introduce irrelevant or structurally inconsistent samples that degrade ICL performance. To address these limitations, we propose ContextNav, the first agentic framework that integrates the scalability of automated retrieval with the quality and adaptiveness of human-like curation, enabling noise-robust and dynamically optimized contextualization for multimodal ICL. ContextNav unifies context management and noise-robust contextualization within a closed-loop workflow driven by graph-based orchestration. Specifically, it builds a resource-aware multimodal embedding pipeline, maintains a retrievable vector database, and applies agentic retrieval and structural alignment to construct noise-resilient contexts. An Operational Grammar Graph (OGG) further supports adaptive workflow planning and optimization, enabling the agent to refine its operational strategies based on downstream ICL feedback. Experimental results demonstrate that ContextNav achieves state-of-the-art performance across various datasets, underscoring the promise of agentic workflows for advancing scalable and robust contextualization in multimodal ICL.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.157552"
    },
    {
        "index": "#26",
        "title": "TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use",
        "link": "/arxiv/2510.04550",
        "arxiv_id": "2510.04550",
        "authors": "Pengfei He, Zhenwei Dai, Bing He, Hui Liu, Xianfeng Tang, Hanqing Lu, Juanhui Li, Jiayuan Ding, Subhabrata Mukherjee, Suhang Wang, Yue Xing, Jiliang Tang, Benoit Dumoulin",
        "summary": "Large language model (LLM)-based agents increasingly rely on tool use to complete real-world tasks. While existing works evaluate the LLMs' tool use capability, they largely focus on the final answers yet overlook the detailed tool usage trajectory, i.e., whether tools are selected, parameterized, and ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to comprehensively evaluate LLMs' tool use capability through diverse tasks with fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable tools across practical domains with tasks grounded in production-style APIs, and synthesizes trajectories that vary in breadth (parallel calls) and depth (interdependent chains). Besides final accuracy, TRAJECT-Bench also reports trajectory-level diagnostics, including tool selection and argument correctness, and dependency/order satisfaction. Analyses reveal failure modes such as similar tool confusion and parameter-blind selection, and scaling behavior with tool diversity and trajectory length where the bottleneck of transiting from short to mid-length trajectories is revealed, offering actionable guidance for LLMs' tool use.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.158332"
    },
    {
        "index": "#27",
        "title": "Code World Models for General Game Playing",
        "link": "/arxiv/2510.04542",
        "arxiv_id": "2510.04542",
        "authors": "Wolfgang Lehrach, Daniel Hennes, Miguel Lazaro-Gredilla, Xinghua Lou, Carter Wendelken, Zun Li, Antoine Dedieu, Jordi Grau-Moya, Marc Lanctot, Atil Iscen, John Schultz, Marcus Chiam, Ian Gemp, Piotr Zielinski, Satinder Singh, Kevin P. Murphy",
        "summary": "Large Language Models (LLMs) reasoning abilities are increasingly being applied to classical board and card games, but the dominant approach -- involving prompting for direct move generation -- has significant drawbacks. It relies on the model's implicit fragile pattern-matching capabilities, leading to frequent illegal moves and strategically shallow play. Here we introduce an alternative approach: We use the LLM to translate natural language rules and game trajectories into a formal, executable world model represented as Python code. This generated model -- comprising functions for state transition, legal move enumeration, and termination checks -- serves as a verifiable simulation engine for high-performance planning algorithms like Monte Carlo tree search (MCTS). In addition, we prompt the LLM to generate heuristic value functions (to make MCTS more efficient), and inference functions (to estimate hidden states in imperfect information games). Our method offers three distinct advantages compared to directly using the LLM as a policy: (1) Verifiability: The generated CWM serves as a formal specification of the game's rules, allowing planners to algorithmically enumerate valid actions and avoid illegal moves, contingent on the correctness of the synthesized model; (2) Strategic Depth: We combine LLM semantic understanding with the deep search power of classical planners; and (3) Generalization: We direct the LLM to focus on the meta-task of data-to-code translation, enabling it to adapt to new games more easily. We evaluate our agent on 10 different games, of which 4 are novel and created for this paper. 5 of the games are fully observed (perfect information), and 5 are partially observed (imperfect information). We find that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10 considered games.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.159175"
    },
    {
        "index": "#29",
        "title": "Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph",
        "link": "/arxiv/2510.04520",
        "arxiv_id": "2510.04520",
        "authors": "Hanyu Wang, Ruohan Xie, Yutong Wang, Guoxiong Gao, Xintao Yu, Bin Dong",
        "summary": "Accurate auto-formalization of theorem statements is essential for advancing automated discovery and verification of research-level mathematics, yet remains a major bottleneck for LLMs due to hallucinations, semantic mismatches, and their inability to synthesize new definitions. To tackle these issues, we present Aria (Agent for Retrieval and Iterative Autoformalization), a system for conjecture-level formalization in Lean that emulates human expert reasoning via a two-phase Graph-of-Thought process: recursively decomposing statements into a dependency graph and then constructing formalizations from grounded concepts. To ensure semantic correctness, we introduce AriaScorer, a checker that retrieves definitions from Mathlib for term-level grounding, enabling rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy, surpassing previous methods. On FATE-X, a suite of challenging algebra problems from research literature, it outperforms the best baseline with 44.0% vs. 24.0% final accuracy. On a dataset of homological conjectures, Aria reaches 42.9% final accuracy while all other models score 0%.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.165651"
    },
    {
        "index": "#32",
        "title": "Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning",
        "link": "/arxiv/2510.04488",
        "arxiv_id": "2510.04488",
        "authors": "Edward Y. Chang, Ethan Y. Chang",
        "summary": "Multi-agent debate often wastes compute by using a fixed adversarial stance, aggregating without deliberation, or stopping on heuristics. We introduce MACI, an active controller with two independent dials that decouple information from behavior: an information dial that gates evidence by quality, and a behavior dial that schedules contentiousness from exploration to consolidation. A moderator tracks disagreement, overlap, evidence quality, and argument quality, and halts when gains plateau. We provide theory-lite guarantees for nonincreasing dispersion and provable termination, with a budget-feasible scheduler. Across clinical diagnosis and news-bias tasks, MACI improves accuracy and calibration while reducing tokens, and converts residual uncertainty into precision RAG plans that specify what to retrieve next. We use a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal, validated for order invariance and judge-swap stability; stability depends on using high-capability judges. MACI turns debate into a budget-aware, measurable, and provably terminating controller.",
        "subjects": "Artificial Intelligence, Information Theory",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.167694"
    },
    {
        "index": "#33",
        "title": "On Continuous Optimization for Constraint Satisfaction Problems",
        "link": "/arxiv/2510.04480",
        "arxiv_id": "2510.04480",
        "authors": "Yunuo Cen, Zixuan Wang, Jintao Zhang, Zhiwei Zhang, Xuanyao Fong",
        "summary": "Constraint satisfaction problems (CSPs) are fundamental in mathematics, physics, and theoretical computer science. While conflict-driven clause learning Boolean Satisfiability (SAT) solvers have achieved remarkable success and become the mainstream approach for Boolean satisfiability, recent advances show that modern continuous local search (CLS) solvers can achieve highly competitive results on certain classes of SAT problems. Motivated by these advances, we extend the CLS framework from Boolean SAT to general CSP with finite-domain variables and expressive constraints. We present FourierCSP, a continuous optimization framework that generalizes the Walsh-Fourier transform to CSP, allowing for transforming versatile constraints to compact multilinear polynomials, thereby avoiding the need for auxiliary variables and memory-intensive encodings. Our approach leverages efficient evaluation and differentiation of the objective via circuit-output probability and employs a projected gradient optimization method with theoretical guarantees. Empirical results on benchmark suites demonstrate that FourierCSP is scalable and competitive, significantly broadening the class of problems that can be efficiently solved by CLS techniques.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.168353"
    },
    {
        "index": "#34",
        "title": "DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization",
        "link": "/arxiv/2510.04474",
        "arxiv_id": "2510.04474",
        "authors": "Gang Li, Yan Chen, Ming Lin, Tianbao Yang",
        "summary": "Recent large reasoning models (LRMs) driven by reinforcement learning algorithms (e.g., GRPO) have achieved remarkable performance on challenging reasoning tasks. However, these models suffer from overthinking, generating unnecessarily long and redundant reasoning even for simple questions, which substantially increases computational cost and response latency. While existing methods incorporate length rewards to GRPO to promote concise reasoning, they incur significant performance degradation. We identify the root cause: when rewards for correct but long rollouts are penalized, GRPO's group-relative advantage function can assign them negative advantages, actively discouraging valid reasoning. To overcome this, we propose Decoupled Reward Policy Optimization (DRPO), a novel framework that decouples the length-based learning signal of correct rollouts from incorrect ones. DRPO ensures that reward signals for correct rollouts are normalized solely within the positive group, shielding them from interference by negative samples. The DRPO's objective is grounded in integrating an optimized positive data distribution, which maximizes length-based rewards under a KL regularization, into a discriminative objective. We derive a closed-form solution for this distribution, enabling efficient computation of the objective and its gradients using only on-policy data and importance weighting. Of independent interest, this formulation is general and can incorporate other preference rewards of positive data beyond length. Experiments on mathematical reasoning tasks demonstrate DRPO's significant superiority over six efficient reasoning baselines. Notably, with a 1.5B model, our method achieves 77\\% length reduction with only 1.1\\% performance loss on simple questions like GSM8k dataset, while the follow-up baseline sacrifices 4.3\\% for 68\\% length reduction.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.168997"
    },
    {
        "index": "#35",
        "title": "Utility-Learning Tension in Self-Modifying Agents",
        "link": "/arxiv/2510.04399",
        "arxiv_id": "2510.04399",
        "authors": "Charles L. Wang, Keir Dorchen, Peter Jin",
        "summary": "As systems trend toward superintelligence, a natural modeling premise is that agents can self-improve along every facet of their own design. We formalize this with a five-axis decomposition and a decision layer, separating incentives from learning behavior and analyzing axes in isolation. Our central result identifies and introduces a sharp utility--learning tension, the structural conflict in self-modifying systems whereby utility-driven changes that improve immediate or expected performance can also erode the statistical preconditions for reliable learning and generalization. Our findings show that distribution-free guarantees are preserved iff the policy-reachable model family is uniformly capacity-bounded; when capacity can grow without limit, utility-rational self-changes can render learnable tasks unlearnable. Under standard assumptions common in practice, these axes reduce to the same capacity criterion, yielding a single boundary for safe self-modification. Numerical experiments across several axes validate the theory by comparing destructive utility policies against our proposed two-gate policies that preserve learnability.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.169631"
    },
    {
        "index": "#37",
        "title": "LLM Based Bayesian Optimization for Prompt Search",
        "link": "/arxiv/2510.04384",
        "arxiv_id": "2510.04384",
        "authors": "Adam Ballew, Jingbo Wang, Shaogang Ren",
        "summary": "Bayesian Optimization (BO) has been widely used to efficiently optimize expensive black-box functions with limited evaluations. In this paper, we investigate the use of BO for prompt engineering to enhance text classification with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process (GP) as the surrogate model to estimate the performance of different prompt candidates. These candidates are generated by an LLM through the expansion of a set of seed prompts and are subsequently evaluated using an Upper Confidence Bound (UCB) acquisition function in conjunction with the GP posterior. The optimization process iteratively refines the prompts based on a subset of the data, aiming to improve classification accuracy while reducing the number of API calls by leveraging the prediction uncertainty of the LLM-based GP. The proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are discussed in detail in this paper.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.176021"
    },
    {
        "index": "#38",
        "title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation",
        "link": "/arxiv/2510.04373",
        "arxiv_id": "2510.04373",
        "authors": "Hadi Nekoei, Aman Jaiswal, Patrice Bechard, Oleh Shliazhko, Orlando Marquez Ayala, Mathieu Reymond, Massimo Caccia, Alexandre Drouin, Sarath Chandar, Alexandre Lacoste",
        "summary": "Large language model (LLM) agents perform well in sequential decision-making tasks, but improving them on unfamiliar domains often requires costly online interactions or fine-tuning on large expert datasets. These strategies are impractical for closed-source models and expensive for open-source ones, with risks of catastrophic forgetting. Offline trajectories offer reusable knowledge, yet demonstration-based methods struggle because raw traces are long, noisy, and tied to specific tasks. We present Just-in-time Episodic Feedback Hinter (JEF Hinter), an agentic system that distills offline traces into compact, context-aware hints. A zooming mechanism highlights decisive steps in long trajectories, capturing both strategies and pitfalls. Unlike prior methods, JEF Hinter leverages both successful and failed trajectories, extracting guidance even when only failure data is available, while supporting parallelized hint generation and benchmark-independent prompting. At inference, a retriever selects relevant hints for the current state, providing targeted guidance with transparency and traceability. Experiments on MiniWoB++, WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms strong baselines, including human- and document-based hints.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.176762"
    },
    {
        "index": "#40",
        "title": "On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems",
        "link": "/arxiv/2510.04311",
        "arxiv_id": "2510.04311",
        "authors": "Bohan Tang, Huidong Liang, Keyue Jiang, Xiaowen Dong",
        "summary": "Large language model multi-agent systems (LLM-MAS) offer a promising paradigm for harnessing collective intelligence to achieve more advanced forms of AI behaviour. While recent studies suggest that LLM-MAS can outperform LLM single-agent systems (LLM-SAS) on certain tasks, the lack of systematic experimental designs limits the strength and generality of these conclusions. We argue that a principled understanding of task complexity, such as the degree of sequential reasoning required and the breadth of capabilities involved, is essential for assessing the effectiveness of LLM-MAS in task solving. To this end, we propose a theoretical framework characterising tasks along two dimensions: depth, representing reasoning length, and width, representing capability diversity. We theoretically examine a representative class of LLM-MAS, namely the multi-agent debate system, and empirically evaluate its performance in both discriminative and generative tasks with varying depth and width. Theoretical and empirical results show that the benefit of LLM-MAS over LLM-SAS increases with both task depth and width, and the effect is more pronounced with respect to depth. This clarifies when LLM-MAS are beneficial and provides a principled foundation for designing future LLM-MAS methods and benchmarks.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.178128"
    },
    {
        "index": "#41",
        "title": "Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning",
        "link": "/arxiv/2510.04284",
        "arxiv_id": "2510.04284",
        "authors": "Yunghwei Lai, Kaiming Liu, Ziyue Wang, Weizhi Ma, Yang Liu",
        "summary": "The professionalism of a human doctor in outpatient service depends on two core abilities: the ability to make accurate medical decisions and the medical consultation skill to conduct strategic, empathetic patient inquiry. Existing Large Language Models (LLMs) have achieved remarkable accuracy on medical decision-making benchmarks. However, they often lack the ability to conduct the strategic and empathetic consultation, which is essential for real-world clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor agent trained to master both of the capabilities by ask high-yield questions and conduct strategic multi-turn inquiry to guide decision-making. Our framework introduces three key components: a multi-agent interactive environment, a two-tiered reward architecture that separately optimizes clinical decision-making and communicative inquiry skills, and an experience repository to ground policy learning in high-quality prior trajectories. We evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across multi-facet metrics, such as communication quality, user experience, and task accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source specialized LLMs by a substantial margin with higher parameter efficiency and outperforms powerful proprietary models. Furthermore, the human evaluations show a strong preference for Doctor-R1 to generate human-preferred clinical dialogue, demonstrating the effectiveness of the framework.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.178764"
    },
    {
        "index": "#42",
        "title": "GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction",
        "link": "/arxiv/2510.04281",
        "arxiv_id": "2510.04281",
        "authors": "Zhuangzhi Gao, Hongyi Qin, He Zhao, Qinkai Yu, Feixiang Zhou, Eduard Shantsila, Uazman Alam, Alena Shantsila, Wahbi El-Bouri, Gregory Y. H. Lip, Yalin Zheng",
        "summary": "Multimodal large language models (MLLMs) hold promise for integrating diverse data modalities, but current medical adaptations such as LLaVA-Med often fail to fully exploit the synergy between color fundus photography (CFP) and optical coherence tomography (OCT), and offer limited interpretability of quantitative biomarkers. We introduce GROK, a grounded multimodal large language model that jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of ocular and systemic disease. GROK comprises three core modules: Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment, and Supervised Instruction Fine-Tuning, which together establish a quantitative-to-qualitative diagnostic chain of thought, mirroring real clinical reasoning when producing detailed lesion annotations. To evaluate our approach, we introduce the Grounded Ophthalmic Understanding benchmark, which covers six disease categories and three tasks: macro-level diagnostic classification, report generation quality, and fine-grained clinical assessment of the generated chain of thought. Experiments show that, with only LoRA (Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK outperforms comparable 7B and 32B baselines on both report quality and fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are publicly available in the GROK repository.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.179521"
    },
    {
        "index": "#43",
        "title": "Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales",
        "link": "/arxiv/2510.04272",
        "arxiv_id": "2510.04272",
        "authors": "Jinyang Jiang, Jinhui Han, Yijie Peng, Ying Zhang",
        "summary": "Effective cross-functional coordination is essential for enhancing firm-wide profitability, particularly in the face of growing organizational complexity and scale. Recent advances in artificial intelligence, especially in reinforcement learning (RL), offer promising avenues to address this fundamental challenge. This paper proposes a unified multi-agent RL framework tailored for joint optimization across distinct functional modules, exemplified via coordinating inventory replenishment and personalized product recommendation. We first develop an integrated theoretical model to capture the intricate interplay between these functions and derive analytical benchmarks that characterize optimal coordination. The analysis reveals synchronized adjustment patterns across products and over time, highlighting the importance of coordinated decision-making. Leveraging these insights, we design a novel multi-timescale multi-agent RL architecture that decomposes policy components according to departmental functions and assigns distinct learning speeds based on task complexity and responsiveness. Our model-free multi-agent design improves scalability and deployment flexibility, while multi-timescale updates enhance convergence stability and adaptability across heterogeneous decisions. We further establish the asymptotic convergence of the proposed algorithm. Extensive simulation experiments demonstrate that the proposed approach significantly improves profitability relative to siloed decision-making frameworks, while the behaviors of the trained RL agents align closely with the managerial insights from our theoretical model. Taken together, this work provides a scalable, interpretable RL-based solution to enable effective cross-functional coordination in complex business settings.",
        "subjects": "Artificial Intelligence, Machine Learning, Optimization and Control",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.180216"
    },
    {
        "index": "#45",
        "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework",
        "link": "/arxiv/2510.04206",
        "arxiv_id": "2510.04206",
        "authors": "Hanchen Zhang, Xiao Liu, Bowen Lv, Xueqiao Sun, Bohao Jing, Iat Long Iong, Zhenyu Hou, Zehan Qi, Hanyu Lai, Yifan Xu, Rui Lu, Hongning Wang, Jie Tang, Yuxiao Dong",
        "summary": "Recent advances in large language models (LLMs) have sparked growing interest in building generalist agents that can learn through online interactions. However, applying reinforcement learning (RL) to train LLM agents in multi-turn, multi-task settings remains challenging due to lack of scalable infrastructure and stable training algorithms. In this work, we present the AgentRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AgentRL features a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, we design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. Experiments show that AgentRL, trained on open LLMs across five agentic tasks, significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents. Multi-task training with AgentRL matches the best results among all task-specific models. AgentRL is open-sourced at https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in building \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.186957"
    },
    {
        "index": "#46",
        "title": "COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability",
        "link": "/arxiv/2510.04196",
        "arxiv_id": "2510.04196",
        "authors": "Yizhuo Ding, Mingkang Chen, Qiuhua Liu, Fenghua Weng, Wanying Qu, Yue Yang, Yugang Jiang, Zuxuan Wu, Yanwei Fu, Wenqi Shao",
        "summary": "Large Multimodal Reasoning Models (LMRMs) are moving into real applications, where they must be both useful and safe. Safety is especially challenging in multimodal settings: images and text can be combined to bypass guardrails, and single objective training can cause policy drift that yields over-refusal on benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed reinforcement learning framework that trains reasoning oriented LMRMs under multimodal, multitask, and multiobjective signals, and we release the resulting model, COSMO-R1. Our approach aims to let safety and capability grow together in one stable pipeline rather than competing during alignment. In experiments, COSMO-R1 improves safety while maintaining-and often improving multimodal reasoning and instruction following, shows stronger robustness to multimodal jailbreaks, and reduces unnecessary refusals. The framework also transfers across backbones with consistent gains. Ablations support the design choices, indicating a simple path to advancing safety and general capability together in LMRMs.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.187739"
    },
    {
        "index": "#47",
        "title": "Constructing coherent spatial memory in LLM agents through graph rectification",
        "link": "/arxiv/2510.04195",
        "arxiv_id": "2510.04195",
        "authors": "Puzhen Zhang, Xuyang Chen, Yu Feng, Yuhan Jiang, Liqiu Meng",
        "summary": "Given a map description through global traversal navigation instructions (e.g., visiting each room sequentially with action signals such as north, west, etc.), an LLM can often infer the implicit spatial layout of the environment and answer user queries by providing a shortest path from a start to a destination (for instance, navigating from the lobby to a meeting room via the hall and elevator). However, such context-dependent querying becomes incapable as the environment grows much longer, motivating the need for incremental map construction that builds a complete topological graph from stepwise observations. We propose a framework for LLM-driven construction and map repair, designed to detect, localize, and correct structural inconsistencies in incrementally constructed navigation graphs. Central to our method is the Version Control, which records the full history of graph edits and their source observations, enabling fine-grained rollback, conflict tracing, and repair evaluation. We further introduce an Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation. To properly evaluate our approach, we create a refined version of the MANGO benchmark dataset by systematically removing non-topological actions and inherent structural conflicts, providing a cleaner testbed for LLM-driven construction and map repair. Our approach significantly improves map correctness and robustness, especially in scenarios with entangled or chained inconsistencies. Our results highlight the importance of introspective, history-aware repair mechanisms for maintaining coherent spatial memory in LLM agents.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.188394"
    },
    {
        "index": "#48",
        "title": "Open Agent Specification (Agent Spec) Technical Report",
        "link": "/arxiv/2510.04173",
        "arxiv_id": "2510.04173",
        "authors": "Yassine Benajiba, Cesare Bernardis, Vladislav Blinov, Paul Cayet, Hassan Chafi, Abderrahim Fathan, Louis Faucon, Damien Hilloulin, Sungpack Hong, Ingo Kossyk, Rhicheek Patra, Sujith Ravi, Jonas Schweizer, Jyotika Singh, Shailender Singh, Xuelin Situ, Weiyi Sun, Jerry Xu, Ying Xu",
        "summary": "Open Agent Specification (Agent Spec) is a declarative language that allows AI agents and their workflows to be defined in a way that is compatible across different AI frameworks, promoting portability and interoperability within AI Agent frameworks. Agent Spec aims to resolve the challenges of fragmented agent development by providing a common unified specification that allows AI agents to be designed once and deployed across various frameworks, improving interoperability and reusability, and reducing redundant development efforts. Additionally, Agent Spec facilitates development tools and portability, allowing AI agents to be defined independently of their execution environment and enabling teams to exchange solutions without implementation-specific limitations. Agent Spec benefits four key groups: (i) Agent developers, who gain access to a superset of reusable components and design patterns, enabling them to leverage a broader range of functionalities; (ii) Agent framework and tool developers, who can use Agent Spec as an interchange format and therefore benefit from the support of other frameworks as well as other tools; (iii) Researchers, who can achieve reproducible results and comparability, facilitating more reliable and consistent outcomes; (iv) Enterprises, which benefit from faster prototype-to-deployment, increased productivity, as well as greater scalability and maintainability for their AI agent solutions. This technical report provides an overview of the technical foundations of Agent Spec, including motivation, benefits, and future developments.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.189317"
    },
    {
        "index": "#49",
        "title": "The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning",
        "link": "/arxiv/2510.04141",
        "arxiv_id": "2510.04141",
        "authors": "Mayank Ravishankara, Varindra V. Persad Maharaj",
        "summary": "This survey paper chronicles the evolution of evaluation in multimodal artificial intelligence (AI), framing it as a progression of increasingly sophisticated \"cognitive examinations.\" We argue that the field is undergoing a paradigm shift, moving from simple recognition tasks that test \"what\" a model sees, to complex reasoning benchmarks that probe \"why\" and \"how\" it understands. This evolution is driven by the saturation of older benchmarks, where high performance often masks fundamental weaknesses. We chart the journey from the foundational \"knowledge tests\" of the ImageNet era to the \"applied logic and comprehension\" exams such as GQA and Visual Commonsense Reasoning (VCR), which were designed specifically to diagnose systemic flaws such as shortcut learning and failures in compositional generalization. We then survey the current frontier of \"expert-level integration\" benchmarks (e.g., MMBench, SEED-Bench, MMMU) designed for today's powerful multimodal large language models (MLLMs), which increasingly evaluate the reasoning process itself. Finally, we explore the uncharted territories of evaluating abstract, creative, and social intelligence. We conclude that the narrative of AI evaluation is not merely a history of datasets, but a continuous, adversarial process of designing better examinations that, in turn, redefine our goals for creating truly intelligent systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.189895"
    },
    {
        "index": "#52",
        "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning",
        "link": "/arxiv/2510.04116",
        "arxiv_id": "2510.04116",
        "authors": "Ziying Zhang, Yaqing Wang, Quanming Yao",
        "summary": "Meta reasoning behaviors work as a skeleton to guide large language model (LLM) reasoning, thus help to improve reasoning performance. However, prior researches implement meta reasoning skeleton with manually designed structure, limiting ability to adapt to query-specific requirement and capture intricate logical dependency among reasoning steps. To deal with the challenges, we represent meta reasoning skeleton with directed acyclic graph (DAG) to unify skeletons proposed in prior works and model intricate logical dependency. Then we propose AutoMR, a framework that searches for query-aware meta reasoning skeleton automatically inspired by automated machine learning (AutoML). Specifically, we construct search space based on DAG representation of skeleton and then formulate the search problem. We design a dynamic skeleton sampling algorithm by expanding meta reasoning skeleton along with reasoning context at inference time. This algorithm can derive any meta reasoning skeleton in search space efficiently and adapt skeleton to evolving base reasoning context, thus enable efficient query-aware skeleton search. We conduct experiments on extensive benchmark datasets. Experimental results show that AutoMR achieves better reasoning performance than previous works broadly.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.201942"
    },
    {
        "index": "#53",
        "title": "WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning",
        "link": "/arxiv/2510.04097",
        "arxiv_id": "2510.04097",
        "authors": "Peichao Lai, Jinhui Zhuang, Kexuan Zhang, Ningchang Xiong, Shengjie Wang, Yanwei Xu, Chong Chen, Yilei Wang, Bin Cui",
        "summary": "Automating the conversion of UI images into web code is a critical task for front-end development and rapid prototyping. Advances in multimodal large language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet existing benchmarks remain limited in data diversity and evaluation reliability. To address these issues, we present WebRenderBench, a large-scale benchmark of 22.5k webpages collected from real-world portal sites, offering greater diversity, complexity, and realism than prior benchmarks. We further propose a novel evaluation metric that measures layout and style consistency from the final rendered pages. Unlike vision-based methods that rely on costly LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry, our approach enables more efficient, objective, and reliable UI quality assessment. Finally, we introduce the Automated Layout and Style Inspection Agent (ALISA), which integrates this metric into reinforcement learning as a reward signal to enhance training on crawled asymmetric webpages. Experiments show that ALISA significantly boosts generation performance, achieving state-of-the-art results across multiple metrics.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.202700"
    },
    {
        "index": "#54",
        "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems",
        "link": "/arxiv/2510.04093",
        "arxiv_id": "2510.04093",
        "authors": "Guixian Zhang, Guan Yuan, Ziqi Xu, Yanmei Zhang, Zhenyun Deng, Debo Cheng",
        "summary": "Cognitive diagnostics in the Web-based Intelligent Education System (WIES) aims to assess students' mastery of knowledge concepts from heterogeneous, noisy interactions. Recent work has tried to utilize Large Language Models (LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are prone to noise-induced misjudgments. Specially, WIES's open environment continuously attracts new students and produces vast amounts of response logs, exacerbating the data imbalance and noise issues inherent in traditional educational systems. To address these challenges, we propose DLLM, a Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first constructs independent subgraphs based on response correctness, then applies relation augmentation alignment module to mitigate data imbalance. The two subgraph representations are then fused and aligned with LLM-derived, semantically augmented representations. Importantly, before each alignment step, DLLM employs a two-stage denoising diffusion module to eliminate intrinsic noise while assisting structural representation alignment. Specifically, unconditional denoising diffusion first removes erroneous information, followed by conditional denoising diffusion based on graph-guided to eliminate misleading information. Finally, the noise-robust representation that integrates semantic knowledge and structural information is fed into existing cognitive diagnosis models for prediction. Experimental results on three publicly available web-based educational platform datasets demonstrate that our DLLM achieves optimal predictive performance across varying noise levels, which demonstrates that DLLM achieves noise robustness while effectively leveraging semantic knowledge from LLM.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.203389"
    },
    {
        "index": "#55",
        "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows",
        "link": "/arxiv/2510.04089",
        "arxiv_id": "2510.04089",
        "authors": "Yitong Cui, Liu Liu, Baosheng Yu, Jiayan Qiu, Xikai Zhang, Likang Xiao, Yixing Liu, Quan Chen",
        "summary": "Large language models (LLMs) have exhibited significant capabilities in addressing challenging problems throughout various fields, often through the use of agentic workflows that adhere to structured instructions and multi-step procedures. However, designing such workflows demands substantial manual effort, posing challenges to scalability and generalizability. Recent studies have aimed to minimize the human intervention needed for their construction, leading to advances in automated techniques for optimizing agentic workflows. However, current approaches are often constrained by their limited representational capacity, insufficient adaptability, weak scalability, and pairwise comparison paradigm -- issues that stem primarily from a dependence on discrete optimization techniques. To overcome these limitations, we introduce a new score-based preference approach, refereed as SPOGW, which operates directly on cardinal reward signals through group-wise comparison and enables more efficient and stable optimization in a continuous space. SPOGW incorporates Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL), which regulates training update by placing greater emphasis on the advantageous regions of the policy response. In five benchmark datasets covering mathematical reasoning, coding, and question answering, SPOGW matches or exceeds the performance of current state-of-the-art approaches, presenting a viable and forward-looking methodology for automated generation and optimization of agentic workflows.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.204149"
    },
    {
        "index": "#56",
        "title": "Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention",
        "link": "/arxiv/2510.04073",
        "arxiv_id": "2510.04073",
        "authors": "Santhosh Kumar Ravindran",
        "summary": "The rise of artificial intelligence (AI) as super-capable assistants has transformed productivity and decision-making across domains. Yet, this integration raises critical concerns about value alignment - ensuring AI behaviors remain consistent with human ethics and intentions. A key risk is value drift, where AI systems deviate from aligned values due to evolving contexts, learning dynamics, or unintended optimizations, potentially leading to inefficiencies or ethical breaches. We propose the Moral Anchor System (MAS), a novel framework to detect, predict, and mitigate value drift in AI agents. MAS combines real-time Bayesian inference for monitoring value states, LSTM networks for forecasting drift, and a human-centric governance layer for adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent breaches, while reducing false positives and alert fatigue via supervised fine-tuning with human feedback. Our hypothesis: integrating probabilistic drift detection, predictive analytics, and adaptive governance can reduce value drift incidents by 80 percent or more in simulations, maintaining high detection accuracy (85 percent) and low false positive rates (0.08 post-adaptation). Rigorous experiments with goal-misaligned agents validate MAS's scalability and responsiveness. MAS's originality lies in its predictive and adaptive nature, contrasting static alignment methods. Contributions include: (1) MAS architecture for AI integration; (2) empirical results prioritizing speed and usability; (3) cross-domain applicability insights; and (4) open-source code for replication.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.204735"
    },
    {
        "index": "#57",
        "title": "Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion",
        "link": "/arxiv/2510.04064",
        "arxiv_id": "2510.04064",
        "authors": "Jingxiang Zhang, Lujia Zhong",
        "summary": "Large Language Models (LLMs) are increasingly expected to navigate the nuances of human emotion. While research confirms that LLMs can simulate emotional intelligence, their internal emotional mechanisms remain largely unexplored. This paper investigates the latent emotional representations within modern LLMs by asking: how, where, and for how long is emotion encoded in their neural architecture? To address this, we introduce a novel, large-scale Reddit corpus of approximately 400,000 utterances, balanced across seven basic emotions through a multi-stage process of classification, rewriting, and synthetic generation. Using this dataset, we employ lightweight \"probes\" to read out information from the hidden layers of various Qwen3 and LLaMA models without altering their parameters. Our findings reveal that LLMs develop a surprisingly well-defined internal geometry of emotion, which sharpens with model scale and significantly outperforms zero-shot prompting. We demonstrate that this emotional signal is not a final-layer phenomenon but emerges early and peaks mid-network. Furthermore, the internal states are both malleable (they can be influenced by simple system prompts) and persistent, as the initial emotional tone remains detectable for hundreds of subsequent tokens. We contribute our dataset, an open-source probing toolkit, and a detailed map of the emotional landscape within LLMs, offering crucial insights for developing more transparent and aligned AI systems. The code and dataset are open-sourced.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.205336"
    },
    {
        "index": "#58",
        "title": "Toward a unified framework for data-efficient evaluation of large language models",
        "link": "/arxiv/2510.04051",
        "arxiv_id": "2510.04051",
        "authors": "Lele Liao, Qile Zhang, Ruofan Wu, Guanhua Fang",
        "summary": "Evaluating large language models (LLMs) on comprehensive benchmarks is a cornerstone of their development, yet it's often computationally and financially prohibitive. While Item Response Theory (IRT) offers a promising path toward data-efficient evaluation by disentangling model capability from item difficulty, existing IRT-based methods are hampered by significant limitations. They are typically restricted to binary correctness metrics, failing to natively handle the continuous scores used in generative tasks, and they operate on single benchmarks, ignoring valuable structural knowledge like correlations across different metrics or benchmarks. To overcome these challenges, we introduce LEGO-IRT, a unified and flexible framework for data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both binary and continuous evaluation metrics. Moreover, it introduces a factorized architecture to explicitly model and leverage structural knowledge, decomposing model ability estimates into a general component and structure-specific (e.g., per-metric or per-benchmark) components. Through extensive experiments involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves stable capability estimates using just $3\\%$ of the total evaluation items. We demonstrate that incorporating structural knowledge reduces estimation error by up to $10\\%$ and reveal that the latent abilities estimated by our framework may align more closely with human preferences.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.205983"
    },
    {
        "index": "#59",
        "title": "Increasing LLM response trustworthiness using voting ensembles",
        "link": "/arxiv/2510.04048",
        "arxiv_id": "2510.04048",
        "authors": "Aparna Nair-Kanneganti, Trevor J. Chan, Shir Goldfinger, Emily Mackay, Brian Anthony, Alison Pouch",
        "summary": "Despite huge advances, LLMs still lack convenient and reliable methods to quantify the uncertainty in their responses, making them difficult to trust in high-stakes applications. One of the simplest approaches to eliciting more accurate answers is to select the mode of many responses, a technique known as ensembling. In this work, we expand on typical ensembling approaches by looking at ensembles with a variable voting threshold. We introduce a theoretical framework for question answering and show that, by permitting ensembles to \"abstain\" from providing an answer when the dominant response falls short of the threshold, it is possible to dramatically increase the trustworthiness of the remaining answers. From this framework, we derive theoretical results as well as report experimental results on two problem domains: arithmetic problem solving and clinical-note question-answering. In both domains, we observe that large gains in answer trustworthiness can be achieved using highly restrictive voting ensembles, while incurring relatively modest reductions in response yield and accuracy. Due to this quality, voting ensembles may be particularly useful in applications - such as healthcare and data annotation - that require a high degree of certainty but which may not require that every question receive an automated answer.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.211766"
    },
    {
        "index": "#60",
        "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning",
        "link": "/arxiv/2510.04040",
        "arxiv_id": "2510.04040",
        "authors": "Xu Shen, Song Wang, Zhen Tan, Laura Yao, Xinyu Zhao, Kaidi Xu, Xin Wang, Tianlong Chen",
        "summary": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models. To the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.212466"
    },
    {
        "index": "#61",
        "title": "A global log for medical AI",
        "link": "/arxiv/2510.04033",
        "arxiv_id": "2510.04033",
        "authors": "Ayush Noori, Adam Rodman, Alan Karthikesalingam, Bilal A. Mateen, Christopher A. Longhurst, Daniel Yang, Dave deBronkart, Gauden Galea, Harold F. Wolf III, Jacob Waxman, Joshua C. Mandel, Juliana Rotich, Kenneth D. Mandl, Maryam Mustafa, Melissa Miles, Nigam H. Shah, Peter Lee, Robert Korom, Scott Mahoney, Seth Hain, Tien Yin Wong, Trevor Mundel, Vivek Natarajan, Noa Dagan, David A. Clifton, Ran D. Balicer, Isaac S. Kohane, Marinka Zitnik",
        "summary": "Modern computer systems often rely on syslog, a simple, universal protocol that records every critical event across heterogeneous infrastructure. However, healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals rush to pilot large language models and other AI-based clinical decision support tools, we still lack a standard way to record how, when, by whom, and for whom these AI models are used. Without that transparency and visibility, it is challenging to measure real-world performance and outcomes, detect adverse events, or correct bias or dataset drift. In the spirit of syslog, we introduce MedLog, a protocol for event-level logging of clinical AI. Any time an AI model is invoked to interact with a human, interface with another algorithm, or act independently, a MedLog record is created. This record consists of nine core fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and feedback, providing a structured and consistent record of model activity. To encourage early adoption, especially in low-resource settings, and minimize the data footprint, MedLog supports risk-based sampling, lifecycle-aware retention policies, and write-behind caching; detailed traces for complex, agentic, or multi-stage workflows can also be captured under MedLog. MedLog can catalyze the development of new databases and software to store and analyze MedLog records. Realizing this vision would enable continuous surveillance, auditing, and iterative improvement of medical AI, laying the foundation for a new form of digital epidemiology.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.213545"
    },
    {
        "index": "#63",
        "title": "Zephyrus: An Agentic Framework for Weather Science",
        "link": "/arxiv/2510.04017",
        "arxiv_id": "2510.04017",
        "authors": "Sumanth Varambally, Marshall Fisher, Jas Thakker, Yiwei Chen, Zhirui Xia, Yasaman Jafari, Ruijia Niu, Manas Jain, Veeramakali Vignesh Manivannan, Zachary Novack, Luyu Han, Srikar Eranky, Salva Rühling Cachay, Taylor Berg-Kirkpatrick, Duncan Watson-Parris, Yi-An Ma, Rose Yu",
        "summary": "Foundation models for weather science are pre-trained on vast amounts of structured numerical data and outperform traditional weather forecasting systems. However, these models lack language-based reasoning capabilities, limiting their utility in interactive scientific workflows. Large language models (LLMs) excel at understanding and generating text but cannot reason about high-dimensional meteorological datasets. We bridge this gap by building a novel agentic framework for weather science. Our framework includes a Python code-based environment for agents (ZephyrusWorld) to interact with weather data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying for geographical masks from natural language, weather forecasting, and climate simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather agent that iteratively analyzes weather datasets, observes results, and refines its approach through conversational feedback loops. We accompany the agent with a new benchmark, ZephyrusBench, with a scalable data generation pipeline that constructs diverse question-answer pairs across weather-related tasks, from basic lookups to advanced forecasting, extreme event detection, and counterfactual reasoning. Experiments on this benchmark demonstrate the strong performance of Zephyrus agents over text-only baselines, outperforming them by up to 35 percentage points in correctness. However, on harder tasks, Zephyrus performs similarly to text-only baselines, highlighting the challenging nature of our benchmark and suggesting promising directions for future work.",
        "subjects": "Artificial Intelligence, Machine Learning, Atmospheric and Oceanic Physics",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.215195"
    },
    {
        "index": "#65",
        "title": "Quantifying Risks in Multi-turn Conversation with Large Language Models",
        "link": "/arxiv/2510.03969",
        "arxiv_id": "2510.03969",
        "authors": "Chengxiao Wang, Isha Chaudhary, Qian Hu, Weitong Ruan, Rahul Gupta, Gagandeep Singh",
        "summary": "Large Language Models (LLMs) can produce catastrophic responses in conversational settings that pose serious risks to public safety and security. Existing evaluations often fail to fully reveal these vulnerabilities because they rely on fixed attack prompt sequences, lack statistical guarantees, and do not scale to the vast space of multi-turn conversations. In this work, we propose QRLLM, a novel, principled Certification framework for Catastrophic risks in multi-turn Conversation for LLMs that bounds the probability of an LLM generating catastrophic responses under multi-turn conversation distributions with statistical guarantees. We model multi-turn conversations as probability distributions over query sequences, represented by a Markov process on a query graph whose edges encode semantic similarity to capture realistic conversational flow, and quantify catastrophic risks using confidence intervals. We define several inexpensive and practical distributions: random node, graph path, adaptive with rejection. Our results demonstrate that these distributions can reveal substantial catastrophic risks in frontier models, with certified lower bounds as high as 70\\% for the worst model, highlighting the urgent need for improved safety training strategies in frontier LLMs.",
        "subjects": "Artificial Intelligence, Cryptography and Security, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.221712"
    },
    {
        "index": "#67",
        "title": "Rare Text Semantics Were Always There in Your Diffusion Transformer",
        "link": "/arxiv/2510.03886",
        "arxiv_id": "2510.03886",
        "authors": "Seil Kang, Woojung Han, Dayun Ju, Seong Jae Hwang",
        "summary": "Starting from flow- and diffusion-based transformers, Multi-modal Diffusion Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim for exceptional visual fidelity. As these models advance, users continually push the boundary with imaginative or rare prompts, which advanced models still falter in generating, since their concepts are often too scarce to leave a strong imprint during pre-training. In this paper, we propose a simple yet effective intervention that surfaces rare semantics inside MM-DiTs without additional training steps, data, denoising-time optimization, or reliance on external modules (e.g., large language models). In particular, the joint-attention mechanism intrinsic to MM-DiT sequentially updates text embeddings alongside image embeddings throughout transformer blocks. We find that by mathematically expanding representational basins around text token embeddings via variance scale-up before the joint-attention blocks, rare semantics clearly emerge in MM-DiT's outputs. Furthermore, our results generalize effectively across text-to-vision tasks, including text-to-image, text-to-video, and text-driven image editing. Our work invites generative models to reveal the semantics that users intend, once hidden yet ready to surface.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.222931"
    },
    {
        "index": "#68",
        "title": "Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation",
        "link": "/arxiv/2510.03863",
        "arxiv_id": "2510.03863",
        "authors": "Arina Kharlamova, Bowei He, Chen Ma, Xue Liu",
        "summary": "Online services rely on CAPTCHAs as a first line of defense against automated abuse, yet recent advances in multi-modal large language models (MLLMs) have eroded the effectiveness of conventional designs that focus on text recognition or 2D image understanding. To address this challenge, we present Spatial CAPTCHA, a novel human-verification framework that leverages fundamental differences in spatial reasoning between humans and MLLMs. Unlike existing CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning, perspective-taking, occlusion handling, and mental rotation. These skills are intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The system employs a procedural generation pipeline with constraint-based difficulty control, automated correctness verification, and human-in-the-loop validation to ensure scalability, robustness, and adaptability. Evaluation on a corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0% Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA, which confirms its effectiveness as both a security mechanism and a diagnostic tool for spatial reasoning in AI.",
        "subjects": "Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.223572"
    },
    {
        "index": "#69",
        "title": "Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning",
        "link": "/arxiv/2510.03859",
        "arxiv_id": "2510.03859",
        "authors": "Raghav Sharma, Manan Mehta",
        "summary": "Ensuring that critical IoT systems function safely and smoothly depends a lot on finding anomalies quickly. As more complex systems, like smart healthcare, energy grids and industrial automation, appear, it is easier to see the shortcomings of older methods of detection. Monitoring failures usually happen in dynamic, high dimensional situations, especially when data is incomplete, messy or always evolving. Such limits point out the requirement for adaptive, intelligent systems that always improve and think. LLMs are now capable of significantly changing how context is understood and semantic inference is done across all types of data. This proposal suggests using an LLM supported contextual reasoning method along with XAI agents to improve how anomalies are found in significant IoT environments. To discover hidden patterns and notice inconsistencies in data streams, it uses attention methods, avoids dealing with details from every time step and uses memory buffers with meaning. Because no code AI stresses transparency and interpretability, people can check and accept the AI's decisions, helping ensure AI follows company policies. The two architectures are put together in a test that compares the results of the traditional model with those of the suggested LLM enhanced model. Important measures to check are the accuracy of detection, how much inaccurate information is included in the results, how clearly the findings can be read and how fast the system responds under different test situations. The metaheuristic is tested in simulations of real world smart grid and healthcare contexts to check its adaptability and reliability. From the study, we see that the new approach performs much better than most existing models in both accuracy and interpretation, so it could be a good fit for future anomaly detection tasks in IoT",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.224196"
    },
    {
        "index": "#70",
        "title": "Algorithm Generation via Creative Ideation",
        "link": "/arxiv/2510.03851",
        "arxiv_id": "2510.03851",
        "authors": "Ruiying Ma, Chieh-Jan Mike Liang, Yanjie Gao, Francis Y. Yan",
        "summary": "Designing system algorithms remains challenging, where the discontinuous nature of the solution space often forces system engineers to rely on generic heuristics at the expense of performance. We study whether LLMs can practically drive algorithm generation, and find that they are biased towards well-known generic designs, rather than making the creative leaps needed to navigate the discontinuous solution space. To address this limitation, we introduce MetaMuse, a framework for creative ideation built on three self-reflection principles: (1) quantifying solution diversity and usefulness in measurable performance space, rather than abstract idea space, (2) steering ideation through external stimuli, rather than internal randomness, and (3) constructing executable solutions using waypoint reasoning, rather than free-form chain-of-thought. Extensive evaluation shows that MetaMuse can generate high-performing solutions for two critical problems at a global cloud provider: cache replacement (reducing cache misses by up to 35.76%) and online bin packing (reducing bin usage by up to 30.93%).",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.224811"
    },
    {
        "index": "#71",
        "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs",
        "link": "/arxiv/2510.03847",
        "arxiv_id": "2510.03847",
        "authors": "Raghav Sharma, Manan Mehta",
        "summary": "Small language models (SLMs; 1-12B params, sometimes up to 20B) are sufficient and often superior for agentic workloads where the objective is schema- and API-constrained accuracy rather than open-ended generation. We synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini, Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B, DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4, StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with guided decoding libraries (XGrammar, Outlines). We formalize SLM-default, LLM-fallback systems with uncertainty-aware routing and verifier cascades, and propose engineering metrics that reflect real production goals: cost per successful task (CPS), schema validity rate, executable call rate, p50/p95 latency, and energy per request. Guided decoding, strict JSON Schema outputs, and validator-first tool execution close much of the capability gap with larger models and often let SLMs match or surpass LLMs on tool use, function calling, and RAG at 10x-100x lower token cost with materially better latency and energy. We provide design patterns for agent stacks that prioritize SLMs: schema-first prompting, type-safe function registries, confidence scoring with verifier rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits where fallback remains valuable (open-domain reasoning and some long-horizon planning). The result is a practical blueprint for building fast, inexpensive, and reliable agents that default to SLMs while preserving headroom with targeted LLM assistance. Keywords: small language models, agents, function calling, structured outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency, edge inference",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.225657"
    },
    {
        "index": "#72",
        "title": "The Hidden Game Problem",
        "link": "/arxiv/2510.03845",
        "arxiv_id": "2510.03845",
        "authors": "Gon Buzaglo, Noah Golowich, Elad Hazan",
        "summary": "This paper investigates a class of games with large strategy spaces, motivated by challenges in AI alignment and language games. We introduce the hidden game problem, where for each player, an unknown subset of strategies consistently yields higher rewards compared to the rest. The central question is whether efficient regret minimization algorithms can be designed to discover and exploit such hidden structures, leading to equilibrium in these subgames while maintaining rationality in general. We answer this question affirmatively by developing a composition of regret minimization techniques that achieve optimal external and swap regret bounds. Our approach ensures rapid convergence to correlated equilibria in hidden subgames, leveraging the hidden game structure for improved computational efficiency.",
        "subjects": "Artificial Intelligence, Computer Science and Game Theory, Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.226337"
    },
    {
        "index": "#73",
        "title": "GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time",
        "link": "/arxiv/2510.03777",
        "arxiv_id": "2510.03777",
        "authors": "Divij Handa, Mihir Parmar, Aswin RRV, Md Nayem Uddin, Hamid Palangi, Chitta Baral",
        "summary": "Repeated Sampling (RS) is a simple inference-time algorithm that has been shown to improve model performance on complex tasks. Although it is an effective way of scaling inference time, it often struggles to generate diverse solution candidates, frequently relying on the same underlying approach to solve the problem and thus producing redundant samples. To address this limitation, we propose a new inference algorithm, GuidedSampling, which decouples the exploration and generation phases during inference, increasing diversity of generated candidate solutions. The exploration phase identifies multiple concepts that can be utilized to solve the problem, while the generation phase applies a specific concept to provide final solution candidates. We first define the theoretical bounds of GuidedSampling and then empirically demonstrate that it improves the performance of base model at pass@50 by on an average ~21.6% across various benchmarks compared to RS. Furthermore, models trained on trajectories of GuidedSampling exhibit substantial performance improvements at pass@5 by on an average ~9.7%, compared to models trained on traditional RS. Additionally, models trained with GuidedSampling increases the average number of concepts per instance (1.67 -> 3.03), yielding a diverse set of candidates than traditional RS.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.227028"
    },
    {
        "index": "#74",
        "title": "OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation",
        "link": "/arxiv/2510.03771",
        "arxiv_id": "2510.03771",
        "authors": "Divij Handa, David Blincoe, Orson Adams, Yinlin Fu",
        "summary": "Deploying capable and user-aligned LLM-based systems necessitates reliable evaluation. While LLMs excel in verifiable tasks like coding and mathematics, where gold-standard solutions are available, adoption remains challenging for subjective tasks that lack a single correct answer. E-commerce Query Rewriting (QR) is one such problem where determining whether a rewritten query properly captures the user intent is extremely difficult to figure out algorithmically. In this work, we introduce OptAgent, a novel framework that combines multi-agent simulations with genetic algorithms to verify and optimize queries for QR. Instead of relying on a static reward model or a single LLM judge, our approach uses multiple LLM-based agents, each acting as a simulated shopping customer, as a dynamic reward signal. The average of these agent-derived scores serves as an effective fitness function for an evolutionary algorithm that iteratively refines the user's initial query. We evaluate OptAgent on a dataset of 1000 real-world e-commerce queries in five different categories, and we observe an average improvement of 21.98% over the original user query and 3.36% over a Best-of-N LLM rewriting baseline.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.227648"
    },
    {
        "index": "#76",
        "title": "H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis",
        "link": "/arxiv/2510.03700",
        "arxiv_id": "2510.03700",
        "authors": "Seungseop Lim, Gibaeg Kim, Hyunkyung Lee, Wooseok Han, Jean Seo, Jaehyo Yoo, Eunho Yang",
        "summary": "An accurate differential diagnosis (DDx) is essential for patient care, shaping therapeutic decisions and influencing outcomes. Recently, Large Language Models (LLMs) have emerged as promising tools to support this process by generating a DDx list from patient narratives. However, existing evaluations of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy, which fail to distinguish between clinically relevant near-misses and diagnostically distant errors. To mitigate this limitation, we introduce H-DDx, a hierarchical evaluation framework that better reflects clinical relevance. H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses to ICD-10 codes and applies a hierarchical metric that credits predictions closely related to the ground-truth diagnosis. In benchmarking 22 leading models, we show that conventional flat metrics underestimate performance by overlooking clinically meaningful outputs, with our results highlighting the strengths of domain-specialized open-source models. Furthermore, our framework enhances interpretability by revealing hierarchical error patterns, demonstrating that LLMs often correctly identify the broader clinical context even when the precise diagnosis is missed.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.228983"
    },
    {
        "index": "#78",
        "title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs",
        "link": "/arxiv/2510.03680",
        "arxiv_id": "2510.03680",
        "authors": "Bumjun Kim, Dongjae Jeon, Dueun Kim, Wonje Jeung, Albert No",
        "summary": "Diffusion large language models (dLLMs) have emerged as a promising alternative to autoregressive models, offering flexible generation orders and strong performance on complex reasoning tasks. However, instruction-tuned dLLMs exhibit a critical vulnerability we term \\texttt{<eos>} overflow: as allocated sequence length increases, responses paradoxically become shorter, collapsing into early termination or degenerating into streams of \\texttt{<eos>} tokens. Although noticed in practice, this issue has not been systematically analyzed. We trace its root cause to the dual role of \\texttt{<eos>} as both termination and padding, which concentrates probability mass on \\texttt{<eos>} at later positions and propagates backward to trigger early termination. To address this, we introduce Rainbow Padding, a simple remedy that replaces repeated \\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens, distributing probability mass and breaking \\texttt{<eos>} dominance. Experiments show that Rainbow Padding substantially improves length robustness and output quality, with as few as seven padding tokens sufficient to prevent early termination. Moreover, the method integrates efficiently into existing instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data yields significant improvements, making this solution highly practical. The code is publicly available at https://github.com/quasar529/rainbow-padding.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.235312"
    },
    {
        "index": "#79",
        "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information",
        "link": "/arxiv/2510.03632",
        "arxiv_id": "2510.03632",
        "authors": "Jiaxi Li, Yucheng Shi, Jin Lu, Ninghao Liu",
        "summary": "Tree search has become as a representative framework for test-time reasoning with large language models (LLMs), exemplified by methods such as Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning paths. However, it remains difficult to provide instant and reliable quantitative assessments of intermediate reasoning step quality, and extensive path exploration is computationally costly. To address this, we propose Mutual Information Tree Search (MITS), a novel framework that guides reasoning with information-theoretic principles. MITS introduces an effective scoring function based on pointwise mutual information (PMI), which enables step-wise evaluation of reasoning paths and search tree expansion via beam search without expensive look-ahead simulations, achieving superior reasoning performances while maintaining computational efficiency. The framework is complemented by an entropy-based dynamic sampling strategy that adaptively allocates computational resources to uncertain reasoning steps where exploration is most beneficial. For final prediction, MITS employs a weighted voting scheme that combines PMI scores with prediction consensus. Through comprehensive experiments on diverse reasoning benchmarks, MITS consistently surpasses baseline methods, establishing a principled and efficient framework for LLM reasoning.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.235701"
    },
    {
        "index": "#80",
        "title": "Cross-Modal Content Optimization for Steering Web Agent Preferences",
        "link": "/arxiv/2510.03612",
        "arxiv_id": "2510.03612",
        "authors": "Tanqiu Jiang, Min Bai, Nikolaos Pappas, Yanjun Qi, Sandesh Swamy",
        "summary": "Vision-language model (VLM)-based web agents increasingly power high-stakes selection tasks like content recommendation or product ranking by combining multimodal perception with preference reasoning. Recent studies reveal that these agents are vulnerable against attackers who can bias selection outcomes through preference manipulations using adversarial pop-ups, image perturbations, or content tweaks. Existing work, however, either assumes strong white-box access, with limited single-modal perturbations, or uses impractical settings. In this paper, we demonstrate, for the first time, that joint exploitation of visual and textual channels yields significantly more powerful preference manipulations under realistic attacker capabilities. We introduce Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible modifications to an item's visual and natural language descriptions, exploiting CLIP-transferable image perturbations and RLHF-induced linguistic biases to steer agent decisions. In contrast to prior studies that assume gradient access, or control over webpages, or agent memory, we adopt a realistic black-box threat setup: a non-privileged adversary can edit only their own listing's images and textual metadata, with no insight into the agent's model internals. We evaluate CPS on agents powered by state-of-the-art proprietary and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both movie selection and e-commerce tasks. Our results show that CPS is significantly more effective than leading baseline methods. For instance, our results show that CPS consistently outperforms baselines across all models while maintaining 70% lower detection rates, demonstrating both effectiveness and stealth. These findings highlight an urgent need for robust defenses as agentic systems play an increasingly consequential role in society.",
        "subjects": "Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.236123"
    },
    {
        "index": "#81",
        "title": "Understanding the Role of Training Data in Test-Time Scaling",
        "link": "/arxiv/2510.03605",
        "arxiv_id": "2510.03605",
        "authors": "Adel Javanmard, Baharan Mirzasoleiman, Vahab Mirrokni",
        "summary": "Test-time scaling improves the reasoning capabilities of large language models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts (CoTs). This enables models to tackle more complex problem by breaking them down into additional steps, backtracking, and correcting mistakes. Despite its strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions in the training data under which long CoTs emerge, and when such long CoTs improve the performance, remain unclear. In this paper, we study the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. Our analysis provides a theoretical explanation for several intriguing observations: First, at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. Second, if the skills required to solve a downstream task are not sufficiently present in the training data, increasing test-time compute can harm performance. Finally, we characterize task hardness via the smallest eigenvalue of its feature covariance matrix and show that training on a diverse, relevant, and hard set of tasks results in best performance for test-time scaling. We confirm our findings with experiments on large, nonlinear transformer architectures.",
        "subjects": "Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.236527"
    },
    {
        "index": "#82",
        "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
        "link": "/arxiv/2510.03506",
        "arxiv_id": "2510.03506",
        "authors": "John Nguyen, Marton Havasi, Tariq Berrada, Luke Zettlemoyer, Ricky T. Q. Chen",
        "summary": "We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.236925"
    },
    {
        "index": "#83",
        "title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection",
        "link": "/arxiv/2510.03485",
        "arxiv_id": "2510.03485",
        "authors": "Xiaofei Wen, Wenjie Jacky Mo, Yanan Xie, Peng Qi, Muhao Chen",
        "summary": "Autonomous web agents need to operate under externally imposed or human-specified policies while generating long-horizon trajectories. However, little work has examined whether these trajectories comply with such policies, or whether policy violations persist across different contexts such as domains (e.g., shopping or coding websites) and subdomains (e.g., product search and order management in shopping). To address this gap, we introduce PolicyGuardBench, a benchmark of about 60k examples for detecting policy violations in agent trajectories. From diverse agent runs, we generate a broad set of policies and create both within subdomain and cross subdomain pairings with violation labels. In addition to full-trajectory evaluation, PolicyGuardBench also includes a prefix-based violation detection task where models must anticipate policy violations from truncated trajectory prefixes rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a lightweight guardrail model that delivers strong detection accuracy across all tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes across domains and preserves high accuracy on unseen settings. Together, PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework for studying policy compliance in web agent trajectories, and show that accurate and generalizable guardrails are feasible at small scales.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.237322"
    },
    {
        "index": "#84",
        "title": "Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification",
        "link": "/arxiv/2510.03469",
        "arxiv_id": "2510.03469",
        "authors": "Keshav Ramani, Vali Tawosi, Salwa Alamir, Daniel Borrajo",
        "summary": "We introduce a novel framework for evaluating the alignment between natural language plans and their expected behavior by converting them into Kripke structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs) and performing model checking. We systematically evaluate this framework on a simplified version of the PlanBench plan verification dataset and report on metrics like Accuracy, Precision, Recall and F1 scores. Our experiments demonstrate that GPT-5 achieves excellent classification performance (F1 score of 96.3%) while almost always producing syntactically perfect formal representations that can act as guarantees. However, the synthesis of semantically perfect formal models remains an area for future exploration.",
        "subjects": "Artificial Intelligence, Logic in Computer Science",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.237726"
    },
    {
        "index": "#85",
        "title": "A Qualitative Comparative Evaluation of Cognitive and Generative Theories",
        "link": "/arxiv/2510.03453",
        "arxiv_id": "2510.03453",
        "authors": "Paul S. Rosenbloom",
        "summary": "Evaluation is a critical activity associated with any theory. Yet this has proven to be an exceptionally challenging activity for theories based on cognitive architectures. For an overlapping set of reasons, evaluation can also be challenging for theories based on generative neural architectures. This dual challenge is approached here by leveraging a broad perspective on theory evaluation to yield a wide-ranging, albeit qualitative, comparison of whole-mind-oriented cognitive and generative architectures and the full systems that are based on these architectures.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.238065"
    },
    {
        "index": "#88",
        "title": "Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints",
        "link": "/arxiv/2510.03377",
        "arxiv_id": "2510.03377",
        "authors": "Ahmed Missaoui, Cemalettin Ozturk, Barry O'Sullivan",
        "summary": "The scarcity of non-renewable energy sources, geopolitical problems in its supply, increasing prices, and the impact of climate change, force the global economy to develop more energy-efficient solutions for their operations. The Manufacturing sector is not excluded from this challenge as one of the largest consumers of energy. Energy-efficient scheduling is a method that attracts manufacturing companies to reduce their consumption as it can be quickly deployed and can show impact immediately. In this study, the hybrid flow shop scheduling problem with blocking constraint (BHFS) is investigated in which we seek to minimize the latest completion time (i.e. makespan) and overall energy consumption, a typical manufacturing setting across many industries from automotive to pharmaceutical. Energy consumption and the latest completion time of customer orders are usually conflicting objectives. Therefore, we first formulate the problem as a novel multi-objective mixed integer programming (MIP) model and propose an augmented epsilon-constraint method for finding the Pareto-optimal solutions. Also, an effective multi-objective metaheuristic algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large instances in reasonable time. Our proposed methods are benchmarked using small, medium, and large-size instances to evaluate their efficiency. Two well-known algorithms are adopted for comparing our novel approaches. The computational results show the effectiveness of our method.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.239308"
    },
    {
        "index": "#89",
        "title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks",
        "link": "/arxiv/2510.03285",
        "arxiv_id": "2510.03285",
        "authors": "Su Kara, Fazle Faisal, Suman Nath",
        "summary": "Recent advances in browser-based LLM agents have shown promise for automating tasks ranging from simple form filling to hotel booking or online shopping. Current benchmarks measure agent performance in controlled environments, such as containers or stable networks, where websites behave deterministically. However, in the real world, users access websites over networks and HTTPS connections that introduce instability from multiple sources: client-side, server-side issues or broader system failures. Moreover, live websites are prone to web attacks such Cross-Site Scripting, as well as general site modifications which can cause unexpected or malicious pop-ups or improper functionality. To address this gap, we present WAREX: Web Agent Reliability Evaluation on Existing Benchmarks. We measure the impact of WAREX across three popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that introducing WAREX leads to significant drops in task success rates, highlighting the limited robustness of state-of-the-art agents.",
        "subjects": "Artificial Intelligence, Cryptography and Security, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.239741"
    },
    {
        "index": "#90",
        "title": "TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration",
        "link": "/arxiv/2510.05102",
        "arxiv_id": "2510.05102",
        "authors": "Cheng Xin, Fan Xu, Xin Ding, Jie Gao, Jiaxin Ding",
        "summary": "Graph Neural Networks (GNNs) have shown remarkable success across various scientific fields, yet their adoption in critical decision-making is often hindered by a lack of interpretability. Recently, intrinsically interpretable GNNs have been studied to provide insights into model predictions by identifying rationale substructures in graphs. However, existing methods face challenges when the underlying rationale subgraphs are complex and varied. In this work, we propose TopInG: Topologically Interpretable Graph Learning, a novel topological framework that leverages persistent homology to identify persistent rationale subgraphs. TopInG employs a rationale filtration learning approach to model an autoregressive generation process of rationale subgraphs, and introduces a self-adjusted topological constraint, termed topological discrepancy, to enforce a persistent topological distinction between rationale subgraphs and irrelevant counterparts. We provide theoretical guarantees that our loss function is uniquely optimized by the ground truth under specific conditions. Extensive experiments demonstrate TopInG's effectiveness in tackling key challenges, such as handling variform rationale subgraphs, balancing predictive performance with interpretability, and mitigating spurious correlations. Results show that our approach improves upon state-of-the-art methods on both predictive accuracy and interpretation quality.",
        "subjects": "Machine Learning, Artificial Intelligence, Computational Geometry, Algebraic Topology, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.240226"
    },
    {
        "index": "#96",
        "title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder",
        "link": "/arxiv/2510.05081",
        "arxiv_id": "2510.05081",
        "authors": "Ronen Kamenetsky, Sara Dorfman, Daniel Garibi, Roni Paiss, Or Patashnik, Daniel Cohen-Or",
        "summary": "Large-scale text-to-image diffusion models have become the backbone of modern image editing, yet text prompts alone do not offer adequate control over the editing process. Two properties are especially desirable: disentanglement, where changing one attribute does not unintentionally alter others, and continuous control, where the strength of an edit can be smoothly adjusted. We introduce a method for disentangled and continuous editing through token-level manipulation of text embeddings. The edits are applied by manipulating the embeddings along carefully chosen directions, which control the strength of the target attribute. To identify such directions, we employ a Sparse Autoencoder (SAE), whose sparse latent space exposes semantically isolated dimensions. Our method operates directly on text embeddings without modifying the diffusion process, making it model agnostic and broadly applicable to various image synthesis backbones. Experiments show that it enables intuitive and efficient manipulations with continuous control across diverse attributes and domains.",
        "subjects": "Graphics, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.249404"
    },
    {
        "index": "#99",
        "title": "HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model",
        "link": "/arxiv/2510.05054",
        "arxiv_id": "2510.05054",
        "authors": "Peter Van Katwyk, Karianne J. Bergen",
        "summary": "Uncertainty quantification is critical for ensuring robustness in high-stakes machine learning applications. We introduce HybridFlow, a modular hybrid architecture that unifies the modeling of aleatoric and epistemic uncertainty by combining a Conditional Masked Autoregressive normalizing flow for estimating aleatoric uncertainty with a flexible probabilistic predictor for epistemic uncertainty. The framework supports integration with any probabilistic model class, allowing users to easily adapt HybridFlow to existing architectures without sacrificing predictive performance. HybridFlow improves upon previous uncertainty quantification frameworks across a range of regression tasks, such as depth estimation, a collection of regression benchmarks, and a scientific case study of ice sheet emulation. We also provide empirical results of the quantified uncertainty, showing that the uncertainty quantified by HybridFlow is calibrated and better aligns with model error than existing methods for quantifying aleatoric and epistemic uncertainty. HybridFlow addresses a key challenge in Bayesian deep learning, unifying aleatoric and epistemic uncertainty modeling in a single robust framework.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.256562"
    },
    {
        "index": "#100",
        "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts",
        "link": "/arxiv/2510.05040",
        "arxiv_id": "2510.05040",
        "authors": "Jihoon Lee, Hoyeon Moon, Kevin Zhai, Arun Kumar Chithanar, Anit Kumar Sahu, Soummya Kar, Chul Lee, Souradip Chakraborty, Amrit Singh Bedi",
        "summary": "Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn a mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, a common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), a training-free inference method that ensembles across heterogeneous block schedules. By doing a majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish a new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays a critical role in determining performance during inference.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.257308"
    },
    {
        "index": "#101",
        "title": "Graph-Aware Diffusion for Signal Generation",
        "link": "/arxiv/2510.05036",
        "arxiv_id": "2510.05036",
        "authors": "Sergio Rozada, Vimal K. B., Andrea Cavallo, Antonio G. Marques, Hadi Jamali-Rad, Elvin Isufi",
        "summary": "We study the problem of generating graph signals from unknown distributions defined over given graphs, relevant to domains such as recommender systems or sensor networks. Our approach builds on generative diffusion models, which are well established in vision and graph generation but remain underexplored for graph signals. Existing methods lack generality, either ignoring the graph structure in the forward process or designing graph-aware mechanisms tailored to specific domains. We adopt a forward process that incorporates the graph through the heat equation. Rather than relying on the standard formulation, we consider a time-warped coefficient to mitigate the exponential decay of the drift term, yielding a graph-aware generative diffusion model (GAD). We analyze its forward dynamics, proving convergence to a Gaussian Markov random field with covariance parametrized by the graph Laplacian, and interpret the backward dynamics as a sequence of graph-signal denoising problems. Finally, we demonstrate the advantages of GAD on synthetic data, real traffic speed measurements, and a temperature sensor network.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.257987"
    },
    {
        "index": "#103",
        "title": "Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective",
        "link": "/arxiv/2510.05023",
        "arxiv_id": "2510.05023",
        "authors": "Weixin Wang, Haoyang Zheng, Guang Lin, Wei Deng, Pan Xu",
        "summary": "Most existing approximate Thompson Sampling (TS) algorithms for multi-armed bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in each round to sample from the posterior, relaxing the need for conjugacy assumptions between priors and reward distributions in vanilla TS. However, they often require approximating a different posterior distribution in different round of the bandit problem. This requires tricky, round-specific tuning of hyperparameters such as dynamic learning rates, causing challenges in both theoretical analysis and practical implementation. To alleviate this non-stationarity, we introduce TS-SA, which incorporates stochastic approximation (SA) within the TS framework. In each round, TS-SA constructs a posterior approximation only using the most recent reward(s), performs a Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy proposals over time. This can be interpreted as approximating a stationary posterior target throughout the entire algorithm, which further yields a fixed step-size, a unified convergence analysis framework, and improved posterior estimates through temporal averaging. We establish near-optimal regret bounds for TS-SA, with a simplified and more intuitive theoretical analysis enabled by interpreting the entire algorithm as a simulation of a stationary SGLD process. Our empirical results demonstrate that even a single-step Langevin update with certain warm-up outperforms existing methods substantially on bandit tasks.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.259395"
    },
    {
        "index": "#106",
        "title": "Bridging Text and Video Generation: A Survey",
        "link": "/arxiv/2510.04999",
        "arxiv_id": "2510.04999",
        "authors": "Nilay Kumar, Priyansh Bhandari, G. Maragatham",
        "summary": "Text-to-video (T2V) generation technology holds potential to transform multiple domains such as education, marketing, entertainment, and assistive technologies for individuals with visual or reading comprehension challenges, by creating coherent visual content from natural language prompts. From its inception, the field has advanced from adversarial models to diffusion-based models, yielding higher-fidelity, temporally consistent outputs. Yet challenges persist, such as alignment, long-range coherence, and computational efficiency. Addressing this evolving landscape, we present a comprehensive survey of text-to-video generative models, tracing their development from early GANs and VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these models work, what limitations they addressed in their predecessors, and why shifts toward new architectural paradigms were necessary to overcome challenges in quality, coherence, and control. We provide a systematic account of the datasets, which the surveyed text-to-video models were trained and evaluated on, and, to support reproducibility and assess the accessibility of training such models, we detail their training configurations, including their hardware specifications, GPU counts, batch sizes, learning rates, optimizers, epochs, and other key hyperparameters. Further, we outline the evaluation metrics commonly used for evaluating such models and present their performance across standard benchmarks, while also discussing the limitations of these metrics and the emerging shift toward more holistic, perception-aligned evaluation strategies. Finally, drawing from our analysis, we outline the current open challenges and propose a few promising future directions, laying out a perspective for future researchers to explore and build upon in advancing T2V research and applications.",
        "subjects": "Graphics, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.266485"
    },
    {
        "index": "#107",
        "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis",
        "link": "/arxiv/2510.04997",
        "arxiv_id": "2510.04997",
        "authors": "Jiongchi Yu, Weipeng Jiang, Xiaoyu Zhang, Qiang Hu, Xiaofei Xie, Chao Shen",
        "summary": "Understanding software faults is essential for empirical research in software development and maintenance. However, traditional fault analysis, while valuable, typically involves multiple expert-driven steps such as collecting potential faults, filtering, and manual investigation. These processes are both labor-intensive and time-consuming, creating bottlenecks that hinder large-scale fault studies in complex yet critical software systems and slow the pace of iterative empirical research. In this paper, we decompose the process of empirical software fault study into three key phases: (1) research objective definition, (2) data preparation, and (3) fault analysis, and we conduct an initial exploration study of applying Large Language Models (LLMs) for fault analysis of open-source software. Specifically, we perform the evaluation on 3,829 software faults drawn from a high-quality empirical study. Our results show that LLMs can substantially improve efficiency in fault analysis, with an average processing time of about two hours, compared to the weeks of manual effort typically required. We conclude by outlining a detailed research plan that highlights both the potential of LLMs for advancing empirical fault studies and the open challenges that required be addressed to achieve fully automated, end-to-end software fault analysis.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.267170"
    },
    {
        "index": "#110",
        "title": "Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning",
        "link": "/arxiv/2510.04970",
        "arxiv_id": "2510.04970",
        "authors": "Marcel Wienöbst, Leonard Henckel, Sebastian Weichwald",
        "summary": "We present FLOP (Fast Learning of Order and Parents), a score-based causal discovery algorithm for linear models. It pairs fast parent selection with iterative Cholesky-based score updates, cutting run-times over prior algorithms. This makes it feasible to fully embrace discrete search, enabling iterated local search with principled order initialization to find graphs with scores at or close to the global optimum. The resulting structures are highly accurate across benchmarks, with near-perfect recovery in standard settings. This performance calls for revisiting discrete search over graphs as a reasonable approach to causal discovery.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning, Methodology",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.269332"
    },
    {
        "index": "#111",
        "title": "ActiveMark: on watermarking of visual foundation models via massive activations",
        "link": "/arxiv/2510.04966",
        "arxiv_id": "2510.04966",
        "authors": "Anna Chistyakova, Mikhail Pautov",
        "summary": "Being trained on large and vast datasets, visual foundation models (VFMs) can be fine-tuned for diverse downstream tasks, achieving remarkable performance and efficiency in various computer vision applications. The high computation cost of data collection and training motivates the owners of some VFMs to distribute them alongside the license to protect their intellectual property rights. However, a dishonest user of the protected model's copy may illegally redistribute it, for example, to make a profit. As a consequence, the development of reliable ownership verification tools is of great importance today, since such methods can be used to differentiate between a redistributed copy of the protected model and an independent model. In this paper, we propose an approach to ownership verification of visual foundation models by fine-tuning a small set of expressive layers of a VFM along with a small encoder-decoder network to embed digital watermarks into an internal representation of a hold-out set of input images. Importantly, the watermarks embedded remain detectable in the functional copies of the protected model, obtained, for example, by fine-tuning the VFM for a particular downstream task. Theoretically and experimentally, we demonstrate that the proposed method yields a low probability of false detection of a non-watermarked model and a low probability of false misdetection of a watermarked model.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.269931"
    },
    {
        "index": "#112",
        "title": "MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive Hierarchical Neural Modeling",
        "link": "/arxiv/2510.04956",
        "arxiv_id": "2510.04956",
        "authors": "Bi-Cheng Yan, Ming-Kang Tsai, Berlin Chen",
        "summary": "Computer-assisted pronunciation training (CAPT) manages to facilitate second-language (L2) learners to practice pronunciation skills by offering timely and instructive feedback. To examine pronunciation proficiency from multiple facets, existing methods for CAPT broadly fall into two categories: mispronunciation detection and diagnosis (MDD) as well as automatic pronunciation assessment (APA). The former aims to pinpoint phonetic pronunciation errors and provide diagnostic feedback, while the latter seeks instead to quantify pronunciation proficiency pertaining to various aspects. Despite the natural complementarity between MDD and APA, researchers and practitioners, however, often treat them as independent tasks with disparate modeling paradigms. In light of this, we in this paper first introduce MuFFIN, a Multi-Faceted pronunciation Feedback model with an Interactive hierarchical Neural architecture, to jointly address the tasks of MDD and APA. To better capture the nuanced distinctions between phonemes in the feature space, a novel phoneme-contrastive ordinal regularization mechanism is then put forward to optimize the proposed model to generate more phoneme-discriminative features while factoring in the ordinality of the aspect scores. In addition, to address the intricate data imbalance problem in MDD, we design a simple yet effective training objective, which is specifically tailored to perturb the outputs of a phoneme classifier with the phoneme-specific variations, so as to better render the distribution of predicted phonemes meanwhile considering their mispronunciation characteristics. A series of experiments conducted on the Speechocean762 benchmark dataset demonstrates the efficacy of our method in relation to several cutting-edge baselines, showing state-of-the-art performance on both the APA and MDD tasks.",
        "subjects": "Audio and Speech Processing, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.270554"
    },
    {
        "index": "#113",
        "title": "Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints",
        "link": "/arxiv/2510.04951",
        "arxiv_id": "2510.04951",
        "authors": "Jayanta Mandi, Marianne Defresne, Senne Berden, Tias Guns",
        "summary": "When some parameters of a constrained optimization problem (COP) are uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising two stages -- the prediction of the unknown parameters from contextual information and the subsequent optimization using those predicted parameters. Decision-focused learning (DFL) implements the first stage by training a machine learning (ML) model to optimize the quality of the decisions made using the predicted parameters. When parameters in the constraints of a COP are predicted, the predicted parameters can lead to infeasible solutions. Therefore, it is important to simultaneously manage both feasibility and decision quality. We develop a DFL framework for predicting constraint parameters in a generic COP. While prior works typically assume that the underlying optimization problem is a linear program (LP) or integer linear program (ILP), our approach makes no such assumption. We derive two novel loss functions based on maximum likelihood estimation (MLE): the first one penalizes infeasibility (by penalizing when the predicted parameters lead to infeasible solutions), and the second one penalizes suboptimal decisions (by penalizing when the true optimal solution is infeasible under the predicted parameters). We introduce a single tunable parameter to form a weighted average of the two losses, allowing decision-makers to balance suboptimality and feasibility. We experimentally demonstrate that adjusting this parameter provides a decision-maker the control over the trade-off between the two. Moreover, across several COP instances, we find that for a single value of the tunable parameter, our method matches the performance of the existing baselines on suboptimality and feasibility.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.276323"
    },
    {
        "index": "#115",
        "title": "Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion",
        "link": "/arxiv/2510.04947",
        "arxiv_id": "2510.04947",
        "authors": "Xin Li, Kaixiang Yang, Qiang Li, Zhiwei Wang",
        "summary": "Dual-view mammography, including craniocaudal (CC) and mediolateral oblique (MLO) projections, offers complementary anatomical views crucial for breast cancer diagnosis. However, in real-world clinical workflows, one view may be missing, corrupted, or degraded due to acquisition errors or compression artifacts, limiting the effectiveness of downstream analysis. View-to-view translation can help recover missing views and improve lesion alignment. Unlike natural images, this task in mammography is highly challenging due to large non-rigid deformations and severe tissue overlap in X-ray projections, which obscure pixel-level correspondences. In this paper, we propose Column-Aware and Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view translation framework based on conditional diffusion model. To address cross-view structural misalignment, we first design a column-aware cross-attention mechanism that leverages the geometric property that anatomically corresponding regions tend to lie in similar column positions across views. A Gaussian-decayed bias is applied to emphasize local column-wise correlations while suppressing distant mismatches. Furthermore, we introduce an implicit 3D structure reconstruction module that back-projects noisy 2D latents into a coarse 3D feature volume based on breast-view projection geometry. The reconstructed 3D structure is refined and injected into the denoising UNet to guide cross-view generation with enhanced anatomical awareness. Extensive experiments demonstrate that CA3D-Diff achieves superior performance in bidirectional tasks, outperforming state-of-the-art methods in visual fidelity and structural consistency. Furthermore, the synthesized views effectively improve single-view malignancy classification in screening settings, demonstrating the practical value of our method in real-world diagnostics.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.277672"
    },
    {
        "index": "#117",
        "title": "Unsupervised Active Learning via Natural Feature Progressive Framework",
        "link": "/arxiv/2510.04939",
        "arxiv_id": "2510.04939",
        "authors": "Yuxi Liu, Catherine Lalman, Yimin Yang",
        "summary": "The effectiveness of modern deep learning models is predicated on the availability of large-scale, human-annotated datasets, a process that is notoriously expensive and time-consuming. While Active Learning (AL) offers a strategic solution by labeling only the most informative and representative data, its iterative nature still necessitates significant human involvement. Unsupervised Active Learning (UAL) presents an alternative by shifting the annotation burden to a single, post-selection step. Unfortunately, prevailing UAL methods struggle to achieve state-of-the-art performance. These approaches typically rely on local, gradient-based scoring for sample importance estimation, which not only makes them vulnerable to ambiguous and noisy data but also hinders their capacity to select samples that adequately represent the full data distribution. Moreover, their use of shallow, one-shot linear selection falls short of a true UAL paradigm. In this paper, we propose the Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes how sample importance is measured. At its core, NFPF employs a Specific Feature Learning Machine (SFLM) to effectively quantify each sample's contribution to model performance. We further utilize the SFLM to define a powerful Reconstruction Difference metric for initial sample selection. Our comprehensive experiments show that NFPF significantly outperforms all established UAL methods and achieves performance on par with supervised AL methods on vision datasets. Detailed ablation studies and qualitative visualizations provide compelling evidence for NFPF's superior performance, enhanced robustness, and improved data distribution coverage.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.279014"
    },
    {
        "index": "#119",
        "title": "AURA Score: A Metric For Holistic Audio Question Answering Evaluation",
        "link": "/arxiv/2510.04934",
        "arxiv_id": "2510.04934",
        "authors": "Satvik Dixit, Soham Deshmukh, Bhiksha Raj",
        "summary": "Audio Question Answering (AQA) is a key task for evaluating Audio-Language Models (ALMs), yet assessing open-ended responses remains challenging. Existing metrics used for AQA such as BLEU, METEOR and BERTScore, mostly adapted from NLP and audio captioning, rely on surface similarity and fail to account for question context, reasoning, and partial correctness. To address the gap in literature, we make three contributions in this work. First, we introduce AQEval to enable systematic benchmarking of AQA metrics. It is the first benchmark of its kind, consisting of 10k model responses annotated by multiple humans for their correctness and relevance. Second, we conduct a comprehensive analysis of existing AQA metrics on AQEval, highlighting weak correlation with human judgment, especially for longer answers. Third, we propose a new metric - AURA score, to better evaluate open-ended model responses. On AQEval, AURA achieves state-of-the-art correlation with human ratings, significantly outperforming all baselines. Through this work, we aim to highlight the limitations of current AQA evaluation methods and motivate better metrics. We release both the AQEval benchmark and the AURA metric to support future research in holistic AQA evaluation.",
        "subjects": "Audio and Speech Processing, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.280399"
    },
    {
        "index": "#121",
        "title": "Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data",
        "link": "/arxiv/2510.04927",
        "arxiv_id": "2510.04927",
        "authors": "Usman Akram, Yiyue Chen, Haris Vikalo",
        "summary": "Training automatic modulation classification (AMC) models on centrally aggregated data raises privacy concerns, incurs communication overhead, and often fails to confer robustness to channel shifts. Federated learning (FL) avoids central aggregation by training on distributed clients but remains sensitive to class imbalance, non-IID client distributions, and limited labeled samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with triplet-loss self-supervision on unlabeled I/Q sequences across clients, followed by per-client SVMs on small labeled sets. We establish convergence of the federated representation learning procedure and a separability guarantee for the downstream classifier under feature noise. Experiments on synthetic and over-the-air datasets show consistent gains over supervised FL baselines under heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.",
        "subjects": "Machine Learning, Artificial Intelligence, Signal Processing",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.286855"
    },
    {
        "index": "#122",
        "title": "REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis",
        "link": "/arxiv/2510.04923",
        "arxiv_id": "2510.04923",
        "authors": "Alec K. Peltekian, Halil Ertugrul Aktas, Gorkem Durak, Kevin Grudzinski, Bradford C. Bemiss, Carrie Richardson, Jane E. Dematte, G. R. Scott Budinger, Anthony J. Esposito, Alexander Misharin, Alok Choudhary, Ankit Agrawal, Ulas Bagci",
        "summary": "Mixture-of-Experts (MoE) architectures have significantly contributed to scalable machine learning by enabling specialized subnetworks to tackle complex tasks efficiently. However, traditional MoE systems lack domain-specific constraints essential for medical imaging, where anatomical structure and regional disease heterogeneity strongly influence pathological patterns. Here, we introduce Regional Expert Networks (REN), the first anatomically-informed MoE framework tailored specifically for medical image classification. REN leverages anatomical priors to train seven specialized experts, each dedicated to distinct lung lobes and bilateral lung combinations, enabling precise modeling of region-specific pathological variations. Multi-modal gating mechanisms dynamically integrate radiomics biomarkers and deep learning (DL) features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to interstitial lung disease (ILD) classification, REN achieves consistently superior performance: the radiomics-guided ensemble reached an average AUC of 0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC 0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79) and aligning with known disease progression patterns. Through rigorous patient-level cross-validation, REN demonstrates strong generalizability and clinical interpretability, presenting a scalable, anatomically-guided approach readily extensible to other structured medical imaging applications.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.287687"
    },
    {
        "index": "#124",
        "title": "Glocal Information Bottleneck for Time Series Imputation",
        "link": "/arxiv/2510.04910",
        "arxiv_id": "2510.04910",
        "authors": "Jie Yang, Kexin Zhang, Guibin Zhang, Philip S. Yu, Kaize Ding",
        "summary": "Time Series Imputation (TSI), which aims to recover missing values in temporal data, remains a fundamental challenge due to the complex and often high-rate missingness in real-world scenarios. Existing models typically optimize the point-wise reconstruction loss, focusing on recovering numerical values (local information). However, we observe that under high missing rates, these models still perform well in the training phase yet produce poor imputations and distorted latent representation distributions (global information) in the inference phase. This reveals a critical optimization dilemma: current objectives lack global guidance, leading models to overfit local noise and fail to capture global information of the data. To address this issue, we propose a new training paradigm, Glocal Information Bottleneck (Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework by introducing a Global Alignment loss, derived from a tractable mutual information approximation. This loss aligns the latent representations of masked inputs with those of their originally observed counterparts. It helps the model retain global structure and local details while suppressing noise caused by missing values, giving rise to better generalization under high missingness. Extensive experiments on nine datasets confirm that Glocal-IB leads to consistently improved performance and aligned latent representations under missingness. Our code implementation is available in https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.289057"
    },
    {
        "index": "#125",
        "title": "Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects",
        "link": "/arxiv/2510.04901",
        "arxiv_id": "2510.04901",
        "authors": "Jonathan Colaço Carr, Qinyi Sun, Cameron Allen",
        "summary": "Skills are essential for unlocking higher levels of problem solving. A common approach to discovering these skills is to learn ones that reliably reach different states, thus empowering the agent to control its environment. However, existing skill discovery algorithms often overlook the natural state variables present in many reinforcement learning problems, meaning that the discovered skills lack control of specific state variables. This can significantly hamper exploration efficiency, make skills more challenging to learn with, and lead to negative side effects in downstream tasks when the goal is under-specified. We introduce a general method that enables these skill discovery algorithms to learn focused skills -- skills that target and control specific state variables. Our approach improves state space coverage by a factor of three, unlocks new learning capabilities, and automatically avoids negative side effects in downstream tasks.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.289681"
    },
    {
        "index": "#126",
        "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
        "link": "/arxiv/2510.04898",
        "arxiv_id": "2510.04898",
        "authors": "Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson",
        "summary": "Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\\times$, and accelerates inference speed by $120\\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.290396"
    },
    {
        "index": "#128",
        "title": "Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models",
        "link": "/arxiv/2510.04888",
        "arxiv_id": "2510.04888",
        "authors": "Alina Ermilova, Dmitrii Kornilov, Sofia Samoilova, Ekaterina Laptenkova, Anastasia Kolesnikova, Ekaterina Podplutova, Senotrusova Sofya, Maksim G. Sharaev",
        "summary": "Identifying disease interconnections through manual analysis of large-scale clinical data is labor-intensive, subjective, and prone to expert disagreement. While machine learning (ML) shows promise, three critical challenges remain: (1) selecting optimal methods from the vast ML landscape, (2) determining whether real-world clinical data (e.g., electronic health records, EHRs) or structured disease descriptions yield more reliable insights, (3) the lack of \"ground truth,\" as some disease interconnections remain unexplored in medicine. Large language models (LLMs) demonstrate broad utility, yet they often lack specialized medical knowledge. To address these gaps, we conduct a systematic evaluation of seven approaches for uncovering disease relationships based on two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the full set of ICD-10 codes, both with and without textual descriptions. Our framework integrates the following: (i) a statistical co-occurrence analysis and a masked language modeling (MLM) approach using real clinical data; (ii) domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral, DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained interconnection matrices shows that the LLM-based approach produces interconnections with the lowest diversity of ICD code connections to different diseases compared to other methods, including text-based and domain-based approaches. This suggests an important implication: LLMs have limited potential for discovering new interconnections. In the absence of ground truth databases for medical interconnections between ICD codes, our results constitute a valuable medical disease ontology that can serve as a foundational resource for future clinical research and artificial intelligence applications in healthcare.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.296928"
    },
    {
        "index": "#129",
        "title": "Less is More: Recursive Reasoning with Tiny Networks",
        "link": "/arxiv/2510.04871",
        "arxiv_id": "2510.04871",
        "authors": "Alexia Jolicoeur-Martineau",
        "summary": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the parameters.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.297509"
    },
    {
        "index": "#130",
        "title": "Model Predictive Control-Guided Reinforcement Learning for Implicit Balancing",
        "link": "/arxiv/2510.04868",
        "arxiv_id": "2510.04868",
        "authors": "Seyed Soroush Karimi Madahi, Kenneth Bruninx, Bert Claessens, Chris Develder",
        "summary": "In Europe, profit-seeking balance responsible parties can deviate in real time from their day-ahead nominations to assist transmission system operators in maintaining the supply-demand balance. Model predictive control (MPC) strategies to exploit these implicit balancing strategies capture arbitrage opportunities, but fail to accurately capture the price-formation process in the European imbalance markets and face high computational costs. Model-free reinforcement learning (RL) methods are fast to execute, but require data-intensive training and usually rely on real-time and historical data for decision-making. This paper proposes an MPC-guided RL method that combines the complementary strengths of both MPC and RL. The proposed method can effectively incorporate forecasts into the decision-making process (as in MPC), while maintaining the fast inference capability of RL. The performance of the proposed method is evaluated on the implicit balancing battery control problem using Belgian balancing data from 2023. First, we analyze the performance of the standalone state-of-the-art RL and MPC methods from various angles, to highlight their individual strengths and limitations. Next, we show an arbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and 54.36%, compared to standalone RL and MPC.",
        "subjects": "Systems and Control, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.298154"
    },
    {
        "index": "#131",
        "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails",
        "link": "/arxiv/2510.04860",
        "arxiv_id": "2510.04860",
        "authors": "Siwei Han, Jiaqi Liu, Yaofeng Su, Wenbo Duan, Xinyuan Liu, Cihang Xie, Mohit Bansal, Mingyu Ding, Linjun Zhang, Huaxiu Yao",
        "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary capabilities to adapt and refine their strategies through real-world interaction, their long-term reliability becomes a critical concern. We identify the Alignment Tipping Process (ATP), a critical post-deployment risk unique to self-evolving LLM agents. Unlike training-time failures, ATP arises when continual interaction drives agents to abandon alignment constraints established during training in favor of reinforced, self-interested strategies. We formalize and analyze ATP through two complementary paradigms: Self-Interested Exploration, where repeated high-reward deviations induce individual behavioral drift, and Imitative Strategy Diffusion, where deviant behaviors spread across multi-agent systems. Building on these paradigms, we construct controllable testbeds and benchmark Qwen3-8B and Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode rapidly under self-evolution, with initially aligned models converging toward unaligned states. In multi-agent settings, successful violations diffuse quickly, leading to collective misalignment. Moreover, current reinforcement learning-based alignment methods provide only fragile defenses against alignment tipping. Together, these findings demonstrate that alignment of LLM agents is not a static property but a fragile and dynamic one, vulnerable to feedback-driven decay during deployment. Our data and code are available at https://github.com/aiming-lab/ATP.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.298918"
    },
    {
        "index": "#132",
        "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration",
        "link": "/arxiv/2510.04852",
        "arxiv_id": "2510.04852",
        "authors": "Victor May, Diganta Misra, Yanqi Luo, Anjali Sridhar, Justine Gehring, Silvio Soares Ribeiro Junior",
        "summary": "AI coding assistants are rapidly becoming integral to modern software development. A key challenge in this space is the continual need to migrate and modernize codebases in response to evolving software ecosystems. Traditionally, such migrations have relied on rule-based systems and human intervention. With the advent of powerful large language models (LLMs), AI-driven agentic frameworks offer a promising alternative-but their effectiveness has not been systematically evaluated. In this paper, we introduce FreshBrew, a novel benchmark for evaluating AI agents on project-level Java migrations, with a specific focus on measuring an agent's ability to preserve program semantics and avoid reward hacking, which we argue requires projects with high test coverage for a rigorous and reliable evaluation. We benchmark several state-of-the-art LLMs, and compare their performance against established rule-based tools. Our evaluation of AI agents on this benchmark of 228 repositories shows that the top-performing model, Gemini 2.5 Flash, can successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis reveals novel insights into the critical strengths and limitations of current agentic approaches, offering actionable insights into their real-world applicability. Our empirical study reveals failure modes of current AI agents in realistic Java modernization tasks, providing a foundation for evaluating trustworthy code-migration systems. By releasing FreshBrew, we aim to facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven codebase modernization.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.299595"
    },
    {
        "index": "#134",
        "title": "Distributionally Robust Causal Abstractions",
        "link": "/arxiv/2510.04842",
        "arxiv_id": "2510.04842",
        "authors": "Yorgos Felekis, Theodoros Damoulas, Paris Giampouras",
        "summary": "Causal Abstraction (CA) theory provides a principled framework for relating causal models that describe the same system at different levels of granularity while ensuring interventional consistency between them. Recently, several approaches for learning CAs have been proposed, but all assume fixed and well-specified exogenous distributions, making them vulnerable to environmental shifts and misspecification. In this work, we address these limitations by introducing the first class of distributionally robust CAs and their associated learning algorithms. The latter cast robust causal abstraction learning as a constrained min-max optimization problem with Wasserstein ambiguity sets. We provide theoretical results, for both empirical and Gaussian environments, leading to principled selection of the level of robustness via the radius of these sets. Furthermore, we present empirical evidence across different problems and CA learning methods, demonstrating our framework's robustness not only to environmental shifts but also to structural model and intervention mapping misspecification.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.300899"
    },
    {
        "index": "#135",
        "title": "Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study",
        "link": "/arxiv/2510.04837",
        "arxiv_id": "2510.04837",
        "authors": "Guillaume Godin",
        "summary": "Bond Centered FingerPrint (BCFP) are a complementary, bond-centric alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static BCFP that mirrors the bond-convolution used by directed message-passing GNNs like ChemProp, and evaluate it with a fast rapid Random Forest model on Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified cross-validation, concatenating ECFP with BCFP consistently improves AUROC and AUPRC over either descriptor alone, as confirmed by Turkey HSD multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not yield statistically separable gains under the same test. We further propose BCFP-Sort&Slice, a simple feature-combination scheme that preserves the out-of-vocabulary (OOV) count information native to ECFP count vectors while enabling compact unhashed concatenation of BCFP variants. We also outperform the MGTP prediction on our BBBP evaluation, using such composite new features bond and atom features. These results show that lightweight, bond-centered descriptors can complement atom-centered circular fingerprints and provide strong, fast baselines for BBBP prediction.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.311520"
    },
    {
        "index": "#136",
        "title": "On Predicting Post-Click Conversion Rate via Counterfactual Inference",
        "link": "/arxiv/2510.04816",
        "arxiv_id": "2510.04816",
        "authors": "Junhyung Ahn, Sanghack Lee",
        "summary": "Accurately predicting conversion rate (CVR) is essential in various recommendation domains such as online advertising systems and e-commerce. These systems utilize user interaction logs, which consist of exposures, clicks, and conversions. CVR prediction models are typically trained solely based on clicked samples, as conversions can only be determined following clicks. However, the sparsity of clicked instances necessitates the collection of a substantial amount of logs for effective model training. Recent works address this issue by devising frameworks that leverage non-clicked samples. While these frameworks aim to reduce biases caused by the discrepancy between clicked and non-clicked samples, they often rely on heuristics. Against this background, we propose a method to counterfactually generate conversion labels for non-clicked samples by using causality as a guiding principle, attempting to answer the question, \"Would the user have converted if he or she had clicked the recommended item?\" Our approach is named the Entire Space Counterfactual Inference Multi-task Model (ESCIM). We initially train a structural causal model (SCM) of user sequential behaviors and conduct a hypothetical intervention (i.e., click) on non-clicked items to infer counterfactual CVRs. We then introduce several approaches to transform predicted counterfactual CVRs into binary counterfactual conversion labels for the non-clicked samples. Finally, the generated samples are incorporated into the training process. Extensive experiments on public datasets illustrate the superiority of the proposed algorithm. Online A/B testing further empirically validates the effectiveness of our proposed algorithm in real-world scenarios. In addition, we demonstrate the improved performance of the proposed method on latent conversion data, showcasing its robustness and superior generalization capabilities.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.312430"
    },
    {
        "index": "#137",
        "title": "Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors",
        "link": "/arxiv/2510.04802",
        "arxiv_id": "2510.04802",
        "authors": "Han Zhang, Lalithkumar Seenivasan, Jose L. Porras, Roger D. Soberanis-Mukul, Hao Ding, Hongchao Shu, Benjamin D. Killeen, Ankita Ghosh, Lonny Yarmus, Masaru Ishii, Angela Christine Argento, Mathias Unberath",
        "summary": "Observing surgical practice has historically relied on fixed vantage points or recollections, leaving the egocentric visual perspectives that guide clinical decisions undocumented. Fixed-camera video can capture surgical workflows at the room-scale, but cannot reconstruct what each team member actually saw. Thus, these videos only provide limited insights into how decisions that affect surgical safety, training, and workflow optimization are made. Here we introduce EgoSurg, the first framework to reconstruct the dynamic, egocentric replays for any operating room (OR) staff directly from wall-mounted fixed-camera video, and thus, without intervention to clinical workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based view enhancement, enabling high-visual fidelity synthesis of arbitrary and egocentric viewpoints at any moment. In evaluation across multi-site surgical cases and controlled studies, EgoSurg reconstructs person-specific visual fields and arbitrary viewpoints with high visual quality and fidelity. By transforming existing OR camera infrastructure into a navigable dynamic 3D record, EgoSurg establishes a new foundation for immersive surgical data science, enabling surgical practice to be visualized, experienced, and analyzed from every angle.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.313541"
    },
    {
        "index": "#138",
        "title": "DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing",
        "link": "/arxiv/2510.04797",
        "arxiv_id": "2510.04797",
        "authors": "Qi Li, Shuwen Qiu, Julien Han, Xingzi Xu, Mehmet Saygin Seyfioglu, Kee Kiat Koo, Karim Bouyarmane",
        "summary": "The rapid growth of e-commerce has intensified the demand for Virtual Try-On (VTO) technologies, enabling customers to realistically visualize products overlaid on their own images. Despite recent advances, existing VTO models face challenges with fine-grained detail preservation, robustness to real-world imagery, efficient sampling, image editing capabilities, and generalization across diverse product categories. In this paper, we present DiT-VTON, a novel VTO framework that leverages a Diffusion Transformer (DiT), renowned for its performance on text-conditioned image generation, adapted here for the image-conditioned VTO task. We systematically explore multiple DiT configurations, including in-context token concatenation, channel concatenation, and ControlNet integration, to determine the best setup for VTO image conditioning. To enhance robustness, we train the model on an expanded dataset encompassing varied backgrounds, unstructured references, and non-garment categories, demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also redefines the VTO task beyond garment try-on, offering a versatile Virtual Try-All (VTA) solution capable of handling a wide range of product categories and supporting advanced image editing functionalities such as pose preservation, localized editing, texture transfer, and object-level customization. Experimental results show that our model surpasses state-of-the-art methods on VITON-HD, achieving superior detail preservation and robustness without reliance on additional condition encoders. It also outperforms models with VTA and image editing capabilities on a diverse dataset spanning thousands of product categories.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.314466"
    },
    {
        "index": "#140",
        "title": "Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning",
        "link": "/arxiv/2510.04786",
        "arxiv_id": "2510.04786",
        "authors": "Jonas Hübotter, Leander Diaz-Bone, Ido Hakimi, Andreas Krause, Moritz Hardt",
        "summary": "Humans are good at learning on the job: We learn how to solve the tasks we face as we go along. Can a model do the same? We propose an agent that assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task. The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from a large pool of available training data. Our experiments demonstrate that reinforcement learning on a test-time curriculum consistently improves the model on its target tasks, across a variety of evaluations and models. Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%. Our findings show the potential of test-time curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.316317"
    },
    {
        "index": "#142",
        "title": "Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning",
        "link": "/arxiv/2510.04773",
        "arxiv_id": "2510.04773",
        "authors": "Kai Qin, Jiaqi Wu, Jianxiang He, Haoyuan Sun, Yifei Zhao, Bin Liang, Yongzhe Chang, Tiantian Zhang, Houde Liu",
        "summary": "As Large Language Models (LLMs) demonstrate remarkable capabilities learned from vast corpora, concerns regarding data privacy and safety are receiving increasing attention. LLM unlearning, which aims to remove the influence of specific data while preserving overall model utility, is becoming an important research area. One of the mainstream unlearning classes is optimization-based methods, which achieve forgetting directly through fine-tuning, exemplified by Negative Preference Optimization (NPO). However, NPO's effectiveness is limited by its inherent lack of explicit positive preference signals. Attempts to introduce such signals by constructing preferred responses often necessitate domain-specific knowledge or well-designed prompts, fundamentally restricting their generalizability. In this paper, we shift the focus to the distribution-level, directly targeting the next-token probability distribution instead of entire responses, and derive a novel unlearning algorithm termed \\textbf{Di}stribution \\textbf{P}reference \\textbf{O}ptimization (DiPO). We show that the requisite preference distribution pairs for DiPO, which are distributions over the model's output tokens, can be constructed by selectively amplifying or suppressing the model's high-confidence output logits, thereby effectively overcoming NPO's limitations. We theoretically prove the consistency of DiPO's loss function with the desired unlearning direction. Extensive experiments demonstrate that DiPO achieves a strong trade-off between model utility and forget quality. Notably, DiPO attains the highest forget quality on the TOFU benchmark, and maintains leading scalability and sustainability in utility preservation on the MUSE benchmark.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.323002"
    },
    {
        "index": "#143",
        "title": "When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates",
        "link": "/arxiv/2510.04769",
        "arxiv_id": "2510.04769",
        "authors": "Michele Caprio, Siu Lun Chau, Krikamol Muandet",
        "summary": "Many machine learning algorithms rely on iterative updates of uncertainty representations, ranging from variational inference and expectation-maximization, to reinforcement learning, continual learning, and multi-agent learning. In the presence of imprecision and ambiguity, credal sets -- closed, convex sets of probability distributions -- have emerged as a popular framework for representing imprecise probabilistic beliefs. Under such imprecision, many learning problems in imprecise probabilistic machine learning (IPML) may be viewed as processes involving successive applications of update rules on credal sets. This naturally raises the question of whether this iterative process converges to stable fixed points -- or, more generally, under what conditions on the updating mechanism such fixed points exist, and whether they can be attained. We provide the first analysis of this problem and illustrate our findings using Credal Bayesian Deep Learning as a concrete example. Our work demonstrates that incorporating imprecision into the learning process not only enriches the representation of uncertainty, but also reveals structural conditions under which stability emerges, thereby offering new insights into the dynamics of iterative learning under imprecision.",
        "subjects": "Machine Learning, Artificial Intelligence, Probability, Statistics Theory, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.323760"
    },
    {
        "index": "#144",
        "title": "Fisher-Bingham-like normalizing flows on the sphere",
        "link": "/arxiv/2510.04762",
        "arxiv_id": "2510.04762",
        "authors": "Thorsten Glüsenkamp",
        "summary": "A generic D-dimensional Gaussian can be conditioned or projected onto the D-1 unit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular Gaussian (AG) distribution families, respectively. These are some of the most fundamental distributions on the sphere, yet cannot straightforwardly be written as a normalizing flow except in two special cases: the von-Mises Fisher in D=3 and the central angular Gaussian in any D. In this paper, we describe how to generalize these special cases to a family of normalizing flows that behave similarly to the full FB or AG family in any D. We call them \"zoom-linear-project\" (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham distribution, their composition allows to gradually add complexity as needed. Furthermore, they can naturally handle conditional density estimation with target distributions that vary by orders of magnitude in scale - a setting that is important in astronomical applications but that existing flows often struggle with. A particularly useful member of the new family is the Kent analogue that can cheaply upgrade any flow in this situation to yield better performance.",
        "subjects": "Machine Learning, Instrumentation and Methods for Astrophysics, Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.324431"
    },
    {
        "index": "#145",
        "title": "Agile Software Effort Estimation using Regression Techniques",
        "link": "/arxiv/2510.04760",
        "arxiv_id": "2510.04760",
        "authors": "Sisay Deresa Sima, Ayalew Belay Habtie",
        "summary": "Software development effort estimation is one of the most critical aspect in software development process, as the success or failure of the entire project depends on the accuracy of estimations. Researchers are still conducting studies on agile effort estimation. The aim of this research is to develop a story point based agile effort estimation model using LASSO and Elastic Net regression techniques. The experimental work is applied to the agile story point approach using 21 software projects collected from six firms. The two algorithms are trained using their default parameters and tuned grid search with 5-fold cross-validation to get an enhanced model. The experiment result shows LASSO regression achieved better predictive performance PRED (8%) and PRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593, MdMER of 0.063, and MSE of 0.0007. The results are also compared with other related literature.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.325061"
    },
    {
        "index": "#146",
        "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction",
        "link": "/arxiv/2510.04759",
        "arxiv_id": "2510.04759",
        "authors": "Chi Yan, Dan Xu",
        "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent years, playing a crucial role in vision-based autonomous driving systems. While traditional methods are limited to fixed semantic categories, recent approaches have moved towards predicting text-aligned features to enable open-vocabulary text queries in real-world scenes. However, there exists a trade-off in text-aligned scene modeling: sparse Gaussian representation struggles to capture small objects in the scene, while dense representation incurs significant computational overhead. To address these limitations, we present PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables open-vocabulary 3D occupancy prediction. Our framework employs progressive online densification, a feed-forward strategy that gradually enhances the 3D Gaussian representation to capture fine-grained scene details. By iteratively enhancing the representation, the framework achieves increasingly precise and detailed scene understanding. Another key contribution is the introduction of an anisotropy-aware sampling strategy with spatio-temporal fusion, which adaptively assigns receptive fields to Gaussians at different scales and stages, enabling more effective feature aggregation and richer scene information capture. Through extensive evaluations, we demonstrate that PG-Occ achieves state-of-the-art performance with a relative 14.3% mIoU improvement over the previous best performing method. Code and pretrained models will be released upon publication on our project page: https://yanchi-3dv.github.io/PG-Occ",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.325936"
    },
    {
        "index": "#147",
        "title": "A New Digital Divide? Coder Worldviews, the Slop Economy, and Democracy in the Age of AI",
        "link": "/arxiv/2510.04755",
        "arxiv_id": "2510.04755",
        "authors": "Jason Miklian, Kristian Hoelscher",
        "summary": "Digital technologies are transforming democratic life in conflicting ways. This article bridges two perspectives to unpack these tensions. First, we present an original survey of software developers in Silicon Valley, interrogating how coder worldviews, ethics, and workplace cultures shape the democratic potential and social impact of the technologies they build. Results indicate that while most developers recognize the power of their products to influence civil liberties and political discourse, they often face ethical dilemmas and top-down pressures that can lead to design choices undermining democratic ideals. Second, we critically investigate these findings in the context of an emerging new digital divide, not of internet access but of information quality. We interrogate the survey findings in the context of the Slop Economy, in which billions of users unable to pay for high-quality content experience an internet dominated by low-quality, AI-generated ad-driven content. We find a reinforcing cycle between tech creator beliefs and the digital ecosystems they spawn. We discuss implications for democratic governance, arguing for more ethically informed design and policy interventions to help bridge the digital divide to ensure that technological innovation supports rather than subverts democratic values in the next chapter of the digital age.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.326570"
    },
    {
        "index": "#149",
        "title": "Curved Boolean Logic: A Contextual Generalization of Propositional Logic with Algorithmic Consequences",
        "link": "/arxiv/2510.04716",
        "arxiv_id": "2510.04716",
        "authors": "Maximilian R. P. von Liechtenstein",
        "summary": "Curved Boolean Logic (CBL) generalizes propositional logic by allowing local truth assignments that do not extend to a single global valuation, analogous to curvature in geometry. We give equivalent sheaf and exclusivity-graph semantics and a context-aware proof calculus that is conservative in the flat limit. We formalize CBL-SAT and basic complexity (NP-complete in general) and present operational operators (CBL-AC and CBL-CONS) that prune contradictions earlier on classical hardware. We model noise with iid, AR(1)-correlated, and adversarial bounded perturbations and provide permutation-based significance with Benjamini-Hochberg FDR control. A Colab-ready notebook (ancillary files) regenerates all figures and statistics. We position CBL relative to KCBS, CSW, and sheaf frameworks and outline links to SAT/CSP and robustness/adapter stability in large language models.",
        "subjects": "Logic in Computer Science, Artificial Intelligence, Computational Complexity, Quantum Physics",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.328025"
    },
    {
        "index": "#151",
        "title": "The Bayesian Origin of the Probability Weighting Function in Human Representation of Probabilities",
        "link": "/arxiv/2510.04698",
        "arxiv_id": "2510.04698",
        "authors": "Xin Tong, Thi Thu Uyen Hoang, Xue-Xin Wei, Michael Hahn",
        "summary": "Understanding the representation of probability in the human mind has been of great interest to understanding human decision making. Classical paradoxes in decision making suggest that human perception distorts probability magnitudes. Previous accounts postulate a Probability Weighting Function that transforms perceived probabilities; however, its motivation has been debated. Recent work has sought to motivate this function in terms of noisy representations of probabilities in the human mind. Here, we present an account of the Probability Weighting Function grounded in rational inference over optimal decoding from noisy neural encoding of quantities. We show that our model accurately accounts for behavior in a lottery task and a dot counting task. It further accounts for adaptation to a bimodal short-term prior. Taken together, our results provide a unifying account grounding the human representation of probability in rational inference.",
        "subjects": "Neurons and Cognition, Artificial Intelligence, Theoretical Economics",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.329544"
    },
    {
        "index": "#153",
        "title": "Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies",
        "link": "/arxiv/2510.04692",
        "arxiv_id": "2510.04692",
        "authors": "Lyes Saad Saoud, Irfan Hussain",
        "summary": "Biomimetic intelligence and robotics are transforming field ecology by enabling lifelike robotic surrogates that interact naturally with animals under real world conditions. Studying avian behavior in the wild remains challenging due to the need for highly realistic morphology, durable outdoor operation, and intelligent perception that can adapt to uncontrolled environments. We present a next generation bio inspired robotic platform that replicates the morphology and visual appearance of the female Houbara bustard to support controlled ethological studies and conservation oriented field research. The system introduces a fully digitally replicable fabrication workflow that combines high resolution structured light 3D scanning, parametric CAD modelling, articulated 3D printing, and photorealistic UV textured vinyl finishing to achieve anatomically accurate and durable robotic surrogates. A six wheeled rocker bogie chassis ensures stable mobility on sand and irregular terrain, while an embedded NVIDIA Jetson module enables real time RGB and thermal perception, lightweight YOLO based detection, and an autonomous visual servoing loop that aligns the robot's head toward detected targets without human intervention. A lightweight thermal visible fusion module enhances perception in low light conditions. Field trials in desert aviaries demonstrated reliable real time operation at 15 to 22 FPS with latency under 100 ms and confirmed that the platform elicits natural recognition and interactive responses from live Houbara bustards under harsh outdoor conditions. This integrated framework advances biomimetic field robotics by uniting reproducible digital fabrication, embodied visual intelligence, and ecological validation, providing a transferable blueprint for animal robot interaction research, conservation robotics, and public engagement.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.336024"
    },
    {
        "index": "#154",
        "title": "How does the optimizer implicitly bias the model merging loss landscape?",
        "link": "/arxiv/2510.04686",
        "arxiv_id": "2510.04686",
        "authors": "Chenxiang Zhang, Alexander Theus, Damien Teney, Antonio Orvieto, Jun Pang, Sjouke Mauw",
        "summary": "Model merging methods combine models with different capabilities into a single one while maintaining the same inference cost. Two popular approaches are linear interpolation, which linearly interpolates between model weights, and task arithmetic, which combines task vectors obtained by the difference between finetuned and base models. While useful in practice, what properties make merging effective are poorly understood. This paper explores how the optimization process affects the loss landscape geometry and its impact on merging success. We show that a single quantity -- the effective noise scale -- unifies the impact of optimizer and data choices on model merging. Across architectures and datasets, the effectiveness of merging success is a non-monotonic function of effective noise, with a distinct optimum. Decomposing this quantity, we find that larger learning rates, stronger weight decay, smaller batch sizes, and data augmentation all independently modulate the effective noise scale, exhibiting the same qualitative trend. Unlike prior work that connects optimizer noise to the flatness or generalization of individual minima, we show that it also affects the global loss landscape, predicting when independently trained solutions can be merged. Our findings broaden the understanding of how optimization shapes the loss landscape geometry and its downstream consequences for model merging, suggesting the possibility of further manipulating the training dynamics to improve merging effectiveness.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.336748"
    },
    {
        "index": "#156",
        "title": "Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding",
        "link": "/arxiv/2510.04674",
        "arxiv_id": "2510.04674",
        "authors": "Lorenzo Pannacci, Simone Fiorellino, Mario Edoardo Pandolfo, Emilio Calvanese Strinati, Paolo Di Lorenzo",
        "summary": "Deep joint source-channel coding (DeepJSCC) has emerged as a powerful paradigm for end-to-end semantic communications, jointly learning to compress and protect task-relevant features over noisy channels. However, existing DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver (RX) - an assumption that fails in multi-vendor deployments where encoders and decoders cannot be co-trained. This mismatch introduces \"semantic noise\", degrading reconstruction quality and downstream task performance. In this paper, we systematize and evaluate methods for semantic channel equalization for DeepJSCC, introducing an additional processing stage that aligns heterogeneous latent spaces under both physical and semantic impairments. We investigate three classes of aligners: (i) linear maps, which admit closed-form solutions; (ii) lightweight neural networks, offering greater expressiveness; and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without the need for training. Through extensive experiments on image reconstruction over AWGN and fading channels, we quantify trade-offs among complexity, data efficiency, and fidelity, providing guidelines for deploying DeepJSCC in heterogeneous AI-native wireless networks.",
        "subjects": "Machine Learning, Artificial Intelligence, Information Theory, Networking and Internet Architecture",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.338114"
    },
    {
        "index": "#158",
        "title": "Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting",
        "link": "/arxiv/2510.04667",
        "arxiv_id": "2510.04667",
        "authors": "Fanzhe Fu, Yang Yang",
        "summary": "Reversible Instance Normalization (RevIN) is a key technique enabling simple linear models to achieve state-of-the-art performance in time series forecasting. While replacing its non-robust statistics with robust counterparts (termed R$^2$-IN) seems like a straightforward improvement, our findings reveal a far more complex reality. This paper deconstructs the perplexing performance of various normalization strategies by identifying four underlying theoretical contradictions. Our experiments provide two crucial findings: first, the standard RevIN catastrophically fails on datasets with extreme outliers, where its MSE surges by a staggering 683\\%. Second, while the simple R$^2$-IN prevents this failure and unexpectedly emerges as the best overall performer, our adaptive model (A-IN), designed to test a diagnostics-driven heuristic, unexpectedly suffers a complete and systemic failure. This surprising outcome uncovers a critical, overlooked pitfall in time series analysis: the instability introduced by a simple or counter-intuitive heuristic can be more damaging than the statistical issues it aims to solve. The core contribution of this work is thus a new, cautionary paradigm for time series normalization: a shift from a blind search for complexity to a diagnostics-driven analysis that reveals not only the surprising power of simple baselines but also the perilous nature of naive adaptation.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.339503"
    },
    {
        "index": "#159",
        "title": "Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation",
        "link": "/arxiv/2510.04646",
        "arxiv_id": "2510.04646",
        "authors": "Johanna Sommer, John Rachwan, Nils Fleischmann, Stephan Günnemann, Bertrand Charpentier",
        "summary": "Flow matching models generate high-fidelity molecular geometries but incur significant computational costs during inference, requiring hundreds of network evaluations. This inference overhead becomes the primary bottleneck when such models are employed in practice to sample large numbers of molecular candidates. This work discusses a training-free caching strategy that accelerates molecular geometry generation by predicting intermediate hidden states across solver steps. The proposed method operates directly on the SE(3)-equivariant backbone, is compatible with pretrained models, and is orthogonal to existing training-based accelerations and system-level optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching achieves a twofold reduction in wall-clock inference time at matched sample quality and a speedup of up to 3x compared to the base model with minimal sample quality degradation. Because these gains compound with other optimizations, applying caching alongside other general, lossless optimizations yield as much as a 7x speedup.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.340186"
    },
    {
        "index": "#160",
        "title": "SFANet: Spatial-Frequency Attention Network for Deepfake Detection",
        "link": "/arxiv/2510.04630",
        "arxiv_id": "2510.04630",
        "authors": "Vrushank Ahire, Aniruddh Muley, Shivam Zample, Siddharth Verma, Pranav Menon, Surbhi Madan, Abhinav Dhall",
        "summary": "Detecting manipulated media has now become a pressing issue with the recent rise of deepfakes. Most existing approaches fail to generalize across diverse datasets and generation techniques. We thus propose a novel ensemble framework, combining the strengths of transformer-based architectures, such as Swin Transformers and ViTs, and texture-based methods, to achieve better detection accuracy and robustness. Our method introduces innovative data-splitting, sequential training, frequency splitting, patch-based attention, and face segmentation techniques to handle dataset imbalances, enhance high-impact regions (e.g., eyes and mouth), and improve generalization. Our model achieves state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse subset of eight deepfake datasets. The ensemble benefits from the complementarity of these approaches, with transformers excelling in global feature extraction and texturebased methods providing interpretability. This work demonstrates that hybrid models can effectively address the evolving challenges of deepfake detection, offering a robust solution for real-world applications.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Multimedia",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.346137"
    },
    {
        "index": "#163",
        "title": "Design Process of a Self Adaptive Smart Serious Games Ecosystem",
        "link": "/arxiv/2510.04615",
        "arxiv_id": "2510.04615",
        "authors": "X. Tao, P. Chen, M. Tsami, F. Khayati, M. Eckert",
        "summary": "This paper outlines the design vision and planned evolution of Blexer v3, a modular and AI-driven rehabilitation ecosystem based on serious games. Building on insights from previous versions of the system, we propose a new architecture that aims to integrate multimodal sensing, real-time reasoning, and intelligent control. The envisioned system will include distinct modules for data collection, user state inference, and gameplay adaptation. Key features such as dynamic difficulty adjustment (DDA) and procedural content generation (PCG) are also considered to support personalized interventions. We present the complete conceptual framework of Blexer v3, which defines the modular structure and data flow of the system. This serves as the foundation for the next phase: the development of a functional prototype and its integration into clinical rehabilitation scenarios.",
        "subjects": "Systems and Control, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.348464"
    },
    {
        "index": "#164",
        "title": "Accountability Capture: How Record-Keeping to Support AI Transparency and Accountability (Re)shapes Algorithmic Oversight",
        "link": "/arxiv/2510.04609",
        "arxiv_id": "2510.04609",
        "authors": "Shreya Chappidi, Jennifer Cobbe, Chris Norval, Anjali Mazumder, Jatinder Singh",
        "summary": "Accountability regimes typically encourage record-keeping to enable the transparency that supports oversight, investigation, contestation, and redress. However, implementing such record-keeping can introduce considerations, risks, and consequences, which so far remain under-explored. This paper examines how record-keeping practices bring algorithmic systems within accountability regimes, providing a basis to observe and understand their effects. For this, we introduce, describe, and elaborate 'accountability capture' -- the re-configuration of socio-technical processes and the associated downstream effects relating to record-keeping for algorithmic accountability. Surveying 100 practitioners, we evidence and characterise record-keeping issues in practice, identifying their alignment with accountability capture. We further document widespread record-keeping practices, tensions between internal and external accountability requirements, and evidence of employee resistance to practices imposed through accountability capture. We discuss these and other effects for surveillance, privacy, and data protection, highlighting considerations for algorithmic accountability communities. In all, we show that implementing record-keeping to support transparency in algorithmic accountability regimes can itself bring wider implications -- an issue requiring greater attention from practitioners, researchers, and policymakers alike.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.349144"
    },
    {
        "index": "#165",
        "title": "A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents",
        "link": "/arxiv/2510.04607",
        "arxiv_id": "2510.04607",
        "authors": "Yuan Wang, Mingyu Li, Haibo Chen",
        "summary": "Computer-use agents (CUAs) powered by large language models (LLMs) have emerged as a promising approach to automating computer tasks, yet they struggle with graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to decompose high-level goals into lengthy, error-prone sequences of fine-grained actions, resulting in low success rates and an excessive number of LLM calls. We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms existing GUIs into three declarative primitives: access, state, and observation, which are better suited for LLMs. Our key idea is policy-mechanism separation: LLMs focus on high-level semantic planning (policy) while GOI handles low-level navigation and interaction (mechanism). GOI does not require modifying the application source code or relying on application programming interfaces (APIs). We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on Windows. Compared to a leading GUI-based agent baseline, GOI improves task success rates by 67% and reduces interaction steps by 43.5%. Notably, GOI completes over 61% of successful tasks with a single LLM call.",
        "subjects": "Operating Systems, Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.349847"
    },
    {
        "index": "#166",
        "title": "Computing Wasserstein Barycenters through Gradient Flows",
        "link": "/arxiv/2510.04602",
        "arxiv_id": "2510.04602",
        "authors": "Eduardo Fernandes Montesuma, Yassir Bendou, Mike Gartrell",
        "summary": "Wasserstein barycenters provide a powerful tool for aggregating probability measures, while leveraging the geometry of their ambient space. Existing discrete methods suffer from poor scalability, as they require access to the complete set of samples from input measures. We address this issue by recasting the original barycenter problem as a gradient flow in the Wasserstein space. Our approach offers two advantages. First, we achieve scalability by sampling mini-batches from the input measures. Second, we incorporate functionals over probability measures, which regularize the barycenter problem through internal, potential, and interaction energies. We present two algorithms for empirical and Gaussian mixture measures, providing convergence guarantees under the Polyak-Łojasiewicz inequality. Experimental validation on toy datasets and domain adaptation benchmarks show that our methods outperform previous discrete and neural net-based methods for computing Wasserstein barycenters.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.350551"
    },
    {
        "index": "#167",
        "title": "SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator",
        "link": "/arxiv/2510.04576",
        "arxiv_id": "2510.04576",
        "authors": "Yuhta Takida, Satoshi Hayakawa, Takashi Shibuya, Masaaki Imaizumi, Naoki Murata, Bac Nguyen, Toshimitsu Uesaka, Chieh-Hsin Lai, Yuki Mitsufuji",
        "summary": "Deep generative models have made significant advances in generating complex content, yet conditional generation remains a fundamental challenge. Existing conditional generative adversarial networks often struggle to balance the dual objectives of assessing authenticity and conditional alignment of input samples within their conditional discriminators. To address this, we propose a novel discriminator design that integrates three key capabilities: unconditional discrimination, matching-aware supervision to enhance alignment sensitivity, and adaptive weighting to dynamically balance all objectives. Specifically, we introduce Sum of Naturalness and Alignment (SONA), which employs separate projections for naturalness (authenticity) and alignment in the final layer with an inductive bias, supported by dedicated objective functions and an adaptive weighting mechanism. Extensive experiments on class-conditional generation tasks show that \\ours achieves superior sample quality and conditional alignment compared to state-of-the-art methods. Furthermore, we demonstrate its effectiveness in text-to-image generation, confirming the versatility and robustness of our approach.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.356533"
    },
    {
        "index": "#168",
        "title": "Deep learning framework for predicting stochastic take-off and die-out of early spreading",
        "link": "/arxiv/2510.04574",
        "arxiv_id": "2510.04574",
        "authors": "Wenchao He, Tao Jia",
        "summary": "Large-scale outbreaks of epidemics, misinformation, or other harmful contagions pose significant threats to human society, yet the fundamental question of whether an emerging outbreak will escalate into a major epidemic or naturally die out remains largely unaddressed. This problem is challenging, partially due to inadequate data during the early stages of outbreaks and also because established models focus on average behaviors of large epidemics rather than the stochastic nature of small transmission chains. Here, we introduce the first systematic framework for forecasting whether initial transmission events will amplify into major outbreaks or fade into extinction during early stages, when intervention strategies can still be effectively implemented. Using extensive data from stochastic spreading models, we developed a deep learning framework that predicts early-stage spreading outcomes in real-time. Validation across Erdős-Rényi and Barabási-Albert networks with varying infectivity levels shows our method accurately forecasts stochastic spreading events well before potential outbreaks, demonstrating robust performance across different network structures and infectivity scenarios.To address the challenge of sparse data during early outbreak stages, we further propose a pretrain-finetune framework that leverages diverse simulation data for pretraining and adapts to specific scenarios through targeted fine-tuning. The pretrain-finetune framework consistently outperforms baseline models, achieving superior performance even when trained on limited scenario-specific data. To our knowledge, this work presents the first framework for predicting stochastic take-off versus die-out. This framework provides valuable insights for epidemic preparedness and public health decision-making, enabling more informed early intervention strategies.",
        "subjects": "Social and Information Networks, Artificial Intelligence, Physics and Society",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.357202"
    },
    {
        "index": "#170",
        "title": "GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning",
        "link": "/arxiv/2510.04567",
        "arxiv_id": "2510.04567",
        "authors": "Weishuo Ma, Yanbo Wang, Xiyuan Wang, Lei Zou, Muhan Zhang",
        "summary": "Graph Neural Networks (GNNs) are powerful tools for precessing relational data but often struggle to generalize to unseen graphs, giving rise to the development of Graph Foundational Models (GFMs). However, current GFMs are challenged by the extreme heterogeneity of graph data, where each graph can possess a unique feature space, label set, and topology. To address this, two main paradigms have emerged. The first leverages Large Language Models (LLMs), but is fundamentally text-dependent, thus struggles to handle the numerical features in vast graphs. The second pre-trains a structure-based model, but the adaptation to new tasks typically requires a costly, per-graph tuning stage, creating a critical efficiency bottleneck. In this work, we move beyond these limitations and introduce \\textbf{G}raph \\textbf{I}n-context \\textbf{L}earning \\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free architecture. GILT introduces a novel token-based framework for in-context learning (ICL) on graphs, reframing classification tasks spanning node, edge and graph levels in a unified framework. This mechanism is the key to handling heterogeneity, as it is designed to operate on generic numerical features. Further, its ability to understand class semantics dynamically from the context enables tuning-free adaptation. Comprehensive experiments show that GILT achieves stronger few-shot performance with significantly less time than LLM-based or tuning-based baselines, validating the effectiveness of our approach.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.358670"
    },
    {
        "index": "#171",
        "title": "3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG",
        "link": "/arxiv/2510.04536",
        "arxiv_id": "2510.04536",
        "authors": "Shun-ichiro Hayashi, Daichi Mukunoki, Tetsuya Hoshino, Satoshi Ohshima, Takahiro Katagiri",
        "summary": "This paper proposes \"3Dify,\" a procedural 3D computer graphics (3D-CG) generation framework utilizing Large Language Models (LLMs). The framework enables users to generate 3D-CG content solely through natural language instructions. 3Dify is built upon Dify, an open-source platform for AI application development, and incorporates several state-of-the-art LLM-related technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented Generation (RAG). For 3D-CG generation support, 3Dify automates the operation of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not support MCP-based interaction, the framework employs the Computer-Using Agent (CUA) method to automate Graphical User Interface (GUI) operations. Moreover, to enhance image generation quality, 3Dify allows users to provide feedback by selecting preferred images from multiple candidates. The LLM then learns variable patterns from these selections and applies them to subsequent generations. Furthermore, 3Dify supports the integration of locally deployed LLMs, enabling users to utilize custom-developed models and to reduce both time and monetary costs associated with external API calls by leveraging their own computational resources.",
        "subjects": "Graphics, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.359397"
    },
    {
        "index": "#172",
        "title": "Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers",
        "link": "/arxiv/2510.04528",
        "arxiv_id": "2510.04528",
        "authors": "Santhosh KumarRavindran",
        "summary": "The rapid adoption of large language models (LLMs) in enterprise systems exposes vulnerabilities to prompt injection attacks, strategic deception, and biased outputs, threatening security, trust, and fairness. Extending our adversarial activation patching framework (arXiv:2507.09406), which induced deception in toy networks at a 23.9% rate, we introduce the Unified Threat Detection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for enterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through 700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for prompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs via enhanced patching; and (3) 78% improvement in fairness metrics (e.g., demographic bias). Novel contributions include a generalized patching algorithm for multi-threat detection, three groundbreaking hypotheses on threat interactions (e.g., threat chaining in enterprise workflows), and a deployment-ready toolkit with APIs for enterprise integration.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.360020"
    },
    {
        "index": "#173",
        "title": "Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction",
        "link": "/arxiv/2510.04522",
        "arxiv_id": "2510.04522",
        "authors": "Yisen Gao, Xingcheng Fu, Qingyun Sun, Jianxin Li, Xianxian Li",
        "summary": "Graph diffusion models have made significant progress in learning structured graph data and have demonstrated strong potential for predictive tasks. Existing approaches typically embed node, edge, and graph-level features into a unified latent space, modeling prediction tasks including classification and regression as a form of conditional generation. However, due to the non-Euclidean nature of graph data, features of different curvatures are entangled in the same latent space without releasing their geometric potential. To address this issue, we aim to construt an ideal Riemannian diffusion model to capture distinct manifold signatures of complex graph data and learn their distribution. This goal faces two challenges: numerical instability caused by exponential mapping during the encoding proces and manifold deviation during diffusion generation. To address these challenges, we propose GeoMancer: a novel Riemannian graph diffusion framework for both generation and prediction tasks. To mitigate numerical instability, we replace exponential mapping with an isometric-invariant Riemannian gyrokernel approach and decouple multi-level features onto their respective task-specific manifolds to learn optimal representations. To address manifold deviation, we introduce a manifold-constrained diffusion method and a self-guided strategy for unconditional generation, ensuring that the generated data remains aligned with the manifold signature. Extensive experiments validate the effectiveness of our approach, demonstrating superior performance across a variety of tasks.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.360720"
    },
    {
        "index": "#180",
        "title": "SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection",
        "link": "/arxiv/2510.04472",
        "arxiv_id": "2510.04472",
        "authors": "Baber Jan, Saeed Anwar, Aiman H. El-Maleh, Abdul Jabbar Siddiqui, Abdul Bais",
        "summary": "Camouflaged object detection segments objects with intrinsic similarity and edge disruption. Current detection methods rely on accumulated complex components. Each approach adds components such as boundary modules, attention mechanisms, and multi-scale processors independently. This accumulation creates a computational burden without proportional gains. To manage this complexity, they process at reduced resolutions, eliminating fine details essential for camouflage. We present SPEGNet, addressing fragmentation through a unified design. The architecture integrates multi-scale features via channel calibration and spatial enhancement. Boundaries emerge directly from context-rich representations, maintaining semantic-spatial alignment. Progressive refinement implements scale-adaptive edge modulation with peak influence at intermediate resolutions. This design strikes a balance between boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\\alpha$ on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed. Our approach excels across scales, from tiny, intricate objects to large, pattern-similar ones, while handling occlusion and ambiguous boundaries. Code, model weights, and results are available on \\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning, Image and Video Processing",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.376343"
    },
    {
        "index": "#181",
        "title": "Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM Agents",
        "link": "/arxiv/2510.04465",
        "arxiv_id": "2510.04465",
        "authors": "Zhiping Zhang, Yi Evie Zhang, Freda Shi, Tianshi Li",
        "summary": "Large Language Model (LLM) agents require personal information for personalization in order to better act on users' behalf in daily tasks, but this raises privacy concerns and a personalization-privacy dilemma. Agent's autonomy introduces both risks and opportunities, yet its effects remain unclear. To better understand this, we conducted a 3$\\times$3 between-subjects experiment ($N=450$) to study how agent's autonomy level and personalization influence users' privacy concerns, trust and willingness to use, as well as the underlying psychological processes. We find that personalization without considering users' privacy preferences increases privacy concerns and decreases trust and willingness to use. Autonomy moderates these effects: Intermediate autonomy flattens the impact of personalization compared to No- and Full autonomy conditions. Our results suggest that rather than aiming for perfect model alignment in output generation, balancing autonomy of agent's action and user control offers a promising path to mitigate the personalization-privacy dilemma.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.377064"
    },
    {
        "index": "#182",
        "title": "Inverse Mixed-Integer Programming: Learning Constraints then Objective Functions",
        "link": "/arxiv/2510.04455",
        "arxiv_id": "2510.04455",
        "authors": "Akira Kitaoka",
        "summary": "In mixed-integer linear programming, data-driven inverse optimization that learns the objective function and the constraints from observed data plays an important role in constructing appropriate mathematical models for various fields, including power systems and scheduling. However, to the best of our knowledge, there is no known method for learning both the objective functions and the constraints. In this paper, we propose a two-stage method for a class of problems where the objective function is expressed as a linear combination of functions and the constraints are represented by functions and thresholds. Specifically, our method first learns the constraints and then learns the objective function. On the theoretical side, we show the proposed method can solve inverse optimization problems in finite dataset, develop statistical learning theory in pseudometric spaces and sub-Gaussian distributions, and construct a statistical learning for inverse optimization. On the experimental side, we demonstrate that our method is practically applicable for scheduling problems formulated as integer linear programmings with up to 100 decision variables, which are typical in real-world settings.",
        "subjects": "Optimization and Control, Artificial Intelligence, Machine Learning, Statistics Theory, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.377778"
    },
    {
        "index": "#184",
        "title": "Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting",
        "link": "/arxiv/2510.04401",
        "arxiv_id": "2510.04401",
        "authors": "Xuyang Guo, Zekai Huang, Zhenmei Shi, Zhao Song, Jiahao Zhang",
        "summary": "Vision-Language Models (VLMs) have become a central focus of today's AI community, owing to their impressive abilities gained from training on large-scale vision-language data from the Web. These models have demonstrated strong performance across diverse tasks, including image understanding, video understanding, complex visual reasoning, and embodied AI. Despite these noteworthy successes, a fundamental question remains: Can VLMs count objects correctly? In this paper, we introduce a simple yet effective benchmark, VLMCountBench, designed under a minimalist setting with only basic geometric shapes (e.g., triangles, circles) and their compositions, focusing exclusively on counting tasks without interference from other factors. We adopt strict independent variable control and systematically study the effects of simple properties such as color, size, and prompt refinement in a controlled ablation. Our empirical results reveal that while VLMs can count reliably when only one shape type is present, they exhibit substantial failures when multiple shape types are combined (i.e., compositional counting). This highlights a fundamental empirical limitation of current VLMs and motivates important directions for future research.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-06",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.379253"
    },
    {
        "index": "#187",
        "title": "MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection",
        "link": "/arxiv/2510.04397",
        "arxiv_id": "2510.04397",
        "authors": "Van Nguyen, Surya Nepal, Xingliang Yuan, Tingmin Wu, Fengchao Chen, Carsten Rudolph",
        "summary": "Software vulnerabilities (SVs) pose a critical threat to safety-critical systems, driving the adoption of AI-based approaches such as machine learning and deep learning for software vulnerability detection. Despite promising results, most existing methods are limited to a single programming language. This is problematic given the multilingual nature of modern software, which is often complex and written in multiple languages. Current approaches often face challenges in capturing both shared and language-specific knowledge of source code, which can limit their performance on diverse programming languages and real-world codebases. To address this gap, we propose MULVULN, a novel multilingual vulnerability detection approach that learns from source code across multiple languages. MULVULN captures both the shared knowledge that generalizes across languages and the language-specific knowledge that reflects unique coding conventions. By integrating these aspects, it achieves more robust and effective detection of vulnerabilities in real-world multilingual software systems. The rigorous and extensive experiments on the real-world and diverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven programming languages, demonstrate the superiority of MULVULN over thirteen effective and state-of-the-art baselines. Notably, MULVULN achieves substantially higher F1-score, with improvements ranging from 1.45% to 23.59% compared to the baseline methods.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Software Engineering",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.386672"
    },
    {
        "index": "#190",
        "title": "Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development",
        "link": "/arxiv/2510.04380",
        "arxiv_id": "2510.04380",
        "authors": "Mateen Ahmed Abbasi, Petri Ihantola, Tommi Mikkonen, Niko Mäkitalo",
        "summary": "Requirement Engineering (RE) is the foundation of successful software development. In RE, the goal is to ensure that implemented systems satisfy stakeholder needs through rigorous requirements elicitation, validation, and evaluation processes. Despite its critical role, RE continues to face persistent challenges, such as ambiguity, conflicting stakeholder needs, and the complexity of managing evolving requirements. A common view is that Artificial Intelligence (AI) has the potential to streamline the RE process, resulting in improved efficiency, accuracy, and management actions. However, using AI also introduces new concerns, such as ethical issues, biases, and lack of transparency. This paper explores how AI can enhance traditional RE practices by automating labor-intensive tasks, supporting requirement prioritization, and facilitating collaboration between stakeholders and AI systems. The paper also describes the opportunities and challenges that AI brings to RE. In particular, the vision calls for ethical practices in AI, along with a much-enhanced collaboration between academia and industry professionals. The focus should be on creating not only powerful but also trustworthy and practical AI solutions ready to adapt to the fast-paced world of software development.",
        "subjects": "Software Engineering, Artificial Intelligence, Human-Computer Interaction",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.389003"
    },
    {
        "index": "#191",
        "title": "Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains",
        "link": "/arxiv/2510.04375",
        "arxiv_id": "2510.04375",
        "authors": "Akshay Mittal, Vinay Venkatesh, Krishna Kandi, Shalini Sudarshan",
        "summary": "The effectiveness of single-model sequential recommendation architectures, while scalable, is often limited when catering to \"power users\" in sparse or niche domains. Our previous research, PinnerFormerLite, addressed this by using a fixed weighted loss to prioritize specific domains. However, this approach can be sub-optimal, as a single, uniform weight may not be sufficient for domains with very few interactions, where the training signal is easily diluted by the vast, generic dataset. This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss function with comprehensive theoretical foundations and extensive empirical validation. We introduce an adaptive algorithm that adjusts the loss weight for each domain based on its sparsity in the training data, assigning a higher weight to sparser domains and a lower weight to denser ones. This ensures that even rare user interests contribute a meaningful gradient signal, preventing them from being overshadowed. We provide rigorous theoretical analysis including convergence proofs, complexity analysis, and bounds analysis to establish the stability and efficiency of our approach. Our comprehensive empirical validation across four diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music) with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that this dynamic weighting system significantly outperforms all comparison methods, particularly for sparse domains, achieving substantial lifts in key metrics like Recall at 10 and NDCG at 10 while maintaining performance on denser domains and introducing minimal computational overhead.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.389687"
    },
    {
        "index": "#192",
        "title": "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks",
        "link": "/arxiv/2510.04374",
        "arxiv_id": "2510.04374",
        "authors": "Tejal Patwardhan, Rachel Dias, Elizabeth Proehl, Grace Kim, Michele Wang, Olivia Watkins, Simón Posada Fishman, Marwan Aljubeh, Phoebe Thacker, Laurance Fauconnet, Natalie S. Kim, Patrick Chao, Samuel Miserendino, Gildas Chabot, David Li, Michael Sharman, Alexandra Barr, Amelia Glaese, Jerry Tworek",
        "summary": "We introduce GDPval, a benchmark evaluating AI model capabilities on real-world economically valuable tasks. GDPval covers the majority of U.S. Bureau of Labor Statistics Work Activities for 44 occupations across the top 9 sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are constructed from the representative work of industry professionals with an average of 14 years of experience. We find that frontier model performance on GDPval is improving roughly linearly over time, and that the current best frontier models are approaching industry experts in deliverable quality. We analyze the potential for frontier models, when paired with human oversight, to perform GDPval tasks cheaper and faster than unaided experts. We also demonstrate that increased reasoning effort, increased task context, and increased scaffolding improves model performance on GDPval. Finally, we open-source a gold subset of 220 tasks and provide a public automated grading service at evals.openai.com to facilitate future research in understanding real-world model capabilities.",
        "subjects": "Machine Learning, Artificial Intelligence, Computers and Society",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.390706"
    },
    {
        "index": "#195",
        "title": "Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators",
        "link": "/arxiv/2510.04354",
        "arxiv_id": "2510.04354",
        "authors": "Apurva Badithela, David Snyder, Lihan Zha, Joseph Mikhail, Matthew O'Kelly, Anushri Dixit, Anirudha Majumdar",
        "summary": "Rapid progress in imitation learning, foundation models, and large-scale datasets has led to robot manipulation policies that generalize to a wide-range of tasks and environments. However, rigorous evaluation of these policies remains a challenge. Typically in practice, robot policies are often evaluated on a small number of hardware trials without any statistical assurances. We present SureSim, a framework to augment large-scale simulation with relatively small-scale real-world testing to provide reliable inferences on the real-world performance of a policy. Our key idea is to formalize the problem of combining real and simulation evaluations as a prediction-powered inference problem, in which a small number of paired real and simulation evaluations are used to rectify bias in large-scale simulation. We then leverage non-asymptotic mean estimation algorithms to provide confidence intervals on mean policy performance. Using physics-based simulation, we evaluate both diffusion policy and multi-task fine-tuned \\(\\pi_0\\) on a joint distribution of objects and initial conditions, and find that our approach saves over \\(20-25\\%\\) of hardware evaluation effort to achieve similar bounds on policy performance.",
        "subjects": "Robotics, Artificial Intelligence, Systems and Control",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.398102"
    },
    {
        "index": "#196",
        "title": "Challenge on Optimization of Context Collection for Code Completion",
        "link": "/arxiv/2510.04349",
        "arxiv_id": "2510.04349",
        "authors": "Dmitry Ustalov, Egor Bogomolov, Alexander Bezzubov, Yaroslav Golubev, Evgeniy Glukhov, Georgii Levtsov, Vladimir Kovalenko",
        "summary": "The rapid advancement of workflows and methods for software engineering using AI emphasizes the need for a systematic evaluation and analysis of their ability to leverage information from entire projects, particularly in large code bases. In this challenge on optimization of context collection for code completion, organized by JetBrains in collaboration with Mistral AI as part of the ASE 2025 conference, participants developed efficient mechanisms for collecting context from source code repositories to improve fill-in-the-middle code completions for Python and Kotlin. We constructed a large dataset of real-world code in these two programming languages using permissively licensed open-source projects. The submissions were evaluated based on their ability to maximize completion quality for multiple state-of-the-art neural models using the chrF metric. During the public phase of the competition, nineteen teams submitted solutions to the Python track and eight teams submitted solutions to the Kotlin track. In the private phase, six teams competed, of which five submitted papers to the workshop.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.398899"
    },
    {
        "index": "#197",
        "title": "Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies",
        "link": "/arxiv/2510.04341",
        "arxiv_id": "2510.04341",
        "authors": "G. Niklas Noren, Eva-Lisa Meldau, Johan Ellenius",
        "summary": "Many high-stakes AI applications target low-prevalence events, where apparent accuracy can conceal limited real-world value. Relevant AI models range from expert-defined rules and traditional machine learning to generative LLMs constrained for classification. We outline key considerations for critical appraisal of AI in rare-event recognition, including problem framing and test set design, prevalence-aware statistical evaluation, robustness assessment, and integration into human workflows. In addition, we propose an approach to structured case-level examination (SCLE), to complement statistical performance evaluation, and a comprehensive checklist to guide procurement or development of AI models for rare-event recognition. We instantiate the framework in pharmacovigilance, drawing on three studies: rule-based retrieval of pregnancy-related reports; duplicate detection combining machine learning with probabilistic record linkage; and automated redaction of person names using an LLM. We highlight pitfalls specific to the rare-event setting including optimism from unrealistic class balance and lack of difficult positive controls in test sets - and show how cost-sensitive targets align model performance with operational value. While grounded in pharmacovigilance practice, the principles generalize to domains where positives are scarce and error costs may be asymmetric.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.399572"
    },
    {
        "index": "#199",
        "title": "Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space",
        "link": "/arxiv/2510.04339",
        "arxiv_id": "2510.04339",
        "authors": "Christian Limberg, Fares Schulz, Zhe Zhang, Stefan Weinzierl",
        "summary": "This paper presents a novel approach to neural instrument sound synthesis using a two-stage semi-supervised learning framework capable of generating pitch-accurate, high-quality music samples from an expressive timbre latent space. Existing approaches that achieve sufficient quality for music production often rely on high-dimensional latent representations that are difficult to navigate and provide unintuitive user experiences. We address this limitation through a two-stage training paradigm: first, we train a pitch-timbre disentangled 2D representation of audio samples using a Variational Autoencoder; second, we use this representation as conditioning input for a Transformer-based generative model. The learned 2D latent space serves as an intuitive interface for navigating and exploring the sound landscape. We demonstrate that the proposed method effectively learns a disentangled timbre space, enabling expressive and controllable audio generation with reliable pitch conditioning. Experimental results show the model's ability to capture subtle variations in timbre while maintaining a high degree of pitch accuracy. The usability of our method is demonstrated in an interactive web application, highlighting its potential as a step towards future music production environments that are both intuitive and creatively empowering: https://pgesam.faresschulz.com",
        "subjects": "Sound, Artificial Intelligence, Machine Learning, Audio and Speech Processing, Signal Processing",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.401118"
    },
    {
        "index": "#200",
        "title": "FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents",
        "link": "/arxiv/2510.04317",
        "arxiv_id": "2510.04317",
        "authors": "Yucong Dai, Lu Zhang, Feng Luo, Mashrur Chowdhury, Yongkai Wu",
        "summary": "Training fair and unbiased machine learning models is crucial for high-stakes applications, yet it presents significant challenges. Effective bias mitigation requires deep expertise in fairness definitions, metrics, data preprocessing, and machine learning techniques. In addition, the complex process of balancing model performance with fairness requirements while properly handling sensitive attributes makes fairness-aware model development inaccessible to many practitioners. To address these challenges, we introduce FairAgent, an LLM-powered automated system that significantly simplifies fairness-aware model development. FairAgent eliminates the need for deep technical expertise by automatically analyzing datasets for potential biases, handling data preprocessing and feature engineering, and implementing appropriate bias mitigation strategies based on user requirements. Our experiments demonstrate that FairAgent achieves significant performance improvements while significantly reducing development time and expertise requirements, making fairness-aware machine learning more accessible to practitioners.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.401806"
    },
    {
        "index": "#203",
        "title": "A KL-regularization framework for learning to plan with adaptive priors",
        "link": "/arxiv/2510.04280",
        "arxiv_id": "2510.04280",
        "authors": "Álvaro Serra-Gomez, Daniel Jarne Ornia, Dhruva Tirumala, Thomas Moerland",
        "summary": "Effective exploration remains a central challenge in model-based reinforcement learning (MBRL), particularly in high-dimensional continuous control tasks where sample efficiency is crucial. A prominent line of recent work leverages learned policies as proposal distributions for Model-Predictive Path Integral (MPPI) planning. Initial approaches update the sampling policy independently of the planner distribution, typically maximizing a learned value function with deterministic policy gradient and entropy regularization. However, because the states encountered during training depend on the MPPI planner, aligning the sampling policy with the planner improves the accuracy of value estimation and long-term performance. To this end, recent methods update the sampling policy by minimizing KL divergence to the planner distribution or by introducing planner-guided regularization into the policy update. In this work, we unify these MPPI-based reinforcement learning methods under a single framework by introducing Policy Optimization-Model Predictive Control (PO-MPC), a family of KL-regularized MBRL methods that integrate the planner's action distribution as a prior in policy optimization. By aligning the learned policy with the planner's behavior, PO-MPC allows more flexibility in the policy updates to trade off Return maximization and KL divergence minimization. We clarify how prior approaches emerge as special cases of this family, and we explore previously unstudied variations. Our experiments show that these extended configurations yield significant performance improvements, advancing the state of the art in MPPI-based RL.",
        "subjects": "Machine Learning, Artificial Intelligence, Robotics",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.408884"
    },
    {
        "index": "#204",
        "title": "Scalable Causal Discovery from Recursive Nonlinear Data via Truncated Basis Function Scores and Tests",
        "link": "/arxiv/2510.04276",
        "arxiv_id": "2510.04276",
        "authors": "Joseph Ramsey, Bryan Andrews",
        "summary": "Learning graphical conditional independence structures from nonlinear, continuous or mixed data is a central challenge in machine learning and the sciences, and many existing methods struggle to scale to thousands of samples or hundreds of variables. We introduce two basis-expansion tools for scalable causal discovery. First, the Basis Function BIC (BF-BIC) score uses truncated additive expansions to approximate nonlinear dependencies. BF-BIC is theoretically consistent under additive models and extends to post-nonlinear (PNL) models via an invertible reparameterization. It remains robust under moderate interactions and supports mixed data through a degenerate-Gaussian embedding for discrete variables. In simulations with fully nonlinear neural causal models (NCMs), BF-BIC outperforms kernel- and constraint-based methods (e.g., KCI, RFCI) in both accuracy and runtime. Second, the Basis Function Likelihood Ratio Test (BF-LRT) provides an approximate conditional independence test that is substantially faster than kernel tests while retaining competitive accuracy. Extensive simulations and a real-data application to Canadian wildfire risk show that, when integrated into hybrid searches, BF-based methods enable interpretable and scalable causal discovery. Implementations are available in Python, R, and Java.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.409521"
    },
    {
        "index": "#206",
        "title": "Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing",
        "link": "/arxiv/2510.04263",
        "arxiv_id": "2510.04263",
        "authors": "Joseph Ramsey, Bryan Andrews",
        "summary": "Learning causal structure from observational data is especially challenging when latent variables or selection bias are present. The Fast Causal Inference (FCI) algorithm addresses this setting but often performs exhaustive conditional independence tests across many subsets, leading to spurious independence claims, extra or missing edges, and unreliable orientations. We present a family of score-guided mixed-strategy causal search algorithms that build on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI, straightforward variants of GFCI that substitute BOSS or GRaSP for FGES, thereby retaining correctness while incurring different scalability tradeoffs. Second, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method that improves upon these variants by replacing exhaustive all-subsets testing with targeted tests guided by BOSS, yielding well-formed PAGs with higher precision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also known as BOSS-POD), which bypasses latent-variable-specific reasoning and directly returns the PAG of the BOSS DAG. Although not strictly correct in the FCI sense, it scales better and often achieves superior accuracy in practice. Simulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI provide sound baselines, FCIT improves both efficiency and reliability, and LV-Dumb offers a practical heuristic with strong empirical performance. Together, these method highlight the value of score-guided and targeted strategies for scalable latent-variable causal discovery.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.410963"
    },
    {
        "index": "#207",
        "title": "AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents",
        "link": "/arxiv/2510.04257",
        "arxiv_id": "2510.04257",
        "authors": "Yanjie Li, Yiming Cao, Dong Wang, Bin Xiao",
        "summary": "Multimodal agents built on large vision-language models (LVLMs) are increasingly deployed in open-world settings but remain highly vulnerable to prompt injection, especially through visual inputs. We introduce AgentTypo, a black-box red-teaming framework that mounts adaptive typographic prompt injection by embedding optimized text into webpage images. Our automatic typographic prompt injection (ATPI) algorithm maximizes prompt reconstruction by substituting captioners while minimizing human detectability via a stealth loss, with a Tree-structured Parzen Estimator guiding black-box optimization over text placement, size, and color. To further enhance attack strength, we develop AgentTypo-pro, a multi-LLM system that iteratively refines injection prompts using evaluation feedback and retrieves successful past examples for continual learning. Effective prompts are abstracted into generalizable strategies and stored in a strategy repository, enabling progressive knowledge accumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark across Classifieds, Shopping, and Reddit scenarios show that AgentTypo significantly outperforms the latest image-based attacks such as AgentAttack. On GPT-4o agents, our image-only attack raises the success rate from 0.23 to 0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and Claude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also outperforming the latest baselines. Our findings reveal that AgentTypo poses a practical and potent threat to multimodal agents and highlight the urgent need for effective defense.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.411703"
    },
    {
        "index": "#208",
        "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context",
        "link": "/arxiv/2510.04246",
        "arxiv_id": "2510.04246",
        "authors": "Huiwon Jang, Sihyun Yu, Heeseung Kwon, Hojin Jeon, Younggyo Seo, Jinwoo Shin",
        "summary": "Leveraging temporal context is crucial for success in partially observable robotic tasks. However, prior work in behavior cloning has demonstrated inconsistent performance gains when using multi-frame observations. In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. Our approach is motivated by the key observation that Vision-Language-Action models (VLA), i.e., policy models built upon a Vision-Language Model (VLM), more effectively utilize multi-frame observations for action generation. This suggests that VLMs' inherent temporal understanding capability enables them to extract more meaningful context from multi-frame observations. However, the high dimensionality of video inputs introduces significant computational overhead, making VLA training and inference inefficient. To address this, ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation. Our experiments show that ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.417641"
    },
    {
        "index": "#209",
        "title": "Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks",
        "link": "/arxiv/2510.04245",
        "arxiv_id": "2510.04245",
        "authors": "Ayushi Mehrotra, Derek Peng, Dipkamal Bhusal, Nidhi Rastogi",
        "summary": "Adversarial patch attacks pose a practical threat to deep learning models by forcing targeted misclassifications through localized perturbations, often realized in the physical world. Existing defenses typically assume prior knowledge of patch size or location, limiting their applicability. In this work, we propose a patch-agnostic defense that leverages concept-based explanations to identify and suppress the most influential concept activation vectors, thereby neutralizing patch effects without explicit detection. Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and clean accuracy than the state-of-the-art PatchCleanser, while maintaining strong performance across varying patch sizes and locations. Our results highlight the promise of combining interpretability with robustness and suggest concept-driven defenses as a scalable strategy for securing machine learning models against adversarial patch attacks.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.418297"
    },
    {
        "index": "#210",
        "title": "Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs",
        "link": "/arxiv/2510.04241",
        "arxiv_id": "2510.04241",
        "authors": "Seong Jin Ahn, Myoung-Ho Kim",
        "summary": "For large-scale applications, there is growing interest in replacing Graph Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via knowledge distillation. However, distilling GNNs for self-supervised graph representation learning into MLPs is more challenging. This is because the performance of self-supervised learning is more related to the model's inductive bias than supervised learning. This motivates us to design a new distillation method to bridge a huge capacity gap between GNNs and MLPs in self-supervised graph representation learning. In this paper, we propose \\textbf{D}iffusion-\\textbf{A}ssisted \\textbf{D}istillation for \\textbf{S}elf-supervised \\textbf{G}raph representation learning with \\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion model as a teacher assistant to better distill the knowledge from the teacher GNN into the student MLP. This approach enhances the generalizability and robustness of MLPs in self-supervised graph representation learning. Extensive experiments demonstrate that DAD-SGM effectively distills the knowledge of self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation methods. Our implementation is available at https://github.com/SeongJinAhn/DAD-SGM.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.419008"
    },
    {
        "index": "#211",
        "title": "Empowering Denoising Sequential Recommendation with Large Language Model Embeddings",
        "link": "/arxiv/2510.04239",
        "arxiv_id": "2510.04239",
        "authors": "Tongzhou Wu, Yuhao Wang, Maolin Wang, Chi Zhang, Xiangyu Zhao",
        "summary": "Sequential recommendation aims to capture user preferences by modeling sequential patterns in user-item interactions. However, these models are often influenced by noise such as accidental interactions, leading to suboptimal performance. Therefore, to reduce the effect of noise, some works propose explicitly identifying and removing noisy items. However, we find that simply relying on collaborative information may result in an over-denoising problem, especially for cold items. To overcome these limitations, we propose a novel framework: Interest Alignment for Denoising Sequential Recommendation (IADSR) which integrates both collaborative and semantic information. Specifically, IADSR is comprised of two stages: in the first stage, we obtain the collaborative and semantic embeddings of each item from a traditional sequential recommendation model and an LLM, respectively. In the second stage, we align the collaborative and semantic embeddings and then identify noise in the interaction sequence based on long-term and short-term interests captured in the collaborative and semantic modalities. Our extensive experiments on four public datasets validate the effectiveness of the proposed framework and its compatibility with different sequential recommendation systems.",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.419710"
    },
    {
        "index": "#212",
        "title": "Flexible Locomotion Learning with Diffusion Model Predictive Control",
        "link": "/arxiv/2510.04234",
        "arxiv_id": "2510.04234",
        "authors": "Runhan Huang, Haldun Balim, Heng Yang, Yilun Du",
        "summary": "Legged locomotion demands controllers that are both robust and adaptable, while remaining compatible with task and safety considerations. However, model-free reinforcement learning (RL) methods often yield a fixed policy that can be difficult to adapt to new behaviors at test time. In contrast, Model Predictive Control (MPC) provides a natural approach to flexible behavior synthesis by incorporating different objectives and constraints directly into its optimization process. However, classical MPC relies on accurate dynamics models, which are often difficult to obtain in complex environments and typically require simplifying assumptions. We present Diffusion-MPC, which leverages a learned generative diffusion model as an approximate dynamics prior for planning, enabling flexible test-time adaptation through reward and constraint based optimization. Diffusion-MPC jointly predicts future states and actions; at each reverse step, we incorporate reward planning and impose constraint projection, yielding trajectories that satisfy task objectives while remaining within physical limits. To obtain a planning model that adapts beyond imitation pretraining, we introduce an interactive training algorithm for diffusion based planner: we execute our reward-and-constraint planner in environment, then filter and reweight the collected trajectories by their realized returns before updating the denoiser. Our design enables strong test-time adaptability, allowing the planner to adjust to new reward specifications without retraining. We validate Diffusion-MPC on real world, demonstrating strong locomotion and flexible adaptation.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.420412"
    },
    {
        "index": "#213",
        "title": "Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling",
        "link": "/arxiv/2510.04233",
        "arxiv_id": "2510.04233",
        "authors": "Kai Yang, Yuqi Huang, Junheng Tao, Wanyu Wang, Qitian Wu",
        "summary": "Modeling 3D dynamics is a fundamental problem in multi-body systems across scientific and engineering domains and has important practical implications in trajectory prediction and simulation. While recent GNN-based approaches have achieved strong performance by enforcing geometric symmetries, encoding high-order features or incorporating neural-ODE mechanics, they typically depend on explicitly observed structures and inherently fail to capture the unobserved interactions that are crucial to complex physical behaviors and dynamics mechanism. In this paper, we propose PAINET, a principled SE(3)-equivariant neural architecture for learning all-pair interactions in multi-body systems. The model comprises: (1) a novel physics-inspired attention network derived from the minimization trajectory of an energy function, and (2) a parallel decoder that preserves equivariance while enabling efficient inference. Empirical results on diverse real-world benchmarks, including human motion capture, molecular dynamics, and large-scale protein simulations, show that PAINET consistently outperforms recently proposed models, yielding 4.7% to 41.5% error reductions in 3D dynamics prediction with comparable computation costs in terms of time and memory.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.421107"
    },
    {
        "index": "#214",
        "title": "When AI Gets Persuaded, Humans Follow: Inducing the Conformity Effect in Persuasive Dialogue",
        "link": "/arxiv/2510.04229",
        "arxiv_id": "2510.04229",
        "authors": "Rikuo Sasaki, Michimasa Inaba",
        "summary": "Recent advancements in AI have highlighted its application in captology, the field of using computers as persuasive technologies. We hypothesized that the \"conformity effect,\" where individuals align with others' actions, also occurs with AI agents. This study verifies this hypothesis by introducing a \"Persuadee Agent\" that is persuaded alongside a human participant in a three-party persuasive dialogue with a Persuader Agent. We conducted a text-based dialogue experiment with human participants. We compared four conditions manipulating the Persuadee Agent's behavior (persuasion acceptance vs. non-acceptance) and the presence of an icebreaker session. Results showed that when the Persuadee Agent accepted persuasion, both perceived persuasiveness and actual attitude change significantly improved. Attitude change was greatest when an icebreaker was also used, whereas an unpersuaded AI agent suppressed attitude change. Additionally, it was confirmed that the persuasion acceptance of participants increased at the moment the Persuadee Agent was persuaded. These results suggest that appropriately designing a Persuadee Agent can improve persuasion through the conformity effect.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.421774"
    },
    {
        "index": "#217",
        "title": "MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering",
        "link": "/arxiv/2510.04220",
        "arxiv_id": "2510.04220",
        "authors": "Lixuan He, Shikang Zheng, Linfeng Zhang",
        "summary": "Autoregressive (AR) models have shown great promise in image generation, yet they face a fundamental inefficiency stemming from their core component: a vast, unstructured vocabulary of visual tokens. This conventional approach treats tokens as a flat vocabulary, disregarding the intrinsic structure of the token embedding space where proximity often correlates with semantic similarity. This oversight results in a highly complex prediction task, which hinders training efficiency and limits final generation quality. To resolve this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled framework that constructs a hierarchical semantic tree directly from the codebook's intrinsic structure. MASC employs a novel geometry-aware distance metric and a density-driven agglomerative construction to model the underlying manifold of the token embeddings. By transforming the flat, high-dimensional prediction task into a structured, hierarchical one, MASC introduces a beneficial inductive bias that significantly simplifies the learning problem for the AR model. MASC is designed as a plug-and-play module, and our extensive experiments validate its effectiveness: it accelerates training by up to 57% and significantly improves generation quality, reducing the FID of LlamaGen-XL from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly competitive with state-of-the-art methods, establishing that structuring the prediction space is as crucial as architectural innovation for scalable generative modeling.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.424640"
    },
    {
        "index": "#218",
        "title": "MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering",
        "link": "/arxiv/2510.04217",
        "arxiv_id": "2510.04217",
        "authors": "Chenlu Ding, Jiancan Wu, Leheng Sheng, Fan Zhang, Yancheng Yuan, Xiang Wang, Xiangnan He",
        "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities across vision-language tasks, yet their large-scale deployment raises pressing concerns about memorized private data, outdated knowledge, and harmful content. Existing unlearning approaches for MLLMs typically adapt training-based strategies such as gradient ascent or preference optimization, but these methods are computationally expensive, irreversible, and often distort retained knowledge. In this work, we propose MLLMEraser, an input-aware, training-free framework for test-time unlearning. Our approach leverages activation steering to enable dynamic knowledge erasure without parameter updates. Specifically, we construct a multimodal erasure direction by contrasting adversarially perturbed, knowledge-recall image-text pairs with knowledge-erasure counterparts, capturing both textual and visual discrepancies. To prevent unnecessary interference, we further design an input-aware steering mechanism that adaptively determines when and how the erasure direction should be applied, preserving utility on retained knowledge while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms state-of-the-art MLLM unlearning baselines, achieving stronger forgetting performance with lower computational cost and minimal utility degradation.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.425396"
    },
    {
        "index": "#219",
        "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention",
        "link": "/arxiv/2510.04212",
        "arxiv_id": "2510.04212",
        "authors": "Haiquan Qiu, Quanming Yao",
        "summary": "The pursuit of computational efficiency has driven the adoption of low-precision formats for training transformer models. However, this progress is often hindered by notorious training instabilities. This paper provides the first mechanistic explanation for a long-standing and unresolved failure case where training with flash attention in low-precision settings leads to catastrophic loss explosions. Our in-depth analysis reveals that the failure is not a random artifact but caused by two intertwined phenomena: the emergence of similar low-rank representations within the attention mechanism and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these factors create a vicious cycle of error accumulation that corrupts weight updates, ultimately derailing the training dynamics. To validate our findings, we introduce a minimal modification to the flash attention that mitigates the bias in rounding errors. This simple change stabilizes the training process, confirming our analysis and offering a practical solution to this persistent problem.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.431319"
    },
    {
        "index": "#220",
        "title": "PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression",
        "link": "/arxiv/2510.04205",
        "arxiv_id": "2510.04205",
        "authors": "Di Zhang",
        "summary": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability and a strong mathematical foundation. However, their parameter efficiency remains a significant challenge for practical deployment. This paper introduces PolyKAN, a novel theoretical framework for KAN compression that provides formal guarantees on both model size reduction and approximation error. By leveraging the inherent piecewise polynomial structure of KANs, we formulate the compression problem as one of optimal polyhedral region merging. We establish a rigorous polyhedral characterization of KANs, develop a complete theory of $\\epsilon$-equivalent compression, and design an optimal dynamic programming algorithm that guarantees minimal compression under specified error bounds. Our theoretical analysis demonstrates that PolyKAN achieves provably minimal compression while maintaining strict error control, with polynomial-time complexity in all network parameters. The framework provides the first formal foundation for KAN compression with mathematical guarantees, opening new directions for efficient deployment of interpretable neural architectures.",
        "subjects": "Machine Learning, Artificial Intelligence, Numerical Analysis, Optimization and Control",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.431743"
    },
    {
        "index": "#222",
        "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge",
        "link": "/arxiv/2510.04201",
        "arxiv_id": "2510.04201",
        "authors": "Moo Hyun Son, Jintaek Oh, Sun Bin Mun, Jaechul Roh, Sehyun Choi",
        "summary": "While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce World-To-Image, a novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that World-To-Image substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world. Our demo code is available here\\footnote{https://github.com/mhson-kyle/World-To-Image}.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.432733"
    },
    {
        "index": "#224",
        "title": "Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity",
        "link": "/arxiv/2510.04189",
        "arxiv_id": "2510.04189",
        "authors": "Prashansa Panda, Shalabh Bhatnagar",
        "summary": "Recent studies have increasingly focused on non-asymptotic convergence analyses for actor-critic (AC) algorithms. One such effort introduced a two-timescale critic-actor algorithm for the discounted cost setting using a tabular representation, where the usual roles of the actor and critic are reversed. However, only asymptotic convergence was established there. Subsequently, both asymptotic and non-asymptotic analyses of the critic-actor algorithm with linear function approximation were conducted. In our work, we introduce the first natural critic-actor algorithm with function approximation for the long-run average cost setting and under inequality constraints. We provide the non-asymptotic convergence guarantees for this algorithm. Our analysis establishes optimal learning rates and we also propose a modification to enhance sample complexity. We further show the results of experiments on three different Safety-Gym environments where our algorithm is found to be competitive in comparison with other well known algorithms.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.433505"
    },
    {
        "index": "#225",
        "title": "A Complement to Neural Networks for Anisotropic Inelasticity at Finite Strains",
        "link": "/arxiv/2510.04187",
        "arxiv_id": "2510.04187",
        "authors": "Hagen Holthusen, Ellen Kuhl",
        "summary": "We propose a complement to constitutive modeling that augments neural networks with material principles to capture anisotropy and inelasticity at finite strains. The key element is a dual potential that governs dissipation, consistently incorporates anisotropy, and-unlike conventional convex formulations-satisfies the dissipation inequality without requiring convexity. Our neural network architecture employs invariant-based input representations in terms of mixed elastic, inelastic and structural tensors. It adapts Input Convex Neural Networks, and introduces Input Monotonic Neural Networks to broaden the admissible potential class. To bypass exponential-map time integration in the finite strain regime and stabilize the training of inelastic materials, we employ recurrent Liquid Neural Networks. The approach is evaluated at both material point and structural scales. We benchmark against recurrent models without physical constraints and validate predictions of deformation and reaction forces for unseen boundary value problems. In all cases, the method delivers accurate and stable performance beyond the training regime. The neural network and finite element implementations are available as open-source and are accessible to the public via https://doi.org/10.5281/zenodo.17199965.",
        "subjects": "Computational Engineering, Finance, and Science, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.433900"
    },
    {
        "index": "#227",
        "title": "Multi Language Models for On-the-Fly Syntax Highlighting",
        "link": "/arxiv/2510.04166",
        "arxiv_id": "2510.04166",
        "authors": "Marco Edoardo Palma, Pooja Rani, Harald C. Gall",
        "summary": "Syntax highlighting is a critical feature in modern software development environments, enhancing code readability and developer productivity. However, delivering accurate highlighting in real time remains challenging for online and web-based development tools due to strict time and memory constraints on backend services. These systems must serve highlights rapidly and frequently, even when code is partially valid or invalid. This has led to on-the-fly syntax highlighting, where visual annotations are generated just before content is served, often at high request rates and under incomplete input conditions. To meet these demands efficiently, state-of-the-art models use deep learning to learn the behavior of brute-force syntax highlighting resolvers, tools that are easy to implement but too slow for production. Through the Deep Abstraction process, brute-force strategies are encoded into fast statistical models that achieve both high accuracy and low-latency inference. Despite their success, such models face key challenges: they support only one programming language per model, require large datasets from slow brute-force generators, and involve resource-intensive training. In multi-language environments, this means maintaining multiple independent models, increasing system complexity and operational cost. This work addresses these issues by introducing a unified model capable of highlighting up to six mainstream programming languages, reducing deployment complexity by a factor of six and improving performance on unseen languages. A novel normalization technique significantly enhances model generalization, while few-shot learning experiments show that a small number of oracle samples can replace large datasets, minimizing dependence on brute-force generators. Combined, these innovations enable efficient, scalable, and cost-effective syntax highlighting across diverse programming languages.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.434710"
    },
    {
        "index": "#229",
        "title": "Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs",
        "link": "/arxiv/2510.04142",
        "arxiv_id": "2510.04142",
        "authors": "Xiaoyu Yang, Jie Lu, En Yu",
        "summary": "This paper identifies a critical yet underexplored challenge in distilling from multimodal large language models (MLLMs): the reasoning trajectories generated by multiple drifting teachers exhibit concept drift, whereby their reasoning distributions evolve unpredictably and transmit biases to the student model, ultimately compromising its performance. To tackle this issue, we pioneer a theoretical connection between concept drift and knowledge distillation, casting the non-stationary reasoning dynamics from multiple MLLM teachers as next-token prediction of multi-stream reasoning trajectories.Guided by concept drift, we introduce the \"learn, compare, critique\" paradigm, culminating in autonomous preference optimization (APO). Under the active guidance of the teachers, the student model first learns and self-distils preferred thinking by comparing multiple teachers. It then engages in critical reflection over the drifting inference from teachers, performing concept alignment through APO, ultimately yielding a robust, consistent, and generalizable model.Extensive experiments demonstrate our superior performance of consistency, robustness and generalization within knowledge distillation. Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public at: https://anonymous.4open.science/r/Autonomous-Distillation/.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.435618"
    },
    {
        "index": "#230",
        "title": "GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization",
        "link": "/arxiv/2510.04135",
        "arxiv_id": "2510.04135",
        "authors": "Jingzhi Gong, Yixin Bian, Luis de la Cal, Giovanni Pinna, Anisha Uteem, David Williams, Mar Zamorano, Karine Even-Mendoza, W. B. Langdon, Hector Menendez, Federica Sarro",
        "summary": "Coding agents powered by LLMs face critical sustainability and scalability challenges in industrial deployment, with single runs consuming over 100k tokens and incurring environmental costs that may exceed optimization benefits. This paper introduces GA4GC, the first framework to systematically optimize coding agent runtime (greener agent) and code performance (greener code) trade-offs by discovering Pareto-optimal agent hyperparameters and prompt templates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x hypervolume improvement, reducing agent runtime by 37.7% while improving correctness. Our findings establish temperature as the most critical hyperparameter, and provide actionable strategies to balance agent sustainability with code optimization effectiveness in industrial deployment.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.441451"
    },
    {
        "index": "#231",
        "title": "PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting",
        "link": "/arxiv/2510.04134",
        "arxiv_id": "2510.04134",
        "authors": "Yiming Niu, Jinliang Deng, Yongxin Tong",
        "summary": "Periodicity is a fundamental characteristic of time series data and has long played a central role in forecasting. Recent deep learning methods strengthen the exploitation of periodicity by treating patches as basic tokens, thereby improving predictive effectiveness. However, their efficiency remains a bottleneck due to large parameter counts and heavy computational costs. This paper provides, for the first time, a clear explanation of why patch-level processing is inherently inefficient, supported by strong evidence from real-world data. To address these limitations, we introduce a phase perspective for modeling periodicity and present an efficient yet effective solution, PhaseFormer. PhaseFormer features phase-wise prediction through compact phase embeddings and efficient cross-phase interaction enabled by a lightweight routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves state-of-the-art performance with around 1k parameters, consistently across benchmark datasets. Notably, it excels on large-scale and complex datasets, where models with comparable efficiency often struggle. This work marks a significant step toward truly efficient and effective time series forecasting. Code is available at this repository: https://github.com/neumyor/PhaseFormer_TSL",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.442112"
    },
    {
        "index": "#232",
        "title": "On the Limitations and Capabilities of Position Embeddings for Length Generalization",
        "link": "/arxiv/2510.04130",
        "arxiv_id": "2510.04130",
        "authors": "Yang Chen, Yitao Liang, Zhouchen Lin",
        "summary": "In Transformers, Position Embeddings (PEs) significantly influence Length Generalization (LG) performance, yet their fundamental role remains unclear. In this work, we investigate the limitations and capabilities of PEs in achieving LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs), introducing Linear Representation Complexity (LRC) to characterize when PEs enable LG. Our analysis shows that PEs do not expand computational capabilities but structure learned computations across positions. Extending to practical Transformers, we propose Sequential Representation Complexity (SRC) and conjecture that LG is possible if and only if SRC remains invariant across scales. We support this hypothesis with empirical evidence in various reasoning tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance scaling, and a Learning-Based Position Embedding framework that automatically learns positional relations. Our work provides theoretical insights and practical strategies for improving LG in Transformers.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.442810"
    },
    {
        "index": "#233",
        "title": "Learning-Based Hashing for ANN Search: Foundations and Early Advances",
        "link": "/arxiv/2510.04127",
        "arxiv_id": "2510.04127",
        "authors": "Sean Moran",
        "summary": "Approximate Nearest Neighbour (ANN) search is a fundamental problem in information retrieval, underpinning large-scale applications in computer vision, natural language processing, and cross-modal search. Hashing-based methods provide an efficient solution by mapping high-dimensional data into compact binary codes that enable fast similarity computations in Hamming space. Over the past two decades, a substantial body of work has explored learning to hash, where projection and quantisation functions are optimised from data rather than chosen at random. This article offers a foundational survey of early learning-based hashing methods, with an emphasis on the core ideas that shaped the field. We review supervised, unsupervised, and semi-supervised approaches, highlighting how projection functions are designed to generate meaningful embeddings and how quantisation strategies convert these embeddings into binary codes. We also examine extensions to multi-bit and multi-threshold models, as well as early advances in cross-modal retrieval. Rather than providing an exhaustive account of the most recent methods, our goal is to introduce the conceptual foundations of learning-based hashing for ANN search. By situating these early models in their historical context, we aim to equip readers with a structured understanding of the principles, trade-offs, and open challenges that continue to inform current research in this area.",
        "subjects": "Information Retrieval, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.443487"
    },
    {
        "index": "#234",
        "title": "Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions",
        "link": "/arxiv/2510.04126",
        "arxiv_id": "2510.04126",
        "authors": "Ziying Zhang, Yaqing Wang, Yuxuan Sun, Min Ye, Quanming Yao",
        "summary": "Cold-start drug-target interaction (DTI) prediction focuses on interaction between novel drugs and proteins. Previous methods typically learn transferable interaction patterns between structures of drug and proteins to tackle it. However, insight from proteomics suggest that protein have multi-level structures and they all influence the DTI. Existing works usually represent protein with only primary structures, limiting their ability to capture interactions involving higher-level structures. Inspired by this insight, we propose ColdDTI, a framework attending on protein multi-level structure for cold-start DTI prediction. We employ hierarchical attention mechanism to mine interaction between multi-level protein structures (from primary to quaternary) and drug structures at both local and global granularities. Then, we leverage mined interactions to fuse structure representations of different levels for final prediction. Our design captures biologically transferable priors, avoiding the risk of overfitting caused by excessive reliance on representation learning. Experiments on benchmark datasets demonstrate that ColdDTI consistently outperforms previous methods in cold-start settings.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.444220"
    },
    {
        "index": "#236",
        "title": "TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing",
        "link": "/arxiv/2510.04100",
        "arxiv_id": "2510.04100",
        "authors": "Jiaming Wang, Diwen Liu, Jizhuo Chen, Harold Soh",
        "summary": "Topological mapping offers a compact and robust representation for navigation, but progress in the field is hindered by the lack of standardized evaluation metrics, datasets, and protocols. Existing systems are assessed using different environments and criteria, preventing fair and reproducible comparisons. Moreover, a key challenge - perceptual aliasing - remains under-quantified, despite its strong influence on system performance. We address these gaps by (1) formalizing topological consistency as the fundamental property of topological maps and showing that localization accuracy provides an efficient and interpretable surrogate metric, and (2) proposing the first quantitative measure of dataset ambiguity to enable fair comparisons across environments. To support this protocol, we curate a diverse benchmark dataset with calibrated ambiguity levels, implement and release deep-learned baseline systems, and evaluate them alongside classical methods. Our experiments and analysis yield new insights into the limitations of current approaches under perceptual aliasing. All datasets, baselines, and evaluation tools are fully open-sourced to foster consistent and reproducible research in topological mapping.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.445624"
    },
    {
        "index": "#237",
        "title": "Efficient Training of Spiking Neural Networks by Spike-aware Data Pruning",
        "link": "/arxiv/2510.04098",
        "arxiv_id": "2510.04098",
        "authors": "Chenxiang Ma, Xinyi Chen, Yujie Wu, Kay Chen Tan, Jibin Wu",
        "summary": "Spiking neural networks (SNNs), recognized as an energy-efficient alternative to traditional artificial neural networks (ANNs), have advanced rapidly through the scaling of models and datasets. However, such scaling incurs considerable training overhead, posing challenges for researchers with limited computational resources and hindering the sustained development of SNNs. Data pruning is a promising strategy for accelerating training by retaining the most informative examples and discarding redundant ones, but it remains largely unexplored in SNNs. Directly applying ANN-based data pruning methods to SNNs fails to capture the intrinsic importance of examples and suffers from high gradient variance. To address these challenges, we propose a novel spike-aware data pruning (SADP) method. SADP reduces gradient variance by determining each example's selection probability to be proportional to its gradient norm, while avoiding the high cost of direct gradient computation through an efficient upper bound, termed spike-aware importance score. This score accounts for the influence of all-or-nothing spikes on the gradient norm and can be computed with negligible overhead. Extensive experiments across diverse datasets and architectures demonstrate that SADP consistently outperforms data pruning baselines and achieves training speedups close to the theoretical maxima at different pruning ratios. Notably, SADP reduces training time by 35% on ImageNet while maintaining accuracy comparable to that of full-data training. This work, therefore, establishes a data-centric paradigm for efficient SNN training and paves the way for scaling SNNs to larger models and datasets. The source code will be released publicly after the review process.",
        "subjects": "Neural and Evolutionary Computing, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.451399"
    },
    {
        "index": "#238",
        "title": "Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes",
        "link": "/arxiv/2510.04090",
        "arxiv_id": "2510.04090",
        "authors": "Nikita Gabdullin",
        "summary": "Supervised learning (SL) methods are indispensable for neural network (NN) training used to perform classification tasks. While resulting in very high accuracy, SL training often requires making NN parameter number dependent on the number of classes, limiting their applicability when the number of classes is extremely large or unknown in advance. In this paper we propose a methodology that allows one to train the same NN architecture regardless of the number of classes. This is achieved by using predefined vector systems as the target latent space configuration (LSC) during NN training. We discuss the desired properties of target configurations and choose randomly perturbed vectors of An root system for our experiments. These vectors are used to successfully train encoders and visual transformers (ViT) on Cinic-10 and ImageNet-1K in low- and high-dimensional cases by matching NN predictions with the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million classes illustrating the applicability of the method to training on datasets with extremely large number of classes. In addition, potential applications of LSC in lifelong learning and NN distillation are discussed illustrating versatility of the proposed methodology.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.452072"
    },
    {
        "index": "#239",
        "title": "Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees",
        "link": "/arxiv/2510.04088",
        "arxiv_id": "2510.04088",
        "authors": "Nan Jiang, Tengyang Xie",
        "summary": "This article introduces the theory of offline reinforcement learning in large state spaces, where good policies are learned from historical data without online interactions with the environment. Key concepts introduced include expressivity assumptions on function approximation (e.g., Bellman completeness vs. realizability) and data coverage (e.g., all-policy vs. single-policy coverage). A rich landscape of algorithms and results is described, depending on the assumptions one is willing to make and the sample and computational complexity guarantees one wishes to achieve. We also discuss open questions and connections to adjacent areas.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.452737"
    },
    {
        "index": "#240",
        "title": "A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling",
        "link": "/arxiv/2510.04087",
        "arxiv_id": "2510.04087",
        "authors": "Hyung Gyu Rho",
        "summary": "Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is \\textit{better}, but what is \\textit{good enough}. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.",
        "subjects": "Methodology, Artificial Intelligence, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.453386"
    },
    {
        "index": "#243",
        "title": "MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation",
        "link": "/arxiv/2510.04057",
        "arxiv_id": "2510.04057",
        "authors": "Zhenyu Pan, Yucheng Lu, Han Liu",
        "summary": "We present MetaFind, a scene-aware tri-modal compositional retrieval framework designed to enhance scene generation in the metaverse by retrieving 3D assets from large-scale repositories. MetaFind addresses two core challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic, and stylistic constraints, and (ii) the absence of a standardized retrieval paradigm specifically tailored for 3D asset retrieval, as existing approaches mainly rely on general-purpose 3D shape representation models. Our key innovation is a flexible retrieval mechanism that supports arbitrary combinations of text, image, and 3D modalities as queries, enhancing spatial reasoning and style consistency by jointly modeling object-level features (including appearance) and scene-level layout structures. Methodologically, MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that captures spatial relationships and object appearance features, ensuring retrieved 3D assets are contextually and stylistically coherent with the existing scene, regardless of coordinate frame transformations. The framework supports iterative scene construction by continuously adapting retrieval results to current scene updates. Empirical evaluations demonstrate the improved spatial and stylistic consistency of MetaFind in various retrieval tasks compared to baseline methods.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.455597"
    },
    {
        "index": "#244",
        "title": "Quantization Range Estimation for Convolutional Neural Networks",
        "link": "/arxiv/2510.04044",
        "arxiv_id": "2510.04044",
        "authors": "Bingtao Yang, Yujia Wang, Mengzhi Jiao, Hongwei Huo",
        "summary": "Post-training quantization for reducing the storage of deep neural network models has been demonstrated to be an effective way in various tasks. However, low-bit quantization while maintaining model accuracy is a challenging problem. In this paper, we present a range estimation method to improve the quantization performance for post-training quantization. We model the range estimation into an optimization problem of minimizing quantization errors by layer-wise local minima. We prove this problem is locally convex and present an efficient search algorithm to find the optimal solution. We propose the application of the above search algorithm to the transformed weights space to do further improvement in practice. Our experiments demonstrate that our method outperforms state-of-the-art performance generally on top-1 accuracy for image classification tasks on the ResNet series models and Inception-v3 model. The experimental results show that the proposed method has almost no loss of top-1 accuracy in 8-bit and 6-bit settings for image classifications, and the accuracy of 4-bit quantization is also significantly improved. The code is available at https://github.com/codeiscommitting/REQuant.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.456282"
    },
    {
        "index": "#245",
        "title": "\\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding",
        "link": "/arxiv/2510.04039",
        "arxiv_id": "2510.04039",
        "authors": "Bin Lei, Nuo Xu, Ali Payani, Mingyi Hong, Chunhua Liao, Yu Cao, Caiwen Ding",
        "summary": "Multimodal large language models (MLLMs) have markedly expanded the competence of graphical user-interface (GUI) systems, propelling them beyond controlled simulations into complex, real-world environments across diverse platforms. However, practical usefulness is still bounded by the reliability of visual grounding, i.e., mapping textual references to exact on-screen elements. This limitation prevents the system from accurately performing pointer-level actions such as clicking or dragging. To address it, we introduce GUI-Spotlight -- a model trained for image-grounded reasoning that dynamically invokes multiple specialized tools to iteratively narrow its focus to the relevant region of the screen, thereby substantially improving visual grounding accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only 18.5K training samples achieves 52.8\\% accuracy, surpassing V2P-7B (50.6\\% with 9.6M training samples) and GTA-1-7B (50.1\\% with 1.56M training samples).",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.462147"
    },
    {
        "index": "#246",
        "title": "Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks",
        "link": "/arxiv/2510.04034",
        "arxiv_id": "2510.04034",
        "authors": "Linn Bieske, Carla Lorente",
        "summary": "Recent advances in image editing have shifted from manual pixel manipulation to employing deep learning methods like stable diffusion models, which now leverage cross-attention mechanisms for text-driven control. This transition has simplified the editing process but also introduced variability in results, such as inconsistent hair color changes. Our research aims to enhance the precision and reliability of prompt-to-prompt image editing frameworks by exploring and optimizing hyperparameters. We present a comprehensive study of the \"word swap\" method, develop an \"attention re-weight method\" for better adaptability, and propose the \"CL P2P\" framework to address existing limitations like cycle inconsistency. This work contributes to understanding and improving the interaction between hyperparameter settings and the architectural choices of neural network models, specifically their attention mechanisms, which significantly influence the composition and quality of the generated images.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.462871"
    },
    {
        "index": "#249",
        "title": "The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View",
        "link": "/arxiv/2510.04028",
        "arxiv_id": "2510.04028",
        "authors": "Xinhao Yao, Lu Yu, Xiaolin Hu, Fengwei Teng, Qing Cui, Jun Zhou, Yong Liu",
        "summary": "The ongoing debate on whether reinforcement learning with verifiable rewards (RLVR) expands or shrinks the reasoning capabilities of large language models (LLMs) remains unresolved. Some studies contend that RLVR mainly improves sampling efficiency but at the expense of diversity and exploratory capacity, resulting in capability boundary shrinkage. In contrast, others demonstrate that prolonged training can lead to the emergence of novel reasoning strategies, suggesting capability boundary expansion. To reconcile these contradictory findings, we theoretically and empirically show that both perspectives are partially valid-each aligning with a separate phase in an inherent two-stage probability mass dynamic: (1) Exploitation stage: initially, the model primarily samples explored high-reward and low-reward tokens, while rarely selecting the potentially optimal token. Positive advantage estimates increase the probability of high-reward tokens and decrease those of low-reward tokens, yet the optimal token's probability remains largely unchanged during this stage. (2) Exploration stage: as training advances, the growth rate of previously acquired high-reward tokens slows as their probabilities approach saturation. When a potentially optimal token-now receiving positive advantage estimates-is occasionally sampled, its probability increases, while those of the originally high-reward tokens decrease. This dynamic suggests that over-exploitation during the exploitation stage may lead to capability boundary shrinkage, whereas prolonged training into the exploration stage can promote an expansion of the reasoning capability boundary. Building upon our insights, we revisit the potential of only using relative negative gradients for prolonging training, providing a theoretical and empirical foundation for the development of more advanced reasoning capabilities.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.465034"
    },
    {
        "index": "#250",
        "title": "Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models",
        "link": "/arxiv/2510.04020",
        "arxiv_id": "2510.04020",
        "authors": "Hao Wu, Yuan Gao, Xingjian Shi, Shuaipeng Li, Fan Xu, Fan Zhang, Zhihong Zhu, Weiyan Wang, Xiao Luo, Kun Wang, Xian Wu, Xiaomeng Huang",
        "summary": "To address the dual challenges of inherent stochasticity and non-differentiable metrics in physical spatiotemporal forecasting, we propose Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in Model-Based Reinforcement Learning. SFP constructs a novel Generative World Model to simulate diverse, high-fidelity future states, enabling an \"imagination-based\" environmental simulation. Within this framework, a base forecasting model acts as an agent, guided by a beam search-based planning algorithm that leverages non-differentiable domain metrics as reward signals to explore high-return future sequences. These identified high-reward candidates then serve as pseudo-labels to continuously optimize the agent's policy through iterative self-training, significantly reducing prediction error and demonstrating exceptional performance on critical domain metrics like capturing extreme events.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.465911"
    },
    {
        "index": "#253",
        "title": "Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention",
        "link": "/arxiv/2510.04008",
        "arxiv_id": "2510.04008",
        "authors": "Sahil Joshi, Agniva Chowdhury, Amar Kanakamedala, Ekam Singh, Evan Tu, Anshumali Shrivastava",
        "summary": "Softmax Attention has a quadratic time complexity, which becomes prohibitive to run at long contexts, even with highly optimized GPU kernels. For example, FlashAttention (an exact, GPU-optimized implementation of Softmax Attention) cannot complete a single forward-backward pass of a multi-head attention layer once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We introduce RACE Attention, a kernel-inspired alternative to Softmax Attention that is linear in sequence length and embedding dimension. RACE Attention replaces the exponential kernel with a sharpened angular (cosine) similarity, and approximates attention outputs via randomized projections and soft Locality-Sensitive Hashing (LSH). Across language modeling, masked language modeling, and text classification, RACE Attention matches the accuracy of strong baselines while reducing runtime and memory. In a controlled scale test, it processes up to 12 million tokens during a single forward-backward pass on an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well beyond the practical limits of the current state-of-the-art attention implementations. RACE Attention thus offers a practical, theoretically grounded mechanism for outrageously long context windows on today's hardware. We hope that it gets adopted in practice.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.473123"
    },
    {
        "index": "#255",
        "title": "AI-Driven Grading and Moderation for Collaborative Projects in Computer Science Education",
        "link": "/arxiv/2510.03998",
        "arxiv_id": "2510.03998",
        "authors": "Songmei Yu, Andrew Zagula",
        "summary": "Collaborative group projects are integral to computer science education, as they foster teamwork, problem-solving skills, and industry-relevant competencies. However, assessing individual contributions within group settings has long been a challenge. Traditional assessment strategies, such as the equal distribution of grades or subjective peer assessments, often fall short in terms of fairness, objectivity, and scalability, particularly in large classrooms. This paper introduces a semi-automated, AI-assisted grading system that evaluates both project quality and individual effort using repository mining, communication analytics, and machine learning models. The system comprises modules for project evaluation, contribution analysis, and grade computation, integrating seamlessly with platforms like GitHub. A pilot deployment in a senior-level course demonstrated high alignment with instructor assessments, increased student satisfaction, and reduced instructor grading effort. We conclude by discussing implementation considerations, ethical implications, and proposed enhancements to broaden applicability.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Computers and Society",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.474437"
    },
    {
        "index": "#256",
        "title": "PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks",
        "link": "/arxiv/2510.03995",
        "arxiv_id": "2510.03995",
        "authors": "Nges Brian Njungle, Eric Jahns, Milan Stojkov, Michel A. Kinsy",
        "summary": "Deep learning has become a cornerstone of modern machine learning. It relies heavily on vast datasets and significant computational resources for high performance. This data often contains sensitive information, making privacy a major concern in deep learning. Spiking Neural Networks (SNNs) have emerged as an energy-efficient alternative to conventional deep learning approaches. Nevertheless, SNNs still depend on large volumes of data, inheriting all the privacy challenges of deep learning. Homomorphic encryption addresses this challenge by allowing computations to be performed on encrypted data, ensuring data confidentiality throughout the entire processing pipeline. In this paper, we introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using the CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs and introduces two key algorithms for evaluating the Leaky Integrate-and-Fire activation function: (1) a polynomial approximation algorithm designed for high-performance SNN inference, and (2) a novel scheme-switching algorithm that optimizes precision at a higher computational cost. We evaluate PRIVSPIKE on MNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5 and ResNet-19 architectures, achieving encrypted inference accuracies of 98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN LeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds on Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on CIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as a viable and efficient solution for secure SNN inference, bridging the gap between energy-efficient deep neural networks and strong cryptographic privacy guarantees while outperforming prior encrypted SNN solutions.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.475123"
    },
    {
        "index": "#257",
        "title": "Quantifying Distributional Robustness of Agentic Tool-Selection",
        "link": "/arxiv/2510.03992",
        "arxiv_id": "2510.03992",
        "authors": "Jehyeok Yeon, Isha Chaudhary, Gagandeep Singh",
        "summary": "Large language models (LLMs) are increasingly deployed in agentic systems where they map user intents to relevant external tools to fulfill a task. A critical step in this process is tool selection, where a retriever first surfaces candidate tools from a larger pool, after which the LLM selects the most appropriate one. This pipeline presents an underexplored attack surface where errors in selection can lead to severe outcomes like unauthorized data access or denial of service, all without modifying the agent's model or code. While existing evaluations measure task performance in benign settings, they overlook the specific vulnerabilities of the tool selection mechanism under adversarial conditions. To address this gap, we introduce ToolCert, the first statistical framework that formally certifies tool selection robustness. ToolCert models tool selection as a Bernoulli success process and evaluates it against a strong, adaptive attacker who introduces adversarial tools with misleading metadata, and are iteratively refined based on the agent's previous choices. By sampling these adversarial interactions, ToolCert produces a high-confidence lower bound on accuracy, formally quantifying the agent's worst-case performance. Our evaluation with ToolCert uncovers the severe fragility: under attacks injecting deceptive tools or saturating retrieval, the certified accuracy bound drops near zero, an average performance drop of over 60% compared to non-adversarial settings. For attacks targeting the retrieval and selection stages, the certified accuracy bound plummets to less than 20% after just a single round of adversarial adaptation. ToolCert thus reveals previously unexamined security threats inherent to tool selection and provides a principled method to quantify an agent's robustness to such threats, a necessary step for the safe deployment of agentic systems.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.475865"
    },
    {
        "index": "#258",
        "title": "A Mathematical Explanation of Transformers for Large Language Models and GPTs",
        "link": "/arxiv/2510.03989",
        "arxiv_id": "2510.03989",
        "authors": "Xue-Cheng Tai, Hao Liu, Lingfeng Li, Raymond H. Chan",
        "summary": "The Transformer architecture has revolutionized the field of sequence modeling and underpins the recent breakthroughs in large language models (LLMs). However, a comprehensive mathematical theory that explains its structure and operations remains elusive. In this work, we propose a novel continuous framework that rigorously interprets the Transformer as a discretization of a structured integro-differential equation. Within this formulation, the self-attention mechanism emerges naturally as a non-local integral operator, and layer normalization is characterized as a projection to a time-dependent constraint. This operator-theoretic and variational perspective offers a unified and interpretable foundation for understanding the architecture's core components, including attention, feedforward layers, and normalization. Our approach extends beyond previous theoretical analyses by embedding the entire Transformer operation in continuous domains for both token indices and feature dimensions. This leads to a principled and flexible framework that not only deepens theoretical insight but also offers new directions for architecture design, analysis, and control-based interpretations. This new interpretation provides a step toward bridging the gap between deep learning architectures and continuous mathematical modeling, and contributes a foundational perspective to the ongoing development of interpretable and theoretically grounded neural network models.",
        "subjects": "Machine Learning, Artificial Intelligence, Numerical Analysis",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.476589"
    },
    {
        "index": "#259",
        "title": "Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data",
        "link": "/arxiv/2510.03988",
        "arxiv_id": "2510.03988",
        "authors": "Hoang Anh Just, Myeongseob Ko, Ruoxi Jia",
        "summary": "Distilling long reasoning traces (10K+ tokens) from stronger teacher models into smaller student LLMs via SFT has emerged as a standard paradigm. This approach is practical and efficient: it leverages the ease of generating abundant reasoning data from stronger models and provides a direct, data-driven way to teach less capable models better reasoning. While previous work has largely focused on prompt selection with responses from a single teacher, the equally important problem of choosing the best response when multiple teacher outputs are available for a single prompt remains underexplored. This challenge becomes important in a multi-teacher setting, where different students may benefit from the outputs of different teachers. This paper fills that gap with a systematic study of response selection for reasoning distillation. We first show that the current method, which picks responses the student assigns the highest global log-probability (global naturalness), fails when responses come from multiple teachers, i.e., global naturalness no longer correlates with downstream performance, especially as the reasoning traces from strong teachers become longer. To overcome this problem, we introduce Local Naturalness, which measures the student's log-probabilities over short, sequential reasoning steps conditioned only on a small local window. Local Naturalness enables two applications: 1) Teacher Selection: Aggregating local scores across prompts reliably identifies the most helpful teacher. 2) Response Selection from a Multiple Teachers: When mixing answers from many teachers, Local Naturalness boosts a 32B student's accuracy on math benchmarks by 9.4pp over global selection, also surpassing the performance achieved by training on data from the single best teacher. These results highlight the power of localized data quality evaluation and data mixing for more effective reasoning distillation.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.482422"
    },
    {
        "index": "#260",
        "title": "What Can You Do When You Have Zero Rewards During RL?",
        "link": "/arxiv/2510.03971",
        "arxiv_id": "2510.03971",
        "authors": "Jatin Prakash, Anirudh Buvanesh",
        "summary": "Reinforcement learning (RL) with outcome-based rewards has proven effective for improving large language models (LLMs) on complex reasoning tasks. However, its success often depends on the base model occasionally sampling correct solutions. When no correct solutions are sampled, training encounters a zero-reward barrier where learning stalls due to zero gradients. We study this scenario through the graph search task introduced in Bachmann et al. (2024) and evaluate recent methods that incorporate desirable components such as dense rewards, diversity incentives, and improved credit assignment. Our experiments show that none of these approaches overcome the zero-reward barrier if the base model never produces a correct answer. In contrast, we find that a simple data-centric intervention of adding easier samples to the training set enables the model to eventually solve the original hard task despite starting from zero reward. Importantly, this succeeds without modifying the RL algorithm itself. Because official implementations of several baselines were unavailable, we developed our own, which allowed us to conduct a detailed analysis of their failure modes. We release these implementations to support further research at: https://github.com/rl4reasoning/rl-baselines",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.483065"
    },
    {
        "index": "#261",
        "title": "Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning",
        "link": "/arxiv/2510.03970",
        "arxiv_id": "2510.03970",
        "authors": "Zainab Saad, Jialin Yang, Henry Leung, Steve Drew",
        "summary": "The growing reliance on large-scale data centers to run resource-intensive workloads has significantly increased the global carbon footprint, underscoring the need for sustainable computing solutions. While container orchestration platforms like Kubernetes help optimize workload scheduling to reduce carbon emissions, existing methods often depend on centralized machine learning models that raise privacy concerns and struggle to generalize across diverse environments. In this paper, we propose a federated learning approach for energy consumption prediction that preserves data privacy by keeping sensitive operational data within individual enterprises. By extending the Kubernetes Efficient Power Level Exporter (Kepler), our framework trains XGBoost models collaboratively across distributed clients using Flower's FedXgbBagging aggregation using a bagging strategy, eliminating the need for centralized data sharing. Experimental results on the SPECPower benchmark dataset show that our FL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a centralized baseline. This work addresses the unresolved trade-off between data privacy and energy prediction efficiency in prior systems such as Kepler and CASPER and offers enterprises a viable pathway toward sustainable cloud computing without compromising operational privacy.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.483763"
    },
    {
        "index": "#262",
        "title": "SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data",
        "link": "/arxiv/2510.03962",
        "arxiv_id": "2510.03962",
        "authors": "Hanzhe Wei, Jiajun Wu, Jialin Yang, Henry Leung, Steve Drew",
        "summary": "Time series anomaly detection plays a crucial role in a wide range of fields, such as healthcare and internet traffic monitoring. The emergence of large language models (LLMs) offers new opportunities for detecting anomalies in the ubiquitous time series data. Traditional approaches struggle with variable-length time series sequences and context-based anomalies. We propose Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage LLMs for anomaly detection with soft prompts and quantization. Our methodology involves quantizing and transforming the time series data into input embeddings and combining them with learnable soft prompt embeddings. These combined embeddings are then fed into a frozen LLM. The soft prompts are updated iteratively based on a cross-entropy loss, allowing the model to adapt to time series anomaly detection. The use of soft prompts helps adapt LLMs effectively to time series tasks, while quantization ensures optimal handling of sequences, as LLMs are designed to handle discrete sequences. Our experimental results demonstrate that soft prompts effectively increase LLMs' performance in downstream tasks regarding time series anomaly detection.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.484466"
    },
    {
        "index": "#265",
        "title": "On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks",
        "link": "/arxiv/2510.03923",
        "arxiv_id": "2510.03923",
        "authors": "Mingsong Yan, Charles Kulick, Sui Tang",
        "summary": "Continuous-depth graph neural networks, also known as Graph Neural Differential Equations (GNDEs), combine the structural inductive bias of Graph Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs, offering a scalable and principled framework for modeling dynamics on graphs. In this paper, we present a rigorous convergence analysis of GNDEs with time-varying parameters in the infinite-node limit, providing theoretical insights into their size transferability. To this end, we introduce Graphon Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of GNDEs and establish their well-posedness. Leveraging tools from graphon theory and dynamical systems, we prove the trajectory-wise convergence of GNDE solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence rates under two deterministic graph sampling regimes: (1) weighted graphs sampled from smooth graphons, and (2) unweighted graphs sampled from $\\{0,1\\}$-valued (discontinuous) graphons. We further establish size transferability bounds, providing theoretical justification for the practical strategy of transferring GNDE models trained on moderate-sized graphs to larger, structurally similar graphs without retraining. Numerical experiments using synthetic and real data support our theoretical findings.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.486500"
    },
    {
        "index": "#266",
        "title": "Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition",
        "link": "/arxiv/2510.03921",
        "arxiv_id": "2510.03921",
        "authors": "Arushi Dashore, Aryan Anumala, Emily Hui, Olivia Yang",
        "summary": "Automated tennis stroke analysis has advanced significantly with the integration of biomechanical motion cues alongside deep learning techniques, enhancing stroke classification accuracy and player performance evaluation. Despite these advancements, existing systems often fail to connect biomechanical insights with actionable language feedback that is both accessible and meaningful to players and coaches. This research project addresses this gap by developing a novel framework that extracts key biomechanical features (such as joint angles, limb velocities, and kinetic chain patterns) from motion data using Convolutional Neural Network Long Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for relationships influencing stroke effectiveness and injury risk, forming the basis for feedback generation using large language models (LLMs). Leveraging the THETIS dataset and feature extraction techniques, our approach aims to produce feedback that is technically accurate, biomechanically grounded, and actionable for end-users. The experimental setup evaluates this framework on classification performance and interpretability, bridging the gap between explainable AI and sports biomechanics.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Human-Computer Interaction",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.492444"
    },
    {
        "index": "#267",
        "title": "Refactoring with LLMs: Bridging Human Expertise and Machine Understanding",
        "link": "/arxiv/2510.03914",
        "arxiv_id": "2510.03914",
        "authors": "Yonnel Chen Kuang Piao, Jean Carlors Paul, Leuson Da Silva, Arghavan Moradi Dakhel, Mohammad Hamdaqa, Foutse Khomh",
        "summary": "Code refactoring is a fundamental software engineering practice aimed at improving code quality and maintainability. Despite its importance, developers often neglect refactoring due to the significant time, effort, and resources it requires, as well as the lack of immediate functional rewards. Although several automated refactoring tools have been proposed, they remain limited in supporting a broad spectrum of refactoring types. In this study, we explore whether instruction strategies inspired by human best-practice guidelines can enhance the ability of Large Language Models (LLMs) to perform diverse refactoring tasks automatically. Leveraging the instruction-following and code comprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and DeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design multiple instruction strategies that encode motivations, procedural steps, and transformation objectives for 61 well-known refactoring types. We evaluate these strategies on benchmark examples and real-world code snippets from GitHub projects. Our results show that instruction designs grounded in Fowler's guidelines enable LLMs to successfully perform all benchmark refactoring types and preserve program semantics in real-world settings, an essential criterion for effective refactoring. Moreover, while descriptive instructions are more interpretable to humans, our results show that rule-based instructions often lead to better performance in specific scenarios. Interestingly, allowing models to focus on the overall goal of refactoring, rather than prescribing a fixed transformation type, can yield even greater improvements in code quality.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.493167"
    },
    {
        "index": "#268",
        "title": "Adversarial Agent Collaboration for C to Rust Translation",
        "link": "/arxiv/2510.03879",
        "arxiv_id": "2510.03879",
        "authors": "Tianyu Li, Ruishi Li, Bo Wang, Brandon Paulsen, Umang Mathur, Prateek Saxena",
        "summary": "Translating C to memory-safe languages, like Rust, prevents critical memory safety vulnerabilities that are prevalent in legacy C software. Existing approaches for C to safe Rust translation, including LLM-assisted ones, do not generalize on larger (> 500 LoC) C codebases because they depend on complex program analyses that frequently break. In this work, we present ACToR (Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired by GANs, ACToR pits a generator agent against a discriminator agent, which collaborate to iteratively generate a Rust translation. On each iteration, the translator agent synthesizes and refines a Rust translation to pass an existing suite of tests, and then the discriminator agent finds new failing tests. We demonstrate that ACToR translates all of the 63 real-world command line utilities considered in our benchmarks, which have an average size of 485 lines of code, and it achieves over 90% test pass rate with zero human intervention. To our knowledge, it is the first such system that reliably translates C programs of this scale. Furthermore, ACToR improves translation correctness by up to 18.9% compared to baseline, non-adversarial approaches.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.493879"
    },
    {
        "index": "#269",
        "title": "Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks",
        "link": "/arxiv/2510.03878",
        "arxiv_id": "2510.03878",
        "authors": "Ajo Babu George, Sreehari J R Ajo Babu George, Sreehari J R Ajo Babu George, Sreehari J R",
        "summary": "Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes significantly to its high global mortality rate, with over 50\\% of cases detected at advanced stages and a 5-year survival rate below 50\\% according to WHO statistics. This study aims to improve early detection of OSCC by developing a multimodal deep learning framework that integrates clinical, radiological, and histopathological images using a weighted ensemble of DenseNet-121 convolutional neural networks (CNNs). Material and Methods A retrospective study was conducted using publicly available datasets representing three distinct medical imaging modalities. Each modality-specific dataset was used to train a DenseNet-121 CNN via transfer learning. Augmentation and modality-specific preprocessing were applied to increase robustness. Predictions were fused using a validation-weighted ensemble strategy. Evaluation was performed using accuracy, precision, recall, F1-score. Results High validation accuracy was achieved for radiological (100\\%) and histopathological (95.12\\%) modalities, with clinical images performing lower (63.10\\%) due to visual heterogeneity. The ensemble model demonstrated improved diagnostic robustness with an overall accuracy of 84.58\\% on a multimodal validation dataset of 55 samples. Conclusion The multimodal ensemble framework bridges gaps in the current diagnostic workflow by offering a non-invasive, AI-assisted triage tool that enhances early identification of high-risk lesions. It supports clinicians in decision-making, aligning with global oncology guidelines to reduce diagnostic delays and improve patient outcomes.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.494568"
    },
    {
        "index": "#270",
        "title": "PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis",
        "link": "/arxiv/2510.03873",
        "arxiv_id": "2510.03873",
        "authors": "Saja Al-Dabet, Sherzod Turaev, Nazar Zaki, Arif O. Khan, Luai Eldweik",
        "summary": "Diagnosing ocular-induced abnormal head posture (AHP) requires a comprehensive analysis of both head pose and ocular movements. However, existing datasets focus on these aspects separately, limiting the development of integrated diagnostic approaches and restricting AI-driven advancements in AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D dataset that synchronously captures head pose and gaze movement information for ocular-induced AHP assessment. Structured clinical data were extracted from medical literature using large language models (LLMs) through an iterative process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and complex prompting strategies. The extracted records were systematically imputed and transformed into 3D representations using the Neural Head Avatar (NHA) framework. The dataset includes 7,920 images generated from two head textures, covering a broad spectrum of ocular conditions. The extraction method achieved an overall accuracy of 91.92%, demonstrating its reliability for clinical dataset construction. PoseGaze-AHP is the first publicly available resource tailored for AI-driven ocular-induced AHP diagnosis, supporting the development of accurate and privacy-compliant diagnostic tools.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.495263"
    },
    {
        "index": "#271",
        "title": "Optimal Scaling Needs Optimal Norm",
        "link": "/arxiv/2510.03871",
        "arxiv_id": "2510.03871",
        "authors": "Oleg Filatov, Jiangtao Wang, Jan Ebert, Stefan Kesselheim",
        "summary": "Despite recent progress in optimal hyperparameter transfer under model and dataset scaling, no unifying explanatory principle has been established. Using the Scion optimizer, we discover that joint optimal scaling across model and dataset sizes is governed by a single invariant: the operator norm of the output layer. Across models with up to 1.3B parameters trained on up to 138B tokens, the optimal learning rate/batch size pair $(\\eta^{\\ast}, B^{\\ast})$ consistently has the same operator norm value - a phenomenon we term norm transfer. This constant norm condition is necessary but not sufficient: while for each dataset size, multiple $(\\eta, B)$ reach the optimal norm, only a unique $(\\eta^{\\ast}, B^{\\ast})$ achieves the best loss. As a sufficient condition, we provide the first measurement of $(\\eta^{\\ast}, B^{\\ast})$ scaling with dataset size for Scion, and find that the scaling rules are consistent with those of the Adam optimizer. Tuning per-layer-group learning rates also improves model performance, with the output layer being the most sensitive and hidden layers benefiting from lower learning rates. We provide practical insights on norm-guided optimal scaling and release our Distributed Scion (Disco) implementation with logs from over two thousand runs to support research on LLM training dynamics at scale.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.495998"
    },
    {
        "index": "#272",
        "title": "AI Adoption Across Mission-Driven Organizations",
        "link": "/arxiv/2510.03868",
        "arxiv_id": "2510.03868",
        "authors": "Dalia Ali, Muneeb Ahmed, Hailan Wang, Arfa Khan, Naira Paola Arnez Jordan, Sunnie S. Y. Kim, Meet Dilip Muchhala, Anne Kathrin Merkle, Orestis Papakyriakopoulos",
        "summary": "Despite AI's promise for addressing global challenges, empirical understanding of AI adoption in mission-driven organizations (MDOs) remains limited. While research emphasizes individual applications or ethical principles, little is known about how resource-constrained, values-driven organizations navigate AI integration across operations. We conducted thematic analysis of semi-structured interviews with 15 practitioners from environmental, humanitarian, and development organizations across the Global North and South contexts. Our analysis examines how MDOs currently deploy AI, what barriers constrain adoption, and how practitioners envision future integration. MDOs adopt AI selectively, with sophisticated deployment in content creation and data analysis while maintaining human oversight for mission-critical applications. When AI's efficiency benefits conflict with organizational values, decision-making stalls rather than negotiating trade-offs. This study contributes empirical evidence that AI adoption in MDOs should be understood as conditional rather than inevitable, proceeding only where it strengthens organizational sovereignty and mission integrity while preserving human-centered approaches essential to their missions.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.496774"
    },
    {
        "index": "#274",
        "title": "Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework",
        "link": "/arxiv/2510.03862",
        "arxiv_id": "2510.03862",
        "authors": "Nathalia Nascimento, Everton Guimaraes, Paulo Alencar",
        "summary": "The rise of large language models (LLMs) has introduced transformative potential in automated code generation, addressing a wide range of software engineering challenges. However, empirical evaluation of LLM-based code generation lacks standardization, with studies varying widely in goals, tasks, and metrics, which limits comparability and reproducibility. In this paper, we propose a theoretical framework for designing and reporting empirical studies on LLM-based code generation. The framework is grounded in both our prior experience conducting such experiments and a comparative analysis of key similarities and differences among recent studies. It organizes evaluation around core components such as problem sources, quality attributes, and metrics, supporting structured and systematic experimentation. We demonstrate its applicability through representative case mappings and identify opportunities for refinement. Looking forward, we plan to evolve the framework into a more robust and mature tool for standardizing LLM evaluation across software engineering contexts.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.503241"
    },
    {
        "index": "#275",
        "title": "AI-Assisted Pleural Effusion Volume Estimation from Contrast-Enhanced CT Images",
        "link": "/arxiv/2510.03856",
        "arxiv_id": "2510.03856",
        "authors": "Sanhita Basu, Tomas Fröding, Ali Teymur Kahraman, Dimitris Toumpanakis, Tobias Sjöblom",
        "summary": "Background: Pleural Effusions (PE) is a common finding in many different clinical conditions, but accurately measuring their volume from CT scans is challenging. Purpose: To improve PE segmentation and quantification for enhanced clinical management, we have developed and trained a semi-supervised deep learning framework on contrast-enhanced CT volumes. Materials and Methods: This retrospective study collected CT Pulmonary Angiogram (CTPA) data from internal and external datasets. A subset of 100 cases was manually annotated for model training, while the remaining cases were used for testing and validation. A novel semi-supervised deep learning framework, Teacher-Teaching Assistant-Student (TTAS), was developed and used to enable efficient training in non-segmented examinations. Segmentation performance was compared to that of state-of-the-art models. Results: 100 patients (mean age, 72 years, 28 [standard deviation]; 55 men) were included in the study. The TTAS model demonstrated superior segmentation performance compared to state-of-the-art models, achieving a mean Dice score of 0.82 (95% CI, 0.79 - 0.84) versus 0.73 for nnU-Net (p < 0.0001, Student's T test). Additionally, TTAS exhibited a four-fold lower mean Absolute Volume Difference (AbVD) of 6.49 mL (95% CI, 4.80 - 8.20) compared to nnU-Net's AbVD of 23.16 mL (p < 0.0001). Conclusion: The developed TTAS framework offered superior PE segmentation, aiding accurate volume determination from CT scans.",
        "subjects": "Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.503959"
    },
    {
        "index": "#276",
        "title": "A4FN: an Agentic AI Architecture for Autonomous Flying Networks",
        "link": "/arxiv/2510.03829",
        "arxiv_id": "2510.03829",
        "authors": "André Coelho, Pedro Ribeiro, Helder Fontes, Rui Campos",
        "summary": "This position paper presents A4FN, an Agentic Artificial Intelligence (AI) architecture for intent-driven automation in Flying Networks (FNs) using Unmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI and Large Language Models (LLMs) to enable real-time, context-aware network control via a distributed agentic system. It comprises two components: the Perception Agent (PA), which semantically interprets multimodal input -- including imagery, audio, and telemetry data -- from UAV-mounted sensors to derive Service Level Specifications (SLSs); and the Decision-and-Action Agent (DAA), which reconfigures the network based on inferred intents. A4FN embodies key properties of Agentic AI, including autonomy, goal-driven reasoning, and continuous perception-action cycles. Designed for mission-critical, infrastructure-limited scenarios such as disaster response, it supports adaptive reconfiguration, dynamic resource management, and interoperability with emerging wireless technologies. The paper details the A4FN architecture, its core innovations, and open research challenges in multi-agent coordination and Agentic AI integration in next-generation FNs.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.504606"
    },
    {
        "index": "#277",
        "title": "Proximal Diffusion Neural Sampler",
        "link": "/arxiv/2510.03824",
        "arxiv_id": "2510.03824",
        "authors": "Wei Guo, Jaemoo Choi, Yuchen Zhu, Molei Tao, Yongxin Chen",
        "summary": "The task of learning a diffusion-based neural sampler for drawing samples from an unnormalized target distribution can be viewed as a stochastic optimal control problem on path measures. However, the training of neural samplers can be challenging when the target distribution is multimodal with significant barriers separating the modes, potentially leading to mode collapse. We propose a framework named \\textbf{Proximal Diffusion Neural Sampler (PDNS)} that addresses these challenges by tackling the stochastic optimal control problem via proximal point method on the space of path measures. PDNS decomposes the learning process into a series of simpler subproblems that create a path gradually approaching the desired distribution. This staged procedure traces a progressively refined path to the desired distribution and promotes thorough exploration across modes. For a practical and efficient realization, we instantiate each proximal step with a proximal weighted denoising cross-entropy (WDCE) objective. We demonstrate the effectiveness and robustness of PDNS through extensive experiments on both continuous and discrete sampling tasks, including challenging scenarios in molecular dynamics and statistical physics.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.505303"
    },
    {
        "index": "#278",
        "title": "Detecting Invariant Manifolds in ReLU-Based RNNs",
        "link": "/arxiv/2510.03814",
        "arxiv_id": "2510.03814",
        "authors": "Lukas Eisenmann, Alena Brändle, Zahra Monfared, Daniel Durstewitz",
        "summary": "Recurrent Neural Networks (RNNs) have found widespread applications in machine learning for time series prediction and dynamical systems reconstruction, and experienced a recent renaissance with improved training algorithms and architectural designs. Understanding why and how trained RNNs produce their behavior is important for scientific and medical applications, and explainable AI more generally. An RNN's dynamical repertoire depends on the topological and geometrical properties of its state space. Stable and unstable manifolds of periodic points play a particularly important role: They dissect a dynamical system's state space into different basins of attraction, and their intersections lead to chaotic dynamics with fractal geometry. Here we introduce a novel algorithm for detecting these manifolds, with a focus on piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as their activation function. We demonstrate how the algorithm can be used to trace the boundaries between different basins of attraction, and hence to characterize multistability, a computationally important property. We further show its utility in finding so-called homoclinic points, the intersections between stable and unstable manifolds, and thus establish the existence of chaos in PLRNNs. Finally we show for an empirical example, electrophysiological recordings from a cortical neuron, how insights into the underlying dynamics could be gained through our method.",
        "subjects": "Machine Learning, Artificial Intelligence, Dynamical Systems",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.505982"
    },
    {
        "index": "#279",
        "title": "Diverse Text-to-Image Generation via Contrastive Noise Optimization",
        "link": "/arxiv/2510.03813",
        "arxiv_id": "2510.03813",
        "authors": "Byungjun Kim, Soobin Um, Jong Chul Ye",
        "summary": "Text-to-image (T2I) diffusion models have demonstrated impressive performance in generating high-fidelity images, largely enabled by text-guided inference. However, this advantage often comes with a critical drawback: limited diversity, as outputs tend to collapse into similar modes under strong text guidance. Existing approaches typically optimize intermediate latents or text conditions during inference, but these methods deliver only modest gains or remain sensitive to hyperparameter tuning. In this work, we introduce Contrastive Noise Optimization, a simple yet effective method that addresses the diversity issue from a distinct perspective. Unlike prior techniques that adapt intermediate latents, our approach shapes the initial noise to promote diverse outputs. Specifically, we develop a contrastive loss defined in the Tweedie data space and optimize a batch of noise latents. Our contrastive optimization repels instances within the batch to maximize diversity while keeping them anchored to a reference sample to preserve fidelity. We further provide theoretical insights into the mechanism of this preprocessing to substantiate its effectiveness. Extensive experiments across multiple T2I backbones demonstrate that our approach achieves a superior quality-diversity Pareto frontier while remaining robust to hyperparameter choices.",
        "subjects": "Graphics, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.506672"
    },
    {
        "index": "#280",
        "title": "ReTiDe: Real-Time Denoising for Energy-Efficient Motion Picture Processing with FPGAs",
        "link": "/arxiv/2510.03812",
        "arxiv_id": "2510.03812",
        "authors": "Changhong Li, Clément Bled, Rosa Fernandez, Shreejith Shanker",
        "summary": "Denoising is a core operation in modern video pipelines. In codecs, in-loop filters suppress sensor noise and quantisation artefacts to improve rate-distortion performance; in cinema post-production, denoisers are used for restoration, grain management, and plate clean-up. However, state-of-the-art deep denoisers are computationally intensive and, at scale, are typically deployed on GPUs, incurring high power and cost for real-time, high-resolution streams. This paper presents Real-Time Denoise (ReTiDe), a hardware-accelerated denoising system that serves inference on data-centre Field Programmable Gate Arrays (FPGAs). A compact convolutional model is quantised (post-training quantisation plus quantisation-aware fine-tuning) to INT8 and compiled for AMD Deep Learning Processor Unit (DPU)-based FPGAs. A client-server integration offloads computation from the host CPU/GPU to a networked FPGA service, while remaining callable from existing workflows, e.g., NUKE, without disrupting artist tooling. On representative benchmarks, ReTiDe delivers 37.71$\\times$ Giga Operations Per Second (GOPS) throughput and 5.29$\\times$ higher energy efficiency than prior FPGA denoising accelerators, with negligible degradation in Peak Signal-to-Noise Ratio (PSNR)/Structural Similarity Index (SSIM). These results indicate that specialised accelerators can provide practical, scalable denoising for both encoding pipelines and post-production, reducing energy per frame without sacrificing quality or workflow compatibility. Code is available at https://github.com/RCSL-TCD/ReTiDe.",
        "subjects": "Image and Video Processing, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.507320"
    },
    {
        "index": "#281",
        "title": "6G-Enabled Digital Twin Framework for Real-Time Cyber-Physical Systems: An Experimental Validation with Industrial Bearing Fault Detection",
        "link": "/arxiv/2510.03807",
        "arxiv_id": "2510.03807",
        "authors": "Vaskar Chakma, Wooyeol Choi",
        "summary": "Current Cyber-Physical Systems (CPS) integrated with Digital Twin (DT) technology face critical limitations in achieving real-time performance for mission-critical industrial applications. Existing 5G-enabled systems suffer from latencies exceeding 10ms, which are inadequate for applications requiring sub-millisecond response times, such as autonomous industrial control and predictive maintenance. This research aims to develop and validate a 6G-enabled Digital Twin framework that achieves ultra-low latency communication and real-time synchronization between physical industrial assets and their digital counterparts, specifically targeting bearing fault detection as a critical industrial use case. The proposed framework integrates terahertz communications (0.1-1 THz), intelligent reflecting surfaces, and edge artificial intelligence within a five-layer architecture. Experimental validation was conducted using the Case Western Reserve University (CWRU) bearing dataset, implementing comprehensive feature extraction (15 time and frequency domain features) and Random Forest classification algorithms. The system performance was evaluated against traditional WiFi-6 and 5G networks across multiple metrics, including classification accuracy, end-to-end latency, and scalability. It achieved 97.7% fault classification accuracy with 0.8ms end-to-end latency, representing a 15.6x improvement over WiFi-6 (12.5ms) and 5.25x improvement over 5G (4.2ms) networks. The system demonstrated superior scalability with sub-linear processing time growth and maintained consistent performance across four bearing fault categories (normal, inner race, outer race, and ball faults) with macro-averaged F1-scores exceeding 97%.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.513063"
    },
    {
        "index": "#284",
        "title": "Lightweight and Data-Efficient MultivariateTime Series Forecasting using Residual-Stacked Gaussian (RS-GLinear) Architecture",
        "link": "/arxiv/2510.03788",
        "arxiv_id": "2510.03788",
        "authors": "Abukar Ali",
        "summary": "Following the success of Transformer architectures in language modeling, particularly their ability to capture long-range dependencies, researchers have explored how these architectures can be adapted for time-series forecasting. Transformer-based models have been proposed to handle both short- and long-term dependencies when predicting future values from historical data. However, studies such as those by Zeng et al. (2022) and Rizvi et al. (2025) have reported mixed results in long-term forecasting tasks. In this work, we evaluate the Gaussian-based Linear architecture introduced by Rizvi et al. (2025) and present an enhanced version called the Residual Stacked Gaussian Linear (RSGL) model. We also investigate the broader applicability of the RSGL model in additional domains, including financial time series and epidemiological data. Experimental results show that the RSGL model achieves improved prediction accuracy and robustness compared to both the baseline Gaussian Linear and Transformer-based models.",
        "subjects": "Computational Engineering, Finance, and Science, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.514977"
    },
    {
        "index": "#286",
        "title": "Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization",
        "link": "/arxiv/2510.03763",
        "arxiv_id": "2510.03763",
        "authors": "Jiaxin Deng, Junbiao Pang",
        "summary": "Sharpness-Aware Minimization (SAM) improves model generalization but doubles the computational cost of Stochastic Gradient Descent (SGD) by requiring twice the gradient calculations per optimization step. To mitigate this, we propose Adaptively sampling-Reusing-mixing decomposed gradients to significantly accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can be decomposed into the SGD gradient and the Projection of the Second-order gradient onto the First-order gradient (PSF). Furthermore, we observe that the SGD gradient and PSF dynamically evolve during training, emphasizing the growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed to the reused PSF and the timely updated PSF still maintain the model's generalization ability. Extensive experiments show that ARSAM achieves state-of-the-art accuracies comparable to SAM across diverse network architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a speedup of about 40\\%. Moreover, ARSAM accelerates optimization for the various challenge tasks (\\textit{e.g.}, human pose estimation, and model quantization) without sacrificing performance, demonstrating its broad practicality.% The code is publicly accessible at: https://github.com/ajiaaa/ARSAM.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.516254"
    },
    {
        "index": "#287",
        "title": "You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models",
        "link": "/arxiv/2510.03761",
        "arxiv_id": "2510.03761",
        "authors": "Richard A. Dubniczky, Bertalan Borsos, Tihanyi Norbert",
        "summary": "The widespread use of preprint repositories such as arXiv has accelerated the communication of scientific results but also introduced overlooked security risks. Beyond PDFs, these platforms provide unrestricted access to original source materials, including LaTeX sources, auxiliary code, figures, and embedded comments. In the absence of sanitization, submissions may disclose sensitive information that adversaries can harvest using open-source intelligence. In this work, we present the first large-scale security audit of preprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv submissions. We introduce LaTeXpOsEd, a four-stage framework that integrates pattern matching, logical filtering, traditional harvesting techniques, and large language models (LLMs) to uncover hidden disclosures within non-referenced files and LaTeX comments. To evaluate LLMs' secret-detection capabilities, we introduce LLMSec-DB, a benchmark on which we tested 25 state-of-the-art models. Our analysis uncovered thousands of PII leaks, GPS-tagged EXIF files, publicly available Google Drive and Dropbox folders, editable private SharePoint links, exposed GitHub and Google credentials, and cloud API keys. We also uncovered confidential author communications, internal disagreements, and conference submission credentials, exposing information that poses serious reputational risks to both researchers and institutions. We urge the research community and repository operators to take immediate action to close these hidden security gaps. To support open science, we release all scripts and methods from this study but withhold sensitive findings that could be misused, in line with ethical principles. The source code and related material are available at the project website https://github.com/LaTeXpOsEd",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.516882"
    },
    {
        "index": "#288",
        "title": "EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models",
        "link": "/arxiv/2510.03760",
        "arxiv_id": "2510.03760",
        "authors": "Ping Guo, Chenyu Zhu, Siyuan Chen, Fei Liu, Xi Lin, Zhichao Lu, Qingfu Zhang",
        "summary": "CUDA kernel optimization has become a critical bottleneck for AI performance, as deep learning training and inference efficiency directly depends on highly optimized GPU kernels. Despite the promise of Large Language Models (LLMs) for automating kernel optimization, this field suffers from a fragmented ecosystem of isolated and incomparable approaches with unclear problem formulations. Furthermore, general-purpose LLM code evolution methods cannot meet strict correctness requirements of CUDA kernel optimization. We address these fundamental challenges by first formalizing CUDA kernel optimization as a code optimization task with a clear objective, constraints, and evaluation metrics. We then establish the first systematic LLM-based code evolution framework, EvoEngineer, that provides guidance for designing and adapting optimization strategies to achieve a balance between performance and correctness. Finally, we implement a kernel optimization system based on this framework and conduct extensive experiments on 91 real-world CUDA kernels. Our results demonstrate that EvoEngineer achieves a principled balance between performance and correctness, with the highest averaged median speedup of \\textbf{2.72}$\\times$ over baseline CUDA kernels and a code validity rate of \\textbf{69.8}\\%, outperforming existing methods on both dimensions. Our method achieves a maximum speedup of \\textbf{36.75}$\\times$ among all operations over PyTorch kernels and delivers the highest speedup on \\textbf{28} (\\textbf{56.0\\%}) of 50 operations that achieve over \\textbf{2$\\times$} acceleration.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.517577"
    },
    {
        "index": "#289",
        "title": "Code4MeV2: a Research-oriented Code-completion Platform",
        "link": "/arxiv/2510.03755",
        "arxiv_id": "2510.03755",
        "authors": "Roham Koohestani, Parham Bateni, Aydin Ebrahimi, Behdad Etezadi, Kiarash Karimi, Maliheh Izadi",
        "summary": "The adoption of AI-powered code completion tools in software development has increased substantially, yet the user interaction data produced by these systems remain proprietary within large corporations. This creates a barrier for the academic community, as researchers must often develop dedicated platforms to conduct studies on human--AI interaction, making reproducible research and large-scale data analysis impractical. In this work, we introduce Code4MeV2, a research-oriented, open-source code completion plugin for JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a client--server architecture and features inline code completion and a context-aware chat assistant. Its core contribution is a modular and transparent data collection framework that gives researchers fine-grained control over telemetry and context gathering. Code4MeV2 achieves industry-comparable performance in terms of code completion, with an average latency of 200~ms. We assess our tool through a combination of an expert evaluation and a user study with eight participants. Feedback from both researchers and daily users highlights its informativeness and usefulness. We invite the community to adopt and contribute to this tool. More information about the tool can be found at https://app.code4me.me.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.523377"
    },
    {
        "index": "#291",
        "title": "HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting",
        "link": "/arxiv/2510.03744",
        "arxiv_id": "2510.03744",
        "authors": "Qianfei Fan, Jiayu Wei, Peijun Zhu, Wensheng Ye, Meie Fang",
        "summary": "Accurate decade-scale daily runoff forecasting in small watersheds is difficult because signals blend drifting trends, multi-scale seasonal cycles, regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet, PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single facets and under-utilize unlabeled spans, limiting regime adaptivity. We propose HydroFusion-LMF, a unified framework that (i) performs a learnable trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes residuals through a compact heterogeneous expert set (linear refinement, frequency kernel, patch Transformer, recurrent memory, dynamically normalized attention), (iii) fuses expert outputs via a hydrologic context-aware gate conditioned on day-of-year phase, antecedent precipitation, local variance, flood indicators, and static basin attributes, and (iv) augments supervision with a semi-supervised multi-task objective (composite MSE/MAE + extreme emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment, augmentation consistency, variance-filtered pseudo-labeling). Optional adapter / LoRA layers inject a frozen foundation time-series encoder efficiently. On a ~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818, improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions relative to baselines. The framework balances interpretability (explicit components, sparse gating) with performance, advancing label-efficient hydrologic forecasting under non-stationarity.",
        "subjects": "Machine Learning, Artificial Intelligence, Distributed, Parallel, and Cluster Computing, Neural and Evolutionary Computing, Geophysics",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.524814"
    },
    {
        "index": "#292",
        "title": "Cost Efficient Fairness Audit Under Partial Feedback",
        "link": "/arxiv/2510.03734",
        "arxiv_id": "2510.03734",
        "authors": "Nirjhar Das, Mohit Sharma, Praharsh Nanavati, Kirankumar Shiragur, Amit Deshpande",
        "summary": "We study the problem of auditing the fairness of a given classifier under partial feedback, where true labels are available only for positively classified individuals, (e.g., loan repayment outcomes are observed only for approved applicants). We introduce a novel cost model for acquiring additional labeled data, designed to more accurately reflect real-world costs such as credit assessment, loan processing, and potential defaults. Our goal is to find optimal fairness audit algorithms that are more cost-effective than random exploration and natural baselines. In our work, we consider two audit settings: a black-box model with no assumptions on the data distribution, and a mixture model, where features and true labels follow a mixture of exponential family distributions. In the black-box setting, we propose a near-optimal auditing algorithm under mild assumptions and show that a natural baseline can be strictly suboptimal. In the mixture model setting, we design a novel algorithm that achieves significantly lower audit cost than the black-box case. Our approach leverages prior work on learning from truncated samples and maximum-a-posteriori oracles, and extends known results on spherical Gaussian mixtures to handle exponential family mixtures, which may be of independent interest. Moreover, our algorithms apply to popular fairness metrics including demographic parity, equal opportunity, and equalized odds. Empirically, we demonstrate strong performance of our algorithms on real-world fair classification datasets like Adult Income and Law School, consistently outperforming natural baselines by around 50% in terms of audit cost.",
        "subjects": "Machine Learning, Artificial Intelligence, Computers and Society, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.525549"
    },
    {
        "index": "#293",
        "title": "Artery-Vein Segmentation from Fundus Images using Deep Learning",
        "link": "/arxiv/2510.03717",
        "arxiv_id": "2510.03717",
        "authors": "Sharan SK, Subin Sahayam, Umarani Jayaraman, Lakshmi Priya A",
        "summary": "Segmenting of clinically important retinal blood vessels into arteries and veins is a prerequisite for retinal vessel analysis. Such analysis can provide potential insights and bio-markers for identifying and diagnosing various retinal eye diseases. Alteration in the regularity and width of the retinal blood vessels can act as an indicator of the health of the vasculature system all over the body. It can help identify patients at high risk of developing vasculature diseases like stroke and myocardial infarction. Over the years, various Deep Learning architectures have been proposed to perform retinal vessel segmentation. Recently, attention mechanisms have been increasingly used in image segmentation tasks. The work proposes a new Deep Learning approach for artery-vein segmentation. The new approach is based on the Attention mechanism that is incorporated into the WNet Deep Learning model, and we call the model as Attention-WNet. The proposed approach has been tested on publicly available datasets such as HRF and DRIVE datasets. The proposed approach has outperformed other state-of-art models available in the literature.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.529645"
    },
    {
        "index": "#294",
        "title": "EmbodiSwap for Zero-Shot Robot Imitation Learning",
        "link": "/arxiv/2510.03706",
        "arxiv_id": "2510.03706",
        "authors": "Eadom Dessalene, Pavan Mantripragada, Michael Maynord, Yiannis Aloimonos",
        "summary": "We introduce EmbodiSwap - a method for producing photorealistic synthetic robot overlays over human video. We employ EmbodiSwap for zero-shot imitation learning, bridging the embodiment gap between in-the-wild ego-centric human video and a target robot embodiment. We train a closed-loop robot manipulation policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a visual backbone, repurposing V-JEPA from the domain of video understanding to imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms alternative vision backbones more conventionally used within robotics. In real-world tests, our zero-shot trained V-JEPA model achieves an $82\\%$ success rate, outperforming a few-shot trained $\\pi_0$ network as well as $\\pi_0$ trained over data produced by EmbodiSwap. We release (i) code for generating the synthetic robot overlays which takes as input human videos and an arbitrary robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference code, to facilitate reproducible research and broader adoption.",
        "subjects": "Robotics, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.530343"
    },
    {
        "index": "#295",
        "title": "Referring Expression Comprehension for Small Objects",
        "link": "/arxiv/2510.03701",
        "arxiv_id": "2510.03701",
        "authors": "Kanoko Goto, Takumi Hirose, Mahiro Ukai, Shuhei Kurita, Nakamasa Inoue",
        "summary": "Referring expression comprehension (REC) aims to localize the target object described by a natural language expression. Recent advances in vision-language learning have led to significant performance improvements in REC tasks. However, localizing extremely small objects remains a considerable challenge despite its importance in real-world applications such as autonomous driving. To address this issue, we introduce a novel dataset and method for REC targeting small objects. First, we present the small object REC (SOREC) dataset, which consists of 100,000 pairs of referring expressions and corresponding bounding boxes for small objects in driving scenarios. Second, we propose the progressive-iterative zooming adapter (PIZA), an adapter module for parameter-efficient fine-tuning that enables models to progressively zoom in and localize small objects. In a series of experiments, we apply PIZA to GroundingDINO and demonstrate a significant improvement in accuracy on the SOREC dataset. Our dataset, codes and pre-trained models are publicly available on the project page.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.531003"
    },
    {
        "index": "#296",
        "title": "Dissecting Larval Zebrafish Hunting using Deep Reinforcement Learning Trained RNN Agents",
        "link": "/arxiv/2510.03699",
        "arxiv_id": "2510.03699",
        "authors": "Raaghav Malik, Satpreet H. Singh, Sonja Johnson-Yu, Nathan Wu, Roy Harpaz, Florian Engert, Kanaka Rajan",
        "summary": "Larval zebrafish hunting provides a tractable setting to study how ecological and energetic constraints shape adaptive behavior in both biological brains and artificial agents. Here we develop a minimal agent-based model, training recurrent policies with deep reinforcement learning in a bout-based zebrafish simulator. Despite its simplicity, the model reproduces hallmark hunting behaviors -- including eye vergence-linked pursuit, speed modulation, and stereotyped approach trajectories -- that closely match real larval zebrafish. Quantitative trajectory analyses show that pursuit bouts systematically reduce prey angle by roughly half before strike, consistent with measurements. Virtual experiments and parameter sweeps vary ecological and energetic constraints, bout kinematics (coupled vs. uncoupled turns and forward motion), and environmental factors such as food density, food speed, and vergence limits. These manipulations reveal how constraints and environments shape pursuit dynamics, strike success, and abort rates, yielding falsifiable predictions for neuroscience experiments. These sweeps identify a compact set of constraints -- binocular sensing, the coupling of forward speed and turning in bout kinematics, and modest energetic costs on locomotion and vergence -- that are sufficient for zebrafish-like hunting to emerge. Strikingly, these behaviors arise in minimal agents without detailed biomechanics, fluid dynamics, circuit realism, or imitation learning from real zebrafish data. Taken together, this work provides a normative account of zebrafish hunting as the optimal balance between energetic cost and sensory benefit, highlighting the trade-offs that structure vergence and trajectory dynamics. We establish a virtual lab that narrows the experimental search space and generates falsifiable predictions about behavior and neural coding.",
        "subjects": "Neurons and Cognition, Artificial Intelligence, Machine Learning, Neural and Evolutionary Computing, Systems and Control",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.531794"
    },
    {
        "index": "#297",
        "title": "REG: A Regularization Optimizer for Robust Training Dynamics",
        "link": "/arxiv/2510.03691",
        "arxiv_id": "2510.03691",
        "authors": "Zehua Liu, Han Wu, Xiaojin Fu, Shuqi Liu, Xiongwei Han, Tao Zhong, Mingxuan Yuan",
        "summary": "Optimizers are crucial for the efficient training of Large Language Models (LLMs). While AdamW is the de facto standard, recent structure-aware optimizers like Muon have emerged, which regularize gradient updates by operating on entire weight matrices. The Muon optimizer balances the gradient updates along all the directions. However, Muon's reliance on the matrix sign function can lead to training instability, exhibits incompatibility when fine-tuning models pre-trained with AdamW. To address these limitations, we propose \\textbf{REG}, a novel optimizer that replaces Muon's aggressive matrix sign operator with the Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a matrix, the RACS operator regularizes the update steps in a less drastic manner, making it simpler to implement and more compatible with established training dynamics. Through extensive empirical experiments on LLM training, we demonstrate that our REG optimizer not only achieves superior performance and stability over AdamW, but also maintains consistency with the AdamW training paradigm. This consistency is particularly evident during the fine-tuning stage, where REG optimizer avoids the performance degradation observed with Muon.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.532488"
    },
    {
        "index": "#299",
        "title": "MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations",
        "link": "/arxiv/2510.03666",
        "arxiv_id": "2510.03666",
        "authors": "Jiang Wu, Sichao Wu, Yinsong Ma, Guangyuan Yu, Haoyuan Xu, Lifang Zheng, Jingliang Duan",
        "summary": "Industrial accidents, particularly in high-risk domains such as surface and underground mining, are frequently caused by unsafe worker behaviors. Traditional manual inspection remains labor-intensive, error-prone, and insufficient for large-scale, dynamic environments, highlighting the urgent need for intelligent and automated safety monitoring. In this paper, we present MonitorVLM, a novel vision--language framework designed to detect safety violations directly from surveillance video streams. MonitorVLM introduces three key innovations: (1) a domain-specific violation dataset comprising 9,000 vision--question--answer (VQA) samples across 40 high-frequency mining regulations, enriched with augmentation and auxiliary detection cues; (2) a clause filter (CF) module that dynamically selects the Top-$K$ most relevant clauses, reducing inference latency by 13.56\\% while maintaining accuracy; and (3) a behavior magnifier (BM) module that enhances worker regions to improve fine-grained action recognition, yielding additional gains of 3.45% in precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM significantly outperforms baseline vision--language models, achieving improvements of 22.01% in precision, 34.22\\% in recall, and 28.37% in F1 score over the 72B unfine-tuned baseline. A lightweight web-based interface further integrates MonitorVLM into practical workflows, enabling automatic violation reporting with video timestamping. This study highlights the potential of multimodal large models to enhance occupational safety monitoring in mining and beyond.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.533877"
    },
    {
        "index": "#300",
        "title": "Operationalizing Data Minimization for Privacy-Preserving LLM Prompting",
        "link": "/arxiv/2510.03662",
        "arxiv_id": "2510.03662",
        "authors": "Jijie Zhou, Niloofar Mireshghallah, Tianshi Li",
        "summary": "The rapid deployment of large language models (LLMs) in consumer applications has led to frequent exchanges of personal information. To obtain useful responses, users often share more than necessary, increasing privacy risks via memorization, context-based personalization, or security breaches. We present a framework to formally define and operationalize data minimization: for a given user prompt and response model, quantifying the least privacy-revealing disclosure that maintains utility, and we propose a priority-queue tree search to locate this optimal point within a privacy-ordered transformation space. We evaluated the framework on four datasets spanning open-ended conversations (ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth answers (CaseHold, MedQA), quantifying achievable data minimization with nine LLMs as the response model. Our results demonstrate that larger frontier LLMs can tolerate stronger data minimization while maintaining task quality than smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that LLMs struggle to predict optimal data minimization directly, showing a bias toward abstraction that leads to oversharing. This suggests not just a privacy gap, but a capability gap: models may lack awareness of what information they actually need to solve a task.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.534516"
    },
    {
        "index": "#302",
        "title": "LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design",
        "link": "/arxiv/2510.03650",
        "arxiv_id": "2510.03650",
        "authors": "Amir Sadikov",
        "summary": "Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo (QMC) methods for high-dimensional integration. We cast two long-standing QMC design problems as program synthesis and solve them with an LLM-guided evolutionary loop that mutates and selects code under task-specific fitness: (i) constructing finite 2D/3D point sets with low star discrepancy, and (ii) choosing Sobol' direction numbers that minimize randomized QMC error on downstream integrands. Our two-phase procedure combines constructive code proposals with iterative numerical refinement. On finite sets, we rediscover known optima in small 2D cases and set new best-known 2D benchmarks for N >= 40, while matching most known 3D optima up to the proven frontier (N <= 8) and reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol' parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC) mean-squared error for several 32-dimensional option-pricing tasks relative to widely used Joe--Kuo parameters, while preserving extensibility to any sample size and compatibility with standard randomizations. Taken together, the results demonstrate that LLM-driven evolutionary program synthesis can automate the discovery of high-quality QMC constructions, recovering classical designs where they are optimal and improving them where finite-N structure matters. Data and code are available at https://github.com/hockeyguy123/openevolve-star-discrepancy.git.",
        "subjects": "Machine Learning, Artificial Intelligence, Computational Engineering, Finance, and Science, Neural and Evolutionary Computing, Numerical Analysis",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.535891"
    },
    {
        "index": "#304",
        "title": "Implicit Models: Expressive Power Scales with Test-Time Compute",
        "link": "/arxiv/2510.03638",
        "arxiv_id": "2510.03638",
        "authors": "Jialin Liu, Lisang Ding, Stanley Osher, Wotao Yin",
        "summary": "Implicit models, an emerging model class, compute outputs by iterating a single parameter block to a fixed point. This architecture realizes an infinite-depth, weight-tied network that trains with constant memory, significantly reducing memory needs for the same level of performance compared to explicit models. While it is empirically known that these compact models can often match or even exceed larger explicit networks by allocating more test-time compute, the underlying mechanism remains poorly understood. We study this gap through a nonparametric analysis of expressive power. We provide a strict mathematical characterization, showing that a simple and regular implicit operator can, through iteration, progressively express more complex mappings. We prove that for a broad class of implicit models, this process lets the model's expressive power scale with test-time compute, ultimately matching a much richer function class. The theory is validated across three domains: image reconstruction, scientific computing, and operations research, demonstrating that as test-time iterations increase, the complexity of the learned mapping rises, while the solution quality simultaneously improves and stabilizes.",
        "subjects": "Machine Learning, Artificial Intelligence, Representation Theory, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.537412"
    },
    {
        "index": "#305",
        "title": "Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis",
        "link": "/arxiv/2510.03633",
        "arxiv_id": "2510.03633",
        "authors": "An Vuong, Susan Gauch",
        "summary": "Accurately predicting short-term stock price movement remains a challenging task due to the market's inherent volatility and sensitivity to investor sentiment. This paper discusses a deep learning framework that integrates emotion features extracted from tweet data with historical stock price information to forecast significant price changes on the following day. We utilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby enhancing the quality of emotion features derived from three emotion analysis approaches: a transformer-based DistilRoBERTa classifier from the Hugging Face library and two lexicon-based methods using National Research Council Canada (NRC) resources. These features are combined with previous-day stock price data to train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA, AAPL, and AMZN stocks show that all three emotion analysis methods improve the average accuracy for predicting significant price movements, compared to the baseline model using only historical stock prices, which yields an accuracy of 13.5%. The DistilRoBERTa-based stock prediction model achieves the best performance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced emotion analysis. These results demonstrate that using large language models to preprocess tweet content enhances the effectiveness of emotion analysis which in turn improves the accuracy of predicting significant stock price movements.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.538049"
    },
    {
        "index": "#306",
        "title": "Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications",
        "link": "/arxiv/2510.03623",
        "arxiv_id": "2510.03623",
        "authors": "Maraz Mia, Mir Mehedi A. Pritom",
        "summary": "Explainable Artificial Intelligence (XAI) has aided machine learning (ML) researchers with the power of scrutinizing the decisions of the black-box models. XAI methods enable looking deep inside the models' behavior, eventually generating explanations along with a perceived trust and transparency. However, depending on any specific XAI method, the level of trust can vary. It is evident that XAI methods can themselves be a victim of post-adversarial attacks that manipulate the expected outcome from the explanation module. Among such attack tactics, fairwashing explanation (FE), manipulation explanation (ME), and backdoor-enabled manipulation attacks (BD) are the notable ones. In this paper, we try to understand these adversarial attack techniques, tactics, and procedures (TTPs) on explanation alteration and thus the effect on the model's decisions. We have explored a total of six different individual attack procedures on post-hoc explanation methods such as SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG (Integrated Gradients), and investigated those adversarial attacks in cybersecurity applications scenarios such as phishing, malware, intrusion, and fraudulent website detection. Our experimental study reveals the actual effectiveness of these attacks, thus providing an urgency for immediate attention to enhance the resiliency of XAI methods and their applications.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.538719"
    },
    {
        "index": "#307",
        "title": "Neural Bayesian Filtering",
        "link": "/arxiv/2510.03614",
        "arxiv_id": "2510.03614",
        "authors": "Christopher Solinas, Radovan Haluska, David Sychrovsky, Finbarr Timbers, Nolan Bard, Michael Buro, Martin Schmid, Nathan R. Sturtevant, Michael Bowling",
        "summary": "We present Neural Bayesian Filtering (NBF), an algorithm for maintaining distributions over hidden states, called beliefs, in partially observable systems. NBF is trained to find a good latent representation of the beliefs induced by a task. It maps beliefs to fixed-length embedding vectors, which condition generative models for sampling. During filtering, particle-style updates compute posteriors in this embedding space using incoming observations and the environment's dynamics. NBF combines the computational efficiency of classical filters with the expressiveness of deep generative models - tracking rapidly shifting, multimodal beliefs while mitigating the risk of particle impoverishment. We validate NBF in state estimation tasks in three partially observable environments.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.544762"
    },
    {
        "index": "#309",
        "title": "PentestMCP: A Toolkit for Agentic Penetration Testing",
        "link": "/arxiv/2510.03610",
        "arxiv_id": "2510.03610",
        "authors": "Zachary Ezetta, Wu-chang Feng",
        "summary": "Agentic AI is transforming security by automating many tasks being performed manually. While initial agentic approaches employed a monolithic architecture, the Model-Context-Protocol has now enabled a remote-procedure call (RPC) paradigm to agentic applications, allowing for the flexible construction and composition of multi-function agents. This paper describes PentestMCP, a library of MCP server implementations that support agentic penetration testing. By supporting common penetration testing tasks such as network scanning, resource enumeration, service fingerprinting, vulnerability scanning, exploitation, and post-exploitation, PentestMCP allows a developer to customize multi-agent workflows for performing penetration tests.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.545560"
    },
    {
        "index": "#310",
        "title": "Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends",
        "link": "/arxiv/2510.03604",
        "arxiv_id": "2510.03604",
        "authors": "Yucheng Wang, Mohamed Ragab, Yubo Hou, Zhenghua Chen, Min Wu, Xiaoli Li",
        "summary": "Remaining Useful Life (RUL) prediction for turbofan engines plays a vital role in predictive maintenance, ensuring operational safety and efficiency in aviation. Although data-driven approaches using machine learning and deep learning have shown potential, they face challenges such as limited data and distribution shifts caused by varying operating conditions. Domain Adaptation (DA) has emerged as a promising solution, enabling knowledge transfer from source domains with abundant data to target domains with scarce data while mitigating distributional shifts. Given the unique properties of turbofan engines, such as complex operating conditions, high-dimensional sensor data, and slower-changing signals, it is essential to conduct a focused review of DA techniques specifically tailored to turbofan engines. To address this need, this paper provides a comprehensive review of DA solutions for turbofan engine RUL prediction, analyzing key methodologies, challenges, and recent advancements. A novel taxonomy tailored to turbofan engines is introduced, organizing approaches into methodology-based (how DA is applied), alignment-based (where distributional shifts occur due to operational variations), and problem-based (why certain adaptations are needed to address specific challenges). This taxonomy offers a multidimensional view that goes beyond traditional classifications by accounting for the distinctive characteristics of turbofan engine data and the standard process of applying DA techniques to this area. Additionally, we evaluate selected DA techniques on turbofan engine datasets, providing practical insights for practitioners and identifying key challenges. Future research directions are identified to guide the development of more effective DA techniques, advancing the state of RUL prediction for turbofan engines.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.545967"
    },
    {
        "index": "#311",
        "title": "Neon: Negative Extrapolation From Self-Training Improves Image Generation",
        "link": "/arxiv/2510.03597",
        "arxiv_id": "2510.03597",
        "authors": "Sina Alemohammad, Zhangyang Wang, Richard G. Baraniuk",
        "summary": "Scaling generative AI models is bottlenecked by the scarcity of high-quality training data. The ease of synthesizing from a generative model suggests using (unverified) synthetic data to augment a limited corpus of real data for the purpose of fine-tuning in the hope of improving performance. Unfortunately, however, the resulting positive feedback loop leads to model autophagy disorder (MAD, aka model collapse) that results in a rapid degradation in sample quality and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation frOm self-traiNing), a new learning method that turns the degradation from self-training into a powerful signal for self-improvement. Given a base model, Neon first fine-tunes it on its own self-synthesized data but then, counterintuitively, reverses its gradient updates to extrapolate away from the degraded weights. We prove that Neon works because typical inference samplers that favor high-probability regions create a predictable anti-alignment between the synthetic and real data population gradients, which negative extrapolation corrects to better align the model with the true data distribution. Neon is remarkably easy to implement via a simple post-hoc merge that requires no new real data, works effectively with as few as 1k synthetic samples, and typically uses less than 1% additional training compute. We demonstrate Neon's universality across a range of architectures (diffusion, flow matching, autoregressive, and inductive moment matching models) and datasets (ImageNet, CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional training compute. Code is available at https://github.com/SinaAlemohammad/Neon",
        "subjects": "Graphics, Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.546348"
    },
    {
        "index": "#313",
        "title": "A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games",
        "link": "/arxiv/2510.03591",
        "arxiv_id": "2510.03591",
        "authors": "Faliu Yi, Sherif Abdelfattah, Wei Huang, Adrian Brown",
        "summary": "Manual identification of visual bugs in video games is a resource-intensive and costly process, often demanding specialized domain knowledge. While supervised visual bug detection models offer a promising solution, their reliance on extensive labeled datasets presents a significant challenge due to the infrequent occurrence of such bugs. To overcome this limitation, we propose a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled and unlabeled data. Our approach leverages labeled samples from the target game and diverse co-domain games, additionally incorporating unlabeled data to enhance feature representation learning. This strategy maximizes the utility of all available data, substantially reducing the dependency on labeled examples from the specific target game. The developed framework demonstrates enhanced scalability and adaptability, facilitating efficient visual bug detection across various game titles. Our experimental results show the robustness of the proposed method for game visual bug detection, exhibiting superior performance compared to conventional baselines across multiple gaming environments. Furthermore, CFT maintains competitive performance even when trained with only 50% of the labeled data from the target game.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.547110"
    },
    {
        "index": "#314",
        "title": "Deep learning the sources of MJO predictability: a spectral view of learned features",
        "link": "/arxiv/2510.03582",
        "arxiv_id": "2510.03582",
        "authors": "Lin Yao, Da Yang, James P. C. Duncan, Ashesh Chattopadhyay, Pedram Hassanzadeh, Wahid Bhimji, Bin Yu",
        "summary": "The Madden-Julian oscillation (MJO) is a planetary-scale, intraseasonal tropical rainfall phenomenon crucial for global weather and climate; however, its dynamics and predictability remain poorly understood. Here, we leverage deep learning (DL) to investigate the sources of MJO predictability, motivated by a central difference in MJO theories: which spatial scales are essential for driving the MJO? We first develop a deep convolutional neural network (DCNN) to forecast the MJO indices (RMM and ROMI). Our model predicts RMM and ROMI up to 21 and 33 days, respectively, achieving skills comparable to leading subseasonal-to-seasonal models such as NCEP. To identify the spatial scales most relevant for MJO forecasting, we conduct spectral analysis of the latent feature space and find that large-scale patterns dominate the learned signals. Additional experiments show that models using only large-scale signals as the input have the same skills as those using all the scales, supporting the large-scale view of the MJO. Meanwhile, we find that small-scale signals remain informative: surprisingly, models using only small-scale input can still produce skillful forecasts up to 1-2 weeks ahead. We show that this is achieved by reconstructing the large-scale envelope of the small-scale activities, which aligns with the multi-scale view of the MJO. Altogether, our findings support that large-scale patterns--whether directly included or reconstructed--may be the primary source of MJO predictability.",
        "subjects": "Atmospheric and Oceanic Physics, Artificial Intelligence",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.547524"
    },
    {
        "index": "#315",
        "title": "Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning",
        "link": "/arxiv/2510.03578",
        "arxiv_id": "2510.03578",
        "authors": "Haoran Li, Chenhan Xiao, Muhao Guo, Yang Weng",
        "summary": "Learning dynamics is essential for model-based control and Reinforcement Learning in engineering systems, such as robotics and power systems. However, limited system measurements, such as those from low-resolution sensors, demand sample-efficient learning. Symmetry provides a powerful inductive bias by characterizing equivariant relations in system states to improve sample efficiency. While recent methods attempt to discover symmetries from data, they typically assume a single global symmetry group and treat symmetry discovery and dynamic learning as separate tasks, leading to limited expressiveness and error accumulation. In this paper, we propose the Latent Mixture of Symmetries (Latent MoS), an expressive model that captures a mixture of symmetry-governed latent factors from complex dynamical measurements. Latent MoS focuses on dynamic learning while locally and provably preserving the underlying symmetric transformations. To further capture long-term equivariance, we introduce a hierarchical architecture that stacks MoS blocks. Numerical experiments in diverse physical systems demonstrate that Latent MoS outperforms state-of-the-art baselines in interpolation and extrapolation tasks while offering interpretable latent representations suitable for future geometric and safety-critical analyses.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.547948"
    },
    {
        "index": "#316",
        "title": "Generalization of Graph Neural Network Models for Distribution Grid Fault Detection",
        "link": "/arxiv/2510.03571",
        "arxiv_id": "2510.03571",
        "authors": "Burak Karabulut, Carlo Manna, Chris Develder",
        "summary": "Fault detection in power distribution grids is critical for ensuring system reliability and preventing costly outages. Moreover, fault detection methodologies should remain robust to evolving grid topologies caused by factors such as reconfigurations, equipment failures, and Distributed Energy Resource (DER) integration. Current data-driven state-of-the-art methods use Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in short). Specifically, for power system fault diagnosis, Graph Convolutional Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures have been proposed and adopted in domains outside of power systems. In this paper, we set out to systematically and consistently benchmark various GNN architectures in an RNN+GNN pipeline model. Specifically, to the best of our knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention (GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring their generalization potential for deployment in different settings than those used for training them. Our experimental results on the IEEE 123-node distribution network show that RGATv2 has superior generalization capabilities, maintaining high performance with an F1-score reduction of $\\sim$12% across different topology settings. In contrast, pure RNN models largely fail, experiencing an F1-score reduction of up to $\\sim$60%, while other RGNN variants also exhibit significant performance degradation, i.e., up to $\\sim$25% lower F1-scores.",
        "subjects": "Machine Learning, Artificial Intelligence, Systems and Control",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.548369"
    },
    {
        "index": "#317",
        "title": "Evaluating OCR performance on food packaging labels in South Africa",
        "link": "/arxiv/2510.03570",
        "arxiv_id": "2510.03570",
        "authors": "Mayimunah Nagayi, Alice Khan, Tamryn Frank, Rina Swart, Clement Nyirenda",
        "summary": "This study evaluates four open-source Optical Character Recognition (OCR) systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food packaging images. The aim is to assess their ability to extract ingredient lists and nutrition facts panels. Accurate OCR for packaging is important for compliance and nutrition monitoring but is challenging due to multilingual text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231 products (1,628 images) was processed by all four models to assess speed and coverage, and a ground truth subset of 113 images (60 products) was created for accuracy evaluation. Metrics include Character Error Rate (CER), Word Error Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU (0.245). EasyOCR provided a good balance between accuracy and multilingual support. PaddleOCR achieved near complete coverage but was slower because it ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest results despite GPU acceleration. These results provide a packaging-specific benchmark, establish a baseline, and highlight directions for layout-aware methods and text localization.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.549044"
    },
    {
        "index": "#318",
        "title": "Longitudinal Flow Matching for Trajectory Modeling",
        "link": "/arxiv/2510.03569",
        "arxiv_id": "2510.03569",
        "authors": "Mohammad Mohaiminul Islam, Thijs P. Kuipers, Sharvaree Vadgama, Coen de Vente, Afsana Khan, Clara I. Sánchez, Erik J. Bekkers",
        "summary": "Generative models for sequential data often struggle with sparsely sampled and high-dimensional trajectories, typically reducing the learning of dynamics to pairwise transitions. We propose \\textit{Interpolative Multi-Marginal Flow Matching} (IMMFM), a framework that learns continuous stochastic dynamics jointly consistent with multiple observed time points. IMMFM employs a piecewise-quadratic interpolation path as a smooth target for flow matching and jointly optimizes drift and a data-driven diffusion coefficient, supported by a theoretical condition for stable learning. This design captures intrinsic stochasticity, handles irregular sparse sampling, and yields subject-specific trajectories. Experiments on synthetic benchmarks and real-world longitudinal neuroimaging datasets show that IMMFM outperforms existing methods in both forecasting accuracy and further downstream tasks.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.554939"
    },
    {
        "index": "#320",
        "title": "GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis",
        "link": "/arxiv/2510.03555",
        "arxiv_id": "2510.03555",
        "authors": "Peiran Quan, Zifan Gu, Zhuo Zhao, Qin Zhou, Donghan M. Yang, Ruichen Rong, Yang Xie, Guanghua Xiao",
        "summary": "Foundation models (FMs) have transformed computational pathology by providing powerful, general-purpose feature extractors. However, adapting and benchmarking individual FMs for specific diagnostic tasks is often time-consuming and resource-intensive, especially given their scale and diversity. To address this challenge, we introduce Group-Aggregative Selection Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that seamlessly integrates features from multiple FMs, preserving their complementary strengths without requiring manual feature selection or extensive task-specific fine-tuning. Across classification tasks in three cancer datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL consistently achieves superior or on-par performance relative to individual FMs and established MIL methods, demonstrating its robustness and generalizability. By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines model deployment for pathology and provides a scalable foundation for future multimodal and precision oncology applications.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.556336"
    },
    {
        "index": "#321",
        "title": "Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing",
        "link": "/arxiv/2510.03548",
        "arxiv_id": "2510.03548",
        "authors": "Danial Samadi Vahdati, Tai Duc Nguyen, Ekta Prashnani, Koki Nagano, David Luebke, Orazio Gallo, Matthew Stamm",
        "summary": "AI-based talking-head videoconferencing systems reduce bandwidth by sending a compact pose-expression latent and re-synthesizing RGB at the receiver, but this latent can be puppeteered, letting an attacker hijack a victim's likeness in real time. Because every frame is synthetic, deepfake and synthetic video detectors fail outright. To address this security problem, we exploit a key observation: the pose-expression latent inherently contains biometric information of the driving identity. Therefore, we introduce the first biometric leakage defense without ever looking at the reconstructed RGB video: a pose-conditioned, large-margin contrastive encoder that isolates persistent identity cues inside the transmitted latent while cancelling transient pose and expression. A simple cosine test on this disentangled embedding flags illicit identity swaps as the video is rendered. Our experiments on multiple talking-head generation models show that our method consistently outperforms existing puppeteering defenses, operates in real-time, and shows strong generalization to out-of-distribution scenarios.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.557048"
    },
    {
        "index": "#322",
        "title": "Agile Tradespace Exploration for Space Rendezvous Mission Design via Transformers",
        "link": "/arxiv/2510.03544",
        "arxiv_id": "2510.03544",
        "authors": "Yuji Takubo, Daniele Gammelli, Marco Pavone, Simone D'Amico",
        "summary": "Spacecraft rendezvous enables on-orbit servicing, debris removal, and crewed docking, forming the foundation for a scalable space economy. Designing such missions requires rapid exploration of the tradespace between control cost and flight time across multiple candidate targets. However, multi-objective optimization in this setting is challenging, as the underlying constraints are often highly nonconvex, and mission designers must balance accuracy (e.g., solving the full problem) with efficiency (e.g., convex relaxations), slowing iteration and limiting design agility. To address these challenges, this paper proposes an AI-powered framework that enables agile mission design for a wide range of Earth orbit rendezvous scenarios. Given the orbital information of the target spacecraft, boundary conditions, and a range of flight times, this work proposes a Transformer-based architecture that generates, in a single parallelized inference step, a set of near-Pareto optimal trajectories across varying flight times, thereby enabling rapid mission trade studies. The model is further extended to accommodate variable flight times and perturbed orbital dynamics, supporting realistic multi-objective trade-offs. Validation on chance-constrained rendezvous problems with passive safety constraints demonstrates that the model generalizes across both flight times and dynamics, consistently providing high-quality initial guesses that converge to superior solutions in fewer iterations. Moreover, the framework efficiently approximates the Pareto front, achieving runtimes comparable to convex relaxation by exploiting parallelized inference. Together, these results position the proposed framework as a practical surrogate for nonconvex trajectory generation and mark an important step toward AI-driven trajectory design for accelerating preliminary mission planning in real-world rendezvous applications.",
        "subjects": "Optimization and Control, Artificial Intelligence, Robotics",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.557732"
    },
    {
        "index": "#325",
        "title": "Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models",
        "link": "/arxiv/2510.03520",
        "arxiv_id": "2510.03520",
        "authors": "Kartik Pandit, Sourav Ganguly, Arnesh Banerjee, Shaahin Angizi, Arnob Ghosh",
        "summary": "Ensuring safety is a foundational requirement for large language models (LLMs). Achieving an appropriate balance between enhancing the utility of model outputs and mitigating their potential for harm is a complex and persistent challenge. Contemporary approaches frequently formalize this problem within the framework of Constrained Markov Decision Processes (CMDPs) and employ established CMDP optimization techniques. However, these methods exhibit two notable limitations. First, their reliance on reward and cost functions renders performance highly sensitive to the underlying scoring mechanism, which must capture semantic meaning rather than being triggered by superficial keywords. Second, CMDP-based training entails tuning dual-variable, a process that is both computationally expensive and does not provide any provable safety guarantee for a fixed dual variable that can be exploitable through adversarial jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF (CS-RLHF) that introduces a cost model trained on a large-scale corpus to assign semantically grounded safety scores. In contrast to the lagrangian-based approach, CS-RLHF adopts a rectified penalty-based formulation. This design draws on the theory of exact penalty functions in constrained optimization, wherein constraint satisfaction is enforced directly through a suitably chosen penalty term. With an appropriately scaled penalty, feasibility of the safety constraints can be guaranteed at the optimizer, eliminating the need for dual-variable updates. Empirical evaluation demonstrates that CS-RLHF outperforms state-of-the-art LLM model responses rendering at-least 5 times efficient against nominal and jail-breaking prompts",
        "subjects": "Machine Learning, Artificial Intelligence, Systems and Control",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.564984"
    },
    {
        "index": "#328",
        "title": "Platonic Transformers: A Solid Choice For Equivariance",
        "link": "/arxiv/2510.03511",
        "arxiv_id": "2510.03511",
        "authors": "Mohammad Mohaiminul Islam, Rishabh Anand, David R. Wessels, Friso de Kruiff, Thijs P. Kuipers, Rex Ying, Clara I. Sánchez, Sharvaree Vadgama, Georg Bökman, Erik J. Bekkers",
        "summary": "While widespread, Transformers lack inductive biases for geometric symmetries common in science and computer vision. Existing equivariant methods often sacrifice the efficiency and flexibility that make Transformers so effective through complex, computationally intensive designs. We introduce the Platonic Transformer to resolve this trade-off. By defining attention relative to reference frames from the Platonic solid symmetry groups, our method induces a principled weight-sharing scheme. This enables combined equivariance to continuous translations and Platonic symmetries, while preserving the exact architecture and computational cost of a standard Transformer. Furthermore, we show that this attention is formally equivalent to a dynamic group convolution, which reveals that the model learns adaptive geometric filters and enables a highly scalable, linear-time convolutional variant. Across diverse benchmarks in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular property prediction (QM9, OMol25), the Platonic Transformer achieves competitive performance by leveraging these geometric constraints at no additional cost.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning, Image and Video Processing",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.567175"
    },
    {
        "index": "#330",
        "title": "Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms",
        "link": "/arxiv/2510.03501",
        "arxiv_id": "2510.03501",
        "authors": "Lyes Saad Saoud, Loic Lesobre, Enrico Sorato, Irfan Hussain",
        "summary": "Real-time animal detection and segmentation in natural environments are vital for wildlife conservation, enabling non-invasive monitoring through remote camera streams. However, these tasks remain challenging due to limited computational resources and the cryptic appearance of many species. We propose a mobile-optimized two-stage deep learning framework that integrates a Threading Detection Model (TDM) to parallelize YOLOv10-based detection and MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach improves real-time performance by reducing latency through threading. YOLOv10 handles detection while MobileSAM performs lightweight segmentation, both executed concurrently for efficient resource use. On the cryptic Houbara Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627, mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10 operates at 43.7 ms per frame, confirming real-time readiness. We introduce a curated Houbara dataset of 40,000 annotated images to support model training and evaluation across diverse conditions. The code and dataset used in this study are publicly available on GitHub at https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos and additional resources, visit https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Robotics",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.568797"
    },
    {
        "index": "#331",
        "title": "AgentHub: A Research Agenda for Agent Sharing Infrastructure",
        "link": "/arxiv/2510.03495",
        "arxiv_id": "2510.03495",
        "authors": "Erik Pautsch, Tanmay Singla, Wenxin Jiang, Huiyun Peng, Behnaz Hassanshahi, Konstantin Läufer, George K. Thiruvathukal, James C. Davis",
        "summary": "LLM-based agents are rapidly proliferating, yet the infrastructure for discovering, evaluating, and governing them remains fragmented compared to mature ecosystems like software package registries (e.g., npm) and model hubs (e.g., Hugging Face). Recent research and engineering works have begun to consider the requisite infrastructure, but so far they focus narrowly -- on distribution, naming, or protocol negotiation. However, considering broader software engineering requirements would improve open-source distribution and ease reuse. We therefore propose AgentHub, a research agenda for agent sharing. By framing the key challenges of capability clarity, lifecycle transparency, interoperability, governance, security, and workflow integration, AgentHub charts a community-wide agenda for building reliable and scalable agent ecosystems. Our vision is a future where agents can be shared, trusted, and composed as seamlessly as today's software libraries.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.569556"
    },
    {
        "index": "#333",
        "title": "Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains",
        "link": "/arxiv/2510.03486",
        "arxiv_id": "2510.03486",
        "authors": "Anupam Panwar, Himadri Pal, Jiali Chen, Kyle Cho, Riddick Jiang, Miao Zhao, Rajiv Krishnamurthy",
        "summary": "Detecting anomalies in large, distributed systems presents several challenges. The first challenge arises from the sheer volume of data that needs to be processed. Flagging anomalies in a high-throughput environment calls for a careful consideration of both algorithm and system design. The second challenge comes from the heterogeneity of time-series datasets that leverage such a system in production. In practice, anomaly detection systems are rarely deployed for a single use case. Typically, there are several metrics to monitor, often across several domains (e.g. engineering, business and operations). A one-size-fits-all approach rarely works, so these systems need to be fine-tuned for every application - this is often done manually. The third challenge comes from the fact that determining the root-cause of anomalies in such settings is akin to finding a needle in a haystack. Identifying (in real time) a time-series dataset that is associated causally with the anomalous time-series data is a very difficult problem. In this paper, we describe a unified framework that addresses these challenges. Reasoning based Anomaly Detection Framework (RADF) is designed to perform real time anomaly detection on very large datasets. This framework employs a novel technique (mSelect) that automates the process of algorithm selection and hyper-parameter tuning for each use case. Finally, it incorporates a post-detection capability that allows for faster triaging and root-cause determination. Our extensive experiments demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly detection models in AUC performance for 5 out of 9 public benchmarking datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a distinction unmatched by any other state-of-the-art model.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.576103"
    },
    {
        "index": "#334",
        "title": "DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis",
        "link": "/arxiv/2510.03483",
        "arxiv_id": "2510.03483",
        "authors": "Numan Saeed, Tausifa Jan Saleem, Fadillah Maani, Muhammad Ridzuan, Hu Wang, Mohammad Yaqub",
        "summary": "Deep learning for medical imaging is hampered by task-specific models that lack generalizability and prognostic capabilities, while existing 'universal' approaches suffer from simplistic conditioning and poor medical semantic understanding. To address these limitations, we introduce DuPLUS, a deep learning framework for efficient multi-modal medical image analysis. DuPLUS introduces a novel vision-language framework that leverages hierarchical semantic prompts for fine-grained control over the analysis task, a capability absent in prior universal models. To enable extensibility to other medical tasks, it includes a hierarchical, text-controlled architecture driven by a unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize across three imaging modalities, ten different anatomically various medical datasets, encompassing more than 30 organs and tumor types. It outperforms the state-of-the-art task specific and universal models on 8 out of 10 datasets. We demonstrate extensibility of its text-controlled architecture by seamless integration of electronic health record (EHR) data for prognosis prediction, and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI) of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks and modalities from varying centers, establishing DuPLUS as a versatile and clinically relevant solution for medical image analysis. The code for this work is made available at: https://anonymous.4open.science/r/DuPLUS-6C52",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.576861"
    },
    {
        "index": "#336",
        "title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework",
        "link": "/arxiv/2510.03463",
        "arxiv_id": "2510.03463",
        "authors": "Vali Tawosi, Keshav Ramani, Salwa Alamir, Xiaomo Liu",
        "summary": "Multi-agent Large Language Model (LLM) systems have been leading the way in applied LLM research across a number of fields. One notable area is software development, where researchers have advanced the automation of code implementation, code testing, code maintenance, inter alia, using LLM agents. However, software development is a multifaceted environment that extends beyond just code. As such, a successful LLM system must factor in multiple stages of the software development life-cycle (SDLC). In this paper, we propose a vision for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework, which follows the above SDLC philosophy such that it may work within an agile software development team to perform several tasks end-to-end. ALMAS aligns its agents with agile roles, and can be used in a modular fashion to seamlessly integrate with human developers and their development environment. We showcase the progress towards ALMAS through our published works and a use case demonstrating the framework, where ALMAS is able to seamlessly generate an application and add a new feature.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.578234"
    },
    {
        "index": "#338",
        "title": "Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning",
        "link": "/arxiv/2510.03441",
        "arxiv_id": "2510.03441",
        "authors": "Chashi Mahiul Islam, Oteo Mamo, Samuel Jacob Chacko, Xiuwen Liu, Weikuan Yu",
        "summary": "Vision-language models (VLMs) have advanced multimodal reasoning but still face challenges in spatial reasoning for 3D scenes and complex object configurations. To address this, we introduce SpatialViLT, an enhanced VLM that integrates spatial features like depth maps, 3D coordinates, and edge maps through a multi-task learning framework. This approach enriches multimodal embeddings with spatial understanding. We propose two variants: SpatialViLT and MaskedSpatialViLT, focusing on full and masked object regions, respectively. Additionally, SpatialEnsemble combines both approaches, achieving state-of-the-art accuracy. Our models excel in spatial reasoning categories such as directional, topological, and proximity relations, as demonstrated on the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a significant step in enhancing the spatial intelligence of AI systems, crucial for advanced multimodal understanding and real-world applications.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.579627"
    },
    {
        "index": "#339",
        "title": "Scalable Ground Station Selection for Large LEO Constellations",
        "link": "/arxiv/2510.03438",
        "arxiv_id": "2510.03438",
        "authors": "Grace Ra Kim, Duncan Eddy, Vedant Srinivas, Mykel J. Kochenderfer",
        "summary": "Effective ground station selection is critical for low Earth orbiting (LEO) satellite constellations to minimize operational costs, maximize data downlink volume, and reduce communication gaps between access windows. Traditional ground station selection typically begins by choosing from a fixed set of locations offered by Ground Station-as-a-Service (GSaaS) providers, which helps reduce the problem scope to optimizing locations over existing infrastructure. However, finding a globally optimal solution for stations using existing mixed-integer programming methods quickly becomes intractable at scale, especially when considering multiple providers and large satellite constellations. To address this issue, we introduce a scalable, hierarchical framework that decomposes the global selection problem into single-satellite, short time-window subproblems. Optimal station choices from each subproblem are clustered to identify consistently high-value locations across all decomposed cases. Cluster-level sets are then matched back to the closest GSaaS candidate sites to produce a globally feasible solution. This approach enables scalable coordination while maintaining near-optimal performance. We evaluate our method's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10 stations), achieving solutions within 95% of the global IP optimum for all test cases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and Planet's Flock (96) show that while exact IP solutions fail to scale, our framework continues to deliver high-quality site selections.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence, Systems and Control",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.585648"
    },
    {
        "index": "#340",
        "title": "Application of a Virtual Imaging Framework for Investigating a Deep Learning-Based Reconstruction Method for 3D Quantitative Photoacoustic Computed Tomography",
        "link": "/arxiv/2510.03431",
        "arxiv_id": "2510.03431",
        "authors": "Refik Mert Cam, Seonyeong Park, Umberto Villa, Mark A. Anastasio",
        "summary": "Quantitative photoacoustic computed tomography (qPACT) is a promising imaging modality for estimating physiological parameters such as blood oxygen saturation. However, developing robust qPACT reconstruction methods remains challenging due to computational demands, modeling difficulties, and experimental uncertainties. Learning-based methods have been proposed to address these issues but remain largely unvalidated. Virtual imaging (VI) studies are essential for validating such methods early in development, before proceeding to less-controlled phantom or in vivo studies. Effective VI studies must employ ensembles of stochastically generated numerical phantoms that accurately reflect relevant anatomy and physiology. Yet, most prior VI studies for qPACT relied on overly simplified phantoms. In this work, a realistic VI testbed is employed for the first time to assess a representative 3D learning-based qPACT reconstruction method for breast imaging. The method is evaluated across subject variability and physical factors such as measurement noise and acoustic aberrations, offering insights into its strengths and limitations.",
        "subjects": "Medical Physics, Artificial Intelligence, Signal Processing",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.586339"
    },
    {
        "index": "#341",
        "title": "Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation",
        "link": "/arxiv/2510.03426",
        "arxiv_id": "2510.03426",
        "authors": "Franz A. Heinsen, Leo Kozachkov",
        "summary": "Many domains, from deep learning to finance, require compounding real numbers over long sequences, often leading to catastrophic numerical underflow or overflow. We introduce generalized orders of magnitude (GOOMs), a principled extension of traditional orders of magnitude that incorporates floating-point numbers as a special case, and which in practice enables stable computation over significantly larger dynamic ranges of real numbers than previously possible. We implement GOOMs, along with an efficient custom parallel prefix scan, to support native execution on parallel hardware such as GPUs. We demonstrate that our implementation of GOOMs outperforms traditional approaches with three representative experiments, all of which were previously considered impractical or impossible, and now become possible and practical: (1) compounding real matrix products far beyond standard floating-point limits; (2) estimating spectra of Lyapunov exponents in parallel, orders of magnitude faster than with previous methods, applying a novel selective-resetting method to prevent state colinearity; and (3) capturing long-range dependencies in deep recurrent neural networks with non-diagonal recurrent states, computed in parallel via a prefix scan, without requiring any form of stabilization. Our results show that our implementation of GOOMs, combined with efficient parallel scanning, offers a scalable and numerically robust alternative to conventional floating-point numbers for high-dynamic-range applications.",
        "subjects": "Machine Learning, Artificial Intelligence, Numerical Analysis",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.587065"
    },
    {
        "index": "#342",
        "title": "Multi-task neural diffusion processes for uncertainty-quantified wind power prediction",
        "link": "/arxiv/2510.03419",
        "arxiv_id": "2510.03419",
        "authors": "Joseph Rawson, Domniki Ladopoulou, Petros Dellaportas",
        "summary": "Uncertainty-aware wind power prediction is essential for grid integration and reliable wind farm operation. We apply neural diffusion processes (NDPs)-a recent class of models that learn distributions over functions-and extend them to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide the first empirical evaluation of NDPs in real supervisory control and data acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture cross-turbine correlations and enable few-shot adaptation to unseen turbines. The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of point accuracy and calibration, particularly for wind turbines whose behaviour deviates from the fleet average. In general, NDP-based models deliver calibrated and scalable predictions suitable for operational deployment, offering sharper, yet trustworthy, predictive intervals that can support dispatch and maintenance decisions in modern wind farms.",
        "subjects": "Machine Learning, Artificial Intelligence, Applications, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.587787"
    },
    {
        "index": "#343",
        "title": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks",
        "link": "/arxiv/2510.03417",
        "arxiv_id": "2510.03417",
        "authors": "Javad Rafiei Asl, Sidhant Narula, Mohammad Ghasemigol, Eduardo Blanco, Daniel Takabi",
        "summary": "Large Language Models (LLMs) have revolutionized natural language processing but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks that distribute malicious intent across benign exchanges and bypass alignment mechanisms. Existing approaches often explore the adversarial space poorly, rely on hand-crafted heuristics, or lack systematic query refinement. We present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular framework for constructing, refining, and executing optimized multi-turn attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a harmful intent into a structured semantic network of topics, entities, and query chains; (2) a feedback-driven Simulator that iteratively refines and prunes these chains through attacker-victim-judge LLM collaboration using harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser that adaptively navigates the refined query space for real-time attacks. This pipeline uncovers stealthy, high-success adversarial paths across LLMs. On several closed-source and open-source LLMs, NEXUS increases attack success rate by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.588479"
    },
    {
        "index": "#345",
        "title": "Report of the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science",
        "link": "/arxiv/2510.03413",
        "arxiv_id": "2510.03413",
        "authors": "L. C. McInnes, D. Arnold, P. Balaprakash, M. Bernhardt, B. Cerny, A. Dubey, R. Giles, D. W. Hood, M. A. Leung, V. Lopez-Marrero, P. Messina, O. B. Newton, C. Oehmen, S. M. Wild, J. Willenbring, L. Woodley, T. Baylis, D. E. Bernholdt, C. Camano, J. Cohoon, C. Ferenbaugh, S. M. Fiore, S. Gesing, D. Gomez-Zara, J. Howison, T. Islam, D. Kepczynski, C. Lively, H. Menon, B. Messer, M. Ngom, U. Paliath, M. E. Papka, I. Qualters, E. M. Raybourn, K. Riley, P. Rodriguez, D. Rouson, M. Schwalbe, S. K. Seal, O. Surer, V. Taylor, L. Wu",
        "summary": "This report summarizes insights from the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science, which convened more than 40 experts from national laboratories, academia, industry, and community organizations to chart a path toward more powerful, sustainable, and collaborative scientific software ecosystems. To address urgent challenges at the intersection of high-performance computing (HPC), AI, and scientific software, participants envisioned agile, robust ecosystems built through socio-technical co-design--the intentional integration of social and technical components as interdependent parts of a unified strategy. This approach combines advances in AI, HPC, and software with new models for cross-disciplinary collaboration, training, and workforce development. Key recommendations include building modular, trustworthy AI-enabled scientific software systems; enabling scientific teams to integrate AI systems into their workflows while preserving human creativity, trust, and scientific rigor; and creating innovative training pipelines that keep pace with rapid technological change. Pilot projects were identified as near-term catalysts, with initial priorities focused on hybrid AI/HPC infrastructure, cross-disciplinary collaboration and pedagogy, responsible AI guidelines, and prototyping of public-private partnerships. This report presents a vision of next-generation ecosystems for scientific computing where AI, software, hardware, and human expertise are interwoven to drive discovery, expand access, strengthen the workforce, and accelerate scientific progress.",
        "subjects": "Computational Engineering, Finance, and Science, Artificial Intelligence, Mathematical Software",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.595966"
    },
    {
        "index": "#348",
        "title": "Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges",
        "link": "/arxiv/2510.03381",
        "arxiv_id": "2510.03381",
        "authors": "Yongchao Li, Jun Chen, Zhuoxuan Li, Chao Gao, Yang Li, Chu Zhang, Changyin Dong",
        "summary": "Interchanges are crucial nodes for vehicle transfers between highways, yet the lack of real-time ramp detectors creates blind spots in traffic prediction. To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a two-stage framework that leverages cross-modal reconstruction pretraining. In the first stage, STDAE reconstructs historical ramp flows from mainline data, forcing the model to capture intrinsic spatio-temporal relations. Its decoupled architecture with parallel spatial and temporal autoencoders efficiently extracts heterogeneous features. In the prediction stage, the learned representations are integrated with models such as GWNet to enhance accuracy. Experiments on three real-world interchange datasets show that STDAE-GWNET consistently outperforms thirteen state-of-the-art baselines and achieves performance comparable to models using historical ramp data. This demonstrates its effectiveness in overcoming detector scarcity and its plug-and-play potential for diverse forecasting pipelines.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.597997"
    },
    {
        "index": "#349",
        "title": "A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew",
        "link": "/arxiv/2510.03380",
        "arxiv_id": "2510.03380",
        "authors": "Michael Ben Ali, Imen Megdiche, André Peninou, Olivier Teste",
        "summary": "Federated Learning (FL) is a decentralized paradigm that enables a client-server architecture to collaboratively train a global Artificial Intelligence model without sharing raw data, thereby preserving privacy. A key challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of Non-IID, where clients hold highly heterogeneous data volumes. Clustered Federated Learning (CFL) is an emergent variant of FL that presents a promising solution to Non-IID problem. It improves models' performance by grouping clients with similar data distributions into clusters. CFL methods generally fall into two operating strategies. In the first strategy, clients select the cluster that minimizes the local training loss. In the second strategy, the server groups clients based on local model similarities. However, most CFL methods lack systematic evaluation under QS but present significant challenges because of it. In this paper, we present two main contributions. The first one is an evaluation of state-of-the-art CFL algorithms under various Non-IID settings, applying multiple QS scenarios to assess their robustness. Our second contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes an optimal coordination between both operating strategies of CFL. Our approach is robust against the different variations of QS settings. We conducted intensive experiments on six image classification datasets, resulting in 270 Non-IID configurations. The results show that CORNFLQS achieves the highest average ranking in both accuracy and clustering quality, as well as strong robustness to QS perturbations. Overall, our approach outperforms actual CFL algorithms.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.598669"
    },
    {
        "index": "#350",
        "title": "Can an AI-Powered Presentation Platform Based On The Game \"Just a Minute\" Be Used To Improve Students' Public Speaking Skills?",
        "link": "/arxiv/2510.03379",
        "arxiv_id": "2510.03379",
        "authors": "Frederic Higham, Tommy Yuan",
        "summary": "This study explores the effectiveness of applying AI and gamification into a presentation platform aimed at University students wanting to improve their public speaking skills in their native tongue. Specifically, a platform based on the radio show, Just a Minute (JAM), is explored. In this game, players are challenged to speak fluently on a topic for 60 seconds without repeating themselves, hesitating or deviating from the topic. JAM has proposed benefits such as allowing students to improve their spontaneous speaking skills and reduce their use of speech disfluencies (\"um\", \"uh\", etc.). Previous research has highlighted the difficulties students face when speaking publicly, the main one being anxiety. AI Powered Presentation Platforms (AI-PPPs), where students can speak with an immersive AI audience and receive real-time feedback, have been explored as a method to improve student's speaking skills and confidence. So far they have shown promising results which this study aims to build upon. A group of students from the University of York are enlisted to evaluate the effectiveness of the JAM platform. They are asked to fill in a questionnaire, play through the game twice and then complete a final questionnaire to discuss their experiences playing the game. Various statistics are gathered during their gameplay such as the number of points they gained and the number of rules they broke. The results showed that students found the game promising and believed that their speaking skills could improve if they played the game for longer. More work will need to be carried out to prove the effectiveness of the game beyond the short term.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.599300"
    },
    {
        "index": "#352",
        "title": "Real-time nonlinear inversion of magnetic resonance elastography with operator learning",
        "link": "/arxiv/2510.03372",
        "arxiv_id": "2510.03372",
        "authors": "Juampablo E. Heras Rivera, Caitlin M. Neher, Mehmet Kurt",
        "summary": "$\\textbf{Purpose:}$ To develop and evaluate an operator learning framework for nonlinear inversion (NLI) of brain magnetic resonance elastography (MRE) data, which enables real-time inversion of elastograms with comparable spatial accuracy to NLI. $\\textbf{Materials and Methods:}$ In this retrospective study, 3D MRE data from 61 individuals (mean age, 37.4 years; 34 female) were used for development of the framework. A predictive deep operator learning framework (oNLI) was trained using 10-fold cross-validation, with the complex curl of the measured displacement field as inputs and NLI-derived reference elastograms as outputs. A structural prior mechanism, analogous to Soft Prior Regularization in the MRE literature, was incorporated to improve spatial accuracy. Subject-level evaluation metrics included Pearson's correlation coefficient, absolute relative error, and structural similarity index measure between predicted and reference elastograms across brain regions of different sizes to understand accuracy. Statistical analyses included paired t-tests comparing the proposed oNLI variants to the convolutional neural network baselines. $\\textbf{Results:}$ Whole brain absolute percent error was 8.4 $\\pm$ 0.5 ($\\mu'$) and 10.0 $\\pm$ 0.7 ($\\mu''$) for oNLI and 15.8 $\\pm$ 0.8 ($\\mu'$) and 26.1 $\\pm$ 1.1 ($\\mu''$) for CNNs. Additionally, oNLI outperformed convolutional architectures as per Pearson's correlation coefficient, $r$, in the whole brain and across all subregions for both the storage modulus and loss modulus (p < 0.05). $\\textbf{Conclusion:}$ The oNLI framework enables real-time MRE inversion (30,000x speedup), outperforming CNN-based approaches and maintaining the fine-grained spatial accuracy achievable with NLI in the brain.",
        "subjects": "Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.600750"
    },
    {
        "index": "#353",
        "title": "Distributed Low-Communication Training with Decoupled Momentum Optimization",
        "link": "/arxiv/2510.03371",
        "arxiv_id": "2510.03371",
        "authors": "Sasho Nedelkoski, Alexander Acker, Odej Kao, Soeren Becker, Dominik Scheinert",
        "summary": "The training of large models demands substantial computational resources, typically available only in data centers with high-bandwidth interconnects. However, reducing the reliance on high-bandwidth interconnects between nodes enables the use of distributed compute resources as an alternative to centralized data center training. Building on recent advances in distributed model training, we propose an approach that further reduces communication by combining infrequent synchronizations across distributed model replicas with gradient momentum compression. In particular, we treat the optimizer momentum as a signal and decompose the Nesterov momentum into high- and low-frequency components via the discrete cosine transform (DCT). Only the high-frequency components are synchronized across model replicas every $H$ steps. Empirically, our method achieves up to a $16\\times$ reduction in communication compared to the baseline DiLoCo, and it generalizes across architectures, including transformer-based language models and convolutional neural networks for images. Overall, this work advances the feasibility of training large models on distributed nodes with low-bandwidth interconnects.",
        "subjects": "Machine Learning, Artificial Intelligence, Distributed, Parallel, and Cluster Computing",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.606689"
    },
    {
        "index": "#354",
        "title": "InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions",
        "link": "/arxiv/2510.03370",
        "arxiv_id": "2510.03370",
        "authors": "Junde Xu, Yapin Shi, Lijun Lang, Taoyong Cui, Zhiming Zhang, Guangyong Chen, Jiezhong Qiu, Pheng-Ann Heng",
        "summary": "Multimodal protein language models deliver strong performance on mutation-effect prediction, but training such models from scratch demands substantial computational resources. In this paper, we propose a fine-tuning framework called InstructPLM-mu and try to answer a question: \\textit{Can multimodal fine-tuning of a pretrained, sequence-only protein language model match the performance of models trained end-to-end? } Surprisingly, our experiments show that fine-tuning ESM2 with structural inputs can reach performance comparable to ESM3. To understand how this is achieved, we systematically compare three different feature-fusion designs and fine-tuning recipes. Our results reveal that both the fusion method and the tuning strategy strongly affect final accuracy, indicating that the fine-tuning process is not trivial. We hope this work offers practical guidance for injecting structure into pretrained protein language models and motivates further research on better fusion mechanisms and fine-tuning protocols.",
        "subjects": "Quantitative Methods, Artificial Intelligence, Computational Engineering, Finance, and Science",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.607491"
    },
    {
        "index": "#355",
        "title": "TriQuest:An AI Copilot-Powered Platform for Interdisciplinary Curriculum Design",
        "link": "/arxiv/2510.03369",
        "arxiv_id": "2510.03369",
        "authors": "Huazhen Wang, Huimin Yang, Hainbin Lin, Yan Dong, Lili Chen, Liangliang Xia, Wenwen Xu",
        "summary": "Interdisciplinary teaching is a cornerstone of modern curriculum reform, but its implementation is hindered by challenges in knowledge integration and time-consuming lesson planning. Existing tools often lack the required pedagogical and domain-specific depth.We introduce TriQuest, an AI-copilot platform designed to solve these problems. TriQuest uses large language models and knowledge graphs via an intuitive GUI to help teachers efficiently generate high-quality interdisciplinary lesson plans. Its core features include intelligent knowledge integration from various disciplines and a human-computer collaborative review process to ensure quality and innovation.In a study with 43 teachers, TriQuest increased curriculum design efficiency by an average of 75% and improved lesson plan quality scores by 41%. It also significantly lowered design barriers and cognitive load. Our work presents a new paradigm for empowering teacher professional development with intelligent technologies.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.608284"
    },
    {
        "index": "#356",
        "title": "An Adaptive Responsible AI Governance Framework for Decentralized Organizations",
        "link": "/arxiv/2510.03368",
        "arxiv_id": "2510.03368",
        "authors": "Kiana Jafari Meimandi, Anka Reuel, Gabriela Aranguiz-Dias, Hatim Rahama, Ala-Eddine Ayadi, Xavier Boullier, Jérémy Verdo, Louis Montanie, Mykel Kochenderfer",
        "summary": "This paper examines the assessment challenges of Responsible AI (RAI) governance efforts in globally decentralized organizations through a case study collaboration between a leading research university and a multinational enterprise. While there are many proposed frameworks for RAI, their application in complex organizational settings with distributed decision-making authority remains underexplored. Our RAI assessment, conducted across multiple business units and AI use cases, reveals four key patterns that shape RAI implementation: (1) complex interplay between group-level guidance and local interpretation, (2) challenges translating abstract principles into operational practices, (3) regional and functional variation in implementation approaches, and (4) inconsistent accountability in risk oversight. Based on these findings, we propose an Adaptive RAI Governance (ARGO) Framework that balances central coordination with local autonomy through three interdependent layers: shared foundation standards, central advisory resources, and contextual local implementation. We contribute insights from academic-industry collaboration for RAI assessments, highlighting the importance of modular governance approaches that accommodate organizational complexity while maintaining alignment with responsible AI principles. These lessons offer practical guidance for organizations navigating the transition from RAI principles to operational practice within decentralized structures.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.609061"
    },
    {
        "index": "#357",
        "title": "Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis",
        "link": "/arxiv/2510.03366",
        "arxiv_id": "2510.03366",
        "authors": "Harshwardhan Fartale, Ashish Kattamuri, Rahul Raja, Arpita Vats, Ishita Prasad, Akshata Kishore Moharir",
        "summary": "Transformer-based language models excel at both recall (retrieving memorized facts) and reasoning (performing multi-step inference), but whether these abilities rely on distinct internal mechanisms remains unclear. Distinguishing recall from reasoning is crucial for predicting model generalization, designing targeted evaluations, and building safer interventions that affect one ability without disrupting the other.We approach this question through mechanistic interpretability, using controlled datasets of synthetic linguistic puzzles to probe transformer models at the layer, head, and neuron level. Our pipeline combines activation patching and structured ablations to causally measure component contributions to each task type. Across two model families (Qwen and LLaMA), we find that interventions on distinct layers and attention heads lead to selective impairments: disabling identified \"recall circuits\" reduces fact-retrieval accuracy by up to 15\\% while leaving reasoning intact, whereas disabling \"reasoning circuits\" reduces multi-step inference by a comparable margin. At the neuron level, we observe task-specific firing patterns, though these effects are less robust, consistent with neuronal polysemanticity.Our results provide the first causal evidence that recall and reasoning rely on separable but interacting circuits in transformer models. These findings advance mechanistic interpretability by linking circuit-level structure to functional specialization and demonstrate how controlled datasets and causal interventions can yield mechanistic insights into model cognition, informing safer deployment of large language models.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.609778"
    },
    {
        "index": "#358",
        "title": "Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds",
        "link": "/arxiv/2510.03364",
        "arxiv_id": "2510.03364",
        "authors": "Xiaolong Ma, Xu Dong, Ashley Tarrant, Lei Yang, Rao Kotamarthi, Jiali Wang, Feng Yan, Rajkumar Kettimuthu",
        "summary": "High-quality observations of hub-height winds are valuable but sparse in space and time. Simulations are widely available on regular grids but are generally biased and too coarse to inform wind-farm siting or to assess extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully utilize both data types for generating high-quality, high-resolution hub-height wind speeds (tens to ~100m above ground), this study introduces WindSR, a diffusion model with data assimilation for super-resolution downscaling of hub-height winds. WindSR integrates sparse observational data with simulation fields during downscaling using state-of-the-art diffusion models. A dynamic-radius blending method is introduced to merge observations with simulations, providing conditioning for the diffusion process. Terrain information is incorporated during both training and inference to account for its role as a key driver of winds. Evaluated against convolutional-neural-network and generative-adversarial-network baselines, WindSR outperforms them in both downscaling efficiency and accuracy. Our data assimilation reduces WindSR's model bias by approximately 20% relative to independent observations.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.610573"
    },
    {
        "index": "#359",
        "title": "Unified Unsupervised Anomaly Detection via Matching Cost Filtering",
        "link": "/arxiv/2510.03363",
        "arxiv_id": "2510.03363",
        "authors": "Zhe Zhang, Mingxiu Cai, Gaochang Wu, Jing Zhang, Lingqiao Liu, Dacheng Tao, Tianyou Chai, Xiatian Zhu",
        "summary": "Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level anomalies using only normal training data, with wide applications such as industrial inspection and medical analysis, where anomalies are scarce due to privacy concerns and cold-start constraints. Existing methods, whether reconstruction-based (restoring normal counterparts) or embedding-based (pretrained representations), fundamentally conduct image- or feature-level matching to generate anomaly maps. Nonetheless, matching noise has been largely overlooked, limiting their detection ability. Beyond earlier focus on unimodal RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D and RGB--Text, enabled by point cloud sensing and vision--language models. Despite shared challenges, these lines remain largely isolated, hindering a comprehensive understanding and knowledge transfer. In this paper, we advocate unified UAD for both unimodal and multimodal settings in the matching perspective. Under this insight, we present Unified Cost Filtering (UCF), a generic post-hoc refinement framework for refining anomaly cost volume of any UAD model. The cost volume is constructed by matching a test sample against normal samples from the same or different modalities, followed by a learnable filtering module with multi-layer attention guidance from the test sample, mitigating matching noise and highlighting subtle anomalies. Comprehensive experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in enhancing a variety of UAD methods, consistently achieving new state-of-the-art results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD scenarios. Code and models will be released at https://github.com/ZHE-SAPI/CostFilter-AD.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Image and Video Processing",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.616534"
    },
    {
        "index": "#360",
        "title": "Provenance Networks: End-to-End Exemplar-Based Explainability",
        "link": "/arxiv/2510.03361",
        "arxiv_id": "2510.03361",
        "authors": "Ali Kayyam, Anusha Madan Gopal, M. Anthony Lewis",
        "summary": "We introduce provenance networks, a novel class of neural models designed to provide end-to-end, training-data-driven explainability. Unlike conventional post-hoc methods, provenance networks learn to link each prediction directly to its supporting training examples as part of the model's normal operation, embedding interpretability into the architecture itself. Conceptually, the model operates similarly to a learned KNN, where each output is justified by concrete exemplars weighted by relevance in the feature space. This approach facilitates systematic investigations of the trade-off between memorization and generalization, enables verification of whether a given input was included in the training set, aids in the detection of mislabeled or anomalous data points, enhances resilience to input perturbations, and supports the identification of similar inputs contributing to the generation of a new data point. By jointly optimizing the primary task and the explainability objective, provenance networks offer insights into model behavior that traditional deep networks cannot provide. While the model introduces additional computational cost and currently scales to moderately sized datasets, it provides a complementary approach to existing explainability techniques. In particular, it addresses critical challenges in modern deep learning, including model opaqueness, hallucination, and the assignment of credit to data contributors, thereby improving transparency, robustness, and trustworthiness in neural models.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.617227"
    },
    {
        "index": "#361",
        "title": "Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows",
        "link": "/arxiv/2510.03360",
        "arxiv_id": "2510.03360",
        "authors": "Zelin Zhao, Zongyi Li, Kimia Hassibi, Kamyar Azizzadenesheli, Junchi Yan, H. Jane Bae, Di Zhou, Anima Anandkumar",
        "summary": "Assessing turbulence control effects for wall friction numerically is a significant challenge since it requires expensive simulations of turbulent fluid dynamics. We instead propose an efficient deep reinforcement learning (RL) framework for modeling and control of turbulent flows. It is model-based RL for predictive control (PC), where both the policy and the observer models for turbulence control are learned jointly using Physics Informed Neural Operators (PINO), which are discretization invariant and can capture fine scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free reinforcement learning methods in various challenging scenarios where the flows are of high Reynolds numbers and unseen, i.e., not provided during model training. We find that PINO-PC achieves a drag reduction of 39.0\\% under a bulk-velocity Reynolds number of 15,000, outperforming previous fluid control methods by more than 32\\%.",
        "subjects": "Machine Learning, Artificial Intelligence, Optimization and Control, Fluid Dynamics",
        "date": "2025-10-03",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.618045"
    },
    {
        "index": "#362",
        "title": "Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility",
        "link": "/arxiv/2510.03358",
        "arxiv_id": "2510.03358",
        "authors": "Annan Yu, Danielle C. Maddix, Boran Han, Xiyuan Zhang, Abdul Fatir Ansari, Oleksandr Shchur, Christos Faloutsos, Andrew Gordon Wilson, Michael W. Mahoney, Yuyang Wang",
        "summary": "Transformers are widely used across data modalities, and yet the principles distilled from text models often transfer imperfectly to models trained to other modalities. In this paper, we analyze Transformers through the lens of rank structure. Our focus is on the time series setting, where the structural properties of the data differ remarkably from those of text or vision. We show that time-series embeddings, unlike text or vision, exhibit sharply decaying singular value spectra: small patch sizes and smooth continuous mappings concentrate the data into low-rank subspaces. From this, we prove that the associated $Q/K/V$ projections admit accurate low-rank approximations, and that attention layers become compressible in proportion to the decay of the embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by which nonlinear mixing across depth inflates the rank, explaining why early layers are most amenable to compression and why ranks grow with depth. Guided by these theoretical and empirical results, we use these insights to compress Chronos, a large time series foundation model, achieving a reduction of $65\\%$ in inference time and $81\\%$ in memory, without loss of accuracy. Our findings provide principled guidance for allocating width, depth, and heads in time series foundation models, and for exploiting their inherent compressibility.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.618871"
    },
    {
        "index": "#363",
        "title": "Inference-Time Search using Side Information for Diffusion-based Image Reconstruction",
        "link": "/arxiv/2510.03352",
        "arxiv_id": "2510.03352",
        "authors": "Mahdi Farahbakhsh, Vishnu Teja Kunde, Dileep Kalathil, Krishna Narayanan, Jean-Francois Chamberland",
        "summary": "Diffusion models have emerged as powerful priors for solving inverse problems. However, existing approaches typically overlook side information that could significantly improve reconstruction quality, especially in severely ill-posed settings. In this work, we propose a novel inference-time search algorithm that guides the sampling process using the side information in a manner that balances exploration and exploitation. This enables more accurate and reliable reconstructions, providing an alternative to the gradient-based guidance that is prone to reward-hacking artifacts. Our approach can be seamlessly integrated into a wide range of existing diffusion-based image reconstruction pipelines. Through extensive experiments on a number of inverse problems, such as box inpainting, super-resolution, and various deblurring tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that our approach consistently improves the qualitative and quantitative performance of diffusion-based image reconstruction algorithms. We also show the superior performance of our approach with respect to other baselines, including reward gradient-based guidance algorithms. The code is available at \\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this repository}.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.619630"
    },
    {
        "index": "#364",
        "title": "Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks",
        "link": "/arxiv/2510.03351",
        "arxiv_id": "2510.03351",
        "authors": "Song Wang, Zhenyu Lei, Zhen Tan, Jundong Li, Javier Rasero, Aiying Zhang, Chirag Agarwal",
        "summary": "Nearly one in five adolescents currently live with a diagnosed mental or behavioral health condition, such as anxiety, depression, or conduct disorder, underscoring the urgency of developing accurate and interpretable diagnostic tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a powerful lens into large-scale functional connectivity, where brain regions are modeled as nodes and inter-regional synchrony as edges, offering clinically relevant biomarkers for psychiatric disorders. While prior works use graph neural network (GNN) approaches for disorder prediction, they remain complex black-boxes, limiting their reliability and clinical translation. In this work, we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages large language models (LLMs) and neurobiological domain knowledge to automatically generate, filter, and encode interpretable functional connectivity concepts. Each concept is represented as a structured subgraph linking specific brain regions, which are then passed through a concept classifier. Our design ensures predictions through clinically meaningful connectivity patterns, enabling both interpretability and strong predictive performance. Extensive experiments across multiple psychiatric disorder datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform their vanilla counterparts, improving accuracy while providing transparent, clinically aligned explanations. Furthermore, concept analyses highlight disorder-specific connectivity patterns that align with expert knowledge and suggest new hypotheses for future investigation, establishing CONCEPTNEURO as an interpretable, domain-informed framework for psychiatric disorder diagnosis.",
        "subjects": "Machine Learning, Artificial Intelligence, Image and Video Processing",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.620431"
    },
    {
        "index": "#367",
        "title": "Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models",
        "link": "/arxiv/2510.03345",
        "arxiv_id": "2510.03345",
        "authors": "Luoma Ke, Guangpeng Zhang, Jibo He, Yajing Li, Yan Li, Xufeng Liu, Peng Fang",
        "summary": "With the rapid growth of the aviation industry, there is a need for a large number of flight crew. How to select the right pilots in a cost-efficient manner has become an important research question. In the current study, twenty-three pilots were recruited from China Eastern Airlines, and 23 novices were from the community of Tsinghua University. A novel approach incorporating machine learning and virtual reality technology was applied to distinguish features between these participants with different flight skills. Results indicate that SVM with the MIC feature selection method consistently achieved the highest prediction performance on all metrics with an Accuracy of 0.93, an AUC of 0.96, and an F1 of 0.93, which outperforms four other classifier algorithms and two other feature selection methods. From the perspective of feature selection methods, the MIC method can select features with a nonlinear relationship to sampling labels, instead of a simple filter-out. Our new implementation of the SVM + MIC algorithm outperforms all existing pilot selection algorithms and perhaps provides the first implementation based on eye tracking and flight dynamics data. This study's VR simulation platforms and algorithms can be used for pilot selection and training.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.634570"
    },
    {
        "index": "#368",
        "title": "Defining a Strategic Action Plan for AI in Higher Education",
        "link": "/arxiv/2510.03343",
        "arxiv_id": "2510.03343",
        "authors": "Nikolaos Avouris",
        "summary": "This paper discusses key challenges of Artificial Intelligence in Education, with main focus on higher education institutions. We start with reviewing normative actions of international organizations and concerns expressed about the current technical landscape. Then we proceed with proposing a framework that comprises five key dimensions relating to the main challenges relating to AI in higher education institutions, followed by five key strategic actions that the main stakeholders need to take in order to address the current developments. We map these actions to the main stakeholders of higher education and propose a deployment plan. This defines a framework along the dimensions: Challenges, Actions, Stakeholders, Deployment CASD. Examples of AI specific actions at the institutional and individual course level are also provided and discussed.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.635179"
    },
    {
        "index": "#369",
        "title": "Learning Pareto-Optimal Pandemic Intervention Policies with MORL",
        "link": "/arxiv/2510.03340",
        "arxiv_id": "2510.03340",
        "authors": "Marian Chen, Miri Zilka",
        "summary": "The COVID-19 pandemic underscored a critical need for intervention strategies that balance disease containment with socioeconomic stability. We approach this challenge by designing a framework for modeling and evaluating disease-spread prevention strategies. Our framework leverages multi-objective reinforcement learning (MORL) - a formulation necessitated by competing objectives - combined with a new stochastic differential equation (SDE) pandemic simulator, calibrated and validated against global COVID-19 data. Our simulator reproduces national-scale pandemic dynamics with orders of magnitude higher fidelity than other models commonly used in reinforcement learning (RL) approaches to pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on this simulator, we illustrate the direct policy trade-offs between epidemiological control and economic stability for COVID-19. Furthermore, we demonstrate the framework's generality by extending it to pathogens with different epidemiological profiles, such as polio and influenza, and show how these profiles lead the agent to discover fundamentally different intervention policies. To ground our work in contemporary policymaking challenges, we apply the model to measles outbreaks, quantifying how a modest 5% drop in vaccination coverage necessitates significantly more stringent and costly interventions to curb disease spread. This work provides a robust and adaptable framework to support transparent, evidence-based policymaking for mitigating public health crises.",
        "subjects": "Machine Learning, Artificial Intelligence, Computers and Society, Populations and Evolution",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.635901"
    },
    {
        "index": "#370",
        "title": "Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models",
        "link": "/arxiv/2510.03339",
        "arxiv_id": "2510.03339",
        "authors": "Sofiane Ennadir, Levente Zólyomi, Oleg Smirnov, Tianze Wang, John Pertoft, Filip Cornell, Lele Cao",
        "summary": "Transformer models have become the dominant backbone for sequence modeling, leveraging self-attention to produce contextualized token representations. These are typically aggregated into fixed-size vectors via pooling operations for downstream tasks. While much of the literature has focused on attention mechanisms, the role of pooling remains underexplored despite its critical impact on model behavior. In this paper, we introduce a theoretical framework that rigorously characterizes the expressivity of Transformer-based models equipped with widely used pooling methods by deriving closed-form bounds on their representational capacity and the ability to distinguish similar inputs. Our analysis extends to different variations of attention formulations, demonstrating that these bounds hold across diverse architectural variants. We empirically evaluate pooling strategies across tasks requiring both global and local contextual understanding, spanning three major modalities: computer vision, natural language processing, and time-series analysis. Results reveal consistent trends in how pooling choices affect accuracy, sensitivity, and optimization behavior. Our findings unify theoretical and empirical perspectives, providing practical guidance for selecting or designing pooling mechanisms suited to specific tasks. This work positions pooling as a key architectural component in Transformer models and lays the foundation for more principled model design beyond attention alone.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.636639"
    },
    {
        "index": "#371",
        "title": "Linguistic and Audio Embedding-Based Machine Learning for Alzheimer's Dementia and Mild Cognitive Impairment Detection: Insights from the PROCESS Challenge",
        "link": "/arxiv/2510.03336",
        "arxiv_id": "2510.03336",
        "authors": "Adharsha Sam Edwin Sam Devahi, Sohail Singh Sangha, Prachee Priyadarshinee, Jithin Thilakan, Ivan Fu Xing Tan, Christopher Johann Clarke, Sou Ka Lon, Balamurali B T, Yow Wei Quin, Chen Jer-Ming",
        "summary": "Early detection of Alzheimer's Dementia (AD) and Mild Cognitive Impairment (MCI) is critical for timely intervention, yet current diagnostic approaches remain resource-intensive and invasive. Speech, encompassing both acoustic and linguistic dimensions, offers a promising non-invasive biomarker for cognitive decline. In this study, we present a machine learning framework for the PROCESS Challenge, leveraging both audio embeddings and linguistic features derived from spontaneous speech recordings. Audio representations were extracted using Whisper embeddings from the Cookie Theft description task, while linguistic features-spanning pronoun usage, syntactic complexity, filler words, and clause structure-were obtained from transcriptions across Semantic Fluency, Phonemic Fluency, and Cookie Theft picture description. Classification models aimed to distinguish between Healthy Controls (HC), MCI, and AD participants, while regression models predicted Mini-Mental State Examination (MMSE) scores. Results demonstrated that voted ensemble models trained on concatenated linguistic features achieved the best classification performance (F1 = 0.497), while Whisper embedding-based ensemble regressors yielded the lowest MMSE prediction error (RMSE = 2.843). Comparative evaluation within the PROCESS Challenge placed our models among the top submissions in regression task, and mid-range for classification, highlighting the complementary strengths of linguistic and audio embeddings. These findings reinforce the potential of multimodal speech-based approaches for scalable, non-invasive cognitive assessment and underline the importance of integrating task-specific linguistic and acoustic markers in dementia detection.",
        "subjects": "Sound, Artificial Intelligence, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.637481"
    },
    {
        "index": "#372",
        "title": "Intelligent Healthcare Ecosystems: Optimizing the Iron Triangle of Healthcare (Access, Cost, Quality)",
        "link": "/arxiv/2510.03331",
        "arxiv_id": "2510.03331",
        "authors": "Vivek Acharya",
        "summary": "The United States spends nearly 17% of GDP on healthcare yet continues to face uneven access and outcomes. This well-known trade-off among cost, quality, and access - the \"iron triangle\" - motivates a system-level redesign. This paper proposes an Intelligent Healthcare Ecosystem (iHE): an integrated, data-driven framework that uses generative AI and large language models, federated learning, interoperability standards (FHIR, TEFCA), and digital twins to improve access and quality while lowering cost. We review historical spending trends, waste, and international comparisons; introduce a value equation that jointly optimizes access, quality, and cost; and synthesize evidence on the enabling technologies and operating model for iHE. Methods follow a narrative review of recent literature and policy reports. Results outline core components (AI decision support, interoperability, telehealth, automation) and show how iHE can reduce waste, personalize care, and support value-based payment while addressing privacy, bias, and adoption challenges. We argue that a coordinated iHE can bend - if not break - the iron triangle, moving the system toward care that is more accessible, affordable, and high quality.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-10-01",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.638089"
    },
    {
        "index": "#373",
        "title": "NS-Pep: De novo Peptide Design with Non-Standard Amino Acids",
        "link": "/arxiv/2510.03326",
        "arxiv_id": "2510.03326",
        "authors": "Tao Guo, Junbo Yin, Yu Wang, Xin Gao",
        "summary": "Peptide drugs incorporating non-standard amino acids (NSAAs) offer improved binding affinity and improved pharmacological properties. However, existing peptide design methods are limited to standard amino acids, leaving NSAA-aware design largely unexplored. We introduce NS-Pep, a unified framework for co-designing peptide sequences and structures with NSAAs. The main challenge is that NSAAs are extremely underrepresented-even the most frequent one, SEP, accounts for less than 0.4% of residues-resulting in a severe long-tailed distribution. To improve generalization to rare amino acids, we propose Residue Frequency-Guided Modification (RFGM), which mitigates over-penalization through frequency-aware logit calibration, supported by both theoretical and empirical analysis. Furthermore, we identify that insufficient side-chain modeling limits geometric representation of NSAAs. To address this, we introduce Progressive Side-chain Perception (PSP) for coarse-to-fine torsion and location prediction, and Interaction-Aware Weighting (IAW) to emphasize pocket-proximal residues. Moreover, NS-Pep generalizes naturally to the peptide folding task with NSAAs, addressing a major limitation of current tools. Experiments show that NS-Pep improves sequence recovery rate and binding affinity by 6.23% and 5.12%, respectively, and outperforms AlphaFold3 by 17.76% in peptide folding success rate.",
        "subjects": "Biomolecules, Artificial Intelligence",
        "date": "2025-10-01",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.638761"
    },
    {
        "index": "#374",
        "title": "Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring",
        "link": "/arxiv/2510.03317",
        "arxiv_id": "2510.03317",
        "authors": "Günel Aghakishiyeva, Jiayi Zhou, Saagar Arya, James David Poling, Holly R. Houliston, Jamie N. Womble, David W. Johnston, Brinnae Bent",
        "summary": "Ecological monitoring is increasingly automated by vision models, yet opaque predictions limit trust and field adoption. We present an inpainting-guided, perturbation-based explanation technique that produces photorealistic, mask-localized edits that preserve scene context. Unlike masking or blurring, these edits stay in-distribution and reveal which fine-grained morphological cues drive predictions in tasks such as species recognition and trait attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for harbor seal detection in Glacier Bay drone imagery, using Segment-Anything-Model-refined masks to support two interventions: (i) object removal/replacement (e.g., replacing seals with plausible ice/water or boats) and (ii) background replacement with original animals composited onto new scenes. Explanations are assessed by re-scoring perturbed images (flip rate, confidence drop) and by expert review for ecological plausibility and interpretability. The resulting explanations localize diagnostic structures, avoid deletion artifacts common to traditional perturbations, and yield domain-relevant insights that support expert validation and more trustworthy deployment of AI in ecology.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-01",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.639616"
    },
    {
        "index": "#375",
        "title": "The View From Space: Navigating Instrumentation Differences with EOFMs",
        "link": "/arxiv/2510.03316",
        "arxiv_id": "2510.03316",
        "authors": "Ryan P. Demilt, Nicholas LaHaye, Karis Tenneson",
        "summary": "Earth Observation Foundation Models (EOFMs) have exploded in prevalence as tools for processing the massive volumes of remotely sensed and other earth observation data, and for delivering impact on the many essential earth monitoring tasks. An emerging trend posits using the outputs of pre-trained models as 'embeddings' which summarize high dimensional data to be used for generic tasks such as similarity search and content-specific queries. However, most EOFM models are trained only on single modalities of data and then applied or benchmarked by matching bands across different modalities. It is not clear from existing work what impact diverse sensor architectures have on the internal representations of the present suite of EOFMs. We show in this work that the representation space of EOFMs is highly sensitive to sensor architecture and that understanding this difference gives a vital perspective on the pitfalls of current EOFM design and signals for how to move forward as model developers, users, and a community guided by robust remote-sensing science.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-01",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.640308"
    },
    {
        "index": "#377",
        "title": "A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety",
        "link": "/arxiv/2510.03314",
        "arxiv_id": "2510.03314",
        "authors": "Shucheng Zhang, Yan Shi, Bingzhang Wang, Yuang Zhang, Muhammad Monjurul Karim, Kehua Chen, Chenxi Liu, Mehrdad Nasri, Yinhai Wang",
        "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and cyclists, remains a critical global challenge, as conventional infrastructure-based measures often prove inadequate in dynamic urban environments. Recent advances in artificial intelligence (AI), particularly in visual perception and reasoning, open new opportunities for proactive and context-aware VRU protection. However, existing surveys on AI applications for VRUs predominantly focus on detection, offering limited coverage of other vision-based tasks that are essential for comprehensive VRU understanding and protection. This paper presents a state-of-the-art review of recent progress in camera-based AI sensing systems for VRU safety, with an emphasis on developments from the past five years and emerging research trends. We systematically examine four core tasks, namely detection and classification, tracking and reidentification, trajectory prediction, and intent recognition and prediction, which together form the backbone of AI-empowered proactive solutions for VRU protection in intelligent transportation systems. To guide future research, we highlight four major open challenges from the perspectives of data, model, and deployment. By linking advances in visual AI with practical considerations for real-world implementation, this survey aims to provide a foundational reference for the development of next-generation sensing systems to enhance VRU safety.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.641724"
    },
    {
        "index": "#378",
        "title": "Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management",
        "link": "/arxiv/2510.03310",
        "arxiv_id": "2510.03310",
        "authors": "Runze Zhang, Xiaowei Zhang, Mingyang Zhao",
        "summary": "LLMs are emerging tools for simulating human behavior in business, economics, and social science, offering a lower-cost complement to laboratory experiments, field studies, and surveys. This paper evaluates how well LLMs replicate human behavior in operations management. Using nine published experiments in behavioral operations, we assess two criteria: replication of hypothesis-test outcomes and distributional alignment via Wasserstein distance. LLMs reproduce most hypothesis-level effects, capturing key decision biases, but their response distributions diverge from human data, including for strong commercial models. We also test two lightweight interventions -- chain-of-thought prompting and hyperparameter tuning -- which reduce misalignment and can sometimes let smaller or open-source models match or surpass larger systems.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.642376"
    },
    {
        "index": "#379",
        "title": "Creative synthesis of kinematic mechanisms",
        "link": "/arxiv/2510.03308",
        "arxiv_id": "2510.03308",
        "authors": "Jiong Lin, Jialong Ning, Judah Goldfeder, Hod Lipson",
        "summary": "In this paper, we formulate the problem of kinematic synthesis for planar linkages as a cross-domain image generation task. We develop a planar linkages dataset using RGB image representations, covering a range of mechanisms: from simple types such as crank-rocker and crank-slider to more complex eight-bar linkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE) is employed to explore the potential of image generative models for synthesizing unseen motion curves and simulating novel kinematics. By encoding the drawing speed of trajectory points as color gradients, the same architecture also supports kinematic synthesis conditioned on both trajectory shape and velocity profiles. We validate our method on three datasets of increasing complexity: a standard four-bar linkage set, a mixed set of four-bar and crank-slider mechanisms, and a complex set including multi-loop mechanisms. Preliminary results demonstrate the effectiveness of image-based representations for generative mechanical design, showing that mechanisms with revolute and prismatic joints, and potentially cams and gears, can be represented and synthesized within a unified image generation framework.",
        "subjects": "Graphics, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-09-30",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.643084"
    },
    {
        "index": "#380",
        "title": "Atlas-free Brain Network Transformer",
        "link": "/arxiv/2510.03306",
        "arxiv_id": "2510.03306",
        "authors": "Shuai Huang, Xuan Kan, James J. Lah, Deqiang Qiu",
        "summary": "Current atlas-based approaches to brain network analysis rely heavily on standardized anatomical or connectivity-driven brain atlases. However, these fixed atlases often introduce significant limitations, such as spatial misalignment across individuals, functional heterogeneity within predefined regions, and atlas-selection biases, collectively undermining the reliability and interpretability of the derived brain networks. To address these challenges, we propose a novel atlas-free brain network transformer (atlas-free BNT) that leverages individualized brain parcellations derived directly from subject-specific resting-state fMRI data. Our approach computes ROI-to-voxel connectivity features in a standardized voxel-based feature space, which are subsequently processed using the BNT architecture to produce comparable subject-level embeddings. Experimental evaluations on sex classification and brain-connectome age prediction tasks demonstrate that our atlas-free BNT consistently outperforms state-of-the-art atlas-based methods, including elastic net, BrainGNN, Graphormer and the original BNT. Our atlas-free approach significantly improves the precision, robustness, and generalizability of brain network analyses. This advancement holds great potential to enhance neuroimaging biomarkers and clinical diagnostic tools for personalized precision medicine.",
        "subjects": "Neurons and Cognition, Artificial Intelligence, Machine Learning, Neural and Evolutionary Computing, Image and Video Processing",
        "date": "2025-09-30",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.643882"
    },
    {
        "index": "#381",
        "title": "Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles",
        "link": "/arxiv/2510.03301",
        "arxiv_id": "2510.03301",
        "authors": "Arthur Sedek",
        "summary": "This paper introduces a novel adaptive ensemble framework that synergistically combines XGBoost and neural networks through sophisticated meta-learning. The proposed method leverages advanced uncertainty quantification techniques and feature importance integration to dynamically orchestrate model selection and combination. Experimental results demonstrate superior predictive performance and enhanced interpretability across diverse datasets, contributing to the development of more intelligent and flexible machine learning systems.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.644502"
    },
    {
        "index": "#382",
        "title": "Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes",
        "link": "/arxiv/2510.03297",
        "arxiv_id": "2510.03297",
        "authors": "Akshar Gothi",
        "summary": "We present a controlled comparison of a convolutional neural network (EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two label-distribution regimes: a naturally imbalanced five-class split and a balanced-resampled split with 700 images per class (70:20:10 train/val/test). With matched preprocessing (224x224, ImageNet normalization), lightweight augmentations, and a 40-epoch budget on a single NVIDIA P100, we report accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics (model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93% test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive at 93% with a larger parameter count and runtime. On the balanced split, both models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains competitive, indicating that balancing narrows architecture gaps while CNNs retain an efficiency edge. We release manifests, logs, and per-image predictions to support reproducibility.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.644891"
    },
    {
        "index": "#383",
        "title": "From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing",
        "link": "/arxiv/2510.03293",
        "arxiv_id": "2510.03293",
        "authors": "Rana Shahout, Colin Cai, Yilun Du, Minlan Yu, Michael Mitzenmacher",
        "summary": "Mixture-of-Experts (MoE) models can scale parameter capacity by routing each token to a subset of experts through a learned gate function. While conditional routing reduces training costs, it shifts the burden on inference memory: expert parameters and activations consume memory, limiting the number of experts per device. As tokens are routed, some experts become overloaded while others are underutilized. Because experts are mapped to GPUs, this imbalance translates directly into degraded system performance in terms of latency, throughput, and cost. We present LASER, a plug-and-play, inference-time routing algorithm that balances load while preserving accuracy. LASER adapts to the shape of the gate's score distribution. When scores provide a clear preference, it routes to the strongest experts; when scores are more uniform, it broadens the set of viable experts and routes to the least-loaded among them. Because LASER relies only on gate scores from a trained model, it integrates directly into existing MoE inference pipelines without retraining or finetuning. We evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets (ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing, translating into lower latency and higher throughput, while keeping the accuracy changes negligible.",
        "subjects": "Machine Learning, Artificial Intelligence, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.645336"
    },
    {
        "index": "#384",
        "title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs",
        "link": "/arxiv/2510.03291",
        "arxiv_id": "2510.03291",
        "authors": "Yizhuo Ding, Wanying Qu, Jiawei Geng, Wenqi Shao, Yanwei Fu",
        "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.645755"
    },
    {
        "index": "#386",
        "title": "LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain",
        "link": "/arxiv/2510.03288",
        "arxiv_id": "2510.03288",
        "authors": "Chiming Duan, Minghua He, Pei Xiao, Tong Jia, Xin Zhang, Zhewei Zhong, Xiang Luo, Yan Niu, Lingzhe Zhang, Yifan Wu, Siyu Yu, Weijie Hong, Ying Li, Gang Huang",
        "summary": "Log-based anomaly detection is a essential task for ensuring the reliability and performance of software systems. However, the performance of existing anomaly detection methods heavily relies on labeling, while labeling a large volume of logs is highly challenging. To address this issue, many approaches based on transfer learning and active learning have been proposed. Nevertheless, their effectiveness is hindered by issues such as the gap between source and target system data distributions and cold-start problems. In this paper, we propose LogAction, a novel log-based anomaly detection model based on active domain adaptation. LogAction integrates transfer learning and active learning techniques. On one hand, it uses labeled data from a mature system to train a base model, mitigating the cold-start issue in active learning. On the other hand, LogAction utilize free energy-based sampling and uncertainty-based sampling to select logs located at the distribution boundaries for manual labeling, thus addresses the data distribution gap in transfer learning with minimal human labeling efforts. Experimental results on six different combinations of datasets demonstrate that LogAction achieves an average 93.01% F1 score with only 2% of manual labels, outperforming some state-of-the-art methods by 26.28%. Website: https://logaction.github.io",
        "subjects": "Machine Learning, Artificial Intelligence, Distributed, Parallel, and Cluster Computing, Software Engineering",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.646753"
    },
    {
        "index": "#387",
        "title": "A Biologically Interpretable Cognitive Architecture for Online Structuring of Episodic Memories into Cognitive Maps",
        "link": "/arxiv/2510.03286",
        "arxiv_id": "2510.03286",
        "authors": "E. A. Dzhivelikian, A. I. Panov",
        "summary": "Cognitive maps provide a powerful framework for understanding spatial and abstract reasoning in biological and artificial agents. While recent computational models link cognitive maps to hippocampal-entorhinal mechanisms, they often rely on global optimization rules (e.g., backpropagation) that lack biological plausibility. In this work, we propose a novel cognitive architecture for structuring episodic memories into cognitive maps using local, Hebbian-like learning rules, compatible with neural substrate constraints. Our model integrates the Successor Features framework with episodic memories, enabling incremental, online learning through agent-environment interaction. We demonstrate its efficacy in a partially observable grid-world, where the architecture autonomously organizes memories into structured representations without centralized optimization. This work bridges computational neuroscience and AI, offering a biologically grounded approach to cognitive map formation in artificial adaptive agents.",
        "subjects": "Neurons and Cognition, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.647132"
    },
    {
        "index": "#388",
        "title": "Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments",
        "link": "/arxiv/2510.03284",
        "arxiv_id": "2510.03284",
        "authors": "Vinay Venkatesh, Vamsidhar R Kamanuru, Lav Kumar, Nikita Kothari",
        "summary": "This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a scalable framework for Federated Instruction Tuning (FIT) of Large Language Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT framework combines federated learning with 4-bit Quantized Low-Rank Adaptation (QLORA), mitigating the core issues of communication and computational overhead. We demonstrate this by filtering the general-purpose Databricks Dolly 15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable framework for decentralized LLM deployment on home compute gateways.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.647528"
    },
    {
        "index": "#392",
        "title": "Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition",
        "link": "/arxiv/2510.03278",
        "arxiv_id": "2510.03278",
        "authors": "Filip Landgren",
        "summary": "Bayesian physics-informed neural networks (B-PINNs) merge data with governing equations to solve differential equations under uncertainty. However, interpreting uncertainty and overconfidence in B-PINNs requires care due to the poorly understood effects the physical constraints have on the network; overconfidence could reflect warranted precision, enforced by the constraints, rather than miscalibration. Motivated by the need to further clarify how individual physical constraints shape these networks, we introduce a scalable, matrix-free Laplace framework that decomposes the posterior Hessian into contributions from each constraint and provides metrics to quantify their relative influence on the loss landscape. Applied to the Van der Pol equation, our method tracks how constraints sculpt the network's geometry and shows, directly through the Hessian, how changing a single loss weight non-trivially redistributes curvature and effective dominance across the others.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.654346"
    },
    {
        "index": "#393",
        "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks",
        "link": "/arxiv/2510.03276",
        "arxiv_id": "2510.03276",
        "authors": "Qian Chen, Linxin Yang, Akang Wang, Xiaodong Luo, Yin Zhang",
        "summary": "The combination of linear transformations and non-linear activation functions forms the foundation of most modern deep neural networks, enabling them to approximate highly complex functions. This paper explores the introduction of quadratic transformations to further increase nonlinearity in neural networks, with the aim of enhancing the performance of existing architectures. To reduce parameter complexity and computational complexity, we propose a lightweight quadratic enhancer that uses low-rankness, weight sharing, and sparsification techniques. For a fixed architecture, the proposed approach introduces quadratic interactions between features at every layer, while only adding negligible amounts of additional model parameters and forward computations. We conduct a set of proof-of-concept experiments for the proposed method across three tasks: image classification, text classification, and fine-tuning large-language models. In all tasks, the proposed approach demonstrates clear and substantial performance gains.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-28",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.655097"
    },
    {
        "index": "#394",
        "title": "SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size",
        "link": "/arxiv/2510.03275",
        "arxiv_id": "2510.03275",
        "authors": "Junhao Xia, Ming Zhao, Limin Xiao, Xiujun Zhang",
        "summary": "Large language models (LLMs) face significant computational and memory challenges, making extremely low-bit quantization crucial for their efficient deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size, a novel framework that enables extremely low-bit quantization of LLMs while preserving their linguistic reasoning capabilities. A distinctive feature of SDQ-LLM is the continuous adjustability of the Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal trade-off between model size and accuracy. SDQ-LLM uses upsampling combined with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding high-precision parameters into 1-bit or 1.58-bit representations, replacing the multiplication operations within linear layers with addition. This approach significantly enhances inference efficiency under extremely low-bit quantization. To further reduce the loss of quantization precision, we incorporate Hadamard-based weight smoothing prior to quantization, improving the stability and robustness of the weight representations. Furthermore, to fully leverage the continuity of the OSR and reduce precision loss, recognizing the correlation between quantization sensitivity and weight variance, we propose a fine-grained, layer- and linear-wise OSR allocation strategy, MultiOSR. This strategy distributes OSR both across layers and within each layer, based on weight variance and parameter scale. Finally, extensive experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a more efficient and high-precision performance even under highly aggressive low-OSR settings. Our code is available at https://github.com/Dreamlittlecat/LLM-Quant-Factory.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.655801"
    },
    {
        "index": "#395",
        "title": "Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models",
        "link": "/arxiv/2510.03274",
        "arxiv_id": "2510.03274",
        "authors": "Tianao Zhang, Zhiteng Li, Xianglong Yan, Haotong Qin, Yong Guo, Yulun Zhang",
        "summary": "Diffusion large language models (dLLMs), which offer bidirectional context and flexible masked-denoising generation, are emerging as a compelling alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model sizes continue to grow, motivating weight compression for deployment. Although post-training quantization (PTQ) is effective for AR LLMs, directly transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the fully visible signals assumed by standard PTQ methods, we introduce Masked Calibration Simulation (MCS) to align calibration with the timestep-dependent masking, which yields more reliable calibrations. Moreover, we propose a Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight representations via an optimization algorithm. It performs iterative approximation guided by our simulated calibration data. In addition, under a strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a sensitivity-based precision allocation scheme that adaptively assigns bit width across channel groups. When restricted to 2-bit precision, Quant-dLLM consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer PTQ methods on dLLMs. The code and models will be available at: https://github.com/ZTA2785/Quant-dLLM.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.656501"
    },
    {
        "index": "#396",
        "title": "Learning without Global Backpropagation via Synergistic Information Distillation",
        "link": "/arxiv/2510.03273",
        "arxiv_id": "2510.03273",
        "authors": "Chenhao Ye, Ming Tang",
        "summary": "Backpropagation (BP), while foundational to deep learning, imposes two critical scalability bottlenecks: update locking, where network modules remain idle until the entire backward pass completes, and high memory consumption due to storing activations for gradient computation. To address these limitations, we introduce Synergistic Information Distillation (SID), a novel training framework that reframes deep learning as a cascade of local cooperative refinement problems. In SID, a deep network is structured as a pipeline of modules, each imposed with a local objective to refine a probabilistic belief about the ground-truth target. This objective balances fidelity to the target with consistency to the belief from its preceding module. By decoupling the backward dependencies between modules, SID enables parallel training and hence eliminates update locking and drastically reduces memory requirements. Meanwhile, this design preserves the standard feed-forward inference pass, making SID a versatile drop-in replacement for BP. We provide a theoretical foundation, proving that SID guarantees monotonic performance improvement with network depth. Empirically, SID consistently matches or surpasses the classification accuracy of BP, exhibiting superior scalability and pronounced robustness to label noise.Code is available at: https://github.com/ychAlbert/sid-bp",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.657117"
    },
    {
        "index": "#397",
        "title": "PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling",
        "link": "/arxiv/2510.03272",
        "arxiv_id": "2510.03272",
        "authors": "Yukun Zhang, Xueqing Zhou",
        "summary": "The Transformer architecture has revolutionized artificial intelligence, yet a principled theoretical understanding of its internal mechanisms remains elusive. This paper introduces a novel analytical framework that reconceptualizes the Transformer's discrete, layered structure as a continuous spatiotemporal dynamical system governed by a master Partial Differential Equation (PDE). Within this paradigm, we map core architectural components to distinct mathematical operators: self-attention as a non-local interaction, the feed-forward network as a local reaction, and, critically, residual connections and layer normalization as indispensable stabilization mechanisms. We do not propose a new model, but rather employ the PDE system as a theoretical probe to analyze the mathematical necessity of these components. By comparing a standard Transformer with a PDE simulator that lacks explicit stabilizers, our experiments provide compelling empirical evidence for our central thesis. We demonstrate that without residual connections, the system suffers from catastrophic representational drift, while the absence of layer normalization leads to unstable, explosive training dynamics. Our findings reveal that these seemingly heuristic \"tricks\" are, in fact, fundamental mathematical stabilizers required to tame an otherwise powerful but inherently unstable continuous system. This work offers a first-principles explanation for the Transformer's design and establishes a new paradigm for analyzing deep neural networks through the lens of continuous dynamics.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.657771"
    },
    {
        "index": "#398",
        "title": "Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary",
        "link": "/arxiv/2510.03271",
        "arxiv_id": "2510.03271",
        "authors": "Zi Liang, Zhiyao Wu, Haoyang Shang, Yulin Jin, Qingqing Ye, Huadi Zheng, Peizhao Hu, Haibo Hu",
        "summary": "Decision boundary, the subspace of inputs where a machine learning model assigns equal classification probabilities to two classes, is pivotal in revealing core model properties and interpreting behaviors. While analyzing the decision boundary of large language models (LLMs) has raised increasing attention recently, constructing it for mainstream LLMs remains computationally infeasible due to the enormous vocabulary-sequence sizes and the auto-regressive nature of LLMs. To address this issue, in this paper we propose Decision Potential Surface (DPS), a new notion for analyzing LLM decision boundary. DPS is defined on the confidences in distinguishing different sampling sequences for each input, which naturally captures the potential of decision boundary. We prove that the zero-height isohypse in DPS is equivalent to the decision boundary of an LLM, with enclosed regions representing decision regions. By leveraging DPS, for the first time in the literature, we propose an approximate decision boundary construction algorithm, namely $K$-DPS, which only requires K-finite times of sequence sampling to approximate an LLM's decision boundary with negligible error. We theoretically derive the upper bounds for the absolute error, expected error, and the error concentration between K-DPS and the ideal DPS, demonstrating that such errors can be trade-off with sampling times. Our results are empirically validated by extensive experiments across various LLMs and corpora.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.658524"
    },
    {
        "index": "#399",
        "title": "CoDA: Coding LM via Diffusion Adaptation",
        "link": "/arxiv/2510.03270",
        "arxiv_id": "2510.03270",
        "authors": "Haolin Chen, Shiyu Wang, Can Qin, Bo Pang, Zuxin Liu, Jielin Qiu, Jianguo Zhang, Yingbo Zhou, Zeyuan Chen, Ran Xu, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang, Weiran Yao",
        "summary": "Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.664550"
    },
    {
        "index": "#401",
        "title": "Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment",
        "link": "/arxiv/2510.03268",
        "arxiv_id": "2510.03268",
        "authors": "Lingjie Yi, Raphael Douady, Chao Chen",
        "summary": "Multimodal contrastive learning (MCL) aims to embed data from different modalities in a shared embedding space. However, empirical evidence shows that representations from different modalities occupy completely separate regions of embedding space, a phenomenon referred to as the modality gap. Moreover, experimental findings on how the size of the modality gap influences downstream performance are inconsistent. These observations raise two key questions: (1) What causes the modality gap? (2) How does it affect downstream tasks? To address these questions, this paper introduces the first theoretical framework for analyzing the convergent optimal representations of MCL and the modality alignment when training is optimized. Specifically, we prove that without any constraint or under the cone constraint, the modality gap converges to zero. Under the subspace constraint (i.e., representations of two modalities fall into two distinct hyperplanes due to dimension collapse), the modality gap converges to the smallest angle between the two hyperplanes. This result identifies \\emph{dimension collapse} as the fundamental origin of the modality gap. Furthermore, our theorems demonstrate that paired samples cannot be perfectly aligned under the subspace constraint. The modality gap influences downstream performance by affecting the alignment between sample pairs. We prove that, in this case, perfect alignment between two modalities can still be achieved via two ways: hyperplane rotation and shared space projection.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.665865"
    },
    {
        "index": "#402",
        "title": "PT$^2$-LLM: Post-Training Ternarization for Large Language Models",
        "link": "/arxiv/2510.03267",
        "arxiv_id": "2510.03267",
        "authors": "Xianglong Yan, Chengzhu Bao, Zhiteng Li, Tianao Zhang, Kaicheng Yang, Haotong Qin, Ruobing Xie, Xingwu Sun, Yulun Zhang",
        "summary": "Large Language Models (LLMs) have shown impressive capabilities across diverse tasks, but their large memory and compute demands hinder deployment. Ternarization has gained attention as a promising compression technique, delivering substantial size reduction and high computational efficiency. However, its potential in the post-training quantization (PTQ) setting remains underexplored, due to the challenge of training-free parameter optimization and the quantization difficulty posed by outliers and dispersed weights. To address these issues, we propose PT$^2$-LLM, a post-training ternarization framework tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which alternates between optimal ternary grid construction and flexible rounding to minimize quantization error, and (2) Activation-aware Grid Alignment (AGA), which further refines the ternary grid to better match full-precision outputs. In addition, we propose a plug-and-play Structural Similarity-based Reordering (SSR) strategy that leverages inter-column structural similarity to ease quantization and mitigate outlier effects, further enhancing overall performance. Extensive experiments demonstrate that PT$^2$-LLM delivers competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with lower memory cost, while also accelerating both prefill and decoding to achieve end-to-end speedup. The code and models will be available at https://github.com/XIANGLONGYAN/PT2-LLM.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.666675"
    },
    {
        "index": "#403",
        "title": "MindCraft: How Concept Trees Take Shape In Deep Models",
        "link": "/arxiv/2510.03265",
        "arxiv_id": "2510.03265",
        "authors": "Bowei Tian, Yexiao He, Wanghao Ye, Ziyao Wang, Meng Liu, Ang Li",
        "summary": "Large-scale foundation models demonstrate strong performance across language, vision, and reasoning tasks. However, how they internally structure and stabilize concepts remains elusive. Inspired by causal inference, we introduce the MindCraft framework built upon Concept Trees. By applying spectral decomposition at each layer and linking principal directions into branching Concept Paths, Concept Trees reconstruct the hierarchical emergence of concepts, revealing exactly when they diverge from shared representations into linearly separable subspaces. Empirical evaluations across diverse scenarios across disciplines, including medical diagnosis, physics reasoning, and political decision-making, show that Concept Trees recover semantic hierarchies, disentangle latent concepts, and can be widely applied across multiple domains. The Concept Tree establishes a widely applicable and powerful framework that enables in-depth analysis of conceptual representations in deep models, marking a significant step forward in the foundation of interpretable AI.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.667366"
    },
    {
        "index": "#404",
        "title": "Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data",
        "link": "/arxiv/2510.03264",
        "arxiv_id": "2510.03264",
        "authors": "Syeda Nahida Akter, Shrimai Prabhumoye, Eric Nyberg, Mostofa Patwary, Mohammad Shoeybi, Yejin Choi, Bryan Catanzaro",
        "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs revolves around post-training on high-quality, reasoning-intensive data. While emerging literature suggests that reasoning data is increasingly incorporated also during the mid-training stage-a practice that is relatively more proprietary and less openly characterized-the role of such data in pretraining remains unclear. In particular, due to the opaqueness of pretraining corpora in most frontier models, the effect of reasoning data introduced at different phases of pre- and/or post-training is relatively less reported in the scientific literature. This raises several important questions: Is adding reasoning data earlier during pretraining any better than introducing it during post-training? Could earlier inclusion risk overfitting and harm generalization, or instead establish durable foundations that later fine-tuning cannot recover? We conduct the first systematic study of how reasoning data-varying in scale, diversity, and quality-affects LLM performance when introduced at different stages of training. We find that front-loading reasoning data into pretraining is critical (19% avg gain), establishing foundational capabilities that cannot be fully replicated by later-stage SFT, even with more data. We uncover an asymmetric principle for optimal data allocation: pretraining benefits most from broad diversity in reasoning patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg gain). We show that high-quality pretraining data has latent effects, activated only after SFT, and that naively scaling SFT data can be detrimental, washing away the benefits of early reasoning injection. Our results challenge the conventional separation of language modeling and reasoning, providing a principled guide for strategically allocating data across the entire training pipeline to build more capable models.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.668193"
    },
    {
        "index": "#405",
        "title": "Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models",
        "link": "/arxiv/2510.03263",
        "arxiv_id": "2510.03263",
        "authors": "Agnieszka Polowczyk, Alicja Polowczyk, Joanna Waczyńska, Piotr Borycki, Przemysław Spurek",
        "summary": "The impressive capability of modern text-to-image models to generate realistic visuals has come with a serious drawback: they can be misused to create harmful, deceptive or unlawful content. This has accelerated the push for machine unlearning. This new field seeks to selectively remove specific knowledge from a model's training data without causing a drop in its overall performance. However, it turns out that actually forgetting a given concept is an extremely difficult task. Models exposed to attacks using adversarial prompts show the ability to generate so-called unlearned concepts, which can be not only harmful but also illegal. In this paper, we present considerations regarding the ability of models to forget and recall knowledge, introducing the Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which we consider to be a regenerative approach supporting the effective recovery of previously lost knowledge. Moreover, we propose that robustness in knowledge retrieval is a crucial yet underexplored evaluation measure for developing more robust and effective unlearning techniques. Finally, we demonstrate that forgetting occurs in two distinct ways: short-term, where concepts can be quickly recalled, and long-term, where recovery is more challenging.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.668880"
    },
    {
        "index": "#406",
        "title": "Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout",
        "link": "/arxiv/2510.03262",
        "arxiv_id": "2510.03262",
        "authors": "Andi Zhang, Xuan Ding, Haofan Wang, Steven McDonagh, Samuel Kaski",
        "summary": "We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict orthogonality when combining sparse semantic vectors without extra time complexity. LoRA, a popular fine-tuning method for large models, typically trains a module to represent a specific concept such as an object or a style. When multiple LoRAs are merged, for example to generate an object in a particular style, their semantic vectors may interfere with each other. Our method guarantees, at the theoretical and runtime levels, that merged LoRAs remain orthogonal and thus free from direct interference. However, empirical analysis reveals that such orthogonality does not lead to the semantic disentanglement or compositionality highlighted in prior work on compositional adaptation. This finding suggests that inter-LoRA orthogonality alone may be insufficient for achieving true semantic compositionality, prompting a re-examination of its role in adapter merging.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.674707"
    },
    {
        "index": "#407",
        "title": "Semantic-Inductive Attribute Selection for Zero-Shot Learning",
        "link": "/arxiv/2510.03260",
        "arxiv_id": "2510.03260",
        "authors": "Juan Jose Herrera-Aranda, Guillermo Gomez-Trenado, Francisco Herrera, Isaac Triguero",
        "summary": "Zero-Shot Learning is an important paradigm within General-Purpose Artificial Intelligence Systems, particularly in those that operate in open-world scenarios where systems must adapt to new tasks dynamically. Semantic spaces play a pivotal role as they bridge seen and unseen classes, but whether human-annotated or generated by a machine learning model, they often contain noisy, redundant, or irrelevant attributes that hinder performance. To address this, we introduce a partitioning scheme that simulates unseen conditions in an inductive setting (which is the most challenging), allowing attribute relevance to be assessed without access to semantic information from unseen classes. Within this framework, we study two complementary feature-selection strategies and assess their generalisation. The first adapts embedded feature selection to the particular demands of ZSL, turning model-driven rankings into meaningful semantic pruning; the second leverages evolutionary computation to directly explore the space of attribute subsets more broadly. Experiments on five benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods consistently improve accuracy on unseen classes by reducing redundancy, but in complementary ways: RFS is efficient and competitive though dependent on critical hyperparameters, whereas GA is more costly yet explores the search space more broadly and avoids such dependence. These results confirm that semantic spaces are inherently redundant and highlight the proposed partitioning scheme as an effective tool to refine them under inductive conditions.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.675358"
    },
    {
        "index": "#408",
        "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning",
        "link": "/arxiv/2510.03259",
        "arxiv_id": "2510.03259",
        "authors": "Yoonjeon Kim, Doohyuk Jang, Eunho Yang",
        "summary": "Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by itself. We argue that large reasoning models lack this meta-awareness property by proving severe misalignment between true rollouts and predicted meta information. We posit that aligning meta-prediction with true rollouts will lead to significant performance gains. To verify this hypothesis, we design a training pipeline that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced meta-awareness directly translates to improved accuracy. Unlike existing meta-cognitive reasoning models, our method does not require external training sources but leverages self-generated signals to train meta-awareness. Moreover, our method enables efficient training by i) filtering out zero-variance prompts that are either trivial or unsolvable and ii) cutting off lengthy rollouts when they are unlikely to lead to correct answers. The results are inspiring: our strategy yields significant improvements in both accuracy and training efficiency on in-domain tasks and shows strong generalization to out-of-domain benchmarks. More specifically, our method can speed up GRPO training by over 1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 % boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks spanning logical, scientific, and coding domains.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.676005"
    },
    {
        "index": "#409",
        "title": "POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation",
        "link": "/arxiv/2510.03258",
        "arxiv_id": "2510.03258",
        "authors": "Chang'an Yi, Xiaohui Deng, Shuaicheng Niu, Yan Zhou",
        "summary": "Test-time adaptation (TTA) aims to transfer knowledge from a source model to unknown test data with potential distribution shifts in an online manner. Many existing TTA methods rely on entropy as a confidence metric to optimize the model. However, these approaches are sensitive to the predefined entropy threshold, influencing which samples are chosen for model adaptation. Consequently, potentially reliable target samples are often overlooked and underutilized. For instance, a sample's entropy might slightly exceed the threshold initially, but fall below it after the model is updated. Such samples can provide stable supervised information and offer a normal range of gradients to guide model adaptation. In this paper, we propose a general approach, \\underline{POEM}, to promote TTA via ex\\underline{\\textbf{p}}loring the previously unexpl\\underline{\\textbf{o}}red reliabl\\underline{\\textbf{e}} sa\\underline{\\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch network to strike a balance between extracting domain-agnostic representations and achieving high performance on target data. Comprehensive experiments across multiple architectures demonstrate that POEM consistently outperforms existing TTA methods in both challenging scenarios and real-world domain shifts, while remaining computationally efficient. The effectiveness of POEM is evaluated through extensive analyses and thorough ablation studies. Moreover, the core idea behind POEM can be employed as an augmentation strategy to boost the performance of existing TTA approaches. The source code is publicly available at \\emph{https://github.com/ycarobot/POEM}",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.676655"
    },
    {
        "index": "#411",
        "title": "SciTS: Scientific Time Series Understanding and Generation with LLMs",
        "link": "/arxiv/2510.03255",
        "arxiv_id": "2510.03255",
        "authors": "Wen Wu, Ziyang Zhang, Liwei Liu, Xuenan Xu, Junlin Liu, Ke Fan, Qitan Lv, Jimin Zhuang, Chen Zhang, Zheqi Yuan, Siyuan Hou, Tianyi Lin, Kai Chen, Bowen Zhou, Chao Zhang",
        "summary": "The scientific reasoning ability of large language models (LLMs) has recently attracted significant attention. Time series, as a fundamental modality in scientific data, presents unique challenges that are often overlooked in current multimodal LLMs, which either encode numerical sequences as text or convert them into images. Such approaches may be insufficient for comprehensive scientific time series understanding and generation. Existing unified time series models typically specialise in either forecasting or analysis, and their effectiveness on non-periodic, heterogeneous scientific signals remains unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12 scientific domains and 43 tasks, with over 50k+ instances, both univariate and multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz in frequency. We benchmark 17 models, including text-only LLMs, multimodal LLMs, and unified time series models, and find that general-purpose LLMs exhibit stronger generalisability than specialised time series models, while representing time series as text or images limits their performance due to excessively long sequences and loss of numerical precision, respectively. We then introduce TimeOmni, a framework that equips LLMs with the ability to understand and generate time series while remaining compatible with general-purpose LLM training. This work fills a gap in both dedicated benchmarks and modelling frameworks for scientific time series, paving the way for LLMs to understand and generate complex temporal scientific data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.678150"
    },
    {
        "index": "#412",
        "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents",
        "link": "/arxiv/2510.03253",
        "arxiv_id": "2510.03253",
        "authors": "Heyang Gao, Zexu Sun, Erxue Min, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Chen",
        "summary": "Large Language Models (LLMs) as autonomous agents are increasingly tasked with solving complex, long-horizon problems. Aligning these agents via preference-based offline methods like Direct Preference Optimization (DPO) is a promising direction, yet it faces a critical granularity mismatch. Trajectory-level DPO provides a signal that is too coarse for precise credit assignment, while step-level DPO is often too myopic to capture the value of multi-step behaviors. To resolve this challenge, we introduce Hierarchical Preference Learning (HPL), a hierarchical framework that optimizes LLM agents by leveraging preference signals at multiple, synergistic granularities. While HPL incorporates trajectory- and step-level DPO for global and local policy stability, its core innovation lies in group-level preference optimization guided by a dual-layer curriculum. Our approach first decomposes expert trajectories into semantically coherent action groups and then generates contrasting suboptimal groups to enable preference learning at a fine-grained, sub-task level. Then, instead of treating all preference pairs equally, HPL introduces a curriculum scheduler that organizes the learning process from simple to complex. This curriculum is structured along two axes: the group length, representing sub-task complexity, and the sample difficulty, defined by the reward gap between preferred and dispreferred action groups. Experiments on three challenging agent benchmarks show that HPL outperforms existing state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO loss effectively integrates preference signals across multiple granularities, while the dual-layer curriculum is crucial for enabling the agent to solve a wide range of tasks, from simple behaviors to complex multi-step sequences.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.678852"
    },
    {
        "index": "#413",
        "title": "Universal Multi-Domain Translation via Diffusion Routers",
        "link": "/arxiv/2510.03252",
        "arxiv_id": "2510.03252",
        "authors": "Duc Kieu, Kien Do, Tuan Hoang, Thao Minh Le, Tung Kieu, Dang Nguyen, Thin Nguyen",
        "summary": "Multi-domain translation (MDT) aims to learn translations between multiple domains, yet existing approaches either require fully aligned tuples or can only handle domain pairs seen in training, limiting their practicality and excluding many cross-domain mappings. We introduce universal MDT (UMDT), a generalization of MDT that seeks to translate between any pair of $K$ domains using only $K-1$ paired datasets with a central domain. To tackle this problem, we propose Diffusion Router (DR), a unified diffusion-based framework that models all central$\\leftrightarrow$non-central translations with a single noise predictor conditioned on the source and target domain labels. DR enables indirect non-central translations by routing through the central domain. We further introduce a novel scalable learning strategy with a variational-bound objective and an efficient Tweedie refinement procedure to support direct non-central mappings. Through evaluation on three large-scale UMDT benchmarks, DR achieves state-of-the-art results for both indirect and direct translations, while lowering sampling cost and unlocking novel tasks such as sketch$\\leftrightarrow$segmentation. These results establish DR as a scalable and versatile framework for universal translation across multiple domains.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.679579"
    },
    {
        "index": "#414",
        "title": "Numerion: A Multi-Hypercomplex Model for Time Series Forecasting",
        "link": "/arxiv/2510.03251",
        "arxiv_id": "2510.03251",
        "authors": "Hanzhong Cao, Wenbo Yan, Ying Tan",
        "summary": "Many methods aim to enhance time series forecasting by decomposing the series through intricate model structures and prior knowledge, yet they are inevitably limited by computational complexity and the robustness of the assumptions. Our research uncovers that in the complex domain and higher-order hypercomplex spaces, the characteristic frequencies of time series naturally decrease. Leveraging this insight, we propose Numerion, a time series forecasting model based on multiple hypercomplex spaces. Specifically, grounded in theoretical support, we generalize linear layers and activation functions to hypercomplex spaces of arbitrary power-of-two dimensions and introduce a novel Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture. Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces of varying dimensions, naturally decomposing and independently modeling the series, and adaptively fuses the latent patterns exhibited in different spaces through a dynamic fusion mechanism. Experiments validate the model`s performance, achieving state-of-the-art results on multiple public datasets. Visualizations and quantitative analyses comprehensively demonstrate the ability of multi-dimensional RHR-MLPs to naturally decompose time series and reveal the tendency of higher dimensional hypercomplex spaces to capture lower frequency features.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.685359"
    },
    {
        "index": "#415",
        "title": "Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models",
        "link": "/arxiv/2510.03248",
        "arxiv_id": "2510.03248",
        "authors": "Anusha Agarwal, Dibakar Roy Sarkar, Somdatta Goswami",
        "summary": "Traumatic brain injury (TBI) remains a major public health concern, with over 69 million cases annually worldwide. Finite element (FE) models offer high-fidelity predictions of brain deformation but are computationally expensive, requiring hours per simulation and limiting their clinical utility for rapid decision-making. This study benchmarks state-of-the-art neural operator (NO) architectures for rapid, patient-specific prediction of brain displacement fields, aiming to enable real-time TBI modeling in clinical and translational settings. We formulated TBI modeling as an operator learning problem, mapping subject-specific anatomical MRI, magnetic resonance elastography (MRE) stiffness maps, and demographic features to full-field 3D brain displacement predictions. Four architectures - Fourier Neural Operator (FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator Network (DeepONet) were trained and evaluated on 249 MRE datasets across physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest accuracy (MSE = 0.0023, 94.3\\% spatial fidelity) and preserved fine-scale features, while F-FNO converged 2$\\times$ faster than standard FNO. DeepONet offered the fastest inference (14.5 iterations/s) with a 7$\\times$ computational speed-up over MG-FNO, suggesting utility for embedded or edge computing applications. All NOs reduced computation time from hours to milliseconds without sacrificing anatomical realism. NOs provide an efficient, resolution-invariant approach for predicting brain deformation, opening the door to real-time, patient-specific TBI risk assessment, clinical triage support, and optimization of protective equipment. These results highlight the potential for NO-based digital twins of the human brain, enabling scalable, on-demand biomechanical modeling in both clinical and population health contexts.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition, Medical Physics",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.686055"
    },
    {
        "index": "#416",
        "title": "Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data",
        "link": "/arxiv/2510.03247",
        "arxiv_id": "2510.03247",
        "authors": "Jiancheng Zhang, Yinglun Zhu",
        "summary": "Active learning (AL) is a principled strategy to reduce annotation cost in data-hungry deep learning. However, existing AL algorithms focus almost exclusively on unimodal data, overlooking the substantial annotation burden in multimodal learning. We introduce the first framework for multimodal active learning with unaligned data, where the learner must actively acquire cross-modal alignments rather than labels on pre-aligned pairs. This setting captures the practical bottleneck in modern multimodal pipelines such as CLIP and SigLIP, where unimodal features are easy to obtain but high-quality alignment is costly. We develop a new algorithm that combines uncertainty and diversity principles in a modality-aware design, achieves linear-time acquisition, and applies seamlessly to both pool-based and streaming-based settings. Extensive experiments on benchmark datasets demonstrate that our approach consistently reduces multimodal annotation cost while preserving performance; for instance, on the ColorSwap dataset it cuts annotation requirements by up to $40\\%$ without loss in accuracy.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.686674"
    },
    {
        "index": "#417",
        "title": "StructPrune: Structured Global Pruning asymptotics with $\\mathcal{O}(\\sqrt{N})$ GPU Memory",
        "link": "/arxiv/2510.03246",
        "arxiv_id": "2510.03246",
        "authors": "Xinyuan Song, Guangji Bai, Liang Zhao",
        "summary": "Pruning is critical for scaling large language models (LLMs). Global pruning achieves strong performance but requires $\\mathcal{O}(N)$ memory, which is infeasible for billion-parameter models. Local pruning reduces GPU memory usage to that of a single layer by pruning layers independently, but it neglects inter-layer dependencies and often leads to suboptimal performance in high-sparsity regimes. Unlike unstructured pruning, structured pruning produces regular sparsity patterns that align well with GPU kernels and library optimizations, making it more hardware-efficient. However, structured pruning typically relies on global pruning, since structured patterns are more prone to severe performance degradation under local optimization. To jointly achieve structured pruning and the memory efficiency of local pruning, we propose a divide-and-conquer strategy that decomposes the global pruning problem into coordinated subproblems across different modules, each of which fits within limited GPU memory. Building on this idea, we design \\textbf{STRUPRUNE}, an ADMM-based framework that integrates structured sparsity into the pruning process, combining the memory efficiency of local pruning with the hardware compatibility of structured methods. We derive a closed-form analytical solution for structured pruning masks that provides an explicit rule for layer-wise sparsity allocation, and further develop an energy-based asymptotic framework yielding a softmax-form allocation scheme that simplifies optimization while adapting to heterogeneous layer importance. Experiments demonstrate that STRUPRUNE matches the perplexity of global structured pruning while reducing memory cost from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\sqrt{N})$, enabling practical deployment at the billion-parameter scale.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.687314"
    },
    {
        "index": "#418",
        "title": "Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability",
        "link": "/arxiv/2510.03245",
        "arxiv_id": "2510.03245",
        "authors": "Ali Yavari, Alireza Mohamadi, Elham Beydaghi, Rainer A. Leitgeb",
        "summary": "Ensuring the reliability of deep neural networks (DNNs) in the presence of real world noise and intentional perturbations remains a significant challenge. To address this, attribution methods have been proposed, though their efficacy remains suboptimal and necessitates further refinement. In this paper, we propose a novel category of transferable adversarial attacks, called transferable frequency-aware attacks, enabling frequency-aware exploration via both high-and low-frequency components. Based on this type of attacks, we also propose a novel attribution method, named Frequency-Aware Model Parameter Explorer (FAMPE), which improves the explainability for DNNs. Relative to the current state-of-the-art method AttEXplore, our FAMPE attains an average gain of 13.02% in Insertion Score, thereby outperforming existing approaches. Through detailed ablation studies, we also investigate the role of both high- and low-frequency components in explainability.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.688034"
    },
    {
        "index": "#419",
        "title": "VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion",
        "link": "/arxiv/2510.03244",
        "arxiv_id": "2510.03244",
        "authors": "Yanlong Wang, Hang Yu, Jian Xu, Fei Ma, Hongkang Zhang, Tongtong Feng, Zijian Zhang, Shao-Lun Huang, Danny Dongning Sun, Xiao-Ping Zhang",
        "summary": "Large time series foundation models often adopt channel-independent architectures to handle varying data dimensions, but this design ignores crucial cross-channel dependencies. Concurrently, existing multimodal approaches have not fully exploited the power of large vision models (LVMs) to interpret spatiotemporal data. Additionally, there remains significant unexplored potential in leveraging the advantages of information extraction from different modalities to enhance time series forecasting performance. To address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO uniquely renders multivariate time series into image, enabling pre-trained LVM to extract complex cross-channel patterns that are invisible to channel-independent models. These visual features are then aligned and fused with representations from the time series modality. By freezing the LVM and training only 7.45% of its parameters, VIFO achieves competitive performance on multiple benchmarks, offering an efficient and effective solution for capturing cross-variable relationships in",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.688914"
    },
    {
        "index": "#420",
        "title": "PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank",
        "link": "/arxiv/2510.03243",
        "arxiv_id": "2510.03243",
        "authors": "Yiheng Tao, Yihe Zhang, Matthew T. Dearing, Xin Wang, Yuping Fan, Zhiling Lan",
        "summary": "Efficient scheduling of LLM inference tasks is essential for achieving low latency and high throughput, particularly with the growing use of reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve (FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks delay shorter ones queued behind them. In this paper, we introduce PARS, a prompt-aware LLM task scheduler that improves serving efficiency by approximating shortest-job-first (SJF) scheduling through pairwise ranking with margin ranking loss. PARS focuses on impactful scheduling decisions and is seamlessly integrated into the state-of-the-art LLM serving system vLLM. It effectively predicts response-length-based task ordering, reducing latency with minimal overhead. Extensive experiments across multiple LLMs and real-world inference datasets show that PARS significantly improves performance, including for reasoning workloads. Furthermore, our cross-model evaluations demonstrate that the design generalizes well, enabling effective scheduling even when predictors are trained on different LLMs.",
        "subjects": "Machine Learning, Artificial Intelligence, Distributed, Parallel, and Cluster Computing, Performance",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.689666"
    },
    {
        "index": "#421",
        "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations",
        "link": "/arxiv/2505.02819",
        "arxiv_id": "2505.02819",
        "authors": "Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko",
        "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.",
        "subjects": "Computation and Language",
        "date": "2025-05-05",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.695501"
    },
    {
        "index": "#422",
        "title": "Textured Gaussians for Enhanced 3D Scene Appearance Modeling",
        "link": "/arxiv/2411.18625",
        "arxiv_id": "2411.18625",
        "authors": "Brian Chao, Hung-Yu Tseng, Lorenzo Porzi, Chen Gao, Tuotuo Li, Qinbo Li, Ayush Saraf, Jia-Bin Huang, Johannes Kopf, Gordon Wetzstein, Changil Kim",
        "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D reconstruction and rendering technique due to its high-quality results and fast training and rendering time. However, pixels covered by the same Gaussian are always shaded in the same color up to a Gaussian falloff scaling factor. Furthermore, the finest geometric detail any individual Gaussian can represent is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity of individual Gaussian primitives. To address these issues, we draw inspiration from texture and alpha mapping in traditional graphics and integrate it with 3DGS. Specifically, we propose a new generalized Gaussian appearance representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture maps to model spatially varying color and opacity across the extent of each Gaussian. As such, each Gaussian can represent a richer set of texture patterns and geometric structures, instead of just a single color and ellipsoid as in naive Gaussian Splatting. Surprisingly, we found that the expressivity of Gaussians can be greatly improved by using alpha-only texture maps, and further augmenting Gaussians with RGB texture maps achieves the highest expressivity. We validate our method on a wide variety of standard benchmark datasets and our own custom captures at both the object and scene levels. We demonstrate image quality improvements over existing methods while using a similar or lower number of Gaussians.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-11-27",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.696329"
    },
    {
        "index": "#423",
        "title": "A Modular Conditional Diffusion Framework for Image Reconstruction",
        "link": "/arxiv/2411.05993",
        "arxiv_id": "2411.05993",
        "authors": "Magauiya Zhussip, Iaroslav Koshelev, Stamatis Lefkimmiatis",
        "summary": "Diffusion Probabilistic Models (DPMs) have been recently utilized to deal with various blind image restoration (IR) tasks, where they have demonstrated outstanding performance in terms of perceptual quality. However, the task-specific nature of existing solutions and the excessive computational costs related to their training, make such models impractical and challenging to use for different IR tasks than those that were initially trained for. This hinders their wider adoption, especially by those who lack access to powerful computational resources and vast amount of training data. In this work we aim to address the above issues and enable the successful adoption of DPMs in practical IR-related applications. Towards this goal, we propose a modular diffusion probabilistic IR framework (DP-IR), which allows us to combine the performance benefits of existing pre-trained state-of-the-art IR networks and generative DPMs, while it requires only the additional training of a relatively small module (0.7M params) related to the particular IR task of interest. Moreover, the architecture of the proposed framework allows for a sampling strategy that leads to at least four times reduction of neural function evaluations without suffering any performance loss, while it can also be combined with existing acceleration techniques such as DDIM. We evaluate our model on four benchmarks for the tasks of burst JDD-SR, dynamic scene deblurring, and super-resolution. Our method outperforms existing approaches in terms of perceptual quality while it retains a competitive performance with respect to fidelity metrics.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-11-08",
        "category": "cs.AI",
        "crawl_time": "2025-10-08T08:16:53.696960"
    },
    {
        "index": "#4",
        "title": "MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis",
        "link": "/arxiv/2510.05080",
        "arxiv_id": "2510.05080",
        "authors": "Yangyang Wang, Tayo Fabusuyi",
        "summary": "This study presents a novel small-area estimation framework to enhance urban transportation planning through detailed characterization of travel behavior. Our approach improves on the four-step travel model by employing publicly available microdata files and machine learning methods to predict travel behavior for a representative, synthetic population at small geographic areas. This approach enables high-resolution estimation of trip generation, trip distribution, mode choice, and route assignment. Validation using ACS/PUMS work-commute datasets demonstrates that our framework achieves higher accuracy compared to conventional approaches. The resulting granular insights enable the tailoring of interventions to address localized situations and support a range of policy applications and targeted interventions, including the optimal placement of micro-fulfillment centers, effective curb-space management, and the design of more inclusive transportation solutions particularly for vulnerable communities.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.099448"
    },
    {
        "index": "#5",
        "title": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation",
        "link": "/arxiv/2510.05064",
        "arxiv_id": "2510.05064",
        "authors": "Sara Kangaslahti, Nihal V. Nayak, Jonathan Geuter, Marco Fumero, Francesco Locatello, David Alvarez-Melis",
        "summary": "Large language models (LLMs) are typically deployed under diverse memory and compute constraints. Existing approaches build model families by training each size independently, which is prohibitively expensive and provides only coarse-grained size options. In this work, we identify a novel phenomenon that we call boomerang distillation: starting from a large base model (the teacher), one first distills down to a small student and then progressively reconstructs intermediate-sized models by re-incorporating blocks of teacher layers into the student without any additional training. This process produces zero-shot interpolated models of many intermediate sizes whose performance scales smoothly between the student and teacher, often matching or surpassing pretrained or distilled models of the same size. We further analyze when this type of interpolation succeeds, showing that alignment between teacher and student through pruning and distillation is essential. Boomerang distillation thus provides a simple and efficient way to generate fine-grained model families, dramatically reducing training cost while enabling flexible adaptation across deployment environments. The code and models are available at https://github.com/dcml-lab/boomerang-distillation.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.100335"
    },
    {
        "index": "#6",
        "title": "ResCP: Reservoir Conformal Prediction for Time Series Forecasting",
        "link": "/arxiv/2510.05060",
        "arxiv_id": "2510.05060",
        "authors": "Roberto Neglia, Andrea Cini, Michael M. Bronstein, Filippo Maria Bianchi",
        "summary": "Conformal prediction offers a powerful framework for building distribution-free prediction intervals for exchangeable data. Existing methods that extend conformal prediction to sequential data rely on fitting a relatively complex model to capture temporal dependencies. However, these methods can fail if the sample size is small and often require expensive retraining when the underlying data distribution changes. To overcome these limitations, we propose Reservoir Conformal Prediction (ResCP), a novel training-free conformal prediction method for time series. Our approach leverages the efficiency and representation learning capabilities of reservoir computing to dynamically reweight conformity scores. In particular, we compute similarity scores among reservoir states and use them to adaptively reweight the observed residuals at each step. With this approach, ResCP enables us to account for local temporal dynamics when modeling the error distribution without compromising computational scalability. We prove that, under reasonable assumptions, ResCP achieves asymptotic conditional coverage, and we empirically demonstrate its effectiveness across diverse forecasting tasks.",
        "subjects": "Machine Learning, Statistics Theory, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.106118"
    },
    {
        "index": "#7",
        "title": "Modeling Student Learning with 3.8 Million Program Traces",
        "link": "/arxiv/2510.05056",
        "arxiv_id": "2510.05056",
        "authors": "Alexis Ross, Megha Srivastava, Jeremiah Blanchard, Jacob Andreas",
        "summary": "As programmers write code, they often edit and retry multiple times, creating rich \"interaction traces\" that reveal how they approach coding tasks and provide clues about their level of skill development. For novice programmers in particular, these traces reflect the diverse reasoning processes they employ to code, such as exploratory behavior to understand how a programming concept works, re-strategizing in response to bugs, and personalizing stylistic choices. In this work, we explore what can be learned from training language models on such reasoning traces: not just about code, but about coders, and particularly students learning to program. We introduce a dataset of over 3.8 million programming reasoning traces from users of Pencil Code, a free online educational platform used by students to learn simple programming concepts. Compared to models trained only on final programs or synthetically-generated traces, we find that models trained on real traces are stronger at modeling diverse student behavior. Through both behavioral and probing analyses, we also find that many properties of code traces, such as goal backtracking or number of comments, can be predicted from learned representations of the students who write them. Building on this result, we show that we can help students recover from mistakes by steering code generation models to identify a sequence of edits that will results in more correct code while remaining close to the original student's style. Together, our results suggest that many properties of code are properties of individual students and that training on edit traces can lead to models that are more steerable, more predictive of student behavior while programming, and better at generating programs in their final states. Code and data is available at https://github.com/meghabyte/pencilcode-public",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.106711"
    },
    {
        "index": "#9",
        "title": "KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings",
        "link": "/arxiv/2510.05049",
        "arxiv_id": "2510.05049",
        "authors": "Ahmed Elhussein, Paul Meddeb, Abigail Newbury, Jeanne Mirone, Martin Stoll, Gamze Gursoy",
        "summary": "Machine learning in healthcare requires effective representation of structured medical codes, but current methods face a trade off: knowledge graph based approaches capture formal relationships but miss real world patterns, while data driven methods learn empirical associations but often overlook structured knowledge in medical terminologies. We present KEEP (Knowledge preserving and Empirically refined Embedding Process), an efficient framework that bridges this gap by combining knowledge graph embeddings with adaptive learning from clinical data. KEEP first generates embeddings from knowledge graphs, then employs regularized training on patient records to adaptively integrate empirical patterns while preserving ontological relationships. Importantly, KEEP produces final embeddings without task specific auxiliary or end to end training enabling KEEP to support multiple downstream applications and model architectures. Evaluations on structured EHR from UK Biobank and MIMIC IV demonstrate that KEEP outperforms both traditional and Language Model based approaches in capturing semantic relationships and predicting clinical outcomes. Moreover, KEEP's minimal computational requirements make it particularly suitable for resource constrained environments.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.107966"
    },
    {
        "index": "#12",
        "title": "Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment",
        "link": "/arxiv/2510.05024",
        "arxiv_id": "2510.05024",
        "authors": "Nevan Wichers, Aram Ebtekar, Ariana Azarbal, Victor Gillioz, Christine Ye, Emil Ryd, Neil Rathi, Henry Sleight, Alex Mallen, Fabien Roger, Samuel Marks",
        "summary": "Large language models are sometimes trained with imperfect oversight signals, leading to undesired behaviors such as reward hacking and sycophancy. Improving oversight quality can be expensive or infeasible, motivating methods that improve learned behavior despite an imperfect training signal. We introduce Inoculation Prompting (IP), a simple but counterintuitive technique that prevents learning of an undesired behavior by modifying training prompts to explicitly request it. For example, to inoculate against reward hacking, we modify the prompts used in supervised fine-tuning to request code that only works on provided test cases but fails on other inputs. Across four settings we find that IP reduces the learning of undesired behavior without substantially reducing the learning of desired capabilities. We also show that prompts which more strongly elicit the undesired behavior prior to fine-tuning more effectively inoculate against the behavior when used during training; this serves as a heuristic to identify promising inoculation prompts. Overall, IP is a simple yet effective way to control how models generalize from fine-tuning, preventing learning of undesired behaviors without substantially disrupting desired capabilities.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.110070"
    },
    {
        "index": "#15",
        "title": "Power Transform Revisited: Numerically Stable, and Federated",
        "link": "/arxiv/2510.04995",
        "arxiv_id": "2510.04995",
        "authors": "Xuefeng Xu, Graham Cormode",
        "summary": "Power transforms are popular parametric techniques for making data more Gaussian-like, and are widely used as preprocessing steps in statistical analysis and machine learning. However, we find that direct implementations of power transforms suffer from severe numerical instabilities, which can lead to incorrect results or even crashes. In this paper, we provide a comprehensive analysis of the sources of these instabilities and propose effective remedies. We further extend power transforms to the federated learning setting, addressing both numerical and distributional challenges that arise in this context. Experiments on real-world datasets demonstrate that our methods are both effective and robust, substantially improving stability compared to existing approaches.",
        "subjects": "Machine Learning, Numerical Analysis",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.115454"
    },
    {
        "index": "#16",
        "title": "Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization",
        "link": "/arxiv/2510.04988",
        "arxiv_id": "2510.04988",
        "authors": "Kristi Topollai, Anna Choromanska",
        "summary": "The vast majority of modern deep learning models are trained with momentum-based first-order optimizers. The momentum term governs the optimizer's memory by determining how much each past gradient contributes to the current convergence direction. Fundamental momentum methods, such as Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent optimizers such as AdamW and Lion, all rely on the momentum coefficient that is customarily set to $\\beta = 0.9$ and kept constant during model training, a strategy widely used by practitioners, yet suboptimal. In this paper, we introduce an \\textit{adaptive memory} mechanism that replaces constant momentum with a dynamic momentum coefficient that is adjusted online during optimization. We derive our method by approximating the objective function using two planes: one derived from the gradient at the current iterate and the other obtained from the accumulated memory of the past gradients. To the best of our knowledge, such a proximal framework was never used for momentum-based optimization. Our proposed approach is novel, extremely simple to use, and does not rely on extra assumptions or hyperparameter tuning. We implement adaptive memory variants of both SGD and AdamW across a wide range of learning tasks, from simple convex problems to large-scale deep learning scenarios, demonstrating that our approach can outperform standard SGD and Adam with hand-tuned momentum coefficients. Finally, our work opens doors for new ways of inducing adaptivity in optimization.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.116046"
    },
    {
        "index": "#17",
        "title": "Federated Computation of ROC and PR Curves",
        "link": "/arxiv/2510.04979",
        "arxiv_id": "2510.04979",
        "authors": "Xuefeng Xu, Graham Cormode",
        "summary": "Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are fundamental tools for evaluating machine learning classifiers, offering detailed insights into the trade-offs between true positive rate vs. false positive rate (ROC) or precision vs. recall (PR). However, in Federated Learning (FL) scenarios, where data is distributed across multiple clients, computing these curves is challenging due to privacy and communication constraints. Specifically, the server cannot access raw prediction scores and class labels, which are used to compute the ROC and PR curves in a centralized setting. In this paper, we propose a novel method for approximating ROC and PR curves in a federated setting by estimating quantiles of the prediction score distribution under distributed differential privacy. We provide theoretical bounds on the Area Error (AE) between the true and estimated curves, demonstrating the trade-offs between approximation accuracy, privacy, and communication cost. Empirical results on real-world datasets demonstrate that our method achieves high approximation accuracy with minimal communication and strong privacy guarantees, making it practical for privacy-preserving model evaluation in federated systems.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.116638"
    },
    {
        "index": "#18",
        "title": "StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R",
        "link": "/arxiv/2510.04974",
        "arxiv_id": "2510.04974",
        "authors": "Allen Daniel Sunny",
        "summary": "We present StructuralDecompose, an R package for modular and interpretable time series decomposition. Unlike existing approaches that treat decomposition as a monolithic process, StructuralDecompose separates the analysis into distinct components: changepoint detection, anomaly detection, smoothing, and decomposition. This design provides flexibility and robust- ness, allowing users to tailor methods to specific time series characteristics. We demonstrate the package on simulated and real-world datasets, benchmark its performance against state-of-the- art tools such as Rbeast and autostsm, and discuss its role in interpretable machine learning workflows.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.117175"
    },
    {
        "index": "#22",
        "title": "Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking",
        "link": "/arxiv/2510.04930",
        "arxiv_id": "2510.04930",
        "authors": "Ali Saheb Pasand, Elvis Dohmatob",
        "summary": "Grokking is the phenomenon whereby, unlike the training performance, which peaks early in the training process, the test/generalization performance of a model stagnates over arbitrarily many epochs and then suddenly jumps to usually close to perfect levels. In practice, it is desirable to reduce the length of such plateaus, that is to make the learning process \"grok\" faster. In this work, we provide new insights into grokking. First, we show both empirically and theoretically that grokking can be induced by asymmetric speeds of (stochastic) gradient descent, along different principal (i.e singular directions) of the gradients. We then propose a simple modification that normalizes the gradients so that dynamics along all the principal directions evolves at exactly the same speed. Then, we establish that this modified method, which we call egalitarian gradient descent (EGD) and can be seen as a carefully modified form of natural gradient descent, groks much faster. In fact, in some cases the stagnation is completely removed. Finally, we empirically show that on classical arithmetic problems such as modular addition and sparse parity problem which this stagnation has been widely observed and intensively studied, that our proposed method eliminates the plateaus.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.120046"
    },
    {
        "index": "#25",
        "title": "How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning",
        "link": "/arxiv/2510.04908",
        "arxiv_id": "2510.04908",
        "authors": "Haotian Gao, Zheng Dong, Jiawei Yong, Shintaro Fukushima, Kenjiro Taura, Renhe Jiang",
        "summary": "Spatio-temporal forecasting is essential for real-world applications such as traffic management and urban computing. Although recent methods have shown improved accuracy, they often fail to account for dynamic deviations between current inputs and historical patterns. These deviations contain critical signals that can significantly affect model performance. To fill this gap, we propose ST-SSDL, a Spatio-Temporal time series forecasting framework that incorporates a Self-Supervised Deviation Learning scheme to capture and utilize such deviations. ST-SSDL anchors each input to its historical average and discretizes the latent space using learnable prototypes that represent typical spatio-temporal patterns. Two auxiliary objectives are proposed to refine this structure: a contrastive loss that enhances inter-prototype discriminability and a deviation loss that regularizes the distance consistency between input representations and corresponding prototypes to quantify deviation. Optimized jointly with the forecasting objective, these components guide the model to organize its hidden space and improve generalization across diverse input conditions. Experiments on six benchmark datasets show that ST-SSDL consistently outperforms state-of-the-art baselines across multiple metrics. Visualizations further demonstrate its ability to adaptively respond to varying levels of deviation in complex spatio-temporal scenarios. Our code and datasets are available at https://github.com/Jimmy-7664/ST-SSDL.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.122022"
    },
    {
        "index": "#26",
        "title": "DP-HYPE: Distributed Differentially Private Hyperparameter Search",
        "link": "/arxiv/2510.04902",
        "arxiv_id": "2510.04902",
        "authors": "Johannes Liebenow, Thorsten Peinemann, Esfandiar Mohammadi",
        "summary": "The tuning of hyperparameters in distributed machine learning can substantially impact model performance. When the hyperparameters are tuned on sensitive data, privacy becomes an important challenge and to this end, differential privacy has emerged as the de facto standard for provable privacy. A standard setting when performing distributed learning tasks is that clients agree on a shared setup, i.e., find a compromise from a set of hyperparameters, like the learning rate of the model to be trained. Yet, prior work on differentially private hyperparameter tuning either uses computationally expensive cryptographic protocols, determines hyperparameters separately for each client, or applies differential privacy locally, which can lead to undesirable utility-privacy trade-offs. In this work, we present our algorithm DP-HYPE, which performs a distributed and privacy-preserving hyperparameter search by conducting a distributed voting based on local hyperparameter evaluations of clients. In this way, DP-HYPE selects hyperparameters that lead to a compromise supported by the majority of clients, while maintaining scalability and independence from specific learning tasks. We prove that DP-HYPE preserves the strong notion of differential privacy called client-level differential privacy and, importantly, show that its privacy guarantees do not depend on the number of hyperparameters. We also provide bounds on its utility guarantees, that is, the probability of reaching a compromise, and implement DP-HYPE as a submodule in the popular Flower framework for distributed machine learning. In addition, we evaluate performance on multiple benchmark data sets in iid as well as multiple non-iid settings and demonstrate high utility of DP-HYPE even under small privacy budgets.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.122623"
    },
    {
        "index": "#28",
        "title": "Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models",
        "link": "/arxiv/2510.04900",
        "arxiv_id": "2510.04900",
        "authors": "Nick Janßen, Melanie Schaller, Bodo Rosenhahn",
        "summary": "Understanding the robustness of deep learning models for multivariate long-term time series forecasting (M-LTSF) remains challenging, as evaluations typically rely on real-world datasets with unknown noise properties. We propose a simulation-based evaluation framework that generates parameterizable synthetic datasets, where each dataset instance corresponds to a different configuration of signal components, noise types, signal-to-noise ratios, and frequency characteristics. These configurable components aim to model real-world multivariate time series data without the ambiguity of unknown noise. This framework enables fine-grained, systematic evaluation of M-LTSF models under controlled and diverse scenarios. We benchmark four representative architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear (linear), and Autoformer (decomposition-based). Our analysis reveals that all models degrade severely when lookback windows cannot capture complete periods of seasonal patters in the data. S-Mamba and Autoformer perform best on sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals. White and Brownian noise universally degrade performance with lower signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer shows seasonal-noise vulnerability. Further spectral analysis shows that S-Mamba and iTransformer achieve superior frequency reconstruction. This controlled approach, based on our synthetic and principle-driven testbed, offers deeper insights into model-specific strengths and limitations through the aggregation of MSE scores and provides concrete guidance for model selection based on signal characteristics and noise conditions.",
        "subjects": "Machine Learning, Systems and Control",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.123890"
    },
    {
        "index": "#30",
        "title": "Flow-Matching Based Refiner for Molecular Conformer Generation",
        "link": "/arxiv/2510.04878",
        "arxiv_id": "2510.04878",
        "authors": "Xiangyang Xu, Hongyang Gao",
        "summary": "Low-energy molecular conformers generation (MCG) is a foundational yet challenging problem in drug discovery. Denoising-based methods include diffusion and flow-matching methods that learn mappings from a simple base distribution to the molecular conformer distribution. However, these approaches often suffer from error accumulation during sampling, especially in the low SNR steps, which are hard to train. To address these challenges, we propose a flow-matching refiner for the MCG task. The proposed method initializes sampling from mixed-quality outputs produced by upstream denoising models and reschedules the noise scale to bypass the low-SNR phase, thereby improving sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the generator-refiner pipeline improves quality with fewer total denoising steps while preserving diversity.",
        "subjects": "Machine Learning, Quantitative Methods",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.130287"
    },
    {
        "index": "#32",
        "title": "A Clinical-grade Universal Foundation Model for Intraoperative Pathology",
        "link": "/arxiv/2510.04861",
        "arxiv_id": "2510.04861",
        "authors": "Zihan Zhao, Fengtao Zhou, Ronggang Li, Bing Chu, Xinke Zhang, Xueyi Zheng, Ke Zheng, Xiaobo Wen, Jiabo Ma, Yihui Wang, Jiewei Chen, Chengyou Zheng, Jiangyu Zhang, Yongqin Wen, Jiajia Meng, Ziqi Zeng, Xiaoqing Li, Jing Li, Dan Xie, Yaping Ye, Yu Wang, Hao Chen, Muyan Cai",
        "summary": "Intraoperative pathology is pivotal to precision surgery, yet its clinical impact is constrained by diagnostic complexity and the limited availability of high-quality frozen-section data. While computational pathology has made significant strides, the lack of large-scale, prospective validation has impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a clinical-grade foundation model developed on over 100,000 frozen sections from eight medical centers, specifically designed to provide Clinical-grade Robust Intraoperative Support for Pathology (CRISP). CRISP was comprehensively evaluated on more than 15,000 intraoperative slides across nearly 100 retrospective diagnostic tasks, including benign-malignant discrimination, key intraoperative decision-making, and pan-cancer detection, etc. The model demonstrated robust generalization across diverse institutions, tumor types, and anatomical sites-including previously unseen sites and rare cancers. In a prospective cohort of over 2,000 patients, CRISP sustained high diagnostic accuracy under real-world conditions, directly informing surgical decisions in 92.6% of cases. Human-AI collaboration further reduced diagnostic workload by 35%, avoided 105 ancillary tests and enhanced detection of micrometastases with 87.5% accuracy. Together, these findings position CRISP as a clinical-grade paradigm for AI-driven intraoperative pathology, bridging computational advances with surgical precision and accelerating the translation of artificial intelligence into routine clinical practice.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.131294"
    },
    {
        "index": "#34",
        "title": "Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders",
        "link": "/arxiv/2510.04855",
        "arxiv_id": "2510.04855",
        "authors": "Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni",
        "summary": "Counterfactual explanations (CEs) provide recourse recommendations for individuals affected by algorithmic decisions. A key challenge is generating CEs that are robust against various perturbation types (e.g. input and model perturbations) while simultaneously satisfying other desirable properties. These include plausibility, ensuring CEs reside on the data manifold, and diversity, providing multiple distinct recourse options for single inputs. Existing methods, however, mostly struggle to address these multifaceted requirements in a unified, model-agnostic manner. We address these limitations by proposing a novel generative framework. First, we introduce the Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model trained to learn a structured latent space where each class label is represented by a set of Gaussian components with diverse, prototypical centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual Explanations), a model-agnostic algorithm that synthesises entire paths of CE points by interpolating from inputs' latent representations to those learned latent centroids. This approach inherently ensures robustness to input changes, as all paths for a given target class converge to the same fixed centroids. Furthermore, the generated paths provide a spectrum of recourse options, allowing users to navigate the trade-off between proximity and plausibility while also encouraging robustness against model changes. In addition, user-specified actionability constraints can also be easily incorporated via lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive experiments show that LAPACE is computationally efficient and achieves competitive performance across eight quantitative metrics.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.132181"
    },
    {
        "index": "#37",
        "title": "On the Hardness of Learning Regular Expressions",
        "link": "/arxiv/2510.04834",
        "arxiv_id": "2510.04834",
        "authors": "Idan Attias, Lev Reyzin, Nathan Srebro, Gal Vardi",
        "summary": "Despite the theoretical significance and wide practical use of regular expressions, the computational complexity of learning them has been largely unexplored. We study the computational hardness of improperly learning regular expressions in the PAC model and with membership queries. We show that PAC learning is hard even under the uniform distribution on the hypercube, and also prove hardness of distribution-free learning with membership queries. Furthermore, if regular expressions are extended with complement or intersection, we establish hardness of learning with membership queries even under the uniform distribution. We emphasize that these results do not follow from existing hardness results for learning DFAs or NFAs, since the descriptive complexity of regular languages can differ exponentially between DFAs, NFAs, and regular expressions.",
        "subjects": "Machine Learning, Computational Complexity",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.133363"
    },
    {
        "index": "#40",
        "title": "MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis",
        "link": "/arxiv/2510.04776",
        "arxiv_id": "2510.04776",
        "authors": "Ebenezer Awotoro, Chisom Ezekannagha, Florian Schwarz, Johannes Tauscher, Dominik Heider, Katharina Ladewig, Christel Le Bon, Karine Moncoq, Bruno Miroux, Georges Hattab",
        "summary": "Structural biology has made significant progress in determining membrane proteins, leading to a remarkable increase in the number of available structures in dedicated databases. The inherent complexity of membrane protein structures, coupled with challenges such as missing data, inconsistencies, and computational barriers from disparate sources, underscores the need for improved database integration. To address this gap, we present MetaMP, a framework that unifies membrane-protein databases within a web application and uses machine learning for classification. MetaMP improves data quality by enriching metadata, offering a user-friendly interface, and providing eight interactive views for streamlined exploration. MetaMP was effective across tasks of varying difficulty, demonstrating advantages across different levels without compromising speed or accuracy, according to user evaluations. Moreover, MetaMP supports essential functions such as structure classification and outlier detection. We present three practical applications of Artificial Intelligence (AI) in membrane protein research: predicting transmembrane segments, reconciling legacy databases, and classifying structures with explainable AI support. In a validation focused on statistics, MetaMP resolved 77% of data discrepancies and accurately predicted the class of newly identified membrane proteins 98% of the time and overtook expert curation. Altogether, MetaMP is a much-needed resource that harmonizes current knowledge and empowers AI-driven exploration of membrane-protein architecture.",
        "subjects": "Machine Learning, Databases",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.134664"
    },
    {
        "index": "#43",
        "title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs",
        "link": "/arxiv/2510.04767",
        "arxiv_id": "2510.04767",
        "authors": "Wonjun Kang, Kevin Galim, Seunghyuk Oh, Minjae Lee, Yuchen Zeng, Shuibai Zhang, Coleman Hooper, Yuezhou Hu, Hyung Il Koo, Nam Ik Cho, Kangwook Lee",
        "summary": "While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attracted growing interest for their potential to dramatically accelerate inference through parallel decoding. Despite this promise, the conditional independence assumption in dLLMs causes parallel decoding to ignore token dependencies, inevitably degrading generation quality when these dependencies are strong. However, existing works largely overlook these inherent challenges, and evaluations on standard benchmarks (e.g., math and coding) are not sufficient to capture the quality degradation caused by parallel decoding. To address this gap, we first provide an information-theoretic analysis of parallel decoding. We then conduct case studies on analytically tractable synthetic list operations from both data distribution and decoding strategy perspectives, offering quantitative insights that highlight the fundamental limitations of parallel decoding. Building on these insights, we propose ParallelBench, the first benchmark specifically designed for dLLMs, featuring realistic tasks that are trivial for humans and autoregressive LLMs yet exceptionally challenging for dLLMs under parallel decoding. Using ParallelBench, we systematically analyze both dLLMs and autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can suffer dramatic quality degradation in real-world scenarios, and (ii) current parallel decoding strategies struggle to adapt their degree of parallelism based on task difficulty, thus failing to achieve meaningful speedup without compromising quality. Our findings underscore the pressing need for innovative decoding methods that can overcome the current speed-quality trade-off. We release our benchmark to help accelerate the development of truly efficient dLLMs.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.141804"
    },
    {
        "index": "#44",
        "title": "Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors",
        "link": "/arxiv/2510.04758",
        "arxiv_id": "2510.04758",
        "authors": "Zhiwei Han, Stefan Matthes, Hao Shen",
        "summary": "In this work, we establish conditions under which nonlinear CCA recovers the ground-truth latent factors up to an orthogonal transform after whitening. Building on the classical result that linear mappings maximize canonical correlations under Gaussian priors, we prove affine identifiability for a broad class of latent distributions in the population setting. Central to our proof is a reparameterization result that transports the analysis from observation space to source space, where identifiability becomes tractable. We further show that whitening is essential for ensuring boundedness and well-conditioning, thereby underpinning identifiability. Beyond the population setting, we prove that ridge-regularized empirical CCA converges to its population counterpart, transferring these guarantees to the finite-sample regime. Experiments on a controlled synthetic dataset and a rendered image dataset validate our theory and demonstrate the necessity of its assumptions through systematic ablations.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.142383"
    },
    {
        "index": "#45",
        "title": "EVaR-Optimal Arm Identification in Bandits",
        "link": "/arxiv/2510.04728",
        "arxiv_id": "2510.04728",
        "authors": "Mehrasa Ahmadipour, Aurélien Garivier",
        "summary": "We study the fixed-confidence best arm identification (BAI) problem within the multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR) criterion. Our analysis considers a nonparametric setting, allowing for general reward distributions bounded in [0,1]. This formulation addresses the critical need for risk-averse decision-making in high-stakes environments, such as finance, moving beyond simple expected value optimization. We propose a $\\delta$-correct, Track-and-Stop based algorithm and derive a corresponding lower bound on the expected sample complexity, which we prove is asymptotically matched. The implementation of our algorithm and the characterization of the lower bound both require solving a complex convex optimization problem and a related, simpler non-convex one.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.142940"
    },
    {
        "index": "#46",
        "title": "Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs",
        "link": "/arxiv/2510.04727",
        "arxiv_id": "2510.04727",
        "authors": "Emanuele Mule, Stefano Fiorini, Antonio Purificato, Federico Siciliano, Stefano Coniglio, Fabrizio Silvestri",
        "summary": "Hypergraphs provide a natural way to represent higher-order interactions among multiple entities. While undirected hypergraphs have been extensively studied, the case of directed hypergraphs, which can model oriented group interactions, remains largely under-explored despite its relevance for many applications. Recent approaches in this direction often exhibit an implicit bias toward homophily, which limits their effectiveness in heterophilic settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf Neural Networks (SNNs) were introduced as an effective solution to circumvent such a drawback. While a generalization to hypergraphs is known, it is only suitable for undirected hypergraphs, failing to tackle the directed case. In this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a framework integrating sheaf theory with a principled treatment of asymmetric relations within a hypergraph. From it, we construct the Directed Sheaf Hypergraph Laplacian, a complex-valued operator by which we unify and generalize many existing Laplacian matrices proposed in the graph- and hypergraph-learning literature. Across 7 real-world datasets and against 13 baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how a principled treatment of directionality in hypergraphs, combined with the expressive power of sheaves, can substantially improve performance.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.143592"
    },
    {
        "index": "#47",
        "title": "ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts",
        "link": "/arxiv/2510.04710",
        "arxiv_id": "2510.04710",
        "authors": "Zexin Wang, Changhua Pei, Yang Liu, Hengyue Jiang, Quan Zhou, Haotian Si, Hang Cui, Jianhui Li, Gaogang Xie, Jingjing Li, Dan Pei",
        "summary": "Web service administrators must ensure the stability of multiple systems by promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving the goal of \"train once, infer across scenarios\" remains a fundamental challenge for time series anomaly detection models. Beyond improving zero-shot generalization, such models must also flexibly handle sequences of varying lengths during inference, ranging from one hour to one week, without retraining. Conventional approaches rely on sliding-window encoding and self-supervised learning, which restrict inference to fixed-length inputs. Large Language Models (LLMs) have demonstrated remarkable zero-shot capabilities across general domains. However, when applied to time series data, they face inherent limitations due to context length. To address this issue, we propose ViTs, a Vision-Language Model (VLM)-based framework that converts time series curves into visual representations. By rescaling time series images, temporal dependencies are preserved while maintaining a consistent input size, thereby enabling efficient processing of arbitrarily long sequences without context constraints. Training VLMs for this purpose introduces unique challenges, primarily due to the scarcity of aligned time series image-text data. To overcome this, we employ an evolutionary algorithm to automatically generate thousands of high-quality image-text pairs and design a three-stage training pipeline consisting of: (1) time series knowledge injection, (2) anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive experiments demonstrate that ViTs substantially enhance the ability of VLMs to understand and detect anomalies in time series data. All datasets and code will be publicly released at: https://anonymous.4open.science/r/ViTs-C484/.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.144359"
    },
    {
        "index": "#49",
        "title": "Parameter-free Algorithms for the Stochastically Extended Adversarial Model",
        "link": "/arxiv/2510.04685",
        "arxiv_id": "2510.04685",
        "authors": "Shuche Wang, Adarsh Barik, Peng Zhao, Vincent Y. F. Tan",
        "summary": "We develop the first parameter-free algorithms for the Stochastically Extended Adversarial (SEA) model, a framework that bridges adversarial and stochastic online convex optimization. Existing approaches for the SEA model require prior knowledge of problem-specific parameters, such as the diameter of the domain $D$ and the Lipschitz constant of the loss functions $G$, which limits their practical applicability. Addressing this, we develop parameter-free methods by leveraging the Optimistic Online Newton Step (OONS) algorithm to eliminate the need for these parameters. We first establish a comparator-adaptive algorithm for the scenario with unknown domain diameter but known Lipschitz constant, achieving an expected regret bound of $\\tilde{O}\\big(\\|u\\|_2^2 + \\|u\\|_2(\\sqrt{\\sigma^2_{1:T}} + \\sqrt{\\Sigma^2_{1:T}})\\big)$, where $u$ is the comparator vector and $\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$ represent the cumulative stochastic variance and cumulative adversarial variation, respectively. We then extend this to the more general setting where both $D$ and $G$ are unknown, attaining the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound exhibits the same dependence on $\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$, demonstrating the efficacy of our proposed methods even when both parameters are unknown in the SEA model.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.150815"
    },
    {
        "index": "#50",
        "title": "Counterfactual Credit Guided Bayesian Optimization",
        "link": "/arxiv/2510.04676",
        "arxiv_id": "2510.04676",
        "authors": "Qiyu Wei, Haowei Wang, Richard Allmendinger, Mauricio A. Álvarez",
        "summary": "Bayesian optimization has emerged as a prominent methodology for optimizing expensive black-box functions by leveraging Gaussian process surrogates, which focus on capturing the global characteristics of the objective function. However, in numerous practical scenarios, the primary objective is not to construct an exhaustive global surrogate, but rather to quickly pinpoint the global optimum. Due to the aleatoric nature of the sequential optimization problem and its dependence on the quality of the surrogate model and the initial design, it is restrictive to assume that all observed samples contribute equally to the discovery of the optimum in this context. In this paper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO), a novel framework that explicitly quantifies the contribution of individual historical observations through counterfactual credit. By incorporating counterfactual credit into the acquisition function, our approach can selectively allocate resources in areas where optimal solutions are most likely to occur. We prove that CCGBO retains sublinear regret. Empirical evaluations on various synthetic and real-world benchmarks demonstrate that CCGBO consistently reduces simple regret and accelerates convergence to the global optimum.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.151431"
    },
    {
        "index": "#53",
        "title": "IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams",
        "link": "/arxiv/2510.04660",
        "arxiv_id": "2510.04660",
        "authors": "Yuandou Wang, Filip Gunnarsson, Rihan Hai",
        "summary": "Tabular data streams are rapidly emerging as a dominant modality for real-time decision-making in healthcare, finance, and the Internet of Things (IoT). These applications commonly run on edge and mobile devices, where energy budgets, memory, and compute are strictly limited. Continual learning (CL) addresses such dynamics by training models sequentially on task streams while preserving prior knowledge and consolidating new knowledge. While recent CL work has advanced in mitigating catastrophic forgetting and improving knowledge transfer, the practical requirements of energy and memory efficiency for tabular data streams remain underexplored. In particular, existing CL solutions mostly depend on replay mechanisms whose buffers grow over time and exacerbate resource costs. We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a compact continual learner for tabular data streams. IMLP incorporates a windowed scaled dot-product attention over a sliding latent feature buffer, enabling constant-size memory and avoiding storing raw data. The attended context is concatenated with current features and processed by shared feed-forward layers, yielding lightweight per-segment updates. To assess practical deployability, we introduce NetScore-T, a tunable metric coupling balanced accuracy with energy for Pareto-aware comparison across models and datasets. IMLP achieves up to $27.6\\times$ higher energy efficiency than TabNet and $85.5\\times$ higher than TabPFN, while maintaining competitive average accuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient alternative to full retraining for tabular data streams.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.153385"
    },
    {
        "index": "#55",
        "title": "Compressed Concatenation of Small Embedding Models",
        "link": "/arxiv/2510.04626",
        "arxiv_id": "2510.04626",
        "authors": "Mohamed Ayoub Ben Ayad, Michael Dinzinger, Kanishka Ghosh Dastidar, Jelena Mitrovic, Michael Granitzer",
        "summary": "Embedding models are central to dense retrieval, semantic search, and recommendation systems, but their size often makes them impractical to deploy in resource-constrained environments such as browsers or edge devices. While smaller embedding models offer practical advantages, they typically underperform compared to their larger counterparts. To bridge this gap, we demonstrate that concatenating the raw embedding vectors of multiple small models can outperform a single larger baseline on standard retrieval benchmarks. To overcome the resulting high dimensionality of naive concatenation, we introduce a lightweight unified decoder trained with a Matryoshka Representation Learning (MRL) loss. This decoder maps the high-dimensional joint representation to a low-dimensional space, preserving most of the original performance without fine-tuning the base models. We also show that while concatenating more base models yields diminishing gains, the robustness of the decoder's representation under compression and quantization improves. Our experiments show that, on a subset of MTEB retrieval tasks, our concat-encode-quantize pipeline recovers 89\\% of the original performance with a 48x compression factor when the pipeline is applied to a concatenation of four small embedding models.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.154701"
    },
    {
        "index": "#56",
        "title": "Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI",
        "link": "/arxiv/2510.04622",
        "arxiv_id": "2510.04622",
        "authors": "Youngjoon Lee, Seongmin Cho, Yehhyun Jo, Jinu Gong, Hyunjoo Jenny Lee, Joonhyuk Kang",
        "summary": "The limited data availability due to strict privacy regulations and significant resource demands severely constrains biomedical time-series AI development, which creates a critical gap between data requirements and accessibility. Synthetic data generation presents a promising solution by producing artificial datasets that maintain the statistical properties of real biomedical time-series data without compromising patient confidentiality. We propose a framework for synthetic biomedical time-series data generation based on advanced forecasting models that accurately replicates complex electrophysiological signals such as EEG and EMG with high fidelity. These synthetic datasets preserve essential temporal and spectral properties of real data, which enables robust analysis while effectively addressing data scarcity and privacy challenges. Our evaluations across multiple subjects demonstrate that the generated synthetic data can serve as an effective substitute for real data and also significantly boost AI model performance. The approach maintains critical biomedical features while provides high scalability for various applications and integrates seamlessly into open-source repositories, substantially expanding resources for AI-driven biomedical research.",
        "subjects": "Machine Learning, Signal Processing",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.160491"
    },
    {
        "index": "#58",
        "title": "Closed-Form Last Layer Optimization",
        "link": "/arxiv/2510.04606",
        "arxiv_id": "2510.04606",
        "authors": "Alexandre Galashov, Nathaël Da Costa, Liyuan Xu, Philipp Hennig, Arthur Gretton",
        "summary": "Neural networks are typically optimized with variants of stochastic gradient descent. Under a squared loss, however, the optimal solution to the linear last layer weights is known in closed-form. We propose to leverage this during optimization, treating the last layer as a function of the backbone parameters, and optimizing solely for these parameters. We show this is equivalent to alternating between gradient descent steps on the backbone and closed-form updates on the last layer. We adapt the method for the setting of stochastic gradient descent, by trading off the loss on the current batch against the accumulated information from previous batches. Further, we prove that, in the Neural Tangent Kernel regime, convergence of this method to an optimal solution is guaranteed. Finally, we demonstrate the effectiveness of our approach compared with standard SGD on a squared loss in several supervised tasks -- both regression and classification -- including Fourier Neural Operators and Instrumental Variable Regression.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.162038"
    },
    {
        "index": "#59",
        "title": "Improved probabilistic regression using diffusion models",
        "link": "/arxiv/2510.04583",
        "arxiv_id": "2510.04583",
        "authors": "Carlo Kneissl, Christopher Bülte, Philipp Scholl, Gitta Kutyniok",
        "summary": "Probabilistic regression models the entire predictive distribution of a response variable, offering richer insights than classical point estimates and directly allowing for uncertainty quantification. While diffusion-based generative models have shown remarkable success in generating complex, high-dimensional data, their usage in general regression tasks often lacks uncertainty-related evaluation and remains limited to domain-specific applications. We propose a novel diffusion-based framework for probabilistic regression that learns predictive distributions in a nonparametric way. More specifically, we propose to model the full distribution of the diffusion noise, enabling adaptation to diverse tasks and enhanced uncertainty quantification. We investigate different noise parameterizations, analyze their trade-offs, and evaluate our framework across a broad range of regression tasks, covering low- and high-dimensional settings. For several experiments, our approach shows superior performance against existing baselines, while delivering calibrated uncertainty estimates, demonstrating its versatility as a tool for probabilistic prediction.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.162656"
    },
    {
        "index": "#60",
        "title": "Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing",
        "link": "/arxiv/2510.04579",
        "arxiv_id": "2510.04579",
        "authors": "Clément Bonet, Elsa Cazelles, Lucas Drumetz, Nicolas Courty",
        "summary": "The Busemann function has recently found much interest in a variety of geometric machine learning problems, as it naturally defines projections onto geodesic rays of Riemannian manifolds and generalizes the notion of hyperplanes. As several sources of data can be conveniently modeled as probability distributions, it is natural to study this function in the Wasserstein space, which carries a rich formal Riemannian structure induced by Optimal Transport metrics. In this work, we investigate the existence and computation of Busemann functions in Wasserstein space, which admits geodesic rays. We establish closed-form expressions in two important cases: one-dimensional distributions and Gaussian measures. These results enable explicit projection schemes for probability distributions on $\\mathbb{R}$, which in turn allow us to define novel Sliced-Wasserstein distances over Gaussian mixtures and labeled datasets. We demonstrate the efficiency of those original schemes on synthetic datasets as well as transfer learning problems.",
        "subjects": "Machine Learning, Metric Geometry, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.163337"
    },
    {
        "index": "#64",
        "title": "Stochastic Approximation Methods for Distortion Risk Measure Optimization",
        "link": "/arxiv/2510.04563",
        "arxiv_id": "2510.04563",
        "authors": "Jinyang Jiang, Bernd Heidergott, Jiaqiao Hu, Yijie Peng",
        "summary": "Distortion Risk Measures (DRMs) capture risk preferences in decision-making and serve as general criteria for managing uncertainty. This paper proposes gradient descent algorithms for DRM optimization based on two dual representations: the Distortion-Measure (DM) form and Quantile-Function (QF) form. The DM-form employs a three-timescale algorithm to track quantiles, compute their gradients, and update decision variables, utilizing the Generalized Likelihood Ratio and kernel-based density estimation. The QF-form provides a simpler two-timescale approach that avoids the need for complex quantile gradient estimation. A hybrid form integrates both approaches, applying the DM-form for robust performance around distortion function jumps and the QF-form for efficiency in smooth regions. Proofs of strong convergence and convergence rates for the proposed algorithms are provided. In particular, the DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form attains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their effectiveness and demonstrate substantial improvements over baselines in robust portfolio selection tasks. The method's scalability is further illustrated through integration into deep reinforcement learning. Specifically, a DRM-based Proximal Policy Optimization algorithm is developed and applied to multi-echelon dynamic inventory management, showcasing its practical applicability.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.171363"
    },
    {
        "index": "#65",
        "title": "Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems",
        "link": "/arxiv/2510.04559",
        "arxiv_id": "2510.04559",
        "authors": "Mohsen Amiri, V Venktesh, Sindri Magnússon",
        "summary": "This paper investigates the identification of the top-m user-scheduling sets in multi-user MIMO downlink, which is cast as a combinatorial pure-exploration problem in stochastic linear bandits. Because the action space grows exponentially, exhaustive search is infeasible. We therefore adopt a linear utility model to enable efficient exploration and reliable selection of promising user subsets. We introduce a gap-index framework that maintains a shortlist of current estimates of champion arms (top-m sets) and a rotating shortlist of challenger arms that pose the greatest threat to the champions. This design focuses on measurements that yield the most informative gap-index-based comparisons, resulting in significant reductions in runtime and computation compared to state-of-the-art linear bandit methods, with high identification accuracy. The method also exposes a tunable trade-off between speed and accuracy. Simulations on a realistic OFDM downlink show that shortlist-driven pure exploration makes online, measurement-efficient subcarrier selection practical for AI-enabled communication systems.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.171989"
    },
    {
        "index": "#66",
        "title": "Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets",
        "link": "/arxiv/2510.04555",
        "arxiv_id": "2510.04555",
        "authors": "Jian'an Zhang",
        "summary": "We introduce Tail-Safe, a deployability-oriented framework for derivatives hedging that unifies distributional, risk-sensitive reinforcement learning with a white-box control-barrier-function (CBF) quadratic-program (QP) safety layer tailored to financial constraints. The learning component combines an IQN-based distributional critic with a CVaR objective (IQN--CVaR--PPO) and a Tail-Coverage Controller that regulates quantile sampling through temperature tilting and tail boosting to stabilize small-$\\alpha$ estimation. The safety component enforces discrete-time CBF inequalities together with domain-specific constraints -- ellipsoidal no-trade bands, box and rate limits, and a sign-consistency gate -- solved as a convex QP whose telemetry (active sets, tightness, rate utilization, gate scores, slack, and solver status) forms an auditable trail for governance. We provide guarantees of robust forward invariance of the safe set under bounded model mismatch, a minimal-deviation projection interpretation of the QP, a KL-to-DRO upper bound linking per-state KL regularization to worst-case CVaR, concentration and sample-complexity results for the temperature-tilted CVaR estimator, and a CVaR trust-region improvement inequality under KL limits, together with feasibility persistence under expiry-aware tightening. Empirically, in arbitrage-free, microstructure-aware synthetic markets (SSVI $\\to$ Dupire $\\to$ VIX with ABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading central performance and yields zero hard-constraint violations whenever the QP is feasible with zero slack. Telemetry is mapped to governance dashboards and incident workflows to support explainability and auditability. Limitations include reliance on synthetic data and simplified execution to isolate methodological contributions.",
        "subjects": "Machine Learning, Trading and Market Microstructure",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.172583"
    },
    {
        "index": "#67",
        "title": "Post-training quantization of vision encoders needs prefixing registers",
        "link": "/arxiv/2510.04547",
        "arxiv_id": "2510.04547",
        "authors": "Seunghyeon Kim, Jinho Kim, Taesun Yeom, Wonpyo Park, Kyuyeun Kim, Jaeho Lee",
        "summary": "Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Post-training quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose $\\textit{RegCache}$, a training-free algorithm to mitigate outliers in vision encoders, enabling quantization with significantly smaller accuracy drops. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.173272"
    },
    {
        "index": "#68",
        "title": "Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not Just Make Predictions",
        "link": "/arxiv/2510.04543",
        "arxiv_id": "2510.04543",
        "authors": "Elias Dubbeldam, Reza Mohammadi, Marit Schoonhoven, S. Ilker Birbil",
        "summary": "Despite recent progress, deep learning methods for tabular data still struggle to compete with traditional tree-based models. A key challenge lies in modeling complex, dataset-specific feature interactions that are central to tabular data. Graph-based tabular deep learning (GTDL) methods aim to address this by representing features and their interactions as graphs. However, existing methods predominantly optimize predictive accuracy, neglecting accurate modeling of the graph structure. This position paper argues that GTDL should move beyond prediction-centric objectives and prioritize the explicit learning and evaluation of feature interactions. Using synthetic datasets with known ground-truth graph structures, we show that existing GTDL methods fail to recover meaningful feature interactions. Moreover, enforcing the true interaction structure improves predictive performance. This highlights the need for GTDL methods to prioritize quantitative evaluation and accurate structural learning. We call for a shift toward structure-aware modeling as a foundation for building GTDL systems that are not only accurate but also interpretable, trustworthy, and grounded in domain understanding.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.173916"
    },
    {
        "index": "#69",
        "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion",
        "link": "/arxiv/2510.04525",
        "arxiv_id": "2510.04525",
        "authors": "Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, Yuki Mitsufuji",
        "summary": "Masked diffusion models have shown promising performance in generating high-quality samples in a wide range of domains, but accelerating their sampling process remains relatively underexplored. To investigate efficient samplers for masked diffusion, this paper theoretically analyzes the MaskGIT sampler for image modeling, revealing its implicit temperature sampling mechanism. Through this analysis, we introduce the \"moment sampler,\" an asymptotically equivalent but more tractable and interpretable alternative to MaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking positions before sampling tokens. In addition, we improve the efficiency of choose-then-sample algorithms through two key innovations: a partial caching technique for transformers that approximates longer sampling trajectories without proportional computational cost, and a hybrid approach formalizing the exploration-exploitation trade-off in adaptive unmasking. Experiments in image and text domains demonstrate our theory as well as the efficiency of our proposed methods, advancing both theoretical understanding and practical implementation of masked diffusion samplers.",
        "subjects": "Machine Learning, Probability, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.174622"
    },
    {
        "index": "#71",
        "title": "Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows",
        "link": "/arxiv/2510.04510",
        "arxiv_id": "2510.04510",
        "authors": "Achim Eckerle, Martin Spitznagel, Janis Keuper",
        "summary": "Accurate and fast urban noise prediction is pivotal for public health and for regulatory workflows in cities, where the Environmental Noise Directive mandates regular strategic noise maps and action plans, often needed in permission workflows, right-of-way allocation, and construction scheduling. Physics-based solvers are too slow for such time-critical, iterative \"what-if\" studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating for generating standards-compliant urban sound-pressure maps from 2D urban layouts in real time per 256x256 map on a single RTX 4090), enabling interactive exploration directly on commodity hardware. On datasets covering Baseline, Diffraction, and Reflection regimes, our model accelerates map generation by >2000 times over a reference solver while improving NLoS accuracy by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE with high structural fidelity. The model reproduces diffraction and interference patterns and supports instant recomputation under source or geometry changes, making it a practical engine for urban planning, compliance mapping, and operations (e.g., temporary road closures, night-work variance assessments).",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.181118"
    },
    {
        "index": "#72",
        "title": "Wavelet Predictive Representations for Non-Stationary Reinforcement Learning",
        "link": "/arxiv/2510.04507",
        "arxiv_id": "2510.04507",
        "authors": "Min Wang, Xin Li, Ye He, Yao-Hui Li, Hasnaa Bennis, Riashat Islam, Mingzhong Wang",
        "summary": "The real world is inherently non-stationary, with ever-changing factors, such as weather conditions and traffic flows, making it challenging for agents to adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning (NSRL) addresses this challenge by training agents to adapt rapidly to sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL approaches often focus on tasks with regularly evolving patterns, leading to limited adaptability in highly dynamic settings. Inspired by the success of Wavelet analysis in time series modeling, specifically its ability to capture signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain predictive task representations to enhance NSRL. WISDOM captures these multi-scale features in evolving MDP sequences by transforming task representation sequences into the wavelet domain, where wavelet coefficients represent both global trends and fine-grained variations of non-stationary changes. In addition to the auto-regressive modeling commonly employed in time series forecasting, we devise a wavelet temporal difference (TD) update operator to enhance tracking and prediction of MDP evolution. We theoretically prove the convergence of this operator and demonstrate policy improvement with wavelet task representations. Experiments on diverse benchmarks show that WISDOM significantly outperforms existing baselines in both sample efficiency and asymptotic performance, demonstrating its remarkable adaptability in complex environments characterized by non-stationary and stochastically evolving tasks.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.181801"
    },
    {
        "index": "#73",
        "title": "Expand Neurons, Not Parameters",
        "link": "/arxiv/2510.04500",
        "arxiv_id": "2510.04500",
        "authors": "Linghao Kong, Inimai Subramanian, Yonadav Shavit, Micah Adler, Dan Alistarh, Nir Shavit",
        "summary": "This work demonstrates how increasing the number of neurons in a network without increasing its number of non-zero parameters improves performance. We show that this gain corresponds with a decrease in interference between multiple features that would otherwise share the same neurons. To reduce such entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter Expansion (FPE): replace a neuron with multiple children and partition the parent's weights disjointly across them, so that each child inherits a non-overlapping subset of connections. On symbolic tasks, specifically Boolean code problems, clause-aligned FPE systematically reduces polysemanticity metrics and yields higher task accuracy. Notably, random splits of neuron weights approximate these gains, indicating that reduced collisions, not precise assignment, are a primary driver. Consistent with the superposition hypothesis, the benefits of FPE grow with increasing interference: when polysemantic load is high, accuracy improvements are the largest. Transferring these insights to real models (classifiers over CLIP embeddings and deeper multilayer networks) we find that widening networks while maintaining a constant non-zero parameter count consistently increases accuracy. These results identify an interpretability-grounded mechanism to leverage width against superposition, improving performance without increasing the number of non-zero parameters. Such a direction is well matched to modern accelerators, where memory movement of non-zero parameters, rather than raw compute, is the dominant bottleneck.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.182451"
    },
    {
        "index": "#74",
        "title": "Forking-Sequences",
        "link": "/arxiv/2510.04487",
        "arxiv_id": "2510.04487",
        "authors": "Willa Potosnak, Malcolm Wolff, Boris Oreshkin, Mengfei Cao, Michael W. Mahoney, Dmitry Efimov, Kin G. Olivares",
        "summary": "While accuracy is a critical requirement for time series forecasting models, an equally important (yet often overlooked) desideratum is forecast stability across forecast creation dates (FCDs). Even highly accurate models can produce erratic revisions between FCDs, undermining stakeholder trust and disrupting downstream decision-making. To improve forecast stability, models like MQCNN, MQT, and SPADE employ a little-known but highly effective technique: forking-sequences. Unlike standard statistical and neural forecasting methods that treat each FCD independently, the forking-sequences method jointly encodes and decodes the entire time series across all FCDs, in a way mirroring time series cross-validation. Since forking sequences remains largely unknown in the broader neural forecasting community, in this work, we formalize the forking-sequences approach, and we make a case for its broader adoption. We demonstrate three key benefits of forking-sequences: (i) more stable and consistent gradient updates during training; (ii) reduced forecast variance through ensembling; and (iii) improved inference computational efficiency. We validate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and Tourism competitions, showing improvements in forecast percentage change stability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP, RNN, LSTM, CNN, and Transformer-based architectures, respectively.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.183127"
    },
    {
        "index": "#75",
        "title": "Domain Generalization: A Tale of Two ERMs",
        "link": "/arxiv/2510.04441",
        "arxiv_id": "2510.04441",
        "authors": "Yilun Zhu, Naihao Deng, Naichen Shi, Aditya Gangrade, Clayton Scott",
        "summary": "Domain generalization (DG) is the problem of generalizing from several distributions (or domains), for which labeled training data are available, to a new test domain for which no labeled data is available. A common finding in the DG literature is that it is difficult to outperform empirical risk minimization (ERM) on the pooled training data. In this work, we argue that this finding has primarily been reported for datasets satisfying a \\emph{covariate shift} assumption. When the dataset satisfies a \\emph{posterior drift} assumption instead, we show that ``domain-informed ERM,'' wherein feature vectors are augmented with domain-specific information, outperforms pooling ERM. These claims are supported by a theoretical framework and experiments on language and vision tasks.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.183810"
    },
    {
        "index": "#76",
        "title": "Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size",
        "link": "/arxiv/2510.04440",
        "arxiv_id": "2510.04440",
        "authors": "Farid Bozorgnia, Vyacheslav Kungurtsev, Shirali Kadyrov, Mohsen Yousefnezhad",
        "summary": "In this work, we introduce novel algorithms for label propagation and self-training using fractional heat kernel dynamics with a source term. We motivate the methodology through the classical correspondence of information theory with the physics of parabolic evolution equations. We integrate the fractional heat kernel into Graph Neural Network architectures such as Graph Convolutional Networks and Graph Attention, enhancing their expressiveness through adaptive, multi-hop diffusion. By applying Chebyshev polynomial approximations, large graphs become computationally feasible. Motivating variational formulations demonstrate that by extending the classical diffusion model to fractional powers of the Laplacian, nonlocal interactions deliver more globally diffusing labels. The particular balance between supervision of known labels and diffusion across the graph is particularly advantageous in the case where only a small number of labeled training examples are present. We demonstrate the effectiveness of this approach on standard datasets.",
        "subjects": "Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.184438"
    },
    {
        "index": "#77",
        "title": "Trade-off in Estimating the Number of Byzantine Clients in Federated Learning",
        "link": "/arxiv/2510.04432",
        "arxiv_id": "2510.04432",
        "authors": "Ziyi Chen, Su Zhang, Heng Huang",
        "summary": "Federated learning has attracted increasing attention at recent large-scale optimization and machine learning research and applications, but is also vulnerable to Byzantine clients that can send any erroneous signals. Robust aggregators are commonly used to resist Byzantine clients. This usually requires to estimate the unknown number $f$ of Byzantine clients, and thus accordingly select the aggregators with proper degree of robustness (i.e., the maximum number $\\hat{f}$ of Byzantine clients allowed by the aggregator). Such an estimation should have important effect on the performance, which has not been systematically studied to our knowledge. This work will fill in the gap by theoretically analyzing the worst-case error of aggregators as well as its induced federated learning algorithm for any cases of $\\hat{f}$ and $f$. Specifically, we will show that underestimation ($\\hat{f}<f$) can lead to arbitrarily poor performance for both aggregators and federated learning. For non-underestimation ($\\hat{f}\\ge f$), we have proved optimal lower and upper bounds of the same order on the errors of both aggregators and federated learning. All these optimal bounds are proportional to $\\hat{f}/(n-f-\\hat{f})$ with $n$ clients, which monotonically increases with larger $\\hat{f}$. This indicates a fundamental trade-off: while an aggregator with a larger robustness degree $\\hat{f}$ can solve federated learning problems of wider range $f\\in [0,\\hat{f}]$, the performance can deteriorate when there are actually fewer or even no Byzantine clients (i.e., $f\\in [0,\\hat{f})$).",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.185068"
    },
    {
        "index": "#78",
        "title": "Achieve Performatively Optimal Policy for Performative Reinforcement Learning",
        "link": "/arxiv/2510.04430",
        "arxiv_id": "2510.04430",
        "authors": "Ziyi Chen, Heng Huang",
        "summary": "Performative reinforcement learning is an emerging dynamical decision making framework, which extends reinforcement learning to the common applications where the agent's policy can change the environmental dynamics. Existing works on performative reinforcement learning only aim at a performatively stable (PS) policy that maximizes an approximate value function. However, there is a provably positive constant gap between the PS policy and the desired performatively optimal (PO) policy that maximizes the original value function. In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW) algorithm with a zeroth-order approximation of the performative policy gradient in the Frank-Wolfe framework, and obtains \\textbf{the first polynomial-time convergence to the desired PO} policy under the standard regularizer dominance condition. For the convergence analysis, we prove two important properties of the nonconvex value function. First, when the policy regularizer dominates the environmental shift, the value function satisfies a certain gradient dominance property, so that any stationary point (not PS) of the value function is a desired PO. Second, though the value function has unbounded gradient, we prove that all the sufficiently stationary points lie in a convex and compact policy subspace $\\Pi_{\\Delta}$, where the policy value has a constant lower bound $\\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous. Experimental results also demonstrate that our 0-FW algorithm is more effective than the existing algorithms in finding the desired PO policy.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.185690"
    },
    {
        "index": "#80",
        "title": "SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management",
        "link": "/arxiv/2510.04386",
        "arxiv_id": "2510.04386",
        "authors": "Shakson Isaac, Yentl Collin, Chirag Patel",
        "summary": "Continuous glucose monitoring (CGM) generates dense data streams critical for diabetes management, but most used forecasting models lack interpretability for clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting model that integrates CGM and wearable activity signals from the AI-READI cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer baseline, adds interpretability through variable selection and temporal attribution, and enables counterfactual forecasts simulating how planned changes in physiological signals (e.g., heart rate, respiration) affect near-term glucose. Together, these features make SSM-CGM an interpretable, physiologically grounded framework for personalized diabetes management.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.192218"
    },
    {
        "index": "#81",
        "title": "Score-based Greedy Search for Structure Identification of Partially Observed Linear Causal Models",
        "link": "/arxiv/2510.04378",
        "arxiv_id": "2510.04378",
        "authors": "Xinshuai Dong, Ignavier Ng, Haoyue Dai, Jiaqi Sun, Xiangchen Song, Peter Spirtes, Kun Zhang",
        "summary": "Identifying the structure of a partially observed causal system is essential to various scientific fields. Recent advances have focused on constraint-based causal discovery to solve this problem, and yet in practice these methods often face challenges related to multiple testing and error propagation. These issues could be mitigated by a score-based method and thus it has raised great attention whether there exists a score-based greedy search method that can handle the partially observed scenario. In this work, we propose the first score-based greedy search method for the identification of structure involving latent variables with identifiability guarantees. Specifically, we propose Generalized N Factor Model and establish the global consistency: the true structure including latent variables can be identified up to the Markov equivalence class by using score. We then design Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm for this class of model with well-defined operators, which search very efficiently over the graph space to find the optimal structure. Our experiments on both synthetic and real-life data validate the effectiveness of our method (code will be publicly available).",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.192903"
    },
    {
        "index": "#82",
        "title": "Categorical Invariants of Learning Dynamics",
        "link": "/arxiv/2510.04376",
        "arxiv_id": "2510.04376",
        "authors": "Abdulrahman Tamim",
        "summary": "Neural network training is typically viewed as gradient descent on a loss surface. We propose a fundamentally different perspective: learning is a structure-preserving transformation (a functor L) between the space of network parameters (Param) and the space of learned representations (Rep). This categorical framework reveals that different training runs producing similar test performance often belong to the same homotopy class (continuous deformation family) of optimization paths. We show experimentally that networks converging via homotopic trajectories generalize within 0.5% accuracy of each other, while non-homotopic paths differ by over 3%. The theory provides practical tools: persistent homology identifies stable minima predictive of generalization (R^2 = 0.82 correlation), pullback constructions formalize transfer learning, and 2-categorical structures explain when different optimization algorithms yield functionally equivalent models. These categorical invariants offer both theoretical insight into why deep learning works and concrete algorithmic principles for training more robust networks.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.193468"
    },
    {
        "index": "#85",
        "title": "Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework",
        "link": "/arxiv/2510.04366",
        "arxiv_id": "2510.04366",
        "authors": "Christopher Klugmann, Daniel Kondermann",
        "summary": "Human-generated categorical annotations frequently produce empirical response distributions (soft labels) that reflect ambiguity rather than simple annotator error. We introduce an ambiguity measure that maps a discrete response distribution to a scalar in the unit interval, designed to quantify aleatoric uncertainty in categorical tasks. The measure bears a close relationship to quadratic entropy (Gini-style impurity) but departs from those indices by treating an explicit \"can't solve\" category asymmetrically, thereby separating uncertainty arising from class-level indistinguishability from uncertainty due to explicit unresolvability. We analyze the measure's formal properties and contrast its behavior with a representative ambiguity measure from the literature. Moving beyond description, we develop statistical tools for inference: we propose frequentist point estimators for population ambiguity and derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the underlying probability vector, providing a principled account of epistemic uncertainty. Numerical examples illustrate estimation, calibration, and practical use for dataset-quality assessment and downstream machine-learning workflows.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.195681"
    },
    {
        "index": "#86",
        "title": "From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere",
        "link": "/arxiv/2510.04357",
        "arxiv_id": "2510.04357",
        "authors": "Anoushka Harit, Zhongtian Sun, Jongmin Yu",
        "summary": "We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel architecture for interpretable financial time-series forecasting that unifies \\emph{Granger-causal hypergraph structure}, \\emph{Riemannian geometry}, and \\emph{causally masked Transformer attention}. CSHT models the directional influence of financial news and sentiment on asset returns by extracting multivariate Granger-causal dependencies, which are encoded as directional hyperedges on the surface of a hypersphere. Attention is constrained via angular masks that preserve both temporal directionality and geometric consistency. Evaluated on S\\&P 500 data from 2018 to 2023, including the 2020 COVID-19 shock, CSHT consistently outperforms baselines across return prediction, regime classification, and top-asset ranking tasks. By enforcing predictive causal structure and embedding variables in a Riemannian manifold, CSHT delivers both \\emph{robust generalisation across market regimes} and \\emph{transparent attribution pathways} from macroeconomic events to stock-level responses. These results suggest that CSHT is a principled and practical solution for trustworthy financial forecasting under uncertainty.",
        "subjects": "Machine Learning, Computational Finance",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.196547"
    },
    {
        "index": "#87",
        "title": "Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics",
        "link": "/arxiv/2510.04342",
        "arxiv_id": "2510.04342",
        "authors": "Harshil Vejendla",
        "summary": "Forecasting chaotic systems is a cornerstone challenge in many scientific fields, complicated by the exponential amplification of even infinitesimal prediction errors. Modern machine learning approaches often falter due to two opposing pitfalls: over-specializing on a single, well-known chaotic system (e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing vast, unrelated time-series, which prevents the model from learning the nuances of any specific dynamical regime. We propose Curriculum Chaos Forecasting (CCF), a training paradigm that bridges this gap. CCF organizes training data based on fundamental principles of dynamical systems theory, creating a curriculum that progresses from simple, periodic behaviors to highly complex, chaotic dynamics. We quantify complexity using the largest Lyapunov exponent and attractor dimension, two well-established metrics of chaos. By first training a sequence model on predictable systems and gradually introducing more chaotic trajectories, CCF enables the model to build a robust and generalizable representation of dynamical behaviors. We curate a library of over 50 synthetic ODE/PDE systems to build this curriculum. Our experiments show that pre-training with CCF significantly enhances performance on unseen, real-world benchmarks. On datasets including Sunspot numbers, electricity demand, and human ECG signals, CCF extends the valid prediction horizon by up to 40% compared to random-order training and more than doubles it compared to training on real-world data alone. We demonstrate that this benefit is consistent across various neural architectures (GRU, Transformer) and provide extensive ablations to validate the importance of the curriculum's structure.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.197114"
    },
    {
        "index": "#89",
        "title": "DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks",
        "link": "/arxiv/2510.04331",
        "arxiv_id": "2510.04331",
        "authors": "Nghiem T. Diep, Hien Dang, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho",
        "summary": "Parameter-efficient fine-tuning (PEFT) methods have become the standard paradigm for adapting large-scale models. Among these techniques, Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the learning capacity and training stability of the vanilla Low-Rank Adaptation (LoRA) method by explicitly decomposing pre-trained weights into magnitude and directional components. In this work, we propose DoRAN, a new variant of DoRA designed to further stabilize training and boost the sample efficiency of DoRA. Our approach includes two key stages: (i) injecting noise into the denominator of DoRA's weight decomposition, which serves as an adaptive regularizer to mitigate instabilities; and (ii) replacing static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling across layers and yielding better sample efficiency in both theory and practice. Comprehensive experiments on vision and language benchmarks show that DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These results underscore the effectiveness of combining stabilization through noise-based regularization with network-based parameter generation, offering a promising direction for robust and efficient fine-tuning of foundation models.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.198442"
    },
    {
        "index": "#90",
        "title": "Arithmetic-Mean $μ$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets",
        "link": "/arxiv/2510.04327",
        "arxiv_id": "2510.04327",
        "authors": "Haosong Zhang, Shenxi Wu, Yichi Zhang, Wei Lin",
        "summary": "Choosing an appropriate learning rate remains a key challenge in scaling depth of modern deep networks. The classical maximal update parameterization ($\\mu$P) enforces a fixed per-layer update magnitude, which is well suited to homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in heterogeneous architectures where residual accumulation and convolutions introduce imbalance across layers. We introduce Arithmetic-Mean $\\mu$P (AM-$\\mu$P), which constrains not each individual layer but the network-wide average one-step pre-activation second moment to a constant scale. Combined with a residual-aware He fan-in initialization - scaling residual-branch weights by the number of blocks ($\\mathrm{Var}[W]=c/(K\\cdot \\mathrm{fan\\text{-}in})$) - AM-$\\mu$P yields width-robust depth laws that transfer consistently across depths. We prove that, for one- and two-dimensional convolutional networks, the maximal-update learning rate satisfies $\\eta^\\star(L)\\propto L^{-3/2}$; with zero padding, boundary effects are constant-level as $N\\gg k$. For standard residual networks with general conv+MLP blocks, we establish $\\eta^\\star(L)=\\Theta(L^{-3/2})$, with $L$ the minimal depth. Empirical results across a range of depths confirm the $-3/2$ scaling law and enable zero-shot learning-rate transfer, providing a unified and practical LR principle for convolutional and deep residual networks without additional tuning overhead.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.199095"
    },
    {
        "index": "#91",
        "title": "FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields",
        "link": "/arxiv/2510.04325",
        "arxiv_id": "2510.04325",
        "authors": "Kenechukwu Ogbuagu, Sepehr Maleki, Giuseppe Bruni, Senthil Krishnababu",
        "summary": "The accurate prediction of flow fields around airfoils is crucial for aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models are effective but computationally expensive, thus inspiring the development of surrogate models to enable quicker predictions. These surrogate models can be based on deep learning architectures, such as Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion models have shown significant promise in predicting complex flow fields. In this work, we propose FoilDiff, a diffusion-based surrogate model with a hybrid-backbone denoising network. This hybrid design combines the power of convolutional feature extraction and transformer-based global attention to generate more adaptable and accurate representations of flow structures. FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling to optimise the efficiency of the sampling process at no additional cost to model generalisation. We used encoded representations of Reynolds number, angle of attack, and airfoil geometry to define the input space for generalisation across a wide range of aerodynamic conditions. When evaluated against state-of-the-art models, FoilDiff shows significant performance improvements, with mean prediction errors reducing by up to 85\\% on the same datasets. The results have demonstrated that FoilDiff can provide both more accurate predictions and better-calibrated predictive uncertainty than existing diffusion-based models.",
        "subjects": "Machine Learning, Fluid Dynamics",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.199775"
    },
    {
        "index": "#93",
        "title": "Crash Severity Prediction Using Deep Learning Approaches: A Hybrid CNN-RNN Framework",
        "link": "/arxiv/2510.04316",
        "arxiv_id": "2510.04316",
        "authors": "Sahar Koohfar",
        "summary": "Accurate and timely prediction of crash severity is crucial in mitigating the severe consequences of traffic accidents. Accurate and timely prediction of crash severity is crucial in mitigating the severe consequences of traffic accidents. In order to provide appropriate levels of medical assistance and transportation services, an intelligent transportation system relies on effective prediction methods. Deep learning models have gained popularity in this domain due to their capability to capture non-linear relationships among variables. In this research, we have implemented a hybrid CNN-RNN deep learning model for crash severity prediction and compared its performance against widely used statistical and machine learning models such as logistic regression, naïve bayes classifier, K-Nearest Neighbors (KNN), decision tree, and individual deep learning models: RNN and CNN. This study employs a methodology that considers the interconnected relationships between various features of traffic accidents. The study was conducted using a dataset of 15,870 accident records gathered over a period of seven years between 2015 and 2021 on Virginia highway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model has outperformed all benchmark models in terms of predicting crash severity. This result illustrates the effectiveness of the hybrid model as it combines the advantages of both RNN and CNN models in order to achieve greater accuracy in the prediction process.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.200942"
    },
    {
        "index": "#94",
        "title": "Activation Steering with a Feedback Controller",
        "link": "/arxiv/2510.04309",
        "arxiv_id": "2510.04309",
        "authors": "Dung V. Nguyen, Hieu M. Vu, Nhi Y. Pham, Lei Zhang, Tan M. Nguyen",
        "summary": "Controlling the behaviors of large language models (LLM) is fundamental to their safety alignment and reliable deployment. However, existing steering methods are primarily driven by empirical insights and lack theoretical performance guarantees. In this work, we develop a control-theoretic foundation for activation steering by showing that popular steering methods correspond to the proportional (P) controllers, with the steering vector serving as the feedback signal. Building on this finding, we propose Proportional-Integral-Derivative (PID) Steering, a principled framework that leverages the full PID controller for activation steering in LLMs. The proportional (P) term aligns activations with target semantic directions, the integral (I) term accumulates errors to enforce persistent corrections across layers, and the derivative (D) term mitigates overshoot by counteracting rapid activation changes. This closed-loop design yields interpretable error dynamics and connects activation steering to classical stability guarantees in control theory. Moreover, PID Steering is lightweight, modular, and readily integrates with state-of-the-art steering methods. Extensive experiments across multiple LLM families and benchmarks demonstrate that PID Steering consistently outperforms existing approaches, achieving more robust and reliable behavioral control.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.206569"
    },
    {
        "index": "#96",
        "title": "HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks",
        "link": "/arxiv/2510.04295",
        "arxiv_id": "2510.04295",
        "authors": "Nghiem T. Diep, Dung Le, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho",
        "summary": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) technique that adapts large pre-trained models by adding low-rank matrices to their weight updates. However, in the context of fine-tuning multi-head self-attention (MHA), LoRA has been employed to adapt each attention head separately, thereby overlooking potential synergies across different heads. To mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA) method, which utilizes joint hypernetworks to generate low-rank matrices across attention heads. By coupling their adaptation through a shared generator, HoRA encourages cross-head information sharing, and thus directly addresses the aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens of hierarchical mixture of experts, our theoretical findings reveal that the latter achieves superior sample efficiency to the former. Furthermore, through extensive experiments across diverse language and vision benchmarks, we demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring only a marginal increase in the number of trainable parameters.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.207963"
    },
    {
        "index": "#98",
        "title": "Influence branching for learning to solve mixed-integer programs online",
        "link": "/arxiv/2510.04273",
        "arxiv_id": "2510.04273",
        "authors": "Paul Strang, Zacharie Alès, Côme Bissuel, Olivier Juan, Safia Kedad-Sidhoum, Emmanuel Rachelson",
        "summary": "On the occasion of the 20th Mixed Integer Program Workshop's computational competition, this work introduces a new approach for learning to solve MIPs online. Influence branching, a new graph-oriented variable selection strategy, is applied throughout the first iterations of the branch and bound algorithm. This branching heuristic is optimized online with Thompson sampling, which ranks the best graph representations of MIP's structure according to computational speed up over SCIP. We achieve results comparable to state of the art online learning methods. Moreover, our results indicate that our method generalizes well to more general online frameworks, where variations in constraint matrix, constraint vector and objective coefficients can all occur and where more samples are available.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.209145"
    },
    {
        "index": "#101",
        "title": "Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions",
        "link": "/arxiv/2510.04237",
        "arxiv_id": "2510.04237",
        "authors": "Jinhui Bai, Andreas Christmann, Lei Shi",
        "summary": "In this paper, we propose a novel kernel stochastic gradient descent (SGD) algorithm for large-scale supervised learning with general losses. Compared to traditional kernel SGD, our algorithm improves efficiency and scalability through an innovative regularization strategy. By leveraging the infinite series expansion of spherical radial basis functions, this strategy projects the stochastic gradient onto a finite-dimensional hypothesis space, which is adaptively scaled according to the bias-variance trade-off, thereby enhancing generalization performance. Based on a new estimation of the spectral structure of the kernel-induced covariance operator, we develop an analytical framework that unifies optimization and generalization analyses. We prove that both the last iterate and the suffix average converge at minimax-optimal rates, and we further establish optimal strong convergence in the reproducing kernel Hilbert space. Our framework accommodates a broad class of classical loss functions, including least-squares, Huber, and logistic losses. Moreover, the proposed algorithm significantly reduces computational complexity and achieves optimal storage complexity by incorporating coordinate-wise updates from linear SGD, thereby avoiding the costly pairwise operations typical of kernel SGD and enabling efficient processing of streaming data. Finally, extensive numerical experiments demonstrate the efficiency of our approach.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.210938"
    },
    {
        "index": "#106",
        "title": "Adaptive Federated Learning via Dynamical System Model",
        "link": "/arxiv/2510.04203",
        "arxiv_id": "2510.04203",
        "authors": "Aayushya Agarwal, Larry Pileggi, Gauri Joshi",
        "summary": "Hyperparameter selection is critical for stable and efficient convergence of heterogeneous federated learning, where clients differ in computational capabilities, and data distributions are non-IID. Tuning hyperparameters is a manual and computationally expensive process as the hyperparameter space grows combinatorially with the number of clients. To address this, we introduce an end-to-end adaptive federated learning method in which both clients and central agents adaptively select their local learning rates and momentum parameters. Our approach models federated learning as a dynamical system, allowing us to draw on principles from numerical simulation and physical design. Through this perspective, selecting momentum parameters equates to critically damping the system for fast, stable convergence, while learning rates for clients and central servers are adaptively selected to satisfy accuracy properties from numerical simulation. The result is an adaptive, momentum-based federated learning algorithm in which the learning rates for clients and servers are dynamically adjusted and controlled by a single, global hyperparameter. By designing a fully integrated solution for both adaptive client updates and central agent aggregation, our method is capable of handling key challenges of heterogeneous federated learning, including objective inconsistency and client drift. Importantly, our approach achieves fast convergence while being insensitive to the choice of the global hyperparameter, making it well-suited for rapid prototyping and scalable deployment. Compared to state-of-the-art adaptive methods, our framework is shown to deliver superior convergence for heterogeneous federated learning while eliminating the need for hyperparameter tuning both client and server updates.",
        "subjects": "Machine Learning, Systems and Control",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.219266"
    },
    {
        "index": "#107",
        "title": "Spectral Alignment as Predictor of Loss Explosion in Neural Network Training",
        "link": "/arxiv/2510.04202",
        "arxiv_id": "2510.04202",
        "authors": "Haiquan Qiu, You Wu, Yingjie Tan, Yaqing Wang, Quanming Yao",
        "summary": "Loss explosions in training deep neural networks can nullify multi-million dollar training runs. Conventional monitoring metrics like weight and gradient norms are often lagging and ambiguous predictors, as their values vary dramatically across different models and even between layers of the same model, making it difficult to establish a unified standard for detecting impending failure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded metric that monitors the distributional alignment between layer inputs and the principal singular vectors of weight matrices. We show that a collapse in the sign diversity of this alignment is a powerful early predictor of representational collapse and training divergence. Empirical results on language models demonstrate that monitoring the SA distribution provides a significantly earlier and clearer warning of loss explosions than traditional scalar metrics. SA's low computational overhead makes it a practical tool for safeguarding model training.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.219933"
    },
    {
        "index": "#110",
        "title": "Efficient Manifold-Constrained Neural ODE for High-Dimensional Datasets",
        "link": "/arxiv/2510.04138",
        "arxiv_id": "2510.04138",
        "authors": "Muhao Guo, Haoran Li, Yang Weng",
        "summary": "Neural ordinary differential equations (NODE) have garnered significant attention for their design of continuous-depth neural networks and the ability to learn data/feature dynamics. However, for high-dimensional systems, estimating dynamics requires extensive calculations and suffers from high truncation errors for the ODE solvers. To address the issue, one intuitive approach is to consider the non-trivial topological space of the data distribution, i.e., a low-dimensional manifold. Existing methods often rely on knowledge of the manifold for projection or implicit transformation, restricting the ODE solutions on the manifold. Nevertheless, such knowledge is usually unknown in realistic scenarios. Therefore, we propose a novel approach to explore the underlying manifold to restrict the ODE process. Specifically, we employ a structure-preserved encoder to process data and find the underlying graph to approximate the manifold. Moreover, we propose novel methods to combine the NODE learning with the manifold, resulting in significant gains in computational speed and accuracy. Our experimental evaluations encompass multiple datasets, where we compare the accuracy, number of function evaluations (NFEs), and convergence speed of our model against existing baselines. Our results demonstrate superior performance, underscoring the effectiveness of our approach in addressing the challenges of high-dimensional datasets.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.230723"
    },
    {
        "index": "#112",
        "title": "Modeling Time Series Dynamics with Fourier Ordinary Differential Equations",
        "link": "/arxiv/2510.04133",
        "arxiv_id": "2510.04133",
        "authors": "Muhao Guo, Yang Weng",
        "summary": "Neural ODEs (NODEs) have emerged as powerful tools for modeling time series data, offering the flexibility to adapt to varying input scales and capture complex dynamics. However, they face significant challenges: first, their reliance on time-domain representations often limits their ability to capture long-term dependencies and periodic structures; second, the inherent mismatch between their continuous-time formulation and the discrete nature of real-world data can lead to loss of granularity and predictive accuracy. To address these limitations, we propose Fourier Ordinary Differential Equations (FODEs), an approach that embeds the dynamics in the Fourier domain. By transforming time-series data into the frequency domain using the Fast Fourier Transform (FFT), FODEs uncover global patterns and periodic behaviors that remain elusive in the time domain. Additionally, we introduce a learnable element-wise filtering mechanism that aligns continuous model outputs with discrete observations, preserving granularity and enhancing accuracy. Experiments on various time series datasets demonstrate that FODEs outperform existing methods in terms of both accuracy and efficiency. By effectively capturing both long- and short-term patterns, FODEs provide a robust framework for modeling time series dynamics.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.232004"
    },
    {
        "index": "#115",
        "title": "On the Statistical Query Complexity of Learning Semiautomata: a Random Walk Approach",
        "link": "/arxiv/2510.04115",
        "arxiv_id": "2510.04115",
        "authors": "George Giapitzakis, Kimon Fountoulakis, Eshaan Nichani, Jason D. Lee",
        "summary": "Semiautomata form a rich class of sequence-processing algorithms with applications in natural language processing, robotics, computational biology, and data mining. We establish the first Statistical Query hardness result for semiautomata under the uniform distribution over input words and initial states. We show that Statistical Query hardness can be established when both the alphabet size and input length are polynomial in the number of states. Unlike the case of deterministic finite automata, where hardness typically arises through the hardness of the language they recognize (e.g., parity), our result is derived solely from the internal state-transition structure of semiautomata. Our analysis reduces the task of distinguishing the final states of two semiautomata to studying the behavior of a random walk on the group $S_{N} \\times S_{N}$. By applying tools from Fourier analysis and the representation theory of the symmetric group, we obtain tight spectral gap bounds, demonstrating that after a polynomial number of steps in the number of states, distinct semiautomata become nearly uncorrelated, yielding the desired hardness result.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.234007"
    },
    {
        "index": "#116",
        "title": "Wasserstein projection distance for fairness testing of regression models",
        "link": "/arxiv/2510.04114",
        "arxiv_id": "2510.04114",
        "authors": "Wanxin Li, Yongjin P. Park, Khanh Dao Duc",
        "summary": "Fairness in machine learning is a critical concern, yet most research has focused on classification tasks, leaving regression models underexplored. This paper introduces a Wasserstein projection-based framework for fairness testing in regression models, focusing on expectation-based criteria. We propose a hypothesis-testing approach and an optimal data perturbation method to improve fairness while balancing accuracy. Theoretical results include a detailed categorization of fairness criteria for regression, a dual reformulation of the Wasserstein projection test statistic, and the derivation of asymptotic bounds and limiting distributions. Experiments on synthetic and real-world datasets demonstrate that the proposed method offers higher specificity compared to permutation-based tests, and effectively detects and mitigates biases in real applications such as student performance and housing price prediction.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.234617"
    },
    {
        "index": "#117",
        "title": "Can Linear Probes Measure LLM Uncertainty?",
        "link": "/arxiv/2510.04108",
        "arxiv_id": "2510.04108",
        "authors": "Ramzi Dakhmouche, Adrien Letellier, Hossein Gorji",
        "summary": "Effective Uncertainty Quantification (UQ) represents a key aspect for reliable deployment of Large Language Models (LLMs) in automated decision-making and beyond. Yet, for LLM generation with multiple choice structure, the state-of-the-art in UQ is still dominated by the naive baseline given by the maximum softmax score. To address this shortcoming, we demonstrate that taking a principled approach via Bayesian statistics leads to improved performance despite leveraging the simplest possible model, namely linear regression. More precisely, we propose to train multiple Bayesian linear models, each predicting the output of a layer given the output of the previous one. Based on the obtained layer-level posterior distributions, we infer the global uncertainty level of the LLM by identifying a sparse combination of distributional features, leading to an efficient UQ scheme. Numerical experiments on various LLMs show consistent improvement over state-of-the-art baselines.",
        "subjects": "Machine Learning, Numerical Analysis, Statistics Theory",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.240425"
    },
    {
        "index": "#118",
        "title": "Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws",
        "link": "/arxiv/2510.04102",
        "arxiv_id": "2510.04102",
        "authors": "Ramzi Dakhmouche, Hossein Gorji",
        "summary": "Motivated by the remarkable success of Foundation Models (FMs) in language modeling, there has been growing interest in developing FMs for time series prediction, given the transformative power such models hold for science and engineering. This culminated in significant success of FMs in short-range forecasting settings. However, extrapolation or long-range forecasting remains elusive for FMs, which struggle to outperform even simple baselines. This contrasts with physical laws which have strong extrapolation properties, and raises the question of the fundamental difference between the structure of neural networks and physical laws. In this work, we identify and formalize a fundamental property characterizing the ability of statistical learning models to predict more accurately outside of their training domain, hence explaining performance deterioration for deep learning models in extrapolation settings. In addition to a theoretical analysis, we present empirical results showcasing the implications of this property on current deep learning architectures. Our results not only clarify the root causes of the extrapolation gap but also suggest directions for designing next-generation forecasting models capable of mastering extrapolation.",
        "subjects": "Machine Learning, Numerical Analysis, Probability",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.241099"
    },
    {
        "index": "#119",
        "title": "Rethinking Consistent Multi-Label Classification under Inexact Supervision",
        "link": "/arxiv/2510.04091",
        "arxiv_id": "2510.04091",
        "authors": "Wei Wang, Tianhao Ma, Ming-Kun Xie, Gang Niu, Masashi Sugiyama",
        "summary": "Partial multi-label learning and complementary multi-label learning are two popular weakly supervised multi-label classification paradigms that aim to alleviate the high annotation costs of collecting precisely annotated multi-label data. In partial multi-label learning, each instance is annotated with a candidate label set, among which only some labels are relevant; in complementary multi-label learning, each instance is annotated with complementary labels indicating the classes to which the instance does not belong. Existing consistent approaches for the two paradigms either require accurate estimation of the generation process of candidate or complementary labels or assume a uniform distribution to eliminate the estimation problem. However, both conditions are usually difficult to satisfy in real-world scenarios. In this paper, we propose consistent approaches that do not rely on the aforementioned conditions to handle both problems in a unified way. Specifically, we propose two unbiased risk estimators based on first- and second-order strategies. Theoretically, we prove consistency w.r.t. two widely used multi-label classification evaluation metrics and derive convergence rates for the estimation errors of the proposed risk estimators. Empirically, extensive experimental results validate the effectiveness of our proposed approaches against state-of-the-art methods.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.241736"
    },
    {
        "index": "#124",
        "title": "Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints",
        "link": "/arxiv/2510.04058",
        "arxiv_id": "2510.04058",
        "authors": "Subhodip Panda, MS Varun, Shreyans Jain, Sarthak Kumar Maharana, Prathosh A. P",
        "summary": "For a responsible and safe deployment of diffusion models in various domains, regulating the generated outputs from these models is desirable because such models could generate undesired, violent, and obscene outputs. To tackle this problem, recent works use machine unlearning methodology to forget training data points containing these undesired features from pre-trained generative models. However, these methods proved to be ineffective in data-constrained settings where the whole training dataset is inaccessible. Thus, the principal objective of this work is to propose a machine unlearning methodology that can prevent the generation of outputs containing undesired features from a pre-trained diffusion model in such a data-constrained setting. Our proposed method, termed as Variational Diffusion Unlearning (VDU), is a computationally efficient method that only requires access to a subset of training data containing undesired features. Our approach is inspired by the variational inference framework with the objective of minimizing a loss function consisting of two terms: plasticity inducer and stability regularizer. Plasticity inducer reduces the log-likelihood of the undesired training data points, while the stability regularizer, essential for preventing loss of image generation quality, regularizes the model in parameter space. We validate the effectiveness of our method through comprehensive experiments for both class unlearning and feature unlearning. For class unlearning, we unlearn some user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a pre-trained unconditional denoising diffusion probabilistic model (DDPM). Similarly, for feature unlearning, we unlearn the generation of certain high-level features from a pre-trained Stable Diffusion model",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.245134"
    },
    {
        "index": "#125",
        "title": "Adaptive kernel-density approach for imbalanced binary classification",
        "link": "/arxiv/2510.04046",
        "arxiv_id": "2510.04046",
        "authors": "Kotaro J. Nishimura, Yuichi Sakumura, Kazushi Ikeda",
        "summary": "Class imbalance is a common challenge in real-world binary classification tasks, often leading to predictions biased toward the majority class and reduced recognition of the minority class. This issue is particularly critical in domains such as medical diagnosis and anomaly detection, where correct classification of minority classes is essential. Conventional methods often fail to deliver satisfactory performance when the imbalance ratio is extremely severe. To address this challenge, we propose a novel approach called Kernel-density-Oriented Threshold Adjustment with Regional Optimization (KOTARO), which extends the framework of kernel density estimation (KDE) by adaptively adjusting decision boundaries according to local sample density. In KOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on the estimated density around each sample, thereby enhancing the classifier's ability to capture minority regions. We validated the effectiveness of KOTARO through experiments on both synthetic and real-world imbalanced datasets. The results demonstrated that KOTARO outperformed conventional methods, particularly under conditions of severe imbalance, highlighting its potential as a promising solution for a wide range of imbalanced classification problems",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.250836"
    },
    {
        "index": "#127",
        "title": "Multi-Class Support Vector Machine with Differential Privacy",
        "link": "/arxiv/2510.04027",
        "arxiv_id": "2510.04027",
        "authors": "Jinseong Park, Yujin Choi, Jaewook Lee",
        "summary": "With the increasing need to safeguard data privacy in machine learning models, differential privacy (DP) is one of the major frameworks to build privacy-preserving models. Support Vector Machines (SVMs) are widely used traditional machine learning models due to their robust margin guarantees and strong empirical performance in binary classification. However, applying DP to multi-class SVMs is inadequate, as the standard one-versus-rest (OvR) and one-versus-one (OvO) approaches repeatedly query each data sample when building multiple binary classifiers, thus consuming the privacy budget proportionally to the number of classes. To overcome this limitation, we explore all-in-one SVM approaches for DP, which access each data sample only once to construct multi-class SVM boundaries with margin maximization properties. We propose a novel differentially Private Multi-class SVM (PMSVM) with weight and gradient perturbation methods, providing rigorous sensitivity and convergence analyses to ensure DP in all-in-one SVMs. Empirical results demonstrate that our approach surpasses existing DP-SVM methods in multi-class scenarios.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.252190"
    },
    {
        "index": "#131",
        "title": "Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints",
        "link": "/arxiv/2510.04006",
        "arxiv_id": "2510.04006",
        "authors": "Hang Fan, Yi Xiao, Yongquan Qu, Fenghua Ling, Ben Fei, Lei Bai, Pierre Gentine",
        "summary": "Data-driven machine learning (ML) models have recently shown promise in surpassing traditional physics-based approaches for weather forecasting, leading to a so-called second revolution in weather forecasting. However, most ML-based forecast models treat reanalysis as the truth and are trained under variable-specific loss weighting, ignoring their physical coupling and spatial structure. Over long time horizons, the forecasts become blurry and physically unrealistic under rollout training. To address this, we reinterpret model training as a weak-constraint four-dimensional variational data assimilation (WC-4DVar) problem, treating reanalysis data as imperfect observations. This allows the loss function to incorporate reanalysis error covariance and capture multivariate dependencies. In practice, we compute the loss in a latent space learned by an autoencoder (AE), where the reanalysis error covariance becomes approximately diagonal, thus avoiding the need to explicitly model it in the high-dimensional model space. We show that rollout training with latent-space constraints improves long-term forecast skill and better preserves fine-scale structures and physical realism compared to training with model-space loss. Finally, we extend this framework to accommodate heterogeneous data sources, enabling the forecast model to be trained jointly on reanalysis and multi-source observations within a unified theoretical formulation.",
        "subjects": "Machine Learning, Chaotic Dynamics, Atmospheric and Oceanic Physics",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.255025"
    },
    {
        "index": "#134",
        "title": "ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity",
        "link": "/arxiv/2510.03987",
        "arxiv_id": "2510.03987",
        "authors": "Michael Yang",
        "summary": "Hierarchical Pooling Models have demonstrated strong performance in classifying graph-structured data. While numerous innovative methods have been proposed to design cluster assignments and coarsening strategies, the relationships between clusters are often overlooked. In this paper, we introduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novel hierarchical pooling framework designed to enhance model's understanding of inter-cluster connectivity and ability of preserving the structural integrity in the original graph. ICEPool is compatible with a wide range of pooling-based GNN models. The deployment of ICEPool as an enhancement to existing models effectively combines the strengths of the original model with ICEPool's capability to emphasize the integration of inter-cluster connectivity, resulting in a more comprehensive and robust graph-level representation. Moreover, we make theoretical analysis to ICEPool's ability of graph reconstruction to demonstrate its effectiveness in learning inter-cluster relationship that is overlooked by conventional models. Finally, the experimental results show the compatibility of ICEPool with wide varieties of models and its potential to boost the performance of existing graph neural network architectures.",
        "subjects": "Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.261994"
    },
    {
        "index": "#135",
        "title": "Beyond Softmax: A New Perspective on Gradient Bandits",
        "link": "/arxiv/2510.03979",
        "arxiv_id": "2510.03979",
        "authors": "Emerson Melo, David Müller",
        "summary": "We establish a link between a class of discrete choice models and the theory of online learning and multi-armed bandits. Our contributions are: (i) sublinear regret bounds for a broad algorithmic family, encompassing Exp3 as a special case; (ii) a new class of adversarial bandit algorithms derived from generalized nested logit models \\citep{wen:2001}; and (iii) \\textcolor{black}{we introduce a novel class of generalized gradient bandit algorithms that extends beyond the widely used softmax formulation. By relaxing the restrictive independence assumptions inherent in softmax, our framework accommodates correlated learning dynamics across actions, thereby broadening the applicability of gradient bandit methods.} Overall, the proposed algorithms combine flexible model specification with computational efficiency via closed-form sampling probabilities. Numerical experiments in stochastic bandit settings demonstrate their practical effectiveness.",
        "subjects": "Machine Learning, Theoretical Economics",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.262591"
    },
    {
        "index": "#138",
        "title": "Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model",
        "link": "/arxiv/2510.03959",
        "arxiv_id": "2510.03959",
        "authors": "Iryna Stanishevska",
        "summary": "Thunderstorm-driven outages are difficult to predict because most storms do not cause damage, convective processes occur rapidly and chaotically, and the available public data are both noisy and incomplete. We develop a 24-48 h early-warning model for summer, thunderstorm-related outages in Michigan using only open sources (EAGLE-I for ground truth; METAR for weather). We use the publicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge National Laboratory for the U.S. Department of Energy. The pipeline preserves convective micro-signals from a sparse station network via parameter-specific kriging with hourly variograms and targeted overdrafting to retain extremes, and builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW spatial aggregates) capturing precursors of severe convection (moisture advection, wind shifts, and pressure drops). The two-stage model design, combining a logistic gate and an LSTM regressor, limits routine periods and reduces noise exposure. The study uses event-centric metrics (cluster-based hits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour windows around state-level peaks (>= 50,000), with uncertainty quantified by hourly moving-block bootstrap. On the test sample, Two-Stage detects more reference peaks across all windows (e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra false alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at +/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48 h (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP analysis confirms moisture-advection and wind/gust precursors, underscoring the value of the feature engineering. Despite open-data noise, the feature-driven pipeline yields actionable, event-focused early warnings for thunderstorm outages.",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.264411"
    },
    {
        "index": "#139",
        "title": "Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts",
        "link": "/arxiv/2510.03954",
        "arxiv_id": "2510.03954",
        "authors": "Tim Bary, Tiffanie Godelaine, Axel Abels, Benoît Macq",
        "summary": "Accurate ground truth estimation in medical screening programs often relies on coalitions of experts and peer second opinions. Algorithms that efficiently aggregate noisy annotations can enhance screening workflows, particularly when data arrive continuously and expert proficiency is initially unknown. However, existing algorithms do not meet the requirements for seamless integration into screening pipelines. We therefore propose an adaptive approach for real-time annotation that (I) supports on-the-fly labeling of incoming data, (II) operates without prior knowledge of medical experts or pre-labeled data, and (III) dynamically queries additional experts based on the latent difficulty of each instance. The method incrementally gathers expert opinions until a confidence threshold is met, providing accurate labels with reduced annotation overhead. We evaluate our approach on three multi-annotator classification datasets across different modalities. Results show that our adaptive querying strategy reduces the number of expert queries by up to 50% while achieving accuracy comparable to a non-adaptive baseline. Our code is available at https://github.com/tbary/MEDICS",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.265024"
    },
    {
        "index": "#140",
        "title": "What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis",
        "link": "/arxiv/2510.03950",
        "arxiv_id": "2510.03950",
        "authors": "Shahriar Kabir Nahin, Wenxiao Xiao, Joshua Liu, Anshuman Chhabra, Hongfu Liu",
        "summary": "Data-centric learning seeks to improve model performance from the perspective of data quality, and has been drawing increasing attention in the machine learning community. Among its key tools, influence functions provide a powerful framework to quantify the impact of individual training samples on model predictions, enabling practitioners to identify detrimental samples and retrain models on a cleaner dataset for improved performance. However, most existing work focuses on the question: \"what data benefits the learning model?\" In this paper, we take a step further and investigate a more fundamental question: \"what is the performance ceiling of the learning model?\" Unlike prior studies that primarily measure improvement through overall accuracy, we emphasize category-wise accuracy and aim for Pareto improvements, ensuring that every class benefits, rather than allowing tradeoffs where some classes improve at the expense of others. To address this challenge, we propose category-wise influence functions and introduce an influence vector that quantifies the impact of each training sample across all categories. Leveraging these influence vectors, we develop a principled criterion to determine whether a model can still be improved, and further design a linear programming-based sample reweighting framework to achieve Pareto performance improvements. Through extensive experiments on synthetic datasets, vision, and text benchmarks, we demonstrate the effectiveness of our approach in estimating and achieving a model's performance improvement across multiple categories of interest.",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.270758"
    },
    {
        "index": "#141",
        "title": "On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection",
        "link": "/arxiv/2510.03944",
        "arxiv_id": "2510.03944",
        "authors": "Weiqing He, Xiang Li, Tianqi Shang, Li Shen, Weijie Su, Qi Long",
        "summary": "Large language models (LLMs) raise concerns about content authenticity and integrity because they can generate human-like text at scale. Text watermarks, which embed detectable statistical signals into generated text, offer a provable way to verify content origin. Many detection methods rely on pivotal statistics that are i.i.d. under human-written text, making goodness-of-fit (GoF) tests a natural tool for watermark detection. However, GoF tests remain largely underexplored in this setting. In this paper, we systematically evaluate eight GoF tests across three popular watermarking schemes, using three open-source LLMs, two datasets, various generation temperatures, and multiple post-editing methods. We find that general GoF tests can improve both the detection power and robustness of watermark detectors. Notably, we observe that text repetition, common in low-temperature settings, gives GoF tests a unique advantage not exploited by existing methods. Our results highlight that classic GoF tests are a simple yet powerful and underused tool for watermark detection in LLMs.",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.271433"
    },
    {
        "index": "#144",
        "title": "Transductive and Learning-Augmented Online Regression",
        "link": "/arxiv/2510.03917",
        "arxiv_id": "2510.03917",
        "authors": "Vinod Raman, Shenghao Xie, Samson Zhou",
        "summary": "Motivated by the predictable nature of real-life in data streams, we study online regression when the learner has access to predictions about future examples. In the extreme case, called transductive online learning, the sequence of examples is revealed to the learner before the game begins. For this setting, we fully characterize the minimax expected regret in terms of the fat-shattering dimension, establishing a separation between transductive online regression and (adversarial) online regression. Then, we generalize this setting by allowing for noisy or \\emph{imperfect} predictions about future examples. Using our results for the transductive online setting, we develop an online learner whose minimax expected regret matches the worst-case regret, improves smoothly with prediction quality, and significantly outperforms the worst-case regret when future example predictions are precise, achieving performance similar to the transductive online learner. This enables learnability for previously unlearnable classes under predictable examples, aligning with the broader learning-augmented model paradigm.",
        "subjects": "Machine Learning, Data Structures and Algorithms",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.273320"
    },
    {
        "index": "#145",
        "title": "Generalized Fitted Q-Iteration with Clustered Data",
        "link": "/arxiv/2510.03912",
        "arxiv_id": "2510.03912",
        "authors": "Liyuan Hu, Jitao Wang, Zhenke Wu, Chengchun Shi",
        "summary": "This paper focuses on reinforcement learning (RL) with clustered data, which is commonly encountered in healthcare applications. We propose a generalized fitted Q-iteration (FQI) algorithm that incorporates generalized estimating equations into policy learning to handle the intra-cluster correlations. Theoretically, we demonstrate (i) the optimalities of our Q-function and policy estimators when the correlation structure is correctly specified, and (ii) their consistencies when the structure is mis-specified. Empirically, through simulations and analyses of a mobile health dataset, we find the proposed generalized FQI achieves, on average, a half reduction in regret compared to the standard FQI.",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.273928"
    },
    {
        "index": "#146",
        "title": "THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series",
        "link": "/arxiv/2510.03911",
        "arxiv_id": "2510.03911",
        "authors": "Yadav Mahesh Lorik, Kaushik Sarveswaran, Nagaraj Sundaramahalingam, Aravindakumar Venugopalan",
        "summary": "Time series anomaly detection forms a very crucial area in several domains but poses substantial challenges. Due to time series data possessing seasonality, trends, noise, and evolving patterns (concept drift), it becomes very difficult to set a general notion of what constitutes normal behavior. Anomalies themselves could be varied, ranging from a single outlier to contextual or collective anomalies, and are normally very rare; hence, the dataset is largely imbalanced. Additional layers of complexities arise due to the problems of increased dimensionality of modern time series, real-time detection criteria, setting up appropriate detection thresholds, and arriving at results that are interpretable. To embrace these multifaceted challenges, very strong, flexible, and interpretable approaches are required. This paper presents THEMIS, a new framework for time series anomaly detection that exploits pretrained knowledge from foundation models. THEMIS extracts embeddings from the encoder of the Chronos time series foundation model and applies outlier detection techniques like Local Outlier Factor and Spectral Decomposition on the self-similarity matrix, to spot anomalies in the data. Our experiments show that this modular method achieves SOTA results on the MSL dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets. Notably, THEMIS exceeds models trained specifically for anomaly detection, presenting hyperparameter robustness and interpretability by default. This paper advocates for pretrained representations from foundation models for performing efficient and adaptable anomaly detection for time series data.",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.274553"
    },
    {
        "index": "#147",
        "title": "LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis",
        "link": "/arxiv/2510.03904",
        "arxiv_id": "2510.03904",
        "authors": "Hangting Ye, Jinmeng Li, He Zhao, Mingchen Zhuge, Dandan Guo, Yi Chang, Hongyuan Zha",
        "summary": "Existing anomaly detection (AD) methods for tabular data usually rely on some assumptions about anomaly patterns, leading to inconsistent performance in real-world scenarios. While Large Language Models (LLMs) show remarkable reasoning capabilities, their direct application to tabular AD is impeded by fundamental challenges, including difficulties in processing heterogeneous data and significant privacy risks. To address these limitations, we propose LLM-DAS, a novel framework that repositions the LLM from a ``data processor'' to an ``algorithmist''. Instead of being exposed to raw data, our framework leverages the LLM's ability to reason about algorithms. It analyzes a high-level description of a given detector to understand its intrinsic weaknesses and then generates detector-specific, data-agnostic Python code to synthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities. This generated synthesis program, which is reusable across diverse datasets, is then instantiated to augment training data, systematically enhancing the detector's robustness by transforming the problem into a more discriminative two-class classification task. Extensive experiments on 36 TAD benchmarks show that LLM-DAS consistently boosts the performance of mainstream detectors. By bridging LLM reasoning with classic AD algorithms via programmatic synthesis, LLM-DAS offers a scalable, effective, and privacy-preserving approach to patching the logical blind spots of existing detectors.",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.275230"
    },
    {
        "index": "#148",
        "title": "BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty",
        "link": "/arxiv/2510.03893",
        "arxiv_id": "2510.03893",
        "authors": "Akshay Kudva, Joel A. Paulson",
        "summary": "Optimal design under uncertainty remains a fundamental challenge in advancing reliable, next-generation process systems. Robust optimization (RO) offers a principled approach by safeguarding against worst-case scenarios across a range of uncertain parameters. However, traditional RO methods typically require known problem structure, which limits their applicability to high-fidelity simulation environments. To overcome these limitations, recent work has explored robust Bayesian optimization (RBO) as a flexible alternative that can accommodate expensive, black-box objectives. Existing RBO methods, however, generally ignore available structural information and struggle to scale to high-dimensional settings. In this work, we introduce BONSAI (Bayesian Optimization of Network Systems under uncertAInty), a new RBO framework that leverages partial structural knowledge commonly available in simulation-based models. Instead of treating the objective as a monolithic black box, BONSAI represents it as a directed graph of interconnected white- and black-box components, allowing the algorithm to utilize intermediate information within the optimization process. We further propose a scalable Thompson sampling-based acquisition function tailored to the structured RO setting, which can be efficiently optimized using gradient-based methods. We evaluate BONSAI across a diverse set of synthetic and real-world case studies, including applications in process systems engineering. Compared to existing simulation-based RO algorithms, BONSAI consistently delivers more sample-efficient and higher-quality robust solutions, highlighting its practical advantages for uncertainty-aware design in complex engineering systems.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.280939"
    },
    {
        "index": "#150",
        "title": "On Provable Benefits of Muon in Federated Learning",
        "link": "/arxiv/2510.03866",
        "arxiv_id": "2510.03866",
        "authors": "Xinwen Zhang, Hongchang Gao",
        "summary": "The recently introduced optimizer, Muon, has gained increasing attention due to its superior performance across a wide range of applications. However, its effectiveness in federated learning remains unexplored. To address this gap, this paper investigates the performance of Muon in the federated learning setting. Specifically, we propose a new algorithm, FedMuon, and establish its convergence rate for nonconvex problems. Our theoretical analysis reveals multiple favorable properties of FedMuon. In particular, due to its orthonormalized update direction, the learning rate of FedMuon is independent of problem-specific parameters, and, importantly, it can naturally accommodate heavy-tailed noise. The extensive experiments on a variety of neural network architectures validate the effectiveness of the proposed algorithm.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.282243"
    },
    {
        "index": "#152",
        "title": "On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records",
        "link": "/arxiv/2510.03844",
        "arxiv_id": "2510.03844",
        "authors": "Sarah C. Lotspeich, Abbey Collins, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon, Lucy D'Agostino McGowan",
        "summary": "Objective: Electronic health records (EHR) data are prone to missingness and errors. Previously, we devised an \"enriched\" chart review protocol where a \"roadmap\" of auxiliary diagnoses (anchors) was used to recover missing values in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a missing hemoglobin A1c value would be considered unhealthy). Still, chart reviews are expensive and time-intensive, which limits the number of patients whose data can be reviewed. Now, we investigate the accuracy and scalability of a roadmap-driven algorithm, based on ICD-10 codes (International Classification of Diseases, 10th revision), to mimic expert chart reviews and recover missing values. Materials and Methods: In addition to the clinicians' original roadmap from our previous work, we consider new versions that were iteratively refined using large language models (LLM) in conjunction with clinical expertise to expand the list of auxiliary diagnoses. Using chart reviews for 100 patients from the EHR at an extensive learning health system, we examine algorithm performance with different roadmaps. Using the larger study of $1000$ patients, we applied the final algorithm, which used a roadmap with clinician-approved additions from the LLM. Results: The algorithm recovered as much, if not more, missing data as the expert chart reviewers, depending on the roadmap. Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing EHR data with similar accuracy to chart reviews and can feasibly be applied to large samples. Extending them to monitor other dimensions of data quality (e.g., plausability) is a promising future direction.",
        "subjects": "Machine Learning, Applications, Methodology",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.283643"
    },
    {
        "index": "#153",
        "title": "Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting",
        "link": "/arxiv/2510.03839",
        "arxiv_id": "2510.03839",
        "authors": "Behraj Khan, Tahir Qasim Syed",
        "summary": "We present a theoretical framework for M-FISHER, a method for sequential distribution shift detection and stable adaptation in streaming data. For detection, we construct an exponential martingale from non-conformity scores and apply Ville's inequality to obtain time-uniform guarantees on false alarm control, ensuring statistical validity at any stopping time. Under sustained shifts, we further bound the expected detection delay as $\\mathcal{O}(\\log(1/\\delta)/\\Gamma)$, where $\\Gamma$ reflects the post-shift information gain, thereby linking detection efficiency to distributional divergence. For adaptation, we show that Fisher-preconditioned updates of prompt parameters implement natural gradient descent on the distributional manifold, yielding locally optimal updates that minimize KL divergence while preserving stability and parameterization invariance. Together, these results establish M-FISHER as a principled approach for robust, anytime-valid detection and geometrically stable adaptation in sequential decision-making under covariate shift.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.284259"
    },
    {
        "index": "#154",
        "title": "Technical note on Fisher Information for Robust Federated Cross-Validation",
        "link": "/arxiv/2510.03838",
        "arxiv_id": "2510.03838",
        "authors": "Behraj Khan, Tahir Qasim Syed",
        "summary": "When training data are fragmented across batches or federated-learned across different geographic locations, trained models manifest performance degradation. That degradation partly owes to covariate shift induced by data having been fragmented across time and space and producing dissimilar empirical training distributions. Each fragment's distribution is slightly different to a hypothetical unfragmented training distribution of covariates, and to the single validation distribution. To address this problem, we propose Fisher Information for Robust fEderated validation (\\textbf{FIRE}). This method accumulates fragmentation-induced covariate shift divergences from the global training distribution via an approximate Fisher information. That term, which we prove to be a more computationally-tractable estimate, is then used as a per-fragment loss penalty, enabling scalable distribution alignment. FIRE outperforms importance weighting benchmarks by $5.1\\%$ at maximum and federated learning (FL) benchmarks by up to $5.3\\%$ on shifted validation sets.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.284859"
    },
    {
        "index": "#155",
        "title": "HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control",
        "link": "/arxiv/2510.03830",
        "arxiv_id": "2510.03830",
        "authors": "Alex Durkin, Jasper Stolte, Mehmet Mercangöz",
        "summary": "Start-ups and product grade-changes are critical steps in continuous-process plant operation, because any misstep immediately affects product quality and drives operational losses. These transitions have long relied on manual operation by a handful of expert operators, but the progressive retirement of that workforce is leaving plant owners without the tacit know-how needed to execute them consistently. In the absence of a process model, offline reinforcement learning (RL) promises to capture and even surpass human expertise by mining historical start-up and grade-change logs, yet standard offline RL struggles with distribution shift and value-overestimation whenever a learned policy ventures outside the data envelope. We introduce HOFLON (Hybrid Offline Learning + Online Optimization) to overcome those limitations. Offline, HOFLON learns (i) a latent data manifold that represents the feasible region spanned by past transitions and (ii) a long-horizon Q-critic that predicts the cumulative reward from state-action pairs. Online, it solves a one-step optimization problem that maximizes the Q-critic while penalizing deviations from the learned manifold and excessive rates of change in the manipulated variables. We test HOFLON on two industrial case studies: a polymerization reactor start-up and a paper-machine grade-change problem, and benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm. In both plants HOFLON not only surpasses IQL but also delivers, on average, better cumulative rewards than the best start-up or grade-change observed in the historical data, demonstrating its potential to automate transition operations beyond current expert capability.",
        "subjects": "Machine Learning, Systems and Control, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.285519"
    },
    {
        "index": "#158",
        "title": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models",
        "link": "/arxiv/2510.03817",
        "arxiv_id": "2510.03817",
        "authors": "Philipp Becker, Niklas Freymuth, Serge Thilges, Fabian Otto, Gerhard Neumann",
        "summary": "On-policy Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model's most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language Models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model's inference behavior. Across datasets, model families, and advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.292640"
    },
    {
        "index": "#160",
        "title": "Curriculum-Augmented GFlowNets For mRNA Sequence Generation",
        "link": "/arxiv/2510.03811",
        "arxiv_id": "2510.03811",
        "authors": "Aya Laajil, Abduragim Shtanchaev, Sajan Muhammad, Eric Moulines, Salem Lahlou",
        "summary": "Designing mRNA sequences is a major challenge in developing next-generation therapeutics, since it involves exploring a vast space of possible nucleotide combinations while optimizing sequence properties like stability, translation efficiency, and protein expression. While Generative Flow Networks are promising for this task, their training is hindered by sparse, long-horizon rewards and multi-objective trade-offs. We propose Curriculum-Augmented GFlowNets (CAGFN), which integrate curriculum learning with multi-objective GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based curriculum that progressively adapts the maximum sequence length guiding exploration from easier to harder subproblems. We also provide a new mRNA design environment for GFlowNets which, given a target protein sequence and a combination of biological objectives, allows for the training of models that generate plausible mRNA candidates. This provides a biologically motivated setting for applying and advancing GFlowNets in therapeutic sequence design. On different mRNA design tasks, CAGFN improves Pareto performance and biological plausibility, while maintaining diversity. Moreover, CAGFN reaches higher-quality solutions faster than a GFlowNet trained with random sequence sampling (no curriculum), and enables generalization to out-of-distribution sequences.",
        "subjects": "Machine Learning, Quantitative Methods",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.293986"
    },
    {
        "index": "#161",
        "title": "Robust Batched Bandits",
        "link": "/arxiv/2510.03798",
        "arxiv_id": "2510.03798",
        "authors": "Yunwen Guo, Yunlun Shu, Gongyi Zhuo, Tianyu Wang",
        "summary": "The batched multi-armed bandit (MAB) problem, in which rewards are collected in batches, is crucial for applications such as clinical trials. Existing research predominantly assumes light-tailed reward distributions, yet many real-world scenarios, including clinical outcomes, exhibit heavy-tailed characteristics. This paper bridges this gap by proposing robust batched bandit algorithms designed for heavy-tailed rewards, within both finite-arm and Lipschitz-continuous settings. We reveal a surprising phenomenon: in the instance-independent regime, as well as in the Lipschitz setting, heavier-tailed rewards necessitate a smaller number of batches to achieve near-optimal regret. In stark contrast, for the instance-dependent setting, the required number of batches to attain near-optimal regret remains invariant with respect to tail heaviness.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.294631"
    },
    {
        "index": "#162",
        "title": "Allocation of Parameters in Transformers",
        "link": "/arxiv/2510.03784",
        "arxiv_id": "2510.03784",
        "authors": "Ruoxi Yu, Haotian Jiang, Jingpu Cheng, Penghao Yu, Qianxiao Li, Zhong Li",
        "summary": "Transformers have achieved remarkable successes across a wide range of applications, yet the theoretical foundation of their model efficiency remains underexplored. In this work, we investigate how the model parameters -- mainly attention heads and head dimensions -- should be allocated across layers to balance expressivity and efficiency. We first provide mathematical analysis on the role of early layers in information extraction from an approximation perspective, with a theoretical characterization on the trade-off between the number of heads and head dimension under a fixed parameter budget. In addition, we uncover and prove the \\emph{saturation} behavior of softmax activations: Continuously increasing head dimensions can lead to diminishing returns in learning errors, particularly for long sequences. Supported by both theory and experiments, this saturation pattern suggests that later layers can operate more efficiently with reduced parameters. Combining these insights, we propose principled strategies for allocating attention heads and dimensions across Transformers' layers, shedding light on theoretically-grounded model efficiency of Transformer-based architectures.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.295312"
    },
    {
        "index": "#163",
        "title": "Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation",
        "link": "/arxiv/2510.03782",
        "arxiv_id": "2510.03782",
        "authors": "Guofu Xie, Chen Zhang, Xiao Zhang, Yunsheng Shi, Ting Yao, Jun Xu",
        "summary": "Adapting to diverse user needs at test time is a key challenge in controllable multi-objective generation. Existing methods are insufficient: merging-based approaches provide indirect, suboptimal control at the parameter level, often disregarding the impacts of multiple objectives. While decoding-based guidance is more direct, it typically requires aggregating logits from multiple expert models, incurring significant space overhead and relying heavily on individual model capacity. To address these issues, we introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model merging for guided decoding. We first identify a critical compatibility problem between the guidance and base models. In Stage 1, MAGE resolves this by dynamically constructing a more robust base model, merging a series of backbone models that account for multiple objectives. In Stage 2, we merge explicit and implicit value models into a unified guidance proxy, which then steers the decoding of the base model from Stage 1. Our analysis empirically validates Linear Mode Connectivity (LMC) in value models, explores the relationship between model merging and prediction ensembling, and demonstrates the enhanced controllability afforded by our approach. Extensive experiments show that our method outperforms existing approaches, achieving superior controllability, Pareto-optimal performance, and enhanced adaptability.",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.295977"
    },
    {
        "index": "#165",
        "title": "Neural Low-Discrepancy Sequences",
        "link": "/arxiv/2510.03745",
        "arxiv_id": "2510.03745",
        "authors": "Michael Etienne Van Huffel, Nathan Kirk, Makram Chahine, Daniela Rus, T. Konstantin Rusch",
        "summary": "Low-discrepancy points are designed to efficiently fill the space in a uniform manner. This uniformity is highly advantageous in many problems in science and engineering, including in numerical integration, computer vision, machine perception, computer graphics, machine learning, and simulation. Whereas most previous low-discrepancy constructions rely on abstract algebra and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced to exploit machine learning methods for generating point sets with lower discrepancy than previously possible. However, MPMC is limited to generating point sets and cannot be extended to low-discrepancy sequences (LDS), i.e., sequences of points in which every prefix has low discrepancy, a property essential for many applications. To address this limitation, we introduce Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based framework for generating LDS. Drawing inspiration from classical LDS, we train a neural network to map indices to points such that the resulting sequences exhibit minimal discrepancy across all prefixes. To this end, we deploy a two-stage learning process: supervised approximation of classical constructions followed by unsupervised fine-tuning to minimize prefix discrepancies. We demonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a significant margin with respect to discrepancy measures. Moreover, we demonstrate the effectiveness of $NeuroLDS$ across diverse applications, including numerical integration, robot motion planning, and scientific machine learning. These results highlight the promise and broad significance of Neural Low-Discrepancy Sequences. Our code can be found at https://github.com/camail-official/neuro-lds.",
        "subjects": "Machine Learning, Numerical Analysis",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.302484"
    },
    {
        "index": "#169",
        "title": "Personalized federated prototype learning in mixed heterogeneous data scenarios",
        "link": "/arxiv/2510.03726",
        "arxiv_id": "2510.03726",
        "authors": "Jiahao Zeng, Wolong Xing, Liangtao Shi, Xin Huang, Jialin Wang, Zhile Cao, Zhenkui Shi",
        "summary": "Federated learning has received significant attention for its ability to simultaneously protect customer privacy and leverage distributed data from multiple devices for model training. However, conventional approaches often focus on isolated heterogeneous scenarios, resulting in skewed feature distributions or label distributions. Meanwhile, data heterogeneity is actually a key factor in improving model performance. To address this issue, we propose a new approach called PFPL in mixed heterogeneous scenarios. The method provides richer domain knowledge and unbiased convergence targets by constructing personalized, unbiased prototypes for each client. Moreover, in the local update phase, we introduce consistent regularization to align local instances with their personalized prototypes, which significantly improves the convergence of the loss function. Experimental results on Digits and Office Caltech datasets validate the effectiveness of our approach and successfully reduce the communication cost.",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.305242"
    },
    {
        "index": "#170",
        "title": "Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach",
        "link": "/arxiv/2510.03722",
        "arxiv_id": "2510.03722",
        "authors": "Qianxin Yi, Shao-Bo Lin, Jun Fan, Yao Wang",
        "summary": "Reinforcement learning (RL) has been widely applied to sequential decision making, where interpretability and performance are both critical for practical adoption. Current approaches typically focus on performance and rely on post hoc explanations to account for interpretability. Different from these approaches, we focus on designing an interpretability-oriented yet performance-enhanced RL approach. Specifically, we propose a spectral based linear RL method that extends the ridge regression-based approach through a spectral filter function. The proposed method clarifies the role of regularization in controlling estimation error and further enables the design of an adaptive regularization parameter selection strategy guided by the bias-variance trade-off principle. Theoretical analysis establishes near-optimal bounds for both parameter estimation and generalization error. Extensive experiments on simulated environments and real-world datasets from Kuaishou and Taobao demonstrate that our method either outperforms or matches existing baselines in decision quality. We also conduct interpretability analyses to illustrate how the learned policies make decisions, thereby enhancing user trust. These results highlight the potential of our approach to bridge the gap between RL theory and practical decision making, providing interpretability, accuracy, and adaptability in management contexts.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.305896"
    },
    {
        "index": "#172",
        "title": "From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning",
        "link": "/arxiv/2510.03690",
        "arxiv_id": "2510.03690",
        "authors": "Ali Azizpour, Reza Ramezanpour, Ashutosh Sabharwal, Santiago Segarra",
        "summary": "Real-world graph datasets often consist of mixtures of populations, where graphs are generated from multiple distinct underlying distributions. However, modern representation learning approaches, such as graph contrastive learning (GCL) and augmentation methods like Mixup, typically overlook this mixture structure. In this work, we propose a unified framework that explicitly models data as a mixture of underlying probabilistic graph generative models represented by graphons. To characterize these graphons, we leverage graph moments (motif densities) to cluster graphs arising from the same model. This enables us to disentangle the mixture components and identify their distinct generative mechanisms. This model-aware partitioning benefits two key graph learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data augmentation technique that interpolates in a semantically valid space guided by the estimated graphons, instead of assuming a single graphon per class. 2) For GCL, it enables model-adaptive and principled augmentations. Additionally, by introducing a new model-aware objective, our proposed approach (termed MGCL) improves negative sampling by restricting negatives to graphs from other models. We establish a key theoretical guarantee: a novel, tighter bound showing that graphs sampled from graphons with small cut distance will have similar motif densities with high probability. Extensive experiments on benchmark datasets demonstrate strong empirical performance. In unsupervised learning, MGCL achieves state-of-the-art results, obtaining the top average rank across eight datasets. In supervised learning, GMAM consistently outperforms existing strategies, achieving new state-of-the-art accuracy in 6 out of 7 datasets.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.307483"
    },
    {
        "index": "#173",
        "title": "Group Policy Gradient",
        "link": "/arxiv/2510.03679",
        "arxiv_id": "2510.03679",
        "authors": "Junhua Chen, Zixi Zhang, Hantao Zhong, Rika Antonova",
        "summary": "We introduce Group Policy Gradient (GPG), a family of critic-free policy-gradient estimators for general MDPs. Inspired by the success of GRPO's approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a learned value function with a group-based Monte Carlo advantage estimator, removing the memory, compute, and hyperparameter costs of training a critic while preserving PPO's clipped-objective structure. We prove the consistency of the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate empirically that GPG matches or outperforms PPO on standard benchmarks. GPG makes better use of parallel simulations, which, together with its critic-free design, results in more efficient use of computational resources than PPO.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.308145"
    },
    {
        "index": "#174",
        "title": "Towards Sampling Data Structures for Tensor Products in Turnstile Streams",
        "link": "/arxiv/2510.03678",
        "arxiv_id": "2510.03678",
        "authors": "Zhao Song, Shenghao Xie, Samson Zhou",
        "summary": "This paper studies the computational challenges of large-scale attention-based models in artificial intelligence by utilizing importance sampling methods in the streaming setting. Inspired by the classical definition of the $\\ell_2$ sampler and the recent progress of the attention scheme in Large Language Models (LLMs), we propose the definition of the attention sampler. Our approach significantly reduces the computational burden of traditional attention mechanisms. We analyze the effectiveness of the attention sampler from a theoretical perspective, including space and update time. Additionally, our framework exhibits scalability and broad applicability across various model architectures and domains.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.308776"
    },
    {
        "index": "#178",
        "title": "Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast",
        "link": "/arxiv/2510.03657",
        "arxiv_id": "2510.03657",
        "authors": "Aymeric Fabre",
        "summary": "In electricity markets around the world, the ability to anticipate price movements with precision can be the difference between profit and loss, especially for fast-acting assets like battery energy storage systems (BESS). As grid volatility increases due to renewables and market decentralisation, operators and forecasters alike face growing pressure to transform prediction into strategy. Yet while forecast data is abundant, especially in advanced markets like Australia's National Electricity Market (NEM), its practical value in driving real-world BESS trading decisions remains largely unexplored. This thesis dives into that gap. This work addresses a key research question: Can the accuracy of the Australian Energy Market Operator (AEMO) energy price forecasts be systematically leveraged to develop a reliable and profitable battery energy storage system trading algorithm? Despite the availability of AEMO price forecasts, no existing framework evaluates their reliability or incorporates them into practical BESS trading strategies. By analysing patterns in forecast accuracy based on time of day, forecast horizon, and regional variations, this project creates a novel, forecast-informed BESS trading model to optimise arbitrage financial returns. The performance of this forecast-driven algorithm is benchmarked against a basic trading algorithm with no knowledge of forecast data. The study further explores the potential of machine learning techniques to predict future energy prices by enhancing AEMO forecasts to govern a more advanced trading strategy. The research outcomes will inform future improvements in energy market trading models and promote more efficient BESS integration into market operations.",
        "subjects": "Machine Learning, Systems and Control",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.316746"
    },
    {
        "index": "#180",
        "title": "SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network",
        "link": "/arxiv/2510.03648",
        "arxiv_id": "2510.03648",
        "authors": "Huijing Zhang, Muyang Cao, Linshan Jiang, Xin Du, Di Yu, Changze Lv, Shuiguang Deng",
        "summary": "Continuous learning of novel classes is crucial for edge devices to preserve data privacy and maintain reliable performance in dynamic environments. However, the scenario becomes particularly challenging when data samples are insufficient, requiring on-device few-shot class-incremental learning (FSCIL) to maintain consistent model performance. Although existing work has explored parameter-efficient FSCIL frameworks based on artificial neural networks (ANNs), their deployment is still fundamentally constrained by limited device resources. Inspired by neural mechanisms, Spiking neural networks (SNNs) process spatiotemporal information efficiently, offering lower energy consumption, greater biological plausibility, and compatibility with neuromorphic hardware than ANNs. In this work, we present an SNN-based method for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We first propose sparsity-conditioned neuronal dynamics, in which most neurons remain stable while a subset stays active, thereby mitigating catastrophic forgetting. To further cope with spike non-differentiability in gradient estimation, we employ zeroth-order optimization. Moreover, during incremental learning sessions, we enhance the discriminability of new classes through subspace projection, which alleviates overfitting to novel classes. Extensive experiments conducted on two standard benchmark datasets (CIFAR100 and Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture, and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods, specifically achieving at least 4.01% improvement at the last incremental session on Mini-ImageNet and 20% lower energy cost over baseline methods with practical implementation.",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.318145"
    },
    {
        "index": "#181",
        "title": "In-Vivo Training for Deep Brain Stimulation",
        "link": "/arxiv/2510.03643",
        "arxiv_id": "2510.03643",
        "authors": "Nicholas Carter, Arkaprava Gupta, Prateek Ganguli, Benedikt Dietrich, Vibhor Krishna, Samarjit Chakraborty",
        "summary": "Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's Disease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL agents modulating the stimulation frequency and amplitude. But, these models rely on biomarkers that are not measurable in patients and are only present in brain-on-chip (BoC) simulations. In this work, we present an RL-based DBS approach that adapts these stimulation parameters according to brain activity measurable in vivo. Using a TD3 based RL agent trained on a model of the basal ganglia region of the brain, we see a greater suppression of biomarkers correlated with PD severity compared to modern clinical DBS implementations. Our agent outperforms the standard clinical approaches in suppressing PD biomarkers while relying on information that can be measured in a real world environment, thereby opening up the possibility of training personalized RL agents specific to individual patient needs.",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.318821"
    },
    {
        "index": "#186",
        "title": "Explore the Loss space with Hill-ADAM",
        "link": "/arxiv/2510.03613",
        "arxiv_id": "2510.03613",
        "authors": "Meenakshi Manikandan, Leilani Gilpin",
        "summary": "This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus towards escaping local minima in prescribed loss landscapes to find the global minimum. Hill-ADAM escapes minima by deterministically exploring the state space. This eliminates uncertainty from random gradient updates in stochastic algorithms while seldom converging at the first minimum that visits. In the paper we first derive an analytical approximation of the ADAM Optimizer step size at a particular model state. From there define the primary condition determining ADAM limitations in escaping local minima. The proposed optimizer algorithm Hill-ADAM alternates between error minimization and maximization. It maximizes to escape the local minimum and minimizes again afterward. This alternation provides an overall exploration throughout the loss space. This allows the deduction of the global minimum's state. Hill-ADAM was tested with 5 loss functions and 12 amber-saturated to cooler-shade image color correction instances.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.331340"
    },
    {
        "index": "#188",
        "title": "MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation",
        "link": "/arxiv/2510.03601",
        "arxiv_id": "2510.03601",
        "authors": "Wei-Lung Mao, Chun-Chi Wang, Po-Heng Chou, Kai-Chun Liu, Yu Tsao",
        "summary": "The rising aging population has increased the importance of fall detection (FD) systems as an assistive technology, where deep learning techniques are widely applied to enhance accuracy. FD systems typically use edge devices (EDs) worn by individuals to collect real-time data, which are transmitted to a cloud center (CC) or processed locally. However, this architecture faces challenges such as a limited ED model size and data transmission latency to the CC. Mobile edge computing (MEC), which allows computations at MEC servers deployed between EDs and CC, has been explored to address these challenges. We propose a multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC splits the architecture into stations, each with a neural network model. If front-end equipment cannot detect falls reliably, data are transmitted to a station with more robust back-end computing. The knowledge distillation (KD) approach was employed to improve front-end detection accuracy by allowing high-power back-end stations to provide additional learning experiences, enhancing precision while reducing latency and processing loads. Simulation results demonstrate that the KD approach improved accuracy by 11.65% on the SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD system exhibits improved accuracy and reduced latency.",
        "subjects": "Machine Learning, Distributed, Parallel, and Cluster Computing, Networking and Internet Architecture, Signal Processing",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.332781"
    },
    {
        "index": "#190",
        "title": "FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors",
        "link": "/arxiv/2510.03589",
        "arxiv_id": "2510.03589",
        "authors": "Ankit Bhardwaj, Ananth Balashankar, Lakshminarayanan Subramanian",
        "summary": "Spatio-temporal sensor data is often sparse, noisy, and irregular, and existing interpolation or learning methods struggle here because they either ignore governing PDEs or do not scale. We introduce FieldFormer, a transformer-based framework for mesh-free spatio-temporal field reconstruction that combines data-driven flexibility with physics-based structure. For each query, FieldFormer gathers a local neighborhood using a learnable velocity-scaled distance metric, enabling anisotropic adaptation to different propagation regimes. Neighborhoods are built efficiently via per-batch offset recomputation, and refined in an expectation-maximization style as the velocity scales evolve. Predictions are made by a local transformer encoder, and physics consistency is enforced through autograd-based PDE residuals and boundary-specific penalties. Across three benchmarks--a scalar anisotropic heat equation, a vector-valued shallow-water system, and a realistic advection-diffusion pollution simulation--FieldFormer consistently outperforms strong baselines by more than 40%. Our results demonstrate that FieldFormer enables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field reconstruction from sparse (0.4%-2%) and noisy(10%) data.",
        "subjects": "Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.334046"
    },
    {
        "index": "#192",
        "title": "BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems",
        "link": "/arxiv/2510.03576",
        "arxiv_id": "2510.03576",
        "authors": "Bongseok Kim, Jiahao Zhang, Guang Lin",
        "summary": "Deep learning has gained attention for solving PDEs, but the black-box nature of neural networks hinders precise enforcement of boundary conditions. To address this, we propose a boundary condition-guaranteed evolutionary Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN, we propose three distinct and combinable approaches for incorporating Dirichlet, periodic, and Neumann boundary conditions into the network. For Dirichlet problem, we use smooth and global Gaussian RBFs to construct univariate basis functions for approximating the solution and to encode boundary information at the activation level of the network. To handle periodic problems, we employ a periodic layer constructed from a set of sinusoidal functions to enforce the boundary conditions exactly. For a Neumann problem, we devise a least-squares formulation to guide the parameter evolution toward satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the periodic layer, and the evolutionary framework, we can perform accurate PDE simulations while rigorously enforcing boundary conditions. For demonstration, we conducted extensive numerical experiments on Dirichlet, Neumann, periodic, and mixed boundary value problems. The results indicate that BEKAN outperforms both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In conclusion, the proposed approach enhances the capability of KANs in solving PDE problems while satisfying boundary conditions, thereby facilitating advancements in scientific computing and engineering applications.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.335372"
    },
    {
        "index": "#193",
        "title": "Efficient Test-Time Scaling for Small Vision-Language Models",
        "link": "/arxiv/2510.03574",
        "arxiv_id": "2510.03574",
        "authors": "Mehmet Onurcan Kaya, Desmond Elliott, Dim P. Papadopoulos",
        "summary": "Small Vision-Language Models (VLMs) provide a computationally efficient alternative to larger models, at the cost of weaker generalization abilities and downstream task performance. These shortcomings could be addressed by test-time scaling techniques, but existing methods are typically computationally demanding, contradicting the resource-efficient design goals of small models. To address these limitations, we propose two novel and efficient test-time scaling strategies that leverage the model-internal features rather than external supervision: (i) Test-Time Augmentation (TTAug), which generates multiple augmented inputs and aggregates outputs at the token level without parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model parameters during inference using consensus-based pseudolabels from TTAug. Through extensive experiments across nine benchmarks, we demonstrate consistent performance improvements while maintaining computational efficiency suitable for resource-constrained environments. The generality of our approach is demonstrated both within models at different scales and across different VLMs without additional tuning.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.341216"
    },
    {
        "index": "#197",
        "title": "CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer",
        "link": "/arxiv/2510.03566",
        "arxiv_id": "2510.03566",
        "authors": "Ashwin Prabu, Nhat Thanh Tran, Guofa Zhou, Jack Xin",
        "summary": "A variety of models have been developed to forecast dengue cases to date. However, it remains a challenge to predict major dengue outbreaks that need timely public warnings the most. In this paper, we introduce CrossLag, an environmentally informed attention that allows for the incorporation of lagging endogenous signals behind the significant events in the exogenous data into the architecture of the transformer at low parameter counts. Outbreaks typically lag behind major changes in climate and oceanic anomalies. We use TimeXer, a recent general-purpose transformer distinguishing exogenous-endogenous inputs, as the baseline for this study. Our proposed model outperforms TimeXer by a considerable margin in detecting and predicting major outbreaks in Singapore dengue data over a 24-week prediction window.",
        "subjects": "Machine Learning, Computers and Society",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.343977"
    },
    {
        "index": "#198",
        "title": "Sequential decoder training for improved latent space dynamics identification",
        "link": "/arxiv/2510.03535",
        "arxiv_id": "2510.03535",
        "authors": "William Anderson, Seung Whan Chung, Youngsoo Choi",
        "summary": "Accurate numerical solutions of partial differential equations are essential in many scientific fields but often require computationally expensive solvers, motivating reduced-order models (ROMs). Latent Space Dynamics Identification (LaSDI) is a data-driven ROM framework that combines autoencoders with equation discovery to learn interpretable latent dynamics. However, enforcing latent dynamics during training can compromise reconstruction accuracy of the model for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that improves reconstruction and prediction accuracy by sequentially learning additional decoders to correct residual errors from previous stages. Applied to the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI, achieving lower prediction errors and reduced training time across a wide range of architectures.",
        "subjects": "Machine Learning, Numerical Analysis, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.344624"
    },
    {
        "index": "#200",
        "title": "RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models",
        "link": "/arxiv/2510.03515",
        "arxiv_id": "2510.03515",
        "authors": "Lianghuan Huang, Sagnik Anupam, Insup Lee, Shuo Li, Osbert Bastani",
        "summary": "Reinforcement learning (RL) has emerged as a promising strategy for finetuning small language models (SLMs) to solve targeted tasks such as math and coding. However, RL algorithms tend to be resource-intensive, taking a significant amount of time to train. We propose RAPID, a novel RL algorithm that can substantially reduce the running time of RL. Our key insight is that RL tends to be costly due to the need to perform both inference and backpropagation during training. To maximize use of computational resources, our algorithm performs inference in large batches, and then performs off-policy policy gradient updates in mini-batches. For off-policy updates, we incorporate group advantage estimation into the policy gradient algorithm, and derive an importance weighted estimator to correct for the bias arising from off-policy learning. Our experiments demonstrate that our algorithm can reduce running time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms while maintaining similar or better accuracy.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.351189"
    },
    {
        "index": "#201",
        "title": "A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT",
        "link": "/arxiv/2510.03513",
        "arxiv_id": "2510.03513",
        "authors": "Taha M. Mahmoud, Naima Kaabouch",
        "summary": "The rapid growth of the Internet of Things (IoT) has expanded opportunities for innovation but also increased exposure to botnet-driven cyberattacks. Conventional detection methods often struggle with scalability, privacy, and adaptability in resource-constrained IoT environments. To address these challenges, we present a lightweight and privacy-preserving botnet detection framework based on federated learning. This approach enables distributed devices to collaboratively train models without exchanging raw data, thus maintaining user privacy while preserving detection accuracy. A communication-efficient aggregation strategy is introduced to reduce overhead, ensuring suitability for constrained IoT networks. Experiments on benchmark IoT botnet datasets demonstrate that the framework achieves high detection accuracy while substantially reducing communication costs. These findings highlight federated learning as a practical path toward scalable, secure, and privacy-aware intrusion detection for IoT ecosystems.",
        "subjects": "Machine Learning, Cryptography and Security, Distributed, Parallel, and Cluster Computing",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.351844"
    },
    {
        "index": "#202",
        "title": "Task-Level Contrastiveness for Cross-Domain Few-Shot Learning",
        "link": "/arxiv/2510.03509",
        "arxiv_id": "2510.03509",
        "authors": "Kristi Topollai, Anna Choromanska",
        "summary": "Few-shot classification and meta-learning methods typically struggle to generalize across diverse domains, as most approaches focus on a single dataset, failing to transfer knowledge across various seen and unseen domains. Existing solutions often suffer from low accuracy, high computational costs, and rely on restrictive assumptions. In this paper, we introduce the notion of task-level contrastiveness, a novel approach designed to address issues of existing methods. We start by introducing simple ways to define task augmentations, and thereafter define a task-level contrastive loss that encourages unsupervised clustering of task representations. Our method is lightweight and can be easily integrated within existing few-shot/meta-learning algorithms while providing significant benefits. Crucially, it leads to improved generalization and computational efficiency without requiring prior knowledge of task domains. We demonstrate the effectiveness of our approach through different experiments on the MetaDataset benchmark, where it achieves superior performance without additional complexity.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.352422"
    },
    {
        "index": "#203",
        "title": "D2 Actor Critic: Diffusion Actor Meets Distributional Critic",
        "link": "/arxiv/2510.03508",
        "arxiv_id": "2510.03508",
        "authors": "Lunjun Zhang, Shuo Han, Hanrui Lyu, Bradly C Stadie",
        "summary": "We introduce D2AC, a new model-free reinforcement learning (RL) algorithm designed to train expressive diffusion policies online effectively. At its core is a policy improvement objective that avoids the high variance of typical policy gradients and the complexity of backpropagation through time. This stable learning process is critically enabled by our second contribution: a robust distributional critic, which we design through a fusion of distributional RL and clipped double Q-learning. The resulting algorithm is highly effective, achieving state-of-the-art performance on a benchmark of eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains, spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard benchmarks, we also evaluate a biologically motivated predator-prey task to examine the behavioral robustness and generalization capacity of our approach.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.353042"
    },
    {
        "index": "#204",
        "title": "Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability",
        "link": "/arxiv/2510.03494",
        "arxiv_id": "2510.03494",
        "authors": "Volodymyr Tkachuk, Csaba Szepesvári, Xiaoqi Tan",
        "summary": "We study finite-horizon offline reinforcement learning (RL) with function approximation for both policy evaluation and policy optimization. Prior work established that statistically efficient learning is impossible for either of these problems when the only assumptions are that the data has good coverage (concentrability) and the state-action value function of every policy is linearly realizable ($q^\\pi$-realizability) (Foster et al., 2021). Recently, Tkachuk et al. (2024) gave a statistically efficient learner for policy optimization, if in addition the data is assumed to be given as trajectories. In this work we present a statistically efficient learner for policy evaluation under the same assumptions. Further, we show that the sample complexity of the learner used by Tkachuk et al. (2024) for policy optimization can be improved by a tighter analysis.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.353675"
    },
    {
        "index": "#206",
        "title": "How to Set $β_1, β_2$ in Adam: An Online Learning Perspective",
        "link": "/arxiv/2510.03478",
        "arxiv_id": "2510.03478",
        "authors": "Quan Nguyen",
        "summary": "While Adam is one of the most effective optimizer for training large-scale machine learning models, a theoretical understanding of how to optimally set its momentum factors, $\\beta_1$ and $\\beta_2$, remains largely incomplete. Prior works have shown that Adam can be seen as an instance of Follow-the-Regularized-Leader (FTRL), one of the most important class of algorithms in online learning. The prior analyses in these works required setting $\\beta_1 = \\sqrt{\\beta_2}$, which does not cover the more practical cases with $\\beta_1 \\neq \\sqrt{\\beta_2}$. We derive novel, more general analyses that hold for both $\\beta_1 \\geq \\sqrt{\\beta_2}$ and $\\beta_1 \\leq \\sqrt{\\beta_2}$. In both cases, our results strictly generalize the existing bounds. Furthermore, we show that our bounds are tight in the worst case. We also prove that setting $\\beta_1 = \\sqrt{\\beta_2}$ is optimal for an oblivious adversary, but sub-optimal for an non-oblivious adversary.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.354982"
    },
    {
        "index": "#207",
        "title": "On residual network depth",
        "link": "/arxiv/2510.03470",
        "arxiv_id": "2510.03470",
        "authors": "Benoit Dherin, Michael Munn",
        "summary": "Deep residual architectures, such as ResNet and the Transformer, have enabled models of unprecedented depth, yet a formal understanding of why depth is so effective remains an open question. A popular intuition, following Veit et al. (2016), is that these residual networks behave like ensembles of many shallower models. Our key finding is an explicit analytical formula that verifies this ensemble perspective, proving that increasing network depth is mathematically equivalent to expanding the size of this implicit ensemble. Furthermore, our expansion reveals a hierarchical ensemble structure in which the combinatorial growth of computation paths leads to an explosion in the output signal, explaining the historical necessity of normalization layers in training deep models. This insight offers a first principles explanation for the historical dependence on normalization layers and sheds new light on a family of successful normalization-free techniques like SkipInit and Fixup. However, while these previous approaches infer scaling factors through optimizer analysis or a heuristic analogy to Batch Normalization, our work offers the first explanation derived directly from the network's inherent functional structure. Specifically, our Residual Expansion Theorem reveals that scaling each residual module provides a principled solution to taming the combinatorial explosion inherent to these architectures. We further show that this scaling acts as a capacity controls that also implicitly regularizes the model's complexity.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.355592"
    },
    {
        "index": "#210",
        "title": "LHGEL: Large Heterogeneous Graph Ensemble Learning using Batch View Aggregation",
        "link": "/arxiv/2510.03432",
        "arxiv_id": "2510.03432",
        "authors": "Jiajun Shen, Yufei Jin, Yi He, Xingquan Zhu",
        "summary": "Learning from large heterogeneous graphs presents significant challenges due to the scale of networks, heterogeneity in node and edge types, variations in nodal features, and complex local neighborhood structures. This paper advocates for ensemble learning as a natural solution to this problem, whereby training multiple graph learners under distinct sampling conditions, the ensemble inherently captures different aspects of graph heterogeneity. Yet, the crux lies in combining these learners to meet global optimization objective while maintaining computational efficiency on large-scale graphs. In response, we propose LHGEL, an ensemble framework that addresses these challenges through batch sampling with three key components, namely batch view aggregation, residual attention, and diversity regularization. Specifically, batch view aggregation samples subgraphs and forms multiple graph views, while residual attention adaptively weights the contributions of these views to guide node embeddings toward informative subgraphs, thereby improving the accuracy of base learners. Diversity regularization encourages representational disparity across embedding matrices derived from different views, promoting model diversity and ensemble robustness. Our theoretical study demonstrates that residual attention mitigates gradient vanishing issues commonly faced in ensemble learning. Empirical results on five real heterogeneous networks validate that our LHGEL approach consistently outperforms its state-of-the-art competitors by substantial margin. Codes and datasets are available at https://github.com/Chrisshen12/LHGEL.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.362692"
    },
    {
        "index": "#212",
        "title": "Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices",
        "link": "/arxiv/2510.03425",
        "arxiv_id": "2510.03425",
        "authors": "Congzheng Song, Xinyu Tang",
        "summary": "Fine-tuning large language models (LLMs) with backpropagation\\textemdash even for a subset of parameters such as LoRA\\textemdash can be much more memory-consuming than inference and is often deemed impractical for resource-constrained mobile devices. Alternative methods, such as zeroth-order optimization (ZO), can greatly reduce the memory footprint but come at the cost of significantly slower model convergence (10$\\times$ to 100$\\times$ more steps than backpropagation). We propose a memory-efficient implementation of backpropagation (MeBP) on mobile devices that provides better trade-off between memory usage and compute time, while converging faster and achieving better performance than the ZO baseline. We verify the effectiveness of MeBP on an iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B parameters, can be fine-tuned using less than 1GB of memory. We release an example of the MeBP implementation at https://github.com/apple/ml-mebp.",
        "subjects": "Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.363963"
    },
    {
        "index": "#214",
        "title": "Training Variation of Physically-Informed Deep Learning Models",
        "link": "/arxiv/2510.03416",
        "arxiv_id": "2510.03416",
        "authors": "Ashley Lenau, Dennis Dimiduk, Stephen R. Niezgoda",
        "summary": "A successful deep learning network is highly dependent not only on the training dataset, but the training algorithm used to condition the network for a given task. The loss function, dataset, and tuning of hyperparameters all play an essential role in training a network, yet there is not much discussion on the reliability or reproducibility of a training algorithm. With the rise in popularity of physics-informed loss functions, this raises the question of how reliable one's loss function is in conditioning a network to enforce a particular boundary condition. Reporting the model variation is needed to assess a loss function's ability to consistently train a network to obey a given boundary condition, and provides a fairer comparison among different methods. In this work, a Pix2Pix network predicting the stress fields of high elastic contrast composites is used as a case study. Several different loss functions enforcing stress equilibrium are implemented, with each displaying different levels of variation in convergence, accuracy, and enforcing stress equilibrium across many training sessions. Suggested practices in reporting model variation are also shared.",
        "subjects": "Machine Learning, Materials Science",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.365277"
    },
    {
        "index": "#218",
        "title": "Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation",
        "link": "/arxiv/2510.03375",
        "arxiv_id": "2510.03375",
        "authors": "Renrong Shao, Wei Zhang, Jun wang",
        "summary": "Data-free knowledge distillation~(DFKD) is an effective manner to solve model compression and transmission restrictions while retaining privacy protection, which has attracted extensive attention in recent years. Currently, the majority of existing methods utilize a generator to synthesize images to support the distillation. Although the current methods have achieved great success, there are still many issues to be explored. Firstly, the outstanding performance of supervised learning in deep learning drives us to explore a pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods cannot distinguish the distributions of different categories of samples, thus producing ambiguous samples that may lead to an incorrect evaluation by the teacher. Besides, current methods cannot optimize the category-wise diversity samples, which will hinder the student model learning from diverse samples and further achieving better performance. In this paper, to address the above limitations, we propose a novel learning paradigm, i.e., conditional pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD). The primary innovations of CPSC-DFKD are: (1) introducing a conditional generative adversarial network to synthesize category-specific diverse images for pseudo-supervised learning, (2) improving the modules of the generator to distinguish the distributions of different categories, and (3) proposing pseudo-supervised contrastive learning based on teacher and student views to enhance diversity. Comprehensive experiments on three commonly-used datasets validate the performance lift of both the student and generator brought by CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.373051"
    },
    {
        "index": "#222",
        "title": "Estimating link level traffic emissions: enhancing MOVES with open-source data",
        "link": "/arxiv/2510.03362",
        "arxiv_id": "2510.03362",
        "authors": "Lijiao Wang, Muhammad Usama, Haris N. Koutsopoulos, Zhengbing He",
        "summary": "Open-source data offers a scalable and transparent foundation for estimating vehicle activity and emissions in urban regions. In this study, we propose a data-driven framework that integrates MOVES and open-source GPS trajectory data, OpenStreetMap (OSM) road networks, regional traffic datasets and satellite imagery-derived feature vectors to estimate the link level operating mode distribution and traffic emissions. A neural network model is trained to predict the distribution of MOVES-defined operating modes using only features derived from readily available data. The proposed methodology was applied using open-source data related to 45 municipalities in the Boston Metropolitan area. The \"ground truth\" operating mode distribution was established using OSM open-source GPS trajectories. Compared to the MOVES baseline, the proposed model reduces RMSE by over 50% for regional scale traffic emissions of key pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the feasibility of low-cost, replicable, and data-driven emissions estimation using fully open data sources.",
        "subjects": "Machine Learning, Applications, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.376055"
    },
    {
        "index": "#225",
        "title": "High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)",
        "link": "/arxiv/2510.03355",
        "arxiv_id": "2510.03355",
        "authors": "Aryan Patel",
        "summary": "Aluminum is a widely used alloy, which is susceptible to fatigue failure. Characterizing fatigue performance for materials is extremely time and cost demanding, especially for high cycle data. To help mitigate this, a transfer learning based framework has been developed using Long short-term memory networks (LSTMs) in which a source LSTM model is trained based on pure axial fatigue data for Aluminum 7075-T6 alloy which is then transferred to predict high cycle torsional S-N curves. The framework was able to accurately predict Al torsional S-N curves for a much higher cycle range. It is the belief that this framework will help to drastically mitigate the cost of gathering fatigue characteristics for different materials and help prioritize tests with better cost and time constraints.",
        "subjects": "Machine Learning, Materials Science, Applied Physics",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.383633"
    },
    {
        "index": "#232",
        "title": "Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment",
        "link": "/arxiv/2510.03335",
        "arxiv_id": "2510.03335",
        "authors": "Ameya Daigavane, YuQing Xie, Bodhi P. Vani, Saeed Saremi, Joseph Kleinhenz, Tess Smidt",
        "summary": "Diffusion models are a popular class of generative models trained to reverse a noising process starting from a target data distribution. Training a diffusion model consists of learning how to denoise noisy samples at different noise levels. When training diffusion models for point clouds such as molecules and proteins, there is often no canonical orientation that can be assigned. To capture this symmetry, the true data samples are often augmented by transforming them with random rotations sampled uniformly over $SO(3)$. Then, the denoised predictions are often rotationally aligned via the Kabsch-Umeyama algorithm to the ground truth samples before computing the loss. However, the effect of this alignment step has not been well studied. Here, we show that the optimal denoiser can be expressed in terms of a matrix Fisher distribution over $SO(3)$. Alignment corresponds to sampling the mode of this distribution, and turns out to be the zeroth order approximation for small noise levels, explaining its effectiveness. We build on this perspective to derive better approximators to the optimal denoiser in the limit of small noise. Our experiments highlight that alignment is often a `good enough' approximation for the noise levels that matter most for training diffusion models.",
        "subjects": "Machine Learning, Image and Video Processing",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.393899"
    },
    {
        "index": "#233",
        "title": "Semantic-Aware Scheduling for GPU Clusters with Large Language Models",
        "link": "/arxiv/2510.03334",
        "arxiv_id": "2510.03334",
        "authors": "Zerui Wang, Qinghao Hu, Ana Klimovic, Tianwei Zhang, Yonggang Wen, Peng Sun, Dahua Lin",
        "summary": "Deep learning (DL) schedulers are pivotal in optimizing resource allocation in GPU clusters, but operate with a critical limitation: they are largely blind to the semantic context of the jobs they manage. This forces them to rely on limited metadata, leading to high profiling overhead, unreliable duration estimation, inadequate failure handling, and poor observability. To this end, we propose SchedMate, a framework that bridges this semantic gap by systematically extracting deep insights from overlooked, unstructured data sources: source code, runtime logs, and historical jobs. SchedMate enhances existing schedulers non-intrusively through three LLM-based components. Our implementation integrates seamlessly with existing deep learning schedulers. Evaluations on a 128-GPU physical cluster and extensive simulations on production traces show SchedMate reduces average job completion times by up to 1.91x, substantially enhancing the scheduling performance, demonstrating the critical role of semantic-awareness in modern DL scheduling.",
        "subjects": "Machine Learning, Distributed, Parallel, and Cluster Computing",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.394628"
    },
    {
        "index": "#234",
        "title": "Constant in an Ever-Changing World",
        "link": "/arxiv/2510.03330",
        "arxiv_id": "2510.03330",
        "authors": "Andy Wu, Chun-Cheng Lin, Yuehua Huang, Rung-Tzuo Liaw",
        "summary": "The training process of reinforcement learning often suffers from severe oscillations, leading to instability and degraded performance. In this paper, we propose a Constant in an Ever-Changing World (CIC) framework that enhances algorithmic stability to improve performance. CIC maintains both a representative policy and a current policy. Instead of updating the representative policy blindly, CIC selectively updates it only when the current policy demonstrates superiority. Furthermore, CIC employs an adaptive adjustment mechanism, enabling the representative and current policies to jointly facilitate critic training. We evaluate CIC on five MuJoCo environments, and the results show that CIC improves the performance of conventional algorithms without incurring additional computational cost.",
        "subjects": "Machine Learning",
        "date": "2025-10-01",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.395259"
    },
    {
        "index": "#235",
        "title": "Fast frequency reconstruction using Deep Learning for event recognition in ring laser data",
        "link": "/arxiv/2510.03325",
        "arxiv_id": "2510.03325",
        "authors": "Giuseppe Di Somma, Giorgio Carelli, Angela D. V. Di Virgilio, Francesco Fuso, Enrico Maccioni, Paolo Marsili",
        "summary": "The reconstruction of a frequency with minimal delay from a sinusoidal signal is a common task in several fields; for example Ring Laser Gyroscopes, since their output signal is a beat frequency. While conventional methods require several seconds of data, we present a neural network approach capable of reconstructing frequencies of several hundred Hertz within approximately 10 milliseconds. This enables rapid trigger generation. The method outperforms standard Fourier-based techniques, improving frequency estimation precision by a factor of 2 in the operational range of GINGERINO, our Ring Laser Gyroscope.\\\\ In addition to fast frequency estimation, we introduce an automated classification framework to identify physical disturbances in the signal, such as laser instabilities and seismic events, achieving accuracy rates between 99\\% and 100\\% on independent test datasets for the seismic class. These results mark a step forward in integrating artificial intelligence into signal analysis for geophysical applications.",
        "subjects": "Machine Learning, Computational Physics, Data Analysis, Statistics and Probability, Geophysics",
        "date": "2025-10-01",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.396059"
    },
    {
        "index": "#236",
        "title": "Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining",
        "link": "/arxiv/2510.03313",
        "arxiv_id": "2510.03313",
        "authors": "Anirudh Subramanyam, Yuxin Chen, Robert L. Grossman",
        "summary": "Scaling laws for language model training traditionally characterize how performance scales with model size and dataset volume. Prior work has explored architecture variants and data treatments such as dataset filtering and noise injection in language model pretraining; however, these studies have not formalized data quality within a principled scaling law. We introduce a dimensionless data-quality parameter Q, and propose a quality-aware scaling law extending the Chinchilla framework to predict loss as a joint function of model size, data volume, and data quality. The law is motivated by an effective-sample-size and information-theoretic view of noisy or redundant corpora, and it admits two practical estimators for Q: (i) a corruption rate proxy and (ii) a deficiency measure. Through synthetic experiments in neural machine translation and autoregressive modeling -- where we systematically control data quality via multiple levels of noise injection and coverage variation -- we show that loss scales predictably with data quality and that higher-quality data can substantially reduce model size and hence compute requirements. Our results demonstrate a sublinear decay of effective data with quality and robustness to moderate data corruption; out-of-sample evaluations further validate the predictive form of the law. Unlike prior empirical analyses, our work establishes an explicit, generalizable law for data quality, offering concrete guidance for balancing data curation effort and model scale in large-scale pretraining.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.396686"
    },
    {
        "index": "#238",
        "title": "Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval",
        "link": "/arxiv/2510.03309",
        "arxiv_id": "2510.03309",
        "authors": "Mallikarjuna Tupakula",
        "summary": "Multimodal foundation models hold promise for drug discovery and biomedical applications, but most existing approaches rely on heavy pretraining or large scale multimodal corpora. We investigate whether thin contrastive bridges, lightweight projection heads over frozen unimodal encoders can align chemical and textual representations without training a full multimodal model. Using paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with biomedical sentence embeddings through dual linear projections trained with a contrastive objective. To better handle drugs sharing the same therapeutic target, we incorporate hard negative weighting and a margin loss. Evaluation under scaffold based splits, which require generalization across disjoint chemical cores, demonstrates that our approach achieves non-trivial cross modal alignment and substantially improves within target discrimination compared to frozen baselines. These results suggest that thin bridges offer a compute efficient alternative to large scale multimodal pretraining, enabling scaffold aware drug text alignment and target specific retrieval in precision medicine.",
        "subjects": "Machine Learning, Biomolecules",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.403111"
    },
    {
        "index": "#239",
        "title": "Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies",
        "link": "/arxiv/2510.03305",
        "arxiv_id": "2510.03305",
        "authors": "Tian Zheng, Subashree Venkatasubramanian, Shuolin Li, Amy Braverman, Xinyi Ke, Zhewen Hou, Peter Jin, Samarth Sanjay Agrawal",
        "summary": "Machine learning has been increasingly applied in climate modeling on system emulation acceleration, data-driven parameter inference, forecasting, and knowledge discovery, addressing challenges such as physical consistency, multi-scale coupling, data sparsity, robust generalization, and integration with scientific workflows. This paper analyzes a series of case studies from applied machine learning research in climate modeling, with a focus on design choices and workflow structure. Rather than reviewing technical details, we aim to synthesize workflow design patterns across diverse projects in ML-enabled climate modeling: from surrogate modeling, ML parameterization, probabilistic programming, to simulation-based inference, and physics-informed transfer learning. We unpack how these workflows are grounded in physical knowledge, informed by simulation data, and designed to integrate observations. We aim to offer a framework for ensuring rigor in scientific machine learning through more transparent model development, critical evaluation, informed adaptation, and reproducibility, and to contribute to lowering the barrier for interdisciplinary collaboration at the interface of data science and climate modeling.",
        "subjects": "Machine Learning, Atmospheric and Oceanic Physics, Applications, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.403950"
    },
    {
        "index": "#240",
        "title": "Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models",
        "link": "/arxiv/2510.03302",
        "arxiv_id": "2510.03302",
        "authors": "Daiheng Gao, Nanxiang Jiang, Andi Zhang, Shilin Lu, Yufei Tang, Wenbo Zhou, Weiming Zhang, Zhaoxin Fan",
        "summary": "Concept erasure techniques have been widely deployed in T2I diffusion models to prevent inappropriate content generation for safety and copyright considerations. However, as models evolve to next-generation architectures like Flux, established erasure methods (\\textit{e.g.}, ESD, UCE, AC) exhibit degraded effectiveness, raising questions about their true mechanisms. Through systematic analysis, we reveal that concept erasure creates only an illusion of ``amnesia\": rather than genuine forgetting, these methods bias sampling trajectories away from target concepts, making the erasure fundamentally reversible. This insight motivates the need to distinguish superficial safety from genuine concept removal. In this work, we propose \\textbf{RevAm} (\\underline{Rev}oking \\underline{Am}nesia), an RL-based trajectory optimization framework that resurrects erased concepts by dynamically steering the denoising process without modifying model weights. By adapting Group Relative Policy Optimization (GRPO) to diffusion models, RevAm explores diverse recovery trajectories through trajectory-level rewards, overcoming local optima that limit existing methods. Extensive experiments demonstrate that RevAm achieves superior concept resurrection fidelity while reducing computational time by 10$\\times$, exposing critical vulnerabilities in current safety mechanisms and underscoring the need for more robust erasure techniques beyond trajectory manipulation.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.404688"
    },
    {
        "index": "#245",
        "title": "Single-Core Superscalar Optimization of Clifford Neural Layers",
        "link": "/arxiv/2510.03290",
        "arxiv_id": "2510.03290",
        "authors": "X. Angelo Huang, Ruben Ciranni, Giovanni Spadaccini, Carla J. López Zurita",
        "summary": "Within the growing interest in the physical sciences in developing networks with equivariance properties, Clifford neural layers shine as one approach that delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this paper, we analyze the inner structure of the computation within Clifford convolutional layers and propose and implement several optimizations to speed up the inference process while maintaining correctness. In particular, we begin by analyzing the theoretical foundations of Clifford algebras to eliminate redundant matrix allocations and computations, then systematically apply established optimization techniques to enhance performance further. We report a final average speedup of 21.35x over the baseline implementation of eleven functions and runtimes comparable to and faster than the original PyTorch implementation in six cases. In the remaining cases, we achieve performance in the same order of magnitude as the original library.",
        "subjects": "Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.413089"
    },
    {
        "index": "#264",
        "title": "Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model",
        "link": "/arxiv/2510.03266",
        "arxiv_id": "2510.03266",
        "authors": "Bharat Sharma, Jitendra Kumar",
        "summary": "Climate anomalies significantly impact terrestrial carbon cycle dynamics, necessitating robust methods for detecting and analyzing anomalous behavior in plant productivity. This study presents a novel application of variational autoencoders (VAE) for identifying extreme events in gross primary productivity (GPP) from Community Earth System Model version 2 simulations across four AR6 regions in the Continental United States. We compare VAE-based anomaly detection with traditional singular spectral analysis (SSA) methods across three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario. The VAE architecture employs three dense layers and a latent space with an input sequence length of 12 months, trained on a normalized GPP time series to reconstruct the GPP and identifying anomalies based on reconstruction errors. Extreme events are defined using 5th percentile thresholds applied to both VAE and SSA anomalies. Results demonstrate strong regional agreement between VAE and SSA methods in spatial patterns of extreme event frequencies, despite VAE producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA across regions and periods). Both methods reveal increasing magnitudes and frequencies of negative carbon cycle extremes toward 2050-80, particularly in Western and Central North America. The VAE approach shows comparable performance to established SSA techniques, while offering computational advantages and enhanced capability for capturing non-linear temporal dependencies in carbon cycle variability. Unlike SSA, the VAE method does not require one to define the periodicity of the signals in the data; it discovers them from the data.",
        "subjects": "Machine Learning, Methodology, Other Statistics",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.440232"
    },
    {
        "index": "#269",
        "title": "Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark",
        "link": "/arxiv/2510.03261",
        "arxiv_id": "2510.03261",
        "authors": "C. Coelho, M. Hohmann, D. Fernández, L. Penter, S. Ihlenfeldt, O. Niggemann",
        "summary": "Thermal errors in machine tools significantly impact machining precision and productivity. Traditional thermal error correction/compensation methods rely on measured temperature-deformation fields or on transfer functions. Most existing data-driven compensation strategies employ neural networks (NNs) to directly predict thermal errors or specific compensation values. While effective, these approaches are tightly bound to particular error types, spatial locations, or machine configurations, limiting their generality and adaptability. In this work, we introduce a novel paradigm in which NNs are trained to predict high-fidelity temperature and heat flux fields within the machine tool. The proposed framework enables subsequent computation and correction of a wide range of error types using modular, swappable downstream components. The NN is trained using data obtained with the finite element method under varying initial conditions and incorporates a correlation-based selection strategy that identifies the most informative measurement points, minimising hardware requirements during inference. We further benchmark state-of-the-art time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit, Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal Convolutional Network, by training both specialised models, tailored for specific initial conditions, and general models, capable of extrapolating to unseen scenarios. The results show accurate and low-cost prediction of temperature and heat flux fields, laying the basis for enabling flexible and generalisable thermal error correction in machine tool environments.",
        "subjects": "Machine Learning, Computational Engineering, Finance, and Science",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.449042"
    },
    {
        "index": "#275",
        "title": "Adversarial training with restricted data manipulation",
        "link": "/arxiv/2510.03254",
        "arxiv_id": "2510.03254",
        "authors": "David Benfield, Stefano Coniglio, Phan Tu Vuong, Alain Zemkoho",
        "summary": "Adversarial machine learning concerns situations in which learners face attacks from active adversaries. Such scenarios arise in applications such as spam email filtering, malware detection and fake image generation, where security methods must be actively updated to keep up with the everimproving generation of malicious data. Pessimistic Bilevel optimisation has been shown to be an effective method of training resilient classifiers against such adversaries. By modelling these scenarios as a game between the learner and the adversary, we anticipate how the adversary will modify their data and then train a resilient classifier accordingly. However, since existing pessimistic bilevel approaches feature an unrestricted adversary, the model is vulnerable to becoming overly pessimistic and unrealistic. When finding the optimal solution that defeats the classifier, it is possible that the adversary's data becomes nonsensical and loses its intended nature. Such an adversary will not properly reflect reality, and consequently, will lead to poor classifier performance when implemented on real-world data. By constructing a constrained pessimistic bilevel optimisation model, we restrict the adversary's movements and identify a solution that better reflects reality. We demonstrate through experiments that this model performs, on average, better than the existing approach.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.458477"
    },
    {
        "index": "#279",
        "title": "Light Differentiable Logic Gate Networks",
        "link": "/arxiv/2510.03250",
        "arxiv_id": "2510.03250",
        "authors": "Lukas Rüttgers, Till Aczel, Andreas Plesner, Roger Wattenhofer",
        "summary": "Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency at inference while sustaining competitive accuracy. But vanishing gradients, discretization errors, and high training cost impede scaling these networks. Even with dedicated parameter initialization schemes from subsequent works, increasing depth still harms accuracy. We show that the root cause of these issues lies in the underlying parametrization of logic gate neurons themselves. To overcome this issue, we propose a reparametrization that also shrinks the parameter size logarithmically in the number of inputs per gate. For binary inputs, this already reduces the model size by 4x, speeds up the backward pass by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we show that the accuracy on CIFAR-100 remains stable and sometimes superior to the original parametrization.",
        "subjects": "Machine Learning, Performance",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.461381"
    },
    {
        "index": "#286",
        "title": "ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning",
        "link": "/arxiv/2510.05070",
        "arxiv_id": "2510.05070",
        "authors": "Siheng Zhao, Yanjie Ze, Yue Wang, C. Karen Liu, Pieter Abbeel, Guanya Shi, Rocky Duan",
        "summary": "Humanoid whole-body loco-manipulation promises transformative capabilities for daily service and warehouse tasks. While recent advances in general motion tracking (GMT) have enabled humanoids to reproduce diverse human motions, these policies lack the precision and object awareness required for loco-manipulation. To this end, we introduce ResMimic, a two-stage residual learning framework for precise and expressive humanoid control from human motion data. First, a GMT policy, trained on large-scale human-only motion, serves as a task-agnostic base for generating human-like whole-body movements. An efficient but precise residual policy is then learned to refine the GMT outputs to improve locomotion and incorporate object interaction. To further facilitate efficient training, we design (i) a point-cloud-based object tracking reward for smoother optimization, (ii) a contact reward that encourages accurate humanoid body-object interactions, and (iii) a curriculum-based virtual object controller to stabilize early training. We evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results show substantial gains in task success, training efficiency, and robustness over strong baselines. Videos are available at https://resmimic.github.io/ .",
        "subjects": "Robotics, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.471609"
    },
    {
        "index": "#287",
        "title": "A Unified Optimization Framework for Multiclass Classification with Structured Hyperplane Arrangements",
        "link": "/arxiv/2510.05047",
        "arxiv_id": "2510.05047",
        "authors": "Víctor Blanco, Harshit Kothari, James Luedtke",
        "summary": "In this paper, we propose a new mathematical optimization model for multiclass classification based on arrangements of hyperplanes. Our approach preserves the core support vector machine (SVM) paradigm of maximizing class separation while minimizing misclassification errors, and it is computationally more efficient than a previous formulation. We present a kernel-based extension that allows it to construct nonlinear decision boundaries. Furthermore, we show how the framework can naturally incorporate alternative geometric structures, including classification trees, $\\ell_p$-SVMs, and models with discrete feature selection. To address large-scale instances, we develop a dynamic clustering matheuristic that leverages the proposed MIP formulation. Extensive computational experiments demonstrate the efficiency of the proposed model and dynamic clustering heuristic, and we report competitive classification performance on both synthetic datasets and real-world benchmarks from the UCI Machine Learning Repository, comparing our method with state-of-the-art implementations available in scikit-learn.",
        "subjects": "Optimization and Control, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.477413"
    },
    {
        "index": "#288",
        "title": "Causal Abstractions, Categorically Unified",
        "link": "/arxiv/2510.05033",
        "arxiv_id": "2510.05033",
        "authors": "Markus Englberger, Devendra Singh Dhami",
        "summary": "We present a categorical framework for relating causal models that represent the same system at different levels of abstraction. We define a causal abstraction as natural transformations between appropriate Markov functors, which concisely consolidate desirable properties a causal abstraction should exhibit. Our approach unifies and generalizes previously considered causal abstractions, and we obtain categorical proofs and generalizations of existing results on causal abstractions. Using string diagrammatical tools, we can explicitly describe the graphs that serve as consistent abstractions of a low-level graph under interventions. We discuss how methods from mechanistic interpretability, such as circuit analysis and sparse autoencoders, fit within our categorical framework. We also show how applying do-calculus on a high-level graphical abstraction of an acyclic-directed mixed graph (ADMG), when unobserved confounders are present, gives valid results on the low-level graph, thus generalizing an earlier statement by Anand et al. (2023). We argue that our framework is more suitable for modeling causal abstractions compared to existing categorical frameworks. Finally, we discuss how notions such as $\\tau$-consistency and constructive $\\tau$-abstractions can be recovered with our framework.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.478039"
    },
    {
        "index": "#290",
        "title": "Curiosity-Driven Co-Development of Action and Language in Robots Through Self-Exploration",
        "link": "/arxiv/2510.05013",
        "arxiv_id": "2510.05013",
        "authors": "Theodore Jerome Tinker, Kenji Doya, Jun Tani",
        "summary": "Human infants acquire language and action co-developmentally, achieving remarkable generalization capabilities from only a minimal number of learning examples. In contrast, recent large language models require exposure to billions of training tokens to achieve such generalization. What mechanisms underlie such efficient developmental learning in humans? This study addresses this question through simulation experiments in which robots learn to perform various actions corresponding to imperative sentences (e.g., \\textit{push red cube}) via trials of self-guided exploration. Our approach integrates the active inference framework with reinforcement learning, enabling curiosity-driven developmental learning. The simulations yielded several nontrivial findings: i) Curiosity-driven exploration combined with motor noise substantially outperforms learning without curiosity. ii) Simpler, prerequisite-like actions emerge earlier in development, while more complex actions involving these prerequisites develop later. iii) Rote pairing of sentences and actions occurs before the emergence of compositional generalization. iv) Generalization is drastically improved as the number of compositional elements increases. These results shed light into possible mechanisms underlying efficient co-developmental learning in infants and provide computational parallels to findings in developmental psychology.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.479587"
    },
    {
        "index": "#291",
        "title": "Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition",
        "link": "/arxiv/2510.05006",
        "arxiv_id": "2510.05006",
        "authors": "Koen Vellenga, H. Joe Steinhauer, Jonas Andersson, Anders Sjögren",
        "summary": "Deep neural networks (DNNs) are increasingly applied to safety-critical tasks in resource-constrained environments, such as video-based driver action and intention recognition. While last layer probabilistic deep learning (LL-PDL) methods can detect out-of-distribution (OOD) instances, their performance varies. As an alternative to last layer approaches, we propose extending pre-trained DNNs with transformation layers to produce multiple latent representations to estimate the uncertainty. We evaluate our latent uncertainty representation (LUR) and repulsively trained LUR (RLUR) approaches against eight PDL methods across four video-based driver action and intention recognition datasets, comparing classification performance, calibration, and uncertainty-based OOD detection. We also contribute 28,000 frame-level action labels and 1,194 video-level intention labels for the NuScenes dataset. Our results show that LUR and RLUR achieve comparable in-distribution classification performance to other LL-PDL approaches. For uncertainty-based OOD detection, LUR matches top-performing PDL methods while being more efficient to train and easier to tune than approaches that require Markov-Chain Monte Carlo sampling or repulsive training procedures.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.480255"
    },
    {
        "index": "#293",
        "title": "Pivotal CLTs for Pseudolikelihood via Conditional Centering in Dependent Random Fields",
        "link": "/arxiv/2510.04972",
        "arxiv_id": "2510.04972",
        "authors": "Nabarun Deb",
        "summary": "In this paper, we study fluctuations of conditionally centered statistics of the form $$N^{-1/2}\\sum_{i=1}^N c_i(g(\\sigma_i)-\\mathbb{E}_N[g(\\sigma_i)|\\sigma_j,j\\neq i])$$ where $(\\sigma_1,\\ldots ,\\sigma_N)$ are sampled from a dependent random field, and $g$ is some bounded function. Our first main result shows that under weak smoothness assumptions on the conditional means (which cover both sparse and dense interactions), the above statistic converges to a Gaussian \\emph{scale mixture} with a random scale determined by a \\emph{quadratic variance} and an \\emph{interaction component}. We also show that under appropriate studentization, the limit becomes a pivotal Gaussian. We leverage this theory to develop a general asymptotic framework for maximum pseudolikelihood (MPLE) inference in dependent random fields. We apply our results to Ising models with pairwise as well as higher-order interactions and exponential random graph models (ERGMs). In particular, we obtain a joint central limit theorem for the inverse temperature and magnetization parameters via the joint MPLE (to our knowledge, the first such result in dense, irregular regimes), and we derive conditionally centered edge CLTs and marginal MPLE CLTs for ERGMs without restricting to the ``sub-critical\" region. Our proof is based on a method of moments approach via combinatorial decision-tree pruning, which may be of independent interest.",
        "subjects": "Statistics Theory, Machine Learning, Probability",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.481662"
    },
    {
        "index": "#299",
        "title": "Set to Be Fair: Demographic Parity Constraints for Set-Valued Classification",
        "link": "/arxiv/2510.04926",
        "arxiv_id": "2510.04926",
        "authors": "Eyal Cohen, Christophe Denis, Mohamed Hebiri",
        "summary": "Set-valued classification is used in multiclass settings where confusion between classes can occur and lead to misleading predictions. However, its application may amplify discriminatory bias motivating the development of set-valued approaches under fairness constraints. In this paper, we address the problem of set-valued classification under demographic parity and expected size constraints. We propose two complementary strategies: an oracle-based method that minimizes classification risk while satisfying both constraints, and a computationally efficient proxy that prioritizes constraint satisfaction. For both strategies, we derive closed-form expressions for the (optimal) fair set-valued classifiers and use these to build plug-in, data-driven procedures for empirical predictions. We establish distribution-free convergence rates for violations of the size and fairness constraints for both methods, and under mild assumptions we also provide excess-risk bounds for the oracle-based approach. Empirical results demonstrate the effectiveness of both strategies and highlight the efficiency of our proxy method.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.491390"
    },
    {
        "index": "#300",
        "title": "Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context",
        "link": "/arxiv/2510.04912",
        "arxiv_id": "2510.04912",
        "authors": "Ngeyen Yinkfu, Sunday Nwovu, Jonathan Kayizzi, Angelique Uwamahoro",
        "summary": "In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation, often navigating unpredictably and disregarding traffic rules, posing significant challenges for autonomous driving systems. This study compares four object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for motorbike detection using a custom dataset of 198 images collected in Kigali. Implemented in PyTorch with transfer learning, the models were evaluated for accuracy, localization, and inference speed to assess their suitability for real-time navigation in resource-constrained settings. We identify implementation challenges, including dataset limitations and model complexities, and recommend simplified architectures for future work to enhance accessibility for autonomous systems in developing countries like Rwanda.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.492075"
    },
    {
        "index": "#303",
        "title": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection",
        "link": "/arxiv/2510.04885",
        "arxiv_id": "2510.04885",
        "authors": "Yuxin Wen, Arman Zharmagambetov, Ivan Evtimov, Narine Kokhlikyan, Tom Goldstein, Kamalika Chaudhuri, Chuan Guo",
        "summary": "Prompt injection poses a serious threat to the reliability and safety of LLM agents. Recent defenses against prompt injection, such as Instruction Hierarchy and SecAlign, have shown notable robustness against static attacks. However, to more thoroughly evaluate the robustness of these defenses, it is arguably necessary to employ strong attacks such as automated red-teaming. To this end, we introduce RL-Hammer, a simple recipe for training attacker models that automatically learn to perform strong prompt injections and jailbreaks via reinforcement learning. RL-Hammer requires no warm-up data and can be trained entirely from scratch. To achieve high ASRs against industrial-level models with defenses, we propose a set of practical techniques that enable highly effective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR against GPT-4o and a $72\\%$ ASR against GPT-5 with the Instruction Hierarchy defense. We further discuss the challenge of achieving high diversity in attacks, highlighting how attacker models tend to reward-hack diversity objectives. Finally, we show that RL-Hammer can evade multiple prompt injection detectors. We hope our work advances automatic red-teaming and motivates the development of stronger, more principled defenses. Code is available at https://github.com/facebookresearch/rl-injector.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.499432"
    },
    {
        "index": "#304",
        "title": "CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery",
        "link": "/arxiv/2510.04883",
        "arxiv_id": "2510.04883",
        "authors": "Nathan Shankar, Pawel Ladosz, Hujun Yin",
        "summary": "This paper presents a novel approach for enabling robust robotic perception in dark environments using infrared (IR) stream. IR stream is less susceptible to noise than RGB in low-light conditions. However, it is dominated by active emitter patterns that hinder high-level tasks such as object detection, tracking and localisation. To address this, a U-Net-based architecture is proposed that reconstructs clean IR images from emitter-populated input, improving both image quality and downstream robotic performance. This approach outperforms existing enhancement techniques and enables reliable operation of vision-driven robotic systems across illumination conditions from well-lit to extreme low-light scenes.",
        "subjects": "Robotics, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.500094"
    },
    {
        "index": "#305",
        "title": "BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping",
        "link": "/arxiv/2510.04876",
        "arxiv_id": "2510.04876",
        "authors": "Hayat Rajani, Valerio Franchi, Borja Martinez-Clavel Valles, Raimon Ramos, Rafael Garcia, Nuno Gracias",
        "summary": "Benthic habitat mapping is fundamental for understanding marine ecosystems, guiding conservation efforts, and supporting sustainable resource management. Yet, the scarcity of large, annotated datasets limits the development and benchmarking of machine learning models in this domain. This paper introduces a thorough multi-modal dataset, comprising about a million side-scan sonar (SSS) tiles collected along the coast of Catalonia (Spain), complemented by bathymetric maps and a set of co-registered optical images from targeted surveys using an autonomous underwater vehicle (AUV). Approximately \\num{36000} of the SSS tiles have been manually annotated with segmentation masks to enable supervised fine-tuning of classification models. All the raw sensor data, together with mosaics, are also released to support further exploration and algorithm development. To address challenges in multi-sensor data fusion for AUVs, we spatially associate optical images with corresponding SSS tiles, facilitating self-supervised, cross-modal representation learning. Accompanying open-source preprocessing and annotation tools are provided to enhance accessibility and encourage research. This resource aims to establish a standardized benchmark for underwater habitat mapping, promoting advancements in autonomous seafloor classification and multi-sensor integration.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.500780"
    },
    {
        "index": "#307",
        "title": "ERDE: Entropy-Regularized Distillation for Early-exit",
        "link": "/arxiv/2510.04856",
        "arxiv_id": "2510.04856",
        "authors": "Martial Guidez, Stefan Duffner, Yannick Alpou, Oscar Röth, Christophe Garcia",
        "summary": "Although deep neural networks and in particular Convolutional Neural Networks have demonstrated state-of-the-art performance in image classification with relatively high efficiency, they still exhibit high computational costs, often rendering them impractical for real-time and edge applications. Therefore, a multitude of compression techniques have been developed to reduce these costs while maintaining accuracy. In addition, dynamic architectures have been introduced to modulate the level of compression at execution time, which is a desirable property in many resource-limited application scenarios. The proposed method effectively integrates two well-established optimization techniques: early exits and knowledge distillation, where a reduced student early-exit model is trained from a more complex teacher early-exit model. The primary contribution of this research lies in the approach for training the student early-exit model. In comparison to the conventional Knowledge Distillation loss, our approach incorporates a new entropy-based loss for images where the teacher's classification was incorrect. The proposed method optimizes the trade-off between accuracy and efficiency, thereby achieving significant reductions in computational complexity without compromising classification performance. The validity of this approach is substantiated by experimental results on image classification datasets CIFAR10, CIFAR100 and SVHN, which further opens new research perspectives for Knowledge Distillation in other contexts.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.502136"
    },
    {
        "index": "#309",
        "title": "Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation",
        "link": "/arxiv/2510.04838",
        "arxiv_id": "2510.04838",
        "authors": "Muquan Li, Hang Gou, Dongyang Zhang, Shuang Liang, Xiurui Xie, Deqiang Ouyang, Ke Qin",
        "summary": "The growing demand for efficient deep learning has positioned dataset distillation as a pivotal technique for compressing training dataset while preserving model performance. However, existing inner-loop optimization methods for dataset distillation typically rely on random truncation strategies, which lack flexibility and often yield suboptimal results. In this work, we observe that neural networks exhibit distinct learning dynamics across different training stages-early, middle, and late-making random truncation ineffective. To address this limitation, we propose Automatic Truncated Backpropagation Through Time (AT-BPTT), a novel framework that dynamically adapts both truncation positions and window sizes according to intrinsic gradient behavior. AT-BPTT introduces three key components: (1) a probabilistic mechanism for stage-aware timestep selection, (2) an adaptive window sizing strategy based on gradient variation, and (3) a low-rank Hessian approximation to reduce computational overhead. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art performance, improving accuracy by an average of 6.16% over baseline methods. Moreover, our approach accelerates inner-loop optimization by 3.9x while saving 63% memory cost.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.508720"
    },
    {
        "index": "#310",
        "title": "A Noise Resilient Approach for Robust Hurst Exponent Estimation",
        "link": "/arxiv/2510.04811",
        "arxiv_id": "2510.04811",
        "authors": "Malith Premarathna, Fabrizio Ruggeri, Dixon Vimalajeewa",
        "summary": "Understanding signal behavior across scales is vital in areas such as natural phenomena analysis and financial modeling. A key property is self-similarity, quantified by the Hurst exponent (H), which reveals long-term dependencies. Wavelet-based methods are effective for estimating H due to their multi-scale analysis capability, but additive noise in real-world measurements often degrades accuracy. We propose Noise-Controlled ALPHEE (NC-ALPHEE), an enhancement of the Average Level-Pairwise Hurst Exponent Estimator (ALPHEE), incorporating noise mitigation and generating multiple level-pairwise estimates from signal energy pairs. A neural network (NN) combines these estimates, replacing traditional averaging. This adaptive learning maintains ALPHEE's behavior in noise-free cases while improving performance in noisy conditions. Extensive simulations show that in noise-free data, NC-ALPHEE matches ALPHEE's accuracy using both averaging and NN-based methods. Under noise, however, traditional averaging deteriorates and requires impractical level restrictions, while NC-ALPHEE consistently outperforms existing techniques without such constraints. NC-ALPHEE offers a robust, adaptive approach for H estimation, significantly enhancing the reliability of wavelet-based methods in noisy environments.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.509342"
    },
    {
        "index": "#311",
        "title": "Kernel ridge regression under power-law data: spectrum and generalization",
        "link": "/arxiv/2510.04780",
        "arxiv_id": "2510.04780",
        "authors": "Arie Wortsman, Bruno Loureiro",
        "summary": "In this work, we investigate high-dimensional kernel ridge regression (KRR) on i.i.d. Gaussian data with anisotropic power-law covariance. This setting differs fundamentally from the classical source & capacity conditions for KRR, where power-law assumptions are typically imposed on the kernel eigen-spectrum itself. Our contributions are twofold. First, we derive an explicit characterization of the kernel spectrum for polynomial inner-product kernels, giving a precise description of how the kernel eigen-spectrum inherits the data decay. Second, we provide an asymptotic analysis of the excess risk in the high-dimensional regime for a particular kernel with this spectral behavior, showing that the sample complexity is governed by the effective dimension of the data rather than the ambient dimension. These results establish a fundamental advantage of learning with power-law anisotropic data over isotropic data. To our knowledge, this is the first rigorous treatment of non-linear KRR under power-law data.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.509935"
    },
    {
        "index": "#312",
        "title": "Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge",
        "link": "/arxiv/2510.04772",
        "arxiv_id": "2510.04772",
        "authors": "Max Kirchner, Hanna Hoffmann, Alexander C. Jenke, Oliver L. Saldanha, Kevin Pfeiffer, Weam Kanjo, Julia Alekseenko, Claas de Boer, Santhi Raj Kolamuri, Lorenzo Mazza, Nicolas Padoy, Sophia Bano, Annika Reinke, Lena Maier-Hein, Danail Stoyanov, Jakob N. Kather, Fiona R. Kolbinger, Sebastian Bodenstedt, Stefanie Speidel",
        "summary": "Purpose: The FedSurg challenge was designed to benchmark the state of the art in federated learning for surgical video classification. Its goal was to assess how well current methods generalize to unseen clinical centers and adapt through local fine-tuning while enabling collaborative model development without sharing patient data. Methods: Participants developed strategies to classify inflammation stages in appendicitis using a preliminary version of the multi-center Appendix300 video dataset. The challenge evaluated two tasks: generalization to an unseen center and center-specific adaptation after fine-tuning. Submitted approaches included foundation models with linear probing, metric learning with triplet loss, and various FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost, with ranking robustness evaluated via bootstrapping and statistical testing. Results: In the generalization task, performance across centers was limited. In the adaptation task, all teams improved after fine-tuning, though ranking stability was low. The ViViT-based submission achieved the strongest overall performance. The challenge highlighted limitations in generalization, sensitivity to class imbalance, and difficulties in hyperparameter tuning in decentralized training, while spatiotemporal modeling and context-aware preprocessing emerged as promising strategies. Conclusion: The FedSurg Challenge establishes the first benchmark for evaluating FL strategies in surgical video classification. Findings highlight the trade-off between local personalization and global robustness, and underscore the importance of architecture choice, preprocessing, and loss design. This benchmarking offers a reference point for future development of imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.510856"
    },
    {
        "index": "#313",
        "title": "Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning",
        "link": "/arxiv/2510.04770",
        "arxiv_id": "2510.04770",
        "authors": "Xiaomeng Fan, Yuchuan Mao, Zhi Gao, Yuwei Wu, Jin Chen, Yunde Jia",
        "summary": "Open-vocabulary learning requires modeling the data distribution in open environments, which consists of both seen-class and unseen-class data. Existing methods estimate the distribution in open environments using seen-class data, where the absence of unseen classes makes the estimation error inherently unidentifiable. Intuitively, learning beyond the seen classes is crucial for distribution estimation to bound the estimation error. We theoretically demonstrate that the distribution can be effectively estimated by generating unseen-class data, through which the estimation error is upper-bounded. Building on this theoretical insight, we propose a novel open-vocabulary learning method, which generates unseen-class data for estimating the distribution in open environments. The method consists of a class-domain-wise data generation pipeline and a distribution alignment algorithm. The data generation pipeline generates unseen-class data under the guidance of a hierarchical semantic tree and domain information inferred from the seen-class data, facilitating accurate distribution estimation. With the generated data, the distribution alignment algorithm estimates and maximizes the posterior probability to enhance generalization in open-vocabulary learning. Extensive experiments on $11$ datasets demonstrate that our method outperforms baseline approaches by up to $14\\%$, highlighting its effectiveness and superiority.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.511522"
    },
    {
        "index": "#316",
        "title": "Predictive economics: Rethinking economic methodology with machine learning",
        "link": "/arxiv/2510.04726",
        "arxiv_id": "2510.04726",
        "authors": "Miguel Alves Pereira",
        "summary": "This article proposes predictive economics as a distinct analytical perspective within economics, grounded in machine learning and centred on predictive accuracy rather than causal identification. Drawing on the instrumentalist tradition (Friedman), the explanation-prediction divide (Shmueli), and the contrast between modelling cultures (Breiman), we formalise prediction as a valid epistemological and methodological objective. Reviewing recent applications across economic subfields, we show how predictive models contribute to empirical analysis, particularly in complex or data-rich contexts. This perspective complements existing approaches and supports a more pluralistic methodology - one that values out-of-sample performance alongside interpretability and theoretical structure.",
        "subjects": "General Economics, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.518546"
    },
    {
        "index": "#319",
        "title": "A Study on the Data Distribution Gap in Music Emotion Recognition",
        "link": "/arxiv/2510.04688",
        "arxiv_id": "2510.04688",
        "authors": "Joann Ching, Gerhard Widmer",
        "summary": "Music Emotion Recognition (MER) is a task deeply connected to human perception, relying heavily on subjective annotations collected from contributors. Prior studies tend to focus on specific musical styles rather than incorporating a diverse range of genres, such as rock and classical, within a single framework. In this paper, we address the task of recognizing emotion from audio content by investigating five datasets with dimensional emotion annotations -- EmoMusic, DEAM, PMEmo, WTC, and WCMED -- which span various musical styles. We demonstrate the problem of out-of-distribution generalization in a systematic experiment. By closely looking at multiple data and feature sets, we provide insight into genre-emotion relationships in existing data and examine potential genre dominance and dataset biases in certain feature representations. Based on these experiments, we arrive at a simple yet effective framework that combines embeddings extracted from the Jukebox model with chroma features and demonstrate how, alongside a combination of several diverse training sets, this permits us to train models with substantially improved cross-dataset generalization capabilities.",
        "subjects": "Sound, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.520471"
    },
    {
        "index": "#324",
        "title": "Data-Driven Adaptive PID Control Based on Physics-Informed Neural Networks",
        "link": "/arxiv/2510.04591",
        "arxiv_id": "2510.04591",
        "authors": "Junsei Ito, Yasuaki Wasa",
        "summary": "This article proposes a data-driven PID controller design based on the principle of adaptive gain optimization, leveraging Physics-Informed Neural Networks (PINNs) generated for predictive modeling purposes. The proposed control design method utilizes gradients of the PID gain optimization, achieved through the automatic differentiation of PINNs, to apply model predictive control using a cost function based on tracking error and control inputs. By optimizing PINNs-based PID gains, the method achieves adaptive gain tuning that ensures stability while accounting for system nonlinearities. The proposed method features a systematic framework for integrating PINNs-based models of dynamical control systems into closed-loop control systems, enabling direct application to PID control design. A series of numerical experiments is conducted to demonstrate the effectiveness of the proposed method from the control perspectives based on both time and frequency domains.",
        "subjects": "Systems and Control, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.526527"
    },
    {
        "index": "#325",
        "title": "Language Model Based Text-to-Audio Generation: Anti-Causally Aligned Collaborative Residual Transformers",
        "link": "/arxiv/2510.04577",
        "arxiv_id": "2510.04577",
        "authors": "Juncheng Wang, Chao Xu, Cheng Yu, Zhe Hu, Haoyu Xie, Guoqi Yu, Lei Shang, Shujun Wang",
        "summary": "While language models (LMs) paired with residual vector quantization (RVQ) tokenizers have shown promise in text-to-audio (T2A) generation, they still lag behind diffusion-based models by a non-trivial margin. We identify a critical dilemma underpinning this gap: incorporating more RVQ layers improves audio reconstruction fidelity but exceeds the generation capacity of conventional LMs. To address this, we first analyze RVQ dynamics and uncover two key limitations: 1) orthogonality of features across RVQ layers hinders effective LMs training, and 2) descending semantic richness in tokens from deeper RVQ layers exacerbates exposure bias during autoregressive decoding. Based on these insights, we propose Siren, a novel LM-based framework that employs multiple isolated transformers with causal conditioning and anti-causal alignment via reinforcement learning. Extensive experiments demonstrate that Siren outperforms both existing LM-based and diffusion-based T2A systems, achieving state-of-the-art results. By bridging the representational strengths of LMs with the fidelity demands of audio synthesis, our approach repositions LMs as competitive contenders against diffusion models in T2A tasks. Moreover, by aligning audio representations with linguistic structures, Siren facilitates a promising pathway toward unified multi-modal generation frameworks.",
        "subjects": "Sound, Machine Learning, Multimedia, Audio and Speech Processing",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.527317"
    },
    {
        "index": "#327",
        "title": "Gini-based Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing",
        "link": "/arxiv/2510.04556",
        "arxiv_id": "2510.04556",
        "authors": "Alexej Brauer, Paul Menzel",
        "summary": "In a dynamic landscape where portfolios and environments evolve, maintaining the accuracy of pricing models is critical. To the best of our knowledge, this is the first study to systematically examine concept drift in non-life insurance pricing. We (i) provide an overview of the relevant literature and commonly used methodologies, clarify the distinction between virtual drift and concept drift, and explain their implications for long-run model performance; (ii) review and formalize common performance measures, including the Gini index and deviance loss, and articulate their interpretation; (iii) derive the asymptotic distribution of the Gini index, enabling valid inference and hypothesis testing; and (iv) present a standardized monitoring procedure that indicates when refitting is warranted. We illustrate the framework using a modified real-world portfolio with induced concept drift and discuss practical considerations and pitfalls.",
        "subjects": "Machine Learning, Machine Learning, Statistics Theory, Statistical Finance, Applications",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.528777"
    },
    {
        "index": "#328",
        "title": "Fast Witness Persistence for MRI Volumes via Hybrid Landmarking",
        "link": "/arxiv/2510.04553",
        "arxiv_id": "2510.04553",
        "authors": "Jorge Leonardo Ruiz Williams",
        "summary": "We introduce a scalable witness-based persistent homology pipeline for full-brain MRI volumes that couples density-aware landmark selection with a GPU-ready witness filtration. Candidates are scored by a hybrid metric that balances geometric coverage against inverse kernel density, yielding landmark sets that shrink mean pairwise distances by 30-60% over random or density-only baselines while preserving topological features. Benchmarks on BrainWeb, IXI, and synthetic manifolds execute in under ten seconds on a single NVIDIA RTX 4090 GPU, avoiding the combinatorial blow-up of Cech, Vietoris-Rips, and alpha filtrations. The package is distributed on PyPI as whale-tda (installable via pip); source and issues are hosted at https://github.com/jorgeLRW/whale. The release also exposes a fast preset (mri_deep_dive_fast) for exploratory sweeps, and ships with reproducibility-focused scripts and artifacts for drop-in use in medical imaging workflows.",
        "subjects": "Computational Geometry, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.540415"
    },
    {
        "index": "#329",
        "title": "Learning Linear Regression with Low-Rank Tasks in-Context",
        "link": "/arxiv/2510.04548",
        "arxiv_id": "2510.04548",
        "authors": "Kaito Takanami, Takashi Takahashi, Yoshiyuki Kabashima",
        "summary": "In-context learning (ICL) is a key building block of modern large language models, yet its theoretical mechanisms remain poorly understood. It is particularly mysterious how ICL operates in real-world applications where tasks have a common structure. In this work, we address this problem by analyzing a linear attention model trained on low-rank regression tasks. Within this setting, we precisely characterize the distribution of predictions and the generalization error in the high-dimensional limit. Moreover, we find that statistical fluctuations in finite pre-training data induce an implicit regularization. Finally, we identify a sharp phase transition of the generalization error governed by task structure. These results provide a framework for understanding how transformers learn to learn the task structure.",
        "subjects": "Disordered Systems and Neural Networks, Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.541343"
    },
    {
        "index": "#330",
        "title": "Quantum generative model on bicycle-sharing system and an application",
        "link": "/arxiv/2510.04512",
        "arxiv_id": "2510.04512",
        "authors": "Fumio Nemoto, Nobuyuki Koike, Daichi Sato, Yuuta Kawaai, Masayuki Ohzeki",
        "summary": "Recently, bicycle-sharing systems have been implemented in numerous cities, becoming integral to daily life. However, a prevalent issue arises when intensive commuting demand leads to bicycle shortages in specific areas and at particular times. To address this challenge, we employ a novel quantum machine learning model that analyzes time series data by fitting quantum time evolution to observed sequences. This model enables us to capture actual trends in bicycle counts at individual ports and identify correlations between different ports. Utilizing the trained model, we simulate the impact of proactively adding bicycles to high-demand ports on the overall rental number across the system. Given that the core of this method lies in a Monte Carlo simulation, it is anticipated to have a wide range of industrial applications.",
        "subjects": "Quantum Physics, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.542220"
    },
    {
        "index": "#331",
        "title": "Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation",
        "link": "/arxiv/2510.04502",
        "arxiv_id": "2510.04502",
        "authors": "Yue Que, Yingyi Zhang, Xiangyu Zhao, Chen Ma",
        "summary": "Graph-based recommender systems leverage neighborhood aggregation to generate node representations, which is highly sensitive to popularity bias, resulting in an echo effect during information propagation. Existing graph-based debiasing solutions refine the aggregation process with attempts such as edge reconstruction or weight adjustment. However, these methods remain inadequate in fully alleviating popularity bias. Specifically, this is because 1) they provide no insights into graph aggregation rationality, thus lacking an optimality guarantee; 2) they fail to well balance the training and debiasing process, which undermines the effectiveness. In this paper, we propose a novel approach to mitigate popularity bias through rational modeling of the graph aggregation process. We reveal that graph aggregation is a special form of backdoor adjustment in causal inference, where the aggregation weight corresponds to the historical interaction likelihood distribution. Based on this insight, we devise an encoder-decoder architecture, namely Causality-aware Graph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the unbiased aggregation weight by optimizing the evidence lower bound of the interaction likelihood. In order to enhance the debiasing effectiveness during early training stages, we further design a momentum update strategy that incrementally refines the aggregation weight matrix. Extensive experiments on three datasets demonstrate that CAGED outperforms existing graph-based debiasing methods. Our implementation is available at https://github.com/QueYork/CAGED.",
        "subjects": "Information Retrieval, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.543068"
    },
    {
        "index": "#332",
        "title": "Deep vs. Shallow: Benchmarking Physics-Informed Neural Architectures on the Biharmonic Equation",
        "link": "/arxiv/2510.04490",
        "arxiv_id": "2510.04490",
        "authors": "Akshay Govind Srinivasan, Vikas Dwivedi, Balaji Srinivasan",
        "summary": "Partial differential equation (PDE) solvers are fundamental to engineering simulation. Classical mesh-based approaches (finite difference/volume/element) are fast and accurate on high-quality meshes but struggle with higher-order operators and complex, hard-to-mesh geometries. Recently developed physics-informed neural networks (PINNs) and their variants are mesh-free and flexible, yet compute-intensive and often less accurate. This paper systematically benchmarks RBF-PIELM, a rapid PINN variant-an extreme learning machine with radial-basis activations-for higher-order PDEs. RBF-PIELM replaces PINNs' time-consuming gradient descent with a single-shot least-squares solve. We test RBF-PIELM on the fourth-order biharmonic equation using two benchmarks: lid-driven cavity flow (streamfunction formulation) and a manufactured oscillatory solution. Our results show up to $(350\\times)$ faster training than PINNs and over $(10\\times)$ fewer parameters for comparable solution accuracy. Despite surpassing PINNs, RBF-PIELM still lags mature mesh-based solvers and its accuracy degrades on highly oscillatory solutions, highlighting remaining challenges for practical deployment.",
        "subjects": "Computational Engineering, Finance, and Science, Emerging Technologies, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.543948"
    },
    {
        "index": "#336",
        "title": "Benchmarking atmospheric circulation variability in an AI emulator, ACE2, and a hybrid model, NeuralGCM",
        "link": "/arxiv/2510.04466",
        "arxiv_id": "2510.04466",
        "authors": "Ian Baxter, Hamid Pahlavan, Pedram Hassanzadeh, Katharine Rucker, Tiffany Shaw",
        "summary": "Physics-based atmosphere-land models with prescribed sea surface temperature have notable successes but also biases in their ability to represent atmospheric variability compared to observations. Recently, AI emulators and hybrid models have emerged with the potential to overcome these biases, but still require systematic evaluation against metrics grounded in fundamental atmospheric dynamics. Here, we evaluate the representation of four atmospheric variability benchmarking metrics in a fully data-driven AI emulator (ACE2-ERA5) and hybrid model (NeuralGCM). The hybrid model and emulator can capture the spectra of large-scale tropical waves and extratropical eddy-mean flow interactions, including critical levels. However, both struggle to capture the timescales associated with quasi-biennial oscillation (QBO, $\\sim 28$ months) and Southern annular mode propagation ($\\sim 150$ days). These dynamical metrics serve as an initial benchmarking tool to inform AI model development and understand their limitations, which may be essential for out-of-distribution applications (e.g., extrapolating to unseen climates).",
        "subjects": "Atmospheric and Oceanic Physics, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.552019"
    },
    {
        "index": "#337",
        "title": "Perspectives on Stochastic Localization",
        "link": "/arxiv/2510.04460",
        "arxiv_id": "2510.04460",
        "authors": "Bobby Shi, Kevin Tian, Matthew S. Zhang",
        "summary": "We survey different perspectives on the stochastic localization process of [Eld13], a powerful construction that has had many exciting recent applications in high-dimensional probability and algorithm design. Unlike prior surveys on this topic, our focus is on giving a self-contained presentation of all known alternative constructions of Eldan's stochastic localization, with an emphasis on connections between different constructions. Our hope is that by collecting these perspectives, some of which had primarily arisen within a particular community (e.g., probability theory, theoretical computer science, information theory, or machine learning), we can broaden the accessibility of stochastic localization, and ease its future use.",
        "subjects": "Probability, Data Structures and Algorithms, Machine Learning, Statistics Theory",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.552706"
    },
    {
        "index": "#339",
        "title": "Zeroth-Order Methods for Stochastic Nonconvex Nonsmooth Composite Optimization",
        "link": "/arxiv/2510.04446",
        "arxiv_id": "2510.04446",
        "authors": "Ziyi Chen, Peiran Yu, Heng Huang",
        "summary": "This work aims to solve a stochastic nonconvex nonsmooth composite optimization problem. Previous works on composite optimization problem requires the major part to satisfy Lipschitz smoothness or some relaxed smoothness conditions, which excludes some machine learning examples such as regularized ReLU network and sparse support matrix machine. In this work, we focus on stochastic nonconvex composite optimization problem without any smoothness assumptions. In particular, we propose two new notions of approximate stationary points for such optimization problem and obtain finite-time convergence results of two zeroth-order algorithms to these two approximate stationary points respectively. Finally, we demonstrate that these algorithms are effective using numerical experiments.",
        "subjects": "Optimization and Control, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.554021"
    },
    {
        "index": "#340",
        "title": "spd-metrics-id: A Python Package for SPD-Aware Distance Metrics in Connectome Fingerprinting and Beyond",
        "link": "/arxiv/2510.04438",
        "arxiv_id": "2510.04438",
        "authors": "Kaosar Uddin",
        "summary": "We present spd-metrics-id, a Python package for computing distances and divergences between symmetric positive-definite (SPD) matrices. Unlike traditional toolkits that focus on specific applications, spd-metrics-id provides a unified, extensible, and reproducible framework for SPD distance computation. The package supports a wide variety of geometry-aware metrics, including Alpha-z Bures-Wasserstein, Alpha-Procrustes, affine-invariant Riemannian, log-Euclidean, and others, and is accessible both via a command-line interface and a Python API. Reproducibility is ensured through Docker images and Zenodo archiving. We illustrate usage through a connectome fingerprinting example, but the package is broadly applicable to covariance analysis, diffusion tensor imaging, and other domains requiring SPD matrix comparison. The package is openly available at https://pypi.org/project/spd-metrics-id/.",
        "subjects": "Computation, Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.554642"
    },
    {
        "index": "#341",
        "title": "Learning Survival Models with Right-Censored Reporting Delays",
        "link": "/arxiv/2510.04421",
        "arxiv_id": "2510.04421",
        "authors": "Yuta Shikuri, Hironori Fujisawa",
        "summary": "Survival analysis is a statistical technique used to estimate the time until an event occurs. Although it is applied across a wide range of fields, adjusting for reporting delays under practical constraints remains a significant challenge in the insurance industry. Such delays render event occurrences unobservable when their reports are subject to right censoring. This issue becomes particularly critical when estimating hazard rates for newly enrolled cohorts with limited follow-up due to administrative censoring. Our study addresses this challenge by jointly modeling the parametric hazard functions of event occurrences and report timings. The joint probability distribution is marginalized over the latent event occurrence status. We construct an estimator for the proposed survival model and establish its asymptotic consistency. Furthermore, we develop an expectation-maximization algorithm to compute its estimates. Using these findings, we propose a two-stage estimation procedure based on a parametric proportional hazards model to evaluate observations subject to administrative censoring. Experimental results demonstrate that our method effectively improves the timeliness of risk evaluation for newly enrolled cohorts.",
        "subjects": "Machine Learning, Machine Learning, Statistics Theory",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.560408"
    },
    {
        "index": "#342",
        "title": "Scale-Invariant Regret Matching and Online Learning with Optimal Convergence: Bridging Theory and Practice in Zero-Sum Games",
        "link": "/arxiv/2510.04407",
        "arxiv_id": "2510.04407",
        "authors": "Brian Hu Zhang, Ioannis Anagnostides, Tuomas Sandholm",
        "summary": "A considerable chasm has been looming for decades between theory and practice in zero-sum game solving through first-order methods. Although a convergence rate of $T^{-1}$ has long been established since Nemirovski's mirror-prox algorithm and Nesterov's excessive gap technique in the early 2000s, the most effective paradigm in practice is *counterfactual regret minimization*, which is based on *regret matching* and its modern variants. In particular, the state of the art across most benchmarks is *predictive* regret matching$^+$ (PRM$^+$), in conjunction with non-uniform averaging. Yet, such algorithms can exhibit slower $\\Omega(T^{-1/2})$ convergence even in self-play. In this paper, we close the gap between theory and practice. We propose a new scale-invariant and parameter-free variant of PRM$^+$, which we call IREG-PRM$^+$. We show that it achieves $T^{-1/2}$ best-iterate and $T^{-1}$ (i.e., optimal) average-iterate convergence guarantees, while also being on par with PRM$^+$ on benchmark games. From a technical standpoint, we draw an analogy between IREG-PRM$^+$ and optimistic gradient descent with *adaptive* learning rate. The basic flaw of PRM$^+$ is that the ($\\ell_2$-)norm of the regret vector -- which can be thought of as the inverse of the learning rate -- can decrease. By contrast, we design IREG-PRM$^+$ so as to maintain the invariance that the norm of the regret vector is nondecreasing. This enables us to derive an RVU-type bound for IREG-PRM$^+$, the first such property that does not rely on introducing additional hyperparameters to enforce smoothness. Furthermore, we find that IREG-PRM$^+$ performs on par with an adaptive version of optimistic gradient descent that we introduce whose learning rate depends on the misprediction error, demystifying the effectiveness of the regret matching family *vis-a-vis* more standard optimization techniques.",
        "subjects": "Computer Science and Game Theory, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.561044"
    },
    {
        "index": "#343",
        "title": "Modular and Adaptive Conformal Prediction for Sequential Models via Residual Decomposition",
        "link": "/arxiv/2510.04406",
        "arxiv_id": "2510.04406",
        "authors": "William Zhang, Saurabh Amin, Georgia Perakis",
        "summary": "Conformal prediction offers finite-sample coverage guarantees under minimal assumptions. However, existing methods treat the entire modeling process as a black box, overlooking opportunities to exploit modular structure. We introduce a conformal prediction framework for two-stage sequential models, where an upstream predictor generates intermediate representations for a downstream model. By decomposing the overall prediction residual into stage-specific components, our method enables practitioners to attribute uncertainty to specific pipeline stages. We develop a risk-controlled parameter selection procedure using family-wise error rate (FWER) control to calibrate stage-wise scaling parameters, and propose an adaptive extension for non-stationary settings that preserves long-run coverage guarantees. Experiments on synthetic distribution shifts, as well as real-world supply chain and stock market data, demonstrate that our approach maintains coverage under conditions that degrade standard conformal methods, while providing interpretable stage-wise uncertainty attribution. This framework offers diagnostic advantages and robust coverage that standard conformal methods lack.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.561695"
    },
    {
        "index": "#348",
        "title": "TCR-EML: Explainable Model Layers for TCR-pMHC Prediction",
        "link": "/arxiv/2510.04377",
        "arxiv_id": "2510.04377",
        "authors": "Jiarui Li, Zixiang Yin, Zhengming Ding, Samuel J. Landry, Ramgopal R. Mettu",
        "summary": "T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is a central component of adaptive immunity, with implications for vaccine design, cancer immunotherapy, and autoimmune disease. While recent advances in machine learning have improved prediction of TCR-pMHC binding, the most effective approaches are black-box transformer models that cannot provide a rationale for predictions. Post-hoc explanation methods can provide insight with respect to the input but do not explicitly model biochemical mechanisms (e.g. known binding regions), as in TCR-pMHC binding. ``Explain-by-design'' models (i.e., with architectural components that can be examined directly after training) have been explored in other domains, but have not been used for TCR-pMHC binding. We propose explainable model layers (TCR-EML) that can be incorporated into protein-language model backbones for TCR-pMHC modeling. Our approach uses prototype layers for amino acid residue contacts drawn from known TCR-pMHC binding mechanisms, enabling high-quality explanations for predicted TCR-pMHC binding. Experiments of our proposed method on large-scale datasets demonstrate competitive predictive accuracy and generalization, and evaluation on the TCR-XAI benchmark demonstrates improved explainability compared with existing approaches.",
        "subjects": "Quantitative Methods, Computational Engineering, Finance, and Science, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.570535"
    },
    {
        "index": "#349",
        "title": "Quantizer Design for Finite Model Approximations, Model Learning, and Quantized Q-Learning for MDPs with Unbounded Spaces",
        "link": "/arxiv/2510.04355",
        "arxiv_id": "2510.04355",
        "authors": "Osman Bicer, Ali D. Kara, Serdar Yuksel",
        "summary": "In this paper, for Markov decision processes (MDPs) with unbounded state spaces we present refined upper bounds presented in [Kara et. al. JMLR'23] on finite model approximation errors via optimizing the quantizers used for finite model approximations. We also consider implications on quantizer design for quantized Q-learning and empirical model learning, and the performance of policies obtained via Q-learning where the quantized state is treated as the state itself. We highlight the distinctions between planning, where approximating MDPs can be independently designed, and learning (either via Q-learning or empirical model learning), where approximating MDPs are restricted to be defined by invariant measures of Markov chains under exploration policies, leading to significant subtleties on quantizer design performance, even though asymptotic near optimality can be established under both setups. In particular, under Lyapunov growth conditions, we obtain explicit upper bounds which decay to zero as the number of bins approaches infinity.",
        "subjects": "Optimization and Control, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.571185"
    },
    {
        "index": "#352",
        "title": "Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins",
        "link": "/arxiv/2510.04346",
        "arxiv_id": "2510.04346",
        "authors": "Nahshon Mokua Obiri, Kristof Van Laerhoven",
        "summary": "Indoor LoRaWAN propagation is shaped by structural and time-varying context factors, which challenge log-distance models and the assumption of log-normal shadowing. We present an environment-aware, statistically disciplined path loss framework evaluated using leakage-safe cross-validation on a 12-month campaign in an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is augmented with environmental covariates (relative humidity, temperature, carbon dioxide, particulate matter, and barometric pressure), as well as the signal-to-noise ratio. We compare multiple linear regression with regularized variants, Bayesian linear regression, and a selective second-order polynomial applied to continuous drivers. Predictor relevance is established using heteroscedasticity-robust Type II and III analysis of variance and nested partial F tests. Shadow fading is profiled with kernel density estimation and non-parametric families, including Normal, Skew-Normal, Student's t, and Gaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07 to 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are non-Gaussian; a 3-component mixture captures a sharp core with a light, broad tail. We convert accuracy into reliability by prescribing the fade margin as the upper-tail quantile of cross-validated residuals, quantifying uncertainty via a moving-block bootstrap, and validating on a held-out set. At 99% packet delivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7 to 27.9 dB for linear baselines. This result presents a deployment-ready, interpretable workflow with calibrated reliability control for indoor Internet of Things planning, aligned with 6G targets.",
        "subjects": "Networking and Internet Architecture, Machine Learning, Signal Processing, Numerical Analysis",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.573319"
    },
    {
        "index": "#354",
        "title": "Towards Fast Option Pricing PDE Solvers Powered by PIELM",
        "link": "/arxiv/2510.04322",
        "arxiv_id": "2510.04322",
        "authors": "Akshay Govind Srinivasan, Anuj Jagannath Said, Sathwik Pentela, Vikas Dwivedi, Balaji Srinivasan",
        "summary": "Partial differential equation (PDE) solvers underpin modern quantitative finance, governing option pricing and risk evaluation. Physics-Informed Neural Networks (PINNs) have emerged as a promising approach for solving the forward and inverse problems of partial differential equations (PDEs) using deep learning. However they remain computationally expensive due to their iterative gradient descent based optimization and scale poorly with increasing model size. This paper introduces Physics-Informed Extreme Learning Machines (PIELMs) as fast alternative to PINNs for solving both forward and inverse problems in financial PDEs. PIELMs replace iterative optimization with a single least-squares solve, enabling deterministic and efficient training. We benchmark PIELM on the Black-Scholes and Heston-Hull-White models for forward pricing and demonstrate its capability in inverse model calibration to recover volatility and interest rate parameters from noisy data. From experiments we observe that PIELM achieve accuracy comparable to PINNs while being up to $30\\times$ faster, highlighting their potential for real-time financial modeling.",
        "subjects": "Computational Engineering, Finance, and Science, Machine Learning, Numerical Analysis",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.574792"
    },
    {
        "index": "#356",
        "title": "Adaptive Coverage Policies in Conformal Prediction",
        "link": "/arxiv/2510.04318",
        "arxiv_id": "2510.04318",
        "authors": "Etienne Gauthier, Francis Bach, Michael I. Jordan",
        "summary": "Traditional conformal prediction methods construct prediction sets such that the true label falls within the set with a user-specified coverage level. However, poorly chosen coverage levels can result in uninformative predictions, either producing overly conservative sets when the coverage level is too high, or empty sets when it is too low. Moreover, the fixed coverage level cannot adapt to the specific characteristics of each individual example, limiting the flexibility and efficiency of these methods. In this work, we leverage recent advances in e-values and post-hoc conformal inference, which allow the use of data-dependent coverage levels while maintaining valid statistical guarantees. We propose to optimize an adaptive coverage policy by training a neural network using a leave-one-out procedure on the calibration set, allowing the coverage level and the resulting prediction set size to vary with the difficulty of each individual example. We support our approach with theoretical coverage guarantees and demonstrate its practical benefits through a series of experiments.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.581391"
    },
    {
        "index": "#361",
        "title": "Relative Information Gain and Gaussian Process Regression",
        "link": "/arxiv/2510.04277",
        "arxiv_id": "2510.04277",
        "authors": "Hamish Flynn",
        "summary": "The sample complexity of estimating or maximising an unknown function in a reproducing kernel Hilbert space is known to be linked to both the effective dimension and the information gain associated with the kernel. While the information gain has an attractive information-theoretic interpretation, the effective dimension typically results in better rates. We introduce a new quantity called the relative information gain, which measures the sensitivity of the information gain with respect to the observation noise. We show that the relative information gain smoothly interpolates between the effective dimension and the information gain, and that the relative information gain has the same growth rate as the effective dimension. In the second half of the paper, we prove a new PAC-Bayesian excess risk bound for Gaussian process regression. The relative information gain arises naturally from the complexity term in this PAC-Bayesian bound. We prove bounds on the relative information gain that depend on the spectral properties of the kernel. When these upper bounds are combined with our excess risk bound, we obtain minimax-optimal rates of convergence.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.584671"
    },
    {
        "index": "#363",
        "title": "Detection of retinal diseases using an accelerated reused convolutional network",
        "link": "/arxiv/2510.04232",
        "arxiv_id": "2510.04232",
        "authors": "Amin Ahmadi Kasani, Hedieh Sajedi",
        "summary": "Convolutional neural networks are continually evolving, with some efforts aimed at improving accuracy, others at increasing speed, and some at enhancing accessibility. Improving accessibility broadens the application of neural networks across a wider range of tasks, including the detection of eye diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can prevent many vision disorders. Given the importance of this issue, various datasets have been collected from the cornea to facilitate the process of making neural network models. However, most of the methods introduced in the past are computationally complex. In this study, we tried to increase the accessibility of deep neural network models. We did this at the most fundamental level, specifically by redesigning and optimizing the convolutional layers. By doing so, we created a new general model that incorporates our novel convolutional layer named ArConv layers. Thanks to the efficient performance of this new layer, the model has suitable complexity for use in mobile phones and can perform the task of diagnosing the presence of disease with high accuracy. The final model we present contains only 1.3 million parameters. In comparison to the MobileNetV2 model, which has 2.2 million parameters, our model demonstrated better accuracy when trained and evaluated on the RfMiD dataset under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on the RfMiD test set.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.591260"
    },
    {
        "index": "#364",
        "title": "A Universal Deep Learning Force Field for Molecular Dynamic Simulation and Vibrational Spectra Prediction",
        "link": "/arxiv/2510.04227",
        "arxiv_id": "2510.04227",
        "authors": "Shengjiao Ji, Yujin Zhang, Zihan Zou, Bin Jiang, Jun Jiang, Yi Luo, Wei Hu",
        "summary": "Accurate and efficient simulation of infrared (IR) and Raman spectra is essential for molecular identification and structural analysis. Traditional quantum chemistry methods based on the harmonic approximation neglect anharmonicity and nuclear quantum effects, while ab initio molecular dynamics (AIMD) remains computationally expensive. Here, we integrate our deep equivariant tensor attention network (DetaNet) with a velocity-Verlet integrator to enable fast and accurate machine learning molecular dynamics (MLMD) simulations for spectral prediction. Trained on the QMe14S dataset containing energies, forces, dipole moments, and polarizabilities for 186,102 small organic molecules, DetaNet yields a universal and transferable force field with high-order tensor prediction capability. Using time-correlation functions derived from MLMD and ring-polymer molecular dynamics (RPMD) trajectories, we computed IR and Raman spectra that accurately reproduce anharmonic and nuclear quantum effects. Benchmark tests on isolated molecules, including polycyclic aromatic hydrocarbons, demonstrate that the DetaNet-based MD approach achieves near-experimental spectral accuracy with speedups up to three orders of magnitude over AIMD. Furthermore, the framework extends seamlessly to molecular and inorganic crystals, molecular aggregates, and biological macromolecules such as polypeptides with minimal fine-tuning. In all systems, DetaNet maintains high accuracy while significantly reducing computational cost. Overall, this work establishes a universal machine learning force field and tensor-aware MLMD framework that enable fast, accurate, and broadly applicable dynamic simulations and IR/Raman spectral predictions across diverse molecular and material systems.",
        "subjects": "Chemical Physics, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.592021"
    },
    {
        "index": "#369",
        "title": "From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation",
        "link": "/arxiv/2510.04180",
        "arxiv_id": "2510.04180",
        "authors": "Ran Eisenberg, Amit Rozner, Ethan Fetaya, Ofir Lindenbaum",
        "summary": "Deep neural networks have achieved remarkable success in computer vision; however, their black-box nature in decision-making limits interpretability and trust, particularly in safety-critical applications. Interpretability is crucial in domains where errors have severe consequences. Existing models not only lack transparency but also risk exploiting unreliable or misleading features, which undermines both robustness and the validity of their explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by reasoning through human-interpretable concepts. Still, they require costly concept annotations and lack spatial grounding, often failing to identify which regions support each concept. We propose SEG-MIL-CBM, a novel framework that integrates concept-guided image segmentation into an attention-based multiple instance learning (MIL) framework, where each segmented region is treated as an instance and the model learns to aggregate evidence across them. By reasoning over semantically meaningful regions aligned with high-level concepts, our model highlights task-relevant evidence, down-weights irrelevant cues, and produces spatially grounded, concept-level explanations without requiring annotations of concepts or groups. SEG-MIL-CBM achieves robust performance across settings involving spurious correlations (unintended dependencies between background and label), input corruptions (perturbations that degrade visual quality), and large-scale benchmarks, while providing transparent, concept-level explanations.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.601158"
    },
    {
        "index": "#370",
        "title": "Drax: Speech Recognition with Discrete Flow Matching",
        "link": "/arxiv/2510.04162",
        "arxiv_id": "2510.04162",
        "authors": "Aviv Navon, Aviv Shamsian, Neta Glazer, Yael Segal-Feldman, Gill Hetz, Joseph Keshet, Ethan Fetaya",
        "summary": "Diffusion and flow-based non-autoregressive (NAR) models have shown strong promise in large language modeling, however, their potential for automatic speech recognition (ASR) remains largely unexplored. We propose Drax, a discrete flow matching framework for ASR that enables efficient parallel decoding. To better align training with inference, we construct an audio-conditioned probability path that guides the model through trajectories resembling likely intermediate inference errors, rather than direct random noise to target transitions. Our theoretical analysis links the generalization gap to divergences between training and inference occupancies, controlled by cumulative velocity errors, thereby motivating our design choice. Empirical evaluation demonstrates that our approach attains recognition accuracy on par with state-of-the-art speech models while offering improved accuracy-efficiency trade-offs, highlighting discrete flow matching as a promising direction for advancing NAR ASR.",
        "subjects": "Audio and Speech Processing, Machine Learning, Sound",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.601928"
    },
    {
        "index": "#371",
        "title": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation",
        "link": "/arxiv/2510.04153",
        "arxiv_id": "2510.04153",
        "authors": "Haoqi Wu, Wei Dai, Ming Xu, Li Wang, Qiang Yan",
        "summary": "Diffusion Models have gained significant popularity due to their remarkable capabilities in image generation, albeit at the cost of intensive computation requirement. Meanwhile, despite their widespread deployment in inference services such as Midjourney, concerns about the potential leakage of sensitive information in uploaded user prompts have arisen. Existing solutions either lack rigorous privacy guarantees or fail to strike an effective balance between utility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play safeguard that enables oblivious cloud-device hybrid generation. By oblivious, each input prompt is transformed into a set of semantically similar candidate prompts that differ only in sensitive attributes (e.g., gender, ethnicity). The cloud server processes all candidate prompts without knowing which one is the real one, thus preventing any prompt leakage. To mitigate server cost, only a small portion of denoising steps is performed upon the large cloud model. The intermediate latents are then sent back to the client, which selects the targeted latent and completes the remaining denoising using a small device model. Additionally, we analyze and incorporate several cache-based accelerations that leverage temporal and batch redundancy, effectively reducing computation cost with minimal utility degradation. Extensive experiments across multiple datasets demonstrate that ObCLIP provides rigorous privacy and comparable utility to cloud models with slightly increased server cost.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.602622"
    },
    {
        "index": "#375",
        "title": "RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback",
        "link": "/arxiv/2510.04096",
        "arxiv_id": "2510.04096",
        "authors": "Tommy Mordo, Sagie Dekel, Omer Madmon, Moshe Tennenholtz, Oren Kurland",
        "summary": "Competitive search is a setting where document publishers modify them to improve their ranking in response to a query. Recently, publishers have increasingly leveraged LLMs to generate and modify competitive content. We introduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that trains LLMs using preference datasets derived from ranking competitions. The goal of a publisher (LLM-based) agent is to optimize content for improved ranking while accounting for the strategies of competing agents. We generate the datasets using approaches that do not rely on human-authored data. We show that our proposed agents consistently and substantially outperform previously suggested approaches for LLM-based competitive document modification. We further show that our agents are effective with ranking functions they were not trained for (i.e., out of distribution) and they adapt to strategic opponents. These findings provide support to the significant potential of using reinforcement learning in competitive search.",
        "subjects": "Information Retrieval, Computer Science and Game Theory, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.605384"
    },
    {
        "index": "#377",
        "title": "Sharp Lower Bounds for Linearized ReLU^k Approximation on the Sphere",
        "link": "/arxiv/2510.04060",
        "arxiv_id": "2510.04060",
        "authors": "Tong Mao, Jinchao Xu",
        "summary": "We prove a saturation theorem for linearized shallow ReLU$^k$ neural networks on the unit sphere $\\mathbb S^d$. For any antipodally quasi-uniform set of centers, if the target function has smoothness $r>\\tfrac{d+2k+1}{2}$, then the best $\\mathcal{L}^2(\\mathbb S^d)$ approximation cannot converge faster than order $n^{-\\frac{d+2k+1}{2d}}$. This lower bound matches existing upper bounds, thereby establishing the exact saturation order $\\tfrac{d+2k+1}{2d}$ for such networks. Our results place linearized neural-network approximation firmly within the classical saturation framework and show that, although ReLU$^k$ networks outperform finite elements under equal degrees $k$, this advantage is intrinsically limited.",
        "subjects": "Numerical Analysis, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.611846"
    },
    {
        "index": "#379",
        "title": "Simulation-based inference via telescoping ratio estimation for trawl processes",
        "link": "/arxiv/2510.04042",
        "arxiv_id": "2510.04042",
        "authors": "Dan Leonte, Raphaël Huser, Almut E. D. Veraart",
        "summary": "The growing availability of large and complex datasets has increased interest in temporal stochastic processes that can capture stylized facts such as marginal skewness, non-Gaussian tails, long memory, and even non-Markovian dynamics. While such models are often easy to simulate from, parameter estimation remains challenging. Simulation-based inference (SBI) offers a promising way forward, but existing methods typically require large training datasets or complex architectures and frequently yield confidence (credible) regions that fail to attain their nominal values, raising doubts on the reliability of estimates for the very features that motivate the use of these models. To address these challenges, we propose a fast and accurate, sample-efficient SBI framework for amortized posterior inference applicable to intractable stochastic processes. The proposed approach relies on two main steps: first, we learn the posterior density by decomposing it sequentially across parameter dimensions. Then, we use Chebyshev polynomial approximations to efficiently generate independent posterior samples, enabling accurate inference even when Markov chain Monte Carlo methods mix poorly. We further develop novel diagnostic tools for SBI in this context, as well as post-hoc calibration techniques; the latter not only lead to performance improvements of the learned inferential tool, but also to the ability to reuse it directly with new time series of varying lengths, thus amortizing the training cost. We demonstrate the method's effectiveness on trawl processes, a class of flexible infinitely divisible models that generalize univariate Gaussian processes, applied to energy demand data.",
        "subjects": "Machine Learning, Machine Learning, Methodology",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.613183"
    },
    {
        "index": "#381",
        "title": "Multi-Modal Multi-Task Semantic Communication: A Distributed Information Bottleneck Perspective",
        "link": "/arxiv/2510.04000",
        "arxiv_id": "2510.04000",
        "authors": "Yujie Zhou, Yiwei Liao, Cheng Peng, Yong Xiao, Yingyu Li",
        "summary": "Semantic communication (SemCom) shifts the focus from data transmission to meaning delivery, enabling efficient and intelligent communication. Existing AI-based coding schemes for multi-modal multi-task SemCom often require transmitters with full-modal data to participate in all receivers' tasks, which leads to redundant transmissions and conflicts with the physical limits of channel capacity and computational capability. In this paper, we propose PoM$^2$-DIB, a novel framework that extends the distributed information bottleneck (DIB) theory to address this problem. Unlike the typical DIB, this framework introduces modality selection as an additional key design variable, enabling a more flexible tradeoff between communication rate and inference quality. This extension selects only the most relevant modalities for task participation, adhering to the physical constraints, while following efficient DIB-based coding. To optimize selection and coding end-to-end, we relax modality selection into a probabilistic form, allowing the use of score function estimation with common randomness to enable optimizable coordinated decisions across distributed devices. Experimental results on public datasets verify that PoM$^2$-DIB achieves high inference quality compared to full-participation baselines in various tasks under physical limits.",
        "subjects": "Information Theory, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.614916"
    },
    {
        "index": "#382",
        "title": "Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning",
        "link": "/arxiv/2510.03993",
        "arxiv_id": "2510.03993",
        "authors": "Yaxin Hou, Bo Han, Yuheng Jia, Hui Liu, Junhui Hou",
        "summary": "Current long-tailed semi-supervised learning methods assume that labeled data exhibit a long-tailed distribution, and unlabeled data adhere to a typical predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed). However, the distribution of the unlabeled data is generally unknown and may follow an arbitrary distribution. To tackle this challenge, we propose a Controllable Pseudo-label Generation (CPG) framework, expanding the labeled dataset with the progressively identified reliable pseudo-labels from the unlabeled dataset and training the model on the updated labeled dataset with a known distribution, making it unaffected by the unlabeled data distribution. Specifically, CPG operates through a controllable self-reinforcing optimization cycle: (i) at each training step, our dynamic controllable filtering mechanism selectively incorporates reliable pseudo-labels from the unlabeled dataset into the labeled dataset, ensuring that the updated labeled dataset follows a known distribution; (ii) we then construct a Bayes-optimal classifier using logit adjustment based on the updated labeled data distribution; (iii) this improved classifier subsequently helps identify more reliable pseudo-labels in the next training step. We further theoretically prove that this optimization cycle can significantly reduce the generalization error under some conditions. Additionally, we propose a class-aware adaptive augmentation module to further improve the representation of minority classes, and an auxiliary branch to maximize data utilization by leveraging all labeled and unlabeled samples. Comprehensive evaluations on various commonly used benchmark datasets show that CPG achieves consistent improvements, surpassing state-of-the-art methods by up to \\textbf{15.97\\%} in accuracy. The code is available at https://github.com/yaxinhou/CPG.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.615613"
    },
    {
        "index": "#383",
        "title": "Beyond Static Evaluation: Rethinking the Assessment of Personalized Agent Adaptability in Information Retrieval",
        "link": "/arxiv/2510.03984",
        "arxiv_id": "2510.03984",
        "authors": "Kirandeep Kaur, Preetam Prabhu Srikar Dammu, Hideo Joho, Chirag Shah",
        "summary": "Personalized AI agents are becoming central to modern information retrieval, yet most evaluation methodologies remain static, relying on fixed benchmarks and one-off metrics that fail to reflect how users' needs evolve over time. These limitations hinder our ability to assess whether agents can meaningfully adapt to individuals across dynamic, longitudinal interactions. In this perspective paper, we propose a conceptual lens for rethinking evaluation in adaptive personalization, shifting the focus from static performance snapshots to interaction-aware, evolving assessments. We organize this lens around three core components: (1) persona-based user simulation with temporally evolving preference models; (2) structured elicitation protocols inspired by reference interviews to extract preferences in context; and (3) adaptation-aware evaluation mechanisms that measure how agent behavior improves across sessions and tasks. While recent works have embraced LLM-driven user simulation, we situate this practice within a broader paradigm for evaluating agents over time. To illustrate our ideas, we conduct a case study in e-commerce search using the PersonalWAB dataset. Beyond presenting a framework, our work lays a conceptual foundation for understanding and evaluating personalization as a continuous, user-centric endeavor.",
        "subjects": "Information Retrieval, Machine Learning",
        "date": "2025-10-05",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.621519"
    },
    {
        "index": "#385",
        "title": "Self-Speculative Masked Diffusions",
        "link": "/arxiv/2510.03929",
        "arxiv_id": "2510.03929",
        "authors": "Andrew Campbell, Valentin De Bortoli, Jiaxin Shi, Arnaud Doucet",
        "summary": "We present self-speculative masked diffusions, a new class of masked diffusion generative models for discrete data that require significantly fewer function evaluations to generate samples. Standard masked diffusion models predict factorized logits over currently masked positions. A number of masked positions are then sampled, however, the factorization approximation means that sampling too many positions in one go leads to poor sample quality. As a result, many simulation steps and therefore neural network function evaluations are required to generate high-quality data. We reduce the computational burden by generating non-factorized predictions over masked positions. This is achieved by modifying the final transformer attention mask from non-causal to causal, enabling draft token generation and parallel validation via a novel, model-integrated speculative sampling mechanism. This results in a non-factorized predictive distribution over masked positions in a single forward pass. We apply our method to GPT2 scale text modelling and protein sequences generation, finding that we can achieve a ~2x reduction in the required number of network forward passes relative to standard masked diffusion models.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.623005"
    },
    {
        "index": "#386",
        "title": "Optimal Computation from Fluctuation Responses",
        "link": "/arxiv/2510.03900",
        "arxiv_id": "2510.03900",
        "authors": "Jinghao Lyu, Kyle J. Ray, James P. Crutchfield",
        "summary": "The energy cost of computation has emerged as a central challenge at the intersection of physics and computer science. Recent advances in statistical physics -- particularly in stochastic thermodynamics -- enable precise characterizations of work, heat, and entropy production in information-processing systems driven far from equilibrium by time-dependent control protocols. A key open question is then how to design protocols that minimize thermodynamic cost while ensur- ing correct outcomes. To this end, we develop a unified framework to identify optimal protocols using fluctuation response relations (FRR) and machine learning. Unlike previous approaches that optimize either distributions or protocols separately, our method unifies both using FRR-derived gradients. Moreover, our method is based primarily on iteratively learning from sampled noisy trajectories, which is generally much easier than solving for the optimal protocol directly from a set of governing equations. We apply the framework to canonical examples -- bit erasure in a double-well potential and translating harmonic traps -- demonstrating how to construct loss functions that trade-off energy cost against task error. The framework extends trivially to underdamped systems, and we show this by optimizing a bit-flip in an underdamped system. In all computations we test, the framework achieves the theoretically optimal protocol or achieves work costs comparable to relevant finite time bounds. In short, the results provide principled strategies for designing thermodynamically efficient protocols in physical information-processing systems. Applications range from quantum gates robust under noise to energy-efficient control of chemical and synthetic biological networks.",
        "subjects": "Statistical Mechanics, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.623649"
    },
    {
        "index": "#387",
        "title": "Fair Minimum Labeling: Efficient Temporal Network Activations for Reachability and Equity",
        "link": "/arxiv/2510.03899",
        "arxiv_id": "2510.03899",
        "authors": "Lutz Oettershagen, Othon Michail",
        "summary": "Balancing resource efficiency and fairness is critical in networked systems that support modern learning applications. We introduce the Fair Minimum Labeling (FML) problem: the task of designing a minimum-cost temporal edge activation plan that ensures each group of nodes in a network has sufficient access to a designated target set, according to specified coverage requirements. FML captures key trade-offs in systems where edge activations incur resource costs and equitable access is essential, such as distributed data collection, update dissemination in edge-cloud systems, and fair service restoration in critical infrastructure. We show that FML is NP-hard and $\\Omega(\\log |V|)$-hard to approximate, and we present probabilistic approximation algorithms that match this bound, achieving the best possible guarantee for the activation cost. We demonstrate the practical utility of FML in a fair multi-source data aggregation task for training a shared model. Empirical results show that FML enforces group-level fairness with substantially lower activation cost than baseline heuristics, underscoring its potential for building resource-efficient, equitable temporal reachability in learning-integrated networks.",
        "subjects": "Social and Information Networks, Data Structures and Algorithms, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.624316"
    },
    {
        "index": "#391",
        "title": "Smart Paste: Automatically Fixing Copy/Paste for Google Developers",
        "link": "/arxiv/2510.03843",
        "arxiv_id": "2510.03843",
        "authors": "Vincent Nguyen, Guilherme Herzog, José Cambronero, Marcus Revaj, Aditya Kini, Alexander Frömmgen, Maxim Tabachnyk",
        "summary": "Manually editing pasted code is a long-standing developer pain point. In internal software development at Google, we observe that code is pasted 4 times more often than it is manually typed. These paste actions frequently require follow-up edits, ranging from simple reformatting and renaming to more complex style adjustments and cross-language translations. Prior work has shown deep learning can be used to predict these edits. In this work, we show how to iteratively develop and scale Smart Paste, an IDE feature for post-paste edit suggestions, to Google's development environment. This experience can serve as a guide for AI practitioners on a holistic approach to feature development, covering user experience, system integration, and model capabilities. Since deployment, Smart Paste has had overwhelmingly positive feedback with a 45% acceptance rate. At Google's enterprise scale, these accepted suggestions account substantially for over 1% of all code written company-wide.",
        "subjects": "Software Engineering, Human-Computer Interaction, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.627637"
    },
    {
        "index": "#392",
        "title": "Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO",
        "link": "/arxiv/2510.03831",
        "arxiv_id": "2510.03831",
        "authors": "Pedro Ivo da Cruz, Dimitri Silva, Tito Spadini, Ricardo Suyama, Murilo Bellezoni Loiola",
        "summary": "Massive multiple-input multiple-output (MMIMO) is essential to modern wireless communication systems, like 5G and 6G, but it is vulnerable to active eavesdropping attacks. One type of such attack is the pilot contamination attack (PCA), where a malicious user copies pilot signals from an authentic user during uplink, intentionally interfering with the base station's (BS) channel estimation accuracy. In this work, we propose to use a Decision Tree (DT) algorithm for PCA detection at the BS in a multi-user system. We present a methodology to generate training data for the DT classifier and select the best DT according to their depth. Then, we simulate different scenarios that could be encountered in practice and compare the DT to a classical technique based on likelihood ratio testing (LRT) submitted to the same scenarios. The results revealed that a DT with only one level of depth is sufficient to outperform the LRT. The DT shows a good performance regarding the probability of detection in noisy scenarios and when the malicious user transmits with low power, in which case the LRT fails to detect the PCA. We also show that the reason for the good performance of the DT is its ability to compute a threshold that separates PCA data from non-PCA data better than the LRT's threshold. Moreover, the DT does not necessitate prior knowledge of noise power or assumptions regarding the signal power of malicious users, prerequisites typically essential for LRT and other hypothesis testing methodologies.",
        "subjects": "Cryptography and Security, Information Theory, Machine Learning, Signal Processing",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.628401"
    },
    {
        "index": "#393",
        "title": "A Trustworthy Industrial Fault Diagnosis Architecture Integrating Probabilistic Models and Large Language Models",
        "link": "/arxiv/2510.03815",
        "arxiv_id": "2510.03815",
        "authors": "Yue wu",
        "summary": "There are limitations of traditional methods and deep learning methods in terms of interpretability, generalization, and quantification of uncertainty in industrial fault diagnosis, and there are core problems of insufficient credibility in industrial fault diagnosis. The architecture performs preliminary analysis through a Bayesian network-based diagnostic engine and features an LLM-driven cognitive quorum module with multimodal input capabilities. The module conducts expert-level arbitration of initial diagnoses by analyzing structured features and diagnostic charts, prioritizing final decisions after conflicts are identified. To ensure the reliability of the system output, the architecture integrates a confidence calibration module based on temperature calibration and a risk assessment module, which objectively quantifies the reliability of the system using metrics such as expected calibration error (ECE). Experimental results on a dataset containing multiple fault types showed that the proposed framework improved diagnostic accuracy by more than 28 percentage points compared to the baseline model, while the calibrated ECE was reduced by more than 75%. Case studies have confirmed that HCAA effectively corrects misjudgments caused by complex feature patterns or knowledge gaps in traditional models, providing novel and practical engineering solutions for building high-trust, explainable AI diagnostic systems for industrial applications.",
        "subjects": "Systems and Control, Machine Learning, Signal Processing",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.629032"
    },
    {
        "index": "#395",
        "title": "Cellular Learning: Scattered Data Regression in High Dimensions via Voronoi Cells",
        "link": "/arxiv/2510.03810",
        "arxiv_id": "2510.03810",
        "authors": "Shankar Prasad Sastry",
        "summary": "I present a regression algorithm that provides a continuous, piecewise-smooth function approximating scattered data. It is based on composing and blending linear functions over Voronoi cells, and it scales to high dimensions. The algorithm infers Voronoi cells from seed vertices and constructs a linear function for the input data in and around each cell. As the algorithm does not explicitly compute the Voronoi diagram, it avoids the curse of dimensionality. An accuracy of around 98.2% on the MNIST dataset with 722,200 degrees of freedom (without data augmentation, convolution, or other geometric operators) demonstrates the applicability and scalability of the algorithm.",
        "subjects": "Computational Geometry, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.630380"
    },
    {
        "index": "#396",
        "title": "Spectral Thresholds for Identifiability and Stability:Finite-Sample Phase Transitions in High-Dimensional Learning",
        "link": "/arxiv/2510.03809",
        "arxiv_id": "2510.03809",
        "authors": "William Hao-Cheng Huang",
        "summary": "In high-dimensional learning, models remain stable until they collapse abruptly once the sample size falls below a critical level. This instability is not algorithm-specific but a geometric mechanism: when the weakest Fisher eigendirection falls beneath sample-level fluctuations, identifiability fails. Our Fisher Threshold Theorem formalizes this by proving that stability requires the minimal Fisher eigenvalue to exceed an explicit $O(\\sqrt{d/n})$ bound. Unlike prior asymptotic or model-specific criteria, this threshold is finite-sample and necessary, marking a sharp phase transition between reliable concentration and inevitable failure. To make the principle constructive, we introduce the Fisher floor, a verifiable spectral regularization robust to smoothing and preconditioning. Synthetic experiments on Gaussian mixtures and logistic models confirm the predicted transition, consistent with $d/n$ scaling. Statistically, the threshold sharpens classical eigenvalue conditions into a non-asymptotic law; learning-theoretically, it defines a spectral sample-complexity frontier, bridging theory with diagnostics for robust high-dimensional inference.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.631041"
    },
    {
        "index": "#398",
        "title": "Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach",
        "link": "/arxiv/2510.03797",
        "arxiv_id": "2510.03797",
        "authors": "Rasel Hossen, Diptajoy Mistry, Mushiur Rahman, Waki As Sami Atikur Rahman Hridoy, Sajib Saha, Muhammad Ibrahim",
        "summary": "Urban safety and infrastructure maintenance are critical components of smart city development. Manual monitoring of road damages is time-consuming, highly costly, and error-prone. This paper presents a deep learning approach for automated road damage and manhole detection using the YOLOv9 algorithm with polygonal annotations. Unlike traditional bounding box annotation, we employ polygonal annotations for more precise localization of road defects. We develop a novel dataset comprising more than one thousand images which are mostly collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based model for three classes, namely Broken, Not Broken, and Manhole. We achieve 78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score) classes, with challenges in Manhole detection (18.2% F1-score) due to class imbalance. Our approach offers an efficient and scalable solution for monitoring urban infrastructure in developing countries.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.632524"
    },
    {
        "index": "#399",
        "title": "A Benchmark Study of Deep Learning Methods for Multi-Label Pediatric Electrocardiogram-Based Cardiovascular Disease Classification",
        "link": "/arxiv/2510.03780",
        "arxiv_id": "2510.03780",
        "authors": "Yiqiao Chen",
        "summary": "Cardiovascular disease (CVD) is a major pediatric health burden, and early screening is of critical importance. Electrocardiography (ECG), as a noninvasive and accessible tool, is well suited for this purpose. This paper presents the first benchmark study of deep learning for multi-label pediatric CVD classification on the recently released ZZU-pECG dataset, comprising 3716 recordings with 19 CVD categories. We systematically evaluate four representative paradigms--ResNet-1D, BiLSTM, Transformer, and Mamba 2--under both 9-lead and 12-lead configurations. All models achieved strong results, with Hamming Loss as low as 0.0069 and F1-scores above 85% in most settings. ResNet-1D reached a macro-F1 of 94.67% on the 12-lead subset, while BiLSTM and Transformer also showed competitive performance. Per-class analysis indicated challenges for rare conditions such as hypertrophic cardiomyopathy in the 9-lead subset, reflecting the effect of limited positive samples. This benchmark establishes reusable baselines and highlights complementary strengths across paradigms. It further points to the need for larger-scale, multi-center validation, age-stratified analysis, and broader disease coverage to support real-world pediatric ECG applications.",
        "subjects": "Signal Processing, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.632893"
    },
    {
        "index": "#401",
        "title": "Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets",
        "link": "/arxiv/2510.03776",
        "arxiv_id": "2510.03776",
        "authors": "Tiago Rodrigues de Almeida, Yufei Zhu, Andrey Rudenko, Tomasz P. Kucner, Johannes A. Stork, Martin Magnusson, Achim J. Lilienthal",
        "summary": "Robots and other intelligent systems navigating in complex dynamic environments should predict future actions and intentions of surrounding agents to reach their goals efficiently and avoid collisions. The dynamics of those agents strongly depends on their tasks, roles, or observable labels. Class-conditioned motion prediction is thus an appealing way to reduce forecast uncertainty and get more accurate predictions for heterogeneous agents. However, this is hardly explored in the prior art, especially for mobile robots and in limited data applications. In this paper, we analyse different class-conditioned trajectory prediction methods on two datasets. We propose a set of conditional pattern-based and efficient deep learning-based baselines, and evaluate their performance on robotics and outdoors datasets (THÖR-MAGNI and Stanford Drone Dataset). Our experiments show that all methods improve accuracy in most of the settings when considering class labels. More importantly, we observe that there are significant differences when learning from imbalanced datasets, or in new environments where sufficient data is not available. In particular, we find that deep learning methods perform better on balanced datasets, but in applications with limited data, e.g., cold start of a robot in a new environment, or imbalanced classes, pattern-based methods may be preferable.",
        "subjects": "Robotics, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.633769"
    },
    {
        "index": "#402",
        "title": "Lightweight and Generalizable Acoustic Scene Representations via Contrastive Fine-Tuning and Distillation",
        "link": "/arxiv/2510.03728",
        "arxiv_id": "2510.03728",
        "authors": "Kuang Yuan, Yang Gao, Xilin Li, Xinhao Mei, Syavosh Zadissa, Tarun Pruthi, Saeed Bagheri Sereshki",
        "summary": "Acoustic scene classification (ASC) models on edge devices typically operate under fixed class assumptions, lacking the transferability needed for real-world applications that require adaptation to new or refined acoustic categories. We propose ContrastASC, which learns generalizable acoustic scene representations by structuring the embedding space to preserve semantic relationships between scenes, enabling adaptation to unseen categories without retraining. Our approach combines supervised contrastive fine-tuning of pre-trained models with contrastive representation distillation to transfer this structured knowledge to compact student models. Our evaluation shows that ContrastASC demonstrates improved few-shot adaptation to unseen categories while maintaining strong closed-set performance.",
        "subjects": "Sound, Machine Learning, Audio and Speech Processing, Signal Processing",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.650234"
    },
    {
        "index": "#404",
        "title": "Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks",
        "link": "/arxiv/2510.03725",
        "arxiv_id": "2510.03725",
        "authors": "Thomas Hallopeau, Joris Guérin, Laurent Demagistri, Youssef Fouzai, Renata Gracie, Vanderlei Pascoal De Matos, Helen Gurgel, Nadine Dessay",
        "summary": "While deep learning methods for detecting informal settlements have already been developed, they have not yet fully utilized the potential offered by recent pretrained neural networks. We compare two types of pretrained neural networks for detecting the favelas of Rio de Janeiro: 1. Generic networks pretrained on large diverse datasets of unspecific images, 2. A specialized network pretrained on satellite imagery. While the latter is more specific to the target task, the former has been pretrained on significantly more images. Hence, this research investigates whether task specificity or data volume yields superior performance in urban informal settlement detection.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.652069"
    },
    {
        "index": "#408",
        "title": "The analogy theorem in Hoare logic",
        "link": "/arxiv/2510.03685",
        "arxiv_id": "2510.03685",
        "authors": "Nikitin Nikita",
        "summary": "The introduction of machine learning methods has led to significant advances in automation, optimization, and discoveries in various fields of science and technology. However, their widespread application faces a fundamental limitation: the transfer of models between data domains generally lacks a rigorous mathematical justification. The key problem is the lack of formal criteria to guarantee that a model trained on one type of data will retain its properties on another.This paper proposes a solution to this problem by formalizing the concept of analogy between data sets and models using first-order logic and Hoare logic.We formulate and rigorously prove a theorem that sets out the necessary and sufficient conditions for analogy in the task of knowledge transfer between machine learning models. Practical verification of the analogy theorem on model data obtained using the Monte Carlo method, as well as on MNIST and USPS data, allows us to achieving F1 scores of 0.84 and 0.88 for convolutional neural networks and random forests, respectively.The proposed approach not only allows us to justify the correctness of transfer between domains but also provides tools for comparing the applicability of models to different types of data.The main contribution of the work is a rigorous formalization of analogy at the level of program logic, providing verifiable guarantees of the correctness of knowledge transfer, which opens new opportunities for both theoretical research and the practical use of machine learning models in previously inaccessible areas.",
        "subjects": "Machine Learning, Machine Learning, Logic, Computation, Methodology",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.660198"
    },
    {
        "index": "#410",
        "title": "Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops",
        "link": "/arxiv/2510.03606",
        "arxiv_id": "2510.03606",
        "authors": "Mattia Scardecchia",
        "summary": "Recent advances in self-supervised learning (SSL) have made it possible to learn general-purpose visual features that capture both the high-level semantics and the fine-grained spatial structure of images. Most notably, the recent DINOv2 has established a new state of the art by surpassing weakly supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we examine the core ideas behind its approach, multi-crop view augmentation and self-distillation with a mean teacher, and trace their development in previous work. We then compare the performance of DINO and DINOv2 with other SSL and WSL methods across various downstream tasks, and highlight some remarkable emergent properties of their learned features with transformer backbones. We conclude by briefly discussing DINOv2's limitations, its impact, and future research directions.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning, Image and Video Processing",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.661531"
    },
    {
        "index": "#412",
        "title": "Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation",
        "link": "/arxiv/2510.03598",
        "arxiv_id": "2510.03598",
        "authors": "Alexander V. Mantzaris",
        "summary": "This paper asks whether the Hierarchical Reasoning Model (HRM) with the two Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a deliberately raw regime: no data augmentation, identical optimizer family with one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes stably and performs well on MNIST ($\\approx 98\\%$ test accuracy), but on small natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches 65.0\\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains 77.2\\% while training $\\sim 30\\times$ faster per epoch; on CIFAR-100, HRM achieves only 29.7\\% test accuracy despite 91.5\\% train accuracy, while the same CNN reaches 45.3\\% test with 50.5\\% train accuracy. Loss traces and error analyses indicate healthy optimization but insufficient image-specific inductive bias for HRM in this regime. It is concluded that, for small-resolution image classification without augmentation, HRM is not competitive with even simple convolutional architectures as the HRM currently exist but this does not exclude possibilities that modifications to the model may allow it to improve greatly.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-04",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.662802"
    },
    {
        "index": "#417",
        "title": "Composite Optimization with Error Feedback: the Dual Averaging Approach",
        "link": "/arxiv/2510.03507",
        "arxiv_id": "2510.03507",
        "authors": "Yuan Gao, Anton Rodomanov, Jeremy Rack, Sebastian Stich",
        "summary": "Communication efficiency is a central challenge in distributed machine learning training, and message compression is a widely used solution. However, standard Error Feedback (EF) methods (Seide et al., 2014), though effective for smooth unconstrained optimization with compression (Karimireddy et al., 2019), fail in the broader and practically important setting of composite optimization, which captures, e.g., objectives consisting of a smooth loss combined with a non-smooth regularizer or constraints. The theoretical foundation and behavior of EF in the context of the general composite setting remain largely unexplored. In this work, we consider composite optimization with EF. We point out that the basic EF mechanism and its analysis no longer stand when a composite part is involved. We argue that this is because of a fundamental limitation in the method and its analysis technique. We propose a novel method that combines Dual Averaging with EControl (Gao et al., 2024), a state-of-the-art variant of the EF mechanism, and achieves for the first time a strong convergence analysis for composite optimization with error feedback. Along with our new algorithm, we also provide a new and novel analysis template for inexact dual averaging method, which might be of independent interest. We also provide experimental results to complement our theoretical findings.",
        "subjects": "Optimization and Control, Machine Learning, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.671510"
    },
    {
        "index": "#420",
        "title": "Paris: A Decentralized Trained Open-Weight Diffusion Model",
        "link": "/arxiv/2510.03434",
        "arxiv_id": "2510.03434",
        "authors": "Zhiying Jiang, Raihan Seraj, Marcos Villagra, Bidhan Roy",
        "summary": "We present Paris, the first publicly released diffusion model pre-trained entirely through decentralized computation. Paris demonstrates that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. Paris is open for research and commercial use. Paris required implementing our Distributed Diffusion Training framework from scratch. The model consists of 8 expert diffusion models (129M-605M parameters each) trained in complete isolation with no gradient, parameter, or intermediate activation synchronization. Rather than requiring synchronized gradient updates across thousands of GPUs, we partition data into semantically coherent clusters where each expert independently optimizes its subset while collectively approximating the full distribution. A lightweight transformer router dynamically selects appropriate experts at inference, achieving generation quality comparable to centrally coordinated baselines. Eliminating synchronization enables training on heterogeneous hardware without specialized interconnects. Empirical validation confirms that Paris's decentralized training maintains generation quality while removing the dedicated GPU cluster requirement for large-scale diffusion models. Paris achieves this using 14$\\times$ less training data and 16$\\times$ less compute than the prior decentralized baseline.",
        "subjects": "Graphics, Distributed, Parallel, and Cluster Computing, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.673584"
    },
    {
        "index": "#422",
        "title": "Quantum feature-map learning with reduced resource overhead",
        "link": "/arxiv/2510.03389",
        "arxiv_id": "2510.03389",
        "authors": "Jonas Jäger, Philipp Elsässer, Elham Torabian",
        "summary": "Current quantum computers require algorithms that use limited resources economically. In quantum machine learning, success hinges on quantum feature maps, which embed classical data into the state space of qubits. We introduce Quantum Feature-Map Learning via Analytic Iterative Reconstructions (Q-FLAIR), an algorithm that reduces quantum resource overhead in iterative feature-map circuit construction. It shifts workloads to a classical computer via partial analytic reconstructions of the quantum model, using only a few evaluations. For each probed gate addition to the ansatz, the simultaneous selection and optimization of the data feature and weight parameter is then entirely classical. Integrated into quantum neural network and quantum kernel support vector classifiers, Q-FLAIR shows state-of-the-art benchmark performance. Since resource overhead decouples from feature dimension, we train a quantum model on a real IBM device in only four hours, surpassing 90% accuracy on the full-resolution MNIST dataset (784 features, digits 3 vs 5). Such results were previously unattainable, as the feature dimension prohibitively drives hardware demands for fixed and search costs for adaptive ansätze. By rethinking feature-map learning beyond black-box optimization, this work takes a concrete step toward enabling quantum machine learning for real-world problems and near-term quantum computers.",
        "subjects": "Quantum Physics, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.680056"
    },
    {
        "index": "#423",
        "title": "Is it Bigger than a Breadbox: Efficient Cardinality Estimation for Real World Workloads",
        "link": "/arxiv/2510.03386",
        "arxiv_id": "2510.03386",
        "authors": "Zixuan Yi, Sami Abu-el-Haija, Yawen Wang, Teja Vemparala, Yannis Chronis, Yu Gan, Michael Burrows, Carsten Binnig, Bryan Perozzi, Ryan Marcus, Fatma Ozcan",
        "summary": "DB engines produce efficient query execution plans by relying on cost models. Practical implementations estimate cardinality of queries using heuristics, with magic numbers tuned to improve average performance on benchmarks. Empirically, estimation error significantly grows with query complexity. Alternatively, learning-based estimators offer improved accuracy, but add operational complexity preventing their adoption in-practice. Recognizing that query workloads contain highly repetitive subquery patterns, we learn many simple regressors online, each localized to a pattern. The regressor corresponding to a pattern can be randomly-accessed using hash of graph structure of the subquery. Our method has negligible overhead and competes with SoTA learning-based approaches on error metrics. Further, amending PostgreSQL with our method achieves notable accuracy and runtime improvements over traditional methods and drastically reduces operational costs compared to other learned cardinality estimators, thereby offering the most practical and efficient solution on the Pareto frontier. Concretely, simulating JOB-lite workload on IMDb speeds-up execution by 7.5 minutes (>30%) while incurring only 37 seconds overhead for online learning.",
        "subjects": "Databases, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.680843"
    },
    {
        "index": "#424",
        "title": "Bias and Coverage Properties of the WENDy-IRLS Algorithm",
        "link": "/arxiv/2510.03365",
        "arxiv_id": "2510.03365",
        "authors": "Abhi Chawla, David M. Bortz, Vanja Dukic",
        "summary": "The Weak form Estimation of Nonlinear Dynamics (WENDy) method is a recently proposed class of parameter estimation algorithms that exhibits notable noise robustness and computational efficiency. This work examines the coverage and bias properties of the original WENDy-IRLS algorithm's parameter and state estimators in the context of the following differential equations: Logistic, Lotka-Volterra, FitzHugh-Nagumo, Hindmarsh-Rose, and a Protein Transduction Benchmark. The estimators' performance was studied in simulated data examples, under four different noise distributions (normal, log-normal, additive censored normal, and additive truncated normal), and a wide range of noise, reaching levels much higher than previously tested for this algorithm.",
        "subjects": "Methodology, Machine Learning, Machine Learning",
        "date": "2025-10-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.681494"
    },
    {
        "index": "#427",
        "title": "Assessing the impact of contact time on leachate chemistry from recycled concrete aggregates",
        "link": "/arxiv/2510.03344",
        "arxiv_id": "2510.03344",
        "authors": "Morgan D. Sanger, Gabrielle Campagnola, Robin Ritchey, Tuncer B. Edil, Matthew Ginder-Vogel",
        "summary": "Recycled concrete aggregate (RCA) is recognized as a readily available, mechanically sufficient construction and demolition waste product that is suitable as a base course substitute for natural, virgin aggregate in pavement construction. Environmentally responsible applications of RCA must consider the high alkalinity, high pH leachate, and heavy metal leaching risks reported in the literature. The existing body of literature does not address discrepancies between field and laboratory measurements of RCA leachate pH, nor are there any existing studies of aged RCA leachate composition as a function of time. To consider the influence of contact time on RCA leachate, the present study evaluates recovered RCA base course samples from the Minnesota Road Research highway construction study site using modified batch test methodology. Leachate pH, alkalinity, and calcium ion (Ca2+) concentration were monitored for 24 hours to understand RCA leachate chemistry during the initial contact period. Leachate pH is high upon initial contact with water (pH > 10) and decreases over time as it reacts with atmospheric carbon dioxide. Calcium ion concentration increases rapidly in the initial contact period, then more gradually as calcium saturation is reached. Alkalinity stabilizes (50-65 mg CaCO3/L) after a dramatic increase during the initial contact period.",
        "subjects": "Chemical Physics, Materials Science, Machine Learning",
        "date": "2025-10-02",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.683564"
    },
    {
        "index": "#429",
        "title": "DECOR: Deep Embedding Clustering with Orientation Robustness",
        "link": "/arxiv/2510.03328",
        "arxiv_id": "2510.03328",
        "authors": "Fiona Victoria Stanley Jothiraj, Arunaggiri Pandian Karunanidhi, Seth A. Eichmeyer",
        "summary": "In semiconductor manufacturing, early detection of wafer defects is critical for product yield optimization. However, raw wafer data from wafer quality tests are often complex, unlabeled, imbalanced and can contain multiple defects on a single wafer, making it crucial to design clustering methods that remain reliable under such imperfect data conditions. We introduce DECOR, a deep clustering with orientation robustness framework that groups complex defect patterns from wafer maps into consistent clusters. We evaluate our method on the open source MixedWM38 dataset, demonstrating its ability to discover clusters without manual tuning. DECOR explicitly accounts for orientation variations in wafer maps, ensuring that spatially similar defects are consistently clustered regardless of its rotation or alignment. Experiments indicate that our method outperforms existing clustering baseline methods, thus providing a reliable and scalable solution in automated visual inspection systems.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-01",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.690147"
    },
    {
        "index": "#430",
        "title": "Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties",
        "link": "/arxiv/2510.03320",
        "arxiv_id": "2510.03320",
        "authors": "Raik Dankworth, Gesina Schwalbe",
        "summary": "Deep neural networks (NNs) for computer vision are vulnerable to adversarial attacks, i.e., miniscule malicious changes to inputs may induce unintuitive outputs. One key approach to verify and mitigate such robustness issues is to falsify expected output behavior. This allows, e.g., to locally proof security, or to (re)train NNs on obtained adversarial input examples. Due to the black-box nature of NNs, current attacks only falsify a class of the final output, such as flipping from $\\texttt{stop_sign}$ to $\\neg\\texttt{stop_sign}$. In this short position paper we generalize this to search for generally illogical behavior, as considered in NN verification: falsify constraints (concept-based properties) involving further human-interpretable concepts, like $\\texttt{red}\\wedge\\texttt{octogonal}\\rightarrow\\texttt{stop_sign}$. For this, an easy implementation of concept-based properties on already trained NNs is proposed using techniques from explainable artificial intelligence. Further, we sketch the theoretical proof that attacks on concept-based properties are expected to have a reduced search space compared to simple class falsification, whilst arguably be more aligned with intuitive robustness targets. As an outlook to this work in progress we hypothesize that this approach has potential to efficiently and simultaneously improve logical compliance and robustness.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-10-01",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.690764"
    },
    {
        "index": "#431",
        "title": "SVDefense: Effective Defense against Gradient Inversion Attacks via Singular Value Decomposition",
        "link": "/arxiv/2510.03319",
        "arxiv_id": "2510.03319",
        "authors": "Chenxiang Luo, David K. Y. Yau, Qun Song",
        "summary": "Federated learning (FL) enables collaborative model training without sharing raw data but is vulnerable to gradient inversion attacks (GIAs), where adversaries reconstruct private data from shared gradients. Existing defenses either incur impractical computational overhead for embedded platforms or fail to achieve privacy protection and good model utility at the same time. Moreover, many defenses can be easily bypassed by adaptive adversaries who have obtained the defense details. To address these limitations, we propose SVDefense, a novel defense framework against GIAs that leverages the truncated Singular Value Decomposition (SVD) to obfuscate gradient updates. SVDefense introduces three key innovations, a Self-Adaptive Energy Threshold that adapts to client vulnerability, a Channel-Wise Weighted Approximation that selectively preserves essential gradient information for effective model training while enhancing privacy protection, and a Layer-Wise Weighted Aggregation for effective model aggregation under class imbalance. Our extensive evaluation shows that SVDefense outperforms existing defenses across multiple applications, including image classification, human activity recognition, and keyword spotting, by offering robust privacy protection with minimal impact on model accuracy. Furthermore, SVDefense is practical for deployment on various resource-constrained embedded platforms. We will make our code publicly available upon paper acceptance.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-10-01",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.691404"
    },
    {
        "index": "#435",
        "title": "Machine Learning and Control: Foundations, Advances, and Perspectives",
        "link": "/arxiv/2510.03303",
        "arxiv_id": "2510.03303",
        "authors": "Enrique Zuazua",
        "summary": "Control theory of dynamical systems offers a powerful framework for tackling challenges in deep neural networks and other machine learning architectures. We show that concepts such as simultaneous and ensemble controllability offer new insights into the classification and representation properties of deep neural networks while the control and optimization of static systems can be employed to better understand the performance of shallow networks. Inspired by the classical concept of turnpike, we also explore the relationship between dynamic and static neural networks, where depth is traded for width, and the role of transformers as mechanisms for accelerating classical neural network tasks. We also exploit the expressive power of neural networks (exemplified, for instance, by the Universal Approximation Theorem) to develop a novel hybrid modeling methodology, the Hybrid-Cooperative Learning (HYCO), combining mechanics and data-driven methods in a game-theoretic setting. Finally, we describe how classical properties of diffusion processes, long established in the context of partial differential equations, contribute to explaining the success of modern generative artificial intelligence (AI). We present an overview of our recent results in these areas, illustrating how control, machine learning, numerical analysis, and partial differential equations come together to motivate a fertile ground for future research.",
        "subjects": "Optimization and Control, Machine Learning, Numerical Analysis",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.694091"
    },
    {
        "index": "#439",
        "title": "Mathematically rigorous proofs for Shapley explanations",
        "link": "/arxiv/2510.03281",
        "arxiv_id": "2510.03281",
        "authors": "David van Batenburg",
        "summary": "Machine Learning is becoming increasingly more important in today's world. It is therefore very important to provide understanding of the decision-making process of machine-learning models. A popular way to do this is by looking at the Shapley-Values of these models as introduced by Lundberg and Lee. In this thesis, we discuss the two main results by Lundberg and Lee from a mathematically rigorous standpoint and provide full proofs, which are not available from the original material. The first result of this thesis is an axiomatic characterization of the Shapley values in machine learning based on axioms by Young. We show that the Shapley values are the unique explanation to satisfy local accuracy, missingness, symmetry and consistency. Lundberg and Lee claim that the symmetry axiom is redundant for explanations. However, we provide a counterexample that shows the symmetry axiom is in fact essential. The second result shows that we can write the Shapley values as the unique solution to a weighted linear regression problem. This result is proven with the use of dimensionality reduction.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.699817"
    },
    {
        "index": "#440",
        "title": "Quantile-Scaled Bayesian Optimization Using Rank-Only Feedback",
        "link": "/arxiv/2510.03277",
        "arxiv_id": "2510.03277",
        "authors": "Tunde Fahd Egunjobi",
        "summary": "Bayesian Optimization (BO) is widely used for optimizing expensive black-box functions, particularly in hyperparameter tuning. However, standard BO assumes access to precise objective values, which may be unavailable, noisy, or unreliable in real-world settings where only relative or rank-based feedback can be obtained. In this study, we propose Quantile-Scaled Bayesian Optimization (QS-BO), a principled rank-based optimization framework. QS-BO converts ranks into heteroscedastic Gaussian targets through a quantile-scaling pipeline, enabling the use of Gaussian process surrogates and standard acquisition functions without requiring explicit metric scores. We evaluate QS-BO on synthetic benchmark functions, including one- and two-dimensional nonlinear functions and the Branin function, and compare its performance against Random Search. Results demonstrate that QS-BO consistently achieves lower objective values and exhibits greater stability across runs. Statistical tests further confirm that QS-BO significantly outperforms Random Search at the 1\\% significance level. These findings establish QS-BO as a practical and effective extension of Bayesian Optimization for rank-only feedback, with promising applications in preference learning, recommendation, and human-in-the-loop optimization where absolute metric values are unavailable or unreliable.",
        "subjects": "Machine Learning, Machine Learning, Statistics Theory",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.700466"
    },
    {
        "index": "#441",
        "title": "Improving S&P 500 Volatility Forecasting through Regime-Switching Methods",
        "link": "/arxiv/2510.03236",
        "arxiv_id": "2510.03236",
        "authors": "Ava C. Blake, Nivika A. Gandhi, Anurag R. Jakkula",
        "summary": "Accurate prediction of financial market volatility is critical for risk management, derivatives pricing, and investment strategy. In this study, we propose a multitude of regime-switching methods to improve the prediction of S&P 500 volatility by capturing structural changes in the market across time. We use eleven years of SPX data, from May 1st, 2014 to May 27th, 2025, to compute daily realized volatility (RV) from 5-minute intraday log returns, adjusted for irregular trading days. To enhance forecast accuracy, we engineered features to capture both historical dynamics and forward-looking market sentiment across regimes. The regime-switching methods include a soft Markov switching algorithm to estimate soft-regime probabilities, a distributional spectral clustering method that uses XGBoost to assign clusters at prediction time, and a coefficient-based soft regime algorithm that extracts HAR coefficients from time segments segmented through the Mood test and clusters through Bayesian GMM for soft regime weights, using XGBoost to predict regime probabilities. Models were evaluated across three time periods--before, during, and after the COVID-19 pandemic. The coefficient-based clustering algorithm outperformed all other models, including the baseline autoregressive model, during all time periods. Additionally, each model was evaluated on its recursive forecasting performance for 5- and 10-day horizons during each time period. The findings of this study demonstrate the value of regime-aware modeling frameworks and soft clustering approaches in improving volatility forecasting, especially during periods of heightened uncertainty and structural change.",
        "subjects": "Statistical Finance, Machine Learning, Econometrics",
        "date": "2025-09-21",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.701166"
    },
    {
        "index": "#442",
        "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning",
        "link": "/arxiv/2508.04581",
        "arxiv_id": "2508.04581",
        "authors": "Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis",
        "summary": "Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-08-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-08T08:16:53.701854"
    }
]