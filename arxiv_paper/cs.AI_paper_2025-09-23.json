[
    {
        "index": "#2",
        "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration",
        "link": "/arxiv/2509.19236",
        "arxiv_id": "2509.19236",
        "authors": "Chunhao Tian, Yutong Wang, Xuebo Liu, Zhexuan Wang, Liang Ding, Miao Zhang, Min Zhang",
        "summary": "Proper initialization is crucial for any system, particularly in multi-agent systems (MAS), where it plays a pivotal role in determining both the system's efficiency and effectiveness. However, existing MAS initialization methods do not fully account for the collaborative needs of the generated agents in subsequent stages. Inspired by the principles of effective team composition, we propose AgentInit, which aims to optimize the structure of agent teams. Specifically, in addition to multi-round interactions and reflections between agents during agent generation, AgentInit incorporates a Natural Language to Format mechanism to ensure consistency and standardization. Balanced team selection strategies using Pareto principles are subsequently applied to jointly consider agent team diversity and task relevance to promote effective and efficient collaboration and enhance overall system performance. Experiments show that AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving an overall performance improvement of up to 1.2 and 1.6, respectively, while also significantly reducing token consumption. Further analysis confirms its strong transferability to similar tasks and verifies the effectiveness of its key components, demonstrating its capability and adaptability as a reliable MAS initialization method. Source code and models are available at https://github.com/1737423697/AgentInit.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.978202"
    },
    {
        "index": "#3",
        "title": "Code Driven Planning with Domain-Adaptive Critic",
        "link": "/arxiv/2509.19077",
        "arxiv_id": "2509.19077",
        "authors": "Zikang Tian, Shaohui Peng, Du Huang, Jiaming Guo, Ruizhi Chen, Rui Zhang, Xishan Zhang, Yuxuan Guo, Zidong Du, Qi Guo, Ling Li, Yewen Pu, Xing Hu, Yunji Chen",
        "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI agents in sequential decision-making problems, leveraging their extensive world knowledge. However, the gap between their general knowledge and environment-specific requirements often leads to inaccurate plans. To address this, existing approaches rely on frequent LLM queries to iteratively refine plans based on immediate environmental feedback, which incurs substantial query costs. However, this refinement is typically guided by short-term environmental feedback, limiting LLMs from developing plans aligned with long-term rewards. We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of relying on frequent queries, CoPiC employs LLMs to generate a diverse set of high-level planning programs, which iteratively produce and refine candidate plans. A trained domain-adaptive critic then evaluates these candidates and selects the one most aligned with long-term rewards for execution. Using high-level planning programs as planner and domain-adaptive critic as estimator, CoPiC improves planning while significantly reducing query costs. Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in query costs.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.978600"
    },
    {
        "index": "#4",
        "title": "Towards Causal Representation Learning with Observable Sources as Auxiliaries",
        "link": "/arxiv/2509.19058",
        "arxiv_id": "2509.19058",
        "authors": "Kwonho Kim, Heejeong Nam, Inwoo Hwang, Sanghack Lee",
        "summary": "Causal representation learning seeks to recover latent factors that generate observational data through a mixing function. Needing assumptions on latent structures or relationships to achieve identifiability in general, prior works often build upon conditional independence given known auxiliary variables. However, prior frameworks limit the scope of auxiliary variables to be external to the mixing function. Yet, in some cases, system-driving latent factors can be easily observed or extracted from data, possibly facilitating identification. In this paper, we introduce a framework of observable sources being auxiliaries, serving as effective conditioning variables. Our main results show that one can identify entire latent variables up to subspace-wise transformations and permutations using volume-preserving encoders. Moreover, when multiple known auxiliary variables are available, we offer a variable-selection scheme to choose those that maximize recoverability of the latent factors given knowledge of the latent causal graph. Finally, we demonstrate the effectiveness of our framework through experiments on synthetic graph and image data, thereby extending the boundaries of current approaches.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.978808"
    },
    {
        "index": "#5",
        "title": "Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action",
        "link": "/arxiv/2509.19030",
        "arxiv_id": "2509.19030",
        "authors": "Victoire Herv√©, Henrik Warpefelt, Christoph Salge",
        "summary": "Algorithmic evaluation of procedurally generated content struggles to find metrics that align with human experience, particularly for composite artefacts. Automatic decomposition as a possible solution requires concepts that meet a range of properties. To this end, drawing on Games Studies and Game AI research, we introduce the nested concepts of \\textit{Landmarks}, \\textit{Monuments}, and \\textit{Beacons}. These concepts are based on the artefact's perceivability, evocativeness, and Call to Action, all from a player-centric perspective. These terms are generic to games and usable across genres. We argue that these entities can be found and evaluated with techniques currently used in both research and industry, opening a path towards a fully automated decomposition of PCG, and evaluation of the salient sub-components. Although the work presented here emphasises mixed-initiative PCG and compositional PCG, we believe it applies beyond those domains. With this approach, we intend to create a connection between humanities and technical game research and allow for better computational PCG evaluation",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.978993"
    },
    {
        "index": "#6",
        "title": "Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)",
        "link": "/arxiv/2509.18986",
        "arxiv_id": "2509.18986",
        "authors": "Erik Penther, Michael Grohs, Jana-Rebecca Rehse",
        "summary": "Predictive process monitoring is a sub-domain of process mining which aims to forecast the future of ongoing process executions. One common prediction target is the remaining time, meaning the time that will elapse until a process execution is completed. In this paper, we compare four different remaining time prediction approaches in a real-life outbound warehouse process of a logistics company in the aviation business. For this process, the company provided us with a novel and original event log with 169,523 traces, which we can make publicly available. Unsurprisingly, we find that deep learning models achieve the highest accuracy, but shallow methods like conventional boosting techniques achieve competitive accuracy and require significantly fewer computational resources.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.979168"
    },
    {
        "index": "#7",
        "title": "From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system",
        "link": "/arxiv/2509.18980",
        "arxiv_id": "2509.18980",
        "authors": "Maxime Manderlier, Fabian Lecron, Olivier Vu Thanh, Nicolas Gillis",
        "summary": "We investigate whether large language models (LLMs) can generate effective, user-facing explanations from a mathematically interpretable recommendation model. The model is based on constrained matrix factorization, where user types are explicitly represented and predicted item scores share the same scale as observed ratings, making the model's internal representations and predicted scores directly interpretable. This structure is translated into natural language explanations using carefully designed LLM prompts. Many works in explainable AI rely on automatic evaluation metrics, which often fail to capture users' actual needs and perceptions. In contrast, we adopt a user-centered approach: we conduct a study with 326 participants who assessed the quality of the explanations across five key dimensions-transparency, effectiveness, persuasion, trust, and satisfaction-as well as the recommendations themselves.To evaluate how different explanation strategies are perceived, we generate multiple explanation types from the same underlying model, varying the input information provided to the LLM. Our analysis reveals that all explanation types are generally well received, with moderate statistical differences between strategies. User comments further underscore how participants react to each type of explanation, offering complementary insights beyond the quantitative results.",
        "subjects": "Artificial Intelligence, Human-Computer Interaction, Information Retrieval",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.979371"
    },
    {
        "index": "#8",
        "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions",
        "link": "/arxiv/2509.18970",
        "arxiv_id": "2509.18970",
        "authors": "Xixun Lin, Yucheng Ning, Jingwen Zhang, Yan Dong, Yilong Liu, Yongxuan Wu, Xiaohua Qi, Nan Sun, Yanmin Shang, Pengfei Cao, Lixin Zou, Xu Chen, Chuan Zhou, Jia Wu, Shirui Pan, Bin Wang, Yanan Cao, Kai Chen, Songlin Hu, Li Guo",
        "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based agents have emerged as powerful intelligent systems capable of human-like cognition, reasoning, and interaction. These agents are increasingly being deployed across diverse real-world applications, including student education, scientific research, and financial analysis. However, despite their remarkable potential, LLM-based agents remain vulnerable to hallucination issues, which can result in erroneous task execution and undermine the reliability of the overall system design. Addressing this critical challenge requires a deep understanding and a systematic consolidation of recent advances on LLM-based agents. To this end, we present the first comprehensive survey of hallucinations in LLM-based agents. By carefully analyzing the complete workflow of agents, we propose a new taxonomy that identifies different types of agent hallucinations occurring at different stages. Furthermore, we conduct an in-depth examination of eighteen triggering causes underlying the emergence of agent hallucinations. Through a detailed review of a large number of existing studies, we summarize approaches for hallucination mitigation and detection, and highlight promising directions for future research. We hope this survey will inspire further efforts toward addressing hallucinations in LLM-based agents, ultimately contributing to the development of more robust and reliable agent systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.979654"
    },
    {
        "index": "#9",
        "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning",
        "link": "/arxiv/2509.18942",
        "arxiv_id": "2509.18942",
        "authors": "Xiao Han, Zimo Zhao, Wanyu Wang, Maolin Wang, Zitao Liu, Yi Chang, Xiangyu Zhao",
        "summary": "Recent advancements in Large Language Models (LLMs) have emphasized the critical role of fine-tuning (FT) techniques in adapting LLMs to specific tasks, especially when retraining from scratch is computationally infeasible. Fine-tuning enables LLMs to leverage task- or domain-specific data, producing models that more effectively meet the requirements of targeted applications. However, con- ventional FT approaches often suffer from catastrophic forgetting and suboptimal data efficiency, limiting their real-world applicability. To address these challenges, this paper proposes DEAL, a novel framework that integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy. By incorporating knowledge retention and adaptive parameter update modules, the framework mitigates the lim- itations of existing FT methods while maintaining efficiency in privacy-preserving settings. Experiments on 15 diverse datasets show that DEAL consistently outper- forms baseline methods, yielding substantial gains in task accuracy and resource efficiency. These findings demonstrate the potential of our approach to advance continual adaptation in LLMs by enhancing task performance while improving resource efficiency.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.979893"
    },
    {
        "index": "#10",
        "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective",
        "link": "/arxiv/2509.18905",
        "arxiv_id": "2509.18905",
        "authors": "Songsong Yu, Yuxin Chen, Hao Ju, Lianjie Jia, Fuxi Zhang, Shaofei Huang, Yuhan Wu, Rundi Cui, Binghao Ran, Zaibin Zhang, Zhedong Zheng, Zhipeng Zhang, Yifan Wang, Lin Song, Lijun Wang, Yanwei Li, Ying Shan, Huchuan Lu",
        "summary": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a critical requirement for advancing embodied intelligence and autonomous systems. Despite recent progress in Vision-Language Models (VLMs), achieving human-level VSR remains highly challenging due to the complexity of representing and reasoning over three-dimensional space. In this paper, we present a systematic investigation of VSR in VLMs, encompassing a review of existing methodologies across input modalities, model architectures, training strategies, and reasoning mechanisms. Furthermore, we categorize spatial intelligence into three levels of capability, ie, basic perception, spatial understanding, spatial planning, and curate SIBench, a spatial intelligence benchmark encompassing nearly 20 open-source datasets across 23 task settings. Experiments with state-of-the-art VLMs reveal a pronounced gap between perception and reasoning, as models show competence in basic perceptual tasks but consistently underperform in understanding and planning tasks, particularly in numerical estimation, multi-view reasoning, temporal dynamics, and spatial imagination. These findings underscore the substantial challenges that remain in achieving spatial intelligence, while providing both a systematic roadmap and a comprehensive benchmark to drive future research in the field. The related resources of this study are accessible at https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.980228"
    },
    {
        "index": "#11",
        "title": "LongCat-Flash-Thinking Technical Report",
        "link": "/arxiv/2509.18883",
        "arxiv_id": "2509.18883",
        "authors": "Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chengcheng Han, Chenhui Yang, Chi Zhang, Chong Peng, Chuyu Zhang, Cong Chen, Fengcun Li, Gang Xu, Guoyuan Lin, Hao Jiang, Hao Liang, Haomin Fu, Haoxiang Ma, Hong Liu, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiahao Liu, Jiahuan Li, Jialin Liu, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiaqi Sun, Jiaqi Zhang, Jiarong Shi, Jiawei Yang, Jingang Wang, Jinrui Ding, Jun Kuang, Jun Xu, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Li Wei, Liang Shi, Lin Qiu, Lingbin Kong, Lingchuan Liu, Linsen Guo, Longfei An, Mai Xia, Meng Zhou, Mengshen Zhu, Peng Pei, Pengcheng Jia, Qi Gu, Qi Guo, Qiong Huang, Quan Chen, Quanchi Weng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shanglin Lei, Shuai Du, Shuaikang Liu, Shuang Zhou, Shuhao Hu, Siyu Xu, Songshan Gong, Tao Liang, Tianhao Hu, Wei He, Wei Shi, Wei Wang, Wei Wu, Wei Zhuo, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Xi Su, Xiangcheng Liu, Xiangyu Xi, Xiangzhou Huang, Xiao Liu, Xiaochen Jiang, Xiaowei Shi, Xiaowen Shi, Xiaoyu Li, Xin Chen, Xinyue Zhao, Xuan Huang, Xuemiao Zhang, Xuezhi Cao, Xunliang Cai, Yajie Zhang, Yang Chen, Yang Liu, Yang Liu, Yang Zheng, Yaoming Wang, Yaqi Huo, Yerui Sun, Yifan Lu, Yiyang Li, Youshao Xiao, Yuanzhe Lei, Yuchen Xie, Yueqing Sun, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunke Zhao, Yuqing Ding, Yuwei Jiang, Zhaohua Yang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhongda Su, Ziran Li, Ziwen Wang, Ziyuan Zhuang, Zongyu Wang, Zunyuan Yang",
        "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities are cultivated through a meticulously crafted training process, beginning with long Chain-of-Thought (CoT) data cold-start and culminating in large-scale Reinforcement Learning (RL). We first employ a well-designed cold-start training strategy, which significantly enhances the reasoning potential and equips the model with specialized skills in both formal and agentic reasoning. Then, a core innovation is our domain-parallel training scheme, which decouples optimization across distinct domains (e.g., STEM, Code, Agentic) and subsequently fuses the resulting expert models into a single, nearly Pareto-optimal model. This entire process is powered by our Dynamic ORchestration for Asynchronous rollout (DORA) system, a large-scale RL framework that delivers a greater than threefold training speedup over synchronous methods on tens of thousands of accelerators. As a result, LongCat-Flash-Thinking achieves state-of-the-art performance among open-source models on a suite of complex reasoning tasks. The model exhibits exceptional efficiency in agentic reasoning, reducing average token consumption by 64.5% (from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We release LongCat-Flash-Thinking to promote further advances in reasoning systems and agentic AI research.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.981206"
    },
    {
        "index": "#12",
        "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution",
        "link": "/arxiv/2509.18868",
        "arxiv_id": "2509.18868",
        "authors": "Dianxing Zhang, Wendong Li, Kani Song, Jiaye Lu, Gang Li, Liuchun Yang, Sheng Li",
        "summary": "Under a unified operational definition, we define LLM memory as a persistent state written during pretraining, finetuning, or inference that can later be addressed and that stably influences outputs. We propose a four-part taxonomy (parametric, contextual, external, procedural/episodic) and a memory quadruple (location, persistence, write/access path, controllability). We link mechanism, evaluation, and governance via the chain write -> read -> inhibit/update. To avoid distorted comparisons across heterogeneous setups, we adopt a three-setting protocol (parametric only, offline retrieval, online retrieval) that decouples capability from information availability on the same data and timeline. On this basis we build a layered evaluation: parametric (closed-book recall, edit differential, memorization/privacy), contextual (position curves and the mid-sequence drop), external (answer correctness vs snippet attribution/faithfulness), and procedural/episodic (cross-session consistency and timeline replay, E MARS+). The framework integrates temporal governance and leakage auditing (freshness hits, outdated answers, refusal slices) and uncertainty reporting via inter-rater agreement plus paired tests with multiple-comparison correction. For updating and forgetting, we present DMM Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC), and RAG to form an auditable loop covering admission thresholds, rollout, monitoring, rollback, and change audits, with specs for timeliness, conflict handling, and long-horizon consistency. Finally, we give four testable propositions: minimum identifiability; a minimal evaluation card; causally constrained editing with verifiable forgetting; and when retrieval with small-window replay outperforms ultra-long-context reading. This yields a reproducible, comparable, and governable coordinate system for research and deployment.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.981418"
    },
    {
        "index": "#13",
        "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling",
        "link": "/arxiv/2509.18864",
        "arxiv_id": "2509.18864",
        "authors": "Yingxin Li, Jianbo Zhao, Xueyu Ren, Jie Tang, Wangjie You, Xu Chen, Kan Zhou, Chao Feng, Jiao Ran, Yuan Meng, Zhi Wang",
        "summary": "User profiling, as a core technique for user understanding, aims to infer structural attributes from user information. Large Language Models (LLMs) provide a promising avenue for user profiling, yet the progress is hindered by the lack of comprehensive benchmarks. To bridge this gap, we propose ProfileBench, an industrial benchmark derived from a real-world video platform, encompassing heterogeneous user data and a well-structured profiling taxonomy. However, the profiling task remains challenging due to the difficulty of collecting large-scale ground-truth labels, and the heterogeneous and noisy user information can compromise the reliability of LLMs. To approach label-free and reliable user profiling, we propose a Confidence-driven Profile reasoning framework Conf-Profile, featuring a two-stage paradigm. We first synthesize high-quality labels by leveraging advanced LLMs with confidence hints, followed by confidence-weighted voting for accuracy improvement and confidence calibration for a balanced distribution. The multiple profile results, rationales, and confidence scores are aggregated and distilled into a lightweight LLM. We further enhance the reasoning ability via confidence-guided unsupervised reinforcement learning, which exploits confidence for difficulty filtering, quasi-ground truth voting, and reward weighting. Experimental results demonstrate that Conf-Profile delivers substantial performance through the two-stage training, improving F1 by 13.97 on Qwen3-8B.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.981655"
    },
    {
        "index": "#14",
        "title": "MAPO: Mixed Advantage Policy Optimization",
        "link": "/arxiv/2509.18849",
        "arxiv_id": "2509.18849",
        "authors": "Wenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, Huanjin Yao, Guancheng Wan, Ke Liang, Wenwen He, Mingjun Li, Leszek Rutkowski, Mang Ye, Bo Du, Dacheng Tao",
        "summary": "Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.981908"
    },
    {
        "index": "#15",
        "title": "Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning",
        "link": "/arxiv/2509.18846",
        "arxiv_id": "2509.18846",
        "authors": "Hong-Jie Dai, Zheng-Hao Li, An-Tai Lu, Bo-Tsz Shain, Ming-Ta Li, Tatheer Hussain Mir, Kuang-Te Wang, Min-I Su, Pei-Kang Liu, Ming-Ju Tsai",
        "summary": "Accurate International Classification of Diseases (ICD) coding is critical for clinical documentation, billing, and healthcare analytics, yet it remains a labour-intensive and error-prone task. Although large language models (LLMs) show promise in automating ICD coding, their challenges in base model selection, input contextualization, and training data redundancy limit their effectiveness. We propose a modular framework for ICD-10 Clinical Modification (ICD-10-CM) code prediction that addresses these challenges through principled model selection, redundancy-aware data sampling, and structured input design. The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce aggregation to assess and rank open-source LLMs based on their intrinsic comprehension of ICD-10-CM code definitions. We introduced embedding-based similarity measures, a redundancy-aware sampling strategy to remove semantically duplicated discharge summaries. We leverage structured discharge summaries from Taiwanese hospitals to evaluate contextual effects and examine section-wise content inclusion under universal and section-specific modelling paradigms. Experiments across two institutional datasets demonstrate that the selected base model after fine-tuning consistently outperforms baseline LLMs in internal and external evaluations. Incorporating more clinical sections consistently improves prediction performance. This study uses open-source LLMs to establish a practical and principled approach to ICD-10-CM code prediction. The proposed framework provides a scalable, institution-ready solution for real-world deployment of automated medical coding systems by combining informed model selection, efficient data refinement, and context-aware prompting.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.982196"
    },
    {
        "index": "#16",
        "title": "Bounded PCTL Model Checking of Large Language Model Outputs",
        "link": "/arxiv/2509.18836",
        "arxiv_id": "2509.18836",
        "authors": "Dennis Gross, Helge Spieker, Arnaud Gotlieb",
        "summary": "In this paper, we introduce LLMCHECKER, a model-checking-based verification method to verify the probabilistic computation tree logic (PCTL) properties of an LLM text generation process. We empirically show that only a limited number of tokens are typically chosen during text generation, which are not always the same. This insight drives the creation of $\\alpha$-$k$-bounded text generation, narrowing the focus to the $\\alpha$ maximal cumulative probability on the top-$k$ tokens at every step of the text generation process. Our verification method considers an initial string and the subsequent top-$k$ tokens while accommodating diverse text quantification methods, such as evaluating text quality and biases. The threshold $\\alpha$ further reduces the selected tokens, only choosing those that exceed or meet it in cumulative probability. LLMCHECKER then allows us to formally verify the PCTL properties of $\\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our knowledge, this is the first time PCTL-based model checking has been used to check the consistency of the LLM text generation process.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.982395"
    },
    {
        "index": "#17",
        "title": "The AGNTCY Agent Directory Service: Architecture and Implementation",
        "link": "/arxiv/2509.18787",
        "arxiv_id": "2509.18787",
        "authors": "Luca Muscariello, Vijoy Pandey, Ramiz Polic",
        "summary": "The Agent Directory Service (ADS) is a distributed directory for the discovery of AI agent capabilities, metadata, and provenance. It leverages content-addressed storage, hierarchical taxonomies, and cryptographic signing to enable efficient, verifiable, and multi-dimensional discovery across heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema Framework (OASF), ADS decouples capability indexing from content location through a two-level mapping realized over a Kademlia-based Distributed Hash Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact distribution, integrates Sigstore for provenance, and supports schema-driven extensibility for emerging agent modalities (LLM prompt agents, MCP servers, A2A-enabled components). This paper formalizes the architectural model, describes storage and discovery layers, explains security and performance properties, and positions ADS within the broader landscape of emerging agent registry and interoperability initiatives.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.982581"
    },
    {
        "index": "#18",
        "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models",
        "link": "/arxiv/2509.18771",
        "arxiv_id": "2509.18771",
        "authors": "Xingkun Yin, Kaibin Huang, Dong In Kim, Hongyang Du",
        "summary": "Scaling model size, training data, and compute power have driven advances in large language models (LLMs), but these approaches are reaching saturation as human-generated text is exhausted and further gains diminish. We propose experience scaling, a framework for continuous post-deployment evolution for LLMs through autonomous interaction with the environment and collaborative sharing of accumulated experience. The framework captures raw interactions, distills them into compact, reusable knowledge, and periodically refines stored content to preserve relevance and efficiency. We validate the framework in simulated real-world scenarios involving generalization to previously unseen but related tasks, repetitive queries, and over-saturated knowledge stores. Across all settings, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations. These results demonstrate that structured post-deployment learning can extend LLM capabilities beyond the limits of static human-generated data, offering a scalable path for continued intelligence progress.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.982776"
    },
    {
        "index": "#19",
        "title": "Autonomous Data Agents: A New Opportunity for Smart Data",
        "link": "/arxiv/2509.18710",
        "arxiv_id": "2509.18710",
        "authors": "Yanjie Fu, Dongjie Wang, Wangyang Ying, Xiangliang Zhang, Huan Liu, Jian Pei",
        "summary": "As data continues to grow in scale and complexity, preparing, transforming, and analyzing it remains labor-intensive, repetitive, and difficult to scale. Since data contains knowledge and AI learns knowledge from it, the alignment between AI and data is essential. However, data is often not structured in ways that are optimal for AI utilization. Moreover, an important question arises: how much knowledge can we pack into data through intensive data operations? Autonomous data agents (DataAgents), which integrate LLM reasoning with task decomposition, action reasoning and grounding, and tool calling, can autonomously interpret data task descriptions, decompose tasks into subtasks, reason over actions, ground actions into python code or tool calling, and execute operations. Unlike traditional data management and engineering tools, DataAgents dynamically plan workflows, call powerful tools, and adapt to diverse data tasks at scale. This report argues that DataAgents represent a paradigm shift toward autonomous data-to-knowledge systems. DataAgents are capable of handling collection, integration, preprocessing, selection, transformation, reweighing, augmentation, reprogramming, repairs, and retrieval. Through these capabilities, DataAgents transform complex and unstructured data into coherent and actionable knowledge. We first examine why the convergence of agentic AI and data-to-knowledge systems has emerged as a critical trend. We then define the concept of DataAgents and discuss their architectural design, training strategies, as well as the new skills and capabilities they enable. Finally, we call for concerted efforts to advance action workflow optimization, establish open datasets and benchmark ecosystems, safeguard privacy, balance efficiency with scalability, and develop trustworthy DataAgent guardrails to prevent malicious actions.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.982979"
    },
    {
        "index": "#20",
        "title": "Advances in Large Language Models for Medicine",
        "link": "/arxiv/2509.18690",
        "arxiv_id": "2509.18690",
        "authors": "Zhiyu Kan, Wensheng Gan, Zhenlian Qi, Philip S. Yu",
        "summary": "Artificial intelligence (AI) technology has advanced rapidly in recent years, with large language models (LLMs) emerging as a significant breakthrough. LLMs are increasingly making an impact across various industries, with the medical field standing out as the most prominent application area. This paper systematically reviews the up-to-date research progress of LLMs in the medical field, providing an in-depth analysis of training techniques for large medical models, their adaptation in healthcare settings, related applications, as well as their strengths and limitations. Furthermore, it innovatively categorizes medical LLMs into three distinct types based on their training methodologies and classifies their evaluation approaches into two categories. Finally, the study proposes solutions to existing challenges and outlines future research directions based on identified issues in the field of medical LLMs. By systematically reviewing previous and advanced research findings, we aim to highlight the necessity of developing medical LLMs, provide a deeper understanding of their current state of development, and offer clear guidance for subsequent research.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.983163"
    },
    {
        "index": "#21",
        "title": "Implementation of airborne ML models with semantics preservation",
        "link": "/arxiv/2509.18681",
        "arxiv_id": "2509.18681",
        "authors": "Nicolas Valot, Louis Fabre, Benjamin Lesage, Ammar Mechouche, Claire Pagetti",
        "summary": "Machine Learning (ML) may offer new capabilities in airborne systems. However, as any piece of airborne systems, ML-based systems will be required to guarantee their safe operation. Thus, their development will have to be demonstrated to be compliant with the adequate guidance. So far, the European Union Aviation Safety Agency (EASA) has published a concept paper and an EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level objectives to confirm the ML model achieves its intended function and maintains training performance in the target environment. The paper aims to clarify the difference between an ML model and its corresponding unambiguous description, referred to as the Machine Learning Model Description (MLMD). It then refines the essential notion of semantics preservation to ensure the accurate replication of the model. We apply our contributions to several industrial use cases to build and compare several target models.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.983452"
    },
    {
        "index": "#22",
        "title": "TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation",
        "link": "/arxiv/2509.18667",
        "arxiv_id": "2509.18667",
        "authors": "Qiao Xiao, Hong Ting Tsang, Jiaxin Bai",
        "summary": "Graph-based Retrieval-augmented generation (RAG) has become a widely studied approach for improving the reasoning, accuracy, and factuality of Large Language Models. However, many existing graph-based RAG systems overlook the high cost associated with LLM token usage during graph construction, hindering large-scale adoption. To address this, we propose TERAG, a simple yet effective framework designed to build informative graphs at a significantly lower cost. Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the retrieval phase, and we achieve at least 80% of the accuracy of widely used graph-based RAG methods while consuming only 3%-11% of the output tokens.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.983652"
    },
    {
        "index": "#23",
        "title": "Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents",
        "link": "/arxiv/2509.18633",
        "arxiv_id": "2509.18633",
        "authors": "Yara Mohajerani",
        "summary": "Climate risk assessment requires modelling complex interactions between spatially heterogeneous hazards and adaptive economic systems. We present a novel geospatial agent-based model that integrates climate hazard data with evolutionary learning for economic agents. Our framework combines Mesa-based spatial modelling with CLIMADA climate impact assessment, introducing adaptive learning behaviours that allow firms to evolve strategies for budget allocation, pricing, wages, and risk adaptation through fitness-based selection and mutation. We demonstrate the framework using riverine flood projections under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to converge with baseline (no hazard) production levels after decades of disruption due to climate stress. Our results reveal systemic risks where even agents that are not directly exposed to floods face impacts through supply chain disruptions, with the end-of-century average price of goods 5.6% higher under RCP8.5 compared to the baseline. This open-source framework provides financial institutions and companies with tools to quantify both direct and cascading climate risks while evaluating cost-effective adaptation strategies.",
        "subjects": "Artificial Intelligence, Risk Management",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.983830"
    },
    {
        "index": "#24",
        "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation",
        "link": "/arxiv/2509.18565",
        "arxiv_id": "2509.18565",
        "authors": "Mitchell Piehl, Dillon Wilson, Ananya Kalita, Jugal Kalita",
        "summary": "Large Language Models (LLMs) excel at various tasks, including problem-solving and question-answering. However, LLMs often find Math Word Problems (MWPs) challenging because solving them requires a range of reasoning and mathematical abilities with which LLMs seem to struggle. Recent efforts have helped LLMs solve more complex MWPs with improved prompts. This study proposes a novel method that initially prompts an LLM to create equations from a decomposition of the question, followed by using an external symbolic equation solver to produce an answer. To ensure the accuracy of the obtained answer, inspired by an established recommendation of math teachers, the LLM is instructed to solve the MWP a second time, but this time with the objective of estimating the correct answer instead of solving it exactly. The estimation is then compared to the generated answer to verify. If verification fails, an iterative rectification process is employed to ensure the correct answer is eventually found. This approach achieves new state-of-the-art results on datasets used by prior published research on numeric and algebraic MWPs, improving the previous best results by nearly two percent on average. In addition, the approach obtains satisfactory results on trigonometric MWPs, a task not previously attempted to the authors' best knowledge. This study also introduces two new datasets, SVAMPClean and Trig300, to further advance the testing of LLMs' reasoning abilities.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.984048"
    },
    {
        "index": "#25",
        "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs",
        "link": "/arxiv/2509.18557",
        "arxiv_id": "2509.18557",
        "authors": "Tom Pawelek, Raj Patel, Charlotte Crowell, Noorbakhsh Amiri, Sudip Mittal, Shahram Rahimi, Andy Perkins",
        "summary": "Compared to traditional models, agentic AI represents a highly valuable target for potential attackers as they possess privileged access to data sources and API tools, which are traditionally not incorporated into classical agents. Unlike a typical software application residing in a Demilitarized Zone (DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI (only defining a final goal, leaving the path selection to LLM). This characteristic introduces substantial security risk to both operational security and information security. Most common existing defense mechanism rely on detection of malicious intent and preventing it from reaching the LLM agent, thus protecting against jailbreak attacks such as prompt injection. In this paper, we present an alternative approach, LLMZ+, which moves beyond traditional detection-based approaches by implementing prompt whitelisting. Through this method, only contextually appropriate and safe messages are permitted to interact with the agentic LLM. By leveraging the specificity of context, LLMZ+ guarantees that all exchanges between external users and the LLM conform to predefined use cases and operational boundaries. Our approach streamlines the security framework, enhances its long-term resilience, and reduces the resources required for sustaining LLM information security. Our empirical evaluation demonstrates that LLMZ+ provides strong resilience against the most common jailbreak prompts. At the same time, legitimate business communications are not disrupted, and authorized traffic flows seamlessly between users and the agentic LLM. We measure the effectiveness of approach using false positive and false negative rates, both of which can be reduced to 0 in our experimental setting.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.984294"
    },
    {
        "index": "#26",
        "title": "FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning",
        "link": "/arxiv/2509.18527",
        "arxiv_id": "2509.18527",
        "authors": "Ziwen Chen, Zhong Wang",
        "summary": "The sport of fencing, like many other sports, faces challenges in refereeing: subjective calls, human errors, bias, and limited availability in practice environments. We present FERA (Fencing Referee Assistant), a prototype AI referee for foil fencing which integrates pose-based multi-label action recognition and rule-based reasoning. FERA extracts 2D joint positions from video, normalizes them, computes a 101-dimensional kinematic feature set, and applies a Transformer for multi-label move and blade classification. To determine priority and scoring, FERA applies a distilled language model with encoded right-of-way rules, producing both a decision and an explanation for each exchange. With limited hand-labeled data, a 5-fold cross-validation achieves an average macro-F1 score of 0.549, outperforming multiple baselines, including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla Transformer. While not ready for deployment, these results demonstrate a promising path towards automated referee assistance in foil fencing and new opportunities for AI applications, such as coaching in the field of fencing.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.984469"
    },
    {
        "index": "#28",
        "title": "Instruction-Following Evaluation in Function Calling for Large Language Models",
        "link": "/arxiv/2509.18420",
        "arxiv_id": "2509.18420",
        "authors": "Nikolai Skripko",
        "summary": "Function calling is a core capability of large language models, essential for AI agents. Existing benchmarks such as the Berkeley Function Calling Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench (arXiv:2501.12851) evaluate argument correctness but do not test adherence to format instructions embedded in parameter descriptions, such as enclosing values in double quotes or using ISO date formats. We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911) that assesses precise instruction following in function calling. IFEval-FC encodes verifiable formats directly within JSON schema descriptions, for example specifying that a value must not contain punctuation. It includes 750 test cases, each consisting of a function with an embedded format for one of its input parameters and a corresponding user query. Evaluation is fully algorithmic, ensuring objectivity, reproducibility, and scalability. Our results show that even state-of-the-art proprietary models, including GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules, highlighting a practical limitation for real-world agent systems. The complete codebase and data are publicly available at https://github.com/Skripkon/IFEval-FC.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.984886"
    },
    {
        "index": "#29",
        "title": "ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification",
        "link": "/arxiv/2509.18400",
        "arxiv_id": "2509.18400",
        "authors": "Pritish Yuvraj, Siva Devarakonda",
        "summary": "Accurate classification of products under the Harmonized Tariff Schedule (HTS) is a critical bottleneck in global trade, yet it has received little attention from the machine learning community. Misclassification can halt shipments entirely, with major postal operators suspending deliveries to the U.S. due to incomplete customs documentation. We introduce the first benchmark for HTS code classification, derived from the U.S. Customs Rulings Online Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit classifications and 57.5 percent correct 6-digit classifications, improvements of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking. Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to guarantee data privacy in high-stakes trade and compliance workflows. While Atlas sets a strong baseline, the benchmark remains highly challenging, with only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim to position HTS classification as a new community benchmark task and invite future work in retrieval, reasoning, and alignment.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.985052"
    },
    {
        "index": "#30",
        "title": "G√∂del Test: Can Large Language Models Solve Easy Conjectures?",
        "link": "/arxiv/2509.18383",
        "arxiv_id": "2509.18383",
        "authors": "Moran Feldman, Amin Karbasi",
        "summary": "Recent announcements from frontier AI model labs have highlighted strong results on high-school and undergraduate math competitions. Yet it remains unclear whether large language models can solve new, simple conjectures in more advanced areas of mathematics. We propose the G√∂del Test: evaluating whether a model can produce correct proofs for very simple, previously unsolved conjectures. To this end, we study the performance of GPT-5 on five conjectures in combinatorial optimization. For each problem, we provided one or two source papers from which the conjecture arose, withheld our own conjecture, and then assessed the model's reasoning in detail. On the three easier problems, GPT-5 produced nearly correct solutions; for Problem 2 it even derived a different approximation guarantee that, upon checking, refuted our conjecture while providing a valid solution. The model failed on Problem 4, which required combining results from two papers. On Problem 5, a harder case without a validated conjecture, GPT-5 proposed the same algorithm we had in mind but failed in the analysis, suggesting the proof is more challenging than expected. Although our sample is small, the results point to meaningful progress on routine reasoning, occasional flashes of originality, and clear limitations when cross-paper synthesis is required. GPT-5 may represent an early step toward frontier models eventually passing the G√∂del Test.",
        "subjects": "Artificial Intelligence, Discrete Mathematics, Machine Learning",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.985245"
    },
    {
        "index": "#31",
        "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints",
        "link": "/arxiv/2509.18382",
        "arxiv_id": "2509.18382",
        "authors": "Adarsha Balaji, Le Chen, Rajeev Thakur, Franck Cappello, Sandeep Madireddy",
        "summary": "Test-time compute scaling has demonstrated the ability to improve the performance of reasoning language models by generating longer chain-of-thought (CoT) sequences. However, this increase in performance comes with a significant increase in computational cost. In this work, we investigate two compute constraint strategies: (1) reasoning length constraint and (2) model quantization, as methods to reduce the compute demand of reasoning models and study their impact on their safety performance. Specifically, we explore two approaches to apply compute constraints to reasoning models: (1) fine-tuning reasoning models using a length controlled policy optimization (LCPO) based reinforcement learning method to satisfy a user-defined CoT reasoning length, and (2) applying quantization to maximize the generation of CoT sequences within a user-defined compute constraint. Furthermore, we study the trade-off between the computational efficiency and the safety of the model.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.985431"
    },
    {
        "index": "#33",
        "title": "Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces",
        "link": "/arxiv/2509.18230",
        "arxiv_id": "2509.18230",
        "authors": "Zihan Dong, Xinyu Fan, Zixiang Tang, Yunqing Li",
        "summary": "Controlling desktop applications via software remains a fundamental yet under-served problem. Existing multi-modal large language models (MLLMs) ingest screenshots and task instructions to generate keystrokes and mouse events, but they suffer from prohibitive inference latency, poor sample efficiency on long-horizon sparse-reward tasks, and infeasible on-device deployment. We introduce a lightweight hierarchical reinforcement learning framework, ComputerAgent, that formulates OS control as a two-level option process (manager and subpolicy), employs a triple-modal state encoder (screenshot, task ID, numeric state) to handle visual and contextual diversity, integrates meta-actions with an early-stop mechanism to reduce wasted interactions, and uses a compact vision backbone plus small policy networks for on-device inference (15M parameters). On a suite of 135 real-world desktop tasks, ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on simple scenarios while reducing model size by over four orders of magnitude and halving inference time. These results demonstrate that hierarchical RL offers a practical, scalable alternative to monolithic MLLM-based automation for computer control.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.985952"
    },
    {
        "index": "#34",
        "title": "An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems",
        "link": "/arxiv/2509.18229",
        "arxiv_id": "2509.18229",
        "authors": "Anthony Patera, Rohan Abeyaratne",
        "summary": "Generative AI, and specifically GPT, can produce a remarkable solution to a mechanical engineering analysis problem - but also, on occasion, a flawed solution. For example, an elementary mechanics problem is solved flawlessly in one GPT instance and incorrectly in a subsequent GPT instance, with a success probability of only 85%. This unreliability renders \"out-of-the-box\" GPT unsuitable for deployment in education or engineering practice. We introduce an \"N-Plus-1\" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering Problem Statements. Agency first launches N instantiations of Agent Solve to yield N independent Proposed Problem Solution Realizations; Agency then invokes Agent Compare to summarize and compare the N Proposed Problem Solution Realizations and to provide a Recommended Problem Solution. We argue from Condorcet's Jury Theorem that, for a Problem Statement characterized by per-Solve success probability greater than 1/2 (and N sufficiently large), the Predominant (Agent Compare) Proposed Problem Solution will, with high probability, correspond to a Correct Proposed Problem Solution. Furthermore, Agent Compare can also incorporate aspects of Secondary (Agent Compare) Proposed Problem Solutions, in particular when the latter represent alternative Problem Statement interpretations - different Mathematical Models - or alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a commercial multi-agent model, show similarities in design and performance, but also important differences in emphasis: our Agency focuses on transparency and pedagogical value.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.986172"
    },
    {
        "index": "#35",
        "title": "From \"What to Eat?\" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation",
        "link": "/arxiv/2509.18226",
        "arxiv_id": "2509.18226",
        "authors": "Yu Fu, Linyue Cai, Ruoyu Wu, Yong Zhao",
        "summary": "Personalized recipe recommendation faces challenges in handling fuzzy user intent, ensuring semantic accuracy, and providing sufficient detail coverage. We propose ChefMind, a hybrid architecture combining Chain of Exploration (CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large Language Model (LLM). CoE refines ambiguous queries into structured conditions, KG offers semantic reasoning and interpretability, RAG supplements contextual culinary details, and LLM integrates outputs into coherent recommendations. We evaluate ChefMind on the Xiachufang dataset and manually annotated queries, comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that ChefMind achieves superior performance in accuracy, relevance, completeness, and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models. Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in handling fuzzy demands.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.986373"
    },
    {
        "index": "#36",
        "title": "Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models",
        "link": "/arxiv/2509.18221",
        "arxiv_id": "2509.18221",
        "authors": "Dingxin Lu, Shurui Wu, Xinyi Huang",
        "summary": "With the rising global burden of chronic diseases and the multimodal and heterogeneous clinical data (medical imaging, free-text recordings, wearable sensor streams, etc.), there is an urgent need for a unified multimodal AI framework that can proactively predict individual health risks. We propose VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer with a large language model (LLM) inference head embedded in its top layer. The system builds on the dual-stream architecture of existing visual-linguistic models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with cross-modal comparison and fine-grained alignment of radiological images, fundus maps, and wearable device photos with corresponding clinical narratives using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion block that integrates irregular visit sequences into the causal Transformer decoder through adaptive time interval position coding; (iii) a disease ontology map adapter that injects ICD-10 codes into visual and textual channels in layers and infers comorbid patterns with the help of a graph attention mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an average AUROC of 0.90 with an expected calibration error of 2.7 percent.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.986559"
    },
    {
        "index": "#37",
        "title": "Similarity Field Theory: A Mathematical Framework for Intelligence",
        "link": "/arxiv/2509.18218",
        "arxiv_id": "2509.18218",
        "authors": "Kei-Sing Ng",
        "summary": "We posit that persisting and transforming similarity relations form the structural basis of any comprehensible dynamic system. This paper introduces Similarity Field Theory, a mathematical framework that formalizes the principles governing similarity values among entities and their evolution. We define: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed relational field (asymmetry and non-transitivity are allowed); (2) the evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by $p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers $F_{\\alpha}(K) = { E \\in U \\mid S(E,K) \\ge \\alpha }$, i.e., superlevel sets of the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that produces new entities. Within this framework, we formalize a generative definition of intelligence: an operator $G$ is intelligent with respect to a concept $K$ if, given a system containing entities belonging to the fiber of $K$, it generates new entities that also belong to that fiber. Similarity Field Theory thus offers a foundational language for characterizing, comparing, and constructing intelligent systems. We prove two theorems: (i) asymmetry blocks mutual inclusion; and (ii) stability requires either an anchor coordinate or eventual confinement within a level set of $f$. These results ensure that the evolution of similarity fields is both constrained and interpretable, culminating in an exploration of how the framework allows us to interpret large language models and use them as experimental probes into societal cognition.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-21",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.986721"
    },
    {
        "index": "#38",
        "title": "nDNA -- the Semantic Helix of Artificial Cognition",
        "link": "/arxiv/2509.18216",
        "arxiv_id": "2509.18216",
        "authors": "Amitava Das",
        "summary": "As AI foundation models grow in capability, a deeper question emerges: What shapes their internal cognitive identity -- beyond fluency and output? Benchmarks measure behavior, but the soul of a model resides in its latent geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic representation that captures this latent identity through the intrinsic geometry of belief. At its core, nDNA is synthesized from three principled and indispensable dimensions of latent geometry: spectral curvature, which reveals the curvature of conceptual flow across layers; thermodynamic length, which quantifies the semantic effort required to traverse representational transitions through layers; and belief vector field, which delineates the semantic torsion fields that guide a model's belief directional orientations. Like biological DNA, it encodes ancestry, mutation, and semantic inheritance, found in finetuning and alignment scars, cultural imprints, and architectural drift. In naming it, we open a new field: Neural Genomics, where models are not just tools, but digital semantic organisms with traceable inner cognition. Modeling statement. We read AI foundation models as semantic fluid--dynamics: meaning is transported through layers like fluid in a shaped conduit; nDNA is the physics-grade readout of that flow -- a geometry-first measure of how meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free neural DNA fingerprint tied to on-input behavior; with this fingerprint we cross into biology: tracing lineages across pretraining, fine-tuning, alignment, pruning, distillation, and merges; measuring inheritance between checkpoints; detecting drift as traits shift under new data or objectives; and, ultimately, studying the evolution of artificial cognition to compare models, diagnose risks, and govern change over time.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-21",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.986891"
    },
    {
        "index": "#41",
        "title": "An Outcome-Based Educational Recommender System",
        "link": "/arxiv/2509.18186",
        "arxiv_id": "2509.18186",
        "authors": "Nursultan Askarbekuly, Timur Fayzrakhmanov, Sladjan Babarogiƒá, Ivan Lukoviƒá",
        "summary": "Most educational recommender systems are tuned and judged on click- or rating-based relevance, leaving their true pedagogical impact unclear. We introduce OBER-an Outcome-Based Educational Recommender that embeds learning outcomes and assessment items directly into the data schema, so any algorithm can be evaluated on the mastery it fosters. OBER uses a minimalist entity-relation model, a log-driven mastery formula, and a plug-in architecture. Integrated into an e-learning system in non-formal domain, it was evaluated trough a two-week randomized split test with over 5 700 learners across three methods: fixed expert trajectory, collaborative filtering (CF), and knowledge-based (KB) filtering. CF maximized retention, but the fixed path achieved the highest mastery. Because OBER derives business, relevance, and learning metrics from the same logs, it lets practitioners weigh relevance and engagement against outcome mastery with no extra testing overhead. The framework is method-agnostic and readily extensible to future adaptive or context-aware recommenders.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-18",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.987547"
    },
    {
        "index": "#42",
        "title": "Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling",
        "link": "/arxiv/2509.18181",
        "arxiv_id": "2509.18181",
        "authors": "Mustafa Sameen, Xiaojian Zhang, Xilei Zhao",
        "summary": "Accurate modeling of ridesourcing mode choices is essential for designing and implementing effective traffic management policies for reducing congestion, improving mobility, and allocating resources more efficiently. Existing models for predicting ridesourcing mode choices often suffer from limited predictive accuracy due to their inability to capture key psychological factors, and are further challenged by severe class imbalance, as ridesourcing trips comprise only a small fraction of individuals' daily travel. To address these limitations, this paper introduces the Synthesizing Attitudes, Predicting Actions (SAPA) framework, a hierarchical approach that uses Large Language Models (LLMs) to synthesize theory-grounded latent attitudes to predict ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler personas from raw travel survey data and then trains a propensity-score model on demographic and behavioral features, enriched by those personas, to produce an individual-level score. Next, the LLM assigns quantitative scores to theory-driven latent variables (e.g., time and cost sensitivity), and a final classifier integrates the propensity score, latent-variable scores (with their interaction terms), and observable trip attributes to predict ridesourcing mode choice. Experiments on a large-scale, multi-year travel survey show that SAPA significantly outperforms state-of-the-art baselines, improving ridesourcing choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set. This study provides a powerful tool for accurately predicting ridesourcing mode choices, and provides a methodology that is readily transferable to various applications.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-18",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.987741"
    },
    {
        "index": "#43",
        "title": "Large Language Models and Operations Research: A Structured Survey",
        "link": "/arxiv/2509.18180",
        "arxiv_id": "2509.18180",
        "authors": "Yang Wang, Kai Li",
        "summary": "Operations research (OR) provides fundamental methodologies for complex system decision-making, with established applications in transportation, supply chain management, and production scheduling. Traditional approaches, which depend on expert-based modeling and manual parameter adjustment, often face challenges in handling large-scale, dynamic, and multi-constraint problems. Recently, large language models (LLMs) have shown potential to address these limitations through semantic understanding, structured generation, and reasoning control. LLMs can translate natural language descriptions into mathematical models or executable code, generate heuristics, evolve algorithms, and directly tackle optimization tasks. This paper surveys recent progress on the integration of LLMs into OR, organizing methods into three main directions: automatic modeling, auxiliary optimization, and direct solving. It further reviews evaluation benchmarks and domain-specific applications, and summarizes key open issues such as unstable semantic-to-structure mapping, fragmented research progress, limited generalization, and insufficient evaluation systems. Finally, the survey outlines possible research avenues for advancing the role of LLMs in OR.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-18",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.987910"
    },
    {
        "index": "#44",
        "title": "Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM",
        "link": "/arxiv/2509.18178",
        "arxiv_id": "2509.18178",
        "authors": "Ling Yue, Nithin Somasekharan, Tingwen Zhang, Yadi Cao, Shaowu Pan",
        "summary": "Computational Fluid Dynamics (CFD) is an essential simulation tool in engineering, yet its steep learning curve and complex manual setup create significant barriers. To address these challenges, we introduce Foam-Agent, a multi-agent framework that automates the entire end-to-end OpenFOAM workflow from a single natural language prompt. Our key innovations address critical gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation: Foam-Agent is the first system to manage the full simulation pipeline, including advanced pre-processing with a versatile Meshing Agent capable of handling external mesh files and generating new geometries via Gmsh, automatic generation of HPC submission scripts, and post-simulation visualization via ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent, the framework uses Model Context Protocol (MCP) to expose its core functions as discrete, callable tools. This allows for flexible integration and use by other agentic systems, such as Claude-code, for more exploratory workflows. 3. High-Fidelity Configuration Generation: We achieve superior accuracy through a Hierarchical Multi-Index RAG for precise context retrieval and a dependency-aware generation process that ensures configuration consistency. Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2% success rate with Claude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the expertise barrier for CFD, demonstrating how specialized multi-agent systems can democratize complex scientific computing. The code is public at https://github.com/csml-rpi/Foam-Agent.",
        "subjects": "Artificial Intelligence, Computational Engineering, Finance, and Science, Machine Learning",
        "date": "2025-09-17",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.988154"
    },
    {
        "index": "#45",
        "title": "HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics",
        "link": "/arxiv/2509.18168",
        "arxiv_id": "2509.18168",
        "authors": "Dong Liu, Yanxuan Yu",
        "summary": "Semantic parsing of long documents remains challenging due to quadratic growth in pairwise composition and memory requirements. We introduce \\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that decomposes an input of length $N$ into $M$ meaningful segments, constructs \\emph{Local Semantic Graphs} on each segment, and extracts compact \\emph{summary nodes} to form a \\emph{Global Graph Memory}. HSGM supports \\emph{incremental updates} -- only newly arrived segments incur local graph construction and summary-node integration -- while \\emph{Hierarchical Query Processing} locates relevant segments via top-$K$ retrieval over summary nodes and then performs fine-grained reasoning within their local graphs. Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to $O\\!\\left(N\\,k + (N/k)^2\\right)$, with segment size $k \\ll N$, and we derive Frobenius-norm bounds on the approximation error introduced by node summarization and sparsification thresholds. Empirically, on three benchmarks -- long-document AMR parsing, segment-level semantic role labeling (OntoNotes), and legal event extraction -- HSGM achieves \\emph{2--4$\\times$ inference speedup}, \\emph{$>60\\%$ reduction} in peak memory, and \\emph{$\\ge 95\\%$} of baseline accuracy. Our approach unlocks scalable, accurate semantic modeling for ultra-long texts, enabling real-time and resource-constrained NLP applications.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-17",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.988396"
    },
    {
        "index": "#46",
        "title": "Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI",
        "link": "/arxiv/2509.18132",
        "arxiv_id": "2509.18132",
        "authors": "Xiuyi Fan",
        "summary": "Uncertainty is a fundamental challenge in medical practice, but current medical AI systems fail to explicitly quantify or communicate uncertainty in a way that aligns with clinical reasoning. Existing XAI works focus on interpreting model predictions but do not capture the confidence or reliability of these predictions. Conversely, uncertainty estimation (UE) techniques provide confidence measures but lack intuitive explanations. The disconnect between these two areas limits AI adoption in medicine. To address this gap, we propose Explainable Uncertainty Estimation (XUE) that integrates explainability with uncertainty quantification to enhance trust and usability in medical AI. We systematically map medical uncertainty to AI uncertainty concepts and identify key challenges in implementing XUE. We outline technical directions for advancing XUE, including multimodal uncertainty quantification, model-agnostic visualization techniques, and uncertainty-aware decision support systems. Lastly, we propose guiding principles to ensure effective XUE realisation. Our analysis highlights the need for AI systems that not only generate reliable predictions but also articulate confidence levels in a clinically meaningful way. This work contributes to the development of trustworthy medical AI by bridging explainability and uncertainty, paving the way for AI systems that are aligned with real-world clinical complexities.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-14",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.988570"
    },
    {
        "index": "#47",
        "title": "SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture",
        "link": "/arxiv/2509.18123",
        "arxiv_id": "2509.18123",
        "authors": "Yeonju Lee, Rui Qi Chen, Joseph Oboamah, Po Nien Su, Wei-zhen Liang, Yeyin Shi, Lu Gan, Yongsheng Chen, Xin Qiao, Jing Li",
        "summary": "Accurate interpretation of soil moisture patterns is critical for irrigation scheduling and crop management, yet existing approaches for soil moisture time-series analysis either rely on threshold-based rules or data-hungry machine learning or deep learning models that are limited in adaptability and interpretability. In this study, we introduce SPADE (Soil moisture Pattern and Anomaly DEtection), an integrated framework that leverages large language models (LLMs) to jointly detect irrigation patterns and anomalies in soil moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced reasoning and instruction-following capabilities, enabling zero-shot analysis without requiring task-specific annotation or fine-tuning. By converting time-series data into a textual representation and designing domain-informed prompt templates, SPADE identifies irrigation events, estimates net irrigation gains, detects, classifies anomalies, and produces structured, interpretable reports. Experiments were conducted on real-world soil moisture sensor data from commercial and experimental farms cultivating multiple crops across the United States. Results demonstrate that SPADE outperforms the existing method in anomaly detection, achieving higher recall and F1 scores and accurately classifying anomaly types. Furthermore, SPADE achieved high precision and recall in detecting irrigation events, indicating its strong capability to capture irrigation patterns accurately. SPADE's reports provide interpretability and usability of soil moisture analytics. This study highlights the potential of LLMs as scalable, adaptable tools for precision agriculture, which is capable of integrating qualitative knowledge and data-driven reasoning to produce actionable insights for accurate soil moisture monitoring and improved irrigation scheduling from soil moisture time-series data.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-10",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.988803"
    },
    {
        "index": "#48",
        "title": "A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services",
        "link": "/arxiv/2509.18101",
        "arxiv_id": "2509.18101",
        "authors": "Guanzhong Pan, Haibo Wang",
        "summary": "Large language models (LLMs) are becoming increasingly widespread. Organizations that want to use AI for productivity now face an important decision. They can subscribe to commercial LLM services or deploy models on their own infrastructure. Cloud services from providers such as OpenAI, Anthropic, and Google are attractive because they provide easy access to state-of-the-art models and are easy to scale. However, concerns about data privacy, the difficulty of switching service providers, and long-term operating costs have driven interest in local deployment of open-source models. This paper presents a cost-benefit analysis framework to help organizations determine when on-premise LLM deployment becomes economically viable compared to commercial subscription services. We consider the hardware requirements, operational expenses, and performance benchmarks of the latest open-source models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost of deploying these models locally with the major cloud providers subscription fee. Our findings provide an estimated breakeven point based on usage levels and performance needs. These results give organizations a practical framework for planning their LLM strategies.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-08-30",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.988976"
    },
    {
        "index": "#49",
        "title": "Audio-Based Pedestrian Detection in the Presence of Vehicular Noise",
        "link": "/arxiv/2509.19295",
        "arxiv_id": "2509.19295",
        "authors": "Yonghyun Kim, Chaeyeon Han, Akash Sarode, Noah Posner, Subhrajit Guhathakurta, Alexander Lerch",
        "summary": "Audio-based pedestrian detection is a challenging task and has, thus far, only been explored in noise-limited environments. We present a new dataset, results, and a detailed analysis of the state-of-the-art in audio-based pedestrian detection in the presence of vehicular noise. In our study, we conduct three analyses: (i) cross-dataset evaluation between noisy and noise-limited environments, (ii) an assessment of the impact of noisy data on model performance, highlighting the influence of acoustic context, and (iii) an evaluation of the model's predictive robustness on out-of-domain sounds. The new dataset is a comprehensive 1321-hour roadside dataset. It incorporates traffic-rich soundscapes. Each recording includes 16kHz audio synchronized with frame-level pedestrian annotations and 1fps video thumbnails.",
        "subjects": "Audio and Speech Processing, Artificial Intelligence, Machine Learning, Sound",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.989197"
    },
    {
        "index": "#50",
        "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",
        "link": "/arxiv/2509.19292",
        "arxiv_id": "2509.19292",
        "authors": "Yang Jin, Jun Lv, Han Xue, Wendi Chen, Chuan Wen, Cewu Lu",
        "summary": "Intelligent agents progress by continually refining their capabilities through actively exploring environments. Yet robot policies often lack sufficient exploration capability due to action mode collapse. Existing methods that encourage exploration typically rely on random perturbations, which are unsafe and induce unstable, erratic behaviors, thereby limiting their effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a framework that enhances policy exploration and improvement in robotic manipulation. SOE learns a compact latent representation of task-relevant factors and constrains exploration to the manifold of valid actions, ensuring safety, diversity, and effectiveness. It can be seamlessly integrated with arbitrary policy models as a plug-in module, augmenting exploration without degrading the base policy performance. Moreover, the structured latent space enables human-guided exploration, further improving efficiency and controllability. Extensive experiments in both simulation and real-world tasks demonstrate that SOE consistently outperforms prior methods, achieving higher task success rates, smoother and safer exploration, and superior sample efficiency. These results establish on-manifold exploration as a principled approach to sample-efficient policy self-improvement. Project website: https://ericjin2002.github.io/SOE",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.989413"
    },
    {
        "index": "#59",
        "title": "FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity",
        "link": "/arxiv/2509.19220",
        "arxiv_id": "2509.19220",
        "authors": "Ferdinand Kahenga, Antoine Bagula, Patrick Sello, Sajal K. Das",
        "summary": "Federated learning in practice must contend with heterogeneous feature spaces, severe non-IID data, and scarce labels across clients. We present FedFusion, a federated transfer-learning framework that unifies domain adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn, DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via confidence-filtered pseudo-labels and domain-adaptive transfer, while clients maintain personalised encoders tailored to local data. To preserve global coherence under heterogeneity, FedFusion employs similarity-weighted classifier coupling (with optional cluster-wise averaging), mitigating dominance by data-rich sites and improving minority-client performance. The frugal-labelling pipeline combines self-/semi-supervised pretext training with selective fine-tuning, reducing annotation demands without sharing raw data. Across tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes, FedFusion consistently outperforms state-of-the-art baselines in accuracy, robustness, and fairness while maintaining comparable communication and computation budgets. These results show that harmonising personalisation, domain adaptation, and label efficiency is an effective recipe for robust federated learning under real-world constraints.",
        "subjects": "Machine Learning, Artificial Intelligence, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.991514"
    },
    {
        "index": "#62",
        "title": "YAC: Bridging Natural Language and Interactive Visual Exploration with Generative AI for Biomedical Data Discovery",
        "link": "/arxiv/2509.19182",
        "arxiv_id": "2509.19182",
        "authors": "Devin Lange, Shanghua Gao, Pengwei Sui, Austen Money, Priya Misner, Marinka Zitnik, Nils Gehlenborg",
        "summary": "Incorporating natural language input has the potential to improve the capabilities of biomedical data discovery interfaces. However, user interface elements and visualizations are still powerful tools for interacting with data, even in the new world of generative AI. In our prototype system, YAC, Yet Another Chatbot, we bridge the gap between natural language and interactive visualizations by generating structured declarative output with a multi-agent system and interpreting that output to render linked interactive visualizations and apply data filters. Furthermore, we include widgets, which allow users to adjust the values of that structured output through user interface elements. We reflect on the capabilities and design of this system with an analysis of its technical dimensions and illustrate the capabilities through four usage scenarios.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.992174"
    },
    {
        "index": "#65",
        "title": "Generative Propaganda",
        "link": "/arxiv/2509.19147",
        "arxiv_id": "2509.19147",
        "authors": "Madeleine I. G. Daepp, Alejandro Cuevas, Robert Osazuwa Ness, Vickie Yu-Ping Wang, Bharat Kumar Nayak, Dibyendu Mishra, Ti-Chung Cheng, Shaily Desai, Joyojeet Pal",
        "summary": "Generative propaganda is the use of generative artificial intelligence (AI) to shape public opinion. To characterize its use in real-world settings, we conducted interviews with defenders (e.g., factcheckers, journalists, officials) in Taiwan and creators (e.g., influencers, political consultants, advertisers) as well as defenders in India, centering two places characterized by high levels of online propaganda. The term \"deepfakes\", we find, exerts outsized discursive power in shaping defenders' expectations of misuse and, in turn, the interventions that are prioritized. To better characterize the space of generative propaganda, we develop a taxonomy that distinguishes between obvious versus hidden and promotional versus derogatory use. Deception was neither the main driver nor the main impact vector of AI's use; instead, Indian creators sought to persuade rather than to deceive, often making AI's use obvious in order to reduce legal and reputational risks, while Taiwan's defenders saw deception as a subset of broader efforts to distort the prevalence of strategic narratives online. AI was useful and used, however, in producing efficiency gains in communicating across languages and modes, and in evading human and algorithmic detection. Security researchers should reconsider threat models to clearly differentiate deepfakes from promotional and obvious uses, to complement and bolster the social factors that constrain misuse by internal actors, and to counter efficiency gains globally.",
        "subjects": "Computers and Society, Artificial Intelligence, Social and Information Networks",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.992834"
    },
    {
        "index": "#67",
        "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language",
        "link": "/arxiv/2509.19136",
        "arxiv_id": "2509.19136",
        "authors": "S√©bastien Salva, Redha Taguelmimt",
        "summary": "The use of natural language (NL) test cases for validating graphical user interface (GUI) applications is emerging as a promising direction to manually written executable test scripts, which are costly to develop and difficult to maintain. Recent advances in large language models (LLMs) have opened the possibility of the direct execution of NL test cases by LLM agents. This paper investigates this direction, focusing on the impact on NL test case unsoundness and on test case execution consistency. NL test cases are inherently unsound, as they may yield false failures due to ambiguous instructions or unpredictable agent behaviour. Furthermore, repeated executions of the same NL test case may lead to inconsistent outcomes, undermining test reliability. To address these challenges, we propose an algorithm for executing NL test cases with guardrail mechanisms and specialised agents that dynamically verify the correct execution of each test step. We introduce measures to evaluate the capabilities of LLMs in test execution and one measure to quantify execution consistency. We propose a definition of weak unsoundness to characterise contexts in which NL test case execution remains acceptable, with respect to the industrial quality levels Six Sigma. Our experimental evaluation with eight publicly available LLMs, ranging from 3B to 70B parameters, demonstrates both the potential and current limitations of current LLM agents for GUI testing. Our experiments show that Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case execution with high execution consistency (above the level 3-sigma). We provide prototype tools, test suites, and results.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.993430"
    },
    {
        "index": "#68",
        "title": "GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding",
        "link": "/arxiv/2509.19135",
        "arxiv_id": "2509.19135",
        "authors": "Wenying Luo, Zhiyuan Lin, Wenhao Xu, Minghao Liu, Zhi Li",
        "summary": "Human mobility traces, often recorded as sequences of check-ins, provide a unique window into both short-term visiting patterns and persistent lifestyle regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal framework designed to advance mobility analysis by explicitly modeling the semantic and temporal complexity of human movement. The framework consists of four key innovations. First, a Spatio-Temporal Concept Encoder (STCE) integrates geographic location, POI category semantics, and periodic temporal rhythms into unified vector representations. Second, a Cognitive Trajectory Memory (CTM) adaptively filters historical visits, emphasizing recent and behaviorally salient events in order to capture user intent more effectively. Third, a Lifestyle Concept Bank (LCB) contributes structured human preference cues, such as activity types and lifestyle patterns, to enhance interpretability and personalization. Finally, task-oriented generative heads transform the learned representations into predictions for multiple downstream tasks. We conduct extensive experiments on four widely used real-world datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate performance on three benchmark tasks: next-location prediction, trajectory-user identification, and time estimation. The results demonstrate consistent and substantial improvements over strong baselines, confirming the effectiveness of GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond raw performance gains, our findings also suggest that generative modeling provides a promising foundation for building more robust, interpretable, and generalizable systems for human mobility intelligence.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.994040"
    },
    {
        "index": "#69",
        "title": "Analysis on distribution and clustering of weight",
        "link": "/arxiv/2509.19122",
        "arxiv_id": "2509.19122",
        "authors": "Chunming Ye, Wenquan Tian, Yalan Gao, Songzhou Li",
        "summary": "The study on architecture and parameter characteristics remains the hot topic in the research of large language models. In this paper we concern with the characteristics of weight which are used to analyze the correlations and differences between models. Two kinds of vectors-standard deviation vector and clustering vector-are proposed to describe features of models. In the first case, the weights are assumed to follow normal distribution. The standard deviation values of projection matrices are normalized to form Standard-Deviation Vector, representing the distribution characteristics of models. In the second case, the singular values from each weight projection matrix are extracted and grouped by K-Means algorithm. The grouped data with the same type matrix are combined as Clustering Vector to represent the correlation characteristics of models' weights. The study reveals that these two vectors can effectively distinguish between different models and clearly show the similarities among models of the same family. Moreover, after conducting LoRA fine-tuning with different datasets and models, it is found that the distribution of weights represented by standard deviation vector is directly influenced by the dataset, but the correlations between different weights represented by clustering vector remain unaffected and maintain a high consistency with the pre-trained model.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.994317"
    },
    {
        "index": "#70",
        "title": "FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI",
        "link": "/arxiv/2509.19120",
        "arxiv_id": "2509.19120",
        "authors": "Ferdinand Kahenga, Antoine Bagula, Sajal K. Das, Patrick Sello",
        "summary": "Federated Learning (FL) has emerged as a powerful paradigm for privacy-preserving model training, yet deployments in sensitive domains such as healthcare face persistent challenges from non-IID data, client unreliability, and adversarial manipulation. This paper introduces FedFiTS, a trust and fairness-aware selective FL framework that advances the FedFaSt line by combining fitness-based client election with slotted aggregation. FedFiTS implements a three-phase participation strategy-free-for-all training, natural selection, and slotted team participation-augmented with dynamic client scoring, adaptive thresholding, and cohort-based scheduling to balance convergence efficiency with robustness. A theoretical convergence analysis establishes bounds for both convex and non-convex objectives under standard assumptions, while a communication-complexity analysis shows reductions relative to FedAvg and other baselines. Experiments on diverse datasets-medical imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and resilience to poisoning attacks. By integrating trust-aware aggregation with fairness-oriented client selection, FedFiTS advances scalable and secure FL, making it well suited for real-world healthcare and cross-domain deployments.",
        "subjects": "Machine Learning, Artificial Intelligence, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.994535"
    },
    {
        "index": "#71",
        "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation",
        "link": "/arxiv/2509.19112",
        "arxiv_id": "2509.19112",
        "authors": "Hugo Math, Rainer Lienhart",
        "summary": "Understanding causality in event sequences where outcome labels such as diseases or system failures arise from preceding events like symptoms or error codes is critical. Yet remains an unsolved challenge across domains like healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label causal discovery method for sparse, high-dimensional event sequences comprising of thousands of unique event types. Using two pretrained causal Transformers as domain-specific foundation models for event sequences. CARGO infers in parallel, per sequence one-shot causal graphs and aggregates them using an adaptive frequency fusion to reconstruct the global Markov boundaries of labels. This two-stage approach enables efficient probabilistic reasoning at scale while bypassing the intractable cost of full-dataset conditional independence testing. Our results on a challenging real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels demonstrate CARGO's ability to perform structured reasoning.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.994784"
    },
    {
        "index": "#73",
        "title": "Algorithms for Adversarially Robust Deep Learning",
        "link": "/arxiv/2509.19100",
        "arxiv_id": "2509.19100",
        "authors": "Alexander Robey",
        "summary": "Given the widespread use of deep learning models in safety-critical applications, ensuring that the decisions of such models are robust against adversarial exploitation is of fundamental importance. In this thesis, we discuss recent progress toward designing algorithms that exhibit desirable robustness properties. First, we discuss the problem of adversarial examples in computer vision, for which we introduce new technical results, training paradigms, and certification algorithms. Next, we consider the problem of domain generalization, wherein the task is to train neural networks to generalize from a family of training distributions to unseen test distributions. We present new algorithms that achieve state-of-the-art generalization in medical imaging, molecular identification, and image classification. Finally, we study the setting of jailbreaking large language models (LLMs), wherein an adversarial user attempts to design prompts that elicit objectionable content from an LLM. We propose new attacks and defenses, which represent the frontier of progress toward designing robust language-based agents.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.995284"
    },
    {
        "index": "#75",
        "title": "Training Flow Matching Models with Reliable Labels via Self-Purification",
        "link": "/arxiv/2509.19091",
        "arxiv_id": "2509.19091",
        "authors": "Hyeongju Kim, Yechan Yu, June Young Yi, Juheon Lee",
        "summary": "Training datasets are inherently imperfect, often containing mislabeled samples due to human annotation errors, limitations of tagging models, and other sources of noise. Such label contamination can significantly degrade the performance of a trained model. In this work, we introduce Self-Purifying Flow Matching (SPFM), a principled approach to filtering unreliable data within the flow-matching framework. SPFM identifies suspicious data using the model itself during the training process, bypassing the need for pretrained models or additional modules. Our experiments demonstrate that models trained with SPFM generate samples that accurately adhere to the specified conditioning, even when trained on noisy labels. Furthermore, we validate the robustness of SPFM on the TITW dataset, which consists of in-the-wild speech data, achieving performance that surpasses existing baselines.",
        "subjects": "Audio and Speech Processing, Artificial Intelligence, Sound",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.995735"
    },
    {
        "index": "#77",
        "title": "A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement",
        "link": "/arxiv/2509.19088",
        "arxiv_id": "2509.19088",
        "authors": "Tiany Peng, George Gui, Daniel J. Merlau, Grace Jiarui Fan, Malek Ben Sliman, Melanie Brucks, Eric J. Johnson, Vicki Morwitz, Abdullah Althenayyan, Silvia Bellezza, Dante Donati, Hortense Fong, Elizabeth Friedman, Ariana Guevara, Mohamed Hussein, Kinshuk Jerath, Bruce Kogut, Kristen Lane, Hannah Li, Patryk Perkowski, Oded Netzer, Olivier Toubia",
        "summary": "Do \"digital twins\" capture individual responses in surveys and experiments? We run 19 pre-registered studies on a national U.S. panel and their LLM-powered digital twins (constructed based on previously-collected extensive individual-level data) and compare twin and human answers across 164 outcomes. The correlation between twin and human answers is modest (approximately 0.2 on average) and twin responses are less variable than human responses. While constructing digital twins based on rich individual-level data improves our ability to capture heterogeneity across participants and predict relative differences between them, it does not substantially improve our ability to predict the exact answers given by specific participants or enhance predictions of population means. Twin performance varies by domain and is higher among more educated, higher-income, and ideologically moderate participants. These results suggest current digital twins can capture some degree of relative differences but are unreliable for individual-level predictions and sample mean and variance estimation, underscoring the need for careful validation before use. Our data and code are publicly available for researchers and practitioners interested in optimizing digital twin pipelines.",
        "subjects": "Computers and Society, Artificial Intelligence, Human-Computer Interaction, Applications",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.996310"
    },
    {
        "index": "#78",
        "title": "Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying",
        "link": "/arxiv/2509.19084",
        "arxiv_id": "2509.19084",
        "authors": "Asela Hevapathige",
        "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success across various graph-based tasks. However, they face some fundamental limitations: feature oversmoothing can cause node representations to become indistinguishable in deeper networks, they struggle to effectively manage heterogeneous relationships where connected nodes differ significantly, and they process entire feature vectors as indivisible units, which limits flexibility. We seek to address these limitations. We propose AxelGNN, a novel GNN architecture inspired by Axelrod's cultural dissemination model that addresses these limitations through a unified framework. AxelGNN incorporates similarity-gated probabilistic interactions that adaptively promote convergence or divergence based on node similarity, implements trait-level copying mechanisms for fine-grained feature aggregation at the segment level, and maintains global polarization to preserve node distinctiveness across multiple representation clusters. The model's bistable convergence dynamics naturally handle both homophilic and heterophilic graphs within a single architecture. Extensive experiments on node classification and influence estimation benchmarks demonstrate that AxelGNN consistently outperforms or matches state-of-the-art GNN methods across diverse graph structures with varying homophily-heterophily characteristics.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.996491"
    },
    {
        "index": "#79",
        "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation",
        "link": "/arxiv/2509.19080",
        "arxiv_id": "2509.19080",
        "authors": "Zhennan Jiang, Kai Liu, Yuxin Qin, Shuai Tian, Yupeng Zheng, Mingcai Zhou, Chao Yu, Haoran Li, Dongbin Zhao",
        "summary": "Robotic manipulation policies are commonly initialized through imitation learning, but their performance is limited by the scarcity and narrow coverage of expert data. Reinforcement learning can refine polices to alleviate this limitation, yet real-robot training is costly and unsafe, while training in simulators suffers from the sim-to-real gap. Recent advances in generative models have demonstrated remarkable capabilities in real-world simulation, with diffusion models in particular excelling at generation. This raises the question of how diffusion model-based world models can be combined to enhance pre-trained policies in robotic manipulation. In this work, we propose World4RL, a framework that employs diffusion-based world models as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation. Unlike prior works that primarily employ world models for planning, our framework enables direct end-to-end policy optimization. World4RL is designed around two principles: pre-training a diffusion world model that captures diverse dynamics on multi-task datasets and refining policies entirely within a frozen world model to avoid online real-world interactions. We further design a two-hot action encoding scheme tailored for robotic manipulation and adopt diffusion backbones to improve modeling fidelity. Extensive simulation and real-world experiments demonstrate that World4RL provides high-fidelity environment modeling and enables consistent policy refinement, yielding significantly higher success rates compared to imitation learning and other baselines. More visualization results are available at https://world4rl.github.io/.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.996719"
    },
    {
        "index": "#80",
        "title": "Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training",
        "link": "/arxiv/2509.19063",
        "arxiv_id": "2509.19063",
        "authors": "Przemys≈Çaw Spyra",
        "summary": "The rising computational and energy demands of deep neural networks (DNNs), driven largely by backpropagation (BP), challenge sustainable AI development. This paper rigorously investigates three BP-free training methods: the Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF) algorithms, tracing their progression from foundational concepts to a demonstrably superior solution. A robust comparative framework was established: each algorithm was implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and benchmarked against an equivalent BP-trained model. Hyperparameters were optimized with Optuna, and consistent early stopping criteria were applied based on validation performance, ensuring all models were optimally tuned before comparison. Results show that MF not only competes with but consistently surpasses BP in classification accuracy on its native MLPs. Its superior generalization stems from converging to a more favorable minimum in the validation loss landscape, challenging the assumption that global optimization is required for state-of-the-art results. Measured at the hardware level using the NVIDIA Management Library (NVML) API, MF reduces energy consumption by up to 41% and shortens training time by up to 34%, translating to a measurably smaller carbon footprint as estimated by CodeCarbon. Beyond this primary result, we present a hardware-level analysis that explains the efficiency gains: exposing FF's architectural inefficiencies, validating MF's computationally lean design, and challenging the assumption that all BP-free methods are inherently more memory-efficient. By documenting the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and sustainability, this work offers a clear, data-driven roadmap for future energy-efficient deep learning.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.996894"
    },
    {
        "index": "#81",
        "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion",
        "link": "/arxiv/2509.19023",
        "arxiv_id": "2509.19023",
        "authors": "Shuai Liu, Meng Cheng Lau",
        "summary": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a two-stage reinforcement learning framework for humanoid walking that requires no motion capture data or elaborate reward shaping. In the first stage, a compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via Proximal Policy Optimization. This generates energy-efficient gait templates. In the second stage, those dynamically consistent trajectories guide a full-body policy trained with Soft Actor--Critic augmented by an adversarial discriminator, ensuring the student's five-dimensional gait feature distribution matches the ROM's demonstrations. Experiments at 1 meter-per-second and 4 meter-per-second show that ROM-GRL produces stable, symmetric gaits with substantially lower tracking error than a pure-reward baseline. By distilling lightweight ROM guidance into high-dimensional policies, ROM-GRL bridges the gap between reward-only and imitation-based locomotion methods, enabling versatile, naturalistic humanoid behaviors without any human demonstrations.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.997125"
    },
    {
        "index": "#82",
        "title": "Fully Learnable Neural Reward Machines",
        "link": "/arxiv/2509.19017",
        "arxiv_id": "2509.19017",
        "authors": "Hazem Dewidar, Elena Umili",
        "summary": "Non-Markovian Reinforcement Learning (RL) tasks present significant challenges, as agents must reason over entire trajectories of state-action pairs to make optimal decisions. A common strategy to address this is through symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which provide a structured way to express temporally extended objectives. However, these approaches often rely on restrictive assumptions -- such as the availability of a predefined Symbol Grounding (SG) function mapping raw observations to high-level symbolic representations, or prior knowledge of the temporal task. In this work, we propose a fully learnable version of Neural Reward Machines (NRM), which can learn both the SG function and the automaton end-to-end, removing any reliance on prior knowledge. Our approach is therefore as easily applicable as classic deep RL (DRL) approaches, while being far more explainable, because of the finite and compact nature of automata. Furthermore, we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL, our method outperforms previous approaches based on Recurrent Neural Networks (RNNs).",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.997322"
    },
    {
        "index": "#83",
        "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
        "link": "/arxiv/2509.19012",
        "arxiv_id": "2509.19012",
        "authors": "Dapeng Zhang, Jin Sun, Chenghui Hu, Xiaoyan Wu, Zhenlong Yuan, Rui Zhou, Fei Shen, Qingguo Zhou",
        "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.997545"
    },
    {
        "index": "#85",
        "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
        "link": "/arxiv/2509.18953",
        "arxiv_id": "2509.18953",
        "authors": "Hanqing Liu, Jiahuan Long, Junqi Wu, Jiacheng Hou, Huili Tang, Tingsong Jiang, Weien Zhou, Wen Yao",
        "summary": "Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.998045"
    },
    {
        "index": "#86",
        "title": "Towards Privacy-Aware Bayesian Networks: A Credal Approach",
        "link": "/arxiv/2509.18949",
        "arxiv_id": "2509.18949",
        "authors": "Niccol√≤ Rocchi, Fabio Stella, Cassio de Campos",
        "summary": "Bayesian networks (BN) are probabilistic graphical models that enable efficient knowledge representation and inference. These have proven effective across diverse domains, including healthcare, bioinformatics and economics. The structure and parameters of a BN can be obtained by domain experts or directly learned from available data. However, as privacy concerns escalate, it becomes increasingly critical for publicly released models to safeguard sensitive information in training data. Typically, released models do not prioritize privacy by design. In particular, tracing attacks from adversaries can combine the released BN with auxiliary data to determine whether specific individuals belong to the data from which the BN was learned. State-of-the-art protection tecniques involve introducing noise into the learned parameters. While this offers robust protection against tracing attacks, it significantly impacts the model's utility, in terms of both the significance and accuracy of the resulting inferences. Hence, high privacy may be attained at the cost of releasing a possibly ineffective model. This paper introduces credal networks (CN) as a novel solution for balancing the model's privacy and utility. After adapting the notion of tracing attacks, we demonstrate that a CN enables the masking of the learned BN, thereby reducing the probability of successful attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve meaningful inferences while safeguarding privacy. Moreover, we identify key learning information that must be concealed to prevent attackers from recovering the underlying BN. Finally, we conduct a set of numerical experiments to analyze how privacy gains can be modulated by tuning the CN hyperparameters. Our results confirm that CNs provide a principled, practical, and effective approach towards the development of privacy-aware probabilistic graphical models.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.998233"
    },
    {
        "index": "#88",
        "title": "Accurate and Efficient Prediction of Wi-Fi Link Quality Based on Machine Learning",
        "link": "/arxiv/2509.18933",
        "arxiv_id": "2509.18933",
        "authors": "Gabriele Formis, Gianluca Cena, Lukasz Wisniewski, Stefano Scanzio",
        "summary": "Wireless communications are characterized by their unpredictability, posing challenges for maintaining consistent communication quality. This paper presents a comprehensive analysis of various prediction models, with a focus on achieving accurate and efficient Wi-Fi link quality forecasts using machine learning techniques. Specifically, the paper evaluates the performance of data-driven models based on the linear combination of exponential moving averages, which are designed for low-complexity implementations and are then suitable for hardware platforms with limited processing resources. Accuracy of the proposed approaches was assessed using experimental data from a real-world Wi-Fi testbed, considering both channel-dependent and channel-independent training data. Remarkably, channel-independent models, which allow for generalized training by equipment manufacturers, demonstrated competitive performance. Overall, this study provides insights into the practical deployment of machine learning-based prediction models for enhancing Wi-Fi dependability in industrial environments.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.998625"
    },
    {
        "index": "#89",
        "title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning",
        "link": "/arxiv/2509.18930",
        "arxiv_id": "2509.18930",
        "authors": "Alex Schutz, Victor-Alexandru Darvariu, Efimia Panagiotaki, Bruno Lacerda, Nick Hawes",
        "summary": "Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks to execute classic algorithms by supervised learning. Despite its successes, important limitations remain: inability to construct valid solutions without post-processing and to reason about multiple correct ones, poor performance on combinatorial NP-hard problems, and inapplicability to problems for which strong algorithms are not yet known. To address these limitations, we reframe the problem of learning algorithm trajectories as a Markov Decision Process, which imposes structure on the solution construction procedure and unlocks the powerful tools of imitation and reinforcement learning (RL). We propose the GNARL framework, encompassing the methodology to translate problem formulations from NAR to RL and a learning architecture suitable for a wide range of graph-based problems. We achieve very high graph accuracy results on several CLRS-30 problems, performance matching or exceeding much narrower NAR approaches for NP-hard problems and, remarkably, applicability even when lacking an expert algorithm.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.998822"
    },
    {
        "index": "#91",
        "title": "The AI Literacy Heptagon: A Structured Approach to AI Literacy in Higher Education",
        "link": "/arxiv/2509.18900",
        "arxiv_id": "2509.18900",
        "authors": "Veronika Hackl, Alexandra Mueller, Maximilian Sailer",
        "summary": "The integrative literature review addresses the conceptualization and implementation of AI Literacy (AIL) in Higher Education (HE) by examining recent research literature. Through an analysis of publications (2021-2024), we explore (1) how AIL is defined and conceptualized in current research, particularly in HE, and how it can be delineated from related concepts such as Data Literacy, Media Literacy, and Computational Literacy; (2) how various definitions can be synthesized into a comprehensive working definition, and (3) how scientific insights can be effectively translated into educational practice. Our analysis identifies seven central dimensions of AIL: technical, applicational, critical thinking, ethical, social, integrational, and legal. These are synthesized in the AI Literacy Heptagon, deepening conceptual understanding and supporting the structured development of AIL in HE. The study aims to bridge the gap between theoretical AIL conceptualizations and the practical implementation in academic curricula.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.999261"
    },
    {
        "index": "#93",
        "title": "When Ads Become Profiles: Large-Scale Audit of Algorithmic Biases and LLM Profiling Risks",
        "link": "/arxiv/2509.18874",
        "arxiv_id": "2509.18874",
        "authors": "Baiyu Chen, Benjamin Tag, Hao Xue, Daniel Angus, Flora Salim",
        "summary": "Automated ad targeting on social media is opaque, creating risks of exploitation and invisibility to external scrutiny. Users may be steered toward harmful content while independent auditing of these processes remains blocked. Large Language Models (LLMs) raise a new concern: the potential to reverse-engineer sensitive user attributes from exposure alone. We introduce a multi-stage auditing framework to investigate these risks. First, a large-scale audit of over 435,000 ad impressions delivered to 891 Australian Facebook users reveals algorithmic biases, including disproportionate Gambling and Politics ads shown to socioeconomically vulnerable and politically aligned groups. Second, a multimodal LLM can reconstruct users' demographic profiles from ad streams, outperforming census-based baselines and matching or exceeding human performance. Our results provide the first empirical evidence that ad streams constitute rich digital footprints for public AI inference, highlighting urgent privacy risks and the need for content-level auditing and governance.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Computers and Society",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.999660"
    },
    {
        "index": "#94",
        "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization",
        "link": "/arxiv/2509.18851",
        "arxiv_id": "2509.18851",
        "authors": "Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu, Dexun Wang, Chunmei Xie, Weiqi Xiong, Xianzhou Zeng, Qixuan Zhou, Yadong Li, Xingzhong Xu",
        "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs) across various tasks. However, GRPO, a representative RLVR algorithm, suffers from a critical limitation: when all responses within a group are either entirely correct or entirely incorrect, the model fails to learn from these homogeneous responses. This is particularly problematic for homogeneously incorrect groups, where GRPO's advantage function yields a value of zero, leading to null gradients and the loss of valuable learning signals. To overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy Optimization), an algorithm designed to convert homogeneous errors into robust learning signals. First, NGRPO introduces Advantage Calibration. This mechanism hypothesizes the existence of a virtual maximum-reward sample during advantage calculation, thereby altering the mean and variance of rewards within a group and ensuring that the advantages for homogeneously incorrect samples are no longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the update magnitude for positive samples while imposing stricter constraints on that of negative samples. This serves to stabilize the exploration pressure introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO, DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and AIME2025. These results validate NGRPO's ability to learn from homogeneous errors, leading to stable and substantial improvements in mathematical reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:34.999893"
    },
    {
        "index": "#98",
        "title": "Detection of security smells in IaC scripts through semantics-aware code and language processing",
        "link": "/arxiv/2509.18790",
        "arxiv_id": "2509.18790",
        "authors": "Aicha War, Adnan A. Rawass, Abdoul K. Kabore, Jordan Samhi, Jacques Klein, Tegawende F. Bissyande",
        "summary": "Infrastructure as Code (IaC) automates the provisioning and management of IT infrastructure through scripts and tools, streamlining software deployment. Prior studies have shown that IaC scripts often contain recurring security misconfigurations, and several detection and mitigation approaches have been proposed. Most of these rely on static analysis, using statistical code representations or Machine Learning (ML) classifiers to distinguish insecure configurations from safe code. In this work, we introduce a novel approach that enhances static analysis with semantic understanding by jointly leveraging natural language and code representations. Our method builds on two complementary ML models: CodeBERT, to capture semantics across code and text, and LongFormer, to represent long IaC scripts without losing contextual information. We evaluate our approach on misconfiguration datasets from two widely used IaC tools, Ansible and Puppet. To validate its effectiveness, we conduct two ablation studies (removing code text from the natural language input and truncating scripts to reduce context) and compare against four large language models (LLMs) and prior work. Results show that semantic enrichment substantially improves detection, raising precision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from 0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Machine Learning, Software Engineering",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.000754"
    },
    {
        "index": "#99",
        "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models",
        "link": "/arxiv/2509.18778",
        "arxiv_id": "2509.18778",
        "authors": "Shijia Ge, Yinxin Zhang, Shuzhao Xie, Weixiang Zhang, Mingcai Zhou, Zhi Wang",
        "summary": "Visual imitation learning frameworks allow robots to learn manipulation skills from expert demonstrations. While existing approaches mainly focus on policy design, they often neglect the structure and capacity of visual encoders, limiting spatial understanding and generalization. Inspired by biological vision systems, which rely on both visual and proprioceptive cues for robust control, we propose VGGT-DP, a visuomotor policy framework that integrates geometric priors from a pretrained 3D perception model with proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer (VGGT) as the visual encoder and introduce a proprioception-guided visual learning strategy to align perception with internal robot states, improving spatial grounding and closed-loop control. To reduce inference latency, we design a frame-wise token reuse mechanism that compacts multi-view tokens into an efficient spatial representation. We further apply random token pruning to enhance policy robustness and reduce overfitting. Experiments on challenging MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines such as DP and DP3, particularly in precision-critical and long-horizon scenarios.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.000950"
    },
    {
        "index": "#104",
        "title": "Security smells in infrastructure as code: a taxonomy update beyond the seven sins",
        "link": "/arxiv/2509.18761",
        "arxiv_id": "2509.18761",
        "authors": "Aicha War, Serge L. B. Nikiema, Jordan Samhi, Jacques Klein, Tegawende F. Bissyande",
        "summary": "Infrastructure as Code (IaC) has become essential for modern software management, yet security flaws in IaC scripts can have severe consequences, as exemplified by the recurring exploits of Cloud Web Services. Prior work has recognized the need to build a precise taxonomy of security smells in IaC scripts as a first step towards developing approaches to improve IaC security. This first effort led to the unveiling of seven sins, limited by the focus on a single IaC tool as well as by the extensive, and potentially biased, manual effort that was required. We propose, in our work, to revisit this taxonomy: first, we extend the study of IaC security smells to a more diverse dataset with scripts associated with seven popular IaC tools, including Terraform, Ansible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some automation for the analysis by relying on an LLM. While we leverage LLMs for initial pattern processing, all taxonomic decisions underwent systematic human validation and reconciliation with established security standards. Our study yields a comprehensive taxonomy of 62 security smell categories, significantly expanding beyond the previously known seven. We demonstrate actionability by implementing new security checking rules within linters for seven popular IaC tools, often achieving 1.00 precision score. Our evolution study of security smells in GitHub projects reveals that these issues persist for extended periods, likely due to inadequate detection and mitigation tools. This work provides IaC practitioners with insights for addressing common security smells and systematically adopting DevSecOps practices to build safer infrastructure code.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Machine Learning, Software Engineering",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.002114"
    },
    {
        "index": "#105",
        "title": "Complexity of Activity Patterns in a Bio-Inspired Hopfield-Type Network in Different Topologies",
        "link": "/arxiv/2509.18758",
        "arxiv_id": "2509.18758",
        "authors": "Marco Cafiso, Paolo Paradisi",
        "summary": "Neural network models capable of storing memory have been extensively studied in computer science and computational neuroscience. The Hopfield network is a prototypical example of a model designed for associative, or content-addressable, memory and has been analyzed in many forms. Further, ideas and methods from complex network theory have been incorporated into artificial neural networks and learning, emphasizing their structural properties. Nevertheless, the temporal dynamics also play a vital role in biological neural networks, whose temporal structure is a crucial feature to examine. Biological neural networks display complex intermittency and, thus, can be studied through the lens of the temporal complexity (TC) theory. The TC approach look at the metastability of self-organized states, characterized by a power-law decay in the inter-event time distribution and in the total activity distribution or a scaling behavior in the corresponding event-driven diffusion processes. In this study, we present a temporal complexity (TC) analysis of a biologically-inspired Hopfield-type neural network model. We conducted a comparative assessment between scale-free and random network topologies, with particular emphasis on their global activation patterns. Our parametric analysis revealed comparable dynamical behaviors across both neural network architectures. Furthermore, our investigation into temporal complexity characteristics uncovered that seemingly distinct dynamical patterns exhibit similar temporal complexity behaviors. In particular, similar power-law decay in the activity distribution and similar complexity levels are observed in both topologies, but with a much reduced noise in the scale-free topology. Notably, most of the complex dynamical profiles were consistently observed in scale-free network configurations, thus confirming the crucial role of hubs in neural network dynamics.",
        "subjects": "Neurons and Cognition, Artificial Intelligence, Adaptation and Self-Organizing Systems, Biological Physics",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.002311"
    },
    {
        "index": "#106",
        "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning",
        "link": "/arxiv/2509.18757",
        "arxiv_id": "2509.18757",
        "authors": "Omar Rayyan, John Abanes, Mahmoud Hafez, Anthony Tzes, Fares Abu-Dakka",
        "summary": "Recent advances in imitation learning have shown great promise for developing robust robot manipulation policies from demonstrations. However, this promise is contingent on the availability of diverse, high-quality datasets, which are not only challenging and costly to collect but are often constrained to a specific robot embodiment. Portable handheld grippers have recently emerged as intuitive and scalable alternatives to traditional robotic teleoperation methods for data collection. However, their reliance solely on first-person view wrist-mounted cameras often creates limitations in capturing sufficient scene contexts. In this paper, we present MV-UMI (Multi-View Universal Manipulation Interface), a framework that integrates a third-person perspective with the egocentric camera to overcome this limitation. This integration mitigates domain shifts between human demonstration and robot deployment, preserving the cross-embodiment advantages of handheld data-collection devices. Our experimental results, including an ablation study, demonstrate that our MV-UMI framework improves performance in sub-tasks requiring broad scene understanding by approximately 47% across 3 tasks, confirming the effectiveness of our approach in expanding the range of feasible manipulation tasks that can be learned using handheld gripper systems, without compromising the cross-embodiment advantages inherent to such systems.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.002515"
    },
    {
        "index": "#108",
        "title": "A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications",
        "link": "/arxiv/2509.18714",
        "arxiv_id": "2509.18714",
        "authors": "Zhenyu Tao, Wei Xu, Xiaohu You",
        "summary": "The bisimulation metric (BSM) is a powerful tool for computing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to multiple-MDP scenarios, such as policy transfer, remains challenging. Prior work has attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis of its mathematical properties has limited further theoretical progress. In this work, we formally establish a generalized bisimulation metric (GBSM) between pairs of MDPs, which is rigorously proven with the three fundamental properties: GBSM symmetry, inter-MDP triangle inequality, and the distance bound on identical state spaces. Leveraging these properties, we theoretically analyse policy transfer, state aggregation, and sampling-based estimation in MDPs, obtaining explicit bounds that are strictly tighter than those derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.002974"
    },
    {
        "index": "#111",
        "title": "An overview of neural architectures for self-supervised audio representation learning from masked spectrograms",
        "link": "/arxiv/2509.18691",
        "arxiv_id": "2509.18691",
        "authors": "Sarthak Yadav, Sergios Theodoridis, Zheng-Hua Tan",
        "summary": "In recent years, self-supervised learning has amassed significant interest for training deep neural representations without labeled data. One such self-supervised learning approach is masked spectrogram modeling, where the objective is to learn semantically rich contextual representations by predicting removed or hidden portions of the input audio spectrogram. With the Transformer neural architecture at its core, masked spectrogram modeling has emerged as the prominent approach for learning general purpose audio representations, a.k.a. audio foundation models. Meanwhile, addressing the issues of the Transformer architecture, in particular the underlying Scaled Dot-product Attention operation, which scales quadratically with input sequence length, has led to renewed interest in recurrent sequence modeling approaches. Among them, Selective structured state space models (such as Mamba) and extended Long Short-Term Memory (xLSTM) are the two most promising approaches which have experienced widespread adoption. While the body of work on these two topics continues to grow, there is currently a lack of an adequate overview encompassing the intersection of these topics. In this paper, we present a comprehensive overview of the aforementioned research domains, covering masked spectrogram modeling and the previously mentioned neural sequence modeling architectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and xLSTM based masked spectrogram models in a unified, reproducible framework on ten diverse downstream audio classification tasks, which will help interested readers to make informed decisions regarding suitability of the evaluated approaches to adjacent applications.",
        "subjects": "Sound, Artificial Intelligence, Audio and Speech Processing",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.003767"
    },
    {
        "index": "#113",
        "title": "NaviSense: A Multimodal Assistive Mobile application for Object Retrieval by Persons with Visual Impairment",
        "link": "/arxiv/2509.18672",
        "arxiv_id": "2509.18672",
        "authors": "Ajay Narayanan Sridhar, Fuli Qiao, Nelson Daniel Troncoso Aldas, Yanpei Shi, Mehrdad Mahdavi, Laurent Itti, Vijaykrishnan Narayanan",
        "summary": "People with visual impairments often face significant challenges in locating and retrieving objects in their surroundings. Existing assistive technologies present a trade-off: systems that offer precise guidance typically require pre-scanning or support only fixed object categories, while those with open-world object recognition lack spatial feedback for reaching the object. To address this gap, we introduce 'NaviSense', a mobile assistive system that combines conversational AI, vision-language models, augmented reality (AR), and LiDAR to support open-world object detection with real-time audio-haptic guidance. Users specify objects via natural language and receive continuous spatial feedback to navigate toward the target without needing prior setup. Designed with insights from a formative study and evaluated with 12 blind and low-vision participants, NaviSense significantly reduced object retrieval time and was preferred over existing tools, demonstrating the value of integrating open-world perception with precise, accessible guidance.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.004193"
    },
    {
        "index": "#114",
        "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer",
        "link": "/arxiv/2509.18648",
        "arxiv_id": "2509.18648",
        "authors": "Yarden As, Chengrui Qu, Benjamin Unger, Dongho Kang, Max van der Hart, Laixi Shi, Stelian Coros, Adam Wierman, Andreas Krause",
        "summary": "Safety remains a major concern for deploying reinforcement learning (RL) in real-world applications. Simulators provide safe, scalable training environments, but the inevitable sim-to-real gap introduces additional safety concerns, as policies must satisfy constraints in real-world conditions that differ from simulation. To address this challenge, robust safe RL techniques offer principled methods, but are often incompatible with standard scalable training pipelines. In contrast, domain randomization, a simple and popular sim-to-real technique, stands out as a promising alternative, although it often results in unsafe behaviors in practice. We present SPiDR, short for Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with provable guarantees for safe sim-to-real transfer. SPiDR uses domain randomization to incorporate the uncertainty about the sim-to-real gap into the safety constraints, making it versatile and highly compatible with existing training pipelines. Through extensive experiments on sim-to-sim benchmarks and two distinct real-world robotic platforms, we demonstrate that SPiDR effectively ensures safety despite the sim-to-real gap while maintaining strong performance.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.004418"
    },
    {
        "index": "#115",
        "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
        "link": "/arxiv/2509.18644",
        "arxiv_id": "2509.18644",
        "authors": "Juntu Zhao, Wenbo Lu, Di Zhang, Yufeng Liu, Yushen Liang, Tianluo Zhang, Yifeng Cao, Junyuan Xie, Yingdong Hu, Shengjie Wang, Junliang Guo, Dequan Wang, Yang Gao",
        "summary": "Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0\\% to 85\\% in height generalization and from 6\\% to 64\\% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.004662"
    },
    {
        "index": "#117",
        "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training",
        "link": "/arxiv/2509.18631",
        "arxiv_id": "2509.18631",
        "authors": "Shuo Cheng, Liqian Ma, Zhenyang Chen, Ajay Mandlekar, Caelan Garrett, Danfei Xu",
        "summary": "Behavior cloning has shown promise for robot manipulation, but real-world demonstrations are costly to acquire at scale. While simulated data offers a scalable alternative, particularly with advances in automated demonstration generation, transferring policies to the real world is hampered by various simulation and real domain gaps. In this work, we propose a unified sim-and-real co-training framework for learning generalizable manipulation policies that primarily leverages simulation and only requires a few real-world demonstrations. Central to our approach is learning a domain-invariant, task-relevant feature space. Our key insight is that aligning the joint distributions of observations and their corresponding actions across domains provides a richer signal than aligning observations (marginals) alone. We achieve this by embedding an Optimal Transport (OT)-inspired loss within the co-training framework, and extend this to an Unbalanced OT framework to handle the imbalance between abundant simulation data and limited real-world examples. We validate our method on challenging manipulation tasks, showing it can leverage abundant simulation data to achieve up to a 30% improvement in the real-world success rate and even generalize to scenarios seen only in simulation.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.005210"
    },
    {
        "index": "#118",
        "title": "HyperAdapt: Simple High-Rank Adaptation",
        "link": "/arxiv/2509.18629",
        "arxiv_id": "2509.18629",
        "authors": "Abel Gurung, Joseph Campbell",
        "summary": "Foundation models excel across diverse tasks, but adapting them to specialized applications often requires fine-tuning, an approach that is memory and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate this by updating only a small subset of weights. In this paper, we introduce HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces the number of trainable parameters compared to state-of-the-art methods like LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying row- and column-wise scaling through diagonal matrices, thereby inducing a high-rank update while requiring only $n+m$ trainable parameters for an $n \\times m$ matrix. Theoretically, we establish an upper bound on the rank of HyperAdapt's updates, and empirically, we confirm that it consistently induces high-rank transformations across model layers. Experiments on GLUE, arithmetic reasoning, and commonsense reasoning benchmarks with models up to 14B parameters demonstrate that HyperAdapt matches or nearly matches the performance of full fine-tuning and state-of-the-art PEFT methods while using orders of magnitude fewer trainable parameters.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.005398"
    },
    {
        "index": "#119",
        "title": "BRAID: Input-Driven Nonlinear Dynamical Modeling of Neural-Behavioral Data",
        "link": "/arxiv/2509.18627",
        "arxiv_id": "2509.18627",
        "authors": "Parsa Vahidi, Omid G. Sani, Maryam M. Shanechi",
        "summary": "Neural populations exhibit complex recurrent structures that drive behavior, while continuously receiving and integrating external inputs from sensory stimuli, upstream regions, and neurostimulation. However, neural populations are often modeled as autonomous dynamical systems, with little consideration given to the influence of external inputs that shape the population activity and behavioral outcomes. Here, we introduce BRAID, a deep learning framework that models nonlinear neural dynamics underlying behavior while explicitly incorporating any measured external inputs. Our method disentangles intrinsic recurrent neural population dynamics from the effects of inputs by including a forecasting objective within input-driven recurrent neural networks. BRAID further prioritizes the learning of intrinsic dynamics that are related to a behavior of interest by using a multi-stage optimization scheme. We validate BRAID with nonlinear simulations, showing that it can accurately learn the intrinsic dynamics shared between neural and behavioral modalities. We then apply BRAID to motor cortical activity recorded during a motor task and demonstrate that our method more accurately fits the neural-behavioral data by incorporating measured sensory stimuli into the model and improves the forecasting of neural-behavioral data compared with various baseline methods, whether input-driven or not.",
        "subjects": "Neurons and Cognition, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.005594"
    },
    {
        "index": "#120",
        "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving",
        "link": "/arxiv/2509.18626",
        "arxiv_id": "2509.18626",
        "authors": "Jay Patrikar, Apoorva Sharma, Sushant Veer, Boyi Li, Sebastian Scherer, Marco Pavone",
        "summary": "Learning-based autonomous driving systems are trained mostly on incident-free data, offering little guidance near safety-performance boundaries. Real crash reports contain precisely the contrastive evidence needed, but they are hard to use: narratives are unstructured, third-person, and poorly grounded to sensor views. We address these challenges by normalizing crash narratives to ego-centric language and converting both logs and crashes into a unified scene-action representation suitable for retrieval. At decision time, our system adjudicates proposed actions by retrieving relevant precedents from this unified index; an agentic counterfactual extension proposes plausible alternatives, retrieves for each, and reasons across outcomes before deciding. On a nuScenes benchmark, precedent retrieval substantially improves calibration, with recall on contextually preferred actions rising from 24% to 53%. The counterfactual variant preserves these gains while sharpening decisions near risk.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.005795"
    },
    {
        "index": "#121",
        "title": "Flow marching for a generative PDE foundation model",
        "link": "/arxiv/2509.18611",
        "arxiv_id": "2509.18611",
        "authors": "Zituo Chen, Sili Deng",
        "summary": "Pretraining on large-scale collections of PDE-governed spatiotemporal trajectories has recently shown promise for building generalizable models of dynamical systems. Yet most existing PDE foundation models rely on deterministic Transformer architectures, which lack generative flexibility for many science and engineering applications. We propose Flow Marching, an algorithm that bridges neural operator learning with flow matching motivated by an analysis of error accumulation in physical dynamical systems, and we build a generative PDE foundation model on top of it. By jointly sampling the noise level and the physical time step between adjacent states, the model learns a unified velocity field that transports a noisy current state toward its clean successor, reducing long-term rollout drift while enabling uncertainty-aware ensemble generations. Alongside this core algorithm, we introduce a Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states into a compact latent space, and an efficient Flow Marching Transformer (FMT) that combines a diffusion-forcing scheme with latent temporal pyramids, achieving up to 15x greater computational efficiency than full-length video diffusion models and thereby enabling large-scale pretraining at substantially reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE families and train suites of P2VAEs and FMTs at multiple scales. On downstream evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot adaptation, demonstrate long-term rollout stability over deterministic counterparts, and present uncertainty-stratified ensemble results, highlighting the importance of generative PDE foundation models for real-world applications.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.005970"
    },
    {
        "index": "#122",
        "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning",
        "link": "/arxiv/2509.18608",
        "arxiv_id": "2509.18608",
        "authors": "Ana Luiza Mineiro, Francisco Affonso, Marcelo Becker",
        "summary": "Reliable navigation in under-canopy agricultural environments remains a challenge due to GNSS unreliability, cluttered rows, and variable lighting. To address these limitations, we present an end-to-end learning-based navigation system that maps raw 3D LiDAR data directly to control commands using a deep reinforcement learning policy trained entirely in simulation. Our method includes a voxel-based downsampling strategy that reduces LiDAR input size by 95.83%, enabling efficient policy learning without relying on labeled datasets or manually designed control interfaces. The policy was validated in simulation, achieving a 100% success rate in straight-row plantations and showing a gradual decline in performance as row curvature increased, tested across varying sinusoidal frequencies and amplitudes.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.006152"
    },
    {
        "index": "#123",
        "title": "FlexSED: Towards Open-Vocabulary Sound Event Detection",
        "link": "/arxiv/2509.18606",
        "arxiv_id": "2509.18606",
        "authors": "Jiarui Hai, Helin Wang, Weizhe Guo, Mounya Elhilali",
        "summary": "Despite recent progress in large-scale sound event detection (SED) systems capable of handling hundreds of sound classes, existing multi-class classification frameworks remain fundamentally limited. They cannot process free-text sound queries, which enable more flexible and user-friendly interaction, and they lack zero-shot capabilities and offer poor few-shot adaptability. Although text-query-based separation methods have been explored, they primarily focus on source separation and are ill-suited for SED tasks that require precise temporal localization and efficient detection across large and diverse sound vocabularies. In this paper, we propose FlexSED, an open-vocabulary sound event detection system. FlexSED builds on a pretrained audio SSL model and the CLAP text encoder, introducing an encoder-decoder composition and an adaptive fusion strategy to enable effective continuous training from pretrained weights. To ensure robust supervision, it also employs large language models (LLMs) to assist in event query selection during training, addressing challenges related to missing labels. As a result, FlexSED achieves superior performance compared to vanilla SED models on AudioSet-Strong, while demonstrating strong zero-shot and few-shot capabilities. We release the code and pretrained models to support future research and applications based on FlexSED.",
        "subjects": "Audio and Speech Processing, Artificial Intelligence, Sound",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.006347"
    },
    {
        "index": "#124",
        "title": "SynSonic: Augmenting Sound Event Detection through Text-to-Audio Diffusion ControlNet and Effective Sample Filtering",
        "link": "/arxiv/2509.18603",
        "arxiv_id": "2509.18603",
        "authors": "Jiarui Hai, Mounya Elhilali",
        "summary": "Data synthesis and augmentation are essential for Sound Event Detection (SED) due to the scarcity of temporally labeled data. While augmentation methods like SpecAugment and Mix-up can enhance model performance, they remain constrained by the diversity of existing samples. Recent generative models offer new opportunities, yet their direct application to SED is challenging due to the lack of precise temporal annotations and the risk of introducing noise through unreliable filtering. To address these challenges and enable generative-based augmentation for SED, we propose SynSonic, a data augmentation method tailored for this task. SynSonic leverages text-to-audio diffusion models guided by an energy-envelope ControlNet to generate temporally coherent sound events. A joint score filtering strategy with dual classifiers ensures sample quality, and we explore its practical integration into training pipelines. Experimental results show that SynSonic improves Polyphonic Sound Detection Scores (PSDS1 and PSDS2), enhancing both temporal localization and sound class discrimination.",
        "subjects": "Audio and Speech Processing, Artificial Intelligence, Sound",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.006527"
    },
    {
        "index": "#128",
        "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA",
        "link": "/arxiv/2509.18576",
        "arxiv_id": "2509.18576",
        "authors": "Zeyi Kang, Liang He, Yanxin Zhang, Zuheng Ming, Kaixing Zhao",
        "summary": "Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.007442"
    },
    {
        "index": "#129",
        "title": "The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking",
        "link": "/arxiv/2509.18575",
        "arxiv_id": "2509.18575",
        "authors": "Yaoyao Qian, Yifan Zeng, Yuchao Jiang, Chelsi Jain, Huazheng Wang",
        "summary": "Large Language Models (LLMs) have demonstrated strong performance in information retrieval tasks like passage ranking. Our research examines how instruction-following capabilities in LLMs interact with multi-document comparison tasks, identifying what we term the \"Ranking Blind Spot\", a characteristic of LLM decision processes during comparative evaluation. We analyze how this ranking blind spot affects LLM evaluation systems through two approaches: Decision Objective Hijacking, which alters the evaluation goal in pairwise ranking systems, and Decision Criteria Hijacking, which modifies relevance standards across ranking schemes. These approaches demonstrate how content providers could potentially influence LLM-based ranking systems to affect document positioning. These attacks aim to force the LLM ranker to prefer a specific passage and rank it at the top. Malicious content providers can exploit this weakness, which helps them gain additional exposure by attacking the ranker. In our experiment, We empirically show that the proposed attacks are effective in various LLMs and can be generalized to multiple ranking schemes. We apply these attack to realistic examples to show their effectiveness. We also found stronger LLMs are more vulnerable to these attacks. Our code is available at: https://github.com/blindspotorg/RankingBlindSpot",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.007640"
    },
    {
        "index": "#130",
        "title": "Interaction Topological Transformer for Multiscale Learning in Porous Materials",
        "link": "/arxiv/2509.18573",
        "arxiv_id": "2509.18573",
        "authors": "Dong Chen, Jian Liu, Chun-Long Chen, Guo-Wei Wei",
        "summary": "Porous materials exhibit vast structural diversity and support critical applications in gas storage, separations, and catalysis. However, predictive modeling remains challenging due to the multiscale nature of structure-property relationships, where performance is governed by both local chemical environments and global pore-network topology. These complexities, combined with sparse and unevenly distributed labeled data, hinder generalization across material families. We propose the Interaction Topological Transformer (ITT), a unified data-efficient framework that leverages novel interaction topology to capture materials information across multiple scales and multiple levels, including structural, elemental, atomic, and pairwise-elemental organization. ITT extracts scale-aware features that reflect both compositional and relational structure within complex porous frameworks, and integrates them through a built-in Transformer architecture that supports joint reasoning across scales. Trained using a two-stage strategy, i.e., self-supervised pretraining on 0.6 million unlabeled structures followed by supervised fine-tuning, ITT achieves state-of-the-art, accurate, and transferable predictions for adsorption, transport, and stability properties. This framework provides a principled and scalable path for learning-guided discovery in structurally and chemically diverse porous materials.",
        "subjects": "Machine Learning, Materials Science, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.007841"
    },
    {
        "index": "#131",
        "title": "Explore the Reinforcement Learning for the LLM based ASR and TTS system",
        "link": "/arxiv/2509.18569",
        "arxiv_id": "2509.18569",
        "authors": "Changfeng Gao, Yabin Li, Keyu An, Zhifu Gao, Zhihao Du, Han Zhao, Xiangang Li",
        "summary": "In recent years, large language models (LLMs) have played an important role in automatic speech recognition (ASR) and text-to-speech (TTS) systems. While reinforcement learning (RL) has significantly enhanced LLM performance in text-based tasks, its application to ASR and TTS remains underexplored due to the complexity of training audio-based models. In this study, we propose a lightweight RL framework tailored for audio-based LLMs that can process audio inputs and generate audio outputs. Based on this framework, we evaluate the effectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR task, we experiment with different rule-based reward functions within the Group Relative Policy Optimization (GRPO) framework and investigate the impact of RL data construction. For the TTS task, we compare GRPO with Differentiable Reward Optimization (DiffRO) and further combine the two approaches to achieve improved performance. Our experiments demonstrate that RL can significantly enhance the performance of both ASR and TTS systems, even with limited training data and a small number of optimization steps.",
        "subjects": "Sound, Artificial Intelligence, Audio and Speech Processing",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.008060"
    },
    {
        "index": "#132",
        "title": "CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese Patronizing and Condescending Language Detection",
        "link": "/arxiv/2509.18562",
        "arxiv_id": "2509.18562",
        "authors": "Jiaxun Yang, Yifei Han, Long Zhang, Liu Yujie, Bin Li, Bo Gao, Yangfan He, Kejia Zhan",
        "summary": "Chinese Patronizing and Condescending Language (CPCL) is an implicitly discriminatory toxic speech targeting vulnerable groups on Chinese video platforms. The existing dataset lacks user comments, which are a direct reflection of video content. This undermines the model's understanding of video content and results in the failure to detect some CPLC videos. To make up for this loss, this research reconstructs a new dataset PCLMMPLUS that includes 103k comment entries and expands the dataset size. We also propose the CPCLDetector model with alignment selection and knowledge-enhanced comment content modules. Extensive experiments show the proposed CPCLDetector outperforms the SOTA on PCLMM and achieves higher performance on PCLMMPLUS . CPLC videos are detected more accurately, supporting content governance and protecting vulnerable groups. Code and dataset are available at https://github.com/jiaxunyang256/PCLD.",
        "subjects": "Multimedia, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.008274"
    },
    {
        "index": "#133",
        "title": "SoundCompass: Navigating Target Sound Extraction With Effective Directional Clue Integration In Complex Acoustic Scenes",
        "link": "/arxiv/2509.18561",
        "arxiv_id": "2509.18561",
        "authors": "Dayun Choi, Jung-Woo Choi",
        "summary": "Recent advances in target sound extraction (TSE) utilize directional clues derived from direction of arrival (DoA), which represent an inherent spatial property of sound available in any acoustic scene. However, previous DoA-based methods rely on hand-crafted features or discrete encodings, which lose fine-grained spatial information and limit adaptability. We propose SoundCompass, an effective directional clue integration framework centered on a Spectral Pairwise INteraction (SPIN) module that captures cross-channel spatial correlations in the complex spectrogram domain to preserve full spatial information in multichannel signals. The input feature expressed in terms of spatial correlations is fused with a DoA clue represented as spherical harmonics (SH) encoding. The fusion is carried out across overlapping frequency subbands, inheriting the benefits reported in the previous band-split architectures. We also incorporate the iterative refinement strategy, chain-of-inference (CoI), in the TSE framework, which recursively fuses DoA with sound event activation estimated from the previous inference stage. Experiments demonstrate that SoundCompass, combining SPIN, SH embedding, and CoI, robustly extracts target sources across diverse signal classes and spatial configurations.",
        "subjects": "Audio and Speech Processing, Artificial Intelligence, Sound",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.008460"
    },
    {
        "index": "#134",
        "title": "Global Minimizers of Sigmoid Contrastive Loss",
        "link": "/arxiv/2509.18552",
        "arxiv_id": "2509.18552",
        "authors": "Kiril Bangachev, Guy Bresler, Iliyas Noman, Yury Polyanskiy",
        "summary": "The meta-task of obtaining and aligning representations through contrastive pretraining is steadily gaining importance since its introduction in CLIP and ALIGN. In this paper we theoretically explain the advantages of synchronizing with trainable inverse temperature and bias under the sigmoid loss, as implemented in the recent SigLIP and SigLIP2 models of Google DeepMind. Temperature and bias can drive the loss function to zero for a rich class of configurations that we call $(\\mathsf{m}, \\mathsf{b}_{\\mathsf{rel}})$-Constellations. $(\\mathsf{m}, \\mathsf{b}_{\\mathsf{rel}})$-Constellations are a novel combinatorial object related to spherical codes and are parametrized by a margin $\\mathsf{m}$ and relative bias $\\mathsf{b}_{\\mathsf{rel}}$. We use our characterization of constellations to theoretically justify the success of SigLIP on retrieval, to explain the modality gap present in SigLIP, and to identify the necessary dimension for producing high-quality representations. Finally, we propose a reparameterization of the sigmoid loss with explicit relative bias, which improves training dynamics in experiments with synthetic data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.008689"
    },
    {
        "index": "#135",
        "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts",
        "link": "/arxiv/2509.18542",
        "arxiv_id": "2509.18542",
        "authors": "Qi Wang, Hanyang Peng, Yue Yu",
        "summary": "Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To circumvent the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a single lightweight stage of router training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.008904"
    },
    {
        "index": "#138",
        "title": "Automatic coherence-driven inference on arguments",
        "link": "/arxiv/2509.18523",
        "arxiv_id": "2509.18523",
        "authors": "Steve Huntsman",
        "summary": "Inconsistencies are ubiquitous in law, administration, and jurisprudence. Though a cure is too much to hope for, we propose a technological remedy. Large language models (LLMs) can accurately extract propositions from arguments and compile them into natural data structures that enable coherence-driven inference (CDI) via combinatorial optimization. This neurosymbolic architecture naturally separates concerns and enables meaningful judgments about the coherence of arguments that can inform legislative and policy analysis and legal reasoning.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.010031"
    },
    {
        "index": "#139",
        "title": "APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation",
        "link": "/arxiv/2509.18521",
        "arxiv_id": "2509.18521",
        "authors": "Yuzhen Zhou, Jiajun Li, Yusheng Su, Gowtham Ramesh, Zilin Zhu, Xiang Long, Chenyang Zhao, Jin Pan, Xiaodong Yu, Ze Wang, Kangrui Du, Jialian Wu, Ximeng Sun, Jiang Liu, Qiaolin Yu, Hao Chen, Zicheng Liu, Emad Barsoum",
        "summary": "Reinforcement learning (RL) has become a cornerstone in advancing large-scale pre-trained language models (LLMs). Successive generations, including GPT-o series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale RL training to enhance reasoning and coding capabilities. To meet the community's growing RL needs, numerous RL frameworks have been proposed. Most of these frameworks primarily rely on inference engines for rollout generation and training engines for policy updates. However, RL training remains computationally expensive, with rollout generation accounting for more than 90% of total runtime. In addition, its efficiency is often constrained by the long-tail distribution of rollout response lengths, where a few lengthy responses stall entire batches, leaving GPUs idle and underutilized. As model and rollout sizes continue to grow, this bottleneck increasingly limits scalability. To address this challenge, we propose Active Partial Rollouts in Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the rollout phase, APRIL over-provisions rollout requests, terminates once the target number of responses is reached, and recycles incomplete responses for continuation in future steps. This strategy ensures that no rollouts are discarded while substantially reducing GPU idle time. Experiments show that APRIL improves rollout throughput by at most 44% across commonly used RL algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8% higher final accuracy across tasks. Moreover, APRIL is both framework and hardware agnostic, already integrated into the slime RL framework, and deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies system-level and algorithmic considerations in proposing APRIL, with the aim of advancing RL training efficiency and inspiring further optimizations in RL systems.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.010346"
    },
    {
        "index": "#140",
        "title": "Coherence-driven inference for cybersecurity",
        "link": "/arxiv/2509.18520",
        "arxiv_id": "2509.18520",
        "authors": "Steve Huntsman",
        "summary": "Large language models (LLMs) can compile weighted graphs on natural language data to enable automatic coherence-driven inference (CDI) relevant to red and blue team operations in cybersecurity. This represents an early application of automatic CDI that holds near- to medium-term promise for decision-making in cybersecurity and eventually also for autonomous blue team operations.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.010528"
    },
    {
        "index": "#147",
        "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction",
        "link": "/arxiv/2509.18447",
        "arxiv_id": "2509.18447",
        "authors": "Rishabh Madan, Jiawei Lin, Mahika Goel, Angchen Xie, Xiaoyu Liang, Marcus Lee, Justin Guo, Pranav N. Thakkar, Rohan Banerjee, Jose Barreiros, Kate Tsui, Tom Silver, Tapomayukh Bhattacharjee",
        "summary": "Physical human-robot interaction (pHRI) requires robots to adapt to individual contact preferences, such as where and how much force is applied. Identifying preferences is difficult for a single contact; with whole-arm interaction involving multiple simultaneous contacts between the robot and human, the challenge is greater because different body parts can impose incompatible force requirements. In caregiving tasks, where contact is frequent and varied, such conflicts are unavoidable. With multiple preferences across multiple contacts, no single solution can satisfy all objectives--trade-offs are inherent, making prioritization essential. We present PrioriTouch, a framework for ranking and executing control objectives across multiple contacts. PrioriTouch can prioritize from a general collection of controllers, making it applicable not only to caregiving scenarios such as bed bathing and dressing but also to broader multi-contact settings. Our method combines a novel learning-to-rank approach with hierarchical operational space control, leveraging simulation-in-the-loop rollouts for data-efficient and safe exploration. We conduct a user study on physical assistance preferences, derive personalized comfort thresholds, and incorporate them into PrioriTouch. We evaluate PrioriTouch through extensive simulation and real-world experiments, demonstrating its ability to adapt to user contact preferences, maintain task performance, and enhance safety and comfort. Website: https://emprise.cs.cornell.edu/prioritouch.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.012167"
    },
    {
        "index": "#149",
        "title": "Scattering Transformer: A Training-Free Transformer Architecture for Heart Murmur Detection",
        "link": "/arxiv/2509.18424",
        "arxiv_id": "2509.18424",
        "authors": "Rami Zewail",
        "summary": "In an attempt to address the need for skilled clinicians in heart sound interpretation, recent research efforts on automating cardiac auscultation have explored deep learning approaches. The majority of these approaches have been based on supervised learning that is always challenged in occasions where training data is limited. More recently, there has been a growing interest in potentials of pre-trained self-supervised audio foundation models for biomedical end tasks. Despite exhibiting promising results, these foundational models are typically computationally intensive. Within the context of automatic cardiac auscultation, this study explores a lightweight alternative to these general-purpose audio foundation models by introducing the Scattering Transformer, a novel, training-free transformer architecture for heart murmur detection. The proposed method leverages standard wavelet scattering networks by introducing contextual dependencies in a transformer-like architecture without any backpropagation. We evaluate our approach on the public CirCor DigiScope dataset, directly comparing it against leading general-purpose foundational models. The Scattering Transformer achieves a Weighted Accuracy(WAR) of 0.786 and an Unweighted Average Recall(UAR) of 0.697, demonstrating performance highly competitive with contemporary state of the art methods. This study establishes the Scattering Transformer as a viable and promising alternative in resource-constrained setups.",
        "subjects": "Sound, Artificial Intelligence, Audio and Speech Processing",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.012560"
    },
    {
        "index": "#150",
        "title": "Context Lineage Assurance for Non-Human Identities in Critical Multi-Agent Systems",
        "link": "/arxiv/2509.18415",
        "arxiv_id": "2509.18415",
        "authors": "Sumana Malkapuram, Sameera Gangavarapu, Kailashnath Reddy Kavalakuntla, Ananya Gangavarapu",
        "summary": "The proliferation of autonomous software agents necessitates rigorous frameworks for establishing secure and verifiable agent-to-agent (A2A) interactions, particularly when such agents are instantiated as non-human identities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a cryptographically grounded mechanism for lineage verification, wherein the provenance and evolution of NHIs are anchored in append-only Merkle tree structures modeled after Certificate Transparency (CT) logs. Unlike traditional A2A models that primarily secure point-to-point interactions, our approach enables both agents and external verifiers to cryptographically validate multi-hop provenance, thereby ensuring the integrity of the entire call chain. A federated proof server acts as an auditor across one or more Merkle logs, aggregating inclusion proofs and consistency checks into compact, signed attestations that external parties can verify without access to the full execution trace. In parallel, we augment the A2A agent card to incorporate explicit identity verification primitives, enabling both peer agents and human approvers to authenticate the legitimacy of NHI representations in a standardized manner. Together, these contributions establish a cohesive model that integrates identity attestation, lineage verification, and independent proof auditing, thereby advancing the security posture of inter-agent ecosystems and providing a foundation for robust governance of NHIs in regulated environments such as FedRAMP.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.012815"
    },
    {
        "index": "#151",
        "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections",
        "link": "/arxiv/2509.18407",
        "arxiv_id": "2509.18407",
        "authors": "Navya Tiwari, Joseph Vazhaeparampil, Victoria Preston",
        "summary": "Uncontrolled intersections account for a significant fraction of roadway crashes due to ambiguous right-of-way rules, occlusions, and unpredictable driver behavior. While autonomous vehicle research has explored uncertainty-aware decision making, few systems exist to retrofit human-operated vehicles with assistive navigation support. We present a driver-assist framework for right-of-way reasoning at uncontrolled intersections, formulated as a Partially Observable Markov Decision Process (POMDP). Using a custom simulation testbed with stochastic traffic agents, pedestrians, occlusions, and adversarial scenarios, we evaluate four decision-making approaches: a deterministic finite state machine (FSM), and three probabilistic planners: QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform the rule-based baseline, achieving up to 97.5 percent collision-free navigation under partial observability, with POMCP prioritizing safety and DESPOT balancing efficiency and runtime feasibility. Our findings highlight the importance of uncertainty-aware planning for driver assistance and motivate future integration of sensor fusion and environment perception modules for real-time deployment in realistic traffic environments.",
        "subjects": "Robotics, Artificial Intelligence, Human-Computer Interaction",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.013027"
    },
    {
        "index": "#153",
        "title": "An Artificial Intelligence Value at Risk Approach: Metrics and Models",
        "link": "/arxiv/2509.18394",
        "arxiv_id": "2509.18394",
        "authors": "Luis Enriquez Alvarez",
        "summary": "Artificial intelligence risks are multidimensional in nature, as the same risk scenarios may have legal, operational, and financial risk dimensions. With the emergence of new AI regulations, the state of the art of artificial intelligence risk management seems to be highly immature due to upcoming AI regulations. Despite the appearance of several methodologies and generic criteria, it is rare to find guidelines with real implementation value, considering that the most important issue is customizing artificial intelligence risk metrics and risk models for specific AI risk scenarios. Furthermore, the financial departments, legal departments and Government Risk Compliance teams seem to remain unaware of many technical aspects of AI systems, in which data scientists and AI engineers emerge as the most appropriate implementers. It is crucial to decompose the problem of artificial intelligence risk in several dimensions: data protection, fairness, accuracy, robustness, and information security. Consequently, the main task is developing adequate metrics and risk models that manage to reduce uncertainty for decision-making in order to take informed decisions concerning the risk management of AI systems. The purpose of this paper is to orientate AI stakeholders about the depths of AI risk management. Although it is not extremely technical, it requires a basic knowledge of risk management, quantifying uncertainty, the FAIR model, machine learning, large language models and AI context engineering. The examples presented pretend to be very basic and understandable, providing simple ideas that can be developed regarding specific AI customized environments. There are many issues to solve in AI risk management, and this paper will present a holistic overview of the inter-dependencies of AI risks, and how to model them together, within risk scenarios.",
        "subjects": "Computers and Society, Artificial Intelligence, Risk Management",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.013470"
    },
    {
        "index": "#154",
        "title": "Graph Enhanced Trajectory Anomaly Detection",
        "link": "/arxiv/2509.18386",
        "arxiv_id": "2509.18386",
        "authors": "Jonathan Kabala Mbuya, Dieter Pfoser, Antonios Anastasopoulos",
        "summary": "Trajectory anomaly detection is essential for identifying unusual and unexpected movement patterns in applications ranging from intelligent transportation systems to urban safety and fraud prevention. Existing methods only consider limited aspects of the trajectory nature and its movement space by treating trajectories as sequences of sampled locations, with sampling determined by positioning technology, e.g., GPS, or by high-level abstractions such as staypoints. Trajectories are analyzed in Euclidean space, neglecting the constraints and connectivity information of the underlying movement network, e.g., road or transit networks. The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework tightly integrates road network topology, segment semantics, and historical travel patterns to model trajectory data. GETAD uses a Graph Attention Network to learn road-aware embeddings that capture both physical attributes and transition behavior, and augments these with graph-based positional encodings that reflect the spatial layout of the road network. A Transformer-based decoder models sequential movement, while a multiobjective loss function combining autoregressive prediction and supervised link prediction ensures realistic and structurally coherent representations. To improve the robustness of anomaly detection, we introduce Confidence Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that emphasizes high-confidence deviations. Experiments on real-world and synthetic datasets demonstrate that GETAD achieves consistent improvements over existing methods, particularly in detecting subtle anomalies in road-constrained environments. These results highlight the benefits of incorporating graph structure and contextual semantics into trajectory modeling, enabling more precise and context-aware anomaly detection.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.013669"
    },
    {
        "index": "#156",
        "title": "Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data",
        "link": "/arxiv/2509.18367",
        "arxiv_id": "2509.18367",
        "authors": "Zhuoyu Yao, Yue Wang, Songyang Zhang, Yingshu Li, Zhipeng Cai, Zhi Tian",
        "summary": "Recent advances in distributed swarm learning (DSL) offer a promising paradigm for edge Internet of Things. Such advancements enhance data privacy, communication efficiency, energy saving, and model scalability. However, the presence of non-independent and identically distributed (non-i.i.d.) data pose a significant challenge for multi-access edge computing, degrading learning performance and diverging training behavior of vanilla DSL. Further, there still lacks theoretical guidance on how data heterogeneity affects model training accuracy, which requires thorough investigation. To fill the gap, this paper first study the data heterogeneity by measuring the impact of non-i.i.d. datasets under the DSL framework. This then motivates a new multi-worker selection design for DSL, termed M-DSL algorithm, which works effectively with distributed heterogeneous data. A new non-i.i.d. degree metric is introduced and defined in this work to formulate the statistical difference among local datasets, which builds a connection between the measure of data heterogeneity and the evaluation of DSL performance. In this way, our M-DSL guides effective selection of multiple works who make prominent contributions for global model updates. We also provide theoretical analysis on the convergence behavior of our M-DSL, followed by extensive experiments on different heterogeneous datasets and non-i.i.d. data settings. Numerical results verify performance improvement and network intelligence enhancement provided by our M-DSL beyond the benchmarks.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.014124"
    },
    {
        "index": "#157",
        "title": "FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction",
        "link": "/arxiv/2509.18362",
        "arxiv_id": "2509.18362",
        "authors": "Yuxuan Cai, Xiaozhuan Liang, Xinghua Wang, Jin Ma, Haijin Liang, Jinwen Luo, Xinyu Zuo, Lisheng Duan, Yuyang Yin, Xi Chen",
        "summary": "As large language models (LLMs) become increasingly powerful, the sequential nature of autoregressive generation creates a fundamental throughput bottleneck that limits the practical deployment. While Multi-Token Prediction (MTP) has demonstrated remarkable benefits for model training efficiency and performance, its inherent potential for inference acceleration remains largely unexplored. This paper introduces FastMTP, a simple yet effective method that improves multi-step draft quality by aligning MTP training with its inference pattern, significantly enhancing speculative decoding performance. Our approach fine-tunes a single MTP head with position-shared weights on self-distilled data, enabling it to capture dependencies among consecutive future tokens and maintain high acceptance rates across multiple recursive draft steps. By integrating language-aware dynamic vocabulary compression into the MTP head, we further reduce computational overhead in the drafting process. Experimental results across seven diverse benchmarks demonstrate that FastMTP achieves an average of 2.03x speedup compared to standard next token prediction with lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires only lightweight training and seamlessly integrates with existing inference frameworks, offering a practical and rapidly deployable solution for accelerating LLM inference.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.014382"
    },
    {
        "index": "#158",
        "title": "Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts",
        "link": "/arxiv/2509.18361",
        "arxiv_id": "2509.18361",
        "authors": "Daye Nam, Malgorzata Salawa, Satish Chandra",
        "summary": "Evaluating developer satisfaction with conversational AI assistants at scale is critical but challenging. User studies provide rich insights, but are unscalable, while large-scale quantitative signals from logs or in-product ratings are often too shallow or sparse to be reliable. To address this gap, we propose and evaluate a new approach: using sentiment analysis of developer prompts to identify implicit signals of user satisfaction. With an analysis of industrial usage logs of 372 professional developers, we show that this approach can identify a signal in ~8% of all interactions, a rate more than 13 times higher than explicit user feedback, with reasonable accuracy even with an off-the-shelf sentiment analysis approach. This new practical approach to complement existing feedback channels would open up new directions for building a more comprehensive understanding of the developer experience at scale.",
        "subjects": "Software Engineering, Artificial Intelligence, Human-Computer Interaction",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.014572"
    },
    {
        "index": "#159",
        "title": "Chiplet-Based RISC-V SoC with Modular AI Acceleration",
        "link": "/arxiv/2509.18355",
        "arxiv_id": "2509.18355",
        "authors": "P. Ramkumar, S. S. Bharadwaj",
        "summary": "Achieving high performance, energy efficiency, and cost-effectiveness while maintaining architectural flexibility is a critical challenge in the development and deployment of edge AI devices. Monolithic SoC designs struggle with this complex balance mainly due to low manufacturing yields (below 16%) at advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based RISC-V SoC architecture that addresses these limitations through modular AI acceleration and intelligent system level optimization. Our proposed design integrates 4 different key innovations in a 30mm x 30mm silicon interposer: adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring streaming flow control units and compression-aware transfers; distributed cryptographic security across heterogeneous chiplets; and intelligent sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory stacks, and dedicated power management controllers. Experimental results across industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video processing demonstrate significant performance improvements. The AI-optimized configuration achieves ~14.7% latency reduction, 17.3% throughput improvement, and 16.2% power reduction compared to previous basic chiplet implementations. These improvements collectively translate to a 40.1% efficiency gain corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while maintaining sub-5ms real-time capability across all experimented workloads. These performance upgrades demonstrate that modular chiplet designs can achieve near-monolithic computational density while enabling cost efficiency, scalability and upgradeability, crucial for next-generation edge AI device applications.",
        "subjects": "Hardware Architecture, Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.014768"
    },
    {
        "index": "#163",
        "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies",
        "link": "/arxiv/2509.18282",
        "arxiv_id": "2509.18282",
        "authors": "Jesse Zhang, Marius Memmel, Kevin Kim, Dieter Fox, Jesse Thomason, Fabio Ramos, Erdem Bƒ±yƒ±k, Abhishek Gupta, Anqi Li",
        "summary": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: 1. end-effector paths specifying what actions to take, and 2. task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need--where, what, and how. Website at https://peek-robot.github.io/.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.015627"
    },
    {
        "index": "#164",
        "title": "Perceptions of AI Across Sectors: A Comparative Review of Public Attitudes",
        "link": "/arxiv/2509.18233",
        "arxiv_id": "2509.18233",
        "authors": "Filip Bialy, Mark Elliot, Robert Meckin",
        "summary": "This paper offers a domain-mediated comparative review of 251 studies on public attitudes toward AI, published between 2011 and 2025. Drawing on a systematic literature review, we analyse how different factors including perceived benefits and concerns (or risks) shape public acceptance of - or resistance to - artificial intelligence across domains and use-cases, including healthcare, education, security, public administration, generative AI, and autonomous vehicles. The analysis highlights recurring patterns in individual, contextual, and technical factors influencing perception, while also tracing variations in institutional trust, perceived fairness, and ethical concerns. We show that the public perception in AI is shaped not only by technical design or performance but also by sector-specific considerations as well as imaginaries, cultural narratives, and historical legacies. This comparative approach offers a foundation for developing more tailored and context-sensitive strategies for responsible AI governance.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.015853"
    },
    {
        "index": "#165",
        "title": "Enhanced Interpretable Knowledge Tracing for Students Performance Prediction with Human understandable Feature Space",
        "link": "/arxiv/2509.18231",
        "arxiv_id": "2509.18231",
        "authors": "Sein Minn, Roger Nkambou",
        "summary": "Knowledge Tracing (KT) plays a central role in assessing students skill mastery and predicting their future performance. While deep learning based KT models achieve superior predictive accuracy compared to traditional methods, their complexity and opacity hinder their ability to provide psychologically meaningful explanations. This disconnect between model parameters and cognitive theory poses challenges for understanding and enhancing the learning process, limiting their trustworthiness in educational applications. To address these challenges, we enhance interpretable KT models by exploring human-understandable features derived from students interaction data. By incorporating additional features, particularly those reflecting students learning abilities, our enhanced approach improves predictive accuracy while maintaining alignment with cognitive theory. Our contributions aim to balance predictive power with interpretability, advancing the utility of adaptive learning systems.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.016039"
    },
    {
        "index": "#166",
        "title": "Automatic Classification of Magnetic Chirality of Solar Filaments from H-Alpha Observations",
        "link": "/arxiv/2509.18214",
        "arxiv_id": "2509.18214",
        "authors": "Alexis Chalmers, Azim Ahmadzadeh",
        "summary": "In this study, we classify the magnetic chirality of solar filaments from H-Alpha observations using state-of-the-art image classification models. We establish the first reproducible baseline for solar filament chirality classification on the MAGFiLO dataset. The MAGFiLO dataset contains over 10,000 manually-annotated filaments from GONG H-Alpha observations, making it the largest dataset for filament detection and classification to date. Prior studies relied on much smaller datasets, which limited their generalizability and comparability. We fine-tuned several pre-trained, image classification architectures, including ResNet, WideResNet, ResNeXt, and ConvNeXt, and also applied data augmentation and per-class loss weights to optimize the models. Our best model, ConvNeXtBase, achieves a per-class accuracy of 0.69 for left chirality filaments and $0.73$ for right chirality filaments.",
        "subjects": "Solar and Stellar Astrophysics, Artificial Intelligence",
        "date": "2025-09-21",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.016219"
    },
    {
        "index": "#167",
        "title": "Variational Task Vector Composition",
        "link": "/arxiv/2509.18208",
        "arxiv_id": "2509.18208",
        "authors": "Boyuan Zhang, Yingjun Du, Xiantong Zhen, Ling Shao",
        "summary": "Task vectors capture how a model changes during fine-tuning by recording the difference between pre-trained and task-specific weights. The composition of task vectors, a key operator in task arithmetic, enables models to integrate knowledge from multiple tasks without incurring additional inference costs. In this paper, we propose variational task vector composition, where composition coefficients are taken as latent variables and estimated in a Bayesian inference framework. Unlike previous methods that operate at the task level, our framework focuses on sample-specific composition. Motivated by the observation of structural redundancy in task vectors, we introduce a Spike-and-Slab prior that promotes sparsity and preserves only the most informative components. To further address the high variance and sampling inefficiency in sparse, high-dimensional spaces, we develop a gated sampling mechanism that constructs a controllable posterior by filtering the composition coefficients based on both uncertainty and importance. This yields a more stable and interpretable variational framework by deterministically selecting reliable task components, reducing sampling variance while improving transparency and generalization. Experimental results demonstrate that our method consistently outperforms existing approaches across all datasets by selectively leveraging the most reliable and informative components in task vectors. These findings highlight the practical value of our approach, establishing a new standard for efficient and effective task vector composition.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-21",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.016411"
    },
    {
        "index": "#169",
        "title": "MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech",
        "link": "/arxiv/2509.18196",
        "arxiv_id": "2509.18196",
        "authors": "Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu",
        "summary": "Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing lexical content, but largely fail to recognize nonverbal vocalizations (NVs) embedded in speech, such as sighs, laughs, and coughs. This capability is important for a comprehensive understanding of human communication, as NVs convey crucial emotional and intentional cues. Progress in NV-aware ASR has been hindered by the lack of high-quality, well-annotated datasets. To address this gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech dataset. Unlike most existing corpora that rely on model-based detection, MNV-17's performative nature ensures high-fidelity, clearly articulated NV instances. To the best of our knowledge, MNV-17 provides the most extensive set of nonverbal vocalization categories, comprising 17 distinct and well-balanced classes of common NVs. We benchmarked MNV-17 on four mainstream ASR architectures, evaluating their joint performance on semantic transcription and NV classification. The dataset and the pretrained model checkpoints will be made publicly available to facilitate future research in expressive ASR.",
        "subjects": "Sound, Artificial Intelligence, Audio and Speech Processing",
        "date": "2025-09-19",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.016844"
    },
    {
        "index": "#178",
        "title": "Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks",
        "link": "/arxiv/2509.18161",
        "arxiv_id": "2509.18161",
        "authors": "William H Patty",
        "summary": "Activation functions in neural networks are typically selected from a set of empirically validated, commonly used static functions such as ReLU, tanh, or sigmoid. However, by optimizing the shapes of a network's activation functions, we can train models that are more parameter-efficient and accurate by assigning more optimal activations to the neurons. In this paper, I present and compare 9 training methodologies to explore dual-optimization dynamics in neural networks with parameterized linear B-spline activation functions. The experiments realize up to 94% lower end model error rates in FNNs and 51% lower rates in CNNs compared to traditional ReLU-based models. These gains come at the cost of additional development and training complexity as well as end model latency.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-17",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.019022"
    },
    {
        "index": "#180",
        "title": "WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation",
        "link": "/arxiv/2509.18152",
        "arxiv_id": "2509.18152",
        "authors": "Zhenyu Qi, Qing Yu, Jichen Wang, Yun-Bo Zhao, Zerui Li, Wenjun Lv",
        "summary": "Well-log interpretation is fundamental for subsurface characterization but remains challenged by heterogeneous tool responses, noisy signals, and limited labels. We propose WLFM, a foundation model pretrained on multi-curve logs from 1200 wells, comprising three stages: tokenization of log patches into geological tokens, self-supervised pretraining with masked-token modeling and stratigraphy-aware contrastive learning, and multi-task adaptation with few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines, achieving 0.0041 MSE in porosity estimation and 74.13\\% accuracy in lithology classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\\% accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness, learns a reusable geological vocabulary, and reconstructs masked curves with reasonable fidelity, though systematic offsets are observed in shallow and ultra-deep intervals. Although boundary detection is not explicitly evaluated here, clustering analyses suggest strong potential for future extension. These results establish WLFM as a scalable, interpretable, and transferable backbone for geological AI, with implications for multi-modal integration of logs, seismic, and textual data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.019436"
    },
    {
        "index": "#181",
        "title": "HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork",
        "link": "/arxiv/2509.18151",
        "arxiv_id": "2509.18151",
        "authors": "Jindi Lv, Yuhao Zhou, Yuxin Tian, Qing Ye, Wentao Feng, Jiancheng Lv",
        "summary": "Time-intensive performance evaluations significantly impede progress in Neural Architecture Search (NAS). To address this, neural predictors leverage surrogate models trained on proxy datasets, allowing for direct performance predictions for new architectures. However, these predictors often exhibit poor generalization due to their limited ability to capture intricate relationships among various architectures. In this paper, we propose HyperNAS, a novel neural predictor paradigm for enhancing architecture representation learning. HyperNAS consists of two primary components: a global encoding scheme and a shared hypernetwork. The global encoding scheme is devised to capture the comprehensive macro-structure information, while the shared hypernetwork serves as an auxiliary task to enhance the investigation of inter-architecture patterns. To ensure training stability, we further develop a dynamic adaptive multi-task loss to facilitate personalized exploration on the Pareto front. Extensive experiments across five representative search spaces, including ViTs, demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For instance, HyperNAS strikes new state-of-the-art results, with 97.60\\% top-1 accuracy on CIFAR-10 and 82.4\\% top-1 accuracy on ImageNet, using at least 5.0$\\times$ fewer samples.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.019635"
    },
    {
        "index": "#182",
        "title": "Sparse Training Scheme for Multimodal LLM",
        "link": "/arxiv/2509.18150",
        "arxiv_id": "2509.18150",
        "authors": "Kean Shi, Liang Chen, Haozhe Zhao, Baobao Chang",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated outstanding performance across a variety of domains. However, training MLLMs is often inefficient due to the significantly longer input sequences introduced by multimodal data and the low utilization of inter-layer computations. To address this challenge, we shift the focus to the training process itself and propose a novel training-efficient framework based on sparse representations, termed the Sparse Training Scheme (STS). This scheme consists of two key components: the Visual Token Compressor, which reduces the information load by compressing visual tokens, and the Layer Dynamic Skipper, which mitigates the computational overhead by dynamically skipping unnecessary layers in the language model during both forward and backward passes. Our approach is broadly applicable to diverse MLLM architectures and has been extensively evaluated on multiple benchmarks, demonstrating its effectiveness and efficiency.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.019826"
    },
    {
        "index": "#183",
        "title": "Augmenting Limited and Biased RCTs through Pseudo-Sample Matching-Based Observational Data Fusion Method",
        "link": "/arxiv/2509.18148",
        "arxiv_id": "2509.18148",
        "authors": "Kairong Han, Weidong Huang, Taiyang Zhou, Peng Zhen, Kun Kuang",
        "summary": "In the online ride-hailing pricing context, companies often conduct randomized controlled trials (RCTs) and utilize uplift models to assess the effect of discounts on customer orders, which substantially influences competitive market outcomes. However, due to the high cost of RCTs, the proportion of trial data relative to observational data is small, which only accounts for 0.65\\% of total traffic in our context, resulting in significant bias when generalizing to the broader user base. Additionally, the complexity of industrial processes reduces the quality of RCT data, which is often subject to heterogeneity from potential interference and selection bias, making it difficult to correct. Moreover, existing data fusion methods are challenging to implement effectively in complex industrial settings due to the high dimensionality of features and the strict assumptions that are hard to verify with real-world data. To address these issues, we propose an empirical data fusion method called pseudo-sample matching. By generating pseudo-samples from biased, low-quality RCT data and matching them with the most similar samples from large-scale observational data, the method expands the RCT dataset while mitigating its heterogeneity. We validated the method through simulation experiments, conducted offline and online tests using real-world data. In a week-long online experiment, we achieved a 0.41\\% improvement in profit, which is a considerable gain when scaled to industrial scenarios with hundreds of millions in revenue. In addition, we discuss the harm to model training, offline evaluation, and online economic benefits when the RCT data quality is not high, and emphasize the importance of improving RCT data quality in industrial scenarios. Further details of the simulation experiments can be found in the GitHub repository https://github.com/Kairong-Han/Pseudo-Matching.",
        "subjects": "Methodology, Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.020038"
    },
    {
        "index": "#184",
        "title": "ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks",
        "link": "/arxiv/2509.18147",
        "arxiv_id": "2509.18147",
        "authors": "Xinyu Mu, Hui Dou, Furao Shen, Jian Zhao",
        "summary": "Concept-based interpretability for Convolutional Neural Networks (CNNs) aims to align internal model representations with high-level semantic concepts, but existing approaches largely overlook the semantic roles of individual filters and the dynamic propagation of concepts across layers. To address these limitations, we propose ConceptFlow, a concept-based interpretability framework that simulates the internal \"thinking path\" of a model by tracing how concepts emerge and evolve across layers. ConceptFlow comprises two key components: (i) concept attentions, which associate each filter with relevant high-level concepts to enable localized semantic interpretation, and (ii) conceptual pathways, derived from a concept transition matrix that quantifies how concepts propagate and transform between filters. Together, these components offer a unified and structured view of internal model reasoning. Experimental results demonstrate that ConceptFlow yields semantically meaningful insights into model reasoning, validating the effectiveness of concept attentions and conceptual pathways in explaining decision behavior. By modeling hierarchical conceptual pathways, ConceptFlow provides deeper insight into the internal logic of CNNs and supports the generation of more faithful and human-aligned explanations.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.020252"
    },
    {
        "index": "#185",
        "title": "Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records",
        "link": "/arxiv/2509.18145",
        "arxiv_id": "2509.18145",
        "authors": "Syed Ahmad Chan Bukhari, Amritpal Singh, Shifath Hossain, Iram Wajahat",
        "summary": "Intensive Care Unit (ICU) patients often present with complex, overlapping signs of physiological deterioration that require timely escalation of care. Traditional early warning systems, such as SOFA or MEWS, are limited by their focus on single outcomes and fail to capture the multi-dimensional nature of clinical decline. This study proposes a multi-label classification framework to predict Care Escalation Triggers (CETs), including respiratory failure, hemodynamic instability, renal compromise, and neurological deterioration, using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are defined through rule-based criteria applied to data from hours 24 to 72 (for example, oxygen saturation below 90, mean arterial pressure below 65 mmHg, creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale score greater than 2). Features are extracted from the first 24 hours and include vital sign aggregates, laboratory values, and static demographics. We train and evaluate multiple classification models on a cohort of 85,242 ICU stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation metrics include per-label precision, recall, F1-score, and Hamming loss. XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory, 0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration, outperforming baseline models. Feature analysis shows that clinically relevant parameters such as respiratory rate, blood pressure, and creatinine are the most influential predictors, consistent with the clinical definitions of the CETs. The proposed framework demonstrates practical potential for early, interpretable clinical alerts without requiring complex time-series modeling or natural language processing.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-15",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.020461"
    },
    {
        "index": "#186",
        "title": "AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation",
        "link": "/arxiv/2509.18144",
        "arxiv_id": "2509.18144",
        "authors": "Yubo Yang, Yichen Zhu, Bo Jiang",
        "summary": "Spatio-temporal data abounds in domain like traffic and environmental monitoring. However, it often suffers from missing values due to sensor malfunctions, transmission failures, etc. Recent years have seen continued efforts to improve spatio-temporal data imputation performance. Recently diffusion models have outperformed other approaches in various tasks, including spatio-temporal imputation, showing competitive performance. Extracting and utilizing spatio-temporal dependencies as conditional information is vital in diffusion-based methods. However, previous methods introduce error accumulation in this process and ignore the variability of the dependencies in the noisy data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel spatio-temporal imputation approach based on conditional diffusion model. Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model for pre-imputation with the imputed result used to extract conditional information by our designed Spatio-Temporal Conditionalizer (STC)network. We also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated attention mechanism to capture the variant dependencies across diffusion steps. Extensive experiments on three real-world datasets show that AdaSTI outperforms existing methods in all the settings, with up to 46.4% reduction in imputation error.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-15",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.020670"
    },
    {
        "index": "#187",
        "title": "Weight Mapping Properties of a Dual Tree Single Clock Adiabatic Capacitive Neuron",
        "link": "/arxiv/2509.18143",
        "arxiv_id": "2509.18143",
        "authors": "Mike Smart, Sachin Maheshwari, Himadri Singh Raghav, Alexander Serb",
        "summary": "Dual Tree Single Clock (DTSC) Adiabatic Capacitive Neuron (ACN) circuits offer the potential for highly energy-efficient Artificial Neural Network (ANN) computation in full custom analog IC designs. The efficient mapping of Artificial Neuron (AN) abstract weights, extracted from the software-trained ANNs, onto physical ACN capacitance values has, however, yet to be fully researched. In this paper, we explore the unexpected hidden complexities, challenges and properties of the mapping, as well as, the ramifications for IC designers in terms accuracy, design and implementation. We propose an optimal, AN to ACN methodology, that promotes smaller chip sizes and improved overall classification accuracy, necessary for successful practical deployment. Using TensorFlow and Larq software frameworks, we train three different ANN networks and map their weights into the energy-efficient DTSC ACN capacitance value domain to demonstrate 100% functional equivalency. Finally, we delve into the impact of weight quantization on ACN performance using novel metrics related to practical IC considerations, such as IC floor space and comparator decision-making efficacy.",
        "subjects": "Emerging Technologies, Artificial Intelligence, Machine Learning, Image and Video Processing",
        "date": "2025-09-15",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.020945"
    },
    {
        "index": "#189",
        "title": "A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders",
        "link": "/arxiv/2509.18140",
        "arxiv_id": "2509.18140",
        "authors": "Iram Wajahat, Amritpal Singh, Fazel Keshtkar, Syed Ahmad Chan Bukhari",
        "summary": "Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent a significant global health burden, disproportionately impacting genetically predisposed populations such as the Pima Indians (a Native American tribe from south central Arizona). This study introduces a novel machine learning (ML) framework that integrates predictive modeling with gene-agnostic pathway mapping to identify high-risk individuals and uncover potential therapeutic targets. Using the Pima Indian dataset, logistic regression and t-tests were applied to identify key predictors of T2DM, yielding an overall model accuracy of 78.43%. To bridge predictive analytics with biological relevance, we developed a pathway mapping strategy that links identified predictors to critical signaling networks, including insulin signaling, AMPK, and PPAR pathways. This approach provides mechanistic insights without requiring direct molecular data. Building upon these connections, we propose therapeutic strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1 modulators, and phytochemical, further validated through pathway enrichment analyses. Overall, this framework advances precision medicine by offering interpretable and scalable solutions for early detection and targeted intervention in metabolic disorders. The key contributions of this work are: (1) development of an ML framework combining logistic regression and principal component analysis (PCA) for T2DM risk prediction; (2) introduction of a gene-agnostic pathway mapping approach to generate mechanistic insights; and (3) identification of novel therapeutic strategies tailored for high-risk populations.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-14",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.021380"
    },
    {
        "index": "#190",
        "title": "LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods",
        "link": "/arxiv/2509.18137",
        "arxiv_id": "2509.18137",
        "authors": "Shaoheng Wang, Yao Lu, Yuqi Li, Yaxin Gao, Jiaqi Nie, Shanqing Yu, Yingli Tian, Qi Xuan",
        "summary": "As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation (LoRA) can save significant costs in storage and computing, but its strong adaptability to a single task is often accompanied by insufficient cross-task generalization capabilities. To improve this, existing work combines LoRA with mixture-of-experts (MoE) to enhance the model's adaptability through expert modules and routing mechanisms. However, existing LoRA-MoE methods lack unified standards in models, datasets, hyperparameters, and evaluation methods, making it difficult to conduct fair comparisons between different methods. To this end, we proposed a unified benchmark named LoRALib. Specifically, we standardized datasets from $40$ downstream tasks into a unified format, fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules across $17$ model architectures. Based on this LoRA library, we conduct large-scale experiments on $3$ representative LoRA-MoE methods and different LoRA selection mechanisms using the open-sourced testing tool OpenCompass. Extensive experiments show that LoRAMoE performs best, and that prioritizing LoRAs relevant to the target task can further improve the performance of MoE. We hope these findings will inspire future work. Our datasets and LoRA library are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset and https://huggingface.co/YaoLuzjut/models.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-14",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.021637"
    },
    {
        "index": "#191",
        "title": "From Parameters to Performance: A Data-Driven Study on LLM Structure and Development",
        "link": "/arxiv/2509.18136",
        "arxiv_id": "2509.18136",
        "authors": "Suqing Wang, Zuchao Li, Luohe Shi, Bo Du, Hai Zhao, Yun Li, Qianren Wang",
        "summary": "Large language models (LLMs) have achieved remarkable success across various domains, driving significant technological advancements and innovations. Despite the rapid growth in model scale and capability, systematic, data-driven research on how structural configurations affect performance remains scarce. To address this gap, we present a large-scale dataset encompassing diverse open-source LLM structures and their performance across multiple benchmarks. Leveraging this dataset, we conduct a systematic, data mining-driven analysis to validate and quantify the relationship between structural configurations and performance. Our study begins with a review of the historical development of LLMs and an exploration of potential future trends. We then analyze how various structural choices impact performance across benchmarks and further corroborate our findings using mechanistic interpretability techniques. By providing data-driven insights into LLM optimization, our work aims to guide the targeted development and application of future models. We will release our dataset at https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-14",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.021911"
    },
    {
        "index": "#192",
        "title": "SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting",
        "link": "/arxiv/2509.18135",
        "arxiv_id": "2509.18135",
        "authors": "Shaoxun Wang, Xingjun Zhang, Qianyang Li, Jiawei Cao, Zhendong Tan",
        "summary": "Inter-series correlations are crucial for accurate multivariate time series forecasting, yet these relationships often exhibit complex dynamics across different temporal scales. Existing methods are limited in modeling these multi-scale dependencies and struggle to capture their intricate and evolving nature. To address this challenge, this paper proposes a novel Static-Dynamic Graph Fusion network (SDGF), whose core lies in capturing multi-scale inter-series correlations through a dual-path graph structure learning approach. Specifically, the model utilizes a static graph based on prior knowledge to anchor long-term, stable dependencies, while concurrently employing Multi-level Wavelet Decomposition to extract multi-scale features for constructing an adaptively learned dynamic graph to capture associations at different scales. We design an attention-gated module to fuse these two complementary sources of information intelligently, and a multi-kernel dilated convolutional network is then used to deepen the understanding of temporal patterns. Comprehensive experiments on multiple widely used real-world benchmark datasets demonstrate the effectiveness of our proposed model.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-14",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.022118"
    },
    {
        "index": "#193",
        "title": "Self-Evolving LLMs via Continual Instruction Tuning",
        "link": "/arxiv/2509.18133",
        "arxiv_id": "2509.18133",
        "authors": "Le Huang, Jiazheng Kang, Cheng Hou, Zhe Zhao, Zhenxiang Yan, Chuan Shi, Ting Bai",
        "summary": "In real-world industrial settings, large language models (LLMs) must learn continually to keep pace with diverse and evolving tasks, requiring self-evolution to refine knowledge under dynamic data distributions. However, existing continual learning (CL) approaches, such as replay and parameter isolation, often suffer from catastrophic forgetting: training on new tasks degrades performance on earlier ones by overfitting to the new distribution and weakening generalization.We propose MoE-CL, a parameter-efficient adversarial mixture-of-experts framework for industrial-scale, self-evolving continual instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated LoRA expert per task to preserve task-specific knowledge via parameter independence, mitigating forgetting; and (2) a shared LoRA expert to enable cross-task transfer. To prevent transferring task-irrelevant noise through the shared pathway, we integrate a task-aware discriminator within a GAN. The discriminator encourages the shared expert to pass only task-aligned information during sequential training. Through adversarial learning, the shared expert acquires generalized representations that mimic the discriminator, while dedicated experts retain task-specific details, balancing knowledge retention and cross-task generalization and thereby supporting self-evolution.Extensive experiments on the public MTL5 benchmark and an industrial Tencent3 benchmark validate the effectiveness of MoE-CL for continual instruction tuning. In real-world A/B testing for content compliance review on the Tencent Video platform, MoE-CL reduced manual review costs by 15.3%. These results demonstrate that MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-14",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.022330"
    },
    {
        "index": "#194",
        "title": "Two ways to knowledge?",
        "link": "/arxiv/2509.18131",
        "arxiv_id": "2509.18131",
        "authors": "Jean-Michel Tucny, Abhisek Ganguly, Santosh Ansumali, Sauro Succi",
        "summary": "It is shown that the weight matrices of transformer-based machine learning applications to the solution of two representative physical applications show a random-like character which bears no directly recognizable link to the physical and mathematical structure of the physical problem under study. This suggests that machine learning and the scientific method may represent two distinct and potentially complementary paths to knowledge, even though a strict notion of explainability in terms of direct correspondence between network parameters and physical structures may remain out of reach. It is also observed that drawing a parallel between transformer operation and (generalized) path-integration techniques may account for the random-like nature of the weights, but still does not resolve the tension with explainability. We conclude with some general comments on the hazards of gleaning knowledge without the benefit of Insight.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-12",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.022536"
    },
    {
        "index": "#195",
        "title": "Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model",
        "link": "/arxiv/2509.18130",
        "arxiv_id": "2509.18130",
        "authors": "Zijie Zhou, Huichen Ma",
        "summary": "In the metro intelligent transportation system, accurate transfer passenger flow prediction is a key link in optimizing operation plans and improving transportation efficiency. To further improve the theory of metro internal transfer passenger flow prediction and provide more reliable support for intelligent operation decisions, this paper innovatively proposes a metro transfer passenger flow prediction model that integrates the Seasonal and Trend decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In practical application, the model first relies on the deep learning library Keras to complete the construction and training of the GRU model, laying the foundation for subsequent prediction; then preprocesses the original metro card swiping data, uses the graph-based depth-first search algorithm to identify passengers' travel paths, and further constructs the transfer passenger flow time series; subsequently adopts the STL time series decomposition algorithm to decompose the constructed transfer passenger flow time series into trend component, periodic component and residual component, and uses the 3{\\sigma} principle to eliminate and fill the outliers in the residual component, and finally completes the transfer passenger flow prediction.Taking the transfer passenger flow data of a certain metro station as the research sample, the validity of the model is verified. The results show that compared with Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the STL-GRU combined prediction model significantly improves the prediction accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays and rest days, with the mean absolute percentage error (MAPE) of the prediction results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-12",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.022743"
    },
    {
        "index": "#197",
        "title": "Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning",
        "link": "/arxiv/2509.18126",
        "arxiv_id": "2509.18126",
        "authors": "Bishal K C, Amr Hilal, Pawan Thapa",
        "summary": "Federated Learning (FL) is a decentralized training framework widely used in IoT ecosystems that preserves privacy by keeping raw data local, making it ideal for IoT-enabled cyber-physical systems with sensing and communication like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle infrastructure, securing these IoT-based charging stations against cyber threats has become critical. Centralized Intrusion Detection Systems (IDS) raise privacy concerns due to sensitive network and user data, making FL a promising alternative. However, current FL-based IDS evaluations overlook practical challenges such as system heterogeneity and non-IID data. To address these challenges, we conducted experiments to evaluate the performance of federated learning for anomaly detection in EV charging stations under system and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization approaches, to analyze their effectiveness in anomaly detection. Under IID settings, FedAvg achieves superior performance to centralized models using the same neural network. However, performance degrades with non-IID data and system heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous settings, showing better convergence and higher anomaly detection accuracy. Our results demonstrate that FL can handle heterogeneity in IoT-based EVCS without significant performance loss, with FedAvgM as a promising solution for robust, privacy-preserving EVCS security.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-11",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.023254"
    },
    {
        "index": "#198",
        "title": "NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment",
        "link": "/arxiv/2509.18125",
        "arxiv_id": "2509.18125",
        "authors": "Harsha Koduri",
        "summary": "Healthcare systems face increasing pressure to allocate limited nursing resources efficiently while accounting for skill heterogeneity, patient acuity, staff fatigue, and continuity of care. Traditional optimization and heuristic scheduling methods struggle to capture these dynamic, multi-constraint environments. I propose NurseSchedRL, a reinforcement learning framework for nurse-patient assignment that integrates structured state encoding, constrained action masking, and attention-based representations of skills, fatigue, and geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with feasibility masks to ensure assignments respect real-world constraints, while dynamically adapting to patient arrivals and varying nurse availability. In simulation with realistic nurse and patient data, NurseSchedRL achieves improved scheduling efficiency, better alignment of skills to patient needs, and reduced fatigue compared to baseline heuristic and unconstrained RL approaches. These results highlight the potential of reinforcement learning for decision support in complex, high-stakes healthcare workforce management.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-10",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.023512"
    },
    {
        "index": "#199",
        "title": "A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning",
        "link": "/arxiv/2509.18120",
        "arxiv_id": "2509.18120",
        "authors": "Thanh Linh Nguyen, Quoc-Viet Pham",
        "summary": "Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or banks) to collaboratively train artificial intelligence (AI) models while preserving data privacy by keeping data local. While prior work has primarily addressed statistical heterogeneity across organizations, a critical challenge arises from economic competition, where organizations may act as market rivals, making them hesitant to participate in joint training due to potential utility loss (i.e., reduced net benefit). Furthermore, the combined effects of statistical heterogeneity and inter-organizational competition on organizational behavior and system-wide social welfare remain underexplored. In this paper, we propose CoCoGen, a coopetitive-compatible data generation framework, leveraging generative AI (GenAI) and potential game theory to model, analyze, and optimize collaborative learning under heterogeneous and competitive settings. Specifically, CoCoGen characterizes competition and statistical heterogeneity through learning performance and utility-based formulations and models each training round as a weighted potential game. We then derive GenAI-based data generation strategies that maximize social welfare. Experimental results on the Fashion-MNIST dataset reveal how varying heterogeneity and competition levels affect organizational behavior and demonstrate that CoCoGen consistently outperforms baseline methods.",
        "subjects": "Machine Learning, Artificial Intelligence, Computational Engineering, Finance, and Science, Distributed, Parallel, and Cluster Computing, Computer Science and Game Theory",
        "date": "2025-09-10",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.023729"
    },
    {
        "index": "#200",
        "title": "MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents",
        "link": "/arxiv/2509.18119",
        "arxiv_id": "2509.18119",
        "authors": "Yifan Xu, Xiao Liu, Xinghan Liu, Jiaqi Fu, Hanchen Zhang, Bohao Jing, Shudan Zhang, Yuting Wang, Wenyi Zhao, Yuxiao Dong",
        "summary": "Building general-purpose graphical user interface (GUI) agents has become increasingly promising with the progress in vision language models. However, developing effective mobile GUI agents with reinforcement learning (RL) remains challenging due to the heavy-tailed distribution of task difficulty and the inefficiency of large-scale environment sampling. We present an online agentic reinforcement learning framework MOBILERL to enhance GUI agents in mobile environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO) algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and failure curriculum filtering to adapt the model to different task difficulties. We introduce the shortest path reward adjustment strategy to reshape rewards concerning the task length in multi-turn agentic tasks. Those strategies jointly stabilize RL training, improve sample efficiency, and generate strong performance across diverse mobile apps and tasks. We apply MOBILERL to two open models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B model achieves state-of-the-art results in terms of success rates on both AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted in the AutoGLM products, and also open-sourced at https://github.com/THUDM/MobileRL.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-10",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.023972"
    },
    {
        "index": "#201",
        "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization",
        "link": "/arxiv/2509.18116",
        "arxiv_id": "2509.18116",
        "authors": "Nathan Egbuna, Saatvik Gaur, Sunishchal Dev, Ashwinee Panda, Maheep Chaudhary",
        "summary": "Test-time optimization remains impractical at scale due to prohibitive inference costs\\textemdash techniques like iterative refinement and multi-step verification can require $10$--$100\\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101\\% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-10",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.024319"
    },
    {
        "index": "#203",
        "title": "Solve it with EASE",
        "link": "/arxiv/2509.18108",
        "arxiv_id": "2509.18108",
        "authors": "Adam Viktorin, Tomas Kadavy, Jozef Kovac, Michal Pluhacek, Roman Senkerik",
        "summary": "This paper presents EASE (Effortless Algorithmic Solution Evolution), an open-source and fully modular framework for iterative algorithmic solution generation leveraging large language models (LLMs). EASE integrates generation, testing, analysis, and evaluation into a reproducible feedback loop, giving users full control over error handling, analysis, and quality assessment. Its architecture supports the orchestration of multiple LLMs in complementary roles-such as generator, analyst, and evaluator. By abstracting the complexity of prompt design and model management, EASE provides a transparent and extensible platform for researchers and practitioners to co-design algorithms and other generative solutions across diverse domains.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-09",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.024753"
    },
    {
        "index": "#204",
        "title": "BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand",
        "link": "/arxiv/2509.18105",
        "arxiv_id": "2509.18105",
        "authors": "Nachiket N. Naik, Prathamesh Dinesh Joshi, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat",
        "summary": "We study learning of continuous-time inventory dynamics under stochastic demand and quantify when structure helps or hurts forecasting of the bullwhip effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the entire right-hand side against a physics-informed Universal Differential Equation (UDE) that preserves conservation and order-up-to structure while learning a small residual policy term. Classical supply chain models explain the bullwhip through control/forecasting choices and information sharing, while recent physics-informed and neural differential equation methods blend domain constraints with learned components. It is unclear whether structural bias helps or hinders forecasting under different demand regimes. We address this by using a single-echelon testbed with three demand regimes - AR(1) (autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done on varying fractions of each trajectory, followed by evaluation of multi-step forecasts for inventory I, order rate O, and demand D. Across the structured regimes, UDE consistently generalizes better: with 90% of the training horizon, inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96 to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the flexibility of NODE is better. These trends persist as train18 ing data shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains stable but underreacts to rare spikes. Our results provide concrete guidance: enforce structure when noise is light-tailed or temporally correlated; relax structure when extreme events dominate. Beyond inventory control, the results offer guidance for hybrid modeling in scientific and engineering systems: enforce known structure when conservation laws and modest noise dominate, and relax structure to capture extremes in settings where rare events drive dynamics.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-09",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.025003"
    },
    {
        "index": "#205",
        "title": "Data Valuation and Selection in a Federated Model Marketplace",
        "link": "/arxiv/2509.18104",
        "arxiv_id": "2509.18104",
        "authors": "Wenqian Li, Youjia Yang, Ruoxi Jia, Yan Pang",
        "summary": "In the era of Artificial Intelligence (AI), marketplaces have become essential platforms for facilitating the exchange of data products to foster data sharing. Model transactions provide economic solutions in data marketplaces that enhance data reusability and ensure the traceability of data ownership. To establish trustworthy data marketplaces, Federated Learning (FL) has emerged as a promising paradigm to enable collaborative learning across siloed datasets while safeguarding data privacy. However, effective data valuation and selection from heterogeneous sources in the FL setup remain key challenges. This paper introduces a comprehensive framework centered on a Wasserstein-based estimator tailored for FL. The estimator not only predicts model performance across unseen data combinations but also reveals the compatibility between data heterogeneity and FL aggregation algorithms. To ensure privacy, we propose a distributed method to approximate Wasserstein distance without requiring access to raw data. Furthermore, we demonstrate that model performance can be reliably extrapolated under the neural scaling law, enabling effective data selection without full-scale training. Extensive experiments across diverse scenarios, such as label skew, mislabeled, and unlabeled sources, show that our approach consistently identifies high-performing data combinations, paving the way for more reliable FL-based model marketplaces.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-09",
        "category": "cs.AI",
        "crawl_time": "2025-09-25T09:45:35.025205"
    }
]