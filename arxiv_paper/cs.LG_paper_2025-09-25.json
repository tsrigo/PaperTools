[
    {
        "index": "#2",
        "title": "Optimal Robust Recourse with $L^p$-Bounded Model Change",
        "link": "/arxiv/2509.21293",
        "arxiv_id": "2509.21293",
        "authors": "Phone Kyaw, Kshitij Kayastha, Shahin Jabbari",
        "summary": "Recourse provides individuals who received undesirable labels (e.g., denied a loan) from algorithmic decision-making systems with a minimum-cost improvement suggestion to achieve the desired outcome. However, in practice, models often get updated to reflect changes in the data distribution or environment, invalidating the recourse recommendations (i.e., following the recourse will not lead to the desirable outcome). The robust recourse literature addresses this issue by providing a framework for computing recourses whose validity is resilient to slight changes in the model. However, since the optimization problem of computing robust recourse is non-convex (even for linear models), most of the current approaches do not have any theoretical guarantee on the optimality of the recourse. Recent work by Kayastha et. al. provides the first provably optimal algorithm for robust recourse with respect to generalized linear models when the model changes are measured using the $L^{\\infty}$ norm. However, using the $L^{\\infty}$ norm can lead to recourse solutions with a high price. To address this shortcoming, we consider more constrained model changes defined by the $L^p$ norm, where $p\\geq 1$ but $p\\neq \\infty$, and provide a new algorithm that provably computes the optimal robust recourse for generalized linear models. Empirically, for both linear and non-linear models, we demonstrate that our algorithm achieves a significantly lower price of recourse (up to several orders of magnitude) compared to prior work and also exhibits a better trade-off between the implementation cost of recourse and its validity. Our empirical analysis also illustrates that our approach provides more sparse recourses compared to prior work and remains resilient to post-processing approaches that guarantee feasibility.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.401858"
    },
    {
        "index": "#4",
        "title": "SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips",
        "link": "/arxiv/2509.21271",
        "arxiv_id": "2509.21271",
        "authors": "Xinyu Lian, Masahiro Tanaka, Olatunji Ruwase, Minjia Zhang",
        "summary": "The emergence of Superchips represents a significant advancement in next-generation AI hardware. These Superchips employ a tightly coupled heterogeneous architecture that integrates GPU and CPU on the same package, which offers unprecedented computational power. However, there has been scant research investigating how LLM training benefits from this new architecture. In this work, for the first time, we study LLM training solutions based on offloading for Superchips. We observe important differences between Superchips and traditional loosely-coupled GPU-CPU architecture, which necessitate revisiting prevailing assumptions about offloading. Based on that, we present SuperOffload, a Superchip-centric offloading system that simultaneously uses Hopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently. SuperOffload accomplishes this via a combination of techniques, such as adaptive weight offloading, bucketization repartitioning, Superchip-aware casting, speculative execution, and a highly optimized Adam optimizer for Grace CPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x throughput improvement compared to state-of-the-art offloading-based systems, enabling training of up to 25B model on a single Superchip while achieving high training throughput. We also extend SuperOffload with ZeRO-style data parallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of 13B model with sequence lengths up to 1 million tokens on 8 GH200 while achieving 55% MFU.",
        "subjects": "Machine Learning, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.402280"
    },
    {
        "index": "#6",
        "title": "humancompatible.train: Implementing Optimization Algorithms for Stochastically-Constrained Stochastic Optimization Problems",
        "link": "/arxiv/2509.21254",
        "arxiv_id": "2509.21254",
        "authors": "Andrii Kliachkin, Jana Lepšová, Gilles Bareilles, Jakub Mareček",
        "summary": "There has been a considerable interest in constrained training of deep neural networks (DNNs) recently for applications such as fairness and safety. Several toolkits have been proposed for this task, yet there is still no industry standard. We present humancompatible.train (https://github.com/humancompatible/train), an easily-extendable PyTorch-based Python package for training DNNs with stochastic constraints. We implement multiple previously unimplemented algorithms for stochastically constrained stochastic optimization. We demonstrate the toolkit use by comparing two algorithms on a deep learning task with fairness constraints.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.402648"
    },
    {
        "index": "#7",
        "title": "Federated Flow Matching",
        "link": "/arxiv/2509.21250",
        "arxiv_id": "2509.21250",
        "authors": "Zifan Wang, Anqi Dong, Mahmoud Selim, Michael M. Zavlanos, Karl H. Johansson",
        "summary": "Data today is decentralized, generated and stored across devices and institutions where privacy, ownership, and regulation prevent centralization. This motivates the need to train generative models directly from distributed data locally without central aggregation. In this paper, we introduce Federated Flow Matching (FFM), a framework for training flow matching models under privacy constraints. Specifically, we first examine FFM-vanilla, where each client trains locally with independent source and target couplings, preserving privacy but yielding curved flows that slow inference. We then develop FFM-LOT, which employs local optimal transport couplings to improve straightness within each client but lacks global consistency under heterogeneous data. Finally, we propose FFM-GOT, a federated strategy based on the semi-dual formulation of optimal transport, where a shared global potential function coordinates couplings across clients. Experiments on synthetic and image datasets show that FFM enables privacy-preserving training while enhancing both the flow straightness and sample quality in federated settings, with performance comparable to the centralized baseline.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.402834"
    },
    {
        "index": "#11",
        "title": "Go With The Flow: Churn-Tolerant Decentralized Training of Large Language Models",
        "link": "/arxiv/2509.21221",
        "arxiv_id": "2509.21221",
        "authors": "Nikolay Blagoev, Bart Cox, Jérémie Decouchant, Lydia Y. Chen",
        "summary": "Motivated by the emergence of large language models (LLMs) and the importance of democratizing their training, we propose GWTF, the first crash tolerant practical decentralized training framework for LLMs. Differently from existing distributed and federated training frameworks, GWTF enables the efficient collaborative training of a LLM on heterogeneous clients that volunteer their resources. In addition, GWTF addresses node churn, i.e., clients joining or leaving the system at any time, and network instabilities, i.e., network links becoming unstable or unreliable. The core of GWTF is a novel decentralized flow algorithm that finds the most effective routing that maximizes the number of microbatches trained with the lowest possible delay. We extensively evaluate GWTF on GPT-like and LLaMa-like models and compare it against the prior art. Our results indicate that GWTF reduces the training time by up to 45% in realistic and challenging scenarios that involve heterogeneous client nodes distributed over 10 different geographic locations with a high node churn rate.",
        "subjects": "Machine Learning, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.403556"
    },
    {
        "index": "#12",
        "title": "From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM",
        "link": "/arxiv/2509.21207",
        "arxiv_id": "2509.21207",
        "authors": "Olga Fink, Ismail Nejjar, Vinay Sharma, Keivan Faghih Niresi, Han Sun, Hao Dong, Chenghao Xu, Amaury Wei, Arthur Bizzi, Raffael Theiler, Yuan Tian, Leandro Von Krannichfeldt, Zhan Ma, Sergei Garmaev, Zepeng Zhang, Mengjie Zhao",
        "summary": "Prognostics and Health Management ensures the reliability, safety, and efficiency of complex engineered systems by enabling fault detection, anticipating equipment failures, and optimizing maintenance activities throughout an asset lifecycle. However, real-world PHM presents persistent challenges: sensor data is often noisy or incomplete, available labels are limited, and degradation behaviors and system interdependencies can be highly complex and nonlinear. Physics-informed machine learning has emerged as a promising approach to address these limitations by embedding physical knowledge into data-driven models. This review examines how incorporating learning and observational biases through physics-informed modeling and data strategies can guide models toward physically consistent and reliable predictions. Learning biases embed physical constraints into model training through physics-informed loss functions and governing equations, or by incorporating properties like monotonicity. Observational biases influence data selection and synthesis to ensure models capture realistic system behavior through virtual sensing for estimating unmeasured states, physics-based simulation for data augmentation, and multi-sensor fusion strategies. The review then examines how these approaches enable the transition from passive prediction to active decision-making through reinforcement learning, which allows agents to learn maintenance policies that respect physical constraints while optimizing operational objectives. This closes the loop between model-based predictions, simulation, and actual system operation, empowering adaptive decision-making. Finally, the review addresses the critical challenge of scaling PHM solutions from individual assets to fleet-wide deployment. Fast adaptation methods including meta-learning and few-shot learning are reviewed alongside domain generalization techniques ...",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.403802"
    },
    {
        "index": "#15",
        "title": "Closed-form $\\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\\ell_p$ bias",
        "link": "/arxiv/2509.21181",
        "arxiv_id": "2509.21181",
        "authors": "Shuofeng Zhang, Ard Louis",
        "summary": "For overparameterized linear regression with isotropic Gaussian design and minimum-$\\ell_p$ interpolator $p\\in(1,2]$, we give a unified, high-probability characterization for the scaling of the family of parameter norms $ \\\\{ \\lVert \\widehat{w_p} \\rVert_r \\\\}_{r \\in [1,p]} $ with sample size. We solve this basic, but unresolved question through a simple dual-ray analysis, which reveals a competition between a signal *spike* and a *bulk* of null coordinates in $X^\\top Y$, yielding closed-form predictions for (i) a data-dependent transition $n_\\star$ (the \"elbow\"), and (ii) a universal threshold $r_\\star=2(p-1)$ that separates $\\lVert \\widehat{w_p} \\rVert_r$'s which plateau from those that continue to grow with an explicit exponent. This unified solution resolves the scaling of *all* $\\ell_r$ norms within the family $r\\in [1,p]$ under $\\ell_p$-biased interpolation, and explains in one picture which norms saturate and which increase as $n$ grows. We then study diagonal linear networks (DLNs) trained by gradient descent. By calibrating the initialization scale $\\alpha$ to an effective $p_{\\mathrm{eff}}(\\alpha)$ via the DLN separable potential, we show empirically that DLNs inherit the same elbow/threshold laws, providing a predictive bridge between explicit and implicit bias. Given that many generalization proxies depend on $\\lVert \\widehat {w_p} \\rVert_r$, our results suggest that their predictive power will depend sensitively on which $l_r$ norm is used.",
        "subjects": "Machine Learning, Statistics Theory, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.404509"
    },
    {
        "index": "#16",
        "title": "Inverse Reinforcement Learning Using Just Classification and a Few Regressions",
        "link": "/arxiv/2509.21172",
        "arxiv_id": "2509.21172",
        "authors": "Lars van der Laan, Nathan Kallus, Aurélien Bibaut",
        "summary": "Inverse reinforcement learning (IRL) aims to explain observed behavior by uncovering an underlying reward. In the maximum-entropy or Gumbel-shocks-to-reward frameworks, this amounts to fitting a reward function and a soft value function that together satisfy the soft Bellman consistency condition and maximize the likelihood of observed actions. While this perspective has had enormous impact in imitation learning for robotics and understanding dynamic choices in economics, practical learning algorithms often involve delicate inner-loop optimization, repeated dynamic programming, or adversarial training, all of which complicate the use of modern, highly expressive function approximators like neural nets and boosting. We revisit softmax IRL and show that the population maximum-likelihood solution is characterized by a linear fixed-point equation involving the behavior policy. This observation reduces IRL to two off-the-shelf supervised learning problems: probabilistic classification to estimate the behavior policy, and iterative regression to solve the fixed point. The resulting method is simple and modular across function approximation classes and algorithms. We provide a precise characterization of the optimal solution, a generic oracle-based algorithm, finite-sample error bounds, and empirical results showing competitive or superior performance to MaxEnt IRL.",
        "subjects": "Machine Learning, Econometrics, Optimization and Control, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.404717"
    },
    {
        "index": "#18",
        "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say",
        "link": "/arxiv/2509.21164",
        "arxiv_id": "2509.21164",
        "authors": "Jacob Fein-Ashley, Dhruv Parikh, Rajgopal Kannan, Viktor Prasanna",
        "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain (e.g., math, code, general reasoning), motivating systems that leverage complementary strengths across models. Prior multi-LLM approaches either (i) route a query to one or a few experts and generate independently, (ii) aggregate outputs from each model via costly multi-turn exchanges, or (iii) fuse weights into a single model-typically requiring architectural homogeneity. We introduce Mixture of Thoughts (MoT), a simple method for latent-level collaboration among heterogeneous experts under a global routing scheme. For each query, a lightweight router selects top-$K$ experts and designates a primary expert; uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active (selected) peers. Pre-trained experts remain frozen; only the router and the lightweight interaction layers are trained with a novel joint training objective that improves both the expert selection and inter-expert collaboration. Across five in-distribution (ID) and three out-of-distribution (OOD) benchmarks, MoT surpasses the current routing and aggregation-based state-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further, MoT significantly outperforms the best-performing single model. It achieves this with single-pass inference, runtime comparable to routing baselines, and none of the overheads of iterative aggregation. MoT offers a simple latent-space mechanism for combining heterogeneous LLMs, a practical step toward broader multi-LLM collaboration. Our code is publicly available at https://github.com/jacobfa/mot.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.405083"
    },
    {
        "index": "#19",
        "title": "DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning",
        "link": "/arxiv/2509.21161",
        "arxiv_id": "2509.21161",
        "authors": "Giuseppe Serra, Florian Buettner",
        "summary": "Continual Learning (CL) is recently gaining increasing attention for its ability to enable a single model to learn incrementally from a sequence of new classes. In this scenario, it is important to keep consistent predictive performance across all the classes and prevent the so-called Catastrophic Forgetting (CF). However, in safety-critical applications, predictive performance alone is insufficient. Predictive models should also be able to reliably communicate their uncertainty in a calibrated manner - that is, with confidence scores aligned to the true frequencies of target events. Existing approaches in CL address calibration primarily from a data-centric perspective, relying on a single temperature shared across all tasks. Such solutions overlook task-specific differences, leading to large fluctuations in calibration error across tasks. For this reason, we argue that a more principled approach should adapt the temperature according to the distance to the current task. However, the unavailability of the task information at test time/during deployment poses a major challenge to achieve the intended objective. For this, we propose Distance-Aware Temperature Scaling (DATS), which combines prototype-based distance estimation with distance-aware calibration to infer task proximity and assign adaptive temperatures without prior task information. Through extensive empirical evaluation on both standard benchmarks and real-world, imbalanced datasets taken from the biomedical domain, our approach demonstrates to be stable, reliable and consistent in reducing calibration error across tasks compared to state-of-the-art approaches.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.405245"
    },
    {
        "index": "#21",
        "title": "CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization",
        "link": "/arxiv/2509.21150",
        "arxiv_id": "2509.21150",
        "authors": "Ruiyu Wang, Shizhao Sun, Weijian Ma, Jiang Bian",
        "summary": "Computer-Aided Design (CAD) is a foundational component of industrial prototyping, where models are defined not by raw coordinates but by construction sequences such as sketches and extrusions. This sequential structure enables both efficient prototype initialization and subsequent editing. Text-guided CAD prototyping, which unifies Text-to-CAD generation and CAD editing, has the potential to streamline the entire design pipeline. However, prior work has not explored this setting, largely because standard large language model (LLM) tokenizers decompose CAD sequences into natural-language word pieces, failing to capture primitive-level CAD semantics and hindering attention modules from modeling geometric structure. We conjecture that a multimodal tokenization strategy, aligned with CAD's primitive and structural nature, can provide more effective representations. To this end, we propose CAD-Tokenizer, a framework that represents CAD data with modality-specific tokens using a sequence-based VQ-VAE with primitive-level pooling and constrained decoding. This design produces compact, primitive-aware representations that align with CAD's structural nature. Applied to unified text-guided CAD prototyping, CAD-Tokenizer significantly improves instruction following and generation quality, achieving better quantitative and qualitative performance over both general-purpose LLMs and task-specific baselines.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.405586"
    },
    {
        "index": "#24",
        "title": "EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense",
        "link": "/arxiv/2509.21129",
        "arxiv_id": "2509.21129",
        "authors": "Wei Huang, De-Tian Chu, Lin-Yuan Bai, Wei Kang, Hai-Tao Zhang, Bo Li, Zhi-Mo Han, Jing Ge, Hai-Feng Lin",
        "summary": "Modern email spam and phishing attacks have evolved far beyond keyword blacklists or simple heuristics. Adversaries now craft multi-modal campaigns that combine natural-language text with obfuscated URLs, forged headers, and malicious attachments, adapting their strategies within days to bypass filters. Traditional spam detection systems, which rely on static rules or single-modality models, struggle to integrate heterogeneous signals or to continuously adapt, leading to rapid performance degradation. We propose EvoMail, a self-evolving cognitive agent framework for robust detection of spam and phishing. EvoMail first constructs a unified heterogeneous email graph that fuses textual content, metadata (headers, senders, domains), and embedded resources (URLs, attachments). A Cognitive Graph Neural Network enhanced by a Large Language Model (LLM) performs context-aware reasoning across these sources to identify coordinated spam campaigns. Most critically, EvoMail engages in an adversarial self-evolution loop: a ''red-team'' agent generates novel evasion tactics -- such as character obfuscation or AI-generated phishing text -- while the ''blue-team'' detector learns from failures, compresses experiences into a memory module, and reuses them for future reasoning. Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam, SpamAssassin, and TREC) and synthetic adversarial variants demonstrate that EvoMail consistently outperforms state-of-the-art baselines in detection accuracy, adaptability to evolving spam tactics, and interpretability of reasoning traces. These results highlight EvoMail's potential as a resilient and explainable defense framework against next-generation spam and phishing threats.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.406287"
    },
    {
        "index": "#29",
        "title": "Structure-Attribute Transformations with Markov Chain Boost Graph Domain Adaptation",
        "link": "/arxiv/2509.21059",
        "arxiv_id": "2509.21059",
        "authors": "Zhen Liu, Yongtao Zhang, Shaobo Ren, Yuxin You",
        "summary": "Graph domain adaptation has gained significant attention in label-scarce scenarios across different graph domains. Traditional approaches to graph domain adaptation primarily focus on transforming node attributes over raw graph structures and aligning the distributions of the transformed node features across networks. However, these methods often struggle with the underlying structural heterogeneity between distinct graph domains, which leads to suboptimal distribution alignment. To address this limitation, we propose Structure-Attribute Transformation with Markov Chain (SATMC), a novel framework that sequentially aligns distributions across networks via both graph structure and attribute transformations. To mitigate the negative influence of domain-private information and further enhance the model's generalization, SATMC introduces a private domain information reduction mechanism and an empirical Wasserstein distance. Theoretical proofs suggest that SATMC can achieve a tighter error bound for cross-network node classification compared to existing graph domain adaptation methods. Extensive experiments on nine pairs of publicly available cross-domain datasets show that SATMC outperforms state-of-the-art methods in the cross-network node classification task. The code is available at https://github.com/GiantZhangYT/SATMC.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.407263"
    },
    {
        "index": "#30",
        "title": "SPREAD: Sampling-based Pareto front Refinement via Efficient Adaptive Diffusion",
        "link": "/arxiv/2509.21058",
        "arxiv_id": "2509.21058",
        "authors": "Sedjro Salomon Hotegni, Sebastian Peitz",
        "summary": "Developing efficient multi-objective optimization methods to compute the Pareto set of optimal compromises between conflicting objectives remains a key challenge, especially for large-scale and expensive problems. To bridge this gap, we introduce SPREAD, a generative framework based on Denoising Diffusion Probabilistic Models (DDPMs). SPREAD first learns a conditional diffusion process over points sampled from the decision space and then, at each reverse diffusion step, refines candidates via a sampling scheme that uses an adaptive multiple gradient descent-inspired update for fast convergence alongside a Gaussian RBF-based repulsion term for diversity. Empirical results on multi-objective optimization benchmarks, including offline and Bayesian surrogate-based settings, show that SPREAD matches or exceeds leading baselines in efficiency, scalability, and Pareto front coverage.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.407425"
    },
    {
        "index": "#32",
        "title": "Physics of Learning: A Lagrangian perspective to different learning paradigms",
        "link": "/arxiv/2509.21049",
        "arxiv_id": "2509.21049",
        "authors": "Siyuan Guo, Bernhard Schölkopf",
        "summary": "We study the problem of building an efficient learning system. Efficient learning processes information in the least time, i.e., building a system that reaches a desired error threshold with the least number of observations. Building upon least action principles from physics, we derive classic learning algorithms, Bellman's optimality equation in reinforcement learning, and the Adam optimizer in generative models from first principles, i.e., the Learning $\\textit{Lagrangian}$. We postulate that learning searches for stationary paths in the Lagrangian, and learning algorithms are derivable by seeking the stationary trajectories.",
        "subjects": "Machine Learning, Neural and Evolutionary Computing",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.407809"
    },
    {
        "index": "#34",
        "title": "FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction",
        "link": "/arxiv/2509.21029",
        "arxiv_id": "2509.21029",
        "authors": "Runqi Lin, Alasdair Paren, Suqin Yuan, Muyang Li, Philip Torr, Adel Bibi, Tongliang Liu",
        "summary": "The integration of new modalities enhances the capabilities of multimodal large language models (MLLMs) but also introduces additional vulnerabilities. In particular, simple visual jailbreaking attacks can manipulate open-source MLLMs more readily than sophisticated textual attacks. However, these underdeveloped attacks exhibit extremely limited cross-model transferability, failing to reliably identify vulnerabilities in closed-source MLLMs. In this work, we analyse the loss landscape of these jailbreaking attacks and find that the generated attacks tend to reside in high-sharpness regions, whose effectiveness is highly sensitive to even minor parameter changes during transfer. To further explain the high-sharpness localisations, we analyse their feature representations in both the intermediate layers and the spectral domain, revealing an improper reliance on narrow layer representations and semantically poor frequency components. Building on this, we propose a Feature Over-Reliance CorrEction (FORCE) method, which guides the attack to explore broader feasible regions across layer features and rescales the influence of frequency features according to their semantic content. By eliminating non-generalizable reliance on both layer and spectral features, our method discovers flattened feasible regions for visual jailbreaking attacks, thereby improving cross-model transferability. Extensive experiments demonstrate that our approach effectively facilitates visual red-teaming evaluations against closed-source MLLMs.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.408415"
    },
    {
        "index": "#35",
        "title": "Actor-Critic without Actor",
        "link": "/arxiv/2509.21022",
        "arxiv_id": "2509.21022",
        "authors": "Donghyeon Ki, Hee-Jun Ahn, Kyungyoon Kim, Byung-Jun Lee",
        "summary": "Actor-critic methods constitute a central paradigm in reinforcement learning (RL), coupling policy evaluation with policy improvement. While effective across many domains, these methods rely on separate actor and critic networks, which makes training vulnerable to architectural decisions and hyperparameter tuning. Such complexity limits their scalability in settings that require large function approximators. Recently, diffusion models have recently been proposed as expressive policies that capture multi-modal behaviors and improve exploration, but they introduce additional design choices and computational burdens, hindering efficient deployment. We introduce Actor-Critic without Actor (ACA), a lightweight framework that eliminates the explicit actor network and instead generates actions directly from the gradient field of a noise-level critic. This design removes the algorithmic and computational overhead of actor training while keeping policy improvement tightly aligned with the critic's latest value estimates. Moreover, ACA retains the ability to capture diverse, multi-modal behaviors without relying on diffusion-based actors, combining simplicity with expressiveness. Through extensive experiments on standard online RL benchmarks,ACA achieves more favorable learning curves and competitive performance compared to both standard actor-critic and state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.408659"
    },
    {
        "index": "#41",
        "title": "MAIFormer: Multi-Agent Inverted Transformer for Flight Trajectory Prediction",
        "link": "/arxiv/2509.21004",
        "arxiv_id": "2509.21004",
        "authors": "Seokbin Yoon, Keumjin Lee",
        "summary": "Flight trajectory prediction for multiple aircraft is essential and provides critical insights into how aircraft navigate within current air traffic flows. However, predicting multi-agent flight trajectories is inherently challenging. One of the major difficulties is modeling both the individual aircraft behaviors over time and the complex interactions between flights. Generating explainable prediction outcomes is also a challenge. Therefore, we propose a Multi-Agent Inverted Transformer, MAIFormer, as a novel neural architecture that predicts multi-agent flight trajectories. The proposed framework features two key attention modules: (i) masked multivariate attention, which captures spatio-temporal patterns of individual aircraft, and (ii) agent attention, which models the social patterns among multiple agents in complex air traffic scenes. We evaluated MAIFormer using a real-world automatic dependent surveillance-broadcast flight trajectory dataset from the terminal airspace of Incheon International Airport in South Korea. The experimental results show that MAIFormer achieves the best performance across multiple metrics and outperforms other methods. In addition, MAIFormer produces prediction outcomes that are interpretable from a human perspective, which improves both the transparency of the model and its practical utility in air traffic control.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.409802"
    },
    {
        "index": "#43",
        "title": "Feature Augmentation of GNNs for ILPs: Local Uniqueness Suffices",
        "link": "/arxiv/2509.21000",
        "arxiv_id": "2509.21000",
        "authors": "Qingyu Han, Qian Li, Linxin Yang, Qian Chen, Qingjiang Shi, Ruoyu Sun",
        "summary": "Integer Linear Programs (ILPs) are central to real-world optimizations but notoriously difficult to solve. Learning to Optimize (L2O) has emerged as a promising paradigm, with Graph Neural Networks (GNNs) serving as the standard backbone. However, standard anonymous GNNs are limited in expressiveness for ILPs, and the common enhancement of augmenting nodes with globally unique identifiers (UIDs) typically introduces spurious correlations that severely harm generalization. To address this tradeoff, we propose a parsimonious Local-UID scheme based on d-hop uniqueness coloring, which ensures identifiers are unique only within each node's d-hop neighborhood. Building on this scheme, we introduce ColorGNN, which incorporates color information via color-conditioned embeddings, and ColorUID, a lightweight feature-level variant. We prove that for d-layer networks, Local-UIDs achieve the expressive power of Global-UIDs while offering stronger generalization. Extensive experiments show that our approach (i) yields substantial gains on three ILP benchmarks, (ii) exhibits strong OOD generalization on linear programming datasets, and (iii) further improves a general graph-level task when paired with a state-of-the-art method.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.410284"
    },
    {
        "index": "#45",
        "title": "Learning Ising Models under Hard Constraints using One Sample",
        "link": "/arxiv/2509.20993",
        "arxiv_id": "2509.20993",
        "authors": "Rohan Chauhan, Ioannis Panageas",
        "summary": "We consider the problem of estimating inverse temperature parameter $\\beta$ of an $n$-dimensional truncated Ising model using a single sample. Given a graph $G = (V,E)$ with $n$ vertices, a truncated Ising model is a probability distribution over the $n$-dimensional hypercube $\\{-1,1\\}^n$ where each configuration $\\mathbf{\\sigma}$ is constrained to lie in a truncation set $S \\subseteq \\{-1,1\\}^n$ and has probability $\\Pr(\\mathbf{\\sigma}) \\propto \\exp(\\beta\\mathbf{\\sigma}^\\top A\\mathbf{\\sigma})$ with $A$ being the adjacency matrix of $G$. We adopt the recent setting of [Galanis et al. SODA'24], where the truncation set $S$ can be expressed as the set of satisfying assignments of a $k$-SAT formula. Given a single sample $\\mathbf{\\sigma}$ from a truncated Ising model, with inverse parameter $\\beta^*$, underlying graph $G$ of bounded degree $\\Delta$ and $S$ being expressed as the set of satisfying assignments of a $k$-SAT formula, we design in nearly $O(n)$ time an estimator $\\hat{\\beta}$ that is $O(\\Delta^3/\\sqrt{n})$-consistent with the true parameter $\\beta^*$ for $k \\gtrsim \\log(d^2k)\\Delta^3.$ Our estimator is based on the maximization of the pseudolikelihood, a notion that has received extensive analysis for various probabilistic models without [Chatterjee, Annals of Statistics '07] or with truncation [Galanis et al. SODA '24]. Our approach generalizes recent techniques from [Daskalakis et al. STOC '19, Galanis et al. SODA '24], to confront the more challenging setting of the truncated Ising model.",
        "subjects": "Machine Learning, Data Structures and Algorithms, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.410674"
    },
    {
        "index": "#46",
        "title": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference",
        "link": "/arxiv/2509.20979",
        "arxiv_id": "2509.20979",
        "authors": "Peng Chen, Jiaji Zhang, Hailiang Zhao, Yirong Zhang, Jiahong Yu, Xueyan Tang, Yixuan Wang, Hao Li, Jianping Zou, Gang Xiong, Kingsum Chow, Shuibing He, Shuiguang Deng",
        "summary": "In modern GPU inference, cache efficiency remains a major bottleneck. In recommendation models, embedding hit rates largely determine throughput, while in large language models, KV-cache misses substantially increase time-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often struggle under structured access patterns. Learning-based approaches are promising, but in practice face two major limitations: they degrade sharply when predictions are inaccurate, or they gain little even with accurate predictions due to conservative designs. Some also incur high overhead, further limiting practicality. We present \\textsc{LCR}, a practical framework for learning-based GPU caching that delivers performance gains while ensuring robustness and efficiency. Its core algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned predictions and dynamically adapts to prediction accuracy through online error estimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal performance. With inaccurate predictions, it degrades gracefully to near-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between empirical progress and theoretical advances in learning-based caching. Experiments show that \\textsc{LCR} delivers consistent gains under realistic conditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\% and reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference systems. Even under poor predictions, its performance remains stable, demonstrating practical robustness.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.410910"
    },
    {
        "index": "#50",
        "title": "Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning",
        "link": "/arxiv/2509.20968",
        "arxiv_id": "2509.20968",
        "authors": "Zhengyuan Shi, Jingxin Wang, Wentao Jiang, Chengyu Ma, Ziyang Zheng, Zhufei Chu, Weikang Qian, Qiang Xu",
        "summary": "Multiview learning on Boolean circuits holds immense promise, as different graph-based representations offer complementary structural and semantic information. However, the vast structural heterogeneity between views, such as an And-Inverter Graph (AIG) versus an XOR-Majority Graph (XMG), poses a critical barrier to effective fusion, especially for self-supervised techniques like masked modeling. Naively applying such methods fails, as the cross-view context is perceived as noise. Our key insight is that functional alignment is a necessary precondition to unlock the power of multiview self-supervision. We introduce MixGate, a framework built on a principled training curriculum that first teaches the model a shared, function-aware representation space via an Equivalence Alignment Loss. Only then do we introduce a multiview masked modeling objective, which can now leverage the aligned views as a rich, complementary signal. Extensive experiments, including a crucial ablation study, demonstrate that our alignment-first strategy transforms masked modeling from an ineffective technique into a powerful performance driver.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.411673"
    },
    {
        "index": "#52",
        "title": "Decoupled-Value Attention for Prior-Data Fitted Networks: GP Inference for Physical Equations",
        "link": "/arxiv/2509.20950",
        "arxiv_id": "2509.20950",
        "authors": "Kaustubh Sharma, Simardeep Singh, Parikshit Pareek",
        "summary": "Prior-data fitted networks (PFNs) are a promising alternative to time-consuming Gaussian Process (GP) inference for creating fast surrogates of physical systems. PFN reduces the computational burden of GP-training by replacing Bayesian inference in GP with a single forward pass of a learned prediction model. However, with standard Transformer attention, PFNs show limited effectiveness on high-dimensional regression tasks. We introduce Decoupled-Value Attention (DVA)-- motivated by the GP property that the function space is fully characterized by the kernel over inputs and the predictive mean is a weighted sum of training targets. DVA computes similarities from inputs only and propagates labels solely through values. Thus, the proposed DVA mirrors the Gaussian-process update while remaining kernel-free. We demonstrate that the crucial factor for scaling PFNs is the attention rule rather than the architecture itself. Specifically, our results demonstrate that (a) localized attention consistently reduces out-of-sample validation loss in PFNs across different dimensional settings, with validation loss reduced by more than 50% in five- and ten-dimensional cases, and (b) the role of attention is more decisive than the choice of backbone architecture, showing that CNN-based PFNs can perform at par with their Transformer-based counterparts. The proposed PFNs provide 64-dimensional power flow equation approximations with a mean absolute error of the order of 1E-3, while being over 80x faster than exact GP inference.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.412011"
    },
    {
        "index": "#53",
        "title": "Why Attention Fails: The Degeneration of Transformers into MLPs in Time Series Forecasting",
        "link": "/arxiv/2509.20942",
        "arxiv_id": "2509.20942",
        "authors": "Zida Liang, Jiayi Zhu, Weiqiang Sun",
        "summary": "Transformer-based architectures achieved high performance in natural language processing and computer vision, yet many studies have shown that they have not demonstrated a clear advantage in time series forecasting and even underperform simple linear baselines in some cases. However, most of these studies have not thoroughly explored the reasons behind the failure of transformers. To better understand time-series transformers(TST), we designed a series of experiments, progressively modifying transformers into MLPs to investigate the impact of the attention mechanism. Surprisingly, transformer blocks often degenerate into simple MLPs in existing time-series transformers. We designed a interpretable dataset to investigate the reasons behind the failure of the attention mechanism and revealed that the attention mechanism is not working in the expected way. We theoretically analyzed the reasons behind this phenomenon, demonstrating that the current embedding methods fail to allow transformers to function in a well-structured latent space, and further analyzed the deeper underlying causes of the failure of embedding.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.412240"
    },
    {
        "index": "#54",
        "title": "GenFacts-Generative Counterfactual Explanations for Multi-Variate Time Series",
        "link": "/arxiv/2509.20936",
        "arxiv_id": "2509.20936",
        "authors": "Sarah Seifi, Anass Ibrahimi, Tobias Sukianto, Cecilia Carbonelli, Lorenzo Servadei, Robert Wille",
        "summary": "Counterfactual explanations aim to enhance model transparency by showing how inputs can be minimally altered to change predictions. For multivariate time series, existing methods often generate counterfactuals that are invalid, implausible, or unintuitive. We introduce GenFacts, a generative framework based on a class-discriminative variational autoencoder. It integrates contrastive and classification-consistency objectives, prototype-based initialization, and realism-constrained optimization. We evaluate GenFacts on radar gesture data as an industrial use case and handwritten letter trajectories as an intuitive benchmark. Across both datasets, GenFacts outperforms state-of-the-art baselines in plausibility (+18.7%) and achieves the highest interpretability scores in a human study. These results highlight that plausibility and user-centered interpretability, rather than sparsity alone, are key to actionable counterfactuals in time series data.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.412445"
    },
    {
        "index": "#55",
        "title": "Energy saving in off-road vehicles using leakage compensation technique",
        "link": "/arxiv/2509.20926",
        "arxiv_id": "2509.20926",
        "authors": "Gyan Wrat, J. Das",
        "summary": "The article focuses on enhancing the energy efficiency of linear actuators used in heavy earth moving equipment, particularly in the booms ofexcavation equipment. Two hydraulic circuits are compared in terms of energy efficiency, with one using a conventional proportional directionalcontrol valve (PDCV) and the other using an innovative solution of proportional flow control valve (PFCV) with artificial leakage between thetwo ends of the actuator. The PFCV reduces energy loss in the form of heat by bypassing the extra flow from the pump during position control,unlike the PDCV that uses a pressure relief valve. The hydraulic circuit using PFCV is found to be 8.5% more energy efficient than theconventional circuit using PDCV. The article also discusses the position control of the actuator, which is achieved using a PID controller tuned by a fuzzy controller. Thesimulation of the hydraulic circuit is carried out using MATLAB/Simulink, and the results are compared with experiments. Overall, the proposedapproach could lead to significant improvements in the energy efficiency of linear actuators used in heavy earth moving equipment, therebyreducing their environmental impact and operating costs.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.412617"
    },
    {
        "index": "#57",
        "title": "Deterministic Discrete Denoising",
        "link": "/arxiv/2509.20896",
        "arxiv_id": "2509.20896",
        "authors": "Hideyuki Suzuki, Hiroshi Yamashita",
        "summary": "We propose a deterministic denoising algorithm for discrete-state diffusion models based on Markov chains. The generative reverse process is derandomized by introducing a variant of the herding algorithm with weakly chaotic dynamics, which induces deterministic discrete state transitions. Our approach is a direct replacement for the stochastic denoising process, requiring neither retraining nor continuous state embeddings. We demonstrate consistent improvements in both efficiency and sample quality on text and image generation tasks. Thus, this simple derandomization approach is expected to enhance the significance of discrete diffusion in generative modeling. Furthermore, our results reveal that deterministic reverse processes, well established in continuous diffusion, can also be effective in discrete state spaces.",
        "subjects": "Machine Learning, Chaotic Dynamics",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.413026"
    },
    {
        "index": "#59",
        "title": "Distribution-Controlled Client Selection to Improve Federated Learning Strategies",
        "link": "/arxiv/2509.20877",
        "arxiv_id": "2509.20877",
        "authors": "Christoph Düsing, Philipp Cimiano",
        "summary": "Federated learning (FL) is a distributed learning paradigm that allows multiple clients to jointly train a shared model while maintaining data privacy. Despite its great potential for domains with strict data privacy requirements, the presence of data imbalance among clients is a thread to the success of FL, as it causes the performance of the shared model to decrease. To address this, various studies have proposed enhancements to existing FL strategies, particularly through client selection methods that mitigate the detrimental effects of data imbalance. In this paper, we propose an extension to existing FL strategies, which selects active clients that best align the current label distribution with one of two target distributions, namely a balanced distribution or the federations combined label distribution. Subsequently, we empirically verify the improvements through our distribution-controlled client selection on three common FL strategies and two datasets. Our results show that while aligning the label distribution with a balanced distribution yields the greatest improvements facing local imbalance, alignment with the federation's combined label distribution is superior for global imbalance.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.413374"
    },
    {
        "index": "#64",
        "title": "Causal Time Series Generation via Diffusion Models",
        "link": "/arxiv/2509.20846",
        "arxiv_id": "2509.20846",
        "authors": "Yutong Xia, Chang Xu, Yuxuan Liang, Qingsong Wen, Roger Zimmermann, Jiang Bian",
        "summary": "Time series generation (TSG) synthesizes realistic sequences and has achieved remarkable success. Among TSG, conditional models generate sequences given observed covariates, however, such models learn observational correlations without considering unobserved confounding. In this work, we propose a causal perspective on conditional TSG and introduce causal time series generation as a new TSG task family, formalized within Pearl's causal ladder, extending beyond observational generation to include interventional and counterfactual settings. To instantiate these tasks, we develop CaTSG, a unified diffusion-based framework with backdoor-adjusted guidance that causally steers sampling toward desired interventions and individual counterfactuals while preserving observational fidelity. Specifically, our method derives causal score functions via backdoor adjustment and the abduction-action-prediction procedure, thus enabling principled support for all three levels of TSG. Extensive experiments on both synthetic and real-world datasets show that CaTSG achieves superior fidelity and also supporting interventional and counterfactual generation that existing baselines cannot handle. Overall, we propose the causal TSG family and instantiate it with CaTSG, providing an initial proof-of-concept and opening a promising direction toward more reliable simulation under interventions and counterfactual generation.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.414405"
    },
    {
        "index": "#66",
        "title": "Shaping Initial State Prevents Modality Competition in Multi-modal Fusion: A Two-stage Scheduling Framework via Fast Partial Information Decomposition",
        "link": "/arxiv/2509.20840",
        "arxiv_id": "2509.20840",
        "authors": "Jiaqi Tang, Yinsong Xu, Yang Liu, Qingchao Chen",
        "summary": "Multi-modal fusion often suffers from modality competition during joint training, where one modality dominates the learning process, leaving others under-optimized. Overlooking the critical impact of the model's initial state, most existing methods address this issue during the joint learning stage. In this study, we introduce a two-stage training framework to shape the initial states through unimodal training before the joint training. First, we propose the concept of Effective Competitive Strength (ECS) to quantify a modality's competitive strength. Our theoretical analysis further reveals that properly shaping the initial ECS by unimodal training achieves a provably tighter error bound. However, ECS is computationally intractable in deep neural networks. To bridge this gap, we develop a framework comprising two core components: a fine-grained computable diagnostic metric and an asynchronous training controller. For the metric, we first prove that mutual information(MI) is a principled proxy for ECS. Considering MI is induced by per-modality marginals and thus treats each modality in isolation, we further propose FastPID, a computationally efficient and differentiable solver for partial information decomposition, which decomposes the joint distribution's information into fine-grained measurements: modality-specific uniqueness, redundancy, and synergy. Guided by these measurements, our asynchronous controller dynamically balances modalities by monitoring uniqueness and locates the ideal initial state to start joint training by tracking peak synergy. Experiments on diverse benchmarks demonstrate that our method achieves state-of-the-art performance. Our work establishes that shaping the pre-fusion models' initial state is a powerful strategy that eases competition before it starts, reliably unlocking synergistic multi-modal fusion.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.414814"
    },
    {
        "index": "#67",
        "title": "Explaining Grokking and Information Bottleneck through Neural Collapse Emergence",
        "link": "/arxiv/2509.20829",
        "arxiv_id": "2509.20829",
        "authors": "Keitaro Sakamoto, Issei Sato",
        "summary": "The training dynamics of deep neural networks often defy expectations, even as these models form the foundation of modern machine learning. Two prominent examples are grokking, where test performance improves abruptly long after the training loss has plateaued, and the information bottleneck principle, where models progressively discard input information irrelevant to the prediction task as training proceeds. However, the mechanisms underlying these phenomena and their relations remain poorly understood. In this work, we present a unified explanation of such late-phase phenomena through the lens of neural collapse, which characterizes the geometry of learned representations. We show that the contraction of population within-class variance is a key factor underlying both grokking and information bottleneck, and relate this measure to the neural collapse measure defined on the training set. By analyzing the dynamics of neural collapse, we show that distinct time scales between fitting the training set and the progression of neural collapse account for the behavior of the late-phase phenomena. Finally, we validate our theoretical findings on multiple datasets and architectures.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.414976"
    },
    {
        "index": "#69",
        "title": "T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models",
        "link": "/arxiv/2509.20822",
        "arxiv_id": "2509.20822",
        "authors": "Hwa Hui Tew, Junn Yong Loo, Yee-Fan Tan, Xinyu Tang, Hernando Ombao, Fuad Noman, Raphael C. -W. Phan, Chee-Ming Ting",
        "summary": "Functional Magnetic Resonance Imaging (fMRI) is an advanced neuroimaging method that enables in-depth analysis of brain activity by measuring dynamic changes in the blood oxygenation level-dependent (BOLD) signals. However, the resource-intensive nature of fMRI data acquisition limits the availability of high-fidelity samples required for data-driven brain analysis models. While modern generative models can synthesize fMRI data, they often underperform because they overlook the complex non-stationarity and nonlinear BOLD dynamics. To address these challenges, we introduce T2I-Diff, an fMRI generation framework that leverages time-frequency representation of BOLD signals and classifier-free denoising diffusion. Specifically, our framework first converts BOLD signals into windowed spectrograms via a time-dependent Fourier transform, capturing both the underlying temporal dynamics and spectral evolution. Subsequently, a classifier-free diffusion model is trained to generate class-conditioned frequency spectrograms, which are then reverted to BOLD signals via inverse Fourier transforms. Finally, we validate the efficacy of our approach by demonstrating improved accuracy and generalization in downstream fMRI-based brain network classification.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.415387"
    },
    {
        "index": "#71",
        "title": "Aligning Inductive Bias for Data-Efficient Generalization in State Space Models",
        "link": "/arxiv/2509.20789",
        "arxiv_id": "2509.20789",
        "authors": "Qiyu Chen, Guozhang Chen",
        "summary": "The remarkable success of large-scale models is fundamentally tied to scaling laws, yet the finite nature of high-quality data presents a looming challenge. One of the next frontiers in modeling is data efficiency: the ability to learn more from less. A model's inductive bias is a critical lever for this, but foundational sequence models like State Space Models (SSMs) rely on a fixed bias. This fixed prior is sample-inefficient when a task's underlying structure does not match. In this work, we introduce a principled framework to solve this problem. We first formalize the inductive bias of linear time-invariant SSMs through an SSM-induced kernel, mathematically and empirically proving its spectrum is directly governed by the model's frequency response. Further, we propose a method of Task-Dependent Initialization (TDI): power spectrum matching, a fast and efficient method that aligns the model's inductive bias with the task's spectral characteristics before large-scale training. Our experiments on a diverse set of real-world benchmarks show that TDI significantly improves generalization and sample efficiency, particularly in low-data regimes. This work provides a theoretical and practical tool to create more data-efficient models, a crucial step towards sustainable scaling.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.415812"
    },
    {
        "index": "#72",
        "title": "LiLAW: Lightweight Learnable Adaptive Weighting to Meta-Learn Sample Difficulty and Improve Noisy Training",
        "link": "/arxiv/2509.20786",
        "arxiv_id": "2509.20786",
        "authors": "Abhishek Moturu, Anna Goldenberg, Babak Taati",
        "summary": "Training deep neural networks in the presence of noisy labels and data heterogeneity is a major challenge. We introduce Lightweight Learnable Adaptive Weighting (LiLAW), a novel method that dynamically adjusts the loss weight of each training sample based on its evolving difficulty level, categorized as easy, moderate, or hard. Using only three learnable parameters, LiLAW adaptively prioritizes informative samples throughout training by updating these weights using a single mini-batch gradient descent step on the validation set after each training mini-batch, without requiring excessive hyperparameter tuning or a clean validation set. Extensive experiments across multiple general and medical imaging datasets, noise levels and types, loss functions, and architectures with and without pretraining demonstrate that LiLAW consistently enhances performance, even in high-noise environments. It is effective without heavy reliance on data augmentation or advanced regularization, highlighting its practicality. It offers a computationally efficient solution to boost model generalization and robustness in any neural network training setup.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.416002"
    },
    {
        "index": "#74",
        "title": "Sig2Model: A Boosting-Driven Model for Updatable Learned Indexes",
        "link": "/arxiv/2509.20781",
        "arxiv_id": "2509.20781",
        "authors": "Alireza Heidari, Amirhossein Ahmad, Wei Zhang, Ying Xiong",
        "summary": "Learned Indexes (LIs) represent a paradigm shift from traditional index structures by employing machine learning models to approximate the cumulative distribution function (CDF) of sorted data. While LIs achieve remarkable efficiency for static datasets, their performance degrades under dynamic updates: maintaining the CDF invariant (sum of F(k) equals 1) requires global model retraining, which blocks queries and limits the queries-per-second (QPS) metric. Current approaches fail to address these retraining costs effectively, rendering them unsuitable for real-world workloads with frequent updates. In this paper, we present Sig2Model, an efficient and adaptive learned index that minimizes retraining cost through three key techniques: (1) a sigmoid boosting approximation technique that dynamically adjusts the index model by approximating update-induced shifts in data distribution with localized sigmoid functions while preserving bounded error guarantees and deferring full retraining; (2) proactive update training via Gaussian mixture models (GMMs) that identifies high-update-probability regions for strategic placeholder allocation to speed up updates; and (3) a neural joint optimization framework that continuously refines both the sigmoid ensemble and GMM parameters via gradient-based learning. We evaluate Sig2Model against state-of-the-art updatable learned indexes on real-world and synthetic workloads, and show that Sig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS, and uses up to 1000x less memory.",
        "subjects": "Machine Learning, Databases, Performance",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.416456"
    },
    {
        "index": "#76",
        "title": "The Impact of Audio Watermarking on Audio Anti-Spoofing Countermeasures",
        "link": "/arxiv/2509.20736",
        "arxiv_id": "2509.20736",
        "authors": "Zhenshan Zhang, Xueping Zhang, Yechen Wang, Liwei Jin, Ming Li",
        "summary": "This paper presents the first study on the impact of audio watermarking on spoofing countermeasures. While anti-spoofing systems are essential for securing speech-based applications, the influence of widely used audio watermarking, originally designed for copyright protection, remains largely unexplored. We construct watermark-augmented training and evaluation datasets, named the Watermark-Spoofing dataset, by applying diverse handcrafted and neural watermarking methods to existing anti-spoofing datasets. Experiments show that watermarking consistently degrades anti-spoofing performance, with higher watermark density correlating with higher Equal Error Rates (EERs). To mitigate this, we propose the Knowledge-Preserving Watermark Learning (KPWL) framework, enabling models to adapt to watermark-induced shifts while preserving their original-domain spoofing detection capability. These findings reveal audio watermarking as a previously overlooked domain shift and establish the first benchmark for developing watermark-resilient anti-spoofing systems. All related protocols are publicly available at https://github.com/Alphawarheads/Watermark_Spoofing.git",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.416816"
    },
    {
        "index": "#77",
        "title": "Scaling Laws are Redundancy Laws",
        "link": "/arxiv/2509.20721",
        "arxiv_id": "2509.20721",
        "authors": "Yuda Bi, Vince D Calhoun",
        "summary": "Scaling laws, a defining feature of deep learning, reveal a striking power-law improvement in model performance with increasing dataset and model size. Yet, their mathematical origins, especially the scaling exponent, have remained elusive. In this work, we show that scaling laws can be formally explained as redundancy laws. Using kernel regression, we show that a polynomial tail in the data covariance spectrum yields an excess risk power law with exponent alpha = 2s / (2s + 1/beta), where beta controls the spectral tail and 1/beta measures redundancy. This reveals that the learning curve's slope is not universal but depends on data redundancy, with steeper spectra accelerating returns to scale. We establish the law's universality across boundedly invertible transformations, multi-modal mixtures, finite-width approximations, and Transformer architectures in both linearized (NTK) and feature-learning regimes. This work delivers the first rigorous mathematical explanation of scaling laws as finite-sample redundancy laws, unifying empirical observations with theoretical foundations.",
        "subjects": "Machine Learning, Statistics Theory, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.417001"
    },
    {
        "index": "#78",
        "title": "A Genetic Algorithm for Navigating Synthesizable Molecular Spaces",
        "link": "/arxiv/2509.20719",
        "arxiv_id": "2509.20719",
        "authors": "Alston Lo, Connor W. Coley, Wojciech Matusik",
        "summary": "Inspired by the effectiveness of genetic algorithms and the importance of synthesizability in molecular design, we present SynGA, a simple genetic algorithm that operates directly over synthesis routes. Our method features custom crossover and mutation operators that explicitly constrain it to synthesizable molecular space. By modifying the fitness function, we demonstrate the effectiveness of SynGA on a variety of design tasks, including synthesizable analog search and sample-efficient property optimization, for both 2D and 3D objectives. Furthermore, by coupling SynGA with a machine learning-based filter that focuses the building block set, we boost SynGA to state-of-the-art performance. For property optimization, this manifests as a model-based variant SynGBO, which employs SynGA and block filtering in the inner loop of Bayesian optimization. Since SynGA is lightweight and enforces synthesizability by construction, our hope is that SynGA can not only serve as a strong standalone baseline but also as a versatile module that can be incorporated into larger synthesis-aware workflows in the future.",
        "subjects": "Machine Learning, Quantitative Methods",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.417181"
    },
    {
        "index": "#83",
        "title": "Theoretical Bounds for Stable In-Context Learning",
        "link": "/arxiv/2509.20677",
        "arxiv_id": "2509.20677",
        "authors": "Tongxi Wang, Zhuoyang Xia",
        "summary": "In-context learning (ICL) is flexible but its reliability is highly sensitive to prompt length. This paper establishes a non-asymptotic lower bound that links the minimal number of demonstrations to ICL stability under fixed high-dimensional sub-Gaussian representations. The bound gives explicit sufficient conditions in terms of spectral properties of the covariance, providing a computable criterion for practice. Building on this analysis, we propose a two-stage observable estimator with a one-shot calibration that produces practitioner-ready prompt-length estimates without distributional priors. Experiments across diverse datasets, encoders, and generators show close alignment between the predicted thresholds and empirical knee-points, with the theory acting as a conservative but reliable upper bound; the calibrated variant further tightens this gap. These results connect spectral coverage to stable ICL, bridge theory and deployment, and improve the interpretability and reliability of large-scale prompting in realistic finite-sample regimes.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.418330"
    },
    {
        "index": "#84",
        "title": "Guiding Application Users via Estimation of Computational Resources for Massively Parallel Chemistry Computations",
        "link": "/arxiv/2509.20667",
        "arxiv_id": "2509.20667",
        "authors": "Tanzila Tabassum, Omer Subasi, Ajay Panyala, Epiya Ebiapia, Gerald Baumgartner, Erdal Mutlu, P., Sadayappan, Karol Kowalski",
        "summary": "In this work, we develop machine learning (ML) based strategies to predict resources (costs) required for massively parallel chemistry computations, such as coupled-cluster methods, to guide application users before they commit to running expensive experiments on a supercomputer. By predicting application execution time, we determine the optimal runtime parameter values such as number of nodes and tile sizes. Two key questions of interest to users are addressed. The first is the shortest-time question, where the user is interested in knowing the parameter configurations (number of nodes and tile sizes) to achieve the shortest execution time for a given problem size and a target supercomputer. The second is the cheapest-run question in which the user is interested in minimizing resource usage, i.e., finding the number of nodes and tile size that minimizes the number of node-hours for a given problem size. We evaluate a rich family of ML models and strategies, developed based on the collections of runtime parameter values for the CCSD (Coupled Cluster with Singles and Doubles) application executed on the Department of Energy (DOE) Frontier and Aurora supercomputers. Our experiments show that when predicting the total execution time of a CCSD iteration, a Gradient Boosting (GB) ML model achieves a Mean Absolute Percentage Error (MAPE) of 0.023 and 0.073 for Aurora and Frontier, respectively. In the case where it is expensive to run experiments just to collect data points, we show that active learning can achieve a MAPE of about 0.2 with just around 450 experiments collected from Aurora and Frontier.",
        "subjects": "Machine Learning, Computational Engineering, Finance, and Science, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.418559"
    },
    {
        "index": "#85",
        "title": "Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration",
        "link": "/arxiv/2509.20648",
        "arxiv_id": "2509.20648",
        "authors": "Yiyuan Pan, Zhe Liu, Hesheng Wang",
        "summary": "Autonomous exploration in complex multi-agent reinforcement learning (MARL) with sparse rewards critically depends on providing agents with effective intrinsic motivation. While artificial curiosity offers a powerful self-supervised signal, it often confuses environmental stochasticity with meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform novelty bias, treating all unexpected observations equally. However, peer behavior novelty, which encode latent task dynamics, are often overlooked, resulting in suboptimal exploration in decentralized, communication-free MARL settings. To this end, inspired by how human children adaptively calibrate their own exploratory behaviors via observing peers, we propose a novel approach to enhance multi-agent exploration. We introduce CERMIC, a principled framework that empowers agents to robustly filter noisy surprise signals and guide exploration by dynamically calibrating their intrinsic curiosity with inferred multi-agent context. Additionally, CERMIC generates theoretically-grounded intrinsic rewards, encouraging agents to explore state transitions with high information gain. We evaluate CERMIC on benchmark suites including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that exploration with CERMIC significantly outperforms SoTA algorithms in sparse-reward environments.",
        "subjects": "Machine Learning, Robotics",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.418783"
    },
    {
        "index": "#86",
        "title": "Investigating Modality Contribution in Audio LLMs for Music",
        "link": "/arxiv/2509.20641",
        "arxiv_id": "2509.20641",
        "authors": "Giovana Morais, Magdalena Fuentes",
        "summary": "Audio Large Language Models (Audio LLMs) enable human-like conversation about music, yet it is unclear if they are truly listening to the audio or just using textual reasoning, as recent benchmarks suggest. This paper investigates this issue by quantifying the contribution of each modality to a model's output. We adapt the MM-SHAP framework, a performance-agnostic score based on Shapley values that quantifies the relative contribution of each modality to a model's prediction. We evaluate two models on the MuChoMusic benchmark and find that the model with higher accuracy relies more on text to answer questions, but further inspection shows that even if the overall audio contribution is low, models can successfully localize key sound events, suggesting that audio is not entirely ignored. Our study is the first application of MM-SHAP to Audio LLMs and we hope it will serve as a foundational step for future research in explainable AI and audio.",
        "subjects": "Machine Learning, Sound",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.418968"
    },
    {
        "index": "#88",
        "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning",
        "link": "/arxiv/2509.20616",
        "arxiv_id": "2509.20616",
        "authors": "Hanjiang Hu, Changliu Liu, Na Li, Yebin Wang",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in higher multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks with over 30 steps. We also theoretically and empirically validate the strong cross-task generalizability that the models trained on complex tasks can lead to the successful completion of all simpler subtasks.",
        "subjects": "Machine Learning, Systems and Control",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.419365"
    },
    {
        "index": "#89",
        "title": "Latent Twins",
        "link": "/arxiv/2509.20615",
        "arxiv_id": "2509.20615",
        "authors": "Matthias Chung, Deepanshu Verma, Max Collins, Amit N. Subrahmanya, Varuni Katti Sastry, Vishwas Rao",
        "summary": "Over the past decade, scientific machine learning has transformed the development of mathematical and computational frameworks for analyzing, modeling, and predicting complex systems. From inverse problems to numerical PDEs, dynamical systems, and model reduction, these advances have pushed the boundaries of what can be simulated. Yet they have often progressed in parallel, with representation learning and algorithmic solution methods evolving largely as separate pipelines. With \\emph{Latent Twins}, we propose a unifying mathematical framework that creates a hidden surrogate in latent space for the underlying equations. Whereas digital twins mirror physical systems in the digital world, Latent Twins mirror mathematical systems in a learned latent space governed by operators. Through this lens, classical modeling, inversion, model reduction, and operator approximation all emerge as special cases of a single principle. We establish the fundamental approximation properties of Latent Twins for both ODEs and PDEs and demonstrate the framework across three representative settings: (i) canonical ODEs, capturing diverse dynamical regimes; (ii) a PDE benchmark using the shallow-water equations, contrasting Latent Twin simulations with DeepONet and forecasts with a 4D-Var baseline; and (iii) a challenging real-data geopotential reanalysis dataset, reconstructing and forecasting from sparse, noisy observations. Latent Twins provide a compact, interpretable surrogate for solution operators that evaluate across arbitrary time gaps in a single-shot, while remaining compatible with scientific pipelines such as assimilation, control, and uncertainty quantification. Looking forward, this framework offers scalable, theory-grounded surrogates that bridge data-driven representation learning and classical scientific modeling across disciplines.",
        "subjects": "Machine Learning, Numerical Analysis",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.419561"
    },
    {
        "index": "#90",
        "title": "Policy Compatible Skill Incremental Learning via Lazy Learning Interface",
        "link": "/arxiv/2509.20612",
        "arxiv_id": "2509.20612",
        "authors": "Daehee Lee, Dongsu Lee, TaeYoon Kwack, Wonje Choi, Honguk Woo",
        "summary": "Skill Incremental Learning (SIL) is the process by which an embodied agent expands and refines its skill set over time by leveraging experience gained through interaction with its environment or by the integration of additional data. SIL facilitates efficient acquisition of hierarchical policies grounded in reusable skills for downstream tasks. However, as the skill repertoire evolves, it can disrupt compatibility with existing skill-based policies, limiting their reusability and generalization. In this work, we propose SIL-C, a novel framework that ensures skill-policy compatibility, allowing improvements in incrementally learned skills to enhance the performance of downstream policies without requiring policy re-training or structural adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to dynamically align the subtask space referenced by policies with the skill space decoded into agent behaviors. This enables each subtask, derived from the policy's decomposition of a complex task, to be executed by selecting an appropriate skill based on trajectory distribution similarity. We evaluate SIL-C across diverse SIL scenarios and demonstrate that it maintains compatibility between evolving skills and downstream policies while ensuring efficiency throughout the learning process.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.419790"
    },
    {
        "index": "#92",
        "title": "Function Spaces Without Kernels: Learning Compact Hilbert Space Representations",
        "link": "/arxiv/2509.20605",
        "arxiv_id": "2509.20605",
        "authors": "Su Ann Low, Quentin Rommel, Kevin S. Miller, Adam J. Thorpe, Ufuk Topcu",
        "summary": "Function encoders are a recent technique that learn neural network basis functions to form compact, adaptive representations of Hilbert spaces of functions. We show that function encoders provide a principled connection to feature learning and kernel methods by defining a kernel through an inner product of the learned feature map. This kernel-theoretic perspective explains their ability to scale independently of dataset size while adapting to the intrinsic structure of data, and it enables kernel-style analysis of neural models. Building on this foundation, we develop two training algorithms that learn compact bases: a progressive training approach that constructively grows bases, and a train-then-prune approach that offers a computationally efficient alternative after training. Both approaches use principles from PCA to reveal the intrinsic dimension of the learned space. In parallel, we derive finite-sample generalization bounds using Rademacher complexity and PAC-Bayes techniques, providing inference time guarantees. We validate our approach on a polynomial benchmark with a known intrinsic dimension, and on nonlinear dynamical systems including a Van der Pol oscillator and a two-body orbital model, demonstrating that the same accuracy can be achieved with substantially fewer basis functions. This work suggests a path toward neural predictors with kernel-level guarantees, enabling adaptable models that are both efficient and principled at scale.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.420223"
    },
    {
        "index": "#93",
        "title": "Explicit and Effectively Symmetric Schemes for Neural SDEs",
        "link": "/arxiv/2509.20599",
        "arxiv_id": "2509.20599",
        "authors": "Daniil Shmelev, Cristopher Salvi",
        "summary": "Backpropagation through (neural) SDE solvers is traditionally approached in two ways: discretise-then-optimise, which offers accurate gradients but incurs prohibitive memory costs due to storing the full computational graph (even when mitigated by checkpointing); and optimise-then-discretise, which achieves constant memory cost by solving an auxiliary backward SDE, but suffers from slower evaluation and gradient approximation errors. Algebraically reversible solvers promise both memory efficiency and gradient accuracy, yet existing methods such as the Reversible Heun scheme are often unstable under complex models and large step sizes. We address these limitations by introducing a novel class of stable, near-reversible Runge--Kutta schemes for neural SDEs. These Explicit and Effectively Symmetric (EES) schemes retain the benefits of reversible solvers while overcoming their instability, enabling memory-efficient training without severe restrictions on step size or model complexity. Through numerical experiments, we demonstrate the superior stability and reliability of our schemes, establishing them as a practical foundation for scalable and accurate training of neural SDEs.",
        "subjects": "Machine Learning, Numerical Analysis",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.420413"
    },
    {
        "index": "#94",
        "title": "TSKAN: Interpretable Machine Learning for QoE modeling over Time Series Data",
        "link": "/arxiv/2509.20595",
        "arxiv_id": "2509.20595",
        "authors": "Kamal Singh, Priyanka Rawat, Sami Marouani, Baptiste Jeudy",
        "summary": "Quality of Experience (QoE) modeling is crucial for optimizing video streaming services to capture the complex relationships between different features and user experience. We propose a novel approach to QoE modeling in video streaming applications using interpretable Machine Learning (ML) techniques over raw time series data. Unlike traditional black-box approaches, our method combines Kolmogorov-Arnold Networks (KANs) as an interpretable readout on top of compact frequency-domain features, allowing us to capture temporal information while retaining a transparent and explainable model. We evaluate our method on popular datasets and demonstrate its enhanced accuracy in QoE prediction, while offering transparency and interpretability.",
        "subjects": "Machine Learning, Networking and Internet Architecture",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.420601"
    },
    {
        "index": "#95",
        "title": "Learning Greens Operators through Hierarchical Neural Networks Inspired by the Fast Multipole Method",
        "link": "/arxiv/2509.20591",
        "arxiv_id": "2509.20591",
        "authors": "Emilio McAllister Fognini, Marta M. Betcke, Ben T. Cox",
        "summary": "The Fast Multipole Method (FMM) is an efficient numerical algorithm for computation of long-ranged forces in $N$-body problems within gravitational and electrostatic fields. This method utilizes multipole expansions of the Green's function inherent to the underlying dynamical systems. Despite its widespread application in physics and engineering, the integration of FMM with modern machine learning architectures remains underexplored. In this work, we propose a novel neural network architecture, the Neural FMM, that integrates the information flow of the FMM into a hierarchical machine learning framework for learning the Green's operator of an Elliptic PDE. Our Neural FMM architecture leverages a hierarchical computation flow of the FMM method to split up the local and far-field interactions and efficiently learn their respective representations.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.420770"
    },
    {
        "index": "#96",
        "title": "The Sensitivity of Variational Bayesian Neural Network Performance to Hyperparameters",
        "link": "/arxiv/2509.20574",
        "arxiv_id": "2509.20574",
        "authors": "Scott Koermer, Natalie Klein",
        "summary": "In scientific applications, predictive modeling is often of limited use without accurate uncertainty quantification (UQ) to indicate when a model may be extrapolating or when more data needs to be collected. Bayesian Neural Networks (BNNs) produce predictive uncertainty by propagating uncertainty in neural network (NN) weights and offer the promise of obtaining not only an accurate predictive model but also accurate UQ. However, in practice, obtaining accurate UQ with BNNs is difficult due in part to the approximations used for practical model training and in part to the need to choose a suitable set of hyperparameters; these hyperparameters outnumber those needed for traditional NNs and often have opaque effects on the results. We aim to shed light on the effects of hyperparameter choices for BNNs by performing a global sensitivity analysis of BNN performance under varying hyperparameter settings. Our results indicate that many of the hyperparameters interact with each other to affect both predictive accuracy and UQ. For improved usage of BNNs in real-world applications, we suggest that global sensitivity analysis, or related methods such as Bayesian optimization, should be used to aid in dimensionality reduction and selection of hyperparameters to ensure accurate UQ in BNNs.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.420939"
    },
    {
        "index": "#98",
        "title": "Generalizable Diabetes Risk Stratification via Hybrid Machine Learning Models",
        "link": "/arxiv/2509.20565",
        "arxiv_id": "2509.20565",
        "authors": "Athar Parvez, Muhammad Jawad Mufti",
        "summary": "Background/Purpose: Diabetes affects over 537 million people worldwide and is projected to reach 783 million by 2045. Early risk stratification can benefit from machine learning. We compare two hybrid classifiers and assess their generalizability on an external cohort. Methods: Two hybrids were built: (i) XGBoost + Random Forest (XGB-RF) and (ii) Support Vector Machine + Logistic Regression (SVM-LR). A leakage-safe, standardized pipeline (encoding, imputation, min-max scaling; SMOTE on training folds only; probability calibration for SVM) was fit on the primary dataset and frozen. Evaluation prioritized threshold-independent discrimination (AUROC/AUPRC) and calibration (Brier, slope/intercept). External validation used the PIMA cohort (N=768) with the frozen pipeline; any thresholded metrics on PIMA were computed at the default rule tau = 0.5. Results: On the primary dataset (PR baseline = 0.50), XGB-RF achieved AUROC ~0.995 and AUPRC ~0.998, outperforming SVM-LR (AUROC ~0.978; AUPRC ~0.947). On PIMA (PR baseline ~0.349), XGB-RF retained strong performance (AUROC ~0.990; AUPRC ~0.959); SVM-LR was lower (AUROC ~0.963; AUPRC ~0.875). Thresholded metrics on PIMA at tau = 0.5 were XGB-RF (Accuracy 0.960; Precision 0.941; Recall 0.944; F1 0.942) and SVM-LR (Accuracy 0.900; Precision 0.855; Recall 0.858; F1 0.857). Conclusions: Across internal and external cohorts, XGB-RF consistently dominated SVM-LR and exhibited smaller external attenuation on ROC/PR with acceptable calibration. These results support gradient-boosting-based hybridization as a robust, transferable approach for diabetes risk stratification and motivate prospective, multi-site validation with deployment-time threshold selection based on clinical trade-offs.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.421294"
    },
    {
        "index": "#100",
        "title": "MDBench: Benchmarking Data-Driven Methods for Model Discovery",
        "link": "/arxiv/2509.20529",
        "arxiv_id": "2509.20529",
        "authors": "Amirmohammad Ziaei Bideh, Aleksandra Georgievska, Jonathan Gryak",
        "summary": "Model discovery aims to uncover governing differential equations of dynamical systems directly from experimental data. Benchmarking such methods is essential for tracking progress and understanding trade-offs in the field. While prior efforts have focused mostly on identifying single equations, typically framed as symbolic regression, there remains a lack of comprehensive benchmarks for discovering dynamical models. To address this, we introduce MDBench, an open-source benchmarking framework for evaluating model discovery methods on dynamical systems. MDBench assesses 12 algorithms on 14 partial differential equations (PDEs) and 63 ordinary differential equations (ODEs) under varying levels of noise. Evaluation metrics include derivative prediction accuracy, model complexity, and equation fidelity. We also introduce seven challenging PDE systems from fluid dynamics and thermodynamics, revealing key limitations in current methods. Our findings illustrate that linear methods and genetic programming methods achieve the lowest prediction error for PDEs and ODEs, respectively. Moreover, linear models are in general more robust against noise. MDBench accelerates the advancement of model discovery methods by offering a rigorous, extensible benchmarking framework and a rich, diverse collection of dynamical system datasets, enabling systematic evaluation, comparison, and improvement of equation accuracy and robustness.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.421632"
    },
    {
        "index": "#101",
        "title": "A Recovery Theory for Diffusion Priors: Deterministic Analysis of the Implicit Prior Algorithm",
        "link": "/arxiv/2509.20511",
        "arxiv_id": "2509.20511",
        "authors": "Oscar Leong, Yann Traonmilin",
        "summary": "Recovering high-dimensional signals from corrupted measurements is a central challenge in inverse problems. Recent advances in generative diffusion models have shown remarkable empirical success in providing strong data-driven priors, but rigorous recovery guarantees remain limited. In this work, we develop a theoretical framework for analyzing deterministic diffusion-based algorithms for inverse problems, focusing on a deterministic version of the algorithm proposed by Kadkhodaie \\& Simoncelli \\cite{kadkhodaie2021stochastic}. First, we show that when the underlying data distribution concentrates on a low-dimensional model set, the associated noise-convolved scores can be interpreted as time-varying projections onto such a set. This leads to interpreting previous algorithms using diffusion priors for inverse problems as generalized projected gradient descent methods with varying projections. When the sensing matrix satisfies a restricted isometry property over the model set, we can derive quantitative convergence rates that depend explicitly on the noise schedule. We apply our framework to two instructive data distributions: uniform distributions over low-dimensional compact, convex sets and low-rank Gaussian mixture models. In the latter setting, we can establish global convergence guarantees despite the nonconvexity of the underlying model set.",
        "subjects": "Machine Learning, Signal Processing, Optimization and Control",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.421807"
    },
    {
        "index": "#103",
        "title": "Auto-Regressive U-Net for Full-Field Prediction of Shrinkage-Induced Damage in Concrete",
        "link": "/arxiv/2509.20507",
        "arxiv_id": "2509.20507",
        "authors": "Liya Gaynutdinova, Petr Havlásek, Ondřej Rokoš, Fleur Hendriks, Martin Doškář",
        "summary": "This paper introduces a deep learning approach for predicting time-dependent full-field damage in concrete. The study uses an auto-regressive U-Net model to predict the evolution of the scalar damage field in a unit cell given microstructural geometry and evolution of an imposed shrinkage profile. By sequentially using the predicted damage output as input for subsequent predictions, the model facilitates the continuous assessment of damage progression. Complementarily, a convolutional neural network (CNN) utilises the damage estimations to forecast key mechanical properties, including observed shrinkage and residual stiffness. The proposed dual-network architecture demonstrates high computational efficiency and robust predictive performance on the synthesised datasets. The approach reduces the computational load traditionally associated with full-field damage evaluations and is used to gain insights into the relationship between aggregate properties, such as shape, size, and distribution, and the effective shrinkage and reduction in stiffness. Ultimately, this can help to optimize concrete mix designs, leading to improved durability and reduced internal damage.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.422212"
    },
    {
        "index": "#104",
        "title": "Myosotis: structured computation for attention like layer",
        "link": "/arxiv/2509.20503",
        "arxiv_id": "2509.20503",
        "authors": "Evgenii Egorov, Hanno Ackermann, Markus Nagel, Hong Cai",
        "summary": "Attention layers apply a sequence-to-sequence mapping whose parameters depend on the pairwise interactions of the input elements. However, without any structural assumptions, memory and compute scale quadratically with the sequence length. The two main ways to mitigate this are to introduce sparsity by ignoring a sufficient amount of pairwise interactions or to introduce recurrent dependence along them, as SSM does. Although both approaches are reasonable, they both have disadvantages. We propose a novel algorithm that combines the advantages of both concepts. Our idea is based on the efficient inversion of tree-structured matrices.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.422403"
    },
    {
        "index": "#107",
        "title": "Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations",
        "link": "/arxiv/2509.20478",
        "arxiv_id": "2509.20478",
        "authors": "Vivek Myers, Bill Chunyuan Zheng, Benjamin Eysenbach, Sergey Levine",
        "summary": "Approaches for goal-conditioned reinforcement learning (GCRL) often use learned state representations to extract goal-reaching policies. Two frameworks for representation structure have yielded particularly effective GCRL algorithms: (1) *contrastive representations*, in which methods learn \"successor features\" with a contrastive objective that performs inference over future outcomes, and (2) *temporal distances*, which link the (quasimetric) distance in representation space to the transit time from states to goals. We propose an approach that unifies these two frameworks, using the structure of a quasimetric representation space (triangle inequality) with the right additional constraints to learn successor representations that enable optimal goal-reaching. Unlike past work, our approach is able to exploit a **quasimetric** distance parameterization to learn **optimal** goal-reaching distances, even with **suboptimal** data and in **stochastic** environments. This gives us the best of both worlds: we retain the stability and long-horizon capabilities of Monte Carlo contrastive RL methods, while getting the free stitching capabilities of quasimetric network parameterizations. On existing offline GCRL benchmarks, our representation learning objective improves performance on stitching tasks where methods based on contrastive learning struggle, and on noisy, high-dimensional environments where methods based on quasimetric networks struggle.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.423022"
    },
    {
        "index": "#108",
        "title": "Efficiently Attacking Memorization Scores",
        "link": "/arxiv/2509.20463",
        "arxiv_id": "2509.20463",
        "authors": "Tue Do, Varun Chandrasekaran, Daniel Alabi",
        "summary": "Influence estimation tools -- such as memorization scores -- are widely used to understand model behavior, attribute training data, and inform dataset curation. However, recent applications in data valuation and responsible machine learning raise the question: can these scores themselves be adversarially manipulated? In this work, we present a systematic study of the feasibility of attacking memorization-based influence estimators. We characterize attacks for producing highly memorized samples as highly sensitive queries in the regime where a trained algorithm is accurate. Our attack (calculating the pseudoinverse of the input) is practical, requiring only black-box access to model outputs and incur modest computational overhead. We empirically validate our attack across a wide suite of image classification tasks, showing that even state-of-the-art proxies are vulnerable to targeted score manipulations. In addition, we provide a theoretical analysis of the stability of memorization scores under adversarial perturbations, revealing conditions under which influence estimates are inherently fragile. Our findings highlight critical vulnerabilities in influence-based attribution and suggest the need for robust defenses. All code can be found at https://anonymous.4open.science/r/MemAttack-5413/",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.423196"
    },
    {
        "index": "#109",
        "title": "Bridging Privacy and Utility: Synthesizing anonymized EEG with constraining utility functions",
        "link": "/arxiv/2509.20454",
        "arxiv_id": "2509.20454",
        "authors": "Kay Fuhrmeister, Arne Pelzer, Fabian Radke, Julia Lechinger, Mahzad Gharleghi, Thomas Köllmer, Insa Wolf",
        "summary": "Electroencephalography (EEG) is widely used for recording brain activity and has seen numerous applications in machine learning, such as detecting sleep stages and neurological disorders. Several studies have successfully shown the potential of EEG data for re-identification and leakage of other personal information. Therefore, the increasing availability of EEG consumer devices raises concerns about user privacy, motivating us to investigate how to safeguard this sensitive data while retaining its utility for EEG applications. To address this challenge, we propose a transformer-based autoencoder to create EEG data that does not allow for subject re-identification while still retaining its utility for specific machine learning tasks. We apply our approach to automatic sleep staging by evaluating the re-identification and utility potential of EEG data before and after anonymization. The results show that the re-identifiability of the EEG signal can be substantially reduced while preserving its utility for machine learning.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.423401"
    },
    {
        "index": "#110",
        "title": "mloz: A Highly Efficient Machine Learning-Based Ozone Parameterization for Climate Sensitivity Simulations",
        "link": "/arxiv/2509.20422",
        "arxiv_id": "2509.20422",
        "authors": "Yiling Ma, Nathan Luke Abraham, Stefan Versick, Roland Ruhnke, Andrea Schneidereit, Ulrike Niemeier, Felix Back, Peter Braesicke, Peer Nowack",
        "summary": "Atmospheric ozone is a crucial absorber of solar radiation and an important greenhouse gas. However, most climate models participating in the Coupled Model Intercomparison Project (CMIP) still lack an interactive representation of ozone due to the high computational costs of atmospheric chemistry schemes. Here, we introduce a machine learning parameterization (mloz) to interactively model daily ozone variability and trends across the troposphere and stratosphere in standard climate sensitivity simulations, including two-way interactions of ozone with the Quasi-Biennial Oscillation. We demonstrate its high fidelity on decadal timescales and its flexible use online across two different climate models -- the UK Earth System Model (UKESM) and the German ICOsahedral Nonhydrostatic (ICON) model. With atmospheric temperature profile information as the only input, mloz produces stable ozone predictions around 31 times faster than the chemistry scheme in UKESM, contributing less than 4 percent of the respective total climate model runtimes. In particular, we also demonstrate its transferability to different climate models without chemistry schemes by transferring the parameterization from UKESM to ICON. This highlights the potential for widespread adoption in CMIP-level climate models that lack interactive chemistry for future climate change assessments, particularly when focusing on climate sensitivity simulations, where ozone trends and variability are known to significantly modulate atmospheric feedback processes.",
        "subjects": "Machine Learning, Atmospheric and Oceanic Physics",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.423614"
    },
    {
        "index": "#111",
        "title": "FastEagle: Cascaded Drafting for Accelerating Speculative Decoding",
        "link": "/arxiv/2509.20416",
        "arxiv_id": "2509.20416",
        "authors": "Haiduo Huang, Jiangcheng Song, Wenzhe Zhao, Pengju Ren",
        "summary": "Speculative decoding accelerates generation by drafting candidates and verifying them in parallel, yet state-of-the-art drafters (e.g., EAGLE) still require N sequential passes to propose N tokens. We present FastEagle, a non-autoregressive cascaded drafter that emits an entire draft in a single forward pass. FastEagle replaces temporal steps with a lightweight layer cascade and trains with layer-wise supervision to mitigate error accumulation. Coupled with a constrained draft tree that preserves lossless verification cost, FastEagle delivers substantial wall-clock speedups over strong autoregressive drafters while maintaining competitive acceptance behavior. Across multiple LLMs (Vicuna-13B, LLaMA-Instruct 3.x, and DeepSeek-R1-Distill-LLaMA) and tasks (MT-Bench, HumanEval, GSM8K, CNN/DM, Alpaca), FastEagle consistently outperforms EAGLE-3 in speedup under both greedy and stochastic decoding, with comparable average acceptance lengths. These results indicate that removing sequential dependencies in drafting is a practical path toward lossless LLM inference acceleration.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.423791"
    },
    {
        "index": "#112",
        "title": "A Theory of Multi-Agent Generative Flow Networks",
        "link": "/arxiv/2509.20408",
        "arxiv_id": "2509.20408",
        "authors": "Leo Maxime Brunswic, Haozhi Wang, Shuang Luo, Jianye Hao, Amir Rasouli, Yinchuan Li",
        "summary": "Generative flow networks utilize a flow-matching loss to learn a stochastic policy for generating objects from a sequence of actions, such that the probability of generating a pattern can be proportional to the corresponding given reward. However, a theoretical framework for multi-agent generative flow networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose the theory framework of MA-GFlowNets, which can be applied to multiple agents to generate objects collaboratively through a series of joint actions. We further propose four algorithms: a centralized flow network for centralized training of MA-GFlowNets, an independent flow network for decentralized execution, a joint flow network for achieving centralized training with decentralized execution, and its updated conditional version. Joint Flow training is based on a local-global principle allowing to train a collection of (local) GFN as a unique (global) GFN. This principle provides a loss of reasonable complexity and allows to leverage usual results on GFN to provide theoretical guarantees that the independent policies generate samples with probability proportional to the reward function. Experimental results demonstrate the superiority of the proposed framework compared to reinforcement learning and MCMC-based methods.",
        "subjects": "Machine Learning, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.423986"
    },
    {
        "index": "#114",
        "title": "Maxout Polytopes",
        "link": "/arxiv/2509.21286",
        "arxiv_id": "2509.21286",
        "authors": "Andrei Balakin, Shelby Cox, Georg Loho, Bernd Sturmfels",
        "summary": "Maxout polytopes are defined by feedforward neural networks with maxout activation function and non-negative weights after the first layer. We characterize the parameter spaces and extremal f-vectors of maxout polytopes for shallow networks, and we study the separating hypersurfaces which arise when a layer is added to the network. We also show that maxout polytopes are cubical for generic networks without bottlenecks.",
        "subjects": "Combinatorics, Discrete Mathematics, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.424455"
    },
    {
        "index": "#115",
        "title": "Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds",
        "link": "/arxiv/2509.21281",
        "arxiv_id": "2509.21281",
        "authors": "Luis Augenstein, Noémie Jaquier, Tamim Asfour, Leonel Rozo",
        "summary": "Human-like motion generation for robots often draws inspiration from biomechanical studies, which often categorize complex human motions into hierarchical taxonomies. While these taxonomies provide rich structural information about how movements relate to one another, this information is frequently overlooked in motion generation models, leading to a disconnect between the generated motions and their underlying hierarchical structure. This paper introduces the \\ac{gphdm}, a novel approach that learns latent representations preserving both the hierarchical structure of motions and their temporal dynamics to ensure physical consistency. Our model achieves this by extending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) to the hyperbolic manifold and integrating it with taxonomy-aware inductive biases. Building on this geometry- and taxonomy-aware frameworks, we propose three novel mechanisms for generating motions that are both taxonomically-structured and physically-consistent: two probabilistic recursive approaches and a method based on pullback-metric geodesics. Experiments on generating realistic motion sequences on the hand grasping taxonomy show that the proposed GPHDM faithfully encodes the underlying taxonomy and temporal dynamics, and generates novel physically-consistent trajectories.",
        "subjects": "Robotics, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.424645"
    },
    {
        "index": "#118",
        "title": "Response to Promises and Pitfalls of Deep Kernel Learning",
        "link": "/arxiv/2509.21228",
        "arxiv_id": "2509.21228",
        "authors": "Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing",
        "summary": "This note responds to \"Promises and Pitfalls of Deep Kernel Learning\" (Ober et al., 2021). The marginal likelihood of a Gaussian process can be compartmentalized into a data fit term and a complexity penalty. Ober et al. (2021) shows that if a kernel can be multiplied by a signal variance coefficient, then reparametrizing and substituting in the maximized value of this parameter sets a reparametrized data fit term to a fixed value. They use this finding to argue that the complexity penalty, a log determinant of the kernel matrix, then dominates in determining the other values of kernel hyperparameters, which can lead to data overcorrelation. By contrast, we show that the reparametrization in fact introduces another data-fit term which influences all other kernel hyperparameters. Thus, a balance between data fit and complexity still plays a significant role in determining kernel hyperparameters.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.425293"
    },
    {
        "index": "#120",
        "title": "Data-driven Neural Networks for Windkessel Parameter Calibration",
        "link": "/arxiv/2509.21206",
        "arxiv_id": "2509.21206",
        "authors": "Benedikt Hoock, Tobias Köppl",
        "summary": "In this work, we propose a novel method for calibrating Windkessel (WK) parameters in a dimensionally reduced 1D-0D coupled blood flow model. To this end, we design a data-driven neural network (NN)trained on simulated blood pressures in the left brachial artery. Once trained, the NN emulates the pressure pulse waves across the entire simulated domain, i.e., over time, space and varying WK parameters, with negligible error and computational effort. To calibrate the WK parameters on a measured pulse wave, the NN is extended by dummy neurons and retrained only on these. The main objective of this work is to assess the effectiveness of the method in various scenarios -- particularly, when the exact measurement location is unknown or the data are affected by noise.",
        "subjects": "Tissues and Organs, Machine Learning, Numerical Analysis, Optimization and Control, Quantitative Methods",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.425658"
    },
    {
        "index": "#121",
        "title": "IntSR: An Integrated Generative Framework for Search and Recommendation",
        "link": "/arxiv/2509.21179",
        "arxiv_id": "2509.21179",
        "authors": "Huimin Yan, Longfei Xu, Junjie Sun, Ni Ou, Wei Luo, Xing Tan, Ran Cheng, Kaikui Liu, Xiangxiang Chu",
        "summary": "Generative recommendation has emerged as a promising paradigm, demonstrating remarkable results in both academic benchmarks and industrial applications. However, existing systems predominantly focus on unifying retrieval and ranking while neglecting the integration of search and recommendation (S&R) tasks. What makes search and recommendation different is how queries are formed: search uses explicit user requests, while recommendation relies on implicit user interests. As for retrieval versus ranking, the distinction comes down to whether the queries are the target items themselves. Recognizing the query as central element, we propose IntSR, an integrated generative framework for S&R. IntSR integrates these disparate tasks using distinct query modalities. It also addresses the increased computational complexity associated with integrated S&R behaviors and the erroneous pattern learning introduced by a dynamically changing corpus. IntSR has been successfully deployed across various scenarios in Amap, leading to substantial improvements in digital asset's GMV(+3.02%), POI recommendation's CTR(+2.76%), and travel mode suggestion's ACC(+5.13%).",
        "subjects": "Information Retrieval, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.425871"
    },
    {
        "index": "#122",
        "title": "Breaking the curse of dimensionality for linear rules: optimal predictors over the ellipsoid",
        "link": "/arxiv/2509.21174",
        "arxiv_id": "2509.21174",
        "authors": "Alexis Ayme, Bruno Loureiro",
        "summary": "In this work, we address the following question: What minimal structural assumptions are needed to prevent the degradation of statistical learning bounds with increasing dimensionality? We investigate this question in the classical statistical setting of signal estimation from $n$ independent linear observations $Y_i = X_i^{\\top}\\theta + \\epsilon_i$. Our focus is on the generalization properties of a broad family of predictors that can be expressed as linear combinations of the training labels, $f(X) = \\sum_{i=1}^{n} l_{i}(X) Y_i$. This class -- commonly referred to as linear prediction rules -- encompasses a wide range of popular parametric and non-parametric estimators, including ridge regression, gradient descent, and kernel methods. Our contributions are twofold. First, we derive non-asymptotic upper and lower bounds on the generalization error for this class under the assumption that the Bayes predictor $\\theta$ lies in an ellipsoid. Second, we establish a lower bound for the subclass of rotationally invariant linear prediction rules when the Bayes predictor is fixed. Our analysis highlights two fundamental contributions to the risk: (a) a variance-like term that captures the intrinsic dimensionality of the data; (b) the noiseless error, a term that arises specifically in the high-dimensional regime. These findings shed light on the role of structural assumptions in mitigating the curse of dimensionality.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.426038"
    },
    {
        "index": "#124",
        "title": "WISER: Segmenting watermarked region - an epidemic change-point perspective",
        "link": "/arxiv/2509.21160",
        "arxiv_id": "2509.21160",
        "authors": "Soham Bonnerjee, Sayar Karmakar, Subhrajyoty Roy",
        "summary": "With the increasing popularity of large language models, concerns over content authenticity have led to the development of myriad watermarking schemes. These schemes can be used to detect a machine-generated text via an appropriate key, while being imperceptible to readers with no such keys. The corresponding detection mechanisms usually take the form of statistical hypothesis testing for the existence of watermarks, spurring extensive research in this direction. However, the finer-grained problem of identifying which segments of a mixed-source text are actually watermarked, is much less explored; the existing approaches either lack scalability or theoretical guarantees robust to paraphrase and post-editing. In this work, we introduce a unique perspective to such watermark segmentation problems through the lens of epidemic change-points. By highlighting the similarities as well as differences of these two problems, we motivate and propose WISER: a novel, computationally efficient, watermark segmentation algorithm. We theoretically validate our algorithm by deriving finite sample error-bounds, and establishing its consistency in detecting multiple watermarked segments in a single text. Complementing these theoretical results, our extensive numerical experiments show that WISER outperforms state-of-the-art baseline methods, both in terms of computational speed as well as accuracy, on various benchmark datasets embedded with diverse watermarking schemes. Our theoretical and empirical findings establish WISER as an effective tool for watermark localization in most settings. It also shows how insights from a classical statistical problem can lead to a theoretically valid and computationally efficient solution of a modern and pertinent problem.",
        "subjects": "Machine Learning, Machine Learning, Methodology",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.426489"
    },
    {
        "index": "#127",
        "title": "Physics Informed Neural Networks for design optimisation of diamond particle detectors for charged particle fast-tracking at high luminosity hadron colliders",
        "link": "/arxiv/2509.21123",
        "arxiv_id": "2509.21123",
        "authors": "Alessandro Bombini, Alessandro Rosa, Clarissa Buti, Giovanni Passaleva, Lucio Anderlini",
        "summary": "Future high-luminosity hadron colliders demand tracking detectors with extreme radiation tolerance, high spatial precision, and sub-nanosecond timing. 3D diamond pixel sensors offer these capabilities due to diamond's radiation hardness and high carrier mobility. Conductive electrodes, produced via femtosecond IR laser pulses, exhibit high resistivity that delays signal propagation. This effect necessitates extending the classical Ramo-Shockley weighting potential formalism. We model the phenomenon through a 3rd-order, 3+1D PDE derived as a quasi-stationary approximation of Maxwell's equations. The PDE is solved numerically and coupled with charge transport simulations for realistic 3D sensor geometries. A Mixture-of-Experts Physics-Informed Neural Network, trained on Spectral Method data, provides a meshless solver to assess timing degradation from electrode resistance.",
        "subjects": "Instrumentation and Detectors, Machine Learning, High Energy Physics - Experiment, Computational Physics",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.427132"
    },
    {
        "index": "#130",
        "title": "Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?",
        "link": "/arxiv/2509.21087",
        "arxiv_id": "2509.21087",
        "authors": "Rostislav Makarov, Lea Schönherr, Timo Gerkmann",
        "summary": "Machine learning approaches for speech enhancement are becoming increasingly expressive, enabling ever more powerful modifications of input signals. In this paper, we demonstrate that this expressiveness introduces a vulnerability: advanced speech enhancement models can be susceptible to adversarial attacks. Specifically, we show that adversarial noise, carefully crafted and psychoacoustically masked by the original input, can be injected such that the enhanced speech output conveys an entirely different semantic meaning. We experimentally verify that contemporary predictive speech enhancement models can indeed be manipulated in this way. Furthermore, we highlight that diffusion models with stochastic samplers exhibit inherent robustness to such adversarial attacks by design.",
        "subjects": "Audio and Speech Processing, Machine Learning, Sound",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.427709"
    },
    {
        "index": "#132",
        "title": "MPC-based Deep Reinforcement Learning Method for Space Robotic Control with Fuel Sloshing Mitigation",
        "link": "/arxiv/2509.21045",
        "arxiv_id": "2509.21045",
        "authors": "Mahya Ramezani, M. Amin Alandihallaj, Barış Can Yalçın, Miguel Angel Olivares Mendez, Holger Voos",
        "summary": "This paper presents an integrated Reinforcement Learning (RL) and Model Predictive Control (MPC) framework for autonomous satellite docking with a partially filled fuel tank. Traditional docking control faces challenges due to fuel sloshing in microgravity, which induces unpredictable forces affecting stability. To address this, we integrate Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) RL algorithms with MPC, leveraging MPC's predictive capabilities to accelerate RL training and improve control robustness. The proposed approach is validated through Zero-G Lab of SnT experiments for planar stabilization and high-fidelity numerical simulations for 6-DOF docking with fuel sloshing dynamics. Simulation results demonstrate that SAC-MPC achieves superior docking accuracy, higher success rates, and lower control effort, outperforming standalone RL and PPO-MPC methods. This study advances fuel-efficient and disturbance-resilient satellite docking, enhancing the feasibility of on-orbit refueling and servicing missions.",
        "subjects": "Robotics, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.428111"
    },
    {
        "index": "#135",
        "title": "RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training",
        "link": "/arxiv/2509.21009",
        "arxiv_id": "2509.21009",
        "authors": "Wei Gao, Yuheng Zhao, Dakai An, Tianyuan Wu, Lunxi Cao, Shaopan Xiong, Ju Huang, Weixun Wang, Siran Yang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang",
        "summary": "Reinforcement Learning (RL) is a pivotal post-training technique for enhancing the reasoning capabilities of Large Language Models (LLMs). However, synchronous RL post-training often suffers from significant GPU underutilization, referred to as bubbles, caused by imbalanced response lengths within rollout steps. Many RL systems attempt to alleviate this problem by relaxing synchronization, but this can compromise training accuracy. In this paper, we introduce tail batching, a novel rollout scheduling strategy for synchronous RL that systematically consolidates prompts leading to long-tail responses into a small subset of rollout steps (long rounds), while ensuring that the majority of steps (short rounds) involve only balanced, short rollouts. By excluding long responses from short rounds and rescheduling them into a few designated long rounds, tail batching effectively reduces GPU idle time during rollouts and significantly accelerates RL training without sacrificing accuracy. We present RollPacker, a system that fully harnesses the benefits of tail batching through holistic optimizations across all three RL stages: elastic parallelism adaptation for rollout, dynamic resource allocation and scheduling for reward, and stream-based training. Empirical results show that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5 family of LLMs on up to 128 H800 GPUs.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.428818"
    },
    {
        "index": "#137",
        "title": "Empirical PAC-Bayes bounds for Markov chains",
        "link": "/arxiv/2509.20985",
        "arxiv_id": "2509.20985",
        "authors": "Vahe Karagulyan, Pierre Alquier",
        "summary": "The core of generalization theory was developed for independent observations. Some PAC and PAC-Bayes bounds are available for data that exhibit a temporal dependence. However, there are constants in these bounds that depend on properties of the data-generating process: mixing coefficients, mixing time, spectral gap... Such constants are unknown in practice. In this paper, we prove a new PAC-Bayes bound for Markov chains. This bound depends on a quantity called the pseudo-spectral gap. The main novelty is that we can provide an empirical bound on the pseudo-spectral gap when the state space is finite. Thus, we obtain the first fully empirical PAC-Bayes bound for Markov chains. This extends beyond the finite case, although this requires additional assumptions. On simulated experiments, the empirical version of the bound is essentially as tight as the non-empirical one.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.429182"
    },
    {
        "index": "#140",
        "title": "Reverse Faà di Bruno's Formula for Cartesian Reverse Differential Categories",
        "link": "/arxiv/2509.20931",
        "arxiv_id": "2509.20931",
        "authors": "Aaron Biggin, Jean-Simon Pacaud Lemay",
        "summary": "Reverse differentiation is an essential operation for automatic differentiation. Cartesian reverse differential categories axiomatize reverse differentiation in a categorical framework, where one of the primary axioms is the reverse chain rule, which is the formula that expresses the reverse derivative of a composition. Here, we present the reverse differential analogue of Faa di Bruno's Formula, which gives a higher-order reverse chain rule in a Cartesian reverse differential category. To properly do so, we also define partial reverse derivatives and higher-order reverse derivatives in a Cartesian reverse differential category.",
        "subjects": "Logic in Computer Science, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.429711"
    },
    {
        "index": "#141",
        "title": "Conditionally Whitened Generative Models for Probabilistic Time Series Forecasting",
        "link": "/arxiv/2509.20928",
        "arxiv_id": "2509.20928",
        "authors": "Yanfeng Yang, Siwei Chen, Pingping Hu, Zhaotong Shen, Yingjie Zhang, Zhuoran Sun, Shuai Li, Ziqi Chen, Kenji Fukumizu",
        "summary": "Probabilistic forecasting of multivariate time series is challenging due to non-stationarity, inter-variable dependencies, and distribution shifts. While recent diffusion and flow matching models have shown promise, they often ignore informative priors such as conditional means and covariances. In this work, we propose Conditionally Whitened Generative Models (CW-Gen), a framework that incorporates prior information through conditional whitening. Theoretically, we establish sufficient conditions under which replacing the traditional terminal distribution of diffusion models, namely the standard multivariate normal, with a multivariate normal distribution parameterized by estimators of the conditional mean and covariance improves sample quality. Guided by this analysis, we design a novel Joint Mean-Covariance Estimator (JMCE) that simultaneously learns the conditional mean and sliding-window covariance. Building on JMCE, we introduce Conditionally Whitened Diffusion Models (CW-Diff) and extend them to Conditionally Whitened Flow Matching (CW-Flow). Experiments on five real-world datasets with six state-of-the-art generative models demonstrate that CW-Gen consistently enhances predictive performance, capturing non-stationary dynamics and inter-variable correlations more effectively than prior-free approaches. Empirical results further demonstrate that CW-Gen can effectively mitigate the effects of distribution shift.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.429925"
    },
    {
        "index": "#143",
        "title": "RecIS: Sparse to Dense, A Unified Training Framework for Recommendation Models",
        "link": "/arxiv/2509.20883",
        "arxiv_id": "2509.20883",
        "authors": "Hua Zong, Qingtao Zeng, Zhengxiong Zhou, Zhihua Han, Zhensong Yan, Mingjie Liu, Hechen Sun, Jiawei Liu, Yiwen Hu, Qi Wang, YiHan Xian, Wenjie Guo, Houyuan Xiang, Zhiyuan Zeng, Xiangrong Sheng, Bencheng Yan, Nan Hu, Yuheng Huang, Jinqing Lian, Ziru Xu, Yan Zhang, Ju Huang, Siran Yang, Huimin Yi, Jiamang Wang, Pengjie Wang, Han Zhu, Jian Wu, Dan Ou, Jian Xu, Haihong Tang, Yuning Jiang, Bo Zheng, Lin Qu",
        "summary": "In this paper, we propose RecIS, a unified Sparse-Dense training framework designed to achieve two primary goals: 1. Unified Framework To create a Unified sparse-dense training framework based on the PyTorch ecosystem that meets the training needs of industrial-grade recommendation models that integrated with large models. 2.System Optimization To optimize the sparse component, offering superior efficiency over the TensorFlow-based recommendation models. The dense component, meanwhile, leverages existing optimization technologies within the PyTorch ecosystem. Currently, RecIS is being used in Alibaba for numerous large-model enhanced recommendation training tasks, and some traditional sparse models have also begun training in it.",
        "subjects": "Information Retrieval, Distributed, Parallel, and Cluster Computing, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.430549"
    },
    {
        "index": "#145",
        "title": "Actively Learning Halfspaces without Synthetic Data",
        "link": "/arxiv/2509.20848",
        "arxiv_id": "2509.20848",
        "authors": "Hadley Black, Kasper Green Larsen, Arya Mazumdar, Barna Saha, Geelon So",
        "summary": "In the classic point location problem, one is given an arbitrary dataset $X \\subset \\mathbb{R}^d$ of $n$ points with query access to an unknown halfspace $f : \\mathbb{R}^d \\to \\{0,1\\}$, and the goal is to learn the label of every point in $X$. This problem is extremely well-studied and a nearly-optimal $\\widetilde{O}(d \\log n)$ query algorithm is known due to Hopkins-Kane-Lovett-Mahajan (FOCS 2020). However, their algorithm is granted the power to query arbitrary points outside of $X$ (point synthesis), and in fact without this power there is an $\\Omega(n)$ query lower bound due to Dasgupta (NeurIPS 2004). In this work our goal is to design efficient algorithms for learning halfspaces without point synthesis. To circumvent the $\\Omega(n)$ lower bound, we consider learning halfspaces whose normal vectors come from a set of size $D$, and show tight bounds of $\\Theta(D + \\log n)$. As a corollary, we obtain an optimal $O(d + \\log n)$ query deterministic learner for axis-aligned halfspaces, closing a previous gap of $O(d \\log n)$ vs. $\\Omega(d + \\log n)$. In fact, our algorithm solves the more general problem of learning a Boolean function $f$ over $n$ elements which is monotone under at least one of $D$ provided orderings. Our technical insight is to exploit the structure in these orderings to perform a binary search in parallel rather than considering each ordering sequentially, and we believe our approach may be of broader interest. Furthermore, we use our exact learning algorithm to obtain nearly optimal algorithms for PAC-learning. We show that $O(\\min(D + \\log(1/\\varepsilon), 1/\\varepsilon) \\cdot \\log D)$ queries suffice to learn $f$ within error $\\varepsilon$, even in a setting when $f$ can be adversarially corrupted on a $c\\varepsilon$-fraction of points, for a sufficiently small constant $c$. This bound is optimal up to a $\\log D$ factor, including in the realizable setting.",
        "subjects": "Data Structures and Algorithms, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.430954"
    },
    {
        "index": "#149",
        "title": "Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning",
        "link": "/arxiv/2509.20766",
        "arxiv_id": "2509.20766",
        "authors": "Gawon Lee, Daesol Cho, H. Jin Kim",
        "summary": "Multi-task reinforcement learning (MTRL) offers a promising approach to improve sample efficiency and generalization by training agents across multiple tasks, enabling knowledge sharing between them. However, applying MTRL to robotics remains challenging due to the high cost of collecting diverse task data. To address this, we propose MT-Lévy, a novel exploration strategy that enhances sample efficiency in MTRL environments by combining behavior sharing across tasks with temporally extended exploration inspired by Lévy flight. MT-Lévy leverages policies trained on related tasks to guide exploration towards key states, while dynamically adjusting exploration levels based on task success ratios. This approach enables more efficient state-space coverage, even in complex robotics environments. Empirical results demonstrate that MT-Lévy significantly improves exploration and sample efficiency, supported by quantitative and qualitative analyses. Ablation studies further highlight the contribution of each component, showing that combining behavior sharing with adaptive exploration strategies can significantly improve the practicality of MTRL in robotics applications.",
        "subjects": "Robotics, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.431719"
    },
    {
        "index": "#150",
        "title": "Identifying Group Anchors in Real-World Group Interactions Under Label Scarcity",
        "link": "/arxiv/2509.20762",
        "arxiv_id": "2509.20762",
        "authors": "Fanchen Bu, Geon Lee, Minyoung Choe, Kijung Shin",
        "summary": "Group interactions occur in various real-world contexts, e.g., co-authorship, email communication, and online Q&A. In each group, there is often a particularly significant member, around whom the group is formed. Examples include the first or last author of a paper, the sender of an email, and the questioner in a Q&A session. In this work, we discuss the existence of such individuals in real-world group interactions. We call such individuals group anchors and study the problem of identifying them. First, we introduce the concept of group anchors and the identification problem. Then, we discuss our observations on group anchors in real-world group interactions. Based on our observations, we develop AnchorRadar, a fast and effective method for group anchor identification under realistic settings with label scarcity, i.e., when only a few groups have known anchors. AnchorRadar is a semi-supervised method using information from groups both with and without known group anchors. Finally, through extensive experiments on thirteen real-world datasets, we demonstrate the empirical superiority of AnchorRadar over various baselines w.r.t. accuracy and efficiency. In most cases, AnchorRadar achieves higher accuracy in group anchor identification than all the baselines, while using 10.2$\\times$ less training time than the fastest baseline and 43.6$\\times$ fewer learnable parameters than the most lightweight baseline on average.",
        "subjects": "Social and Information Networks, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.431902"
    },
    {
        "index": "#151",
        "title": "RAPTOR-GEN: RApid PosTeriOR GENerator for Bayesian Learning in Biomanufacturing",
        "link": "/arxiv/2509.20753",
        "arxiv_id": "2509.20753",
        "authors": "Wandi Xu, Wei Xie",
        "summary": "Biopharmaceutical manufacturing is vital to public health but lacks the agility for rapid, on-demand production of biotherapeutics due to the complexity and variability of bioprocesses. To overcome this, we introduce RApid PosTeriOR GENerator (RAPTOR-GEN), a mechanism-informed Bayesian learning framework designed to accelerate intelligent digital twin development from sparse and heterogeneous experimental data. This framework is built on a multi-scale probabilistic knowledge graph (pKG), formulated as a stochastic differential equation (SDE)-based foundational model that captures the nonlinear dynamics of bioprocesses. RAPTOR-GEN consists of two ingredients: (i) an interpretable metamodel integrating linear noise approximation (LNA) that exploits the structural information of bioprocessing mechanisms and a sequential learning strategy to fuse heterogeneous and sparse data, enabling inference of latent state variables and explicit approximation of the intractable likelihood function; and (ii) an efficient Bayesian posterior sampling method that utilizes Langevin diffusion (LD) to accelerate posterior exploration by exploiting the gradients of the derived likelihood. It generalizes the LNA approach to circumvent the challenge of step size selection, facilitating robust learning of mechanistic parameters with provable finite-sample performance guarantees. We develop a fast and robust RAPTOR-GEN algorithm with controllable error. Numerical experiments demonstrate its effectiveness in uncovering the underlying regulatory mechanisms of biomanufacturing processes.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.432075"
    },
    {
        "index": "#152",
        "title": "Real-Time System for Audio-Visual Target Speech Enhancement",
        "link": "/arxiv/2509.20741",
        "arxiv_id": "2509.20741",
        "authors": "T. Aleksandra Ma, Sile Yin, Li-Chia Yang, Shuo Zhang",
        "summary": "We present a live demonstration for RAVEN, a real-time audio-visual speech enhancement system designed to run entirely on a CPU. In single-channel, audio-only settings, speech enhancement is traditionally approached as the task of extracting clean speech from environmental noise. More recent work has explored the use of visual cues, such as lip movements, to improve robustness, particularly in the presence of interfering speakers. However, to our knowledge, no prior work has demonstrated an interactive system for real-time audio-visual speech enhancement operating on CPU hardware. RAVEN fills this gap by using pretrained visual embeddings from an audio-visual speech recognition model to encode lip movement information. The system generalizes across environmental noise, interfering speakers, transient sounds, and even singing voices. In this demonstration, attendees will be able to experience live audio-visual target speech enhancement using a microphone and webcam setup, with clean speech playback through headphones.",
        "subjects": "Audio and Speech Processing, Emerging Technologies, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.432340"
    },
    {
        "index": "#153",
        "title": "PALQO: Physics-informed Model for Accelerating Large-scale Quantum Optimization",
        "link": "/arxiv/2509.20733",
        "arxiv_id": "2509.20733",
        "authors": "Yiming Huang, Yajie Hao, Jing Zhou, Xiao Yuan, Xiaoting Wang, Yuxuan Du",
        "summary": "Variational quantum algorithms (VQAs) are leading strategies to reach practical utilities of near-term quantum devices. However, the no-cloning theorem in quantum mechanics precludes standard backpropagation, leading to prohibitive quantum resource costs when applying VQAs to large-scale tasks. To address this challenge, we reformulate the training dynamics of VQAs as a nonlinear partial differential equation and propose a novel protocol that leverages physics-informed neural networks (PINNs) to model this dynamical system efficiently. Given a small amount of training trajectory data collected from quantum devices, our protocol predicts the parameter updates of VQAs over multiple iterations on the classical side, dramatically reducing quantum resource costs. Through systematic numerical experiments, we demonstrate that our method achieves up to a 30x speedup compared to conventional methods and reduces quantum resource costs by as much as 90\\% for tasks involving up to 40 qubits, including ground state preparation of different quantum systems, while maintaining competitive accuracy. Our approach complements existing techniques aimed at improving the efficiency of VQAs and further strengthens their potential for practical applications.",
        "subjects": "Quantum Physics, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.432557"
    },
    {
        "index": "#154",
        "title": "Cryptographic Backdoor for Neural Networks: Boon and Bane",
        "link": "/arxiv/2509.20714",
        "arxiv_id": "2509.20714",
        "authors": "Anh Tu Ngo, Anupam Chattopadhyay, Subhamoy Maitra",
        "summary": "In this paper we show that cryptographic backdoors in a neural network (NN) can be highly effective in two directions, namely mounting the attacks as well as in presenting the defenses as well. On the attack side, a carefully planted cryptographic backdoor enables powerful and invisible attack on the NN. Considering the defense, we present applications: first, a provably robust NN watermarking scheme; second, a protocol for guaranteeing user authentication; and third, a protocol for tracking unauthorized sharing of the NN intellectual property (IP). From a broader theoretical perspective, borrowing the ideas from Goldwasser et. al. [FOCS 2022], our main contribution is to show that all these instantiated practical protocol implementations are provably robust. The protocols for watermarking, authentication and IP tracking resist an adversary with black-box access to the NN, whereas the backdoor-enabled adversarial attack is impossible to prevent under the standard assumptions. While the theoretical tools used for our attack is mostly in line with the Goldwasser et. al. ideas, the proofs related to the defense need further studies. Finally, all these protocols are implemented on state-of-the-art NN architectures with empirical results corroborating the theoretical claims. Further, one can utilize post-quantum primitives for implementing the cryptographic backdoors, laying out foundations for quantum-era applications in machine learning (ML).",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.432741"
    },
    {
        "index": "#155",
        "title": "Implicit Augmentation from Distributional Symmetry in Turbulence Super-Resolution",
        "link": "/arxiv/2509.20683",
        "arxiv_id": "2509.20683",
        "authors": "Julia Balla, Jeremiah Bailey, Ali Backour, Elyssa Hofgard, Tommi Jaakkola, Tess Smidt, Ryley McConkey",
        "summary": "The immense computational cost of simulating turbulence has motivated the use of machine learning approaches for super-resolving turbulent flows. A central challenge is ensuring that learned models respect physical symmetries, such as rotational equivariance. We show that standard convolutional neural networks (CNNs) can partially acquire this symmetry without explicit augmentation or specialized architectures, as turbulence itself provides implicit rotational augmentation in both time and space. Using 3D channel-flow subdomains with differing anisotropy, we find that models trained on more isotropic mid-plane data achieve lower equivariance error than those trained on boundary layer data, and that greater temporal or spatial sampling further reduces this error. We show a distinct scale-dependence of equivariance error that occurs regardless of dataset anisotropy that is consistent with Kolmogorov's local isotropy hypothesis. These results clarify when rotational symmetry must be explicitly incorporated into learning algorithms and when it can be obtained directly from turbulence, enabling more efficient and symmetry-aware super-resolution.",
        "subjects": "Fluid Dynamics, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.432940"
    },
    {
        "index": "#157",
        "title": "A Hierarchical Variational Graph Fused Lasso for Recovering Relative Rates in Spatial Compositional Data",
        "link": "/arxiv/2509.20636",
        "arxiv_id": "2509.20636",
        "authors": "Joaquim Valerio Teixeira, Ed Reznik, Sudpito Banerjee, Wesley Tansey",
        "summary": "The analysis of spatial data from biological imaging technology, such as imaging mass spectrometry (IMS) or imaging mass cytometry (IMC), is challenging because of a competitive sampling process which convolves signals from molecules in a single pixel. To address this, we develop a scalable Bayesian framework that leverages natural sparsity in spatial signal patterns to recover relative rates for each molecule across the entire image. Our method relies on the use of a heavy-tailed variant of the graphical lasso prior and a novel hierarchical variational family, enabling efficient inference via automatic differentiation variational inference. Simulation results show that our approach outperforms state-of-the-practice point estimate methodologies in IMS, and has superior posterior coverage than mean-field variational inference techniques. Results on real IMS data demonstrate that our approach better recovers the true anatomical structure of known tissue, removes artifacts, and detects active regions missed by the standard analysis approach.",
        "subjects": "Machine Learning, Machine Learning, Methodology",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.433323"
    },
    {
        "index": "#158",
        "title": "Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow",
        "link": "/arxiv/2509.20631",
        "arxiv_id": "2509.20631",
        "authors": "Michael Zhang, Yuan Tian, Mariam Guizani",
        "summary": "As software systems grow in scale and complexity, understanding the distribution of programming language topics within source code becomes increasingly important for guiding technical decisions, improving onboarding, and informing tooling and education. This paper presents the design, implementation, and evaluation of a novel programming language topic classification workflow. Our approach combines a multi-label Support Vector Machine (SVM) with a sliding window and voting strategy to enable fine-grained localization of core language concepts such as operator overloading, virtual functions, inheritance, and templates. Trained on the IBM Project CodeNet dataset, our model achieves an average F1 score of 0.90 across topics and 0.75 in code-topic highlight. Our findings contribute empirical insights and a reusable pipeline for researchers and practitioners interested in code analysis and data-driven software engineering.",
        "subjects": "Software Engineering, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.433499"
    },
    {
        "index": "#160",
        "title": "A Gapped Scale-Sensitive Dimension and Lower Bounds for Offset Rademacher Complexity",
        "link": "/arxiv/2509.20618",
        "arxiv_id": "2509.20618",
        "authors": "Zeyu Jia, Yury Polyanskiy, Alexander Rakhlin",
        "summary": "We study gapped scale-sensitive dimensions of a function class in both sequential and non-sequential settings. We demonstrate that covering numbers for any uniformly bounded class are controlled above by these gapped dimensions, generalizing the results of \\cite{anthony2000function,alon1997scale}. Moreover, we show that the gapped dimensions lead to lower bounds on offset Rademacher averages, thereby strengthening existing approaches for proving lower bounds on rates of convergence in statistical and online learning.",
        "subjects": "Machine Learning, Machine Learning, Statistics Theory",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.433885"
    },
    {
        "index": "#164",
        "title": "Unsupervised Domain Adaptation with an Unobservable Source Subpopulation",
        "link": "/arxiv/2509.20587",
        "arxiv_id": "2509.20587",
        "authors": "Chao Ying, Jun Jin, Haotian Zhang, Qinglong Tian, Yanyuan Ma, Yixuan Li, Jiwei Zhao",
        "summary": "We study an unsupervised domain adaptation problem where the source domain consists of subpopulations defined by the binary label $Y$ and a binary background (or environment) $A$. We focus on a challenging setting in which one such subpopulation in the source domain is unobservable. Naively ignoring this unobserved group can result in biased estimates and degraded predictive performance. Despite this structured missingness, we show that the prediction in the target domain can still be recovered. Specifically, we rigorously derive both background-specific and overall prediction models for the target domain. For practical implementation, we propose the distribution matching method to estimate the subpopulation proportions. We provide theoretical guarantees for the asymptotic behavior of our estimator, and establish an upper bound on the prediction error. Experiments on both synthetic and real-world datasets show that our method outperforms the naive benchmark that does not account for this unobservable source subpopulation.",
        "subjects": "Machine Learning, Machine Learning, Methodology",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.434800"
    },
    {
        "index": "#170",
        "title": "Fast Estimation of Wasserstein Distances via Regression on Sliced Wasserstein Distances",
        "link": "/arxiv/2509.20508",
        "arxiv_id": "2509.20508",
        "authors": "Khai Nguyen, Hai Nguyen, Nhat Ho",
        "summary": "We address the problem of efficiently computing Wasserstein distances for multiple pairs of distributions drawn from a meta-distribution. To this end, we propose a fast estimation method based on regressing Wasserstein distance on sliced Wasserstein (SW) distances. Specifically, we leverage both standard SW distances, which provide lower bounds, and lifted SW distances, which provide upper bounds, as predictors of the true Wasserstein distance. To ensure parsimony, we introduce two linear models: an unconstrained model with a closed-form least-squares solution, and a constrained model that uses only half as many parameters. We show that accurate models can be learned from a small number of distribution pairs. Once estimated, the model can predict the Wasserstein distance for any pair of distributions via a linear combination of SW distances, making it highly efficient. Empirically, we validate our approach on diverse tasks, including Gaussian mixtures, point-cloud classification, and Wasserstein-space visualizations for 3D point clouds. Across various datasets such as MNIST point clouds, ShapeNetV2, MERFISH Cell Niches, and scRNA-seq, our method consistently provides a better approximation of Wasserstein distance than the state-of-the-art Wasserstein embedding model, Wasserstein Wormhole, particularly in low-data regimes. Finally, we demonstrate that our estimator can also accelerate Wormhole training, yielding \\textit{RG-Wormhole}.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.435935"
    },
    {
        "index": "#171",
        "title": "Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens",
        "link": "/arxiv/2509.20485",
        "arxiv_id": "2509.20485",
        "authors": "Ismail Rasim Ulgen, Zongyang Du, Junchen Lu, Philipp Koehn, Berrak Sisman",
        "summary": "Objective evaluation of synthesized speech is critical for advancing speech generation systems, yet existing metrics for intelligibility and prosody remain limited in scope and weakly correlated with human perception. Word Error Rate (WER) provides only a coarse text-based measure of intelligibility, while F0-RMSE and related pitch-based metrics offer a narrow, reference-dependent view of prosody. To address these limitations, we propose TTScore, a targeted and reference-free evaluation framework based on conditional prediction of discrete speech tokens. TTScore employs two sequence-to-sequence predictors conditioned on input text: TTScore-int, which measures intelligibility through content tokens, and TTScore-pro, which evaluates prosody through prosody tokens. For each synthesized utterance, the predictors compute the likelihood of the corresponding token sequences, yielding interpretable scores that capture alignment with intended linguistic content and prosodic structure. Experiments on the SOMOS, VoiceMOS, and TTSArena benchmarks demonstrate that TTScore-int and TTScore-pro provide reliable, aspect-specific evaluation and achieve stronger correlations with human judgments of overall quality than existing intelligibility and prosody-focused metrics.",
        "subjects": "Audio and Speech Processing, Machine Learning, Sound",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.436130"
    },
    {
        "index": "#173",
        "title": "Neural Networks as Surrogate Solvers for Time-Dependent Accretion Disk Dynamics",
        "link": "/arxiv/2509.20447",
        "arxiv_id": "2509.20447",
        "authors": "Shunyuan Mao, Weiqi Wang, Sifan Wang, Ruobing Dong, Lu Lu, Kwang Moo Yi, Paris Perdikaris, Andrea Isella, Sébastien Fabbro, Lile Wang",
        "summary": "Accretion disks are ubiquitous in astrophysics, appearing in diverse environments from planet-forming systems to X-ray binaries and active galactic nuclei. Traditionally, modeling their dynamics requires computationally intensive (magneto)hydrodynamic simulations. Recently, Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative. This approach trains neural networks directly on physical laws without requiring data. We for the first time demonstrate PINNs for solving the two-dimensional, time-dependent hydrodynamics of non-self-gravitating accretion disks. Our models provide solutions at arbitrary times and locations within the training domain, and successfully reproduce key physical phenomena, including the excitation and propagation of spiral density waves and gap formation from disk-companion interactions. Notably, the boundary-free approach enabled by PINNs naturally eliminates the spurious wave reflections at disk edges, which are challenging to suppress in numerical simulations. These results highlight how advanced machine learning techniques can enable physics-driven, data-free modeling of complex astrophysical systems, potentially offering an alternative to traditional numerical simulations in the future.",
        "subjects": "Earth and Planetary Astrophysics, Instrumentation and Methods for Astrophysics, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.436625"
    },
    {
        "index": "#176",
        "title": "Sample completion, structured correlation, and Netflix problems",
        "link": "/arxiv/2509.20404",
        "arxiv_id": "2509.20404",
        "authors": "Leonardo N. Coregliano, Maryanthe Malliaris",
        "summary": "We develop a new high-dimensional statistical learning model which can take advantage of structured correlation in data even in the presence of randomness. We completely characterize learnability in this model in terms of VCN${}_{k,k}$-dimension (essentially $k$-dependence from Shelah's classification theory). This model suggests a theoretical explanation for the success of certain algorithms in the 2006~Netflix Prize competition.",
        "subjects": "Machine Learning, Machine Learning, Logic, Statistics Theory",
        "date": "2025-09-23",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.437196"
    },
    {
        "index": "#178",
        "title": "A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks",
        "link": "/arxiv/2509.20391",
        "arxiv_id": "2509.20391",
        "authors": "Md. Alamgir Hossain, Waqas Ishtiaq, Md. Samiul Islam",
        "summary": "The growing integration of drones into civilian, commercial, and defense sectors introduces significant cybersecurity concerns, particularly with the increased risk of network-based intrusions targeting drone communication protocols. Detecting and classifying these intrusions is inherently challenging due to the dynamic nature of drone traffic and the presence of multiple sophisticated attack vectors such as spoofing, injection, replay, and man-in-the-middle (MITM) attacks. This research aims to develop a robust and interpretable intrusion detection framework tailored for drone networks, with a focus on handling multi-class classification and model explainability. We present a comparative analysis of ensemble-based machine learning models, namely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on a labeled dataset comprising benign traffic and nine distinct intrusion types. Comprehensive data preprocessing was performed, including missing value imputation, scaling, and categorical encoding, followed by model training and extensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews Correlation Coefficient, and Log Loss. Random Forest achieved the highest performance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate the superiority of the models, statistical tests, including Friedmans test, the Wilcoxon signed-rank test with Holm correction, and bootstrapped confidence intervals, were applied. Furthermore, explainable AI methods, SHAP and LIME, were integrated to interpret both global and local feature importance, enhancing model transparency and decision trustworthiness. The proposed approach not only delivers near-perfect accuracy but also ensures interpretability, making it highly suitable for real-time and safety-critical drone operations.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.437556"
    },
    {
        "index": "#183",
        "title": "An Analytical and AI-discovered Stable, Accurate, and Generalizable Subgrid-scale Closure for Geophysical Turbulence",
        "link": "/arxiv/2509.20365",
        "arxiv_id": "2509.20365",
        "authors": "Karan Jakhar, Yifei Guan, Pedram Hassanzadeh",
        "summary": "By combining AI and fluid physics, we discover a closed-form closure for 2D turbulence from small direct numerical simulation (DNS) data. Large-eddy simulation (LES) with this closure is accurate and stable, reproducing DNS statistics including those of extremes. We also show that the new closure could be derived from a 4th-order truncated Taylor expansion. Prior analytical and AI-based work only found the 2nd-order expansion, which led to unstable LES. The additional terms emerge only when inter-scale energy transfer is considered alongside standard reconstruction criterion in the sparse-equation discovery.",
        "subjects": "Atmospheric and Oceanic Physics, Machine Learning",
        "date": "2025-09-05",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.438686"
    },
    {
        "index": "#184",
        "title": "Copycats: the many lives of a publicly available medical imaging dataset",
        "link": "/arxiv/2402.06353",
        "arxiv_id": "2402.06353",
        "authors": "Amelia Jiménez-Sánchez, Natalia-Rozalia Avlona, Dovile Juodelyte, Théo Sourget, Caroline Vang-Larsen, Anna Rogers, Hubert Dariusz Zając, Veronika Cheplygina",
        "summary": "Medical Imaging (MI) datasets are fundamental to artificial intelligence in healthcare. The accuracy, robustness, and fairness of diagnostic algorithms depend on the data (and its quality) used to train and evaluate the models. MI datasets used to be proprietary, but have become increasingly available to the public, including on community-contributed platforms (CCPs) like Kaggle or HuggingFace. While open data is important to enhance the redistribution of data's public value, we find that the current CCP governance model fails to uphold the quality needed and recommended practices for sharing, documenting, and evaluating datasets. In this paper, we conduct an analysis of publicly available machine learning datasets on CCPs, discussing datasets' context, and identifying limitations and gaps in the current CCP landscape. We highlight differences between MI and computer vision datasets, particularly in the potentially harmful downstream effects from poor adoption of recommended dataset management practices. We compare the analyzed datasets across several dimensions, including data sharing, data documentation, and maintenance. We find vague licenses, lack of persistent identifiers and storage, duplicates, and missing metadata, with differences between the platforms. Our research contributes to efforts in responsible data curation and AI algorithms for healthcare.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-02-09",
        "category": "cs.LG",
        "crawl_time": "2025-09-26T21:01:55.438887"
    }
]