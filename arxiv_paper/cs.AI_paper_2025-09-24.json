[
    {
        "index": "#4",
        "title": "Formal Verification of Minimax Algorithms",
        "link": "/arxiv/2509.20138",
        "arxiv_id": "2509.20138",
        "authors": "Wieger Wesselink, Kees Huizing, Huub van de Wetering",
        "summary": "Using the Dafny verification system, we formally verify a range of minimax search algorithms, including variations with alpha-beta pruning and transposition tables. For depth-limited search with transposition tables, we introduce a witness-based correctness criterion and apply it to two representative algorithms. All verification artifacts, including proofs and Python implementations, are publicly available.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.626505"
    },
    {
        "index": "#5",
        "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs",
        "link": "/arxiv/2509.20105",
        "arxiv_id": "2509.20105",
        "authors": "Venkat Margapuri, Garik Kazanjian, Naren Kosaraju",
        "summary": "Large Language Models (LLMs) often struggle with maintaining coherent multi-step reasoning traces, particularly in tasks that require a structured logical flow. This work introduces a quantum-inspired approach to address the challenge by incorporating a fidelity-based reward derived from Projected Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior approaches that use direct supervision or contrastive objectives, the proposed method guides learning through structural consistency, offering a novel approach to enforce global coherence in generated reasoning traces. The proposed framework is evaluated using multiple coherence-determining metrics on diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning arithmetic, intuitive, and entailment-based reasoning. Results show that the proposed quantum-inspired approach offers significant improvements over supervised, contrastive, and pretrained baseline approaches, highlighting the effectiveness of quantum-inspired fidelity as a foundation to improve reasoning trace coherence in LLMs.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.626781"
    },
    {
        "index": "#6",
        "title": "Steerable Adversarial Scenario Generation through Test-Time Preference Alignment",
        "link": "/arxiv/2509.20102",
        "arxiv_id": "2509.20102",
        "authors": "Tong Nie, Yuewen Mei, Yihong Tang, Junlin He, Jie Sun, Haotian Shi, Wei Ma, Jian Sun",
        "summary": "Adversarial scenario generation is a cost-effective approach for safety assessment of autonomous driving systems. However, existing methods are often constrained to a single, fixed trade-off between competing objectives such as adversariality and realism. This yields behavior-specific models that cannot be steered at inference time, lacking the efficiency and flexibility to generate tailored scenarios for diverse training and testing requirements. In view of this, we reframe the task of adversarial scenario generation as a multi-objective preference alignment problem and introduce a new framework named \\textbf{S}teerable \\textbf{A}dversarial scenario \\textbf{GE}nerator (SAGE). SAGE enables fine-grained test-time control over the trade-off between adversariality and realism without any retraining. We first propose hierarchical group-based preference optimization, a data-efficient offline alignment method that learns to balance competing objectives by decoupling hard feasibility constraints from soft preferences. Instead of training a fixed model, SAGE fine-tunes two experts on opposing preferences and constructs a continuous spectrum of policies at inference time by linearly interpolating their weights. We provide theoretical justification for this framework through the lens of linear mode connectivity. Extensive experiments demonstrate that SAGE not only generates scenarios with a superior balance of adversariality and realism but also enables more effective closed-loop training of driving policies. Project page: https://tongnie.github.io/SAGE/.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.627054"
    },
    {
        "index": "#7",
        "title": "From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms",
        "link": "/arxiv/2509.20095",
        "arxiv_id": "2509.20095",
        "authors": "Aymeric Vellinger, Nemanja Antonic, Elio Tuci",
        "summary": "Swarm intelligence emerges from decentralised interactions among simple agents, enabling collective problem-solving. This study establishes a theoretical equivalence between pheromone-mediated aggregation in \\celeg\\ and reinforcement learning (RL), demonstrating how stigmergic signals function as distributed reward mechanisms. We model engineered nematode swarms performing foraging tasks, showing that pheromone dynamics mathematically mirror cross-learning updates, a fundamental RL algorithm. Experimental validation with data from literature confirms that our model accurately replicates empirical \\celeg\\ foraging patterns under static conditions. In dynamic environments, persistent pheromone trails create positive feedback loops that hinder adaptation by locking swarms into obsolete choices. Through computational experiments in multi-armed bandit scenarios, we reveal that introducing a minority of exploratory agents insensitive to pheromones restores collective plasticity, enabling rapid task switching. This behavioural heterogeneity balances exploration-exploitation trade-offs, implementing swarm-level extinction of outdated strategies. Our results demonstrate that stigmergic systems inherently encode distributed RL processes, where environmental signals act as external memory for collective credit assignment. By bridging synthetic biology with swarm robotics, this work advances programmable living systems capable of resilient decision-making in volatile environments.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.627262"
    },
    {
        "index": "#8",
        "title": "MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM",
        "link": "/arxiv/2509.20067",
        "arxiv_id": "2509.20067",
        "authors": "Wenliang Li, Rui Yan, Xu Zhang, Li Chen, Hongji Zhu, Jing Zhao, Junjun Li, Mengru Li, Wei Cao, Zihang Jiang, Wei Wei, Kun Zhang, Shaohua Kevin Zhou",
        "summary": "Large language models (LLMs) have demonstrated notable potential in medical applications, yet they face substantial challenges in handling complex real-world clinical diagnoses using conventional prompting methods. Current prompt engineering and multi-agent approaches typically optimize isolated inferences, neglecting the accumulation of reusable clinical experience. To address this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD) framework, which allows LLMs to self-learn clinical knowledge via a multi-agent pipeline that summarizes, refines, and applies diagnostic insights. It mirrors how physicians develop expertise through experience, enabling more focused and accurate diagnosis on key disease-specific cues. We further extend it to a MACD-human collaborative workflow, where multiple LLM-based diagnostician agents engage in iterative consultations, supported by an evaluator agent and human oversight for cases where agreement is not reached. Evaluated on 4,390 real-world patient cases across seven diseases using diverse open-source LLMs (Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves primary diagnostic accuracy, outperforming established clinical guidelines with gains up to 22.3% (MACD). On the subset of the data, it achieves performance on par with or exceeding that of human physicians (up to 16% improvement over physicians-only diagnosis). Additionally, on the MACD-human workflow, it achieves an 18.6% improvement compared to physicians-only diagnosis. Moreover, self-learned knowledge exhibits strong cross-model stability, transferability, and model-specific personalization, while the system can generate traceable rationales, enhancing explainability. Consequently, this work presents a scalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap between the intrinsic knowledge of LLMs and real-world clinical practice.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.627608"
    },
    {
        "index": "#10",
        "title": "CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain",
        "link": "/arxiv/2509.19925",
        "arxiv_id": "2509.19925",
        "authors": "Ajeet Kumar Singh, Rajsabi Surya, Anurag Tripathi, Santanu Choudhury, Sudhir Bisane",
        "summary": "As enterprises increasingly integrate cloud-based large language models (LLMs) such as ChatGPT and Gemini into their legal document workflows, protecting sensitive contractual information - including Personally Identifiable Information (PII) and commercially sensitive clauses - has emerged as a critical challenge. In this work, we propose CON-QA, a hybrid privacy-preserving framework designed specifically for secure question answering over enterprise contracts, effectively combining local and cloud-hosted LLMs. The CON-QA framework operates through three stages: (i) semantic query decomposition and query-aware document chunk retrieval using a locally deployed LLM analysis, (ii) anonymization of detected sensitive entities via a structured one-to-many mapping scheme, ensuring semantic coherence while preventing cross-session entity inference attacks, and (iii) anonymized response generation by a cloud-based LLM, with accurate reconstruction of the original answer locally using a session-consistent many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world CUAD contract documents, encompassing simple, complex, and summarization-style queries. Empirical evaluations, complemented by detailed human assessments, confirm that CON-QA effectively maintains both privacy and utility, preserves answer quality, maintains fidelity to legal clause semantics, and significantly mitigates privacy risks, demonstrating its practical suitability for secure, enterprise-level contract documents.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.628168"
    },
    {
        "index": "#11",
        "title": "LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation",
        "link": "/arxiv/2509.19839",
        "arxiv_id": "2509.19839",
        "authors": "Huizhen Shu, Xuying Li, Zhuo Li",
        "summary": "Achieving robust safety alignment in large language models (LLMs) while preserving their utility remains a fundamental challenge. Existing approaches often struggle to balance comprehensive safety with fine-grained controllability at the representation level. We introduce LATENTGUARD, a novel three-stage framework that combines behavioral alignment with supervised latent space control for interpretable and precise safety steering. Our approach begins by fine-tuning an LLM on rationalized datasets containing both reasoning-enhanced refusal responses to adversarial prompts and reasoning-enhanced normal responses to benign queries, establishing robust behavioral priors across both safety-critical and utility-preserving scenarios. We then train a structured variational autoencoder (VAE) on intermediate MLP activations, supervised by multi-label annotations including attack types, attack methods, and benign indicators. This supervision enables the VAE to learn disentangled latent representations that capture distinct adversarial characteristics while maintaining semantic interpretability. Through targeted manipulation of learned latent dimensions, LATENTGUARD achieves selective refusal behavior, effectively blocking harmful requests while preserving helpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate significant improvements in both safety controllability and response interpretability without compromising utility. Cross-architecture validation on Mistral-7B confirms the generalizability of our latent steering approach, showing consistent effectiveness across different model families. Our results suggest that structured representation-level intervention offers a promising pathway toward building safer yet practical LLM systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.628373"
    },
    {
        "index": "#12",
        "title": "Analysis of approximate linear programming solution to Markov decision problem with log barrier function",
        "link": "/arxiv/2509.19800",
        "arxiv_id": "2509.19800",
        "authors": "Donghwan Lee, Hyukjun Yang, Bum Geun Park",
        "summary": "There are two primary approaches to solving Markov decision problems (MDPs): dynamic programming based on the Bellman equation and linear programming (LP). Dynamic programming methods are the most widely used and form the foundation of both classical and modern reinforcement learning (RL). By contrast, LP-based methods have been less commonly employed, although they have recently gained attention in contexts such as offline RL. The relative underuse of the LP-based methods stems from the fact that it leads to an inequality-constrained optimization problem, which is generally more challenging to solve effectively compared with Bellman-equation-based methods. The purpose of this paper is to establish a theoretical foundation for solving LP-based MDPs in a more effective and practical manner. Our key idea is to leverage the log-barrier function, widely used in inequality-constrained optimization, to transform the LP formulation of the MDP into an unconstrained optimization problem. This reformulation enables approximate solutions to be obtained easily via gradient descent. While the method may appear simple, to the best of our knowledge, a thorough theoretical interpretation of this approach has not yet been developed. This paper aims to bridge this gap.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.628559"
    },
    {
        "index": "#13",
        "title": "Agentic Metacognition: Designing a \"Self-Aware\" Low-Code Agent for Failure Prediction and Human Handoff",
        "link": "/arxiv/2509.19783",
        "arxiv_id": "2509.19783",
        "authors": "Jiexi Xu",
        "summary": "The inherent non-deterministic nature of autonomous agents, particularly within low-code/no-code (LCNC) environments, presents significant reliability challenges. Agents can become trapped in unforeseen loops, generate inaccurate outputs, or encounter unrecoverable failures, leading to user frustration and a breakdown of trust. This report proposes a novel architectural pattern to address these issues: the integration of a secondary, \"metacognitive\" layer that actively monitors the primary LCNC agent. Inspired by human introspection, this layer is designed to predict impending task failures based on a defined set of triggers, such as excessive latency or repetitive actions. Upon predicting a failure, the metacognitive agent proactively initiates a human handoff, providing the user with a clear summary of the agent's \"thought process\" and a detailed explanation of why it could not proceed. An empirical analysis of a prototype system demonstrates that this approach significantly increases the overall task success rate. However, this performance gain comes with a notable increase in computational overhead. The findings reframe human handoffs not as an admission of defeat but as a core design feature that enhances system resilience, improves user experience, and builds trust by providing transparency into the agent's internal state. The report discusses the practical and ethical implications of this approach and identifies key directions for future research.",
        "subjects": "Artificial Intelligence, Human-Computer Interaction, Software Engineering",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.628813"
    },
    {
        "index": "#14",
        "title": "The Conductor and the Engine: A Path Towards Co-Designed Reasoning",
        "link": "/arxiv/2509.19762",
        "arxiv_id": "2509.19762",
        "authors": "Yuanxin Wang, Pawel Filipczuk, Anisha Garg, Amaan Dhada, Mohammad Hassanpour, David Bick, Ganesh Venkatesh",
        "summary": "Modern LLM reasoning relies on extensive test-time computation, driven by internal model training and external agentic orchestration. However, this synergy is often inefficient, as model verbosity and poor instruction following lead to wasted compute. We analyze this capability-cost trade-off and introduce an optimized reasoning workflow (\\cepo) that empowers smaller open-source models to outperform models multiple times their size. We will open-source this workflow to enable further research. Our work demonstrates a clear path toward co-designing orchestration frameworks with the underlying model capabilities to unlock powerful reasoning in small-to-medium sized models.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.629038"
    },
    {
        "index": "#16",
        "title": "Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving",
        "link": "/arxiv/2509.19681",
        "arxiv_id": "2509.19681",
        "authors": "Anisha Garg, Engin Tekin, Yash More, David Bick, Nishit Neema, Ganesh Venkatesh",
        "summary": "Advanced test-time computing strategies are essential for scaling reasoning models, but their effectiveness is capped by the models' poor self-evaluation. We propose a pairwise Explanatory Verifier, trained via reinforcement learning (GRPO), that produces calibrated confidence scores and associated natural language reasoning for generated solutions. Our verifier improves the accuracy and efficiency of test-time strategies like best-of-n and self-reflection. Crucially, it excels at identifying challenging failure modes, such as when both candidate solutions are identically incorrect, succeeding where standard methods like majority voting fail.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.629562"
    },
    {
        "index": "#17",
        "title": "SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation",
        "link": "/arxiv/2509.19623",
        "arxiv_id": "2509.19623",
        "authors": "Xutao Mao, Tao Liu, Hongying Zan",
        "summary": "Large Language Models (LLMs) struggle with complex Text-to-SQL queries that demand both sophisticated mathematical reasoning and intricate schema navigation. Existing methods often tackle these challenges in isolation, creating a fractured reasoning process that compromises logical and structural correctness. To resolve this, we introduce SteinerSQL, a framework that unifies these dual challenges into a single, graph-centric optimization problem. SteinerSQL operates in three stages: mathematical decomposition to identify required tables (terminals), optimal reasoning scaffold construction via a Steiner tree problem, and multi-level validation to ensure correctness. On the challenging LogicCat and Spider2.0-Lite benchmarks, SteinerSQL establishes a new state-of-the-art with 36.10% and 40.04% execution accuracy, respectively, using Gemini-2.5-Pro. Beyond accuracy, SteinerSQL presents a new, unified paradigm for Text-to-SQL, paving the way for more robust and principled solutions to complex reasoning tasks.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.629761"
    },
    {
        "index": "#18",
        "title": "What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities",
        "link": "/arxiv/2509.19590",
        "arxiv_id": "2509.19590",
        "authors": "Nathanael Jo, Ashia Wilson",
        "summary": "Evaluations of generative models on benchmark data are now ubiquitous, and their outcomes critically shape public and scientific expectations of AI's capabilities. Yet growing skepticism surrounds their reliability. How can we know that a reported accuracy genuinely reflects a model's true performance? Evaluations are often presented as simple measurements, but in reality they are inferences: to treat benchmark scores as evidence of capability is already to assume a theory of what capability is and how it manifests in a test. We make this step explicit by proposing a principled framework for evaluation as inference: begin from a theory of capability, and then derive methods for estimating it. This perspective, familiar in fields such as psychometrics, has not yet become commonplace in AI evaluation. As a proof of concept, we address a central challenge that undermines reliability: sensitivity to perturbations. After formulating a model of ability, we introduce methods that infer ability while accounting for uncertainty from sensitivity and finite samples, including an adaptive algorithm that significantly reduces sample complexity. Together, these contributions lay the groundwork for more reliable and trustworthy estimates of AI capabilities as measured through benchmarks.",
        "subjects": "Artificial Intelligence, Computers and Society, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.629965"
    },
    {
        "index": "#19",
        "title": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics",
        "link": "/arxiv/2509.19566",
        "arxiv_id": "2509.19566",
        "authors": "George Hong, Daniel Trejo Banos",
        "summary": "We investigate the application of Small Language Models (<10 billion parameters) for genomics question answering via agentic framework to address hallucination issues and computational cost challenges. The Nano Bio-Agent (NBA) framework we implemented incorporates task decomposition, tool orchestration, and API access into well-established systems such as NCBI and AlphaGenome. Results show that SLMs combined with such agentic framework can achieve comparable and in many cases superior performance versus existing approaches utilising larger models, with our best model-agent combination achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B parameter models consistently achieve 85-97% accuracy while requiring much lower computational resources than conventional approaches. This demonstrates promising potential for efficiency gains, cost savings, and democratization of ML-powered genomics tools while retaining highly robust and accurate performance.",
        "subjects": "Artificial Intelligence, Genomics",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.635475"
    },
    {
        "index": "#20",
        "title": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation",
        "link": "/arxiv/2509.19524",
        "arxiv_id": "2509.19524",
        "authors": "Ramy ElMallah, Krish Chhajer, Chi-Guhn Lee",
        "summary": "Robot learning papers typically report a single binary success rate (SR), which obscures where a policy succeeds or fails along a multi-step manipulation task. We argue that subgoal-level reporting should become routine: for each trajectory, a vector of per-subgoal SRs that makes partial competence visible (e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware plug-in evaluation framework that utilizes vision-language models (VLMs) as automated judges of subgoal outcomes from recorded images or videos. Rather than proposing new benchmarks or APIs, our contribution is to outline design principles for a scalable, community-driven open-source project. In StepEval, the primary artifact for policy evaluation is the per-subgoal SR vector; however, other quantities (e.g., latency or cost estimates) are also considered for framework-optimization diagnostics to help the community tune evaluation efficiency and accuracy when ground-truth subgoal success labels are available. We discuss how such a framework can remain model-agnostic, support single- or multi-view inputs, and be lightweight enough to adopt across labs. The intended contribution is a shared direction: a minimal, extensible seed that invites open-source contributions, so that scoring the steps, not just the final goal, becomes a standard and reproducible practice.",
        "subjects": "Artificial Intelligence, Robotics",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.635851"
    },
    {
        "index": "#22",
        "title": "Estimating the Self-Consistency of LLMs",
        "link": "/arxiv/2509.19489",
        "arxiv_id": "2509.19489",
        "authors": "Robert Nowak",
        "summary": "Systems often repeat the same prompt to large language models (LLMs) and aggregate responses to improve reliability. This short note analyzes an estimator of the self-consistency of LLMs and the tradeoffs it induces under a fixed compute budget $B=mn$, where $m$ is the number of prompts sampled from the task distribution and $n$ is the number of repeated LLM calls per prompt; the resulting analysis favors a rough split $m,n\\propto\\sqrt{B}$.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.636405"
    },
    {
        "index": "#23",
        "title": "Evaluation-Aware Reinforcement Learning",
        "link": "/arxiv/2509.19464",
        "arxiv_id": "2509.19464",
        "authors": "Shripad Vilasrao Deshmukh, Will Schwarzer, Scott Niekum",
        "summary": "Policy evaluation is often a prerequisite for deploying safety- and performance-critical systems. Existing evaluation approaches frequently suffer from high variance due to limited data and long-horizon tasks, or high bias due to unequal support or inaccurate environmental models. We posit that these challenges arise, in part, from the standard reinforcement learning (RL) paradigm of policy learning without explicit consideration of evaluation. As an alternative, we propose evaluation-aware reinforcement learning (EvA-RL), in which a policy is trained to maximize expected return while simultaneously minimizing expected evaluation error under a given value prediction scheme -- in other words, being \"easy\" to evaluate. We formalize a framework for EvA-RL and design an instantiation that enables accurate policy evaluation, conditioned on a small number of rollouts in an assessment environment that can be different than the deployment environment. However, our theoretical analysis and empirical results show that there is often a tradeoff between evaluation accuracy and policy performance when using a fixed value-prediction scheme within EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an assessment-conditioned state-value predictor alongside the policy. Empirical results across diverse discrete and continuous action domains demonstrate that EvA-RL can substantially reduce evaluation error while maintaining competitive returns. This work lays the foundation for a broad new class of RL methods that treat reliable evaluation as a first-class principle during training.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.636717"
    },
    {
        "index": "#24",
        "title": "The Indispensable Role of User Simulation in the Pursuit of AGI",
        "link": "/arxiv/2509.19456",
        "arxiv_id": "2509.19456",
        "authors": "Krisztian Balog, ChengXiang Zhai",
        "summary": "Progress toward Artificial General Intelligence (AGI) faces significant bottlenecks, particularly in rigorously evaluating complex interactive systems and acquiring the vast interaction data needed for training adaptive agents. This paper posits that user simulation -- creating computational agents that mimic human interaction with AI systems -- is not merely a useful tool, but is a critical catalyst required to overcome these bottlenecks and accelerate AGI development. We argue that realistic simulators provide the necessary environments for scalable evaluation, data generation for interactive learning, and fostering the adaptive capabilities central to AGI. Therefore, research into user simulation technology and intelligent task agents are deeply synergistic and must advance hand-in-hand. This article elaborates on the critical role of user simulation for AGI, explores the interdisciplinary nature of building realistic simulators, identifies key challenges including those posed by large language models, and proposes a future research agenda.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.637069"
    },
    {
        "index": "#28",
        "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing",
        "link": "/arxiv/2509.20336",
        "arxiv_id": "2509.20336",
        "authors": "Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang",
        "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.638392"
    },
    {
        "index": "#30",
        "title": "RAG Security and Privacy: Formalizing the Threat Model and Attack Surface",
        "link": "/arxiv/2509.20324",
        "arxiv_id": "2509.20324",
        "authors": "Atousa Arzanipour, Rouzbeh Behnia, Reza Ebrahimi, Kaushik Dutta",
        "summary": "Retrieval-Augmented Generation (RAG) is an emerging approach in natural language processing that combines large language models (LLMs) with external document retrieval to produce more accurate and grounded responses. While RAG has shown strong potential in reducing hallucinations and improving factual consistency, it also introduces new privacy and security challenges that differ from those faced by traditional LLMs. Existing research has demonstrated that LLMs can leak sensitive information through training data memorization or adversarial prompts, and RAG systems inherit many of these vulnerabilities. At the same time, reliance of RAG on an external knowledge base opens new attack surfaces, including the potential for leaking information about the presence or content of retrieved documents, or for injecting malicious content to manipulate model behavior. Despite these risks, there is currently no formal framework that defines the threat landscape for RAG systems. In this paper, we address a critical gap in the literature by proposing, to the best of our knowledge, the first formal threat model for retrieval-RAG systems. We introduce a structured taxonomy of adversary types based on their access to model components and data, and we formally define key threat vectors such as document-level membership inference and data poisoning, which pose serious privacy and integrity risks in real-world deployments. By establishing formal definitions and attack models, our work lays the foundation for a more rigorous and principled understanding of privacy and security in RAG systems.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.638818"
    },
    {
        "index": "#34",
        "title": "When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity",
        "link": "/arxiv/2509.20293",
        "arxiv_id": "2509.20293",
        "authors": "Benjamin Feuer, Chiung-Yi Tseng, Astitwa Sarthak Lathe, Oussama Elachqar, John P Dickerson",
        "summary": "LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.639863"
    },
    {
        "index": "#35",
        "title": "PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction",
        "link": "/arxiv/2509.20290",
        "arxiv_id": "2509.20290",
        "authors": "Dayu Tan, Jing Chen, Xiaoping Zhou, Yansen Su, Chunhou Zheng",
        "summary": "Infectious diseases continue to pose a serious threat to public health, underscoring the urgent need for effective computational approaches to screen novel anti-infective agents. Oligopeptides have emerged as promising candidates in antimicrobial research due to their structural simplicity, high bioavailability, and low susceptibility to resistance. Despite their potential, computational models specifically designed to predict associations between oligopeptides and infectious diseases remain scarce. This study introduces a prompt-guided graph-based contrastive learning framework (PGCLODA) to uncover potential associations. A tripartite graph is constructed with oligopeptides, microbes, and diseases as nodes, incorporating both structural and semantic information. To preserve critical regions during contrastive learning, a prompt-guided graph augmentation strategy is employed to generate meaningful paired views. A dual encoder architecture, integrating Graph Convolutional Network (GCN) and Transformer, is used to jointly capture local and global features. The fused embeddings are subsequently input into a multilayer perceptron (MLP) classifier for final prediction. Experimental results on a benchmark dataset indicate that PGCLODA consistently outperforms state-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and hyperparameter studies confirm the contribution of each module. Case studies further validate the generalization ability of PGCLODA and its potential to uncover novel, biologically relevant associations. These findings offer valuable insights for mechanism-driven discovery and oligopeptide-based drug development. The source code of PGCLODA is available online at https://github.com/jjnlcode/PGCLODA.",
        "subjects": "Machine Learning, Artificial Intelligence, Quantitative Methods",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.640068"
    },
    {
        "index": "#37",
        "title": "Investigating Security Implications of Automatically Generated Code on the Software Supply Chain",
        "link": "/arxiv/2509.20277",
        "arxiv_id": "2509.20277",
        "authors": "Xiaofan Li, Xing Gao",
        "summary": "In recent years, various software supply chain (SSC) attacks have posed significant risks to the global community. Severe consequences may arise if developers integrate insecure code snippets that are vulnerable to SSC attacks into their products. Particularly, code generation techniques, such as large language models (LLMs), have been widely utilized in the developer community. However, LLMs are known to suffer from inherent issues when generating code, including fabrication, misinformation, and reliance on outdated training data, all of which can result in serious software supply chain threats. In this paper, we investigate the security threats to the SSC that arise from these inherent issues. We examine three categories of threats, including eleven potential SSC-related threats, related to external components in source code, and continuous integration configuration files. We find some threats in LLM-generated code could enable attackers to hijack software and workflows, while some others might cause potential hidden threats that compromise the security of the software over time. To understand these security impacts and severity, we design a tool, SSCGuard, to generate 439,138 prompts based on SSC-related questions collected online, and analyze the responses of four popular LLMs from GPT and Llama. Our results show that all identified SSC-related threats persistently exist. To mitigate these risks, we propose a novel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce fabrication, and a middleware-based defense that informs users of various SSC threats.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.645950"
    },
    {
        "index": "#38",
        "title": "AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving",
        "link": "/arxiv/2509.20253",
        "arxiv_id": "2509.20253",
        "authors": "Jinhao Chai, Anqing Jiang, Hao Jiang, Shiyi Mu, Zichong Gu, Shugong Xu",
        "summary": "End-to-end multi-modal planning has become a transformative paradigm in autonomous driving, effectively addressing behavioral multi-modality and the generalization challenge in long-tail scenarios. We propose AnchDrive, a framework for end-to-end driving that effectively bootstraps a diffusion policy to mitigate the high computational cost of traditional generative models. Rather than denoising from pure noise, AnchDrive initializes its planner with a rich set of hybrid trajectory anchors. These anchors are derived from two complementary sources: a static vocabulary of general driving priors and a set of dynamic, context-aware trajectories. The dynamic trajectories are decoded in real-time by a Transformer that processes dense and sparse perceptual features. The diffusion model then learns to refine these anchors by predicting a distribution of trajectory offsets, enabling fine-grained refinement. This anchor-based bootstrapping design allows for efficient generation of diverse, high-quality trajectories. Experiments on the NAVSIM benchmark confirm that AnchDrive sets a new state-of-the-art and shows strong gen?eralizability",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.646309"
    },
    {
        "index": "#39",
        "title": "A HyperGraphMamba-Based Multichannel Adaptive Model for ncRNA Classification",
        "link": "/arxiv/2509.20240",
        "arxiv_id": "2509.20240",
        "authors": "Xin An, Ruijie Li, Qiao Ning, Hui Li, Qian Ma, Shikai Guo",
        "summary": "Non-coding RNAs (ncRNAs) play pivotal roles in gene expression regulation and the pathogenesis of various diseases. Accurate classification of ncRNAs is essential for functional annotation and disease diagnosis. To address existing limitations in feature extraction depth and multimodal fusion, we propose HGMamba-ncRNA, a HyperGraphMamba-based multichannel adaptive model, which integrates sequence, secondary structure, and optionally available expression features of ncRNAs to enhance classification performance. Specifically, the sequence of ncRNA is modeled using a parallel Multi-scale Convolution and LSTM architecture (MKC-L) to capture both local patterns and long-range dependencies of nucleotides. The structure modality employs a multi-scale graph transformer (MSGraphTransformer) to represent the multi-level topological characteristics of ncRNA secondary structures. The expression modality utilizes a Chebyshev Polynomial-based Kolmogorov-Arnold Network (CPKAN) to effectively model and interpret high-dimensional expression profiles. Finally, by incorporating virtual nodes to facilitate efficient and comprehensive multimodal interaction, HyperGraphMamba is proposed to adaptively align and integrate multichannel heterogeneous modality features. Experiments conducted on three public datasets demonstrate that HGMamba-ncRNA consistently outperforms state-of-the-art methods in terms of accuracy and other metrics. Extensive empirical studies further confirm the model's robustness, effectiveness, and strong transferability, offering a novel and reliable strategy for complex ncRNA functional classification. Code and datasets are available at https://anonymous.4open.science/r/HGMamba-ncRNA-94D0.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.646646"
    },
    {
        "index": "#41",
        "title": "Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization",
        "link": "/arxiv/2509.20230",
        "arxiv_id": "2509.20230",
        "authors": "Wenhan Wu, Zheyuan Liu, Chongyang Gao, Ren Wang, Kaize Ding",
        "summary": "Current LLM unlearning methods face a critical security vulnerability that undermines their fundamental purpose: while they appear to successfully remove sensitive or harmful knowledge, this ``forgotten\" information remains precariously recoverable through relearning attacks. We identify that the root cause is that conventional methods optimizing the forgetting loss at individual data points will drive model parameters toward sharp minima in the loss landscape. In these unstable regions, even minimal parameter perturbations can drastically alter the model's behaviors. Consequently, relearning attacks exploit this vulnerability by using just a few fine-tuning samples to navigate the steep gradients surrounding these unstable regions, thereby rapidly recovering knowledge that was supposedly erased. This exposes a critical robustness gap between apparent unlearning and actual knowledge removal. To address this issue, we propose StableUN, a bi-level feedback-guided optimization framework that explicitly seeks more stable parameter regions via neighborhood-aware optimization. It integrates forgetting feedback, which uses adversarial perturbations to probe parameter neighborhoods, with remembering feedback to preserve model utility, aligning the two objectives through gradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that our method is significantly more robust against both relearning and jailbreaking attacks while maintaining competitive utility performance.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.647093"
    },
    {
        "index": "#42",
        "title": "Multimodal Representation-disentangled Information Bottleneck for Multimodal Recommendation",
        "link": "/arxiv/2509.20225",
        "arxiv_id": "2509.20225",
        "authors": "Hui Wang, Jinghui Qin, Wushao Wen, Qingling Li, Shanshan Zhong, Zhongzhan Huang",
        "summary": "Multimodal data has significantly advanced recommendation systems by integrating diverse information sources to model user preferences and item characteristics. However, these systems often struggle with redundant and irrelevant information, which can degrade performance. Most existing methods either fuse multimodal information directly or use rigid architectural separation for disentanglement, failing to adequately filter noise and model the complex interplay between modalities. To address these challenges, we propose a novel framework, the Multimodal Representation-disentangled Information Bottleneck (MRdIB). Concretely, we first employ a Multimodal Information Bottleneck to compress the input representations, effectively filtering out task-irrelevant noise while preserving rich semantic information. Then, we decompose the information based on its relationship with the recommendation target into unique, redundant, and synergistic components. We achieve this decomposition with a series of constraints: a unique information learning objective to preserve modality-unique signals, a redundant information learning objective to minimize overlap, and a synergistic information learning objective to capture emergent information. By optimizing these objectives, MRdIB guides a model to learn more powerful and disentangled representations. Extensive experiments on several competitive models and three benchmark datasets demonstrate the effectiveness and versatility of our MRdIB in enhancing multimodal recommendation.",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.647382"
    },
    {
        "index": "#43",
        "title": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation",
        "link": "/arxiv/2509.20215",
        "arxiv_id": "2509.20215",
        "authors": "Guang Yang, Wei Zheng, Xiang Chen, Yifan Sun, Fengji Zhang, Terry Yue Zhuo",
        "summary": "LLMs face significant challenges in Verilog generation due to limited domain-specific knowledge. While sampling techniques improve pass@k metrics, hardware engineers need one trustworthy solution rather than uncertain candidates. To bridge this gap, we formulate it as a semantic alignment problem between requirements and Verilog implementations, and propose VCD-RNK, a discriminator model tailored for efficient Verilog code reranking. Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling expert knowledge across three dimensions: code semantic analysis, test case generation, and functional correctness assessment. By explicitly simulating the above reasoning processes during inference, VCD-RNK effectively avoids computationally intensive test execution in existing methods.",
        "subjects": "Software Engineering, Artificial Intelligence, Hardware Architecture",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.647609"
    },
    {
        "index": "#44",
        "title": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment",
        "link": "/arxiv/2509.20214",
        "arxiv_id": "2509.20214",
        "authors": "Deokjae Lee, Hyun Oh Song",
        "summary": "We study weight-only post-training quantization (PTQ), which quantizes the weights of a large language model (LLM) without retraining, using little or no calibration data. Weight-only PTQ is crucial for reducing the memory footprint and latency of LLM inference, especially in memory-bound, small-batch inference scenarios, such as personalized inference on edge devices. Despite its importance, irregular weight distributions with heavy-tailed outliers in LLMs complicate quantization, recently motivating rotation-based methods that transform weights into near-Gaussian distributions, which are more regular with fewer outliers, thereby reducing quantization error. In this work, we first derive the information-theoretically optimal bit allocation for Gaussianized weights under given bit budgets, revealing that fine-grained fractional-bit quantizers approaching the Gaussian distortion-rate bound are essential to achieve near-optimal quantization performance. To bridge this theoretical insight and practical implementation, we introduce Q-Palette, a versatile collection of fractional-bit quantizers that range from trellis-coded quantizers offering near-optimal distortion to simpler vector and scalar quantizers optimized for faster inference, all efficiently implemented with optimized CUDA kernels across various bitwidths. Furthermore, leveraging Q-Palette as a foundational component, we propose a novel mixed-scheme quantization framework, jointly optimizing quantizer choices and layer fusion decisions given resource constraints. The code is available at https://github.com/snu-mllab/Q-Palette.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.647788"
    },
    {
        "index": "#47",
        "title": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation",
        "link": "/arxiv/2509.20190",
        "arxiv_id": "2509.20190",
        "authors": "Tanmay Khule, Stefan Marksteiner, Jose Alguindigue, Hannes Fuchs, Sebastian Fischmeister, Apurva Narayan",
        "summary": "In modern automotive development, security testing is critical for safeguarding systems against increasingly advanced threats. Attack trees are widely used to systematically represent potential attack vectors, but generating comprehensive test cases from these trees remains a labor-intensive, error-prone task that has seen limited automation in the context of testing vehicular systems. This paper introduces STAF (Security Test Automation Framework), a novel approach to automating security test case generation. Leveraging Large Language Models (LLMs) and a four-step self-corrective Retrieval-Augmented Generation (RAG) framework, STAF automates the generation of executable security test cases from attack trees, providing an end-to-end solution that encompasses the entire attack surface. We particularly show the elements and processes needed to provide an LLM to actually produce sensible and executable automotive security test suites, along with the integration with an automated testing framework. We further compare our tailored approach with general purpose (vanilla) LLMs and the performance of different LLMs (namely GPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our operation step-by-step in a concrete case study. Our results show significant improvements in efficiency, accuracy, scalability, and easy integration in any workflow, marking a substantial advancement in automating automotive security testing methodologies. Using TARAs as an input for verfication tests, we create synergies by connecting two vital elements of a secure automotive development process.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.648363"
    },
    {
        "index": "#48",
        "title": "How People Manage Knowledge in their \"Second Brains\"- A Case Study with Industry Researchers Using Obsidian",
        "link": "/arxiv/2509.20187",
        "arxiv_id": "2509.20187",
        "authors": "Juliana Jansen Ferreira, Vincius Segura, Joana Gabriela Souza, Joao Henrique Gallas Brasil",
        "summary": "People face overwhelming information during work activities, necessitating effective organization and management strategies. Even in personal lives, individuals must keep, annotate, organize, and retrieve knowledge from daily routines. The collection of records for future reference is known as a personal knowledge base. Note-taking applications are valuable tools for building and maintaining these bases, often called a ''second brain''. This paper presents a case study on how people build and explore personal knowledge bases for various purposes. We selected the note-taking tool Obsidian and researchers from a Brazilian lab for an in-depth investigation. Our investigation reveals interesting findings about how researchers build and explore their personal knowledge bases. A key finding is that participants' knowledge retrieval strategy influences how they build and maintain their content. We suggest potential features for an AI system to support this process.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.648555"
    },
    {
        "index": "#49",
        "title": "An Improved Time Series Anomaly Detection by Applying Structural Similarity",
        "link": "/arxiv/2509.20184",
        "arxiv_id": "2509.20184",
        "authors": "Tiejun Wang, Rui Wang, Xudong Mou, Mengyuan Ma, Tianyu Wo, Renyu Yang, Xudong Liu",
        "summary": "Effective anomaly detection in time series is pivotal for modern industrial applications and financial systems. Due to the scarcity of anomaly labels and the high cost of manual labeling, reconstruction-based unsupervised approaches have garnered considerable attention. However, accurate anomaly detection remains an unsettled challenge, since the optimization objectives of reconstruction-based methods merely rely on point-by-point distance measures, ignoring the potential structural characteristics of time series and thus failing to tackle complex pattern-wise anomalies. In this paper, we propose StrAD, a novel structure-enhanced anomaly detection approach to enrich the optimization objective by incorporating structural information hidden in the time series and steering the data reconstruction procedure to better capture such structural features. StrAD accommodates the trend, seasonality, and shape in the optimization objective of the reconstruction model to learn latent structural characteristics and capture the intrinsic pattern variation of time series. The proposed structure-aware optimization objective mechanism can assure the alignment between the original data and the reconstructed data in terms of structural features, thereby keeping consistency in global fluctuation and local characteristics. The mechanism is pluggable and applicable to any reconstruction-based methods, enhancing the model sensitivity to both point-wise anomalies and pattern-wise anomalies. Experimental results show that StrAD improves the performance of state-of-the-art reconstruction-based models across five real-world anomaly detection datasets.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.648760"
    },
    {
        "index": "#50",
        "title": "Automated Multi-Agent Workflows for RTL Design",
        "link": "/arxiv/2509.20182",
        "arxiv_id": "2509.20182",
        "authors": "Amulya Bhattaram, Janani Ramamoorthy, Ranit Gupta, Diana Marculescu, Dimitrios Stamoulis",
        "summary": "The rise of agentic AI workflows unlocks novel opportunities for computer systems design and optimization. However, for specialized domains such as program synthesis, the relative scarcity of HDL and proprietary EDA resources online compared to more common programming tasks introduces challenges, often necessitating task-specific fine-tuning, high inference costs, and manually-crafted agent orchestration. In this work, we present VeriMaAS, a multi-agent framework designed to automatically compose agentic workflows for RTL code generation. Our key insight is to integrate formal verification feedback from HDL tools directly into workflow generation, reducing the cost of gradient-based updates or prolonged reasoning traces. Our method improves synthesis performance by 5-7% for pass@k over fine-tuned baselines, while requiring only a few hundred training examples, representing an order-of-magnitude reduction in supervision cost.",
        "subjects": "Hardware Architecture, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.648951"
    },
    {
        "index": "#51",
        "title": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning",
        "link": "/arxiv/2509.20166",
        "arxiv_id": "2509.20166",
        "authors": "Lauren Deason, Adam Bali, Ciprian Bejean, Diana Bolocan, James Crnkovich, Ioana Croitoru, Krishna Durai, Chase Midler, Calin Miron, David Molnar, Brad Moon, Bruno Ostarcevic, Alberto Peltea, Matt Rosenberg, Catalin Sandu, Arthur Saputkin, Sagar Shah, Daniel Stan, Ernest Szocs, Shengye Wan, Spencer Whitman, Sven Krasser, Joshua Saxe",
        "summary": "Today's cyber defenders are overwhelmed by a deluge of security alerts, threat intelligence signals, and shifting business context, creating an urgent need for AI systems to enhance operational security work. While Large Language Models (LLMs) have the potential to automate and scale Security Operations Center (SOC) operations, existing evaluations do not fully assess the scenarios most relevant to real-world defenders. This lack of informed evaluation impacts both AI developers and those applying LLMs to SOC automation. Without clear insight into LLM performance in real-world security scenarios, developers lack a north star for development, and users cannot reliably select the most effective models. Meanwhile, malicious actors are using AI to scale cyber attacks, highlighting the need for open source benchmarks to drive adoption and community-driven improvement among defenders and model developers. To address this, we introduce CyberSOCEval, a new suite of open source benchmarks within CyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in two tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive domains with inadequate coverage in current benchmarks. Our evaluations show that larger, more modern LLMs tend to perform better, confirming the training scaling laws paradigm. We also find that reasoning models leveraging test time scaling do not achieve the same boost as in coding and math, suggesting these models have not been trained to reason about cybersecurity analysis, and pointing to a key opportunity for improvement. Finally, current LLMs are far from saturating our evaluations, showing that CyberSOCEval presents a significant challenge for AI developers to improve cyber defense capabilities.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.649309"
    },
    {
        "index": "#54",
        "title": "Affective Computing and Emotional Data: Challenges and Implications in Privacy Regulations, The AI Act, and Ethics in Large Language Models",
        "link": "/arxiv/2509.20153",
        "arxiv_id": "2509.20153",
        "authors": "Nicola Fabiano",
        "summary": "This paper examines the integration of emotional intelligence into artificial intelligence systems, with a focus on affective computing and the growing capabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to recognize and respond to human emotions. Drawing on interdisciplinary research that combines computer science, psychology, and neuroscience, the study analyzes foundational neural architectures - CNNs for processing facial expressions and RNNs for sequential data, such as speech and text - that enable emotion recognition. It examines the transformation of human emotional experiences into structured emotional data, addressing the distinction between explicit emotional data collected with informed consent in research settings and implicit data gathered passively through everyday digital interactions. That raises critical concerns about lawful processing, AI transparency, and individual autonomy over emotional expressions in digital environments. The paper explores implications across various domains, including healthcare, education, and customer service, while addressing challenges of cultural variations in emotional expression and potential biases in emotion recognition systems across different demographic groups. From a regulatory perspective, the paper examines emotional data in the context of the GDPR and the EU AI Act frameworks, highlighting how emotional data may be considered sensitive personal data that requires robust safeguards, including purpose limitation, data minimization, and meaningful consent mechanisms.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.649882"
    },
    {
        "index": "#57",
        "title": "Discovering Association Rules in High-Dimensional Small Tabular Data",
        "link": "/arxiv/2509.20113",
        "arxiv_id": "2509.20113",
        "authors": "Erkan Karabulut, Daniel Daza, Paul Groth, Victoria Degeler",
        "summary": "Association Rule Mining (ARM) aims to discover patterns between features in datasets in the form of propositional rules, supporting both knowledge discovery and interpretable machine learning in high-stakes decision-making. However, in high-dimensional settings, rule explosion and computational overhead render popular algorithmic approaches impractical without effective search space reduction, challenges that propagate to downstream tasks. Neurosymbolic methods, such as Aerial+, have recently been proposed to address the rule explosion in ARM. While they tackle the high dimensionality of the data, they also inherit limitations of neural networks, particularly reduced performance in low-data regimes. This paper makes three key contributions to association rule discovery in high-dimensional tabular data. First, we empirically show that Aerial+ scales one to two orders of magnitude better than state-of-the-art algorithmic and neurosymbolic baselines across five real-world datasets. Second, we introduce the novel problem of ARM in high-dimensional, low-data settings, such as gene expression data from the biomedicine domain with around 18k features and 50 samples. Third, we propose two fine-tuning approaches to Aerial+ using tabular foundation models. Our proposed approaches are shown to significantly improve rule quality on five real-world datasets, demonstrating their effectiveness in low-data, high-dimensional scenarios.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.650519"
    },
    {
        "index": "#63",
        "title": "One Filters All: A Generalist Filter for State Estimation",
        "link": "/arxiv/2509.20051",
        "arxiv_id": "2509.20051",
        "authors": "Shiqi Liu, Wenhan Cao, Chang Liu, Zeyu He, Tianyi Zhang, Shengbo Eben Li",
        "summary": "Estimating hidden states in dynamical systems, also known as optimal filtering, is a long-standing problem in various fields of science and engineering. In this paper, we introduce a general filtering framework, \\textbf{LLM-Filter}, which leverages large language models (LLMs) for state estimation by embedding noisy observations with text prototypes. In various experiments for classical dynamical systems, we find that first, state estimation can significantly benefit from the reasoning knowledge embedded in pre-trained LLMs. By achieving proper modality alignment with the frozen LLM, LLM-Filter outperforms the state-of-the-art learning-based approaches. Second, we carefully design the prompt structure, System-as-Prompt (SaP), incorporating task instructions that enable the LLM to understand the estimation tasks. Guided by these prompts, LLM-Filter exhibits exceptional generalization, capable of performing filtering tasks accurately in changed or even unseen environments. We further observe a scaling-law behavior in LLM-Filter, where accuracy improves with larger model sizes and longer training times. These findings make LLM-Filter a promising foundation model of filtering.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.657850"
    },
    {
        "index": "#64",
        "title": "Projective Kolmogorov Arnold Neural Networks (P-KANs): Entropy-Driven Functional Space Discovery for Interpretable Machine Learning",
        "link": "/arxiv/2509.20049",
        "arxiv_id": "2509.20049",
        "authors": "Alastair Poole, Stig McArthur, Saravan Kumar",
        "summary": "Kolmogorov-Arnold Networks (KANs) relocate learnable nonlinearities from nodes to edges, demonstrating remarkable capabilities in scientific machine learning and interpretable modeling. However, current KAN implementations suffer from fundamental inefficiencies due to redundancy in high-dimensional spline parameter spaces, where numerous distinct parameterisations yield functionally equivalent behaviors. This redundancy manifests as a \"nuisance space\" in the model's Jacobian, leading to susceptibility to overfitting and poor generalization. We introduce Projective Kolmogorov-Arnold Networks (P-KANs), a novel training framework that guides edge function discovery towards interpretable functional representations through entropy-minimisation techniques from signal analysis and sparse dictionary learning. Rather than constraining functions to predetermined spaces, our approach maintains spline space flexibility while introducing \"gravitational\" terms that encourage convergence towards optimal functional representations. Our key insight recognizes that optimal representations can be identified through entropy analysis of projection coefficients, compressing edge functions to lower-parameter projective spaces (Fourier, Chebyshev, Bessel). P-KANs demonstrate superior performance across multiple domains, achieving up to 80% parameter reduction while maintaining representational capacity, significantly improved robustness to noise compared to standard KANs, and successful application to industrial automated fiber placement prediction. Our approach enables automatic discovery of mixed functional representations where different edges converge to different optimal spaces, providing both compression benefits and enhanced interpretability for scientific machine learning applications.",
        "subjects": "Neural and Evolutionary Computing, Artificial Intelligence, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.658068"
    },
    {
        "index": "#65",
        "title": "Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations",
        "link": "/arxiv/2509.20048",
        "arxiv_id": "2509.20048",
        "authors": "Rami Zewail",
        "summary": "Learning robust representations for biosignals is often hampered by the challenge of designing effective data augmentations.Traditional methods can fail to capture the complex variations inherent in physiological data. Within this context, we propose a novel hybrid framework, Diffusion-Augmented Contrastive Learning (DACL), that fuses concepts from diffusion models and supervised contrastive learning. The DACL framework operates on a latent space created by a lightweight Variational Autoencoder (VAE) trained on our novel Scattering Transformer (ST) features [12]. It utilizes the diffusion forward process as a principled data augmentation technique to generate multiple noisy views of these latent embeddings. A U-Net style encoder is then trained with a supervised contrastive objective to learn a representation that balances class discrimination with robustness to noise across various diffusion time steps. We evaluated this proof-of-concept method on the PhysioNet 2017 ECG dataset, achieving a competitive AUROC of 0.7815. This work establishes a new paradigm for representation learning by using the diffusion process itself to drive the contrastive objective, creating noise-invariant embeddings that demonstrate a strong foundation for class separability.",
        "subjects": "Machine Learning, Artificial Intelligence, Signal Processing",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.658294"
    },
    {
        "index": "#70",
        "title": "Choosing to Be Green: Advancing Green AI via Dynamic Model Selection",
        "link": "/arxiv/2509.19996",
        "arxiv_id": "2509.19996",
        "authors": "Emilio Cruciani, Roberto Verdecchia",
        "summary": "Artificial Intelligence is increasingly pervasive across domains, with ever more complex models delivering impressive predictive performance. This fast technological advancement however comes at a concerning environmental cost, with state-of-the-art models - particularly deep neural networks and large language models - requiring substantial computational resources and energy. In this work, we present the intuition of Green AI dynamic model selection, an approach based on dynamic model selection that aims at reducing the environmental footprint of AI by selecting the most sustainable model while minimizing potential accuracy loss. Specifically, our approach takes into account the inference task, the environmental sustainability of available models, and accuracy requirements to dynamically choose the most suitable model. Our approach presents two different methods, namely Green AI dynamic model cascading and Green AI dynamic model routing. We demonstrate the effectiveness of our approach via a proof of concept empirical example based on a real-world dataset. Our results show that Green AI dynamic model selection can achieve substantial energy savings (up to ~25%) while substantially retaining the accuracy of the most energy greedy solution (up to ~95%). As conclusion, our preliminary findings highlight the potential that hybrid, adaptive model selection strategies withhold to mitigate the energy demands of modern AI systems without significantly compromising accuracy requirements.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.659332"
    },
    {
        "index": "#72",
        "title": "An effective control of large systems of active particles: An application to evacuation problem",
        "link": "/arxiv/2509.19972",
        "arxiv_id": "2509.19972",
        "authors": "Albina Klepach, Egor E. Nuzhin, Alexey A. Tsukanov, Nikolay V. Brilliantov",
        "summary": "Manipulation of large systems of active particles is a serious challenge across diverse domains, including crowd management, control of robotic swarms, and coordinated material transport. The development of advanced control strategies for complex scenarios is hindered, however, by the lack of scalability and robustness of the existing methods, in particular, due to the need of an individual control for each agent. One possible solution involves controlling a system through a leader or a group of leaders, which other agents tend to follow. Using such an approach we develop an effective control strategy for a leader, combining reinforcement learning (RL) with artificial forces acting on the system. To describe the guidance of active particles by a leader we introduce the generalized Vicsek model. This novel method is then applied to the problem of the effective evacuation by a robot-rescuer (leader) of large groups of people from hazardous places. We demonstrate, that while a straightforward application of RL yields suboptimal results, even for advanced architectures, our approach provides a robust and efficient evacuation strategy. The source code supporting this study is publicly available at: https://github.com/cinemere/evacuation.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.659760"
    },
    {
        "index": "#73",
        "title": "2025 Southeast Asia Eleven Nations Influence Index Report",
        "link": "/arxiv/2509.19953",
        "arxiv_id": "2509.19953",
        "authors": "Wei Meng",
        "summary": "This study constructs a fully data-driven and reproducible Southeast Asia Influence Index (SAII v3) to reduce bias from expert scoring and subjective weighting while mapping hierarchical power structures across the eleven ASEAN nations. We aggregate authoritative open-source indicators across four dimensions (economic, military, diplomatic, socio-technological) and apply a three-tiered standardization chain quantile-Box-Cox-min-max to mitigate outliers and skewness. Weights are obtained through equal-weight integration of Entropy Weighting Method (EWM), CRITIC, and PCA. Robustness is assessed via Kendall's tau, +/-20% weight perturbation, and 10,000 bootstrap iterations, with additional checks including +/-10% dimensional sensitivity and V2-V3 bump chart comparisons. Results show integrated weights: Economy 35-40%, Military 20-25%, Diplomacy about 20%, Socio-Technology about 15%. The regional landscape exhibits a one-strong, two-medium, three-stable, and multiple-weak pattern: Indonesia, Singapore, and Malaysia lead, while Thailand, the Philippines, and Vietnam form a mid-tier competitive band. V2 and V3 rankings are highly consistent (Kendall's tau = 0.818), though small mid-tier reorderings appear (Thailand and the Philippines rise, Vietnam falls), indicating that v3 is more sensitive to structural equilibrium. ASEAN-11 average sensitivity highlights military and socio-technological dimensions as having the largest marginal effects (+/-0.002). In conclusion, SAII v3 delivers algorithmic weighting and auditable reproducibility, reveals multidimensional drivers of influence in Southeast Asia, and provides actionable quantitative evidence for resource allocation and policy prioritization by regional governments and external partners.",
        "subjects": "Physics and Society, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.659939"
    },
    {
        "index": "#75",
        "title": "A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers",
        "link": "/arxiv/2509.19947",
        "arxiv_id": "2509.19947",
        "authors": "Zhixiao Wu, Yao Lu, Jie Wen, Hao Sun, Qi Zhou, Guangming Lu",
        "summary": "Poison-only Clean-label Backdoor Attacks aim to covertly inject attacker-desired behavior into DNNs by merely poisoning the dataset without changing the labels. To effectively implant a backdoor, multiple \\textbf{triggers} are proposed for various attack requirements of Attack Success Rate (ASR) and stealthiness. Additionally, sample selection enhances clean-label backdoor attacks' ASR by meticulously selecting ``hard'' samples instead of random samples to poison. Current methods 1) usually handle the sample selection and triggers in isolation, leading to severely limited improvements on both ASR and stealthiness. Consequently, attacks exhibit unsatisfactory performance on evaluation metrics when converted to PCBAs via a mere stacking of methods. Therefore, we seek to explore the bidirectional collaborative relations between the sample selection and triggers to address the above dilemma. 2) Since the strong specificity within triggers, the simple combination of sample selection and triggers fails to substantially enhance both evaluation metrics, with generalization preserved among various attacks. Therefore, we seek to propose a set of components to significantly improve both stealthiness and ASR based on the commonalities of attacks. Specifically, Component A ascertains two critical selection factors, and then makes them an appropriate combination based on the trigger scale to select more reasonable ``hard'' samples for improving ASR. Component B is proposed to select samples with similarities to relevant trigger implanted samples to promote stealthiness. Component C reassigns trigger poisoning intensity on RGB colors through distinct sensitivity of the human visual system to RGB for higher ASR, with stealthiness ensured by sample selection, including Component B. Furthermore, all components can be strategically integrated into diverse PCBAs.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.660422"
    },
    {
        "index": "#79",
        "title": "TABFAIRGDT: A Fast Fair Tabular Data Generator using Autoregressive Decision Trees",
        "link": "/arxiv/2509.19927",
        "arxiv_id": "2509.19927",
        "authors": "Emmanouil Panagiotou, Benot Ronval, Arjun Roy, Ludwig Bothmann, Bernd Bischl, Siegfried Nijssen, Eirini Ntoutsi",
        "summary": "Ensuring fairness in machine learning remains a significant challenge, as models often inherit biases from their training data. Generative models have recently emerged as a promising approach to mitigate bias at the data level while preserving utility. However, many rely on deep architectures, despite evidence that simpler models can be highly effective for tabular data. In this work, we introduce TABFAIRGDT, a novel method for generating fair synthetic tabular data using autoregressive decision trees. To enforce fairness, we propose a soft leaf resampling technique that adjusts decision tree outputs to reduce bias while preserving predictive performance. Our approach is non-parametric, effectively capturing complex relationships between mixed feature types, without relying on assumptions about the underlying data distributions. We evaluate TABFAIRGDT on benchmark fairness datasets and demonstrate that it outperforms state-of-the-art (SOTA) deep generative models, achieving better fairness-utility trade-off for downstream tasks, as well as higher synthetic data quality. Moreover, our method is lightweight, highly efficient, and CPU-compatible, requiring no data pre-processing. Remarkably, TABFAIRGDT achieves a 72% average speedup over the fastest SOTA baseline across various dataset sizes, and can generate fair synthetic data for medium-sized datasets (10 features, 10K samples) in just one second on a standard CPU, making it an ideal solution for real-world fairness-sensitive applications.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.667216"
    },
    {
        "index": "#80",
        "title": "Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches",
        "link": "/arxiv/2509.19924",
        "arxiv_id": "2509.19924",
        "authors": "Remo Sasso, Michelangelo Conserva, Dominik Jeurissen, Paulo Rauber",
        "summary": "Exploration in reinforcement learning (RL) remains challenging, particularly in sparse-reward settings. While foundation models possess strong semantic priors, their capabilities as zero-shot exploration agents in classic RL benchmarks are not well understood. We benchmark LLMs and VLMs on multi-armed bandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Our investigation reveals a key limitation: while VLMs can infer high-level objectives from visual input, they consistently fail at precise low-level control: the \"knowing-doing gap\". To analyze a potential bridge for this gap, we investigate a simple on-policy hybrid framework in a controlled, best-case scenario. Our results in this idealized setting show that VLM guidance can significantly improve early-stage sample efficiency, providing a clear analysis of the potential and constraints of using foundation models to guide exploration rather than for end-to-end control.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.667490"
    },
    {
        "index": "#81",
        "title": "Towards Self-Supervised Foundation Models for Critical Care Time Series",
        "link": "/arxiv/2509.19885",
        "arxiv_id": "2509.19885",
        "authors": "Katja Naasunnguaq Jagd, Rachael DeVries, Ole Winther",
        "summary": "Domain-specific foundation models for healthcare have expanded rapidly in recent years, yet foundation models for critical care time series remain relatively underexplored due to the limited size and availability of datasets. In this work, we introduce an early-stage pre-trained foundation model for critical care time-series based on the Bi-Axial Transformer (BAT), trained on pooled electronic health record datasets. We demonstrate effective transfer learning by fine-tuning the model on a dataset distinct from the training sources for mortality prediction, where it outperforms supervised baselines, particularly for small datasets ($<5,000$). These contributions highlight the potential of self-supervised foundation models for critical care times series to support generalizable and robust clinical applications in resource-limited settings.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.667795"
    },
    {
        "index": "#82",
        "title": "CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance",
        "link": "/arxiv/2509.19883",
        "arxiv_id": "2509.19883",
        "authors": "Junchuan Zhao, Wei Zeng, Tianle Lyu, Ye Wang",
        "summary": "Singing Voice Synthesis (SVS) aims to generate expressive vocal performances from structured musical inputs such as lyrics and pitch sequences. While recent progress in discrete codec-based speech synthesis has enabled zero-shot generation via in-context learning, directly extending these techniques to SVS remains non-trivial due to the requirement for precise melody control. In particular, prompt-based generation often introduces prosody leakage, where pitch information is inadvertently entangled within the timbre prompt, compromising controllability. We present CoMelSinger, a zero-shot SVS framework that enables structured and disentangled melody control within a discrete codec modeling paradigm. Built on the non-autoregressive MaskGCT architecture, CoMelSinger replaces conventional text inputs with lyric and pitch tokens, preserving in-context generalization while enhancing melody conditioning. To suppress prosody leakage, we propose a coarse-to-fine contrastive learning strategy that explicitly regularizes pitch redundancy between the acoustic prompt and melody input. Furthermore, we incorporate a lightweight encoder-only Singing Voice Transcription (SVT) module to align acoustic tokens with pitch and duration, offering fine-grained frame-level supervision. Experimental results demonstrate that CoMelSinger achieves notable improvements in pitch accuracy, timbre consistency, and zero-shot transferability over competitive baselines.",
        "subjects": "Sound, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.668057"
    },
    {
        "index": "#84",
        "title": "Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials",
        "link": "/arxiv/2509.19877",
        "arxiv_id": "2509.19877",
        "authors": "Shi Yin, Zujian Dai, Xinyang Pan, Lixin He",
        "summary": "Deep learning methods for electronic-structure Hamiltonian prediction has offered significant computational efficiency advantages over traditional DFT methods, yet the diversity of atomic types, structural patterns, and the high-dimensional complexity of Hamiltonians pose substantial challenges to the generalization performance. In this work, we contribute on both the methodology and dataset sides to advance universal deep learning paradigm for Hamiltonian prediction. On the method side, we propose NextHAM, a neural E(3)-symmetry and expressive correction method for efficient and generalizable materials electronic-structure Hamiltonian prediction. First, we introduce the zeroth-step Hamiltonians, which can be efficiently constructed by the initial charge density of DFT, as informative descriptors of neural regression model in the input level and initial estimates of the target Hamiltonian in the output level, so that the regression model directly predicts the correction terms to the target ground truths, thereby significantly simplifying the input-output mapping for learning. Second, we present a neural Transformer architecture with strict E(3)-Symmetry and high non-linear expressiveness for Hamiltonian prediction. Third, we propose a novel training objective to ensure the accuracy performance of Hamiltonians in both real space and reciprocal space, preventing error amplification and the occurrence of \"ghost states\" caused by the large condition number of the overlap matrix. On the dataset side, we curate a high-quality broad-coverage large benchmark, namely Materials-HAM-SOC, comprising 17,000 material structures spanning 68 elements from six rows of the periodic table and explicitly incorporating SOC effects. Experimental results on Materials-HAM-SOC demonstrate that NextHAM achieves excellent accuracy and efficiency in predicting Hamiltonians and band structures.",
        "subjects": "Machine Learning, Materials Science, Artificial Intelligence, Computational Physics",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.668625"
    },
    {
        "index": "#86",
        "title": "CollaPipe: Adaptive Segment-Optimized Pipeline Parallelism for Collaborative LLM Training in Heterogeneous Edge Networks",
        "link": "/arxiv/2509.19855",
        "arxiv_id": "2509.19855",
        "authors": "Jiewei Chen, Xiumei Deng, Zehui Xiong, Shaoyong Guo, Xuesong Qiu, Ping Wang, Dusit Niyato",
        "summary": "The increasing demand for intelligent mobile applications has made multi-agent collaboration with Transformer-based large language models (LLMs) essential in mobile edge computing (MEC) networks. However, training LLMs in such environments remains challenging due to heavy computation, high end-to-end latency, and limited model generalization. We introduce CollaPipe, a hybrid distributed learning framework that integrates collaborative pipeline parallelism with federated aggregation to support self-evolving intelligent networks. In CollaPipe, the encoder part is adaptively partitioned into variable-sized segments and deployed across mobile devices for pipeline-parallel training, while the decoder is deployed on edge servers to handle generative tasks. Then we perform global model update via federated aggregation. To enhance training efficiency, we formulate a joint optimization problem that adaptively allocates model segments, micro-batches, bandwidth, and transmission power. We derive and use a closed-form convergence bound to design an Dynamic Segment Scheduling and Resource Allocation (DSSDA) algorithm based on Lyapunov optimization, ensuring system stability under long-term constraints. Extensive experiments on downstream tasks with Transformer and BERT models show that CollaPipe improves computation efficiency by up to 15.09%, reduces end-to-end latency by at least 48.98%, and cuts single device memory usage by more than half, enabling online learning in heterogeneous and dynamic communication environments.",
        "subjects": "Systems and Control, Artificial Intelligence, Networking and Internet Architecture",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.669141"
    },
    {
        "index": "#87",
        "title": "Eliminating stability hallucinations in llm-based tts models via attention guidance",
        "link": "/arxiv/2509.19852",
        "arxiv_id": "2509.19852",
        "authors": "ShiMing Wang, ZhiHao Du, Yang Xiang, TianYu Zhao, Han Zhao, Qian Chen, XianGang Li, HanJie Guo, ZhenHua Ling",
        "summary": "This paper focuses on resolving stability hallucinations (e.g., repetitive or omitted speech) in LLM-based Text-to-Speech (TTS) models by improving and leveraging the attention mechanism. First, we analyzed the alignment mechanism between text tokens and speech tokens in LLMs. We then proposed a metric termed the Optimal Alignment Score (OAS), which employs the Viterbi algorithm to evaluate text-speech alignment quality. Subsequently, OAS was integrated into the training of CosyVoice2 to assist LLMs in learning continuous, stable alignment. Additionally, the pre-trained attention value is employed to guide the training of the student CosyVoice2 via chain-of-thought (CoT), which further reduces stability hallucinations in synthesized speech. Experiments on the Seed-TTS-Eval and CV3-Eval test sets demonstrate that the proposed methods can effectively reduce the stability hallucinations of CosyVoice2 without introducing additional negative effects. The appendix is available at https://wsmzzz.github.io/llm_attn.",
        "subjects": "Sound, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.669433"
    },
    {
        "index": "#88",
        "title": "Analyzing Generalization in Pre-Trained Symbolic Regression",
        "link": "/arxiv/2509.19849",
        "arxiv_id": "2509.19849",
        "authors": "Henrik Voigt, Paul Kahlmeyer, Kai Lawonn, Michael Habeck, Joachim Giesen",
        "summary": "Symbolic regression algorithms search a space of mathematical expressions for formulas that explain given data. Transformer-based models have emerged as a promising, scalable approach shifting the expensive combinatorial search to a large-scale pre-training phase. However, the success of these models is critically dependent on their pre-training data. Their ability to generalize to problems outside of this pre-training distribution remains largely unexplored. In this work, we conduct a systematic empirical study to evaluate the generalization capabilities of pre-trained, transformer-based symbolic regression. We rigorously test performance both within the pre-training distribution and on a series of out-of-distribution challenges for several state of the art approaches. Our findings reveal a significant dichotomy: while pre-trained models perform well in-distribution, the performance consistently degrades in out-of-distribution scenarios. We conclude that this generalization gap is a critical barrier for practitioners, as it severely limits the practical use of pre-trained approaches for real-world applications.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.669740"
    },
    {
        "index": "#91",
        "title": "On the Rate of Convergence of Kolmogorov-Arnold Network Regression Estimators",
        "link": "/arxiv/2509.19830",
        "arxiv_id": "2509.19830",
        "authors": "Wei Liu, Eleni Chatzi, Zhilu Lai",
        "summary": "Kolmogorov-Arnold Networks (KANs) offer a structured and interpretable framework for multivariate function approximation by composing univariate transformations through additive or multiplicative aggregation. This paper establishes theoretical convergence guarantees for KANs when the univariate components are represented by B-splines. We prove that both additive and hybrid additive-multiplicative KANs attain the minimax-optimal convergence rate $O(n^{-2r/(2r+1)})$ for functions in Sobolev spaces of smoothness $r$. We further derive guidelines for selecting the optimal number of knots in the B-splines. The theory is supported by simulation studies that confirm the predicted convergence rates. These results provide a theoretical foundation for using KANs in nonparametric regression and highlight their potential as a structured alternative to existing methods.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.670567"
    },
    {
        "index": "#92",
        "title": "Causal Inference under Threshold Manipulation: Bayesian Mixture Modeling and Heterogeneous Treatment Effects",
        "link": "/arxiv/2509.19814",
        "arxiv_id": "2509.19814",
        "authors": "Kohsuke Kubota, Shonosuke Sugasawa",
        "summary": "Many marketing applications, including credit card incentive programs, offer rewards to customers who exceed specific spending thresholds to encourage increased consumption. Quantifying the causal effect of these thresholds on customers is crucial for effective marketing strategy design. Although regression discontinuity design is a standard method for such causal inference tasks, its assumptions can be violated when customers, aware of the thresholds, strategically manipulate their spending to qualify for the rewards. To address this issue, we propose a novel framework for estimating the causal effect under threshold manipulation. The main idea is to model the observed spending distribution as a mixture of two distributions: one representing customers strategically affected by the threshold, and the other representing those unaffected. To fit the mixture model, we adopt a two-step Bayesian approach consisting of modeling non-bunching customers and fitting a mixture model to a sample around the threshold. We show posterior contraction of the resulting posterior distribution of the causal effect under large samples. Furthermore, we extend this framework to a hierarchical Bayesian setting to estimate heterogeneous causal effects across customer subgroups, allowing for stable inference even with small subgroup sample sizes. We demonstrate the effectiveness of our proposed methods through simulation studies and illustrate their practical implications using a real-world marketing dataset.",
        "subjects": "Methodology, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.670756"
    },
    {
        "index": "#93",
        "title": "RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving",
        "link": "/arxiv/2509.19789",
        "arxiv_id": "2509.19789",
        "authors": "Carlo Bosio, Greg Woelki, Noureldin Hendy, Nicholas Roy, Byungsoo Kim",
        "summary": "Human drivers focus only on a handful of agents at any one time. On the other hand, autonomous driving systems process complex scenes with numerous agents, regardless of whether they are pedestrians on a crosswalk or vehicles parked on the side of the road. While attention mechanisms offer an implicit way to reduce the input to the elements that affect decisions, existing attention mechanisms for capturing agent interactions are quadratic, and generally computationally expensive. We propose RDAR, a strategy to learn per-agent relevance -- how much each agent influences the behavior of the controlled vehicle -- by identifying which agents can be excluded from the input to a pre-trained behavior model. We formulate the masking procedure as a Markov Decision Process where the action consists of a binary mask indicating agent selection. We evaluate RDAR on a large-scale driving dataset, and demonstrate its ability to learn an accurate numerical measure of relevance by achieving comparable driving performance, in terms of overall progress, safety and performance, while processing significantly fewer agents compared to a state of the art behavior model.",
        "subjects": "Machine Learning, Artificial Intelligence, Robotics",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.670960"
    },
    {
        "index": "#95",
        "title": "PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection",
        "link": "/arxiv/2509.19774",
        "arxiv_id": "2509.19774",
        "authors": "Xiaocheng Fang, Jiarui Jin, Haoyu Wang, Che Liu, Jieyi Cai, Guangkun Nie, Jun Li, Hongyan Li, Shenda Hong",
        "summary": "In clinical practice, electrocardiography (ECG) remains the gold standard for cardiac monitoring, providing crucial insights for diagnosing a wide range of cardiovascular diseases (CVDs). However, its reliance on specialized equipment and trained personnel limits feasibility for continuous routine monitoring. Photoplethysmography (PPG) offers accessible, continuous monitoring but lacks definitive electrophysiological information, preventing conclusive diagnosis. Generative models present a promising approach to translate PPG into clinically valuable ECG signals, yet current methods face substantial challenges, including the misalignment of physiological semantics in generative models and the complexity of modeling in high-dimensional signals. To this end, we propose PPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent space via the CardioAlign Encoder and employs latent rectified flow to generate ECGs with high fidelity and interpretability. To the best of our knowledge, this is the first study to experiment on MCMED, a newly released clinical-grade dataset comprising over 10 million paired PPG-ECG samples from more than 118,000 emergency department visits with expert-labeled cardiovascular disease annotations. Results demonstrate the effectiveness of our method for PPG-to-ECG translation and cardiovascular disease detection. Moreover, cardiologist-led evaluations confirm that the synthesized ECGs achieve high fidelity and improve diagnostic reliability, underscoring our method's potential for real-world cardiovascular screening.",
        "subjects": "Machine Learning, Artificial Intelligence, Signal Processing",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.673927"
    },
    {
        "index": "#96",
        "title": "Sobolev acceleration for neural networks",
        "link": "/arxiv/2509.19773",
        "arxiv_id": "2509.19773",
        "authors": "Jong Kwon Oh, Hanbaek Lyu, Hwijae Son",
        "summary": "Sobolev training, which integrates target derivatives into the loss functions, has been shown to accelerate convergence and improve generalization compared to conventional $L^2$ training. However, the underlying mechanisms of this training method remain only partially understood. In this work, we present the first rigorous theoretical framework proving that Sobolev training accelerates the convergence of Rectified Linear Unit (ReLU) networks. Under a student-teacher framework with Gaussian inputs and shallow architectures, we derive exact formulas for population gradients and Hessians, and quantify the improvements in conditioning of the loss landscape and gradient-flow convergence rates. Extensive numerical experiments validate our theoretical findings and show that the benefits of Sobolev training extend to modern deep learning tasks.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.674240"
    },
    {
        "index": "#97",
        "title": "Frictional Q-Learning",
        "link": "/arxiv/2509.19771",
        "arxiv_id": "2509.19771",
        "authors": "Hyunwoo Kim, Hyo Kyung Lee",
        "summary": "We draw an analogy between static friction in classical mechanics and extrapolation error in off-policy RL, and use it to formulate a constraint that prevents the policy from drifting toward unsupported actions. In this study, we present Frictional Q-learning, a deep reinforcement learning algorithm for continuous control, which extends batch-constrained reinforcement learning. Our algorithm constrains the agent's action space to encourage behavior similar to that in the replay buffer, while maintaining a distance from the manifold of the orthonormal action space. The constraint preserves the simplicity of batch-constrained, and provides an intuitive physical interpretation of extrapolation error. Empirically, we further demonstrate that our algorithm is robustly trained and achieves competitive performance across standard continuous control benchmarks.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.674418"
    },
    {
        "index": "#98",
        "title": "FusedANN: Convexified Hybrid ANN via Attribute-Vector Fusion",
        "link": "/arxiv/2509.19767",
        "arxiv_id": "2509.19767",
        "authors": "Alireza Heidari, Wei Zhang, Ying Xiong",
        "summary": "Vector search powers transformers technology, but real-world use demands hybrid queries that combine vector similarity with attribute filters (e.g., \"top document in category X, from 2023\"). Current solutions trade off recall, speed, and flexibility, relying on fragile index hacks that don't scale. We introduce FusedANN (Fused Attribute-Vector Nearest Neighbor), a geometric framework that elevates filtering to ANN optimization constraints and introduces a convex fused space via a Lagrangian-like relaxation. Our method jointly embeds attributes and vectors through transformer-based convexification, turning hard filters into continuous, weighted penalties that preserve top-k semantics while enabling efficient approximate search. We prove that FusedANN reduces to exact filtering under high selectivity, gracefully relaxes to semantically nearest attributes when exact matches are insufficient, and preserves downstream ANN alpha-approximation guarantees. Empirically, FusedANN improves query throughput by eliminating brittle filtering stages, achieving superior recall-latency tradeoffs on standard hybrid benchmarks without specialized index hacks, delivering up to 3 times higher throughput and better recall than state-of-the-art hybrid and graph-based systems. Theoretically, we provide explicit error bounds and parameter selection rules that make FusedANN practical for production. This establishes a principled, scalable, and verifiable bridge between symbolic constraints and vector similarity, unlocking a new generation of filtered retrieval systems for large, hybrid, and dynamic NLP/ML workloads.",
        "subjects": "Information Retrieval, Artificial Intelligence, Databases, Optimization and Control",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.674622"
    },
    {
        "index": "#99",
        "title": "Dynamicasome: a molecular dynamics-guided and AI-driven pathogenicity prediction catalogue for all genetic mutations",
        "link": "/arxiv/2509.19766",
        "arxiv_id": "2509.19766",
        "authors": "Naeyma N Islam, Mathew A Coban, Jessica M Fuller, Caleb Weber, Rohit Chitale, Benjamin Jussila, Trisha J. Brock, Cui Tao, Thomas R Caulfield",
        "summary": "Advances in genomic medicine accelerate the identi cation of mutations in disease-associated genes, but the pathogenicity of many mutations remains unknown, hindering their use in diagnostics and clinical decision-making. Predictive AI models are generated to combat this issue, but current tools display low accuracy when tested against functionally validated datasets. We show that integrating detailed conformational data extracted from molecular dynamics simulations (MDS) into advanced AI-based models increases their predictive power. We carry out an exhaustive mutational analysis of the disease gene PMM2 and subject structural models of each variant to MDS. AI models trained on this dataset outperform existing tools when predicting the known pathogenicity of mutations. Our best performing model, a neuronal networks model, also predicts the pathogenicity of several PMM2 mutations currently considered of unknown signi cance. We believe this model helps alleviate the burden of unknown variants in genomic medicine.",
        "subjects": "Quantitative Methods, Artificial Intelligence, Biological Physics, Molecular Networks",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.674868"
    },
    {
        "index": "#100",
        "title": "ARCADE: A Real-Time Data System for Hybrid and Continuous Query Processing across Diverse Data Modalities",
        "link": "/arxiv/2509.19757",
        "arxiv_id": "2509.19757",
        "authors": "Jingyi Yang, Songsong Mo, Jiachen Shi, Zihao Yu, Kunhao Shi, Xuchen Ding, Gao Cong",
        "summary": "The explosive growth of multimodal data - spanning text, image, video, spatial, and relational modalities, coupled with the need for real-time semantic search and retrieval over these data - has outpaced the capabilities of existing multimodal and real-time database systems, which either lack efficient ingestion and continuous query capability, or fall short in supporting expressive hybrid analytics. We introduce ARCADE, a real-time data system that efficiently supports high-throughput ingestion and expressive hybrid and continuous query processing across diverse data types. ARCADE introduces unified disk-based secondary index on LSM-based storage for vector, spatial, and text data modalities, a comprehensive cost-based query optimizer for hybrid queries, and an incremental materialized view framework for efficient continuous queries. Built on open-source RocksDB storage and MySQL query engine, ARCADE outperforms leading multimodal data systems by up to 7.4x on read-heavy and 1.4x on write-heavy workloads.",
        "subjects": "Databases, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.675075"
    },
    {
        "index": "#102",
        "title": "Cuffless Blood Pressure Prediction from Speech Sentences using Deep Learning Methods",
        "link": "/arxiv/2509.19750",
        "arxiv_id": "2509.19750",
        "authors": "Kainat",
        "summary": "This research presents a novel method for noninvasive arterial blood pressure ABP prediction using speech signals employing a BERT based regression model Arterial blood pressure is a vital indicator of cardiovascular health and accurate monitoring is essential in preventing hypertension related complications Traditional cuff based methods often yield inconsistent results due to factors like whitecoat and masked hypertension Our approach leverages the acoustic characteristics of speech capturing voice features to establish correlations with blood pressure levels Utilizing advanced deep learning techniques we analyze speech signals to extract relevant patterns enabling real time monitoring without the discomfort of conventional methods In our study we employed a dataset comprising recordings from 95 participants ensuring diverse representation The BERT model was fine tuned on extracted features from speech leading to impressive performance metrics achieving a mean absolute error MAE of 136 mmHg for systolic blood pressure SBP and 124 mmHg for diastolic blood pressure DBP with R scores of 099 and 094 respectively These results indicate the models robustness in accurately predicting blood pressure levels Furthermore the training and validation loss analysis demonstrates effective learning and minimal overfitting Our findings suggest that integrating deep learning with speech analysis presents a viable alternative for blood pressure monitoring paving the way for improved applications in telemedicine and remote health monitoring By providing a user friendly and accurate method for blood pressure assessment this research has significant implications for enhancing patient care and proactive management of cardiovascular health",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.675415"
    },
    {
        "index": "#104",
        "title": "SMILES-Inspired Transfer Learning for Quantum Operators in Generative Quantum Eigensolver",
        "link": "/arxiv/2509.19715",
        "arxiv_id": "2509.19715",
        "authors": "Zhi Yin, Xiaoran Li, Shengyu Zhang, Xin Li, Xiaojin Zhang",
        "summary": "Given the inherent limitations of traditional Variational Quantum Eigensolver(VQE) algorithms, the integration of deep generative models into hybrid quantum-classical frameworks, specifically the Generative Quantum Eigensolver(GQE), represents a promising innovative approach. However, taking the Unitary Coupled Cluster with Singles and Doubles(UCCSD) ansatz which is widely used in quantum chemistry as an example, different molecular systems require constructions of distinct quantum operators. Considering the similarity of different molecules, the construction of quantum operators utilizing the similarity can reduce the computational cost significantly. Inspired by the SMILES representation method in computational chemistry, we developed a text-based representation approach for UCCSD quantum operators by leveraging the inherent representational similarities between different molecular systems. This framework explores text pattern similarities in quantum operators and employs text similarity metrics to establish a transfer learning framework. Our approach with a naive baseline setting demonstrates knowledge transfer between different molecular systems for ground-state energy calculations within the GQE paradigm. This discovery offers significant benefits for hybrid quantum-classical computation of molecular ground-state energies, substantially reducing computational resource requirements.",
        "subjects": "Chemical Physics, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.675899"
    },
    {
        "index": "#105",
        "title": "Intuition to Evidence: Measuring AI's True Impact on Developer Productivity",
        "link": "/arxiv/2509.19708",
        "arxiv_id": "2509.19708",
        "authors": "Anand Kumar, Vishal Khare, Deepak Sharma, Satyam Kumar, Vijay Saini, Anshul Yadav, Sachendra Jain, Ankit Rana, Pratham Verma, Vaibhav Meena, Avinash Edubilli",
        "summary": "We present a comprehensive real-world evaluation of AI-assisted software development tools deployed at enterprise scale. Over one year, 300 engineers across multiple teams integrated an in-house AI platform (DeputyDev) that combines code generation and automated review capabilities into their daily workflows. Through rigorous cohort analysis, our study demonstrates statistically significant productivity improvements, including an overall 31.8% reduction in PR review cycle time. Developer adoption was strong, with 85% satisfaction for code review features and 93% expressing a desire to continue using the platform. Adoption patterns showed systematic scaling from 4% engagement in month 1 to 83% peak usage by month 6, stabilizing at 60% active engagement. Top adopters achieved a 61% increase in code volume pushed to production, contributing to approximately 30 to 40% of code shipped to production through this tool, accounting for an overall 28% increase in code shipment volume. Unlike controlled benchmark evaluations, our longitudinal analysis provides empirical evidence from production environments, revealing both the transformative potential and practical deployment challenges of integrating AI into enterprise software development workflows.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.676143"
    },
    {
        "index": "#106",
        "title": "Causal Machine Learning for Surgical Interventions",
        "link": "/arxiv/2509.19705",
        "arxiv_id": "2509.19705",
        "authors": "J. Ben Tamo, Nishant S. Chouhan, Micky C. Nnamdi, Yining Yuan, Shreya S. Chivilkar, Wenqi Shi, Steven W. Hwang, B. Randall Brenn, May D. Wang",
        "summary": "Surgical decision-making is complex and requires understanding causal relationships between patient characteristics, interventions, and outcomes. In high-stakes settings like spinal fusion or scoliosis correction, accurate estimation of individualized treatment effects (ITEs) remains limited due to the reliance on traditional statistical methods that struggle with complex, heterogeneous data. In this study, we develop a multi-task meta-learning framework, X-MultiTask, for ITE estimation that models each surgical decision (e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct task while learning shared representations across tasks. To strengthen causal validity, we incorporate the inverse probability weighting (IPW) into the training objective. We evaluate our approach on two datasets: (1) a public spinal fusion dataset (1,017 patients) to assess the effect of anterior vs. posterior approaches on complication severity; and (2) a private AIS dataset (368 patients) to analyze the impact of posterior spinal fusion (PSF) vs. non-surgical management on patient-reported outcomes (PROs). Our model achieves the highest average AUC (0.84) in the anterior group and maintains competitive performance in the posterior group (0.77). It outperforms baselines in treatment effect estimation with the lowest overall $\\epsilon_{\\text{NN-PEHE}}$ (0.2778) and $\\epsilon_{\\text{ATE}}$ (0.0763). Similarly, when predicting PROs in AIS, X-MultiTask consistently shows superior performance across all domains, with $\\epsilon_{\\text{NN-PEHE}}$ = 0.2551 and $\\epsilon_{\\text{ATE}}$ = 0.0902. By providing robust, patient-specific causal estimates, X-MultiTask offers a powerful tool to advance personalized surgical care and improve patient outcomes. The code is available at https://github.com/Wizaaard/X-MultiTask.",
        "subjects": "Machine Learning, Artificial Intelligence, Applications, Methodology",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.676381"
    },
    {
        "index": "#107",
        "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms",
        "link": "/arxiv/2509.19702",
        "arxiv_id": "2509.19702",
        "authors": "Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama",
        "summary": "We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nystrm extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nystrm extrapolation, highlighting a powerful capability of in-context learning.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.676587"
    },
    {
        "index": "#108",
        "title": "A Unified Noise-Curvature View of Loss of Trainability",
        "link": "/arxiv/2509.19698",
        "arxiv_id": "2509.19698",
        "authors": "Gunbir Singh Baveja, Mark Schmidt",
        "summary": "Loss of trainability (LoT) in continual learning occurs when gradient steps no longer yield improvement as tasks evolve, so accuracy stalls or degrades despite adequate capacity and supervision. We analyze LoT incurred with Adam through an optimization lens and find that single indicators such as Hessian rank, sharpness level, weight or gradient norms, gradient-to-parameter ratios, and unit-sign entropy are not reliable predictors. Instead we introduce two complementary criteria: a batch-size-aware gradient-noise bound and a curvature volatility-controlled bound that combine into a per-layer predictive threshold that anticipates trainability behavior. Using this threshold, we build a simple per-layer scheduler that keeps each layers effective step below a safe limit, stabilizing training and improving accuracy across concatenated ReLU (CReLU), Wasserstein regularization, and L2 weight decay, with learned learning-rate trajectories that mirror canonical decay.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.676766"
    },
    {
        "index": "#109",
        "title": "Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks",
        "link": "/arxiv/2509.19696",
        "arxiv_id": "2509.19696",
        "authors": "Noah Geiger, Tamim Asfour, Neville Hogan, Johannes Lachner",
        "summary": "Learning methods excel at motion generation in the information domain but are not primarily designed for physical interaction in the energy domain. Impedance Control shapes physical interaction but requires task-aware tuning by selecting feasible impedance parameters. We present Diffusion-Based Impedance Learning, a framework that combines both domains. A Transformer-based Diffusion Model with cross-attention to external wrenches reconstructs a simulated Zero-Force Trajectory (sZFT). This captures both translational and rotational task-space behavior. For rotations, we introduce a novel SLERP-based quaternion noise scheduler that ensures geometric consistency. The reconstructed sZFT is then passed to an energy-based estimator that updates stiffness and damping parameters. A directional rule is applied that reduces impedance along non task axes while preserving rigidity along task directions. Training data were collected for a parkour scenario and robotic-assisted therapy tasks using teleoperation with Apple Vision Pro. With only tens of thousands of samples, the model achieved sub-millimeter positional accuracy and sub-degree rotational accuracy. Its compact model size enabled real-time torque control and autonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller achieved smooth parkour traversal within force and velocity limits and 30/30 success rates for cylindrical, square, and star peg insertions without any peg-specific demonstrations in the training data set. All code for the Transformer-based Diffusion Model, the robot controller, and the Apple Vision Pro telemanipulation framework is publicly available. These results mark an important step towards Physical AI, fusing model-based control for physical interaction with learning-based methods for trajectory generation.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.676964"
    },
    {
        "index": "#111",
        "title": "PolicyPad: Collaborative Prototyping of LLM Policies",
        "link": "/arxiv/2509.19680",
        "arxiv_id": "2509.19680",
        "authors": "K. J. Kevin Feng, Tzu-Sheng Kuo, Quan Ze, Chen, Inyoung Cheong, Kenneth Holstein, Amy X. Zhang",
        "summary": "As LLMs gain adoption in high-stakes domains like mental health, domain experts are increasingly consulted to provide input into policies governing their behavior. From an observation of 19 policymaking workshops with 9 experts over 15 weeks, we identified opportunities to better support rapid experimentation, feedback, and iteration for collaborative policy design processes. We present PolicyPad, an interactive system that facilitates the emerging practice of LLM policy prototyping by drawing from established UX prototyping practices, including heuristic evaluation and storyboarding. Using PolicyPad, policy designers can collaborate on drafting a policy in real time while independently testing policy-informed model behavior with usage scenarios. We evaluate PolicyPad through workshops with 8 groups of 22 domain experts in mental health and law, finding that PolicyPad enhanced collaborative dynamics during policy design, enabled tight feedback loops, and led to novel policy contributions. Overall, our work paves participatory paths for advancing AI alignment and safety.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.677448"
    },
    {
        "index": "#112",
        "title": "Thinking While Listening: Simple Test Time Scaling For Audio Classification",
        "link": "/arxiv/2509.19676",
        "arxiv_id": "2509.19676",
        "authors": "Prateek Verma, Mert Pilanci",
        "summary": "We propose a framework that enables neural models to \"think while listening\" to everyday sounds, thereby enhancing audio classification performance. Motivated by recent advances in the reasoning capabilities of large language models, we address two central questions: (i) how can thinking be incorporated into existing audio classification pipelines to enable reasoning in the category space and improve performance, and (ii) can a new architecture be designed from the ground up to support both thinking and test-time scaling? We demonstrate that in both settings, our models exhibit improved classification accuracy. Leveraging test-time scaling, we observe consistent gains as the number of sampled traces increases. Furthermore, we evaluate two open-source reasoning models, GPT-OSS-20B and Qwen3-14B, showing that while such models are capable of zero-shot reasoning, a lightweight approach--retraining only the embedding matrix of a frozen, smaller model like GPT-2--can surpass the performance of billion-parameter text-based reasoning models.",
        "subjects": "Sound, Artificial Intelligence, Machine Learning, Audio and Speech Processing",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.677684"
    },
    {
        "index": "#113",
        "title": "Games Are Not Equal: Classifying Cloud Gaming Contexts for Effective User Experience Measurement",
        "link": "/arxiv/2509.19669",
        "arxiv_id": "2509.19669",
        "authors": "Yifan Wang, Minzhao Lyu, Vijay Sivaraman",
        "summary": "To tap into the growing market of cloud gaming, whereby game graphics is rendered in the cloud and streamed back to the user as a video feed, network operators are creating monetizable assurance services that dynamically provision network resources. However, without accurately measuring cloud gaming user experience, they cannot assess the effectiveness of their provisioning methods. Basic measures such as bandwidth and frame rate by themselves do not suffice, and can only be interpreted in the context of the game played and the player activity within the game. This paper equips the network operator with a method to obtain a real-time measure of cloud gaming experience by analyzing network traffic, including contextual factors such as the game title and player activity stage. Our method is able to classify the game title within the first five seconds of game launch, and continuously assess the player activity stage as being active, passive, or idle. We deploy it in an ISP hosting NVIDIA cloud gaming servers for the region. We provide insights from hundreds of thousands of cloud game streaming sessions over a three-month period into the dependence of bandwidth consumption and experience level on the gameplay contexts.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.677920"
    },
    {
        "index": "#114",
        "title": "Selective Classifier-free Guidance for Zero-shot Text-to-speech",
        "link": "/arxiv/2509.19668",
        "arxiv_id": "2509.19668",
        "authors": "John Zheng, Farhad Maleki",
        "summary": "In zero-shot text-to-speech, achieving a balance between fidelity to the target speaker and adherence to text content remains a challenge. While classifier-free guidance (CFG) strategies have shown promising results in image generation, their application to speech synthesis are underexplored. Separating the conditions used for CFG enables trade-offs between different desired characteristics in speech synthesis. In this paper, we evaluate the adaptability of CFG strategies originally developed for image generation to speech synthesis and extend separated-condition CFG approaches for this domain. Our results show that CFG strategies effective in image generation generally fail to improve speech synthesis. We also find that we can improve speaker similarity while limiting degradation of text adherence by applying standard CFG during early timesteps and switching to selective CFG only in later timesteps. Surprisingly, we observe that the effectiveness of a selective CFG strategy is highly text-representation dependent, as differences between the two languages of English and Mandarin can lead to different results even with the same model.",
        "subjects": "Audio and Speech Processing, Artificial Intelligence, Sound",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.678123"
    },
    {
        "index": "#116",
        "title": "RoboSSM: Scalable In-context Imitation Learning via State-Space Models",
        "link": "/arxiv/2509.19658",
        "arxiv_id": "2509.19658",
        "authors": "Youngju Yoo, Jiaheng Hu, Yifeng Zhu, Bo Liu, Qiang Liu, Roberto Martn-Martn, Peter Stone",
        "summary": "In-context imitation learning (ICIL) enables robots to learn tasks from prompts consisting of just a handful of demonstrations. By eliminating the need for parameter updates at deployment time, this paradigm supports few-shot adaptation to novel tasks. However, recent ICIL methods rely on Transformers, which have computational limitations and tend to underperform when handling longer prompts than those seen during training. In this work, we introduce RoboSSM, a scalable recipe for in-context imitation learning based on state-space models (SSM). Specifically, RoboSSM replaces Transformers with Longhorn -- a state-of-the-art SSM that provides linear-time inference and strong extrapolation capabilities, making it well-suited for long-context prompts. We evaluate our approach on the LIBERO benchmark and compare it against strong Transformer-based ICIL baselines. Experiments show that RoboSSM extrapolates effectively to varying numbers of in-context demonstrations, yields high performance on unseen tasks, and remains robust in long-horizon scenarios. These results highlight the potential of SSMs as an efficient and scalable backbone for ICIL. Our code is available at https://github.com/youngjuY/RoboSSM.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.679154"
    },
    {
        "index": "#118",
        "title": "Where 6G Stands Today: Evolution, Enablers, and Research Gaps",
        "link": "/arxiv/2509.19646",
        "arxiv_id": "2509.19646",
        "authors": "Salma Tika, Abdelkrim Haqiq, Essaid Sabir, Elmahdi Driouch",
        "summary": "As the fifth-generation (5G) mobile communication system continues its global deployment, both industry and academia have started conceptualizing the 6th generation (6G) to address the growing need for a progressively advanced and digital society. Even while 5G offers considerable advancements over LTE, it could struggle to be sufficient to meet all of the requirements, including ultra-high reliability, seamless automation, and ubiquitous coverage. In response, 6G is supposed to bring out a highly intelligent, automated, and ultra-reliable communication system that can handle a vast number of connected devices. This paper offers a comprehensive overview of 6G, beginning with its main stringent requirements while focusing on key enabling technologies such as terahertz (THz) communications, intelligent reflecting surfaces, massive MIMO and AI-driven networking that will shape the 6G networks. Furthermore, the paper lists various 6G applications and usage scenarios that will benefit from these advancements. At the end, we outline the potential challenges that must be addressed to achieve the 6G promises.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.679572"
    },
    {
        "index": "#119",
        "title": "Are We Scaling the Right Thing? A System Perspective on Test-Time Scaling",
        "link": "/arxiv/2509.19645",
        "arxiv_id": "2509.19645",
        "authors": "Youpeng Zhao, Jinpeng LV, Di Wu, Jun Wang, Christopher Gooley",
        "summary": "Test-time scaling (TTS) has recently emerged as a promising direction to exploit the hidden reasoning capabilities of pre-trained large language models (LLMs). However, existing scaling methods narrowly focus on the compute-optimal Pareto-frontier, ignoring the simple fact that compute-optimal is not always system-optimal. In this work, we propose a system-driven perspective on TTS, analyzing how reasoning models scale against practical metrics, such as latency and cost-per-token. By evaluating the impact of popular optimizations such as tensor parallelism and speculative decoding, our preliminary analysis reveals the limitations of current methods and calls for a paradigm shift toward holistic, system-aware evaluations that capture the true essence of scaling laws at inference time.",
        "subjects": "Performance, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.679779"
    },
    {
        "index": "#120",
        "title": "Mamba Modulation: On the Length Generalization of Mamba",
        "link": "/arxiv/2509.19633",
        "arxiv_id": "2509.19633",
        "authors": "Peng Lu, Jerry Huang, Qiuhao Zeng, Xinyu Wang, Boxing Wang, Philippe Langlais, Yufei Cui",
        "summary": "The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\\exp(-\\sum_{t=1}^N\\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.680006"
    },
    {
        "index": "#125",
        "title": "Reverse Engineering User Stories from Code using Large Language Models",
        "link": "/arxiv/2509.19587",
        "arxiv_id": "2509.19587",
        "authors": "Mohamed Ouf, Haoyu Li, Michael Zhang, Mariam Guizani",
        "summary": "User stories are essential in agile development, yet often missing or outdated in legacy and poorly documented systems. We investigate whether large language models (LLMs) can automatically recover user stories directly from source code and how prompt design impacts output quality. Using 1,750 annotated C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs across six prompting strategies. Results show that all models achieve, on average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a single illustrative example enables the smallest model (8B) to match the performance of a much larger 70B model. In contrast, structured reasoning via Chain-of-Thought offers only marginal gains, primarily for larger models.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.681501"
    },
    {
        "index": "#126",
        "title": "A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery",
        "link": "/arxiv/2509.19586",
        "arxiv_id": "2509.19586",
        "authors": "Alexander Ho, Sukyeong Lee, Francis T. F. Tsai",
        "summary": "We introduce FragAtlas-62M, a specialized foundation model trained on the largest fragment dataset to date. Built on the complete ZINC-22 fragment subset comprising over 62 million molecules, it achieves unprecedented coverage of fragment chemical space. Our GPT-2 based model (42.7M parameters) generates 99.90% chemically valid fragments. Validation across 12 descriptors and three fingerprint methods shows generated fragments closely match the training distribution (all effect sizes < 0.4). The model retains 53.6% of known ZINC fragments while producing 22% novel structures with practical relevance. We release FragAtlas-62M with training code, preprocessed data, documentation, and model weights to accelerate adoption.",
        "subjects": "Machine Learning, Artificial Intelligence, Biomolecules",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.681717"
    },
    {
        "index": "#127",
        "title": "Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks",
        "link": "/arxiv/2509.19554",
        "arxiv_id": "2509.19554",
        "authors": "Yi Ren",
        "summary": "This thesis explores how deep learning models learn over time, using ideas inspired by force analysis. Specifically, we zoom in on the model's training procedure to see how one training example affects another during learning, like analyzing how forces move objects. We break this influence into two parts: how similar the two examples are, and how strong the updating force is. This framework helps us understand a wide range of the model's behaviors in different real systems. For example, it explains why certain examples have non-trivial learning paths, why (and why not) some LLM finetuning methods work, and why simpler, more structured patterns tend to be learned more easily. We apply this approach to various learning tasks and uncover new strategies for improving model training. While the method is still developing, it offers a new way to interpret models' behaviors systematically.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.681900"
    },
    {
        "index": "#128",
        "title": "DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions",
        "link": "/arxiv/2509.19538",
        "arxiv_id": "2509.19538",
        "authors": "Zongyue Li, Xiao Han, Yusong Li, Niklas Strauss, Matthias Schubert",
        "summary": "Diffusion-based world models have demonstrated strong capabilities in synthesizing realistic long-horizon trajectories for offline reinforcement learning (RL). However, many existing methods do not directly generate actions alongside states and rewards, limiting their compatibility with standard value-based offline RL algorithms that rely on one-step temporal difference (TD) learning. While prior work has explored joint modeling of states, rewards, and actions to address this issue, such formulations often lead to increased training complexity and reduced performance in practice. We propose \\textbf{DAWM}, a diffusion-based world model that generates future state-reward trajectories conditioned on the current state, action, and return-to-go, paired with an inverse dynamics model (IDM) for efficient action inference. This modular design produces complete synthetic transitions suitable for one-step TD-based offline RL, enabling effective and computationally efficient training. Empirically, we show that conservative offline RL algorithms such as TD3BC and IQL benefit significantly from training on these augmented trajectories, consistently outperforming prior diffusion-based baselines across multiple tasks in the D4RL benchmark.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.682109"
    },
    {
        "index": "#129",
        "title": "Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation",
        "link": "/arxiv/2509.19533",
        "arxiv_id": "2509.19533",
        "authors": "Mengdi Lu, Steven Ding, Furkan Alaca, Philippe Charland",
        "summary": "Security vulnerabilities in Internet-of-Things devices, mobile platforms, and autonomous systems remain critical. Traditional mutation-based fuzzers -- while effectively explore code paths -- primarily perform byte- or bit-level edits without semantic reasoning. Coverage-guided tools such as AFL++ use dictionaries, grammars, and splicing heuristics to impose shallow structural constraints, leaving deeper protocol logic, inter-field dependencies, and domain-specific semantics unaddressed. Conversely, reasoning-capable large language models (LLMs) can leverage pretraining knowledge to understand input formats, respect complex constraints, and propose targeted mutations, much like an experienced reverse engineer or testing expert. However, lacking ground truth for \"correct\" mutation reasoning makes supervised fine-tuning impractical, motivating explorations of off-the-shelf LLMs via prompt-based few-shot learning. To bridge this gap, we present an open-source microservices framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench, tackling asynchronous execution and divergent hardware demands (GPU- vs. CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1) How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt engineering with off-the-shelf models improve fuzzing directly? and (R4) Which open-source reasoning LLMs perform best under prompt-only conditions? Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3 highlight Deepseek as the most promising. Mutation effectiveness depends more on prompt complexity and model choice than shot count. Response latency and throughput bottlenecks remain key obstacles, offering directions for future work.",
        "subjects": "Software Engineering, Artificial Intelligence, Cryptography and Security",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.682419"
    },
    {
        "index": "#130",
        "title": "A Longitudinal Randomized Control Study of Companion Chatbot Use: Anthropomorphism and Its Mediating Role on Social Impacts",
        "link": "/arxiv/2509.19515",
        "arxiv_id": "2509.19515",
        "authors": "Rose E. Guingrich, Michael S. A. Graziano",
        "summary": "Relationships with social artificial intelligence (AI) agents are on the rise. People report forming friendships, mentorships, and romantic partnerships with chatbots such as Replika, a type of social AI agent that is designed specifically for companionship. Concerns that companion chatbot relationships may harm or replace human ones have been raised, but whether and how these social consequences occur remains unclear. Prior research suggests that people's states of social need and their anthropomorphism of the AI agent may play a role in how human-AI interaction impacts human-human interaction. In this longitudinal study (N = 183), participants were randomly assigned to converse with a companion chatbot over text or to play text-based word games for 10 minutes a day for 21 consecutive days. During these 21 days, participants also completed four surveys and two audio-recorded interviews. We found that people's social health and relationships were not significantly impacted by interacting with a companion chatbot across 21 days compared to the control group. However, people who had a higher desire to socially connect anthropomorphized the chatbot more. Those who anthropomorphized the chatbot more indicated that the human-chatbot interaction had greater impacts on their social interactions and relationships with family and friends. A mediation analysis suggested that the impact of human-AI interaction on human-human social outcomes was mediated by the extent to which people anthropomorphized the AI agent, which itself was related to the desire to socially connect.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Computers and Society",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.682648"
    },
    {
        "index": "#132",
        "title": "AIRwaves at CheckThat! 2025: Retrieving Scientific Sources for Implicit Claims on Social Media with Dual Encoders and Neural Re-Ranking",
        "link": "/arxiv/2509.19509",
        "arxiv_id": "2509.19509",
        "authors": "Cem Ashbaugh, Leon Baumgrtner, Tim Gress, Nikita Sidorov, Daniel Werner",
        "summary": "Linking implicit scientific claims made on social media to their original publications is crucial for evidence-based fact-checking and scholarly discourse, yet it is hindered by lexical sparsity, very short queries, and domain-specific language. Team AIRwaves ranked second in Subtask 4b of the CLEF-2025 CheckThat! Lab with an evidence-retrieval approach that markedly outperforms the competition baseline. The optimized sparse-retrieval baseline(BM25) achieves MRR@5 = 0.5025 on the gold label blind test set. To surpass this baseline, a two-stage retrieval pipeline is introduced: (i) a first stage that uses a dual encoder based on E5-large, fine-tuned using in-batch and mined hard negatives and enhanced through chunked tokenization and rich document metadata; and (ii) a neural re-ranking stage using a SciBERT cross-encoder. Replacing purely lexical matching with neural representations lifts performance to MRR@5 = 0.6174, and the complete pipeline further improves to MRR@5 = 0.6828. The findings demonstrate that coupling dense retrieval with neural re-rankers delivers a powerful and efficient solution for tweet-to-study matching and provides a practical blueprint for future evidence-retrieval pipelines.",
        "subjects": "Information Retrieval, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.683269"
    },
    {
        "index": "#133",
        "title": "Generative AI as a catalyst for democratic Innovation: Enhancing citizen engagement in participatory budgeting",
        "link": "/arxiv/2509.19497",
        "arxiv_id": "2509.19497",
        "authors": "Italo Alberto do Nascimento Sousa, Jorge Machado, Jose Carlos Vaz",
        "summary": "This research examines the role of Generative Artificial Intelligence (AI) in enhancing citizen engagement in participatory budgeting. In response to challenges like declining civic participation and increased societal polarization, the study explores how online political participation can strengthen democracy and promote social equity. By integrating Generative AI into public consultation platforms, the research aims to improve citizen proposal formulation and foster effective dialogue between citizens and government. It assesses the capacities governments need to implement AI-enhanced participatory tools, considering technological dependencies and vulnerabilities. Analyzing technological structures, actors, interests, and strategies, the study contributes to understanding how technological advancements can reshape participatory institutions to better facilitate citizen involvement. Ultimately, the research highlights how Generative AI can transform participatory institutions, promoting inclusive, democratic engagement and empowering citizens.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.683468"
    },
    {
        "index": "#134",
        "title": "ArtiFree: Detecting and Reducing Generative Artifacts in Diffusion-based Speech Enhancement",
        "link": "/arxiv/2509.19495",
        "arxiv_id": "2509.19495",
        "authors": "Bhawana Chhaglani, Yang Gao, Julius Richter, Xilin Li, Syavosh Zadissa, Tarun Pruthi, Andrew Lovitt",
        "summary": "Diffusion-based speech enhancement (SE) achieves natural-sounding speech and strong generalization, yet suffers from key limitations like generative artifacts and high inference latency. In this work, we systematically study artifact prediction and reduction in diffusion-based SE. We show that variance in speech embeddings can be used to predict phonetic errors during inference. Building on these findings, we propose an ensemble inference method guided by semantic consistency across multiple diffusion runs. This technique reduces WER by 15% in low-SNR conditions, effectively improving phonetic accuracy and semantic plausibility. Finally, we analyze the effect of the number of diffusion steps, showing that adaptive diffusion steps balance artifact suppression and latency. Our findings highlight semantic priors as a powerful tool to guide generative SE toward artifact-free outputs.",
        "subjects": "Sound, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.684367"
    },
    {
        "index": "#135",
        "title": "Identifying and Addressing User-level Security Concerns in Smart Homes Using \"Smaller\" LLMs",
        "link": "/arxiv/2509.19485",
        "arxiv_id": "2509.19485",
        "authors": "Hafijul Hoque Chowdhury, Riad Ahmed Anonto, Sourov Jajodia, Suryadipta Majumdar, Md. Shohrab Hossain",
        "summary": "With the rapid growth of smart home IoT devices, users are increasingly exposed to various security risks, as evident from recent studies. While seeking answers to know more on those security concerns, users are mostly left with their own discretion while going through various sources, such as online blogs and technical manuals, which may render higher complexity to regular users trying to extract the necessary information. This requirement does not go along with the common mindsets of smart home users and hence threatens the security of smart homes furthermore. In this paper, we aim to identify and address the major user-level security concerns in smart homes. Specifically, we develop a novel dataset of Q&A from public forums, capturing practical security challenges faced by smart home users. We extract major security concerns in smart homes from our dataset by leveraging the Latent Dirichlet Allocation (LDA). We fine-tune relatively \"smaller\" transformer models, such as T5 and Flan-T5, on this dataset to build a QA system tailored for smart home security. Unlike larger models like GPT and Gemini, which are powerful but often resource hungry and require data sharing, smaller models are more feasible for deployment in resource-constrained or privacy-sensitive environments like smart homes. The dataset is manually curated and supplemented with synthetic data to explore its potential impact on model performance. This approach significantly improves the system's ability to deliver accurate and relevant answers, helping users address common security concerns with smart home IoT devices. Our experiments on real-world user concerns show that our work improves the performance of the base models.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.684620"
    },
    {
        "index": "#136",
        "title": "A Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models",
        "link": "/arxiv/2509.19465",
        "arxiv_id": "2509.19465",
        "authors": "Kin G. Olivares, Malcolm Wolff, Tatiana Konstantinova, Shankar Ramasubramanian, Andrew Gordon Wilson, Andres Potapczynski, Willa Potosnak, Mengfei Cao, Boris Oreshkin, Dmitry Efimov",
        "summary": "Cross-frequency transfer learning (CFTL) has emerged as a popular framework for curating large-scale time series datasets to pre-train foundation forecasting models (FFMs). Although CFTL has shown promise, current benchmarking practices fall short of accurately assessing its performance. This shortcoming stems from many factors: an over-reliance on small-scale evaluation datasets; inadequate treatment of sample size when computing summary statistics; reporting of suboptimal statistical models; and failing to account for non-negligible risks of overlap between pre-training and test datasets. To address these limitations, we introduce a unified reimplementation of widely-adopted neural forecasting networks, adapting them for the CFTL setup; we pre-train only on proprietary and synthetic data, being careful to prevent test leakage; and we evaluate on 15 large, diverse public forecast competition datasets. Our empirical analysis reveals that statistical models' accuracy is frequently underreported. Notably, we confirm that statistical models and their ensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and by more than 20% MASE, across datasets. However, we also find that synthetic dataset pre-training does improve the accuracy of a FFM by 7% percent.",
        "subjects": "Machine Learning, Artificial Intelligence, Applications",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.684885"
    },
    {
        "index": "#137",
        "title": "Self-evolved Imitation Learning in Simulated World",
        "link": "/arxiv/2509.19460",
        "arxiv_id": "2509.19460",
        "authors": "Yifan Ye, Jun Cen, Jing Chen, Zhihe Lu",
        "summary": "Imitation learning has been a trend recently, yet training a generalist agent across multiple tasks still requires large-scale expert demonstrations, which are costly and labor-intensive to collect. To address the challenge of limited supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework that progressively improves a few-shot model through simulator interactions. The model first attempts tasksin the simulator, from which successful trajectories are collected as new demonstrations for iterative refinement. To enhance the diversity of these demonstrations, SEIL employs dual-level augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model to collaborate with the primary model, and (ii) Environment-level, introducing slight variations in initial object positions. We further introduce a lightweight selector that filters complementary and informative trajectories from the generated pool to ensure demonstration quality. These curated samples enable the model to achieve competitive performance with far fewer training examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves a new state-of-the-art performance in few-shot imitation learning scenarios. Code is available at https://github.com/Jasper-aaa/SEIL.git.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.685103"
    },
    {
        "index": "#139",
        "title": "Probabilistic Runtime Verification, Evaluation and Risk Assessment of Visual Deep Learning Systems",
        "link": "/arxiv/2509.19419",
        "arxiv_id": "2509.19419",
        "authors": "Birk Torpmann-Hagen, Pl Halvorsen, Michael A. Riegler, Dag Johansen",
        "summary": "Despite achieving excellent performance on benchmarks, deep neural networks often underperform in real-world deployment due to sensitivity to minor, often imperceptible shifts in input data, known as distributional shifts. These shifts are common in practical scenarios but are rarely accounted for during evaluation, leading to inflated performance metrics. To address this gap, we propose a novel methodology for the verification, evaluation, and risk assessment of deep learning systems. Our approach explicitly models the incidence of distributional shifts at runtime by estimating their probability from outputs of out-of-distribution detectors. We combine these estimates with conditional probabilities of network correctness, structuring them in a binary tree. By traversing this tree, we can compute credible and precise estimates of network accuracy. We assess our approach on five different datasets, with which we simulate deployment conditions characterized by differing frequencies of distributional shift. Our approach consistently outperforms conventional evaluation, with accuracy estimation errors typically ranging between 0.01 and 0.1. We further showcase the potential of our approach on a medical segmentation benchmark, wherein we apply our methods towards risk assessment by associating costs with tree nodes, informing cost-benefit analyses and value-judgments. Ultimately, our approach offers a robust framework for improving the reliability and trustworthiness of deep learning systems, particularly in safety-critical applications, by providing more accurate performance estimates and actionable risk assessments.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.685658"
    },
    {
        "index": "#140",
        "title": "EngravingGNN: A Hybrid Graph Neural Network for End-to-End Piano Score Engraving",
        "link": "/arxiv/2509.19412",
        "arxiv_id": "2509.19412",
        "authors": "Emmanouil Karystinaios, Francesco Foscarin, Gerhard Widmer",
        "summary": "This paper focuses on automatic music engraving, i.e., the creation of a humanly-readable musical score from musical content. This step is fundamental for all applications that include a human player, but it remains a mostly unexplored topic in symbolic music processing. In this work, we formalize the problem as a collection of interdependent subtasks, and propose a unified graph neural network (GNN) framework that targets the case of piano music and quantized symbolic input. Our method employs a multi-task GNN to jointly predict voice connections, staff assignments, pitch spelling, key signature, stem direction, octave shifts, and clef signs. A dedicated postprocessing pipeline generates print-ready MusicXML/MEI outputs. Comprehensive evaluation on two diverse piano corpora (J-Pop and DCML Romantic) demonstrates that our unified model achieves good accuracy across all subtasks, compared to existing systems that only specialize in specific subtasks. These results indicate that a shared GNN encoder with lightweight task-specific decoders in a multi-task setting offers a scalable and effective solution for automatic music engraving.",
        "subjects": "Graphics, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.685932"
    },
    {
        "index": "#141",
        "title": "TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via Adaptive Granularity Patch and Segment-wise Decoding",
        "link": "/arxiv/2509.19406",
        "arxiv_id": "2509.19406",
        "authors": "Kuiye Ding, Fanda Fan, Chunyi Hou, Zheya Wang, Lei Wang, Zhengxin Yang, Jianfeng Zhan",
        "summary": "Multivariate time series forecasting is essential in domains such as finance, transportation, climate, and energy. However, existing patch-based methods typically adopt fixed-length segmentation, overlooking the heterogeneity of local temporal dynamics and the decoding heterogeneity of forecasting. Such designs lose details in information-dense regions, introduce redundancy in stable segments, and fail to capture the distinct complexities of short-term and long-term horizons. We propose TimeMosaic, a forecasting framework that aims to address temporal heterogeneity. TimeMosaic employs adaptive patch embedding to dynamically adjust granularity according to local information density, balancing motif reuse with structural clarity while preserving temporal continuity. In addition, it introduces segment-wise decoding that treats each prediction horizon as a related subtask and adapts to horizon-specific difficulty and information requirements, rather than applying a single uniform decoder. Extensive evaluations on benchmark datasets demonstrate that TimeMosaic delivers consistent improvements over existing methods, and our model trained on the large-scale corpus with 321 billion observations achieves performance competitive with state-of-the-art TSFMs.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.686248"
    },
    {
        "index": "#142",
        "title": "Improving Outdoor Multi-cell Fingerprinting-based Positioning via Mobile Data Augmentation",
        "link": "/arxiv/2509.19405",
        "arxiv_id": "2509.19405",
        "authors": "Tony Chahoud, Lorenzo Mario Amorosa, Riccardo Marini, Luca De Nardis",
        "summary": "Accurate outdoor positioning in cellular networks is hindered by sparse, heterogeneous measurement collections and the high cost of exhaustive site surveys. This paper introduces a lightweight, modular mobile data augmentation framework designed to enhance multi-cell fingerprinting-based positioning using operator-collected minimization of drive test (MDT) records. The proposed approach decouples spatial and radio-feature synthesis: kernel density estimation (KDE) models the empirical spatial distribution to generate geographically coherent synthetic locations, while a k-nearest-neighbor (KNN)-based block produces augmented per-cell radio fingerprints. The architecture is intentionally training-free, interpretable, and suitable for distributed or on-premise operator deployments, supporting privacy-aware workflows. We both validate each augmentation module independently and assess its end-to-end impact on fingerprinting-based positioning using a real-world MDT dataset provided by an Italian mobile network operator across diverse urban and peri-urban scenarios. Results show that the proposed KDE-KNN augmentation consistently improves positioning performance, with the largest benefits in sparsely sampled or structurally complex regions; we also observe region-dependent saturation effects as augmentation increases. The framework offers a practical, low-complexity path to enhance operator positioning services using existing mobile data traces.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.686505"
    },
    {
        "index": "#143",
        "title": "Online Adaptation via Dual-Stage Alignment and Self-Supervision for Fast-Calibration Brain-Computer Interfaces",
        "link": "/arxiv/2509.19403",
        "arxiv_id": "2509.19403",
        "authors": "Sheng-Bin Duan, Jian-Long Hao, Tian-Yu Xiang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Zeng-Guang Hou",
        "summary": "Individual differences in brain activity hinder the online application of electroencephalogram (EEG)-based brain computer interface (BCI) systems. To overcome this limitation, this study proposes an online adaptation algorithm for unseen subjects via dual-stage alignment and self-supervision. The alignment process begins by applying Euclidean alignment in the EEG data space and then updates batch normalization statistics in the representation space. Moreover, a self-supervised loss is designed to update the decoder. The loss is computed by soft pseudo-labels derived from the decoder as a proxy for the unknown ground truth, and is calibrated by Shannon entropy to facilitate self-supervised training. Experiments across five public datasets and seven decoders show the proposed algorithm can be integrated seamlessly regardless of BCI paradigm and decoder architecture. In each iteration, the decoder is updated with a single online trial, which yields average accuracy gains of 4.9% on steady-state visual evoked potentials (SSVEP) and 3.6% on motor imagery. These results support fast-calibration operation and show that the proposed algorithm has great potential for BCI applications.",
        "subjects": "Signal Processing, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.686785"
    },
    {
        "index": "#144",
        "title": "FedOC: Multi-Server FL with Overlapping Client Relays in Wireless Edge Networks",
        "link": "/arxiv/2509.19398",
        "arxiv_id": "2509.19398",
        "authors": "Yun Ji, Zeyu Chen, Xiaoxiong Zhong, Yanan Ma, Sheng Zhang, Yuguang Fang",
        "summary": "Multi-server Federated Learning (FL) has emerged as a promising solution to mitigate communication bottlenecks of single-server FL. We focus on a typical multi-server FL architecture, where the regions covered by different edge servers (ESs) may overlap. A key observation of this architecture is that clients located in the overlapping areas can access edge models from multiple ESs. Building on this insight, we propose FedOC (Federated learning with Overlapping Clients), a novel framework designed to fully exploit the potential of these overlapping clients. In FedOC, overlapping clients could serve dual roles: (1) as Relay Overlapping Clients (ROCs), they forward edge models between neighboring ESs in real time to facilitate model sharing among different ESs; and (2) as Normal Overlapping Clients (NOCs), they dynamically select their initial model for local training based on the edge model delivery time, which enables indirect data fusion among different regions of ESs. The overall FedOC workflow proceeds as follows: in every round, each client trains local model based on the earliest received edge model and transmits to the respective ESs for model aggregation. Then each ES transmits the aggregated edge model to neighboring ESs through ROC relaying. Upon receiving the relayed models, each ES performs a second aggregation and subsequently broadcasts the updated model to covered clients. The existence of ROCs enables the model of each ES to be disseminated to the other ESs in a decentralized manner, which indirectly achieves intercell model and speeding up the training process, making it well-suited for latency-sensitive edge environments. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.687021"
    },
    {
        "index": "#145",
        "title": "Self-Alignment Learning to Improve Myocardial Infarction Detection from Single-Lead ECG",
        "link": "/arxiv/2509.19397",
        "arxiv_id": "2509.19397",
        "authors": "Jiarui Jin, Xiaocheng Fang, Haoyu Wang, Jun Li, Che Liu, Donglin Xie, Hongyan Li, Shenda Hong",
        "summary": "Myocardial infarction is a critical manifestation of coronary artery disease, yet detecting it from single-lead electrocardiogram (ECG) remains challenging due to limited spatial information. An intuitive idea is to convert single-lead into multiple-lead ECG for classification by pre-trained models, but generative methods optimized at the signal level in most cases leave a large latent space gap, ultimately degrading diagnostic performance. This naturally raises the question of whether latent space alignment could help. However, most prior ECG alignment methods focus on learning transformation invariance, which mismatches the goal of single-lead detection. To address this issue, we propose SelfMIS, a simple yet effective alignment learning framework to improve myocardial infarction detection from single-lead ECG. Discarding manual data augmentations, SelfMIS employs a self-cutting strategy to pair multiple-lead ECG with their corresponding single-lead segments and directly align them in the latent space. This design shifts the learning objective from pursuing transformation invariance to enriching the single-lead representation, explicitly driving the single-lead ECG encoder to learn a representation capable of inferring global cardiac context from the local signal. Experimentally, SelfMIS achieves superior performance over baseline models across nine myocardial infarction types while maintaining a simpler architecture and lower computational overhead, thereby substantiating the efficacy of direct latent space alignment. Our code and checkpoint will be publicly available after acceptance.",
        "subjects": "Signal Processing, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.687269"
    },
    {
        "index": "#146",
        "title": "OmniFed: A Modular Framework for Configurable Federated Learning from Edge to HPC",
        "link": "/arxiv/2509.19396",
        "arxiv_id": "2509.19396",
        "authors": "Sahil Tyagi, Andrei Cozma, Olivera Kotevska, Feiyi Wang",
        "summary": "Federated Learning (FL) is critical for edge and High Performance Computing (HPC) where data is not centralized and privacy is crucial. We present OmniFed, a modular framework designed around decoupling and clear separation of concerns for configuration, orchestration, communication, and training logic. Its architecture supports configuration-driven prototyping and code-level override-what-you-need customization. We also support different topologies, mixed communication protocols within a single deployment, and popular training algorithms. It also offers optional privacy mechanisms including Differential Privacy (DP), Homomorphic Encryption (HE), and Secure Aggregation (SA), as well as compression strategies. These capabilities are exposed through well-defined extension points, allowing users to customize topology and orchestration, learning logic, and privacy/compression plugins, all while preserving the integrity of the core system. We evaluate multiple models and algorithms to measure various performance metrics. By unifying topology configuration, mixed-protocol communication, and pluggable modules in one stack, OmniFed streamlines FL deployment across heterogeneous environments. Github repository is available at https://github.com/at-aaims/OmniFed.",
        "subjects": "Machine Learning, Artificial Intelligence, Cryptography and Security, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.687492"
    },
    {
        "index": "#147",
        "title": "TensLoRA: Tensor Alternatives for Low-Rank Adaptation",
        "link": "/arxiv/2509.19391",
        "arxiv_id": "2509.19391",
        "authors": "Axel Marmoret, Reda Bensaid, Jonathan Lys, Vincent Gripon, Franois Leduc-Primeau",
        "summary": "Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers by adding trainable low-rank matrices to attention projections. While effective, these matrices are considered independent for each attention projection (Query, Key, and Value) and each layer. Recent extensions have considered joint, tensor-based adaptations, but only in limited forms and without a systematic framework. We introduce TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors and models a broad family of tensor-based low-rank adaptations. Our formulation generalizes existing tensor-based methods and enables mode-specific compression rates, allowing parameter budgets to be tailored according to the modality and task. Experiments on vision and language benchmarks reveal that the tensor construction directly impacts performance, sometimes better than standard LoRA under similar parameter counts.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.687709"
    },
    {
        "index": "#148",
        "title": "Data-Driven Reconstruction of Significant Wave Heights from Sparse Observations",
        "link": "/arxiv/2509.19384",
        "arxiv_id": "2509.19384",
        "authors": "Hongyuan Shi, Yilin Zhai, Ping Dong, Zaijin You, Chao Zhan, Qing Wang",
        "summary": "Reconstructing high-resolution regional significant wave height fields from sparse and uneven buoy observations remains a core challenge for ocean monitoring and risk-aware operations. We introduce AUWave, a hybrid deep learning framework that fuses a station-wise sequence encoder (MLP) with a multi-scale U-Net enhanced by a bottleneck self-attention layer to recover 32$\\times$32 regional SWH fields. A systematic Bayesian hyperparameter search with Optuna identifies the learning rate as the dominant driver of generalization, followed by the scheduler decay and the latent dimension. Using NDBC buoy observations and ERA5 reanalysis over the Hawaii region, AUWave attains a minimum validation loss of 0.043285 and a slightly right-skewed RMSE distribution. Spatial errors are lowest near observation sites and increase with distance, reflecting identifiability limits under sparse sampling. Sensitivity experiments show that AUWave consistently outperforms a representative baseline in data-richer configurations, while the baseline is only marginally competitive in the most underdetermined single-buoy cases. The architecture's multi-scale and attention components translate into accuracy gains when minimal but non-trivial spatial anchoring is available. Error maps and buoy ablations reveal key anchor stations whose removal disproportionately degrades performance, offering actionable guidance for network design. AUWave provides a scalable pathway for gap filling, high-resolution priors for data assimilation, and contingency reconstruction.",
        "subjects": "Signal Processing, Artificial Intelligence, Machine Learning, Atmospheric and Oceanic Physics",
        "date": "2025-09-21",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.688169"
    },
    {
        "index": "#149",
        "title": "Learning from Observation: A Survey of Recent Advances",
        "link": "/arxiv/2509.19379",
        "arxiv_id": "2509.19379",
        "authors": "Returaj Burnwal, Hriday Mehta, Nirav Pravinbhai Bhatt, Balaraman Ravindran",
        "summary": "Imitation Learning (IL) algorithms offer an efficient way to train an agent by mimicking an expert's behavior without requiring a reward function. IL algorithms often necessitate access to state and action information from expert demonstrations. Although expert actions can provide detailed guidance, requiring such action information may prove impractical for real-world applications where expert actions are difficult to obtain. To address this limitation, the concept of learning from observation (LfO) or state-only imitation learning (SOIL) has recently gained attention, wherein the imitator only has access to expert state visitation information. In this paper, we present a framework for LfO and use it to survey and classify existing LfO methods in terms of their trajectory construction, assumptions and algorithm's design choices. This survey also draws connections between several related fields like offline RL, model-based RL and hierarchical RL. Finally, we use our framework to identify open problems and suggest future research directions.",
        "subjects": "Machine Learning, Artificial Intelligence, Robotics, Machine Learning",
        "date": "2025-09-20",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.688425"
    },
    {
        "index": "#150",
        "title": "Solving Freshness in RAG: A Simple Recency Prior and the Limits of Heuristic Trend Detection",
        "link": "/arxiv/2509.19376",
        "arxiv_id": "2509.19376",
        "authors": "Matthew Grofsky",
        "summary": "We address temporal failures in RAG systems using two methods on cybersecurity data. A simple recency prior achieved an accuracy of 1.00 on freshness tasks. In contrast, a clustering heuristic for topic evolution failed (0.08 F1-score), showing trend detection requires methods beyond simple heuristics.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-20",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.688616"
    },
    {
        "index": "#151",
        "title": "Uncertainty Quantification of Large Language Models using Approximate Bayesian Computation",
        "link": "/arxiv/2509.19375",
        "arxiv_id": "2509.19375",
        "authors": "Mridul Sharma, Adeetya Patel, Zaneta D' Souza, Samira Abbasgholizadeh Rahimi, Siva Reddy, Sreenath Madathil",
        "summary": "Despite their widespread applications, Large Language Models (LLMs) often struggle to express uncertainty, posing a challenge for reliable deployment in high stakes and safety critical domains like clinical diagnostics. Existing standard baseline methods such as model logits and elicited probabilities produce overconfident and poorly calibrated estimates. In this work, we propose Approximate Bayesian Computation (ABC), a likelihood-free Bayesian inference, based approach that treats LLMs as a stochastic simulator to infer posterior distributions over predictive probabilities. We evaluate our ABC approach on two clinically relevant benchmarks: a synthetic oral lesion diagnosis dataset and the publicly available GretelAI symptom-to-diagnosis dataset. Compared to standard baselines, our approach improves accuracy by up to 46.9\\%, reduces Brier scores by 74.4\\%, and enhances calibration as measured by Expected Calibration Error (ECE) and predictive entropy.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-09-19",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.688850"
    },
    {
        "index": "#152",
        "title": "Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution",
        "link": "/arxiv/2509.19372",
        "arxiv_id": "2509.19372",
        "authors": "Zuzanna Dubanowska, Maciej elaszczyk, Micha Brzozowski, Paolo Mandica, Micha Karpowicz",
        "summary": "We critically assess the efficacy of the current SOTA in hallucination detection and find that its performance on the RAGTruth dataset is largely driven by a spurious correlation with data. Controlling for this effect, state-of-the-art performs no better than supervised linear probes, while requiring extensive hyperparameter tuning across datasets. Out-of-distribution generalization is currently out of reach, with all of the analyzed methods performing close to random. We propose a set of guidelines for hallucination detection and its evaluation.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-19",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.689059"
    },
    {
        "index": "#157",
        "title": "Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data",
        "link": "/arxiv/2509.19366",
        "arxiv_id": "2509.19366",
        "authors": "Buhe Li, Berkay Kaplan, Maksym Lazirko, Aleksandr Kogan",
        "summary": "This study investigates the effectiveness of unsupervised outlier detection methods in audit analytics, utilizing USA spending data from the U.S. Department of Health and Human Services (DHHS) as a case example. We employ and compare multiple outlier detection algorithms, including Histogram-based Outlier Score (HBOS), Robust Principal Component Analysis (PCA), Minimum Covariance Determinant (MCD), and K-Nearest Neighbors (KNN) to identify anomalies in federal spending patterns. The research addresses the growing need for efficient and accurate anomaly detection in large-scale governmental datasets, where traditional auditing methods may fall short. Our methodology involves data preparation, algorithm implementation, and performance evaluation using precision, recall, and F1 scores. Results indicate that a hybrid approach, combining multiple detection strategies, enhances the robustness and accuracy of outlier identification in complex financial data. This study contributes to the field of audit analytics by providing insights into the comparative effectiveness of various outlier detection models and demonstrating the potential of unsupervised learning techniques in improving audit quality and efficiency. The findings have implications for auditors, policymakers, and researchers seeking to leverage advanced analytics in governmental financial oversight and risk management.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-19",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.690352"
    },
    {
        "index": "#159",
        "title": "Analyzing the Impact of Credit Card Fraud on Economic Fluctuations of American Households Using an Adaptive Neuro-Fuzzy Inference System",
        "link": "/arxiv/2509.19363",
        "arxiv_id": "2509.19363",
        "authors": "Zhuqi Wang, Qinghe Zhang, Zhuopei Cheng",
        "summary": "Credit card fraud is assuming growing proportions as a major threat to the financial position of American household, leading to unpredictable changes in household economic behavior. To solve this problem, in this paper, a new hybrid analysis method is presented by using the Enhanced ANFIS. The model proposes several advances of the conventional ANFIS framework and employs a multi-resolution wavelet decomposition module and a temporal attention mechanism. The model performs discrete wavelet transformations on historical transaction data and macroeconomic indicators to generate localized economic shock signals. The transformed features are then fed into a deep fuzzy rule library which is based on Takagi-Sugeno fuzzy rules with adaptive Gaussian membership functions. The model proposes a temporal attention encoder that adaptively assigns weights to multi-scale economic behavior patterns, increasing the effectiveness of relevance assessment in the fuzzy inference stage and enhancing the capture of long-term temporal dependencies and anomalies caused by fraudulent activities. The proposed method differs from classical ANFIS which has fixed input-output relations since it integrates fuzzy rule activation with the wavelet basis selection and the temporal correlation weights via a modular training procedure. Experimental results show that the RMSE was reduced by 17.8% compared with local neuro-fuzzy models and conventional LSTM models.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-18",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.690815"
    },
    {
        "index": "#160",
        "title": "DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence Models",
        "link": "/arxiv/2509.19362",
        "arxiv_id": "2509.19362",
        "authors": "Benedikt W. Hosp",
        "summary": "Feature attribution is essential for interpreting deep learning models, particularly in time-series domains such as healthcare, biometrics, and human-AI interaction. However, standard attribution methods, such as Integrated Gradients or SHAP, are computationally intensive and not well-suited for real-time applications. We present DeepACTIF, a lightweight and architecture-aware feature attribution method that leverages internal activations of sequence models to estimate feature importance efficiently. Focusing on LSTM-based networks, we introduce an inverse-weighted aggregation scheme that emphasises stability and magnitude of activations across time steps. Our evaluation across three biometric gaze datasets shows that DeepACTIF not only preserves predictive performance under severe feature reduction (top 10% of features) but also significantly outperforms established methods, including SHAP, IG, and DeepLIFT, in terms of both accuracy and statistical robustness. Using Wilcoxon signed-rank tests and effect size analysis, we demonstrate that DeepACTIF yields more informative feature rankings with significantly lower error across all top-k conditions (10 - 40%). Our experiments demonstrate that DeepACTIF not only reduces computation time and memory usage by orders of magnitude but also preserves model accuracy when using only top-ranked features. That makes DeepACTIF a viable solution for real-time interpretability on edge devices such as mobile XR headsets or embedded health monitors.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-18",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.691074"
    },
    {
        "index": "#162",
        "title": "Anti-Money Laundering Systems Using Deep Learning",
        "link": "/arxiv/2509.19359",
        "arxiv_id": "2509.19359",
        "authors": "Mashkhal Abdalwahid Sidiq, Yimamu Kirubel Wondaferew",
        "summary": "In this paper, we focused on using deep learning methods for detecting money laundering in financial transaction networks, in order to demonstrate that it can be used as a complement or instead of the more commonly used rule-based systems and conventional Anti-Money Laundering (AML) systems. The paper explores the pivotal role played by Anti-Money Laundering (AML) activities in the global financial industry. It underscores the drawbacks of conventional AML systems, which exhibit high rates of false positives and lack the sophistication to uncover intricate money laundering schemes. To tackle these challenges, the paper proposes an advanced AML system that capitalizes on link analysis using deep learning techniques. At the heart of this system lies the utilization of centrality algorithms like Degree Centrality, Closeness Centrality, Betweenness Centrality, and PageRank. These algorithms enhance the system's capability to identify suspicious activities by examining the influence and interconnections within networks of financial transactions. The significance of Anti-Money Laundering (AML) efforts within the global financial sector is discussed in this paper. It highlights the limitations of traditional AML systems. The results showed the practicality and superiority of the new implementation of the GCN model, which is a preferable method for connectively structured data, meaning that a transaction or account is analyzed in the context of its financial environment. In addition, the paper delves into the prospects of Anti-Money Laundering (AML) efforts, proposing the integration of emerging technologies such as deep learning and centrality algorithms. This integration holds promise for enhancing the effectiveness of AML systems by refining their capabilities.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-18",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.691645"
    },
    {
        "index": "#166",
        "title": "The Impact of Structural Changes on Learning Capacity in the Fly Olfactory Neural Circuit",
        "link": "/arxiv/2509.19351",
        "arxiv_id": "2509.19351",
        "authors": "Katherine Xie, Gabriel Koch Ocker",
        "summary": "The Drosophila mushroom body (MB) is known to be involved in olfactory learning and memory; the synaptic plasticity of the Kenyon cell (KC) to mushroom body output neuron (MBON) synapses plays a key role in the learning process. Previous research has focused on projection neuron (PN) to Kenyon cell (KC) connectivity within the MB; we examine how perturbations to the mushroom body circuit structure and changes in connectivity, specifically within the KC to mushroom body output neuron (MBON) neural circuit, affect the MBONs' ability to distinguish between odor classes. We constructed a neural network that incorporates the connectivity between PNs, KCs, and MBONs. To train our model, we generated ten artificial input classes, which represent the projection neuron activity in response to different odors. We collected data on the number of KC-to-MBON connections, MBON error rates, and KC-to-MBON synaptic weights, among other metrics. We observed that MBONs with very few presynaptic KCs consistently performed worse than others in the odor classification task. The developmental types of KCs also played a significant role in each MBON's output. We performed random and targeted KC ablation and observed that ablating developmentally mature KCs had a greater negative impact on MBONs' learning capacity than ablating immature KCs. Random and targeted pruning of KC-MBON synaptic connections yielded results largely consistent with the ablation experiments. To further explore the various types of KCs, we also performed rewiring experiments in the PN to KC circuit. Our study furthers our understanding of olfactory neuroplasticity and provides important clues to understanding learning and memory in general. Understanding how the olfactory circuits process and learn can also have potential applications in artificial intelligence and treatments for neurodegenerative diseases.",
        "subjects": "Neurons and Cognition, Artificial Intelligence, Machine Learning, Neural and Evolutionary Computing",
        "date": "2025-09-18",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.692515"
    },
    {
        "index": "#169",
        "title": "Fine-Grained AI Model Caching and Downloading With Coordinated Multipoint Broadcasting in Multi-Cell Edge Networks",
        "link": "/arxiv/2509.19341",
        "arxiv_id": "2509.19341",
        "authors": "Yang Fu, Peng Qin, Yueyue Zhang, Yifei Wang",
        "summary": "6G networks are envisioned to support on-demand AI model downloading to accommodate diverse inference requirements of end users. By proactively caching models at edge nodes, users can retrieve the requested models with low latency for on-device AI inference. However, the substantial size of contemporary AI models poses significant challenges for edge caching under limited storage capacity, as well as for the concurrent delivery of heterogeneous models over wireless channels. To address these challenges, we propose a fine-grained AI model caching and downloading system that exploits parameter reusability, stemming from the common practice of fine-tuning task-specific models from a shared pre-trained model with frozen parameters. This system selectively caches model parameter blocks (PBs) at edge nodes, eliminating redundant storage of reusable parameters across different cached models. Additionally, it incorporates coordinated multipoint (CoMP) broadcasting to simultaneously deliver reusable PBs to multiple users, thereby enhancing downlink spectrum utilization. Under this arrangement, we formulate a model downloading delay minimization problem to jointly optimize PB caching, migration (among edge nodes), and broadcasting beamforming. To tackle this intractable problem, we develop a distributed multi-agent learning framework that enables edge nodes to explicitly learn mutual influence among their actions, thereby facilitating cooperation. Furthermore, a data augmentation approach is proposed to adaptively generate synthetic training samples through a predictive model, boosting sample efficiency and accelerating policy learning. Both theoretical analysis and simulation experiments validate the superior convergence performance of the proposed learning framework.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence, Machine Learning",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.693194"
    },
    {
        "index": "#170",
        "title": "Joint Channel Estimation and Computation Offloading in Fluid Antenna-assisted MEC Networks",
        "link": "/arxiv/2509.19340",
        "arxiv_id": "2509.19340",
        "authors": "Ying Ju, Mingdong Li, Haoyu Wang, Lei Liu, Youyang Qu, Mianxiong Dong, Victor C. M. Leung, Chau Yuen",
        "summary": "With the emergence of fluid antenna (FA) in wireless communications, the capability to dynamically adjust port positions offers substantial benefits in spatial diversity and spectrum efficiency, which are particularly valuable for mobile edge computing (MEC) systems. Therefore, we propose an FA-assisted MEC offloading framework to minimize system delay. This framework faces two severe challenges, which are the complexity of channel estimation due to dynamic port configuration and the inherent non-convexity of the joint optimization problem. Firstly, we propose Information Bottleneck Metric-enhanced Channel Compressed Sensing (IBM-CCS), which advances FA channel estimation by integrating information relevance into the sensing process and capturing key features of FA channels effectively. Secondly, to address the non-convex and high-dimensional optimization problem in FA-assisted MEC systems, which includes FA port selection, beamforming, power control, and resource allocation, we propose a game theory-assisted Hierarchical Twin-Dueling Multi-agent Algorithm (HiTDMA) based offloading scheme, where the hierarchical structure effectively decouples and coordinates the optimization tasks between the user side and the base station side. Crucially, the game theory effectively reduces the dimensionality of power control variables, allowing deep reinforcement learning (DRL) agents to achieve improved optimization efficiency. Numerical results confirm that the proposed scheme significantly reduces system delay and enhances offloading performance, outperforming benchmarks. Additionally, the IBM-CCS channel estimation demonstrates superior accuracy and robustness under varying port densities, contributing to efficient communication under imperfect CSI.",
        "subjects": "Signal Processing, Artificial Intelligence, Information Theory, Networking and Internet Architecture",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.693454"
    },
    {
        "index": "#171",
        "title": "Multi-population Ensemble Genetic Programming via Cooperative Coevolution and Multi-view Learning for Classification",
        "link": "/arxiv/2509.19339",
        "arxiv_id": "2509.19339",
        "authors": "Mohammad Sadegh Khorshidi, Navid Yazdanjue, Hassan Gharoun, Mohammad Reza Nikoo, Fang Chen, Amir H. Gandomi",
        "summary": "This paper introduces Multi-population Ensemble Genetic Programming (MEGP), a computational intelligence framework that integrates cooperative coevolution and the multiview learning paradigm to address classification challenges in high-dimensional and heterogeneous feature spaces. MEGP decomposes the input space into conditionally independent feature subsets, enabling multiple subpopulations to evolve in parallel while interacting through a dynamic ensemble-based fitness mechanism. Each individual encodes multiple genes whose outputs are aggregated via a differentiable softmax-based weighting layer, enhancing both model interpretability and adaptive decision fusion. A hybrid selection mechanism incorporating both isolated and ensemble-level fitness promotes inter-population cooperation while preserving intra-population diversity. This dual-level evolutionary dynamic facilitates structured search exploration and reduces premature convergence. Experimental evaluations across eight benchmark datasets demonstrate that MEGP consistently outperforms a baseline GP model in terms of convergence behavior and generalization performance. Comprehensive statistical analyses validate significant improvements in Log-Loss, Precision, Recall, F1 score, and AUC. MEGP also exhibits robust diversity retention and accelerated fitness gains throughout evolution, highlighting its effectiveness for scalable, ensemble-driven evolutionary learning. By unifying population-based optimization, multi-view representation learning, and cooperative coevolution, MEGP contributes a structurally adaptive and interpretable framework that advances emerging directions in evolutionary machine learning.",
        "subjects": "Neural and Evolutionary Computing, Artificial Intelligence",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.694173"
    },
    {
        "index": "#172",
        "title": "Radio Propagation Modelling: To Differentiate or To Deep Learn, That Is The Question",
        "link": "/arxiv/2509.19337",
        "arxiv_id": "2509.19337",
        "authors": "Stefanos Bakirtzis, Paul Almasan, Jos Surez-Varela, Gabriel O. Ferreira, Michail Kalntis, Andr Felipe Zanella, Ian Wassell, Andra Lutu",
        "summary": "Differentiable ray tracing has recently challenged the status quo in radio propagation modelling and digital twinning. Promising unprecedented speed and the ability to learn from real-world data, it offers a real alternative to conventional deep learning (DL) models. However, no experimental evaluation on production-grade networks has yet validated its assumed scalability or practical benefits. This leaves mobile network operators (MNOs) and the research community without clear guidance on its applicability. In this paper, we fill this gap by employing both differentiable ray tracing and DL models to emulate radio coverage using extensive real-world data collected from the network of a major MNO, covering 13 cities and more than 10,000 antennas. Our results show that, while differentiable ray-tracing simulators have contributed to reducing the efficiency-accuracy gap, they struggle to generalize from real-world data at a large scale, and they remain unsuitable for real-time applications. In contrast, DL models demonstrate higher accuracy and faster adaptation than differentiable ray-tracing simulators across urban, suburban, and rural deployments, achieving accuracy gains of up to 3 dB. Our experimental results aim to provide timely insights into a fundamental open question with direct implications on the wireless ecosystem and future research.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence",
        "date": "2025-09-15",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.699787"
    },
    {
        "index": "#174",
        "title": "CSIYOLO: An Intelligent CSI-based Scatter Sensing Framework for Integrated Sensing and Communication Systems",
        "link": "/arxiv/2509.19335",
        "arxiv_id": "2509.19335",
        "authors": "Xudong Zhang, Jingbo Tan, Zhizhen Ren, Jintao Wang, Yihua Ma, Jian Song",
        "summary": "ISAC is regarded as a promising technology for next-generation communication systems, enabling simultaneous data transmission and target sensing. Among various tasks in ISAC, scatter sensing plays a crucial role in exploiting the full potential of ISAC and supporting applications such as autonomous driving and low-altitude economy. However, most existing methods rely on either waveform and hardware modifications or traditional signal processing schemes, leading to poor compatibility with current communication systems and limited sensing accuracy. To address these challenges, we propose CSIYOLO, a framework that performs scatter localization only using estimated CSI from a single base station-user equipment pair. This framework comprises two main components: anchor-based scatter parameter detection and CSI-based scatter localization. First, by formulating scatter parameter extraction as an image detection problem, we propose an anchor-based scatter parameter detection method inspired by You Only Look Once architectures. After that, a CSI-based localization algorithm is derived to determine scatter locations with extracted parameters. Moreover, to improve localization accuracy and implementation efficiency, we design an extendable network structure with task-oriented optimizations, enabling multi-scale anchor detection and better adaptation to CSI characteristics. A noise injection training strategy is further designed to enhance robustness against channel estimation errors. Since the proposed framework operates solely on estimated CSI without modifying waveforms or signal processing pipelines, it can be seamlessly integrated into existing communication systems as a plugin. Experiments show that our proposed method can significantly outperform existing methods in scatter localization accuracy with relatively low complexities under varying numbers of scatters and estimation errors.",
        "subjects": "Signal Processing, Artificial Intelligence",
        "date": "2025-09-15",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.700477"
    },
    {
        "index": "#177",
        "title": "Holographic Transformers for Complex-Valued Signal Processing: Integrating Phase Interference into Self-Attention",
        "link": "/arxiv/2509.19331",
        "arxiv_id": "2509.19331",
        "authors": "Enhao Huang, Zhiyu Zhang, Tianxiang Xu, Chunshu Xia, Kaichun Hu, Yuchen Yang, Tongtong Pan, Dong Dong, Zhan Qin",
        "summary": "Complex-valued signals encode both amplitude and phase, yet most deep models treat attention as real-valued correlation, overlooking interference effects. We introduce the Holographic Transformer, a physics-inspired architecture that incorporates wave interference principles into self-attention. Holographic attention modulates interactions by relative phase and coherently superimposes values, ensuring consistency between amplitude and phase. A dual-headed decoder simultaneously reconstructs the input and predicts task outputs, preventing phase collapse when losses prioritize magnitude over phase. We demonstrate that holographic attention implements a discrete interference operator and maintains phase consistency under linear mixing. Experiments on PolSAR image classification and wireless channel prediction show strong performance, achieving high classification accuracy and F1 scores, low regression error, and increased robustness to phase perturbations. These results highlight that enforcing physical consistency in attention leads to generalizable improvements in complex-valued learning and provides a unified, physics-based framework for coherent signal modeling. The code is available at https://github.com/EonHao/Holographic-Transformers.",
        "subjects": "Signal Processing, Artificial Intelligence, Machine Learning",
        "date": "2025-09-14",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.701488"
    },
    {
        "index": "#178",
        "title": "LibEMER: A novel benchmark and algorithms library for EEG-based Multimodal Emotion Recognition",
        "link": "/arxiv/2509.19330",
        "arxiv_id": "2509.19330",
        "authors": "Zejun Liu, Yunshan Chen, Chengxi Xie, Huan Liu",
        "summary": "EEG-based multimodal emotion recognition(EMER) has gained significant attention and witnessed notable advancements, the inherent complexity of human neural systems has motivated substantial efforts toward multimodal approaches. However, this field currently suffers from three critical limitations: (i) the absence of open-source implementations. (ii) the lack of standardized and transparent benchmarks for fair performance analysis. (iii) in-depth discussion regarding main challenges and promising research directions is a notable scarcity. To address these challenges, we introduce LibEMER, a unified evaluation framework that provides fully reproducible PyTorch implementations of curated deep learning methods alongside standardized protocols for data preprocessing, model realization, and experimental setups. This framework enables unbiased performance assessment on three widely-used public datasets across two learning tasks. The open-source library is publicly accessible at: https://anonymous.4open.science/r/2025ULUIUBUEUMUEUR485384",
        "subjects": "Signal Processing, Artificial Intelligence, Human-Computer Interaction, Machine Learning, Multimedia",
        "date": "2025-09-14",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.701788"
    },
    {
        "index": "#179",
        "title": "Human Activity Recognition Based on Electrocardiogram Data Only",
        "link": "/arxiv/2509.19328",
        "arxiv_id": "2509.19328",
        "authors": "Sina Montazeri, Waltenegus Dargie, Yunhe Feng, Kewei Sha",
        "summary": "Human activity recognition is critical for applications such as early intervention and health analytics. Traditional activity recognition relies on inertial measurement units (IMUs), which are resource intensive and require calibration. Although electrocardiogram (ECG)-based methods have been explored, these have typically served as supplements to IMUs or have been limited to broad categorical classification such as fall detection or active vs. inactive in daily activities. In this paper, we advance the field by demonstrating, for the first time, robust recognition of activity only with ECG in six distinct activities, which is beyond the scope of previous work. We design and evaluate three new deep learning models, including a CNN classifier with Squeeze-and-Excitation blocks for channel-wise feature recalibration, a ResNet classifier with dilated convolutions for multiscale temporal dependency capture, and a novel CNNTransformer hybrid combining convolutional feature extraction with attention mechanisms for long-range temporal relationship modeling. Tested on data from 54 subjects for six activities, all three models achieve over 94% accuracy for seen subjects, while CNNTransformer hybrid reaching the best accuracy of 72% for unseen subjects, a result that can be further improved by increasing the training population. This study demonstrates the first successful ECG-only activity classification in multiple physical activities, offering significant potential for developing next-generation wearables capable of simultaneous cardiac monitoring and activity recognition without additional motion sensors.",
        "subjects": "Signal Processing, Artificial Intelligence, Machine Learning",
        "date": "2025-09-14",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.702135"
    },
    {
        "index": "#185",
        "title": "Advancing Few-Shot Pediatric Arrhythmia Classification with a Novel Contrastive Loss and Multimodal Learning",
        "link": "/arxiv/2509.19315",
        "arxiv_id": "2509.19315",
        "authors": "Yiqiao Chen, Zijian Huang, Zhenghui Feng",
        "summary": "Pediatric arrhythmias are a major risk factor for disability and sudden cardiac death, yet their automated classification remains challenging due to class imbalance, few-shot categories, and complex signal characteristics, which severely limit the efficiency and reliability of early screening and clinical intervention. To address this problem, we propose a multimodal end-to-end deep learning framework that combines dual-branch convolutional encoders for ECG and IEGM, semantic attention for cross-modal feature alignment, and a lightweight Transformer encoder for global dependency modeling. In addition, we introduce a new contrastive loss fucntion named Adaptive Global Class-Aware Contrastive Loss (AGCACL) to enhance intra-class compactness and inter-class separability through class prototypes and a global similarity matrix. To the best of our knowledge, this is the first systematic study based on the Leipzig Heart Center pediatric/congenital ECG+IEGM dataset, for which we also provide a complete and reproducible preprocessing pipeline. Experimental results demonstrate that the proposed method achieves the overall best performance on this dataset, including 97.76\\% Top-1 Accuracy, 94.08\\% Macro Precision, 91.97\\% Macro Recall, 92.97\\% Macro F1, and 92.36\\% Macro F2, with improvements of +13.64, +15.96, +19.82, and +19.44 percentage points over the strongest baseline in Macro Precision/Recall/F1/F2, respectively. These findings indicate that the framework significantly improves the detectability and robustness for minority arrhythmia classes, offering potential clinical value for rhythm screening, pre-procedural assessment, and postoperative follow-up in pediatric and congenital heart disease populations.",
        "subjects": "Signal Processing, Artificial Intelligence, Machine Learning",
        "date": "2025-09-10",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.704099"
    },
    {
        "index": "#187",
        "title": "E2E Learning Massive MIMO for Multimodal Semantic Non-Orthogonal Transmission and Fusion",
        "link": "/arxiv/2509.19312",
        "arxiv_id": "2509.19312",
        "authors": "Minghui Wu, Zhen Gao",
        "summary": "Massive multiple-input multiple-output (MIMO) promises high spectral efficiency but also leads to high-dimensional downlink channel state information (CSI), which complicates real-time channel acquisition and precoding. To address this, we propose an end-to-end (E2E) uplink-downlink CSI fusion precoding network that jointly models downlink CSI reference signal (CSI-RS) design, CSI feedback, and base-station (BS) precoding within a single E2E neural architecture. Concretely, a projection network built on the MAXIM architecture takes uplink sounding reference signals (SRS) as input and outputs frequency-, beam-, and port-domain projection matrices for designing downlink CSI-RS. User equipment (UE) then compresses/quantizes the resulting CSI-RS observations and feeds back a compact representation. At the base station (BS), two complementary branches produce candidate precoders: one is a feedback-only precoding network driven by quantized downlink observations, and the other is an SRS-only precoding network driven by uplink SRS. These candidate precoders are subsequently combined by a fusion precoding network to yield the final transmit precoder. All the modules are trained with a spectral-efficiency-oriented loss under a three-stage schedule. Simulation results show that the proposed approach effectively harnesses both SRS-derived information and UE feedback, achieving markedly better performance than conventional baselines.",
        "subjects": "Signal Processing, Artificial Intelligence, Information Theory, Machine Learning",
        "date": "2025-09-09",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.704579"
    },
    {
        "index": "#188",
        "title": "A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks",
        "link": "/arxiv/2509.19306",
        "arxiv_id": "2509.19306",
        "authors": "Jingyi Wang, Zhongyuan Zhao, Qingtian Wang, Zexu Li, Yue Wang, Tony Q. S. Quek",
        "summary": "Edge intelligence has emerged as a promising strategy to deliver low-latency and ubiquitous services for mobile devices. Recent advances in fine-tuning mechanisms of foundation models have enabled edge intelligence by integrating low-rank adaptation (LoRA) with federated learning. However, in wireless networks, the device heterogeneity and resource constraints on edge devices pose great threats to the performance of federated fine-tuning. To tackle these issues, we propose to optimize federated fine-tuning in heterogenous wireless networks via online learning. First, the framework of switching-based federated fine-tuning in wireless networks is provided. The edge devices switches to LoRA modules dynamically for federated fine-tuning with base station to jointly mitigate the impact of device heterogeneity and transmission unreliability. Second, a tractable upper bound on the inference risk gap is derived based on theoretical analysis. To improve the generalization capability, we formulate a non-convex mixed-integer programming problem with long-term constraints, and decouple it into model switching, transmit power control, and bandwidth allocation subproblems. An online optimization algorithm is developed to solve the problems with polynomial computational complexity. Finally, the simulation results on the SST-2 and QNLI data sets demonstrate the performance gains in test accuracy and energy efficiency.",
        "subjects": "Signal Processing, Artificial Intelligence, Information Theory, Networking and Internet Architecture",
        "date": "2025-09-05",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.710104"
    },
    {
        "index": "#189",
        "title": "Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning",
        "link": "/arxiv/2509.19305",
        "arxiv_id": "2509.19305",
        "authors": "Yifu Luo, Yongzhe Chang, Xueqian Wang",
        "summary": "Diffusion probability models have shown significant promise in offline reinforcement learning by directly modeling trajectory sequences. However, existing approaches primarily focus on time-domain features while overlooking frequency-domain features, leading to frequency shift and degraded performance according to our observation. In this paper, we investigate the RL problem from a new perspective of the frequency domain. We first observe that time-domain-only approaches inadvertently introduce shifts in the low-frequency components of the frequency domain, which results in trajectory instability and degraded performance. To address this issue, we propose Wavelet Fourier Diffuser (WFDiffuser), a novel diffusion-based RL framework that integrates Discrete Wavelet Transform to decompose trajectories into low- and high-frequency components. To further enhance diffusion modeling for each component, WFDiffuser employs Short-Time Fourier Transform and cross attention mechanisms to extract frequency-domain features and facilitate cross-frequency interaction. Extensive experiment results on the D4RL benchmark demonstrate that WFDiffuser effectively mitigates frequency shift, leading to smoother, more stable trajectories and improved decision-making performance over existing methods.",
        "subjects": "Machine Learning, Artificial Intelligence, Signal Processing",
        "date": "2025-09-04",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.710424"
    },
    {
        "index": "#190",
        "title": "LLMs as verification oracles for Solidity",
        "link": "/arxiv/2509.19153",
        "arxiv_id": "2509.19153",
        "authors": "Massimo Bartoletti, Enrico Lipparini, Livio Pompianu",
        "summary": "Ensuring the correctness of smart contracts is critical, as even subtle flaws can lead to severe financial losses. While bug detection tools able to spot common vulnerability patterns can serve as a first line of defense, most real-world exploits and losses stem from errors in the contract business logic. Formal verification tools such as SolCMC and the Certora Prover address this challenge, but their impact remains limited by steep learning curves and restricted specification languages. Recent works have begun to explore the use of large language models (LLMs) for security-related tasks such as vulnerability detection and test generation. Yet, a fundamental question remains open: can LLMs serve as verification oracles, capable of reasoning about arbitrary contract-specific properties? In this paper, we provide the first systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this role. We benchmark its performance on a large dataset of verification tasks, compare its outputs against those of established formal verification tools, and assess its practical effectiveness in real-world auditing scenarios. Our study combines quantitative metrics with qualitative analysis, and shows that recent reasoning-oriented LLMs can be surprisingly effective as verification oracles, suggesting a new frontier in the convergence of AI and formal methods for secure smart contract development and auditing.",
        "subjects": "Cryptography and Security, Software Engineering",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.710644"
    },
    {
        "index": "#191",
        "title": "GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models",
        "link": "/arxiv/2509.18122",
        "arxiv_id": "2509.18122",
        "authors": "Yue Zhang, Jiaxin Zhang, Qiuyu Ren, Tahsin Saffat, Xiaoxuan Liu, Zitong Yang, Banghua Zhu, Yi Ma",
        "summary": "We introduce \\textbf{GAUSS} (\\textbf{G}eneral \\textbf{A}ssessment of \\textbf{U}nderlying \\textbf{S}tructured \\textbf{S}kills in Mathematics), a benchmark that evaluates LLMs' mathematical abilities across twelve core skill dimensions, grouped into three domains: knowledge and understanding, problem solving and communication, and meta-skills and creativity. By categorizing problems according to cognitive skills and designing tasks that isolate specific abilities, GAUSS constructs comprehensive, fine-grained, and interpretable profiles of models' mathematical abilities. These profiles faithfully represent their underlying mathematical intelligence. To exemplify how to use the \\textsc{GAUSS} benchmark, we have derived the skill profile of \\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its differences relative to \\textsc{o4-mini-high}, thereby underscoring the value of multidimensional, skill-based evaluation.",
        "subjects": "Computation and Language",
        "date": "2025-09-10",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T09:20:55.710880"
    }
]