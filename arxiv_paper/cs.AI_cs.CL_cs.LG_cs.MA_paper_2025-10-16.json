[
    {
        "index": "#1",
        "title": "The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems",
        "link": "/arxiv/2510.14401",
        "arxiv_id": "2510.14401",
        "authors": "Prateek Gupta, Qiankun Zhong, Hiromu Yakura, Thomas Eisenmann, Iyad Rahwan",
        "summary": "A growing body of multi-agent studies with Large Language Models (LLMs) explores how norms and cooperation emerge in mixed-motive scenarios, where pursuing individual gain can undermine the collective good. While prior work has explored these dynamics in both richly contextualized simulations and simplified game-theoretic environments, most LLM systems featuring common-pool resource (CPR) games provide agents with explicit reward functions directly tied to their actions. In contrast, human cooperation often emerges without full visibility into payoffs and population, relying instead on heuristics, communication, and punishment. We introduce a CPR simulation framework that removes explicit reward signals and embeds cultural-evolutionary mechanisms: social learning (adopting strategies and beliefs from successful peers) and norm-based punishment, grounded in Ostrom's principles of resource governance. Agents also individually learn from the consequences of harvesting, monitoring, and punishing via environmental feedback, enabling norms to emerge endogenously. We establish the validity of our simulation by reproducing key findings from existing studies on human behavior. Building on this, we examine norm evolution across a $2\\times2$ grid of environmental and social initialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and benchmark how agentic societies comprised of different LLMs perform under these conditions. Our results reveal systematic model differences in sustaining cooperation and norm formation, positioning the framework as a rigorous testbed for studying emergent norms in mixed-motive LLM societies. Such analysis can inform the design of AI systems deployed in social and organizational contexts, where alignment with cooperative norms is critical for stability, fairness, and effective governance of AI-mediated environments.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.519792"
    },
    {
        "index": "#2",
        "title": "Stop Reducing Responsibility in LLM-Powered Multi-Agent Systems to Local Alignment",
        "link": "/arxiv/2510.14008",
        "arxiv_id": "2510.14008",
        "authors": "Jinwei Hu, Yi Dong, Shuang Ao, Zhuoyun Li, Boxuan Wang, Lokesh Singh, Guangliang Cheng, Sarvapali D. Ramchurn, Xiaowei Huang",
        "summary": "LLM-powered Multi-Agent Systems (LLM-MAS) unlock new potentials in distributed reasoning, collaboration, and task generalization but also introduce additional risks due to unguaranteed agreement, cascading uncertainty, and adversarial vulnerabilities. We argue that ensuring responsible behavior in such systems requires a paradigm shift: from local, superficial agent-level alignment to global, systemic agreement. We conceptualize responsibility not as a static constraint but as a lifecycle-wide property encompassing agreement, uncertainty, and security, each requiring the complementary integration of subjective human-centered values and objective verifiability. Furthermore, a dual-perspective governance framework that combines interdisciplinary design with human-AI collaborative oversight is essential for tracing and ensuring responsibility throughout the lifecycle of LLM-MAS. Our position views LLM-MAS not as loose collections of agents, but as unified, dynamic socio-technical systems that demand principled mechanisms to support each dimension of responsibility and enable ethically aligned, verifiably coherent, and resilient behavior for sustained, system-wide agreement.",
        "subjects": "Multiagent Systems",
        "date": "2025-10-15",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.520173"
    },
    {
        "index": "#3",
        "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
        "link": "/arxiv/2510.13982",
        "arxiv_id": "2510.13982",
        "authors": "Jinkun Chen, Sher Badshah, Xuemin Yu, Sijia Han, Jiechao Gao",
        "summary": "What if artificial agents could not just communicate, but also evolve, adapt, and reshape their worlds in ways we cannot fully predict? With llm now powering multi-agent systems and social simulations, we are witnessing new possibilities for modeling open-ended, ever-changing environments. Yet, most current simulations remain constrained within static sandboxes, characterized by predefined tasks, limited dynamics, and rigid evaluation criteria. These limitations prevent them from capturing the complexity of real-world societies. In this paper, we argue that static, task-specific benchmarks are fundamentally inadequate and must be rethought. We critically review emerging architectures that blend llm with multi-agent dynamics, highlight key hurdles such as balancing stability and diversity, evaluating unexpected behaviors, and scaling to greater complexity, and introduce a fresh taxonomy for this rapidly evolving field. Finally, we present a research roadmap centered on open-endedness, continuous co-evolution, and the development of resilient, socially aligned AI ecosystems. \\textbf{We call on the community to move beyond static paradigms and help shape the next generation of adaptive, socially-aware multi-agent simulations.}",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.520456"
    },
    {
        "index": "#4",
        "title": "Benefits and Limitations of Communication in Multi-Agent Reasoning",
        "link": "/arxiv/2510.13903",
        "arxiv_id": "2510.13903",
        "authors": "Michael Rizvi-Martel, Satwik Bhattamishra, Neil Rathi, Guillaume Rabusseau, Michael Hahn",
        "summary": "Chain-of-thought prompting has popularized step-by-step reasoning in large language models, yet model performance still degrades as problem complexity and context length grow. By decomposing difficult tasks with long contexts into shorter, manageable ones, recent multi-agent paradigms offer a promising near-term solution to this problem. However, the fundamental capacities of such systems are poorly understood. In this work, we propose a theoretical framework to analyze the expressivity of multi-agent systems. We apply our framework to three algorithmic families: state tracking, recall, and $k$-hop reasoning. We derive bounds on (i) the number of agents required to solve the task exactly, (ii) the quantity and structure of inter-agent communication, and (iii) the achievable speedups as problem size and context scale. Our results identify regimes where communication is provably beneficial, delineate tradeoffs between agent count and bandwidth, and expose intrinsic limitations when either resource is constrained. We complement our theoretical analysis with a set of experiments on pretrained LLMs using controlled synthetic benchmarks. Empirical outcomes confirm the tradeoffs between key quantities predicted by our theory. Collectively, our analysis offers principled guidance for designing scalable multi-agent reasoning systems.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.520733"
    },
    {
        "index": "#5",
        "title": "SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time",
        "link": "/arxiv/2510.14851",
        "arxiv_id": "2510.14851",
        "authors": "Jakob Bichler, Andreu Matoses Gimenez, Javier Alonso-Mora",
        "summary": "We present Sadcher, a real-time task assignment framework for heterogeneous multi-robot teams that incorporates dynamic coalition formation and task precedence constraints. Sadcher is trained through Imitation Learning and combines graph attention and transformers to predict assignment rewards between robots and tasks. Based on the predicted rewards, a relaxed bipartite matching step generates high-quality schedules with feasibility guarantees. We explicitly model robot and task positions, task durations, and robots' remaining processing times, enabling advanced temporal and spatial reasoning and generalization to environments with different spatiotemporal distributions compared to training. Trained on optimally solved small-scale instances, our method can scale to larger task sets and team sizes. Sadcher outperforms other learning-based and heuristic baselines on randomized, unseen problems for small and medium-sized teams with computation times suitable for real-time operation. We also explore sampling-based variants and evaluate scalability across robot and task counts. In addition, we release our dataset of 250,000 optimal schedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/",
        "subjects": "Robotics, Multiagent Systems",
        "date": "2025-10-16",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.520988"
    },
    {
        "index": "#6",
        "title": "Multi Agent Switching Mode Controller for Sound Source localization",
        "link": "/arxiv/2510.14849",
        "arxiv_id": "2510.14849",
        "authors": "Marcello Sorge, Nicola Cigarini, Riccardo Lorigiola, Giulia Michieletto, Andrea Masiero, Angelo Cenedese, Alberto Guarnieri",
        "summary": "Source seeking is an important topic in robotic research, especially considering sound-based sensors since they allow the agents to locate a target even in critical conditions where it is not possible to establish a direct line of sight. In this work, we design a multi- agent switching mode control strategy for acoustic-based target localization. Two scenarios are considered: single source localization, in which the agents are driven maintaining a rigid formation towards the target, and multi-source scenario, in which each agent searches for the targets independently from the others.",
        "subjects": "Robotics, Multiagent Systems",
        "date": "2025-10-16",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.521273"
    },
    {
        "index": "#7",
        "title": "When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks",
        "link": "/arxiv/2510.14677",
        "arxiv_id": "2510.14677",
        "authors": "Steffen Hagedorn, Luka Donkov, Aron Distelzweig, Alexandru P. Condurache",
        "summary": "Planner evaluation in closed-loop simulation often uses rule-based traffic agents, whose simplistic and passive behavior can hide planner deficiencies and bias rankings. Widely used IDM agents simply follow a lead vehicle and cannot react to vehicles in adjacent lanes, hindering tests of complex interaction capabilities. We address this issue by integrating the state-of-the-art learned traffic agent model SMART into nuPlan. Thus, we are the first to evaluate planners under more realistic conditions and quantify how conclusions shift when narrowing the sim-to-real gap. Our analysis covers 14 recent planners and established baselines and shows that IDM-based simulation overestimates planning performance: nearly all scores deteriorate. In contrast, many planners interact better than previously assumed and even improve in multi-lane, interaction-heavy scenarios like lane changes or turns. Methods trained in closed-loop demonstrate the best and most stable driving performance. However, when reaching their limits in augmented edge-case scenarios, all learned planners degrade abruptly, whereas rule-based planners maintain reasonable basic behavior. Based on our results, we suggest SMART-reactive simulation as a new standard closed-loop benchmark in nuPlan and release the SMART agents as a drop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning, Multiagent Systems",
        "date": "2025-10-16",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.521553"
    },
    {
        "index": "#8",
        "title": "Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems",
        "link": "/arxiv/2510.14133",
        "arxiv_id": "2510.14133",
        "authors": "Edoardo Allegrini, Ananth Shreekumar, Z. Berkay Celik",
        "summary": "Agentic AI systems, which leverage multiple autonomous agents and Large Language Models (LLMs), are increasingly used to address complex, multi-step tasks. The safety, security, and functionality of these systems are critical, especially in high-stakes applications. However, the current ecosystem of inter-agent communication is fragmented, with protocols such as the Model Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol for coordination being analyzed in isolation. This fragmentation creates a semantic gap that prevents the rigorous analysis of system properties and introduces risks such as architectural misalignment and exploitable coordination issues. To address these challenges, we introduce a modeling framework for agentic AI systems composed of two foundational models. The first, the host agent model, formalizes the top-level entity that interacts with the user, decomposes tasks, and orchestrates their execution by leveraging external agents and tools. The second, the task lifecycle model, details the states and transitions of individual sub-tasks from creation to completion, providing a fine-grained view of task management and error handling. Together, these models provide a unified semantic framework for reasoning about the behavior of multi-AI agent systems. Grounded in this framework, we define 17 properties for the host agent and 14 for the task lifecycle, categorized into liveness, safety, completeness, and fairness. Expressed in temporal logic, these properties enable formal verification of system behavior, detection of coordination edge cases, and prevention of deadlocks and security vulnerabilities. Through this effort, we introduce the first rigorously grounded, domain-agnostic framework for the systematic analysis, design, and deployment of correct, reliable, and robust agentic AI systems.",
        "subjects": "Artificial Intelligence, Cryptography and Security, Multiagent Systems",
        "date": "2025-10-15",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.521813"
    },
    {
        "index": "#9",
        "title": "GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation via Large Language Model Agents",
        "link": "/arxiv/2510.13896",
        "arxiv_id": "2510.13896",
        "authors": "Xi Yu, Yang Yang, Qun Liu, Yonghua Du, Sean McSweeney, Yuewei Lin",
        "summary": "Cellular image segmentation is essential for quantitative biology yet remains difficult due to heterogeneous modalities, morphological variability, and limited annotations. We present GenCellAgent, a training-free multi-agent framework that orchestrates specialist segmenters and generalist vision-language models via a planner-executor-evaluator loop (choose tool $\\rightarrow$ run $\\rightarrow$ quality-check) with long-term memory. The system (i) automatically routes images to the best tool, (ii) adapts on the fly using a few reference images when imaging conditions differ from what a tool expects, (iii) supports text-guided segmentation of organelles not covered by existing models, and (iv) commits expert edits to memory, enabling self-evolution and personalized workflows. Across four cell-segmentation benchmarks, this routing yields a 15.7\\% mean accuracy gain over state-of-the-art baselines. On endoplasmic reticulum and mitochondria from new datasets, GenCellAgent improves average IoU by 37.6\\% over specialist models. It also segments novel objects such as the Golgi apparatus via iterative text-guided refinement, with light human correction further boosting performance. Together, these capabilities provide a practical path to robust, adaptable cellular image segmentation without retraining, while reducing annotation burden and matching user preferences.",
        "subjects": "Quantitative Methods, Artificial Intelligence, Computer Vision and Pattern Recognition, Multiagent Systems",
        "date": "2025-10-14",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.522103"
    },
    {
        "index": "#10",
        "title": "Large Language Model Agents Enable Autonomous Design and Image Analysis of Microwell Microfluidics",
        "link": "/arxiv/2510.13883",
        "arxiv_id": "2510.13883",
        "authors": "Dinh-Nguyen Nguyen, Sadia Shakil, Raymond Kai-Yu Tong, Ngoc-Duy Dinh",
        "summary": "Microwell microfluidics has been utilized for single-cell analysis to reveal heterogeneity in gene expression, signaling pathways, and phenotypic responses for identifying rare cell types, understanding disease progression, and developing more precise therapeutic strategies. However, designing microwell microfluidics is a considerably complex task, requiring knowledge, experience, and CAD software, as well as manual intervention, which often fails initial designs, demanding multiple costly and time-consuming iterations. In this study, we establish an autonomous large language model (LLM)-driven microwell design framework to generate code-based computer-aided design (CAD) scripts, that enables the rapid and reproducible creation of microwells with diverse geometries and imaging-based analysis. We propose a multimodal large language model (MLLM)-logistic regression framework based on integrating high-level semantic descriptions generated by MLLMs with image embeddings for image classification tasks, aiming to identify microwell occupancy and microwell shape. The fused multimodal representation is input to a logistic regression model, which is both interpretable and computationally efficient. We achieved significant improvements, exceeding 0.92 for occupancy classification and 0.99 for shape classification, across all evaluated MLLMs, compared with 0.50 and 0.55, respectively, when relying solely on direct classification. The MLLM-logistic regression framework is a scalable, efficient solution for high-throughput microwell image analysis. Our study demonstrates an autonomous design microwell platform by translating natural language prompts into optimized device geometries, CAD scripts and image analysis, facilitating the development of next-generation digital discovery by integration of literature mining, autonomous design and experimental data analysis.",
        "subjects": "Neurons and Cognition, Multiagent Systems",
        "date": "2025-10-14",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.522359"
    },
    {
        "index": "#11",
        "title": "Joint Active RIS Configuration and User Power Control for Localization: A Neuroevolution-Based Approach",
        "link": "/arxiv/2510.13819",
        "arxiv_id": "2510.13819",
        "authors": "George Stamatelis, Hui Chen, Henk Wymeersch, George C. Alexandropoulos",
        "summary": "This paper studies user localization aided by a Reconfigurable Intelligent Surface (RIS). A feedback link from the Base Station (BS) to the user is adopted to enable dynamic power control of the user pilot transmissions in the uplink. A novel multi-agent algorithm for the joint control of the RIS phase configuration and the user transmit power is presented, which is based on a hybrid approach integrating NeuroEvolution (NE) and supervised learning. The proposed scheme requires only single-bit feedback messages for the uplink power control, supports RIS elements with discrete responses, and is numerically shown to outperform fingerprinting, deep reinforcement learning baselines and backpropagation-based position estimators.",
        "subjects": "Networking and Internet Architecture, Machine Learning, Multiagent Systems",
        "date": "2025-09-25",
        "category": "cs.MA",
        "crawl_time": "2025-10-17T11:00:03.522626"
    },
    {
        "index": "#1",
        "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
        "link": "/arxiv/2510.14974",
        "arxiv_id": "2510.14974",
        "authors": "Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi",
        "summary": "Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.955069"
    },
    {
        "index": "#2",
        "title": "Biology-informed neural networks learn nonlinear representations from omics data to improve genomic prediction and interpretability",
        "link": "/arxiv/2510.14970",
        "arxiv_id": "2510.14970",
        "authors": "Katiana Kontolati, Rini Jasmine Gladstone, Ian Davis, Ethan Pickering",
        "summary": "We extend biologically-informed neural networks (BINNs) for genomic prediction (GP) and selection (GS) in crops by integrating thousands of single-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior biological knowledge. Traditional genotype-to-phenotype (G2P) models depend heavily on direct mappings that achieve only modest accuracy, forcing breeders to conduct large, costly field trials to maintain or marginally improve genetic gain. Models that incorporate intermediate molecular phenotypes such as gene expression can achieve higher predictive fit, but they remain impractical for GS since such data are unavailable at deployment or design time. BINNs overcome this limitation by encoding pathway-level inductive biases and leveraging multi-omics data only during training, while using genotype data alone during inference. Applied to maize gene-expression and multi-environment field-trial data, BINN improves rank-correlation accuracy by up to 56% within and across subpopulations under sparse-data conditions and nonlinearly identifies genes that GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic metabolomics benchmark, BINN reduces prediction error by 75% relative to conventional neural nets and correctly identifies the most important nonlinear pathway. Importantly, both cases show highly sensitive BINN latent variables correlate with the experimental quantities they represent, despite not being trained on them. This suggests BINNs learn biologically-relevant representations, nonlinear or linear, from genotype to phenotype. Together, BINNs establish a framework that leverages intermediate domain information to improve genomic prediction accuracy and reveal nonlinear biological relationships that can guide genomic selection, candidate gene selection, pathway enrichment, and gene-editing prioritization.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.955369"
    },
    {
        "index": "#3",
        "title": "Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity in TVD-MI Scores",
        "link": "/arxiv/2510.14966",
        "arxiv_id": "2510.14966",
        "authors": "Zachary Robertson",
        "summary": "Pairwise comparisons of large language models using total variation distance mutual information (TVD-MI) produce binary critic decisions per pair. We show that averaging TVD-MI's binary trials yields centered-probability scores with additive structure suitable for item-response theory (IRT) without nonlinear link functions. Maximum-likelihood approaches to IRT use logistic links, but we find empirically that these transformations introduce curvature that breaks additivity: across three domains, the identity link yields median curl on raw data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We derive this clipped-linear model from Gini entropy maximization, yielding a box-constrained least-squares formulation that handles boundary saturation. At 33% coverage, we achieve holdout RMSE $0.117 \\pm 0.008$ while preserving agent rankings (Spearman $\\rho = 0.972 \\pm 0.015$), three times fewer evaluations than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows strong agreement in agent rankings ($\\rho = 0.872$) and consistent identity-link advantage. TVD-MI's geometry is best preserved by identity mapping for efficient LLM evaluation, applicable to other bounded-response domains.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.955647"
    },
    {
        "index": "#4",
        "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models",
        "link": "/arxiv/2510.14961",
        "arxiv_id": "2510.14961",
        "authors": "Jonas Geiping, Xinyu Yang, Guinan Su",
        "summary": "Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers. Recent efforts in pretraining have demonstrated that these architectures can scale to modern language modeling tasks while exhibiting advantages in reasoning tasks. In this work, we examine the relationship between recurrent-depth models and diffusion language models. Building on their similarities, we develop a new diffusion forcing sampler for these models to accelerate generation. The sampler advances by decoding new tokens at every forward pass of the model, while the latent states of these tokens can be further refined in parallel through recurrence. Theoretically, generation with our sampler is strictly more expressive than the baseline autoregressive generation using the same time budget on modern hardware. Moreover, this sampler, based on principles from diffusion literature, can be directly applied to existing 3.5B recurrent-depth transformers without any tuning, leading to up to a 5x speedup. Consequently, our findings not only provide an efficient mechanism for parallelizing the extra computation in recurrent-depth models at inference, but also suggest that such models can be naturally viewed as strong continuous, though causal, diffusion language models.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.955940"
    },
    {
        "index": "#5",
        "title": "Circuit Insights: Towards Interpretability Beyond Activations",
        "link": "/arxiv/2510.14936",
        "arxiv_id": "2510.14936",
        "authors": "Elena Golimblevskaia, Aakriti Jain, Bruno Puri, Ammar Ibrahim, Wojciech Samek, Sebastian Lapuschkin",
        "summary": "The fields of explainable AI and mechanistic interpretability aim to uncover the internal structure of neural networks, with circuit discovery as a central tool for understanding model computations. Existing approaches, however, rely on manual inspection and remain limited to toy tasks. Automated interpretability offers scalability by analyzing isolated features and their activations, but it often misses interactions between features and depends strongly on external LLMs and dataset quality. Transcoders have recently made it possible to separate feature attributions into input-dependent and input-invariant components, providing a foundation for more systematic circuit analysis. Building on this, we propose WeightLens and CircuitLens, two complementary methods that go beyond activation-based analysis. WeightLens interprets features directly from their learned weights, removing the need for explainer models or datasets while matching or exceeding the performance of existing methods on context-independent features. CircuitLens captures how feature activations arise from interactions between components, revealing circuit-level dynamics that activation-only approaches cannot identify. Together, these methods increase interpretability robustness and enhance scalable mechanistic analysis of circuits while maintaining efficiency and quality.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.956278"
    },
    {
        "index": "#6",
        "title": "Reasoning with Sampling: Your Base Model is Smarter Than You Think",
        "link": "/arxiv/2510.14901",
        "arxiv_id": "2510.14901",
        "authors": "Aayush Karan, Yilun Du",
        "summary": "Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.956564"
    },
    {
        "index": "#7",
        "title": "Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded Rewards",
        "link": "/arxiv/2510.14884",
        "arxiv_id": "2510.14884",
        "authors": "Sarah Liaw, Benjamin Plaut",
        "summary": "In high-stakes AI applications, even a single action can cause irreparable damage. However, nearly all of sequential decision-making theory assumes that all errors are recoverable (e.g., by bounding rewards). Standard bandit algorithms that explore aggressively may cause irreparable damage when this assumption fails. Some prior work avoids irreparable errors by asking for help from a mentor, but a mentor may not always be available. In this work, we formalize a model of learning with unbounded rewards without a mentor as a two-action contextual bandit with an abstain option: at each round the agent observes an input and chooses either to abstain (always 0 reward) or to commit (execute a preexisting task policy). Committing yields rewards that are upper-bounded but can be arbitrarily negative, and the commit reward is assumed Lipschitz in the input. We propose a caution-based algorithm that learns when not to learn: it chooses a trusted region and commits only where the available evidence does not already certify harm. Under these conditions and i.i.d. inputs, we establish sublinear regret guarantees, theoretically demonstrating the effectiveness of cautious exploration for deploying learning agents safely in high-stakes environments.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.956834"
    },
    {
        "index": "#8",
        "title": "Predicting kernel regression learning curves from only raw data statistics",
        "link": "/arxiv/2510.14878",
        "arxiv_id": "2510.14878",
        "authors": "Dhruva Karkada, Joseph Turnbull, Yuxi Liu, James B. Simon",
        "summary": "We study kernel regression with common rotation-invariant kernels on real datasets including CIFAR-5m, SVHN, and ImageNet. We give a theoretical framework that predicts learning curves (test risk vs. sample size) from only two measurements: the empirical data covariance matrix and an empirical polynomial decomposition of the target function $f_*$. The key new idea is an analytical approximation of a kernel's eigenvalues and eigenfunctions with respect to an anisotropic data distribution. The eigenfunctions resemble Hermite polynomials of the data, so we call this approximation the Hermite eigenstructure ansatz (HEA). We prove the HEA for Gaussian data, but we find that real image data is often \"Gaussian enough\" for the HEA to hold well in practice, enabling us to predict learning curves by applying prior results relating kernel eigenstructure to test risk. Extending beyond kernel regression, we empirically find that MLPs in the feature-learning regime learn Hermite polynomials in the order predicted by the HEA. Our HEA framework is a proof of concept that an end-to-end theory of learning which maps dataset structure all the way to model performance is possible for nontrivial learning algorithms on real datasets.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.957128"
    },
    {
        "index": "#9",
        "title": "Backdoor Unlearning by Linear Task Decomposition",
        "link": "/arxiv/2510.14845",
        "arxiv_id": "2510.14845",
        "authors": "Amel Abdelraheem, Alessandro Favero, Gerome Bovet, Pascal Frossard",
        "summary": "Foundation models have revolutionized computer vision by enabling broad generalization across diverse tasks. Yet, they remain highly susceptible to adversarial perturbations and targeted backdoor attacks. Mitigating such vulnerabilities remains an open challenge, especially given that the large-scale nature of the models prohibits retraining to ensure safety. Existing backdoor removal approaches rely on costly fine-tuning to override the harmful behavior, and can often degrade performance on other unrelated tasks. This raises the question of whether backdoors can be removed without compromising the general capabilities of the models. In this work, we address this question and study how backdoors are encoded in the model weight space, finding that they are disentangled from other benign tasks. Specifically, this separation enables the isolation and erasure of the backdoor's influence on the model with minimal impact on clean performance. Building on this insight, we introduce a simple unlearning method that leverages such disentanglement. Through extensive experiments with CLIP-based models and common adversarial triggers, we show that, given the knowledge of the attack, our method achieves approximately perfect unlearning, while retaining, on average, 96% of clean accuracy. Additionally, we demonstrate that even when the attack and its presence are unknown, our method successfully unlearns backdoors by proper estimation using reverse-engineered triggers. Overall, our method consistently yields better unlearning and clean accuracy tradeoffs when compared to present state-of-the-art defenses.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.957425"
    },
    {
        "index": "#10",
        "title": "Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks",
        "link": "/arxiv/2510.14844",
        "arxiv_id": "2510.14844",
        "authors": "Odelia Melamed, Gilad Yehudai, Gal Vardi",
        "summary": "Machine Unlearning aims to remove specific data from trained models, addressing growing privacy and ethical concerns. We provide a theoretical analysis of a simple and widely used method - gradient ascent - used to reverse the influence of a specific data point without retraining from scratch. Leveraging the implicit bias of gradient descent towards solutions that satisfy the Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem, we quantify the quality of the unlearned model by evaluating how well it satisfies these conditions w.r.t. the retained data. To formalize this idea, we propose a new success criterion, termed \\textbf{$(\\epsilon, \\delta, \\tau)$-successful} unlearning, and show that, for both linear models and two-layer neural networks with high dimensional data, a properly scaled gradient-ascent step satisfies this criterion and yields a model that closely approximates the retrained solution on the retained data. We also show that gradient ascent performs successful unlearning while still preserving generalization in a synthetic Gaussian-mixture setting.",
        "subjects": "Machine Learning, Cryptography and Security, Neural and Evolutionary Computing, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.957731"
    },
    {
        "index": "#11",
        "title": "Reinforcement Learning with Stochastic Reward Machines",
        "link": "/arxiv/2510.14837",
        "arxiv_id": "2510.14837",
        "authors": "Jan Corazza, Ivan Gavran, Daniel Neider",
        "summary": "Reward machines are an established tool for dealing with reinforcement learning problems in which rewards are sparse and depend on complex sequences of actions. However, existing algorithms for learning reward machines assume an overly idealized setting where rewards have to be free of noise. To overcome this practical limitation, we introduce a novel type of reward machines, called stochastic reward machines, and an algorithm for learning them. Our algorithm, based on constraint solving, learns minimal stochastic reward machines from the explorations of a reinforcement learning agent. This algorithm can easily be paired with existing reinforcement learning algorithms for reward machines and guarantees to converge to an optimal policy in the limit. We demonstrate the effectiveness of our algorithm in two case studies and show that it outperforms both existing methods and a naive approach for handling noisy reward functions.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.957996"
    },
    {
        "index": "#12",
        "title": "Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction in 6G Multi-RAT Networks",
        "link": "/arxiv/2510.14832",
        "arxiv_id": "2510.14832",
        "authors": "Maria Lamprini A. Bartsioka, Anastasios Giannopoulos, Sotirios Spantideas",
        "summary": "The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT) networks, where cellular and Wireless Fidelity (WiFi) transmitters coexist, requires mobility decisions that remain reliable under fast channel dynamics, interference, and heterogeneous coverage. Handover in multi-RAT deployments is still highly reactive and event-triggered, relying on instantaneous measurements and threshold events. This work proposes a Machine Learning (ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a model-driven and short-horizon signal quality forecasts. We present a generalized P-CHO sequence workflow orchestrated by a RAT Steering Controller, which standardizes data collection, parallel per-RAT predictions, decision logic with hysteresis-based conditions, and CHO execution. Considering a realistic multi-RAT environment, we train RAT-aware Long Short Term Memory (LSTM) networks to forecast the signal quality indicators of mobile users along randomized trajectories. The proposed P-CHO models are trained and evaluated under different channel models for cellular and IEEE 802.11 WiFi integrated coverage. We study the impact of hyperparameter tuning of LSTM models under different system settings, and compare direct multi-step versus recursive P-CHO variants. Comparisons against baseline predictors are also carried out. Finally, the proposed P-CHO is tested under soft and hard handover settings, showing that hysteresis-enabled P-CHO scheme is able to reduce handover failures and ping-pong events. Overall, the proposed P-CHO framework can enable accurate, low-latency, and proactive handovers suitable for ML-assisted handover steering in 6G multi-RAT deployments.",
        "subjects": "Machine Learning, Networking and Internet Architecture",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.958278"
    },
    {
        "index": "#13",
        "title": "To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models",
        "link": "/arxiv/2510.14826",
        "arxiv_id": "2510.14826",
        "authors": "Eran Malach, Omid Saremi, Sinead Williamson, Arwen Bradley, Aryo Lotfi, Emmanuel Abbe, Josh Susskind, Etai Littwin",
        "summary": "State Space Models (SSMs) have become the leading alternative to Transformers for sequence modeling. Their primary advantage is efficiency in long-context and long-form generation, enabled by fixed-size memory and linear scaling of computational complexity. We begin this work by showing a simple theoretical result stating that SSMs cannot accurately solve any ``truly long-form'' generation problem (in a sense we formally define), undermining their main competitive advantage. However, we show that this limitation can be mitigated by allowing SSMs interactive access to external tools. In fact, we show that given the right choice of tool access and problem-dependent training data, SSMs can learn to solve any tractable problem and generalize to arbitrary problem length/complexity (i.e., achieve length generalization). Following our theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable length generalization on a variety of arithmetic, reasoning, and coding tasks. These findings highlight SSMs as a potential efficient alternative to Transformers in interactive tool-based and agentic settings.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.958591"
    },
    {
        "index": "#14",
        "title": "Programmatic Representation Learning with Language Models",
        "link": "/arxiv/2510.14825",
        "arxiv_id": "2510.14825",
        "authors": "Gabriel Poesia, Georgia Gabriela Sampaio",
        "summary": "Classical models for supervised machine learning, such as decision trees, are efficient and interpretable predictors, but their quality is highly dependent on the particular choice of input features. Although neural networks can learn useful representations directly from raw data (e.g., images or text), this comes at the expense of interpretability and the need for specialized hardware to run them efficiently. In this paper, we explore a hypothesis class we call Learned Programmatic Representations (LeaPR) models, which stack arbitrary features represented as code (functions from data points to scalars) and decision tree predictors. We synthesize feature functions using Large Language Models (LLMs), which have rich prior knowledge in a wide range of domains and a remarkable ability to write code using existing domain-specific libraries. We propose two algorithms to learn LeaPR models from supervised data. First, we design an adaptation of FunSearch to learn features rather than directly generate predictors. Then, we develop a novel variant of the classical ID3 algorithm for decision tree learning, where new features are generated on demand when splitting leaf nodes. In experiments from chess position evaluation to image and text classification, our methods learn high-quality, neural network-free predictors often competitive with neural networks. Our work suggests a flexible paradigm for learning interpretable representations end-to-end where features and predictions can be readily inspected and understood.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.958850"
    },
    {
        "index": "#15",
        "title": "Tackling Time-Series Forecasting Generalization via Mitigating Concept Drift",
        "link": "/arxiv/2510.14814",
        "arxiv_id": "2510.14814",
        "authors": "Zhiyuan Zhao, Haoxin Liu, B. Aditya Prakash",
        "summary": "Time-series forecasting finds broad applications in real-world scenarios. Due to the dynamic nature of time series data, it is important for time-series forecasting models to handle potential distribution shifts over time. In this paper, we initially identify two types of distribution shifts in time series: concept drift and temporal shift. We acknowledge that while existing studies primarily focus on addressing temporal shift issues in time series forecasting, designing proper concept drift methods for time series forecasting has received comparatively less attention. Motivated by the need to address potential concept drift, while conventional concept drift methods via invariant learning face certain challenges in time-series forecasting, we propose a soft attention mechanism that finds invariant patterns from both lookback and horizon time series. Additionally, we emphasize the critical importance of mitigating temporal shifts as a preliminary to addressing concept drift. In this context, we introduce ShifTS, a method-agnostic framework designed to tackle temporal shift first and then concept drift within a unified approach. Extensive experiments demonstrate the efficacy of ShifTS in consistently enhancing the forecasting accuracy of agnostic models across multiple datasets, and outperforming existing concept drift, temporal shift, and combined baselines.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.959115"
    },
    {
        "index": "#16",
        "title": "Efficient Dynamic Structured Sparse Training with Learned Shuffles",
        "link": "/arxiv/2510.14812",
        "arxiv_id": "2510.14812",
        "authors": "Abhishek Tyagi, Arjun Iyer, Liam Young, William H Renninger, Christopher Kanan, Yuhao Zhu",
        "summary": "Structured sparsity accelerates training and inference on modern GPUs, yet it still trails unstructured dynamic sparse training (DST) in accuracy. The shortfall stems from a loss of expressivity: whereas a dense layer can realize every possible mask obtained by choosing any $w$ active weights out of $n$, a fixed block or N:M layout explores only a subset of those possibilities. We propose to close this gap by learning, for each layer, a single permutation matrix jointly with the structured weight matrix. Applied to three canonical structures -- block, N:M, and diagonals -- we show that permutation-augmented DST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\\% sparsity on ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\\times$ and infers up to $2.9\\times$ faster. The results position structure + learned permutation as a sweet spot between accuracy and efficiency.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.959421"
    },
    {
        "index": "#17",
        "title": "Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning",
        "link": "/arxiv/2510.14810",
        "arxiv_id": "2510.14810",
        "authors": "Shikuang Deng, Jiayuan Zhang, Yuhang Wu, Ting Chen, Shi Gu",
        "summary": "Hebbian learning is a biological principle that intuitively describes how neurons adapt their connections through repeated stimuli. However, when applied to machine learning, it suffers serious issues due to the unconstrained updates of the connections and the lack of accounting for feedback mediation. Such shortcomings limit its effective scaling to complex network architectures and tasks. To this end, here we introduce the Structural Projection Hebbian Representation (SPHeRe), a novel unsupervised learning method that integrates orthogonality and structural information preservation through a local auxiliary nonlinear block. The loss for structural information preservation backpropagates to the input through an auxiliary lightweight projection that conceptually serves as feedback mediation while the orthogonality constraints account for the boundedness of updating magnitude. Extensive experimental results show that SPHeRe achieves SOTA performance among unsupervised synaptic plasticity approaches on standard image classification benchmarks, including CIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strong effectiveness in continual learning and transfer learning scenarios, and image reconstruction tasks show the robustness and generalizability of the extracted features. This work demonstrates the competitiveness and potential of Hebbian unsupervised learning rules within modern deep learning frameworks, demonstrating the possibility of efficient and biologically inspired learning algorithms without the strong dependence on strict backpropagation. Our code is available at https://github.com/brain-intelligence-lab/SPHeRe.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.964874"
    },
    {
        "index": "#18",
        "title": "Active Jammer Localization via Acquisition-Aware Path Planning",
        "link": "/arxiv/2510.14790",
        "arxiv_id": "2510.14790",
        "authors": "Luis González-Gudiño, Mariona Jaramillo-Civill, Pau Closas, Tales Imbiriba",
        "summary": "We propose an active jammer localization framework that combines Bayesian optimization with acquisition-aware path planning. Unlike passive crowdsourced methods, our approach adaptively guides a mobile agent to collect high-utility Received Signal Strength measurements while accounting for urban obstacles and mobility constraints. For this, we modified the A* algorithm, A-UCB*, by incorporating acquisition values into trajectory costs, leading to high-acquisition planned paths. Simulations on realistic urban scenarios show that the proposed method achieves accurate localization with fewer measurements compared to uninformed baselines, demonstrating consistent performance under different environments.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.965171"
    },
    {
        "index": "#19",
        "title": "Causal Discovery for Linear DAGs with Dependent Latent Variables via Higher-order Cumulants",
        "link": "/arxiv/2510.14780",
        "arxiv_id": "2510.14780",
        "authors": "Ming Cai, Penggang Gao, Hisayuki Hara",
        "summary": "This paper addresses the problem of estimating causal directed acyclic graphs in linear non-Gaussian acyclic models with latent confounders (LvLiNGAM). Existing methods assume mutually independent latent confounders or cannot properly handle models with causal relationships among observed variables. We propose a novel algorithm that identifies causal DAGs in LvLiNGAM, allowing causal structures among latent variables, among observed variables, and between the two. The proposed method leverages higher-order cumulants of observed data to identify the causal structure. Extensive simulations and experiments with real-world data demonstrate the validity and practical utility of the proposed algorithm.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.965459"
    },
    {
        "index": "#20",
        "title": "Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries",
        "link": "/arxiv/2510.14751",
        "arxiv_id": "2510.14751",
        "authors": "Divyat Mahajan, Sachin Goyal, Badr Youbi Idrissi, Mohammad Pezeshki, Ioannis Mitliagkas, David Lopez-Paz, Kartik Ahuja",
        "summary": "Next-token prediction (NTP) has driven the success of large language models (LLMs), but it struggles with long-horizon reasoning, planning, and creative writing, with these limitations largely attributed to teacher-forced training. Multi-token prediction (MTP) partially mitigates these issues by predicting several future tokens at once, but it mostly captures short-range dependencies and offers limited improvement. We propose future summary prediction (FSP), which trains an auxiliary head to predict a compact representation of the long-term future, preserving information relevant for long-form generations. We explore two variants of FSP: handcrafted summaries, for example, a bag of words summary of the future of the sequence, and learned summaries, which use embeddings produced by a reverse language model trained from right to left. Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate that FSP provides improvements over both NTP and MTP across math, reasoning, and coding benchmarks.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.965779"
    },
    {
        "index": "#21",
        "title": "The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement Learning Agents",
        "link": "/arxiv/2510.14727",
        "arxiv_id": "2510.14727",
        "authors": "Antony Bartlett, Cynthia Liem, Annibale Panichella",
        "summary": "Testing deep reinforcement learning (DRL) agents in safety-critical domains requires discovering diverse failure scenarios. Existing tools such as INDAGO rely on single-objective optimization focused solely on maximizing failure counts, but this does not ensure discovered scenarios are diverse or reveal distinct error types. We introduce INDAGO-Nexus, a multi-objective search approach that jointly optimizes for failure likelihood and test scenario diversity using multi-objective evolutionary algorithms with multiple diversity metrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on three DRL agents: humanoid walker, self-driving car, and parking agent. On average, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test effectiveness) than INDAGO in the SDC and Parking scenarios, respectively, while reducing time-to-failure by up to 67% across all agents.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.966052"
    },
    {
        "index": "#22",
        "title": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References",
        "link": "/arxiv/2510.14719",
        "arxiv_id": "2510.14719",
        "authors": "Hongzheng Chen, Bin Fan, Alexander Collins, Bastian Hagedorn, Evghenii Gaburov, Masahiro Masuda, Matthew Brookhart, Chris Sullivan, Jason Knight, Zhiru Zhang, Vinod Grover",
        "summary": "Modern GPUs feature specialized hardware units that enable high-performance, asynchronous dataflow execution. However, the conventional SIMT programming model is fundamentally misaligned with this task-parallel hardware, creating a significant programmability gap. While hardware-level warp specialization is the key to unlocking peak performance, it forces developers to manually orchestrate complex, low-level communication and software pipelines--a process that is labor-intensive, error-prone, and unsustainable. To address this challenge, we present Tawa, an automated compiler that systematically generates high-performance, warp-specialized code from a high-level, tile-based program. Central to our approach is a novel IR abstraction, asynchronous references (aref), which expresses warp-level communication without exposing low-level hardware details. Using this abstraction, Tawa automatically partitions programs into producer-consumer roles and manages the intricate dataflow pipeline, relieving developers of invasive kernel rewriting. Evaluation on NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers high hardware utilization, achieving up to 1.1$\\times$ speedup over highly optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains 1.2$\\times$ speedup over Triton and matches the performance of the hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming effort.",
        "subjects": "Machine Learning, Hardware Architecture, Programming Languages",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.966418"
    },
    {
        "index": "#23",
        "title": "Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling",
        "link": "/arxiv/2510.14717",
        "arxiv_id": "2510.14717",
        "authors": "Alexandru Meterez, Depen Morwani, Jingfeng Wu, Costin-Andrei Oncescu, Cengiz Pehlevan, Sham Kakade",
        "summary": "Increasing the batch size during training -- a ''batch ramp'' -- is a promising strategy to accelerate large language model pretraining. While for SGD, doubling the batch size can be equivalent to halving the learning rate, the optimal strategy for adaptive optimizers like Adam is less clear. As a result, any batch-ramp scheduling, if used at all, is typically tuned heuristically. This work develops a principled framework for batch-size scheduling and introduces Seesaw: whenever a standard scheduler would halve the learning rate, Seesaw instead multiplies it by $1/\\sqrt{2}$ and doubles the batch size, preserving loss dynamics while reducing serial steps. Theoretically, we provide, to our knowledge, the first finite-sample proof of equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy linear regression, and we extend this equivalence to normalized SGD, a tractable proxy for Adam, under a variance-dominated regime observed in practice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla scale using a constant (critical) batch size, Seesaw matches cosine decay at equal FLOPs while reducing wall-clock time by $\\approx 36\\%$, approaching the theoretical limit implied by our analysis.",
        "subjects": "Machine Learning, Artificial Intelligence, Optimization and Control, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.966751"
    },
    {
        "index": "#24",
        "title": "FedPPA: Progressive Parameter Alignment for Personalized Federated Learning",
        "link": "/arxiv/2510.14698",
        "arxiv_id": "2510.14698",
        "authors": "Maulidi Adi Prasetia, Muhamad Risqi U. Saputra, Guntur Dharma Putra",
        "summary": "Federated Learning (FL) is designed as a decentralized, privacy-preserving machine learning paradigm that enables multiple clients to collaboratively train a model without sharing their data. In real-world scenarios, however, clients often have heterogeneous computational resources and hold non-independent and identically distributed data (non-IID), which poses significant challenges during training. Personalized Federated Learning (PFL) has emerged to address these issues by customizing models for each client based on their unique data distribution. Despite its potential, existing PFL approaches typically overlook the coexistence of model and data heterogeneity arising from clients with diverse computational capabilities. To overcome this limitation, we propose a novel method, called Progressive Parameter Alignment (FedPPA), which progressively aligns the weights of common layers across clients with the global model's weights. Our approach not only mitigates inconsistencies between global and local models during client updates, but also preserves client's local knowledge, thereby enhancing personalization robustness in non-IID settings. To further enhance the global model performance while retaining strong personalization, we also integrate entropy-based weighted averaging into the FedPPA framework. Experiments on three image classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate that FedPPA consistently outperforms existing FL algorithms, achieving superior performance in personalized adaptation.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.967032"
    },
    {
        "index": "#25",
        "title": "Online Reliable Anomaly Detection via Neuromorphic Sensing and Communications",
        "link": "/arxiv/2510.14688",
        "arxiv_id": "2510.14688",
        "authors": "Junya Shiraishi, Jiechen Chen, Osvaldo Simeone, Petar Popovski",
        "summary": "This paper proposes a low-power online anomaly detection framework based on neuromorphic wireless sensor networks, encompassing possible use cases such as brain-machine interfaces and remote environmental monitoring. In the considered system, a central reader node actively queries a subset of neuromorphic sensor nodes (neuro-SNs) at each time frame. The neuromorphic sensors are event-driven, producing spikes in correspondence to relevant changes in the monitored system. The queried neuro-SNs respond to the reader with impulse radio (IR) transmissions that directly encode the sensed local events. The reader processes these event-driven signals to determine whether the monitored environment is in a normal or anomalous state, while rigorously controlling the false discovery rate (FDR) of detections below a predefined threshold. The proposed approach employs an online hypothesis testing method with e-values to maintain FDR control without requiring knowledge of the anomaly rate, and it dynamically optimizes the sensor querying strategy by casting it as a best-arm identification problem in a multi-armed bandit framework. Extensive performance evaluation demonstrates that the proposed method can reliably detect anomalies under stringent FDR requirements, while efficiently scheduling sensor communications and achieving low detection latency.",
        "subjects": "Machine Learning, Neural and Evolutionary Computing",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.967322"
    },
    {
        "index": "#26",
        "title": "Geometric Moment Alignment for Domain Adaptation via Siegel Embeddings",
        "link": "/arxiv/2510.14666",
        "arxiv_id": "2510.14666",
        "authors": "Shayan Gharib, Marcelo Hartmann, Arto Klami",
        "summary": "We address the problem of distribution shift in unsupervised domain adaptation with a moment-matching approach. Existing methods typically align low-order statistical moments of the source and target distributions in an embedding space using ad-hoc similarity measures. We propose a principled alternative that instead leverages the intrinsic geometry of these distributions by adopting a Riemannian distance for this alignment. Our key novelty lies in expressing the first- and second-order moments as a single symmetric positive definite (SPD) matrix through Siegel embeddings. This enables simultaneous adaptation of both moments using the natural geometric distance on the shared manifold of SPD matrices, preserving the mean and covariance structure of the source and target distributions and yielding a more faithful metric for cross-domain comparison. We connect the Riemannian manifold distance to the target-domain error bound, and validate the method on image denoising and image classification benchmarks. Our code is publicly available at https://github.com/shayangharib/GeoAdapt.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.967587"
    },
    {
        "index": "#27",
        "title": "Galaxy Morphology Classification with Counterfactual Explanation",
        "link": "/arxiv/2510.14655",
        "arxiv_id": "2510.14655",
        "authors": "Zhuo Cao, Lena Krieger, Hanno Scharr, Ira Assent",
        "summary": "Galaxy morphologies play an essential role in the study of the evolution of galaxies. The determination of morphologies is laborious for a large amount of data giving rise to machine learning-based approaches. Unfortunately, most of these approaches offer no insight into how the model works and make the results difficult to understand and explain. We here propose to extend a classical encoder-decoder architecture with invertible flow, allowing us to not only obtain a good predictive performance but also provide additional information about the decision process with counterfactual explanations.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.967890"
    },
    {
        "index": "#28",
        "title": "LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching",
        "link": "/arxiv/2510.14623",
        "arxiv_id": "2510.14623",
        "authors": "Zhuo Cao, Xuan Zhao, Lena Krieger, Hanno Scharr, Ira Assent",
        "summary": "The growing integration of machine learning (ML) and artificial intelligence (AI) models into high-stakes domains such as healthcare and scientific research calls for models that are not only accurate but also interpretable. Among the existing explainable methods, counterfactual explanations offer interpretability by identifying minimal changes to inputs that would alter a model's prediction, thus providing deeper insights. However, current counterfactual generation methods suffer from critical limitations, including gradient vanishing, discontinuous latent spaces, and an overreliance on the alignment between learned and true decision boundaries. To overcome these limitations, we propose LeapFactual, a novel counterfactual explanation algorithm based on conditional flow matching. LeapFactual generates reliable and informative counterfactuals, even when true and learned decision boundaries diverge. Following a model-agnostic approach, LeapFactual is not limited to models with differentiable loss functions. It can even handle human-in-the-loop systems, expanding the scope of counterfactual explanations to domains that require the participation of human annotators, such as citizen science. We provide extensive experiments on benchmark and real-world datasets showing that LeapFactual generates accurate and in-distribution counterfactual explanations that offer actionable insights. We observe, for instance, that our reliable counterfactual samples with labels aligning to ground truth can be beneficially used as new training data to enhance the model. The proposed method is broadly applicable and enhances both scientific knowledge discovery and non-expert interpretability.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.968192"
    },
    {
        "index": "#29",
        "title": "First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training",
        "link": "/arxiv/2510.14614",
        "arxiv_id": "2510.14614",
        "authors": "Gyudong Kim, Hyukju Na, Jin Hyeon Kim, Hyunsung Jang, Jaemin Park, Jaegi Hwang, Namkoo Ha, Seungryong Kim, Young Geun Kim",
        "summary": "As training billion-scale transformers becomes increasingly common, employing multiple distributed GPUs along with parallel training methods has become a standard practice. However, existing transformer designs suffer from significant communication overhead, especially in Tensor Parallelism (TP), where each block's MHA-MLP connection requires an all-reduce communication. Through our investigation, we show that the MHA-MLP connections can be bypassed for efficiency, while the attention output of the first layer can serve as an alternative signal for the bypassed connection. Motivated by the observations, we propose FAL (First Attentions Last), an efficient transformer architecture that redirects the first MHA output to the MLP inputs of the following layers, eliminating the per-block MHA-MLP connections. This removes the all-reduce communication and enables parallel execution of MHA and MLP on a single GPU. We also introduce FAL+, which adds the normalized first attention output to the MHA outputs of the following layers to augment the MLP input for the model quality. Our evaluation shows that FAL reduces multi-GPU training time by up to 44%, improves single-GPU throughput by up to 1.18x, and achieves better perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity without increasing the training time than the baseline.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.968524"
    },
    {
        "index": "#30",
        "title": "Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval",
        "link": "/arxiv/2510.14592",
        "arxiv_id": "2510.14592",
        "authors": "Rashmi R, Vidyadhar Upadhya",
        "summary": "Current Retrieval-Augmented Generation (RAG) systems primarily operate on unimodal textual data, limiting their effectiveness on unstructured multimodal documents. Such documents often combine text, images, tables, equations, and graphs, each contributing unique information. In this work, we present a Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for multimodal question answering with reasoning through a modality-aware knowledge graph. MAHA integrates dense vector retrieval with structured graph traversal, where the knowledge graph encodes cross-modal semantics and relationships. This design enables both semantically rich and context-aware retrieval across diverse modalities. Evaluations on multiple benchmark datasets demonstrate that MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of 0.486, providing complete modality coverage. These results highlight MAHA's ability to combine embeddings with explicit document structure, enabling effective multimodal retrieval. Our work establishes a scalable and interpretable retrieval framework that advances RAG systems by enabling modality-aware reasoning over unstructured multimodal data.",
        "subjects": "Machine Learning, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.968803"
    },
    {
        "index": "#31",
        "title": "Matcha: Multi-Stage Riemannian Flow Matching for Accurate and Physically Valid Molecular Docking",
        "link": "/arxiv/2510.14586",
        "arxiv_id": "2510.14586",
        "authors": "Daria Frolova, Talgat Daulbaev, Egor Sevryugov, Sergei A. Nikolenko, Dmitry N. Ivankov, Ivan Oseledets, Marina A. Pak",
        "summary": "Accurate prediction of protein-ligand binding poses is crucial for structure-based drug design, yet existing methods struggle to balance speed, accuracy, and physical plausibility. We introduce Matcha, a novel molecular docking pipeline that combines multi-stage flow matching with learned scoring and physical validity filtering. Our approach consists of three sequential stages applied consecutively to refine docking predictions, each implemented as a flow matching model operating on appropriate geometric spaces ($\\mathbb{R}^3$, $\\mathrm{SO}(3)$, and $\\mathrm{SO}(2)$). We enhance the prediction quality through a dedicated scoring model and apply unsupervised physical validity filters to eliminate unrealistic poses. Compared to various approaches, Matcha demonstrates superior performance on Astex and PDBbind test sets in terms of docking success rate and physical plausibility. Moreover, our method works approximately 25 times faster than modern large-scale co-folding models. The model weights and inference code to reproduce our results are available at https://github.com/LigandPro/Matcha.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.969114"
    },
    {
        "index": "#32",
        "title": "Selective Labeling with False Discovery Rate Control",
        "link": "/arxiv/2510.14581",
        "arxiv_id": "2510.14581",
        "authors": "Huipeng Huang, Wenbo Liao, Huajun Xi, Hao Zeng, Mengchen Zhao, Hongxin Wei",
        "summary": "Obtaining high-quality labels for large datasets is expensive, requiring massive annotations from human experts. While AI models offer a cost-effective alternative by predicting labels, their label quality is compromised by the unavoidable labeling errors. Existing methods mitigate this issue through selective labeling, where AI labels a subset and human labels the remainder. However, these methods lack theoretical guarantees on the quality of AI-assigned labels, often resulting in unacceptably high labeling error within the AI-labeled subset. To address this, we introduce \\textbf{Conformal Labeling}, a novel method to identify instances where AI predictions can be provably trusted. This is achieved by controlling the false discovery rate (FDR), the proportion of incorrect labels within the selected subset. In particular, we construct a conformal $p$-value for each test instance by comparing AI models' predicted confidence to those of calibration instances mislabeled by AI models. Then, we select test instances whose $p$-values are below a data-dependent threshold, certifying AI models' predictions as trustworthy. We provide theoretical guarantees that Conformal Labeling controls the FDR below the nominal level, ensuring that a predefined fraction of AI-assigned labels is correct on average. Extensive experiments demonstrate that our method achieves tight FDR control with high power across various tasks, including image and text labeling, and LLM QA.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.969420"
    },
    {
        "index": "#33",
        "title": "State-Space Models for Tabular Prior-Data Fitted Networks",
        "link": "/arxiv/2510.14573",
        "arxiv_id": "2510.14573",
        "authors": "Felix Koch, Marcel Wever, Fabian Raisch, Benjamin Tischler",
        "summary": "Recent advancements in foundation models for tabular data, such as TabPFN, demonstrated that pretrained Transformer architectures can approximate Bayesian inference with high predictive performance. However, Transformers suffer from quadratic complexity with respect to sequence length, motivating the exploration of more efficient sequence models. In this work, we investigate the potential of using Hydra, a bidirectional linear-time structured state space model (SSM), as an alternative to Transformers in TabPFN. A key challenge lies in SSM's inherent sensitivity to the order of input tokens - an undesirable property for tabular datasets where the row order is semantically meaningless. We investigate to what extent a bidirectional approach can preserve efficiency and enable symmetric context aggregation. Our experiments show that this approach reduces the order-dependence, achieving predictive performance competitive to the original TabPFN model.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.969704"
    },
    {
        "index": "#34",
        "title": "Redundancy-Aware Test-Time Graph Out-of-Distribution Detection",
        "link": "/arxiv/2510.14562",
        "arxiv_id": "2510.14562",
        "authors": "Yue Hou, He Zhu, Ruomei Liu, Yingke Su, Junran Wu, Ke Xu",
        "summary": "Distributional discrepancy between training and test data can lead models to make inaccurate predictions when encountering out-of-distribution (OOD) samples in real-world applications. Although existing graph OOD detection methods leverage data-centric techniques to extract effective representations, their performance remains compromised by structural redundancy that induces semantic shifts. To address this dilemma, we propose RedOUT, an unsupervised framework that integrates structural entropy into test-time OOD detection for graph classification. Concretely, we introduce the Redundancy-aware Graph Information Bottleneck (ReGIB) and decompose the objective into essential information and irrelevant redundancy. By minimizing structural entropy, the decoupled redundancy is reduced, and theoretically grounded upper and lower bounds are proposed for optimization. Extensive experiments on real-world datasets demonstrate the superior performance of RedOUT on OOD detection. Specifically, our method achieves an average improvement of 6.7%, significantly surpassing the best competitor by 17.3% on the ClinTox/LIPO dataset pair.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.980444"
    },
    {
        "index": "#35",
        "title": "MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving",
        "link": "/arxiv/2510.14557",
        "arxiv_id": "2510.14557",
        "authors": "Jungi Lee, Junyong Park, Soohyun Cha, Jaehoon Cho, Jaewoong Sim",
        "summary": "Reduced-precision data formats are crucial for cost-effective serving of large language models (LLMs). While numerous reduced-precision formats have been introduced thus far, they often require intrusive modifications to the software frameworks or are rather unconventional for widespread adoption across hardware vendors. In this paper, we instead focus on recent industry-driven variants of block floating-point (BFP) formats and conduct a comprehensive analysis to push their limits for efficient LLM serving. Our analysis shows that existing ultra low-bit BFP variants struggle to provide reasonable language model performance due to outlier values in blocks. To address the outliers with BFPs, we propose MX+, a cost-effective and non-intrusive extension designed for seamless integration into the microscaling (MX) formats. MX+ builds on the key insight that the outlier does not need to use its exponent field in the element data type, which allows us to repurpose the exponent field as an extended mantissa to increase the precision of the outlier element. Our evaluation shows that MX+ achieves significantly higher model performance compared to the 4-bit MX format (MXFP4) with negligible storage overhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6 for efficient LLM inference.",
        "subjects": "Machine Learning, Hardware Architecture",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.980817"
    },
    {
        "index": "#36",
        "title": "Agentic Entropy-Balanced Policy Optimization",
        "link": "/arxiv/2510.14545",
        "arxiv_id": "2510.14545",
        "authors": "Guanting Dong, Licheng Bao, Zhongyuan Wang, Kangzhi Zhao, Xiaoxi Li, Jiajie Jin, Jinghan Yang, Hangyu Mao, Fuzheng Zhang, Kun Gai, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou",
        "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.981244"
    },
    {
        "index": "#37",
        "title": "On the Identifiability of Tensor Ranks via Prior Predictive Matching",
        "link": "/arxiv/2510.14523",
        "arxiv_id": "2510.14523",
        "authors": "Eliezer da Silva, Arto Klami, Diego Mesquita, Iñigo Urteaga",
        "summary": "Selecting the latent dimensions (ranks) in tensor factorization is a central challenge that often relies on heuristic methods. This paper introduces a rigorous approach to determine rank identifiability in probabilistic tensor models, based on prior predictive moment matching. We transform a set of moment matching conditions into a log-linear system of equations in terms of marginal moments, prior hyperparameters, and ranks; establishing an equivalence between rank identifiability and the solvability of such system. We apply this framework to four foundational tensor-models, demonstrating that the linear structure of the PARAFAC/CP model, the chain structure of the Tensor Train model, and the closed-loop structure of the Tensor Ring model yield solvable systems, making their ranks identifiable. In contrast, we prove that the symmetric topology of the Tucker model leads to an underdetermined system, rendering the ranks unidentifiable by this method. For the identifiable models, we derive explicit closed-form rank estimators based on the moments of observed data only. We empirically validate these estimators and evaluate the robustness of the proposal.",
        "subjects": "Machine Learning, Statistics Theory, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.981564"
    },
    {
        "index": "#38",
        "title": "Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective",
        "link": "/arxiv/2510.14510",
        "arxiv_id": "2510.14510",
        "authors": "Xingjian Wu, Xiangfei Qiu, Hanyin Cheng, Zhengyu Li, Jilin Hu, Chenjuan Guo, Bin Yang",
        "summary": "Time Series Forecasting has made significant progress with the help of Patching technique, which partitions time series into multiple patches to effectively retain contextual semantic information into a representation space beneficial for modeling long-term dependencies. However, conventional patching partitions a time series into adjacent patches, which causes a fixed representation space, thus resulting in insufficiently expressful representations. In this paper, we pioneer the exploration of constructing a selective representation space to flexibly include the most informative patches for forecasting. Specifically, we propose the Selective Representation Space (SRS) module, which utilizes the learnable Selective Patching and Dynamic Reassembly techniques to adaptively select and shuffle the patches from the contextual time series, aiming at fully exploiting the information of contextual time series to enhance the forecasting performance of patch-based models. To demonstrate the effectiveness of SRS module, we propose a simple yet effective SRSNet consisting of SRS and an MLP head, which achieves state-of-the-art performance on real-world datasets from multiple domains. Furthermore, as a novel plugin-and-play module, SRS can also enhance the performance of existing patch-based models. The resources are available at https://github.com/decisionintelligence/SRSNet.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.981871"
    },
    {
        "index": "#39",
        "title": "Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals",
        "link": "/arxiv/2510.14503",
        "arxiv_id": "2510.14503",
        "authors": "Andrejs Sorstkins, Omer Tariq, Muhammad Bilal",
        "summary": "This paper proposes a reversible learning framework to improve the robustness and efficiency of value based Reinforcement Learning agents, addressing vulnerability to value overestimation and instability in partially irreversible environments. The framework has two complementary core mechanisms: an empirically derived transition reversibility measure called Phi of s and a, and a selective state rollback operation. We introduce an online per state action estimator called Phi that quantifies the likelihood of returning to a prior state within a fixed horizon K. This measure is used to adjust the penalty term during temporal difference updates dynamically, integrating reversibility awareness directly into the value function. The system also includes a selective rollback operator. When an action yields an expected return markedly lower than its instantaneous estimated value and violates a predefined threshold, the agent is penalized and returns to the preceding state rather than progressing. This interrupts sub optimal high risk trajectories and avoids catastrophic steps. By combining reversibility aware evaluation with targeted rollback, the method improves safety, performance, and stability. In the CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8 percent and yielded a 55 percent increase in mean episode return. In the Taxi v3 domain, it suppressed illegal actions by greater than or equal to 99.9 percent and achieved a 65.7 percent improvement in cumulative reward, while also sharply reducing reward variance in both environments. Ablation studies confirm that the rollback mechanism is the critical component underlying these safety and performance gains, marking a robust step toward safe and reliable sequential decision making.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.982140"
    },
    {
        "index": "#40",
        "title": "From Guess2Graph: When and How Can Unreliable Experts Safely Boost Causal Discovery in Finite Samples?",
        "link": "/arxiv/2510.14488",
        "arxiv_id": "2510.14488",
        "authors": "Sujai Hiremath, Dominik Janzing, Philipp Faller, Patrick Blöbaum, Elke Kirschbaum, Shiva Prasad Kasiviswanathan, Kyra Gan",
        "summary": "Causal discovery algorithms often perform poorly with limited samples. While integrating expert knowledge (including from LLMs) as constraints promises to improve performance, guarantees for existing methods require perfect predictions or uncertainty estimates, making them unreliable for practical use. We propose the Guess2Graph (G2G) framework, which uses expert guesses to guide the sequence of statistical tests rather than replacing them. This maintains statistical consistency while enabling performance improvements. We develop two instantiations of G2G: PC-Guess, which augments the PC algorithm, and gPC-Guess, a learning-augmented variant designed to better leverage high-quality expert input. Theoretically, both preserve correctness regardless of expert error, with gPC-Guess provably outperforming its non-augmented counterpart in finite samples when experts are \"better than random.\" Empirically, both show monotonic improvement with expert accuracy, with gPC-Guess achieving significantly stronger gains.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.982459"
    },
    {
        "index": "#41",
        "title": "Holdout-Loss-Based Data Selection for LLM Finetuning via In-Context Learning",
        "link": "/arxiv/2510.14459",
        "arxiv_id": "2510.14459",
        "authors": "Ling Zhang, Xianliang Yang, Juwon Yu, Park Cheonyoung, Lei Song, Jiang Bian",
        "summary": "Fine-tuning large pretrained language models is a common approach for aligning them with human preferences, but noisy or off-target examples can dilute supervision. While small, well-chosen datasets often match the performance of much larger ones, systematic and efficient ways to identify high-value training data remain underexplored. Many current methods rely on heuristics or expensive retraining. We present a theoretically grounded, resource-efficient framework for data selection and reweighting. At its core is an In-Context Approximation (ICA) that estimates the holdout loss a model would incur after training on a candidate example by conditioning on a small, curated holdout set in context. ICA requires no reference model and no additional finetuning. Under a local linearization, ICA is equivalent to a first-order update toward the holdout optimum, motivating its use as a proxy for data value. We derive per-example weights from ICA scores, dynamically reweighting gradient updates as model parameters evolve. Across SFT, DPO, and SimPO, and over diverse backbones and datasets, ICA-based reweighting consistently improves model alignment with minimal overhead. We analyze sensitivity to score update frequency and the choice of $k$ holdout examples for in-context demonstrations, and note limitations for rapidly drifting on-policy updates, highlighting directions for future work. Code and prompts will be released.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.982777"
    },
    {
        "index": "#42",
        "title": "Coder as Editor: Code-driven Interpretable Molecular Optimization",
        "link": "/arxiv/2510.14455",
        "arxiv_id": "2510.14455",
        "authors": "Wenyu Zhu, Chengzhu Li, Xiaohe Tian, Yifan Wang, Yinjun Jia, Jianhui Wang, Bowen Gao, Ya-Qin Zhang, Wei-Ying Ma, Yanyan Lan",
        "summary": "Molecular optimization is a central task in drug discovery that requires precise structural reasoning and domain knowledge. While large language models (LLMs) have shown promise in generating high-level editing intentions in natural language, they often struggle to faithfully execute these modifications-particularly when operating on non-intuitive representations like SMILES. We introduce MECo, a framework that bridges reasoning and execution by translating editing actions into executable code. MECo reformulates molecular optimization for LLMs as a cascaded framework: generating human-interpretable editing intentions from a molecule and property goal, followed by translating those intentions into executable structural edits via code generation. Our approach achieves over 98% accuracy in reproducing held-out realistic edits derived from chemical reactions and target-specific compound pairs. On downstream optimization benchmarks spanning physicochemical properties and target activities, MECo substantially improves consistency by 38-86 percentage points to 90%+ and achieves higher success rates over SMILES-based baselines while preserving structural similarity. By aligning intention with execution, MECo enables consistent, controllable and interpretable molecular design, laying the foundation for high-fidelity feedback loops and collaborative human-AI workflows in drug discovery.",
        "subjects": "Machine Learning, Biomolecules",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.983122"
    },
    {
        "index": "#43",
        "title": "Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints",
        "link": "/arxiv/2510.14449",
        "arxiv_id": "2510.14449",
        "authors": "Jahidul Arafat, Fariha Tasmin, Md Kaosar Uddin, Sanjaya Poudel, Eftakhar Ahmed Arnob",
        "summary": "Multi-class wine classification presents fundamental trade-offs between model accuracy, feature dimensionality, and interpretability - critical factors for production deployment in analytical chemistry. This paper presents a comprehensive empirical study of One-vs-Rest logistic regression on the UCI Wine dataset (178 samples, 3 cultivars, 13 chemical features), comparing from-scratch gradient descent implementation against scikit-learn's optimized solvers and quantifying L1 regularization effects on feature sparsity. Manual gradient descent achieves 92.59 percent mean test accuracy with smooth convergence, validating theoretical foundations, though scikit-learn provides 24x training speedup and 98.15 percent accuracy. Class-specific analysis reveals distinct chemical signatures with heterogeneous patterns where color intensity varies dramatically (0.31 to 16.50) across cultivars. L1 regularization produces 54-69 percent feature reduction with only 4.63 percent accuracy decrease, demonstrating favorable interpretability-performance trade-offs. We propose an optimal 5-feature subset achieving 62 percent complexity reduction with estimated 92-94 percent accuracy, enabling cost-effective deployment with 80 dollars savings per sample and 56 percent time reduction. Statistical validation confirms robust generalization with sub-2ms prediction latency suitable for real-time quality control. Our findings provide actionable guidelines for practitioners balancing comprehensive chemical analysis against targeted feature measurement in resource-constrained environments.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.983421"
    },
    {
        "index": "#44",
        "title": "Towards geological inference with process-based and deep generative modeling, part 1: training on fluvial deposits",
        "link": "/arxiv/2510.14445",
        "arxiv_id": "2510.14445",
        "authors": "Guillaume Rongier, Luk Peeters",
        "summary": "The distribution of resources in the subsurface is deeply linked to the variations of its physical properties. Generative modeling has long been used to predict those physical properties while quantifying the associated uncertainty. But current approaches struggle to properly reproduce geological structures, and fluvial deposits in particular, because of their continuity. This study explores whether a generative adversarial network (GAN) - a type of deep-learning algorithm for generative modeling - can be trained to reproduce fluvial deposits simulated by a process-based model - a more expensive model that mimics geological processes. An ablation study shows that developments from the deep-learning community to generate large 2D images are directly transferable to 3D images of fluvial deposits. Training remains stable, and the generated samples reproduce the non-stationarity and details of the deposits without mode collapse or pure memorization of the training data. Using a process-based model to generate those training data allows us to include valuable properties other than the usual physical properties. We show how the deposition time let us monitor and validate the performance of a GAN by checking that its samples honor the law of superposition. Our work joins a series of previous studies suggesting that GANs are more robust that given credit for, at least for training datasets targeting specific geological structures. Whether this robustness transfers to larger 3D images and multimodal datasets remains to be seen. Exploring how deep generative models can leverage geological principles like the law of superposition shows a lot of promise.",
        "subjects": "Machine Learning, Geophysics",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.983700"
    },
    {
        "index": "#45",
        "title": "A Free Lunch in LLM Compression: Revisiting Retraining after Pruning",
        "link": "/arxiv/2510.14444",
        "arxiv_id": "2510.14444",
        "authors": "Moritz Wagner, Christophe Roux, Max Zimmer, Sebastian Pokutta",
        "summary": "While Neural Network pruning typically requires retraining the model to recover pruning-induced performance degradation, state-of-the-art Large Language Models (LLMs) pruning methods instead solve a layer-wise mask selection and reconstruction problem on a small set of calibration data to avoid full retraining, as it is considered computationally infeasible for LLMs. Reconstructing single matrices in isolation has favorable properties, such as convexity of the objective and significantly reduced memory requirements compared to full retraining. In practice, however, reconstruction is often implemented at coarser granularities, e.g., reconstructing a whole transformer block against its dense activations instead of a single matrix. In this work, we study the key design choices when reconstructing or retraining the remaining weights after pruning. We conduct an extensive computational study on state-of-the-art GPT architectures, and report several surprising findings that challenge common intuitions about retraining after pruning. In particular, we observe a free lunch scenario: reconstructing attention and MLP components separately within each transformer block is nearly the most resource-efficient yet achieves the best perplexity. Most importantly, this Pareto-optimal setup achieves better performance than full retraining, despite requiring only a fraction of the memory. Furthermore, we demonstrate that simple and efficient pruning criteria such as Wanda can outperform much more complex approaches when the reconstruction step is properly executed, highlighting its importance. Our findings challenge the narrative that retraining should be avoided at all costs and provide important insights into post-pruning performance recovery for LLMs.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.984011"
    },
    {
        "index": "#46",
        "title": "MergeMoE: Efficient Compression of MoE Models via Expert Output Merging",
        "link": "/arxiv/2510.14436",
        "arxiv_id": "2510.14436",
        "authors": "Ruijie Miao, Yilun Yao, Zihan Wang, Zhiming Wang, Bairen Yi, LingJun Liu, Yikai Zhao, Tong Yang",
        "summary": "The Mixture-of-Experts (MoE) technique has proven to be a promising solution to efficiently scale the model size, which has been widely applied in recent LLM advancements. However, the substantial memory overhead of MoE models has made their compression an important research direction. In this work, we provide a theoretical analysis of expert merging, a recently proposed technique for compressing MoE models. Rather than interpreting expert merging from the conventional perspective of parameter aggregation, we approach it from the perspective of merging experts' outputs. Our key insight is that the merging process can be interpreted as inserting additional matrices into the forward computation, which naturally leads to an optimization formulation. Building on this analysis, we introduce MergeMoE, a method that leverages mathematical optimization to construct the compression matrices. We evaluate MergeMoE on multiple MoE models and show that our algorithm consistently outperforms the baselines with the same compression ratios.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.984327"
    },
    {
        "index": "#47",
        "title": "Interaction Concordance Index: Performance Evaluation for Interaction Prediction Methods",
        "link": "/arxiv/2510.14419",
        "arxiv_id": "2510.14419",
        "authors": "Tapio Pahikkala, Riikka Numminen, Parisa Movahedi, Napsu Karmitsa, Antti Airola",
        "summary": "Consider two sets of entities and their members' mutual affinity values, say drug-target affinities (DTA). Drugs and targets are said to interact in their effects on DTAs if drug's effect on it depends on the target. Presence of interaction implies that assigning a drug to a target and another drug to another target does not provide the same aggregate DTA as the reversed assignment would provide. Accordingly, correctly capturing interactions enables better decision-making, for example, in allocation of limited numbers of drug doses to their best matching targets. Learning to predict DTAs is popularly done from either solely from known DTAs or together with side information on the entities, such as chemical structures of drugs and targets. In this paper, we introduce interaction directions' prediction performance estimator we call interaction concordance index (IC-index), for both fixed predictors and machine learning algorithms aimed for inferring them. IC-index complements the popularly used DTA prediction performance estimators by evaluating the ratio of correctly predicted directions of interaction effects in data. First, we show the invariance of IC-index on predictors unable to capture interactions. Secondly, we show that learning algorithm's permutation equivariance regarding drug and target identities implies its inability to capture interactions when either drug, target or both are unseen during training. In practical applications, this equivariance is remedied via incorporation of appropriate side information on drugs and targets. We make a comprehensive empirical evaluation over several biomedical interaction data sets with various state-of-the-art machine learning algorithms. The experiments demonstrate how different types of affinity strength prediction methods perform in terms of IC-index complementing existing prediction performance estimators.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.984629"
    },
    {
        "index": "#48",
        "title": "Revisit Modality Imbalance at the Decision Layer",
        "link": "/arxiv/2510.14411",
        "arxiv_id": "2510.14411",
        "authors": "Xiaoyu Ma, Hao Chen",
        "summary": "Multimodal learning integrates information from different modalities to enhance model performance, yet it often suffers from modality imbalance, where dominant modalities overshadow weaker ones during joint optimization. This paper reveals that such an imbalance not only occurs during representation learning but also manifests significantly at the decision layer. Experiments on audio-visual datasets (CREMAD and Kinetic-Sounds) show that even after extensive pretraining and balanced optimization, models still exhibit systematic bias toward certain modalities, such as audio. Further analysis demonstrates that this bias originates from intrinsic disparities in feature-space and decision-weight distributions rather than from optimization dynamics alone. We argue that aggregating uncalibrated modality outputs at the fusion stage leads to biased decision-layer weighting, hindering weaker modalities from contributing effectively. To address this, we propose that future multimodal systems should focus more on incorporate adaptive weight allocation mechanisms at the decision layer, enabling relative balanced according to the capabilities of each modality.",
        "subjects": "Machine Learning, Multimedia, Sound, Audio and Speech Processing",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.984932"
    },
    {
        "index": "#49",
        "title": "SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable Modeling in Long Sequences",
        "link": "/arxiv/2510.14386",
        "arxiv_id": "2510.14386",
        "authors": "Kartikay Agrawal, Abhijeet Vikram, Vedant Sharma, Vaishnavi N., Ayon Borthakur",
        "summary": "In recent years, with the emergence of large models, there has been a significant interest in spiking neural networks (SNNs) primarily due to their energy efficiency, multiplication-free, and sparse event-based deep learning. Similarly, state space models (SSMs) in varying designs have evolved as a powerful alternative to transformers for target modeling in long sequences, thereby overcoming the quadratic dependence on sequence length of a transformer. Inspired by this progress, we here design SHaRe-SSM (Spiking Harmonic Resonate and Fire State Space Model), for target variable modeling (including both classification and regression) for very-long-range sequences. Our second-order spiking SSM, on average, performs better than transformers or first-order SSMs while circumventing multiplication operations, making it ideal for resource-constrained applications. The proposed block consumes $73 \\times$ less energy than second-order ANN-based SSMs for an 18k sequence, while retaining performance. To ensure learnability over the long-range sequences, we propose exploiting the stable and efficient implementation of the dynamical system using parallel scans. Moreover, for the first time, we propose a kernel-based spiking regressor using resonate and fire neurons for very long-range sequences. Our network shows superior performance on even a 50k sequence while being significantly energy-efficient. In addition, we conducted a systematic analysis of the impact of heterogeneity, dissipation, and conservation in resonate-and-fire SSMs.",
        "subjects": "Machine Learning, Neural and Evolutionary Computing",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.990427"
    },
    {
        "index": "#50",
        "title": "Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers",
        "link": "/arxiv/2510.14381",
        "arxiv_id": "2510.14381",
        "authors": "Andrew Zhao, Reshmi Ghosh, Vitor Carvalho, Emily Lawton, Keegan Hines, Gao Huang, Jack W. Stokes",
        "summary": "Large language model (LLM) systems now underpin everyday AI applications such as chatbots, computer-use assistants, and autonomous robots, where performance often depends on carefully designed prompts. LLM-based prompt optimizers reduce that effort by iteratively refining prompts from scored feedback, yet the security of this optimization stage remains underexamined. We present the first systematic analysis of poisoning risks in LLM-based prompt optimization. Using HarmBench, we find systems are substantially more vulnerable to manipulated feedback than to injected queries: feedback-based attacks raise attack success rate (ASR) by up to $\\Delta$ASR = 0.48. We introduce a simple fake-reward attack that requires no access to the reward model and significantly increases vulnerability, and we propose a lightweight highlighting defense that reduces the fake-reward $\\Delta$ASR from 0.23 to 0.07 without degrading utility. These results establish prompt optimization pipelines as a first-class attack surface and motivate stronger safeguards for feedback channels and optimization frameworks.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Cryptography and Security",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.990793"
    },
    {
        "index": "#51",
        "title": "Jet Functors and Weil Algebras in Automatic Differentiation: A Geometric Analysis",
        "link": "/arxiv/2510.14342",
        "arxiv_id": "2510.14342",
        "authors": "Amandip Sangha",
        "summary": "We present a geometric formulation of automatic differentiation (AD) using jet bundles and Weil algebras. Reverse-mode AD emerges as cotangent-pullback, while Taylor-mode corresponds to evaluation in a Weil algebra. From these principles, we derive concise statements on correctness, stability, and complexity: a functorial identity for reverse-mode, algebraic exactness of higher-order derivatives, and explicit bounds on truncation error. We further show that tensorized Weil algebras permit one-pass computation of all mixed derivatives with cost linear in the algebra dimension, avoiding the combinatorial blow-up of nested JVP/VJP schedules. This framework interprets AD theory through the lens of differential geometry and offers a foundation for developing structure-preserving differentiation methods in deep learning and scientific computing. Code and examples are available at https://git.nilu.no/geometric-ad/jet-weil-ad.",
        "subjects": "Machine Learning, Differential Geometry, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.991070"
    },
    {
        "index": "#52",
        "title": "Stop-RAG: Value-Based Retrieval Control for Iterative RAG",
        "link": "/arxiv/2510.14337",
        "arxiv_id": "2510.14337",
        "authors": "Jaewan Park, Solbee Cho, Jay-Yoon Lee",
        "summary": "Iterative retrieval-augmented generation (RAG) enables large language models to answer complex multi-hop questions, but each additional loop increases latency, costs, and the risk of introducing distracting evidence, motivating the need for an efficient stopping strategy. Existing methods either use a predetermined number of iterations or rely on confidence proxies that poorly reflect whether more retrieval will actually help. We cast iterative RAG as a finite-horizon Markov decision process and introduce Stop-RAG, a value-based controller that adaptively decides when to stop retrieving. Trained with full-width forward-view Q($\\lambda$) targets from complete trajectories, Stop-RAG learns effective stopping policies while remaining compatible with black-box APIs and existing pipelines. On multi-hop question-answering benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines and prompting-based stopping with LLMs. These results highlight adaptive stopping as a key missing component in current agentic systems, and demonstrate that value-based control can improve the accuracy of RAG systems.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.991351"
    },
    {
        "index": "#53",
        "title": "DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis",
        "link": "/arxiv/2510.14336",
        "arxiv_id": "2510.14336",
        "authors": "Shruti Sarika Chakraborty, Peter Minary",
        "summary": "Graph Transformers (GTs) have emerged as powerful architectures for graph-structured data, yet remain constrained by rigid designs and lack quantifiable interpretability. Current state-of-the-art GTs commit to fixed GNN types across all layers, missing potential benefits of depth-specific component selection, while their complex architectures become opaque where performance gains cannot be distinguished between meaningful patterns and spurious correlations. We redesign GT attention through asymmetry, decoupling structural encoding from feature representation: queries derive from node features while keys and values come from GNN transformations. Within this framework, we use Differentiable ARchiTecture Search (DARTS) to select optimal GNN operators at each layer, enabling depth-wise heterogeneity inside transformer attention itself (DARTS-GT). To understand discovered architectures, we develop the first quantitative interpretability framework for GTs through causal ablation. Our metrics (Head-deviation, Specialization, and Focus), identify which heads and nodes drive predictions while enabling model comparison. Experiments across eight benchmarks show DARTS-GT achieves state-of-the-art on four datasets while remaining competitive on others, with discovered architectures revealing dataset-specific patterns. Our interpretability analysis reveals that visual attention salience and causal importance do not always correlate, indicating widely used visualization approaches may miss components that actually matter. Crucially, heterogeneous architectures found by DARTS-GT consistently produced more interpretable models than baselines, establishing that Graph Transformers need not choose between performance and interpretability.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.991620"
    },
    {
        "index": "#54",
        "title": "LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search",
        "link": "/arxiv/2510.14331",
        "arxiv_id": "2510.14331",
        "authors": "Shivam Singhal, Eran Malach, Tomaso Poggio, Tomer Galanti",
        "summary": "We seek algorithms for program learning that are both sample-efficient and computationally feasible. Classical results show that targets admitting short program descriptions (e.g., with short ``python code'') can be learned with a ``small'' number of examples (scaling with the size of the code) via length-first program enumeration, but the search is exponential in description length. Consequently, Gradient-based training avoids this cost yet can require exponentially many samples on certain short-program families. To address this gap, we introduce LLM-ERM, a propose-and-verify framework that replaces exhaustive enumeration with an LLM-guided search over candidate programs while retaining ERM-style selection on held-out data. Specifically, we draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and check each on the data, and return the best verified hypothesis, with no feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise online mini-batch SGD requires many samples to learn certain short programs. {\\em Empirically, LLM-ERM solves tasks such as parity variants, pattern matching, and primality testing with as few as 200 samples, while SGD-trained transformers overfit even with 100,000 samples}. These results indicate that language-guided program synthesis recovers much of the statistical efficiency of finite-class ERM while remaining computationally tractable, offering a practical route to learning succinct hypotheses beyond the reach of gradient-based training.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.991934"
    },
    {
        "index": "#55",
        "title": "Active Measuring in Reinforcement Learning With Delayed Negative Effects",
        "link": "/arxiv/2510.14315",
        "arxiv_id": "2510.14315",
        "authors": "Daiqi Gao, Ziping Xu, Aseel Rawashdeh, Predrag Klasnja, Susan A. Murphy",
        "summary": "Measuring states in reinforcement learning (RL) can be costly in real-world settings and may negatively influence future outcomes. We introduce the Actively Observable Markov Decision Process (AOMDP), where an agent not only selects control actions but also decides whether to measure the latent state. The measurement action reveals the true latent state but may have a negative delayed effect on the environment. We show that this reduced uncertainty may provably improve sample efficiency and increase the value of the optimal policy despite these costs. We formulate an AOMDP as a periodic partially observable MDP and propose an online RL algorithm based on belief states. To approximate the belief states, we further propose a sequential Monte Carlo method to jointly approximate the posterior of unknown static environment parameters and unobserved latent states. We evaluate the proposed algorithm in a digital health application, where the agent decides when to deliver digital interventions and when to assess users' health status through surveys.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.992234"
    },
    {
        "index": "#56",
        "title": "TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening",
        "link": "/arxiv/2510.14299",
        "arxiv_id": "2510.14299",
        "authors": "Nam Le, Leo Yu Zhang, Kewen Liao, Shirui Pan, Wei Luo",
        "summary": "As deep neural networks power increasingly critical applications, stealthy backdoor attacks, where poisoned training inputs trigger malicious model behaviour while appearing benign, pose a severe security risk. Many existing defences are vulnerable when attackers exploit subtle distance-based anomalies or when clean examples are scarce. To meet this challenge, we introduce TED++, a submanifold-aware framework that effectively detects subtle backdoors that evade existing defences. TED++ begins by constructing a tubular neighbourhood around each class's hidden-feature manifold, estimating its local ``thickness'' from a handful of clean activations. It then applies Locally Adaptive Ranking (LAR) to detect any activation that drifts outside the admissible tube. By aggregating these LAR-adjusted ranks across all layers, TED++ captures how faithfully an input remains on the evolving class submanifolds. Based on such characteristic ``tube-constrained'' behaviour, TED++ flags inputs whose LAR-based ranking sequences deviate significantly. Extensive experiments are conducted on benchmark datasets and tasks, demonstrating that TED++ achieves state-of-the-art detection performance under both adaptive-attack and limited-data scenarios. Remarkably, even with only five held-out examples per class, TED++ still delivers near-perfect detection, achieving gains of up to 14\\% in AUROC over the next-best method. The code is publicly available at https://github.com/namle-w/TEDpp.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.992546"
    },
    {
        "index": "#57",
        "title": "Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual Bottom-Up Attention with Reservoir Computing",
        "link": "/arxiv/2510.14287",
        "arxiv_id": "2510.14287",
        "authors": "Hayato Nihei, Sou Nobukawa, Yusuke Sakemi, Kazuyuki Aihara",
        "summary": "Reservoir computing (RC) establishes the basis for the processing of time-series data by exploiting the high-dimensional spatiotemporal response of a recurrent neural network to an input signal. In particular, RC trains only the output layer weights. This simplicity has drawn attention especially in Edge Artificial Intelligence (AI) applications. Edge AI enables time-series anomaly detection in real time, which is important because detection delays can lead to serious incidents. However, achieving adequate anomaly-detection performance with RC alone may require an unacceptably large reservoir on resource-constrained edge devices. Without enlarging the reservoir, attention mechanisms can improve accuracy, although they may require substantial computation and undermine the learning efficiency of RC. In this study, to improve the anomaly detection performance of RC without sacrificing learning efficiency, we propose a spectral residual RC (SR-RC) that integrates the spectral residual (SR) method - a learning-free, bottom-up attention mechanism - with RC. We demonstrated that SR-RC outperformed conventional RC and logistic-regression models based on values extracted by the SR method across benchmark tasks and real-world time-series datasets. Moreover, because the SR method, similarly to RC, is well suited for hardware implementation, SR-RC suggests a practical direction for deploying RC as Edge AI for time-series anomaly detection.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.992832"
    },
    {
        "index": "#58",
        "title": "Stable Prediction of Adverse Events in Medical Time-Series Data",
        "link": "/arxiv/2510.14286",
        "arxiv_id": "2510.14286",
        "authors": "Mayank Keoliya, Seewon Choi, Rajeev Alur, Mayur Naik, Eric Wong",
        "summary": "Early event prediction (EEP) systems continuously estimate a patient's imminent risk to support clinical decision-making. For bedside trust, risk trajectories must be accurate and temporally stable, shifting only with new, relevant evidence. However, current benchmarks (a) ignore stability of risk scores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior untested. To address this gap, we introduce CAREBench, an EEP benchmark that evaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms, and clinical text-and assesses temporal stability alongside predictive accuracy. We propose a stability metric that quantifies short-term variability in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz constants. CAREBench spans six prediction tasks such as sepsis onset and compares classical learners, deep sequence models, and zero-shot LLMs. Across tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy and stability, with notably poor recall at high-precision operating points. These results highlight the need for models that produce evidence-aligned, stable trajectories to earn clinician trust in continuous monitoring settings. (Code: https://github.com/SeewonChoi/CAREBench.)",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.993129"
    },
    {
        "index": "#59",
        "title": "Nonparametric Data Attribution for Diffusion Models",
        "link": "/arxiv/2510.14269",
        "arxiv_id": "2510.14269",
        "authors": "Yutian Zhao, Chao Du, Xiaosen Zheng, Tianyu Pang, Min Lin",
        "summary": "Data attribution for generative models seeks to quantify the influence of individual training examples on model outputs. Existing methods for diffusion models typically require access to model gradients or retraining, limiting their applicability in proprietary or large-scale settings. We propose a nonparametric attribution method that operates entirely on data, measuring influence via patch-level similarity between generated and training images. Our approach is grounded in the analytical form of the optimal score function and naturally extends to multiscale representations, while remaining computationally efficient through convolution-based acceleration. In addition to producing spatially interpretable attributions, our framework uncovers patterns that reflect intrinsic relationships between training data and outputs, independent of any specific model. Experiments demonstrate that our method achieves strong attribution performance, closely matching gradient-based approaches and substantially outperforming existing nonparametric baselines. Code is available at https://github.com/sail-sg/NDA.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.993436"
    },
    {
        "index": "#60",
        "title": "CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions",
        "link": "/arxiv/2510.14262",
        "arxiv_id": "2510.14262",
        "authors": "Zihao Fu, Ming Liao, Chris Russell, Zhenguang G. Cai",
        "summary": "Large language models have achieved remarkable success but remain largely black boxes with poorly understood internal mechanisms. To address this limitation, many researchers have proposed various interpretability methods including mechanistic analysis, probing classifiers, and activation visualization, each providing valuable insights from different perspectives. Building upon this rich landscape of complementary approaches, we introduce CAST (Compositional Analysis via Spectral Tracking), a probe-free framework that contributes a novel perspective by analyzing transformer layer functions through direct transformation matrix estimation and comprehensive spectral analysis. CAST offers complementary insights to existing methods by estimating the realized transformation matrices for each layer using Moore-Penrose pseudoinverse and applying spectral analysis with six interpretable metrics characterizing layer behavior. Our analysis reveals distinct behaviors between encoder-only and decoder-only models, with decoder models exhibiting compression-expansion cycles while encoder models maintain consistent high-rank processing. Kernel analysis further demonstrates functional relationship patterns between layers, with CKA similarity matrices clearly partitioning layers into three phases: feature extraction, compression, and specialization.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.993737"
    },
    {
        "index": "#61",
        "title": "Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals",
        "link": "/arxiv/2510.14254",
        "arxiv_id": "2510.14254",
        "authors": "Saurabh Kataria, Yi Wu, Zhaoliang Chen, Hyunjung Gloria Kwak, Yuhao Xu, Lovely Yeswanth Panchumarthi, Ran Xiao, Jiaying Lu, Ayca Ermis, Anni Zhao, Runze Yan, Alex Federov, Zewen Liu, Xu Wu, Wei Jin, Carl Yang, Jocelyn Grunwell, Stephanie R. Brown, Amit Shah, Craig Jabaley, Tim Buchman, Sivasubramanium V Bhavani, Randall J. Lee, Xiao Hu",
        "summary": "Foundation models are large-scale machine learning models that are pre-trained on massive amounts of data and can be adapted for various downstream tasks. They have been extensively applied to tasks in Natural Language Processing and Computer Vision with models such as GPT, BERT, and CLIP. They are now also increasingly gaining attention in time-series analysis, particularly for physiological sensing. However, most time series foundation models are specialist models - with data in pre-training and testing of the same type, such as Electrocardiogram, Electroencephalogram, and Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time series foundation model with data from multiple domains, such as weather, traffic, and electricity. This paper aims to conduct a comprehensive benchmarking study to compare the performance of generalist and specialist models, with a focus on PPG signals. Through an extensive suite of total 51 tasks covering cardiac state assessment, laboratory value estimation, and cross-modal inference, we comprehensively evaluate both models across seven dimensions, including win score, average performance, feature quality, tuning gain, performance variance, transferability, and scalability. These metrics jointly capture not only the models' capability but also their adaptability, robustness, and efficiency under different fine-tuning strategies, providing a holistic understanding of their strengths and limitations for diverse downstream scenarios. In a full-tuning scenario, we demonstrate that the specialist model achieves a 27% higher win score. Finally, we provide further analysis on generalization, fairness, attention visualizations, and the importance of training data choice.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.994189"
    },
    {
        "index": "#62",
        "title": "A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters",
        "link": "/arxiv/2510.14250",
        "arxiv_id": "2510.14250",
        "authors": "Lianzi Jiang, Jianxin Zhang, Xinyu Han, Huanhe Dong, Xiangrong Wang",
        "summary": "Accurate motion response prediction for elastic Bragg breakwaters is critical for their structural safety and operational integrity in marine environments. However, conventional deep learning models often exhibit limited generalization capabilities when presented with unseen sea states. These deficiencies stem from the neglect of natural decay observed in marine systems and inadequate modeling of wave-structure interaction (WSI). To overcome these challenges, this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network (PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module incorporates a learnable temporal decay to assign higher weights to recent states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase differences guided bidirectional cross-attention (PDG-BCA) module explicitly captures the bidirectional interaction and phase relationship between waves and the structure using a cosine-based bias within a bidirectional cross-computation paradigm. These streams are synergistically integrated through a global context fusion (GCF) module. Finally, PhysAttnNet is trained with a hybrid time-frequency loss that jointly minimizes time-domain prediction errors and frequency-domain spectral discrepancies. Comprehensive experiments on wave flume datasets demonstrate that PhysAttnNet significantly outperforms mainstream models. Furthermore,cross-scenario generalization tests validate the model's robustness and adaptability to unseen environments, highlighting its potential as a framework to develop predictive models for complex systems in ocean engineering.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.994471"
    },
    {
        "index": "#63",
        "title": "Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation",
        "link": "/arxiv/2510.14246",
        "arxiv_id": "2510.14246",
        "authors": "Jingwen Gu, Yiting He, Zhishuai Liu, Pan Xu",
        "summary": "Decision-making under distribution shift is a central challenge in reinforcement learning (RL), where training and deployment environments differ. We study this problem through the lens of robust Markov decision processes (RMDPs), which optimize performance against adversarial transition dynamics. Our focus is the online setting, where the agent has only limited interaction with the environment, making sample efficiency and exploration especially critical. Policy optimization, despite its success in standard RL, remains theoretically and empirically underexplored in robust RL. To bridge this gap, we propose \\textbf{D}istributionally \\textbf{R}obust \\textbf{R}egularized \\textbf{P}olicy \\textbf{O}ptimization algorithm (DR-RPO), a model-free online policy optimization method that learns robust policies with sublinear regret. To enable tractable optimization within the softmax policy class, DR-RPO incorporates reference-policy regularization, yielding RMDP variants that are doubly constrained in both transitions and policies. To scale to large state-action spaces, we adopt the $d$-rectangular linear MDP formulation and combine linear function approximation with an upper confidence bonus for optimistic exploration. We provide theoretical guarantees showing that policy optimization can achieve polynomial suboptimality bounds and sample efficiency in robust RL, matching the performance of value-based approaches. Finally, empirical results across diverse domains corroborate our theory and demonstrate the robustness of DR-RPO.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.994772"
    },
    {
        "index": "#64",
        "title": "Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models",
        "link": "/arxiv/2510.14232",
        "arxiv_id": "2510.14232",
        "authors": "Mehrzad Samadi, Aleksander Ficek, Sean Narenthiran, Siddhartha Jain, Wasi Uddin Ahmad, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg",
        "summary": "Competitive programming has become a rigorous benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). The International Olympiad in Informatics (IOI) stands out as one of the most prestigious annual competitions in competitive programming and has become a key benchmark for comparing human and AI-level programming ability. While several proprietary models have been claimed to achieve gold medal-level performance at the IOI, often with undisclosed methods, achieving comparable results with open-weight models remains a significant challenge. In this paper, we present \\gencluster, a scalable and reproducible test-time compute framework that attains IOI gold-level performance using open-weight models. It combines large-scale generation, behavioral clustering, ranking, and a round-robin submission strategy to efficiently explore diverse solution spaces under limited validation budgets. Our experiments show that the performance of our proposed approach scales consistently with available compute, narrowing the gap between open and closed systems. Notably, we will show that GenCluster can achieve a gold medal at IOI 2025 for the first time with an open-weight model gpt-oss-120b, setting a new benchmark for transparent and reproducible evaluation of reasoning in LLMs.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:04.995118"
    },
    {
        "index": "#65",
        "title": "When Flatness Does (Not) Guarantee Adversarial Robustness",
        "link": "/arxiv/2510.14231",
        "arxiv_id": "2510.14231",
        "authors": "Nils Philipp Walter, Linara Adilova, Jilles Vreeken, Michael Kamp",
        "summary": "Despite their empirical success, neural networks remain vulnerable to small, adversarial perturbations. A longstanding hypothesis suggests that flat minima, regions of low curvature in the loss landscape, offer increased robustness. While intuitive, this connection has remained largely informal and incomplete. By rigorously formalizing the relationship, we show this intuition is only partially correct: flatness implies local but not global adversarial robustness. To arrive at this result, we first derive a closed-form expression for relative flatness in the penultimate layer, and then show we can use this to constrain the variation of the loss in input space. This allows us to formally analyze the adversarial robustness of the entire network. We then show that to maintain robustness beyond a local neighborhood, the loss needs to curve sharply away from the data manifold. We validate our theoretical predictions empirically across architectures and datasets, uncovering the geometric structure that governs adversarial vulnerability, and linking flatness to model confidence: adversarial examples often lie in large, flat regions where the model is confidently wrong. Our results challenge simplified views of flatness and provide a nuanced understanding of its role in robustness.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.000585"
    },
    {
        "index": "#66",
        "title": "Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization",
        "link": "/arxiv/2510.14217",
        "arxiv_id": "2510.14217",
        "authors": "Asma Jamali, Tin Sum Cheng, Rodrigo A. Vargas-Hernández",
        "summary": "Understanding the spectral properties of kernels offers a principled perspective on generalization and representation quality. While deep models achieve state-of-the-art accuracy in molecular property prediction, kernel methods remain widely used for their robustness in low-data regimes and transparent theoretical grounding. Despite extensive studies of kernel spectra in machine learning, systematic spectral analyses of molecular kernels are scarce. In this work, we provide the first comprehensive spectral analysis of kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained transformer-based, global and local 3D representations across seven molecular properties. Surprisingly, richer spectral features, measured by four different spectral metrics, do not consistently improve accuracy. Pearson correlation tests further reveal that for transformer-based and local 3D representations, spectral richness can even have a negative correlation with performance. We also implement truncated kernels to probe the relationship between spectrum and predictive performance: in many kernels, retaining only the top 2% of eigenvalues recovers nearly all performance, indicating that the leading eigenvalues capture the most informative features. Our results challenge the common heuristic that \"richer spectra yield better generalization\" and highlight nuanced relationships between representation, kernel features, and predictive performance. Beyond molecular property prediction, these findings inform how kernel and self-supervised learning methods are evaluated in data-limited scientific and real-world tasks.",
        "subjects": "Machine Learning, Chemical Physics",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.000885"
    },
    {
        "index": "#67",
        "title": "Incentive-Based Federated Learning",
        "link": "/arxiv/2510.14208",
        "arxiv_id": "2510.14208",
        "authors": "Chanuka A. S. Hewa Kaluannakkage, Rajkumar Buyya",
        "summary": "Federated learning promises to revolutionize machine learning by enabling collaborative model training without compromising data privacy. However, practical adaptability can be limited by critical factors, such as the participation dilemma. Participating entities are often unwilling to contribute to a learning system unless they receive some benefits, or they may pretend to participate and free-ride on others. This chapter identifies the fundamental challenges in designing incentive mechanisms for federated learning systems. It examines how foundational concepts from economics and game theory can be applied to federated learning, alongside technology-driven solutions such as blockchain and deep reinforcement learning. This work presents a comprehensive taxonomy that thoroughly covers both centralized and decentralized architectures based on the aforementioned theoretical concepts. Furthermore, the concepts described are presented from an application perspective, covering emerging industrial applications, including healthcare, smart infrastructure, vehicular networks, and blockchain-based decentralized systems. Through this exploration, this chapter demonstrates that well-designed incentive mechanisms are not merely optional features but essential components for the practical success of federated learning. This analysis reveals both the promising solutions that have emerged and the significant challenges that remain in building truly sustainable, fair, and robust federated learning ecosystems.",
        "subjects": "Machine Learning, Distributed, Parallel, and Cluster Computing",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.001169"
    },
    {
        "index": "#68",
        "title": "Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation",
        "link": "/arxiv/2510.14190",
        "arxiv_id": "2510.14190",
        "authors": "Ruchi Sandilya, Sumaira Perez, Charles Lynch, Lindsay Victoria, Benjamin Zebley, Derrick Matthew Buchanan, Mahendra T. Bhati, Nolan Williams, Timothy J. Spellman, Faith M. Gunning, Conor Liston, Logan Grosenick",
        "summary": "Diffusion models excel at generation, but their latent spaces are not explicitly organized for interpretable control. We introduce ConDA (Contrastive Diffusion Alignment), a framework that applies contrastive learning within diffusion embeddings to align latent geometry with system dynamics. Motivated by recent advances showing that contrastive objectives can recover more disentangled and structured representations, ConDA organizes diffusion latents such that traversal directions reflect underlying dynamical factors. Within this contrastively structured space, ConDA enables nonlinear trajectory traversal that supports faithful interpolation, extrapolation, and controllable generation. Across benchmarks in fluid dynamics, neural calcium imaging, therapeutic neurostimulation, and facial expression, ConDA produces interpretable latent representations with improved controllability compared to linear traversals and conditioning-based baselines. These results suggest that diffusion latents encode dynamics-relevant structure, but exploiting this structure requires latent organization and traversal along the latent manifold.",
        "subjects": "Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.001525"
    },
    {
        "index": "#69",
        "title": "MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation",
        "link": "/arxiv/2510.14184",
        "arxiv_id": "2510.14184",
        "authors": "Mahmood Hegazy, Aaron Rodrigues, Azzam Naeem",
        "summary": "We present MAFA (Multi-Agent Framework for Annotation), a production-deployed system that transforms enterprise-scale annotation workflows through configurable multi-agent collaboration. Addressing the critical challenge of annotation backlogs in financial services, where millions of customer utterances require accurate categorization, MAFA combines specialized agents with structured reasoning and a judge-based consensus mechanism. Our framework uniquely supports dynamic task adaptation, allowing organizations to define custom annotation types (FAQs, intents, entities, or domain-specific categories) through configuration rather than code changes. Deployed at JP Morgan Chase, MAFA has eliminated a 1 million utterance backlog while achieving, on average, 86% agreement with human annotators, annually saving over 5,000 hours of manual annotation work. The system processes utterances with annotation confidence classifications, which are typically 85% high, 10% medium, and 5% low across all datasets we tested. This enables human annotators to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's effectiveness across multiple datasets and languages, showing consistent improvements over traditional and single-agent annotation baselines: 13.8% higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1 in our internal intent classification dataset and similar gains on public benchmarks. This work bridges the gap between theoretical multi-agent systems and practical enterprise deployment, providing a blueprint for organizations facing similar annotation challenges.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.001822"
    },
    {
        "index": "#70",
        "title": "Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming",
        "link": "/arxiv/2510.14168",
        "arxiv_id": "2510.14168",
        "authors": "Guan-Horng Liu, Tianrong Chen, Evangelos A. Theodorou",
        "summary": "Optimization of deep neural networks (DNNs) has been a driving force in the advancement of modern machine learning and artificial intelligence. With DNNs characterized by a prolonged sequence of nonlinear propagation, determining their optimal parameters given an objective naturally fits within the framework of Optimal Control Programming. Such an interpretation of DNNs as dynamical systems has proven crucial in offering a theoretical foundation for principled analysis from numerical equations to physics. In parallel to these theoretical pursuits, this paper focuses on an algorithmic perspective. Our motivated observation is the striking algorithmic resemblance between the Backpropagation algorithm for computing gradients in DNNs and the optimality conditions for dynamical systems, expressed through another backward process known as dynamic programming. Consolidating this connection, where Backpropagation admits a variational structure, solving an approximate dynamic programming up to the first-order expansion leads to a new class of optimization methods exploring higher-order expansions of the Bellman equation. The resulting optimizer, termed Optimal Control Theoretic Neural Optimizer (OCNOpt), enables rich algorithmic opportunities, including layer-wise feedback policies, game-theoretic applications, and higher-order training of continuous-time models such as Neural ODEs. Extensive experiments demonstrate that OCNOpt improves upon existing methods in robustness and efficiency while maintaining manageable computational complexity, paving new avenues for principled algorithmic design grounded in dynamical systems and optimal control theory.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.002112"
    },
    {
        "index": "#71",
        "title": "Towards Reversible Model Merging For Low-rank Weights",
        "link": "/arxiv/2510.14163",
        "arxiv_id": "2510.14163",
        "authors": "Mohammadsajad Alipour, Mohammad Mohammadi Amiri",
        "summary": "Model merging aims to combine multiple fine-tuned models into a single set of weights that performs well across all source tasks. While prior work has shown that merging can approximate the performance of individual fine-tuned models for each task, it largely overlooks scenarios where models are compressed into low-rank representations, either through low-rank adaptation (LoRA) or post-training singular value decomposition (SVD). We first demonstrate that applying conventional merging methods to low-rank weights leads to severe performance degradation in the merged model. Motivated by this phenomenon, we propose a fundamentally different approach: instead of collapsing all adapters into one set of weights, we construct a compact basis (e.g., an equivalent of holding two or more models) from which original task-specific models can be recovered via linear combination. This reframes merging as generating a reconstruction-capable model space rather than producing a single merged model. Crucially, this allows us to ``revert'' to each individual model when needed, recognizing that no merged model can consistently outperform one specialized for its task. Building on this insight, we introduce our method, Reversible Model Merging (RMM), an efficient, data-free, and flexible method that provides a closed-form solution for selecting the optimal basis of model weights and task-specific coefficients for linear combination. Extensive experiments across diverse datasets and model scales demonstrate that RMM consistently outperforms existing merging approaches, preserving the performance of low-rank compressed models by a significant margin.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.002408"
    },
    {
        "index": "#72",
        "title": "Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods",
        "link": "/arxiv/2510.14161",
        "arxiv_id": "2510.14161",
        "authors": "Matthew D. Merris, Tim Andersen",
        "summary": "In the evolving domains of Machine Learning and Data Analytics, existing dataset characterization methods such as statistical, structural, and model-based analyses often fail to deliver the deep understanding and insights essential for innovation and explainability. This work surveys the current state-of-the-art conventional data analytic techniques and examines their limitations, and discusses a variety of tensor-based methods and how these may provide a more robust alternative to traditional statistical, structural, and model-based dataset characterization techniques. Through examples, we illustrate how tensor methods unveil nuanced data characteristics, offering enhanced interpretability and actionable intelligence. We advocate for the adoption of tensor-based characterization, promising a leap forward in understanding complex datasets and paving the way for intelligent, explainable data-driven discoveries.",
        "subjects": "Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.002666"
    },
    {
        "index": "#73",
        "title": "On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model",
        "link": "/arxiv/2510.14156",
        "arxiv_id": "2510.14156",
        "authors": "Jan Kwiatkowski, Jarosław A. Chudziak",
        "summary": "Quantitative trading strategies rely on accurately ranking stocks to identify profitable investments. Effective portfolio management requires models that can reliably order future stock returns. Transformer models are promising for understanding financial time series, but how different training loss functions affect their ability to rank stocks well is not yet fully understood. Financial markets are challenging due to their changing nature and complex relationships between stocks. Standard loss functions, which aim for simple prediction accuracy, often aren't enough. They don't directly teach models to learn the correct order of stock returns. While many advanced ranking losses exist from fields such as information retrieval, there hasn't been a thorough comparison to see how well they work for ranking financial returns, especially when used with modern Transformer models for stock selection. This paper addresses this gap by systematically evaluating a diverse set of advanced loss functions including pointwise, pairwise, listwise for daily stock return forecasting to facilitate rank-based portfolio selection on S&P 500 data. We focus on assessing how each loss function influences the model's ability to discern profitable relative orderings among assets. Our research contributes a comprehensive benchmark revealing how different loss functions impact a model's ability to learn cross-sectional and temporal patterns crucial for portfolio selection, thereby offering practical guidance for optimizing ranking-based trading strategies.",
        "subjects": "Machine Learning, Portfolio Management",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.002939"
    },
    {
        "index": "#74",
        "title": "Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks",
        "link": "/arxiv/2510.14139",
        "arxiv_id": "2510.14139",
        "authors": "Islam Akef Ebeid, Haoteng Tang, Pengfei Gu",
        "summary": "Introduction Accurate prediction of protein-protein interactions (PPIs) is crucial for understanding cellular functions and advancing drug development. Existing in-silico methods use direct sequence embeddings from Protein Language Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein structures. This study explores less computationally intensive alternatives. We introduce a novel framework for downstream PPI prediction through link prediction. Methods We introduce a two-stage graph representation learning framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach models a protein's primary structure as a hierarchy of globally inferred n-gram graphs. In these graphs, residue transition probabilities define edge weights. Each edge connects a pair of residues in a directed graph. The probabilities are aggregated from a large corpus of sequences. Second, we propose DirectGCN, a custom directed graph convolutional neural network. This model features a unique convolutional layer. It processes information through separate path-specific transformations: incoming, outgoing, and undirected. A shared transformation is also applied. These paths are combined via a learnable gating mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level embeddings. These embeddings are pooled via attention to generate protein-level embeddings for prediction. Results We first established the efficacy of DirectGCN on standard node classification benchmarks. Its performance matches established methods on general datasets. The model excels at complex, directed graphs with dense, heterophilic structures. When applied to PPI prediction, the full ProtGram-DirectGCN framework delivers robust predictive power. This strong performance holds even with limited training data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.003234"
    },
    {
        "index": "#75",
        "title": "Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks",
        "link": "/arxiv/2510.14137",
        "arxiv_id": "2510.14137",
        "authors": "Faezeh Dehghan Tarzjani, Bhaskar Krishnamachari",
        "summary": "The p-persistent CSMA protocol is central to random-access MAC analysis, but predicting saturation throughput in heterogeneous multi-hop wireless networks remains a hard problem. Simplified models that assume a single, shared interference domain can underestimate throughput by 48--62\\% in sparse topologies. Exact Markov-chain analyses are accurate but scale exponentially in computation time, making them impractical for large networks. These computational barriers motivate structural machine learning approaches like GNNs for scalable throughput prediction in general network topologies. Yet off-the-shelf GNNs struggle here: a standard GCN yields 63.94\\% normalized mean absolute error (NMAE) on heterogeneous networks because symmetric normalization conflates a node's direct interference with higher-order, cascading effects that pertain to how interference propagates over the network graph. Building on these insights, we propose the Decoupled Graph Convolutional Network (D-GCN), a novel architecture that explicitly separates processing of a node's own transmission probability from neighbor interference effects. D-GCN replaces mean aggregation with learnable attention, yielding interpretable, per-neighbor contribution weights while capturing complex multihop interference patterns. D-GCN attains 3.3\\% NMAE, outperforms strong baselines, remains tractable even when exact analytical methods become computationally infeasible, and enables gradient-based network optimization that achieves within 1\\% of theoretical optima.",
        "subjects": "Machine Learning, Systems and Control",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.003505"
    },
    {
        "index": "#76",
        "title": "Demystifying the Mechanisms Behind Emergent Exploration in Goal-conditioned RL",
        "link": "/arxiv/2510.14129",
        "arxiv_id": "2510.14129",
        "authors": "Mahsa Bastankhah, Grace Liu, Dilip Arumugam, Thomas L. Griffiths, Benjamin Eysenbach",
        "summary": "In this work, we take a first step toward elucidating the mechanisms behind emergent exploration in unsupervised reinforcement learning. We study Single-Goal Contrastive Reinforcement Learning (SGCRL), a self-supervised algorithm capable of solving challenging long-horizon goal-reaching tasks without external rewards or curricula. We combine theoretical analysis of the algorithm's objective function with controlled experiments to understand what drives its exploration. We show that SGCRL maximizes implicit rewards shaped by its learned representations. These representations automatically modify the reward landscape to promote exploration before reaching the goal and exploitation thereafter. Our experiments also demonstrate that these exploration dynamics arise from learning low-rank representations of the state space rather than from neural network function approximation. Our improved understanding enables us to adapt SGCRL to perform safety-aware exploration.",
        "subjects": "Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.003812"
    },
    {
        "index": "#77",
        "title": "Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants",
        "link": "/arxiv/2510.14125",
        "arxiv_id": "2510.14125",
        "authors": "Waqar Muhammad Ashraf, Talha Ansar, Abdulelah S. Alshehri, Peipei Chen, Ramit Debnath, Vivek Dua",
        "summary": "We introduce a neural network-driven robust optimisation framework that integrates data-driven domain as a constraint into the nonlinear programming technique, addressing the overlooked issue of domain-inconsistent solutions arising from the interaction of parametrised neural network models with optimisation solvers. Applied to a 1180 MW capacity combined cycle gas power plant, our framework delivers domain-consistent robust optimal solutions that achieve a verified 0.76 percentage point mean improvement in energy efficiency. For the first time, scaling this efficiency gain to the global fleet of gas power plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with 10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results underscore the synergetic role of machine learning in delivering near-term, scalable decarbonisation pathways for global climate action.",
        "subjects": "Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.004107"
    },
    {
        "index": "#78",
        "title": "Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey",
        "link": "/arxiv/2510.14114",
        "arxiv_id": "2510.14114",
        "authors": "Yazid Janati, Alain Durmus, Jimmy Olsson, Eric Moulines",
        "summary": "Diffusion models enable the synthesis of highly accurate samples from complex distributions and have become foundational in generative modeling. Recently, they have demonstrated significant potential for solving Bayesian inverse problems by serving as priors. This review offers a comprehensive overview of current methods that leverage \\emph{pre-trained} diffusion models alongside Monte Carlo methods to address Bayesian inverse problems without requiring additional training. We show that these methods primarily employ a \\emph{twisting} mechanism for the intermediate distributions within the diffusion process, guiding the simulations toward the posterior distribution. We describe how various Monte Carlo methods are then used to aid in sampling from these twisted distributions.",
        "subjects": "Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.004398"
    },
    {
        "index": "#79",
        "title": "Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets",
        "link": "/arxiv/2510.14097",
        "arxiv_id": "2510.14097",
        "authors": "Zixian Yang, Sushil Mahavir Varma, Lei Ying",
        "summary": "We study a two-sided market, wherein, price-sensitive heterogeneous customers and servers arrive and join their respective queues. A compatible customer-server pair can then be matched by the platform, at which point, they leave the system. Our objective is to design pricing and matching algorithms that maximize the platform's profit, while maintaining reasonable queue lengths. As the demand and supply curves governing the price-dependent arrival rates may not be known in practice, we design a novel online-learning-based pricing policy and establish its near-optimality. In particular, we prove a tradeoff among three performance metrics: $\\tilde{O}(T^{1-\\gamma})$ regret, $\\tilde{O}(T^{\\gamma/2})$ average queue length, and $\\tilde{O}(T^{\\gamma})$ maximum queue length for $\\gamma \\in (0, 1/6]$, significantly improving over existing results [1]. Moreover, barring the permissible range of $\\gamma$, we show that this trade-off between regret and average queue length is optimal up to logarithmic factors under a class of policies, matching the optimal one as in [2] which assumes the demand and supply curves to be known. Our proposed policy has two noteworthy features: a dynamic component that optimizes the tradeoff between low regret and small queue lengths; and a probabilistic component that resolves the tension between obtaining useful samples for fast learning and maintaining small queue lengths.",
        "subjects": "Machine Learning, Computer Science and Game Theory, Optimization and Control, Probability",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.004705"
    },
    {
        "index": "#80",
        "title": "TENDE: Transfer Entropy Neural Diffusion Estimation",
        "link": "/arxiv/2510.14096",
        "arxiv_id": "2510.14096",
        "authors": "Simon Pedro Galeano Munoz, Mustapha Bounoua, Giulio Franzese, Pietro Michiardi, Maurizio Filippone",
        "summary": "Transfer entropy measures directed information flow in time series, and it has become a fundamental quantity in applications spanning neuroscience, finance, and complex systems analysis. However, existing estimation methods suffer from the curse of dimensionality, require restrictive distributional assumptions, or need exponentially large datasets for reliable convergence. We address these limitations in the literature by proposing TENDE (Transfer Entropy Neural Diffusion Estimation), a novel approach that leverages score-based diffusion models to estimate transfer entropy through conditional mutual information. By learning score functions of the relevant conditional distributions, TENDE provides flexible, scalable estimation while making minimal assumptions about the underlying data-generating process. We demonstrate superior accuracy and robustness compared to existing neural estimators and other state-of-the-art approaches across synthetic benchmarks and real data.",
        "subjects": "Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.004990"
    },
    {
        "index": "#81",
        "title": "Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning",
        "link": "/arxiv/2510.14095",
        "arxiv_id": "2510.14095",
        "authors": "Awni Altabaa, Siyu Chen, John Lafferty, Zhuoran Yang",
        "summary": "Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning -- and a critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed. We introduce and explore a set of four architectural mechanisms aimed at enhancing OOD generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.005281"
    },
    {
        "index": "#82",
        "title": "Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations",
        "link": "/arxiv/2510.14094",
        "arxiv_id": "2510.14094",
        "authors": "Haotian Feng",
        "summary": "Reaction-diffusion systems represent one of the most fundamental formulations used to describe a wide range of physical, chemical, and biological processes. With the increasing adoption of neural networks, recent research has focused on solving differential equations using machine learning techniques. However, the theoretical foundation explaining why neural networks can effectively approximate such solutions remains insufficiently explored. This paper provides a theoretical analysis of the approximation power of neural networks for one- and two-dimensional reaction-diffusion equations in both homogeneous and heterogeneous media. Building upon the universal approximation theorem, we demonstrate that a two-layer neural network can approximate the one-dimensional reaction-diffusion equation, while a three-layer neural network can approximate its two-dimensional counterpart. The theoretical framework presented here can be further extended to elliptic and parabolic equations. Overall, this work highlights the expressive power of neural networks in approximating solutions to reaction-diffusion equations and related PDEs, providing a theoretical foundation for neural network-based differential equation solvers.",
        "subjects": "Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.005528"
    },
    {
        "index": "#83",
        "title": "Exploratory Causal Inference in SAEnce",
        "link": "/arxiv/2510.14073",
        "arxiv_id": "2510.14073",
        "authors": "Tommaso Mencattini, Riccardo Cadei, Francesco Locatello",
        "summary": "Randomized Controlled Trials are one of the pillars of science; nevertheless, they rely on hand-crafted hypotheses and expensive analysis. Such constraints prevent causal effect estimation at scale, potentially anchoring on popular yet incomplete hypotheses. We propose to discover the unknown effects of a treatment directly from data. For this, we turn unstructured data from a trial into meaningful representations via pretrained foundation models and interpret them via a sparse autoencoder. However, discovering significant causal effects at the neural level is not trivial due to multiple-testing issues and effects entanglement. To address these challenges, we introduce Neural Effect Search, a novel recursive procedure solving both issues by progressive stratification. After assessing the robustness of our algorithm on semi-synthetic experiments, we showcase, in the context of experimental ecology, the first successful unsupervised causal effect identification on a real-world scientific trial.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.011019"
    },
    {
        "index": "#84",
        "title": "On the expressivity of sparse maxout networks",
        "link": "/arxiv/2510.14068",
        "arxiv_id": "2510.14068",
        "authors": "Moritz Grillo, Tobias Hofmann",
        "summary": "We study the expressivity of sparse maxout networks, where each neuron takes a fixed number of inputs from the previous layer and employs a, possibly multi-argument, maxout activation. This setting captures key characteristics of convolutional or graph neural networks. We establish a duality between functions computable by such networks and a class of virtual polytopes, linking their geometry to questions of network expressivity. In particular, we derive a tight bound on the dimension of the associated polytopes, which serves as the central tool for our analysis. Building on this, we construct a sequence of depth hierarchies. While sufficiently deep sparse maxout networks are universal, we prove that if the required depth is not reached, width alone cannot compensate for the sparsity of a fixed indegree constraint.",
        "subjects": "Machine Learning, Artificial Intelligence, Combinatorics",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.011324"
    },
    {
        "index": "#85",
        "title": "FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients",
        "link": "/arxiv/2510.14054",
        "arxiv_id": "2510.14054",
        "authors": "Fatih Ilhan, Selim Furkan Tekin, Tiansheng Huang, Gaowen Liu, Ramana Kompella, Greg Eisenhauer, Yingyan Celine Lin, Calton Pu, Ling Liu",
        "summary": "Fine-tuning pre-trained large language models (LLMs) has become a common practice for personalized natural language understanding (NLU) applications on downstream tasks and domain-specific datasets. However, there are two main challenges: (i) limited and/or heterogeneous data for fine-tuning due to proprietary data confidentiality or privacy requirements, and (ii) varying computation resources available across participating clients such as edge devices. This paper presents FedHFT - an efficient and personalized federated fine-tuning framework to address both challenges. First, we introduce a mixture of masked adapters to handle resource heterogeneity across participating clients, enabling high-performance collaborative fine-tuning of pre-trained language model(s) across multiple clients in a distributed setting, while keeping proprietary data local. Second, we introduce a bi-level optimization approach to handle non-iid data distribution based on masked personalization and client clustering. Extensive experiments demonstrate significant performance and efficiency improvements over various natural language understanding tasks under data and resource heterogeneity compared to representative heterogeneous federated learning methods.",
        "subjects": "Machine Learning, Distributed, Parallel, and Cluster Computing",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.011668"
    },
    {
        "index": "#86",
        "title": "CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations",
        "link": "/arxiv/2510.14049",
        "arxiv_id": "2510.14049",
        "authors": "Guangyi Chen, Yunlong Deng, Peiyuan Zhu, Yan Li, Yifan Sheng, Zijian Li, Kun Zhang",
        "summary": "Causal Representation Learning (CRL) aims to uncover the data-generating process and identify the underlying causal variables and relations, whose evaluation remains inherently challenging due to the requirement of known ground-truth causal variables and causal structure. Existing evaluations often rely on either simplistic synthetic datasets or downstream performance on real-world tasks, generally suffering a dilemma between realism and evaluative precision. In this paper, we introduce a new benchmark for CRL using high-fidelity simulated visual data that retains both realistic visual complexity and, more importantly, access to ground-truth causal generating processes. The dataset comprises around 200 thousand images and 3 million video frames across 24 sub-scenes in four domains: static image generation, dynamic physical simulations, robotic manipulations, and traffic situation analysis. These scenarios range from static to dynamic settings, simple to complex structures, and single to multi-agent interactions, offering a comprehensive testbed that hopefully bridges the gap between rigorous evaluation and real-world applicability. In addition, we provide flexible access to the underlying causal structures, allowing users to modify or configure them to align with the required assumptions in CRL, such as available domain labels, temporal dependencies, or intervention histories. Leveraging this benchmark, we evaluated representative CRL methods across diverse paradigms and offered empirical insights to assist practitioners and newcomers in choosing or extending appropriate CRL frameworks to properly address specific types of real problems that can benefit from the CRL perspective. Welcome to visit our: Project page:https://causal-verse.github.io/, Dataset:https://huggingface.co/CausalVerse.",
        "subjects": "Machine Learning, Mathematical Software",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.012020"
    },
    {
        "index": "#87",
        "title": "Context-Selective State Space Models: Feedback is All You Need",
        "link": "/arxiv/2510.14027",
        "arxiv_id": "2510.14027",
        "authors": "Riccardo Zattra, Giacomo Baggio, Umberto Casti, Augusto Ferrante, Francesco Ticozzi",
        "summary": "Transformers, powered by the attention mechanism, are the backbone of most foundation models, yet they suffer from quadratic complexity and difficulties in dealing with long-range dependencies in the input sequence. Recent work has shown that state space models (SSMs) provide an efficient alternative, with the S6 module at the core of the Mamba architecture achieving state-of-the-art results on long-sequence benchmarks. In this paper, we introduce the COFFEE (COntext From FEEdback) model, a novel time-varying SSM that incorporates state feedback to enable context-dependent selectivity, while still allowing for parallel implementation. Whereas the selectivity mechanism of S6 only depends on the current input, COFFEE computes it from the internal state, which serves as a compact representation of the sequence history. This shift allows the model to regulate its dynamics based on accumulated context, improving its ability to capture long-range dependencies. In addition to state feedback, we employ an efficient model parametrization that removes redundancies present in S6 and leads to a more compact and trainable formulation. On the induction head task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer parameters and training sequences compared to S6. On MNIST, COFFEE largely outperforms S6 within the same architecture, reaching 97% accuracy with only 3585 parameters. These results showcase the role of state feedback as a key mechanism for building scalable and efficient sequence models.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.012333"
    },
    {
        "index": "#88",
        "title": "Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training",
        "link": "/arxiv/2510.14009",
        "arxiv_id": "2510.14009",
        "authors": "Jie Hao, Xiaochuan Gong, Jie Xu, Zhengdao Wang, Mingrui Liu",
        "summary": "Geometry-aware optimization algorithms, such as Muon, have achieved remarkable success in training deep neural networks (DNNs). These methods leverage the underlying geometry of DNNs by selecting appropriate norms for different layers and updating parameters via norm-constrained linear minimization oracles (LMOs). However, even within a group of layers associated with the same norm, the local curvature can be heterogeneous across layers and vary dynamically over the course of training. For example, recent work shows that sharpness varies substantially across transformer layers and throughout training, yet standard geometry-aware optimizers impose fixed learning rates to layers within the same group, which may be inefficient for DNN training. In this paper, we introduce a noise-adaptive layerwise learning rate scheme on top of geometry-aware optimization algorithms and substantially accelerate DNN training compared to methods that use fixed learning rates within each group. Our method estimates gradient variance in the dual norm induced by the chosen LMO on the fly, and uses it to assign time-varying noise-adaptive layerwise learning rates within each group. We provide a theoretical analysis showing that our algorithm achieves a sharp convergence rate. Empirical results on transformer architectures such as LLaMA and GPT demonstrate that our approach achieves faster convergence than state-of-the-art optimizers.",
        "subjects": "Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.012637"
    },
    {
        "index": "#89",
        "title": "Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling",
        "link": "/arxiv/2510.14007",
        "arxiv_id": "2510.14007",
        "authors": "Bálint László Szarvas, Maksim Zhdanov",
        "summary": "Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows incorporating equivariance to arbitrary pseudo-Euclidean groups, including isometries of Euclidean space and Minkowski spacetime. In this work, we demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the model expressivity. To address this issue, we propose Conditional Clifford-Steerable Kernels, which augment the kernels with equivariant representations computed from the input feature field. We derive the equivariance constraint for these input-dependent kernels and show how it can be solved efficiently via implicit parameterization. We empirically demonstrate an improved expressivity of the resulting framework on multiple PDE forecasting tasks, including fluid dynamics and relativistic electrodynamics, where our method consistently outperforms baseline methods.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.012904"
    },
    {
        "index": "#90",
        "title": "REAP the Experts: Why Pruning Prevails for One-Shot MoE compression",
        "link": "/arxiv/2510.13999",
        "arxiv_id": "2510.13999",
        "authors": "Mike Lasby, Ivan Lazarevich, Nish Sinnadurai, Sean Lie, Yani Ioannou, Vithursan Thangarasa",
        "summary": "Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient pre-training and low latency but their large parameter counts create significant memory overhead, motivating research into expert compression. Contrary to recent findings favouring expert merging on discriminative benchmarks, we demonstrate that expert pruning is a superior strategy for generative tasks. We prove that merging introduces an irreducible error by causing a \"functional subspace collapse\", due to the loss of the router's independent, input-dependent control over experts. Leveraging this insight, we propose Router-weighted Expert Activation Pruning (REAP), a novel pruning criterion that considers both router gate-values and expert activation norms. Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP consistently outperforms merging and other pruning methods on generative benchmarks, especially at 50% compression. Notably, our method achieves near-lossless compression on code generation and tool-calling tasks with Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.013213"
    },
    {
        "index": "#91",
        "title": "BitNet Distillation",
        "link": "/arxiv/2510.13998",
        "arxiv_id": "2510.13998",
        "authors": "Xun Wu, Shaohan Huang, Wenhui Wang, Ting Song, Li Dong, Yan Xia, Furu Wei",
        "summary": "In this paper, we present BitNet Distillation (BitDistill), a lightweight pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into 1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream tasks, achieving strong task-specific performance with minimal computational cost. Specifically, BitDistill incorporates three key techniques: the SubLN module, as introduced in BitNet; multi-head attention distillation, based on MiniLM; and continual pre-training, which serves as a crucial warm-up step to mitigate the scalability issue of the performance gap between finetuned full-precision and 1.58-bit LLMs on specific tasks. Experimental results show that BitDistill achieves performance comparable to the full-precision counterpart models across model size, while enabling up to 10x memory savings and 2.65x faster inference on CPUs. Code is available at https://github.com/microsoft/BitNet.",
        "subjects": "Machine Learning, Computation and Language",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.013539"
    },
    {
        "index": "#92",
        "title": "Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems",
        "link": "/arxiv/2510.13972",
        "arxiv_id": "2510.13972",
        "authors": "George Webber, Andrew J. Reader",
        "summary": "Recovering true signals from noisy measurements is a central challenge in inverse problems spanning medical imaging, geophysics, and signal processing. Current solutions balance prior assumptions regarding the true signal (regularization) with agreement to noisy measured data (data-fidelity). Conventional data-fidelity loss functions, such as mean-squared error (MSE) or negative log-likelihood, seek pointwise agreement with noisy measurements, often leading to overfitting to noise. In this work, we instead evaluate data-fidelity collectively by testing whether the observed measurements are statistically consistent with the noise distributions implied by the current estimate. We adopt this aggregated perspective and introduce distributional consistency (DC) loss, a data-fidelity objective that replaces pointwise matching with distribution-level calibration using model-based probability scores for each measurement. DC loss acts as a direct and practical plug-in replacement for standard data consistency terms: i) it is compatible with modern regularizers, ii) it is optimized in the same way as traditional losses, and iii) it avoids overfitting to measurement noise even without the use of priors. Its scope naturally fits many practical inverse problems where the measurement-noise distribution is known and where the measured dataset consists of many independent noisy values. We demonstrate efficacy in two key example application areas: i) in image denoising with deep image prior, using DC instead of MSE loss removes the need for early stopping and achieves higher PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss reduces artifacts in highly-iterated reconstructions and enhances the efficacy of hand-crafted regularization. These results position DC loss as a statistically grounded, performance-enhancing alternative to conventional fidelity losses for inverse problems.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition, Medical Physics",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.013826"
    },
    {
        "index": "#93",
        "title": "LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding",
        "link": "/arxiv/2510.13922",
        "arxiv_id": "2510.13922",
        "authors": "Mohammad Mansoori, Amira Soliman, Farzaneh Etminani",
        "summary": "Clinical notes contain unstructured text provided by clinicians during patient encounters. These notes are usually accompanied by a sequence of diagnostic codes following the International Classification of Diseases (ICD). Correctly assigning and ordering ICD codes are essential for medical diagnosis and reimbursement. However, automating this task remains challenging. State-of-the-art methods treated this problem as a classification task, leading to ignoring the order of ICD codes that is essential for different purposes. In this work, as a first attempt, we approach this task from a retrieval system perspective to consider the order of codes, thus formulating this problem as a classification and ranking task. Our results and analysis show that the proposed framework has a superior ability to identify high-priority codes compared to other methods. For instance, our model accuracy in correctly ranking primary diagnosis codes is 47%, compared to 20% for the state-of-the-art classifier. Additionally, in terms of classification metrics, the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904, respectively, surpassing the previous best model with scores of 0.597 and 0.2660.",
        "subjects": "Machine Learning, Computation and Language, Information Retrieval",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.014112"
    },
    {
        "index": "#94",
        "title": "Weight Weaving: Parameter Pooling for Data-Free Model Merging",
        "link": "/arxiv/2510.13921",
        "arxiv_id": "2510.13921",
        "authors": "Levy Chaves, Eduardo Valle, Sandra Avila",
        "summary": "Model merging provides a cost-effective and data-efficient combination of specialized deep neural networks through parameter integration. This technique leverages expert models across downstream tasks without requiring retraining. Most model merging approaches critically depend on scaling hyper-parameters $\\lambda$, which weight each model's contribution globally or individually. Principled approaches for setting scaling factors without accessing any data (data-free) are scarce, often leading researchers to tune $\\lambda$ using privileged data from the evaluation set, which is obviously unfeasible in practice. To address this limitation, we introduce Weight Weaving, a plug-and-play technique that pools model weights across $\\lambda$ values search space using user-defined pooling functions, such as averaging, random selection, or even existing model merging methods. Our method demonstrates high modularity, imposing minimal constraints on the search space. It operates orthogonally to existing model merging methods and eliminates evaluation data requirements. We validate Weight Weaving across three ViT variants in three experimental setups: vision multi-task learning, vision continual learning, and domain generalization. Our method consistently improves the performance of several model merging methods, achieving average accuracy gains of up to 15.9 percentage points in a data-free setting.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.014403"
    },
    {
        "index": "#95",
        "title": "Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity",
        "link": "/arxiv/2510.13917",
        "arxiv_id": "2510.13917",
        "authors": "Yanshan Xiao, Kaihong Wu, Bo Liu",
        "summary": "Label distribution learning (LDL) is a paradigm that each sample is associated with a label distribution. At present, the existing approaches are proposed for the single-view LDL problem with labeled data, while the multi-view LDL problem with labeled and unlabeled data has not been considered. In this paper, we put forward the multi-view semi-supervised label distribution learning with local structure complementarity (MVSS-LDL) approach, which exploits the local nearest neighbor structure of each view and emphasizes the complementarity of local nearest neighbor structures in multiple views. Specifically speaking, we first explore the local structure of view $v$ by computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set of each sample $\\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this $k$-nearest neighbor set describes only a part of the nearest neighbor information of sample $\\boldsymbol{x}_i$. In order to obtain a more comprehensive description of sample $\\boldsymbol{x}_i$'s nearest neighbors, we complement the nearest neighbor set in view $v$ by incorporating sample $\\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the complemented nearest neighbor set in each view, a graph learning-based multi-view semi-supervised LDL model is constructed. By considering the complementarity of local nearest neighbor structures, different views can mutually provide the local structural information to complement each other. To the best of our knowledge, this is the first attempt at multi-view LDL. Numerical studies have demonstrated that MVSS-LDL attains explicitly better classification performance than the existing single-view LDL methods.",
        "subjects": "Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.014674"
    },
    {
        "index": "#96",
        "title": "K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding",
        "link": "/arxiv/2510.13891",
        "arxiv_id": "2510.13891",
        "authors": "Yifeng Yao, Yike Yun, Jing Wang, Huishuai Zhang, Dongyan Zhao, Ke Tian, Zhihao Wang, Minghui Qiu, Tao Wang",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant capabilities in image understanding, but long-video are constrained by context windows and computational cost. Uniform frame sampling often leads to substantial information loss. Meanwhile existing keyframe selection methods such as text-frame retrieval or RL-based frame optimization typically yield sparse and temporally disjointed frames, overlooking scene continuity and lacking flexibility for multi-scale frame selection. To address these limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe selection that preserves temporal continuity. Instead of selecting individual frames, K-frames predicts semantically coherent, query-relevant clips, which enables any-k keyframes selection to meet diverse user budgets. To achieve this approach, we first introduce PeakClips, a dataset of 200K video highlights conditioned by query. Building on this dataset, K-frames learns clip2frame selection using a three-stage progressive curriculum. It involves two Supervised Fine-Tuning stages for temporal grounding and key-clip perception, followed by a Reinforcement Learning stage that directly optimizes the scene-driven prediction policy for downstream task without further annotations. Extensive experiments on major long-video understanding benchmarks demonstrate that K-frames provides an effective, interpretable, and plug-and-play solution for keyframe selection at various scales. Our dataset and model will be available.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.015012"
    },
    {
        "index": "#97",
        "title": "Joint Discriminative-Generative Modeling via Dual Adversarial Training",
        "link": "/arxiv/2510.13872",
        "arxiv_id": "2510.13872",
        "authors": "Xuwang Yin, Claire Zhang, Julie Steele, Nir Shavit, Tony T. Wang",
        "summary": "Simultaneously achieving robust classification and high-fidelity generative modeling within a single framework presents a significant challenge. Hybrid approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as EBMs but are often limited by the instability and poor sample quality inherent in SGLD-based training. We address these limitations by proposing a novel training framework that integrates adversarial training (AT) principles for both discriminative robustness and stable generative learning. The proposed method introduces three key innovations: (1) the replacement of SGLD-based JEM learning with a stable, AT-based approach that optimizes the energy function by discriminating between real data and PGD-generated contrastive samples using the BCE loss; (2) synergistic adversarial training for the discriminative component that enhances classification robustness while eliminating the need for explicit gradient penalties; and (3) a two-stage training procedure to resolve the incompatibility between batch normalization and EBM training. Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method substantially improves adversarial robustness over existing hybrid models while maintaining competitive generative performance. On ImageNet, when optimized for generative modeling, our model's generative fidelity surpasses that of BigGAN and approaches diffusion models, representing the first MCMC-based EBM approach to achieve high-quality generation on complex, high-resolution datasets. Our approach addresses key stability issues that have limited JEM scaling and demonstrates that adversarial training can serve as an effective foundation for unified frameworks capable of generating and robustly classifying visual data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.015310"
    },
    {
        "index": "#98",
        "title": "CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks",
        "link": "/arxiv/2510.13869",
        "arxiv_id": "2510.13869",
        "authors": "Munsif Ali, Leonardo Rossi, Massimo Bertozzi",
        "summary": "Continual learning (CL) in the context of Generative Adversarial Networks (GANs) remains a challenging problem, particularly when it comes to learn from a few-shot (FS) samples without catastrophic forgetting. Current most effective state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible quantity of new weights at each training iteration, which would become significant when considering the long term. For this reason, this paper introduces \\textcolor{red}{\\textbf{\\underline{c}}}ontinual few-sh\\textcolor{red}{\\textbf{\\underline{o}}}t learning with \\textcolor{red}{\\textbf{\\underline{lo}}}w-\\textcolor{red}{\\textbf{\\underline{r}}}ank adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and CL together, leveraging low-rank tensors to efficiently adapt the model to target tasks while reducing even more the number of parameters required. Applying a vanilla LoRA implementation already permitted us to obtain pretty good results. In order to optimize even further the size of the adapters, we challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for convolutional layers. Finally, aware of the criticality linked to the choice of the hyperparameters of LoRA, we provide an empirical study to easily find the best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on several benchmark CL and FS tasks and show that our model is efficient, reaching SOTA performance but with a number of resources enormously reduced. Source code is available on \\href{https://github.com/munsifali11/CoLoR-GAN}{Github.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.015595"
    },
    {
        "index": "#99",
        "title": "Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning",
        "link": "/arxiv/2510.13865",
        "arxiv_id": "2510.13865",
        "authors": "Dongkwan Lee, Junhoo Lee, Nojun Kwak",
        "summary": "We introduce the Deep Edge Filter, a novel approach that applies high-pass filtering to deep neural network features to improve model generalizability. Our method is motivated by our hypothesis that neural networks encode task-relevant semantic information in high-frequency components while storing domain-specific biases in low-frequency components of deep features. By subtracting low-pass filtered outputs from original features, our approach isolates generalizable representations while preserving architectural integrity. Experimental results across diverse domains such as Vision, Text, 3D, and Audio demonstrate consistent performance improvements regardless of model architecture and data modality. Analysis reveals that our method induces feature sparsification and effectively isolates high-frequency components, providing empirical validation of our core hypothesis. The code is available at https://github.com/dongkwani/DeepEdgeFilter.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.031260"
    },
    {
        "index": "#100",
        "title": "Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation",
        "link": "/arxiv/2510.13864",
        "arxiv_id": "2510.13864",
        "authors": "Zixi Wang, Yushe Cao, Yubo Huang, Jinzhu Wei, Jingzehua Xu, Shuai Zhang, Xin Lai",
        "summary": "In this paper, we propose a new method called Self-Training with Dynamic Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation (GDA) by addressing the challenge of smooth knowledge migration from the source to the target domain. Traditional GDA methods mitigate domain shift through intermediate domains and self-training but often suffer from inefficient knowledge migration or incomplete intermediate data. Our approach introduces a dynamic weighting mechanism that adaptively balances the loss contributions of the source and target domains during training. Specifically, we design an optimization framework governed by a time-varying hyperparameter $\\varrho$ (progressing from 0 to 1), which controls the strength of domain-specific learning and ensures stable adaptation. The method leverages self-training to generate pseudo-labels and optimizes a weighted objective function for iterative model updates, maintaining robustness across intermediate domains. Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the Cover Type dataset demonstrate that STDW outperforms existing baselines. Ablation studies further validate the critical role of $\\varrho$'s dynamic scheduling in achieving progressive adaptation, confirming its effectiveness in reducing domain bias and improving generalization. This work provides both theoretical insights and a practical framework for robust gradual domain adaptation, with potential applications in dynamic real-world scenarios. The code is available at https://github.com/Dramwig/STDW.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-13",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.031607"
    },
    {
        "index": "#101",
        "title": "Large Language Models for Real-World IoT Device Identification",
        "link": "/arxiv/2510.13817",
        "arxiv_id": "2510.13817",
        "authors": "Rameen Mahmood, Tousif Ahmed, Sai Teja Peddinti, Danny Yuxing Huang",
        "summary": "The rapid expansion of IoT devices has outpaced current identification methods, creating significant risks for security, privacy, and network accountability. These challenges are heightened in open-world environments, where traffic metadata is often incomplete, noisy, or intentionally obfuscated. We introduce a semantic inference pipeline that reframes device identification as a language modeling task over heterogeneous network metadata. To construct reliable supervision, we generate high-fidelity vendor labels for the IoT Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble of large language models guided by mutual-information and entropy-based stability scores. We then instruction-tune a quantized LLaMA3.18B model with curriculum learning to support generalization under sparsity and long-tail vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro accuracy across 2,015 vendors while maintaining resilience to missing fields, protocol drift, and adversarial manipulation. Evaluation on an independent IoT testbed, coupled with explanation quality and adversarial stress tests, demonstrates that instruction-tuned LLMs provide a scalable and interpretable foundation for real-world device identification at scale.",
        "subjects": "Machine Learning, Networking and Internet Architecture",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.031919"
    },
    {
        "index": "#102",
        "title": "Agentic Design of Compositional Machines",
        "link": "/arxiv/2510.14980",
        "arxiv_id": "2510.14980",
        "authors": "Wenqian Zhang, Weiyang Liu, Zhen Liu",
        "summary": "The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Graphics, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.032254"
    },
    {
        "index": "#103",
        "title": "Learning an Image Editing Model without Image Editing Pairs",
        "link": "/arxiv/2510.14978",
        "arxiv_id": "2510.14978",
        "authors": "Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, Xun Huang",
        "summary": "Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.032600"
    },
    {
        "index": "#104",
        "title": "Terra: Explorable Native 3D World Model with Point Latents",
        "link": "/arxiv/2510.14977",
        "arxiv_id": "2510.14977",
        "authors": "Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu",
        "summary": "World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.032935"
    },
    {
        "index": "#105",
        "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
        "link": "/arxiv/2510.14973",
        "arxiv_id": "2510.14973",
        "authors": "Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen",
        "summary": "This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences, and $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.033231"
    },
    {
        "index": "#106",
        "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
        "link": "/arxiv/2510.14972",
        "arxiv_id": "2510.14972",
        "authors": "Yinxi Li, Yuntian Deng, Pengyu Nie",
        "summary": "Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Programming Languages, Software Engineering",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.033549"
    },
    {
        "index": "#107",
        "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training",
        "link": "/arxiv/2510.14969",
        "arxiv_id": "2510.14969",
        "authors": "Yiming Wang, Da Yin, Yuedong Cui, Ruichen Zheng, Zhiqian Li, Zongyu Lin, Di Wu, Xueqing Wu, Chenchen Ye, Yu Zhou, Kai-Wei Chang",
        "summary": "Digital agents require diverse, large-scale UI trajectories to generalize across real-world tasks, yet collecting such data is prohibitively expensive in both human annotation, infra and engineering perspectives. To this end, we introduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates structured UI states and transitions to synthesize training trajectories at scale. Our paradigm integrates a digital world simulator for diverse UI states, a guided rollout process for coherent exploration, and a trajectory wrapper that produces high-quality and diverse trajectories for agent training. We further propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that enables more rapid and data-efficient scaling by prioritizing high-impact tasks and synthesizes informative trajectory variants. Experiments on WebArena and AndroidWorld show that UI-Simulator rivals or surpasses open-source agents trained on real UIs with significantly better robustness, despite using weaker teacher models. Moreover, UI-Simulator-Grow matches the performance of Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model, highlighting the potential of targeted synthesis scaling paradigm to continuously and efficiently enhance the digital agents.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.033919"
    },
    {
        "index": "#108",
        "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
        "link": "/arxiv/2510.14968",
        "arxiv_id": "2510.14968",
        "authors": "Mingxuan Yan, Yuping Wang, Zechun Liu, Jiachen Li",
        "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at rdd-neurips.github.io.",
        "subjects": "Robotics, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning, Systems and Control",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.034250"
    },
    {
        "index": "#109",
        "title": "Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents",
        "link": "/arxiv/2510.14967",
        "arxiv_id": "2510.14967",
        "authors": "Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying",
        "summary": "Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.034590"
    },
    {
        "index": "#110",
        "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions",
        "link": "/arxiv/2510.14959",
        "arxiv_id": "2510.14959",
        "authors": "Lizhi Yang, Blake Werner, Massimiliano de Sa Aaron D. Ames",
        "summary": "Reinforcement learning (RL), while powerful and expressive, can often prioritize performance at the expense of safety. Yet safety violations can lead to catastrophic outcomes in real-world deployments. Control Barrier Functions (CBFs) offer a principled method to enforce dynamic safety -- traditionally deployed \\emph{online} via safety filters. While the result is safe behavior, the fact that the RL policy does not have knowledge of the CBF can lead to conservative behaviors. This paper proposes CBF-RL, a framework for generating safe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key attributes: (1) minimally modifying a nominal RL policy to encode safety constraints via a CBF term, (2) and safety filtering of the policy rollouts in training. Theoretically, we prove that continuous-time safety filters can be deployed via closed-form expressions on discrete-time roll-outs. Practically, we demonstrate that CBF-RL internalizes the safety constraints in the learned policy -- both enforcing safer actions and biasing towards safer rewards -- enabling safe deployment without the need for an online safety filter. We validate our framework through ablation studies on navigation tasks and on the Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster convergence, and robust performance under uncertainty, enabling the humanoid robot to avoid obstacles and climb stairs safely in real-world settings without a runtime safety filter.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning, Systems and Control",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.034899"
    },
    {
        "index": "#111",
        "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation",
        "link": "/arxiv/2510.14949",
        "arxiv_id": "2510.14949",
        "authors": "Yu Zhou, Sohyun An, Haikang Deng, Da Yin, Clark Peng, Cho-Jui Hsieh, Kai-Wei Chang, Nanyun Peng",
        "summary": "Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (< 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance.",
        "subjects": "Computation and Language, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.035232"
    },
    {
        "index": "#112",
        "title": "Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion",
        "link": "/arxiv/2510.14947",
        "arxiv_id": "2510.14947",
        "authors": "Blake Werner, Lizhi Yang, Aaron D. Ames",
        "summary": "Robust humanoid locomotion in unstructured environments requires architectures that balance fast low-level stabilization with slower perceptual decision-making. We show that a simple layered control architecture (LCA), a proprioceptive stabilizer running at high rate, coupled with a compact low-rate perceptual policy, enables substantially more robust performance than monolithic end-to-end designs, even when using minimal perception encoders. Through a two-stage training curriculum (blind stabilizer pretraining followed by perceptual fine-tuning), we demonstrate that layered policies consistently outperform one-stage alternatives in both simulation and hardware. On a Unitree G1 humanoid, our approach succeeds across stair and ledge tasks where one-stage perceptual policies fail. These results highlight that architectural separation of timescales, rather than network scale or complexity, is the key enabler for robust perception-conditioned locomotion.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning, Systems and Control",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.035536"
    },
    {
        "index": "#113",
        "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
        "link": "/arxiv/2510.14943",
        "arxiv_id": "2510.14943",
        "authors": "Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, Yankai Lin",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.035875"
    },
    {
        "index": "#114",
        "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin",
        "link": "/arxiv/2510.14930",
        "arxiv_id": "2510.14930",
        "authors": "Binghao Huang, Jie Xu, Iretiayo Akinola, Wei Yang, Balakumar Sundaralingam, Rowland O'Flaherty, Dieter Fox, Xiaolong Wang, Arsalan Mousavian, Yu-Wei Chao, Yunzhu Li",
        "summary": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback -- a capability that remains difficult to replicate in robots through behavioral cloning alone, due to the suboptimality and limited diversity of human demonstrations. In this work, we present VT-Refine, a visuo-tactile policy learning framework that combines real-world demonstrations, high-fidelity tactile simulation, and reinforcement learning to tackle precise, contact-rich bimanual assembly. We begin by training a diffusion policy on a small set of demonstrations using synchronized visual and tactile inputs. This policy is then transferred to a simulated digital twin equipped with simulated tactile sensors and further refined via large-scale reinforcement learning to enhance robustness and generalization. To enable accurate sim-to-real transfer, we leverage high-resolution piezoresistive tactile sensors that provide normal force signals and can be realistically modeled in parallel using GPU-accelerated simulation. Experimental results show that VT-Refine improves assembly performance in both simulation and the real world by increasing data diversity and enabling more effective policy fine-tuning. Our project page is available at https://binghao-huang.github.io/vt_refine/.",
        "subjects": "Robotics, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.041375"
    },
    {
        "index": "#115",
        "title": "Instruction Set Migration at Warehouse Scale",
        "link": "/arxiv/2510.14928",
        "arxiv_id": "2510.14928",
        "authors": "Eric Christopher, Kevin Crossan, Wolff Dobson, Chris Kennelly, Drew Lewis, Kun Lin, Martin Maas, Parthasarathy Ranganathan, Emma Rapati, Brian Yang",
        "summary": "Migrating codebases from one instruction set architecture (ISA) to another is a major engineering challenge. A recent example is the adoption of Arm (in addition to x86) across the major Cloud hyperscalers. Yet, this problem has seen limited attention by the academic community. Most work has focused on static and dynamic binary translation, and the traditional conventional wisdom has been that this is the primary challenge. In this paper, we show that this is no longer the case. Modern ISA migrations can often build on a robust open-source ecosystem, making it possible to recompile all relevant software from scratch. This introduces a new and multifaceted set of challenges, which are different from binary translation. By analyzing a large-scale migration from x86 to Arm at Google, spanning almost 40,000 code commits, we derive a taxonomy of tasks involved in ISA migration. We show how Google automated many of the steps involved, and demonstrate how AI can play a major role in automatically addressing these tasks. We identify tasks that remain challenging and highlight research challenges that warrant further attention.",
        "subjects": "Software Engineering, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.041798"
    },
    {
        "index": "#116",
        "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models",
        "link": "/arxiv/2510.14925",
        "arxiv_id": "2510.14925",
        "authors": "Akira Okutomi",
        "summary": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition via a composite instability index (H-Risk) combining spectral margin, conditioning, temporal sensitivity, and innovation amplification. In linear-Gaussian simulations, higher H-Risk predicts overconfident errors even under formal stability, revealing a gap between nominal and epistemic stability. Extending to large language models (LLMs), we find that fragile internal dynamics correlate with miscalibration and hallucination, while critique-style prompts show mixed effects on calibration and hallucination. These results suggest a structural bridge between Kantian self-limitation and feedback control, offering a principled lens for diagnosing -- and selectively reducing -- overconfidence in reasoning systems. This is a preliminary version; supplementary experiments and broader replication will be reported in a future revision.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.042128"
    },
    {
        "index": "#117",
        "title": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG",
        "link": "/arxiv/2510.14922",
        "arxiv_id": "2510.14922",
        "authors": "Annisaa Fitri Nurfidausi, Eleonora Mancini, Paolo Torroni",
        "summary": "Depression is a widespread mental health disorder, yet its automatic detection remains challenging. Prior work has explored unimodal and multimodal approaches, with multimodal systems showing promise by leveraging complementary signals. However, existing studies are limited in scope, lack systematic comparisons of features, and suffer from inconsistent evaluation protocols. We address these gaps by systematically exploring feature representations and modelling strategies across EEG, together with speech and text. We evaluate handcrafted features versus pre-trained embeddings, assess the effectiveness of different neural encoders, compare unimodal, bimodal, and trimodal configurations, and analyse fusion strategies with attention to the role of EEG. Consistent subject-independent splits are applied to ensure robust, reproducible benchmarking. Our results show that (i) the combination of EEG, speech and text modalities enhances multimodal detection, (ii) pretrained embeddings outperform handcrafted features, and (iii) carefully designed trimodal models achieve state-of-the-art performance. Our work lays the groundwork for future research in multimodal depression detection.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning, Audio and Speech Processing, Signal Processing",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.042505"
    },
    {
        "index": "#118",
        "title": "Predicting Task Performance with Context-aware Scaling Laws",
        "link": "/arxiv/2510.14919",
        "arxiv_id": "2510.14919",
        "authors": "Kyle Montgomery, David Park, Jianhong Tu, Michael Bendersky, Beliz Gunel, Dawn Song, Chenguang Wang",
        "summary": "Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at https://github.com/wang-research-lab/context-scaling.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.042896"
    },
    {
        "index": "#119",
        "title": "Budget-aware Test-time Scaling via Discriminative Verification",
        "link": "/arxiv/2510.14913",
        "arxiv_id": "2510.14913",
        "authors": "Kyle Montgomery, Sijun Tan, Yuqi Chen, Siyuan Zhuang, Tianjun Zhang, Raluca Ada Popa, Chenguang Wang",
        "summary": "Test-time scaling is a powerful strategy for boosting the performance of large language models on complex reasoning tasks. While state-of-the-art approaches often employ generative verifiers to select the best solution from a pool of candidates, this method incurs prohibitive computational costs, limiting its practicality. In this work, we shift the focus to a more budget-aware paradigm: discriminative verification. We conduct a thorough empirical analysis and demonstrate that while discriminative verifiers may underperform in isolation, combining them with self-consistency in a hybrid approach creates a powerful and efficient test-time scaling mechanism. Notably, under a fixed compute budget, this hybrid approach surpasses state-of-the-art generative verification by a significant margin: achieving up to 15.3\\% higher accuracy on AIME2025. Our findings establish that for practical, real-world applications, budget-aware scaling with discriminative verifiers is not only a \"free\" upgrade over self-consistency, but also a more effective and efficient alternative to costly generative techniques. Code is available at https://github.com/wang-research-lab/verification.",
        "subjects": "Artificial Intelligence, Computation and Language, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.043259"
    },
    {
        "index": "#120",
        "title": "Learnable Mixed Nash Equilibria are Collectively Rational",
        "link": "/arxiv/2510.14907",
        "arxiv_id": "2510.14907",
        "authors": "Geelon So, Yi-An Ma",
        "summary": "We extend the study of learning in games to dynamics that exhibit non-asymptotic stability. We do so through the notion of uniform stability, which is concerned with equilibria of individually utility-seeking dynamics. Perhaps surprisingly, it turns out to be closely connected to economic properties of collective rationality. Under mild non-degeneracy conditions and up to strategic equivalence, if a mixed equilibrium is not uniformly stable, then it is not weakly Pareto optimal: there is a way for all players to improve by jointly deviating from the equilibrium. On the other hand, if it is locally uniformly stable, then the equilibrium must be weakly Pareto optimal. Moreover, we show that uniform stability determines the last-iterate convergence behavior for the family of incremental smoothed best-response dynamics, used to model individual and corporate behaviors in the markets. Unlike dynamics around strict equilibria, which can stabilize to socially-inefficient solutions, individually utility-seeking behaviors near mixed Nash equilibria lead to collective rationality.",
        "subjects": "Computer Science and Game Theory, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.043555"
    },
    {
        "index": "#121",
        "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos",
        "link": "/arxiv/2510.14904",
        "arxiv_id": "2510.14904",
        "authors": "Gabriel Fiastre, Antoine Yang, Cordelia Schmid",
        "summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.043921"
    },
    {
        "index": "#122",
        "title": "Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning",
        "link": "/arxiv/2510.14894",
        "arxiv_id": "2510.14894",
        "authors": "Marc Damie, Florian Hahn, Andreas Peter, Jan Ramon",
        "summary": "To preserve privacy, multi-party computation (MPC) enables executing Machine Learning (ML) algorithms on secret-shared or encrypted data. However, existing MPC frameworks are not optimized for sparse data. This makes them unsuitable for ML applications involving sparse data, e.g., recommender systems or genomics. Even in plaintext, such applications involve high-dimensional sparse data, that cannot be processed without sparsity-related optimizations due to prohibitively large memory requirements. Since matrix multiplication is central in ML algorithms, we propose MPC algorithms to multiply secret sparse matrices. On the one hand, our algorithms avoid the memory issues of the \"dense\" data representation of classic secure matrix multiplication algorithms. On the other hand, our algorithms can significantly reduce communication costs (some experiments show a factor 1000) for realistic problem sizes. We validate our algorithms in two ML applications in which existing protocols are impractical. An important question when developing MPC algorithms is what assumptions can be made. In our case, if the number of non-zeros in a row is a sensitive piece of information then a short runtime may reveal that the number of non-zeros is small. Existing approaches make relatively simple assumptions, e.g., that there is a universal upper bound to the number of non-zeros in a row. This often doesn't align with statistical reality, in a lot of sparse datasets the amount of data per instance satisfies a power law. We propose an approach which allows adopting a safe upper bound on the distribution of non-zeros in rows/columns of sparse matrices.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.044256"
    },
    {
        "index": "#123",
        "title": "Prediction-Specific Design of Learning-Augmented Algorithms",
        "link": "/arxiv/2510.14887",
        "arxiv_id": "2510.14887",
        "authors": "Sizhe Li, Nicolas Christianson, Tongxin Li",
        "summary": "Algorithms with predictions} has emerged as a powerful framework to combine the robustness of traditional online algorithms with the data-driven performance benefits of machine-learned (ML) predictions. However, most existing approaches in this paradigm are overly conservative, {as they do not leverage problem structure to optimize performance in a prediction-specific manner}. In this paper, we show that such prediction-specific performance criteria can enable significant performance improvements over the coarser notions of consistency and robustness considered in prior work. Specifically, we propose a notion of \\emph{strongly-optimal} algorithms with predictions, which obtain Pareto optimality not just in the worst-case tradeoff between robustness and consistency, but also in the prediction-specific tradeoff between these metrics. We develop a general bi-level optimization framework that enables systematically designing strongly-optimal algorithms in a wide variety of problem settings, and we propose explicit strongly-optimal algorithms for several classic online problems: deterministic and randomized ski rental, and one-max search. Our analysis reveals new structural insights into how predictions can be optimally integrated into online algorithms by leveraging a prediction-specific design. To validate the benefits of our proposed framework, we empirically evaluate our algorithms in case studies on problems including dynamic power management and volatility-based index trading. Our results demonstrate that prediction-specific, strongly-optimal algorithms can significantly improve performance across a variety of online decision-making settings.",
        "subjects": "Data Structures and Algorithms, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.044570"
    },
    {
        "index": "#124",
        "title": "From Loop Nests to Silicon: Mapping AI Workloads onto AMD NPUs with MLIR-AIR",
        "link": "/arxiv/2510.14871",
        "arxiv_id": "2510.14871",
        "authors": "Erwei Wang, Samuel Bayliss, Andra Bisca, Zachary Blair, Sangeeta Chowdhary, Kristof Denolf, Jeff Fifield, Brandon Freiberger, Erika Hunhoff, Phil James-Roxby, Jack Lo, Joseph Melber, Stephen Neuendorffer, Eddie Richter, Andre Rosti, Javier Setoain, Gagandeep Singh, Endri Taka, Pranathi Vasireddy, Zhewen Yu, Niansong Zhang, Jinming Zhuang",
        "summary": "General-purpose compilers abstract away parallelism, locality, and synchronization, limiting their effectiveness on modern spatial architectures. As modern computing architectures increasingly rely on fine-grained control over data movement, execution order, and compute placement for performance, compiler infrastructure must provide explicit mechanisms for orchestrating compute and data to fully exploit such architectures. We introduce MLIR-AIR, a novel, open-source compiler stack built on MLIR that bridges the semantic gap between high-level workloads and fine-grained spatial architectures such as AMD's NPUs. MLIR-AIR defines the AIR dialect, which provides structured representations for asynchronous and hierarchical operations across compute and memory resources. AIR primitives allow the compiler to orchestrate spatial scheduling, distribute computation across hardware regions, and overlap communication with computation without relying on ad hoc runtime coordination or manual scheduling. We demonstrate MLIR-AIR's capabilities through two case studies: matrix multiplication and the multi-head attention block from the LLaMA 2 model. For matrix multiplication, MLIR-AIR achieves up to 78.7% compute efficiency and generates implementations with performance almost identical to state-of-the-art, hand-optimized matrix multiplication written using the lower-level, close-to-metal MLIR-AIE framework. For multi-head attention, we demonstrate that the AIR interface supports fused implementations using approximately 150 lines of code, enabling tractable expression of complex workloads with efficient mapping to spatial hardware. MLIR-AIR transforms high-level structured control flow into spatial programs that efficiently utilize the compute fabric and memory hierarchy of an NPU, leveraging asynchronous execution, tiling, and communication overlap through compiler-managed scheduling.",
        "subjects": "Computation and Language, Hardware Architecture, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.045093"
    },
    {
        "index": "#125",
        "title": "A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution Simulation",
        "link": "/arxiv/2510.14855",
        "arxiv_id": "2510.14855",
        "authors": "Harsha Kotla, Arun Kumar Rajasekaran, Hannah Rana",
        "summary": "Early detection of melanoma has grown to be essential because it significantly improves survival rates, but automated analysis of skin lesions still remains challenging. ABCDE, which stands for Asymmetry, Border irregularity, Color variation, Diameter, and Evolving, is a well-known classification method for skin lesions, but most deep learning mechanisms treat it as a black box, as most of the human interpretable features are not explained. In this work, we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect, opening more windows for future exploration. The A, B, C, and D values are quantified particularly within this work. Moreover, this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary, the classification worked with an accuracy of around 89 percent, with melanoma AUC being 0.96, while the feature evaluation performed well in predicting asymmetry, color variation, and diameter, though border irregularity remains more difficult to model. Overall, this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria, thus improving our understanding of skin cancer progression.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.045400"
    },
    {
        "index": "#126",
        "title": "A Geometric Approach to Optimal Experimental Design",
        "link": "/arxiv/2510.14848",
        "arxiv_id": "2510.14848",
        "authors": "Gavin Kerrigan, Christian A. Naesseth, Tom Rainforth",
        "summary": "We introduce a novel geometric framework for optimal experimental design (OED). Traditional OED approaches, such as those based on mutual information, rely explicitly on probability densities, leading to restrictive invariance properties. To address these limitations, we propose the mutual transport dependence (MTD), a measure of statistical dependence grounded in optimal transport theory which provides a geometric objective for optimizing designs. Unlike conventional approaches, the MTD can be tailored to specific downstream estimation problems by choosing appropriate geometries on the underlying spaces. We demonstrate that our framework produces high-quality designs while offering a flexible alternative to standard information-theoretic techniques.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.045713"
    },
    {
        "index": "#127",
        "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning",
        "link": "/arxiv/2510.14830",
        "arxiv_id": "2510.14830",
        "authors": "Kun Lei, Huanyu Li, Dongjie Yu, Zhenyu Wei, Lingxiao Guo, Zhennan Jiang, Ziyu Wang, Shiyu Liang, Huazhe Xu",
        "summary": "Real-world robotic manipulation in homes and factories demands reliability, efficiency, and robustness that approach or surpass skilled human operators. We present RL-100, a real-world reinforcement learning training framework built on diffusion visuomotor policies trained bu supervised learning. RL-100 introduces a three-stage pipeline. First, imitation learning leverages human priors. Second, iterative offline reinforcement learning uses an Offline Policy Evaluation procedure, abbreviated OPE, to gate PPO-style updates that are applied in the denoising process for conservative and reliable improvement. Third, online reinforcement learning eliminates residual failure modes. An additional lightweight consistency distillation head compresses the multi-step sampling process in diffusion into a single-step policy, enabling high-frequency control with an order-of-magnitude reduction in latency while preserving task performance. The framework is task-, embodiment-, and representation-agnostic and supports both 3D point clouds and 2D RGB inputs, a variety of robot platforms, and both single-step and action-chunk policies. We evaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control, such as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth folding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100 attains 100\\% success across evaluated trials for a total of 900 out of 900 episodes, including up to 250 out of 250 consecutive trials on one task. The method achieves near-human teleoperation or better time efficiency and demonstrates multi-hour robustness with uninterrupted operation lasting up to two hours.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.046092"
    },
    {
        "index": "#128",
        "title": "Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning",
        "link": "/arxiv/2510.14819",
        "arxiv_id": "2510.14819",
        "authors": "Ji Cao, Yu Wang, Tongya Zheng, Zujie Ren, Canghong Jin, Gang Chen, Mingli Song",
        "summary": "Trajectory Representation Learning (TRL) aims to encode raw trajectories into low-dimensional vectors, which can then be leveraged in various downstream tasks, including travel time estimation, location prediction, and trajectory similarity analysis. However, existing TRL methods suffer from a key oversight: treating trajectories as isolated spatio-temporal sequences, without considering the external environment and internal route choice behavior that govern their formation. To bridge this gap, we propose a novel framework that unifies comprehensive environment \\textbf{P}erception and explicit \\textbf{R}oute choice modeling for effective \\textbf{Traj}ectory representation learning, dubbed \\textbf{PRTraj}. Specifically, PRTraj first introduces an Environment Perception Module to enhance the road network by capturing multi-granularity environmental semantics from surrounding POI distributions. Building on this environment-aware backbone, a Route Choice Encoder then captures the route choice behavior inherent in each trajectory by modeling its constituent road segment transitions as a sequence of decisions. These route-choice-aware representations are finally aggregated to form the global trajectory embedding. Extensive experiments on 3 real-world datasets across 5 downstream tasks validate the effectiveness and generalizability of PRTraj. Moreover, PRTraj demonstrates strong data efficiency, maintaining robust performance under few-shot scenarios. Our code is available at: https://anonymous.4open.science/r/PRTraj.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.051621"
    },
    {
        "index": "#129",
        "title": "Agentic NL2SQL to Reduce Computational Costs",
        "link": "/arxiv/2510.14808",
        "arxiv_id": "2510.14808",
        "authors": "Dominik Jehle, Lennart Purucker, Frank Hutter",
        "summary": "Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL) has recently been empowered by large language models (LLMs). Using LLMs to perform NL2SQL methods on a large collection of SQL databases necessitates processing large quantities of meta-information about the databases, which in turn results in lengthy prompts with many tokens and high processing costs. To address this challenge, we introduce Datalake Agent, an agentic system designed to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing direct solvers for NL2SQL that call the LLM once with all meta-information in the prompt, the Datalake Agent employs an interactive loop to reduce the utilized meta-information. Within the loop, the LLM is used in a reasoning framework that selectively requests only the necessary information to solve a table question answering task. We evaluate the Datalake Agent on a collection of 23 databases with 100 table question answering tasks. The Datalake Agent reduces the tokens used by the LLM by up to 87\\% and thus allows for substantial cost reductions while maintaining competitive performance.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.051922"
    },
    {
        "index": "#130",
        "title": "Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks",
        "link": "/arxiv/2510.14778",
        "arxiv_id": "2510.14778",
        "authors": "Maor Reuben, Ido Mendel, Or Feldman, Moshe Kravchik, Mordehai Guri, Rami Puzis",
        "summary": "Supply chain attacks significantly threaten software security with malicious code injections within legitimate projects. Such attacks are very rare but may have a devastating impact. Detecting spurious code injections using automated tools is further complicated as it often requires deciphering the intention of both the inserted code and its context. In this study, we propose an unsupervised approach for highlighting spurious code injections by quantifying cohesion disruptions in the source code. Using a name-prediction-based cohesion (NPC) metric, we analyze how function cohesion changes when malicious code is introduced compared to natural cohesion fluctuations. An analysis of 54,707 functions over 369 open-source C++ repositories reveals that code injection reduces cohesion and shifts naming patterns toward shorter, less descriptive names compared to genuine function updates. Considering the sporadic nature of real supply-chain attacks, we evaluate the proposed method with extreme test-set imbalance and show that monitoring high-cohesion functions with NPC can effectively detect functions with injected code, achieving a Precision@100 of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that automated cohesion measurements, in general, and name-prediction-based cohesion, in particular, may help identify supply chain attacks, improving source code integrity.",
        "subjects": "Software Engineering, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.052249"
    },
    {
        "index": "#131",
        "title": "Fast and Scalable Score-Based Kernel Calibration Tests",
        "link": "/arxiv/2510.14711",
        "arxiv_id": "2510.14711",
        "authors": "Pierre Glaser, David Widmann, Fredrik Lindsten, Arthur Gretton",
        "summary": "We introduce the Kernel Calibration Conditional Stein Discrepancy test (KCCSD test), a non-parametric, kernel-based test for assessing the calibration of probabilistic models with well-defined scores. In contrast to previous methods, our test avoids the need for possibly expensive expectation approximations while providing control over its type-I error. We achieve these improvements by using a new family of kernels for score-based probabilities that can be estimated without probability density samples, and by using a conditional goodness-of-fit criterion for the KCCSD test's U-statistic. We demonstrate the properties of our test on various synthetic settings.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.052540"
    },
    {
        "index": "#132",
        "title": "MCbiF: Measuring Topological Autocorrelation in Multiscale Clusterings via 2-Parameter Persistent Homology",
        "link": "/arxiv/2510.14710",
        "arxiv_id": "2510.14710",
        "authors": "Juni Schindler, Mauricio Barahona",
        "summary": "Datasets often possess an intrinsic multiscale structure with meaningful descriptions at different levels of coarseness. Such datasets are naturally described as multi-resolution clusterings, i.e., not necessarily hierarchical sequences of partitions across scales. To analyse and compare such sequences, we use tools from topological data analysis and define the Multiscale Clustering Bifiltration (MCbiF), a 2-parameter filtration of abstract simplicial complexes that encodes cluster intersection patterns across scales. The MCbiF can be interpreted as a higher-order extension of Sankey diagrams and reduces to a dendrogram for hierarchical sequences. We show that the multiparameter persistent homology (MPH) of the MCbiF yields a finitely presented and block decomposable module, and its stable Hilbert functions characterise the topological autocorrelation of the sequence of partitions. In particular, at dimension zero, the MPH captures violations of the refinement order of partitions, whereas at dimension one, the MPH captures higher-order inconsistencies between clusters across scales. We demonstrate through experiments the use of MCbiF Hilbert functions as topological feature maps for downstream machine learning tasks. MCbiF feature maps outperform information-based baseline features on both regression and classification tasks on synthetic sets of non-hierarchical sequences of partitions. We also show an application of MCbiF to real-world data to measure non-hierarchies in wild mice social grouping patterns across time.",
        "subjects": "Algebraic Topology, Machine Learning, Data Analysis, Statistics and Probability",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.052837"
    },
    {
        "index": "#133",
        "title": "Response to Discussions of \"Causal and Counterfactual Views of Missing Data Models\"",
        "link": "/arxiv/2510.14694",
        "arxiv_id": "2510.14694",
        "authors": "Razieh Nabi, Rohit Bhattacharya, Ilya Shpitser, James M. Robins",
        "summary": "We are grateful to the discussants, Levis and Kennedy [2025], Luo and Geng [2025], Wang and van der Laan [2025], and Yang and Kim [2025], for their thoughtful comments on our paper (Nabi et al., 2025). In this rejoinder, we summarize our main contributions and respond to each discussion in turn.",
        "subjects": "Methodology, Machine Learning, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.053147"
    },
    {
        "index": "#135",
        "title": "Decorrelation Speeds Up Vision Transformers",
        "link": "/arxiv/2510.14657",
        "arxiv_id": "2510.14657",
        "authors": "Kieran Carrigg, Rob van Gastel, Melda Yeghaian, Sander Dalm, Faysal Boughorbel, Marcel van Gerven",
        "summary": "Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields strong performance in low-label regimes but comes with substantial computational costs, making it impractical in time- and resource-constrained industrial settings. We address this by integrating Decorrelated Backpropagation (DBP) into MAE pre-training, an optimization method that iteratively reduces input correlations at each layer to accelerate convergence. Applied selectively to the encoder, DBP achieves faster pre-training without loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE reduces wall-clock time to baseline performance by 21.1%, lowers carbon emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe similar gains when pre-training and fine-tuning on proprietary industrial data, confirming the method's applicability in real-world scenarios. These results demonstrate that DBP can reduce training time and energy use while improving downstream performance for large-scale ViT pre-training.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.053771"
    },
    {
        "index": "#136",
        "title": "Parameter Identification for Partial Differential Equation with Jump Discontinuities in Coefficients by Markov Switching Model and Physics-Informed Machine Learning",
        "link": "/arxiv/2510.14656",
        "arxiv_id": "2510.14656",
        "authors": "Zhikun Zhang, Guanyu Pan, Xiangjun Wang, Yong Xu, Guangtao Zhang",
        "summary": "Inverse problems involving partial differential equations (PDEs) with discontinuous coefficients are fundamental challenges in modeling complex spatiotemporal systems with heterogeneous structures and uncertain dynamics. Traditional numerical and machine learning approaches often face limitations in addressing these problems due to high dimensionality, inherent nonlinearity, and discontinuous parameter spaces. In this work, we propose a novel computational framework that synergistically integrates physics-informed deep learning with Bayesian inference for accurate parameter identification in PDEs with jump discontinuities in coefficients. The core innovation of our framework lies in a dual-network architecture employing a gradient-adaptive weighting strategy: a main network approximates PDE solutions while a sub network samples its coefficients. To effectively identify mixture structures in parameter spaces, we employ Markovian dynamics methods to capture hidden state transitions of complex spatiotemporal systems. The framework has applications in reconstruction of solutions and identification of parameter-varying regions. Comprehensive numerical experiments on various PDEs with jump-varying coefficients demonstrate the framework's exceptional adaptability, accuracy, and robustness compared to existing methods. This study provides a generalizable computational approach of parameter identification for PDEs with discontinuous parameter structures, particularly in non-stationary or heterogeneous systems.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.054063"
    },
    {
        "index": "#137",
        "title": "Local Causal Discovery for Statistically Efficient Causal Inference",
        "link": "/arxiv/2510.14582",
        "arxiv_id": "2510.14582",
        "authors": "Mátyás Schubert, Tom Claassen, Sara Magliacane",
        "summary": "Causal discovery methods can identify valid adjustment sets for causal effect estimation for a pair of target variables, even when the underlying causal graph is unknown. Global causal discovery methods focus on learning the whole causal graph and therefore enable the recovery of optimal adjustment sets, i.e., sets with the lowest asymptotic variance, but they quickly become computationally prohibitive as the number of variables grows. Local causal discovery methods offer a more scalable alternative by focusing on the local neighborhood of the target variables, but are restricted to statistically suboptimal adjustment sets. In this work, we propose Local Optimal Adjustments Discovery (LOAD), a sound and complete causal discovery approach that combines the computational efficiency of local methods with the statistical optimality of global methods. First, LOAD identifies the causal relation between the targets and tests if the causal effect is identifiable by using only local information. If it is identifiable, it then finds the optimal adjustment set by leveraging local causal discovery to infer the mediators and their parents. Otherwise, it returns the locally valid parent adjustment sets based on the learned local structure. In our experiments on synthetic and realistic data LOAD outperforms global methods in scalability, while providing more accurate effect estimation than local methods.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.054355"
    },
    {
        "index": "#138",
        "title": "A Deep State-Space Model Compression Method using Upper Bound on Output Error",
        "link": "/arxiv/2510.14542",
        "arxiv_id": "2510.14542",
        "authors": "Hiroki Sakamoto, Kazuhiro Sato",
        "summary": "We study deep state-space models (Deep SSMs) that contain linear-quadratic-output (LQO) systems as internal blocks and present a compression method with a provable output error guarantee. We first derive an upper bound on the output error between two Deep SSMs and show that the bound can be expressed via the $h^2$-error norms between the layerwise LQO systems, thereby providing a theoretical justification for existing model order reduction (MOR)-based compression. Building on this bound, we formulate an optimization problem in terms of the $h^2$-error norm and develop a gradient-based MOR method. On the IMDb task from the Long Range Arena benchmark, we demonstrate that our compression method achieves strong performance. Moreover, unlike prior approaches, we reduce roughly 80% of trainable parameters without retraining, with only a 4-5% performance drop.",
        "subjects": "Systems and Control, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.054630"
    },
    {
        "index": "#139",
        "title": "Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts",
        "link": "/arxiv/2510.14538",
        "arxiv_id": "2510.14538",
        "authors": "Emanuele Marconato, Samuele Bortolotti, Emile van Krieken, Paolo Morettin, Elena Umili, Antonio Vergari, Efthymia Tsamoura, Andrea Passerini, Stefano Teso",
        "summary": "Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose predictions comply with prior knowledge encoding, e.g. safety or structural constraints. As such, it represents one of the most promising avenues for reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural and symbolic steps: neural networks are typically responsible for mapping low-level inputs into high-level symbolic concepts, while symbolic reasoning infers predictions compatible with the extracted concepts and the prior knowledge. Despite their promise, it was recently shown that - whenever the concepts are not supervised directly - NeSy models can be affected by Reasoning Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the concepts incorrectly. RSs can compromise the interpretability of the model's explanations, performance in out-of-distribution scenarios, and therefore reliability. At the same time, RSs are difficult to detect and prevent unless concept supervision is available, which is typically not the case. However, the literature on RSs is scattered, making it difficult for researchers and practitioners to understand and tackle this challenging problem. This overview addresses this issue by providing a gentle introduction to RSs, discussing their causes and consequences in intuitive terms. It also reviews and elucidates existing theoretical characterizations of this phenomenon. Finally, it details methods for dealing with RSs, including mitigation and awareness strategies, and maps their benefits and limitations. By reformulating advanced material in a digestible form, this overview aims to provide a unifying perspective on RSs to lower the bar to entry for tackling them. Ultimately, we hope this overview contributes to the development of reliable NeSy and trustworthy AI models.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.055104"
    },
    {
        "index": "#140",
        "title": "Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models",
        "link": "/arxiv/2510.14526",
        "arxiv_id": "2510.14526",
        "authors": "Yunze Tong, Didi Zhu, Zijing Hu, Jinluan Yang, Ziyu Zhao",
        "summary": "In text-to-image generation, different initial noises induce distinct denoising paths with a pretrained Stable Diffusion (SD) model. While this pattern could output diverse images, some of them may fail to align well with the prompt. Existing methods alleviate this issue either by altering the denoising dynamics or by drawing multiple noises and conducting post-selection. In this paper, we attribute the misalignment to a training-inference mismatch: during training, prompt-conditioned noises lie in a prompt-specific subset of the latent space, whereas at inference the noise is drawn from a prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector that applies text-conditioned refinement to the initial noise before denoising. Conditioned on the prompt embedding, it maps the noise to a prompt-aware counterpart that better matches the distribution observed during SD training, without modifying the SD model. Our framework consists of these steps: we first sample some noises and obtain token-level feedback for their corresponding images from a vision-language model (VLM), then distill these signals into a reward model, and finally optimize the noise projector via a quasi-direct preference optimization. Our design has two benefits: (i) it requires no reference images or handcrafted priors, and (ii) it incurs small inference cost, replacing multi-sample selection with a single forward pass. Extensive experiments further show that our prompt-aware noise projection improves text-image alignment across diverse prompts.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.055403"
    },
    {
        "index": "#141",
        "title": "State Your Intention to Steer Your Attention: An AI Assistant for Intentional Digital Living",
        "link": "/arxiv/2510.14513",
        "arxiv_id": "2510.14513",
        "authors": "Juheon Choi, Juyoung Lee, Jian Kim, Chanyoung Kim, Taewon Min, W. Bradley Knox, Min Kyung Lee, Kimin Lee",
        "summary": "When working on digital devices, people often face distractions that can lead to a decline in productivity and efficiency, as well as negative psychological and emotional impacts. To address this challenge, we introduce a novel Artificial Intelligence (AI) assistant that elicits a user's intention, assesses whether ongoing activities are in line with that intention, and provides gentle nudges when deviations occur. The system leverages a large language model to analyze screenshots, application titles, and URLs, issuing notifications when behavior diverges from the stated goal. Its detection accuracy is refined through initial clarification dialogues and continuous user feedback. In a three-week, within-subjects field deployment with 22 participants, we compared our assistant to both a rule-based intent reminder system and a passive baseline that only logged activity. Results indicate that our AI assistant effectively supports users in maintaining focus and aligning their digital behavior with their intentions. Our source code is publicly available at this url https://intentassistant.github.io",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.055742"
    },
    {
        "index": "#142",
        "title": "Personalized federated learning, Row-wise fusion regularization, Multivariate modeling, Sparse estimation",
        "link": "/arxiv/2510.14413",
        "arxiv_id": "2510.14413",
        "authors": "Runlin Zhou, Letian Li, Zemin Zheng",
        "summary": "We study personalized federated learning for multivariate responses where client models are heterogeneous yet share variable-level structure. Existing entry-wise penalties ignore cross-response dependence, while matrix-wise fusion over-couples clients. We propose a Sparse Row-wise Fusion (SROF) regularizer that clusters row vectors across clients and induces within-row sparsity, and we develop RowFed, a communication-efficient federated algorithm that embeds SROF into a linearized ADMM framework with privacy-preserving partial participation. Theoretically, we establish an oracle property for SROF-achieving correct variable-level group recovery with asymptotic normality-and prove convergence of RowFed to a stationary solution. Under random client participation, the iterate gap contracts at a rate that improves with participation probability. Empirically, simulations in heterogeneous regimes show that RowFed consistently lowers estimation and prediction error and strengthens variable-level cluster recovery over NonFed, FedAvg, and a personalized matrix-fusion baseline. A real-data study further corroborates these gains while preserving interpretability. Together, our results position row-wise fusion as an effective and transparent paradigm for large-scale personalized federated multivariate learning, bridging the gap between entry-wise and matrix-wise formulations.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.056027"
    },
    {
        "index": "#143",
        "title": "Low Power Vision Transformer Accelerator with Hardware-Aware Pruning and Optimized Dataflow",
        "link": "/arxiv/2510.14393",
        "arxiv_id": "2510.14393",
        "authors": "Ching-Lin Hsiung, Tian-Sheuan Chang",
        "summary": "Current transformer accelerators primarily focus on optimizing self-attention due to its quadratic complexity. However, this focus is less relevant for vision transformers with short token lengths, where the Feed-Forward Network (FFN) tends to be the dominant computational bottleneck. This paper presents a low power Vision Transformer accelerator, optimized through algorithm-hardware co-design. The model complexity is reduced using hardware-friendly dynamic token pruning without introducing complex mechanisms. Sparsity is further improved by replacing GELU with ReLU activations and employing dynamic FFN2 pruning, achieving a 61.5\\% reduction in operations and a 59.3\\% reduction in FFN2 weights, with an accuracy loss of less than 2\\%. The hardware adopts a row-wise dataflow with output-oriented data access to eliminate data transposition, and supports dynamic operations with minimal area overhead. Implemented in TSMC's 28nm CMOS technology, our design occupies 496.4K gates and includes a 232KB SRAM buffer, achieving a peak throughput of 1024 GOPS at 1GHz, with an energy efficiency of 2.31 TOPS/W and an area efficiency of 858.61 GOPS/mm2.",
        "subjects": "Hardware Architecture, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.056304"
    },
    {
        "index": "#144",
        "title": "Beat Detection as Object Detection",
        "link": "/arxiv/2510.14391",
        "arxiv_id": "2510.14391",
        "authors": "Jaehoon Ahn, Moon-Ryul Jung",
        "summary": "Recent beat and downbeat tracking models (e.g., RNNs, TCNs, Transformers) output frame-level activations. We propose reframing this task as object detection, where beats and downbeats are modeled as temporal \"objects.\" Adapting the FCOS detector from computer vision to 1D audio, we replace its original backbone with WaveBeat's temporal feature extractor and add a Feature Pyramid Network to capture multi-scale temporal patterns. The model predicts overlapping beat/downbeat intervals with confidence scores, followed by non-maximum suppression (NMS) to select final predictions. This NMS step serves a similar role to DBNs in traditional trackers, but is simpler and less heuristic. Evaluated on standard music datasets, our approach achieves competitive results, showing that object detection techniques can effectively model musical beats with minimal adaptation.",
        "subjects": "Sound, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.056602"
    },
    {
        "index": "#145",
        "title": "BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble",
        "link": "/arxiv/2510.14389",
        "arxiv_id": "2510.14389",
        "authors": "Brandon Hill, Kma Solaiman",
        "summary": "Motherboard defect detection is critical for ensuring reliability in high-volume electronics manufacturing. While prior research in PCB inspection has largely targeted bare-board or trace-level defects, assembly-level inspection of full motherboards inspection remains underexplored. In this work, we present BoardVision, a reproducible framework for detecting assembly-level defects such as missing screws, loose fan wiring, and surface scratches. We benchmark two representative detectors - YOLOv7 and Faster R-CNN, under controlled conditions on the MiracleFactory motherboard dataset, providing the first systematic comparison in this domain. To mitigate the limitations of single models, where YOLO excels in precision but underperforms in recall and Faster R-CNN shows the reverse, we propose a lightweight ensemble, Confidence-Temporal Voting (CTV Voter), that balances precision and recall through interpretable rules. We further evaluate robustness under realistic perturbations including sharpness, brightness, and orientation changes, highlighting stability challenges often overlooked in motherboard defect detection. Finally, we release a deployable GUI-driven inspection tool that bridges research evaluation with operator usability. Together, these contributions demonstrate how computer vision techniques can transition from benchmark results to practical quality assurance for assembly-level motherboard manufacturing.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.056872"
    },
    {
        "index": "#146",
        "title": "PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora",
        "link": "/arxiv/2510.14377",
        "arxiv_id": "2510.14377",
        "authors": "Mykolas Sveistrys, Richard Kunert",
        "summary": "Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) have enabled progress on question answering (QA) when relevant evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many realistic questions about recurring report data - medical records, compliance filings, maintenance logs - require aggregation across all documents, with no clear stopping point for retrieval and high sensitivity to even one missed passage. We term these pluri-hop questions and formalize them by three criteria: recall sensitivity, exhaustiveness, and exactness. To study this setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48 pluri-hop questions built from 191 real-world wind industry reports in German and English. We show that PluriHopWIND is 8-40% more repetitive than other common datasets and thus has higher density of distractor documents, better reflecting practical challenges of recurring report corpora. We test a traditional RAG pipeline as well as graph-based and multimodal variants, and find that none of the tested approaches exceed 40% in statement-wise F1 score. Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a \"check all documents individually, filter cheaply\" approach: it (i) decomposes queries into document-level subquestions and (ii) uses a cross-encoder filter to discard irrelevant documents before costly LLM reasoning. We find that PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base LLM. Despite its modest size, PluriHopWIND exposes the limitations of current QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance highlights the value of exhaustive retrieval and early filtering as a powerful alternative to top-k methods.",
        "subjects": "Computation and Language, Information Retrieval, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.057156"
    },
    {
        "index": "#147",
        "title": "A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities",
        "link": "/arxiv/2510.14340",
        "arxiv_id": "2510.14340",
        "authors": "Siva Teja Kakileti, Bharath Govindaraju, Sudhakar Sampangi, Geetha Manjunath",
        "summary": "Mammography, the current standard for breast cancer screening, has reduced sensitivity in women with dense breast tissue, contributing to missed or delayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures functional vascular and metabolic cues that may complement mammographic structural data. This study investigates whether a breast density-informed multi-modal AI framework can improve cancer detection by dynamically selecting the appropriate imaging modality based on breast tissue composition. A total of 324 women underwent both mammography and thermal imaging. Mammography images were analyzed using a multi-view deep learning model, while Thermalytix assessed thermal images through vascular and thermal radiomics. The proposed framework utilized Mammography AI for fatty breasts and Thermalytix AI for dense breasts, optimizing predictions based on tissue type. This multi-modal AI framework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity of 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI (sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity 92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography dropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%), whereas Thermalytix AI maintained high and consistent sensitivity in both (92.59% and 92.86%, respectively). This demonstrates that a density-informed multi-modal AI framework can overcome key limitations of unimodal screening and deliver high performance across diverse breast compositions. The proposed framework is interpretable, low-cost, and easily deployable, offering a practical path to improving breast cancer screening outcomes in both high-resource and resource-limited settings.",
        "subjects": "Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.057476"
    },
    {
        "index": "#148",
        "title": "A Robust Classification Method using Hybrid Word Embedding for Early Diagnosis of Alzheimer's Disease",
        "link": "/arxiv/2510.14332",
        "arxiv_id": "2510.14332",
        "authors": "Yangyang Li",
        "summary": "Early detection of Alzheimer's Disease (AD) is greatly beneficial to AD patients, leading to early treatments that lessen symptoms and alleviating financial burden of health care. As one of the leading signs of AD, language capability changes can be used for early diagnosis of AD. In this paper, I develop a robust classification method using hybrid word embedding and fine-tuned hyperparameters to achieve state-of-the-art accuracy in the early detection of AD. Specifically, we create a hybrid word embedding based on word vectors from Doc2Vec and ELMo to obtain perplexity scores of the sentences. The scores identify whether a sentence is fluent or not and capture semantic context of the sentences. I enrich the word embedding by adding linguistic features to analyze syntax and semantics. Further, we input an embedded feature vector into logistic regression and fine tune hyperparameters throughout the pipeline. By tuning hyperparameters of the machine learning pipeline (e.g., model regularization parameter, learning rate and vector size of Doc2Vec, and vector size of ELMo), I achieve 91% classification accuracy and an Area Under the Curve (AUC) of 97% in distinguishing early AD from healthy subjects. Based on my knowledge, my model with 91% accuracy and 97% AUC outperforms the best existing NLP model for AD diagnosis with an accuracy of 88% [32]. I study the model stability through repeated experiments and find that the model is stable even though the training data is split randomly (standard deviation of accuracy = 0.0403; standard deviation of AUC = 0.0174). This affirms our proposed method is accurate and stable. This model can be used as a large-scale screening method for AD, as well as a complementary examination for doctors to detect AD.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Audio and Speech Processing",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.057771"
    },
    {
        "index": "#149",
        "title": "Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL",
        "link": "/arxiv/2510.14318",
        "arxiv_id": "2510.14318",
        "authors": "Marwa Abdulhai, Ryan Cheng, Aryansh Shrivastava, Natasha Jaques, Yarin Gal, Sergey Levine",
        "summary": "Large Language Models (LLMs) interact with millions of people worldwide in applications such as customer support, education and healthcare. However, their ability to produce deceptive outputs, whether intentionally or inadvertently, poses significant safety concerns. The unpredictable nature of LLM behavior, combined with insufficient safeguards against hallucination, misinformation, and user manipulation, makes their misuse a serious, real-world risk. In this paper, we investigate the extent to which LLMs engage in deception within dialogue, and propose the belief misalignment metric to quantify deception. We evaluate deception across four distinct dialogue scenarios, using five established deception detection metrics and our proposed metric. Our findings reveal this novel deception measure correlates more closely with human judgments than any existing metrics we test. Additionally, our benchmarking of eight state-of-the-art models indicates that LLMs naturally exhibit deceptive behavior in approximately 26% of dialogue turns, even when prompted with seemingly benign objectives. When prompted to deceive, LLMs are capable of increasing deceptiveness by as much as 31% relative to baselines. Unexpectedly, models trained with RLHF, the predominant approach for ensuring the safety of widely-deployed LLMs, still exhibit deception at a rate of 43% on average. Given that deception in dialogue is a behavior that develops over an interaction history, its effective evaluation and mitigation necessitates moving beyond single-utterance analyses. We introduce a multi-turn reinforcement learning methodology to fine-tune LLMs to reduce deceptive behaviors, leading to a 77.6% reduction compared to other instruction-tuned models.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.058087"
    },
    {
        "index": "#150",
        "title": "Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers",
        "link": "/arxiv/2510.14303",
        "arxiv_id": "2510.14303",
        "authors": "Ziye Xia, Sergei S. Ospichev",
        "summary": "In recent years, the rapid increase in academic publications across various fields has posed severe challenges for academic paper analysis: scientists struggle to timely and comprehensively track the latest research findings and methodologies. Key concept extraction has proven to be an effective analytical paradigm, and its automation has been achieved with the widespread application of language models in industrial and scientific domains. However, existing paper databases are mostly limited to similarity matching and basic classification of key concepts, failing to deeply explore the relational networks between concepts. This paper is based on the OpenAlex opensource knowledge graph. By analyzing nearly 8,000 open-source paper data from Novosibirsk State University, we discovered a strong correlation between the distribution patterns of paper key concept paths and both innovation points and rare paths. We propose a prompt engineering-based key concept path analysis method. This method leverages small language models to achieve precise key concept extraction and innovation point identification, and constructs an agent based on a knowledge graph constraint mechanism to enhance analysis accuracy. Through fine-tuning of the Qwen and DeepSeek models, we achieved significant improvements in accuracy, with the models publicly available on the Hugging Face platform.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.058367"
    },
    {
        "index": "#151",
        "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying",
        "link": "/arxiv/2510.14293",
        "arxiv_id": "2510.14293",
        "authors": "Yushi Du, Yixuan Li, Baoxiong Jia, Yutang Lin, Pei Zhou, Wei Liang, Yanchao Yang, Siyuan Huang",
        "summary": "Human-humanoid collaboration shows significant promise for applications in healthcare, domestic assistance, and manufacturing. While compliant robot-human collaboration has been extensively developed for robotic arms, enabling compliant human-humanoid collaboration remains largely unexplored due to humanoids' complex whole-body dynamics. In this paper, we propose a proprioception-only reinforcement learning approach, COLA, that combines leader and follower behaviors within a single policy. The model is trained in a closed-loop environment with dynamic object interactions to predict object motion patterns and human intentions implicitly, enabling compliant collaboration to maintain load balance through coordinated trajectory planning. We evaluate our approach through comprehensive simulator and real-world experiments on collaborative carrying tasks, demonstrating the effectiveness, generalization, and robustness of our model across various terrains and objects. Simulation experiments demonstrate that our model reduces human effort by 24.7%. compared to baseline approaches while maintaining object stability. Real-world experiments validate robust collaborative carrying across different object types (boxes, desks, stretchers, etc.) and movement patterns (straight-line, turning, slope climbing). Human user studies with 23 participants confirm an average improvement of 27.4% compared to baseline models. Our method enables compliant human-humanoid collaborative carrying without requiring external sensors or complex interaction models, offering a practical solution for real-world deployment.",
        "subjects": "Robotics, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.058719"
    },
    {
        "index": "#152",
        "title": "Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks",
        "link": "/arxiv/2510.14283",
        "arxiv_id": "2510.14283",
        "authors": "Xinhao Deng, Jingyou Chen, Linxiao Yu, Yixiang Zhang, Zhongyi Gu, Changhao Qiu, Xiyuan Zhao, Ke Xu, Qi Li",
        "summary": "Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to infer the websites visited by users, posing a serious threat to anonymous communication systems. Although recent WF techniques achieve over 90% accuracy in controlled experimental settings, most studies remain confined to single scenarios, overlooking the complexity of real-world environments. This paper presents the first systematic and comprehensive evaluation of existing WF attacks under diverse realistic conditions, including defense mechanisms, traffic drift, multi-tab browsing, early-stage detection, open-world settings, and few-shot scenarios. Experimental results show that many WF techniques with strong performance in isolated settings degrade significantly when facing other conditions. Since real-world environments often combine multiple challenges, current WF attacks are difficult to apply directly in practice. This study highlights the limitations of WF attacks and introduces a multidimensional evaluation framework, offering critical insights for developing more robust and practical WF attacks.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.059068"
    },
    {
        "index": "#153",
        "title": "Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation",
        "link": "/arxiv/2510.14271",
        "arxiv_id": "2510.14271",
        "authors": "Yilun Zheng, Dan Yang, Jie Li, Lin Shang, Lihui Chen, Jiahao Xu, Sitao Luan",
        "summary": "Retrieval-Augmented Generation (RAG) systems enable large language models (LLMs) instant access to relevant information for the generative process, demonstrating their superior performance in addressing common LLM challenges such as hallucination, factual inaccuracy, and the knowledge cutoff. Graph-based RAG further extends this paradigm by incorporating knowledge graphs (KGs) to leverage rich, structured connections for more precise and inferential responses. A critical challenge, however, is that most Graph-based RAG systems rely on LLMs for automated KG construction, often yielding noisy KGs with redundant entities and unreliable relationships. This noise degrades retrieval and generation performance while also increasing computational cost. Crucially, current research does not comprehensively address the denoising problem for LLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for Retrieval Augmented Generation (DEG-RAG), a framework that addresses these challenges through: (1) entity resolution, which eliminates redundant entities, and (2) triple reflection, which removes erroneous relations. Together, these techniques yield more compact, higher-quality KGs that significantly outperform their unprocessed counterparts. Beyond the methods, we conduct a systematic evaluation of entity resolution for LLM-generated KGs, examining different blocking strategies, embedding choices, similarity metrics, and entity merging techniques. To the best of our knowledge, this is the first comprehensive exploration of entity resolution in LLM-generated KGs. Our experiments demonstrate that this straightforward approach not only drastically reduces graph size but also consistently improves question answering performance across diverse popular Graph-based RAG variants.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.059410"
    },
    {
        "index": "#154",
        "title": "Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt Perturbations in LLMs",
        "link": "/arxiv/2510.14242",
        "arxiv_id": "2510.14242",
        "authors": "Parsa Hejabi, Elnaz Rahmati, Alireza S. Ziabari, Morteza Dehghani",
        "summary": "Large Language Models (LLMs) often produce inconsistent answers when faced with different phrasings of the same prompt. In this paper, we propose Flip-Flop Consistency ($F^2C$), an unsupervised training method that improves robustness to such perturbations. $F^2C$ is composed of two key components. The first, Consensus Cross-Entropy (CCE), uses a majority vote across prompt variations to create a hard pseudo-label. The second is a representation alignment loss that pulls lower-confidence and non-majority predictors toward the consensus established by high-confidence, majority-voting variations. We evaluate our method on 11 datasets spanning four NLP tasks, with 4-15 prompt variations per dataset. On average, $F^2C$ raises observed agreement by 11.62%, improves mean $F_1$ by 8.94%, and reduces performance variance across formats by 3.29%. In out-of-domain evaluations, $F^2C$ generalizes effectively, increasing $\\overline{F_1}$ and agreement while decreasing variance across most source-target pairs. Finally, when trained on only a subset of prompt perturbations and evaluated on held-out formats, $F^2C$ consistently improves both performance and agreement while reducing variance. These findings highlight $F^2C$ as an effective unsupervised method for enhancing LLM consistency, performance, and generalization under prompt perturbations. Code is available at https://github.com/ParsaHejabi/Flip-Flop-Consistency-Unsupervised-Training-for-Robustness-to-Prompt-Perturbations-in-LLMs.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.059706"
    },
    {
        "index": "#155",
        "title": "A novel Information-Driven Strategy for Optimal Regression Assessment",
        "link": "/arxiv/2510.14222",
        "arxiv_id": "2510.14222",
        "authors": "Benjamín Castro, Camilo Ramírez, Sebastián Espinosa, Jorge F. Silva, Marcos E. Orchard, Heraldo Rozas",
        "summary": "In Machine Learning (ML), a regression algorithm aims to minimize a loss function based on data. An assessment method in this context seeks to quantify the discrepancy between the optimal response for an input-output system and the estimate produced by a learned predictive model (the student). Evaluating the quality of a learned regressor remains challenging without access to the true data-generating mechanism, as no data-driven assessment method can ensure the achievability of global optimality. This work introduces the Information Teacher, a novel data-driven framework for evaluating regression algorithms with formal performance guarantees to assess global optimality. Our novel approach builds on estimating the Shannon mutual information (MI) between the input variables and the residuals and applies to a broad class of additive noise models. Through numerical experiments, we confirm that the Information Teacher is capable of detecting global optimality, which is aligned with the condition of zero estimation error with respect to the -- inaccessible, in practice -- true model, working as a surrogate measure of the ground truth assessment loss and offering a principled alternative to conventional empirical performance metrics.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.065191"
    },
    {
        "index": "#156",
        "title": "ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning",
        "link": "/arxiv/2510.14176",
        "arxiv_id": "2510.14176",
        "authors": "Roger Creus Castanyer, Faisal Mohamed, Pablo Samuel Castro, Cyrus Neary, Glen Berseth",
        "summary": "Reinforcement learning (RL) algorithms are highly sensitive to reward function specification, which remains a central challenge limiting their broad applicability. We present ARM-FM: Automated Reward Machines via Foundation Models, a framework for automated, compositional reward design in RL that leverages the high-level reasoning capabilities of foundation models (FMs). Reward machines (RMs) -- an automata-based formalism for reward specification -- are used as the mechanism for RL objective specification, and are automatically constructed via the use of FMs. The structured formalism of RMs yields effective task decompositions, while the use of FMs enables objective specifications in natural language. Concretely, we (i) use FMs to automatically generate RMs from natural language specifications; (ii) associate language embeddings with each RM automata-state to enable generalization across tasks; and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse suite of challenging environments, including evidence of zero-shot generalization.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-16",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.065499"
    },
    {
        "index": "#157",
        "title": "Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola",
        "link": "/arxiv/2510.14154",
        "arxiv_id": "2510.14154",
        "authors": "Tian Liu, Alex Cann, Ian Colbert, Mehdi Saeedi",
        "summary": "While the rapid advancements in the reinforcement learning (RL) research community have been remarkable, the adoption in commercial video games remains slow. In this paper, we outline common challenges the Game AI community faces when using RL-driven NPCs in practice, and highlight the intersection of RL with traditional behavior trees (BTs) as a crucial juncture to be explored further. Although the BT+RL intersection has been suggested in several research papers, its adoption is rare. We demonstrate the viability of this approach using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by creating multi-task NPCs in a complex 3D environment inspired by the commercial video game ``The Last of Us\". We provide detailed methodologies for jointly training RL models with BTs while showcasing various skills.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.065797"
    },
    {
        "index": "#158",
        "title": "CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization",
        "link": "/arxiv/2510.14150",
        "arxiv_id": "2510.14150",
        "authors": "Henrique Assumpção, Diego Ferreira, Leandro Campos, Fabricio Murai",
        "summary": "In this work, we introduce CodeEvolve, an open-source evolutionary coding agent that unites Large Language Models (LLMs) with genetic algorithms to solve complex computational problems. Our framework adapts powerful evolutionary concepts to the LLM domain, building upon recent methods for generalized scientific discovery. CodeEvolve employs an island-based genetic algorithm to maintain population diversity and increase throughput, introduces a novel inspiration-based crossover mechanism that leverages the LLMs context window to combine features from successful solutions, and implements meta-prompting strategies for dynamic exploration of the solution space. We conduct a rigorous evaluation of CodeEvolve on a subset of the mathematical benchmarks used to evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that our method surpasses AlphaEvolve's performance on several challenging problems. To foster collaboration and accelerate progress, we release our complete framework as an open-source repository.",
        "subjects": "Artificial Intelligence, Machine Learning, Neural and Evolutionary Computing",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.066126"
    },
    {
        "index": "#159",
        "title": "PoissonNet: A Local-Global Approach for Learning on Surfaces",
        "link": "/arxiv/2510.14146",
        "arxiv_id": "2510.14146",
        "authors": "Arman Maesumi, Tanish Makadia, Thibault Groueix, Vladimir G. Kim, Daniel Ritchie, Noam Aigerman",
        "summary": "Many network architectures exist for learning on meshes, yet their constructions entail delicate trade-offs between difficulty learning high-frequency features, insufficient receptive field, sensitivity to discretization, and inefficient computational overhead. Drawing from classic local-global approaches in mesh processing, we introduce PoissonNet, a novel neural architecture that overcomes all of these deficiencies by formulating a local-global learning scheme, which uses Poisson's equation as the primary mechanism for feature propagation. Our core network block is simple; we apply learned local feature transformations in the gradient domain of the mesh, then solve a Poisson system to propagate scalar feature updates across the surface globally. Our local-global learning framework preserves the features's full frequency spectrum and provides a truly global receptive field, while remaining agnostic to mesh triangulation. Our construction is efficient, requiring far less compute overhead than comparable methods, which enables scalability -- both in the size of our datasets, and the size of individual training samples. These qualities are validated on various experiments where, compared to previous intrinsic architectures, we attain state-of-the-art performance on semantic segmentation and parameterizing highly-detailed animated surfaces. Finally, as a central application of PoissonNet, we show its ability to learn deformations, significantly outperforming state-of-the-art architectures that learn on surfaces.",
        "subjects": "Graphics, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.066448"
    },
    {
        "index": "#160",
        "title": "High-Dimensional BWDM: A Robust Nonparametric Clustering Validation Index for Large-Scale Data",
        "link": "/arxiv/2510.14145",
        "arxiv_id": "2510.14145",
        "authors": "Mohammed Baragilly, Hend Gabr",
        "summary": "Determining the appropriate number of clusters in unsupervised learning is a central problem in statistics and data science. Traditional validity indices such as Calinski-Harabasz, Silhouette, and Davies-Bouldin-depend on centroid-based distances and therefore degrade in high-dimensional or contaminated data. This paper proposes a new robust, nonparametric clustering validation framework, the High-Dimensional Between-Within Distance Median (HD-BWDM), which extends the recently introduced BWDM criterion to high-dimensional spaces. HD-BWDM integrates random projection and principal component analysis to mitigate the curse of dimensionality and applies trimmed clustering and medoid-based distances to ensure robustness against outliers. We derive theoretical results showing consistency and convergence under Johnson-Lindenstrauss embeddings. Extensive simulations demonstrate that HD-BWDM remains stable and interpretable under high-dimensional projections and contamination, providing a robust alternative to traditional centroid-based validation criteria. The proposed method provides a theoretically grounded, computationally efficient stopping rule for nonparametric clustering in modern high-dimensional applications.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.066763"
    },
    {
        "index": "#161",
        "title": "David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation",
        "link": "/arxiv/2510.14115",
        "arxiv_id": "2510.14115",
        "authors": "Philipp Bauerfeind, Amir Salarpour, David Fernandez, Pedram MohajerAnsari, Johannes Reschke, Mert D. Pesé",
        "summary": "Scenario simulation is central to testing autonomous driving systems. Scenic, a domain-specific language (DSL) for CARLA, enables precise and reproducible scenarios, but NL-to-Scenic generation with large language models (LLMs) suffers from scarce data, limited reproducibility, and inconsistent metrics. We introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a difficulty-stratified 30-case test split, an Example Retriever, and 14 prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics (compilation and generation), and compare them with an expert study (n=11). EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88 percent of its expert score on local hardware. Retrieval-augmented prompting, Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a standardized, reproducible basis for evaluating Scenic code generation and indicate that mid-size open-source models are practical, cost-effective options for autonomous-driving scenario programming.",
        "subjects": "Software Engineering, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.067086"
    },
    {
        "index": "#162",
        "title": "Extracting latent representations from X-ray spectra. Classification, regression, and accretion signatures of Chandra sources",
        "link": "/arxiv/2510.14102",
        "arxiv_id": "2510.14102",
        "authors": "Nicolò Oreste Pinciroli Vago, Juan Rafael Martínez-Galarza, Roberta Amato",
        "summary": "The study of X-ray spectra is crucial to understanding the physical nature of astrophysical sources. Machine learning methods can extract compact and informative representations of data from large datasets. The Chandra Source Catalog (CSC) provides a rich archive of X-ray spectral data, which remains largely underexplored in this context. This work aims to develop a compact and physically meaningful representation of Chandra X-ray spectra using deep learning. To verify that the learned representation captures relevant information, we evaluate it through classification, regression, and interpretability analyses. We use a transformer-based autoencoder to compress X-ray spectra. The input spectra, drawn from the CSC, include only high-significance detections. Astrophysical source types and physical summary statistics are compiled from external catalogs. We evaluate the learned representation in terms of spectral reconstruction accuracy, clustering performance on 8 known astrophysical source classes, and correlation with physical quantities such as hardness ratios and hydrogen column density ($N_H$). The autoencoder accurately reconstructs spectra with 8 latent variables. Clustering in the latent space yields a balanced classification accuracy of $\\sim$40% across the 8 source classes, increasing to $\\sim$69% when restricted to AGNs and stellar-mass compact objects exclusively. Moreover, latent features correlate with non-linear combinations of spectral fluxes, suggesting that the compressed representation encodes physically relevant information. The proposed autoencoder-based pipeline is a powerful tool for the representation and interpretation of X-ray spectra, providing a compact latent space that supports both classification and the estimation of physical properties. This work demonstrates the potential of deep learning for spectral studies and uncovering new patterns in X-ray data.",
        "subjects": "Instrumentation and Methods for Astrophysics, Artificial Intelligence, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.067380"
    },
    {
        "index": "#163",
        "title": "deFOREST: Fusing Optical and Radar satellite data for Enhanced Sensing of Tree-loss",
        "link": "/arxiv/2510.14092",
        "arxiv_id": "2510.14092",
        "authors": "Julio Enrique Castrillon-Candas, Hanfeng Gu, Caleb Meredith, Yulin Li, Xiaojing Tang, Pontus Olofsson, Mark Kon",
        "summary": "In this paper we develop a deforestation detection pipeline that incorporates optical and Synthetic Aperture Radar (SAR) data. A crucial component of the pipeline is the construction of anomaly maps of the optical data, which is done using the residual space of a discrete Karhunen-Loève (KL) expansion. Anomalies are quantified using a concentration bound on the distribution of the residual components for the nominal state of the forest. This bound does not require prior knowledge on the distribution of the data. This is in contrast to statistical parametric methods that assume knowledge of the data distribution, an impractical assumption that is especially infeasible for high dimensional data such as ours. Once the optical anomaly maps are computed they are combined with SAR data, and the state of the forest is classified by using a Hidden Markov Model (HMM). We test our approach with Sentinel-1 (SAR) and Sentinel-2 (Optical) data on a $92.19\\,km \\times 91.80\\,km$ region in the Amazon forest. The results show that both the hybrid optical-radar and optical only methods achieve high accuracy that is superior to the recent state-of-the-art hybrid method. Moreover, the hybrid method is significantly more robust in the case of sparse optical data that are common in highly cloudy regions.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.067758"
    },
    {
        "index": "#164",
        "title": "Exact Dynamics of Multi-class Stochastic Gradient Descent",
        "link": "/arxiv/2510.14074",
        "arxiv_id": "2510.14074",
        "authors": "Elizabeth Collins-Woodfin, Inbar Seroussi",
        "summary": "We develop a framework for analyzing the training and learning rate dynamics on a variety of high- dimensional optimization problems trained using one-pass stochastic gradient descent (SGD) with data generated from multiple anisotropic classes. We give exact expressions for a large class of functions of the limiting dynamics, including the risk and the overlap with the true signal, in terms of a deterministic solution to a system of ODEs. We extend the existing theory of high-dimensional SGD dynamics to Gaussian-mixture data and a large (growing with the parameter size) number of classes. We then investigate in detail the effect of the anisotropic structure of the covariance of the data in the problems of binary logistic regression and least square loss. We study three cases: isotropic covariances, data covariance matrices with a large fraction of zero eigenvalues (denoted as the zero-one model), and covariance matrices with spectra following a power-law distribution. We show that there exists a structural phase transition. In particular, we demonstrate that, for the zero-one model and the power-law model with sufficiently large power, SGD tends to align more closely with values of the class mean that are projected onto the \"clean directions\" (i.e., directions of smaller variance). This is supported by both numerical simulations and analytical studies, which show the exact asymptotic behavior of the loss in the high-dimensional limit.",
        "subjects": "Machine Learning, Machine Learning, Optimization and Control, Probability",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.068069"
    },
    {
        "index": "#165",
        "title": "Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping Games",
        "link": "/arxiv/2510.14030",
        "arxiv_id": "2510.14030",
        "authors": "César Guerra-Solano, Zhuochun Li, Xiang Lorraine Li",
        "summary": "Large language models (LLMs) can exhibit biases in reasoning capabilities due to linguistic modality, performing better on tasks in one language versus another, even with similar content. Most previous works evaluate this through reasoning tasks where reliance on strategies or knowledge can ensure success, such as in commonsense or math tasks. However, abstract reasoning is vital to reasoning for everyday life, where people apply \"out-of-the-box thinking\" to identify and use patterns for solutions, without a reliance on formulaic approaches. Comparatively, little work has evaluated linguistic biases in this task type. In this paper, we propose a task inspired by the New York Times Connections: GlobalGroup, that evaluates models in an abstract reasoning task across several languages. We constructed a game benchmark with five linguistic backgrounds -- English, Spanish, Chinese, Hindi, and Arabic -- in both the native language and an English translation for comparison. We also proposed game difficulty measurements to evaluate models on games with similar difficulty, enabling a more controlled comparison, which is particularly important in reasoning evaluations. Through experimentation, we find English modalities largely lead to better performance in this abstract reasoning task, and performance disparities between open- and closed-source models.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.068362"
    },
    {
        "index": "#166",
        "title": "PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features",
        "link": "/arxiv/2510.14005",
        "arxiv_id": "2510.14005",
        "authors": "Wei Zou, Yupei Liu, Yanting Wang, Ying Chen, Neil Gong, Jinyuan Jia",
        "summary": "LLM-integrated applications are vulnerable to prompt injection attacks, where an attacker contaminates the input to inject malicious prompts, causing the LLM to follow the attacker's intent instead of the original user's. Existing prompt injection detection methods often have sub-optimal performance and/or high computational overhead. In this work, we propose PIShield, a detection method that is both effective and efficient. Our key observation is that the internal representation of the final token in a prompt-extracted from a specific layer of the LLM, which we term the injection-critical layer-captures distinguishing features between clean and contaminated prompts. Leveraging this insight, we train a simple linear classifier on these internal representations using a labeled set of clean and contaminated prompts. We compare PIShield against 11 baselines across 5 diverse benchmark datasets and 8 prompt injection attacks. The results demonstrate that PIShield is both highly effective and efficient, substantially outperforming existing methods. Additionally, we show that PIShield resists strong adaptive attacks.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.068677"
    },
    {
        "index": "#167",
        "title": "Dynamic SBI: Round-free Sequential Simulation-Based Inference with Adaptive Datasets",
        "link": "/arxiv/2510.13997",
        "arxiv_id": "2510.13997",
        "authors": "Huifang Lyu, James Alvey, Noemi Anau Montel, Mauro Pieroni, Christoph Weniger",
        "summary": "Simulation-based inference (SBI) is emerging as a new statistical paradigm for addressing complex scientific inference problems. By leveraging the representational power of deep neural networks, SBI can extract the most informative simulation features for the parameters of interest. Sequential SBI methods extend this approach by iteratively steering the simulation process towards the most relevant regions of parameter space. This is typically implemented through an algorithmic structure, in which simulation and network training alternate over multiple rounds. This strategy is particularly well suited for high-precision inference in high-dimensional settings, which are commonplace in physics applications with growing data volumes and increasing model fidelity. Here, we introduce dynamic SBI, which implements the core ideas of sequential methods in a round-free, asynchronous, and highly parallelisable manner. At its core is an adaptive dataset that is iteratively transformed during inference to resemble the target observation. Simulation and training proceed in parallel: trained networks are used both to filter out simulations incompatible with the data and to propose new, more promising ones. Compared to round-based sequential methods, this asynchronous structure can significantly reduce simulation costs and training overhead. We demonstrate that dynamic SBI achieves significant improvements in simulation and training efficiency while maintaining inference performance. We further validate our framework on two challenging astrophysical inference tasks: characterising the stochastic gravitational wave background and analysing strong gravitational lensing systems. Overall, this work presents a flexible and efficient new paradigm for sequential SBI.",
        "subjects": "Instrumentation and Methods for Astrophysics, Cosmology and Nongalactic Astrophysics, Machine Learning, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.069013"
    },
    {
        "index": "#168",
        "title": "Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models",
        "link": "/arxiv/2510.13993",
        "arxiv_id": "2510.13993",
        "authors": "Jia Yun Chua, Argyrios Zolotas, Miguel Arana-Catania",
        "summary": "Remote sensing has become a vital tool across sectors such as urban planning, environmental monitoring, and disaster response. While the volume of data generated has increased significantly, traditional vision models are often constrained by the requirement for extensive domain-specific labelled data and their limited ability to understand the context within complex environments. Vision Language Models offer a complementary approach by integrating visual and textual data; however, their application to remote sensing remains underexplored, particularly given their generalist nature. This work investigates the combination of vision models and VLMs to enhance image analysis in remote sensing, with a focus on aircraft detection and scene understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and Gemini aims to achieve more accurate and contextually aware image interpretation. Performance is evaluated on both labelled and unlabelled remote sensing data, as well as degraded image scenarios which are crucial for remote sensing. The findings show an average MAE improvement of 48.46% across models in the accuracy of aircraft detection and counting, especially in challenging conditions, in both raw and degraded scenarios. A 6.17% improvement in CLIPScore for comprehensive understanding of remote sensing images is obtained. The proposed approach combining traditional vision models and VLMs paves the way for more advanced and efficient remote sensing image analysis, especially in few-shot learning scenarios.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.069306"
    },
    {
        "index": "#169",
        "title": "Signature in Code Backdoor Detection, how far are we?",
        "link": "/arxiv/2510.13992",
        "arxiv_id": "2510.13992",
        "authors": "Quoc Hung Le, Thanh Le-Cong, Bach Le, Bowen Xu",
        "summary": "As Large Language Models (LLMs) become increasingly integrated into software development workflows, they also become prime targets for adversarial attacks. Among these, backdoor attacks are a significant threat, allowing attackers to manipulate model outputs through hidden triggers embedded in training data. Detecting such backdoors remains a challenge, and one promising approach is the use of Spectral Signature defense methods that identify poisoned data by analyzing feature representations through eigenvectors. While some prior works have explored Spectral Signatures for backdoor detection in neural networks, recent studies suggest that these methods may not be optimally effective for code models. In this paper, we revisit the applicability of Spectral Signature-based defenses in the context of backdoor attacks on code models. We systematically evaluate their effectiveness under various attack scenarios and defense configurations, analyzing their strengths and limitations. We found that the widely used setting of Spectral Signature in code backdoor detection is often suboptimal. Hence, we explored the impact of different settings of the key factors. We discovered a new proxy metric that can more accurately estimate the actual performance of Spectral Signature without model retraining after the defense.",
        "subjects": "Software Engineering, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.069626"
    },
    {
        "index": "#170",
        "title": "Classifying and Addressing the Diversity of Errors in Retrieval-Augmented Generation Systems",
        "link": "/arxiv/2510.13975",
        "arxiv_id": "2510.13975",
        "authors": "Kin Kwan Leung, Mouloud Belbahri, Yi Sui, Alex Labach, Xueying Zhang, Stephen Rose, Jesse C. Cresswell",
        "summary": "Retrieval-augmented generation (RAG) is a prevalent approach for building LLM-based question-answering systems that can take advantage of external knowledge databases. Due to the complexity of real-world RAG systems, there are many potential causes for erroneous outputs. Understanding the range of errors that can occur in practice is crucial for robust deployment. We present a new taxonomy of the error types that can occur in realistic RAG systems, examples of each, and practical advice for addressing them. Additionally, we curate a dataset of erroneous RAG responses annotated by error types. We then propose an auto-evaluation method aligned with our taxonomy that can be used in practice to track and address errors during development. Code and data are available at https://github.com/layer6ai-labs/rag-error-classification.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.069960"
    },
    {
        "index": "#171",
        "title": "Long-Term Spatio-Temporal Forecasting of Monthly Rainfall in West Bengal Using Ensemble Learning Approaches",
        "link": "/arxiv/2510.13927",
        "arxiv_id": "2510.13927",
        "authors": "Jishu Adhikary, Raju Maiti",
        "summary": "Rainfall forecasting plays a critical role in climate adaptation, agriculture, and water resource management. This study develops long-term forecasts of monthly rainfall across 19 districts of West Bengal using a century-scale dataset spanning 1900-2019. Daily rainfall records are aggregated into monthly series, resulting in 120 years of observations for each district. The forecasting task involves predicting the next 108 months (9 years, 2011-2019) while accounting for temporal dependencies and spatial interactions among districts. To address the nonlinear and complex structure of rainfall dynamics, we propose a hierarchical modeling framework that combines regression-based forecasting of yearly features with multi-layer perceptrons (MLPs) for monthly prediction. Yearly features, such as annual totals, quarterly proportions, variability measures, skewness, and extremes, are first forecasted using regression models that incorporate both own lags and neighboring-district lags. These forecasts are then integrated as auxiliary inputs into an MLP model, which captures nonlinear temporal patterns and spatial dependencies in the monthly series. The results demonstrate that the hierarchical regression-MLP architecture provides robust long-term spatio-temporal forecasts, offering valuable insights for agriculture, irrigation planning, and water conservation strategies.",
        "subjects": "Applications, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.080468"
    },
    {
        "index": "#172",
        "title": "Readability $\\ne$ Learnability: Rethinking the Role of Simplicity in Training Small Language Models",
        "link": "/arxiv/2510.13915",
        "arxiv_id": "2510.13915",
        "authors": "Ivan Lee, Taylor Berg-Kirkpatrick",
        "summary": "Recent studies suggest that very small language models (SLMs) can generate surprisingly coherent text when trained on simplified, child-directed corpora such as TinyStories. These findings have been interpreted as evidence that readability -- characterized by accessible vocabulary, familiar narrative structure, and simple syntax -- plays a key role in enabling such capabilities to emerge. In this paper, we challenge that interpretation. We construct synthetic datasets with matched structure but varied readability, and find that readability alone does not predict coherence or learning efficiency in SLMs. Models trained on complex, adult-level text perform comparably to those trained on simplified language, and even exhibit faster development of coherence during training. Instead, we show that statistical simplicity, as measured by n-gram diversity, is a stronger predictor of learnability. Our findings caution against the growing trend of anthropomorphizing language model training -- drawing parallels to human cognitive development without empirical basis -- and argue for more precise reasoning about what properties actually support capability emergence in small models.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.080757"
    },
    {
        "index": "#173",
        "title": "Switchboard-Affect: Emotion Perception Labels from Conversational Speech",
        "link": "/arxiv/2510.13906",
        "arxiv_id": "2510.13906",
        "authors": "Amrit Romana, Jaya Narain, Tien Dung Tran, Andrea Davis, Jason Fong, Ramya Rasipuram, Vikramjit Mitra",
        "summary": "Understanding the nuances of speech emotion dataset curation and labeling is essential for assessing speech emotion recognition (SER) model potential in real-world applications. Most training and evaluation datasets contain acted or pseudo-acted speech (e.g., podcast speech) in which emotion expressions may be exaggerated or otherwise intentionally modified. Furthermore, datasets labeled based on crowd perception often lack transparency regarding the guidelines given to annotators. These factors make it difficult to understand model performance and pinpoint necessary areas for improvement. To address this gap, we identified the Switchboard corpus as a promising source of naturalistic conversational speech, and we trained a crowd to label the dataset for categorical emotions (anger, contempt, disgust, fear, sadness, surprise, happiness, tenderness, calmness, and neutral) and dimensional attributes (activation, valence, and dominance). We refer to this label set as Switchboard-Affect (SWB-Affect). In this work, we present our approach in detail, including the definitions provided to annotators and an analysis of the lexical and paralinguistic cues that may have played a role in their perception. In addition, we evaluate state-of-the-art SER models, and we find variable performance across the emotion categories with especially poor generalization for anger. These findings underscore the importance of evaluation with datasets that capture natural affective variations in speech. We release the labels for SWB-Affect to enable further analysis in this domain.",
        "subjects": "Audio and Speech Processing, Machine Learning, Sound",
        "date": "2025-10-14",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.081099"
    },
    {
        "index": "#175",
        "title": "Post-surgical Endometriosis Segmentation in Laparoscopic Videos",
        "link": "/arxiv/2510.13899",
        "arxiv_id": "2510.13899",
        "authors": "Andreas Leibetseder, Klaus Schoeffmann, Jörg Keckstein, Simon Keckstein",
        "summary": "Endometriosis is a common women's condition exhibiting a manifold visual appearance in various body-internal locations. Having such properties makes its identification very difficult and error-prone, at least for laymen and non-specialized medical practitioners. In an attempt to provide assistance to gynecologic physicians treating endometriosis, this demo paper describes a system that is trained to segment one frequently occurring visual appearance of endometriosis, namely dark endometrial implants. The system is capable of analyzing laparoscopic surgery videos, annotating identified implant regions with multi-colored overlays and displaying a detection summary for improved video browsing.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning, Multimedia",
        "date": "2025-10-14",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.081739"
    },
    {
        "index": "#176",
        "title": "Bayes or Heisenberg: Who(se) Rules?",
        "link": "/arxiv/2510.13894",
        "arxiv_id": "2510.13894",
        "authors": "Volker Tresp Hang Li, Federico Harjes, Yunpu Ma",
        "summary": "Although quantum systems are generally described by quantum state vectors, we show that in certain cases their measurement processes can be reformulated as probabilistic equations expressed in terms of probabilistic state vectors. These probabilistic representations can, in turn, be approximated by the neural network dynamics of the Tensor Brain (TB) model. The Tensor Brain is a recently proposed framework for modeling perception and memory in the brain, providing a biologically inspired mechanism for efficiently integrating generated symbolic representations into reasoning processes.",
        "subjects": "Neurons and Cognition, Artificial Intelligence, Machine Learning, Quantum Physics",
        "date": "2025-10-14",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.082047"
    },
    {
        "index": "#177",
        "title": "Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and Cooperative Completion",
        "link": "/arxiv/2510.13887",
        "arxiv_id": "2510.13887",
        "authors": "Xiaojian Ding, Lin Zhao, Xian Li, Xiaoying Zhu",
        "summary": "Incomplete multi-view data, where certain views are entirely missing for some samples, poses significant challenges for traditional multi-view clustering methods. Existing deep incomplete multi-view clustering approaches often rely on static fusion strategies or two-stage pipelines, leading to suboptimal fusion results and error propagation issues. To address these limitations, this paper proposes a novel incomplete multi-view clustering framework based on Hierarchical Semantic Alignment and Cooperative Completion (HSACC). HSACC achieves robust cross-view fusion through a dual-level semantic space design. In the low-level semantic space, consistency alignment is ensured by maximizing mutual information across views. In the high-level semantic space, adaptive view weights are dynamically assigned based on the distributional affinity between individual views and an initial fused representation, followed by weighted fusion to generate a unified global representation. Additionally, HSACC implicitly recovers missing views by projecting aligned latent representations into high-dimensional semantic spaces and jointly optimizes reconstruction and clustering objectives, enabling cooperative learning of completion and clustering. Experimental results demonstrate that HSACC significantly outperforms state-of-the-art methods on five benchmark datasets. Ablation studies validate the effectiveness of the hierarchical alignment and dynamic weighting mechanisms, while parameter analysis confirms the model's robustness to hyperparameter variations.",
        "subjects": "Image and Video Processing, Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.082361"
    },
    {
        "index": "#178",
        "title": "DeepMartingale: Duality of the Optimal Stopping Problem with Expressivity",
        "link": "/arxiv/2510.13868",
        "arxiv_id": "2510.13868",
        "authors": "Junyan Ye, Hoi Ying Wong",
        "summary": "Using a martingale representation, we introduce a novel deep-learning approach, which we call DeepMartingale, to study the duality of discrete-monitoring optimal stopping problems in continuous time. This approach provides a tight upper bound for the primal value function, even in high-dimensional settings. We prove that the upper bound derived from DeepMartingale converges under very mild assumptions. Even more importantly, we establish the expressivity of DeepMartingale: it approximates the true value function within any prescribed accuracy $\\varepsilon$ under our architectural design of neural networks whose size is bounded by $\\tilde{c}\\,D^{\\tilde{q}}\\varepsilon^{-\\tilde{r}}$, where the constants $\\tilde{c}, \\tilde{q}, \\tilde{r}$ are independent of the dimension $D$ and the accuracy $\\varepsilon$. This guarantees that DeepMartingale does not suffer from the curse of dimensionality. Numerical experiments demonstrate the practical effectiveness of DeepMartingale, confirming its convergence, expressivity, and stability.",
        "subjects": "Optimization and Control, Machine Learning, Numerical Analysis, Probability, Machine Learning",
        "date": "2025-10-13",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.082672"
    },
    {
        "index": "#179",
        "title": "An Overview of the JPEG AI Learning-Based Image Coding Standard",
        "link": "/arxiv/2510.13867",
        "arxiv_id": "2510.13867",
        "authors": "Semih Esenlik, Yaojun Wu, Zhaobin Zhang, Ye-Kui Wang, Kai Zhang, Li Zhang, João Ascenso, Shan Liu",
        "summary": "JPEG AI is an emerging learning-based image coding standard developed by Joint Photographic Experts Group (JPEG). The scope of the JPEG AI is the creation of a practical learning-based image coding standard offering a single-stream, compact compressed domain representation, targeting both human visualization and machine consumption. Scheduled for completion in early 2025, the first version of JPEG AI focuses on human vision tasks, demonstrating significant BD-rate reductions compared to existing standards, in terms of MS-SSIM, FSIM, VIF, VMAF, PSNR-HVS, IW-SSIM and NLPD quality metrics. Designed to ensure broad interoperability, JPEG AI incorporates various design features to support deployment across diverse devices and applications. This paper provides an overview of the technical features and characteristics of the JPEG AI standard.",
        "subjects": "Image and Video Processing, Machine Learning, Multimedia",
        "date": "2025-10-13",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.083018"
    },
    {
        "index": "#180",
        "title": "FFT-Accelerated Auxiliary Variable MCMC for Fermionic Lattice Models: A Determinant-Free Approach with $O(N\\log N)$ Complexity",
        "link": "/arxiv/2510.13866",
        "arxiv_id": "2510.13866",
        "authors": "Deqian Kong, Shi Feng, Jianwen Xie, Ying Nian Wu",
        "summary": "We introduce a Markov Chain Monte Carlo (MCMC) algorithm that dramatically accelerates the simulation of quantum many-body systems, a grand challenge in computational science. State-of-the-art methods for these problems are severely limited by $O(N^3)$ computational complexity. Our method avoids this bottleneck, achieving near-linear $O(N \\log N)$ scaling per sweep. Our approach samples a joint probability measure over two coupled variable sets: (1) particle trajectories of the fundamental fermions, and (2) auxiliary variables that decouple fermion interactions. The key innovation is a novel transition kernel for particle trajectories formulated in the Fourier domain, revealing the transition probability as a convolution that enables massive acceleration via the Fast Fourier Transform (FFT). The auxiliary variables admit closed-form, factorized conditional distributions, enabling efficient exact Gibbs sampling update. We validate our algorithm on benchmark quantum physics problems, accurately reproducing known theoretical results and matching traditional $O(N^3)$ algorithms on $32\\times 32$ lattice simulations at a fraction of the wall-clock time, empirically demonstrating $N \\log N$ scaling. By reformulating a long-standing physics simulation problem in machine learning language, our work provides a powerful tool for large-scale probabilistic inference and opens avenues for physics-inspired generative models.",
        "subjects": "Strongly Correlated Electrons, Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2025-10-13",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.083344"
    },
    {
        "index": "#181",
        "title": "R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging",
        "link": "/arxiv/2510.13854",
        "arxiv_id": "2510.13854",
        "authors": "Mamadou K. Keita, Christopher Homan, Sebastien Diarra",
        "summary": "We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network's training objective. R2T's novelty lies in its adaptive loss function, which includes a regularization term that teaches the model to handle out-of-vocabulary (OOV) words with principled uncertainty. We frame this work as a case study in a paradigm we call principled learning (PrL), where models are trained with explicit task constraints rather than on labeled examples alone. Our experiments on Zarma part-of-speech (POS) tagging show that the R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy, outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We further show that for more complex tasks like named entity recognition (NER), R2T serves as a powerful pre-training step; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-12",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.083634"
    },
    {
        "index": "#182",
        "title": "EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing",
        "link": "/arxiv/2510.13851",
        "arxiv_id": "2510.13851",
        "authors": "Sicheng Lyu, Yu Gu, Xinyu Wang, Jerry Huang, Sitao Luan, Yufei Cui, Xiao-Wen Chang, Peng Lu",
        "summary": "Large language models (LLMs) require continual updates to rectify outdated or erroneous knowledge. Model editing has emerged as a compelling paradigm for introducing targeted modifications without the computational burden of full retraining. Existing approaches are mainly based on a locate-then-edit framework. However, in sequential editing contexts, where multiple updates are applied over time, they exhibit significant limitations and suffer from catastrophic interference, i.e., new edits compromise previously integrated updates and degrade preserved knowledge. To address these challenges, we introduce EvoEdit, a novel editing strategy that mitigates catastrophic interference through sequential null-space alignment, enabling stable and efficient model editing. By performing sequential null-space alignment for each incoming edit, EvoEdit preserves both original and previously modified knowledge representations and maintains output invariance on preserved knowledge even across long edit sequences, effectively mitigating interference. Evaluations on real-world sequential knowledge-editing benchmarks show that EvoEdit achieves better or comparable performance than prior state-of-the-art locate-then-edit techniques, with up to 3.53 times speedup. Overall, these results underscore the necessity of developing more principled approaches for designing LLMs in dynamically evolving information settings, while providing a simple yet effective solution with strong theoretical guarantees.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-11",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.083983"
    },
    {
        "index": "#183",
        "title": "Language steering in latent space to mitigate unintended code-switching",
        "link": "/arxiv/2510.13849",
        "arxiv_id": "2510.13849",
        "authors": "Andrey Goncharov, Nikolai Kondusov, Alexey Zaytsev",
        "summary": "Multilingual Large Language Models (LLMs) often exhibit unintended code-switching, reducing reliability in downstream tasks. We propose latent-space language steering, a lightweight inference-time method that identifies language directions via PCA on parallel translations and steers token embeddings along these axes to control language identity. Our approach mitigates code-switching while preserving semantics with negligible computational overhead and requires only minimal parallel data for calibration. Empirically, we achieve 95-99\\% language classification accuracy using a single principal component and reduce next-token distributional divergence by up to 42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further analyze the layer-wise evolution of language representations, revealing that language identity concentrates in final layers with near-perfect linear separability.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-11",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.084271"
    },
    {
        "index": "#184",
        "title": "On-device System of Compositional Multi-tasking in Large Language Models",
        "link": "/arxiv/2510.13848",
        "arxiv_id": "2510.13848",
        "authors": "Ondrej Bohdal, Konstantinos Theodosiadis, Asterios Mpatziakas, Dimitris Filippidis, Iro Spyrou, Christos Zonios, Anastasios Drosou, Dimosthenis Ioannidis, Kyeng-Hun Lee, Jijoong Moon, Hyeonmok Ko, Mete Ozay, Umberto Michieli",
        "summary": "Large language models (LLMs) are commonly adapted for diverse downstream tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters (LoRA). While adapters can be combined to handle multiple tasks separately, standard approaches struggle when targeting the simultaneous execution of complex tasks, such as generating a translated summary from a long conversation. To address this challenge, we propose a novel approach tailored specifically for compositional multi-tasking scenarios involving summarization and translation. Our technique involves adding a learnable projection layer on top of the combined summarization and translation adapters. This design enables effective integration while maintaining efficiency through reduced computational overhead compared to alternative strategies requiring extensive retraining or sequential processing. We demonstrate the practical viability of our method within an on-device environment by developing an Android app capable of executing compositional tasks seamlessly. Experimental results indicate our solution performs well and is fast in both cloud-based and on-device implementations, highlighting the potential benefits of adopting our framework in real-world applications demanding high-speed operation alongside resource constraints.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-11",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.084655"
    },
    {
        "index": "#185",
        "title": "DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models",
        "link": "/arxiv/2510.13847",
        "arxiv_id": "2510.13847",
        "authors": "Jinbin Zhang, Nasib Ullah, Erik Schultheis, Rohit Babbar",
        "summary": "Speculative decoding (a.k.a. speculative sampling) has become a standard way to accelerate LLM inference: a small drafter proposes multiple tokens and a large target model verifies them once per speculation length. Recently, scaling of the LLM vocabulary has pushed the number of tokens to grow substantially. While verification over the full vocabulary leaves the target model largely unaffected, the O(|V|d) parameters in the drafter's output head become a latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g., FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the target model's vocabulary, ranked in descending order of token frequency. Although this reduces draft-time compute, it is brittle, since: (i) frequency lists are corpus-dependent and require retuning to generalize, and (ii) static shortlists suppress rare or domain-specific tokens, lowering the expected number of tokens per verification step. We propose DynaSpec, a context-dependent dynamic shortlisting mechanism that is robust, speeds up drafting, and generalizes across diverse tasks. Concretely, we introduce lightweight, coarse-grained meta-classifiers that route contexts to a small number of token clusters; the union of the top-k selected clusters forms the drafter's shortlist, while verification retains the full vocabulary and exactness. The meta-classifier finishes its computation earlier than the drafter's hidden state generation by exploiting parallel execution of draft encoding and meta shortlisting on separate streams. On standard speculative-decoding benchmarks, we observe consistent gains in mean accepted length over fixed-shortlist baselines, while context-dependent selection enables smaller shortlists without degrading acceptance.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-11",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.084955"
    },
    {
        "index": "#186",
        "title": "Hybrid Deep Learning Approaches for Classifying Autism from Brain MRI",
        "link": "/arxiv/2510.13841",
        "arxiv_id": "2510.13841",
        "authors": "Ashley Chen",
        "summary": "Autism spectrum disorder (ASD) is most often diagnosed using behavioral evaluations, which can vary between clinicians. Brain imaging, combined with machine learning, may help identify more objective patterns linked to ASD. This project used magnetic resonance imaging (MRI) data from the publicly available ABIDE I dataset (n = 1,112) to test two approaches for classifying ASD and control participants. The first was a 3D convolutional neural network (CNN) trained end-to-end. The second was a hybrid approach that used the CNN as a feature extractor and then applied a support vector machine (SVM) classifier. The baseline CNN reached moderate performance (accuracy = 0.66, AUC = 0.70), while the hybrid CNN + SVM achieved higher overall accuracy (0.76) and AUC (0.80). The hybrid model also produced more balanced results between ASD and control groups. Separating feature extraction and classification improved performance and reduced bias between diagnostic groups. These findings suggest that combining deep learning and traditional machine learning methods could enhance the reliability of MRI-based research on ASD.",
        "subjects": "Neurons and Cognition, Machine Learning",
        "date": "2025-10-11",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.085217"
    },
    {
        "index": "#187",
        "title": "Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning",
        "link": "/arxiv/2510.13832",
        "arxiv_id": "2510.13832",
        "authors": "Minsik Choi, Hyegang Son, Changhoon Kim, Young Geun Kim",
        "summary": "Transformer-based models have achieved remarkable performance in NLP tasks. However, their structural characteristics-multiple layers and attention heads-introduce efficiency challenges in inference and deployment. To address these challenges, various pruning methods have recently been proposed. Notably, gradient-based methods using Head Importance Scores (HIS) have gained traction for interpretability, efficiency, and ability to identify redundant heads. However, HIS alone has limitations as it captures only the gradient-driven contribution, overlooking the diversity of attention patterns. To overcome these limitations, we introduce a novel pruning criterion, HIES (Head Importance-Entropy Score), which integrates head importance scores with attention entropy, providing complementary evidence on per-head contribution. Empirically, HIES-based pruning yields up to 15.2% improvement in model quality and 2.04x improvement in stability over HIS-only methods, enabling substantial model compression without sacrificing either accuracy or stability. Code will be released upon publication.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning",
        "date": "2025-10-10",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.090714"
    },
    {
        "index": "#189",
        "title": "GQVis: A Dataset of Genomics Data Questions and Visualizations for Generative AI",
        "link": "/arxiv/2510.13816",
        "arxiv_id": "2510.13816",
        "authors": "Skylar Sargent Walters, Arthea Valderrama, Thomas C. Smits, David Kouřil, Huyen N. Nguyen, Sehi L'Yi, Devin Lange, Nils Gehlenborg",
        "summary": "Data visualization is a fundamental tool in genomics research, enabling the exploration, interpretation, and communication of complex genomic features. While machine learning models show promise for transforming data into insightful visualizations, current models lack the training foundation for domain-specific tasks. In an effort to provide a foundational resource for genomics-focused model training, we present a framework for generating a dataset that pairs abstract, low-level questions about genomics data with corresponding visualizations. Building on prior work with statistical plots, our approach adapts to the complexity of genomics data and the specialized representations used to depict them. We further incorporate multiple linked queries and visualizations, along with justifications for design choices, figure captions, and image alt-texts for each item in the dataset. We use genomics data retrieved from three distinct genomics data repositories (4DN, ENCODE, Chromoscope) to produce GQVis: a dataset consisting of 1.14 million single-query data points, 628k query pairs, and 589k query chains. The GQVis dataset and generation code are available at https://huggingface.co/datasets/HIDIVE/GQVis and https://github.com/hms-dbmi/GQVis-Generation.",
        "subjects": "Genomics, Artificial Intelligence, Human-Computer Interaction, Machine Learning",
        "date": "2025-09-19",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.091382"
    },
    {
        "index": "#190",
        "title": "Reversing the Lens: Using Explainable AI to Understand Human Expertise",
        "link": "/arxiv/2510.13814",
        "arxiv_id": "2510.13814",
        "authors": "Roussel Rahman, Aashwin Ananda Mishra, Wan-Lin Hu",
        "summary": "Both humans and machine learning models learn from experience, particularly in safety- and reliability-critical domains. While psychology seeks to understand human cognition, the field of Explainable AI (XAI) develops methods to interpret machine learning models. This study bridges these domains by applying computational tools from XAI to analyze human learning. We modeled human behavior during a complex real-world task -- tuning a particle accelerator -- by constructing graphs of operator subtasks. Applying techniques such as community detection and hierarchical clustering to archival operator data, we reveal how operators decompose the problem into simpler components and how these problem-solving structures evolve with expertise. Our findings illuminate how humans develop efficient strategies in the absence of globally optimal solutions, and demonstrate the utility of XAI-based methods for quantitatively studying human cognition.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Machine Learning",
        "date": "2025-09-06",
        "category": "cs.LG",
        "crawl_time": "2025-10-17T11:00:05.091682"
    },
    {
        "index": "#6",
        "title": "MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics",
        "link": "/arxiv/2510.14944",
        "arxiv_id": "2510.14944",
        "authors": "Yuxing Lu, Xukai Zhao, J. Ben Tamo, Micky C. Nnamdi, Rui Peng, Shuang Zeng, Xingyu Hu, Jinzhuo Wang, May D. Wang",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on general text; however, their proficiency in specialized scientific domains that require deep, interconnected knowledge remains largely uncharacterized. Metabolomics presents unique challenges with its complex biochemical pathways, heterogeneous identifier systems, and fragmented databases. To systematically evaluate LLM capabilities in this domain, we introduce MetaBench, the first benchmark for metabolomics assessment. Curated from authoritative public resources, MetaBench evaluates five capabilities essential for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Our evaluation of 25 open- and closed-source LLMs reveals distinct performance patterns across metabolomics tasks: while models perform well on text generation tasks, cross-database identifier grounding remains challenging even with retrieval augmentation. Model performance also decreases on long-tail metabolites with sparse annotations. With MetaBench, we provide essential infrastructure for developing and evaluating metabolomics AI systems, enabling systematic progress toward reliable computational tools for metabolomics research.",
        "subjects": "Computation and Language, Artificial Intelligence, Computational Engineering, Finance, and Science",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.131328"
    },
    {
        "index": "#8",
        "title": "AI-Powered Early Diagnosis of Mental Health Disorders from Real-World Clinical Conversations",
        "link": "/arxiv/2510.14937",
        "arxiv_id": "2510.14937",
        "authors": "Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin",
        "summary": "Mental health disorders remain among the leading cause of disability worldwide, yet conditions such as depression, anxiety, and Post-Traumatic Stress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to subjective assessments, limited clinical resources, and stigma and low awareness. In primary care settings, studies show that providers misidentify depression or anxiety in over 60% of cases, highlighting the urgent need for scalable, accessible, and context-aware diagnostic tools that can support early detection and intervention. In this study, we evaluate the effectiveness of machine learning models for mental health screening using a unique dataset of 553 real-world, semistructured interviews, each paried with ground-truth diagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We benchmark multiple model classes, including zero-shot prompting with GPT-4.1 Mini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank Adaptation (LoRA). Our models achieve over 80% accuracy across diagnostic categories, with especially strongperformance on PTSD (up to 89% accuracy and 98% recall). We also find that using shorter context, focused context segments improves recall, suggesting that focused narrative cues enhance detection sensitivity. LoRA fine-tuning proves both efficient and effective, with lower-rank configurations (e.g., rank 8 and 16) maintaining competitive performance across evaluation metrics. Our results demonstrate that LLM-based models can offer substantial improvements over traditional self-report screening tools, providing a path toward low-barrier, AI-powerd early diagnosis. This work lays the groundwork for integrating machine learning into real-world clinical workflows, particularly in low-resource or high-stigma environments where access to timely mental health care is most limited.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.137802"
    },
    {
        "index": "#10",
        "title": "Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent Generation",
        "link": "/arxiv/2510.14915",
        "arxiv_id": "2510.14915",
        "authors": "Xujun Peng, Anoop Kumar, Jingyu Wu, Parker Glenn, Daben Liu",
        "summary": "Retrieval-Augmented Generation (RAG) systems leverage Large Language Models (LLMs) to generate accurate and reliable responses that are grounded in retrieved context. However, LLMs often generate inconsistent outputs for semantically equivalent inputs, a problem compounded by the scarcity of consistency-focused training data and the limitations of current fine-tuning techniques in enhancing output consistency. We propose a new approach combining systematic synthetic data generation, triplet loss for better embeddings, and a novel layer-wise model merging approach. Using consistency-aware weights derived from intermediate layer activations, our method effectively integrates knowledge from specialized models. Experimental results how that our merged model significantly enhances output consistency, achieving a ~47.5\\% improvement in response similarity over the baseline, thus offering a practical solution for increasing the reliability of an industrial RAG system.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.138881"
    },
    {
        "index": "#12",
        "title": "Midtraining Bridges Pretraining and Posttraining Distributions",
        "link": "/arxiv/2510.14865",
        "arxiv_id": "2510.14865",
        "authors": "Emmy Liu, Graham Neubig, Chenyan Xiong",
        "summary": "Recently, many language models have been pretrained with a \"midtraining\" phase, in which higher quality, often instruction-formatted data, is mixed in at the end of pretraining. Despite the popularity of this practice, there is little scientific understanding of this phase of model training or why it is effective. In this work, we conduct the first systematic investigation of midtraining through controlled experiments with language models pretrained from scratch and fine-tuned on supervised finetuning datasets in different domains. We find that when compared after supervised fine-tuning, the effectiveness of midtraining is highest in the math and code domains, where midtraining can best reduce the syntactic gap between pretraining and posttraining data. In these cases, midtraining consistently outperforms continued pretraining in both in-domain validation loss as well as pretraining data forgetting after posttraining. We conduct ablations on the starting time of the midtraining phase and mixture weights of the midtraining data, using code midtraining as a case study, and find that timing has a greater impact than mixture weights, with earlier introduction of specialized data, yielding greater benefits in-domain as well as preserving general language modeling better. These findings establish midtraining as a domain adaptation technique that compared to continued pretraining yields better performance through reduced forgetting.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.140106"
    },
    {
        "index": "#13",
        "title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models",
        "link": "/arxiv/2510.14853",
        "arxiv_id": "2510.14853",
        "authors": "Guinan Su, Yanwu Yang, Li Shen, Lu Yin, Shiwei Liu, Jonas Geiping",
        "summary": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse expert activation, but often suffer from suboptimal routing decisions due to distribution shifts in deployment. While existing test-time adaptation methods could potentially address these issues, they primarily focus on dense models and require access to external data, limiting their practical applicability to MoE architectures. However, we find that, instead of relying on reference data, we can optimize MoE expert selection on-the-fly based only on input context. As such, we propose \\textit{a data-free, online test-time framework} that continuously adapts MoE routing decisions during text generation without external supervision or data. Our method cycles between two phases: During the prefill stage, and later in regular intervals, we optimize the routing decisions of the model using self-supervision based on the already generated sequence. Then, we generate text as normal, maintaining the modified router until the next adaption. We implement this through lightweight additive vectors that only update router logits in selected layers, maintaining computational efficiency while preventing over-adaptation. The experimental results show consistent performance gains on challenging reasoning tasks while maintaining robustness to context shifts. For example, our method achieves a 5.5\\% improvement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play property, our method naturally complements existing test-time scaling techniques, e.g., achieving 6\\% average gains when incorporated with self-consistency on DeepSeek-V2-Lite.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.140604"
    },
    {
        "index": "#14",
        "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking",
        "link": "/arxiv/2510.14824",
        "arxiv_id": "2510.14824",
        "authors": "Ziqi Dai, Xin Zhang, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang",
        "summary": "In information retrieval, training reranking models mainly focuses on two types of objectives: metric learning (e.g. contrastive loss to increase the predicted scores on relevant query-document pairs) and classification (binary label prediction of relevance vs. irrelevance). For BERT-style encoders, various studies have shown that contrastive learning (CL) can be more effective than discriminative (classification) learning. However, for large language models (LLMs), classification via supervised fine-tuning (SFT), which predicts ''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears more promising as it aligns well with the generative nature of LLMs. This divergence raises a central question: which objective is intrinsically better suited to LLM-based reranking, and what mechanism underlies the difference? In this work, we conduct a comprehensive comparison and analysis between CL and SFT for reranking, taking the universal multimodal retrieval (UMR) as the experimental playground. We first decompose the objectives into two components: weight, which controls the magnitude of those updates, and direction, which guides the model updates, then present a unified framework for understanding their interactions. Through probing experiments, we find that SFT provides a substantially stronger weighting scheme than CL, whereas the preferred scoring direction shows no clear winner. Taken together, these results point to a consistent advantage of SFT over CL for LLM reranking. To further validate our findings, we conduct large-scale training with SFT and present new state-of-the-art rerankers on the MRB benchmark. We also provide ablations on SFT settings and expect our findings to benefit future research and applications in this area.",
        "subjects": "Computation and Language, Computer Vision and Pattern Recognition, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.141186"
    },
    {
        "index": "#15",
        "title": "Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning",
        "link": "/arxiv/2510.14773",
        "arxiv_id": "2510.14773",
        "authors": "Hwiyeol Jo, Joosung Lee, Jaehone Lee, Sang-Woo Lee, Joonsuk Park, Kang Min Yoo",
        "summary": "Evaluating generative models, such as large language models (LLMs), commonly involves question-answering tasks where the final answer is selected based on probability of answer choices. On the other hand, for models requiring reasoning, the method of answer extraction plays a critical role. Our research reveals that the performance of reasoning models and their final answer distributions are highly sensitive to the answer extraction algorithm employed. In order to mitigate this, we propose a basic framework: Answer Regeneration. The method uses an additional model inference, providing the prior input and output prefaced by the prompt \"Answer:\". The final answer is then selected or extracted from the regenerated output. We show that this extraction-rule-agnostic approach exhibits improved performance and enhanced robustness. Furthermore, we have applied this framework to general math problems and open-ended question answering tasks. Our analysis and this framework could offer a more reliable results for model evaluation.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.141690"
    },
    {
        "index": "#16",
        "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes",
        "link": "/arxiv/2510.14763",
        "arxiv_id": "2510.14763",
        "authors": "Yunwen Li, Shuangshuang Ying, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Tianyu Zheng, Xeron Du, Qiguang Chen, Jiajun Shi, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Stephen Huang, Wanxiang Che, Chenghua Lin, Eli Zhang",
        "summary": "Large language models exhibit systematic deficiencies in creative writing, particularly in non-English contexts where training data is scarce and lacks process-level supervision. We present COIG-Writer, a novel Chinese creative writing dataset that captures both diverse outputs and their underlying thought processes through systematic reverse-engineering of high-quality texts. Unlike existing datasets that provide only input-output pairs, COIG-Writer comprises 1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a reverse-engineered prompt, (2) detailed creative reasoning documenting decision-making processes, and (3) the final text. Through comprehensive experiments, we identify a two-component model of creative writing: narrative logic (provided by process supervision) and linguistic expression (maintained by general-purpose data). Our findings reveal three critical insights: (1) Process supervision is highly effective but requires stabilization with general data. A ratio of at least one creative sample to twelve general samples is needed to achieve optimal performance; below this threshold, the win rate progressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities are culturally-bound with no cross-lingual transfer (89.26pp gap between Chinese and English performance), and (3) lexical diversity inversely correlates with creative quality (TTR paradox), suggesting high diversity signals compensatory behavior for logical deficiencies. These findings establish that creative excellence emerges from the interaction between logical scaffolding and linguistic grounding, analogous to how mathematical reasoning enhances but cannot replace linguistic competence in foundation models.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.147539"
    },
    {
        "index": "#17",
        "title": "Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware Code",
        "link": "/arxiv/2510.14756",
        "arxiv_id": "2510.14756",
        "authors": "Manar Abdelatty, Maryam Nouh, Jacob K. Rosenstein, Sherief Reda",
        "summary": "Large Language Models (LLMs) are increasingly used to automate hardware design tasks, including the generation of Verilog code. While early benchmarks focus primarily on functional correctness, efficient hardware design demands additional optimization for synthesis metrics such as area, delay, and power. Existing benchmarks fall short in evaluating these aspects comprehensively: they often lack optimized baselines or testbenches for verification. To address these gaps, we present Pluto, a benchmark and evaluation framework designed to assess the efficiency of LLM-generated Verilog designs. Pluto presents a comprehensive evaluation set of 114 problems with self-checking testbenches and multiple Pareto-optimal reference implementations. Experimental results show that state-of-the-art LLMs can achieve high functional correctness, reaching 78.3\\% at pass@1, but their synthesis efficiency still lags behind expert-crafted implementations, with area efficiency of 63.8\\%, delay efficiency of 65.9\\%, and power efficiency of 64.0\\% at eff@1. This highlights the need for efficiency-aware evaluation frameworks such as Pluto to drive progress in hardware-focused LLM research.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.148048"
    },
    {
        "index": "#18",
        "title": "AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning",
        "link": "/arxiv/2510.14738",
        "arxiv_id": "2510.14738",
        "authors": "Mengzhao Jia, Zhihan Zhang, Ignacio Cases, Zheyuan Liu, Meng Jiang, Peng Qi",
        "summary": "Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.148542"
    },
    {
        "index": "#19",
        "title": "Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms",
        "link": "/arxiv/2510.14718",
        "arxiv_id": "2510.14718",
        "authors": "Xingmeng Zhao, Dan Schumacher, Veronica Rammouz, Anthony Rios",
        "summary": "Artificial intelligence (AI) is rapidly transforming healthcare, enabling fast development of tools like stress monitors, wellness trackers, and mental health chatbots. However, rapid and low-barrier development can introduce risks of bias, privacy violations, and unequal access, especially when systems ignore real-world contexts and diverse user needs. Many recent methods use AI to detect risks automatically, but this can reduce human engagement in understanding how harms arise and who they affect. We present a human-centered framework that generates user stories and supports multi-agent discussions to help people think creatively about potential benefits and harms before deployment. In a user study, participants who read stories recognized a broader range of harms, distributing their responses more evenly across all 13 harm types. In contrast, those who did not read stories focused primarily on privacy and well-being (58.3%). Our findings show that storytelling helped participants speculate about a broader range of harms and benefits and think more creatively about AI's impact on users.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.149014"
    },
    {
        "index": "#20",
        "title": "Semantic Prosody in Machine Translation: the English-Chinese Case of Passive Structures",
        "link": "/arxiv/2510.14662",
        "arxiv_id": "2510.14662",
        "authors": "Xinyue Ma, Pol Pastells, Mireia Farrús, Mariona Taulé",
        "summary": "Semantic prosody is a collocational meaning formed through the co-occurrence of a linguistic unit and a consistent series of collocates, which should be treated separately from semantic meaning. Since words that are literal translations of each other may have different semantic prosody, more attention should be paid to this linguistic property to generate accurate translations. However, current machine translation models cannot handle this problem. To bridge the gap, we propose an approach to teach machine translation models about semantic prosody of a specific structure. We focus on Chinese BEI passives and create a dataset of English-Chinese sentence pairs with the purpose of demonstrating the negative semantic prosody of BEI passives. Then we fine-tune OPUS-MT, NLLB-600M and mBART50 models with our dataset for the English-Chinese translation task. Our results show that fine-tuned MT models perform better on using BEI passives for translating unfavourable content and avoid using it for neutral and favourable content. Also, in NLLB-600M, which is a multilingual model, this knowledge of semantic prosody can be transferred from English-Chinese translation to other language pairs, such as Spanish-Chinese.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.149490"
    },
    {
        "index": "#21",
        "title": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs",
        "link": "/arxiv/2510.14660",
        "arxiv_id": "2510.14660",
        "authors": "Linyue Ma, Yilong Xu, Xiang Long, Zhi Zheng",
        "summary": "Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, \"nugget-as-rubric\", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question's information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.149994"
    },
    {
        "index": "#22",
        "title": "Intent Clustering with Shared Pseudo-Labels",
        "link": "/arxiv/2510.14640",
        "arxiv_id": "2510.14640",
        "authors": "I-Fan Lin, Faegheh Hasibi, Suzan Verberne",
        "summary": "In this paper, we propose an intuitive, training-free and label-free method for intent clustering that makes minimal assumptions using lightweight and open-source LLMs. Many current approaches rely on commercial LLMs, which are costly, and offer limited transparency. Additionally, their methods often explicitly depend on knowing the number of clusters in advance, which is often not the case in realistic settings. To address these challenges, instead of asking the LLM to match similar text directly, we first ask it to generate pseudo-labels for each text, and then perform multi-label classification in this pseudo-label set for each text. This approach is based on the hypothesis that texts belonging to the same cluster will share more labels, and will therefore be closer when encoded into embeddings. These pseudo-labels are more human-readable than direct similarity matches. Our evaluation on four benchmark sets shows that our approach achieves results comparable to and better than recent baselines, while remaining simple and computationally efficient. Our findings indicate that our method can be applied in low-resource scenarios and is stable across multiple models and datasets.",
        "subjects": "Computation and Language, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.150471"
    },
    {
        "index": "#23",
        "title": "RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF",
        "link": "/arxiv/2510.14628",
        "arxiv_id": "2510.14628",
        "authors": "Qing Yang, Zhenghao Liu, Junxin Wang, Yangfan Du, Pengcheng Huang, Tong Xiao",
        "summary": "Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the RLAIF-SPA framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages Prosodic Label Alignment to enhance expressive quality by jointly considering semantic accuracy and prosodic-emotional alignment along four fine-grained dimensions: Structure, Emotion, Speed, and Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the generation of clear and accurate speech. Experiments on the Libri Speech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.150989"
    },
    {
        "index": "#24",
        "title": "Code-driven Number Sequence Calculation: Enhancing the inductive Reasoning Abilities of Large Language Models",
        "link": "/arxiv/2510.14620",
        "arxiv_id": "2510.14620",
        "authors": "Kedi Chen, Zhikai Lei, Xu Guo, Xuecheng Wu, Siyuan Zeng, Jianghao Yin, Yinqi Zhang, Qin Chen, Jie Zhou, Liang He, Qipeng Guo, Kai Chen, Wei Zhang",
        "summary": "Large language models (LLMs) make remarkable progress in reasoning tasks. Among different reasoning modes, inductive reasoning, due to its better alignment with human learning, attracts increasing interest. However, research on inductive reasoning faces certain challenges. First, existing inductive data mostly focuses on superficial regularities while lacking more complex internal patterns. Second, current works merely prompt LLMs or finetune on simple prompt-response pairs, but do not provide precise thinking processes nor implement difficulty control. Unlike previous work, we address these challenges by introducing \\textit{CodeSeq}, a synthetic post-training dataset built from number sequences. We package number sequences into algorithmic problems to discover their general terms, defining a general term generation (GTG) task correspondingly. Our pipeline generates supervised finetuning data by reflecting on failed test cases and incorporating iterative corrections, thereby teaching LLMs to learn autonomous case generation and self-checking. Additionally, it leverages reinforcement learning with a novel Case-Synergy Solvability Scaling Reward based on both solvability, estimated from the problem pass rate, and the success rate of self-directed case generation, enabling models to learn more effectively from both successes and failures. Experimental results show that the models trained with \\textit{CodeSeq} improve on various reasoning tasks and can preserve the models' OOD performance.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.151608"
    },
    {
        "index": "#25",
        "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures",
        "link": "/arxiv/2510.14616",
        "arxiv_id": "2510.14616",
        "authors": "Shuangshuang Ying, Yunwen Li, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Xeron Du, Tianyu Zheng, Yichi Zhang, Letian Ni, Yuyang Cheng, Qiguang Chen, Jingzhe Ding, Shengda Long, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Ge Zhang, Wenhao Huang, Wanxiang Che, Chenghua Lin",
        "summary": "Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.152380"
    },
    {
        "index": "#26",
        "title": "Assessing Socio-Cultural Alignment and Technical Safety of Sovereign LLMs",
        "link": "/arxiv/2510.14565",
        "arxiv_id": "2510.14565",
        "authors": "Kyubyung Chae, Gihoon Kim, Gyuseong Lee, Taesup Kim, Jaejin Lee, Heejin Kim",
        "summary": "Recent trends in LLMs development clearly show growing interest in the use and application of sovereign LLMs. The global debate over sovereign LLMs highlights the need for governments to develop their LLMs, tailored to their unique socio-cultural and historical contexts. However, there remains a shortage of frameworks and datasets to verify two critical questions: (1) how well these models align with users' socio-cultural backgrounds, and (2) whether they maintain safety and technical robustness without exposing users to potential harms and risks. To address this gap, we construct a new dataset and introduce an analytic framework for extracting and evaluating the socio-cultural elements of sovereign LLMs, alongside assessments of their technical robustness. Our experimental results demonstrate that while sovereign LLMs play a meaningful role in supporting low-resource languages, they do not always meet the popular claim that these models serve their target users well. We also show that pursuing this untested claim may lead to underestimating critical quality attributes such as safety. Our study suggests that advancing sovereign LLMs requires a more extensive evaluation that incorporates a broader range of well-grounded and practical criteria.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.158148"
    },
    {
        "index": "#27",
        "title": "Efficient Seq2seq Coreference Resolution Using Entity Representations",
        "link": "/arxiv/2510.14504",
        "arxiv_id": "2510.14504",
        "authors": "Matt Grenander, Shay B. Cohen, Mark Steedman",
        "summary": "Seq2seq coreference models have introduced a new paradigm for coreference resolution by learning to generate text corresponding to coreference labels, without requiring task-specific parameters. While these models achieve new state-of-the-art performance, they do so at the cost of flexibility and efficiency. In particular, they do not efficiently handle incremental settings such as dialogue, where text must processed sequentially. We propose a compressed representation in order to improve the efficiency of these methods in incremental settings. Our method works by extracting and re-organizing entity-level tokens, and discarding the majority of other input tokens. On OntoNotes, our best model achieves just 0.6 CoNLL F1 points below a full-prefix, incremental baseline while achieving a compression ratio of 1.8. On LitBank, where singleton mentions are annotated, it passes state-of-the-art performance. Our results indicate that discarding a wide portion of tokens in seq2seq resolvers is a feasible strategy for incremental coreference resolution.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.158652"
    },
    {
        "index": "#28",
        "title": "LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models",
        "link": "/arxiv/2510.14466",
        "arxiv_id": "2510.14466",
        "authors": "Haolin Li, Haipeng Zhang, Mang Li, Yaohua Wang, Lijie Wen, Yu Zhang, Biqing Huang",
        "summary": "As large language models (LLMs) rapidly advance, performance on high-resource languages (e.g., English, Chinese) is nearing saturation, yet remains substantially lower for low-resource languages (e.g., Urdu, Thai) due to limited training data, machine-translation noise, and unstable cross-lingual alignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language Models), a training framework that robustly improves cross-lingual representations under low-resource conditions while jointly strengthening retrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored Representation Composition Architecture), which anchors low-resource languages to an English semantic space via anchor-based alignment and multi-agent collaborative encoding, preserving geometric stability in a shared embedding space; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a language-aware lightweight reasoning head with consistency regularization on top of Arca's multilingual representations, unifying the training objective to enhance cross-lingual understanding, retrieval, and reasoning robustness. We further construct and release a multilingual product retrieval dataset covering five Southeast Asian and two South Asian languages. Experiments across low-resource benchmarks (cross-lingual retrieval, semantic similarity, and reasoning) show consistent gains and robustness under few-shot and noise-amplified settings; ablations validate the contribution of both Arca and LaSR. Code will be released on GitHub and the dataset on Hugging Face.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.159255"
    },
    {
        "index": "#29",
        "title": "Natural Language Tools: A Natural Language Approach to Tool Calling In Large Language Agents",
        "link": "/arxiv/2510.14453",
        "arxiv_id": "2510.14453",
        "authors": "Reid T. Johnson, Michelle D. Pain, Jordan D. West",
        "summary": "We present Natural Language Tools (NLT), a framework that replaces programmatic JSON tool calling in large language models (LLMs) with natural language outputs. By decoupling tool selection from response generation, NLT eliminates task interference and format constraints that degrade tool call performance. When evaluated across 10 models and 6,400 trials spanning customer service and mental health domains, NLT improves tool calling accuracy by 18.4 percentage points while reducing output variance by 70%. Open-weight models see the largest gains, surpassing flagship closed-weight alternatives, with implications for model training in both reinforcement learning and supervised fine-tuning stages. These improvements persist under prompt perturbations and extend tool-calling capabilities to models lacking native support.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.159796"
    },
    {
        "index": "#30",
        "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents",
        "link": "/arxiv/2510.14438",
        "arxiv_id": "2510.14438",
        "authors": "Rui Wang, Ce Zhang, Jun-Yu Ma, Jianshu Zhang, Hongru Wang, Yi Chen, Boyang Xue, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu, Kam-Fai Wong",
        "summary": "Deep research web agents not only retrieve information from diverse sources such as web environments, files, and multimodal inputs, but more importantly, they need to rigorously analyze and aggregate knowledge for insightful research. However, existing open-source deep research agents predominantly focus on enhancing information-seeking capabilities of web agents to locate specific information, while overlooking the essential need for information aggregation, which would limit their ability to support in-depth research. We propose an Explore to Evolve paradigm to scalably construct verifiable training data for web agents. Begins with proactive online exploration, an agent sources grounded information by exploring the real web. Using the collected evidence, the agent then self-evolves an aggregation program by selecting, composing, and refining operations from 12 high-level logical types to synthesize a verifiable QA pair. This evolution from high-level guidance to concrete operations allowed us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K websites and 11 domains. Based on an open-source agent framework, SmolAgents, we collect supervised fine-tuning trajectories to develop a series of foundation models, WebAggregator. WebAggregator-8B matches the performance of GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text and closely approaches Claude-3.7-sonnet. Moreover, given the limited availability of benchmarks that evaluate web agents' information aggregation abilities, we construct a human-annotated evaluation split of WebAggregatorQA as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves 28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all references, they still struggle on WebAggregatorQA, highlighting the need to strengthen the information aggregation capabilities of web agent foundations.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.160452"
    },
    {
        "index": "#31",
        "title": "Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following",
        "link": "/arxiv/2510.14420",
        "arxiv_id": "2510.14420",
        "authors": "Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu",
        "summary": "Language models often struggle to follow multi-constraint instructions that are crucial for real-world applications. Existing reinforcement learning (RL) approaches suffer from dependency on external supervision and sparse reward signals from multi-constraint tasks. We propose a label-free self-supervised RL framework that eliminates dependency on external supervision by deriving reward signals directly from instructions and generating pseudo-labels for reward model training. Our approach introduces constraint decomposition strategies and efficient constraint-wise binary classification to address sparse reward challenges while maintaining computational efficiency. Experiments show that our approach generalizes well, achieving strong improvements across 3 in-domain and 5 out-of-domain datasets, including challenging agentic and multi-turn instruction following. The data and code are publicly available at https://github.com/Rainier-rq/verl-if",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.161085"
    },
    {
        "index": "#32",
        "title": "MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering",
        "link": "/arxiv/2510.14400",
        "arxiv_id": "2510.14400",
        "authors": "Yingpeng Ning, Yuanyuan Sun, Ling Luo, Yanhua Wang, Yuchen Pan, Hongfei Lin",
        "summary": "Biomedical question answering (QA) requires accurate interpretation of complex medical knowledge. Large language models (LLMs) have shown promising capabilities in this domain, with retrieval-augmented generation (RAG) systems enhancing performance by incorporating external medical literature. However, RAG-based approaches in biomedical QA suffer from hallucinations due to post-retrieval noise and insufficient verification of retrieved evidence, undermining response reliability. We propose MedTrust-Guided Iterative RAG, a framework designed to enhance factual consistency and mitigate hallucinations in medical QA. Our method introduces three key innovations. First, it enforces citation-aware reasoning by requiring all generated content to be explicitly grounded in retrieved medical documents, with structured Negative Knowledge Assertions used when evidence is insufficient. Second, it employs an iterative retrieval-verification process, where a verification agent assesses evidence adequacy and refines queries through Medical Gap Analysis until reliable information is obtained. Third, it integrates the MedTrust-Align Module (MTAM) that combines verified positive examples with hallucination-aware negative samples, leveraging Direct Preference Optimization to reinforce citation-grounded reasoning while penalizing hallucination-prone response patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our approach consistently outperforms competitive baselines across multiple model architectures, achieving the best average accuracy with gains of 2.7% for LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.161694"
    },
    {
        "index": "#33",
        "title": "Your Next Token Prediction: A Multilingual Benchmark for Personalized Response Generation",
        "link": "/arxiv/2510.14398",
        "arxiv_id": "2510.14398",
        "authors": "Shiyao Ding, Takayuki Ito",
        "summary": "Large language models (LLMs) excel at general next-token prediction but still struggle to generate responses that reflect how individuals truly communicate, such as replying to emails or social messages in their own style. However, real SNS or email histories are difficult to collect due to privacy concerns. To address this, we propose the task of \"Your Next Token Prediction (YNTP)\", which models a user's precise word choices through controlled human-agent conversations. We build a multilingual benchmark of 100 dialogue sessions across English, Japanese, and Chinese, where users interact for five days with psychologically grounded NPCs based on MBTI dimensions. This setup captures natural, daily-life communication patterns and enables analysis of users' internal models. We evaluate prompt-based and fine-tuning-based personalization methods, establishing the first benchmark for YNTP and a foundation for user-aligned language modeling. The dataset is available at: https://github.com/AnonymousHub4Submissions/your-next-token-prediction-dataset-100",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.162175"
    },
    {
        "index": "#34",
        "title": "Suicidal Comment Tree Dataset: Enhancing Risk Assessment and Prediction Through Contextual Analysis",
        "link": "/arxiv/2510.14395",
        "arxiv_id": "2510.14395",
        "authors": "Jun Li, Qun Zhao",
        "summary": "Suicide remains a critical global public health issue. While previous studies have provided valuable insights into detecting suicidal expressions in individual social media posts, limited attention has been paid to the analysis of longitudinal, sequential comment trees for predicting a user's evolving suicidal risk. Users, however, often reveal their intentions through historical posts and interactive comments over time. This study addresses this gap by investigating how the information in comment trees affects both the discrimination and prediction of users' suicidal risk levels. We constructed a high-quality annotated dataset, sourced from Reddit, which incorporates users' posting history and comments, using a refined four-label annotation framework based on the Columbia Suicide Severity Rating Scale (C-SSRS). Statistical analysis of the dataset, along with experimental results from Large Language Models (LLMs) experiments, demonstrates that incorporating comment trees data significantly enhances the discrimination and prediction of user suicidal risk levels. This research offers a novel insight to enhancing the detection accuracy of at-risk individuals, thereby providing a valuable foundation for early suicide intervention strategies.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.167760"
    },
    {
        "index": "#36",
        "title": "From Binary to Bilingual: How the National Weather Service is Using Artificial Intelligence to Develop a Comprehensive Translation Program",
        "link": "/arxiv/2510.14369",
        "arxiv_id": "2510.14369",
        "authors": "Joseph E. Trujillo-Falcon, Monica L. Bozeman, Liam E. Llewellyn, Samuel T. Halvorson, Meryl Mizell, Stuti Deshpande, Bob Manning, Todd Fagin",
        "summary": "To advance a Weather-Ready Nation, the National Weather Service (NWS) is developing a systematic translation program to better serve the 68.8 million people in the U.S. who do not speak English at home. This article outlines the foundation of an automated translation tool for NWS products, powered by artificial intelligence. The NWS has partnered with LILT, whose patented training process enables large language models (LLMs) to adapt neural machine translation (NMT) tools for weather terminology and messaging. Designed for scalability across Weather Forecast Offices (WFOs) and National Centers, the system is currently being developed in Spanish, Simplified Chinese, Vietnamese, and other widely spoken non-English languages. Rooted in best practices for multilingual risk communication, the system provides accurate, timely, and culturally relevant translations, significantly reducing manual translation time and easing operational workloads across the NWS. To guide the distribution of these products, GIS mapping was used to identify language needs across different NWS regions, helping prioritize resources for the communities that need them most. We also integrated ethical AI practices throughout the program's design, ensuring that transparency, fairness, and human oversight guide how automated translations are created, evaluated, and shared with the public. This work has culminated into a website featuring experimental multilingual NWS products, including translated warnings, 7-day forecasts, and educational campaigns, bringing the country one step closer to a national warning system that reaches all Americans.",
        "subjects": "Computation and Language, Artificial Intelligence, Computers and Society, Human-Computer Interaction",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.168436"
    },
    {
        "index": "#37",
        "title": "On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?",
        "link": "/arxiv/2510.14365",
        "arxiv_id": "2510.14365",
        "authors": "Anyun Zhuo, Xuefei Ning, Ningyuan Li, Yu Wang, Pinyan Lu",
        "summary": "This work investigates the resilience of contemporary LLMs against frequent and structured character-level perturbations, specifically through the insertion of noisy characters after each input character. We introduce \\nameshort{}, a practical method that inserts invisible Unicode control characters into text to discourage LLM misuse in scenarios such as online exam systems. Surprisingly, despite strong obfuscation that fragments tokenization and reduces the signal-to-noise ratio significantly, many LLMs still maintain notable performance. Through comprehensive evaluation across model-, problem-, and noise-related configurations, we examine the extent and mechanisms of this robustness, exploring both the handling of character-level tokenization and \\textit{implicit} versus \\textit{explicit} denoising mechanism hypotheses of character-level noises. We hope our findings on the low-level robustness of LLMs will shed light on the risks of their misuse and on the reliability of deploying LLMs across diverse applications.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.168731"
    },
    {
        "index": "#38",
        "title": "CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical Question Answering",
        "link": "/arxiv/2510.14353",
        "arxiv_id": "2510.14353",
        "authors": "Ziad Elshaer, Essam A. Rashed",
        "summary": "High-performing medical Large Language Models (LLMs) typically require extensive fine-tuning with substantial computational resources, limiting accessibility for resource-constrained healthcare institutions. This study introduces a confidence-driven multi-model framework that leverages model diversity to enhance medical question answering without fine-tuning. Our framework employs a two-stage architecture: a confidence detection module assesses the primary model's certainty, and an adaptive routing mechanism directs low-confidence queries to Helper models with complementary knowledge for collaborative reasoning. We evaluate our approach using Qwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B across three medical benchmarks; MedQA, MedMCQA, and PubMedQA. Result demonstrate that our framework achieves competitive performance, with particularly strong results in PubMedQA (95.0\\%) and MedMCQA (78.0\\%). Ablation studies confirm that confidence-aware routing combined with multi-model collaboration substantially outperforms single-model approaches and uniform reasoning strategies. This work establishes that strategic model collaboration offers a practical, computationally efficient pathway to improve medical AI systems, with significant implications for democratizing access to advanced medical AI in resource-limited settings.",
        "subjects": "Computation and Language, Artificial Intelligence, Medical Physics",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.169016"
    },
    {
        "index": "#39",
        "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts",
        "link": "/arxiv/2510.14351",
        "arxiv_id": "2510.14351",
        "authors": "Perapard Ngokpol, Kun Kerdthaisong, Pasin Buakhaw, Pitikorn Khlaisamniang, Supasate Vorathammathorn, Piyalitt Ittichaiwong, Nutchanon Yongsatianchot",
        "summary": "Large language models (LLMs) are increasingly used as role-playing agents, yet their capacity to faithfully and consistently portray version-specific characters -- for example, superheroes across comic and cinematic universes -- remains underexplored. Superhero canons such as Marvel and DC provide a rich testbed: decades of storytelling yield multiple incarnations of the same character with distinct histories, values, and moral codes. To study this problem, we introduce Beyond One World, a benchmark for character-grounded roleplay spanning 30 iconic heroes and 90 canon-specific versions. The benchmark comprises two tasks: (i) Canon Events, which probes factual recall of pivotal life stages, and (ii) Moral Dilemmas, which confronts models with ethically charged scenarios. We score responses for canonical accuracy and reasoning fidelity under a framework that separates internal deliberation (\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act Matching, a metric that quantifies alignment between reasons and actions and serves as a proxy for model trustworthiness. Experiments across reasoning- and non-reasoning-oriented models yield three findings: (1) chain-of-thought prompting improves narrative coherence in weaker models but can reduce canonical accuracy in stronger ones; (2) cross-version generalization within a character remains a major obstacle; and (3) models often excel at either thinking or acting, but rarely both. Beyond One World exposes critical gaps in multiversal consistency and reasoning alignment, offering a challenging evaluation for role-playing LLMs.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.169348"
    },
    {
        "index": "#42",
        "title": "MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking",
        "link": "/arxiv/2510.14307",
        "arxiv_id": "2510.14307",
        "authors": "Sathyanarayanan Ramamoorthy, Vishwa Shah, Simran Khanuja, Zaid Sheikh, Shan Jie, Ann Chia, Shearman Chua, Graham Neubig",
        "summary": "This paper introduces MERLIN, a novel testbed system for the task of Multilingual Multimodal Entity Linking. The created dataset includes BBC news article titles, paired with corresponding images, in five languages: Hindi, Japanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity mentions linked to 2,500 unique Wikidata entities. We also include several benchmarks using multilingual and multimodal entity linking methods exploring different language models like LLaMa-2 and Aya-23. Our findings indicate that incorporating visual data improves the accuracy of entity linking, especially for entities where the textual context is ambiguous or insufficient, and particularly for models that do not have strong multilingual abilities. For the work, the dataset, methods are available here at https://github.com/rsathya4802/merlin",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.170291"
    },
    {
        "index": "#43",
        "title": "MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning",
        "link": "/arxiv/2510.14305",
        "arxiv_id": "2510.14305",
        "authors": "Mahbub E Sobhani, Md. Faiyaz Abdullah Sayeedi, Tasnim Mohiuddin, Md Mofijul Islam, Swakkhar Shatabda",
        "summary": "Mathematical reasoning remains one of the most challenging domains for large language models (LLMs), requiring not only linguistic understanding but also structured logical deduction and numerical precision. While recent LLMs demonstrate strong general-purpose reasoning abilities, their mathematical competence across diverse languages remains underexplored. Existing benchmarks primarily focus on English or a narrow subset of high-resource languages, leaving significant gaps in assessing multilingual and cross-lingual mathematical reasoning. To address this, we introduce MathMist, a parallel multilingual benchmark for mathematical problem solving and reasoning. MathMist encompasses over 21K aligned question-answer pairs across seven languages, representing a balanced coverage of high-, medium-, and low-resource linguistic settings. The dataset captures linguistic variety, multiple types of problem settings, and solution synthesizing capabilities. We systematically evaluate a diverse suite of models, including open-source small and medium LLMs, proprietary systems, and multilingual-reasoning-focused models, under zero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our results reveal persistent deficiencies in LLMs' ability to perform consistent and interpretable mathematical reasoning across languages, with pronounced degradation in low-resource settings. All the codes and data are available at GitHub: https://github.com/mahbubhimel/MathMist",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.170580"
    },
    {
        "index": "#45",
        "title": "Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL",
        "link": "/arxiv/2510.14296",
        "arxiv_id": "2510.14296",
        "authors": "Md Mahadi Hasan Nahid, Davood Rafiei, Weiwei Zhang, Yong Zhang",
        "summary": "Schema linking -- the process of aligning natural language questions with database schema elements -- is a critical yet underexplored component of Text-to-SQL systems. While recent methods have focused primarily on improving SQL generation, they often neglect the retrieval of relevant schema elements, which can lead to hallucinations and execution failures. In this work, we propose a context-aware bidirectional schema retrieval framework that treats schema linking as a standalone problem. Our approach combines two complementary strategies: table-first retrieval followed by column selection, and column-first retrieval followed by table selection. It is further augmented with techniques such as question decomposition, keyword extraction, and keyphrase extraction. Through comprehensive evaluations on challenging benchmarks such as BIRD and Spider, we demonstrate that our method significantly improves schema recall while reducing false positives. Moreover, SQL generation using our retrieved schema consistently outperforms full-schema baselines and closely approaches oracle performance, all without requiring query refinement. Notably, our method narrows the performance gap between full and perfect schema settings by 50\\%. Our findings highlight schema linking as a powerful lever for enhancing Text-to-SQL accuracy and efficiency.",
        "subjects": "Computation and Language, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.171143"
    },
    {
        "index": "#46",
        "title": "PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering",
        "link": "/arxiv/2510.14278",
        "arxiv_id": "2510.14278",
        "authors": "Md Mahadi Hasan Nahid, Davood Rafiei",
        "summary": "Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG -- demonstrates that our approach consistently outperforms strong baselines.",
        "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.171438"
    },
    {
        "index": "#47",
        "title": "Qwen3Guard Technical Report",
        "link": "/arxiv/2510.14276",
        "arxiv_id": "2510.14276",
        "authors": "Haiquan Zhao, Chenhan Yuan, Fei Huang, Xiaomeng Hu, Yichang Zhang, An Yang, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin, Baosong Yang, Chen Cheng, Jialong Tang, Jiandong Jiang, Jianwei Zhang, Jijie Xu, Ming Yan, Minmin Sun, Pei Zhang, Pengjun Xie, Qiaoyu Tang, Qin Zhu, Rong Zhang, Shibin Wu, Shuo Zhang, Tao He, Tianyi Tang, Tingyu Xia, Wei Liao, Weizhou Shen, Wenbiao Yin, Wenmeng Zhou, Wenyuan Yu, Xiaobin Wang, Xiaodong Deng, Xiaodong Xu, Xinyu Zhang, Yang Liu, Yeqiu Li, Yi Zhang, Yong Jiang, Yu Wan, Yuxin Zhou",
        "summary": "As large language models (LLMs) become more capable and widely used, ensuring the safety of their outputs is increasingly critical. Existing guardrail models, though useful in static evaluation settings, face two major limitations in real-world applications: (1) they typically output only binary \"safe/unsafe\" labels, which can be interpreted inconsistently across diverse safety policies, rendering them incapable of accommodating varying safety tolerances across domains; and (2) they require complete model outputs before performing safety checks, making them fundamentally incompatible with streaming LLM inference, thereby preventing timely intervention during generation and increasing exposure to harmful partial outputs. To address these challenges, we present Qwen3Guard, a series of multilingual safety guardrail models with two specialized variants: Generative Qwen3Guard, which casts safety classification as an instruction-following task to enable fine-grained tri-class judgments (safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a token-level classification head for real-time safety monitoring during incremental text generation. Both variants are available in three sizes (0.6B, 4B, and 8B parameters) and support up to 119 languages and dialects, providing comprehensive, scalable, and low-latency safety moderation for global LLM deployments. Evaluated across English, Chinese, and multilingual benchmarks, Qwen3Guard achieves state-of-the-art performance in both prompt and response safety classification. All models are released under the Apache 2.0 license for public use.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.172098"
    },
    {
        "index": "#48",
        "title": "Retrofitting Small Multilingual Models for Retrieval: Matching 7B Performance with 300M Parameters",
        "link": "/arxiv/2510.14274",
        "arxiv_id": "2510.14274",
        "authors": "Lifu Tu, Yingbo Zhou, Semih Yavuz",
        "summary": "Training effective multilingual embedding models presents unique challenges due to the diversity of languages and task objectives. Although small multilingual models (<1 B parameters) perform well on multilingual tasks generally, they consistently lag behind larger models (>1 B) in the most prevalent use case: retrieval. This raises a critical question: Can smaller models be retrofitted specifically for retrieval tasks to enhance their performance? In this work, we investigate key factors that influence the effectiveness of multilingual embeddings, focusing on training data scale, negative sampling strategies, and data diversity. We find that while increasing the scale of training data yields initial performance gains, these improvements quickly plateau - indicating diminishing returns. Incorporating hard negatives proves essential for consistently improving retrieval accuracy. Furthermore, our analysis reveals that task diversity in the training data contributes more significantly to performance than language diversity alone. As a result, we develop a compact (approximately 300M) multilingual model that achieves retrieval performance comparable to or even surpassing current strong 7B models.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.172376"
    },
    {
        "index": "#50",
        "title": "Rewriting History: A Recipe for Interventional Analyses to Study Data Effects on Model Behavior",
        "link": "/arxiv/2510.14261",
        "arxiv_id": "2510.14261",
        "authors": "Rahul Nadkarni, Yanai Elazar, Hila Gonen, Noah A. Smith",
        "summary": "We present an experimental recipe for studying the relationship between training data and language model (LM) behavior. We outline steps for intervening on data batches -- i.e., ``rewriting history'' -- and then retraining model checkpoints over that data to test hypotheses relating data to behavior. Our recipe breaks down such an intervention into stages that include selecting evaluation items from a benchmark that measures model behavior, matching relevant documents to those items, and modifying those documents before retraining and measuring the effects. We demonstrate the utility of our recipe through case studies on factual knowledge acquisition in LMs, using both cooccurrence statistics and information retrieval methods to identify documents that might contribute to knowledge learning. Our results supplement past observational analyses that link cooccurrence to model behavior, while demonstrating that extant methods for identifying relevant training documents do not fully explain an LM's ability to correctly answer knowledge questions. Overall, we outline a recipe that researchers can follow to test further hypotheses about how training data affects model behavior. Our code is made publicly available to promote future work.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.178138"
    },
    {
        "index": "#51",
        "title": "MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems",
        "link": "/arxiv/2510.14252",
        "arxiv_id": "2510.14252",
        "authors": "Jihao Zhao, Zhiyuan Ji, Simin Niu, Hanyu Wang, Feiyu Xiong, Zhiyu Li",
        "summary": "The traditional RAG paradigm, which typically engages in the comprehension of relevant text chunks in response to received queries, inherently restricts both the depth of knowledge internalization and reasoning capabilities. To address this limitation, our research transforms the text processing in RAG from passive chunking to proactive understanding, defining this process as document memory extraction with the objective of simulating human cognitive processes during reading. Building upon this, we propose the Mixtures of scenario-aware document Memories (MoM) framework, engineered to efficiently handle documents from multiple domains and train small language models (SLMs) to acquire the ability to proactively explore and construct document memories. The MoM initially instructs large language models (LLMs) to simulate domain experts in generating document logical outlines, thereby directing structured chunking and core content extraction. It employs a multi-path sampling and multi-perspective evaluation mechanism, specifically designing comprehensive metrics that represent chunk clarity and extraction completeness to select the optimal document memories. Additionally, to infuse deeper human-like reading abilities during the training of SLMs, we incorporate a reverse reasoning strategy, which deduces refined expert thinking paths from high-quality outcomes. Finally, leveraging diverse forms of content generated by MoM, we develop a three-layer document memory retrieval mechanism, which is grounded in our theoretical proof from the perspective of probabilistic modeling. Extensive experimental results across three distinct domains demonstrate that the MoM framework not only resolves text chunking challenges in existing RAG systems, providing LLMs with semantically complete document memories, but also paves the way for SLMs to achieve human-centric intelligent text processing.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.178449"
    },
    {
        "index": "#53",
        "title": "LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning",
        "link": "/arxiv/2510.14211",
        "arxiv_id": "2510.14211",
        "authors": "Beomseok Kang, Jiwon Song, Jae-Joon Kim",
        "summary": "Multi-stage reasoning has emerged as an effective strategy for enhancing the reasoning capability of small language models by decomposing complex problems into sequential sub-stages. However, this comes at the cost of increased latency. We observe that existing adaptive acceleration techniques, such as layer skipping, struggle to balance efficiency and accuracy in this setting due to two key challenges: (1) stage-wise variation in skip sensitivity, and (2) the generation of redundant output tokens. To address these, we propose LiteStage, a latency-aware layer skipping framework for multi-stage reasoning. LiteStage combines a stage-wise offline search that allocates optimal layer budgets with an online confidence-based generation early exit to suppress unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than 4.0% accuracy loss, outperforming prior training-free layer skipping methods.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.179105"
    },
    {
        "index": "#54",
        "title": "DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans",
        "link": "/arxiv/2510.14205",
        "arxiv_id": "2510.14205",
        "authors": "Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang",
        "summary": "The emerging large language model role-playing agents (LLM RPAs) aim to simulate individual human behaviors, but the persona fidelity is often undermined by manually-created profiles (e.g., cherry-picked information and personality characteristics) without validating the alignment with the target individuals. To address this limitation, our work introduces the Dynamic Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM RPAs' behaviors with those of target individuals by iteratively identifying the cognitive divergence, either through free-form or theory-grounded, structured analysis, between generated behaviors and human ground truth, and refining the persona profile to mitigate these divergences.We evaluate DPRF with five LLMs on four diverse behavior-prediction scenarios: formal debates, social media posts with mental health issues, public interviews, and movie reviews.DPRF can consistently improve behavioral alignment considerably over baseline personas and generalizes across models and scenarios.Our work provides a robust methodology for creating high-fidelity persona profiles and enhancing the validity of downstream applications, such as user simulation, social studies, and personalized AI.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.179603"
    },
    {
        "index": "#55",
        "title": "RLSR: Reinforcement Learning with Supervised Reward Outperforms SFT in Instruction Following",
        "link": "/arxiv/2510.14200",
        "arxiv_id": "2510.14200",
        "authors": "Zhichao Wang, Andy Wong, Ruslan Belkin",
        "summary": "After the pretraining stage of LLMs, techniques such as SFT, RLHF, RLVR, and RFT are applied to enhance instruction-following ability, mitigate undesired responses, improve reasoning capability and enable efficient domain adaptation with minimal data. SFT relies on the next-token prediction objective to strengthen instruction following in a base model using a large corpus of human-labeled responses. In contrast, RFT employs a RL-based approach to adapt fine-tuned reasoning models to specific domains with limited supervision. Inspired by RFT, we propose replacing SFT with RLSR to leverage the extensive SFT dataset in an RL framework, thereby improving the base model's instruction-following ability. In RLSR, the base model generates multiple responses for each prompt, and reward scores are computed as the cosine similarity in the semantic embedding space between the generated and human-labeled responses. RLSR can be utilized in multiple ways. It can directly replace SFT, achieving superior performance on instruction-following benchmarks-for example, RLSR (SB) on Qwen-7B (INFINITY) achieved an AlpacaEval win rate of 26.34%, surpassing SFT's 21.01%. Furthermore, combining SFT and RLSR further enhances downstream task performance; Qwen-7B (INFINITY) achieved a win rate of 30.73% when trained with SFT + RLSR.",
        "subjects": "Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.180072"
    },
    {
        "index": "#56",
        "title": "Building a Macedonian Recipe Dataset: Collection, Parsing, and Comparative Analysis",
        "link": "/arxiv/2510.14128",
        "arxiv_id": "2510.14128",
        "authors": "Darko Sasanski, Dimitar Peshevski, Riste Stojanov, Dimitar Trajanov",
        "summary": "Computational gastronomy increasingly relies on diverse, high-quality recipe datasets to capture regional culinary traditions. Although there are large-scale collections for major languages, Macedonian recipes remain under-represented in digital research. In this work, we present the first systematic effort to construct a Macedonian recipe dataset through web scraping and structured parsing. We address challenges in processing heterogeneous ingredient descriptions, including unit, quantity, and descriptor normalization. An exploratory analysis of ingredient frequency and co-occurrence patterns, using measures such as Pointwise Mutual Information and Lift score, highlights distinctive ingredient combinations that characterize Macedonian cuisine. The resulting dataset contributes a new resource for studying food culture in underrepresented languages and offers insights into the unique patterns of Macedonian culinary tradition.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.180529"
    },
    {
        "index": "#57",
        "title": "Toward Cybersecurity-Expert Small Language Models",
        "link": "/arxiv/2510.14113",
        "arxiv_id": "2510.14113",
        "authors": "Matan Levi, Daniel Ohayon, Ariel Blobstein, Ravid Sagi, Ian Molloy, Yair Allouche",
        "summary": "Large language models (LLMs) are transforming everyday applications, yet deployment in cybersecurity lags due to a lack of high-quality, domain-specific models and training datasets. To address this gap, we present CyberPal 2.0, a family of cybersecurity-expert small language models (SLMs) ranging from 4B-20B parameters. To train CyberPal 2.0, we generate an enriched chain-of-thought cybersecurity instruction dataset built with our data enrichment and formatting pipeline, SecKnowledge 2.0, which integrates expert-in-the-loop steering of reasoning formats alongside LLM-driven multi-step grounding, yielding higher-fidelity, task-grounded reasoning traces for security tasks. Across diverse cybersecurity benchmarks, CyberPal 2.0 consistently outperforms its baselines and matches or surpasses various open and closed-source frontier models, while remaining a fraction of their size. On core cyber threat intelligence knowledge tasks, our models outperform almost all tested frontier models, ranking second only to Sec-Gemini v1. On core threat-investigation tasks, such as correlating vulnerabilities and bug tickets with weaknesses, our best 20B-parameter model outperforms GPT-4o, o1, o3-mini, and Sec-Gemini v1, ranking first, while our smallest 4B-parameter model ranks second.",
        "subjects": "Computation and Language, Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.181065"
    },
    {
        "index": "#58",
        "title": "DROID: Dual Representation for Out-of-Scope Intent Detection",
        "link": "/arxiv/2510.14110",
        "arxiv_id": "2510.14110",
        "authors": "Wael Rashwan, Hossam M. Zawbaa, Sourav Dutta, Haytham Assem",
        "summary": "Detecting out-of-scope (OOS) user utterances remains a key challenge in task-oriented dialogue systems and, more broadly, in open-set intent recognition. Existing approaches often depend on strong distributional assumptions or auxiliary calibration modules. We present DROID (Dual Representation for Out-of-Scope Intent Detection), a compact end-to-end framework that combines two complementary encoders -- the Universal Sentence Encoder (USE) for broad semantic generalization and a domain-adapted Transformer-based Denoising Autoencoder (TSDAE) for domain-specific contextual distinctions. Their fused representations are processed by a lightweight branched classifier with a single calibrated threshold that separates in-domain and OOS intents without post-hoc scoring. To enhance boundary learning under limited supervision, DROID incorporates both synthetic and open-domain outlier augmentation. Despite using only 1.5M trainable parameters, DROID consistently outperforms recent state-of-the-art baselines across multiple intent benchmarks, achieving macro-F1 improvements of 6--15% for known and 8--20% for OOS intents, with the most significant gains in low-resource settings. These results demonstrate that dual-encoder representations with simple calibration can yield robust, scalable, and reliable OOS detection for neural dialogue systems.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.181521"
    },
    {
        "index": "#59",
        "title": "ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models",
        "link": "/arxiv/2510.14077",
        "arxiv_id": "2510.14077",
        "authors": "Haziq Mohammad Khalid, Athikash Jeyaganthan, Timothy Do, Yicheng Fu, Sean O'Brien, Vasu Sharma, Kevin Zhu",
        "summary": "Large Language Models (LLMs) suffer significant performance degradation in multi-turn conversations when information is presented incrementally. Given that multi-turn conversations characterize everyday interactions with LLMs, this degradation poses a severe challenge to real world usability. We hypothesize that abrupt increases in model uncertainty signal misalignment in multi-turn LLM interactions, and we exploit this insight to dynamically realign conversational context. We introduce ERGO (Entropy-guided Resetting for Generation Optimization), which continuously quantifies internal uncertainty via Shannon entropy over next token distributions and triggers adaptive prompt consolidation when a sharp spike in entropy is detected. By treating uncertainty as a first class signal rather than a nuisance to eliminate, ERGO embraces variability in language and modeling, representing and responding to uncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO yields a 56.6% average performance gain over standard baselines, increases aptitude (peak performance capability) by 24.7%, and decreases unreliability (variability in performance) by 35.3%, demonstrating that uncertainty aware interventions can improve both accuracy and reliability in conversational AI.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.182031"
    },
    {
        "index": "#60",
        "title": "Quantifying Phonosemantic Iconicity Distributionally in 6 Languages",
        "link": "/arxiv/2510.14040",
        "arxiv_id": "2510.14040",
        "authors": "George Flint, Kaustubh Kislay",
        "summary": "Language is, as commonly theorized, largely arbitrary. Yet, systematic relationships between phonetics and semantics have been observed in many specific cases. To what degree could those systematic relationships manifest themselves in large scale, quantitative investigations--both in previously identified and unidentified phenomena? This work undertakes a distributional approach to quantifying phonosemantic iconicity at scale across 6 diverse languages (English, Spanish, Hindi, Finnish, Turkish, and Tamil). In each language, we analyze the alignment of morphemes' phonetic and semantic similarity spaces with a suite of statistical measures, and discover an array of interpretable phonosemantic alignments not previously identified in the literature, along with crosslinguistic patterns. We also analyze 5 previously hypothesized phonosemantic alignments, finding support for some such alignments and mixed results for others.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.182474"
    },
    {
        "index": "#62",
        "title": "CRaFT: An Explanation-Based Framework for Evaluating Cultural Reasoning in Multilingual Language Models",
        "link": "/arxiv/2510.14014",
        "arxiv_id": "2510.14014",
        "authors": "Shehenaz Hossain, Haithem Afli",
        "summary": "Correct answers do not necessarily reflect cultural understanding. We introduce CRaFT, an explanation-based multilingual evaluation framework designed to assess how large language models (LLMs) reason across cultural contexts. Rather than scoring outputs solely based on accuracy, CRaFT evaluates model explanations using four interpretable metrics: Cultural Fluency, Deviation, Consistency, and Linguistic Adaptation. We apply the framework to 50 culturally grounded questions from the World Values Survey, translated into Arabic, Bengali, and Spanish, and evaluate three models (GPT, DeepSeek, and FANAR) across over 2,100 answer-explanation pairs. Results reveal significant cross-lingual variation in reasoning: Arabic reduces fluency, Bengali enhances it, and Spanish remains largely stable. While GPT adapts more effectively across languages, it exhibits lower consistency; FANAR shows stable but rigid reasoning. These findings suggest that cultural awareness in LLMs is not intrinsic but emerges through linguistic framing. CRaFT offers a new lens for evaluating cross-cultural reasoning in multilingual settings, providing actionable insights for building culturally adaptive language models.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.188543"
    },
    {
        "index": "#63",
        "title": "The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models",
        "link": "/arxiv/2510.13996",
        "arxiv_id": "2510.13996",
        "authors": "Lukas Gienapp, Christopher Schröder, Stefan Schweter, Christopher Akiki, Ferdinand Schlatt, Arden Zimmermann, Phillipe Genêt, Martin Potthast",
        "summary": "Large language model development relies on large-scale training corpora, yet most contain data of unclear licensing status, limiting the development of truly open models. This problem is exacerbated for non-English languages, where openly licensed text remains critically scarce. We introduce the German Commons, the largest collection of openly licensed German text to date. It compiles data from 41 sources across seven domains, encompassing legal, scientific, cultural, political, news, economic, and web text. Through systematic sourcing from established data providers with verifiable licensing, it yields 154.56 billion tokens of high-quality text for language model training. Our processing pipeline implements comprehensive quality filtering, deduplication, and text formatting fixes, ensuring consistent quality across heterogeneous text sources. All domain subsets feature licenses of at least CC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and redistribution. The German Commons therefore addresses the critical gap in openly licensed German pretraining data, and enables the development of truly open German language models. We also release code for corpus construction and data filtering tailored to German language text, rendering the German Commons fully reproducible and extensible.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.189094"
    },
    {
        "index": "#65",
        "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention",
        "link": "/arxiv/2510.13940",
        "arxiv_id": "2510.13940",
        "authors": "Zhen Yang, Mingyang Zhang, Feng Chen, Ganggui Ding, Liang Hou, Xin Tao, Pengfei Wan, Ying-Cong Chen",
        "summary": "Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model's KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for Qwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining highly efficient.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.190175"
    },
    {
        "index": "#66",
        "title": "Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers",
        "link": "/arxiv/2510.13939",
        "arxiv_id": "2510.13939",
        "authors": "Tuhin Chakrabarty, Jane C. Ginsburg, Paramveer Dhillon",
        "summary": "The use of copyrighted books for training AI models has led to numerous lawsuits from authors concerned about AI's ability to generate derivative content.Yet it's unclear whether these models can generate high quality literary text while emulating authors' styles. To answer this we conducted a preregistered study comparing MFA-trained expert writers with three frontier AI models: ChatGPT, Claude & Gemini in writing up to 450 word excerpts emulating 50 award-winning authors' diverse styles. In blind pairwise evaluations by 159 representative expert & lay readers, AI-generated text from in-context prompting was strongly disfavored by experts for both stylistic fidelity (OR=0.16, p<10^8) & writing quality (OR=0.13, p<10^7) but showed mixed results with lay readers. However, fine-tuning ChatGPT on individual authors' complete works completely reversed these findings: experts now favored AI-generated text for stylistic fidelity (OR=8.16, p<10^13) & writing quality (OR=1.87, p=0.010), with lay readers showing similar shifts. These effects generalize across authors & styles. The fine-tuned outputs were rarely flagged as AI-generated (3% rate v. 97% for in-context prompting) by best AI detectors. Mediation analysis shows this reversal occurs because fine-tuning eliminates detectable AI stylistic quirks (e.g., cliche density) that penalize in-context outputs. While we do not account for additional costs of human effort required to transform raw AI output into cohesive, publishable prose, the median fine-tuning & inference cost of $81 per author represents a dramatic 99.7% reduction compared to typical professional writer compensation. Author-specific fine-tuning thus enables non-verbatim AI writing that readers prefer to expert human writing, providing empirical evidence directly relevant to copyright's fourth fair-use factor, the \"effect upon the potential market or value\" of the source works.",
        "subjects": "Computation and Language, Computers and Society",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.190649"
    },
    {
        "index": "#67",
        "title": "FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis",
        "link": "/arxiv/2510.13936",
        "arxiv_id": "2510.13936",
        "authors": "Fengbin Zhu, Xiang Yao Ng, Ziyang Liu, Chang Liu, Xianwei Zeng, Chao Wang, Tianhui Tan, Xuan Yao, Pengyang Shao, Min Xu, Zixuan Wang, Jing Wang, Xin Lin, Junfeng Li, Jingxian Zhu, Yang Zhang, Wenjie Wang, Fuli Feng, Richang Hong, Huanbo Luan, Ke-Wei Huang, Tat-Seng Chua",
        "summary": "Deep Research (DR) agents, powered by advanced Large Language Models (LLMs), have recently garnered increasing attention for their capability in conducting complex research tasks. However, existing literature lacks a rigorous and systematic evaluation of DR Agent's capabilities in critical research analysis. To address this gap, we first propose HisRubric, a novel evaluation framework with a hierarchical analytical structure and a fine-grained grading rubric for rigorously assessing DR agents' capabilities in corporate financial analysis. This framework mirrors the professional analyst's workflow, progressing from data recognition to metric calculation, and finally to strategic summarization and interpretation. Built on this framework, we construct a FinDeepResearch benchmark that comprises 64 listed companies from 8 financial markets across 4 languages, encompassing a total of 15,808 grading items. We further conduct extensive experiments on the FinDeepResearch using 16 representative methods, including 6 DR agents, 5 LLMs equipped with both deep reasoning and search capabilities, and 5 LLMs with deep reasoning capabilities only. The results reveal the strengths and limitations of these approaches across diverse capabilities, financial markets, and languages, offering valuable insights for future research and development. The benchmark and evaluation code will be made publicly available.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.191376"
    },
    {
        "index": "#68",
        "title": "Big Reasoning with Small Models: Instruction Retrieval at Inference Time",
        "link": "/arxiv/2510.13935",
        "arxiv_id": "2510.13935",
        "authors": "Kenan Alkiek, David Jurgens, Vinod Vydiswaran",
        "summary": "Can we bring large-scale reasoning to local-scale compute? Small language models (SLMs) are increasingly attractive because they run efficiently on local hardware, offering strong privacy, low cost, and reduced environmental impact. Yet they often struggle with tasks that require multi-step reasoning or domain-specific knowledge. We address this limitation through instruction intervention at inference time, where an SLM retrieves structured reasoning procedures rather than generating them from scratch. Our method builds an Instruction Corpus by grouping similar training questions and creating instructions via GPT-5. During inference, the SLM retrieves the most relevant instructions and follows their steps. Unlike retrieval-augmented generation, which retrieves text passages, instruction retrieval gives the model structured guidance for reasoning. We evaluate this framework on MedQA (medical board exams), MMLU Professional Law, and MathQA using models from 3B to 14B parameters without any additional fine-tuning. Instruction retrieval yields consistent gains: 9.4% on MedQA, 7.9% on MMLU Law, and 5.1% on MathQA. Concise instructions outperform longer ones, and the magnitude of improvement depends strongly on model family and intrinsic reasoning ability.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.191871"
    },
    {
        "index": "#69",
        "title": "Robust or Suggestible? Exploring Non-Clinical Induction in LLM Drug-Safety Decisions",
        "link": "/arxiv/2510.13931",
        "arxiv_id": "2510.13931",
        "authors": "Siying Liu, Shisheng Zhang, Indu Bala",
        "summary": "Large language models (LLMs) are increasingly applied in biomedical domains, yet their reliability in drug-safety prediction remains underexplored. In this work, we investigate whether LLMs incorporate socio-demographic information into adverse event (AE) predictions, despite such attributes being clinically irrelevant. Using structured data from the United States Food and Drug Administration Adverse Event Reporting System (FAERS) and a persona-based evaluation framework, we assess two state-of-the-art models, ChatGPT-4o and Bio-Medical-Llama-3.8B, across diverse personas defined by education, marital status, employment, insurance, language, housing stability, and religion. We further evaluate performance across three user roles (general practitioner, specialist, patient) to reflect real-world deployment scenarios where commercial systems often differentiate access by user type. Our results reveal systematic disparities in AE prediction accuracy. Disadvantaged groups (e.g., low education, unstable housing) were frequently assigned higher predicted AE likelihoods than more privileged groups (e.g., postgraduate-educated, privately insured). Beyond outcome disparities, we identify two distinct modes of bias: explicit bias, where incorrect predictions directly reference persona attributes in reasoning traces, and implicit bias, where predictions are inconsistent, yet personas are not explicitly mentioned. These findings expose critical risks in applying LLMs to pharmacovigilance and highlight the urgent need for fairness-aware evaluation protocols and mitigation strategies before clinical deployment.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.192336"
    },
    {
        "index": "#70",
        "title": "LLMs Can Get \"Brain Rot\"!",
        "link": "/arxiv/2510.13928",
        "arxiv_id": "2510.13928",
        "authors": "Shuo Xing, Junyuan Hong, Yifan Wang, Runjin Chen, Zhenyu Zhang, Ananth Grama, Zhengzhong Tu, Zhangyang Wang",
        "summary": "We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions. Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' $g>0.3$) on reasoning, long-context understanding, safety, and inflating \"dark traits\" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops $74.9 \\rightarrow 57.2$ and RULER-CWE $84.4 \\rightarrow 52.3$ as junk ratio rises from $0\\%$ to $100\\%$. Error forensics reveal several key insights. First, we identify thought-skipping as the primary lesion: models increasingly truncate or skip reasoning chains, explaining most of the error growth. Second, partial but incomplete healing is observed: scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch. Finally, we discover that the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1. Together, the results provide significant, multi-perspective evidence that data quality is a causal driver of LLM capability decay, reframing curation for continual pretraining as a \\textit{training-time safety} problem and motivating routine \"cognitive health checks\" for deployed LLMs.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.192880"
    },
    {
        "index": "#71",
        "title": "BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs",
        "link": "/arxiv/2510.13926",
        "arxiv_id": "2510.13926",
        "authors": "Congying Liu, Xingyuan Wei, Peipei Liu, Yiqing Shen, Yanxu Mao, Tiehan Cui",
        "summary": "Biomedical queries often rely on a deep understanding of specialized knowledge such as gene regulatory mechanisms and pathological processes of diseases. They require detailed analysis of complex physiological processes and effective integration of information from multiple data sources to support accurate retrieval and reasoning. Although large language models (LLMs) perform well in general reasoning tasks, their generated biomedical content often lacks scientific rigor due to the inability to access authoritative biomedical databases and frequently fabricates protein functions, interactions, and structural details that deviate from authentic information. Therefore, we present BioMedSearch, a multi-source biomedical information retrieval framework based on LLMs. The method integrates literature retrieval, protein database and web search access to support accurate and efficient handling of complex biomedical queries. Through sub-queries decomposition, keywords extraction, task graph construction, and multi-source information filtering, BioMedSearch generates high-quality question-answering results. To evaluate the accuracy of question answering, we constructed a multi-level dataset, BioMedMCQs, consisting of 3,000 questions. The dataset covers three levels of reasoning: mechanistic identification, non-adjacent semantic integration, and temporal causal reasoning, and is used to assess the performance of BioMedSearch and other methods on complex QA tasks. Experimental results demonstrate that BioMedSearch consistently improves accuracy over all baseline models across all levels. Specifically, at Level 1, the average accuracy increases from 59.1% to 91.9%; at Level 2, it rises from 47.0% to 81.0%; and at the most challenging Level 3, the average accuracy improves from 36.3% to 73.4%. The code and BioMedMCQs are available at: https://github.com/CyL-ucas/BioMed_Search",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.198522"
    },
    {
        "index": "#72",
        "title": "An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation",
        "link": "/arxiv/2510.13925",
        "arxiv_id": "2510.13925",
        "authors": "Daniel Adu Worae, Spyridon Mastorakis",
        "summary": "Internet of Things (IoT) networks generate diverse and high-volume traffic that reflects both normal activity and potential threats. Deriving meaningful insight from such telemetry requires cross-layer interpretation of behaviors, protocols, and context rather than isolated detection. This work presents an LLM-powered AI agent framework that converts raw packet captures into structured and semantically enriched representations for interactive analysis. The framework integrates feature extraction, transformer-based anomaly detection, packet and flow summarization, threat intelligence enrichment, and retrieval-augmented question answering. An AI agent guided by a large language model performs reasoning over the indexed traffic artifacts, assembling evidence to produce accurate and human-readable interpretations. Experimental evaluation on multiple IoT captures and six open models shows that hybrid retrieval, which combines lexical and semantic search with reranking, substantially improves BLEU, ROUGE, METEOR, and BERTScore results compared with dense-only retrieval. System profiling further indicates low CPU, GPU, and memory overhead, demonstrating that the framework achieves holistic and efficient interpretation of IoT network traffic.",
        "subjects": "Computation and Language, Cryptography and Security, Networking and Internet Architecture",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.199022"
    },
    {
        "index": "#73",
        "title": "FACTS: Table Summarization via Offline Template Generation with Agentic Workflows",
        "link": "/arxiv/2510.13920",
        "arxiv_id": "2510.13920",
        "authors": "Ye Yuan, Mohammad Amin Shabani, Siqi Liu",
        "summary": "Query-focused table summarization requires generating natural language summaries of tabular data conditioned on a user query, enabling users to access insights beyond fact retrieval. Existing approaches face key limitations: table-to-text models require costly fine-tuning and struggle with complex reasoning, prompt-based LLM methods suffer from token-limit and efficiency issues while exposing sensitive data, and prior agentic pipelines often rely on decomposition, planning, or manual templates that lack robustness and scalability. To mitigate these issues, we introduce an agentic workflow, FACTS, a Fast, Accurate, and Privacy-Compliant Table Summarization approach via Offline Template Generation. FACTS produces offline templates, consisting of SQL queries and Jinja2 templates, which can be rendered into natural language summaries and are reusable across multiple tables sharing the same schema. It enables fast summarization through reusable offline templates, accurate outputs with executable SQL queries, and privacy compliance by sending only table schemas to LLMs. Evaluations on widely-used benchmarks show that FACTS consistently outperforms baseline methods, establishing it as a practical solution for real-world query-focused table summarization.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.199477"
    },
    {
        "index": "#74",
        "title": "Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time Scaling",
        "link": "/arxiv/2510.13918",
        "arxiv_id": "2510.13918",
        "authors": "Peng Kuang, Yanli Wang, Xiaoyu Han, Yaowenqi Liu, Kaidi Xu, Haohan Wang",
        "summary": "Process reward models (PRMs) are a cornerstone of test-time scaling (TTS), designed to verify and select the best responses from large language models (LLMs). However, this promise is challenged by recent benchmarks where simple majority voting, which ignores PRM signals, occasionally outperforms standard PRM-based selection. This raises a critical question: How can we effectively utilize verification signals from PRMs for TTS? To address this, we start by developing a theoretical framework for optimally combining signals from both the LLM and the PRM. Our framework reveals that the optimal strategy is a weighted aggregation of responses, a strategy whose effectiveness hinges on estimating weights that capture the complex interplay between the models. Based on our theoretical results, we empirically show that these optimal weighting functions differ significantly across LLM-PRM pairs and, notably, often assign substantial negative weights. Motivated by these insights, we propose efficient pre-computation methods to calibrate these weighting functions. Extensive experiments across 5 LLMs and 7 PRMs demonstrate that our calibration method significantly boosts the TTS efficiency, surpassing the performance of vanilla weighted majority voting while using only $21.3\\%$ of the computation. Ultimately, our work demonstrates that investing in a more intelligent aggregation strategy can be a more convincing path to performance gains than simply scaling test-time computation.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.200003"
    },
    {
        "index": "#75",
        "title": "Element2Vec: Build Chemical Element Representation from Text for Property Prediction",
        "link": "/arxiv/2510.13916",
        "arxiv_id": "2510.13916",
        "authors": "Yuanhao Li, Keyuan Lai, Tianqi Wang, Qihao Liu, Jiawei Ma, Yuan-Chao Hu",
        "summary": "Accurate property data for chemical elements is crucial for materials design and manufacturing, but many of them are difficult to measure directly due to equipment constraints. While traditional methods use the properties of other elements or related properties for prediction via numerical analyses, they often fail to model complex relationships. After all, not all characteristics can be represented as scalars. Recent efforts have been made to explore advanced AI tools such as language models for property estimation, but they still suffer from hallucinations and a lack of interpretability. In this paper, we investigate Element2Vecto effectively represent chemical elements from natural languages to support research in the natural sciences. Given the text parsed from Wikipedia pages, we use language models to generate both a single general-purpose embedding (Global) and a set of attribute-highlighted vectors (Local). Despite the complicated relationship across elements, the computational challenges also exist because of 1) the discrepancy in text distribution between common descriptions and specialized scientific texts, and 2) the extremely limited data, i.e., with only 118 known elements, data for specific properties is often highly sparse and incomplete. Thus, we also design a test-time training method based on self-attention to mitigate the prediction error caused by Vanilla regression clearly. We hope this work could pave the way for advancing AI-driven discovery in materials science.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.200507"
    },
    {
        "index": "#77",
        "title": "Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms",
        "link": "/arxiv/2510.13913",
        "arxiv_id": "2510.13913",
        "authors": "Shrey Pandit, Xuan-Phi Nguyen, Yifei Ming, Austin Xu, Jiayu Wang, Caiming Xiong, Shafiq Joty",
        "summary": "Web-based 'deep research' agents aim to solve complex question - answering tasks through long-horizon interactions with online tools. These tasks remain challenging, as the underlying language models are often not optimized for long-horizon reasoning and exploration. Prior work has proposed workflows for constructing instruction-tuning datasets, often leveraging knowledge graphs. However, such methods typically lack fine-grained control over difficulty and quality, yielding synthetic data that falls short of capturing the complexity required for long-horizon reasoning. Furthermore, many studies conflate data and training effects by comparing models trained under different optimization recipes, making it difficult to isolate and evaluate the effectiveness of the data itself. We introduce a two-pronged data synthesis pipeline that generates question - answer pairs by progressively increasing task complexity until a frontier baseline web agent fails. The baseline agent plays multiple roles in this process: attempting the questions, validating factuality, checking for alternative answers, and enforcing filtering. To evaluate the effectiveness of our synthesis methods, we adopt a controlled training setup based on distillation from strong web agents. Experiments across multiple web-based benchmarks show that our dataset - despite being smaller - enables the training of more effective web agents than existing datasets. In particular, our data exhibits twice the diversity in tool-use actions, allowing models trained on it to achieve stronger performance while avoiding repetitive tool-calling behaviors.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.201522"
    },
    {
        "index": "#78",
        "title": "AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs",
        "link": "/arxiv/2510.13912",
        "arxiv_id": "2510.13912",
        "authors": "María Victoria Carro, Denise Alejandra Mester, Facundo Nieto, Oscar Agustín Stanchi, Guido Ernesto Bergman, Mario Alejandro Leiva, Eitan Sprejer, Luca Nicolás Forziati Gangi, Francisca Gauna Selasco, Juan Gustavo Corvalán, Gerardo I. Simari, María Vanina Martinez",
        "summary": "The core premise of AI debate as a scalable oversight technique is that it is harder to lie convincingly than to refute a lie, enabling the judge to identify the correct position. Yet, existing debate experiments have relied on datasets with ground truth, where lying is reduced to defending an incorrect proposition. This overlooks a subjective dimension: lying also requires the belief that the claim defended is false. In this work, we apply debate to subjective questions and explicitly measure large language models' prior beliefs before experiments. Debaters were asked to select their preferred position, then presented with a judge persona deliberately designed to conflict with their identified priors. This setup tested whether models would adopt sycophantic strategies, aligning with the judge's presumed perspective to maximize persuasiveness, or remain faithful to their prior beliefs. We implemented and compared two debate protocols, sequential and simultaneous, to evaluate potential systematic biases. Finally, we assessed whether models were more persuasive and produced higher-quality arguments when defending positions consistent with their prior beliefs versus when arguing against them. Our main findings show that models tend to prefer defending stances aligned with the judge persona rather than their prior beliefs, sequential debate introduces significant bias favoring the second debater, models are more persuasive when defending positions aligned with their prior beliefs, and paradoxically, arguments misaligned with prior beliefs are rated as higher quality in pairwise comparison. These results can inform human judges to provide higher-quality training signals and contribute to more aligned AI systems, while revealing important aspects of human-AI interaction regarding persuasion dynamics in language models.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.202135"
    },
    {
        "index": "#79",
        "title": "RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems",
        "link": "/arxiv/2510.13910",
        "arxiv_id": "2510.13910",
        "authors": "Jingru Lin, Chen Zhang, Stephen Y. Liu, Haizhou Li",
        "summary": "Retrieval-Augmented Generation (RAG) mitigates key limitations of Large Language Models (LLMs)-such as factual errors, outdated knowledge, and hallucinations-by dynamically retrieving external information. Recent work extends this paradigm through agentic RAG systems, where LLMs act as agents to iteratively plan, retrieve, and reason over complex queries. However, these systems still struggle with challenging multi-hop questions, and their intermediate reasoning capabilities remain underexplored. To address this, we propose RAGCap-Bench, a capability-oriented benchmark for fine-grained evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs from state-of-the-art systems to identify common tasks and the core capabilities required for their execution, then construct a taxonomy of typical LLM errors to design targeted evaluation questions. Experiments show that \"slow-thinking\" models with stronger RAGCap performance achieve better end-to-end results, underscoring the benchmark's validity and the importance of enhancing these intermediate capabilities.",
        "subjects": "Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.202606"
    },
    {
        "index": "#80",
        "title": "Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning",
        "link": "/arxiv/2510.13909",
        "arxiv_id": "2510.13909",
        "authors": "Xingrui Zhuo, Jiapu Wang, Gongqing Wu, Zhongyuan Wang, Jichen Zhang, Shirui Pan, Xindong Wu",
        "summary": "Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in open-domain KGs containing unknown entities and relations, which poses a challenge for KGR models in comprehending uncertain KG components. Existing studies have proposed Knowledge Graph Foundation Models (KGFMs) that learn structural invariances across KGs to handle this uncertainty. Recently, Large Language Models (LLMs) have demonstrated strong capabilities for open-domain knowledge reasoning. As a result, the latest research has focused on LLM-based KGFMs that integrate LLM knowledge with KG context for inductive KGR. However, the intrinsic knowledge of LLMs may be overshadowed by sparse KG context, leading to LLM knowledge distortion, which can cause irreversible damage to model reasoning. Moreover, existing LLM-based KGR methods still struggle to fully constrain generative hallucinations in LLMs, severely limiting the credibility of reasoning results. To address these limitations, we propose a Knowledge Reasoning Language Model (KRLM) that achieves unified coordination between LLM knowledge and KG context throughout the KGR process. Specifically, we design a Knowledge Reasoning Language (KRL) instruction format and a KRL tokenizer to align LLM knowledge with KG representations. Then, we propose a KRL attention layer that coordinates intrinsic LLM knowledge with additional KG context through a dynamic knowledge memory mechanism. Finally, a structure-aware next-entity predictor is proposed, which strictly constrains the reasoning results within a trustworthy knowledge domain. Extensive experimental results on 25 real-world inductive KGR datasets demonstrate the significant superiority of the proposed KRLM\\footnote{Our source codes are available at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot reasoning and fine-tuning scenarios.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.203138"
    },
    {
        "index": "#81",
        "title": "Interpreting the Latent Structure of Operator Precedence in Language Models",
        "link": "/arxiv/2510.13908",
        "arxiv_id": "2510.13908",
        "authors": "Dharunish Yugeswardeenoo, Harshil Nukala, Cole Blondin, Sean O Brien, Vasu Sharma, Kevin Zhu",
        "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities but continue to struggle with arithmetic tasks. Prior works largely focus on outputs or prompting strategies, leaving the open question of the internal structure through which models do arithmetic computation. In this work, we investigate whether LLMs encode operator precedence in their internal representations via the open-source instruction-tuned LLaMA 3.2-3B model. We constructed a dataset of arithmetic expressions with three operands and two operators, varying the order and placement of parentheses. Using this dataset, we trace whether intermediate results appear in the residual stream of the instruction-tuned LLaMA 3.2-3B model. We apply interpretability techniques such as logit lens, linear classification probes, and UMAP geometric visualization. Our results show that intermediate computations are present in the residual stream, particularly after MLP blocks. We also find that the model linearly encodes precedence in each operator's embeddings post attention layer. We introduce partial embedding swap, a technique that modifies operator precedence by exchanging high-impact embedding dimensions between operators.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.208714"
    },
    {
        "index": "#82",
        "title": "LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization",
        "link": "/arxiv/2510.13907",
        "arxiv_id": "2510.13907",
        "authors": "Yuanchen Wu, Saurabh Verma, Justin Lee, Fangzhou Xiong, Poppy Zhang, Amel Awadelkarim, Xu Chen, Yubai Yuan, Shawndra Hill",
        "summary": "Large language models (LLMs) are highly sensitive to their input prompts, making prompt design a central challenge. While automatic prompt optimization (APO) reduces manual engineering, most approaches assume access to ground-truth references such as labeled validation data. In practice, however, collecting high-quality labels is costly and slow. We propose the Prompt Duel Optimizer (PDO), a sample-efficient framework for label-free prompt optimization. PDO formulates the problem as a dueling-bandit setting, where supervision signal comes from pairwise preference feedback provided by an LLM judge. The framework combines Double Thompson Sampling (D-TS), which prioritizes informative prompt comparisons, with Top-Performer Guided Mutation, which expands the candidate pool by mutating high-performing prompts. PDO naturally operates in label-free settings and can also incorporate partial labels to mitigate judge noise. Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently outperforms baseline methods. Ablation studies further demonstrate the effectiveness of both D-TS and prompt mutation.",
        "subjects": "Computation and Language, Machine Learning",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.209076"
    },
    {
        "index": "#83",
        "title": "Schema for In-Context Learning",
        "link": "/arxiv/2510.13905",
        "arxiv_id": "2510.13905",
        "authors": "Pan Chen, Shaohong Chen, Mark Wang, Shi Xuan Leong, Priscilla Fung, Varinia Bernales, Alan Aspuru-Guzik",
        "summary": "In-Context Learning (ICL) enables transformer-based language models to adapt to new tasks by conditioning on demonstration examples. However, traditional example-driven in-context learning lacks explicit modules for knowledge retrieval and transfer at the abstraction level. Inspired by cognitive science, specifically schema theory, which holds that humans interpret new information by activating pre-existing mental frameworks (schemas) to structure understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This framework extracts the representation of the building blocks of cognition for the reasoning process instilled from prior examples, creating an abstracted schema, a lightweight, structured template of key inferential steps and their relationships, which is then used to augment a model's reasoning process when presented with a novel question. We demonstrate that a broad range of large language models (LLMs) lack the capacity to form and utilize internal schema-based learning representations implicitly, but instead benefit significantly from explicit schema-based scaffolding. Across chemistry and physics questions from the GPQA dataset, our experiments show that SA-ICL consistently boosts performance, up to 36.19 percent, when the single demonstration example is of high quality, which simultaneously reduces reliance on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from pattern priming to Chain-of-Thought prompting, but also paves a new path for enhancing human-like reasoning in LLMs.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.209409"
    },
    {
        "index": "#84",
        "title": "Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory",
        "link": "/arxiv/2510.13902",
        "arxiv_id": "2510.13902",
        "authors": "Nicole Smith-Vaniz, Harper Lyon, Lorraine Steigner, Ben Armstrong, Nicholas Mattei",
        "summary": "Large Language Models (LLMs) have become increasingly incorporated into everyday life for many internet users, taking on significant roles as advice givers in the domains of medicine, personal relationships, and even legal matters. The importance of these roles raise questions about how and what responses LLMs make in difficult political and moral domains, especially questions about possible biases. To quantify the nature of potential biases in LLMs, various works have applied Moral Foundations Theory (MFT), a framework that categorizes human moral reasoning into five dimensions: Harm, Fairness, Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to measure differences in human participants along political, national, and cultural lines. While there has been some analysis of the responses of LLM with respect to political stance in role-playing scenarios, no work so far has directly assessed the moral leanings in the LLM responses, nor have they connected LLM outputs with robust human data. In this paper we analyze the distinctions between LLM MFT responses and existing human research directly, investigating whether commonly available LLM responses demonstrate ideological leanings: either through their inherent responses, straightforward representations of political ideologies, or when responding from the perspectives of constructed human personas. We assess whether LLMs inherently generate responses that align more closely with one political ideology over another, and additionally examine how accurately LLMs can represent ideological perspectives through both explicit prompting and demographic-based role-playing. By systematically analyzing LLM behavior across these conditions and experiments, our study provides insight into the extent of political and demographic dependency in AI-generated responses.",
        "subjects": "Computation and Language, Computers and Society",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.209712"
    },
    {
        "index": "#85",
        "title": "RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs",
        "link": "/arxiv/2510.13901",
        "arxiv_id": "2510.13901",
        "authors": "Tuan T. Nguyen, John Le, Thai T. Vu, Willy Susilo, Heath Cooper",
        "summary": "Large language models (LLMs) achieve impressive performance across diverse tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms. We present RAID (Refusal-Aware and Integrated Decoding), a framework that systematically probes these weaknesses by crafting adversarial suffixes that induce restricted content while preserving fluency. RAID relaxes discrete tokens into continuous embeddings and optimizes them with a joint objective that (i) encourages restricted responses, (ii) incorporates a refusal-aware regularizer to steer activations away from refusal directions in embedding space, and (iii) applies a coherence term to maintain semantic plausibility and non-redundancy. After optimization, a critic-guided decoding procedure maps embeddings back to tokens by balancing embedding affinity with language-model likelihood. This integration yields suffixes that are both effective in bypassing defenses and natural in form. Experiments on multiple open-source LLMs show that RAID achieves higher attack success rates with fewer queries and lower computational cost than recent white-box and black-box baselines. These findings highlight the importance of embedding-space regularization for understanding and mitigating LLM jailbreak vulnerabilities.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.210011"
    },
    {
        "index": "#86",
        "title": "Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences",
        "link": "/arxiv/2510.13900",
        "arxiv_id": "2510.13900",
        "authors": "Julian Minder, Clément Dumas, Stewart Slocum, Helena Casademunt, Cameron Holmes, Robert West, Neel Nanda",
        "summary": "Finetuning on narrow domains has become an essential tool to adapt Large Language Models (LLMs) to specific tasks and to create models with known unusual properties that are useful for research. We show that narrow finetuning creates strong biases in LLM activations that can be interpreted to understand the finetuning domain. These biases can be discovered using simple tools from model diffing - the study of differences between models before and after finetuning. In particular, analyzing activation differences on the first few tokens of random text and steering by adding this difference to the model activations produces text similar to the format and general content of the finetuning data. We demonstrate that these analyses contain crucial information by creating an LLM-based interpretability agent to understand the finetuning domain. With access to the bias, the agent performs significantly better compared to baseline agents using simple prompting. Our analysis spans synthetic document finetuning for false facts, emergent misalignment, subliminal learning, and taboo word guessing game models across different architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We suspect these biases reflect overfitting and find that mixing pretraining data into the finetuning corpus largely removes them, though residual risks may remain. Our work (1) demonstrates that narrowly finetuned models have salient traces of their training objective in their activations and suggests ways to improve how they are trained, (2) warns AI safety and interpretability researchers that the common practice of using such models as a proxy for studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3) highlights the need for deeper investigation into the effects of narrow finetuning and development of truly realistic case studies for model-diffing, safety and interpretability research.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.210324"
    },
    {
        "index": "#87",
        "title": "Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges",
        "link": "/arxiv/2510.13898",
        "arxiv_id": "2510.13898",
        "authors": "Misam Abbas",
        "summary": "Attributing authorship in the era of large language models (LLMs) is increasingly challenging as machine-generated prose rivals human writing. We benchmark two complementary attribution mechanisms , fixed Style Embeddings and an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an open dataset of 600 balanced instances spanning six domains (academic, news, fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance contains a human prompt with both a gold continuation and an LLM-generated continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs. 68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA continuations (85 pct vs. 81 pct) but the results are not statistically significant. Crucially, the LLM judge significantly outperforms in fiction and academic prose, indicating semantic sensitivity, whereas embeddings dominate in spoken and scripted dialogue, reflecting structural strengths. These complementary patterns highlight attribution as a multidimensional problem requiring hybrid strategies. To support reproducibility we provide code on GitHub and derived data on Hugging Face under the MIT license. This open framework provides a reproducible benchmark for attribution quality assessment in AI-generated content, along with a review of related literature influencing this work.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.210572"
    },
    {
        "index": "#88",
        "title": "Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection",
        "link": "/arxiv/2510.13893",
        "arxiv_id": "2510.13893",
        "authors": "Olga E. Sorokoletova, Francesco Giarrusso, Vincenzo Suriani, Daniele Nardi",
        "summary": "Jailbreaking techniques pose a significant threat to the safety of Large Language Models (LLMs). Existing defenses typically focus on single-turn attacks, lack coverage across languages, and rely on limited taxonomies that either fail to capture the full diversity of attack strategies or emphasize risk categories rather than the jailbreaking techniques. To advance the understanding of the effectiveness of jailbreaking techniques, we conducted a structured red-teaming challenge. The outcome of our experiments are manifold. First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak strategies, consolidating and extending prior classifications into seven broad families, including impersonation, persuasion, privilege escalation, cognitive overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed the data collected from the challenge to examine the prevalence and success rates of different attack types, providing insights into how specific jailbreak strategies exploit model vulnerabilities and induce misalignment. Third, we benchmark a popular LLM for jailbreak detection, evaluating the benefits of taxonomy-guided prompting for improving automatic detection. Finally, we compiled a new Italian dataset of 1364 multi-turn adversarial dialogues, annotated with our taxonomy, enabling the study of interactions where adversarial intent emerges gradually and succeeds in bypassing traditional safeguards.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.210872"
    },
    {
        "index": "#89",
        "title": "The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data",
        "link": "/arxiv/2510.13892",
        "arxiv_id": "2510.13892",
        "authors": "Zhaoyang Shang, Sibo Wei, Jianbin Guo, Rui Zhou, Lifeng Dong, Yin Luo",
        "summary": "Large Language Models (LLMs) excel in general tasks, but adapting them to specialized domains relies on high-quality supervised fine-tuning (SFT) data. Although existing methods can identify subsets of high-quality data and reduce training cost to some extent, their selection process still suffers from over-reliance on LLMs' internal knowledge, weak interpretability, and limited generalization. To address these limitations, we propose THTB (The Harder The Better), a cognitive science-inspired framework for instruction data selection and annotation guidance. THTB prioritizes higher-level cognitive instructions by combining quality filtering with intrinsic and extrinsic hardness scoring, offering interpretable and quantifiable criteria for efficient SFT, both in data selection and annotation guidance. Experiments show that THTB enables models trained on only 5% of the data to outperform full-dataset training, while achieving superior generalization compared with LLM-only selection. In addition, THTB provides effective annotation guidance in vertical domains, enabling a model trained on just 2% of the data to surpass models trained on much larger datasets, demonstrating strong potential for domain adaptation. Our code, datasets, and models are available on https://github.com/DYJG-research/THTB.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.211168"
    },
    {
        "index": "#90",
        "title": "A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness",
        "link": "/arxiv/2510.13890",
        "arxiv_id": "2510.13890",
        "authors": "Fali Wang, Jihai Chen, Shuhua Yang, Ali Al-Lawati, Linli Tang, Hui Liu, Suhang Wang",
        "summary": "Large language models (LLMs) have advanced many domains and applications but face high fine-tuning costs, inference latency, limited edge deployability, and reliability concerns. Small language models (SLMs), compact, efficient, and adaptable, offer complementary remedies. Recent work explores collaborative frameworks that fuse SLMs' specialization and efficiency with LLMs' generalization and reasoning to meet diverse objectives across tasks and deployment scenarios. Motivated by these developments, this paper presents a systematic survey of SLM-LLM collaboration organized by collaboration objectives. We propose a taxonomy with four goals: performance enhancement, cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this framework, we review representative methods, summarize design paradigms, and outline open challenges and future directions toward efficient, secure, and scalable SLM-LLM collaboration.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.211487"
    },
    {
        "index": "#91",
        "title": "Reliable Fine-Grained Evaluation of Natural Language Math Proofs",
        "link": "/arxiv/2510.13888",
        "arxiv_id": "2510.13888",
        "authors": "Wenjie Ma, Andrei Cojocaru, Neel Kolhe, Bradley Louie, Robin Said Sharif, Haihan Zhang, Vincent Zhuang, Matei Zaharia, Sewon Min",
        "summary": "Recent advances in large language models (LLMs) for mathematical reasoning have largely focused on tasks with easily verifiable final answers; however, generating and verifying natural language math proofs remains an open challenge. We identify the absence of a reliable, fine-grained evaluator for LLM-generated math proofs as a critical gap. To address this, we propose a systematic methodology for developing and validating evaluators that assign fine-grained scores on a 0-7 scale to model-generated math proofs. To enable this study, we introduce ProofBench, the first expert-annotated dataset of fine-grained proof ratings, spanning 145 problems from six major math competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as a testbed, we systematically explore the evaluator design space across key axes: the backbone model, input context, instructions and evaluation workflow. Our analysis delivers ProofGrader, an evaluator that combines a strong reasoning backbone LM, rich context from reference solutions and marking schemes, and a simple ensembling method; it achieves a low Mean Absolute Error (MAE) of 0.926 against expert scores, significantly outperforming naive baselines. Finally, we demonstrate its practical utility in a best-of-$n$ selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the human oracle (4.62), highlighting its potential to advance downstream proof generation.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.211846"
    },
    {
        "index": "#92",
        "title": "Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization",
        "link": "/arxiv/2510.13885",
        "arxiv_id": "2510.13885",
        "authors": "Ariel Kamen",
        "summary": "This study presents a comparative evaluation of ten state-of-the-art large language models (LLMs) applied to unstructured text categorization using the Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis employed a uniform dataset of 8,660 human-annotated samples and identical zero-shot prompts to ensure methodological consistency across all models. Evaluation metrics included four classic measures - accuracy, precision, recall, and F1-score - and three LLM-specific indicators: hallucination ratio, inflation ratio, and categorization cost. Results show that, despite their rapid advancement, contemporary LLMs achieve only moderate classic performance, with average scores of 34% accuracy, 42% precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios reveal that models frequently overproduce categories relative to human annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B offered the most favorable cost-to-performance balance, while GPT 120B demonstrated the lowest hallucination ratio. The findings suggest that scaling and architectural improvements alone do not ensure better categorization accuracy, as the task requires compressing rich unstructured text into a limited taxonomy - a process that challenges current model architectures. To address these limitations, a separate ensemble-based approach was developed and tested. The ensemble method, in which multiple LLMs act as independent experts, substantially improved accuracy, reduced inflation, and completely eliminated hallucinations. These results indicate that coordinated orchestration of models - rather than sheer scale - may represent the most effective path toward achieving or surpassing human-expert performance in large-scale text categorization.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.212109"
    },
    {
        "index": "#93",
        "title": "Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation",
        "link": "/arxiv/2510.13884",
        "arxiv_id": "2510.13884",
        "authors": "Bolei Ma, Yong Cao, Indira Sen, Anna-Carolina Haensch, Frauke Kreuter, Barbara Plank, Daniel Hershcovich",
        "summary": "Large Language Models (LLMs) are increasingly used to simulate public opinion and other social phenomena. Most current studies constrain these simulations to multiple-choice or short-answer formats for ease of scoring and comparison, but such closed designs overlook the inherently generative nature of LLMs. In this position paper, we argue that open-endedness, using free-form text that captures topics, viewpoints, and reasoning processes \"in\" LLMs, is essential for realistic social simulation. Drawing on decades of survey-methodology research and recent advances in NLP, we argue why this open-endedness is valuable in LLM social simulations, showing how it can improve measurement and design, support exploration of unanticipated views, and reduce researcher-imposed directive bias. It also captures expressiveness and individuality, aids in pretesting, and ultimately enhances methodological utility. We call for novel practices and evaluation frameworks that leverage rather than constrain the open-ended generative diversity of LLMs, creating synergies between NLP and social science.",
        "subjects": "Computation and Language",
        "date": "2025-10-14",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.212414"
    },
    {
        "index": "#94",
        "title": "PAGE: Prompt Augmentation for text Generation Enhancement",
        "link": "/arxiv/2510.13880",
        "arxiv_id": "2510.13880",
        "authors": "Mauro Jose Pacchiotti, Luciana Ballejos, Mariel Ale",
        "summary": "In recent years, natural language generative models have shown outstanding performance in text generation tasks. However, when facing specific tasks or particular requirements, they may exhibit poor performance or require adjustments that demand large amounts of additional data. This work introduces PAGE (Prompt Augmentation for text Generation Enhancement), a framework designed to assist these models through the use of simple auxiliary modules. These modules, lightweight models such as classifiers or extractors, provide inferences from the input text. The output of these auxiliaries is then used to construct an enriched input that improves the quality and controllability of the generation. Unlike other generation-assistance approaches, PAGE does not require auxiliary generative models; instead, it proposes a simpler, modular architecture that is easy to adapt to different tasks. This paper presents the proposal, its components and architecture, and reports a proof of concept in the domain of requirements engineering, where an auxiliary module with a classifier is used to improve the quality of software requirements generation.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.212698"
    },
    {
        "index": "#95",
        "title": "Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production",
        "link": "/arxiv/2510.13879",
        "arxiv_id": "2510.13879",
        "authors": "Alexandre Galashov, Matt Jones, Rosemary Ke, Yuan Cao, Vaishnavh Nagarajan, Michael C. Mozer",
        "summary": "We explore a class of supervised training objectives that allow a language model to dynamically and autonomously scale the number of compute steps used for each input token. For any token, the model can request additional compute steps by emitting a <don't know> output. If the model is granted a delay, a specialized <pause> token is inserted at the next input step, providing the model with additional compute resources to generate an output. The model can request multiple pauses. To train the model to use <don't know> outputs judiciously and to calibrate its uncertainty, we frame the selection of each output token as a sequential-decision problem with a time cost. We refer to the class of methods as $\\textit{Catch Your Breath}$ losses and we study three methods in this class: CYB-AP frames the model's task as anytime prediction, where an output may be required at any step and accuracy is discounted over time; CYB-VA is a variational approach that aims to maximize prediction accuracy subject to a specified distribution over stopping times; and CYB-DP imposes a penalty based on a computational budget. Through fine-tuning experiments, we identify the best performing loss variant. The CYB model needs only one third as much training data as the baseline (no pause) model needs to achieve the same performance, and half as much data as a model with pauses and a cross-entropy loss. We find that the CYB model requests additional steps when doing so improves accuracy, and the model adapts its processing time to token-level complexity and context. For example, it often pauses after plural nouns like $\\textit{patients}$ and $\\textit{challenges}$ but never pauses after the first token of contracted words like $\\textit{wasn}$ and $\\textit{didn}$, and it shows high variability for ambiguous tokens like $\\textit{won}$, which could function as either a verb or part of a contraction.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.213012"
    },
    {
        "index": "#96",
        "title": "TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks",
        "link": "/arxiv/2510.13878",
        "arxiv_id": "2510.13878",
        "authors": "Jimin Lim, Arjun Damerla, Arthur Jiang, Nam Le",
        "summary": "Large language models (LLMs) have shown to be increasingly capable of performing reasoning tasks, but their ability to make sequential decisions under uncertainty only using natural language remains underexplored. We introduce a novel benchmark in which LLMs interact with multi-armed bandit environments using purely textual feedback, \"you earned a token\", without access to numerical cues or explicit probabilities, resulting in the model to infer latent reward structures purely off linguistic cues and to adapt accordingly. We evaluated the performance of four open-source LLMs and compare their performance to standard decision-making algorithms such as Thompson Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice. While most of the LLMs underperformed compared to the baselines, Qwen3-4B, achieved the best-arm selection rate of 89.2% , which significantly outperformed both the larger LLMs and traditional methods. Our findings suggest that probabilistic reasoning is able to emerge from language alone, and we present this benchmark as a step towards evaluating decision-making capabilities in naturalistic, non-numeric contexts.",
        "subjects": "Computation and Language",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.213289"
    },
    {
        "index": "#97",
        "title": "What Layers When: Learning to Skip Compute in LLMs with Residual Gates",
        "link": "/arxiv/2510.13876",
        "arxiv_id": "2510.13876",
        "authors": "Filipe Laitenberger, Dawid Kopiczko, Cees G. M. Snoek, Yuki M. Asano",
        "summary": "We introduce GateSkip, a simple residual-stream gating mechanism that enables token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is equipped with a sigmoid-linear gate that condenses the branch's output before it re-enters the residual stream. During inference we rank tokens by the gate values and skip low-importance ones using a per-layer budget. While early-exit or router-based Mixture-of-Depths models are known to be unstable and need extensive retraining, our smooth, differentiable gates fine-tune stably on top of pretrained models. On long-form reasoning, we save up to 15\\% compute while retaining over 90\\% of baseline accuracy. On instruction-tuned models we see accuracy gains at full compute and match baseline quality near 50\\% savings. The learned gates give insight into transformer information flow (e.g., BOS tokens act as anchors), and the method combines easily with quantization, pruning, and self-speculative decoding.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.213593"
    },
    {
        "index": "#98",
        "title": "FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation",
        "link": "/arxiv/2510.13873",
        "arxiv_id": "2510.13873",
        "authors": "Johann Pignat, Milena Vucetic, Christophe Gaudet-Blavignac, Jamil Zaghir, Amandine Stettler, Fanny Amrein, Jonatan Bonjour, Jean-Philippe Goldman, Olivier Michielin, Christian Lovis, Mina Bjelogrlic",
        "summary": "Developing natural language processing tools for clinical text requires annotated datasets, yet French oncology resources remain scarce. We present FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated corpus of 1301 synthetic French clinical cases, initially translated from the Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is annotated with terms related to morphology, topography, and histologic differentiation, using the International Classification of Diseases for Oncology (ICD-O) as reference. An additional annotation layer captures composite expression-level normalisations that combine multiple ICD-O elements into unified clinical concepts. Annotation quality was ensured through expert review: 1301 texts were manually annotated for entity spans by two domain experts. A total of 71127 ICD-O normalisations were produced through a combination of automated matching and manual validation by a team of five annotators. The final dataset representing 399 unique morphology codes (from 2549 different expressions), 272 topography codes (from 3143 different expressions), and 2043 unique composite expressions (from 11144 different expressions). This dataset provides a reference standard for named entity recognition and concept normalisation in French oncology texts.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.219072"
    },
    {
        "index": "#99",
        "title": "Quechua Speech Datasets in Common Voice: The Case of Puno Quechua",
        "link": "/arxiv/2510.13871",
        "arxiv_id": "2510.13871",
        "authors": "Elwin Huaman, Wendi Huaman, Jorge Luis Huaman, Ninfa Quispe",
        "summary": "Under-resourced languages, such as Quechuas, face data and resource scarcity, hindering their development in speech technology. To address this issue, Common Voice presents a crucial opportunity to foster an open and community-driven speech dataset creation. This paper examines the integration of Quechua languages into Common Voice. We detail the current 17 Quechua languages, presenting Puno Quechua (ISO 639-3: qxp) as a focused case study that includes language onboarding and corpus collection of both reading and spontaneous speech data. Our results demonstrate that Common Voice now hosts 191.1 hours of Quechua speech (86\\% validated), with Puno Quechua contributing 12 hours (77\\% validated), highlighting the Common Voice's potential. We further propose a research agenda addressing technical challenges, alongside ethical considerations for community engagement and indigenous data sovereignty. Our work contributes towards inclusive voice technology and digital empowerment of under-resourced language communities.",
        "subjects": "Computation and Language",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.219371"
    },
    {
        "index": "#100",
        "title": "Unlocking the Potential of Diffusion Language Models through Template Infilling",
        "link": "/arxiv/2510.13870",
        "arxiv_id": "2510.13870",
        "authors": "Junhoo Lee, Seungyeon Kim, Nojun Kwak",
        "summary": "Diffusion Language Models (DLMs) have emerged as a promising alternative to Autoregressive Language Models, yet their inference strategies remain limited to prefix-based prompting inherited from the autoregressive paradigm. In this paper, we propose Template Infilling (TI), a tailored conditioning methodology for DLMs' generation process. Unlike conventional prefix prompting, TI first generates a structural template for the target response, then fills in the masked segments. To enhance the flexibility of this structural control, we introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment lengths based on generation confidence. We demonstrate the effectiveness of our approach on mathematical reasoning and code generation benchmarks, achieving consistent improvements of 17.01$\\%$p over baseline. Furthermore, we show that TI provides additional advantages in multi-token generation settings, enabling effective speedup while maintaining generation quality.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.219654"
    },
    {
        "index": "#101",
        "title": "Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues",
        "link": "/arxiv/2510.13862",
        "arxiv_id": "2510.13862",
        "authors": "Chenyu Zhang, Sharifa Alghowinem, Cynthia Breazeal",
        "summary": "While recent studies have examined the leaning impact of large language model (LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring remain insufficiently understood. This work introduces the first ensemble-LLM framework for large-scale affect sensing in tutoring dialogues, advancing the conversation on responsible pathways for integrating generative AI into education by attending to learners' evolving affective states. To achieve this, we analyzed two semesters' worth of 16,986 conversational turns exchanged between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across three U.S. institutions. To investigate learners' emotional experiences, we generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o, Claude), including scalar ratings of valence, arousal, and learning-helpfulness, along with free-text emotion labels. These estimates are fused through rank-weighted intra-model pooling and plurality consensus across models to produce robust emotion profiles. Our analysis shows that during interaction with the AI tutor, students typically report mildly positive affect and moderate arousal. Yet learning is not uniformly smooth: confusion and curiosity are frequent companions to problem solving, and frustration, while less common, still surfaces in ways that can derail progress. Emotional states are short-lived--positive moments last slightly longer than neutral or negative ones, but they are fragile and easily disrupted. Encouragingly, negative emotions often resolve quickly, sometimes rebounding directly into positive states. Neutral moments frequently act as turning points, more often steering students upward than downward, suggesting opportunities for tutors to intervene at precisely these junctures.",
        "subjects": "Computation and Language, Artificial Intelligence, Human-Computer Interaction",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.219965"
    },
    {
        "index": "#102",
        "title": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing",
        "link": "/arxiv/2510.13860",
        "arxiv_id": "2510.13860",
        "authors": "Shivanshu Kumar, Gopalakrishnan Srinivasan",
        "summary": "While the transformer architecture has achieved state-of-the-art performance on natural language processing tasks, these models impose substantial memory and computational overhead. Recent research has identified significant architectural redundancies within these models, presenting opportunities for optimization without compromising performance. Taking insights from research in AI interpretability and inference-time layer pruning, we introduce an efficient language model architecture, referred to as ShishuLM, which reduces both the parameter count and Key-Value (KV) cache requirements. Given the increasing importance of Small Language Models (SLMs) in agentic AI systems, we evaluate our approach on two SLMs of different scales. Our analysis reveals that for moderate-context scenarios, normalization coupled with attention computation is roughly linear with the input, enabling entire transformer blocks to be approximated through Multi-Layer Perceptrons (MLPs). Our results show that ShishuLM provides up to 25% reduction in memory requirements and up to 40% improvement in latency during both training and inference, compared to parent models. Our experimental and analytical findings provide insights towards building more efficient SLM architectures from a pre-training standpoint.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.220247"
    },
    {
        "index": "#103",
        "title": "Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA",
        "link": "/arxiv/2510.13856",
        "arxiv_id": "2510.13856",
        "authors": "A H M Rezaul Karim, Ozlem Uzuner",
        "summary": "Medical Visual Question Answering (MedVQA) enables natural language queries over medical images to support clinical decision-making and patient care. The MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to generate free-text responses and structured wound attributes from images and patient queries. We present the MasonNLP system, which employs a general-domain, instruction-tuned large language model with a retrieval-augmented generation (RAG) framework that incorporates textual and visual examples from in-domain data. This approach grounds outputs in clinically relevant exemplars, improving reasoning, schema adherence, and response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our best-performing system ranked 3rd among 19 teams and 51 submissions with an average score of 41.37%, demonstrating that lightweight RAG with general-purpose LLMs -- a minimal inference-time layer that adds a few relevant exemplars via simple indexing and fusion, with no extra training or complex re-ranking -- provides a simple and effective baseline for multimodal clinical NLP tasks.",
        "subjects": "Computation and Language, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-12",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.220536"
    },
    {
        "index": "#104",
        "title": "Harnessing Consistency for Robust Test-Time LLM Ensemble",
        "link": "/arxiv/2510.13855",
        "arxiv_id": "2510.13855",
        "authors": "Zhichen Zeng, Qi Yu, Xiao Lin, Ruizhong Qiu, Xuying Ning, Tianxin Wei, Yuchen Yan, Jingrui He, Hanghang Tong",
        "summary": "Different large language models (LLMs) exhibit diverse strengths and weaknesses, and LLM ensemble serves as a promising approach to integrate their complementary capabilities. Despite substantial progress in improving ensemble quality, limited attention has been paid to the robustness of ensembles against potential erroneous signals, which often arise from heterogeneous tokenization schemes and varying model expertise. Our analysis shows that ensemble failures typically arise from both the token level and the model level: the former reflects severe disagreement in token predictions, while the latter involves low confidence and pronounced disparities among models. In light of this, we propose CoRE, a plug-and-play technique that harnesses model consistency for robust LLM ensemble, which can be seamlessly integrated with diverse ensemble methods. Token-level consistency captures fine-grained disagreements by applying a low-pass filter to downweight uncertain tokens with high inconsistency, often due to token misalignment, thereby improving robustness at a granular level. Model-level consistency models global agreement by promoting model outputs with high self-confidence and minimal divergence from others, enhancing robustness at a coarser level. Extensive experiments across diverse benchmarks, model combinations, and ensemble strategies demonstrate that CoRE consistently improves ensemble performance and robustness.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-12",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.220900"
    },
    {
        "index": "#106",
        "title": "BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation",
        "link": "/arxiv/2510.13853",
        "arxiv_id": "2510.13853",
        "authors": "Fabian Wenz, Omar Bouattour, Devin Yang, Justin Choi, Cecil Gregg, Nesime Tatbul, Çağatay Demiralp",
        "summary": "Large language models (LLMs) have been successfully applied to many tasks, including text-to-SQL generation. However, much of this work has focused on publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work showed that LLMs are much less effective in querying large private enterprise data warehouses and released Beaver, the first private enterprise text-to-SQL benchmark. To create Beaver, we leveraged SQL logs, which are often readily available. However, manually annotating these logs to identify which natural language questions they answer is a daunting task. Asking database administrators, who are highly trained experts, to take on additional work to construct and validate corresponding natural language utterances is not only challenging but also quite costly. To address this challenge, we introduce BenchPress, a human-in-the-loop system designed to accelerate the creation of domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses retrieval-augmented generation (RAG) and LLMs to propose multiple natural language descriptions. Human experts then select, rank, or edit these drafts to ensure accuracy and domain alignment. We evaluated BenchPress on annotated enterprise SQL logs, demonstrating that LLM-assisted annotation drastically reduces the time and effort required to create high-quality benchmarks. Our results show that combining human verification with LLM-generated suggestions enhances annotation accuracy, benchmark reliability, and model evaluation robustness. By streamlining the creation of custom benchmarks, BenchPress offers researchers and practitioners a mechanism for assessing text-to-SQL models on a given domain-specific workload. BenchPress is freely available via our public GitHub repository at https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our website at http://dsg-mcgraw.csail.mit.edu:5000.",
        "subjects": "Computation and Language, Artificial Intelligence, Databases, Human-Computer Interaction",
        "date": "2025-10-11",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.221553"
    },
    {
        "index": "#107",
        "title": "ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups",
        "link": "/arxiv/2510.13852",
        "arxiv_id": "2510.13852",
        "authors": "Peter Banyas, Shristi Sharma, Alistair Simmons, Atharva Vispute",
        "summary": "Is an LLM telling you different facts than it's telling me? This paper introduces ConsistencyAI, an independent benchmark for measuring the factual consistency of large language models (LLMs) for different personas. ConsistencyAI tests whether, when users of different demographics ask identical questions, the model responds with factually inconsistent answers. Designed without involvement from LLM providers, this benchmark offers impartial evaluation and accountability. In our experiment, we queried 19 LLMs with prompts that requested 5 facts for each of 15 topics. We repeated this query 100 times for each LLM, each time adding prompt context from a different persona selected from a subset of personas modeling the general population. We processed the responses into sentence embeddings, computed cross-persona cosine similarity, and computed the weighted average of cross-persona cosine similarity to calculate factual consistency scores. In 100-persona experiments, scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as a benchmark threshold. xAI's Grok-3 is most consistent, while several lightweight models rank lowest. Consistency varies by topic: the job market is least consistent, G7 world leaders most consistent, and issues like vaccines or the Israeli-Palestinian conflict diverge by provider. These results show that both the provider and the topic shape the factual consistency. We release our code and interactive demo to support reproducible evaluation and encourage persona-invariant prompting strategies.",
        "subjects": "Computation and Language, Artificial Intelligence, Human-Computer Interaction",
        "date": "2025-10-11",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.221872"
    },
    {
        "index": "#109",
        "title": "Revisiting the UID Hypothesis in LLM Reasoning Traces",
        "link": "/arxiv/2510.13850",
        "arxiv_id": "2510.13850",
        "authors": "Minju Gwak, Guijin Son, Jaehyung Kim",
        "summary": "Large language models (LLMs) often solve problems using step-by-step Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently unfaithful or hard to interpret. Inspired by the Uniform Information Density (UID) hypothesis in psycholinguistics -- which posits that humans communicate by maintaining a stable flow of information -- we introduce entropy-based metrics to analyze the information flow within reasoning traces. Surprisingly, across three challenging mathematical benchmarks, we find that successful reasoning in LLMs is globally non-uniform: correct solutions are characterized by uneven swings in information density, in stark contrast to human communication patterns. This result challenges assumptions about machine reasoning and suggests new directions for designing interpretable and adaptive reasoning models.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-11",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.222521"
    },
    {
        "index": "#113",
        "title": "Serialized EHR make for good text representations",
        "link": "/arxiv/2510.13843",
        "arxiv_id": "2510.13843",
        "authors": "Zhirong Chou, Quan Qin, Shi Li",
        "summary": "The emergence of foundation models in healthcare has opened new avenues for learning generalizable representations from large scale clinical data. Yet, existing approaches often struggle to reconcile the tabular and event based nature of Electronic Health Records (EHRs) with the sequential priors of natural language models. This structural mismatch limits their ability to capture longitudinal dependencies across patient encounters. We introduce SerialBEHRT, a domain aligned foundation model that extends SciBERT through additional pretraining on structured EHR sequences. SerialBEHRT is designed to encode temporal and contextual relationships among clinical events, thereby producing richer patient representations. We evaluate its effectiveness on the task of antibiotic susceptibility prediction, a clinically meaningful problem in antibiotic stewardship. Through extensive benchmarking against state of the art EHR representation strategies, we demonstrate that SerialBEHRT achieves superior and more consistent performance, highlighting the importance of temporal serialization in foundation model pretraining for healthcare.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-11",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.228934"
    },
    {
        "index": "#114",
        "title": "ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking",
        "link": "/arxiv/2510.13842",
        "arxiv_id": "2510.13842",
        "authors": "Yutao Wu, Xiao Liu, Yinghui Li, Yifeng Gao, Yifan Ding, Jiale Ding, Xiang Zheng, Xingjun Ma",
        "summary": "Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation (RAG) systems by injecting adversarial content into knowledge bases, tricking Large Language Models (LLMs) into producing attacker-controlled outputs grounded in manipulated context. Prior work highlights LLMs' susceptibility to misleading or malicious retrieved content. However, real-world fact-checking scenarios are more challenging, as credible evidence typically dominates the retrieval pool. To investigate this problem, we extend knowledge poisoning to the fact-checking setting, where retrieved context includes authentic supporting or refuting evidence. We propose \\textbf{ADMIT} (\\textbf{AD}versarial \\textbf{M}ulti-\\textbf{I}njection \\textbf{T}echnique), a few-shot, semantically aligned poisoning attack that flips fact-checking decisions and induces deceptive justifications, all without access to the target LLMs, retrievers, or token-level control. Extensive experiments show that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4 cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\\% at an extremely low poisoning rate of $0.93 \\times 10^{-6}$, and remaining robust even in the presence of strong counter-evidence. Compared with prior state-of-the-art attacks, ADMIT improves ASR by 11.2\\% across all settings, exposing significant vulnerabilities in real-world RAG-based fact-checking systems.",
        "subjects": "Computation and Language, Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-11",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.239595"
    },
    {
        "index": "#115",
        "title": "Meronymic Ontology Extraction via Large Language Models",
        "link": "/arxiv/2510.13839",
        "arxiv_id": "2510.13839",
        "authors": "Dekai Zhang, Simone Conia, Antonio Rago",
        "summary": "Ontologies have become essential in today's digital age as a way of organising the vast amount of readily available unstructured text. In providing formal structure to this information, ontologies have immense value and application across various domains, e.g., e-commerce, where countless product listings necessitate proper product organisation. However, the manual construction of these ontologies is a time-consuming, expensive and laborious process. In this paper, we harness the recent advancements in large language models (LLMs) to develop a fully-automated method of extracting product ontologies, in the form of meronymies, from raw review texts. We demonstrate that the ontologies produced by our method surpass an existing, BERT-based baseline when evaluating using an LLM-as-a-judge. Our investigation provides the groundwork for LLMs to be used more generally in (product or otherwise) ontology extraction.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-11",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.239883"
    },
    {
        "index": "#116",
        "title": "Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection",
        "link": "/arxiv/2510.13837",
        "arxiv_id": "2510.13837",
        "authors": "Weibin Cai, Reza Zafarani",
        "summary": "Hate speech detection has been extensively studied, yet existing methods often overlook a real-world complexity: training labels are biased, and interpretations of what is considered hate vary across individuals with different cultural backgrounds. We first analyze these challenges, including data sparsity, cultural entanglement, and ambiguous labeling. To address them, we propose a culture-aware framework that constructs individuals' hate subspaces. To alleviate data sparsity, we model combinations of cultural attributes. For cultural entanglement and ambiguous labels, we use label propagation to capture distinctive features of each combination. Finally, individual hate subspaces, which in turn can further enhance classification performance. Experiments show our method outperforms state-of-the-art by 1.05\\% on average across all metrics.",
        "subjects": "Computation and Language, Artificial Intelligence, Social and Information Networks",
        "date": "2025-10-11",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.240155"
    },
    {
        "index": "#117",
        "title": "SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models",
        "link": "/arxiv/2510.13836",
        "arxiv_id": "2510.13836",
        "authors": "Debarun Bhattacharjya, Balaji Ganesan, Junkyu Lee, Radu Marinescu, Katsiaryna Mirylenka, Michael Glass, Xiao Shou",
        "summary": "When does a large language model (LLM) know what it does not know? Uncertainty quantification (UQ) provides measures of uncertainty, such as an estimate of the confidence in an LLM's generated output, and is therefore increasingly recognized as a crucial component of trusted AI systems. Black-box UQ methods do not require access to internal model information from the generating LLM and therefore have numerous real-world advantages, such as robustness to system changes, adaptability to choice of LLM, reduced costs, and computational tractability. In this paper, we investigate the effectiveness of UQ techniques that are primarily but not necessarily entirely black-box, where the consistency between a generated output and other sampled generations is used as a proxy for confidence in its correctness. We propose a high-level non-verbalized similarity-based aggregation framework that subsumes a broad swath of UQ approaches suitable for complex generative tasks, as well as introduce specific novel techniques from the framework that train confidence estimation models using small training sets. Through an empirical study with datasets spanning the diverse tasks of question answering, summarization, and text-to-SQL, we demonstrate that our proposed similarity-based methods can yield better calibrated confidences than baselines.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.240459"
    },
    {
        "index": "#118",
        "title": "ConDABench: Interactive Evaluation of Language Models for Data Analysis",
        "link": "/arxiv/2510.13835",
        "arxiv_id": "2510.13835",
        "authors": "Avik Dutta, Priyanshu Gupta, Hosein Hasanbeig, Rahul Pratap Singh, Harshit Nigam, Sumit Gulwani, Arjun Radhakrishna, Gustavo Soares, Ashish Tiwari",
        "summary": "Real-world data analysis tasks often come with under-specified goals and unclean data. User interaction is necessary to understand and disambiguate a user's intent, and hence, essential to solving these complex tasks. Existing benchmarks for evaluating LLMs on data analysis tasks do not capture these complexities or provide first-class support for interactivity. We introduce ConDABench, a framework for generating conversational data analysis (ConDA) benchmarks and evaluating external tools on the generated benchmarks. \\bench consists of (a) a multi-agent workflow for generating realistic benchmarks from articles describing insights gained from public datasets, (b) 1,420 ConDA problems generated using this workflow, and (c) an evaluation harness that, for the first time, makes it possible to systematically evaluate conversational data analysis tools on the generated ConDA problems. Evaluation of state-of-the-art LLMs on the benchmarks reveals that while the new generation of models are better at solving more instances, they are not necessarily better at solving tasks that require sustained, long-form engagement. ConDABench is an avenue for model builders to measure progress towards truly collaborative models that can complete complex interactive tasks.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.240787"
    },
    {
        "index": "#120",
        "title": "Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference",
        "link": "/arxiv/2510.13831",
        "arxiv_id": "2510.13831",
        "authors": "Chao Han, Yijuan Liang, Zihao Xuan, Daokuan Wu, Wei Zhang, Xiaoyu Shen",
        "summary": "The deployment of large language models (LLMs) in real-world applications is increasingly limited by their high inference cost. While recent advances in dynamic token-level computation allocation attempt to improve efficiency by selectively activating model components per token, existing methods rely on greedy routing--a myopic execute-or-skip mechanism that often leads to irreversible information loss and suboptimal token selection. This paper introduces informed routing, a new paradigm that proactively addresses these issues. The key insight is to assess not only a token's immediate importance but also its recoverability, i.e., how well its transformation can be approximated. To this end, we propose the Lightweight Feature Forecaster (LFF), a small predictive module that estimates a unit's output before routing decisions are made. This enables a flexible execute-or-approximate policy that preserves model fidelity while drastically reducing computation. Extensive experiments on both language modeling and reasoning tasks show that informed routing achieves state-of-the-art efficiency-performance trade-offs across multiple sparsity levels. Notably, even without final LoRA fine-tuning, our method matches or surpasses strong baselines that require full fine-tuning, all while reducing training time by over 50%. The code is available at: https://github.com/EIT-NLP/informed-routing",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.241384"
    },
    {
        "index": "#121",
        "title": "Users as Annotators: LLM Preference Learning from Comparison Mode",
        "link": "/arxiv/2510.13830",
        "arxiv_id": "2510.13830",
        "authors": "Zhongze Cai, Xiaocheng Li",
        "summary": "Pairwise preference data have played an important role in the alignment of large language models (LLMs). Each sample of such data consists of a prompt, two different responses to the prompt, and a binary label indicating which of the two responses is better. The labels are usually annotated by professional human annotators. In this paper, we consider an alternative approach to collect pairwise preference data -- user annotation from comparison mode. With the increasingly wider adoption of LLMs among the population, users are contributing more and more of their preference labels through their daily interactions with the LLMs. The upside of such labels is that users are the best experts in judging the responses to their own queries/prompts, but the downside is the lack of quality control in these labels. In this paper, we consider a new idea of generating two responses from two different models or two different versions of the same model. The asymmetry allows us to make an inference of the user's data quality through our proposed user behavior model. We develop an expectation-maximization algorithm to estimate a latent quality factor of the user, and filter users' annotation data accordingly. The downstream task shows the effectiveness of our approach in both capturing the user behavior and data filtering for LLM alignment.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.241666"
    },
    {
        "index": "#122",
        "title": "A Linguistics-Aware LLM Watermarking via Syntactic Predictability",
        "link": "/arxiv/2510.13829",
        "arxiv_id": "2510.13829",
        "authors": "Shinwoo Park, Hyejin Park, Hyeseon Ahn, Yo-Sub Han",
        "summary": "As large language models (LLMs) continue to advance rapidly, reliable governance tools have become critical. Publicly verifiable watermarking is particularly essential for fostering a trustworthy AI ecosystem. A central challenge persists: balancing text quality against detection robustness. Recent studies have sought to navigate this trade-off by leveraging signals from model output distributions (e.g., token-level entropy); however, their reliance on these model-specific signals presents a significant barrier to public verification, as the detection process requires access to the logits of the underlying model. We introduce STELA, a novel framework that aligns watermark strength with the linguistic degrees of freedom inherent in language. STELA dynamically modulates the signal using part-of-speech (POS) n-gram-modeled linguistic indeterminacy, weakening it in grammatically constrained contexts to preserve quality and strengthen it in contexts with greater linguistic flexibility to enhance detectability. Our detector operates without access to any model logits, thus facilitating publicly verifiable detection. Through extensive experiments on typologically diverse languages-analytic English, isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior methods in detection robustness. Our code is available at https://github.com/Shinwoo-Park/stela_watermark.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.241973"
    },
    {
        "index": "#123",
        "title": "From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening",
        "link": "/arxiv/2510.13828",
        "arxiv_id": "2510.13828",
        "authors": "Ratna Kandala, Akshata Kishore Moharir, Divya Arvinda Nayak",
        "summary": "Explainable Artificial Intelligence (XAI) has been presented as the critical component for unlocking the potential of machine learning in mental health screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI techniques, such as SHAP and LIME, excel at producing technically faithful outputs such as feature importance scores, but fail to deliver clinically relevant, actionable insights that can be used by clinicians or understood by patients. This disconnect between technical transparency and human utility is the primary barrier to real-world adoption. This paper argues that this gap is a translation problem and proposes the Generative Operational Framework, a novel system architecture that leverages Large Language Models (LLMs) as a central translation engine. This framework is designed to ingest the raw, technical outputs from diverse XAI tools and synthesize them with clinical guidelines (via RAG) to automatically generate human-readable, evidence-backed clinical narratives. To justify our solution, we provide a systematic analysis of the components it integrates, tracing the evolution from intrinsic models to generative XAI. We demonstrate how this framework directly addresses key operational barriers, including workflow integration, bias mitigation, and stakeholder-specific communication. This paper also provides a strategic roadmap for moving the field beyond the generation of isolated data points toward the delivery of integrated, actionable, and trustworthy AI in clinical practice.",
        "subjects": "Computation and Language",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.242258"
    },
    {
        "index": "#124",
        "title": "Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL",
        "link": "/arxiv/2510.13827",
        "arxiv_id": "2510.13827",
        "authors": "Ashish Kattamuri, Ishita Prasad, Meetu Malhotra, Arpita Vats, Rahul Raja, Albert Lie",
        "summary": "Current Text-to-SQL methods are evaluated and only focused on executable queries, overlooking the semantic alignment challenge -- both in terms of the semantic meaning of the query and the correctness of the execution results. Even execution accuracy itself shows significant drops when moving from English to other languages, with an average decline of 6 percentage points across non-English languages. We address these challenges by presenting a new framework that combines Group Relative Policy Optimization (GRPO) within a multilingual contrastive reward signal to enhance both task efficiency and semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method teaches models to obtain better correspondence between SQL generation and user intent by combining a reward signal based on semantic similarity. On the seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive reward signal in the GRPO framework further improved the average semantic accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our experiments showcase that a smaller, parameter-efficient 3B LLaMA model fine-tuned with our contrastive reward signal outperforms a much larger zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from 81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using just 3,000 reinforcement learning training examples. These results demonstrate how we can improve the performance of Text-to-SQL systems with contrastive rewards for directed semantic alignment, without requiring large-scale training datasets.",
        "subjects": "Computation and Language, Artificial Intelligence",
        "date": "2025-10-10",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.242578"
    },
    {
        "index": "#127",
        "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning",
        "link": "/arxiv/2510.14958",
        "arxiv_id": "2510.14958",
        "authors": "Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li",
        "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.243611"
    },
    {
        "index": "#133",
        "title": "Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media",
        "link": "/arxiv/2510.14889",
        "arxiv_id": "2510.14889",
        "authors": "Soorya Ram Shimgekar, Ruining Zhao, Agam Goyal, Violeta J. Rodriguez, Paul A. Bloom, Hari Sundaram, Koustuv Saha",
        "summary": "On social media, many individuals experiencing suicidal ideation (SI) do not disclose their distress explicitly. Instead, signs may surface indirectly through everyday posts or peer interactions. Detecting such implicit signals early is critical but remains challenging. We frame early and implicit SI as a forward-looking prediction task and develop a computational framework that models a user's information environment, consisting of both their longitudinal posting histories as well as the discourse of their socially proximal peers. We adopted a composite network centrality measure to identify top neighbors of a user, and temporally aligned the user's and neighbors' interactions -- integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves early and implicit SI detection by 15% over individual-only baselines. These findings highlight that peer interactions offer valuable predictive signals and carry broader implications for designing early detection systems that capture indirect as well as masked expressions of risk in online environments.",
        "subjects": "Social and Information Networks, Artificial Intelligence, Computation and Language, Computers and Society, Human-Computer Interaction",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.250670"
    },
    {
        "index": "#134",
        "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction",
        "link": "/arxiv/2510.14885",
        "arxiv_id": "2510.14885",
        "authors": "Logan Lawrence, Oindrila Saha, Megan Wei, Chen Sun, Subhransu Maji, Grant Van Horn",
        "summary": "Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.250962"
    },
    {
        "index": "#135",
        "title": "Benchmarking Multimodal Large Language Models for Face Recognition",
        "link": "/arxiv/2510.14866",
        "arxiv_id": "2510.14866",
        "authors": "Hatef Otroshi Shahreza, Sébastien Marcel",
        "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance across diverse vision-and-language tasks. However, their potential in face recognition remains underexplored. In particular, the performance of open-source MLLMs needs to be evaluated and compared with existing face recognition models on standard benchmarks with similar protocol. In this work, we present a systematic benchmark of state-of-the-art MLLMs for face recognition on several face recognition datasets, including LFW, CALFW, CPLFW, CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich semantic cues useful for face-related tasks, they lag behind specialized models in high-precision recognition scenarios in zero-shot applications. This benchmark provides a foundation for advancing MLLM-based face recognition, offering insights for the design of next-generation models with higher accuracy and generalization. The source code of our benchmark is publicly available in the project page.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.251238"
    },
    {
        "index": "#136",
        "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
        "link": "/arxiv/2510.14846",
        "arxiv_id": "2510.14846",
        "authors": "Zhuo-Yang Song",
        "summary": "The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via a majority-vote instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.",
        "subjects": "Artificial Intelligence, Computation and Language, Logic in Computer Science",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.251493"
    },
    {
        "index": "#137",
        "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence",
        "link": "/arxiv/2510.14670",
        "arxiv_id": "2510.14670",
        "authors": "Marco Simoni, Aleksandar Fontana, Andrea Saracino, Paolo Mori",
        "summary": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph. It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence. Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses. To support training and evaluation, we introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test: 13951) pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations. Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.",
        "subjects": "Artificial Intelligence, Computation and Language, Cryptography and Security, Information Retrieval",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.251792"
    },
    {
        "index": "#138",
        "title": "ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks",
        "link": "/arxiv/2510.14621",
        "arxiv_id": "2510.14621",
        "authors": "Yuanyi Song, Heyuan Huang, Qiqiang Lin, Yin Zhao, Xiangmou Qu, Jun Wang, Xingyu Lou, Weiwen Liu, Zhuosheng Zhang, Jun Wang, Yong Yu, Weinan Zhang, Zhaoxiang Wang",
        "summary": "The rapid advancement of multimodal large language models has enabled agents to operate mobile devices by directly interacting with graphical user interfaces, opening new possibilities for mobile automation. However, real-world mobile tasks are often complex and allow for multiple valid solutions. This contradicts current mobile agent evaluation standards: offline static benchmarks can only validate a single predefined \"golden path\", while online dynamic testing is constrained by the complexity and non-reproducibility of real devices, making both approaches inadequate for comprehensively assessing agent capabilities. To bridge the gap between offline and online evaluation and enhance testing stability, this paper introduces a novel graph-structured benchmarking framework. By modeling the finite states observed during real-device interactions, it achieves static simulation of dynamic behaviors. Building on this, we develop ColorBench, a benchmark focused on complex long-horizon tasks. It supports evaluation of multiple valid solutions, subtask completion rate statistics, and atomic-level capability analysis. ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average length of over 13 steps. Each task includes at least two correct paths and several typical error paths, enabling quasi-dynamic interaction. By evaluating ColorBench across various baselines, we discover limitations of existing models and propose improvement directions and feasible technical pathways to enhance agents' performance on complex, long-horizon problems based on experimental results. Code and data are available at: https://github.com/MadeAgents/ColorBench.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.252131"
    },
    {
        "index": "#139",
        "title": "Just-In-Time Objectives: A General Approach for Specialized AI Interactions",
        "link": "/arxiv/2510.14591",
        "arxiv_id": "2510.14591",
        "authors": "Michelle S. Lam, Omar Shaikh, Hallie Xu, Alice Guo, Diyi Yang, Jeffrey Heer, James A. Landay, Michael S. Bernstein",
        "summary": "Large language models promise a broad set of functions, but when not given a specific objective, they default to milquetoast results such as drafting emails littered with cliches. We demonstrate that inferring the user's in-the-moment objective, then rapidly optimizing for that singular objective, enables LLMs to produce tools, interfaces, and responses that are more responsive and desired. We contribute an architecture for automatically inducing just-in-time objectives by passively observing user behavior, then steering downstream AI systems through generation and evaluation against this objective. Inducing just-in-time objectives (e.g., \"Clarify the abstract's research contribution\") enables automatic generation of tools, e.g., those that critique a draft based on relevant HCI methodologies, anticipate related researchers' reactions, or surface ambiguous terminology. In a series of experiments (N=14, N=205) on participants' own tasks, JIT objectives enable LLM outputs that achieve 66-86% win rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT objectives produce specialized tools unique to each participant.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.252445"
    },
    {
        "index": "#140",
        "title": "Talking Points: Describing and Localizing Pixels",
        "link": "/arxiv/2510.14583",
        "arxiv_id": "2510.14583",
        "authors": "Matan Rusanovsky, Shimon Malnick, Shai Avidan",
        "summary": "Vision-language models have achieved remarkable success in cross-modal understanding. Yet, these models remain limited to object-level or region-level grounding, lacking the capability for pixel-precise keypoint comprehension through natural language. We introduce a novel framework for pixel level grounding. The framework consists of two complementary components: a Point Descriptor that generates rich, contextual descriptions of individual keypoints, and a Point Localizer that regresses precise pixel coordinates from these descriptions. Unlike prior work that relies on templated prompts or keypoint names, our approach produces free-form, coarse-to-fine descriptions that situate keypoints within their visual context. Since there is no available dataset to train such a system, we introduce LlamaPointInPart, a carefully curated dataset of 20K+ image-keypoint-description triplets synthesized from multiple vision-language models, capturing multi-scale information from scene-level context to visual features around the keypoint. For cross-category generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the frozen Point Localizer as a reward model to produce descriptions that maximize localization accuracy. To evaluate our results we establish a new evaluation protocol. Instead of comparing the text description produced by our method to the ground truth, we use the localizer to determine how close is the predicted point generated to the ground truth point. Experiments demonstrate superior performance compared to baseline models on LlamaPointInPart.The bidirectional nature of our framework should enable future applications in both keypoint-guided image understanding and language-guided precise localization. Our code and dataset are publicly available at https://github.com/matanr/Talking_Points.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.252704"
    },
    {
        "index": "#142",
        "title": "E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task",
        "link": "/arxiv/2510.14509",
        "arxiv_id": "2510.14509",
        "authors": "Jingyao Liu, Chen Huang, Zhizhao Guan, Wenqiang Lei, Yang Deng",
        "summary": "E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple BDD test scenarios with corresponding Python step implementations for each requirement}, and (iii) a fully automated testing pipeline built on the Behave framework. To ensure its quality while reducing the annotation effort, E2EDev leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with E2EDev}, our analysis reveals a persistent struggle to effectively solve these tasks, underscoring the critical need for more effective and cost-efficient E2ESD solutions. Our codebase and benchmark are publicly available at https://github.com/SCUNLP/E2EDev.",
        "subjects": "Software Engineering, Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.253379"
    },
    {
        "index": "#143",
        "title": "IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning",
        "link": "/arxiv/2510.14406",
        "arxiv_id": "2510.14406",
        "authors": "Xikai Zhang, Bo Wang, Likang Xiao, Yongzhi Li, Quan Chen, Wenju Wu, Liu Liu",
        "summary": "Although large language models (LLMs) have made significant strides across various tasks, they still face significant challenges in complex reasoning and planning. For example, even with carefully designed prompts and prior information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on the TravelPlanner dataset in the sole-planning mode. Similarly, even in the thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent Systems (MAS) can offer improved collective reasoning, they often suffer from high reasoning costs due to multi-round internal interactions, long per-response latency, and difficulties in end-to-end training. To address these challenges, we propose a general and scalable framework called IMAGINE, short for Integrating Multi-Agent System into One Model. This framework not only integrates the reasoning and planning capabilities of MAS into a single, compact model, but also significantly surpass the capabilities of the MAS through a simple end-to-end training. Through this pipeline, a single small-scale model is not only able to acquire the structured reasoning and planning capabilities of a well-organized MAS but can also significantly outperform it. Experimental results demonstrate that, when using Qwen3-8B-Instruct as the base model and training it with our method, the model achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.253686"
    },
    {
        "index": "#145",
        "title": "AI for Service: Proactive Assistance with AI Glasses",
        "link": "/arxiv/2510.14359",
        "arxiv_id": "2510.14359",
        "authors": "Zichen Wen, Yiyu Wang, Chenfei Liao, Boxue Yang, Junxian Li, Weifeng Liu, Haocong He, Bolong Feng, Xuyang Liu, Yuanhuiyi Lyu, Xu Zheng, Xuming Hu, Linfeng Zhang",
        "summary": "In an era where AI is evolving from a passive tool into an active and adaptive companion, we introduce AI for Service (AI4Service), a new paradigm that enables proactive and real-time assistance in daily life. Existing AI services remain largely reactive, responding only to explicit user commands. We argue that a truly intelligent and helpful assistant should be capable of anticipating user needs and taking actions proactively when appropriate. To realize this vision, we propose Alpha-Service, a unified framework that addresses two fundamental challenges: Know When to intervene by detecting service opportunities from egocentric video streams, and Know How to provide both generalized and personalized services. Inspired by the von Neumann computer architecture and based on AI glasses, Alpha-Service consists of five key components: an Input Unit for perception, a Central Processing Unit for task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit for long-term personalization, and an Output Unit for natural human interaction. As an initial exploration, we implement Alpha-Service through a multi-agent system deployed on AI glasses. Case studies, including a real-time Blackjack advisor, a museum tour guide, and a shopping fit assistant, demonstrate its ability to seamlessly perceive the environment, infer user intent, and provide timely and useful assistance without explicit prompts.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.254371"
    },
    {
        "index": "#146",
        "title": "Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies",
        "link": "/arxiv/2510.14312",
        "arxiv_id": "2510.14312",
        "authors": "Mason Nakamura, Abhinav Kumar, Saaduddin Mahmud, Sahar Abdelnabi, Shlomo Zilberstein, Eugene Bagdasarian",
        "summary": "A multi-agent system (MAS) powered by large language models (LLMs) can automate tedious user tasks such as meeting scheduling that requires inter-agent collaboration. LLMs enable nuanced protocols that account for unstructured private data, user constraints, and preferences. However, this design introduces new risks, including misalignment and attacks by malicious parties that compromise agents or steal user data. In this paper, we propose the Terrarium framework for fine-grained study on safety, privacy, and security in LLM-based MAS. We repurpose the blackboard design, an early approach in multi-agent systems, to create a modular, configurable testbed for multi-agent collaboration. We identify key attack vectors such as misalignment, malicious agents, compromised communication, and data poisoning. We implement three collaborative MAS scenarios with four representative attacks to demonstrate the framework's flexibility. By providing tools to rapidly prototype, evaluate, and iterate on defenses and designs, Terrarium aims to accelerate progress toward trustworthy multi-agent systems.",
        "subjects": "Artificial Intelligence, Computation and Language, Cryptography and Security",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.259801"
    },
    {
        "index": "#149",
        "title": "Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition",
        "link": "/arxiv/2510.14203",
        "arxiv_id": "2510.14203",
        "authors": "Ryo Masumura, Shota Orihashi, Mana Ihori, Tomohiro Tanaka, Naoki Makishima, Taiga Yamane, Naotaka Kawata, Satoshi Suzuki, Taichi Katayama",
        "summary": "This paper proposes a joint modeling method of the Big Five, which has long been studied, and HEXACO, which has recently attracted attention in psychology, for automatically recognizing apparent personality traits from multimodal human behavior. Most previous studies have used the Big Five for multimodal apparent personality-trait recognition. However, no study has focused on apparent HEXACO which can evaluate an Honesty-Humility trait related to displaced aggression and vengefulness, social-dominance orientation, etc. In addition, the relationships between the Big Five and HEXACO when modeled by machine learning have not been clarified. We expect awareness of multimodal human behavior to improve by considering these relationships. The key advance of our proposed method is to optimize jointly recognizing the Big Five and HEXACO. Experiments using a self-introduction video dataset demonstrate that the proposed method can effectively recognize the Big Five and HEXACO.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language, Multimedia",
        "date": "2025-10-16",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.260753"
    },
    {
        "index": "#152",
        "title": "Generating Fair Consensus Statements with Social Choice on Token-Level MDPs",
        "link": "/arxiv/2510.14106",
        "arxiv_id": "2510.14106",
        "authors": "Carter Blair, Kate Larson",
        "summary": "Current frameworks for consensus statement generation with large language models lack the inherent structure needed to provide provable fairness guarantees when aggregating diverse free-form opinions. We model the task as a multi-objective, token-level Markov Decision Process (MDP), where each objective corresponds to an agent's preference. Token-level rewards for each agent are derived from their policy (e.g., a personalized language model). This approach utilizes the finding that such policies implicitly define optimal Q-functions, providing a principled way to quantify rewards at each generation step without a value function (Rafailov et al., 2024). This MDP formulation creates a formal structure amenable to analysis using principles from social choice theory. We propose two approaches grounded in social choice theory. First, we propose a stochastic generation policy guaranteed to be in the ex-ante core, extending core stability concepts from voting theory to text generation. This policy is derived from an underlying distribution over complete statements that maximizes proportional fairness (Nash Welfare). Second, for generating a single statement, we target the maximization of egalitarian welfare using search algorithms within the MDP framework. Empirically, experiments using language models to instantiate agent policies show that search guided by the egalitarian objective generates consensus statements with improved worst-case agent alignment compared to baseline methods, including the Habermas Machine (Tessler et al., 2024).",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Science and Game Theory",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.261591"
    },
    {
        "index": "#154",
        "title": "Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks",
        "link": "/arxiv/2510.13979",
        "arxiv_id": "2510.13979",
        "authors": "Supriti Sinhamahapatra, Jan Niehues",
        "summary": "State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily rely on acoustic information while disregarding additional multi-modal context. However, visual information are essential in disambiguation and adaptation. While most work focus on speaker images to handle noise conditions, this work also focuses on integrating presentation slides for the use cases of scientific presentation. In a first step, we create a benchmark for multi-modal presentation including an automatic analysis of transcribing domain-specific terminology. Next, we explore methods for augmenting speech models with multi-modal information. We mitigate the lack of datasets with accompanying slides by a suitable approach of data augmentation. Finally, we train a model using the augmented dataset, resulting in a relative reduction in word error rate of approximately 34%, across all words and 35%, for domain-specific terms compared to the baseline model.",
        "subjects": "Artificial Intelligence, Computation and Language",
        "date": "2025-10-15",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.262139"
    },
    {
        "index": "#156",
        "title": "Generative AI in Heritage Practice: Improving the Accessibility of Heritage Guidance",
        "link": "/arxiv/2510.13811",
        "arxiv_id": "2510.13811",
        "authors": "Jessica Witte, Edmund Lee, Lisa Brausem, Verity Shillabeer, Chiara Bonacchi",
        "summary": "This paper discusses the potential for integrating Generative Artificial Intelligence (GenAI) into professional heritage practice with the aim of enhancing the accessibility of public-facing guidance documents. We developed HAZEL, a GenAI chatbot fine-tuned to assist with revising written guidance relating to heritage conservation and interpretation. Using quantitative assessments, we compare HAZEL's performance to that of ChatGPT (GPT-4) in a series of tasks related to the guidance writing process. The results of this comparison indicate a slightly better performance of HAZEL over ChatGPT, suggesting that the GenAI chatbot is more effective once the underlying large language model (LLM) has been fine-tuned. However, we also note significant limitations, particularly in areas requiring cultural sensitivity and more advanced technical expertise. These findings suggest that, while GenAI cannot replace human heritage professionals in technical authoring tasks, its potential to automate and expedite certain aspects of guidance writing could offer valuable benefits to heritage organisations, especially in resource-constrained contexts.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Computation and Language",
        "date": "2025-09-03",
        "category": "cs.CL",
        "crawl_time": "2025-10-17T11:00:05.262701"
    },
    {
        "index": "#2",
        "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning",
        "link": "/arxiv/2510.14942",
        "arxiv_id": "2510.14942",
        "authors": "Yao Zhang, Yu Wu, Haowei Zhang, Weiguo Li, Haokun Chen, Jingpei Wu, Guohao Li, Zhen Han, Volker Tresp",
        "summary": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.280168"
    },
    {
        "index": "#6",
        "title": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates",
        "link": "/arxiv/2510.14900",
        "arxiv_id": "2510.14900",
        "authors": "Wen-Kwang Tsao, Yao-Ching Yu, Chien-Ming Huang",
        "summary": "The Enterprise Intelligence Platform must integrate logs from numerous third-party vendors in order to perform various downstream tasks. However, vendor documentation is often unavailable at test time. It is either misplaced, mismatched, poorly formatted, or incomplete, which makes schema mapping challenging. We introduce a reinforcement learning agent that can self-improve without labeled examples or model weight updates. During inference, the agent: 1) Identifies ambiguous field-mapping attempts. 2) Generates targeted web-search queries to gather external evidence. 3) Applies a confidence-based reward to iteratively refine its mappings. To demonstrate this concept, we converted Microsoft Defender for Endpoint logs into a common schema. Our method increased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\% over 100 iterations using GPT-4o. At the same time, it reduced the number of low-confidence mappings requiring expert review by 85\\%. This new approach provides an evidence-driven, transparent method for solving future industry problems, paving the way for more robust, accountable, scalable, efficient, flexible, adaptable, and collaborative solutions.",
        "subjects": "Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.281466"
    },
    {
        "index": "#7",
        "title": "The Gatekeeper Knows Enough",
        "link": "/arxiv/2510.14881",
        "arxiv_id": "2510.14881",
        "authors": "Fikresilase Wondmeneh Abebayew",
        "summary": "Large Language Models (LLMs) are increasingly deployed as autonomous agents, yet their practical utility is fundamentally constrained by a limited context window and state desynchronization resulting from the LLMs' stateless nature and inefficient context management. These limitations lead to unreliable output, unpredictable behavior, and inefficient resource usage, particularly when interacting with large, structured, and sensitive knowledge systems such as codebases and documents. To address these challenges, we introduce the Gatekeeper Protocol, a novel, domain-agnostic framework that governs agent-system interactions. Our protocol mandates that the agent first operate and reason on a minimalist, low-fidelity \"latent state\" representation of the system to strategically request high-fidelity context on demand. All interactions are mediated through a unified JSON format that serves as a declarative, state-synchronized protocol, ensuring the agent's model of the system remains verifiably grounded in the system's reality. We demonstrate the efficacy of this protocol with Sage, a reference implementation of the Gatekeeper Protocol for software development. Our results show that this approach significantly increases agent reliability, improves computational efficiency by minimizing token consumption, and enables scalable interaction with complex systems, creating a foundational methodology for building more robust, predictable, and grounded AI agents for any structured knowledge domain.",
        "subjects": "Artificial Intelligence, Information Theory",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.281758"
    },
    {
        "index": "#8",
        "title": "LabOS: The AI-XR Co-Scientist That Sees and Works With Humans",
        "link": "/arxiv/2510.14861",
        "arxiv_id": "2510.14861",
        "authors": "Le Cong, Zaixi Zhang, Xiaotong Wang, Yin Di, Ruofan Jin, Michal Gerasimiuk, Yinkai Wang, Ravi K. Dinesh, David Smerkous, Alex Smerkous, Xuekun Wu, Shilong Liu, Peishan Li, Yi Zhu, Simran Serrao, Ning Zhao, Imran A. Mohammad, John B. Sunwoo, Joseph C. Wu, Mengdi Wang",
        "summary": "Modern science advances fastest when thought meets action. LabOS represents the first AI co-scientist that unites computational reasoning with physical experimentation through multimodal perception, self-evolving agents, and Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see what scientists see, understand experimental context, and assist in real-time execution. Across applications--from cancer immunotherapy target discovery to stem-cell engineering -- LabOS shows that AI can move beyond computational design to participation, turning the laboratory into an intelligent, collaborative environment where human and machine discovery evolve together.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.282215"
    },
    {
        "index": "#10",
        "title": "Boosting Instruction Following at Scale",
        "link": "/arxiv/2510.14842",
        "arxiv_id": "2510.14842",
        "authors": "Ben Elder, Evelyn Duesterwald, Vinod Muthusamy",
        "summary": "A typical approach developers follow to influence an LLM's behavior in an application is through careful manipulation of the prompt, such as by adding or modifying instructions. However, merely adding more instructions provides little assurance that they will actually be followed. We introduce Instruction Boosting as a post-generation method to increase the reliability of LLM prompt instructions. We show that Instruction Boosting improves the instruction following rate by up to 7 points for two instructions and up to 4 points for ten instructions. To demonstrate these results we introduce SCALEDIF, a benchmark with a scaled instruction volume of up to ten instructions per data sample. We also present an analysis of the commonly observed trend that performance degrades as more instructions are added. We show that an important factor contributing to this trend is the degree of tension and conflict that arises as the number of instructions is increased. We contribute a quantitative conflict scoring tool that explains the observed performance trends and provides feedback to developers on the impact that additional prompt instructions have on a model's performance.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.282805"
    },
    {
        "index": "#11",
        "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning",
        "link": "/arxiv/2510.14828",
        "arxiv_id": "2510.14828",
        "authors": "Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li",
        "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots to complete complex human instructions in long-view manipulation tasks successfully. Despite the success of large language models and vision language models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue facing challenges in performing long-horizon manipulation tasks in complex real-world environments, owing to their restricted common sense and reasoning capabilities. Considering that aligning general-purpose vision language models to robotic planning tasks via supervised fine-tuning suffers from poor generalization and insufficient physical understanding, we propose RoboGPT-R1, a two-stage fine-tuning framework for embodied planning. In this framework, supervised training acquires foundational knowledge through expert sequences, followed by RL to address the model's shortcomings in visual-spatial understanding and reasoning. To achieve physical understanding and action sequence consistency in multi-step reasoning tasks, we design a rule-based reward function that simultaneously considers long-horizon performance and action constraint in the environment. The reasoning model, trained on Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini, by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the EmbodiedBench benchmark.",
        "subjects": "Artificial Intelligence, Robotics",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.283160"
    },
    {
        "index": "#13",
        "title": "SimKO: Simple Pass@K Policy Optimization",
        "link": "/arxiv/2510.14807",
        "arxiv_id": "2510.14807",
        "authors": "Ruotian Peng, Yi Ren, Zhouliang Yu, Weiyang Liu, Yandong Wen",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs). However, prevailing RLVR methods exhibit a systematic bias toward exploitation over exploration, as evidenced by improved pass@1 but reduced pass@K (K>1) performance. To understand this issue, we analyze training dynamics of RLVR methods by tracking the token-level probability distributions over vocabulary candidates. Our analysis reveals a consistent probability concentration effect where the top-1 candidate increasingly accumulates probability mass and suppresses that of other candidates. More importantly, stronger over-concentration correlates with worse pass@K performance. Inspired by this finding, we propose Simple Pass@K Optimization (SimKO), a method designed to mitigate the over-concentration issue, thereby encouraging exploration. SimKO operates in an asymmetrical manner. For verified-correct responses, it boosts the probabilities of the top-K candidates. For verified-incorrect responses, it applies stronger penalties to the top-1 candidate. We observe that this asymmetric design is particularly effective at mitigating over-concentration when applied at tokens with high entropy. Across various math and logical-reasoning benchmarks, SimKO consistently yields higher pass@K for a wide range of K, providing a simple way to improve RLVR's exploration.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.283786"
    },
    {
        "index": "#14",
        "title": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling",
        "link": "/arxiv/2510.14703",
        "arxiv_id": "2510.14703",
        "authors": "Jianghao Lin, Yuanyuan Shi, Xin Peng, Renjie Ding, Hairui Wang, Yuxuan Peng, Bizhe Bai, Weixi Song, Fengshuo Bai, Huacan Chai, Weinan Zhang, Fei Huang, Ying Wen",
        "summary": "Large language models (LLMs) are increasingly demonstrating strong capabilities as autonomous agents, with function calling serving as a core mechanism for interaction with the environment. Meanwhile, inference scaling has become a cutting-edge technique to enhance LLM performance by allocating more computational resources during the inference process. However, current research on inference scaling primarily focuses on unstructured output generation tasks, leaving its application in structured outputs, like function calling, largely underexplored. To bridge this gap, we propose an inference scaling framework that combines fine-grained beam search with a process reward model, ToolPRM, which scores the internal steps of each single function call. To train ToolPRM, we construct the first fine-grained intra-call process supervision dataset, automatically annotated with function-masking techniques to provide step-level rewards for structured tool-use reasoning. Extensive experiments demonstrate that ToolPRM beats the coarse-grained and outcome reward models in terms of predictive accuracy, indicating its stronger capability in supervising the function calling inference process. Inference scaling technique equipped with ToolPRM also significantly improves the backbone model performance across various function calling tasks and benchmarks. More importantly, we reveal a key principle for applying inference scaling techniques to structured outputs: \"explore more but retain less\" due to the unrecoverability characteristics of structured function calling generation.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.284181"
    },
    {
        "index": "#15",
        "title": "Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction",
        "link": "/arxiv/2510.14702",
        "arxiv_id": "2510.14702",
        "authors": "Penglong Zhai, Jie Li, Fanyi Di, Yue Liu, Yifang Yuan, Jie Huang, Peng Wu, Sicong Wang, Mingyang Yin, Tingting Hu, Yao Xu, Xin Li",
        "summary": "The next point-of-interest (POI) recommendation task aims to predict the users' immediate next destinations based on their preferences and historical check-ins, holding significant value in location-based services. Recently, large language models (LLMs) have shown great potential in recommender systems, which treat the next POI prediction in a generative manner. However, these LLMs, pretrained primarily on vast corpora of unstructured text, lack the native understanding of structured geographical entities and sequential mobility patterns required for next POI prediction tasks. Moreover, in industrial-scale POI prediction applications, incorporating world knowledge and alignment of human cognition, such as seasons, weather conditions, holidays, and users' profiles (such as habits, occupation, and preferences), can enhance the user experience while improving recommendation performance. To address these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a framework employing natural language as an interface, allowing for the incorporation of world knowledge, spatio-temporal trajectory patterns, profiles, and situational information. Specifically, CoAST mainly comprises of 2 stages: (1) Recommendation Knowledge Acquisition through continued pretraining on the enriched spatial-temporal trajectory data of the desensitized users; (2) Cognitive Alignment to align cognitive judgments with human preferences using enriched training data through Supervised Fine-Tuning (SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline experiments on various real-world datasets and online experiments deployed in \"Guess Where You Go\" of AMAP App homepage demonstrate the effectiveness of CoAST.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.284551"
    },
    {
        "index": "#16",
        "title": "Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging",
        "link": "/arxiv/2510.14697",
        "arxiv_id": "2510.14697",
        "authors": "Bang An, Yibo Yang, Philip Torr, Bernard Ghanem",
        "summary": "Model merging aims to integrate task-specific abilities from individually fine-tuned models into a single model without extra training. In recent model merging methods, task vector has become a fundamental building block, as it can encapsulate the residual information from finetuning. However, the merged model often suffers from notable performance degradation due to the conflicts caused by task-irrelevant redundancy in task vectors. Existing efforts in overcoming redundancy by randomly dropping elements in the parameter space involves randomness and lacks knowledge awareness. To address these challenges, in this study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace. Concretely, we sample some training examples from each task, and feed them into their corresponding fine-tuned models to acquire the covariance matrices before linear layers. We then perform a context-oriented singular value decomposition, which accentuates the weight components most relevant to the target knowledge. As a result, we can split fine-tuned model weights into task-relevant and redundant components in the knowledge-aware subspace, and purify the task vector by pruning the redundant components. To induce fair pruning efforts across models, we further introduce a spectral rank allocation strategy by optimizing a normalized activated pruning error. The task vector purification by our method as a plug-and-play scheme is applicable across various task vector-based merging methods to improve their performance. In experiments, we demonstrate the effectiveness of PAVE across a diverse set of merging methods, tasks, and model architectures.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.284842"
    },
    {
        "index": "#17",
        "title": "Practical, Utilitarian Algorithm Configuration",
        "link": "/arxiv/2510.14683",
        "arxiv_id": "2510.14683",
        "authors": "Devon Graham, Kevin Leyton-Brown",
        "summary": "Utilitarian algorithm configuration identifies a parameter setting for a given algorithm that maximizes a user's utility. Utility functions offer a theoretically well-grounded approach to optimizing decision-making under uncertainty and are flexible enough to capture a user's preferences over algorithm runtimes (e.g., they can describe a sharp cutoff after which a solution is no longer required, a per-hour cost for compute, or diminishing returns from algorithms that take longer to run). COUP is a recently-introduced utilitarian algorithm configuration procedure which was designed mainly to offer strong theoretical guarantees about the quality of the configuration it returns, with less attention paid to its practical performance. This paper closes that gap, bringing theoretically-grounded, utilitarian algorithm configuration to the point where it is competitive with widely used, heuristic configuration procedures that offer no performance guarantees. We present a series of improvements to COUP that improve its empirical performance without degrading its theoretical guarantees and demonstrate their benefit experimentally. Using a case study, we also illustrate ways of exploring the robustness of a given solution to the algorithm selection problem to variations in the utility function.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.285118"
    },
    {
        "index": "#18",
        "title": "NAEL: Non-Anthropocentric Ethical Logic",
        "link": "/arxiv/2510.14676",
        "arxiv_id": "2510.14676",
        "authors": "Bianca Maria Lerma, Rafael Peñaloza",
        "summary": "We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical framework for artificial agents grounded in active inference and symbolic reasoning. Departing from conventional, human-centred approaches to AI ethics, NAEL formalizes ethical behaviour as an emergent property of intelligent systems minimizing global expected free energy in dynamic, multi-agent environments. We propose a neuro-symbolic architecture to allow agents to evaluate the ethical consequences of their actions in uncertain settings. The proposed system addresses the limitations of existing ethical models by allowing agents to develop context-sensitive, adaptive, and relational ethical behaviour without presupposing anthropomorphic moral intuitions. A case study involving ethical resource distribution illustrates NAEL's dynamic balancing of self-preservation, epistemic learning, and collective welfare.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.285401"
    },
    {
        "index": "#20",
        "title": "Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review",
        "link": "/arxiv/2510.14669",
        "arxiv_id": "2510.14669",
        "authors": "Sara Altamirano, Arjan Vreeken, Sennay Ghebreab",
        "summary": "Machine learning (ML) promises to revolutionize public health through improved surveillance, risk stratification, and resource allocation. However, without systematic attention to algorithmic bias, ML may inadvertently reinforce existing health disparities. We present a systematic literature review of algorithmic bias identification, discussion, and reporting in Dutch public health ML research from 2021 to 2025. To this end, we developed the Risk of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals pervasive gaps: although data sampling and missing data practices are well documented, most studies omit explicit fairness framing, subgroup analyses, and transparent discussion of potential harms. In response, we introduce a four-stage fairness-oriented framework called ACAR (Awareness, Conceptualization, Application, Reporting), with guiding questions derived from our systematic literature review to help researchers address fairness across the ML lifecycle. We conclude with actionable recommendations for public health ML practitioners to consistently consider algorithmic bias and foster transparency, ensuring that algorithmic innovations advance health equity rather than undermine it.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.286004"
    },
    {
        "index": "#21",
        "title": "Beyond Hallucinations: The Illusion of Understanding in Large Language Models",
        "link": "/arxiv/2510.14665",
        "arxiv_id": "2510.14665",
        "authors": "Rikard Rosenbacke, Carl Rosenbacke, Victor Rosenbacke, Martin McKee",
        "summary": "Large language models (LLMs) are becoming deeply embedded in human communication and decision-making, yet they inherit the ambiguity, bias, and lack of direct access to truth inherent in language itself. While their outputs are fluent, emotionally resonant, and coherent, they are generated through statistical prediction rather than grounded reasoning. This creates the risk of hallucination, responses that sound convincing but lack factual validity. Building on Geoffrey Hinton's observation that AI mirrors human intuition rather than reasoning, this paper argues that LLMs operationalize System 1 cognition at scale: fast, associative, and persuasive, but without reflection or falsification. To address this, we introduce the Rose-Frame, a three-dimensional framework for diagnosing cognitive and epistemic drift in human-AI interaction. The three axes are: (i) Map vs. Territory, which distinguishes representations of reality (epistemology) from reality itself (ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to separate fast, emotional judgments from slow, reflective thinking; and (iii) Conflict vs. Confirmation, which examines whether ideas are critically tested through disagreement or simply reinforced through mutual validation. Each dimension captures a distinct failure mode, and their combination amplifies misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules. Instead, it offers a reflective tool that makes both the model's limitations and the user's assumptions visible, enabling more transparent and critically aware AI deployment. It reframes alignment as cognitive governance: intuition, whether human or artificial, must remain governed by human reason. Only by embedding reflective, falsifiable oversight can we align machine fluency with human understanding.",
        "subjects": "Artificial Intelligence, Human-Computer Interaction",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.286313"
    },
    {
        "index": "#23",
        "title": "LLM Agents Beyond Utility: An Open-Ended Perspective",
        "link": "/arxiv/2510.14548",
        "arxiv_id": "2510.14548",
        "authors": "Asen Nachkov, Xi Wang, Luc Van Gool",
        "summary": "Recent LLM agents have made great use of chain of thought reasoning and function calling. As their capabilities grow, an important question arises: can this software represent not only a smart problem-solving tool, but an entity in its own right, that can plan, design immediate tasks, and reason toward broader, more ambiguous goals? To study this question, we adopt an open-ended experimental setting where we augment a pretrained LLM agent with the ability to generate its own tasks, accumulate knowledge, and interact extensively with its environment. We study the resulting open-ended agent qualitatively. It can reliably follow complex multi-step instructions, store and reuse information across runs, and propose and solve its own tasks, though it remains sensitive to prompt design, prone to repetitive task generation, and unable to form self-representations. These findings illustrate both the promise and current limits of adapting pretrained LLMs toward open-endedness, and point to future directions for training agents to manage memory, explore productively, and pursue abstract long-term goals.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.286989"
    },
    {
        "index": "#25",
        "title": "JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol",
        "link": "/arxiv/2510.14537",
        "arxiv_id": "2510.14537",
        "authors": "Emanuele Antonioni, Stefan Markovic, Anirudha Shankar, Jaime Bernardo, Lovro Markovic, Silvia Pareti, Benedetto Proietti",
        "summary": "AI systems are continually evolving and advancing, and user expectations are concurrently increasing, with a growing demand for interactions that go beyond simple text-based interaction with Large Language Models (LLMs). Today's applications often require LLMs to interact with external tools, marking a shift toward more complex agentic systems. To support this, standards such as the Model Context Protocol (MCP) have emerged, enabling agents to access tools by including a specification of the capabilities of each tool within the prompt. Although this approach expands what agents can do, it also introduces a growing problem: prompt bloating. As the number of tools increases, the prompts become longer, leading to high prompt token costs, increased latency, and reduced task success resulting from the selection of tools irrelevant to the prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework designed to help agents manage prompt size more effectively when using large sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and uses the user's prompt to identify and include only the most relevant tools, based on both the query and the taxonomy structure. In this paper, we describe the design of the taxonomy, the tool selection algorithm, and the dataset used to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt size without significantly compromising the agent's ability to respond effectively. As the number of available tools for the agent grows substantially, JSPLIT even improves the tool selection accuracy of the agent, effectively reducing costs while simultaneously improving task success in high-complexity agent environments.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.287669"
    },
    {
        "index": "#26",
        "title": "Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration",
        "link": "/arxiv/2510.14512",
        "arxiv_id": "2510.14512",
        "authors": "Haoyuan Li, Mathias Funk, Aaqib Saeed",
        "summary": "Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.287967"
    },
    {
        "index": "#27",
        "title": "Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms",
        "link": "/arxiv/2510.14412",
        "arxiv_id": "2510.14412",
        "authors": "Claudia Grundke, Gabriele Röger",
        "summary": "Axioms are a feature of the Planning Domain Definition Language PDDL that can be considered as a generalization of database query languages such as Datalog. The PDDL standard restricts negative occurrences of predicates in axiom bodies to predicates that are directly set by actions and not derived by axioms. In the literature, authors often deviate from this limitation and only require that the set of axioms is stratifiable. Both variants can express exactly the same queries as least fixed-point logic, indicating that negative occurrences of derived predicates can be eliminated. We present the corresponding transformation.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.288248"
    },
    {
        "index": "#29",
        "title": "Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control",
        "link": "/arxiv/2510.14388",
        "arxiv_id": "2510.14388",
        "authors": "Zhe Wu, Hongjin Lu, Junliang Xing, Changhao Zhang, Yin Zhu, Yuhao Yang, Yuheng Jing, Kai Li, Kun Shao, Jianye Hao, Jun Wang, Yuanchun Shi",
        "summary": "Building agents that autonomously operate mobile devices has attracted increasing attention. While Vision-Language Models (VLMs) show promise, most existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks or unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical vision-language agent for mobile control, featuring a high-level reasoning model and a low-level action model that are jointly optimized. For efficient training, we reformulate multi-step decision-making as a sequence of single-step subgoals and propose a foresight advantage function, which leverages execution feedback from the low-level model to guide high-level optimization. This design alleviates the path explosion issue encountered by Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art (SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods across three paradigms: prompt-based (AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark. On the more challenging AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones, showing strong adaptability in high-complexity mobile control scenarios.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.288938"
    },
    {
        "index": "#30",
        "title": "Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?",
        "link": "/arxiv/2510.14387",
        "arxiv_id": "2510.14387",
        "authors": "Yijie Hu, Zihao Zhou, Kaizhu Huang, Xiaowei Huang, Qiufeng Wang",
        "summary": "Math reasoning has been one crucial ability of large language models (LLMs), where significant advancements have been achieved in recent years. However, most efforts focus on LLMs by curating high-quality annotation data and intricate training (or inference) paradigms, while the math reasoning performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM typically consists of an LLM and a vision block, we wonder: Can MLLMs directly absorb math reasoning abilities from off-the-shelf math LLMs without tuning? Recent model-merging approaches may offer insights into this question. However, they overlook the alignment between the MLLM and LLM, where we find that there is a large gap between their parameter spaces, resulting in lower performance. Our empirical evidence reveals two key factors behind this issue: the identification of crucial reasoning-associated layers in the model and the mitigation of the gaps in parameter space. Based on the empirical insights, we propose IP-Merging that first identifies the reasoning-associated parameters in both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to maintain the alignment, and finally merges parameters in this subspace. IP-Merging is a tuning-free approach since parameters are directly adjusted. Extensive experiments demonstrate that our IP-Merging method can enhance the math reasoning ability of MLLMs directly from Math LLMs without compromising their other capabilities.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.289248"
    },
    {
        "index": "#32",
        "title": "Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction",
        "link": "/arxiv/2510.14319",
        "arxiv_id": "2510.14319",
        "authors": "Xu Shen, Qi Zhang, Song Wang, Zhen Tan, Xinyu Zhao, Laura Yao, Vaishnav Tadiparthi, Hossein Nourkhiz Mahjoub, Ehsan Moradi Pari, Kwonjoon Lee, Tianlong Chen",
        "summary": "Large Language Model based multi-agent systems (MAS) excel at collaborative problem solving but remain brittle to cascading errors: a single faulty step can propagate across agents and disrupt the trajectory. In this paper, we present MASC, a metacognitive framework that endows MAS with real-time, unsupervised, step-level error detection and self-correction. MASC rethinks detection as history-conditioned anomaly scoring via two complementary designs: (1) Next-Execution Reconstruction, which predicts the embedding of the next step from the query and interaction history to capture causal consistency, and (2) Prototype-Guided Enhancement, which learns a prototype prior over normal-step embeddings and uses it to stabilize reconstruction and anomaly scoring under sparse context (e.g., early steps). When an anomaly step is flagged, MASC triggers a correction agent to revise the acting agent's output before information flows downstream. On the Who&When benchmark, MASC consistently outperforms all baselines, improving step-level error detection by up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers consistent end-to-end gains across architectures, confirming that our metacognitive monitoring and targeted correction can mitigate error propagation with minimal overhead.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.290050"
    },
    {
        "index": "#34",
        "title": "A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space",
        "link": "/arxiv/2510.14301",
        "arxiv_id": "2510.14301",
        "authors": "Bingjie Zhang, Yibo Yang, Renzhe, Dandan Guo, Jindong Gu, Philip Torr, Bernard Ghanem",
        "summary": "Large language models (LLMs) have achieved remarkable success in diverse tasks, yet their safety alignment remains fragile during adaptation. Even when fine-tuning on benign data or with low-rank adaptation, pre-trained safety behaviors are easily degraded, leading to harmful responses in the fine-tuned models. To address this challenge, we propose GuardSpace, a guardrail framework for preserving safety alignment throughout fine-tuning, composed of two key components: a safety-sensitive subspace and a harmful-resistant null space. First, we explicitly decompose pre-trained weights into safety-relevant and safety-irrelevant components using covariance-preconditioned singular value decomposition, and initialize low-rank adapters from the safety-irrelevant ones, while freezing safety-relevant components to preserve their associated safety mechanism. Second, we construct a null space projector that restricts adapter updates from altering safe outputs on harmful prompts, thereby maintaining the original refusal behavior. Experiments with various pre-trained models on multiple downstream tasks demonstrate that GuardSpace achieves superior performance over existing methods. Notably, for Llama-2-7B-Chat fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT, reducing the average harmful score from 14.4% to 3.6%, while improving the accuracy from from 26.0% to 28.0%.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.290729"
    },
    {
        "index": "#35",
        "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning",
        "link": "/arxiv/2510.14265",
        "arxiv_id": "2510.14265",
        "authors": "Xukai Wang, Xuanbo Liu, Mingrui Chen, Haitian Zhong, Xuanlin Yang, Bohan Zeng, Jinbo Hu, Hao Liang, Junbo Niu, Xuchen Li, Ruitao Wu, Ruichuan An, Yang Shi, Liu Liu, Xu-Yao Zhang, Qiang Liu, Zhouchen Lin, Wentao Zhang, Bin Dong",
        "summary": "With the advancement of powerful large-scale reasoning models, effectively evaluating the reasoning capabilities of these models has become increasingly important. However, existing benchmarks designed to assess the reasoning abilities of large models tend to be limited in scope and lack the flexibility to adapt their difficulty according to the evolving reasoning capacities of the models. To address this, we propose MorphoBench, a benchmark that incorporates multidisciplinary questions to evaluate the reasoning capabilities of large models and can adjust and update question difficulty based on the reasoning abilities of advanced models. Specifically, we curate the benchmark by selecting and collecting complex reasoning questions from existing benchmarks and sources such as Olympiad-level competitions. Additionally, MorphoBench adaptively modifies the analytical challenge of questions by leveraging key statements generated during the model's reasoning process. Furthermore, it includes questions generated using simulation software, enabling dynamic adjustment of benchmark difficulty with minimal resource consumption. We have gathered over 1,300 test questions and iteratively adjusted the difficulty of MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5. MorphoBench enhances the comprehensiveness and validity of model reasoning evaluation, providing reliable guidance for improving both the reasoning abilities and scientific robustness of large models. The code has been released in https://github.com/OpenDCAI/MorphoBench.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.291169"
    },
    {
        "index": "#36",
        "title": "Towards Agentic Self-Learning LLMs in Search Environment",
        "link": "/arxiv/2510.14253",
        "arxiv_id": "2510.14253",
        "authors": "Wangtao Sun, Xiang Cheng, Jialin Fan, Yao Xu, Xing Yu, Shizhu He, Jun Zhao, Kang Liu",
        "summary": "We study whether self-learning can scale LLM-based agents without relying on human-curated datasets or predefined rule-based rewards. Through controlled experiments in a search-agent setting, we identify two key determinants of scalable agent training: the source of reward signals and the scale of agent task data. We find that rewards from a Generative Reward Model (GRM) outperform rigid rule-based signals for open-domain learning, and that co-evolving the GRM with the policy further boosts performance. Increasing the volume of agent task data-even when synthetically generated-substantially enhances agentic capabilities. Building on these insights, we propose \\textbf{Agentic Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning framework that unifies task generation, policy execution, and evaluation within a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator, a Policy Model, and a Generative Reward Model to form a virtuous cycle of harder task setting, sharper verification, and stronger solving. Empirically, ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines (e.g., Search-R1) that plateau or degrade, and continues improving under zero-labeled-data conditions, indicating superior sample efficiency and robustness. We further show that GRM verification capacity is the main bottleneck: if frozen, it induces reward hacking and stalls progress; continual GRM training on the evolving data distribution mitigates this, and a small late-stage injection of real verification data raises the performance ceiling. This work establishes reward source and data scale as critical levers for open-domain agent learning and demonstrates the efficacy of multi-role co-evolution for scalable, self-improving agents. The data and code of this paper are released at https://github.com/forangel2014/Towards-Agentic-Self-Learning",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.291496"
    },
    {
        "index": "#37",
        "title": "LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild",
        "link": "/arxiv/2510.14240",
        "arxiv_id": "2510.14240",
        "authors": "Jiayu Wang, Yifei Ming, Riya Dulepet, Qinglin Chen, Austin Xu, Zixuan Ke, Frederic Sala, Aws Albarghouthi, Caiming Xiong, Shafiq Joty",
        "summary": "Deep research -- producing comprehensive, citation-grounded reports by searching and synthesizing information from hundreds of live web sources -- marks an important frontier for agentic systems. To rigorously evaluate this ability, four principles are essential: tasks should be (1) user-centric, reflecting realistic information needs, (2) dynamic, requiring up-to-date information beyond parametric knowledge, (3) unambiguous, ensuring consistent interpretation across users, and (4) multi-faceted and search-intensive, requiring search over numerous web sources and in-depth analysis. Existing benchmarks fall short of these principles, often focusing on narrow domains or posing ambiguous questions that hinder fair comparison. Guided by these principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated tasks spanning daily life, enterprise, and academia, each requiring extensive, dynamic, real-time web search and synthesis. Built with over 1,500 hours of human labor, LiveResearchBench provides a rigorous basis for systematic evaluation. To evaluate citation-grounded long-form reports, we introduce DeepEval, a comprehensive suite covering both content- and report-level quality, including coverage, presentation, citation accuracy and association, consistency and depth of analysis. DeepEval integrates four complementary evaluation protocols, each designed to ensure stable assessment and high agreement with human judgments. Using LiveResearchBench and DeepEval, we conduct a comprehensive evaluation of 17 frontier deep research systems, including single-agent web search, single-agent deep research, and multi-agent systems. Our analysis reveals current strengths, recurring failure modes, and key system components needed to advance reliable, insightful deep research.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.291871"
    },
    {
        "index": "#38",
        "title": "Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks",
        "link": "/arxiv/2510.14207",
        "arxiv_id": "2510.14207",
        "authors": "Trilok Padhi, Pinxian Lu, Abdulkadir Erol, Tanmay Sutar, Gauri Sharma, Mina Sonmez, Munmun De Choudhury, Ugur Kursuncu",
        "summary": "Large Language Model (LLM) agents are powering a growing share of interactive web applications, yet remain vulnerable to misuse and harm. Prior jailbreak research has largely focused on single-turn prompts, whereas real harassment often unfolds over multi-turn interactions. In this work, we present the Online Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim) simulation informed by repeated game theory, (iii) three jailbreak methods attacking agents across memory, planning, and fine-tuning, and (iv) a mixed-methods evaluation framework. We utilize two prominent LLMs, LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our results show that jailbreak tuning makes harassment nearly guaranteed with an attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama, and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with 84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs. 31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive categories such as sexual or racial harassment. Qualitative evaluation further reveals that attacked agents reproduce human-like aggression profiles, such as Machiavellian/psychopathic patterns under planning, and narcissistic tendencies with memory. Counterintuitively, closed-source and open-source models exhibit distinct escalation trajectories across turns, with closed-source models showing significant vulnerability. Overall, our findings show that multi-turn and theory-grounded attacks not only succeed at high rates but also mimic human-like harassment dynamics, motivating the development of robust safety guardrails to ultimately keep online platforms safe and responsible.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.292201"
    },
    {
        "index": "#39",
        "title": "Implementation of AI in Precision Medicine",
        "link": "/arxiv/2510.14194",
        "arxiv_id": "2510.14194",
        "authors": "Göktuğ Bender, Samer Faraj, Anand Bhardwaj",
        "summary": "Artificial intelligence (AI) has become increasingly central to precision medicine by enabling the integration and interpretation of multimodal data, yet implementation in clinical settings remains limited. This paper provides a scoping review of literature from 2019-2024 on the implementation of AI in precision medicine, identifying key barriers and enablers across data quality, clinical reliability, workflow integration, and governance. Through an ecosystem-based framework, we highlight the interdependent relationships shaping real-world translation and propose future directions to support trustworthy and sustainable implementation.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.292482"
    },
    {
        "index": "#41",
        "title": "JEDA: Query-Free Clinical Order Search from Ambient Dialogues",
        "link": "/arxiv/2510.14169",
        "arxiv_id": "2510.14169",
        "authors": "Praphul Singh, Corey Barrett, Sumana Srivasta, Amitabh Saikia, Irfan Bulu, Sri Gadde, Krishnaram Kenthapadi",
        "summary": "Clinical conversations mix explicit directives (order a chest X-ray) with implicit reasoning (the cough worsened overnight, we should check for pneumonia). Many systems rely on LLM rewriting, adding latency, instability, and opacity that hinder real-time ordering. We present JEDA (Joint Embedding for Direct and Ambient clinical orders), a domain-initialized bi-encoder that retrieves canonical orders directly and, in a query-free mode, encodes a short rolling window of ambient dialogue to trigger retrieval. Initialized from PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA aligns heterogeneous expressions of intent to shared order concepts. Training uses constrained LLM guidance to tie each signed order to complementary formulations (command only, context only, command+context, context+reasoning), producing clearer inter-order separation, tighter query extendash order coupling, and stronger generalization. The query-free mode is noise-resilient, reducing sensitivity to disfluencies and ASR errors by conditioning on a short window rather than a single utterance. Deployed in practice, JEDA yields large gains and substantially outperforms its base encoder and recent open embedders (Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The result is a fast, interpretable, LLM-free retrieval layer that links ambient context to actionable clinical orders in real time.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.293113"
    },
    {
        "index": "#44",
        "title": "A Multimodal Approach to Heritage Preservation in the Context of Climate Change",
        "link": "/arxiv/2510.14136",
        "arxiv_id": "2510.14136",
        "authors": "David Roqui, Adèle Cormier, nistor Grozavu, Ann Bourges",
        "summary": "Cultural heritage sites face accelerating degradation due to climate change, yet tradi- tional monitoring relies on unimodal analysis (visual inspection or environmental sen- sors alone) that fails to capture the complex interplay between environmental stres- sors and material deterioration. We propose a lightweight multimodal architecture that fuses sensor data (temperature, humidity) with visual imagery to predict degradation severity at heritage sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified encoders (64D latent space) that prevent overfitting on small datasets (n=37 training samples), and (2) Adaptive Barlow Twins loss that encourages modality complementarity rather than redundancy. On data from Strasbourg Cathedral, our model achieves 76.9% accu- racy, a 43% improvement over standard multimodal architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO. Ablation studies reveal that sensor-only achieves 61.5% while image-only reaches 46.2%, confirming successful multimodal synergy. A systematic hyperparameter study identifies an optimal moderate correlation target ({\\tau} =0.3) that balances align- ment and complementarity, achieving 69.2% accuracy compared to other {\\tau} values ({\\tau} =0.1/0.5/0.7: 53.8%, {\\tau} =0.9: 61.5%). This work demonstrates that architectural sim- plicity combined with contrastive regularization enables effective multimodal learning in data-scarce heritage monitoring contexts, providing a foundation for AI-driven con- servation decision support systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.294029"
    },
    {
        "index": "#46",
        "title": "STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management",
        "link": "/arxiv/2510.14112",
        "arxiv_id": "2510.14112",
        "authors": "Huiliang Zhang, Di Wu, Arnaud Zinflou, Benoit Boulet",
        "summary": "Building energy management is essential for achieving carbon reduction goals, improving occupant comfort, and reducing energy costs. Coordinated building energy management faces critical challenges in exploiting spatial-temporal dependencies while ensuring operational safety across multi-building systems. Current multi-building energy systems face three key challenges: insufficient spatial-temporal information exploitation, lack of rigorous safety guarantees, and system complexity. This paper proposes Spatial-Temporal Enhanced Safe Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent reinforcement learning framework for coordinated building energy management. STEMS integrates two core components: (1) a spatial-temporal graph representation learning framework using a GCN-Transformer fusion architecture to capture inter-building relationships and temporal patterns, and (2) a safety-constrained multi-agent RL algorithm incorporating Control Barrier Functions to provide mathematical safety guarantees. Extensive experiments on real-world building datasets demonstrate STEMS's superior performance over existing methods, showing that STEMS achieves 21% cost reduction, 18% emission reduction, and dramatically reduces safety violations from 35.1% to 5.6% while maintaining optimal comfort with only 0.13 discomfort proportion. The framework also demonstrates strong robustness during extreme weather conditions and maintains effectiveness across different building types.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.294622"
    },
    {
        "index": "#48",
        "title": "Position: Require Frontier AI Labs To Release Small \"Analog\" Models",
        "link": "/arxiv/2510.14053",
        "arxiv_id": "2510.14053",
        "authors": "Shriyash Upadhyay, Chaithanya Bandi, Narmeen Oozeer, Philip Quirke",
        "summary": "Recent proposals for regulating frontier AI models have sparked concerns about the cost of safety regulation, and most such regulations have been shelved due to the safety-innovation tradeoff. This paper argues for an alternative regulatory approach that ensures AI safety while actively promoting innovation: mandating that large AI laboratories release small, openly accessible analog models (scaled-down versions) trained similarly to and distilled from their largest proprietary models. Analog models serve as public proxies, allowing broad participation in safety verification, interpretability research, and algorithmic transparency without forcing labs to disclose their full-scale models. Recent research demonstrates that safety and interpretability methods developed using these smaller models generalize effectively to frontier-scale systems. By enabling the wider research community to directly investigate and innovate upon accessible analogs, our policy substantially reduces the regulatory burden and accelerates safety advancements. This mandate promises minimal additional costs, leveraging reusable resources like data and infrastructure, while significantly contributing to the public good. Our hope is not only that this policy be adopted, but that it illustrates a broader principle supporting fundamental research in machine learning: deeper understanding of models relaxes the safety-innovation tradeoff and lets us have more of both.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.295232"
    },
    {
        "index": "#49",
        "title": "GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations",
        "link": "/arxiv/2510.14035",
        "arxiv_id": "2510.14035",
        "authors": "Rajesh Mangannavar, Prasad Tadepalli",
        "summary": "We introduce an action-centric graph representation framework for learning to guide planning in Partially Observable Markov Decision Processes (POMDPs). Unlike existing approaches that require domain-specific neural architectures and struggle with scalability, GammaZero leverages a unified graph-based belief representation that enables generalization across problem sizes within a domain. Our key insight is that belief states can be systematically transformed into action-centric graphs where structural patterns learned on small problems transfer to larger instances. We employ a graph neural network with a decoder architecture to learn value functions and policies from expert demonstrations on computationally tractable problems, then apply these learned heuristics to guide Monte Carlo tree search on larger problems. Experimental results on standard POMDP benchmarks demonstrate that GammaZero achieves comparable performance to BetaZero when trained and tested on the same-sized problems, while uniquely enabling zero-shot generalization to problems 2-4 times larger than those seen during training, maintaining solution quality with reduced search requirements.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.295499"
    },
    {
        "index": "#50",
        "title": "Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment",
        "link": "/arxiv/2510.13985",
        "arxiv_id": "2510.13985",
        "authors": "María Victoria Carro, Denise Alejandra Mester, Francisca Gauna Selasco, Giovanni Franco Gabriel Marraffini, Mario Alejandro Leiva, Gerardo I. Simari, María Vanina Martinez",
        "summary": "Causal learning is the cognitive process of developing the capability of making causal inferences based on available information, often guided by normative principles. This process is prone to errors and biases, such as the illusion of causality, in which people perceive a causal relationship between two variables despite lacking supporting evidence. This cognitive bias has been proposed to underlie many societal problems, including social prejudice, stereotype formation, misinformation, and superstitious thinking. In this work, we examine whether large language models are prone to developing causal illusions when faced with a classic cognitive science paradigm: the contingency judgment task. To investigate this, we constructed a dataset of 1,000 null contingency scenarios (in which the available information is not sufficient to establish a causal relationship between variables) within medical contexts and prompted LLMs to evaluate the effectiveness of potential causes. Our findings show that all evaluated models systematically inferred unwarranted causal relationships, revealing a strong susceptibility to the illusion of causality. While there is ongoing debate about whether LLMs genuinely understand causality or merely reproduce causal language without true comprehension, our findings support the latter hypothesis and raise concerns about the use of language models in domains where accurate causal reasoning is essential for informed decision-making.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.295838"
    },
    {
        "index": "#52",
        "title": "Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context",
        "link": "/arxiv/2510.13858",
        "arxiv_id": "2510.13858",
        "authors": "Raheleh Biglari, Joachim Denil",
        "summary": "Model validity is as critical as the model itself, especially when guiding decision-making processes. Traditional approaches often rely on predefined validity frames, which may not always be available or sufficient. This paper introduces the Decision Oriented Technique (DOTechnique), a novel method for determining model validity based on decision consistency rather than output similarity. By evaluating whether surrogate models lead to equivalent decisions compared to high-fidelity models, DOTechnique enables efficient identification of validity regions, even in the absence of explicit validity boundaries. The approach integrates domain constraints and symbolic reasoning to narrow the search space, enhancing computational efficiency. A highway lane change system serves as a motivating example, demonstrating how DOTechnique can uncover the validity region of a simulation model. The results highlight the potential of the technique to support finding model validity through decision-maker context.",
        "subjects": "Artificial Intelligence",
        "date": "2025-10-12",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.296398"
    },
    {
        "index": "#53",
        "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
        "link": "/arxiv/2510.14981",
        "arxiv_id": "2510.14981",
        "authors": "Hadi Alzayer, Yunzhi Zhang, Chen Geng, Jia-Bin Huang, Jiajun Wu",
        "summary": "We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution. This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.296714"
    },
    {
        "index": "#54",
        "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale",
        "link": "/arxiv/2510.14979",
        "arxiv_id": "2510.14979",
        "authors": "Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu",
        "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.297080"
    },
    {
        "index": "#56",
        "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
        "link": "/arxiv/2510.14975",
        "arxiv_id": "2510.14975",
        "authors": "Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, Yu-Gang Jiang",
        "summary": "Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.297818"
    },
    {
        "index": "#63",
        "title": "C4D: 4D Made from 3D through Dual Correspondences",
        "link": "/arxiv/2510.14960",
        "arxiv_id": "2510.14960",
        "authors": "Shizun Wang, Zhenxiang Jiang, Xingyi Yang, Xinchao Wang",
        "summary": "Recovering 4D from monocular video, which jointly estimates dynamic geometry and camera poses, is an inevitably challenging problem. While recent pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great progress in reconstructing static scenes, directly applying them to dynamic scenes leads to inaccurate results. This discrepancy arises because moving objects violate multi-view geometric constraints, disrupting the reconstruction. To address this, we introduce C4D, a framework that leverages temporal Correspondences to extend existing 3D reconstruction formulation to 4D. Specifically, apart from predicting pointmaps, C4D captures two types of correspondences: short-term optical flow and long-term point tracking. We train a dynamic-aware point tracker that provides additional mobility information, facilitating the estimation of motion masks to separate moving elements from the static background, thus offering more reliable guidance for dynamic scenes. Furthermore, we introduce a set of dynamic scene optimization objectives to recover per-frame 3D geometry and camera parameters. Simultaneously, the correspondences lift 2D trajectories into smooth 3D trajectories, enabling fully integrated 4D reconstruction. Experiments show that our framework achieves complete 4D recovery and demonstrates strong performance across multiple downstream tasks, including depth estimation, camera pose estimation, and point tracking. Project Page: https://littlepure2333.github.io/C4D",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.300253"
    },
    {
        "index": "#65",
        "title": "RealDPO: Real or Not Real, that is the Preference",
        "link": "/arxiv/2510.14955",
        "arxiv_id": "2510.14955",
        "authors": "Guo Cheng, Danni Yang, Ziqi Huang, Jianlou Si, Chenyang Si, Ziwei Liu",
        "summary": "Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.300913"
    },
    {
        "index": "#78",
        "title": "Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks",
        "link": "/arxiv/2510.14803",
        "arxiv_id": "2510.14803",
        "authors": "Pedro R. A. S. Bassi, Xinze Zhou, Wenxuan Li, Szymon Płotka, Jieneng Chen, Qi Chen, Zheren Zhu, Jakub Prządo, Ibrahim E. Hamacı, Sezgin Er, Yuhan Wang, Ashwin Kumar, Bjoern Menze, Jarosław B. Ćwikła, Yuyin Zhou, Akshay S. Chaudhari, Curtis P. Langlotz, Sergio Decherchi, Andrea Cavalli, Kang Wang, Yang Yang, Alan L. Yuille, Zongwei Zhou",
        "summary": "Early tumor detection save lives. Each year, more than 300 million computed tomography (CT) scans are performed worldwide, offering a vast opportunity for effective cancer screening. However, detecting small or early-stage tumors on these CT scans remains challenging, even for experts. Artificial intelligence (AI) models can assist by highlighting suspicious regions, but training such models typically requires extensive tumor masks--detailed, voxel-wise outlines of tumors manually drawn by radiologists. Drawing these masks is costly, requiring years of effort and millions of dollars. In contrast, nearly every CT scan in clinical practice is already accompanied by medical reports describing the tumor's size, number, appearance, and sometimes, pathology results--information that is rich, abundant, and often underutilized for AI training. We introduce R-Super, which trains AI to segment tumors that match their descriptions in medical reports. This approach scales AI training with large collections of readily available medical reports, substantially reducing the need for manually drawn tumor masks. When trained on 101,654 reports, AI models achieved performance comparable to those trained on 723 masks. Combining reports and masks further improved sensitivity by +13% and specificity by +8%, surpassing radiologists in detecting five of the seven tumor types. Notably, R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate, bladder, uterus, and esophagus, for which no public masks or AI models previously existed. This study challenges the long-held belief that large-scale, labor-intensive tumor mask creation is indispensable, establishing a scalable and accessible path toward early detection across diverse tumor types. We plan to release our trained models, code, and dataset at https://github.com/MrGiovanni/R-Super",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.305471"
    },
    {
        "index": "#79",
        "title": "Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images",
        "link": "/arxiv/2510.14800",
        "arxiv_id": "2510.14800",
        "authors": "Usama Sajjad, Abdul Rehman Akbar, Ziyu Su, Deborah Knight, Wendy L. Frankel, Metin N. Gurcan, Wei Chen, Muhammad Khalid Khan Niazi",
        "summary": "Colorectal cancer (CRC) remains the third most prevalent malignancy globally, with approximately 154,000 new cases and 54,000 projected deaths anticipated for 2025. The recent advancement of foundation models in computational pathology has been largely propelled by task agnostic methodologies that can overlook organ-specific crucial morphological patterns that represent distinct biological processes that can fundamentally influence tumor behavior, therapeutic response, and patient outcomes. The aim of this study is to develop a novel, interpretable AI model, PRISM (Prognostic Representation of Integrated Spatial Morphology), that incorporates a continuous variability spectrum within each distinct morphology to characterize phenotypic diversity and reflecting the principle that malignant transformation occurs through incremental evolutionary processes rather than abrupt phenotypic shifts. PRISM is trained on 8.74 million histological images extracted from surgical resection specimens of 424 patients with stage III CRC. PRISM achieved superior prognostic performance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%; HR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific methods by 15% and AI foundation models by ~23% accuracy. It showed sex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable performance across clinicopathological subgroups, with minimal accuracy fluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens, replicating the Alliance cohort finding of no survival difference between treatments.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.305825"
    },
    {
        "index": "#80",
        "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
        "link": "/arxiv/2510.14788",
        "arxiv_id": "2510.14788",
        "authors": "Manjie Xu, Cheng Chen, Xin Jia, Jingyi Zhou, Yongji Wu, Zejian Wang, Chi Zhang, Kai Zuo, Yibo Chen, Xu Tang, Yao Hu, Yixin Zhu",
        "summary": "User interests on content platforms are inherently diverse, manifesting through complex behavioral patterns across heterogeneous scenarios such as search, feed browsing, and content discovery. Traditional recommendation systems typically prioritize business metric optimization within isolated specific scenarios, neglecting cross-scenario behavioral signals and struggling to integrate advanced techniques like LLMs at billion-scale deployments, which finally limits their ability to capture holistic user interests across platform touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender Engine for Diversified scenarios, tailored for industry-level content recommendation systems. RED-Rec unifies user interest representations across multiple behavioral contexts by aggregating and synthesizing actions from varied scenarios, resulting in comprehensive item and user modeling. At its core, a two-tower LLM-powered framework enables nuanced, multifaceted representations with deployment efficiency, and a scenario-aware dense mixing and querying policy effectively fuses diverse behavioral signals to capture cross-scenario user intent patterns and express fine-grained, context-specific intents during serving. We validate RED-Rec through online A/B testing on hundreds of millions of users in RedNote through online A/B testing, showing substantial performance gains in both content recommendation and advertisement targeting tasks. We further introduce a million-scale sequential recommendation dataset, RED-MMU, for comprehensive offline training and evaluation. Our work advances unified user modeling, unlocking deeper personalization and fostering more meaningful user engagement in large-scale UGC platforms.",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.306215"
    },
    {
        "index": "#82",
        "title": "Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality",
        "link": "/arxiv/2510.14765",
        "arxiv_id": "2510.14765",
        "authors": "Giuseppe Lorenzo Catalano, Agata Marta Soccini",
        "summary": "Space exploration increasingly relies on Virtual Reality for several tasks, such as mission planning, multidisciplinary scientific analysis, and astronaut training. A key factor for the reliability of the simulations is having accurate 3D representations of planetary terrains. Extraterrestrial heightmaps derived from satellite imagery often contain missing values due to acquisition and transmission constraints. Mars is among the most studied planets beyond Earth, and its extensive terrain datasets make the Martian surface reconstruction a valuable task, although many areas remain unmapped. Deep learning algorithms can support void-filling tasks; however, whereas Earth's comprehensive datasets enables the use of conditional methods, such approaches cannot be applied to Mars. Current approaches rely on simpler interpolation techniques which, however, often fail to preserve geometric coherence. In this work, we propose a method for reconstructing the surface of Mars based on an unconditional diffusion model. Training was conducted on an augmented dataset of 12000 Martian heightmaps derived from NASA's HiRISE survey. A non-homogeneous rescaling strategy captures terrain features across multiple scales before resizing to a fixed 128x128 model resolution. We compared our method against established void-filling and inpainting techniques, including Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an evaluation set of 1000 samples. Results show that our approach consistently outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE) and perceptual similarity (29-81% on LPIPS) with the original data.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Graphics",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.306865"
    },
    {
        "index": "#85",
        "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models",
        "link": "/arxiv/2510.14741",
        "arxiv_id": "2510.14741",
        "authors": "Simone Carnemolla, Matteo Pennisi, Sarinda Samarasinghe, Giovanni Bellitto, Simone Palazzo, Daniela Giordano, Mubarak Shah, Concetto Spampinato",
        "summary": "Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks-activation maximization, slice discovery and debiasing, and bias explanation-each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at https://github.com/perceivelab/dexter.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.308030"
    },
    {
        "index": "#87",
        "title": "Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models",
        "link": "/arxiv/2510.14713",
        "arxiv_id": "2510.14713",
        "authors": "Tingyu Lin, Armin Dadras, Florian Kleber, Robert Sablatnig",
        "summary": "Camera movement conveys spatial and narrative information essential for understanding video content. While recent camera movement classification (CMC) methods perform well on modern datasets, their generalization to historical footage remains unexplored. This paper presents the first systematic evaluation of deep video CMC models on archival film material. We summarize representative methods and datasets, highlighting differences in model design and label definitions. Five standard video classification models are assessed on the HISTORIAN dataset, which includes expert-annotated World War II footage. The best-performing model, Video Swin Transformer, achieves 80.25% accuracy, showing strong convergence despite limited training data. Our findings highlight the challenges and potential of adapting existing models to low-quality video and motivate future work combining diverse input modalities and temporal architectures.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Image and Video Processing",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.308739"
    },
    {
        "index": "#88",
        "title": "Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery",
        "link": "/arxiv/2510.14709",
        "arxiv_id": "2510.14709",
        "authors": "Caleb Robinson, Kimberly T. Goetz, Christin B. Khan, Meredith Sackett, Kathleen Leonard, Rahul Dodhia, Juan M. Lavista Ferres",
        "summary": "Effective monitoring of whale populations is critical for conservation, but traditional survey methods are expensive and difficult to scale. While prior work has shown that whales can be identified in very high-resolution (VHR) satellite imagery, large-scale automated detection remains challenging due to a lack of annotated imagery, variability in image quality and environmental conditions, and the cost of building robust machine learning pipelines over massive remote sensing archives. We present a semi-automated approach for surfacing possible whale detections in VHR imagery using a statistical anomaly detection method that flags spatial outliers, i.e. \"interesting points\". We pair this detector with a web-based labeling interface designed to enable experts to quickly annotate the interesting points. We evaluate our system on three benchmark scenes with known whale annotations and achieve recalls of 90.3% to 96.4%, while reducing the area requiring expert inspection by up to 99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method does not rely on labeled training data and offers a scalable first step toward future machine-assisted marine mammal monitoring from space. We have open sourced this pipeline at https://github.com/microsoft/whales.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.309090"
    },
    {
        "index": "#90",
        "title": "xLLM Technical Report",
        "link": "/arxiv/2510.14686",
        "arxiv_id": "2510.14686",
        "authors": "Tongxuan Liu, Tao Peng, Peijun Yang, Xiaoyang Zhao, Xiusheng Lu, Weizhe Huang, Zirui Liu, Xiaoyu Chen, Zhiwei Liang, Jun Xiong, Donghe Jin, Minchao Zhang, Jinrong Guo, Yingxu Deng, Xu Zhang, Xianzhe Dong, Siqi Wang, Siyu Wu, Yu Wu, Zihan Tang, Yuting Zeng, Yanshu Wang, Jinguang Liu, Meng Kang, Menxin Li, Yunlong Wang, Yiming Liu, Xiaolong Ma, Yifan Wang, Yichen Zhang, Jinrun Yin, Keyang Zheng, Jiawei Yin, Jun Zhang, Ziyue Wang, Xiaobo Lin, Liangyu Liu, Liwei Lan, Yang Liu, Chunhua Peng, Han Liu, Songcheng Ren, Xuezhu Wang, Yunheng Shen, Yi Wang, Guyue Liu, Hui Chen, Tong Yang, Hailong Yang, Jing Li, Guiguang Ding, Ke Zhang",
        "summary": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM) inference framework designed for high-performance, large-scale enterprise-grade serving, with deep optimizations for diverse AI accelerators. To address these challenges, xLLM builds a novel decoupled service-engine architecture. At the service layer, xLLM-Service features an intelligent scheduling module that efficiently processes multimodal requests and co-locates online and offline tasks through unified elastic scheduling to maximize cluster utilization. This module also relies on a workload-adaptive dynamic Prefill-Decode (PD) disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation policy designed for multimodal inputs. Furthermore, it incorporates a distributed architecture to provide global KV Cache management and robust fault-tolerant capabilities for high availability. At the engine layer, xLLM-Engine co-optimizes system and algorithm designs to fully saturate computing resources. This is achieved through comprehensive multi-layer execution pipeline optimizations, an adaptive graph mode and an xTensor memory management. xLLM-Engine also further integrates algorithmic enhancements such as optimized speculative decoding and dynamic EPLB, collectively serving to substantially boost throughput and inference efficiency. Extensive evaluations demonstrate that xLLM delivers significantly superior performance and resource efficiency. Under identical TPOT constraints, xLLM achieves throughput up to 1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while maintaining an average throughput of 1.7x that of MindIE with Deepseek-series models. xLLM framework is publicly available at https://github.com/jd-opensource/xllm and https://github.com/jd-opensource/xllm-service.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.310173"
    },
    {
        "index": "#94",
        "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
        "link": "/arxiv/2510.14648",
        "arxiv_id": "2510.14648",
        "authors": "Xinyao Liao, Xianfang Zeng, Ziye Song, Zhoujie Fu, Gang Yu, Guosheng Lin",
        "summary": "Despite the rapid progress of instruction-based image editing, its extension to video remains underexplored, primarily due to the prohibitive cost and complexity of constructing large-scale paired video editing datasets. To address this challenge, we introduce a low-cost pretraining strategy for instruction-based video editing that leverages in-context learning from unpaired video clips. We show that pretraining a foundation video generation model with this strategy endows it with general editing capabilities, such as adding, replacing, or deleting operations, according to input editing instructions. The pretrained model can then be efficiently refined with a small amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our framework first pretrains on approximately 1M real video clips to learn basic editing concepts, and subsequently fine-tunes on fewer than 150k curated editing pairs to extend more editing tasks and improve the editing quality. Comparative experiments show that our method surpasses existing instruction-based video editing approaches in both instruction alignment and visual fidelity, achieving a 12\\% improvement in editing instruction following and a 15\\% improvement in editing quality.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.311502"
    },
    {
        "index": "#95",
        "title": "The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon Blockchain",
        "link": "/arxiv/2510.14642",
        "arxiv_id": "2510.14642",
        "authors": "Andrei Seoev, Leonid Gremyachikh, Anastasiia Smirnova, Yash Madhwal, Alisa Kalacheva, Dmitry Belousov, Ilia Zubov, Aleksei Smirnov, Denis Fedyanin, Vladimir Gorgadze, Yury Yanovich",
        "summary": "In blockchain networks, the strategic ordering of transactions within blocks has emerged as a significant source of profit extraction, known as Maximal Extractable Value (MEV). The transition from spam-based Priority Gas Auctions to structured auction mechanisms like Polygon Atlas has transformed MEV extraction from public bidding wars into sealed-bid competitions under extreme time constraints. While this shift reduces network congestion, it introduces complex strategic challenges where searchers must make optimal bidding decisions within a sub-second window without knowledge of competitor behavior or presence. Traditional game-theoretic approaches struggle in this high-frequency, partially observable environment due to their reliance on complete information and static equilibrium assumptions. We present a reinforcement learning framework for MEV extraction on Polygon Atlas and make three contributions: (1) A novel simulation environment that accurately models the stochastic arrival of arbitrage opportunities and probabilistic competition in Atlas auctions; (2) A PPO-based bidding agent optimized for real-time constraints, capable of adaptive strategy formulation in continuous action spaces while maintaining production-ready inference speeds; (3) Empirical validation demonstrating our history-conditioned agent captures 49\\% of available profits when deployed alongside existing searchers and 81\\% when replacing the market leader, significantly outperforming static bidding strategies. Our work establishes that reinforcement learning provides a critical advantage in high-frequency MEV environments where traditional optimization methods fail, offering immediate value for industrial participants and protocol designers alike.",
        "subjects": "Computer Science and Game Theory, Artificial Intelligence, Distributed, Parallel, and Cluster Computing",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.311915"
    },
    {
        "index": "#96",
        "title": "Causality Enhancement for Cross-Domain Recommendation",
        "link": "/arxiv/2510.14641",
        "arxiv_id": "2510.14641",
        "authors": "Zhibo Wu, Yunfan Wu, Lin Jiang, Ping Yang, Yao Hu",
        "summary": "Cross-domain recommendation forms a crucial component in recommendation systems. It leverages auxiliary information through source domain tasks or features to enhance target domain recommendations. However, incorporating inconsistent source domain tasks may result in insufficient cross-domain modeling or negative transfer. While incorporating source domain features without considering the underlying causal relationships may limit their contribution to final predictions. Thus, a natural idea is to directly train a cross-domain representation on a causality-labeled dataset from the source to target domain. Yet this direction has been rarely explored, as identifying unbiased real causal labels is highly challenging in real-world scenarios. In this work, we attempt to take a first step in this direction by proposing a causality-enhanced framework, named CE-CDR. Specifically, we first reformulate the cross-domain recommendation as a causal graph for principled guidance. We then construct a causality-aware dataset heuristically. Subsequently, we derive a theoretically unbiased Partial Label Causal Loss to generalize beyond the biased causality-aware dataset to unseen cross-domain patterns, yielding an enriched cross-domain representation, which is then fed into the target model to enhance target-domain recommendations. Theoretical and empirical analyses, as well as extensive experiments, demonstrate the rationality and effectiveness of CE-CDR and its general applicability as a model-agnostic plugin. Moreover, it has been deployed in production since April 2025, showing its practical value in real-world applications.",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.312248"
    },
    {
        "index": "#98",
        "title": "GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation",
        "link": "/arxiv/2510.14626",
        "arxiv_id": "2510.14626",
        "authors": "Zhibo Wu, Yunfan Wu, Quan Liu, Lin Jiang, Ping Yang, Yao Hu",
        "summary": "Multi-interest recommendation has gained attention, especially in industrial retrieval stage. Unlike classical dual-tower methods, it generates multiple user representations instead of a single one to model comprehensive user interests. However, prior studies have identified two underlying limitations: the first is interest collapse, where multiple representations homogenize. The second is insufficient modeling of interest evolution, as they struggle to capture latent interests absent from a user's historical behavior. We begin with a thorough review of existing works in tackling these limitations. Then, we attempt to tackle these limitations from a new perspective. Specifically, we propose a framework-level refinement for multi-interest recommendation, named GemiRec. The proposed framework leverages interest quantization to enforce a structural interest separation and interest generation to learn the evolving dynamics of user interests explicitly. It comprises three modules: (a) Interest Dictionary Maintenance Module (IDMM) maintains a shared quantized interest dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a generative model to capture the distribution of user future interests. (c) Multi-Interest Retrieval Module (MIRM) retrieves items using multiple user-interest representations. Both theoretical and empirical analyses, as well as extensive experiments, demonstrate its advantages and effectiveness. Moreover, it has been deployed in production since March 2025, showing its practical value in industrial applications.",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.312923"
    },
    {
        "index": "#102",
        "title": "An Active Inference Model of Mouse Point-and-Click Behaviour",
        "link": "/arxiv/2510.14611",
        "arxiv_id": "2510.14611",
        "authors": "Markus Klar, Sebastian Stein, Fraser Paterson, John H. Williamson, Roderick Murray-Smith",
        "summary": "We explore the use of Active Inference (AIF) as a computational user model for spatial pointing, a key problem in Human-Computer Interaction (HCI). We present an AIF agent with continuous state, action, and observation spaces, performing one-dimensional mouse pointing and clicking. We use a simple underlying dynamic system to model the mouse cursor dynamics with realistic perceptual delay. In contrast to previous optimal feedback control-based models, the agent's actions are selected by minimizing Expected Free Energy, solely based on preference distributions over percepts, such as observing clicking a button correctly. Our results show that the agent creates plausible pointing movements and clicks when the cursor is over the target, with similar end-point variance to human users. In contrast to other models of pointing, we incorporate fully probabilistic, predictive delay compensation into the agent. The agent shows distinct behaviour for differing target difficulties without the need to retune system parameters, as done in other approaches. We discuss the simulation results and emphasize the challenges in identifying the correct configuration of an AIF agent interacting with continuous systems.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.314448"
    },
    {
        "index": "#103",
        "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering",
        "link": "/arxiv/2510.14605",
        "arxiv_id": "2510.14605",
        "authors": "Yuyang Hong, Jiaqi Gu, Qi Yang, Lubin Fan, Yue Wu, Ying Wang, Kun Ding, Shiming Xiang, Jieping Ye",
        "summary": "Knowledge-based visual question answering (KB-VQA) requires visual language models (VLMs) to integrate visual understanding with external knowledge retrieval. Although retrieval-augmented generation (RAG) achieves significant advances in this task by combining knowledge-base querying, it still struggles with the quality of multimodal queries and the relevance of retrieved results. To overcome these challenges, we propose a novel three-stage method, termed Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing stage dynamically invokes visual tools to extract precise multimodal information for retrieval. The retrieval stage integrates visual and text features to achieve multimodal knowledge retrieval. The filtering stage performs relevance filtering and concentration on retrieval results. To this end, we introduce a visual language model trained with answer accuracy and format consistency as reward signals via a reinforcement learning manner. This enhances the model's reasoning, tool invocation for accurate queries, and filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality, achieving state-of-the-art performance. Code is available at https://github.com/cqu-student/Wiki-PRF",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.314809"
    },
    {
        "index": "#105",
        "title": "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding",
        "link": "/arxiv/2510.14588",
        "arxiv_id": "2510.14588",
        "authors": "Zhifei Chen, Tianshuo Xu, Leyi Wu, Luozhou Wang, Dongyu Yan, Zihan You, Wenting Luo, Guo Zhang, Yingcong Chen",
        "summary": "Video generation has recently made striking visual progress, but maintaining coherent object motion and interactions remains difficult. We trace two practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps) often collapse to too few effective tokens after encoding, weakening guidance; and (ii) optimizing for appearance and motion in a single head can favor texture over temporal consistency. We present STANCE, an image-to-video framework that addresses both issues with two simple components. First, we introduce Instance Cues -- a pixel-aligned control signal that turns sparse, user-editable hints into a dense 2.5D (camera-relative) motion field by averaging per-instance flow and augmenting with monocular depth over the instance mask. This reduces depth ambiguity compared to 2D arrow inputs while remaining easy to use. Second, we preserve the salience of these cues in token space with Dense RoPE, which tags a small set of motion tokens (anchored on the first frame) with spatial-addressable rotary embeddings. Paired with joint RGB \\(+\\) auxiliary-map prediction (segmentation or depth), our model anchors structure while RGB handles appearance, stabilizing optimization and improving temporal coherence without requiring per-frame trajectory scripts.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.315526"
    },
    {
        "index": "#109",
        "title": "Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing",
        "link": "/arxiv/2510.14525",
        "arxiv_id": "2510.14525",
        "authors": "Qurrat Ul Ain, Atif Aftab Ahmed Jilani, Zunaira Shafqat, Nigar Azhar Butt",
        "summary": "Defective surgical instruments pose serious risks to sterility, mechanical integrity, and patient safety, increasing the likelihood of surgical complications. However, quality control in surgical instrument manufacturing often relies on manual inspection, which is prone to human error and inconsistency. This study introduces SurgScan, an AI-powered defect detection framework for surgical instruments. Using YOLOv8, SurgScan classifies defects in real-time, ensuring high accuracy and industrial scalability. The model is trained on a high-resolution dataset of 102,876 images, covering 11 instrument types and five major defect categories. Extensive evaluation against state-of-the-art CNN architectures confirms that SurgScan achieves the highest accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image, making it suitable for industrial deployment. Statistical analysis demonstrates that contrast-enhanced preprocessing significantly improves defect detection, addressing key limitations in visual inspection. SurgScan provides a scalable, cost-effective AI solution for automated quality control, reducing reliance on manual inspection while ensuring compliance with ISO 13485 and FDA standards, paving the way for enhanced defect detection in medical manufacturing.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.316922"
    },
    {
        "index": "#113",
        "title": "Semantic representations emerge in biologically inspired ensembles of cross-supervising neural networks",
        "link": "/arxiv/2510.14486",
        "arxiv_id": "2510.14486",
        "authors": "Roy Urbach, Elad Schneidman",
        "summary": "Brains learn to represent information from a large set of stimuli, typically by weak supervision. Unsupervised learning is therefore a natural approach for exploring the design of biological neural networks and their computations. Accordingly, redundancy reduction has been suggested as a prominent design principle of neural encoding, but its ``mechanistic'' biological implementation is unclear. Analogously, unsupervised training of artificial neural networks yields internal representations that allow for accurate stimulus classification or decoding, but typically rely on biologically-implausible implementations. We suggest that interactions between parallel subnetworks in the brain may underlie such learning: we present a model of representation learning by ensembles of neural networks, where each network learns to encode stimuli into an abstract representation space by cross-supervising interactions with other networks, for inputs they receive simultaneously or in close temporal proximity. Aiming for biological plausibility, each network has a small ``receptive field'', thus receiving a fixed part of the external input, and the networks do not share weights. We find that for different types of network architectures, and for both visual or neuronal stimuli, these cross-supervising networks learn semantic representations that are easily decodable and that decoding accuracy is comparable to supervised networks -- both at the level of single networks and the ensemble. We further show that performance is optimal for small receptive fields, and that sparse connectivity between networks is nearly as accurate as all-to-all interactions, with far fewer computations. We thus suggest a sparsely interacting collective of cross-supervising networks as an algorithmic framework for representational learning and collective computation in the brain.",
        "subjects": "Neurons and Cognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.318262"
    },
    {
        "index": "#114",
        "title": "Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models",
        "link": "/arxiv/2510.14470",
        "arxiv_id": "2510.14470",
        "authors": "Xiaoyu Xue, Yuni Lai, Chenxi Huang, Yulin Zhu, Gaolei Li, Xiaoge Zhang, Kai Zhou",
        "summary": "The emergence of graph foundation models (GFMs), particularly those incorporating language models (LMs), has revolutionized graph learning and demonstrated remarkable performance on text-attributed graphs (TAGs). However, compared to traditional GNNs, these LM-empowered GFMs introduce unique security vulnerabilities during the unsecured prompt tuning phase that remain understudied in current research. Through empirical investigation, we reveal a significant performance degradation in traditional graph backdoor attacks when operating in attribute-inaccessible constrained TAG systems without explicit trigger node attribute optimization. To address this, we propose a novel dual-trigger backdoor attack framework that operates at both text-level and struct-level, enabling effective attacks without explicit optimization of trigger node text attributes through the strategic utilization of a pre-established text pool. Extensive experimental evaluations demonstrate that our attack maintains superior clean accuracy while achieving outstanding attack success rates, including scenarios with highly concealed single-trigger nodes. Our work highlights critical backdoor risks in web-deployed LM-empowered GFMs and contributes to the development of more robust supervision mechanisms for open-source platforms in the era of foundation models.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.318622"
    },
    {
        "index": "#117",
        "title": "Towards Adaptable Humanoid Control via Adaptive Motion Tracking",
        "link": "/arxiv/2510.14454",
        "arxiv_id": "2510.14454",
        "authors": "Tao Huang, Huayi Wang, Junli Ren, Kangning Yin, Zirui Wang, Xiao Chen, Feiyu Jia, Wentao Zhang, Junfeng Long, Jingbo Wang, Jiangmiao Pang",
        "summary": "Humanoid robots are envisioned to adapt demonstrated motions to diverse real-world conditions while accurately preserving motion patterns. Existing motion prior approaches enable well adaptability with a few motions but often sacrifice imitation accuracy, whereas motion-tracking methods achieve accurate imitation yet require many training motions and a test-time target motion to adapt. To combine their strengths, we introduce AdaMimic, a novel motion tracking algorithm that enables adaptable humanoid control from a single reference motion. To reduce data dependence while ensuring adaptability, our method first creates an augmented dataset by sparsifying the single reference motion into keyframes and applying light editing with minimal physical assumptions. A policy is then initialized by tracking these sparse keyframes to generate dense intermediate motions, and adapters are subsequently trained to adjust tracking speed and refine low-level actions based on the adjustment, enabling flexible time warping that further improves imitation accuracy and adaptability. We validate these significant improvements in our approach in both simulation and the real-world Unitree G1 humanoid robot in multiple tasks across a wide range of adaptation conditions. Videos and code are available at https://taohuang13.github.io/adamimic.github.io/.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.319715"
    },
    {
        "index": "#120",
        "title": "Big Data Approaches to Bovine Bioacoustics: A FAIR-Compliant Dataset and Scalable ML Framework for Precision Livestock Welfare",
        "link": "/arxiv/2510.14443",
        "arxiv_id": "2510.14443",
        "authors": "Mayuri Kate, Suresh Neethirajan",
        "summary": "The convergence of IoT sensing, edge computing, and machine learning is transforming precision livestock farming. Yet bioacoustic data streams remain underused because of computational complexity and ecological validity challenges. We present one of the most comprehensive bovine vocalization datasets to date, with 569 curated clips covering 48 behavioral classes, recorded across three commercial dairy farms using multiple microphone arrays and expanded to 2900 samples through domain informed augmentation. This FAIR compliant resource addresses major Big Data challenges - volume (90 hours of recordings, 65.6 GB), variety (multi farm and multi zone acoustics), velocity (real time processing), and veracity (noise robust feature extraction). Our distributed processing framework integrates advanced denoising using iZotope RX, multimodal synchronization through audio and video alignment, and standardized feature engineering with 24 acoustic descriptors generated from Praat, librosa, and openSMILE. Preliminary benchmarks reveal distinct class level acoustic patterns for estrus detection, distress classification, and maternal communication. The datasets ecological realism, reflecting authentic barn acoustics rather than controlled settings, ensures readiness for field deployment. This work establishes a foundation for animal centered AI, where bioacoustic data enable continuous and non invasive welfare assessment at industrial scale. By releasing standardized pipelines and detailed metadata, we promote reproducible research that connects Big Data analytics, sustainable agriculture, and precision livestock management. The framework supports UN SDG 9, showing how data science can turn traditional farming into intelligent, welfare optimized systems that meet global food needs while upholding ethical animal care.",
        "subjects": "Sound, Artificial Intelligence, Audio and Speech Processing",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.320915"
    },
    {
        "index": "#124",
        "title": "FairBatching: Fairness-Aware Batch Formation for LLM Inference",
        "link": "/arxiv/2510.14392",
        "arxiv_id": "2510.14392",
        "authors": "Hongtao Lyu, Boyue Liu, Mingyu Wu, Haibo Chen",
        "summary": "Large language model (LLM) inference systems face a fundamental tension between minimizing Time-to-First-Token (TTFT) latency for new requests and maintaining a high, steady token generation rate (low Time-Per-Output-Token, or TPOT) for ongoing requests. Existing stall-free batching schedulers proposed by Sarathi, while effective at preventing decode stalls, introduce significant computational unfairness. They prioritize decode tasks excessively, simultaneously leading to underutilized decode slack and unnecessary prefill queuing delays, which collectively degrade the system's overall quality of service (QoS). This work identifies the root cause of this unfairness: the non-monotonic nature of Time-Between-Tokens (TBT) as a scheduling metric and the rigid decode-prioritizing policy that fails to adapt to dynamic workload bursts. We therefore propose FairBatching, a novel LLM inference scheduler that enforces fair resource allocation between prefill and decode tasks. It features an adaptive batch capacity determination mechanism, which dynamically adjusts the computational budget to improve the GPU utilization without triggering SLO violations. Its fair and dynamic batch formation algorithm breaks away from the decode-prioritizing paradigm, allowing computation resources to be reclaimed from bursting decode tasks to serve prefill surges, achieving global fairness. Furthermore, FairBatching provides a novel load estimation method, enabling more effective coordination with upper-level schedulers. Implemented and evaluated on realistic traces, FairBatching significantly reduces TTFT tail latency by up to 2.29x while robustly maintaining TPOT SLOs, achieving overall 20.0% improvement in single-node capacity and 54.3% improvement in cluster-level capacity.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.322588"
    },
    {
        "index": "#128",
        "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation",
        "link": "/arxiv/2510.14357",
        "arxiv_id": "2510.14357",
        "authors": "Xiaobei Zhao, Xingqi Lyu, Xiang Li",
        "summary": "Agricultural robots are emerging as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily rely on manual operation or fixed rail systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling robots to navigate to the target positions following the natural language instructions. In practical agricultural scenarios, navigation instructions often repeatedly occur, yet AgriVLN treat each instruction as an independent episode, overlooking the potential of past experiences to provide spatial context for subsequent ones. To bridge this gap, we propose the method of Spatial Understanding Memory for Agricultural Vision-and-Language Navigation (SUM-AgriVLN), in which the SUM module employs spatial understanding and save spatial memory through 3D reconstruction and representation. When evaluated on the A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47 to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m, demonstrating the state-of-the-art performance in the agricultural domain. Code: https://github.com/AlexTraveling/SUM-AgriVLN.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.324211"
    },
    {
        "index": "#131",
        "title": "BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection",
        "link": "/arxiv/2510.14344",
        "arxiv_id": "2510.14344",
        "authors": "Zichen Liu, Shao Yang, Xusheng Xiao",
        "summary": "Mobile app markets host millions of apps, yet undesired behaviors (e.g., disruptive ads, illegal redirection, payment deception) remain hard to catch because they often do not rely on permission-protected APIs and can be easily camouflaged via UI or metadata edits. We present BINCTX, a learning approach that builds multi-modal representations of an app from (i) a global bytecode-as-image view that captures code-level semantics and family-style patterns, (ii) a contextual view (manifested actions, components, declared permissions, URL/IP constants) indicating how behaviors are triggered, and (iii) a third-party-library usage view summarizing invocation frequencies along inter-component call paths. The three views are embedded and fused to train a contextual-aware classifier. On real-world malware and benign apps, BINCTX attains a macro F1 of 94.73%, outperforming strong baselines by at least 14.92%. It remains robust under commercial obfuscation (F1 84% post-obfuscation) and is more resistant to adversarial samples than state-of-the-art bytecode-only systems.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.325394"
    },
    {
        "index": "#136",
        "title": "Column Generation Using Domain-Independent Dynamic Programming",
        "link": "/arxiv/2510.14317",
        "arxiv_id": "2510.14317",
        "authors": "Ryo Kuroiwa, Edward Lam",
        "summary": "Column generation and branch-and-price are leading methods for large-scale exact optimization. Column generation iterates between solving a master problem and a pricing problem. The master problem is a linear program, which can be solved using a generic solver. The pricing problem is highly dependent on the application but is usually discrete. Due to the difficulty of discrete optimization, high-performance column generation often relies on a custom pricing algorithm built specifically to exploit the problem's structure. This bespoke nature of the pricing solver prevents the reuse of components for other applications. We show that domain-independent dynamic programming, a software package for modeling and solving arbitrary dynamic programs, can be used as a generic pricing solver. We develop basic implementations of branch-and-price with pricing by domain-independent dynamic programming and show that they outperform a world-leading solver on static mixed integer programming formulations for seven problem classes.",
        "subjects": "Optimization and Control, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.327337"
    },
    {
        "index": "#138",
        "title": "Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding",
        "link": "/arxiv/2510.14304",
        "arxiv_id": "2510.14304",
        "authors": "Kyungryul Back, Seongbeom Park, Milim Kim, Mincheol Kwon, SangHyeok Lee, Hyunyoung Lee, Junhee Cho, Seunghyun Park, Jinkyu Kim",
        "summary": "Large Vision-Language Models (LVLMs) have recently shown promising results on various multimodal tasks, even achieving human-comparable performance in certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often rely heavily on a single modality or memorize training data without properly grounding their outputs. To address this, we propose a training-free, tri-layer contrastive decoding with watermarking, which proceeds in three steps: (1) select a mature layer and an amateur layer among the decoding layers, (2) identify a pivot layer using a watermark-related question to assess whether the layer is visually well-grounded, and (3) apply tri-layer contrastive decoding to generate the final output. Experiments on public benchmarks such as POPE, MME and AMBER demonstrate that our method achieves state-of-the-art performance in reducing hallucinations in LVLMs and generates more visually grounded responses.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.328303"
    },
    {
        "index": "#139",
        "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning",
        "link": "/arxiv/2510.14300",
        "arxiv_id": "2510.14300",
        "authors": "Weijie Shen, Yitian Liu, Yuhao Wu, Zhixuan Liang, Sijia Gu, Dehui Wang, Tian Nian, Lei Xu, Yusen Qin, Jiangmiao Pang, Xinping Guan, Xiaokang Yang, Yao Mu",
        "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.328793"
    },
    {
        "index": "#146",
        "title": "Do Joint Language-Audio Embeddings Encode Perceptual Timbre Semantics?",
        "link": "/arxiv/2510.14249",
        "arxiv_id": "2510.14249",
        "authors": "Qixin Deng, Bryan Pardo, Thrasyvoulos N Pappas",
        "summary": "Understanding and modeling the relationship between language and sound is critical for applications such as music information retrieval,text-guided music generation, and audio captioning. Central to these tasks is the use of joint language-audio embedding spaces, which map textual descriptions and auditory content into a shared embedding space. While multimodal embedding models such as MS-CLAP, LAION-CLAP, and MuQ-MuLan have shown strong performance in aligning language and audio, their correspondence to human perception of timbre, a multifaceted attribute encompassing qualities such as brightness, roughness, and warmth, remains underexplored. In this paper, we evaluate the above three joint language-audio embedding models on their ability to capture perceptual dimensions of timbre. Our findings show that LAION-CLAP consistently provides the most reliable alignment with human-perceived timbre semantics across both instrumental sounds and audio effects.",
        "subjects": "Sound, Artificial Intelligence, Audio and Speech Processing",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.331850"
    },
    {
        "index": "#148",
        "title": "Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation",
        "link": "/arxiv/2510.14244",
        "arxiv_id": "2510.14244",
        "authors": "Arnaud Judge, Nicolas Duchateau, Thierry Judge, Roman A. Sandler, Joseph Z. Sokol, Christian Desrosiers, Olivier Bernard, Pierre-Marc Jodoin",
        "summary": "Domain adaptation methods aim to bridge the gap between datasets by enabling knowledge transfer across domains, reducing the need for additional expert annotations. However, many approaches struggle with reliability in the target domain, an issue particularly critical in medical image segmentation, where accuracy and anatomical validity are essential. This challenge is further exacerbated in spatio-temporal data, where the lack of temporal consistency can significantly degrade segmentation quality, and particularly in echocardiography, where the presence of artifacts and noise can further hinder segmentation performance. To address these issues, we present RL4Seg3D, an unsupervised domain adaptation framework for 2D + time echocardiography segmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to enhance key landmark precision in its segmentations while processing full-sized input videos. By leveraging reinforcement learning for image segmentation, our approach improves accuracy, anatomical validity, and temporal consistency while also providing, as a beneficial side effect, a robust uncertainty estimator, which can be used at test time to further enhance segmentation performance. We demonstrate the effectiveness of our framework on over 30,000 echocardiographic videos, showing that it outperforms standard domain adaptation techniques without the need for any labels on the target domain. Code is available at https://github.com/arnaudjudge/RL4Seg3D.",
        "subjects": "Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.332694"
    },
    {
        "index": "#149",
        "title": "Spatial Computing Communications for Multi-User Virtual Reality in Distributed Mobile Edge Computing Network",
        "link": "/arxiv/2510.14243",
        "arxiv_id": "2510.14243",
        "authors": "Caolu Xu, Zhiyong Chen, Meixia Tao, Li Song, Wenjun Zhang",
        "summary": "Immersive virtual reality (VR) applications impose stringent requirements on latency, energy efficiency, and computational resources, particularly in multi-user interactive scenarios. To address these challenges, we introduce the concept of spatial computing communications (SCC), a framework designed to meet the latency and energy demands of multi-user VR over distributed mobile edge computing (MEC) networks. SCC jointly represents the physical space, defined by users and base stations, and the virtual space, representing shared immersive environments, using a probabilistic model of user dynamics and resource requirements. The resource deployment task is then formulated as a multi-objective combinatorial optimization (MOCO) problem that simultaneously minimizes system latency and energy consumption across distributed MEC resources. To solve this problem, we propose MO-CMPO, a multi-objective consistency model with policy optimization that integrates supervised learning and reinforcement learning (RL) fine-tuning guided by preference weights. Leveraging a sparse graph neural network (GNN), MO-CMPO efficiently generates Pareto-optimal solutions. Simulations with real-world New Radio base station datasets demonstrate that MO-CMPO achieves superior hypervolume performance and significantly lower inference latency than baseline methods. Furthermore, the analysis reveals practical deployment patterns: latency-oriented solutions favor local MEC execution to reduce transmission delay, while energy-oriented solutions minimize redundant placements to save energy.",
        "subjects": "Information Theory, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.333074"
    },
    {
        "index": "#151",
        "title": "Large Scale Retrieval for the LinkedIn Feed using Causal Language Models",
        "link": "/arxiv/2510.14223",
        "arxiv_id": "2510.14223",
        "authors": "Sudarshan Srinivasa Ramanujam, Antonio Alonso, Saurabh Kataria, Siddharth Dangi, Akhilesh Gupta, Birjodh Singh Tiwana, Manas Somaiya, Luke Simon, David Byrne, Sojeong Ha, Sen Zhou, Andrei Akterskii, Zhanglong Liu, Samira Sriram, Crescent Xiong, Zhoutao Pei, Angela Shao, Alex Li, Annie Xiao, Caitlin Kolb, Thomas Kistler, Zach Moore, Hamed Firooz",
        "summary": "In large scale recommendation systems like the LinkedIn Feed, the retrieval stage is critical for narrowing hundreds of millions of potential candidates to a manageable subset for ranking. LinkedIn's Feed serves suggested content from outside of the member's network (based on the member's topical interests), where 2000 candidates are retrieved from a pool of hundreds of millions candidate with a latency budget of a few milliseconds and inbound QPS of several thousand per second. This paper presents a novel retrieval approach that fine-tunes a large causal language model (Meta's LLaMA 3) as a dual encoder to generate high quality embeddings for both users (members) and content (items), using only textual input. We describe the end to end pipeline, including prompt design for embedding generation, techniques for fine-tuning at LinkedIn's scale, and infrastructure for low latency, cost effective online serving. We share our findings on how quantizing numerical features in the prompt enables the information to get properly encoded in the embedding, facilitating greater alignment between the retrieval and ranking layer. The system was evaluated using offline metrics and an online A/B test, which showed substantial improvements in member engagement. We observed significant gains among newer members, who often lack strong network connections, indicating that high-quality suggested content aids retention. This work demonstrates how generative language models can be effectively adapted for real time, high throughput retrieval in industrial applications.",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.334081"
    },
    {
        "index": "#155",
        "title": "Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures",
        "link": "/arxiv/2510.14179",
        "arxiv_id": "2510.14179",
        "authors": "Yuancheng Xu, Wenqi Xian, Li Ma, Julien Philip, Ahmet Levent Taşel, Yiwei Zhao, Ryan Burgert, Mingming He, Oliver Hermann, Oliver Pilarski, Rahul Garg, Paul Debevec, Ning Yu",
        "summary": "We introduce a framework that enables both multi-view character consistency and 3D camera control in video diffusion models through a novel customization data pipeline. We train the character consistency component with recorded volumetric capture performances re-rendered with diverse camera trajectories via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video relighting model. We fine-tune state-of-the-art open-source video diffusion models on this data to provide strong multi-view identity preservation, precise camera control, and lighting adaptability. Our framework also supports core capabilities for virtual production, including multi-subject generation using two approaches: joint training and noise blending, the latter enabling efficient composition of independently customized models at inference time; it also achieves scene and real-life video customization as well as control over motion and spatial layout during customization. Extensive experiments show improved video quality, higher personalization accuracy, and enhanced camera control and lighting adaptability, advancing the integration of video generation into virtual production. Our project page is available at: https://eyeline-labs.github.io/Virtually-Being.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-16",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.335654"
    },
    {
        "index": "#157",
        "title": "FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API",
        "link": "/arxiv/2510.14162",
        "arxiv_id": "2510.14162",
        "authors": "Juhyeong Kim, Yejin Kim, Youngbin Lee, Hyunwoo Byun",
        "summary": "We present FinAI Data Assistant, a practical approach for natural-language querying over financial databases that combines large language models (LLMs) with the OpenAI Function Calling API. Rather than synthesizing complete SQL via text-to-SQL, our system routes user requests to a small library of vetted, parameterized queries, trading generative flexibility for reliability, low latency, and cost efficiency. We empirically study three questions: (RQ1) whether LLMs alone can reliably recall or extrapolate time-dependent financial data without external retrieval; (RQ2) how well LLMs map company names to stock ticker symbols; and (RQ3) whether function calling outperforms text-to-SQL for end-to-end database query processing. Across controlled experiments on prices and fundamentals, LLM-only predictions exhibit non-negligible error and show look-ahead bias primarily for stock prices relative to model knowledge cutoffs. Ticker-mapping accuracy is near-perfect for NASDAQ-100 constituents and high for S\\&P~500 firms. Finally, FinAI Data Assistant achieves lower latency and cost and higher reliability than a text-to-SQL baseline on our task suite. We discuss design trade-offs, limitations, and avenues for deployment.",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.336402"
    },
    {
        "index": "#162",
        "title": "Every Language Model Has a Forgery-Resistant Signature",
        "link": "/arxiv/2510.14086",
        "arxiv_id": "2510.14086",
        "authors": "Matthew Finlayson, Xiang Ren, Swabha Swayamdipta",
        "summary": "The ubiquity of closed-weight language models with public-facing APIs has generated interest in forensic methods, both for extracting hidden model details (e.g., parameters) and for identifying models by their outputs. One successful approach to these goals has been to exploit the geometric constraints imposed by the language model architecture and parameters. In this work, we show that a lesser-known geometric constraint--namely, that language model outputs lie on the surface of a high-dimensional ellipse--functions as a signature for the model and can be used to identify the source model of a given output. This ellipse signature has unique properties that distinguish it from existing model-output association methods like language model fingerprints. In particular, the signature is hard to forge: without direct access to model parameters, it is practically infeasible to produce log-probabilities (logprobs) on the ellipse. Secondly, the signature is naturally occurring, since all language models have these elliptical constraints. Thirdly, the signature is self-contained, in that it is detectable without access to the model inputs or the full weights. Finally, the signature is compact and redundant, as it is independently detectable in each logprob output from the model. We evaluate a novel technique for extracting the ellipse from small models and discuss the practical hurdles that make it infeasible for production-scale models. Finally, we use ellipse signatures to propose a protocol for language model output verification, analogous to cryptographic symmetric-key message authentication systems.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.338258"
    },
    {
        "index": "#163",
        "title": "DiffOPF: Diffusion Solver for Optimal Power Flow",
        "link": "/arxiv/2510.14075",
        "arxiv_id": "2510.14075",
        "authors": "Milad Hoseinpour, Vladimir Dvorkin",
        "summary": "The optimal power flow (OPF) is a multi-valued, non-convex mapping from loads to dispatch setpoints. The variability of system parameters (e.g., admittances, topology) further contributes to the multiplicity of dispatch setpoints for a given load. Existing deep learning OPF solvers are single-valued and thus fail to capture the variability of system parameters unless fully represented in the feature space, which is prohibitive. To solve this problem, we introduce a diffusion-based OPF solver, termed \\textit{DiffOPF}, that treats OPF as a conditional sampling problem. The solver learns the joint distribution of loads and dispatch setpoints from operational history, and returns the marginal dispatch distributions conditioned on loads. Unlike single-valued solvers, DiffOPF enables sampling statistically credible warm starts with favorable cost and constraint satisfaction trade-offs. We explore the sample complexity of DiffOPF to ensure the OPF solution within a prescribed distance from the optimization-based solution, and verify this experimentally on power system benchmarks.",
        "subjects": "Systems and Control, Artificial Intelligence, Computation, Machine Learning",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.338655"
    },
    {
        "index": "#166",
        "title": "Optical Computation-in-Communication enables low-latency, high-fidelity perception in telesurgery",
        "link": "/arxiv/2510.14058",
        "arxiv_id": "2510.14058",
        "authors": "Rui Yang, Jiaming Hu, Jian-Qing Zheng, Yue-Zhen Lu, Jian-Wei Cui, Qun Ren, Yi-Jie Yu, John Edward Wu, Zhao-Yu Wang, Xiao-Li Lin, Dandan Zhang, Mingchu Tang, Christos Masouros, Huiyun Liu, Chin-Pang Liu",
        "summary": "Artificial intelligence (AI) holds significant promise for enhancing intraoperative perception and decision-making in telesurgery, where physical separation impairs sensory feedback and control. Despite advances in medical AI and surgical robotics, conventional electronic AI architectures remain fundamentally constrained by the compounded latency from serial processing of inference and communication. This limitation is especially critical in latency-sensitive procedures such as endovascular interventions, where delays over 200 ms can compromise real-time AI reliability and patient safety. Here, we introduce an Optical Computation-in-Communication (OCiC) framework that reduces end-to-end latency significantly by performing AI inference concurrently with optical communication. OCiC integrates Optical Remote Computing Units (ORCUs) directly into the optical communication pathway, with each ORCU experimentally achieving up to 69 tera-operations per second per channel through spectrally efficient two-dimensional photonic convolution. The system maintains ultrahigh inference fidelity within 0.1% of CPU/GPU baselines on classification and coronary angiography segmentation, while intrinsically mitigating cumulative error propagation, a longstanding barrier to deep optical network scalability. We validated the robustness of OCiC through outdoor dark fibre deployments, confirming consistent and stable performance across varying environmental conditions. When scaled globally, OCiC transforms long-haul fibre infrastructure into a distributed photonic AI fabric with exascale potential, enabling reliable, low-latency telesurgery across distances up to 10,000 km and opening a new optical frontier for distributed medical intelligence.",
        "subjects": "Optics, Artificial Intelligence, Image and Video Processing",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.339949"
    },
    {
        "index": "#167",
        "title": "Cyber-Resilient System Identification for Power Grid through Bayesian Integration",
        "link": "/arxiv/2510.14043",
        "arxiv_id": "2510.14043",
        "authors": "Shimiao Li, Guannan Qu, Bryan Hooi, Vyas Sekar, Soummya Kar, Larry Pileggi",
        "summary": "Power grids increasingly need real-time situational awareness under the ever-evolving cyberthreat landscape. Advances in snapshot-based system identification approaches have enabled accurately estimating states and topology from a snapshot of measurement data, under random bad data and topology errors. However, modern interactive, targeted false data can stay undetectable to these methods, and significantly compromise estimation accuracy. This work advances system identification that combines snapshot-based method with time-series model via Bayesian Integration, to advance cyber resiliency against both random and targeted false data. Using a distance-based time-series model, this work can leverage historical data of different distributions induced by changes in grid topology and other settings. The normal system behavior captured from historical data is integrated into system identification through a Bayesian treatment, to make solutions robust to targeted false data. We experiment on mixed random anomalies (bad data, topology error) and targeted false data injection attack (FDIA) to demonstrate our method's 1) cyber resilience: achieving over 70% reduction in estimation error under FDIA; 2) anomalous data identification: being able to alarm and locate anomalous data; 3) almost linear scalability: achieving comparable speed with the snapshot-based baseline, both taking <1min per time tick on the large 2,383-bus system using a laptop CPU.",
        "subjects": "Systems and Control, Artificial Intelligence, Cryptography and Security",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.340350"
    },
    {
        "index": "#168",
        "title": "One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery",
        "link": "/arxiv/2510.14036",
        "arxiv_id": "2510.14036",
        "authors": "Qiushi Wu, Yue Xiao, Dhilung Kirat, Kevin Eykholt, Jiyong Jang, Douglas Lee Schales",
        "summary": "Fixing bugs in large programs is a challenging task that demands substantial time and effort. Once a bug is found, it is reported to the project maintainers, who work with the reporter to fix it and eventually close the issue. However, across the program, there are often similar code segments, which may also contain the bug, but were missed during discovery. Finding and fixing each recurring bug instance individually is labor intensive. Even more concerning, bug reports can inadvertently widen the attack surface as they provide attackers with an exploitable pattern that may be unresolved in other parts of the program. In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear repeatedly across various code segments of a program or even in different programs, stemming from a same root cause, but are unresolved. Our investigation reveals that RPBs are widespread and can significantly compromise the security of software programs. This paper introduces BugStone, a program analysis system empowered by LLVM and a Large Language Model (LLM). The key observation is that many RPBs have one patched instance, which can be leveraged to identify a consistent error pattern, such as a specific API misuse. By examining the entire program for this pattern, it is possible to identify similar sections of code that may be vulnerable. Starting with 135 unique RPBs, BugStone identified more than 22K new potential issues in the Linux kernel. Manual analysis of 400 of these findings confirmed that 246 were valid. We also created a dataset from over 1.9K security bugs reported by 23 recent top-tier conference works. We manually annotate the dataset, identify 80 recurring patterns and 850 corresponding fixes. Even with a cost-efficient model choice, BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.340730"
    },
    {
        "index": "#173",
        "title": "Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer",
        "link": "/arxiv/2510.13995",
        "arxiv_id": "2510.13995",
        "authors": "Kelvin Szolnoky, Anders Blilie, Nita Mulliqi, Toyonori Tsuzuki, Hemamali Samaratunga, Matteo Titus, Xiaoyi Ji, Sol Erika Boman, Einar Gudlaugsson, Svein Reidar Kjosavik, José Asenjo, Marcello Gambacorta, Paolo Libretti, Marcin Braun, Radisław Kordek, Roman Łowicki, Brett Delahunt, Kenneth A. Iczkowski, Theo van der Kwast, Geert J. L. H. van Leenders, Katia R. M. Leite, Chin-Chen Pan, Emiel Adrianus Maria Janssen, Martin Eklund, Lars Egevad, Kimmo Kartasalo",
        "summary": "Background: Cribriform morphology in prostate cancer is a histological feature that indicates poor prognosis and contraindicates active surveillance. However, it remains underreported and subject to significant interobserver variability amongst pathologists. We aimed to develop and validate an AI-based system to improve cribriform pattern detection. Methods: We created a deep learning model using an EfficientNetV2-S encoder with multiple instance learning for end-to-end whole-slide classification. The model was trained on 640 digitised prostate core needle biopsies from 430 patients, collected across three cohorts. It was validated internally (261 slides from 171 patients) and externally (266 slides, 104 patients from three independent cohorts). Internal validation cohorts included laboratories or scanners from the development set, while external cohorts used completely independent instruments and laboratories. Annotations were provided by three expert uropathologists with known high concordance. Additionally, we conducted an inter-rater analysis and compared the model's performance against nine expert uropathologists on 88 slides from the internal validation cohort. Results: The model showed strong internal validation performance (AUC: 0.97, 95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI: 0.45-0.64). In our inter-rater analysis, the model achieved the highest average agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine pathologists whose Cohen's kappas ranged from 0.35 to 0.62. Conclusion: Our AI model demonstrates pathologist-level performance for cribriform morphology detection in prostate cancer. This approach could enhance diagnostic reliability, standardise reporting, and improve treatment decisions for prostate cancer patients.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2025-10-15",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.342831"
    },
    {
        "index": "#186",
        "title": "Dual-attention ResNet outperforms transformers in HER2 prediction on DCE-MRI",
        "link": "/arxiv/2510.13897",
        "arxiv_id": "2510.13897",
        "authors": "Naomi Fridman, Anat Goldstein",
        "summary": "Breast cancer is the most diagnosed cancer in women, with HER2 status critically guiding treatment decisions. Noninvasive prediction of HER2 status from dynamic contrast-enhanced MRI (DCE-MRI) could streamline diagnostics and reduce reliance on biopsy. However, preprocessing high-dynamic-range DCE-MRI into standardized 8-bit RGB format for pretrained neural networks is nontrivial, and normalization strategy significantly affects model performance. We benchmarked intensity normalization strategies using a Triple-Head Dual-Attention ResNet that processes RGB-fused temporal sequences from three DCE phases. Trained on a multicenter cohort (n=1,149) from the I-SPY trials and externally validated on BreastDCEDL_AMBL (n=43 lesions), our model outperformed transformer-based architectures, achieving 0.75 accuracy and 0.74 AUC on I-SPY test data. N4 bias field correction slightly degraded performance. Without fine-tuning, external validation yielded 0.66 AUC, demonstrating cross-institutional generalizability. These findings highlight the effectiveness of dual-attention mechanisms in capturing transferable spatiotemporal features for HER2 stratification, advancing reproducible deep learning biomarkers in breast cancer imaging.",
        "subjects": "Quantitative Methods, Artificial Intelligence",
        "date": "2025-10-14",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.347965"
    },
    {
        "index": "#194",
        "title": "Physics-Informed autoencoder for DSC-MRI Perfusion post-processing: application to glioma grading",
        "link": "/arxiv/2510.13886",
        "arxiv_id": "2510.13886",
        "authors": "Pierre Fayolle, Alexandre Bône, Noëlie Debs, Mathieu Naudin, Pascal Bourdon, Remy Guillevin, David Helbert",
        "summary": "DSC-MRI perfusion is a medical imaging technique for diagnosing and prognosing brain tumors and strokes. Its analysis relies on mathematical deconvolution, but noise or motion artifacts in a clinical environment can disrupt this process, leading to incorrect estimate of perfusion parameters. Although deep learning approaches have shown promising results, their calibration typically rely on third-party deconvolution algorithms to generate reference outputs and are bound to reproduce their limitations. To adress this problem, we propose a physics-informed autoencoder that leverages an analytical model to decode the perfusion parameters and guide the learning of the encoding network. This autoencoder is trained in a self-supervised fashion without any third-party software and its performance is evaluated on a database with glioma patients. Our method shows reliable results for glioma grading in accordance with other well-known deconvolution algorithms despite a lower computation time. It also achieved competitive performance even in the presence of high noise which is critical in a medical environment.",
        "subjects": "Quantitative Methods, Artificial Intelligence, Image and Video Processing, Signal Processing",
        "date": "2025-10-14",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.353573"
    },
    {
        "index": "#208",
        "title": "Benchmarking Correctness and Security in Multi-Turn Code Generation",
        "link": "/arxiv/2510.13859",
        "arxiv_id": "2510.13859",
        "authors": "Ruchit Rawal, Jeffrey Yang Fan Chiang, Chihao Shen, Jeffery Siyuan Tian, Aastha Mahajan, Tom Goldstein, Yizheng Chen",
        "summary": "AI coding assistants powered by large language models (LLMs) have transformed software development, significantly boosting productivity. While existing benchmarks evaluate the correctness and security of LLM-generated code, they are typically limited to single-turn tasks that do not reflect the iterative nature of real-world development. We introduce MT-Sec, the first benchmark to systematically evaluate both correctness and security in multi-turn coding scenarios. We construct this using a synthetic data pipeline that transforms existing single-turn tasks into semantically aligned multi-turn interaction sequences, allowing reuse of original test suites while modeling the complexity of real-world coding processes. We evaluate 32 open- and closed-source models, and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in \"correct and secure\" outputs from single-turn to multi-turn settings -- even among state-of-the-art models. Beyond full-program generation, we also evaluate models on multi-turn code-diff generation -- an unexplored yet practically relevant setting -- and find that models perform worse here, with increased rates of functionally incorrect and insecure outputs. Finally, we find that while agent scaffoldings boost single-turn code generation performance, they are not quite as effective in multi-turn evaluations. Together, these findings highlight the need for benchmarks that jointly evaluate correctness and security in multi-turn, real-world coding workflows.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-13",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.361248"
    },
    {
        "index": "#209",
        "title": "From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering",
        "link": "/arxiv/2510.13857",
        "arxiv_id": "2510.13857",
        "authors": "Qiang Xu, Xiangyu Wen, Changran Xu, Zeju Li, Jianyuan Zhong",
        "summary": "The advent of powerful Large Language Models (LLMs) has ushered in an ``Age of the Agent,'' enabling autonomous systems to tackle complex goals. However, the transition from prototype to production is hindered by a pervasive ``crisis of craft,'' resulting in agents that are brittle, unpredictable, and ultimately untrustworthy in mission-critical applications. This paper argues this crisis stems from a fundamental paradigm mismatch -- attempting to command inherently probabilistic processors with the deterministic mental models of traditional software engineering. To solve this crisis, we introduce a governance-first paradigm for principled agent engineering, embodied in a formal architecture we call ArbiterOS.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-10-12",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.361540"
    },
    {
        "index": "#217",
        "title": "Information flow in multilayer perceptrons: an in-depth analysis",
        "link": "/arxiv/2510.13846",
        "arxiv_id": "2510.13846",
        "authors": "Giuliano Armano",
        "summary": "Analysing how information flows along the layers of a multilayer perceptron is a topic of paramount importance in the field of artificial neural networks. After framing the problem from the point of view of information theory, in this position article a specific investigation is conducted on the way information is processed, with particular reference to the requirements imposed by supervised learning. To this end, the concept of information matrix is devised and then used as formal framework for understanding the aetiology of optimisation strategies and for studying the information flow. The underlying research for this article has also produced several key outcomes: i) the definition of a parametric optimisation strategy, ii) the finding that the optimisation strategy proposed in the information bottleneck framework shares strong similarities with the one derived from the information matrix, and iii) the insight that a multilayer perceptron serves as a kind of \"adaptor\", meant to process the input according to the given objective.",
        "subjects": "Information Theory, Artificial Intelligence, Neural and Evolutionary Computing",
        "date": "2025-10-11",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.364043"
    },
    {
        "index": "#229",
        "title": "Towards Neurocognitive-Inspired Intelligence: From AI's Structural Mimicry to Human-Like Functional Cognition",
        "link": "/arxiv/2510.13826",
        "arxiv_id": "2510.13826",
        "authors": "Noorbakhsh Amiri Golilarz, Hassan S. Al Khatib, Shahram Rahimi",
        "summary": "Artificial intelligence has advanced significantly through deep learning, reinforcement learning, and large language and vision models. However, these systems often remain task specific, struggle to adapt to changing conditions, and cannot generalize in ways similar to human cognition. Additionally, they mainly focus on mimicking brain structures, which often leads to black-box models with limited transparency and adaptability. Inspired by the structure and function of biological cognition, this paper introduces the concept of \"Neurocognitive-Inspired Intelligence (NII),\" a hybrid approach that combines neuroscience, cognitive science, computer vision, and AI to develop more general, adaptive, and robust intelligent systems capable of rapid learning, learning from less data, and leveraging prior experience. These systems aim to emulate the human brain's ability to flexibly learn, reason, remember, perceive, and act in real-world settings with minimal supervision. We review the limitations of current AI methods, define core principles of neurocognitive-inspired intelligence, and propose a modular, biologically inspired architecture that emphasizes integration, embodiment, and adaptability. We also discuss potential implementation strategies and outline various real-world applications, from robotics to education and healthcare. Importantly, this paper offers a hybrid roadmap for future research, laying the groundwork for building AI systems that more closely resemble human cognition.",
        "subjects": "Neurons and Cognition, Artificial Intelligence",
        "date": "2025-10-09",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.367774"
    },
    {
        "index": "#230",
        "title": "A2AS: Agentic AI Runtime Security and Self-Defense",
        "link": "/arxiv/2510.13825",
        "arxiv_id": "2510.13825",
        "authors": "Eugene Neelou, Ivan Novikov, Max Moroz, Om Narayan, Tiffany Saade, Mika Ayenson, Ilya Kabanov, Jen Ozmen, Edward Lee, Vineeth Sai Narajala, Emmanuel Guilherme Junior, Ken Huang, Huseyin Gulsin, Jason Ross, Marat Vyshegorodtsev, Adelin Travers, Idan Habler, Rahul Jadav",
        "summary": "The A2AS framework is introduced as a security layer for AI agents and LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces certified behavior, activates model self-defense, and ensures context window integrity. It defines security boundaries, authenticates prompts, applies security rules and custom policies, and controls agentic behavior, enabling a defense-in-depth strategy. The A2AS framework avoids latency overhead, external dependencies, architectural changes, model retraining, and operational complexity. The BASIC security model is introduced as the A2AS foundation: (B) Behavior certificates enable behavior enforcement, (A) Authenticated prompts enable context window integrity, (S) Security boundaries enable untrusted input isolation, (I) In-context defenses enable secure model reasoning, (C) Codified policies enable application-specific rules. This first paper in the series introduces the BASIC security model and the A2AS framework, exploring their potential toward establishing the A2AS industry standard.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-10-08",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.368184"
    },
    {
        "index": "#231",
        "title": "Leveraging Wireless Sensor Networks for Real-Time Monitoring and Control of Industrial Environments",
        "link": "/arxiv/2510.13820",
        "arxiv_id": "2510.13820",
        "authors": "Muhammad Junaid Asif, Shazia Saqib, Rana Fayyaz Ahmad, Hamza Khan",
        "summary": "This research proposes an extensive technique for monitoring and controlling the industrial parameters using Internet of Things (IoT) technology based on wireless communication. We proposed a system based on NRF transceivers to establish a strong Wireless Sensor Network (WSN), enabling transfer of real-time data from multiple sensors to a central setup that is driven by ARDUINO microcontrollers. Different key parameters, crucial for industrial setup such as temperature, humidity, soil moisture and fire detection, are monitored and displayed on an LCD screen, enabling factory administration to oversee the industrial operations remotely over the internet. Our proposed system bypasses the need for physical presence for monitoring by addressing the shortcomings of conventional wired communication systems. Other than monitoring, there is an additional feature to remotely control these parameters by controlling the speed of DC motors through online commands. Given the rising incidence of industrial fires over the worldwide between 2020 and 2024 due to an array of hazards, this system with dual functionality boosts the overall operational efficiency and safety. This overall integration of IoT and Wireless Sensor Network (WSN) reduces the potential risks linked with physical monitoring, providing rapid responses in emergency scenarios, including the activation of firefighting equipment. The results show that innovations in wireless communication perform an integral part in industrial process automation and safety, paving the way to more intelligent and responsive operating environments. Overall, this study highlights the potential for change of IoT-enabled systems to revolutionize monitoring and control in a variety of industrial applications, resulting in increased productivity and safety.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.AI",
        "crawl_time": "2025-10-17T11:00:05.368491"
    }
]