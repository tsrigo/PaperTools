[
    {
        "index": "#1",
        "title": "Voting-Bloc Entropy: A New Metric for DAO Decentralization",
        "link": "/arxiv/2509.22620",
        "arxiv_id": "2509.22620",
        "authors": "Andrés Fábrega, Amy Zhao, Jay Yu, James Austgen, Sarah Allen, Kushal Babel, Mahimna Kelkar, Ari Juels",
        "summary": "Decentralized Autonomous Organizations (DAOs) use smart contracts to foster communities working toward common goals. Existing definitions of decentralization, however -- the 'D' in DAO -- fall short of capturing the key properties characteristic of diverse and equitable participation. This work proposes a new framework for measuring DAO decentralization called Voting-Bloc Entropy (VBE, pronounced ''vibe''). VBE is based on the idea that voters with closely aligned interests act as a centralizing force and should be modeled as such. VBE formalizes this notion by measuring the similarity of participants' utility functions across a set of voting rounds. Unlike prior, ad hoc definitions of decentralization, VBE derives from first principles: We introduce a simple (yet powerful) reinforcement learning-based conceptual model for voting, that in turn implies VBE. We first show VBE's utility as a theoretical tool. We prove a number of results about the (de)centralizing effects of vote delegation, proposal bundling, bribery, etc. that are overlooked in previous notions of DAO decentralization. Our results lead to practical suggestions for enhancing DAO decentralization. We also show how VBE can be used empirically by presenting measurement studies and VBE-based governance experiments. We make the tools we developed for these results available to the community in the form of open-source artifacts in order to facilitate future study of DAO decentralization.",
        "subjects": "Multiagent Systems, Cryptography and Security",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.532273"
    },
    {
        "index": "#2",
        "title": "Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives",
        "link": "/arxiv/2509.22596",
        "arxiv_id": "2509.22596",
        "authors": "Qixin Zhang, Yan Sun, Can Jin, Xikun Zhang, Yao Shu, Puning Zhao, Li Shen, Dacheng Tao",
        "summary": "In this paper, we present two effective policy learning algorithms for multi-agent online coordination(MA-OC) problem. The first one, \\texttt{MA-SPL}, not only can achieve the optimal $(1-\\frac{c}{e})$-approximation guarantee for the MA-OC problem with submodular objectives but also can handle the unexplored $\\alpha$-weakly DR-submodular and $(\\gamma,\\beta)$-weakly submodular scenarios, where $c$ is the curvature of the investigated submodular functions, $\\alpha$ denotes the diminishing-return(DR) ratio and the tuple $(\\gamma,\\beta)$ represents the submodularity ratios. Subsequently, in order to reduce the reliance on the unknown parameters $\\alpha,\\gamma,\\beta$ inherent in the \\texttt{MA-SPL} algorithm, we further introduce the second online algorithm named \\texttt{MA-MPL}. This \\texttt{MA-MPL} algorithm is entirely \\emph{parameter-free} and simultaneously can maintain the same approximation ratio as the first \\texttt{MA-SPL} algorithm. The core of our \\texttt{MA-SPL} and \\texttt{MA-MPL} algorithms is a novel continuous-relaxation technique termed as \\emph{policy-based continuous extension}. Compared with the well-established \\emph{multi-linear extension}, a notable advantage of this new \\emph{policy-based continuous extension} is its ability to provide a lossless rounding scheme for any set function, thereby enabling us to tackle the challenging weakly submodular objectives. Finally, extensive simulations are conducted to validate the effectiveness of our proposed algorithms.",
        "subjects": "Multiagent Systems, Machine Learning, Optimization and Control",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.532610"
    },
    {
        "index": "#3",
        "title": "VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture",
        "link": "/arxiv/2509.22218",
        "arxiv_id": "2509.22218",
        "authors": "Sandaru Fernando, Imasha Jayarathne, Sithumini Abeysekara, Shanuja Sithamparanthan, Thushari Silva, Deshan Jayawardana",
        "summary": "Data visualization is essential for interpreting complex datasets, yet traditional tools often require technical expertise, limiting accessibility. VizGen is an AI-assisted graph generation system that empowers users to create meaningful visualizations using natural language. Leveraging advanced NLP and LLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries into SQL and recommends suitable graph types. Built on a multi-agent architecture, VizGen handles SQL generation, graph creation, customization, and insight extraction. Beyond visualization, it analyzes data for patterns, anomalies, and correlations, and enhances user understanding by providing explanations enriched with contextual information gathered from the internet. The system supports real-time interaction with SQL databases and allows conversational graph refinement, making data analysis intuitive and accessible. VizGen democratizes data visualization by bridging the gap between technical complexity and user-friendly design.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Databases",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.532907"
    },
    {
        "index": "#4",
        "title": "Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach",
        "link": "/arxiv/2509.22216",
        "arxiv_id": "2509.22216",
        "authors": "Ahmet Onur Akman, Anastasia Psarou, Zoltán György Varga, Grzegorz Jamróz, Rafał Kucharski",
        "summary": "This study examines the potential impact of reinforcement learning (RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic environment. We focus on a simplified day-to-day route choice problem in a multi-agent setting. We consider a city network where human drivers travel through their chosen routes to reach their destinations in minimum travel time. Then, we convert one-third of the population into AVs, which are RL agents employing Deep Q-learning algorithm. We define a set of optimization targets, or as we call them behaviors, namely selfish, collaborative, competitive, social, altruistic, and malicious. We impose a selected behavior on AVs through their rewards. We run our simulations using our in-house developed RL framework PARCOUR. Our simulations reveal that AVs optimize their travel times by up to 5\\%, with varying impacts on human drivers' travel times depending on the AV behavior. In all cases where AVs adopt a self-serving behavior, they achieve shorter travel times than human drivers. Our findings highlight the complexity differences in learning tasks of each target behavior. We demonstrate that the multi-agent RL setting is applicable for collective routing on traffic networks, though their impact on coexisting parties greatly varies with the behaviors adopted.",
        "subjects": "Multiagent Systems, Artificial Intelligence",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.533185"
    },
    {
        "index": "#5",
        "title": "Multi-Agent Path Finding via Offline RL and LLM Collaboration",
        "link": "/arxiv/2509.22130",
        "arxiv_id": "2509.22130",
        "authors": "Merve Atasever, Matthew Hong, Mihir Nitin Kulkarni, Qingpei Li, Jyotirmoy V. Deshmukh",
        "summary": "Multi-Agent Path Finding (MAPF) poses a significant and challenging problem critical for applications in robotics and logistics, particularly due to its combinatorial complexity and the partial observability inherent in realistic environments. Decentralized reinforcement learning methods commonly encounter two substantial difficulties: first, they often yield self-centered behaviors among agents, resulting in frequent collisions, and second, their reliance on complex communication modules leads to prolonged training times, sometimes spanning weeks. To address these challenges, we propose an efficient decentralized planning framework based on the Decision Transformer (DT), uniquely leveraging offline reinforcement learning to substantially reduce training durations from weeks to mere hours. Crucially, our approach effectively handles long-horizon credit assignment and significantly improves performance in scenarios with sparse and delayed rewards. Furthermore, to overcome adaptability limitations inherent in standard RL methods under dynamic environmental changes, we integrate a large language model (GPT-4o) to dynamically guide agent policies. Extensive experiments in both static and dynamically changing environments demonstrate that our DT-based approach, augmented briefly by GPT-4o, significantly enhances adaptability and performance.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.533464"
    },
    {
        "index": "#6",
        "title": "RobustFlow: Towards Robust Agentic Workflow Generation",
        "link": "/arxiv/2509.21834",
        "arxiv_id": "2509.21834",
        "authors": "Shengxiang Xu, Jiayi Zhang, Shimin Di, Yuyu Luo, Liang Yao, Hanmo Liu, Jia Zhu, Fan Liu, Min-Ling Zhang",
        "summary": "The automated generation of agentic workflows is a promising frontier for enabling large language models (LLMs) to solve complex tasks. However, our investigation reveals that the robustness of agentic workflow remains a critical, unaddressed challenge. Current methods often generate wildly inconsistent workflows when provided with instructions that are semantically identical but differently phrased. This brittleness severely undermines their reliability and trustworthiness for real-world applications. To quantitatively diagnose this instability, we propose metrics based on nodal and topological similarity to evaluate workflow consistency against common semantic variations such as paraphrasing and noise injection. Subsequently, we further propose a novel training framework, RobustFlow, that leverages preference optimization to teach models invariance to instruction variations. By training on sets of synonymous task descriptions, RobustFlow boosts workflow robustness scores to 70\\% - 90\\%, which is a substantial improvement over existing approaches. The code is publicly available at https://github.com/DEFENSE-SEU/RobustFlow.",
        "subjects": "Multiagent Systems",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.533751"
    },
    {
        "index": "#7",
        "title": "Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow",
        "link": "/arxiv/2509.21789",
        "arxiv_id": "2509.21789",
        "authors": "Xinlei Yu, Chengming Xu, Guibin Zhang, Yongbo He, Zhangquan Chen, Zhucun Xue, Jiangning Zhang, Yue Liao, Xiaobin Hu, Yu-Gang Jiang, Shuicheng Yan",
        "summary": "Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables challenging tasks but suffers from a novel failure term, multi-agent visual hallucination snowballing, where hallucinations are seeded in a single agent and amplified by following ones due to the over-reliance on textual flow to relay visual information. Through turn-, layer-, and token-wise attention analyses, we provide detailed insights into the essence of hallucination snowballing regarding the reduction of visual attention allocation. It leads us to identify a subset of vision tokens with a unimodal attention peak in middle layers that best preserve visual evidence but gradually diminish in deeper agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we propose ViF, a lightweight, plug-and-play mitigation paradigm that relays inter-agent messages with Visual Flow powered by the selected visual relay tokens and applies attention reallocation to amplify this pattern. The experiment results demonstrate that our method markedly reduces hallucination snowballing, consistently improving the performance across eight benchmarks based on four common MAS structures and ten base models. The source code will be available at: https://github.com/YU-deep/ViF.git.",
        "subjects": "Multiagent Systems, Computer Vision and Pattern Recognition",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.534069"
    },
    {
        "index": "#8",
        "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning",
        "link": "/arxiv/2509.22601",
        "arxiv_id": "2509.22601",
        "authors": "Yulei Qin, Xiaoyu Tan, Zhengbao He, Gang Li, Haojia Lin, Zongyi Li, Zihan Xu, Yuchen Shi, Siqi Cai, Renting Rui, Shaofei Cai, Yuzheng Cai, Xuan Zhang, Sheng Ye, Ke Li, Xing Sun",
        "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.",
        "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Multiagent Systems",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.534441"
    },
    {
        "index": "#9",
        "title": "Learning from Delayed Feedback in Games via Extra Prediction",
        "link": "/arxiv/2509.22426",
        "arxiv_id": "2509.22426",
        "authors": "Yuma Fujimoto, Kenshi Abe, Kaito Ariu",
        "summary": "This study raises and addresses the problem of time-delayed feedback in learning in games. Because learning in games assumes that multiple agents independently learn their strategies, a discrepancy in optimization often emerges among the agents. To overcome this discrepancy, the prediction of the future reward is incorporated into algorithms, typically known as Optimistic Follow-the-Regularized-Leader (OFTRL). However, the time delay in observing the past rewards hinders the prediction. Indeed, this study firstly proves that even a single-step delay worsens the performance of OFTRL from the aspects of regret and convergence. This study proposes the weighted OFTRL (WOFTRL), where the prediction vector of the next reward in OFTRL is weighted $n$ times. We further capture an intuition that the optimistic weight cancels out this time delay. We prove that when the optimistic weight exceeds the time delay, our WOFTRL recovers the good performances that the regret is constant ($O(1)$-regret) in general-sum normal-form games, and the strategies converge to the Nash equilibrium as a subsequence (best-iterate convergence) in poly-matrix zero-sum games. The theoretical results are supported and strengthened by our experiments.",
        "subjects": "Machine Learning, Computer Science and Game Theory, Multiagent Systems, Optimization and Control",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.534713"
    },
    {
        "index": "#10",
        "title": "Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach",
        "link": "/arxiv/2509.22137",
        "arxiv_id": "2509.22137",
        "authors": "Seoyoung Lee, Seonbin Yoon, Seongbeen Lee, Hyesoo Kim, Joo Yong Sim",
        "summary": "GUI task automation streamlines repetitive tasks, but existing LLM or VLM-based planner-executor agents suffer from brittle generalization, high latency, and limited long-horizon coherence. Their reliance on single-shot reasoning or static plans makes them fragile under UI changes or complex tasks. Log2Plan addresses these limitations by combining a structured two-level planning framework with a task mining approach over user behavior logs, enabling robust and adaptable GUI automation. Log2Plan constructs high-level plans by mapping user commands to a structured task dictionary, enabling consistent and generalizable automation. To support personalization and reuse, it employs a task mining approach from user behavior logs that identifies user-specific patterns. These high-level plans are then grounded into low-level action sequences by interpreting real-time GUI context, ensuring robust execution across varying interfaces. We evaluated Log2Plan on 200 real-world tasks, demonstrating significant improvements in task success rate and execution time. Notably, it maintains over 60.0% success rate even on long-horizon task sequences, highlighting its robustness in complex, multi-step workflows.",
        "subjects": "Artificial Intelligence, Human-Computer Interaction, Multiagent Systems, Robotics",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.534998"
    },
    {
        "index": "#11",
        "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration",
        "link": "/arxiv/2509.21981",
        "arxiv_id": "2509.21981",
        "authors": "Zhimin Wang, Shaokang He, Duo Wu, Jinghe Wang, Linjia Kang, Jing Yu, Zhi Wang",
        "summary": "Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs) have emerged as promising autonomous agents for collaborative task solving. However, existing collaboration frameworks for LLMs overlook their reasoning potential for dynamic intent inference, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. To bridge this gap, we propose CoBel-World, a novel framework that equips LLM agents with a collaborative belief world -- an internal representation jointly modeling the physical environment and collaborators' mental states. CoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World significantly reduces communication costs by 22-60% and improves task completion efficiency by 4-28% compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.535276"
    },
    {
        "index": "#12",
        "title": "Reimagining Agent-based Modeling with Large Language Model Agents via Shachi",
        "link": "/arxiv/2509.21862",
        "arxiv_id": "2509.21862",
        "authors": "So Kuroki, Yingtao Tian, Kou Misaki, Takashi Ikegami, Takuya Akiba, Yujin Tang",
        "summary": "The study of emergent behaviors in large language model (LLM)-driven multi-agent systems is a critical research challenge, yet progress is limited by a lack of principled methodologies for controlled experimentation. To address this, we introduce Shachi, a formal methodology and modular framework that decomposes an agent's policy into core cognitive components: Configuration for intrinsic traits, Memory for contextual persistence, and Tools for expanded capabilities, all orchestrated by an LLM reasoning engine. This principled architecture moves beyond brittle, ad-hoc agent designs and enables the systematic analysis of how specific architectural choices influence collective behavior. We validate our methodology on a comprehensive 10-task benchmark and demonstrate its power through novel scientific inquiries. Critically, we establish the external validity of our approach by modeling a real-world U.S. tariff shock, showing that agent behaviors align with observed market reactions only when their cognitive architecture is appropriately configured with memory and tools. Our work provides a rigorous, open-source foundation for building and evaluating LLM agents, aimed at fostering more cumulative and scientifically grounded research.",
        "subjects": "Artificial Intelligence, Multiagent Systems, Social and Information Networks, General Economics",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.535570"
    },
    {
        "index": "#13",
        "title": "Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models",
        "link": "/arxiv/2509.21849",
        "arxiv_id": "2509.21849",
        "authors": "Ziqi Liu, Ziyang Zhou, Yilin Li, Haiyang Zhang, Yangbin Chen",
        "summary": "Empathetic response generation is a crucial task for creating more human-like and supportive conversational agents. However, existing methods face a core trade-off between the analytical depth of specialized models and the generative fluency of Large Language Models (LLMs). To address this, we propose TRACE, Task-decomposed Reasoning for Affective Communication and Empathy, a novel framework that models empathy as a structured cognitive process by decomposing the task into a pipeline for analysis and synthesis. By building a comprehensive understanding before generation, TRACE unites deep analysis with expressive generation. Experimental results show that our framework significantly outperforms strong baselines in both automatic and LLM-based evaluations, confirming that our structured decomposition is a promising paradigm for creating more capable and interpretable empathetic agents. Our code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.",
        "subjects": "Computation and Language, Multiagent Systems",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.535850"
    },
    {
        "index": "#14",
        "title": "Axiomatic Choice and the Decision-Evaluation Paradox",
        "link": "/arxiv/2509.21836",
        "arxiv_id": "2509.21836",
        "authors": "Ben Abramowitz, Nicholas Mattei",
        "summary": "We introduce a framework for modeling decisions with axioms that are statements about decisions, e.g., ethical constraints. Using our framework we define a taxonomy of decision axioms based on their structural properties and demonstrate a tension between the use of axioms to make decisions and the use of axioms to evaluate decisions which we call the Decision-Evaluation Paradox. We argue that the Decision-Evaluation Paradox arises with realistic axiom structures, and the paradox illuminates why one must be exceptionally careful when training models on decision data or applying axioms to make and evaluate decisions.",
        "subjects": "Artificial Intelligence, Multiagent Systems",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.536100"
    },
    {
        "index": "#15",
        "title": "Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning",
        "link": "/arxiv/2509.21828",
        "arxiv_id": "2509.21828",
        "authors": "The Viet Bui, Tien Mai, Hong Thanh Nguyen",
        "summary": "We study the problem of online multi-agent reinforcement learning (MARL) in environments with sparse rewards, where reward feedback is not provided at each interaction but only revealed at the end of a trajectory. This setting, though realistic, presents a fundamental challenge: the lack of intermediate rewards hinders standard MARL algorithms from effectively guiding policy learning. To address this issue, we propose a novel framework that integrates online inverse preference learning with multi-agent on-policy optimization into a unified architecture. At its core, our approach introduces an implicit multi-agent reward learning model, built upon a preference-based value-decomposition network, which produces both global and local reward signals. These signals are further used to construct dual advantage streams, enabling differentiated learning targets for the centralized critic and decentralized actors. In addition, we demonstrate how large language models (LLMs) can be leveraged to provide preference labels that enhance the quality of the learned reward model. Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and SMACv2, show that our method achieves superior performance compared to existing baselines, highlighting its effectiveness in addressing sparse-reward challenges in online MARL.",
        "subjects": "Machine Learning, Multiagent Systems",
        "date": "2025-09-26",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.536352"
    },
    {
        "index": "#16",
        "title": "Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective",
        "link": "/arxiv/2509.21613",
        "arxiv_id": "2509.21613",
        "authors": "Lingxiao Kong, Cong Yang, Oya Deniz Beyan, Zeyd Boukhers",
        "summary": "Multi-Objective Reinforcement Learning (MORL) presents significant challenges and opportunities for optimizing multiple objectives in Large Language Models (LLMs). We introduce a MORL taxonomy and examine the advantages and limitations of various MORL methods when applied to LLM optimization, identifying the need for efficient and flexible approaches that accommodate personalization functionality and inherent complexities in LLMs and RL. We propose a vision for a MORL benchmarking framework that addresses the effects of different methods on diverse objective relationships. As future research directions, we focus on meta-policy MORL development that can improve efficiency and flexibility through its bi-level learning paradigm, highlighting key research questions and potential solutions for improving LLM performance.",
        "subjects": "Computation and Language, Artificial Intelligence, Machine Learning, Multiagent Systems",
        "date": "2025-09-25",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.536636"
    },
    {
        "index": "#17",
        "title": "AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need",
        "link": "/arxiv/2509.21553",
        "arxiv_id": "2509.21553",
        "authors": "Ahmed Jaber, Wangshu Zhu, Karthick Jayavelu, Justin Downes, Sameer Mohamed, Candace Agonafir, Linnia Hawkins, Tian Zheng",
        "summary": "Climate data science faces persistent barriers stemming from the fragmented nature of data sources, heterogeneous formats, and the steep technical expertise required to identify, acquire, and process datasets. These challenges limit participation, slow discovery, and reduce the reproducibility of scientific workflows. In this paper, we present a proof of concept for addressing these barriers through the integration of a curated knowledge graph (KG) with AI agents designed for cloud-native scientific workflows. The KG provides a unifying layer that organizes datasets, tools, and workflows, while AI agents -- powered by generative AI services -- enable natural language interaction, automated data access, and streamlined analysis. Together, these components drastically lower the technical threshold for engaging in climate data science, enabling non-specialist users to identify and analyze relevant datasets. By leveraging existing cloud-ready API data portals, we demonstrate that \"a knowledge graph is all you need\" to unlock scalable and agentic workflows for scientific inquiry. The open-source design of our system further supports community contributions, ensuring that the KG and associated tools can evolve as a shared commons. Our results illustrate a pathway toward democratizing access to climate data and establishing a reproducible, extensible framework for human--AI collaboration in scientific research.",
        "subjects": "Artificial Intelligence, Computational Engineering, Finance, and Science, Human-Computer Interaction, Machine Learning, Multiagent Systems",
        "date": "2025-09-25",
        "category": "cs.MA",
        "crawl_time": "2025-10-06T22:42:00.536952"
    }
]