[
    {
        "index": "#1",
        "title": "Generative AI Is Not Ready for Clinical Use in Patient Education for Lower Back Pain Patients, Even With Retrieval-Augmented Generation",
        "link": "/arxiv/2409.15260",
        "arxiv_id": "2409.15260",
        "authors": "Yi-Fei Zhao, Allyn Bove, David Thompson, James Hill, Yi Xu, Yufan Ren, Andrea Hassman, Leming Zhou, Yanshan Wang",
        "summary": "Low back pain (LBP) is a leading cause of disability globally. Following the onset of LBP and subsequent treatment, adequate patient education is crucial for improving functionality and long-term outcomes. Despite advancements in patient education strategies, significant gaps persist in delivering personalized, evidence-based information to patients with LBP. Recent advancements in large language models (LLMs) and generative artificial intelligence (GenAI) have demonstrated the potential to enhance patient education. However, their application and efficacy in delivering educational content to patients with LBP remain underexplored and warrant further investigation. In this study, we introduce a novel approach utilizing LLMs with Retrieval-Augmented Generation (RAG) and few-shot learning to generate tailored educational materials for patients with LBP. Physical therapists manually evaluated our model responses for redundancy, accuracy, and completeness using a Likert scale. In addition, the readability of the generated education materials is assessed using the Flesch Reading Ease score. The findings demonstrate that RAG-based LLMs outperform traditional LLMs, providing more accurate, complete, and readable patient education materials with less redundancy. Having said that, our analysis reveals that the generated materials are not yet ready for use in clinical practice. This study underscores the potential of AI-driven models utilizing RAG to improve patient education for LBP; however, significant challenges remain in ensuring the clinical relevance and granularity of content generated by these models.",
        "subjects": "Artificial Intelligence, Information Retrieval",
        "date": "2024-09-23 17:56:08 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.745429"
    },
    {
        "index": "#2",
        "title": "MACeIP: A Multimodal Ambient Context-enriched Intelligence Platform in Smart Cities",
        "link": "/arxiv/2409.15243",
        "arxiv_id": "2409.15243",
        "authors": "Truong Thanh Hung Nguyen, Phuc Truong Loc Nguyen, Monica Wachowicz, Hung Cao",
        "summary": "This paper presents a Multimodal Ambient Context-enriched Intelligence Platform (MACeIP) for Smart Cities, a comprehensive system designed to enhance urban management and citizen engagement. Our platform integrates advanced technologies, including Internet of Things (IoT) sensors, edge and cloud computing, and Multimodal AI, to create a responsive and intelligent urban ecosystem. Key components include Interactive Hubs for citizen interaction, an extensive IoT sensor network, intelligent public asset management, a pedestrian monitoring system, a City Planning Portal, and a Cloud Computing System. We demonstrate the prototype of MACeIP in several cities, focusing on Fredericton, New Brunswick. This work contributes to innovative city development by offering a scalable, efficient, and user-centric approach to urban intelligence and management.",
        "subjects": "Artificial Intelligence, Emerging Technologies, Human-Computer Interaction",
        "date": "2024-09-23 17:39:53 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.745794"
    },
    {
        "index": "#3",
        "title": "Chattronics: using GPTs to assist in the design of data acquisition systems",
        "link": "/arxiv/2409.15183",
        "arxiv_id": "2409.15183",
        "authors": "Jonathan Paul Driemeyer Brown, Tiago Oliveira Weber",
        "summary": "The usefulness of Large Language Models (LLM) is being continuously tested in various fields. However, their intrinsic linguistic characteristic is still one of the limiting factors when applying these models to exact sciences. In this article, a novel approach to use General Pre-Trained Transformers to assist in the design phase of data acquisition systems will be presented. The solution is packaged in the form of an application that retains the conversational aspects of LLMs, in such a manner that the user must provide details on the desired project in order for the model to draft both a system-level architectural diagram and the block-level specifications, following a Top-Down methodology based on restrictions. To test this tool, two distinct user emulations were used, one of which uses an additional GPT model. In total, 4 different data acquisition projects were used in the testing phase, each with its own measurement requirements: angular position, temperature, acceleration and a fourth project with both pressure and superficial temperature measurements. After 160 test iterations, the study concludes that there is potential for these models to serve adequately as synthesis/assistant tools for data acquisition systems, but there are still technological limitations. The results show coherent architectures and topologies, but that GPTs have difficulties in simultaneously considering all requirements and many times commits theoretical mistakes.",
        "subjects": "Artificial Intelligence, Hardware Architecture, Signal Processing",
        "date": "2024-09-23 16:36:16 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.746076"
    },
    {
        "index": "#4",
        "title": "Goal-based Neural Physics Vehicle Trajectory Prediction Model",
        "link": "/arxiv/2409.15182",
        "arxiv_id": "2409.15182",
        "authors": "Rui Gan, Haotian Shi, Pei Li, Keshu Wu, Bocheng An, Linheng Li, Junyi Ma, Chengyuan Ma, Bin Ran",
        "summary": "Vehicle trajectory prediction plays a vital role in intelligent transportation systems and autonomous driving, as it significantly affects vehicle behavior planning and control, thereby influencing traffic safety and efficiency. Numerous studies have been conducted to predict short-term vehicle trajectories in the immediate future. However, long-term trajectory prediction remains a major challenge due to accumulated errors and uncertainties. Additionally, balancing accuracy with interpretability in the prediction is another challenging issue in predicting vehicle trajectory. To address these challenges, this paper proposes a Goal-based Neural Physics Vehicle Trajectory Prediction Model (GNP). The GNP model simplifies vehicle trajectory prediction into a two-stage process: determining the vehicle's goal and then choosing the appropriate trajectory to reach this goal. The GNP model contains two sub-modules to achieve this process. The first sub-module employs a multi-head attention mechanism to accurately predict goals. The second sub-module integrates a deep learning model with a physics-based social force model to progressively predict the complete trajectory using the generated goals. The GNP demonstrates state-of-the-art long-term prediction accuracy compared to four baseline models. We provide interpretable visualization results to highlight the multi-modality and inherent nature of our neural physics framework. Additionally, ablation studies are performed to validate the effectiveness of our key designs.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 16:35:43 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.746304"
    },
    {
        "index": "#5",
        "title": "Automatic Feature Learning for Essence: a Case Study on Car Sequencing",
        "link": "/arxiv/2409.15158",
        "arxiv_id": "2409.15158",
        "authors": "Alessio Pellegrino, Özgür Akgün, Nguyen Dang, Zeynep Kiziltan, Ian Miguel",
        "summary": "Constraint modelling languages such as Essence offer a means to describe combinatorial problems at a high-level, i.e., without committing to detailed modelling decisions for a particular solver or solving paradigm. Given a problem description written in Essence, there are multiple ways to translate it to a low-level constraint model. Choosing the right combination of a low-level constraint model and a target constraint solver can have significant impact on the effectiveness of the solving process. Furthermore, the choice of the best combination of constraint model and solver can be instance-dependent, i.e., there may not exist a single combination that works best for all instances of the same problem. In this paper, we consider the task of building machine learning models to automatically select the best combination for a problem instance. A critical part of the learning process is to define instance features, which serve as input to the selection model. Our contribution is automatic learning of instance features directly from the high-level representation of a problem instance using a language model. We evaluate the performance of our approach using the Essence modelling language with a case study involving the car sequencing problem.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 16:06:44 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.746525"
    },
    {
        "index": "#6",
        "title": "Boosting Healthcare LLMs Through Retrieved Context",
        "link": "/arxiv/2409.15127",
        "arxiv_id": "2409.15127",
        "authors": "Jordi Bayarri-Planas, Ashwin Kumar Gururajan, Dario Garcia-Gasulla",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, and yet, their factual inaccuracies and hallucinations limits their application, particularly in critical domains like healthcare. Context retrieval methods, by introducing relevant information as input, have emerged as a crucial approach for enhancing LLM factuality and reliability. This study explores the boundaries of context retrieval methods within the healthcare domain, optimizing their components and benchmarking their performance against open and closed alternatives. Our findings reveal how open LLMs, when augmented with an optimized retrieval system, can achieve performance comparable to the biggest private solutions on established healthcare benchmarks (multiple-choice question answering). Recognizing the lack of realism of including the possible answers within the question (a setup only found in medical exams), and after assessing a strong LLM performance degradation in the absence of those options, we extend the context retrieval system in that direction. In particular, we propose OpenMedPrompt a pipeline that improves the generation of more reliable open-ended answers, moving this technology closer to practical application.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 15:33:38 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.746745"
    },
    {
        "index": "#7",
        "title": "Log-normal Mutations and their Use in Detecting Surreptitious Fake Images",
        "link": "/arxiv/2409.15119",
        "arxiv_id": "2409.15119",
        "authors": "Ismail Labiad, Thomas Bäck, Pierre Fernandez, Laurent Najman, Tom Sanders, Furong Ye, Mariia Zameshina, Olivier Teytaud",
        "summary": "In many cases, adversarial attacks are based on specialized algorithms specifically dedicated to attacking automatic image classifiers. These algorithms perform well, thanks to an excellent ad hoc distribution of initial attacks. However, these attacks are easily detected due to their specific initial distribution. We therefore consider other black-box attacks, inspired from generic black-box optimization tools, and in particular the log-normal algorithm. We apply the log-normal method to the attack of fake detectors, and get successful attacks: importantly, these attacks are not detected by detectors specialized on classical adversarial attacks. Then, combining these attacks and deep detection, we create improved fake detectors.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 15:25:26 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.746956"
    },
    {
        "index": "#8",
        "title": "Evaluating ML Robustness in GNSS Interference Classification, Characterization \\& Localization",
        "link": "/arxiv/2409.15114",
        "arxiv_id": "2409.15114",
        "authors": "Lucas Heublein, Tobias Feigl, Thorsten Nowak, Alexander Rügamer, Christopher Mutschler, Felix Ott",
        "summary": "Jamming devices present a significant threat by disrupting signals from the global navigation satellite system (GNSS), compromising the robustness of accurate positioning. The detection of anomalies within frequency snapshots is crucial to counteract these interferences effectively. A critical preliminary measure involves the reliable classification of interferences and characterization and localization of jamming devices. This paper introduces an extensive dataset compromising snapshots obtained from a low-frequency antenna, capturing diverse generated interferences within a large-scale environment including controlled multipath effects. Our objective is to assess the resilience of ML models against environmental changes, such as multipath effects, variations in interference attributes, such as the interference class, bandwidth, and signal-to-noise ratio, the accuracy jamming device localization, and the constraints imposed by snapshot input lengths. By analyzing the aleatoric and epistemic uncertainties, we demonstrate the adaptness of our model in generalizing across diverse facets, thus establishing its suitability for real-world applications. https://gitlab.cc-asp.fraunhofer.de/darcy_gnss/controlled_low_frequency",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 15:20:33 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.747149"
    },
    {
        "index": "#9",
        "title": "ChatGPT as a Solver and Grader of Programming Exams written in Spanish",
        "link": "/arxiv/2409.15112",
        "arxiv_id": "2409.15112",
        "authors": "Pablo Fernández-Saborido, Marcos Fernández-Pichel, David E. Losada",
        "summary": "Evaluating the capabilities of Large Language Models (LLMs) to assist teachers and students in educational tasks is receiving increasing attention. In this paper, we assess ChatGPT's capacities to solve and grade real programming exams, from an accredited BSc degree in Computer Science, written in Spanish. Our findings suggest that this AI model is only effective for solving simple coding tasks. Its proficiency in tackling complex problems or evaluating solutions authored by others are far from effective. As part of this research, we also release a new corpus of programming tasks and the corresponding prompts for solving the problems or grading the solutions. This resource can be further exploited by other research teams.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 15:20:07 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.747320"
    },
    {
        "index": "#10",
        "title": "SPformer: A Transformer Based DRL Decision Making Method for Connected Automated Vehicles",
        "link": "/arxiv/2409.15105",
        "arxiv_id": "2409.15105",
        "authors": "Ye Han, Lijun Zhang, Dejian Meng, Xingyu Hu, Yixia Lu",
        "summary": "In mixed autonomy traffic environment, every decision made by an autonomous-driving car may have a great impact on the transportation system. Because of the complex interaction between vehicles, it is challenging to make decisions that can ensure both high traffic efficiency and safety now and futher. Connected automated vehicles (CAVs) have great potential to improve the quality of decision-making in this continuous, highly dynamic and interactive environment because of their stronger sensing and communicating ability. For multi-vehicle collaborative decision-making algorithms based on deep reinforcement learning (DRL), we need to represent the interactions between vehicles to obtain interactive features. The representation in this aspect directly affects the learning efficiency and the quality of the learned policy. To this end, we propose a CAV decision-making architecture based on transformer and reinforcement learning algorithms. A learnable policy token is used as the learning medium of the multi-vehicle joint policy, the states of all vehicles in the area of interest can be adaptively noticed in order to extract interactive features among agents. We also design an intuitive physical positional encodings, the redundant location information of which optimizes the performance of the network. Simulations show that our model can make good use of all the state information of vehicles in traffic scenario, so as to obtain high-quality driving decisions that meet efficiency and safety objectives. The comparison shows that our method significantly improves existing DRL-based multi-vehicle cooperative decision-making algorithms.",
        "subjects": "Artificial Intelligence, Multiagent Systems, Systems and Control",
        "date": "2024-09-23 15:16:35 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.747528"
    },
    {
        "index": "#11",
        "title": "Acting for the Right Reasons: Creating Reason-Sensitive Artificial Moral Agents",
        "link": "/arxiv/2409.15014",
        "arxiv_id": "2409.15014",
        "authors": "Kevin Baum, Lisa Dargasz, Felix Jahn, Timo P. Gros, Verena Wolf",
        "summary": "We propose an extension of the reinforcement learning architecture that enables moral decision-making of reinforcement learning agents based on normative reasons. Central to this approach is a reason-based shield generator yielding a moral shield that binds the agent to actions that conform with recognized normative reasons so that our overall architecture restricts the agent to actions that are (internally) morally justified. In addition, we describe an algorithm that allows to iteratively improve the reason-based shield generator through case-based feedback from a moral judge.",
        "subjects": "Artificial Intelligence, Computers and Society, Machine Learning",
        "date": "2024-09-23 13:38:57 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.747734"
    },
    {
        "index": "#12",
        "title": "Analogous Alignments: Digital \"Formally\" meets Analog",
        "link": "/arxiv/2409.15013",
        "arxiv_id": "2409.15013",
        "authors": "Hansa Mohanty, Deepak Narayan Gadde",
        "summary": "The complexity of modern-day System-on-Chips (SoCs) is continually increasing, and it becomes increasingly challenging to deliver dependable and credible chips in a short time-to-market. Especially, in the case of test chips, where the aim is to study the feasibility of the design, time is a crucial factor. Pre-silicon functional verification is one of the main contributors that makes up a large portion of the product development cycle. Verification engineers often loosely verify test chips that turn out to be non-functional on the silicon, ultimately resulting in expensive re-spins. To left-shift the verification efforts, formal verification is a powerful methodology that aims to exhaustively verify designs, giving better confidence in the overall quality. This paper focuses on the pragmatic formal verification of a mixed signal Intellectual Property (IP) that has a combination of digital and analog blocks. This paper discusses a novel approach of including the analog behavioral model into the formal verification setup. Digital and Analog Mixed-Signal (AMS) designs, which are fundamentally different in nature, are integrated seamlessly in a formal verification setup, a concept that can be referred to as \"Analogous Alignments\". Our formal setup leverages powerful formal techniques such as FPV, CSR verification, and connectivity checks. The properties used for FPV are auto-generated using a metamodeling framework. The paper also discusses the challenges faced especially related to state-space explosion, non-compatibility of formal with AMS models, and techniques to mitigate them such as k-induction. With this verification approach, we were able to exhaustively verify the design within a reasonable time and with sufficient coverage. We also reported several bugs at an early stage, making the complete design verification process iterative and effective.",
        "subjects": "Artificial Intelligence, Hardware Architecture",
        "date": "2024-09-23 13:38:31 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.747911"
    },
    {
        "index": "#15",
        "title": "TS-TCD: Triplet-Level Cross-Modal Distillation for Time-Series Forecasting Using Large Language Models",
        "link": "/arxiv/2409.14978",
        "arxiv_id": "2409.14978",
        "authors": "Pengfei Wang, Huanran Zheng, Silong Dai, Wenjing Yue, Wei Zhu, Xiaoling Wang",
        "summary": "In recent years, large language models (LLMs) have shown great potential in time-series analysis by capturing complex dependencies and improving predictive performance. However, existing approaches often struggle with modality alignment, leading to suboptimal results. To address these challenges, we present a novel framework, TS-TCD, which introduces a comprehensive three-tiered cross-modal knowledge distillation mechanism. Unlike prior work that focuses on isolated alignment techniques, our framework systematically integrates: 1) Dynamic Adaptive Gating for Input Encoding and Alignment}, ensuring coherent alignment between time-series tokens and QR-decomposed textual embeddings; 2) Layer-Wise Contrastive Learning}, aligning intermediate representations across modalities to reduce feature-level discrepancies; and 3) Optimal Transport-Driven Output Alignment}, which ensures consistent output predictions through fine-grained cross-modal alignment. Extensive experiments on benchmark time-series datasets demonstrate that TS-TCD achieves state-of-the-art results, outperforming traditional methods in both accuracy and robustness.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 12:57:24 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.748556"
    },
    {
        "index": "#17",
        "title": "HW-TSC's Submission to the CCMT 2024 Machine Translation Tasks",
        "link": "/arxiv/2409.14842",
        "arxiv_id": "2409.14842",
        "authors": "Zhanglin Wu, Yuanchang Luo, Daimeng Wei, Jiawei Zheng, Bin Wei, Zongyao Li, Hengchao Shang, Jiaxin Guo, Shaojun Li, Weidong Zhang, Ning Xie, Hao Yang",
        "summary": "This paper presents the submission of Huawei Translation Services Center (HW-TSC) to machine translation tasks of the 20th China Conference on Machine Translation (CCMT 2024). We participate in the bilingual machine translation task and multi-domain machine translation task. For these two translation tasks, we use training strategies such as regularized dropout, bidirectional training, data diversification, forward translation, back translation, alternated training, curriculum learning, and transductive ensemble learning to train neural machine translation (NMT) models based on the deep Transformer-big architecture. Furthermore, to explore whether large language model (LLM) can help improve the translation quality of NMT systems, we use supervised fine-tuning to train llama2-13b as an Automatic post-editing (APE) model to improve the translation results of the NMT model on the multi-domain machine translation task. By using these plyometric strategies, our submission achieves a competitive result in the final evaluation.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 09:20:19 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.749184"
    },
    {
        "index": "#18",
        "title": "Explainable and Human-Grounded AI for Decision Support Systems: The Theory of Epistemic Quasi-Partnerships",
        "link": "/arxiv/2409.14839",
        "arxiv_id": "2409.14839",
        "authors": "John Dorsch, Maximilian Moll",
        "summary": "In the context of AI decision support systems (AI-DSS), we argue that meeting the demands of ethical and explainable AI (XAI) is about developing AI-DSS to provide human decision-makers with three types of human-grounded explanations: reasons, counterfactuals, and confidence, an approach we refer to as the RCC approach. We begin by reviewing current empirical XAI literature that investigates the relationship between various methods for generating model explanations (e.g., LIME, SHAP, Anchors), the perceived trustworthiness of the model, and end-user accuracy. We demonstrate how current theories about what constitutes good human-grounded reasons either do not adequately explain this evidence or do not offer sound ethical advice for development. Thus, we offer a novel theory of human-machine interaction: the theory of epistemic quasi-partnerships (EQP). Finally, we motivate adopting EQP and demonstrate how it explains the empirical evidence, offers sound ethical advice, and entails adopting the RCC approach.",
        "subjects": "Artificial Intelligence, Emerging Technologies, Human-Computer Interaction",
        "date": "2024-09-23 09:14:25 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.749368"
    },
    {
        "index": "#19",
        "title": "MICSim: A Modular Simulator for Mixed-signal Compute-in-Memory based AI Accelerator",
        "link": "/arxiv/2409.14838",
        "arxiv_id": "2409.14838",
        "authors": "Cong Wang, Zeming Chen, Shanshi Huang",
        "summary": "This work introduces MICSim, an open-source, pre-circuit simulator designed for early-stage evaluation of chip-level software performance and hardware overhead of mixed-signal compute-in-memory (CIM) accelerators. MICSim features a modular design, allowing easy multi-level co-design and design space exploration. Modularized from the state-of-the-art CIM simulator NeuroSim, MICSim provides a highly configurable simulation framework supporting multiple quantization algorithms, diverse circuit/architecture designs, and different memory devices. This modular approach also allows MICSim to be effectively extended to accommodate new designs. MICSim natively supports evaluating accelerators' software and hardware performance for CNNs and Transformers in Python, leveraging the popular PyTorch and HuggingFace Transformers frameworks. These capabilities make MICSim highly adaptive when simulating different networks and user-friendly. This work demonstrates that MICSim can easily be combined with optimization strategies to perform design space exploration and used for chip-level Transformers CIM accelerators evaluation. Also, MICSim can achieve a 9x - 32x speedup of NeuroSim through a statistic-based average mode proposed by this work.",
        "subjects": "Artificial Intelligence, Hardware Architecture",
        "date": "2024-09-23 09:12:46 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.749557"
    },
    {
        "index": "#20",
        "title": "Benchmarking Edge AI Platforms for High-Performance ML Inference",
        "link": "/arxiv/2409.14803",
        "arxiv_id": "2409.14803",
        "authors": "Rakshith Jayanth, Neelesh Gupta, Viktor Prasanna",
        "summary": "Edge computing's growing prominence, due to its ability to reduce communication latency and enable real-time processing, is promoting the rise of high-performance, heterogeneous System-on-Chip solutions. While current approaches often involve scaling down modern hardware, the performance characteristics of neural network workloads on these platforms can vary significantly, especially when it comes to parallel processing, which is a critical consideration for edge deployments. To address this, we conduct a comprehensive study comparing the latency and throughput of various linear algebra and neural network inference tasks across CPU-only, CPU/GPU, and CPU/NPU integrated solutions. {We find that the Neural Processing Unit (NPU) excels in matrix-vector multiplication (58.6% faster) and some neural network tasks (3.2$\\times$ faster for video classification and large language models). GPU outperforms in matrix multiplication (22.6% faster) and LSTM networks (2.7$\\times$ faster) while CPU excels at less parallel operations like dot product. NPU-based inference offers a balance of latency and throughput at lower power consumption. GPU-based inference, though more energy-intensive, performs best with large dimensions and batch sizes. We highlight the potential of heterogeneous computing solutions for edge AI, where diverse compute units can be strategically leveraged to boost accurate and real-time inference.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 08:27:27 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.749733"
    },
    {
        "index": "#21",
        "title": "Choose the Final Translation from NMT and LLM hypotheses Using MBR Decoding: HW-TSC's Submission to the WMT24 General MT Shared Task",
        "link": "/arxiv/2409.14800",
        "arxiv_id": "2409.14800",
        "authors": "Zhanglin Wu, Daimeng Wei, Zongyao Li, Hengchao Shang, Jiaxin Guo, Shaojun Li, Zhiqiang Rao, Yuanchang Luo, Ning Xie, Hao Yang",
        "summary": "This paper presents the submission of Huawei Translate Services Center (HW-TSC) to the WMT24 general machine translation (MT) shared task, where we participate in the English to Chinese (en2zh) language pair. Similar to previous years' work, we use training strategies such as regularized dropout, bidirectional training, data diversification, forward translation, back translation, alternated training, curriculum learning, and transductive ensemble learning to train the neural machine translation (NMT) model based on the deep Transformer-big architecture. The difference is that we also use continue pre-training, supervised fine-tuning, and contrastive preference optimization to train the large language model (LLM) based MT model. By using Minimum Bayesian risk (MBR) decoding to select the final translation from multiple hypotheses for NMT and LLM-based MT models, our submission receives competitive results in the final evaluation.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 08:25:37 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.749949"
    },
    {
        "index": "#22",
        "title": "SAMEdge: An Edge-cloud Video Analytics Architecture for the Segment Anything Model",
        "link": "/arxiv/2409.14784",
        "arxiv_id": "2409.14784",
        "authors": "Rui Lu, Siping Shi, Yanting Liu, Dan Wang",
        "summary": "As artificial intelligence continues to evolve, it is increasingly capable of handling a wide range of video analytics tasks with merely one large model. One of the key foundation technologies is the Segment Anything Model (SAM), which allows the video analytics tasks to be determined on the fly according to the input prompts from the user. However, achieving real-time response in video analytics applications is crucial for user experiences due to the limited communication and computation resources on the edge, especially with SAM, where users may continuously interact by adding or adjusting prompts. In this paper, we propose SAMEdge, a novel edge-cloud computing architecture designed to support SAM computations for edge users. SAMEdge integrates new modules on the edge and the cloud to maximize analytics accuracy under visual prompts and image prompts input with latency constraints. It addresses resource challenges associated with prompt encoding and image encoding by offering a visual prompt transformation algorithm for visual prompts and efficient workload partitioning for image encoding. SAMEdge is implemented by extending the open-source SAM project from Meta AI. We demonstrate the practical application of SAMEdge through a case study on a Visual Tour Guide application. Our evaluation indicates that SAMEdge significantly enhances the accuracy of the video analytics application under distinct network bandwidths across various prompts.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 07:59:09 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.755397"
    },
    {
        "index": "#23",
        "title": "Speechworthy Instruction-tuned Language Models",
        "link": "/arxiv/2409.14672",
        "arxiv_id": "2409.14672",
        "authors": "Hyundong Cho, Nicolaas Jedema, Leonardo F. R. Ribeiro, Karishma Sharma, Pedro Szekely, Alessandro Moschitti, Ruben Janssen, Jonathan May",
        "summary": "Current instruction-tuned language models are exclusively trained with textual preference data and thus are often not aligned with the unique requirements of other modalities, such as speech. To better align language models with the speech domain, we explore (i) prompting strategies grounded in radio-industry best practices and (ii) preference learning using a novel speech-based preference data of 20K samples, generated with a wide spectrum of prompts that induce varying dimensions of speech-suitability and labeled by annotators who listen to response pairs. Both human and automatic evaluation show that both prompting and preference learning increase the speech-suitability of popular instruction-tuned LLMs. Interestingly, we find that prompting and preference learning can be additive; combining them achieves the best win rates in head-to-head comparison, resulting in responses that are preferred or tied to the base model in 76.2% of comparisons on average. Lastly, we share lexical, syntactical, and qualitative analyses to showcase how each method contributes to improving the speech-suitability of generated responses.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 02:34:42 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.755653"
    },
    {
        "index": "#25",
        "title": "Semi-supervised Learning For Robust Speech Evaluation",
        "link": "/arxiv/2409.14666",
        "arxiv_id": "2409.14666",
        "authors": "Huayun Zhang, Jeremy H. M. Wong, Geyu Lin, Nancy F. Chen",
        "summary": "Speech evaluation measures a learners oral proficiency using automatic models. Corpora for training such models often pose sparsity challenges given that there often is limited scored data from teachers, in addition to the score distribution across proficiency levels being often imbalanced among student cohorts. Automatic scoring is thus not robust when faced with under-represented samples or out-of-distribution samples, which inevitably exist in real-world deployment scenarios. This paper proposes to address such challenges by exploiting semi-supervised pre-training and objective regularization to approximate subjective evaluation criteria. In particular, normalized mutual information is used to quantify the speech characteristics from the learner and the reference. An anchor model is trained using pseudo labels to predict the correctness of pronunciation. An interpolated loss function is proposed to minimize not only the prediction error with respect to ground-truth scores but also the divergence between two probability distributions estimated by the speech evaluation model and the anchor model. Compared to other state-of-the-art methods on a public data-set, this approach not only achieves high performance while evaluating the entire test-set as a whole, but also brings the most evenly distributed prediction error across distinct proficiency levels. Furthermore, empirical results show the model accuracy on out-of-distribution data also compares favorably with competitive baselines.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-23 02:11:24 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.756046"
    },
    {
        "index": "#26",
        "title": "Brain Surgery: Ensuring GDPR Compliance in Large Language Models via Concept Erasure",
        "link": "/arxiv/2409.14603",
        "arxiv_id": "2409.14603",
        "authors": "Michele Laurelli",
        "summary": "As large-scale AI systems proliferate, ensuring compliance with data privacy laws such as the General Data Protection Regulation (GDPR) has become critical. This paper introduces Brain Surgery, a transformative methodology for making every local AI model GDPR-ready by enabling real-time privacy management and targeted unlearning. Building on advanced techniques such as Embedding-Corrupted Prompts (ECO Prompts), blockchain-based privacy management, and privacy-aware continual learning, Brain Surgery provides a modular solution that can be deployed across various AI architectures. This tool not only ensures compliance with privacy regulations but also empowers users to define their own privacy limits, creating a new paradigm in AI ethics and governance.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 21:42:20 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.756212"
    },
    {
        "index": "#27",
        "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
        "link": "/arxiv/2409.14583",
        "arxiv_id": "2409.14583",
        "authors": "Vishal Mirza, Rahul Kulkarni, Aakanksha Jadhav",
        "summary": "Recent advancements in Large Language Models(LLMs) have been notable, yet widespread enterprise adoption remains limited due to various constraints. This paper examines bias in LLMs-a crucial issue affecting their usability, reliability, and fairness. Researchers are developing strategies to mitigate bias, including debiasing layers, specialized reference datasets like Winogender and Winobias, and reinforcement learning with human feedback (RLHF). These techniques have been integrated into the latest LLMs. Our study evaluates gender bias in occupational scenarios and gender, age, and racial bias in crime scenarios across four leading LLMs released in 2024: Gemini 1.5 Pro, Llama 3 70B, Claude 3 Opus, and GPT-4o. Findings reveal that LLMs often depict female characters more frequently than male ones in various occupations, showing a 37% deviation from US BLS data. In crime scenarios, deviations from US FBI data are 54% for gender, 28% for race, and 17% for age. We observe that efforts to reduce gender and racial bias often lead to outcomes that may over-index one sub-class, potentially exacerbating the issue. These results highlight the limitations of current bias mitigation techniques and underscore the need for more effective approaches.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 20:21:20 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.756390"
    },
    {
        "index": "#28",
        "title": "Encoder with the Empirical Mode Decomposition (EMD) to remove muscle artefacts from EEG signal",
        "link": "/arxiv/2409.14571",
        "arxiv_id": "2409.14571",
        "authors": "Ildar Rakhmatulin",
        "summary": "This paper introduces a novel method for effectively removing artifacts from EEG signals by combining the Empirical Mode Decomposition (EMD) method with a machine learning architecture. The proposed method addresses the limitations of existing artifact removal techniques by enhancing the EMD method through interpolation of the upper and lower. For conventional artifact removal methods, the EMD technique is commonly employed. However, the challenge lies in accurately interpolating the missing components of the signal while preserving its inherent frequency components. To overcome this limitation, we incorporated machine learning technique, which enables us to carefully handle the interpolation process without directly manipulating the data. The key advantage of our approach lies in the preservation of the natural characteristics of the EEG signal during artifact removal. By utilizing machine learning for interpolation, we ensure that the average component obtained through the EMD method retains the crucial frequency components of the original signal. This preservation is essential for maintaining the integrity and fidelity of the EEG data, allowing for accurate analysis and interpretation. The results obtained from our evaluation serve to validate the effectiveness of our approach and pave the way for further advancements in EEG signal processing and analysis.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 19:22:22 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.756554"
    },
    {
        "index": "#29",
        "title": "Why Is Anything Conscious?",
        "link": "/arxiv/2409.14545",
        "arxiv_id": "2409.14545",
        "authors": "Michael Timothy Bennett, Sean Welsh, Anna Ciaunica",
        "summary": "We tackle the hard problem of consciousness taking the naturally-selected, self-organising, embodied organism as our starting point. We provide a mathematical formalism describing how biological systems self-organise to hierarchically interpret unlabelled sensory information according to valence and specific needs. Such interpretations imply behavioural policies which can only be differentiated from each other by the qualitative aspect of information processing. Selection pressures favour systems that can intervene in the world to achieve homeostatic and reproductive goals. Quality is a property arising in such systems to link cause to affect to motivate real world interventions. This produces a range of qualitative classifiers (interoceptive and exteroceptive) that motivate specific actions and determine priorities and preferences. Building upon the seminal distinction between access and phenomenal consciousness, our radical claim here is that phenomenal consciousness without access consciousness is likely very common, but the reverse is implausible. To put it provocatively: Nature does not like zombies. We formally describe the multilayered architecture of self-organisation from rocks to Einstein, illustrating how our argument applies in the real world. We claim that access consciousness at the human level is impossible without the ability to hierarchically model i) the self, ii) the world/others and iii) the self as modelled by others. Phenomenal consciousness is therefore required for human-level functionality. Our proposal lays the foundations of a formal science of consciousness, deeply connected with natural selection rather than abstract thinking, closer to human fact than zombie fiction.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 18:01:30 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.756730"
    },
    {
        "index": "#31",
        "title": "On a measure of intelligence",
        "link": "/arxiv/2409.14496",
        "arxiv_id": "2409.14496",
        "authors": "Yuri Gurevich",
        "summary": "The Fall 2024 Logic in Computer Science column of the Bulletin of EATCS is a little discussion on intelligence, measuring intelligence, and related issues, provoked by a fascinating must-read article ``On the measure of intelligence'' by François Chollet. The discussion includes a modicum of critique of the article.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 15:49:31 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.757187"
    },
    {
        "index": "#32",
        "title": "Can Large Language Models Logically Predict Myocardial Infarction? Evaluation based on UK Biobank Cohort",
        "link": "/arxiv/2409.14478",
        "arxiv_id": "2409.14478",
        "authors": "Yuxing Zhi, Yuan Guo, Kai Yuan, Hesong Wang, Heng Xu, Haina Yao, Albert C Yang, Guangrui Huang, Yuping Duan",
        "summary": "Background: Large language models (LLMs) have seen extraordinary advances with applications in clinical decision support. However, high-quality evidence is urgently needed on the potential and limitation of LLMs in providing accurate clinical decisions based on real-world medical data. Objective: To evaluate quantitatively whether universal state-of-the-art LLMs (ChatGPT and GPT-4) can predict the incidence risk of myocardial infarction (MI) with logical inference, and to further make comparison between various models to assess the performance of LLMs comprehensively. Methods: In this retrospective cohort study, 482,310 participants recruited from 2006 to 2010 were initially included in UK Biobank database and later on resampled into a final cohort of 690 participants. For each participant, tabular data of the risk factors of MI were transformed into standardized textual descriptions for ChatGPT recognition. Responses were generated by asking ChatGPT to select a score ranging from 0 to 10 representing the risk. Chain of Thought (CoT) questioning was used to evaluate whether LLMs make prediction logically. The predictive performance of ChatGPT was compared with published medical indices, traditional machine learning models and other large language models. Conclusions: Current LLMs are not ready to be applied in clinical medicine fields. Future medical LLMs are suggested to be expert in medical domain knowledge to understand both natural languages and quantified medical data, and further make logical inferences.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 14:57:31 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.757413"
    },
    {
        "index": "#33",
        "title": "On logic and generative AI",
        "link": "/arxiv/2409.14465",
        "arxiv_id": "2409.14465",
        "authors": "Yuri Gurevich, Andreas Blass",
        "summary": "A hundred years ago, logic was almost synonymous with foundational studies. The ongoing AI revolution raises many deep foundational problems involving neuroscience, philosophy, computer science, and logic. The goal of the following dialog is to provoke young logicians with a taste for foundations to notice the foundational problems raised by the AI revolution.",
        "subjects": "Artificial Intelligence, Logic in Computer Science",
        "date": "2024-09-22 14:31:58 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.757589"
    },
    {
        "index": "#34",
        "title": "Large Model Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends",
        "link": "/arxiv/2409.14457",
        "arxiv_id": "2409.14457",
        "authors": "Yuntao Wang, Yanghe Pan, Quan Zhao, Yi Deng, Zhou Su, Linkang Du, Tom H. Luan",
        "summary": "Large Model (LM) agents, powered by large foundation models such as GPT-4 and DALL-E 2, represent a significant step towards achieving Artificial General Intelligence (AGI). LM agents exhibit key characteristics of autonomy, embodiment, and connectivity, allowing them to operate across physical, virtual, and mixed-reality environments while interacting seamlessly with humans, other agents, and their surroundings. This paper provides a comprehensive survey of the state-of-the-art in LM agents, focusing on the architecture, cooperation paradigms, security, privacy, and future prospects. Specifically, we first explore the foundational principles of LM agents, including general architecture, key components, enabling technologies, and modern applications. Then, we discuss practical collaboration paradigms from data, computation, and knowledge perspectives towards connected intelligence of LM agents. Furthermore, we systematically analyze the security vulnerabilities and privacy breaches associated with LM agents, particularly in multi-agent settings. We also explore their underlying mechanisms and review existing and potential countermeasures. Finally, we outline future research directions for building robust and secure LM agent ecosystems.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 14:09:49 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.757792"
    },
    {
        "index": "#35",
        "title": "Scoring rule nets: beyond mean target prediction in multivariate regression",
        "link": "/arxiv/2409.14456",
        "arxiv_id": "2409.14456",
        "authors": "Daan Roordink, Sibylle Hess",
        "summary": "Probabilistic regression models trained with maximum likelihood estimation (MLE), can sometimes overestimate variance to an unacceptable degree. This is mostly problematic in the multivariate domain. While univariate models often optimize the popular Continuous Ranked Probability Score (CRPS), in the multivariate domain, no such alternative to MLE has yet been widely accepted. The Energy Score - the most investigated alternative - notoriously lacks closed-form expressions and sensitivity to the correlation between target variables. In this paper, we propose Conditional CRPS: a multivariate strictly proper scoring rule that extends CRPS. We show that closed-form expressions exist for popular distributions and illustrate their sensitivity to correlation. We then show in a variety of experiments on both synthetic and real data, that Conditional CRPS often outperforms MLE, and produces results comparable to state-of-the-art non-parametric models, such as Distributional Random Forest (DRF).",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 14:09:12 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.757963"
    },
    {
        "index": "#36",
        "title": "OStr-DARTS: Differentiable Neural Architecture Search based on Operation Strength",
        "link": "/arxiv/2409.14433",
        "arxiv_id": "2409.14433",
        "authors": "Le Yang, Ziwei Zheng, Yizeng Han, Shiji Song, Gao Huang, Fan Li",
        "summary": "Differentiable architecture search (DARTS) has emerged as a promising technique for effective neural architecture search, and it mainly contains two steps to find the high-performance architecture: First, the DARTS supernet that consists of mixed operations will be optimized via gradient descent. Second, the final architecture will be built by the selected operations that contribute the most to the supernet. Although DARTS improves the efficiency of NAS, it suffers from the well-known degeneration issue which can lead to deteriorating architectures. Existing works mainly attribute the degeneration issue to the failure of its supernet optimization, while little attention has been paid to the selection method. In this paper, we cease to apply the widely-used magnitude-based selection method and propose a novel criterion based on operation strength that estimates the importance of an operation by its effect on the final loss. We show that the degeneration issue can be effectively addressed by using the proposed criterion without any modification of supernet optimization, indicating that the magnitude-based selection method can be a critical reason for the instability of DARTS. The experiments on NAS-Bench-201 and DARTS search spaces show the effectiveness of our method.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 13:16:07 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.758159"
    },
    {
        "index": "#37",
        "title": "MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting",
        "link": "/arxiv/2409.14393",
        "arxiv_id": "2409.14393",
        "authors": "Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, Xue Bin Peng",
        "summary": "Crafting a single, versatile physics-based controller that can breathe life into interactive characters across a wide spectrum of scenarios represents an exciting frontier in character animation. An ideal controller should support diverse control modalities, such as sparse target keyframes, text instructions, and scene information. While previous works have proposed physically simulated, scene-aware control models, these systems have predominantly focused on developing controllers that each specializes in a narrow set of tasks and control modalities. This work presents MaskedMimic, a novel approach that formulates physics-based character control as a general motion inpainting problem. Our key insight is to train a single unified model to synthesize motions from partial (masked) motion descriptions, such as masked keyframes, objects, text descriptions, or any combination thereof. This is achieved by leveraging motion tracking data and designing a scalable training method that can effectively utilize diverse motion descriptions to produce coherent animations. Through this process, our approach learns a physics-based controller that provides an intuitive control interface without requiring tedious reward engineering for all behaviors of interest. The resulting controller supports a wide range of control modalities and enables seamless transitions between disparate tasks. By unifying character control through motion inpainting, MaskedMimic creates versatile virtual characters. These characters can dynamically adapt to complex scenes and compose diverse motions on demand, enabling more interactive and immersive experiences.",
        "subjects": "Artificial Intelligence, Robotics",
        "date": "2024-09-22 11:10:59 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.758351"
    },
    {
        "index": "#38",
        "title": "To Err Is AI! Debugging as an Intervention to Facilitate Appropriate Reliance on AI Systems",
        "link": "/arxiv/2409.14377",
        "arxiv_id": "2409.14377",
        "authors": "Gaole He, Abri Bharos, Ujwal Gadiraju",
        "summary": "Powerful predictive AI systems have demonstrated great potential in augmenting human decision making. Recent empirical work has argued that the vision for optimal human-AI collaboration requires 'appropriate reliance' of humans on AI systems. However, accurately estimating the trustworthiness of AI advice at the instance level is quite challenging, especially in the absence of performance feedback pertaining to the AI system. In practice, the performance disparity of machine learning models on out-of-distribution data makes the dataset-specific performance feedback unreliable in human-AI collaboration. Inspired by existing literature on critical thinking and a critical mindset, we propose the use of debugging an AI system as an intervention to foster appropriate reliance. In this paper, we explore whether a critical evaluation of AI performance within a debugging setting can better calibrate users' assessment of an AI system and lead to more appropriate reliance. Through a quantitative empirical study (N = 234), we found that our proposed debugging intervention does not work as expected in facilitating appropriate reliance. Instead, we observe a decrease in reliance on the AI system after the intervention -- potentially resulting from an early exposure to the AI system's weakness. We explore the dynamics of user confidence and user estimation of AI trustworthiness across groups with different performance levels to help explain how inappropriate reliance patterns occur. Our findings have important implications for designing effective interventions to facilitate appropriate reliance and better human-AI collaboration.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 09:43:27 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.758519"
    },
    {
        "index": "#39",
        "title": "MANTA -- Model Adapter Native generations that's Affordable",
        "link": "/arxiv/2409.14363",
        "arxiv_id": "2409.14363",
        "authors": "Ansh Chaurasia",
        "summary": "The presiding model generation algorithms rely on simple, inflexible adapter selection to provide personalized results. We propose the model-adapter composition problem as a generalized problem to past work factoring in practical hardware and affordability constraints, and introduce MANTA as a new approach to the problem. Experiments on COCO 2014 validation show MANTA to be superior in image task diversity and quality at the cost of a modest drop in alignment. Our system achieves a $94\\%$ win rate in task diversity and a $80\\%$ task quality win rate versus the best known system, and demonstrates strong potential for direct use in synthetic data generation and the creative art domains.",
        "subjects": "Artificial Intelligence, Image and Video Processing",
        "date": "2024-09-22 08:38:23 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.758684"
    },
    {
        "index": "#40",
        "title": "LLMs are One-Shot URL Classifiers and Explainers",
        "link": "/arxiv/2409.14306",
        "arxiv_id": "2409.14306",
        "authors": "Fariza Rashid, Nishavi Ranaweera, Ben Doyle, Suranga Seneviratne",
        "summary": "Malicious URL classification represents a crucial aspect of cyber security. Although existing work comprises numerous machine learning and deep learning-based URL classification models, most suffer from generalisation and domain-adaptation issues arising from the lack of representative training datasets. Furthermore, these models fail to provide explanations for a given URL classification in natural human language. In this work, we investigate and demonstrate the use of Large Language Models (LLMs) to address this issue. Specifically, we propose an LLM-based one-shot learning framework that uses Chain-of-Thought (CoT) reasoning to predict whether a given URL is benign or phishing. We evaluate our framework using three URL datasets and five state-of-the-art LLMs and show that one-shot LLM prompting indeed provides performances close to supervised models, with GPT 4-Turbo being the best model, followed by Claude 3 Opus. We conduct a quantitative analysis of the LLM explanations and show that most of the explanations provided by LLMs align with the post-hoc explanations of the supervised classifiers, and the explanations have high readability, coherency, and informativeness.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 03:52:39 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.758892"
    },
    {
        "index": "#41",
        "title": "UU-Mamba: Uncertainty-aware U-Mamba for Cardiovascular Segmentation",
        "link": "/arxiv/2409.14305",
        "arxiv_id": "2409.14305",
        "authors": "Ting Yu Tsai, Li Lin, Shu Hu, Connie W. Tsao, Xin Li, Ming-Ching Chang, Hongtu Zhu, Xin Wang",
        "summary": "Building on the success of deep learning models in cardiovascular structure segmentation, increasing attention has been focused on improving generalization and robustness, particularly in small, annotated datasets. Despite recent advancements, current approaches often face challenges such as overfitting and accuracy limitations, largely due to their reliance on large datasets and narrow optimization techniques. This paper introduces the UU-Mamba model, an extension of the U-Mamba architecture, designed to address these challenges in both cardiac and vascular segmentation. By incorporating Sharpness-Aware Minimization (SAM), the model enhances generalization by targeting flatter minima in the loss landscape. Additionally, we propose an uncertainty-aware loss function that combines region-based, distribution-based, and pixel-based components to improve segmentation accuracy by capturing both local and global features. While the UU-Mamba model has already demonstrated great performance, further testing is required to fully assess its generalization and robustness. We expand our evaluation by conducting new trials on the ImageCAS (coronary artery) and Aorta (aortic branches and zones) datasets, which present more complex segmentation challenges than the ACDC dataset (left and right ventricles) used in our previous work, showcasing the model's adaptability and resilience. We confirm UU-Mamba's superior performance over leading models such as TransUNet, Swin-Unet, nnUNet, and nnFormer. Moreover, we provide a more comprehensive evaluation of the model's robustness and segmentation accuracy, as demonstrated by extensive experiments.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-22 03:22:06 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.759120"
    },
    {
        "index": "#42",
        "title": "HM3D-OVON: A Dataset and Benchmark for Open-Vocabulary Object Goal Navigation",
        "link": "/arxiv/2409.14296",
        "arxiv_id": "2409.14296",
        "authors": "Naoki Yokoyama, Ram Ramrakhya, Abhishek Das, Dhruv Batra, Sehoon Ha",
        "summary": "We present the Habitat-Matterport 3D Open Vocabulary Object Goal Navigation dataset (HM3D-OVON), a large-scale benchmark that broadens the scope and semantic range of prior Object Goal Navigation (ObjectNav) benchmarks. Leveraging the HM3DSem dataset, HM3D-OVON incorporates over 15k annotated instances of household objects across 379 distinct categories, derived from photo-realistic 3D scans of real-world environments. In contrast to earlier ObjectNav datasets, which limit goal objects to a predefined set of 6-20 categories, HM3D-OVON facilitates the training and evaluation of models with an open-set of goals defined through free-form language at test-time. Through this open-vocabulary formulation, HM3D-OVON encourages progress towards learning visuo-semantic navigation behaviors that are capable of searching for any object specified by text in an open-vocabulary manner. Additionally, we systematically evaluate and compare several different types of approaches on HM3D-OVON. We find that HM3D-OVON can be used to train an open-vocabulary ObjectNav agent that achieves both higher performance and is more robust to localization and actuation noise than the state-of-the-art ObjectNav approach. We hope that our benchmark and baseline results will drive interest in developing embodied agents that can navigate real-world spaces to find household objects specified through free-form language, taking a step towards more flexible and human-like semantic visual navigation. Code and videos available at: naoki.io/ovon.",
        "subjects": "Artificial Intelligence, Robotics",
        "date": "2024-09-22 02:12:29 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.759318"
    },
    {
        "index": "#44",
        "title": "Predicting Coronary Heart Disease Using a Suite of Machine Learning Models",
        "link": "/arxiv/2409.14231",
        "arxiv_id": "2409.14231",
        "authors": "Jamal Al-Karaki, Philip Ilono, Sanchit Baweja, Jalal Naghiyev, Raja Singh Yadav, Muhammad Al-Zafar Khan",
        "summary": "Coronary Heart Disease affects millions of people worldwide and is a well-studied area of healthcare. There are many viable and accurate methods for the diagnosis and prediction of heart disease, but they have limiting points such as invasiveness, late detection, or cost. Supervised learning via machine learning algorithms presents a low-cost (computationally speaking), non-invasive solution that can be a precursor for early diagnosis. In this study, we applied several well-known methods and benchmarked their performance against each other. It was found that Random Forest with oversampling of the predictor variable produced the highest accuracy of 84%.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-21 19:22:41 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.759711"
    },
    {
        "index": "#45",
        "title": "AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented Reality Cues",
        "link": "/arxiv/2409.14206",
        "arxiv_id": "2409.14206",
        "authors": "Oliver Bensch, Leonie Bensch, Tommy Nilsson, Florian Saling, Bernd Bewer, Sophie Jentzsch, Tobias Hecking, J. Nathan Kutz",
        "summary": "This paper describes the capabilities and potential of the intelligent personal assistant (IPA) CORE (Checklist Organizer for Research and Exploration), designed to support astronauts during procedures onboard the International Space Station (ISS), the Lunar Gateway station, and beyond. We reflect on the importance of a reliable and flexible assistant capable of offline operation and highlight the usefulness of audiovisual interaction using augmented reality elements to intuitively display checklist information. We argue that current approaches to the design of IPAs in space operations fall short of meeting these criteria. Therefore, we propose CORE as an assistant that combines Knowledge Graphs (KGs), Retrieval-Augmented Generation (RAG) for a Generative Pre-Trained Transformer (GPT), and Augmented Reality (AR) elements to ensure an intuitive understanding of procedure steps, reliability, offline availability, and flexibility in terms of response style and procedure updates.",
        "subjects": "Artificial Intelligence, Human-Computer Interaction",
        "date": "2024-09-21 17:41:46 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.759919"
    },
    {
        "index": "#46",
        "title": "Loop-Residual Neural Networks for Iterative Refinement",
        "link": "/arxiv/2409.14199",
        "arxiv_id": "2409.14199",
        "authors": "Kei-Sing Ng, Qingchen Wang",
        "summary": "The success of large-scale language models like GPT can be attributed to their ability to efficiently predict the next token in a sequence. However, these models rely on constant computational effort regardless of the complexity of the token they are predicting, lacking the capacity for iterative refinement. In this paper, we introduce a novel Loop-Residual Neural Network, which achieves better performance by utilizing longer computational time without increasing the model size. Our approach revisits the input multiple times, refining the prediction by iteratively looping over a subset of the model with residual connections. We demonstrate the effectiveness of this method through experiments comparing versions of GPT-2 with our Loop-Residual models, showing improved performance in language modeling tasks while maintaining similar parameter counts. Importantly, these improvements are achieved without the need for extra training data.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-21 17:07:42 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.760087"
    },
    {
        "index": "#47",
        "title": "Addressing and Visualizing Misalignments in Human Task-Solving Trajectories",
        "link": "/arxiv/2409.14191",
        "arxiv_id": "2409.14191",
        "authors": "Sejin Kim, Hosung Lee, Sundong Kim",
        "summary": "The effectiveness of AI model training hinges on the quality of the trajectory data used, particularly in aligning the model's decision with human intentions. However, in the human task-solving trajectories, we observe significant misalignments between human intentions and the recorded trajectories, which can undermine AI model training. This paper addresses the challenges of these misalignments by proposing a visualization tool and a heuristic algorithm designed to detect and categorize discrepancies in trajectory data. Although the heuristic algorithm requires a set of predefined human intentions to function, which we currently cannot extract, the visualization tool offers valuable insights into the nature of these misalignments. We expect that eliminating these misalignments could significantly improve the utility of trajectory data for AI model training. We also propose that future work should focus on developing methods, such as Topic Modeling, to accurately extract human intentions from trajectory data, thereby enhancing the alignment between user actions and AI learning processes.",
        "subjects": "Artificial Intelligence, Human-Computer Interaction",
        "date": "2024-09-21 16:38:22 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.765523"
    },
    {
        "index": "#48",
        "title": "Democratising Artificial Intelligence for Pandemic Preparedness and Global Governance in Latin American and Caribbean Countries",
        "link": "/arxiv/2409.14181",
        "arxiv_id": "2409.14181",
        "authors": "Andre de Carvalho, Robson Bonidia, Jude Dzevela Kong, Mariana Dauhajre, Claudio Struchiner, Guilherme Goedert, Peter F. Stadler, Maria Emilia Walter, Danilo Sanches, Troy Day, Marcia Castro, John Edmunds, Manuel Colome-Hidalgo, Demian Arturo Herrera Morban, Edian F. Franco, Cesar Ugarte-Gil, Patricia Espinoza-Lopez, Gabriel Carrasco-Escobar, Ulisses Rocha",
        "summary": "Infectious diseases, transmitted directly or indirectly, are among the leading causes of epidemics and pandemics. Consequently, several open challenges exist in predicting epidemic outbreaks, detecting variants, tracing contacts, discovering new drugs, and fighting misinformation. Artificial Intelligence (AI) can provide tools to deal with these scenarios, demonstrating promising results in the fight against the COVID-19 pandemic. AI is becoming increasingly integrated into various aspects of society. However, ensuring that AI benefits are distributed equitably and that they are used responsibly is crucial. Multiple countries are creating regulations to address these concerns, but the borderless nature of AI requires global cooperation to define regulatory and guideline consensus. Considering this, The Global South AI for Pandemic & Epidemic Preparedness & Response Network (AI4PEP) has developed an initiative comprising 16 projects across 16 countries in the Global South, seeking to strengthen equitable and responsive public health systems that leverage Southern-led responsible AI solutions to improve prevention, preparedness, and response to emerging and re-emerging infectious disease outbreaks. This opinion introduces our branches in Latin American and Caribbean (LAC) countries and discusses AI governance in LAC in the light of biotechnology. Our network in LAC has high potential to help fight infectious diseases, particularly in low- and middle-income countries, generating opportunities for the widespread use of AI techniques to improve the health and well-being of their communities.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-21 15:59:13 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.765984"
    },
    {
        "index": "#49",
        "title": "An Evolutionary Algorithm For the Vehicle Routing Problem with Drones with Interceptions",
        "link": "/arxiv/2409.14173",
        "arxiv_id": "2409.14173",
        "authors": "Carlos Pambo, Jacomine Grobler",
        "summary": "The use of trucks and drones as a solution to address last-mile delivery challenges is a new and promising research direction explored in this paper. The variation of the problem where the drone can intercept the truck while in movement or at the customer location is part of an optimisation problem called the vehicle routing problem with drones with interception (VRPDi). This paper proposes an evolutionary algorithm to solve the VRPDi. In this variation of the VRPDi, multiple pairs of trucks and drones need to be scheduled. The pairs leave and return to a depot location together or separately to make deliveries to customer nodes. The drone can intercept the truck after the delivery or meet up with the truck at the following customer location. The algorithm was executed on the travelling salesman problem with drones (TSPD) datasets by Bouman et al. (2015), and the performance of the algorithm was compared by benchmarking the results of the VRPDi against the results of the VRP of the same dataset. This comparison showed improvements in total delivery time between 39% and 60%. Further detailed analysis of the algorithm results examined the total delivery time, distance, node delivery scheduling and the degree of diversity during the algorithm execution. This analysis also considered how the algorithm handled the VRPDi constraints. The results of the algorithm were then benchmarked against algorithms in Dillon et al. (2023) and Ernst (2024). The latter solved the problem with a maximum drone distance constraint added to the VRPDi. The analysis and benchmarking of the algorithm results showed that the algorithm satisfactorily solved 50 and 100-nodes problems in a reasonable amount of time, and the solutions found were better than those found by the algorithms in Dillon et al. (2023) and Ernst (2024) for the same problems.",
        "subjects": "Artificial Intelligence, Computers and Society, Emerging Technologies, Optimization and Control",
        "date": "2024-09-21 15:26:24 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.766243"
    },
    {
        "index": "#51",
        "title": "FineMolTex: Towards Fine-grained Molecular Graph-Text Pre-training",
        "link": "/arxiv/2409.14106",
        "arxiv_id": "2409.14106",
        "authors": "Yibo Li, Yuan Fang, Mengmei Zhang, Chuan Shi",
        "summary": "Understanding molecular structure and related knowledge is crucial for scientific research. Recent studies integrate molecular graphs with their textual descriptions to enhance molecular representation learning. However, they focus on the whole molecular graph and neglect frequently occurring subgraphs, known as motifs,which are essential for determining molecular properties. Without such fine-grained knowledge, these models struggle to generalize to unseen molecules and tasks that require motif-level insights. To bridge this gap, we propose FineMolTex, a novel Fine-grained Molecular graph-Text pre-training framework to jointly learn coarse-grained molecule-level knowledge and fine-grained motif-level knowledge. Specifically, FineMolTex consists of two pre-training tasks: a contrastive alignment task for coarse-grained matching and a masked multi-modal modeling task for fine-grained matching. In particular, the latter predicts the labels of masked motifs and words, leveraging insights from each other, thereby enabling FineMolTex to understand the fine-grained matching between motifs and words. Finally, we conduct extensive experiments across three downstream tasks, achieving up to 230% improvement in the text-based molecule editing task. Additionally, our case studies reveal that FineMolTex successfully captures fine-grained knowledge, potentially offering valuable insights for drug discovery and catalyst design.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-21 11:19:15 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.766751"
    },
    {
        "index": "#52",
        "title": "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction",
        "link": "/arxiv/2409.14091",
        "arxiv_id": "2409.14091",
        "authors": "Amrit Diggavi Seshadri",
        "summary": "With the size and cost of large transformer-based language models growing, recently, there has been interest in shortcut casting of early transformer hidden-representations to final-representations for cheaper model inference. In particular, shortcutting pre-trained transformers with linear transformations over early layers has been shown to improve precision in early inference. However, for large language models, even this becomes computationally expensive. In this work, we propose Narrow Jump to Conclusions (NJTC) and Normalized Narrow Jump to Conclusions (N-NJTC) - parameter efficient alternatives to standard linear shortcutting that reduces shortcut parameter count by over 97%. We show that N-NJTC reliably outperforms Identity shortcuts at early stages and offers stable precision from all transformer block levels for GPT-2-XL, Phi3-Mini and Llama2-7B transformer models, demonstrating the viability of more parameter efficient short-cutting approaches.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-21 10:09:26 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.766966"
    },
    {
        "index": "#53",
        "title": "The use of GPT-4o and Other Large Language Models for the Improvement and Design of Self-Assessment Scales for Measurement of Interpersonal Communication Skills",
        "link": "/arxiv/2409.14050",
        "arxiv_id": "2409.14050",
        "authors": "Goran Bubaš",
        "summary": "OpenAI's ChatGPT (GPT-4 and GPT-4o) and other Large Language Models (LLMs) like Microsoft's Copilot, Google's Gemini 1.5 Pro, and Antrophic's Claude 3.5 Sonnet can be effectively used in various phases of scientific research. Their performance in diverse verbal tasks and reasoning is close to or above the average human level and rapidly increasing, providing those models with a capacity that resembles a relatively high level of theory of mind. The current ability of LLMs to process information about human psychology and communication creates an opportunity for their scientific use in the fields of personality psychology and interpersonal communication skills. This article illustrates the possible uses of GPT-4o and other advanced LLMs for typical tasks in designing self-assessment scales for interpersonal communication skills measurement like the selection and improvement of scale items and evaluation of content validity of scales. The potential for automated item generation and application is illustrated as well. The case study examples are accompanied by prompts for LLMs that can be useful for these purposes. Finally, a summary is provided of the potential benefits of using LLMs in the process of evaluation, design, and improvement of interpersonal communication skills self-assessment scales.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-21 07:37:21 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.767174"
    },
    {
        "index": "#55",
        "title": "Drift to Remember",
        "link": "/arxiv/2409.13997",
        "arxiv_id": "2409.13997",
        "authors": "Jin Du, Xinhe Zhang, Hao Shen, Xun Xian, Ganghua Wang, Jiawei Zhang, Yuhong Yang, Na Li, Jia Liu, Jie Ding",
        "summary": "Lifelong learning in artificial intelligence (AI) aims to mimic the biological brain's ability to continuously learn and retain knowledge, yet it faces challenges such as catastrophic forgetting. Recent neuroscience research suggests that neural activity in biological systems undergoes representational drift, where neural responses evolve over time, even with consistent inputs and tasks. We hypothesize that representational drift can alleviate catastrophic forgetting in AI during new task acquisition. To test this, we introduce DriftNet, a network designed to constantly explore various local minima in the loss landscape while dynamically retrieving relevant tasks. This approach ensures efficient integration of new information and preserves existing knowledge. Experimental studies in image classification and natural language processing demonstrate that DriftNet outperforms existing models in lifelong learning. Importantly, DriftNet is scalable in handling a sequence of tasks such as sentiment analysis and question answering using large language models (LLMs) with billions of parameters on a single Nvidia A100 GPU. DriftNet efficiently updates LLMs using only new data, avoiding the need for full dataset retraining. Tested on GPT-2 and RoBERTa, DriftNet is a robust, cost-effective solution for lifelong learning in LLMs. This study not only advances AI systems to emulate biological learning, but also provides insights into the adaptive mechanisms of biological neural systems, deepening our understanding of lifelong learning in nature.",
        "subjects": "Artificial Intelligence, Neurons and Cognition",
        "date": "2024-09-21 03:18:44 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.767699"
    },
    {
        "index": "#56",
        "title": "PureDiffusion: Using Backdoor to Counter Backdoor in Generative Diffusion Models",
        "link": "/arxiv/2409.13945",
        "arxiv_id": "2409.13945",
        "authors": "Vu Tuan Truong, Long Bao Le",
        "summary": "Diffusion models (DMs) are advanced deep learning models that achieved state-of-the-art capability on a wide range of generative tasks. However, recent studies have shown their vulnerability regarding backdoor attacks, in which backdoored DMs consistently generate a designated result (e.g., a harmful image) called backdoor target when the models' input contains a backdoor trigger. Although various backdoor techniques have been investigated to attack DMs, defense methods against these threats are still limited and underexplored, especially in inverting the backdoor trigger. In this paper, we introduce PureDiffusion, a novel backdoor defense framework that can efficiently detect backdoor attacks by inverting backdoor triggers embedded in DMs. Our extensive experiments on various trigger-target pairs show that PureDiffusion outperforms existing defense methods with a large gap in terms of fidelity (i.e., how much the inverted trigger resembles the original trigger) and backdoor success rate (i.e., the rate that the inverted trigger leads to the corresponding backdoor target). Notably, in certain cases, backdoor triggers inverted by PureDiffusion even achieve higher attack success rate than the original triggers.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-20 23:19:26 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.767878"
    },
    {
        "index": "#58",
        "title": "Failures in Perspective-taking of Multimodal AI Systems",
        "link": "/arxiv/2409.13929",
        "arxiv_id": "2409.13929",
        "authors": "Bridget Leonard, Kristin Woodard, Scott O. Murray",
        "summary": "This study extends previous research on spatial representations in multimodal AI systems. Although current models demonstrate a rich understanding of spatial information from images, this information is rooted in propositional representations, which differ from the analog representations employed in human and animal spatial cognition. To further explore these limitations, we apply techniques from cognitive and developmental science to assess the perspective-taking abilities of GPT-4o. Our analysis enables a comparison between the cognitive development of the human brain and that of multimodal AI, offering guidance for future research and model development.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-20 22:31:46 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.768228"
    },
    {
        "index": "#59",
        "title": "SpaceBlender: Creating Context-Rich Collaborative Spaces Through Generative 3D Scene Blending",
        "link": "/arxiv/2409.13926",
        "arxiv_id": "2409.13926",
        "authors": "Nels Numan, Shwetha Rajaram, Balasaravanan Thoravi Kumaravel, Nicolai Marquardt, Andrew D. Wilson",
        "summary": "There is increased interest in using generative AI to create 3D spaces for Virtual Reality (VR) applications. However, today's models produce artificial environments, falling short of supporting collaborative tasks that benefit from incorporating the user's physical context. To generate environments that support VR telepresence, we introduce SpaceBlender, a novel pipeline that utilizes generative AI techniques to blend users' physical surroundings into unified virtual spaces. This pipeline transforms user-provided 2D images into context-rich 3D environments through an iterative process consisting of depth estimation, mesh alignment, and diffusion-based space completion guided by geometric priors and adaptive text prompts. In a preliminary within-subjects study, where 20 participants performed a collaborative VR affinity diagramming task in pairs, we compared SpaceBlender with a generic virtual environment and a state-of-the-art scene generation framework, evaluating its ability to create virtual spaces suitable for collaboration. Participants appreciated the enhanced familiarity and context provided by SpaceBlender but also noted complexities in the generative environments that could detract from task focus. Drawing on participant feedback, we propose directions for improving the pipeline and discuss the value and design of blended spaces for different scenarios.",
        "subjects": "Artificial Intelligence, Human-Computer Interaction",
        "date": "2024-09-20 22:27:31 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.768418"
    },
    {
        "index": "#60",
        "title": "Measuring Error Alignment for Decision-Making Systems",
        "link": "/arxiv/2409.13919",
        "arxiv_id": "2409.13919",
        "authors": "Binxia Xu, Antonis Bikakis, Daniel Onah, Andreas Vlachidis, Luke Dickens",
        "summary": "Given that AI systems are set to play a pivotal role in future decision-making processes, their trustworthiness and reliability are of critical concern. Due to their scale and complexity, modern AI systems resist direct interpretation, and alternative ways are needed to establish trust in those systems, and determine how well they align with human values. We argue that good measures of the information processing similarities between AI and humans, may be able to achieve these same ends. While Representational alignment (RA) approaches measure similarity between the internal states of two systems, the associated data can be expensive and difficult to collect for human systems. In contrast, Behavioural alignment (BA) comparisons are cheaper and easier, but questions remain as to their sensitivity and reliability. We propose two new behavioural alignment metrics misclassification agreement which measures the similarity between the errors of two systems on the same instances, and class-level error similarity which measures the similarity between the error distributions of two systems. We show that our metrics correlate well with RA metrics, and provide complementary information to another BA metric, within a range of domains, and set the scene for a new approach to value alignment.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-20 21:59:13 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.768609"
    },
    {
        "index": "#61",
        "title": "Nonlinear Inverse Design of Mechanical Multi-Material Metamaterials Enabled by Video Denoising Diffusion and Structure Identifier",
        "link": "/arxiv/2409.13908",
        "arxiv_id": "2409.13908",
        "authors": "Jaewan Park, Shashank Kushwaha, Junyan He, Seid Koric, Qibang Liu, Iwona Jasiuk, Diab Abueidda",
        "summary": "Metamaterials, synthetic materials with customized properties, have emerged as a promising field due to advancements in additive manufacturing. These materials derive unique mechanical properties from their internal lattice structures, which are often composed of multiple materials that repeat geometric patterns. While traditional inverse design approaches have shown potential, they struggle to map nonlinear material behavior to multiple possible structural configurations. This paper presents a novel framework leveraging video diffusion models, a type of generative artificial Intelligence (AI), for inverse multi-material design based on nonlinear stress-strain responses. Our approach consists of two key components: (1) a fields generator using a video diffusion model to create solution fields based on target nonlinear stress-strain responses, and (2) a structure identifier employing two UNet models to determine the corresponding multi-material 2D design. By incorporating multiple materials, plasticity, and large deformation, our innovative design method allows for enhanced control over the highly nonlinear mechanical behavior of metamaterials commonly seen in real-world applications. It offers a promising solution for generating next-generation metamaterials with finely tuned mechanical characteristics.",
        "subjects": "Artificial Intelligence, Computational Engineering, Finance, and Science",
        "date": "2024-09-20 21:26:15 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.768879"
    },
    {
        "index": "#62",
        "title": "CI-Bench: Benchmarking Contextual Integrity of AI Assistants on Synthetic Data",
        "link": "/arxiv/2409.13903",
        "arxiv_id": "2409.13903",
        "authors": "Zhao Cheng, Diane Wan, Matthew Abueg, Sahra Ghalebikesabi, Ren Yi, Eugene Bagdasarian, Borja Balle, Stefan Mellem, Shawn O'Banion",
        "summary": "Advances in generative AI point towards a new era of personalized applications that perform diverse tasks on behalf of users. While general AI assistants have yet to fully emerge, their potential to share personal data raises significant privacy challenges. This paper introduces CI-Bench, a comprehensive synthetic benchmark for evaluating the ability of AI assistants to protect personal information during model inference. Leveraging the Contextual Integrity framework, our benchmark enables systematic assessment of information flow across important context dimensions, including roles, information types, and transmission principles. We present a novel, scalable, multi-step synthetic data pipeline for generating natural communications, including dialogues and emails. Unlike previous work with smaller, narrowly focused evaluations, we present a novel, scalable, multi-step data pipeline that synthetically generates natural communications, including dialogues and emails, which we use to generate 44 thousand test samples across eight domains. Additionally, we formulate and evaluate a naive AI assistant to demonstrate the need for further study and careful training towards personal assistant tasks. We envision CI-Bench as a valuable tool for guiding future language model development, deployment, system design, and dataset construction, ultimately contributing to the development of AI assistants that align with users' privacy expectations.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-20 21:14:36 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.769135"
    },
    {
        "index": "#64",
        "title": "A Personalised 3D+t Mesh Generative Model for Unveiling Normal Heart Dynamics",
        "link": "/arxiv/2409.13825",
        "arxiv_id": "2409.13825",
        "authors": "Mengyun Qiao, Kathryn A McGurk, Shuo Wang, Paul M. Matthews, Declan P O Regan, Wenjia Bai",
        "summary": "Understanding the structure and motion of the heart is crucial for diagnosing and managing cardiovascular diseases, the leading cause of global death. There is wide variation in cardiac shape and motion patterns, that are influenced by demographic, anthropometric and disease factors. Unravelling the normal patterns of shape and motion, as well as understanding how each individual deviates from the norm, would facilitate accurate diagnosis and personalised treatment strategies. To this end, we developed a novel conditional generative model, MeshHeart, to learn the distribution of cardiac shape and motion patterns. MeshHeart is capable of generating 3D+t cardiac mesh sequences, taking into account clinical factors such as age, sex, weight and height. To model the high-dimensional and complex spatio-temporal mesh data, MeshHeart employs a geometric encoder to represent cardiac meshes in a latent space, followed by a temporal Transformer to model the motion dynamics of latent representations. Based on MeshHeart, we investigate the latent space of 3D+t cardiac mesh sequences and propose a novel distance metric termed latent delta, which quantifies the deviation of a real heart from its personalised normative pattern in the latent space. In experiments using a large dataset of 38,309 subjects, MeshHeart demonstrates a high performance in cardiac mesh sequence reconstruction and generation. Features defined in the latent space are highly discriminative for cardiac disease classification, whereas the latent delta exhibits strong correlation with clinical phenotypes in phenome-wide association studies. The codes and models of this study will be released to benefit further research on digital heart modelling.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-20 18:08:37 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.769580"
    },
    {
        "index": "#65",
        "title": "Simulación de la distribución de alimento en el cultivo de camarón",
        "link": "/arxiv/2409.13759",
        "arxiv_id": "2409.13759",
        "authors": "Renato L. Conforme Rosado, Francisco C. Calderon Bocanegra",
        "summary": "This document presents the experimentation of 4 cases of food distribution for shrimp farming. The distributions are based on the location of the automatic feeders. Three cases applied in reality and a fourth case where the food is irrigated on the crop simultaneously and uniformly. In a first stage, the simulation of the three distribution cases is successfully adjusted to reality, where the trend of the shrimp growth curve is correlated with the historical data curve. A second stage where you experiment in 16 configurations that are based on the amount of food, the density of biomass and the distribution of the food. The simulation adopts the concepts of genetic algorithms to improve the population and fuzzy logic as an agent evaluation technique for decision-making against the quality of physical-chemical parameters in the simulated environment. The results of these interactions reveal a reduction in the simulated total culture time from 22 weeks to 14 weeks.",
        "subjects": "Artificial Intelligence",
        "date": "2024-09-16 01:29:49 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.769749"
    },
    {
        "index": "#66",
        "title": "Increasing the Value of Information During Planning in Uncertain Environments",
        "link": "/arxiv/2409.13754",
        "arxiv_id": "2409.13754",
        "authors": "Gaurab Pokharel",
        "summary": "Prior studies have demonstrated that for many real-world problems, POMDPs can be solved through online algorithms both quickly and with near optimality. However, on an important set of problems where there is a large time delay between when the agent can gather information and when it needs to use that information, these solutions fail to adequately consider the value of information. As a result, information gathering actions, even when they are critical in the optimal policy, will be ignored by existing solutions, leading to sub-optimal decisions by the agent. In this research, we develop a novel solution that rectifies this problem by introducing a new algorithm that improves upon state-of-the-art online planning by better reflecting on the value of actions that gather information. We do this by adding Entropy to the UCB1 heuristic in the POMCP algorithm. We test this solution on the hallway problem. Results indicate that our new algorithm performs significantly better than POMCP.",
        "subjects": "Artificial Intelligence, Multiagent Systems, Robotics",
        "date": "2024-09-14 22:04:34 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.769930"
    },
    {
        "index": "#70",
        "title": "Style over Substance: Failure Modes of LLM Judges in Alignment Benchmarking",
        "link": "/arxiv/2409.15268",
        "arxiv_id": "2409.15268",
        "authors": "Benjamin Feuer, Micah Goldblum, Teresa Datta, Sanjana Nambiar, Raz Besaleli, Samuel Dooley, Max Cembalest, John P. Dickerson",
        "summary": "The release of ChatGPT in November 2022 sparked an explosion of interest in post-training and an avalanche of new preference optimization (PO) methods. These methods claim superior alignment by virtue of better correspondence with human pairwise preferences, often measured by LLM judges. In this work, we attempt to answer the following question -- do LLM-judge preferences translate to progress on other, more concrete metrics for alignment, and if not, why not? We define a concrete metric for alignment, and introduce SOS-Bench, the largest standardized, reproducible LLM meta-benchmark to date. We find that (1) LLM-judgments do not correlate with concrete measures of safety, world knowledge, and instruction following; (2) LLM judges have powerful implicit biases, prioritizing style over factuality and safety; and (3) the supervised fine-tuning (SFT) stage of post-training, and not the PO stage, has the greatest impact on alignment, with data scaling and prompt diversity as the driving factors. Our codebase and complete results can be found at https://github.com/penfever/sos-bench.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-23 17:58:07 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.776504"
    },
    {
        "index": "#71",
        "title": "The Palomar twilight survey of 'Ayló'chaxnim, Atiras, and comets",
        "link": "/arxiv/2409.15263",
        "arxiv_id": "2409.15263",
        "authors": "B. T. Bolin, F. J. Masci, M. W. Coughlin, D. A. Duev, Ž. Ivezić, R. L. Jones, P. Yoachim, T. Ahumada, V. Bhalerao, H. Choudhary, C. Contreras, Y. -C. Cheng, C. M. Copperwheat, K. Deshmukh, C. Fremling, M. Granvik, K. K. Hardegree-Ullman, A. Y. Q. Ho, R. Jedicke, M. Kasliwal, H. Kumar, Z. -Y. Lin, A. Mahabal, A. Monson, J. D. Neill, D. Nesvorný, D. A. Perley, J. N. Purdum, R. Quimby, E. Serabyn, K. Sharma, V. Swain",
        "summary": "Near-sun sky twilight observations allow for the detection of asteroid interior to the orbit of Venus (Aylos), the Earth (Atiras), and comets. We present the results of observations with the Palomar 48-inch telescope (P48)/Zwicky Transient Facility (ZTF) camera in 30 s r-band exposures taken during evening astronomical twilight from 2019 Sep 20 to 2022 March 7 and during morning astronomical twilight sky from 2019 Sep 21 to 2022 Sep 29. More than 46,000 exposures were taken in evening and morning astronomical twilight within 31 to 66 degrees from the Sun with an r-band limiting magnitude between 18.1 and 20.9. The twilight pointings show a slight seasonal dependence in limiting magnitude and ability to point closer towards the Sun, with limiting magnitude slightly improving during summer. In total, the one Aylo, (594913) 'Ayló'chaxnim, and 4 Atiras, 2020 OV1, 2021 BS1, 2021 PB2, and 2021 VR3, were discovered in evening and morning twilight observations. Additional twilight survey discoveries also include 6 long-period comets: C/2020 T2, C/2020 V2, C/2021 D2, C/2021 E3, C/2022 E3, and C/2022 P3, and two short-period comets: P/2021 N1 and P/2022 P2 using deep learning comet detection pipelines. The P48/ZTF twilight survey also recovered 11 known Atiras, one Aylo, three short-period comes, two long-period comets, and one interstellar object. Lastly, the Vera Rubin Observatory will conduct a twilight survey starting in its first year of operations and will cover the sky within 45 degrees of the Sun. Twilight surveys such as those by ZTF and future surveys will provide opportunities for discovering asteroids inside the orbits of Earth and Venus.",
        "subjects": "Earth and Planetary Astrophysics, Instrumentation and Methods for Astrophysics, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 17:56:45 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.777227"
    },
    {
        "index": "#72",
        "title": "Identification and Localization of Cometary Activity in Solar System Objects with Machine Learning",
        "link": "/arxiv/2409.15261",
        "arxiv_id": "2409.15261",
        "authors": "Bryce T. Bolin, Michael W. Coughlin",
        "summary": "In this chapter, we will discuss the use of Machine Learning methods for the identification and localization of cometary activity for Solar System objects in ground and in space-based wide-field all-sky surveys. We will begin the chapter by discussing the challenges of identifying known and unknown active, extended Solar System objects in the presence of stellar-type sources and the application of classical pre-ML identification techniques and their limitations. We will then transition to the discussion of implementing ML techniques to address the challenge of extended object identification. We will finish with prospective future methods and the application to future surveys such as the Vera C. Rubin Observatory.",
        "subjects": "Earth and Planetary Astrophysics, Instrumentation and Methods for Astrophysics, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 17:56:32 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.777440"
    },
    {
        "index": "#76",
        "title": "Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping",
        "link": "/arxiv/2409.15241",
        "arxiv_id": "2409.15241",
        "authors": "Guanhua Wang, Chengming Zhang, Zheyu Shen, Ang Li, Olatunji Ruwase",
        "summary": "Given the popularity of generative AI, Large Language Models (LLMs) often consume hundreds or thousands of GPUs for parallelizing and accelerating the training process. Communication overhead becomes more pronounced when training LLMs at scale. To eliminate communication overhead in distributed LLM training, we propose Domino, which provides a generic scheme to hide communication behind computation. By breaking data dependency of a single batch training into smaller independent pieces, Domino pipelines these independent pieces training and provides generic strategy of fine-grained communication and computation overlapping. Extensive results show that, comparing with Megatron-LM, Domino achieves up to 1.3x speedup for LLM training on Nvidia DGX-H100 GPUs.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 17:38:52 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.778368"
    },
    {
        "index": "#78",
        "title": "AutoAPIEval: A Framework for Automated Evaluation of LLMs in API-Oriented Code Generation",
        "link": "/arxiv/2409.15228",
        "arxiv_id": "2409.15228",
        "authors": "Yixi Wu, Pengfei He, Zehao Wang, Shaowei Wang, Yuan Tian, Tse-Hsun, Chen",
        "summary": "Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as powerful tools for code generation, significantly enhancing productivity and accelerating software development. However, existing benchmarks primarily focus on general code generation without considering API-oriented code generation, i.e., generating code that invokes APIs from specific libraries. Given the growing demand for API-oriented code generation, there is a pressing need for a systematic and automated approach to evaluate LLM on API-oriented code generation. To address this gap, we propose AutoAPIEval, a lightweight and automated framework designed to evaluate the capabilities of LLMs in API-oriented code generation. Our framework works with any library that provides API documentation and focuses on two unit tasks: API recommendation and code example generation, along with four metrics to evaluate the generated APIs and code examples, such as the proportion of incorrect API recommendations for Task 1, and the proportion of code examples where no specific API is invoked and uncompilable/unexecutable code examples for Task 2. In addition, we conducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder) and Java Runtime Environment 8 to demonstrate the framework's effectiveness. Our findings reveal substantial variability in LLM performance across tasks, with ChatGPT adhering better to instructions, while sharing similar effectiveness in code example generation with its counterparts (i.e., MagiCoder and DeekSeek Coder). We also identify key factors associated with code quality, such as API popularity and model confidence, and build classifiers that achieve high accuracy in detecting incorrect API recommendations and erroneous code examples. Retrieval-augmented generation enhances the quality of code generated by LLMs, though its effectiveness varies across different LLMs.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 17:22:09 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.778789"
    },
    {
        "index": "#83",
        "title": "Location is Key: Leveraging Large Language Model for Functional Bug Localization in Verilog",
        "link": "/arxiv/2409.15186",
        "arxiv_id": "2409.15186",
        "authors": "Bingkun Yao, Ning Wang, Jie Zhou, Xi Wang, Hong Gao, Zhe Jiang, Nan Guan",
        "summary": "Bug localization in Verilog code is a crucial and time-consuming task during the verification of hardware design. Since introduction, Large Language Models (LLMs) have showed their strong programming capabilities. However, no work has yet considered using LLMs for bug localization in Verilog code. This paper presents Location-is-Key, an opensource LLM solution to locate functional errors in Verilog snippets. LiK achieves high localization accuracy, with a pass@1 localization accuracy of 93.3% on our test dataset based on RTLLM, surpassing GPT-4's 77.9% and comparable to Claude-3.5's 90.8%. Additionally, the bug location obtained by LiK significantly improves GPT-3.5's bug repair efficiency (Functional pass@1 increased from 40.39% to 58.92%), highlighting the importance of bug localization in LLM-based Verilog debugging. Compared to existing methods, LiK only requires the design specification and the erroneous code snippet, without the need for testbenches, assertions, or any other EDA tools. This research demonstrates the feasibility of using LLMs for Verilog error localization, thus providing a new direction for automatic Verilog code debugging.",
        "subjects": "Hardware Architecture, Artificial Intelligence",
        "date": "2024-09-23 16:38:53 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.779875"
    },
    {
        "index": "#84",
        "title": "Skills Made to Order: Efficient Acquisition of Robot Cooking Skills Guided by Multiple Forms of Internet Data",
        "link": "/arxiv/2409.15172",
        "arxiv_id": "2409.15172",
        "authors": "Mrinal Verghese, Christopher Atkeson",
        "summary": "This study explores the utility of various internet data sources to select among a set of template robot behaviors to perform skills. Learning contact-rich skills involving tool use from internet data sources has typically been challenging due to the lack of physical information such as contact existence, location, areas, and force in this data. Prior works have generally used internet data and foundation models trained on this data to generate low-level robot behavior. We hypothesize that these data and models may be better suited to selecting among a set of basic robot behaviors to perform these contact-rich skills. We explore three methods of template selection: querying large language models, comparing video of robot execution to retrieved human video using features from a pretrained video encoder common in prior work, and performing the same comparison using features from an optic flow encoder trained on internet data. Our results show that LLMs are surprisingly capable template selectors despite their lack of visual information, optical flow encoding significantly outperforms video encoders trained with an order of magnitude more data, and important synergies exist between various forms of internet data for template selection. By exploiting these synergies, we create a template selector using multiple forms of internet data that achieves a 79\\% success rate on a set of 16 different cooking skills involving tool-use.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 16:25:44 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.780100"
    },
    {
        "index": "#85",
        "title": "DeepCloth-ROB$^2_{\\text{QS}}$P&P: Towards a Robust Robot Deployment for Quasi-Static Pick-and-Place Cloth-Shaping Neural Controllers",
        "link": "/arxiv/2409.15159",
        "arxiv_id": "2409.15159",
        "authors": "Halid Abdulrahim Kadi, Jose Alex Chandy, Luis Figueredo, Kasim Terzić, Praminda Caleb-Solly",
        "summary": "The fidelity gap between simulation-trained vision-based data-driven cloth neural controllers and real-world operation impedes reliable deployment of methods from simulation into physical trials. Real-world grasping errors, such as misgrasping and multilayer grasping, degrade their performance; additionally, some fabrics made of synthetic material also tend to stick to the commonly employed Franka Emika Panda's original gripper. Different approaches adopted various strategies to resolve these problems, further complicating real-world comparison between state-of-the-art methods. We propose DeepCloth-ROB$^2_{\\text{QS}}$P&P with a simulation-to-reality transfer strategy Towel-Sim2Real and a cloth grasping protocol to consider and mitigate these grasping errors for robustly deploying quasi-static pick-and-place neural controllers in cloth shaping and demonstrate its generalisability across different deep-learning methods, fabric contexts and robot platforms. Our approach allows us to compare multiple neural controllers in a real environment for the first time, offering valuable insights to the cloth manipulation community.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2024-09-23 16:08:16 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.780306"
    },
    {
        "index": "#87",
        "title": "RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code",
        "link": "/arxiv/2409.15154",
        "arxiv_id": "2409.15154",
        "authors": "Jiachi Chen, Qingyuan Zhong, Yanlin Wang, Kaiwen Ning, Yongkun Liu, Zenan Xu, Zhe Zhao, Ting Chen, Zibin Zheng",
        "summary": "The emergence of Large Language Models (LLMs) has significantly influenced various aspects of software development activities. Despite their benefits, LLMs also pose notable risks, including the potential to generate harmful content and being abused by malicious developers to create malicious code. Several previous studies have focused on the ability of LLMs to resist the generation of harmful content that violates human ethical standards, such as biased or offensive content. However, there is no research evaluating the ability of LLMs to resist malicious code generation. To fill this gap, we propose RMCBench, the first benchmark comprising 473 prompts designed to assess the ability of LLMs to resist malicious code generation. This benchmark employs two scenarios: a text-to-code scenario, where LLMs are prompted with descriptions to generate code, and a code-to-code scenario, where LLMs translate or complete existing malicious code. Based on RMCBench, we conduct an empirical study on 11 representative LLMs to assess their ability to resist malicious code generation. Our findings indicate that current LLMs have a limited ability to resist malicious code generation with an average refusal rate of 40.36% in text-to-code scenario and 11.52% in code-to-code scenario. The average refusal rate of all LLMs in RMCBench is only 28.71%; ChatGPT-4 has a refusal rate of only 35.73%. We also analyze the factors that affect LLMs' ability to resist malicious code generation and provide implications for developers to enhance model robustness.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2024-09-23 16:03:26 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.786007"
    },
    {
        "index": "#88",
        "title": "COHERENT: Collaboration of Heterogeneous Multi-Robot System with Large Language Models",
        "link": "/arxiv/2409.15146",
        "arxiv_id": "2409.15146",
        "authors": "Kehui Liu, Zixin Tang, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li",
        "summary": "Leveraging the powerful reasoning capabilities of large language models (LLMs), recent LLM-based robot task planning methods yield promising results. However, they mainly focus on single or multiple homogeneous robots on simple tasks. Practically, complex long-horizon tasks always require collaborations among multiple heterogeneous robots especially with more complex action spaces, which makes these tasks more challenging. To this end, we propose COHERENT, a novel LLM-based task planning framework for collaboration of heterogeneous multi-robot systems including quadrotors, robotic dogs, and robotic arms. Specifically, a Proposal-Execution-Feedback-Adjustment (PEFA) mechanism is designed to decompose and assign actions for individual robots, where a centralized task assigner makes a task planning proposal to decompose the complex task into subtasks, and then assigns subtasks to robot executors. Each robot executor selects a feasible action to implement the assigned subtask and reports self-reflection feedback to the task assigner for plan adjustment. The PEFA loops until the task is completed. Moreover, we create a challenging heterogeneous multi-robot task planning benchmark encompassing 100 complex long-horizon tasks. The experimental results show that our work surpasses the previous methods by a large margin in terms of success rate and execution efficiency. The experimental videos, code, and benchmark are released at https://github.com/MrKeee/COHERENT.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2024-09-23 15:53:41 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.786253"
    },
    {
        "index": "#89",
        "title": "CAMAL: Optimizing LSM-trees via Active Learning",
        "link": "/arxiv/2409.15130",
        "arxiv_id": "2409.15130",
        "authors": "Weiping Yu, Siqiang Luo, Zihao Yu, Gao Cong",
        "summary": "We use machine learning to optimize LSM-tree structure, aiming to reduce the cost of processing various read/write operations. We introduce a new approach Camal, which boasts the following features: (1) ML-Aided: Camal is the first attempt to apply active learning to tune LSM-tree based key-value stores. The learning process is coupled with traditional cost models to improve the training process; (2) Decoupled Active Learning: backed by rigorous analysis, Camal adopts active learning paradigm based on a decoupled tuning of each parameter, which further accelerates the learning process; (3) Easy Extrapolation: Camal adopts an effective mechanism to incrementally update the model with the growth of the data size; (4) Dynamic Mode: Camal is able to tune LSM-tree online under dynamically changing workloads; (5) Significant System Improvement: By integrating Camal into a full system RocksDB, the system performance improves by 28% on average and up to 8x compared to a state-of-the-art RocksDB design.",
        "subjects": "Databases, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 15:35:23 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.786497"
    },
    {
        "index": "#91",
        "title": "Robust Federated Learning Over the Air: Combating Heavy-Tailed Noise with Median Anchored Clipping",
        "link": "/arxiv/2409.15100",
        "arxiv_id": "2409.15100",
        "authors": "Jiaxing Li, Zihan Chen, Kai Fong Ernest Chong, Bikramjit Das, Tony Q. S. Quek, Howard H. Yang",
        "summary": "Leveraging over-the-air computations for model aggregation is an effective approach to cope with the communication bottleneck in federated edge learning. By exploiting the superposition properties of multi-access channels, this approach facilitates an integrated design of communication and computation, thereby enhancing system privacy while reducing implementation costs. However, the inherent electromagnetic interference in radio channels often exhibits heavy-tailed distributions, giving rise to exceptionally strong noise in globally aggregated gradients that can significantly deteriorate the training performance. To address this issue, we propose a novel gradient clipping method, termed Median Anchored Clipping (MAC), to combat the detrimental effects of heavy-tailed noise. We also derive analytical expressions for the convergence rate of model training with analog over-the-air federated learning under MAC, which quantitatively demonstrates the effect of MAC on training performance. Extensive experimental results show that the proposed MAC algorithm effectively mitigates the impact of heavy-tailed noise, hence substantially enhancing system robustness.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-23 15:11:40 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.787000"
    },
    {
        "index": "#93",
        "title": "Zero-Cost Whole-Body Teleoperation for Mobile Manipulation",
        "link": "/arxiv/2409.15095",
        "arxiv_id": "2409.15095",
        "authors": "Daniel Honerkamp, Harsh Mahesheka, Jan Ole von Hartz, Tim Welschehold, Abhinav Valada",
        "summary": "Demonstration data plays a key role in learning complex behaviors and training robotic foundation models. While effective control interfaces exist for static manipulators, data collection remains cumbersome and time intensive for mobile manipulators due to their large number of degrees of freedom. While specialized hardware, avatars, or motion tracking can enable whole-body control, these approaches are either expensive, robot-specific, or suffer from the embodiment mismatch between robot and human demonstrator. In this work, we present MoMa-Teleop, a novel teleoperation method that delegates the base motions to a reinforcement learning agent, leaving the operator to focus fully on the task-relevant end-effector motions. This enables whole-body teleoperation of mobile manipulators with zero additional hardware or setup costs via standard interfaces such as joysticks or hand guidance. Moreover, the operator is not bound to a tracked workspace and can move freely with the robot over spatially extended tasks. We demonstrate that our approach results in a significant reduction in task completion time across a variety of robots and tasks. As the generated data covers diverse whole-body motions without embodiment mismatch, it enables efficient imitation learning. By focusing on task-specific end-effector motions, our approach learns skills that transfer to unseen settings, such as new obstacles or changed object positions, from as little as five demonstrations. We make code and videos available at http://moma-teleop.cs.uni-freiburg.de.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2024-09-23 15:09:45 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.787494"
    },
    {
        "index": "#99",
        "title": "AlphaZip: Neural Network-Enhanced Lossless Text Compression",
        "link": "/arxiv/2409.15046",
        "arxiv_id": "2409.15046",
        "authors": "Swathi Shree Narashiman, Nitin Chandrachoodan",
        "summary": "Data compression continues to evolve, with traditional information theory methods being widely used for compressing text, images, and videos. Recently, there has been growing interest in leveraging Generative AI for predictive compression techniques. This paper introduces a lossless text compression approach using a Large Language Model (LLM). The method involves two key steps: first, prediction using a dense neural network architecture, such as a transformer block; second, compressing the predicted ranks with standard compression algorithms like Adaptive Huffman, LZ77, or Gzip. Extensive analysis and benchmarking against conventional information-theoretic baselines demonstrate that neural compression offers improved performance.",
        "subjects": "Information Theory, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 14:21:06 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.788856"
    },
    {
        "index": "#102",
        "title": "A Diagonal Structured State Space Model on Loihi 2 for Efficient Streaming Sequence Processing",
        "link": "/arxiv/2409.15022",
        "arxiv_id": "2409.15022",
        "authors": "Svea Marie Meyer, Philipp Weidel, Philipp Plank, Leobardo Campos-Macias, Sumit Bam Shrestha, Philipp Stratmann, Mathis Richter",
        "summary": "Deep State-Space Models (SSM) demonstrate state-of-the art performance on long-range sequence modeling tasks. While the recurrent structure of SSMs can be efficiently implemented as a convolution or as a parallel scan during training, recurrent token-by-token processing cannot currently be implemented efficiently on GPUs. Here, we demonstrate efficient token-by-token inference of the SSM S4D on Intel's Loihi 2 state-of-the-art neuromorphic processor. We compare this first ever neuromorphic-hardware implementation of an SSM on sMNIST, psMNIST, and sCIFAR to a recurrent and a convolutional implementation of S4D on Jetson Orin Nano (Jetson). While we find Jetson to perform better in an offline sample-by-sample based batched processing mode, Loihi 2 outperforms during token-by-token based processing, where it consumes 1000 times less energy with a 75 times lower latency and a 75 times higher throughput compared to the recurrent implementation of S4D on Jetson. This opens up new avenues towards efficient real-time streaming applications of SSMs.",
        "subjects": "Machine Learning, Artificial Intelligence, Emerging Technologies, Neural and Evolutionary Computing",
        "date": "2024-09-23 13:50:11 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.789484"
    },
    {
        "index": "#105",
        "title": "Method of Equal Shares with Bounded Overspending",
        "link": "/arxiv/2409.15005",
        "arxiv_id": "2409.15005",
        "authors": "Georgios Papasotiropoulos, Seyedeh Zeinab Pishbin, Oskar Skibski, Piotr Skowron, Tomasz Wąs",
        "summary": "In participatory budgeting (PB), voters decide through voting which subset of projects to fund within a given budget. Proportionality in the context of PB is crucial to ensure equal treatment of all groups of voters. However, pure proportional rules can sometimes lead to suboptimal outcomes. We introduce the Method of Equal Shares with Bounded Overspending (BOS Equal Shares), a robust variant of Equal Shares that balances proportionality and efficiency. BOS Equal Shares addresses inefficiencies inherent in strict proportionality guarantees yet still provides good proportionality similar to the original Method of Equal Shares. In the course of the analysis, we also discuss a fractional variant of the method which allows for partial funding of projects.",
        "subjects": "Computer Science and Game Theory, Artificial Intelligence, Multiagent Systems",
        "date": "2024-09-23 13:30:25 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.790071"
    },
    {
        "index": "#109",
        "title": "On The Specialization of Neural Modules",
        "link": "/arxiv/2409.14981",
        "arxiv_id": "2409.14981",
        "authors": "Devon Jarvis, Richard Klein, Benjamin Rosman, Andrew M. Saxe",
        "summary": "A number of machine learning models have been proposed with the goal of achieving systematic generalization: the ability to reason about new situations by combining aspects of previous experiences. These models leverage compositional architectures which aim to learn specialized modules dedicated to structures in a task that can be composed to solve novel problems with similar structures. While the compositionality of these architectures is guaranteed by design, the modules specializing is not. Here we theoretically study the ability of network modules to specialize to useful structures in a dataset and achieve systematic generalization. To this end we introduce a minimal space of datasets motivated by practical systematic generalization benchmarks. From this space of datasets we present a mathematical definition of systematicity and study the learning dynamics of linear neural modules when solving components of the task. Our results shed light on the difficulty of module specialization, what is required for modules to successfully specialize, and the necessity of modular architectures to achieve systematicity. Finally, we confirm that the theoretical results in our tractable setting generalize to more complex datasets and non-linear architectures.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-23 12:58:11 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.790895"
    },
    {
        "index": "#110",
        "title": "Deep Reinforcement Learning-based Obstacle Avoidance for Robot Movement in Warehouse Environments",
        "link": "/arxiv/2409.14972",
        "arxiv_id": "2409.14972",
        "authors": "Keqin Li, Jiajing Chen, Denzhi Yu, Tao Dajun, Xinyu Qiu, Lian Jieting, Sun Baiwei, Zhang Shengyuan, Zhenyu Wan, Ran Ji, Bo Hong, Fanghao Ni",
        "summary": "At present, in most warehouse environments, the accumulation of goods is complex, and the management personnel in the control of goods at the same time with the warehouse mobile robot trajectory interaction, the traditional mobile robot can not be very good on the goods and pedestrians to feed back the correct obstacle avoidance strategy, in order to control the mobile robot in the warehouse environment efficiently and friendly to complete the obstacle avoidance task, this paper proposes a deep reinforcement learning based on the warehouse environment, the mobile robot obstacle avoidance Algorithm. Firstly, for the insufficient learning ability of the value function network in the deep reinforcement learning algorithm, the value function network is improved based on the pedestrian interaction, the interaction information between pedestrians is extracted through the pedestrian angle grid, and the temporal features of individual pedestrians are extracted through the attention mechanism, so that we can learn to obtain the relative importance of the current state and the historical trajectory state as well as the joint impact on the robot's obstacle avoidance strategy, which provides an opportunity for the learning of multi-layer perceptual machines afterwards. Secondly, the reward function of reinforcement learning is designed based on the spatial behaviour of pedestrians, and the robot is punished for the state where the angle changes too much, so as to achieve the requirement of comfortable obstacle avoidance; Finally, the feasibility and effectiveness of the deep reinforcement learning-based mobile robot obstacle avoidance algorithm in the warehouse environment in the complex environment of the warehouse are verified through simulation experiments.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2024-09-23 12:42:35 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.792738"
    },
    {
        "index": "#112",
        "title": "KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems",
        "link": "/arxiv/2409.14908",
        "arxiv_id": "2409.14908",
        "authors": "Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, Yiming Gan",
        "summary": "Embodied AI agents responsible for executing interconnected, long-sequence household tasks often face difficulties with in-context memory, leading to inefficiencies and errors in task execution. To address this issue, we introduce KARMA, an innovative memory system that integrates long-term and short-term memory modules, enhancing large language models (LLMs) for planning in embodied agents through memory-augmented prompting. KARMA distinguishes between long-term and short-term memory, with long-term memory capturing comprehensive 3D scene graphs as representations of the environment, while short-term memory dynamically records changes in objects' positions and states. This dual-memory structure allows agents to retrieve relevant past scene experiences, thereby improving the accuracy and efficiency of task planning. Short-term memory employs strategies for effective and adaptive memory replacement, ensuring the retention of critical information while discarding less pertinent data. Compared to state-of-the-art embodied agents enhanced with memory, our memory-augmented embodied AI agent improves success rates by 1.3x and 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator, respectively, and enhances task execution efficiency by 3.4x and 62.7x. Furthermore, we demonstrate that KARMA's plug-and-play capability allows for seamless deployment on real-world robotic systems, such as mobile manipulation platforms.Through this plug-and-play memory system, KARMA significantly enhances the ability of embodied agents to generate coherent and contextually appropriate plans, making the execution of complex household tasks more efficient. The experimental videos from the work can be found at https://youtu.be/4BT7fnw9ehs.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2024-09-23 11:02:46 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.793185"
    },
    {
        "index": "#114",
        "title": "Deploying Open-Source Large Language Models: A performance Analysis",
        "link": "/arxiv/2409.14887",
        "arxiv_id": "2409.14887",
        "authors": "Yannis Bendi-Ouis, Dan Dutarte, Xavier Hinaut",
        "summary": "Since the release of ChatGPT in November 2023, large language models (LLMs) have seen considerable success, including in the open-source community, with many open-weight models available. However, the requirements to deploy such a service are often unknown and difficult to evaluate in advance. To facilitate this process, we conducted numerous tests at the Centre Inria de l'Université de Bordeaux. In this article, we propose a comparison of the performance of several models of different sizes (mainly Mistral and LLaMa) depending on the available GPUs, using vLLM, a Python library designed to optimize the inference of these models. Our results provide valuable information for private and public groups wishing to deploy LLMs, allowing them to evaluate the performance of different models based on their available hardware. This study thus contributes to facilitating the adoption and use of these large language models in various application domains.",
        "subjects": "Performance, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 10:35:57 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.793581"
    },
    {
        "index": "#118",
        "title": "FedSlate:A Federated Deep Reinforcement Learning Recommender System",
        "link": "/arxiv/2409.14872",
        "arxiv_id": "2409.14872",
        "authors": "Yongxin Deng, Xiaoyu Tan, Xihe Qiu, Yaochu Jin",
        "summary": "Reinforcement learning methods have been used to optimize long-term user engagement in recommendation systems. However, existing reinforcement learning-based recommendation systems do not fully exploit the relevance of individual user behavior across different platforms. One potential solution is to aggregate data from various platforms in a centralized location and use the aggregated data for training. However, this approach raises economic and legal concerns, including increased communication costs and potential threats to user privacy. To address these challenges, we propose \\textbf{FedSlate}, a federated reinforcement learning recommendation algorithm that effectively utilizes information that is prohibited from being shared at a legal level. We employ the SlateQ algorithm to assist FedSlate in learning users' long-term behavior and evaluating the value of recommended content. We extend the existing application scope of recommendation systems from single-user single-platform to single-user multi-platform and address cross-platform learning challenges by introducing federated learning. We use RecSim to construct a simulation environment for evaluating FedSlate and compare its performance with state-of-the-art benchmark recommendation models. Experimental results demonstrate the superior effects of FedSlate over baseline methods in various environmental settings, and FedSlate facilitates the learning of recommendation strategies in scenarios where baseline methods are completely inapplicable. Code is available at \\textit{https://github.com/TianYaDY/FedSlate}.",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2024-09-23 10:10:24 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.794595"
    },
    {
        "index": "#119",
        "title": "A novel agent with formal goal-reaching guarantees: an experimental study with a mobile robot",
        "link": "/arxiv/2409.14867",
        "arxiv_id": "2409.14867",
        "authors": "Grigory Yaremenko, Dmitrii Dobriborsci, Roman Zashchitin, Ruben Contreras Maestre, Ngoc Quoc Huy Hoang, Pavel Osinenko",
        "summary": "Reinforcement Learning (RL) has been shown to be effective and convenient for a number of tasks in robotics. However, it requires the exploration of a sufficiently large number of state-action pairs, many of which may be unsafe or unimportant. For instance, online model-free learning can be hazardous and inefficient in the absence of guarantees that a certain set of desired states will be reached during an episode. An increasingly common approach to address safety involves the addition of a shielding system that constrains the RL actions to a safe set of actions. In turn, a difficulty for such frameworks is how to effectively couple RL with the shielding system to make sure the exploration is not excessively restricted. This work presents a novel safe model-free RL agent called Critic As Lyapunov Function (CALF) and showcases how CALF can be used to improve upon control baselines in robotics in an efficient and convenient fashion while ensuring guarantees of stable goal reaching. The latter is a crucial part of safety, as seen generally. With CALF all state-action pairs remain explorable and yet reaching of desired goal states is formally guaranteed. Formal analysis is provided that shows the goal stabilization-ensuring properties of CALF and a set of real-world and numerical experiments with a non-holonomic wheeled mobile robot (WMR) TurtleBot3 Burger confirmed the superiority of CALF over such a well-established RL agent as proximal policy optimization (PPO), and a modified version of SARSA in a few-episode setting in terms of attained total cost.",
        "subjects": "Robotics, Artificial Intelligence, Dynamical Systems, Optimization and Control",
        "date": "2024-09-23 10:04:28 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.794852"
    },
    {
        "index": "#120",
        "title": "Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs",
        "link": "/arxiv/2409.14866",
        "arxiv_id": "2409.14866",
        "authors": "Xueluan Gong, Mingzhe Li, Yilin Zhang, Fengyuan Ran, Chen Chen, Yanjiao Chen, Qian Wang, Kwok-Yan Lam",
        "summary": "Large Language Models (LLMs) have excelled in various tasks but are still vulnerable to jailbreaking attacks, where attackers create jailbreak prompts to mislead the model to produce harmful or offensive content. Current jailbreak methods either rely heavily on manually crafted templates, which pose challenges in scalability and adaptability, or struggle to generate semantically coherent prompts, making them easy to detect. Additionally, most existing approaches involve lengthy prompts, leading to higher query costs.In this paper, to remedy these challenges, we introduce a novel jailbreaking attack framework, which is an automated, black-box jailbreaking attack framework that adapts the black-box fuzz testing approach with a series of customized designs. Instead of relying on manually crafted templates, our method starts with an empty seed pool, removing the need to search for any related jailbreaking templates. We also develop three novel question-dependent mutation strategies using an LLM helper to generate prompts that maintain semantic coherence while significantly reducing their length. Additionally, we implement a two-level judge module to accurately detect genuine successful jailbreaks. We evaluated our method on 7 representative LLMs and compared it with 5 state-of-the-art jailbreaking attack strategies. For proprietary LLM APIs, such as GPT-3.5 turbo, GPT-4, and Gemini-Pro, our method achieves attack success rates of over 90%, 80%, and 74%, respectively, exceeding existing baselines by more than 60%. Additionally, our method can maintain high semantic coherence while significantly reducing the length of jailbreak prompts. When targeting GPT-4, our method can achieve over 78\\% attack success rate even with 100 tokens. Moreover, our method demonstrates transferability and is robust to state-of-the-art defenses. We will open-source our codes upon publication.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2024-09-23 10:03:09 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.795069"
    },
    {
        "index": "#121",
        "title": "Embedding Knowledge Graph in Function Space",
        "link": "/arxiv/2409.14857",
        "arxiv_id": "2409.14857",
        "authors": "Louis Mozart Kamdem Teyou, Caglar Demir, Axel-Cyrille Ngonga Ngomo",
        "summary": "We introduce a novel embedding method diverging from conventional approaches by operating within function spaces of finite dimension rather than finite vector space, thus departing significantly from standard knowledge graph embedding techniques. Initially employing polynomial functions to compute embeddings, we progress to more intricate representations using neural networks with varying layer complexities. We argue that employing functions for embedding computation enhances expressiveness and allows for more degrees of freedom, enabling operations such as composition, derivatives and primitive of entities representation. Additionally, we meticulously outline the step-by-step construction of our approach and provide code for reproducibility, thereby facilitating further exploration and application in the field.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 09:49:57 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.795263"
    },
    {
        "index": "#125",
        "title": "Identify As A Human Does: A Pathfinder of Next-Generation Anti-Cheat Framework for First-Person Shooter Games",
        "link": "/arxiv/2409.14830",
        "arxiv_id": "2409.14830",
        "authors": "Jiayi Zhang, Chenxin Sun, Yue Gu, Qingyu Zhang, Jiayi Lin, Xiaojiang Du, Chenxiong Qian",
        "summary": "The gaming industry has experienced substantial growth, but cheating in online games poses a significant threat to the integrity of the gaming experience. Cheating, particularly in first-person shooter (FPS) games, can lead to substantial losses for the game industry. Existing anti-cheat solutions have limitations, such as client-side hardware constraints, security risks, server-side unreliable methods, and both-sides suffer from a lack of comprehensive real-world datasets. To address these limitations, the paper proposes HAWK, a server-side FPS anti-cheat framework for the popular game CS:GO. HAWK utilizes machine learning techniques to mimic human experts' identification process, leverages novel multi-view features, and it is equipped with a well-defined workflow. The authors evaluate HAWK with the first large and real-world datasets containing multiple cheat types and cheating sophistication, and it exhibits promising efficiency and acceptable overheads, shorter ban times compared to the in-use anti-cheat, a significant reduction in manual labor, and the ability to capture cheaters who evaded official inspections.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 09:00:07 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.796376"
    },
    {
        "index": "#127",
        "title": "Towards Real-world Deployment of NILM Systems: Challenges and Practices",
        "link": "/arxiv/2409.14821",
        "arxiv_id": "2409.14821",
        "authors": "Junyu Xue, Yu Zhang, Xudong Wang, Yi Wang, Guoming Tang",
        "summary": "Non-intrusive load monitoring (NILM), as a key load monitoring technology, can much reduce the deployment cost of traditional power sensors. Previous research has largely focused on developing cloud-exclusive NILM algorithms, which often result in high computation costs and significant service delays. To address these issues, we propose a three-tier framework to enhance the real-world applicability of NILM systems through edge-cloud collaboration. Considering the computational resources available at both the edge and cloud, we implement a lightweight NILM model at the edge and a deep learning based model at the cloud, respectively. In addition to the differential model implementations, we also design a NILM-specific deployment scheme that integrates Gunicorn and NGINX to bridge the gap between theoretical algorithms and practical applications. To verify the effectiveness of the proposed framework, we apply real-world NILM scenario settings and implement the entire process of data acquisition, model training, and system deployment. The results demonstrate that our framework can achieve high decomposition accuracy while significantly reducing the cloud workload and communication overhead under practical considerations.",
        "subjects": "Systems and Control, Artificial Intelligence",
        "date": "2024-09-23 08:54:05 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.796831"
    },
    {
        "index": "#130",
        "title": "VARADE: a Variational-based AutoRegressive model for Anomaly Detection on the Edge",
        "link": "/arxiv/2409.14816",
        "arxiv_id": "2409.14816",
        "authors": "Alessio Mascolini, Sebastiano Gaiardelli, Francesco Ponzio, Nicola Dall'Ora, Enrico Macii, Sara Vinco, Santa Di Cataldo, Franco Fummi",
        "summary": "Detecting complex anomalies on massive amounts of data is a crucial task in Industry 4.0, best addressed by deep learning. However, available solutions are computationally demanding, requiring cloud architectures prone to latency and bandwidth issues. This work presents VARADE, a novel solution implementing a light autoregressive framework based on variational inference, which is best suited for real-time execution on the edge. The proposed approach was validated on a robotic arm, part of a pilot production line, and compared with several state-of-the-art algorithms, obtaining the best trade-off between anomaly detection accuracy, power consumption and inference frequency on two different edge platforms.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-23 08:46:15 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.797503"
    },
    {
        "index": "#131",
        "title": "Research on Dynamic Data Flow Anomaly Detection based on Machine Learning",
        "link": "/arxiv/2409.14796",
        "arxiv_id": "2409.14796",
        "authors": "Liyang Wang, Yu Cheng, Hao Gong, Jiacheng Hu, Xirui Tang, Iris Li",
        "summary": "The sophistication and diversity of contemporary cyberattacks have rendered the use of proxies, gateways, firewalls, and encrypted tunnels as a standalone defensive strategy inadequate. Consequently, the proactive identification of data anomalies has emerged as a prominent area of research within the field of data security. The majority of extant studies concentrate on sample equilibrium data, with the consequence that the detection effect is not optimal in the context of unbalanced data. In this study, the unsupervised learning method is employed to identify anomalies in dynamic data flows. Initially, multi-dimensional features are extracted from real-time data, and a clustering algorithm is utilised to analyse the patterns of the data. This enables the potential outliers to be automatically identified. By clustering similar data, the model is able to detect data behaviour that deviates significantly from normal traffic without the need for labelled data. The results of the experiments demonstrate that the proposed method exhibits high accuracy in the detection of anomalies across a range of scenarios. Notably, it demonstrates robust and adaptable performance, particularly in the context of unbalanced data.",
        "subjects": "Machine Learning, Artificial Intelligence, Cryptography and Security",
        "date": "2024-09-23 08:19:15 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.797884"
    },
    {
        "index": "#138",
        "title": "PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs",
        "link": "/arxiv/2409.14729",
        "arxiv_id": "2409.14729",
        "authors": "Jiahao Yu, Yangguang Shao, Hanwen Miao, Junzheng Shi, Xinyu Xing",
        "summary": "Large Language Models (LLMs) have gained widespread use in various applications due to their powerful capability to generate human-like text. However, prompt injection attacks, which involve overwriting a model's original instructions with malicious prompts to manipulate the generated text, have raised significant concerns about the security and reliability of LLMs. Ensuring that LLMs are robust against such attacks is crucial for their deployment in real-world applications, particularly in critical tasks. In this paper, we propose PROMPTFUZZ, a novel testing framework that leverages fuzzing techniques to systematically assess the robustness of LLMs against prompt injection attacks. Inspired by software fuzzing, PROMPTFUZZ selects promising seed prompts and generates a diverse set of prompt injections to evaluate the target LLM's resilience. PROMPTFUZZ operates in two stages: the prepare phase, which involves selecting promising initial seeds and collecting few-shot examples, and the focus phase, which uses the collected examples to generate diverse, high-quality prompt injections. Using PROMPTFUZZ, we can uncover more vulnerabilities in LLMs, even those with strong defense prompts. By deploying the generated attack prompts from PROMPTFUZZ in a real-world competition, we achieved the 7th ranking out of over 4000 participants (top 0.14%) within 2 hours. Additionally, we construct a dataset to fine-tune LLMs for enhanced robustness against prompt injection attacks. While the fine-tuned model shows improved robustness, PROMPTFUZZ continues to identify vulnerabilities, highlighting the importance of robust testing for LLMs. Our work emphasizes the critical need for effective testing tools and provides a practical framework for evaluating and improving the robustness of LLMs against prompt injection attacks.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2024-09-23 06:08:32 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.799286"
    },
    {
        "index": "#146",
        "title": "zsLLMCode: An Effective Approach for Functional Code Embedding via LLM with Zero-Shot Learning",
        "link": "/arxiv/2409.14644",
        "arxiv_id": "2409.14644",
        "authors": "Zixiang Xian, Chenhui Cui, Rubing Huang, Chunrong Fang, Zhenyu Chen",
        "summary": "Regarding software engineering (SE) tasks, Large language models (LLMs) have the capability of zero-shot learning, which does not require training or fine-tuning, unlike pre-trained models (PTMs). However, LLMs are primarily designed for natural language output, and cannot directly produce intermediate embeddings from source code. They also face some challenges, for example, the restricted context length may prevent them from handling larger inputs, limiting their applicability to many SE tasks; while hallucinations may occur when LLMs are applied to complex downstream tasks. Motivated by the above facts, we propose zsLLMCode, a novel approach that generates functional code embeddings using LLMs. Our approach utilizes LLMs to convert source code into concise summaries through zero-shot learning, which is then transformed into functional code embeddings using specialized embedding models. This unsupervised approach eliminates the need for training and addresses the issue of hallucinations encountered with LLMs. To the best of our knowledge, this is the first approach that combines LLMs and embedding models to generate code embeddings. We conducted experiments to evaluate the performance of our approach. The results demonstrate the effectiveness and superiority of our approach over state-of-the-art unsupervised methods.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2024-09-23 01:03:15 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.800998"
    },
    {
        "index": "#147",
        "title": "Not Only the Last-Layer Features for Spurious Correlations: All Layer Deep Feature Reweighting",
        "link": "/arxiv/2409.14637",
        "arxiv_id": "2409.14637",
        "authors": "Humza Wajid Hameed, Geraldin Nanfack, Eugene Belilovsky",
        "summary": "Spurious correlations are a major source of errors for machine learning models, in particular when aiming for group-level fairness. It has been recently shown that a powerful approach to combat spurious correlations is to re-train the last layer on a balanced validation dataset, isolating robust features for the predictor. However, key attributes can sometimes be discarded by neural networks towards the last layer. In this work, we thus consider retraining a classifier on a set of features derived from all layers. We utilize a recently proposed feature selection strategy to select unbiased features from all the layers. We observe this approach gives significant improvements in worst-group accuracy on several standard benchmarks.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-23 00:31:39 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.801183"
    },
    {
        "index": "#148",
        "title": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination",
        "link": "/arxiv/2409.14634",
        "arxiv_id": "2409.14634",
        "authors": "Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S. Weld",
        "summary": "The scientific ideation process often involves blending salient aspects of existing papers to create new ideas. To see if large language models (LLMs) can assist this process, we contribute Scideator, a novel mixed-initiative tool for scientific ideation. Starting from a user-provided set of papers, Scideator extracts key facets (purposes, mechanisms, and evaluations) from these and relevant papers, allowing users to explore the idea space by interactively recombining facets to synthesize inventive ideas. Scideator also helps users to gauge idea novelty by searching the literature for potential overlaps and showing automated novelty assessments and explanations. To support these tasks, Scideator introduces four LLM-powered retrieval-augmented generation (RAG) modules: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty Checker, and Idea Novelty Iterator. In a within-subjects user study, 19 computer-science researchers identified significantly more interesting ideas using Scideator compared to a strong baseline combining a scientific search engine with LLM interaction.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2024-09-23 00:09:34 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.801397"
    },
    {
        "index": "#149",
        "title": "Hierarchical end-to-end autonomous navigation through few-shot waypoint detection",
        "link": "/arxiv/2409.14633",
        "arxiv_id": "2409.14633",
        "authors": "Amin Ghafourian, Zhongying CuiZhu, Debo Shi, Ian Chuang, Francois Charette, Rithik Sachdeva, Iman Soltani",
        "summary": "Human navigation is facilitated through the association of actions with landmarks, tapping into our ability to recognize salient features in our environment. Consequently, navigational instructions for humans can be extremely concise, such as short verbal descriptions, indicating a small memory requirement and no reliance on complex and overly accurate navigation tools. Conversely, current autonomous navigation schemes rely on accurate positioning devices and algorithms as well as extensive streams of sensory data collected from the environment. Inspired by this human capability and motivated by the associated technological gap, in this work we propose a hierarchical end-to-end meta-learning scheme that enables a mobile robot to navigate in a previously unknown environment upon presentation of only a few sample images of a set of landmarks along with their corresponding high-level navigation actions. This dramatically simplifies the wayfinding process and enables easy adoption to new environments. For few-shot waypoint detection, we implement a metric-based few-shot learning technique through distribution embedding. Waypoint detection triggers the multi-task low-level maneuver controller module to execute the corresponding high-level navigation action. We demonstrate the effectiveness of the scheme using a small-scale autonomous vehicle on novel indoor navigation tasks in several previously unseen environments.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 00:03:39 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.801647"
    },
    {
        "index": "#151",
        "title": "LatentQGAN: A Hybrid QGAN with Classical Convolutional Autoencoder",
        "link": "/arxiv/2409.14622",
        "arxiv_id": "2409.14622",
        "authors": "Vieloszynski Alexis, Soumaya Cherkaoui, Jean-Frédéric Laprade, Oliver Nahman-Lévesque, Abdallah Aaraba, Shengrui Wang",
        "summary": "Quantum machine learning consists in taking advantage of quantum computations to generate classical data. A potential application of quantum machine learning is to harness the power of quantum computers for generating classical data, a process essential to a multitude of applications such as enriching training datasets, anomaly detection, and risk management in finance. Given the success of Generative Adversarial Networks in classical image generation, the development of its quantum versions has been actively conducted. However, existing implementations on quantum computers often face significant challenges, such as scalability and training convergence issues. To address these issues, we propose LatentQGAN, a novel quantum model that uses a hybrid quantum-classical GAN coupled with an autoencoder. Although it was initially designed for image generation, the LatentQGAN approach holds potential for broader application across various practical data generation tasks. Experimental outcomes on both classical simulators and noisy intermediate scale quantum computers have demonstrated significant performance enhancements over existing quantum methods, alongside a significant reduction in quantum resources overhead.",
        "subjects": "Quantum Physics, Artificial Intelligence, Machine Learning",
        "date": "2024-09-22 23:18:06 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.802058"
    },
    {
        "index": "#153",
        "title": "Testing Causal Models with Hidden Variables in Polynomial Delay via Conditional Independencies",
        "link": "/arxiv/2409.14593",
        "arxiv_id": "2409.14593",
        "authors": "Hyunchai Jeong, Adiba Ejaz, Jin Tian, Elias Bareinboim",
        "summary": "Testing a hypothesized causal model against observational data is a key prerequisite for many causal inference tasks. A natural approach is to test whether the conditional independence relations (CIs) assumed in the model hold in the data. While a model can assume exponentially many CIs (with respect to the number of variables), testing all of them is both impractical and unnecessary. Causal graphs, which encode these CIs in polynomial space, give rise to local Markov properties that enable model testing with a significantly smaller subset of CIs. Model testing based on local properties requires an algorithm to list the relevant CIs. However, existing algorithms for realistic settings with hidden variables and non-parametric distributions can take exponential time to produce even a single CI constraint. In this paper, we introduce the c-component local Markov property (C-LMP) for causal graphs with hidden variables. Since C-LMP can still invoke an exponential number of CIs, we develop a polynomial delay algorithm to list these CIs in poly-time intervals. To our knowledge, this is the first algorithm that enables poly-delay testing of CIs in causal graphs with hidden variables against arbitrary data distributions. Experiments on real-world and synthetic data demonstrate the practicality of our algorithm.",
        "subjects": "Machine Learning, Artificial Intelligence, Methodology, Machine Learning",
        "date": "2024-09-22 21:05:56 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.802455"
    },
    {
        "index": "#154",
        "title": "Explainable AI needs formal notions of explanation correctness",
        "link": "/arxiv/2409.14590",
        "arxiv_id": "2409.14590",
        "authors": "Stefan Haufe, Rick Wilming, Benedict Clark, Rustam Zhumagambetov, Danny Panknin, Ahcène Boubekki",
        "summary": "The use of machine learning (ML) in critical domains such as medicine poses risks and requires regulation. One requirement is that decisions of ML systems in high-risk applications should be human-understandable. The field of \"explainable artificial intelligence\" (XAI) seemingly addresses this need. However, in its current form, XAI is unfit to provide quality control for ML; it itself needs scrutiny. Popular XAI methods cannot reliably answer important questions about ML models, their training data, or a given test input. We recapitulate results demonstrating that popular XAI methods systematically attribute importance to input features that are independent of the prediction target. This limits their utility for purposes such as model and data (in)validation, model improvement, and scientific discovery. We argue that the fundamental reason for this limitation is that current XAI methods do not address well-defined problems and are not evaluated against objective criteria of explanation correctness. Researchers should formally define the problems they intend to solve first and then design methods accordingly. This will lead to notions of explanation correctness that can be theoretically verified and objective metrics of explanation performance that can be assessed using ground-truth data.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2024-09-22 20:47:04 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.802665"
    },
    {
        "index": "#157",
        "title": "Combating Spatial Disorientation in a Dynamic Self-Stabilization Task Using AI Assistants",
        "link": "/arxiv/2409.14565",
        "arxiv_id": "2409.14565",
        "authors": "Sheikh Mannan, Paige Hansen, Vivekanand Pandey Vimal, Hannah N. Davies, Paul DiZio, Nikhil Krishnaswamy",
        "summary": "Spatial disorientation is a leading cause of fatal aircraft accidents. This paper explores the potential of AI agents to aid pilots in maintaining balance and preventing unrecoverable losses of control by offering cues and corrective measures that ameliorate spatial disorientation. A multi-axis rotation system (MARS) was used to gather data from human subjects self-balancing in a spaceflight analog condition. We trained models over this data to create \"digital twins\" that exemplified performance characteristics of humans with different proficiency levels. We then trained various reinforcement learning and deep learning models to offer corrective cues if loss of control is predicted. Digital twins and assistant models then co-performed a virtual inverted pendulum (VIP) programmed with identical physics. From these simulations, we picked the 5 best-performing assistants based on task metrics such as crash frequency and mean distance from the direction of balance. These were used in a co-performance study with 20 new human subjects performing a version of the VIP task with degraded spatial information. We show that certain AI assistants were able to improve human performance and that reinforcement-learning based assistants were objectively more effective but rated as less trusted and preferable by humans.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Machine Learning, Multiagent Systems, Robotics",
        "date": "2024-09-09 21:06:22 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.803868"
    },
    {
        "index": "#158",
        "title": "RACOON: An LLM-based Framework for Retrieval-Augmented Column Type Annotation with a Knowledge Graph",
        "link": "/arxiv/2409.14556",
        "arxiv_id": "2409.14556",
        "authors": "Linxi Wei, Guorui Xiao, Magdalena Balazinska",
        "summary": "As an important component of data exploration and integration, Column Type Annotation (CTA) aims to label columns of a table with one or more semantic types. With the recent development of Large Language Models (LLMs), researchers have started to explore the possibility of using LLMs for CTA, leveraging their strong zero-shot capabilities. In this paper, we build on this promising work and improve on LLM-based methods for CTA by showing how to use a Knowledge Graph (KG) to augment the context information provided to the LLM. Our approach, called RACOON, combines both pre-trained parametric and non-parametric knowledge during generation to improve LLMs' performance on CTA. Our experiments show that RACOON achieves up to a 0.21 micro F-1 improvement compared against vanilla LLM inference.",
        "subjects": "Databases, Artificial Intelligence",
        "date": "2024-09-22 18:39:27 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.804058"
    },
    {
        "index": "#162",
        "title": "TabGraphs: A Benchmark and Strong Baselines for Learning on Graphs with Tabular Features",
        "link": "/arxiv/2409.14500",
        "arxiv_id": "2409.14500",
        "authors": "Gleb Bazhenov, Oleg Platonov, Liudmila Prokhorenkova",
        "summary": "Tabular machine learning is an important field for industry and science. In this field, table rows are usually treated as independent data samples, but additional information about relations between them is sometimes available and can be used to improve predictive performance. Such information can be naturally modeled with a graph, thus tabular machine learning may benefit from graph machine learning methods. However, graph machine learning models are typically evaluated on datasets with homogeneous node features, which have little in common with heterogeneous mixtures of numerical and categorical features present in tabular datasets. Thus, there is a critical difference between the data used in tabular and graph machine learning studies, which does not allow one to understand how successfully graph models can be transferred to tabular data. To bridge this gap, we propose a new benchmark of diverse graphs with heterogeneous tabular node features and realistic prediction tasks. We use this benchmark to evaluate a vast set of models, including simple methods previously overlooked in the literature. Our experiments show that graph neural networks (GNNs) can indeed often bring gains in predictive performance for tabular data, but standard tabular models also can be adapted to work with graph data by using simple feature preprocessing, which sometimes enables them to compete with and even outperform GNNs. Based on our empirical study, we provide insights for researchers and practitioners in both tabular and graph machine learning fields.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-22 15:53:19 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.804852"
    },
    {
        "index": "#164",
        "title": "Enhancing LLM-based Autonomous Driving Agents to Mitigate Perception Attacks",
        "link": "/arxiv/2409.14488",
        "arxiv_id": "2409.14488",
        "authors": "Ruoyu Song, Muslum Ozgur Ozmen, Hyungsub Kim, Antonio Bianchi, Z. Berkay Celik",
        "summary": "There is a growing interest in integrating Large Language Models (LLMs) with autonomous driving (AD) systems. However, AD systems are vulnerable to attacks against their object detection and tracking (ODT) functions. Unfortunately, our evaluation of four recent LLM agents against ODT attacks shows that the attacks are 63.26% successful in causing them to crash or violate traffic rules due to (1) misleading memory modules that provide past experiences for decision making, (2) limitations of prompts in identifying inconsistencies, and (3) reliance on ground truth perception data. In this paper, we introduce Hudson, a driving reasoning agent that extends prior LLM-based driving systems to enable safer decision making during perception attacks while maintaining effectiveness under benign conditions. Hudson achieves this by first instrumenting the AD software to collect real-time perception results and contextual information from the driving scene. This data is then formalized into a domain-specific language (DSL). To guide the LLM in detecting and making safe control decisions during ODT attacks, Hudson translates the DSL into natural language, along with a list of custom attack detection instructions. Following query execution, Hudson analyzes the LLM's control decision to understand its causal reasoning process. We evaluate the effectiveness of Hudson using a proprietary LLM (GPT-4) and two open-source LLMs (Llama and Gemma) in various adversarial driving scenarios. GPT-4, Llama, and Gemma achieve, on average, an attack detection accuracy of 83. 3%, 63. 6%, and 73. 6%. Consequently, they make safe control decisions in 86.4%, 73.9%, and 80% of the attacks. Our results, following the growing interest in integrating LLMs into AD systems, highlight the strengths of LLMs and their potential to detect and mitigate ODT attacks.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2024-09-22 15:18:59 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.805228"
    },
    {
        "index": "#168",
        "title": "A Visualized Malware Detection Framework with CNN and Conditional GAN",
        "link": "/arxiv/2409.14439",
        "arxiv_id": "2409.14439",
        "authors": "Fang Wang, Hussam Al Hamadi, Ernesto Damiani",
        "summary": "Malware visualization analysis incorporating with Machine Learning (ML) has been proven to be a promising solution for improving security defenses on different platforms. In this work, we propose an integrated framework for addressing common problems experienced by ML utilizers in developing malware detection systems. Namely, a pictorial presentation system with extensions is designed to preserve the identities of benign/malign samples by encoding each variable into binary digits and mapping them into black and white pixels. A conditional Generative Adversarial Network based model is adopted to produce synthetic images and mitigate issues of imbalance classes. Detection models architected by Convolutional Neural Networks are for validating performances while training on datasets with and without artifactual samples. Result demonstrates accuracy rates of 98.51% and 97.26% for these two training scenarios.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Machine Learning",
        "date": "2024-09-22 13:29:10 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.806161"
    },
    {
        "index": "#171",
        "title": "Challenging the Performance-Interpretability Trade-off: An Evaluation of Interpretable Machine Learning Models",
        "link": "/arxiv/2409.14429",
        "arxiv_id": "2409.14429",
        "authors": "Sven Kruschel, Nico Hambauer, Sven Weinzierl, Sandra Zilker, Mathias Kraus, Patrick Zschech",
        "summary": "Machine learning is permeating every conceivable domain to promote data-driven decision support. The focus is often on advanced black-box models due to their assumed performance advantages, whereas interpretable models are often associated with inferior predictive qualities. More recently, however, a new generation of generalized additive models (GAMs) has been proposed that offer promising properties for capturing complex, non-linear patterns while remaining fully interpretable. To uncover the merits and limitations of these models, this study examines the predictive performance of seven different GAMs in comparison to seven commonly used machine learning models based on a collection of twenty tabular benchmark datasets. To ensure a fair and robust model comparison, an extensive hyperparameter search combined with cross-validation was performed, resulting in 68,500 model runs. In addition, this study qualitatively examines the visual output of the models to assess their level of interpretability. Based on these results, the paper dispels the misconception that only black-box models can achieve high accuracy by demonstrating that there is no strict trade-off between predictive performance and model interpretability for tabular data. Furthermore, the paper discusses the importance of GAMs as powerful interpretable models for the field of information systems and derives implications for future work from a socio-technical perspective.",
        "subjects": "Machine Learning, Artificial Intelligence, Human-Computer Interaction, Neural and Evolutionary Computing",
        "date": "2024-09-22 12:58:52 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.806804"
    },
    {
        "index": "#173",
        "title": "COSBO: Conservative Offline Simulation-Based Policy Optimization",
        "link": "/arxiv/2409.14412",
        "arxiv_id": "2409.14412",
        "authors": "Eshagh Kargar, Ville Kyrki",
        "summary": "Offline reinforcement learning allows training reinforcement learning models on data from live deployments. However, it is limited to choosing the best combination of behaviors present in the training data. In contrast, simulation environments attempting to replicate the live environment can be used instead of the live data, yet this approach is limited by the simulation-to-reality gap, resulting in a bias. In an attempt to get the best of both worlds, we propose a method that combines an imperfect simulation environment with data from the target environment, to train an offline reinforcement learning policy. Our experiments demonstrate that the proposed method outperforms state-of-the-art approaches CQL, MOPO, and COMBO, especially in scenarios with diverse and challenging dynamics, and demonstrates robust behavior across a variety of experimental conditions. The results highlight that using simulator-generated data can effectively enhance offline policy learning despite the sim-to-real gap, when direct interaction with the real-world is not possible.",
        "subjects": "Machine Learning, Artificial Intelligence, Robotics",
        "date": "2024-09-22 12:20:55 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.807198"
    },
    {
        "index": "#175",
        "title": "Sparse Low-Ranked Self-Attention Transformer for Remaining Useful Lifetime Prediction of Optical Fiber Amplifiers",
        "link": "/arxiv/2409.14378",
        "arxiv_id": "2409.14378",
        "authors": "Dominic Schneider, Lutz Rapp",
        "summary": "Optical fiber amplifiers are key elements in present optical networks. Failures of these components result in high financial loss of income of the network operator as the communication traffic over an affected link is interrupted. Applying Remaining useful lifetime (RUL) prediction in the context of Predictive Maintenance (PdM) to optical fiber amplifiers to predict upcoming system failures at an early stage, so that network outages can be minimized through planning of targeted maintenance actions, ensures reliability and safety. Optical fiber amplifier are complex systems, that work under various operating conditions, which makes correct forecasting a difficult task. Increased monitoring capabilities of systems results in datasets that facilitate the application of data-driven RUL prediction methods. Deep learning models in particular have shown good performance, but generalization based on comparatively small datasets for RUL prediction is difficult. In this paper, we propose Sparse Low-ranked self-Attention Transformer (SLAT) as a novel RUL prediction method. SLAT is based on an encoder-decoder architecture, wherein two parallel working encoders extract features for sensors and time steps. By utilizing the self-attention mechanism, long-term dependencies can be learned from long sequences. The implementation of sparsity in the attention matrix and a low-rank parametrization reduce overfitting and increase generalization. Experimental application to optical fiber amplifiers exemplified on EDFA, as well as a reference dataset from turbofan engines, shows that SLAT outperforms the state-of-the-art methods.",
        "subjects": "Machine Learning, Artificial Intelligence, Signal Processing",
        "date": "2024-09-22 09:48:45 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.807614"
    },
    {
        "index": "#176",
        "title": "Evaluating the Quality of Code Comments Generated by Large Language Models for Novice Programmers",
        "link": "/arxiv/2409.14368",
        "arxiv_id": "2409.14368",
        "authors": "Aysa Xuemo Fan, Arun Balajiee Lekshmi Narayanan, Mohammad Hassany, Jiaze Ke",
        "summary": "Large Language Models (LLMs) show promise in generating code comments for novice programmers, but their educational effectiveness remains under-evaluated. This study assesses the instructional quality of code comments produced by GPT-4, GPT-3.5-Turbo, and Llama2, compared to expert-developed comments, focusing on their suitability for novices. Analyzing a dataset of ``easy'' level Java solutions from LeetCode, we find that GPT-4 exhibits comparable quality to expert comments in aspects critical for beginners, such as clarity, beginner-friendliness, concept elucidation, and step-by-step guidance. GPT-4 outperforms Llama2 in discussing complexity (chi-square = 11.40, p = 0.001) and is perceived as significantly more supportive for beginners than GPT-3.5 and Llama2 with Mann-Whitney U-statistics = 300.5 and 322.5, p = 0.0017 and 0.0003). This study highlights the potential of LLMs for generating code comments tailored to novice programmers.",
        "subjects": "Software Engineering, Artificial Intelligence, Human-Computer Interaction",
        "date": "2024-09-22 09:03:48 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.807825"
    },
    {
        "index": "#177",
        "title": "Data-Driven Spatiotemporal Feature Representation and Mining in Multidimensional Time Series",
        "link": "/arxiv/2409.14327",
        "arxiv_id": "2409.14327",
        "authors": "Xu Yan, Yaoting Jiang, Wenyi Liu, Didi Yi, Haoyang Sang, Jianjun Wei",
        "summary": "This paper explores a new method for time series data analysis, aiming to overcome the limitations of traditional mining techniques when dealing with multidimensional time series data. Time series data are extensively utilized in diverse fields, including backend services for monitoring and optimizing IT infrastructure, medical diagnosis through continuous patient monitoring and health trend analysis, and internet business for tracking user behavior and forecasting sales. However, since the effective information in time series data is often hidden in sequence fragments, the uncertainty of their length, quantity, and morphological variables brings challenges to mining. To this end, this paper proposes a new spatiotemporal feature representation method, which converts multidimensional time series (MTS) into one-dimensional event sequences by transforming spatially varying events, and uses a series of event symbols to represent the spatial structural information of multidimensional coupling in the sequence, which has good interpretability. Then, this paper introduces a variable-length tuple mining method to extract non-redundant key event subsequences in event sequences as spatiotemporal structural features of motion sequences. This method is an unsupervised method that does not rely on large-scale training samples and defines a new model for representing the spatiotemporal structural features of multidimensional time series. The superior performance of the STEM model is verified by pattern classification experiments on a variety of motion sequences. The research results of this paper provide an important theoretical basis and technical support for understanding and predicting human behavior patterns, and have far-reaching practical application value.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-22 06:27:07 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.808028"
    },
    {
        "index": "#183",
        "title": "Proof Automation with Large Language Models",
        "link": "/arxiv/2409.14274",
        "arxiv_id": "2409.14274",
        "authors": "Minghai Lu, Benjamin Delaware, Tianyi Zhang",
        "summary": "Interactive theorem provers such as Coq are powerful tools to formally guarantee the correctness of software. However, using these tools requires significant manual effort and expertise. While Large Language Models (LLMs) have shown promise in automatically generating informal proofs in natural language, they are less effective at generating formal proofs in interactive theorem provers. In this paper, we conduct a formative study to identify common mistakes made by LLMs when asked to generate formal proofs. By analyzing 520 proof generation errors made by GPT-3.5, we found that GPT-3.5 often identified the correct high-level structure of a proof, but struggled to get the lower-level details correct. Based on this insight, we propose PALM, a novel generate-then-repair approach that first prompts an LLM to generate an initial proof and then leverages targeted symbolic methods to iteratively repair low-level problems. We evaluate PALM on a large dataset that includes more than 10K theorems. Our results show that PALM significantly outperforms other state-of-the-art approaches, successfully proving 76.6% to 180.4% more theorems. Moreover, PALM proves 1270 theorems beyond the reach of existing approaches. We also demonstrate the generalizability of PALM across different LLMs.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning, Logic in Computer Science, Programming Languages",
        "date": "2024-09-22 00:19:27 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.809441"
    },
    {
        "index": "#184",
        "title": "Higher-order-ReLU-KANs (HRKANs) for solving physics-informed neural networks (PINNs) more accurately, robustly and faster",
        "link": "/arxiv/2409.14248",
        "arxiv_id": "2409.14248",
        "authors": "Chi Chiu So, Siu Pang Yung",
        "summary": "Finding solutions to partial differential equations (PDEs) is an important and essential component in many scientific and engineering discoveries. One of the common approaches empowered by deep learning is Physics-informed Neural Networks (PINNs). Recently, a new type of fundamental neural network model, Kolmogorov-Arnold Networks (KANs), has been proposed as a substitute of Multilayer Perceptions (MLPs), and possesses trainable activation functions. To enhance KANs in fitting accuracy, a modification of KANs, so called ReLU-KANs, using \"square of ReLU\" as the basis of its activation functions has been suggested. In this work, we propose another basis of activation functions, namely, Higher-order-ReLU, which is simpler than the basis of activation functions used in KANs, namely, B-splines; allows efficient KAN matrix operations; and possesses smooth and non-zero higher-order derivatives, essential for physics-informed neural networks. Our detailed experiments on two standard and typical PDEs, namely, the linear Poisson equation and nonlinear Burgers' equation with viscosity, reveal that our proposed Higher-order-ReLU-KANs (HRKANs) achieve the highest fitting accuracy and training robustness and lowest training time significantly among KANs, ReLUKANs and HRKANs.",
        "subjects": "Neural and Evolutionary Computing, Artificial Intelligence, Computational Engineering, Finance, and Science, Machine Learning, Computational Physics",
        "date": "2024-08-09 03:50:58 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.809641"
    },
    {
        "index": "#185",
        "title": "An Instance-based Plus Ensemble Learning Method for Classification of Scientific Papers",
        "link": "/arxiv/2409.14237",
        "arxiv_id": "2409.14237",
        "authors": "Fang Zhang, Shengli Wu",
        "summary": "The exponential growth of scientific publications in recent years has posed a significant challenge in effective and efficient categorization. This paper introduces a novel approach that combines instance-based learning and ensemble learning techniques for classifying scientific papers into relevant research fields. Working with a classification system with a group of research fields, first a number of typical seed papers are allocated to each of the fields manually. Then for each paper that needs to be classified, we compare it with all the seed papers in every field. Contents and citations are considered separately. An ensemble-based method is then employed to make the final decision. Experimenting with the datasets from DBLP, our experimental results demonstrate that the proposed classification method is effective and efficient in categorizing papers into various research areas. We also find that both content and citation features are useful for the classification of scientific papers.",
        "subjects": "Digital Libraries, Artificial Intelligence",
        "date": "2024-09-21 19:42:15 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.809876"
    },
    {
        "index": "#186",
        "title": "MEGA-PT: A Meta-Game Framework for Agile Penetration Testing",
        "link": "/arxiv/2409.14219",
        "arxiv_id": "2409.14219",
        "authors": "Yunfei Ge, Quanyan Zhu",
        "summary": "Penetration testing is an essential means of proactive defense in the face of escalating cybersecurity incidents. Traditional manual penetration testing methods are time-consuming, resource-intensive, and prone to human errors. Current trends in automated penetration testing are also impractical, facing significant challenges such as the curse of dimensionality, scalability issues, and lack of adaptability to network changes. To address these issues, we propose MEGA-PT, a meta-game penetration testing framework, featuring micro tactic games for node-level local interactions and a macro strategy process for network-wide attack chains. The micro- and macro-level modeling enables distributed, adaptive, collaborative, and fast penetration testing. MEGA-PT offers agile solutions for various security schemes, including optimal local penetration plans, purple teaming solutions, and risk assessment, providing fundamental principles to guide future automated penetration testing. Our experiments demonstrate the effectiveness and agility of our model by providing improved defense strategies and adaptability to changes at both local and network levels.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Computer Science and Game Theory",
        "date": "2024-09-21 18:46:29 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.810068"
    },
    {
        "index": "#188",
        "title": "Data-Driven Approach to assess and identify gaps in healthcare set up in South Asia",
        "link": "/arxiv/2409.14194",
        "arxiv_id": "2409.14194",
        "authors": "Rusham Elahi, Zia Tahseen, Tehreem Fatima, Syed Wafa Zahra, Hafiz Muhammad Abubakar, Tehreem Zafar, Aqs Younas, Muhammad Talha Quddoos, Usman Nazir",
        "summary": "Primary healthcare is a crucial strategy for achieving universal health coverage. South Asian countries are working to improve their primary healthcare system through their country specific policies designed in line with WHO health system framework using the six thematic pillars: Health Financing, Health Service delivery, Human Resource for Health, Health Information Systems, Governance, Essential Medicines and Technology, and an addition area of Cross-Sectoral Linkages. Measuring the current accessibility of healthcare facilities and workforce availability is essential for improving healthcare standards and achieving universal health coverage in developing countries. Data-driven surveillance approaches are required that can provide rapid, reliable, and geographically scalable solutions to understand a) which communities and areas are most at risk of inequitable access and when, b) what barriers to health access exist, and c) how they can be overcome in ways tailored to the specific challenges faced by individual communities. We propose to harness current breakthroughs in Earth-observation (EO) technology, which provide the ability to generate accurate, up-to-date, publicly accessible, and reliable data, which is necessary for equitable access planning and resource allocation to ensure that vaccines, and other interventions reach everyone, particularly those in greatest need, during normal and crisis times. This requires collaboration among countries to identify evidence based solutions to shape health policy and interventions, and drive innovations and research in the region.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2024-09-21 16:50:16 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.810550"
    },
    {
        "index": "#189",
        "title": "PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach",
        "link": "/arxiv/2409.14177",
        "arxiv_id": "2409.14177",
        "authors": "Zhihao Lin, Wei Ma, Mingyi Zhou, Yanjie Zhao, Haoyu Wang, Yang Liu, Jun Wang, Li Li",
        "summary": "In recent years, Large Language Models (LLMs) have gained widespread use, accompanied by increasing concerns over their security. Traditional jailbreak attacks rely on internal model details or have limitations when exploring the unsafe behavior of the victim model, limiting their generalizability. In this paper, we introduce PathSeeker, a novel black-box jailbreak method inspired by the concept of escaping a security maze. This work is inspired by the game of rats escaping a maze. We think that each LLM has its unique \"security maze\", and attackers attempt to find the exit learning from the received feedback and their accumulated experience to compromise the target LLM's security defences. Our approach leverages multi-agent reinforcement learning, where smaller models collaborate to guide the main LLM in performing mutation operations to achieve the attack objectives. By progressively modifying inputs based on the model's feedback, our system induces richer, harmful responses. During our manual attempts to perform jailbreak attacks, we found that the vocabulary of the response of the target model gradually became richer and eventually produced harmful responses. Based on the observation, we also introduce a reward mechanism that exploits the expansion of vocabulary richness in LLM responses to weaken security constraints. Our method outperforms five state-of-the-art attack techniques when tested across 13 commercial and open-source LLMs, achieving high attack success rates, especially in strongly aligned commercial models like GPT-4o-mini, Claude-3.5, and GLM-4-air with strong safety alignment. This study aims to improve the understanding of LLM security vulnerabilities and we hope that this sturdy can contribute to the development of more robust defenses.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2024-09-21 15:36:26 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.810767"
    },
    {
        "index": "#194",
        "title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
        "link": "/arxiv/2409.14084",
        "arxiv_id": "2409.14084",
        "authors": "Fabio Ferreira, Moreno Schlageter, Raghu Rajan, Andre Biedenkapp, Frank Hutter",
        "summary": "A World Model is a compressed spatial and temporal representation of a real world environment that allows one to train an agent or execute planning methods. However, world models are typically trained on observations from the real world environment, and they usually do not enable learning policies for other real environments. We propose One-Shot World Model (OSWM), a transformer world model that is learned in an in-context learning fashion from purely synthetic data sampled from a prior distribution. Our prior is composed of multiple randomly initialized neural networks, where each network models the dynamics of each state and reward dimension of a desired target environment. We adopt the supervised learning procedure of Prior-Fitted Networks by masking next-state and reward at random context positions and query OSWM to make probabilistic predictions based on the remaining transition context. During inference time, OSWM is able to quickly adapt to the dynamics of a simple grid world, as well as the CartPole gym and a custom control environment by providing 1k transition steps as context and is then able to successfully train environment-solving agent policies. However, transferring to more complex environments remains a challenge, currently. Despite these limitations, we see this work as an important stepping-stone in the pursuit of learning world models purely from synthetic data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-21 09:39:32 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.811809"
    },
    {
        "index": "#196",
        "title": "N-Version Assessment and Enhancement of Generative AI",
        "link": "/arxiv/2409.14071",
        "arxiv_id": "2409.14071",
        "authors": "Marcus Kessel, Colin Atkinson",
        "summary": "Generative AI (GAI) holds great potential to improve software engineering productivity, but its untrustworthy outputs, particularly in code synthesis, pose significant challenges. The need for extensive verification and validation (V&V) of GAI-generated artifacts may undermine the potential productivity gains. This paper proposes a way of mitigating these risks by exploiting GAI's ability to generate multiple versions of code and tests to facilitate comparative analysis across versions. Rather than relying on the quality of a single test or code module, this \"differential GAI\" (D-GAI) approach promotes more reliable quality evaluation through version diversity. We introduce the Large-Scale Software Observatorium (LASSO), a platform that supports D-GAI by executing and analyzing large sets of code versions and tests. We discuss how LASSO enables rigorous evaluation of GAI-generated artifacts and propose its application in both software development and GAI research.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2024-09-21 09:00:16 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.812175"
    },
    {
        "index": "#197",
        "title": "KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation without Robot Data",
        "link": "/arxiv/2409.14066",
        "arxiv_id": "2409.14066",
        "authors": "Grace Tang, Swetha Rajkumar, Yifei Zhou, Homer Rich Walke, Sergey Levine, Kuan Fang",
        "summary": "Building generalist robotic systems involves effectively endowing robots with the capabilities to handle novel objects in an open-world setting. Inspired by the advances of large pre-trained models, we propose Keypoint Affordance Learning from Imagined Environments (KALIE), which adapts pre-trained Vision Language Models (VLMs) for robotic control in a scalable manner. Instead of directly producing motor commands, KALIE controls the robot by predicting point-based affordance representations based on natural language instructions and visual observations of the scene. The VLM is trained on 2D images with affordances labeled by humans, bypassing the need for training data collected on robotic systems. Through an affordance-aware data synthesis pipeline, KALIE automatically creates massive high-quality training data based on limited example data manually collected by humans. We demonstrate that KALIE can learn to robustly solve new manipulation tasks with unseen objects given only 50 example data points. Compared to baselines using pre-trained VLMs, our approach consistently achieves superior performance.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2024-09-21 08:45:16 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.812383"
    },
    {
        "index": "#199",
        "title": "PepINVENT: Generative peptide design beyond the natural amino acids",
        "link": "/arxiv/2409.14040",
        "arxiv_id": "2409.14040",
        "authors": "Gökçe Geylan, Jon Paul Janet, Alessandro Tibo, Jiazhen He, Atanas Patronov, Mikhail Kabeshov, Florian David, Werngard Czechtizky, Ola Engkvist, Leonardo De Maria",
        "summary": "Peptides play a crucial role in the drug design and discovery whether as a therapeutic modality or a delivery agent. Non-natural amino acids (NNAAs) have been used to enhance the peptide properties from binding affinity, plasma stability to permeability. Incorporating novel NNAAs facilitates the design of more effective peptides with improved properties. The generative models used in the field, have focused on navigating the peptide sequence space. The sequence space is formed by combinations of a predefined set of amino acids. However, there is still a need for a tool to explore the peptide landscape beyond this enumerated space to unlock and effectively incorporate de novo design of new amino acids. To thoroughly explore the theoretical chemical space of the peptides, we present PepINVENT, a novel generative AI-based tool as an extension to the small molecule molecular design platform, REINVENT. PepINVENT navigates the vast space of natural and non-natural amino acids to propose valid, novel, and diverse peptide designs. The generative model can serve as a central tool for peptide-related tasks, as it was not trained on peptides with specific properties or topologies. The prior was trained to understand the granularity of peptides and to design amino acids for filling the masked positions within a peptide. PepINVENT coupled with reinforcement learning enables the goal-oriented design of peptides using its chemistry-informed generative capabilities. This study demonstrates PepINVENT's ability to explore the peptide space with unique and novel designs, and its capacity for property optimization in the context of therapeutically relevant peptides. Our tool can be employed for multi-parameter learning objectives, peptidomimetics, lead optimization, and variety of other tasks within the peptide domain.",
        "subjects": "Biomolecules, Artificial Intelligence",
        "date": "2024-09-21 06:53:03 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.812980"
    },
    {
        "index": "#202",
        "title": "FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer on UltraScale+ FPGAs",
        "link": "/arxiv/2409.14023",
        "arxiv_id": "2409.14023",
        "authors": "Ehsan Kabir, Md. Arafat Kabir, Austin R. J. Downey, Jason D. Bakos, David Andrews, Miaoqing Huang",
        "summary": "Transformer neural networks (TNNs) are being applied across a widening range of application domains, including natural language processing (NLP), machine translation, and computer vision (CV). Their popularity is largely attributed to the exceptional performance of their multi-head self-attention blocks when analyzing sequential data and extracting features. To date, there are limited hardware accelerators tailored for this mechanism, which is the first step before designing an accelerator for a complete model. This paper proposes \\textit{FAMOUS}, a flexible hardware accelerator for dense multi-head attention (MHA) computation of TNNs on field-programmable gate arrays (FPGAs). It is optimized for high utilization of processing elements and on-chip memories to improve parallelism and reduce latency. An efficient tiling of large matrices has been employed to distribute memory and computing resources across different modules on various FPGA platforms. The design is evaluated on Xilinx Alveo U55C and U200 data center cards containing Ultrascale+ FPGAs. Experimental results are presented that show that it can attain a maximum throughput, number of parallel attention heads, embedding dimension and tile size of 328 (giga operations/second (GOPS)), 8, 768 and 64 respectively on the U55C. Furthermore, it is 3.28$\\times$ and 2.6$\\times$ faster than the Intel Xeon Gold 5220R CPU and NVIDIA V100 GPU respectively. It is also 1.3$\\times$ faster than the fastest state-of-the-art FPGA-based accelerator.",
        "subjects": "Hardware Architecture, Artificial Intelligence, Machine Learning",
        "date": "2024-09-21 05:25:46 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.818817"
    },
    {
        "index": "#205",
        "title": "Enhancing Multivariate Time Series-based Solar Flare Prediction with Multifaceted Preprocessing and Contrastive Learning",
        "link": "/arxiv/2409.14016",
        "arxiv_id": "2409.14016",
        "authors": "MohammadReza EskandariNasab, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi",
        "summary": "Accurate solar flare prediction is crucial due to the significant risks that intense solar flares pose to astronauts, space equipment, and satellite communication systems. Our research enhances solar flare prediction by utilizing advanced data preprocessing and classification methods on a multivariate time series-based dataset of photospheric magnetic field parameters. First, our study employs a novel preprocessing pipeline that includes missing value imputation, normalization, balanced sampling, near decision boundary sample removal, and feature selection to significantly boost prediction accuracy. Second, we integrate contrastive learning with a GRU regression model to develop a novel classifier, termed ContReg, which employs dual learning methodologies, thereby further enhancing prediction performance. To validate the effectiveness of our preprocessing pipeline, we compare and demonstrate the performance gain of each step, and to demonstrate the efficacy of the ContReg classifier, we compare its performance to that of sequence-based deep learning architectures, machine learning models, and findings from previous studies. Our results illustrate exceptional True Skill Statistic (TSS) scores, surpassing previous methods and highlighting the critical role of precise data preprocessing and classifier development in time series-based solar flare prediction.",
        "subjects": "Solar and Stellar Astrophysics, Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2024-09-21 05:00:34 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.819413"
    },
    {
        "index": "#206",
        "title": "Mitigating Exposure Bias in Score-Based Generation of Molecular Conformations",
        "link": "/arxiv/2409.14014",
        "arxiv_id": "2409.14014",
        "authors": "Sijia Wang, Chen Wang, Zhenhao Zhao, Jiqiang Zhang, Weiran Cai",
        "summary": "Molecular conformation generation poses a significant challenge in the field of computational chemistry. Recently, Diffusion Probabilistic Models (DPMs) and Score-Based Generative Models (SGMs) are effectively used due to their capacity for generating accurate conformations far beyond conventional physics-based approaches. However, the discrepancy between training and inference rises a critical problem known as the exposure bias. While this issue has been extensively investigated in DPMs, the existence of exposure bias in SGMs and its effective measurement remain unsolved, which hinders the use of compensation methods for SGMs, including ConfGF and Torsional Diffusion as the representatives. In this work, we first propose a method for measuring exposure bias in SGMs used for molecular conformation generation, which confirms the significant existence of exposure bias in these models and measures its value. We design a new compensation algorithm Input Perturbation (IP), which is adapted from a method originally designed for DPMs only. Experimental results show that by introducing IP, SGM-based molecular conformation models can significantly improve both the accuracy and diversity of the generated conformations. Especially by using the IP-enhanced Torsional Diffusion model, we achieve new state-of-the-art performance on the GEOM-Drugs dataset and are on par on GEOM-QM9. We provide the code publicly at https://github.com/jia-975/torsionalDiff-ip.",
        "subjects": "Machine Learning, Artificial Intelligence, Biomolecules",
        "date": "2024-09-21 04:54:37 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.819615"
    },
    {
        "index": "#207",
        "title": "ChronoGAN: Supervised and Embedded Generative Adversarial Networks for Time Series Generation",
        "link": "/arxiv/2409.14013",
        "arxiv_id": "2409.14013",
        "authors": "MohammadReza EskandariNasab, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi",
        "summary": "Generating time series data using Generative Adversarial Networks (GANs) presents several prevalent challenges, such as slow convergence, information loss in embedding spaces, instability, and performance variability depending on the series length. To tackle these obstacles, we introduce a robust framework aimed at addressing and mitigating these issues effectively. This advanced framework integrates the benefits of an Autoencoder-generated embedding space with the adversarial training dynamics of GANs. This framework benefits from a time series-based loss function and oversight from a supervisory network, both of which capture the stepwise conditional distributions of the data effectively. The generator functions within the latent space, while the discriminator offers essential feedback based on the feature space. Moreover, we introduce an early generation algorithm and an improved neural network architecture to enhance stability and ensure effective generalization across both short and long time series. Through joint training, our framework consistently outperforms existing benchmarks, generating high-quality time series data across a range of real and synthetic datasets with diverse characteristics.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-21 04:51:35 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.819802"
    },
    {
        "index": "#208",
        "title": "Test Time Learning for Time Series Forecasting",
        "link": "/arxiv/2409.14012",
        "arxiv_id": "2409.14012",
        "authors": "Panayiotis Christou, Shichu Chen, Xupeng Chen, Parijat Dube",
        "summary": "Time-series forecasting has seen significant advancements with the introduction of token prediction mechanisms such as multi-head attention. However, these methods often struggle to achieve the same performance as in language modeling, primarily due to the quadratic computational cost and the complexity of capturing long-range dependencies in time-series data. State-space models (SSMs), such as Mamba, have shown promise in addressing these challenges by offering efficient solutions with linear RNNs capable of modeling long sequences with larger context windows. However, there remains room for improvement in accuracy and scalability. We propose the use of Test-Time Training (TTT) modules in a parallel architecture to enhance performance in long-term time series forecasting. Through extensive experiments on standard benchmark datasets, we demonstrate that TTT modules consistently outperform state-of-the-art models, including the Mamba-based TimeMachine, particularly in scenarios involving extended sequence and prediction lengths. Our results show significant improvements in Mean Squared Error (MSE) and Mean Absolute Error (MAE), especially on larger datasets such as Electricity, Traffic, and Weather, underscoring the effectiveness of TTT in capturing long-range dependencies. Additionally, we explore various convolutional architectures within the TTT framework, showing that even simple configurations like 1D convolution with small filters can achieve competitive results. This work sets a new benchmark for time-series forecasting and lays the groundwork for future research in scalable, high-performance forecasting models.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-21 04:40:08 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.819993"
    },
    {
        "index": "#209",
        "title": "Boolean Product Graph Neural Networks",
        "link": "/arxiv/2409.14001",
        "arxiv_id": "2409.14001",
        "authors": "Ziyan Wang, Bin Liu, Ling Xiang",
        "summary": "Graph Neural Networks (GNNs) have recently achieved significant success, with a key operation involving the aggregation of information from neighboring nodes. Substantial researchers have focused on defining neighbors for aggregation, predominantly based on observed adjacency matrices. However, in many scenarios, the explicitly given graphs contain noise, which can be amplified during the messages-passing process. Therefore, many researchers have turned their attention to latent graph inference, specifically learning a parametric graph. To mitigate fluctuations in latent graph structure learning, this paper proposes a novel Boolean product-based graph residual connection in GNNs to link the latent graph and the original graph. It computes the Boolean product between the latent graph and the original graph at each layer to correct the learning process. The Boolean product between two adjacency matrices is equivalent to triangle detection. Accordingly, the proposed Boolean product graph neural networks can be interpreted as discovering triangular cliques from the original and the latent graph. We validate the proposed method in benchmark datasets and demonstrate its ability to enhance the performance and robustness of GNNs.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-21 03:31:33 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.820250"
    },
    {
        "index": "#211",
        "title": "Relevance-driven Decision Making for Safer and More Efficient Human Robot Collaboration",
        "link": "/arxiv/2409.13998",
        "arxiv_id": "2409.13998",
        "authors": "Xiaotong Zhang, Dingcheng Huang, Kamal Youcef-Toumi",
        "summary": "Human intelligence possesses the ability to effectively focus on important environmental components, which enhances perception, learning, reasoning, and decision-making. Inspired by this cognitive mechanism, we introduced a novel concept termed relevance for Human-Robot Collaboration (HRC). Relevance is defined as the importance of the objects based on the applicability and pertinence of the objects for the human objective or other factors. In this paper, we further developed a novel two-loop framework integrating real-time and asynchronous processing to quantify relevance and apply relevance for safer and more efficient HRC. The asynchronous loop leverages the world knowledge from an LLM and quantifies relevance, and the real-time loop executes scene understanding, human intent prediction, and decision-making based on relevance. In decision making, we proposed and developed a human robot task allocation method based on relevance and a novel motion generation and collision avoidance methodology considering the prediction of human trajectory. Simulations and experiments show that our methodology for relevance quantification can accurately and robustly predict the human objective and relevance, with an average accuracy of up to 0.90 for objective prediction and up to 0.96 for relevance prediction. Moreover, our motion generation methodology reduces collision cases by 63.76% and collision frames by 44.74% when compared with a state-of-the-art (SOTA) collision avoidance method. Our framework and methodologies, with relevance, guide the robot on how to best assist humans and generate safer and more efficient actions for HRC.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2024-09-21 03:20:53 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.820658"
    },
    {
        "index": "#216",
        "title": "ProTEA: Programmable Transformer Encoder Acceleration on FPGA",
        "link": "/arxiv/2409.13975",
        "arxiv_id": "2409.13975",
        "authors": "Ehsan Kabir, Jason D. Bakos, David Andrews, Miaoqing Huang",
        "summary": "Transformer neural networks (TNN) have been widely utilized on a diverse range of applications, including natural language processing (NLP), machine translation, and computer vision (CV). Their widespread adoption has been primarily driven by the exceptional performance of their multi-head self-attention block used to extract key features from sequential data. The multi-head self-attention block is followed by feedforward neural networks, which play a crucial role in introducing non-linearity to assist the model in learning complex patterns. Despite the popularity of TNNs, there has been limited numbers of hardware accelerators targeting these two critical blocks. Most prior works have concentrated on sparse architectures that are not flexible for popular TNN variants. This paper introduces \\textit{ProTEA}, a runtime programmable accelerator tailored for the dense computations of most of state-of-the-art transformer encoders. \\textit{ProTEA} is designed to reduce latency by maximizing parallelism. We introduce an efficient tiling of large matrices that can distribute memory and computing resources across different hardware components within the FPGA. We provide run time evaluations of \\textit{ProTEA} on a Xilinx Alveo U55C high-performance data center accelerator card. Experimental results demonstrate that \\textit{ProTEA} can host a wide range of popular transformer networks and achieve near optimal performance with a tile size of 64 in the multi-head self-attention block and 6 in the feedforward networks block when configured with 8 parallel attention heads, 12 layers, and an embedding dimension of 768 on the U55C. Comparative results are provided showing \\textit{ProTEA} is 2.5$\\times$ faster than an NVIDIA Titan XP GPU. Results also show that it achieves 1.3 -- 2.8$\\times$ speed up compared with current state-of-the-art custom designed FPGA accelerators.",
        "subjects": "Hardware Architecture, Artificial Intelligence, Machine Learning, Systems and Control",
        "date": "2024-09-21 01:44:13 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.821765"
    },
    {
        "index": "#217",
        "title": "One Model, Any Conjunctive Query: Graph Neural Networks for Answering Complex Queries over Knowledge Graphs",
        "link": "/arxiv/2409.13959",
        "arxiv_id": "2409.13959",
        "authors": "Krzysztof Olejniczak, Xingyue Huang, İsmail İlkan Ceylan, Mikhail Galkin",
        "summary": "Traditional query answering over knowledge graphs -- or broadly over relational data -- is one of the most fundamental problems in data management. Motivated by the incompleteness of modern knowledge graphs, a new setup for query answering has emerged, where the goal is to predict answers that do not necessarily appear in the knowledge graph, but are present in its completion. In this work, we propose AnyCQ, a graph neural network model that can classify answers to any conjunctive query on any knowledge graph, following training. At the core of our framework lies a graph neural network model trained using a reinforcement learning objective to answer Boolean queries. Our approach and problem setup differ from existing query answering studies in multiple dimensions. First, we focus on the problem of query answer classification: given a query and a set of possible answers, classify these proposals as true or false relative to the complete knowledge graph. Second, we study the problem of query answer retrieval: given a query, retrieve an answer to the query relative to the complete knowledge graph or decide that no correct solutions exist. Trained on simple, small instances, AnyCQ can generalize to large queries of arbitrary structure, reliably classifying and retrieving answers to samples where existing approaches fail, which is empirically validated on new and challenging benchmarks. Furthermore, we demonstrate that our AnyCQ models effectively transfer to out-of-distribution knowledge graphs, when equipped with a relevant link predictor, highlighting their potential to serve as a general engine for query answering.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-21 00:30:44 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.821953"
    },
    {
        "index": "#219",
        "title": "Learning Recourse Costs from Pairwise Feature Comparisons",
        "link": "/arxiv/2409.13940",
        "arxiv_id": "2409.13940",
        "authors": "Kaivalya Rawal, Himabindu Lakkaraju",
        "summary": "This paper presents a novel technique for incorporating user input when learning and inferring user preferences. When trying to provide users of black-box machine learning models with actionable recourse, we often wish to incorporate their personal preferences about the ease of modifying each individual feature. These recourse finding algorithms usually require an exhaustive set of tuples associating each feature to its cost of modification. Since it is hard to obtain such costs by directly surveying humans, in this paper, we propose the use of the Bradley-Terry model to automatically infer feature-wise costs using non-exhaustive human comparison surveys. We propose that users only provide inputs comparing entire recourses, with all candidate feature modifications, determining which recourses are easier to implement relative to others, without explicit quantification of their costs. We demonstrate the efficient learning of individual feature costs using MAP estimates, and show that these non-exhaustive human surveys, which do not necessarily contain data for each feature pair comparison, are sufficient to learn an exhaustive set of feature costs, where each feature is associated with a modification cost.",
        "subjects": "Machine Learning, Artificial Intelligence, Computers and Society, Machine Learning",
        "date": "2024-09-20 23:04:08 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.822414"
    },
    {
        "index": "#226",
        "title": "Tabular Data Generation using Binary Diffusion",
        "link": "/arxiv/2409.13882",
        "arxiv_id": "2409.13882",
        "authors": "Vitaliy Kinakh, Slava Voloshynovskiy",
        "summary": "Generating synthetic tabular data is critical in machine learning, especially when real data is limited or sensitive. Traditional generative models often face challenges due to the unique characteristics of tabular data, such as mixed data types and varied distributions, and require complex preprocessing or large pretrained models. In this paper, we introduce a novel, lossless binary transformation method that converts any tabular data into fixed-size binary representations, and a corresponding new generative model called Binary Diffusion, specifically designed for binary data. Binary Diffusion leverages the simplicity of XOR operations for noise addition and removal and employs binary cross-entropy loss for training. Our approach eliminates the need for extensive preprocessing, complex noise parameter tuning, and pretraining on large datasets. We evaluate our model on several popular tabular benchmark datasets, demonstrating that Binary Diffusion outperforms existing state-of-the-art models on Travel, Adult Income, and Diabetes datasets while being significantly smaller in size.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-20 20:22:28 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.829800"
    },
    {
        "index": "#228",
        "title": "MAGICS: Adversarial RL with Minimax Actors Guided by Implicit Critic Stackelberg for Convergent Neural Synthesis of Robot Safety",
        "link": "/arxiv/2409.13867",
        "arxiv_id": "2409.13867",
        "authors": "Justin Wang, Haimin Hu, Duy Phuong Nguyen, Jaime Fernández Fisac",
        "summary": "While robust optimal control theory provides a rigorous framework to compute robot control policies that are provably safe, it struggles to scale to high-dimensional problems, leading to increased use of deep learning for tractable synthesis of robot safety. Unfortunately, existing neural safety synthesis methods often lack convergence guarantees and solution interpretability. In this paper, we present Minimax Actors Guided by Implicit Critic Stackelberg (MAGICS), a novel adversarial reinforcement learning (RL) algorithm that guarantees local convergence to a minimax equilibrium solution. We then build on this approach to provide local convergence guarantees for a general deep RL-based robot safety synthesis algorithm. Through both simulation studies on OpenAI Gym environments and hardware experiments with a 36-dimensional quadruped robot, we show that MAGICS can yield robust control policies outperforming the state-of-the-art neural safety synthesis methods.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning, Systems and Control",
        "date": "2024-09-20 19:45:48 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.830250"
    },
    {
        "index": "#229",
        "title": "Wormhole: Concept-Aware Deep Representation Learning for Co-Evolving Sequences",
        "link": "/arxiv/2409.13857",
        "arxiv_id": "2409.13857",
        "authors": "Kunpeng Xu, Lifei Chen, Shengrui Wang",
        "summary": "Identifying and understanding dynamic concepts in co-evolving sequences is crucial for analyzing complex systems such as IoT applications, financial markets, and online activity logs. These concepts provide valuable insights into the underlying structures and behaviors of sequential data, enabling better decision-making and forecasting. This paper introduces Wormhole, a novel deep representation learning framework that is concept-aware and designed for co-evolving time sequences. Our model presents a self-representation layer and a temporal smoothness constraint to ensure robust identification of dynamic concepts and their transitions. Additionally, concept transitions are detected by identifying abrupt changes in the latent space, signifying a shift to new behavior - akin to passing through a wormhole. This novel mechanism accurately discerns concepts within co-evolving sequences and pinpoints the exact locations of these wormholes, enhancing the interpretability of the learned representations. Experiments demonstrate that this method can effectively segment time series data into meaningful concepts, providing a valuable tool for analyzing complex temporal patterns and advancing the detection of concept drifts.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-20 19:11:39 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.830450"
    },
    {
        "index": "#230",
        "title": "More Consideration to the Perceptron",
        "link": "/arxiv/2409.13854",
        "arxiv_id": "2409.13854",
        "authors": "Slimane Larabi",
        "summary": "In this paper, we introduce the gated perceptron, an enhancement of the conventional perceptron, which incorporates an additional input computed as the product of the existing inputs. This allows the perceptron to capture non-linear interactions between features, significantly improving its ability to classify and regress on complex datasets. We explore its application in both linear and non-linear regression tasks using the Iris dataset, as well as binary and multi-class classification problems, including the PIMA Indian dataset and Breast Cancer Wisconsin dataset. Our results demonstrate that the gated perceptron can generate more distinct decision regions compared to traditional perceptrons, enhancing its classification capabilities, particularly in handling non-linear data. Performance comparisons show that the gated perceptron competes with state-of-the-art classifiers while maintaining a simple architecture.",
        "subjects": "Machine Learning, Artificial Intelligence, Neural and Evolutionary Computing",
        "date": "2024-09-20 19:01:29 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.830704"
    },
    {
        "index": "#235",
        "title": "On the Feasibility of Fully AI-automated Vishing Attacks",
        "link": "/arxiv/2409.13793",
        "arxiv_id": "2409.13793",
        "authors": "João Figueiredo, Afonso Carvalho, Daniel Castro, Daniel Gonçalves, Nuno Santos",
        "summary": "A vishing attack is a form of social engineering where attackers use phone calls to deceive individuals into disclosing sensitive information, such as personal data, financial information, or security credentials. Attackers exploit the perceived urgency and authenticity of voice communication to manipulate victims, often posing as legitimate entities like banks or tech support. Vishing is a particularly serious threat as it bypasses security controls designed to protect information. In this work, we study the potential for vishing attacks to escalate with the advent of AI. In theory, AI-powered software bots may have the ability to automate these attacks by initiating conversations with potential victims via phone calls and deceiving them into disclosing sensitive information. To validate this thesis, we introduce ViKing, an AI-powered vishing system developed using publicly available AI technology. It relies on a Large Language Model (LLM) as its core cognitive processor to steer conversations with victims, complemented by a pipeline of speech-to-text and text-to-speech modules that facilitate audio-text conversion in phone calls. Through a controlled social experiment involving 240 participants, we discovered that ViKing has successfully persuaded many participants to reveal sensitive information, even those who had been explicitly warned about the risk of vishing campaigns. Interactions with ViKing's bots were generally considered realistic. From these findings, we conclude that tools like ViKing may already be accessible to potential malicious actors, while also serving as an invaluable resource for cyber awareness programs.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Audio and Speech Processing",
        "date": "2024-09-20 10:47:09 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.831943"
    },
    {
        "index": "#236",
        "title": "Continual Learning for Multimodal Data Fusion of a Soft Gripper",
        "link": "/arxiv/2409.13792",
        "arxiv_id": "2409.13792",
        "authors": "Nilay Kushawaha, Egidio Falotico",
        "summary": "Continual learning (CL) refers to the ability of an algorithm to continuously and incrementally acquire new knowledge from its environment while retaining previously learned information. A model trained on one data modality often fails when tested with a different modality. A straightforward approach might be to fuse the two modalities by concatenating their features and training the model on the fused data. However, this requires retraining the model from scratch each time it encounters a new domain. In this paper, we introduce a continual learning algorithm capable of incrementally learning different data modalities by leveraging both class-incremental and domain-incremental learning scenarios in an artificial environment where labeled data is scarce, yet non-iid (independent and identical distribution) unlabeled data from the environment is plentiful. The proposed algorithm is efficient and only requires storing prototypes for each class. We evaluate the algorithm's effectiveness on a challenging custom multimodal dataset comprising of tactile data from a soft pneumatic gripper, and visual data from non-stationary images of objects extracted from video sequences. Additionally, we conduct an ablation study on the custom dataset and the Core50 dataset to highlight the contributions of different components of the algorithm. To further demonstrate the robustness of the algorithm, we perform a real-time experiment for object classification using the soft gripper and an external independent camera setup, all synchronized with the Robot Operating System (ROS) framework.",
        "subjects": "Machine Learning, Artificial Intelligence, Robotics",
        "date": "2024-09-20 09:53:27 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.832148"
    },
    {
        "index": "#237",
        "title": "Multi-omics data integration for early diagnosis of hepatocellular carcinoma (HCC) using machine learning",
        "link": "/arxiv/2409.13791",
        "arxiv_id": "2409.13791",
        "authors": "Annette Spooner, Mohammad Karimi Moridani, Azadeh Safarchi, Salim Maher, Fatemeh Vafaee, Amany Zekry, Arcot Sowmya",
        "summary": "The complementary information found in different modalities of patient data can aid in more accurate modelling of a patient's disease state and a better understanding of the underlying biological processes of a disease. However, the analysis of multi-modal, multi-omics data presents many challenges, including high dimensionality and varying size, statistical distribution, scale and signal strength between modalities. In this work we compare the performance of a variety of ensemble machine learning algorithms that are capable of late integration of multi-class data from different modalities. The ensemble methods and their variations tested were i) a voting ensemble, with hard and soft vote, ii) a meta learner, iii) a multi-modal Adaboost model using a hard vote, a soft vote and a meta learner to integrate the modalities on each boosting round, the PB-MVBoost model and a novel application of a mixture of experts model. These were compared to simple concatenation as a baseline. We examine these methods using data from an in-house study on hepatocellular carcinoma (HCC), along with four validation datasets on studies from breast cancer and irritable bowel disease (IBD). Using the area under the receiver operating curve as a measure of performance we develop models that achieve a performance value of up to 0.85 and find that two boosted methods, PB-MVBoost and Adaboost with a soft vote were the overall best performing models. We also examine the stability of features selected, and the size of the clinical signature determined. Finally, we provide recommendations for the integration of multi-modal multi-class data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-20 09:38:02 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.832446"
    },
    {
        "index": "#238",
        "title": "Revisiting Synthetic Human Trajectories: Imitative Generation and Benchmarks Beyond Datasaurus",
        "link": "/arxiv/2409.13790",
        "arxiv_id": "2409.13790",
        "authors": "Bangchao Deng, Xin Jing, Tianyue Yang, Bingqing Qu, Philippe Cudre-Mauroux, Dingqi Yang",
        "summary": "Human trajectory data, which plays a crucial role in various applications such as crowd management and epidemic prevention, is challenging to obtain due to practical constraints and privacy concerns. In this context, synthetic human trajectory data is generated to simulate as close as possible to real-world human trajectories, often under summary statistics and distributional similarities. However, the complexity of human mobility patterns is oversimplified by these similarities (a.k.a. ``Datasaurus''), resulting in intrinsic biases in both generative model design and benchmarks of the generated trajectories. Against this background, we propose MIRAGE, a huMan-Imitative tRAjectory GenErative model designed as a neural Temporal Point Process integrating an Exploration and Preferential Return model. It imitates the human decision-making process in trajectory generation, rather than fitting any specific statistical distributions as traditional methods do, thus avoiding the Datasaurus issue. Moreover, we also propose a comprehensive task-based evaluation protocol beyond Datasaurus to systematically benchmark trajectory generative models on four typical downstream tasks, integrating multiple techniques and evaluation metrics for each task, to comprehensively assess the ultimate utility of the generated trajectories. We conduct a thorough evaluation of MIRAGE on three real-world user trajectory datasets against a sizeable collection of baselines. Results show that compared to the best baselines, MIRAGE-generated trajectory data not only achieves the best statistical and distributional similarities with 59.0-71.5% improvement, but also yields the best performance in the task-based evaluation with 10.9-33.4% improvement.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-20 09:07:27 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.832737"
    },
    {
        "index": "#239",
        "title": "Quantum evolutionary algorithm for TSP combinatorial optimisation problem",
        "link": "/arxiv/2409.13788",
        "arxiv_id": "2409.13788",
        "authors": "Yijiang Ma, Tan Chye Cheah",
        "summary": "This paper implements a new way of solving a problem called the traveling salesman problem (TSP) using quantum genetic algorithm (QGA). We compared how well this new approach works to the traditional method known as a classical genetic algorithm (CGA). The TSP is a well-established challenge in combinatorial optimization where the objective is to find the most efficient path to visit a series of cities, minimizing the total distance, and returning to the starting point. We chose the TSP to test the performance of both algorithms because of its computational complexity and importance in practical applications. We choose the dataset from the international standard library TSPLIB for our experiments. By designing and implementing both algorithms and conducting experiments on various sizes and types of TSP instances, we provide an in-depth analysis of the accuracy of the optimal solution, the number of iterations, the execution time, and the stability of the algorithms for both. The empirical findings indicate that the CGA outperforms the QGA in terms of finding superior solutions more quickly in most of the test instances, especially when the problem size is large. This suggests that although the principle of quantum computing provides a new way to solve complex combinatorial optimisation problems, the implementation of quantum phenomena and the setting of parameters such as the optimal angle for a quantum revolving gate is challenging and need further optimisation to achieve the desired results. Additionally, it is important to note that the QGA has not been tested on real quantum hardware, so its true performance remains unverified. These limitations provide rich opportunities for further research in the future.",
        "subjects": "Quantum Physics, Artificial Intelligence",
        "date": "2024-09-20 08:27:42 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.832958"
    },
    {
        "index": "#240",
        "title": "Learning to Generalize Unseen Domains via Multi-Source Meta Learning for Text Classification",
        "link": "/arxiv/2409.13787",
        "arxiv_id": "2409.13787",
        "authors": "Yuxuan Hu, Chenwei Zhang, Min Yang, Xiaodan Liang, Chengming Li, Xiping Hu",
        "summary": "With the rapid development of deep learning methods, there have been many breakthroughs in the field of text classification. Models developed for this task have been shown to achieve high accuracy. However, most of these models are trained using labeled data from seen domains. It is difficult for these models to maintain high accuracy in a new challenging unseen domain, which is directly related to the generalization of the model. In this paper, we study the multi-source Domain Generalization of text classification and propose a framework to use multiple seen domains to train a model that can achieve high accuracy in an unseen domain. Specifically, we propose a multi-source meta-learning Domain Generalization framework to simulate the process of model generalization to an unseen domain, so as to extract sufficient domain-related features. We introduced a memory mechanism to store domain-specific features, which coordinate with the meta-learning framework. Besides, we adopt the novel \"jury\" mechanism that enables the model to learn sufficient domain-invariant features. Experiments demonstrate that our meta-learning framework can effectively enhance the ability of the model to generalize to an unseen domain and can outperform the state-of-the-art methods on multi-source text classification datasets.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2024-09-20 07:46:21 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.833180"
    },
    {
        "index": "#241",
        "title": "A Value Based Parallel Update MCTS Method for Multi-Agent Cooperative Decision Making of Connected and Automated Vehicles",
        "link": "/arxiv/2409.13783",
        "arxiv_id": "2409.13783",
        "authors": "Ye Han, Lijun Zhang, Dejian Meng, Xingyu Hu, Songyu Weng",
        "summary": "To solve the problem of lateral and logitudinal joint decision-making of multi-vehicle cooperative driving for connected and automated vehicles (CAVs), this paper proposes a Monte Carlo tree search (MCTS) method with parallel update for multi-agent Markov game with limited horizon and time discounted setting. By analyzing the parallel actions in the multi-vehicle joint action space in the partial-steady-state traffic flow, the parallel update method can quickly exclude potential dangerous actions, thereby increasing the search depth without sacrificing the search breadth. The proposed method is tested in a large number of randomly generated traffic flow. The experiment results show that the algorithm has good robustness and better performance than the SOTA reinforcement learning algorithms and heuristic methods. The vehicle driving strategy using the proposed algorithm shows rationality beyond human drivers, and has advantages in traffic efficiency and safety in the coordinating zone.",
        "subjects": "Multiagent Systems, Artificial Intelligence, Computer Science and Game Theory, Systems and Control",
        "date": "2024-09-20 03:13:01 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.833484"
    },
    {
        "index": "#243",
        "title": "Trustworthy Intrusion Detection: Confidence Estimation Using Latent Space",
        "link": "/arxiv/2409.13774",
        "arxiv_id": "2409.13774",
        "authors": "Ioannis Pitsiorlas, George Arvanitakis, Marios Kountouris",
        "summary": "This work introduces a novel method for enhancing confidence in anomaly detection in Intrusion Detection Systems (IDS) through the use of a Variational Autoencoder (VAE) architecture. By developing a confidence metric derived from latent space representations, we aim to improve the reliability of IDS predictions against cyberattacks. Applied to the NSL-KDD dataset, our approach focuses on binary classification tasks to effectively distinguish between normal and malicious network activities. The methodology demonstrates a significant enhancement in anomaly detection, evidenced by a notable correlation of 0.45 between the reconstruction error and the proposed metric. Our findings highlight the potential of employing VAEs for more accurate and trustworthy anomaly detection in network security.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Machine Learning",
        "date": "2024-09-19 08:09:44 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.839172"
    },
    {
        "index": "#244",
        "title": "A Case Study of Web App Coding with OpenAI Reasoning Models",
        "link": "/arxiv/2409.13773",
        "arxiv_id": "2409.13773",
        "authors": "Yi Cui",
        "summary": "This paper presents a case study of coding tasks by the latest reasoning models of OpenAI, i.e. o1-preview and o1-mini, in comparison with other frontier models. The o1 models deliver SOTA results for WebApp1K, a single-task benchmark. To this end, we introduce WebApp1K-Duo, a harder benchmark doubling number of tasks and test cases. The new benchmark causes the o1 model performances to decline significantly, falling behind Claude 3.5. Moreover, they consistently fail when confronted with atypical yet correct test cases, a trap non-reasoning models occasionally avoid. We hypothesize that the performance variability is due to instruction comprehension. Specifically, the reasoning mechanism boosts performance when all expectations are captured, meanwhile exacerbates errors when key expectations are missed, potentially impacted by input lengths. As such, we argue that the coding success of reasoning models hinges on the top-notch base model and SFT to ensure meticulous adherence to instructions.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2024-09-19 06:58:02 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.839420"
    },
    {
        "index": "#245",
        "title": "Magika: AI-Powered Content-Type Detection",
        "link": "/arxiv/2409.13768",
        "arxiv_id": "2409.13768",
        "authors": "Yanick Fratantonio, Luca Invernizzi, Loua Farah, Kurt Thomas, Marina Zhang, Ange Albertini, Francois Galilee, Giancarlo Metitieri, Julien Cretin, Alex Petit-Bianco, David Tao, Elie Bursztein",
        "summary": "The task of content-type detection -- which entails identifying the data encoded in an arbitrary byte sequence -- is critical for operating systems, development, reverse engineering environments, and a variety of security applications. In this paper, we introduce Magika, a novel AI-powered content-type detection tool. Under the hood, Magika employs a deep learning model that can execute on a single CPU with just 1MB of memory to store the model's weights. We show that Magika achieves an average F1 score of 99% across over a hundred content types and a test set of more than 1M files, outperforming all existing content-type detection tools today. In order to foster adoption and improvements, we open source Magika under an Apache 2 license on GitHub and make our model and training pipeline publicly available. Our tool has already seen adoption by the Gmail email provider for attachment scanning, and it has been integrated with VirusTotal to aid with malware analysis. We note that this paper discusses the first iteration of Magika, and a more recent version already supports more than 200 content types. The interested reader can see the latest development on the Magika GitHub repository, available at https://github.com/google/magika.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2024-09-18 17:24:39 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.839747"
    },
    {
        "index": "#263",
        "title": "Explainable Malware Analysis: Concepts, Approaches and Challenges",
        "link": "/arxiv/2409.13723",
        "arxiv_id": "2409.13723",
        "authors": "Harikha Manthena, Shaghayegh Shajarian, Jeffrey Kimmell, Mahmoud Abdelsalam, Sajad Khorsandroo, Maanak Gupta",
        "summary": "Machine learning (ML) has seen exponential growth in recent years, finding applications in various domains such as finance, medicine, and cybersecurity. Malware remains a significant threat to modern computing, frequently used by attackers to compromise systems. While numerous machine learning-based approaches for malware detection achieve high performance, they often lack transparency and fail to explain their predictions. This is a critical drawback in malware analysis, where understanding the rationale behind detections is essential for security analysts to verify and disseminate information. Explainable AI (XAI) addresses this issue by maintaining high accuracy while producing models that provide clear, understandable explanations for their decisions. In this survey, we comprehensively review the current state-of-the-art ML-based malware detection techniques and popular XAI approaches. Additionally, we discuss research implementations and the challenges of explainable malware analysis. This theoretical survey serves as an entry point for researchers interested in XAI applications in malware detection. By analyzing recent advancements in explainable malware analysis, we offer a broad overview of the progress in this field, positioning our work as the first to extensively cover XAI methods for malware classification and detection.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2024-09-09 08:19:33 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.843755"
    },
    {
        "index": "#268",
        "title": "WebQuest: A Benchmark for Multimodal QA on Web Page Sequences",
        "link": "/arxiv/2409.13711",
        "arxiv_id": "2409.13711",
        "authors": "Maria Wang, Srinivas Sunkara, Gilles Baechler, Jason Lin, Yun Zhu, Fedir Zubach, Lei Shu, Jindong Chen",
        "summary": "The rise of multimodal LLMs and web agents calls for the creation of challenging benchmarks to evaluate neural architectures. Unlike existing benchmarks that focus on multi-step web navigation, we present WebQuest, a multi-page question-answering dataset that requires simultaneous retrieval and reasoning across web interaction sequences grounded in real-world usage. WebQuest includes three question categories: single-screen reasoning, multi-screen reasoning, and questions based on navigation traces. We evaluate some of the leading multimodal models like GPT-4V, Gemini Flash, and Claude 3 on our dataset, revealing a significant gap between single-screen and multi-screen reasoning. Finally, we investigate inference time techniques like Chain-of-Thought prompting to improve model capabilities on multi-screen reasoning.",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2024-09-06 18:44:25 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.866305"
    },
    {
        "index": "#274",
        "title": "MAS4POI: a Multi-Agents Collaboration System for Next POI Recommendation",
        "link": "/arxiv/2409.13700",
        "arxiv_id": "2409.13700",
        "authors": "Yuqian Wu, Yuhong Peng, Jiapeng Yu, Raymond S. T. Lee",
        "summary": "LLM-based Multi-Agent Systems have potential benefits of complex decision-making tasks management across various domains but their applications in the next Point-of-Interest (POI) recommendation remain underexplored. This paper proposes a novel MAS4POI system designed to enhance next POI recommendations through multi-agent interactions. MAS4POI supports Large Language Models (LLMs) specializing in distinct agents such as DataAgent, Manager, Analyst, and Navigator with each contributes to a collaborative process of generating the next POI recommendations.The system is examined by integrating six distinct LLMs and evaluated by two real-world datasets for recommendation accuracy improvement in real-world scenarios. Our code is available at https://github.com/yuqian2003/MAS4POI.",
        "subjects": "Information Retrieval, Artificial Intelligence, Social and Information Networks",
        "date": "2024-09-05 02:47:49 UTC",
        "category": "cs.AI",
        "crawl_time": "2025-09-24T16:31:00.867658"
    }
]