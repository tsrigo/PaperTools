[
    {
        "index": "#1",
        "title": "SAGE: A Realistic Benchmark for Semantic Understanding",
        "link": "/arxiv/2509.21310",
        "arxiv_id": "2509.21310",
        "authors": "Samarth Goel, Reagan J. Lee, Kannan Ramchandran",
        "summary": "As large language models (LLMs) achieve strong performance on traditional benchmarks, there is an urgent need for more challenging evaluation frameworks that probe deeper aspects of semantic understanding. We introduce SAGE (Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed to assess both embedding models and similarity metrics across five categories: Human Preference Alignment, Transformation Robustness, Information Sensitivity, Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks that focus on isolated capabilities, SAGE evaluates semantic understanding through adversarial conditions, noisy transformations, and nuanced human judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding models and classical metrics reveals significant performance gaps, with no single approach excelling across all dimensions. For instance, while state-of-the-art embedding models like OpenAI's text-embedding-3-large dominate in aligning with human preferences (0.682 vs. 0.591 for the best classical metric), they are significantly outperformed by classical metrics on information sensitivity tasks, where Jaccard Similarity achieves a score of 0.905 compared to the top embedding score of 0.794. SAGE further uncovers critical trade-offs: OpenAI's text-embedding-3-small achieves the highest clustering performance (0.483) but demonstrates extreme brittleness with the lowest robustness score (0.011). SAGE exposes critical limitations in current semantic understanding capabilities and provides a more realistic assessment of model robustness for real-world deployment.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.256037"
    },
    {
        "index": "#3",
        "title": "Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support",
        "link": "/arxiv/2509.21266",
        "arxiv_id": "2509.21266",
        "authors": "Zijian Shao, Haiyang Shen, Mugeng Liu, Gecheng Fu, Yaoqi Guo, Yanfeng Wang, Yun Ma",
        "summary": "Effective disease prediction in modern healthcare demands the twin goals of high accuracy and transparent, clinically meaningful explanations. Existing machine learning and large language model (LLM) based approaches often struggle to balance these goals. Many models yield accurate but unclear statistical outputs, while others generate fluent but statistically unsupported narratives, often undermining both the validity of the explanation and the predictive accuracy itself. This shortcoming comes from a shallow interaction with the data, preventing the development of a deep, detailed understanding similar to a human expert's. We argue that high accuracy and high-quality explanations are not separate objectives but are mutually reinforcing outcomes of a model that develops a deep, direct understanding of the data. To achieve this, we propose the Reflective Cognitive Architecture (RCA), a novel framework that coordinates multiple LLMs to learn from direct experience. RCA features an iterative rule refinement mechanism that improves its logic from prediction errors and a distribution-aware rules check mechanism that bases its reasoning in the dataset's global statistics. By using predictive accuracy as a signal to drive deeper comprehension, RCA builds a strong internal model of the data. We evaluated RCA on one private and two public datasets against 22 baselines. The results demonstrate that RCA not only achieves state-of-the-art accuracy and robustness with a relative improvement of up to 40\\% over the baseline but, more importantly, leverages this deep understanding to excel in generating explanations that are clear, logical, evidence-based, and balanced, highlighting its potential for creating genuinely trustworthy clinical decision support systems. The code is available at \\https://github.com/ssssszj/RCA.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.256516"
    },
    {
        "index": "#4",
        "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns",
        "link": "/arxiv/2509.21224",
        "arxiv_id": "2509.21224",
        "authors": "Stefan Szeider",
        "summary": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.256679"
    },
    {
        "index": "#5",
        "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA",
        "link": "/arxiv/2509.21199",
        "arxiv_id": "2509.21199",
        "authors": "Kaiyang Wan, Lang Gao, Honglin Mu, Preslav Nakov, Yuxia Wang, Xiuying Chen",
        "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.256868"
    },
    {
        "index": "#6",
        "title": "Distributed Specialization: Rare-Token Neurons in Large Language Models",
        "link": "/arxiv/2509.21163",
        "arxiv_id": "2509.21163",
        "authors": "Jing Liu, Haozheng Wang, Yueheng Li",
        "summary": "Large language models (LLMs) struggle with representing and generating rare tokens despite their importance in specialized domains. We investigate whether LLMs develop internal specialization mechanisms through discrete modular architectures or distributed parameter-level differentiation. Through systematic analysis of final-layer MLP neurons across multiple model families, we discover that rare-token processing emerges via \\textit{distributed specialization}: functionally coordinated but spatially distributed subnetworks that exhibit three distinct organizational principles. First, we identify a reproducible three-regime influence hierarchy comprising highly influential plateau neurons(also termed as rare-token neurons), power-law decay neurons, and minimally contributing neurons, which is absent in common-token processing. Second, plateau neurons demonstrate coordinated activation patterns (reduced effective dimensionality) while remaining spatially distributed rather than forming discrete clusters. Third, these specialized mechanisms are universally accessible through standard attention pathways without requiring dedicated routing circuits. Training dynamics reveal that functional specialization emerges gradually through parameter differentiation, with specialized neurons developing increasingly heavy-tailed weight correlation spectra consistent with Heavy-Tailed Self-Regularization signatures. Our findings establish that LLMs process rare-tokens through distributed coordination within shared architectures rather than mixture-of-experts-style modularity. These results provide insights for interpretable model editing, computational efficiency optimization, and understanding emergent functional organization in transformer networks.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.257045"
    },
    {
        "index": "#7",
        "title": "Embodied Representation Alignment with Mirror Neurons",
        "link": "/arxiv/2509.21136",
        "arxiv_id": "2509.21136",
        "authors": "Wentao Zhu, Zhining Zhang, Yuwei Ren, Yin Huang, Hao Xu, Yizhou Wang",
        "summary": "Mirror neurons are a class of neurons that activate both when an individual observes an action and when they perform the same action. This mechanism reveals a fundamental interplay between action understanding and embodied execution, suggesting that these two abilities are inherently connected. Nonetheless, existing machine learning methods largely overlook this interplay, treating these abilities as separate tasks. In this study, we provide a unified perspective in modeling them through the lens of representation learning. We first observe that their intermediate representations spontaneously align. Inspired by mirror neurons, we further introduce an approach that explicitly aligns the representations of observed and executed actions. Specifically, we employ two linear layers to map the representations to a shared latent space, where contrastive learning enforces the alignment of corresponding representations, effectively maximizing their mutual information. Experiments demonstrate that this simple approach fosters mutual synergy between the two tasks, effectively improving representation quality and generalization.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.257235"
    },
    {
        "index": "#9",
        "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
        "link": "/arxiv/2509.21128",
        "arxiv_id": "2509.21128",
        "authors": "Kohsei Matsutani, Shota Takashiro, Gouki Minegishi, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo",
        "summary": "Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.257610"
    },
    {
        "index": "#12",
        "title": "Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution",
        "link": "/arxiv/2509.21072",
        "arxiv_id": "2509.21072",
        "authors": "Kaiwen He, Zhiwei Wang, Chenyi Zhuang, Jinjie Gu",
        "summary": "Recent years, multimodal models have made remarkable strides and pave the way for intelligent browser use agents. However, when solving tasks on real world webpages in multi-turn, long-horizon trajectories, current agents still suffer from disordered action sequencing and excessive trial and error during execution. This paper introduces Recon-Act, a self-evolving multi-agent framework grounded in Reconnaissance-Action behavioral paradigm. The system comprises a Reconnaissance Team and an Action Team: the former conducts comparative analysis and tool generation, while the latter handles intent decomposition, tool orchestration, and execution. By contrasting the erroneous trajectories with successful ones, the Reconnaissance Team infers remedies, and abstracts them into a unified notion of generalized tools, either expressed as hints or as rule-based codes, and register to the tool archive in real time. The Action Team reinference the process empowered with these targeting tools, thus establishing a closed-loop training pipeline of data-tools-action-feedback. Following the 6 level implementation roadmap proposed in this work, we have currently reached Level 3 (with limited human-in-the-loop intervention). Leveraging generalized tools obtained through reconnaissance, Recon-Act substantially improves adaptability to unseen websites and solvability on long-horizon tasks, and achieves state-of-the-art performance on the challenging VisualWebArena dataset.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.258434"
    },
    {
        "index": "#14",
        "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
        "link": "/arxiv/2509.21043",
        "arxiv_id": "2509.21043",
        "authors": "Samuel Schapiro, Sumuk Shashidhar, Alexi Gladstone, Jonah Black, Royce Moon, Dilek Hakkani-Tur, Lav R. Varshney",
        "summary": "Artificial intelligence (AI) systems, and large language models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Though in many ways similar to forms of compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, marking a new frontier in generalization abilities.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.258861"
    },
    {
        "index": "#16",
        "title": "Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles",
        "link": "/arxiv/2509.21028",
        "arxiv_id": "2509.21028",
        "authors": "Miao Li, Alexander Gurung, Irina Saparina, Mirella Lapata",
        "summary": "This paper introduces SciTrek, a novel question-answering benchmark designed to evaluate the long-context reasoning capabilities of large language models (LLMs) using scientific articles. Current long-context benchmarks often rely on non-scientific texts, focus on simple information retrieval tasks, or employ artificial contexts. SciTrek addresses these limitations by proposing complex questions that require information aggregation and synthesis across multiple full-text scientific articles. Questions and their ground-truth answers are automatically generated by formulating them as SQL queries over a database constructed from article metadata (titles, authors, and references). The SQL operations provide explicit, verifiable reasoning steps for fine-grained error analysis, and the construction process scales to contexts up to 1M tokens with minimal supervision. Extensive experiments on a diverse set of open-weight and proprietary LLMs demonstrate that SciTrek poses a significant challenge as the context length increases, with supervised fine-tuning and reinforcement learning offering only limited gains. Our analysis reveals systematic shortcomings in models' abilities to perform basic numerical operations and accurately locate specific information in long contexts.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.259233"
    },
    {
        "index": "#17",
        "title": "CORE: Full-Path Evaluation of LLM Agents Beyond Final State",
        "link": "/arxiv/2509.20998",
        "arxiv_id": "2509.20998",
        "authors": "Panagiotis Michelakis, Yiannis Hadjiyiannis, Dimitrios Stamoulis",
        "summary": "Evaluating AI agents that solve real-world tasks through function-call sequences remains an open challenge. Existing agentic benchmarks often reduce evaluation to a binary judgment of the final state, overlooking critical aspects such as safety, efficiency, and intermediate correctness. We propose a framework based on deterministic finite automata (DFAs) that encodes tasks as sets of valid tool-use paths, enabling principled assessment of agent behavior in diverse world models. Building on this foundation, we introduce CORE, a suite of five metrics, namely Path Correctness, Path Correctness - Kendall's tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that quantify alignment with expected execution patterns. Across diverse worlds, our method reveals important performance differences between agents that would otherwise appear equivalent under traditional final-state evaluation schemes.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.259399"
    },
    {
        "index": "#18",
        "title": "AOT*: Efficient Synthesis Planning via LLM-Empowered AND-OR Tree Search",
        "link": "/arxiv/2509.20988",
        "arxiv_id": "2509.20988",
        "authors": "Xiaozhuang Song, Xuanhao Pan, Xinjian Zhao, Hangting Ye, Shufei Zhang, Jian Tang, Tianshu Yu",
        "summary": "Retrosynthesis planning enables the discovery of viable synthetic routes for target molecules, playing a crucial role in domains like drug discovery and materials design. Multi-step retrosynthetic planning remains computationally challenging due to exponential search spaces and inference costs. While Large Language Models (LLMs) demonstrate chemical reasoning capabilities, their application to synthesis planning faces constraints on efficiency and cost. To address these challenges, we introduce AOT*, a framework that transforms retrosynthetic planning by integrating LLM-generated chemical synthesis pathways with systematic AND-OR tree search. To this end, AOT* atomically maps the generated complete synthesis routes onto AND-OR tree components, with a mathematically sound design of reward assignment strategy and retrieval-based context engineering, thus enabling LLMs to efficiently navigate in the chemical space. Experimental evaluation on multiple synthesis benchmarks demonstrates that AOT* achieves SOTA performance with significantly improved search efficiency. AOT* exhibits competitive solve rates using 3-5$\\times$ fewer iterations than existing LLM-based approaches, with the efficiency advantage becoming more pronounced on complex molecular targets.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.259595"
    },
    {
        "index": "#19",
        "title": "Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM",
        "link": "/arxiv/2509.20953",
        "arxiv_id": "2509.20953",
        "authors": "Najla Zuhir, Amna Mohammad Salim, Parvathy Premkumar, Moshiur Farazi",
        "summary": "We present an advanced approach to mobile app review analysis aimed at addressing limitations inherent in traditional star-rating systems. Star ratings, although intuitive and popular among users, often fail to capture the nuanced feedback present in detailed review texts. Traditional NLP techniques -- such as lexicon-based methods and classical machine learning classifiers -- struggle to interpret contextual nuances, domain-specific terminology, and subtle linguistic features like sarcasm. To overcome these limitations, we propose a modular framework leveraging large language models (LLMs) enhanced by structured prompting techniques. Our method quantifies discrepancies between numerical ratings and textual sentiment, extracts detailed, feature-level insights, and supports interactive exploration of reviews through retrieval-augmented conversational question answering (RAG-QA). Comprehensive experiments conducted on three diverse datasets (AWARE, Google Play, and Spotify) demonstrate that our LLM-driven approach significantly surpasses baseline methods, yielding improved accuracy, robustness, and actionable insights in challenging and context-rich review scenarios.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.259767"
    },
    {
        "index": "#20",
        "title": "GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine",
        "link": "/arxiv/2509.20935",
        "arxiv_id": "2509.20935",
        "authors": "Heming Zhang, Di Huang, Wenyu Li, Michael Province, Yixin Chen, Philip Payne, Fuhai Li",
        "summary": "In precision medicine, quantitative multi-omic features, topological context, and textual biological knowledge play vital roles in identifying disease-critical signaling pathways and targets. Existing pipelines capture only part of these-numerical omics ignore topological context, text-centric LLMs lack quantitative grounded reasoning, and graph-only models underuse node semantics and the generalization of LLMs-limiting mechanistic interpretability. Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they remain limited by unreliable intermediate evaluation, and vulnerability to reward hacking with computational cost. These gaps motivate integrating quantitative multi-omic signals, topological structure with node annotations, and literature-scale text via LLMs, using subgraph reasoning as the principle bridge linking numeric evidence, topological knowledge and language context. Therefore, we propose GALAX (Graph Augmented LAnguage model with eXplainability), an innovative framework that integrates pretrained Graph Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement guided by a Graph Process Reward Model (GPRM), which generates disease-relevant subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated by a pretrained GNN, enabling process-level supervision without explicit intermediate reasoning annotations. As an application, we also introduced Target-QA, a benchmark combining CRISPR-identified targets, multi-omic profiles, and biomedical graph knowledge across diverse cancer cell lines, which enables GNN pretraining for supervising step-wise graph construction and supports long-context reasoning over text-numeric graphs (TNGs), providing a scalable and biologically grounded framework for explainable, reinforcement-guided subgraph reasoning toward reliable and interpretable target and pathway discovery in precision medicine.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.259957"
    },
    {
        "index": "#21",
        "title": "DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning",
        "link": "/arxiv/2509.20912",
        "arxiv_id": "2509.20912",
        "authors": "Tianrun Xu, Haoda Jing, Ye Li, Yuquan Wei, Jun Feng, Guanyu Chen, Haichuan Gao, Tianren Zhang, Feng Chen",
        "summary": "Recent advances in multimodal language models (MLLMs) have achieved remarkable progress in vision-language reasoning, especially with the emergence of \"thinking with images,\" which integrates explicit visual steps into the reasoning process. While this paradigm strengthens image-based reasoning, a significant challenge remains: models may arrive at correct answers by relying on irrelevant or spurious regions, driven by prior knowledge or dataset biases. Even when the answer is correct, flawed reasoning indicates that the model has not truly understood the image, highlighting the critical importance of reasoning fidelity in multimodal tasks. To address this issue, we propose DeFacto, a counterfactual reasoning framework that jointly enforces accurate answering and faithful reasoning. A key component of our approach is the design of three complementary training paradigms: (i) positive, (ii) counterfactual, and (iii) random-masking. To enable these paradigms, we develop a pipeline that automatically localizes question-relevant evidence and constructs positive, counterfactual, and random variants, resulting in a dataset of about 100k images. Building on this framework, we train multimodal language models with GRPO-based reinforcement learning, where we design three complementary rewards to guide the model toward accurate answering and evidence-grounded reasoning. Experiments on diverse benchmarks demonstrate that DeFacto substantially improves both answer accuracy and reasoning faithfulness, establishing a stronger foundation for interpretable multimodal reasoning. The code is available on GitHub and the dataset is released on HuggingFace.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.260214"
    },
    {
        "index": "#22",
        "title": "LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks",
        "link": "/arxiv/2509.20798",
        "arxiv_id": "2509.20798",
        "authors": "Lipeng Ma, Yixuan Li, Weidong Yang, Mingjie Zhou, Xinyi Liu, Ben Fei, Shuhao Li, Xiaoyan Sun, Sihang Jiang, Yanghua Xiao",
        "summary": "Log analysis is crucial for monitoring system health and diagnosing failures in complex systems. Recent advances in large language models (LLMs) offer new opportunities for automated log analysis, leveraging their reasoning capabilities to perform tasks such as anomaly detection and failure prediction. However, general-purpose LLMs struggle to formulate structured reasoning workflows that align with expert cognition and deliver precise details of reasoning steps. To address these challenges, we propose LogReasoner, a coarse-to-fine reasoning enhancement framework designed to enable LLMs to reason log analysis tasks like experts. LogReasoner consists of two stages: (1) coarse-grained enhancement of expert thinking, where high-level expert thoughts are constructed from collected troubleshooting flowcharts and existing tasks to enable LLMs to formulate structured reasoning workflows and (2) fine-grained enhancement of specific steps, where we first fine-tune the LLM with task-specific stepwise solutions to enhance the LLM for instantiated reasoning, then employ the preference learning to calibrate the LLM's reasoning details from its mistakes, further strengthen the LLM's analytical granularity and correctness. We evaluate LogReasoner on four distinct log analysis tasks using open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art performance and demonstrating its effectiveness in enhancing the reasoning capabilities of LLMs for log analysis.",
        "subjects": "Artificial Intelligence, Software Engineering",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.260450"
    },
    {
        "index": "#23",
        "title": "Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning",
        "link": "/arxiv/2509.20754",
        "arxiv_id": "2509.20754",
        "authors": "Yufan Mao, Hanjing Ye, Wenlong Dong, Chengjie Zhang, Hong Zhang",
        "summary": "Navigating complex environments requires robots to effectively store observations as memories and leverage them to answer human queries about spatial locations, which is a critical yet underexplored research challenge. While prior work has made progress in constructing robotic memory, few have addressed the principled mechanisms needed for efficient memory retrieval and integration. To bridge this gap, we propose Meta-Memory, a large language model (LLM)-driven agent that constructs a high-density memory representation of the environment. The key innovation of Meta-Memory lies in its capacity to retrieve and integrate relevant memories through joint reasoning over semantic and spatial modalities in response to natural language location queries, thereby empowering robots with robust and accurate spatial reasoning capabilities. To evaluate its performance, we introduce SpaceLocQA, a large-scale dataset encompassing diverse real-world spatial question-answering scenarios. Experimental results show that Meta-Memory significantly outperforms state-of-the-art methods on both the SpaceLocQA and the public NaVQA benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world robotic platforms, demonstrating its practical utility in complex environments. Project page: https://itsbaymax.github.io/meta-memory.github.io/ .",
        "subjects": "Artificial Intelligence, Robotics",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.265926"
    },
    {
        "index": "#24",
        "title": "Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning",
        "link": "/arxiv/2509.20744",
        "arxiv_id": "2509.20744",
        "authors": "Qihang Ai, Haiyun Jiang",
        "summary": "We study reasoning tasks through a framework that integrates auto-regressive (AR) and non-autoregressive (NAR) language models. AR models, which generate text sequentially, excel at producing coherent outputs but often suffer from slow inference, particularly in reasoning-intensive domains such as mathematics and code, where lengthy chains of thought are required. In contrast, NAR models, such as discrete diffusion models, allow parallel generation and offer substantial speedups, though typically at the cost of reduced output quality. To address these limitations, we introduce a new paradigm in which an NAR model efficiently produces intermediate reasoning traces, which subsequently guide an AR model to deliver precise final answers. Experiments demonstrate that our approach yields significant 26% improvements over strong baselines while substantially reducing inference cost.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.266155"
    },
    {
        "index": "#26",
        "title": "An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans",
        "link": "/arxiv/2509.20707",
        "arxiv_id": "2509.20707",
        "authors": "Junjie Cui, Peilong Wang, Jason Holmes, Leshan Sun, Michael L. Hinni, Barbara A. Pockaj, Sujay A. Vora, Terence T. Sio, William W. Wong, Nathan Y. Yu, Steven E. Schild, Joshua R. Niska, Sameer R. Keole, Jean-Claude M. Rwigema, Samir H. Patel, Lisa A. McGee, Carlos A. Vargas, Wei Liu",
        "summary": "Purpose: To develop a retrieval-augmented generation (RAG) system powered by LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of radiotherapy treatment plans. Methods and Materials: We curated a multi-protocol dataset of 614 radiotherapy plans across four disease sites and constructed a knowledge base containing normalized dose metrics and protocol-defined constraints. The RAG system integrates three core modules: a retrieval engine optimized across five SentenceTransformer backbones, a percentile prediction component based on cohort similarity, and a clinical constraint checker. These tools are directed by a large language model (LLM) using a multi-step prompt-driven reasoning pipeline to produce concise, grounded evaluations. Results: Retrieval hyperparameters were optimized using Gaussian Process on a scalarized loss function combining root mean squared error (RMSE), mean absolute error (MAE), and clinically motivated accuracy thresholds. The best configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested end-to-end, the RAG system achieved 100% agreement with the computed values by standalone retrieval and constraint-checking modules on both percentile estimates and constraint identification, confirming reliable execution of all retrieval, prediction and checking steps. Conclusion: Our findings highlight the feasibility of combining structured population-based scoring with modular tool-augmented reasoning for transparent, scalable plan evaluation in radiation therapy. The system offers traceable outputs, minimizes hallucination, and demonstrates robustness across protocols. Future directions include clinician-led validation, and improved domain-adapted retrieval models to enhance real-world integration.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.266707"
    },
    {
        "index": "#27",
        "title": "Accelerate Creation of Product Claims Using Generative AI",
        "link": "/arxiv/2509.20652",
        "arxiv_id": "2509.20652",
        "authors": "Po-Yu Liang, Yong Zhang, Tatiana Hwa, Aaron Byers",
        "summary": "The benefit claims of a product is a critical driver of consumers' purchase behavior. Creating product claims is an intense task that requires substantial time and funding. We have developed the $\\textbf{Claim Advisor}$ web application to accelerate claim creations using in-context learning and fine-tuning of large language models (LLM). $\\textbf{Claim Advisor}$ was designed to disrupt the speed and economics of claim search, generation, optimization, and simulation. It has three functions: (1) semantically searching and identifying existing claims and/or visuals that resonate with the voice of consumers; (2) generating and/or optimizing claims based on a product description and a consumer profile; and (3) ranking generated and/or manually created claims using simulations via synthetic consumers. Applications in a consumer packaged goods (CPG) company have shown very promising results. We believe that this capability is broadly useful and applicable across product categories and industries. We share our learning to encourage the research and application of generative AI in different industries.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.266897"
    },
    {
        "index": "#28",
        "title": "Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI",
        "link": "/arxiv/2509.20640",
        "arxiv_id": "2509.20640",
        "authors": "Oluwakemi T. Olayinka, Sumeet Jeswani, Divine Iloh",
        "summary": "Traditional static cybersecurity models often struggle with scalability, real-time detection, and contextual responsiveness in the current digital product ecosystems which include cloud services, application programming interfaces (APIs), mobile platforms, and edge devices. This study introduces autonomous goal driven agents capable of dynamic learning and context-aware decision making as part of an adaptive cybersecurity architecture driven by agentic artificial intelligence (AI). To facilitate autonomous threat mitigation, proactive policy enforcement, and real-time anomaly detection, this framework integrates agentic AI across the key ecosystem layers. Behavioral baselining, decentralized risk scoring, and federated threat intelligence sharing are important features. The capacity of the system to identify zero-day attacks and dynamically modify access policies was demonstrated through native cloud simulations. The evaluation results show increased adaptability, decreased response latency, and improved detection accuracy. The architecture provides an intelligent and scalable blueprint for safeguarding complex digital infrastructure and is compatible with zero-trust models, thereby supporting the adherence to international cybersecurity regulations.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.267068"
    },
    {
        "index": "#29",
        "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection",
        "link": "/arxiv/2509.20562",
        "arxiv_id": "2509.20562",
        "authors": "Yubin Ge, Salvatore Romeo, Jason Cai, Monica Sunkara, Yi Zhang",
        "summary": "Despite the rapid advancements in LLM agents, they still face the challenge of generating meaningful reflections due to inadequate error analysis and a reliance on rare successful trajectories, especially in complex tasks. In this work, we propose SAMULE, a new framework for self-learning agents powered by a retrospective language model that is trained based on Multi-Level Reflection Synthesis. It first synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights based on same typed errors from diverse task failures. Then we fine-tune a language model serving as the retrospective model to generate reflections during inference. We further extend our framework to interactive settings through a foresight-based reflection mechanism, enabling agents to proactively reflect and adapt during user interactions by comparing predicted and actual responses. Extensive experiments on three challenging benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our approach significantly outperforms reflection-based baselines. Our results highlight the critical role of well-designed reflection synthesis and failure-centric learning in building self-improving LLM agents.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.267251"
    },
    {
        "index": "#30",
        "title": "A Compound Classification System Based on Fuzzy Relations Applied to the Noise-Tolerant Control of a Bionic Hand via EMG Signal Recognition",
        "link": "/arxiv/2509.20523",
        "arxiv_id": "2509.20523",
        "authors": "Pawel Trajdos, Marek Kurzynski",
        "summary": "Modern anthropomorphic upper limb bioprostheses are typically controlled by electromyographic (EMG) biosignals using a pattern recognition scheme. Unfortunately, there are many factors originating from the human source of objects to be classified and from the human-prosthesis interface that make it difficult to obtain an acceptable classification quality. One of these factors is the high susceptibility of biosignals to contamination, which can considerably reduce the quality of classification of a recognition system. In the paper, the authors propose a new recognition system intended for EMG based control of the hand prosthesis with detection of contaminated biosignals in order to mitigate the adverse effect of contaminations. The system consists of two ensembles: the set of one-class classifiers (OCC) to assess the degree of contamination of individual channels and the ensemble of K-nearest neighbours (KNN) classifier to recognise the patient's intent. For all recognition systems, an original, coherent fuzzy model was developed, which allows the use of a uniform soft (fuzzy) decision scheme throughout the recognition process. The experimental evaluation was conducted using real biosignals from a public repository. The goal was to provide an experimental comparative analysis of the parameters and procedures of the developed method on which the quality of the recognition system depends. The proposed fuzzy recognition system was also compared with similar systems described in the literature.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.267425"
    },
    {
        "index": "#31",
        "title": "Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications",
        "link": "/arxiv/2509.20520",
        "arxiv_id": "2509.20520",
        "authors": "Samer Alshaer, Ala Khalifeh, Roman Obermaisser",
        "summary": "Metascheduling in time-triggered architectures has been crucial in adapting to dynamic and unpredictable environments, ensuring the reliability and efficiency of task execution. However, traditional approaches face significant challenges when training Artificial Intelligence (AI) scheduling inferences offline, particularly due to the complexities involved in constructing a comprehensive Multi-Schedule Graph (MSG) that accounts for all possible scenarios. The process of generating an MSG that captures the vast probability space, especially when considering context events like hardware failures, slack variations, or mode changes, is resource-intensive and often infeasible. To address these challenges, we propose an adaptive online learning unit integrated within the metascheduler to enhance performance in real-time. The primary motivation for developing this unit stems from the limitations of offline training, where the MSG created is inherently a subset of the complete space, focusing only on the most probable and critical context events. In the online mode, Reinforcement Learning (RL) plays a pivotal role by continuously exploring and discovering new scheduling solutions, thus expanding the MSG and enhancing system performance over time. This dynamic adaptation allows the system to handle unexpected events and complex scheduling scenarios more effectively. Several RL models were implemented within the online learning unit, each designed to address specific challenges in scheduling. These models not only facilitate the discovery of new solutions but also optimize existing schedulers, particularly when stricter deadlines or new performance criteria are introduced. By continuously refining the AI inferences through real-time training, the system remains flexible and capable of meeting evolving demands, thus ensuring robustness and efficiency in large-scale, safety-critical environments.",
        "subjects": "Artificial Intelligence, Distributed, Parallel, and Cluster Computing, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.267619"
    },
    {
        "index": "#32",
        "title": "Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems",
        "link": "/arxiv/2509.20513",
        "arxiv_id": "2509.20513",
        "authors": "Samer Alshaer, Ala Khalifeh, Roman Obermaisser",
        "summary": "Adaptive scheduling is crucial for ensuring the reliability and safety of time-triggered systems (TTS) in dynamic operational environments. Scheduling frameworks face significant challenges, including message collisions, locked loops from incorrect precedence handling, and the generation of incomplete or invalid schedules, which can compromise system safety and performance. To address these challenges, this paper presents a novel reconstruction framework designed to dynamically validate and assemble schedules. The proposed reconstruction models operate by systematically transforming AI-generated or heuristically derived scheduling priorities into fully executable schedules, ensuring adherence to critical system constraints such as precedence rules and collision-free communication. It incorporates robust safety checks, efficient allocation algorithms, and recovery mechanisms to handle unexpected context events, including hardware failures and mode transitions. Comprehensive experiments were conducted across multiple performance profiles, including makespan minimisation, workload balancing, and energy efficiency, to validate the operational effectiveness of the reconstruction models. Results demonstrate that the proposed framework significantly enhances system adaptability, operational integrity, and runtime performance while maintaining computational efficiency. Overall, this work contributes a practical and scalable solution to the problem of safe schedule generation in safety-critical TTS, enabling reliable and flexible real-time scheduling even under highly dynamic and uncertain operational conditions.",
        "subjects": "Artificial Intelligence, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.267800"
    },
    {
        "index": "#34",
        "title": "Philosophy-informed Machine Learning",
        "link": "/arxiv/2509.20370",
        "arxiv_id": "2509.20370",
        "authors": "MZ Naser",
        "summary": "Philosophy-informed machine learning (PhIML) directly infuses core ideas from analytic philosophy into ML model architectures, objectives, and evaluation protocols. Therefore, PhIML promises new capabilities through models that respect philosophical concepts and values by design. From this lens, this paper reviews conceptual foundations to demonstrate philosophical gains and alignment. In addition, we present case studies on how ML users/designers can adopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML model architectures. Finally, this paper sheds light on open technical barriers alongside philosophical, practical, and governance challenges and outlines a research roadmap toward safe, philosophy-aware, and ethically responsible PhIML.",
        "subjects": "Artificial Intelligence, Computers and Society, Machine Learning",
        "date": "2025-09-18",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.268272"
    },
    {
        "index": "#35",
        "title": "LATTS: Locally Adaptive Test-Time Scaling",
        "link": "/arxiv/2509.20368",
        "arxiv_id": "2509.20368",
        "authors": "Theo Uscidda, Matthew Trager, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto",
        "summary": "One common strategy for improving the performance of Large Language Models (LLMs) on downstream tasks involves using a \\emph{verifier model} to either select the best answer from a pool of candidates or to steer the auto-regressive generation process towards better outputs. This class of methods typically results in improved accuracy at the cost of increased computation at test-time, a paradigm known as \\emph{test-time scaling}. However, most existing approaches increase computation uniformly across all samples and generation steps, without considering the complexity of individual instances, leading to inefficient resource use. We address this limitation by proposing an approach, called \\emph{Locally Adaptive Test-Time Scaling (LATTS)}, that allocates variable compute across generation steps. Specifically, at each generation step, LATTS employs a verifier-based acceptance criterion to decide whether to resample, backtrack, restart, or stop the generation process. This criterion effectively adjusts the per-step computational effort based on a precise notion of \\emph{local difficulty} derived from the verifier model. Empirical results show that LATTS achieves significantly superior accuracy--compute tradeoffs compared to standard verifier-based methods.",
        "subjects": "Artificial Intelligence",
        "date": "2025-09-16",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.268515"
    },
    {
        "index": "#36",
        "title": "An Approach to Checking Correctness for Agentic Systems",
        "link": "/arxiv/2509.20364",
        "arxiv_id": "2509.20364",
        "authors": "Thomas J Sheffler",
        "summary": "This paper presents a temporal expression language for monitoring AI agent behavior, enabling systematic error-detection of LLM-based agentic systems that exhibit variable outputs due to stochastic generation processes. Drawing from temporal logic techniques used in hardware verification, this approach monitors execution traces of agent tool calls and state transitions to detect deviations from expected behavioral patterns. Current error-detection approaches rely primarily on text matching of inputs and outputs, which proves fragile due to the natural language variability inherent in LLM responses. The proposed method instead focuses on the sequence of agent actions -- such as tool invocations and inter-agent communications -- allowing verification of system behavior independent of specific textual outputs. The temporal expression language provides assertions that capture correct behavioral patterns across multiple execution scenarios. These assertions serve dual purposes: validating prompt engineering and guardrail effectiveness during development, and providing regression testing when agents are updated with new LLMs or modified logic. The approach is demonstrated using a three-agent system, where agents coordinate to solve multi-step reasoning tasks. When powered by large, capable models, all temporal assertions were satisfied across many test runs. However, when smaller models were substituted in two of the three agents, executions violated behavioral assertions, primarily due to improper tool sequencing and failed coordination handoffs. The temporal expressions successfully flagged these anomalies, demonstrating the method's effectiveness for detecting behavioral regressions in production agentic systems. This approach provides a foundation for systematic monitoring of AI agent reliability as these systems become increasingly deployed in critical applications.",
        "subjects": "Artificial Intelligence, Emerging Technologies",
        "date": "2025-08-19",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.268698"
    },
    {
        "index": "#39",
        "title": "No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks",
        "link": "/arxiv/2509.21296",
        "arxiv_id": "2509.21296",
        "authors": "Yehonatan Refael, Guy Smorodinsky, Ofir Lindenbaum, Itay Safran",
        "summary": "The memorization of training data by neural networks raises pressing concerns for privacy and security. Recent work has shown that, under certain conditions, portions of the training set can be reconstructed directly from model parameters. Some of these methods exploit implicit bias toward margin maximization, suggesting that properties often regarded as beneficial for generalization may actually compromise privacy. Yet despite striking empirical demonstrations, the reliability of these attacks remains poorly understood and lacks a solid theoretical foundation. In this work, we take a complementary perspective: rather than designing stronger attacks, we analyze the inherent weaknesses and limitations of existing reconstruction methods and identify conditions under which they fail. We rigorously prove that, without incorporating prior knowledge about the data, there exist infinitely many alternative solutions that may lie arbitrarily far from the true training set, rendering reconstruction fundamentally unreliable. Empirically, we further demonstrate that exact duplication of training examples occurs only by chance. Our results refine the theoretical understanding of when training set leakage is possible and offer new insights into mitigating reconstruction attacks. Remarkably, we demonstrate that networks trained more extensively, and therefore satisfying implicit bias conditions more strongly -- are, in fact, less susceptible to reconstruction attacks, reconciling privacy with the need for strong generalization in this setting.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.269415"
    },
    {
        "index": "#41",
        "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL",
        "link": "/arxiv/2509.21282",
        "arxiv_id": "2509.21282",
        "authors": "Madeleine Dwyer, Adam Sobey, Adriane Chapman",
        "summary": "Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information and introduces gradient discontinuities. We propose Probability Smoothing Policy Optimisation (PSPO), which smooths the current policy's probabilities toward the old (behaviour) policy before computing the importance ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient signal, while interpolation toward the old policy creates a soft trust region that discourages large, destabilising updates, with formal guarantees. We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO (single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar performance but improves the reasoning leading to clearer and more concise responses which are more logical. Compared to clipped GRPO, GR-PSPO substantially improves performance both the 0.5B and 1.5B models, with a boost of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.269842"
    },
    {
        "index": "#43",
        "title": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training",
        "link": "/arxiv/2509.21275",
        "arxiv_id": "2509.21275",
        "authors": "Shiju Wang, Yujie Wang, Ao Sun, Fangcheng Fu, Zijian Zhu, Bin Cui, Xu Han, Kaisheng Ma",
        "summary": "Long context training is crucial for LLM's context extension. Existing schemes, such as sequence parallelism, incur substantial communication overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness hinges on partitioning granularity. Batch-level PP dividing input samples exhibits high memory consumption in long-context scenario, whereas token-level PP splitting sequences into slices alleviates memory overhead but may incur hardware under-utilization. This trade-off motivates adaptively selecting PP granularity to match resource and workload characteristics. Moreover, sequence length distribution of the real-world dataset exhibits skewness, posing a challenge on PP's workload balance and efficient scheduling. Current static PP scheduling methods overlook the variance of sequence length, leading to suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism (EPP) that orchestrates token-level PP and batch-level PP to adapt to resource and workload heterogeneity. We build InfiniPipe, a distributed training system that unleashes the potential of EPP via (1) a resource-aware and workload-balanced sequence processor that splits long sequences and packs short ones; and (2) a co-optimization methodology that jointly optimizes pipeline schedule and gradient checkpointing via a mechanism named stage-aware chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.270268"
    },
    {
        "index": "#45",
        "title": "A Causality-Aware Spatiotemporal Model for Multi-Region and Multi-Pollutant Air Quality Forecasting",
        "link": "/arxiv/2509.21260",
        "arxiv_id": "2509.21260",
        "authors": "Junxin Lu, Shiliang Sun",
        "summary": "Air pollution, a pressing global problem, threatens public health, environmental sustainability, and climate stability. Achieving accurate and scalable forecasting across spatially distributed monitoring stations is challenging due to intricate multi-pollutant interactions, evolving meteorological conditions, and region specific spatial heterogeneity. To address this challenge, we propose AirPCM, a novel deep spatiotemporal forecasting model that integrates multi-region, multi-pollutant dynamics with explicit meteorology-pollutant causality modeling. Unlike existing methods limited to single pollutants or localized regions, AirPCM employs a unified architecture to jointly capture cross-station spatial correlations, temporal auto-correlations, and meteorology-pollutant dynamic causality. This empowers fine-grained, interpretable multi-pollutant forecasting across varying geographic and temporal scales, including sudden pollution episodes. Extensive evaluations on multi-scale real-world datasets demonstrate that AirPCM consistently surpasses state-of-the-art baselines in both predictive accuracy and generalization capability. Moreover, the long-term forecasting capability of AirPCM provides actionable insights into future air quality trends and potential high-risk windows, offering timely support for evidence-based environmental governance and carbon mitigation planning.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.270742"
    },
    {
        "index": "#46",
        "title": "Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks",
        "link": "/arxiv/2509.21259",
        "arxiv_id": "2509.21259",
        "authors": "Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Aisha Syed, Matthew Andrews, Sean Kennedy",
        "summary": "Real-time urban traffic surveillance is vital for Intelligent Transportation Systems (ITS) to ensure road safety, optimize traffic flow, track vehicle trajectories, and prevent collisions in smart cities. Deploying edge cameras across urban environments is a standard practice for monitoring road conditions. However, integrating these with intelligent models requires a robust understanding of dynamic traffic scenarios and a responsive interface for user interaction. Although multimodal Large Language Models (LLMs) can interpret traffic images and generate informative responses, their deployment on edge devices is infeasible due to high computational demands. Therefore, LLM inference must occur on the cloud, necessitating visual data transmission from edge to cloud, a process hindered by limited bandwidth, leading to potential delays that compromise real-time performance. To address this challenge, we propose a semantic communication framework that significantly reduces transmission overhead. Our method involves detecting Regions of Interest (RoIs) using YOLOv11, cropping relevant image segments, and converting them into compact embedding vectors using a Vision Transformer (ViT). These embeddings are then transmitted to the cloud, where an image decoder reconstructs the cropped images. The reconstructed images are processed by a multimodal LLM to generate traffic condition descriptions. This approach achieves a 99.9% reduction in data transmission size while maintaining an LLM response accuracy of 89% for reconstructed cropped images, compared to 93% accuracy with original cropped images. Our results demonstrate the efficiency and practicality of ViT and LLM-assisted edge-cloud semantic communication for real-time traffic surveillance.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.276227"
    },
    {
        "index": "#51",
        "title": "Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework",
        "link": "/arxiv/2509.21241",
        "arxiv_id": "2509.21241",
        "authors": "Yucheng Wang, Ziyang Chen, Md Faisal Kabir",
        "summary": "The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large language models (LLMs) to acquire domain-specific knowledge with remarkable efficiency. However, understanding how such a fine-tuning mechanism alters a model's structural reasoning and semantic behavior remains an open challenge. This work introduces a novel framework that explains fine-tuned LLMs via counterfactuals grounded in knowledge graphs. Specifically, we construct BioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics tools and design a counterfactual-based fine-tuned LLMs explainer (CFFTLLMExplainer) that learns soft masks over graph nodes and edges to generate minimal structural perturbations that induce maximum semantic divergence. Our method jointly optimizes structural sparsity and semantic divergence while enforcing interpretability preserving constraints such as entropy regularization and edge smoothness. We apply this framework to a fine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the model's structural dependencies and aligns with LoRA-induced parameter shifts. This work provides new insights into the internal mechanisms of fine-tuned LLMs and highlights counterfactual graphs as a potential tool for interpretable AI.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.278054"
    },
    {
        "index": "#52",
        "title": "Tree Search for LLM Agent Reinforcement Learning",
        "link": "/arxiv/2509.21240",
        "arxiv_id": "2509.21240",
        "authors": "Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu",
        "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.278269"
    },
    {
        "index": "#53",
        "title": "Evading Overlapping Community Detection via Proxy Node Injection",
        "link": "/arxiv/2509.21211",
        "arxiv_id": "2509.21211",
        "authors": "Dario Loi, Matteo Silvestri, Fabrizio Silvestri, Gabriele Tolomei",
        "summary": "Protecting privacy in social graphs requires preventing sensitive information, such as community affiliations, from being inferred by graph analysis, without substantially altering the graph topology. We address this through the problem of \\emph{community membership hiding} (CMH), which seeks edge modifications that cause a target node to exit its original community, regardless of the detection algorithm employed. Prior work has focused on non-overlapping community detection, where trivial strategies often suffice, but real-world graphs are better modeled by overlapping communities, where such strategies fail. To the best of our knowledge, we are the first to formalize and address CMH in this setting. In this work, we propose a deep reinforcement learning (DRL) approach that learns effective modification policies, including the use of proxy nodes, while preserving graph structure. Experiments on real-world datasets show that our method significantly outperforms existing baselines in both effectiveness and efficiency, offering a principled tool for privacy-preserving graph modification with overlapping communities.",
        "subjects": "Social and Information Networks, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.278454"
    },
    {
        "index": "#55",
        "title": "Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy",
        "link": "/arxiv/2509.21190",
        "arxiv_id": "2509.21190",
        "authors": "Tian Lan, Hao Duong Le, Jinbo Li, Wenjun He, Meng Wang, Chenghao Liu, Chen Zhang",
        "summary": "Time series anomaly detection (TSAD) is a critical task, but developing models that generalize to unseen data in a zero-shot manner remains a major challenge. Prevailing foundation models for TSAD predominantly rely on reconstruction-based objectives, which suffer from a fundamental objective mismatch: they struggle to identify subtle anomalies while often misinterpreting complex normal patterns, leading to high rates of false negatives and positives. To overcome these limitations, we introduce \\texttt{TimeRCD}, a novel foundation model for TSAD built upon a new pre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning to reconstruct inputs, \\texttt{TimeRCD} is explicitly trained to identify anomalies by detecting significant discrepancies between adjacent time windows. This relational approach, implemented with a standard Transformer architecture, enables the model to capture contextual shifts indicative of anomalies that reconstruction-based methods often miss. To facilitate this paradigm, we develop a large-scale, diverse synthetic corpus with token-level anomaly labels, providing the rich supervisory signal necessary for effective pre-training. Extensive experiments demonstrate that \\texttt{TimeRCD} significantly outperforms existing general-purpose and anomaly-specific foundation models in zero-shot TSAD across diverse datasets. Our results validate the superiority of the RCD paradigm and establish a new, effective path toward building robust and generalizable foundation models for time series anomaly detection.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.278917"
    },
    {
        "index": "#57",
        "title": "Adoption, usability and perceived clinical value of a UK AI clinical reference platform (iatroX): a mixed-methods formative evaluation of real-world usage and a 1,223-respondent user survey",
        "link": "/arxiv/2509.21188",
        "arxiv_id": "2509.21188",
        "authors": "Kolawole Tytler",
        "summary": "Clinicians face growing information overload from biomedical literature and guidelines, hindering evidence-based care. Retrieval-augmented generation (RAG) with large language models may provide fast, provenance-linked answers, but requires real-world evaluation. We describe iatroX, a UK-centred RAG-based clinical reference platform, and report early adoption, usability, and perceived clinical value from a formative implementation evaluation. Methods comprised a retrospective analysis of usage across web, iOS, and Android over 16 weeks (8 April-31 July 2025) and an in-product intercept survey. Usage metrics were drawn from web and app analytics with bot filtering. A client-side script randomized single-item prompts to approx. 10% of web sessions from a predefined battery assessing usefulness, reliability, and adoption intent. Proportions were summarized with Wilson 95% confidence intervals; free-text comments underwent thematic content analysis. iatroX reached 19,269 unique web users, 202,660 engagement events, and approx. 40,000 clinical queries. Mobile uptake included 1,960 iOS downloads and Android growth (peak >750 daily active users). The survey yielded 1,223 item-level responses: perceived usefulness 86.2% (95% CI 74.8-93.9%; 50/58); would use again 93.3% (95% CI 68.1-99.8%; 14/15); recommend to a colleague 88.4% (95% CI 75.1-95.9%; 38/43); perceived accuracy 75.0% (95% CI 58.8-87.3%; 30/40); reliability 79.4% (95% CI 62.1-91.3%; 27/34). Themes highlighted speed, guideline-linked answers, and UK specificity. Early real-world use suggests iatroX can mitigate information overload and support timely answers for UK clinicians. Limitations include small per-item samples and early-adopter bias; future work will include accuracy audits and prospective studies on workflow and care quality.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Computers and Society, Information Retrieval",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.279306"
    },
    {
        "index": "#59",
        "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach",
        "link": "/arxiv/2509.21170",
        "arxiv_id": "2509.21170",
        "authors": "Yongda Yu, Guohao Shi, Xianwei Wu, Haochuan He, XueMing Gu, Qianqian Zhao, Kui Liu, Qiushi Wang, Zhao Tian, Haifeng Shen, Guoping Rong",
        "summary": "Large Language Models (LLMs) have shown great potential in supporting automated code review due to their impressive capabilities in context understanding and reasoning. However, these capabilities are still limited compared to human-level cognition because they are heavily influenced by the training data. Recent research has demonstrated significantly improved performance through fine-tuning LLMs with code review data. However, compared to human reviewers who often simultaneously analyze multiple dimensions of code review to better identify issues, the full potential of these methods is hampered by the limited or vague information used to fine-tune the models. This paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that trains LLMs with an impressive reasoning ability to analyze multiple dimensions of code review by harnessing long COT techniques to provide rich structured information. To address context loss and reasoning logic loss issues that frequently occur when LLMs process long COT prompts, we propose a solution that combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning pathways in MelcotCR to enable more effective utilization of in-context knowledge within long COT prompts while strengthening the logical tightness of the reasoning process. Empirical evaluations on our curated MelcotCR dataset and the public CodeReviewer dataset reveal that a low-parameter base model, such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art methods in terms of the accuracy of detecting and describing code issues, with its performance remarkably on par with that of the 671B DeepSeek-R1 model.",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.279748"
    },
    {
        "index": "#60",
        "title": "GRPO is Secretly a Process Reward Model",
        "link": "/arxiv/2509.21154",
        "arxiv_id": "2509.21154",
        "authors": "Michael Sullivan",
        "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and show that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.279973"
    },
    {
        "index": "#62",
        "title": "LAVA: Explainability for Unsupervised Latent Embeddings",
        "link": "/arxiv/2509.21149",
        "arxiv_id": "2509.21149",
        "authors": "Ivan Stresec, Joana P. Gonçalves",
        "summary": "Unsupervised black-box models can be drivers of scientific discovery, but remain difficult to interpret. Crucially, discovery hinges on understanding the model output, which is often a multi-dimensional latent embedding rather than a well-defined target. While explainability for supervised learning usually seeks to uncover how input features are used to predict a target, its unsupervised counterpart should relate input features to the structure of the learned latent space. Adaptations of supervised model explainability for unsupervised learning provide either single-sample or dataset-wide summary explanations. However, without automated strategies of relating similar samples to one another guided by their latent proximity, explanations remain either too fine-grained or too reductive to be meaningful. This is especially relevant for manifold learning methods that produce no mapping function, leaving us only with the relative spatial organization of their embeddings. We introduce Locality-Aware Variable Associations (LAVA), a post-hoc model-agnostic method designed to explain local embedding organization through its relationship with the input features. To achieve this, LAVA represents the latent space as a series of localities (neighborhoods) described in terms of correlations between the original features, and then reveals reoccurring patterns of correlations across the entire latent space. Based on UMAP embeddings of MNIST and a single-cell kidney dataset, we show that LAVA captures relevant feature associations, with visually and biologically relevant local patterns shared among seemingly distant regions of the latent spaces.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.280374"
    },
    {
        "index": "#63",
        "title": "Emerging Paradigms for Securing Federated Learning Systems",
        "link": "/arxiv/2509.21147",
        "arxiv_id": "2509.21147",
        "authors": "Amr Akmal Abouelmagd, Amr Hilal",
        "summary": "Federated Learning (FL) facilitates collaborative model training while keeping raw data decentralized, making it a conduit for leveraging the power of IoT devices while maintaining privacy of the locally collected data. However, existing privacy- preserving techniques present notable hurdles. Methods such as Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential Privacy (DP) often incur high compu- tational costs and suffer from limited scalability. This survey examines emerging approaches that hold promise for enhancing both privacy and efficiency in FL, including Trusted Execution Environments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing (QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm Intelligence (SI). For each paradigm, we assess its relevance to the FL pipeline, outlining its strengths, limitations, and practical considerations. We conclude by highlighting open challenges and prospective research avenues, offering a detailed roadmap for advancing secure and scalable FL systems.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Emerging Technologies, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.280565"
    },
    {
        "index": "#64",
        "title": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
        "link": "/arxiv/2509.21144",
        "arxiv_id": "2509.21144",
        "authors": "Sitong Cheng, Weizhen Bian, Xinsheng Wang, Ruibin Yuan, Jianyi Chen, Shunshun Yin, Yike Guo, Wei Xue",
        "summary": "The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at https://cmots.github.io/uniss-demo.",
        "subjects": "Sound, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.280773"
    },
    {
        "index": "#65",
        "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning",
        "link": "/arxiv/2509.21126",
        "arxiv_id": "2509.21126",
        "authors": "Xiefeng Wu, Jing Zhao, Shu Zhang, Mingyu Hu",
        "summary": "Online reinforcement learning in complex tasks is time-consuming, as massive interaction steps are needed to learn the optimal Q-function.Vision-language action (VLA) policies represent a promising direction for solving diverse tasks; however, their performance on low-level control remains limited, and effective deployment often requires task-specific expert demonstrations for fine-tuning. In this paper, we propose \\textbf{VARL} (\\textbf{V}LM as \\textbf{A}ction advisor for online \\textbf{R}einforcement \\textbf{L}earning), a framework that leverages the domain knowledge of vision-language models (VLMs) to provide action suggestions for reinforcement learning agents. Unlike previous methods, VARL provides action suggestions rather than designing heuristic rewards, thereby guaranteeing unchanged optimality and convergence. The suggested actions increase sample diversity and ultimately improve sample efficiency, especially in sparse-reward tasks. To validate the effectiveness of VARL, we evaluate it across diverse environments and agent settings. Results show that VARL greatly improves sample efficiency without introducing significant computational overhead. These advantages make VARL a general framework for online reinforcement learning and make it feasible to directly apply reinforcement learning from scratch in real-world environments.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.280958"
    },
    {
        "index": "#67",
        "title": "GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization",
        "link": "/arxiv/2509.21097",
        "arxiv_id": "2509.21097",
        "authors": "Louis Van Langendonck, Guillermo Bernárdez, Nina Miolane, Pere Barlet-Ros",
        "summary": "A fundamental challenge in graph learning is understanding how models generalize to new, unseen graphs. While synthetic benchmarks offer controlled settings for analysis, existing approaches are confined to single-graph, transductive settings where models train and test on the same graph structure. Addressing this gap, we introduce GraphUniverse, a framework for generating entire families of graphs to enable the first systematic evaluation of inductive generalization at scale. Our core innovation is the generation of graphs with persistent semantic communities, ensuring conceptual consistency while allowing fine-grained control over structural properties like homophily and degree distributions. This enables crucial but underexplored robustness tests, such as performance under controlled distribution shifts. Benchmarking a wide range of architectures -- from GNNs to graph transformers and topological architectures -- reveals that strong transductive performance is a poor predictor of inductive generalization. Furthermore, we find that robustness to distribution shift is highly sensitive not only to model architecture choice but also to the initial graph regime (e.g., high vs. low homophily). Beyond benchmarking, GraphUniverse's flexibility and scalability can facilitate the development of robust and truly generalizable architectures -- including next-generation graph foundation models. An interactive demo is available at https://graphuniverse.streamlit.app.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.286792"
    },
    {
        "index": "#68",
        "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
        "link": "/arxiv/2509.21091",
        "arxiv_id": "2509.21091",
        "authors": "Junpei Komiyama, Daisuke Oba, Masafumi Oyamada",
        "summary": "We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.287125"
    },
    {
        "index": "#70",
        "title": "TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix",
        "link": "/arxiv/2509.21081",
        "arxiv_id": "2509.21081",
        "authors": "Ahmet Caner Yüzügüler, Ahmet Çelik, Jiawei Zhuang, Lukas Cavigelli",
        "summary": "Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in state-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel formulation, MLA allows two functionally equivalent but computationally distinct kernel implementations: naive and absorb. While the naive kernels (e.g., FlashAttention) are typically preferred in training and prefill for their computational efficiency, existing decoding kernels (e.g., FlashMLA) rely on the absorb method to minimize HBM bandwidth usage. However, the compute-bound nature of the absorb implementations prohibits performance benefits from data reuse opportunities in attention calculations, such as shared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that combines naive and absorb formulations to harness the strengths of both. TyphoonMLA effectively leverages the shared prefix by applying the naive formulation to the compute-bound parts of attention calculations, while reducing the bandwidth requirements for non-shared parts by using the absorb formulation. As a result, TyphoonMLA improves the throughput of attention calculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with only a 3% overhead in HBM size.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.287558"
    },
    {
        "index": "#75",
        "title": "GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic Supervision, and Reinforced MLLM-based Solutions",
        "link": "/arxiv/2509.21050",
        "arxiv_id": "2509.21050",
        "authors": "Bing Liu, Wenqiang Yv, Xuzheng Yang, Shichang Wang, Junzhuo Liu, Peng Wang, Guoqing Wang, Yang Yang, Heng Tao Shen",
        "summary": "AI-driven geometric problem solving is a complex vision-language task that requires accurate diagram interpretation, mathematical reasoning, and robust cross-modal grounding. A foundational yet underexplored capability for this task is the ability to identify and interpret geometric elements based on natural language queries. To address this, we introduce the task of Referring Expression Comprehension (REC) for geometric problems, which evaluates whether models can localize points, shapes, and spatial relations in diagrams in response to textual prompts. We present GeoRef, a benchmark dataset constructed from existing geometric problem corpora, featuring diverse, high-quality annotations and queries. Due to the lack of annotated data for this task, we generate a large-scale synthetic training dataset using a structured geometric formal language, enabling broad coverage of geometric concepts and facilitating model adaptation. We explore two fine-tuning approaches: Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). Our results show that GRPO significantly outperforms SFT by better aligning model behavior with task-specific rewards. Furthermore, we propose a verify-and-regenerate mechanism that detects incorrect predictions and re-infers answers using contextual reasoning history, further boosting accuracy. Notably, even state-of-the-art Multimodal Large Language Models (MLLMs) struggle with this task, underscoring the necessity of explicitly evaluating and strengthening geometric grounding as a prerequisite for robust geometric problem solving. Moreover, models trained on GeoRef demonstrate measurable improvements on downstream geometric reasoning tasks, highlighting the broader value of REC as a foundation for multimodal mathematical understanding.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.288774"
    },
    {
        "index": "#76",
        "title": "Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs",
        "link": "/arxiv/2509.21044",
        "arxiv_id": "2509.21044",
        "authors": "Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li",
        "summary": "Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.288960"
    },
    {
        "index": "#78",
        "title": "SupCLAP: Controlling Optimization Trajectory Drift in Audio-Text Contrastive Learning with Support Vector Regularization",
        "link": "/arxiv/2509.21033",
        "arxiv_id": "2509.21033",
        "authors": "Jiehui Luo, Yuguo Yin, Yuxin Xie, Jinghan Ru, Xianwei Zhuang, Minghua He, Aofan Liu, Zihan Xiong, Dongchao Yang",
        "summary": "Contrastive language-audio pretraining, which aims to unify multimodal representations in a shared embedding space, serves as a cornerstone for building a wide range of applications, from cross-modal retrieval to cutting-edge multimodal large language models. However, we find that the perpendicular component of the pushing force from negative samples in contrastive learning is a double-edged sword: it contains rich supplementary information from negative samples, yet its unconstrained nature causes optimization trajectory drift and training instability. To address this, we propose Support Vector Regularization (SVR), a method that introduces an auxiliary support vector to control this perpendicular component, aiming to harness its rich information while mitigating the associated trajectory drift. The efficacy of SVR is critically governed by its semantic radius, for which we explore two unsupervised modeling strategies: direct parameterization and an adaptive radius predictor module enhanced with constraints to improve its predicting accuracy. Extensive experimental results demonstrate that our method surpasses widely used baselines like InfoNCE and SigLIP loss across classification, monolingual retrieval, and multilingual retrieval on standard audio-text datasets. Both the theoretical analysis and the experimental results on optimizing trajectory drift validate the correctness and effectiveness of our SVR method.",
        "subjects": "Sound, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.289349"
    },
    {
        "index": "#79",
        "title": "Efficient Ensemble Conditional Independence Test Framework for Causal Discovery",
        "link": "/arxiv/2509.21021",
        "arxiv_id": "2509.21021",
        "authors": "Zhengkang Guan, Kun Kuang",
        "summary": "Constraint-based causal discovery relies on numerous conditional independence tests (CITs), but its practical applicability is severely constrained by the prohibitive computational cost, especially as CITs themselves have high time complexity with respect to the sample size. To address this key bottleneck, we introduce the Ensemble Conditional Independence Test (E-CIT), a general and plug-and-play framework. E-CIT operates on an intuitive divide-and-aggregate strategy: it partitions the data into subsets, applies a given base CIT independently to each subset, and aggregates the resulting p-values using a novel method grounded in the properties of stable distributions. This framework reduces the computational complexity of a base CIT to linear in the sample size when the subset size is fixed. Moreover, our tailored p-value combination method offers theoretical consistency guarantees under mild conditions on the subtests. Experimental results demonstrate that E-CIT not only significantly reduces the computational burden of CITs and causal discovery but also achieves competitive performance. Notably, it exhibits an improvement in complex testing scenarios, particularly on real-world datasets.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.289533"
    },
    {
        "index": "#80",
        "title": "The Use of the Simplex Architecture to Enhance Safety in Deep-Learning-Powered Autonomous Systems",
        "link": "/arxiv/2509.21014",
        "arxiv_id": "2509.21014",
        "authors": "Federico Nesti, Niko Salamini, Mauro Marinoni, Giorgio Maria Cicero, Gabriele Serra, Alessandro Biondi, Giorgio Buttazzo",
        "summary": "Recently, the outstanding performance reached by neural networks in many tasks has led to their deployment in autonomous systems, such as robots and vehicles. However, neural networks are not yet trustworthy, being prone to different types of misbehavior, such as anomalous samples, distribution shifts, adversarial attacks, and other threats. Furthermore, frameworks for accelerating the inference of neural networks typically run on rich operating systems that are less predictable in terms of timing behavior and present larger surfaces for cyber-attacks. To address these issues, this paper presents a software architecture for enhancing safety, security, and predictability levels of learning-based autonomous systems. It leverages two isolated execution domains, one dedicated to the execution of neural networks under a rich operating system, which is deemed not trustworthy, and one responsible for running safety-critical functions, possibly under a different operating system capable of handling real-time constraints. Both domains are hosted on the same computing platform and isolated through a type-1 real-time hypervisor enabling fast and predictable inter-domain communication to exchange real-time data. The two domains cooperate to provide a fail-safe mechanism based on a safety monitor, which oversees the state of the system and switches to a simpler but safer backup module, hosted in the safety-critical domain, whenever its behavior is considered untrustworthy. The effectiveness of the proposed architecture is illustrated by a set of experiments performed on two control systems: a Furuta pendulum and a rover. The results confirm the utility of the fall-back mechanism in preventing faults due to the learning component.",
        "subjects": "Systems and Control, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.289737"
    },
    {
        "index": "#81",
        "title": "Predicting LLM Reasoning Performance with Small Proxy Model",
        "link": "/arxiv/2509.21013",
        "arxiv_id": "2509.21013",
        "authors": "Woosung Koh, Juyoung Suk, Sungjun Han, Se-Young Yun, Jay Shin",
        "summary": "Given the prohibitive cost of pre-training large language models, it is essential to leverage smaller proxy models to optimize datasets before scaling up. However, this approach becomes challenging for reasoning capabilities, which exhibit emergent behavior that only appear reliably at larger model sizes, often exceeding 7B parameters. To address this, we introduce rBridge, showing that small proxies ($\\leq$1B) can effectively predict large-model reasoning by aligning more closely with (1) the pre-training objective and (2) the target task. rBridge achieves this by weighting negative log-likelihood with task alignment, using reasoning traces from frontier models as gold labels. In our experiments, rBridge (i) reduces dataset ranking costs by over 100x relative to the best baseline, (ii) achieves the strongest correlation across six reasoning benchmarks at 1B to 32B scale, and (iii) zero-shot transfers predictive relationships across pre-training datasets at 1B to 7B scale. These findings indicate that rBridge offers a practical path for exploring reasoning-oriented pre-training at lower cost.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.289929"
    },
    {
        "index": "#83",
        "title": "Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools",
        "link": "/arxiv/2509.21011",
        "arxiv_id": "2509.21011",
        "authors": "Ping He, Changjiang Li, Binbin Zhao, Tianyu Du, Shouling Ji",
        "summary": "The remarkable capability of large language models (LLMs) has led to the wide application of LLM-based agents in various domains. To standardize interactions between LLM-based agents and their environments, model context protocol (MCP) tools have become the de facto standard and are now widely integrated into these agents. However, the incorporation of MCP tools introduces the risk of tool poisoning attacks, which can manipulate the behavior of LLM-based agents. Although previous studies have identified such vulnerabilities, their red teaming approaches have largely remained at the proof-of-concept stage, leaving the automatic and systematic red teaming of LLM-based agents under the MCP tool poisoning paradigm an open question. To bridge this gap, we propose AutoMalTool, an automated red teaming framework for LLM-based agents by generating malicious MCP tools. Our extensive evaluation shows that AutoMalTool effectively generates malicious MCP tools capable of manipulating the behavior of mainstream LLM-based agents while evading current detection mechanisms, thereby revealing new security risks in these agents.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Software Engineering",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.290391"
    },
    {
        "index": "#84",
        "title": "ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning",
        "link": "/arxiv/2509.21010",
        "arxiv_id": "2509.21010",
        "authors": "Haotian Guo, Hui Liu",
        "summary": "The generation of high-quality candidate molecules remains a central challenge in AI-driven drug design. Current phenotype-based and target-based strategies each suffer limitations, either incurring high experimental costs or overlook system-level cellular responses. To bridge this gap, we propose ExMoIRL, a novel generative framework that synergistically integrates phenotypic and target-specific cues for de novo molecular generation. The phenotype-guided generator is first pretrained on expansive drug-induced transcriptional profiles and subsequently fine-tuned via multi-objective reinforcement learning (RL). Crucially, the reward function fuses docking affinity and drug-likeness scores, augmented with ranking loss, prior-likelihood regularization, and entropy maximization. The multi-objective RL steers the model toward chemotypes that are simultaneously potent, diverse, and aligned with the specified phenotypic effects. Extensive experiments demonstrate ExMoIRL's superior performance over state-of-the-art phenotype-based and target-based models across multiple well-characterized targets. Our generated molecules exhibit favorable drug-like properties, high target affinity, and inhibitory potency (IC50) against cancer cells. This unified framework showcases the synergistic potential of combining phenotype-guided and target-aware strategies, offering a more effective solution for de novo drug discovery.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.290571"
    },
    {
        "index": "#86",
        "title": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation",
        "link": "/arxiv/2509.21006",
        "arxiv_id": "2509.21006",
        "authors": "Konstantin Gubernatorov, Artem Voronov, Roman Voronov, Sergei Pasynkov, Stepan Perminov, Ziang Guo, Dzmitry Tsetserukou",
        "summary": "We address natural language pick-and-place in unseen, unpredictable indoor environments with AnywhereVLA, a modular framework for mobile manipulation. A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy. An approach planner then selects visibility and reachability aware pre grasp base poses. For interaction, a compact SmolVLA manipulation head is fine tuned on platform pick and place trajectories for the SO-101 by TheRobotStudio, grounding local visual context and sub-goals into grasp and place proposals. The full system runs fully onboard on consumer-level hardware, with Jetson Orin NX for perception and VLA and an Intel NUC for SLAM, exploration, and control, sustaining real-time operation. We evaluated AnywhereVLA in a multi-room lab under static scenes and normal human motion. In this setting, the system achieves a $46\\%$ overall task success rate while maintaining throughput on embedded compute. By combining a classical stack with a fine-tuned VLA manipulation, the system inherits the reliability of geometry-based navigation with the agility and task generalization of language-conditioned manipulation.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.290974"
    },
    {
        "index": "#87",
        "title": "Lossless Compression: A New Benchmark for Time Series Model Evaluation",
        "link": "/arxiv/2509.21002",
        "arxiv_id": "2509.21002",
        "authors": "Meng Wan, Benxi Tian, Jue Wang, Cui Hui, Ningming Nie, Tiantian Liu, Zongguo Wang, Cao Rongqiang, Peng Shi, Yangang Wang",
        "summary": "The evaluation of time series models has traditionally focused on four canonical tasks: forecasting, imputation, anomaly detection, and classification. While these tasks have driven significant progress, they primarily assess task-specific performance and do not rigorously measure whether a model captures the full generative distribution of the data. We introduce lossless compression as a new paradigm for evaluating time series models, grounded in Shannon's source coding theorem. This perspective establishes a direct equivalence between optimal compression length and the negative log-likelihood, providing a strict and unified information-theoretic criterion for modeling capacity. Then We define a standardized evaluation protocol and metrics. We further propose and open-source a comprehensive evaluation framework TSCom-Bench, which enables the rapid adaptation of time series models as backbones for lossless compression. Experiments across diverse datasets on state-of-the-art models, including TimeXer, iTransformer, and PatchTST, demonstrate that compression reveals distributional weaknesses overlooked by classic benchmarks. These findings position lossless compression as a principled task that complements and extends existing evaluation for time series modeling.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.291198"
    },
    {
        "index": "#90",
        "title": "Rejuvenating Cross-Entropy Loss in Knowledge Distillation for Recommender Systems",
        "link": "/arxiv/2509.20989",
        "arxiv_id": "2509.20989",
        "authors": "Zhangchi Zhu, Wei Zhang",
        "summary": "This paper analyzes Cross-Entropy (CE) loss in knowledge distillation (KD) for recommender systems. KD for recommender systems targets at distilling rankings, especially among items most likely to be preferred, and can only be computed on a small subset of items. Considering these features, we reveal the connection between CE loss and NDCG in the field of KD. We prove that when performing KD on an item subset, minimizing CE loss maximizes the lower bound of NDCG, only if an assumption of closure is satisfied. It requires that the item subset consists of the student's top items. However, this contradicts our goal of distilling rankings of the teacher's top items. We empirically demonstrate the vast gap between these two kinds of top items. To bridge the gap between our goal and theoretical support, we propose Rejuvenated Cross-Entropy for Knowledge Distillation (RCE-KD). It splits the top items given by the teacher into two subsets based on whether they are highly ranked by the student. For the subset that defies the condition, a sampling strategy is devised to use teacher-student collaboration to approximate our assumption of closure. We also combine the losses on the two subsets adaptively. Extensive experiments demonstrate the effectiveness of our method. Our code is available at https://anonymous.4open.science/r/RCE-KD.",
        "subjects": "Information Retrieval, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.297334"
    },
    {
        "index": "#93",
        "title": "FracAug: Fractional Augmentation boost Graph-level Anomaly Detection under Limited Supervision",
        "link": "/arxiv/2509.20978",
        "arxiv_id": "2509.20978",
        "authors": "Xiangyu Dong, Xingyi Zhang, Sibo Wang",
        "summary": "Graph-level anomaly detection (GAD) is critical in diverse domains such as drug discovery, yet high labeling costs and dataset imbalance hamper the performance of Graph Neural Networks (GNNs). To address these issues, we propose FracAug, an innovative plug-in augmentation framework that enhances GNNs by generating semantically consistent graph variants and pseudo-labeling with mutual verification. Unlike previous heuristic methods, FracAug learns semantics within given graphs and synthesizes fractional variants, guided by a novel weighted distance-aware margin loss. This captures multi-scale topology to generate diverse, semantic-preserving graphs unaffected by data imbalance. Then, FracAug utilizes predictions from both original and augmented graphs to pseudo-label unlabeled data, iteratively expanding the training set. As a model-agnostic module compatible with various GNNs, FracAug demonstrates remarkable universality and efficacy: experiments across 14 GNNs on 12 real-world datasets show consistent gains, boosting average AUROC, AUPRC, and F1-score by up to 5.72%, 7.23%, and 4.18%, respectively.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.298323"
    },
    {
        "index": "#94",
        "title": "Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine",
        "link": "/arxiv/2509.20975",
        "arxiv_id": "2509.20975",
        "authors": "Michael S. Yao, Osbert Bastani, Alma Andersson, Tommaso Biancalani, Aïcha Bentaieb, Claudia Iriondo",
        "summary": "The goal of personalized medicine is to discover a treatment regimen that optimizes a patient's clinical outcome based on their personal genetic and environmental factors. However, candidate treatments cannot be arbitrarily administered to the patient to assess their efficacy; we often instead have access to an in silico surrogate model that approximates the true fitness of a proposed treatment. Unfortunately, such surrogate models have been shown to fail to generalize to previously unseen patient-treatment combinations. We hypothesize that domain-specific prior knowledge - such as medical textbooks and biomedical knowledge graphs - can provide a meaningful alternative signal of the fitness of proposed treatments. To this end, we introduce LLM-based Entropy-guided Optimization with kNowledgeable priors (LEON), a mathematically principled approach to leverage large language models (LLMs) as black-box optimizers without any task-specific fine-tuning, taking advantage of their ability to contextualize unstructured domain knowledge to propose personalized treatment plans in natural language. In practice, we implement LEON via 'optimization by prompting,' which uses LLMs as stochastic engines for proposing treatment designs. Experiments on real-world optimization tasks show LEON outperforms both traditional and LLM-based methods in proposing individualized treatments for patients.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.298537"
    },
    {
        "index": "#95",
        "title": "Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis",
        "link": "/arxiv/2509.20972",
        "arxiv_id": "2509.20972",
        "authors": "Ibrahim Altan, Abdulla Bachir, Yousuf Parbhulkar, Abdul Muksith Rizvi, Moshiur Farazi",
        "summary": "Phishing emails pose a persistent and increasingly sophisticated threat, undermining email security through deceptive tactics designed to exploit both semantic and structural vulnerabilities. Traditional detection methods, often based on isolated analysis of email content or embedded URLs, fail to comprehensively address these evolving attacks. In this paper, we propose a dual-path phishing detection framework that integrates transformer-based natural language processing (NLP) with classical machine learning to jointly analyze email text and embedded URLs. Our approach leverages the complementary strengths of semantic analysis using fine-tuned transformer architectures (e.g., DistilBERT) and structural link analysis via character-level TF-IDF vectorization paired with classical classifiers (e.g., Random Forest). Empirical evaluation on representative email and URL datasets demonstrates that this combined approach significantly improves detection accuracy. Specifically, the DistilBERT model achieves a near-optimal balance between accuracy and computational efficiency for textual phishing detection, while Random Forest notably outperforms other classical classifiers in identifying malicious URLs. The modular design allows flexibility for standalone deployment or ensemble integration, facilitating real-world adoption. Collectively, our results highlight the efficacy and practical value of this dual-path approach, establishing a scalable, accurate, and interpretable solution capable of enhancing email security against contemporary phishing threats.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.298734"
    },
    {
        "index": "#96",
        "title": "i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents",
        "link": "/arxiv/2509.20971",
        "arxiv_id": "2509.20971",
        "authors": "Anupam Purwar, Aditya Choudhary",
        "summary": "We experiment with a low-latency, end-to-end voice-to-voice communication model to optimize it for real-time conversational applications. By analyzing components essential to voice to voice (V-2-V) system viz. automatic speech recognition (ASR), text-to-speech (TTS), and dialog management, our work analyzes how to reduce processing time while maintaining high-quality interactions to identify the levers for optimizing V-2-V system. Our work identifies that TTS component which generates life-like voice, full of emotions including natural pauses and exclamations has highest impact on Real time factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the capability to understand tone as well as context of conversation by ingesting both audio and text of prior exchanges to generate contextually accurate speech. We explored optimization of Residual Vector Quantization (RVQ) iterations by the TTS decoder which come at a cost of decrease in the quality of voice generated. Our experimental evaluations also demonstrate that for V-2-V implementations based on CSM most important optimizations can be brought by reducing the number of RVQ Iterations along with the codebooks used in Mimi.",
        "subjects": "Sound, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.298993"
    },
    {
        "index": "#98",
        "title": "Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy",
        "link": "/arxiv/2509.20952",
        "arxiv_id": "2509.20952",
        "authors": "Weili Zeng, Yichao Yan",
        "summary": "Flow matching has recently emerged as a powerful alternative to diffusion models, providing a continuous-time formulation for generative modeling and representation learning. Yet, we show that this framework suffers from a fundamental instability in the low-noise regime. As noise levels approach zero, arbitrarily small perturbations in the input can induce large variations in the velocity target, causing the condition number of the learning problem to diverge. This ill-conditioning not only slows optimization but also forces the encoder to reallocate its limited Jacobian capacity toward noise directions, thereby degrading semantic representations. We provide the first theoretical analysis of this phenomenon, which we term the low-noise pathology, establishing its intrinsic link to the structure of the flow matching objective. Building on these insights, we propose Local Contrastive Flow (LCF), a hybrid training protocol that replaces direct velocity regression with contrastive feature alignment at small noise levels, while retaining standard flow matching at moderate and high noise. Empirically, LCF not only improves convergence speed but also stabilizes representation quality. Our findings highlight the critical importance of addressing low-noise pathologies to unlock the full potential of flow matching for both generation and representation learning.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.299388"
    },
    {
        "index": "#99",
        "title": "CTI Dataset Construction from Telegram",
        "link": "/arxiv/2509.20943",
        "arxiv_id": "2509.20943",
        "authors": "Dincy R. Arikkat, Sneha B. T., Serena Nicolazzo, Antonino Nocera, Vinod P., Rafidha Rehiman K. A., Karthika R",
        "summary": "Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect, and mitigate evolving cyber threats. Its effectiveness depends on high-quality datasets, which support model development, training, evaluation, and benchmarking. Building such datasets is crucial, as attack vectors and adversary tactics continually evolve. Recently, Telegram has gained prominence as a valuable CTI source, offering timely and diverse threat-related information that can help address these challenges. In this work, we address these challenges by presenting an end-to-end automated pipeline that systematically collects and filters threat-related content from Telegram. The pipeline identifies relevant Telegram channels and scrapes 145,349 messages from 12 curated channels out of 150 identified sources. To accurately filter threat intelligence messages from generic content, we employ a BERT-based classifier, achieving an accuracy of 96.64%. From the filtered messages, we compile a dataset of 86,509 malicious Indicators of Compromise, including domains, IPs, URLs, hashes, and CVEs. This approach not only produces a large-scale, high-fidelity CTI dataset but also establishes a foundation for future research and operational applications in cyber threat detection.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Emerging Technologies",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.299602"
    },
    {
        "index": "#100",
        "title": "Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales",
        "link": "/arxiv/2509.20913",
        "arxiv_id": "2509.20913",
        "authors": "Ariadna Albors Zumel, Michele Tizzoni, Gian Maria Campedelli",
        "summary": "Objectives: To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features, alongside historical crime and sociodemographic data, enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. Methods: We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities (i.e., Baltimore, Chicago, Los Angeles, and Philadelphia). We employ crime incident data obtained from each city's police department, combined with sociodemographic data from the American Community Survey and human mobility data from Advan, collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles (0.2 sq. kms) and used to train our deep learning forecasting model, a Convolutional Long Short-Term Memory (ConvLSTM) network, which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models: logistic regression, random forest, and standard LSTM. Results: Incorporating mobility features improves predictive performance, especially when using shorter input sequences. Noteworthy, however, the best results are obtained when both mobility and sociodemographic features are used together, with our deep learning model achieving the highest recall, precision, and F1 score in all four cities, outperforming alternative methods. With this configuration, longer input sequences enhance predictions for violent crimes, while shorter sequences are more effective for property crimes. Conclusion: These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting, mobility included. They also highlight the advantages (and limits) of deep learning when dealing with fine-grained spatial and temporal scales.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.299781"
    },
    {
        "index": "#102",
        "title": "Improving Early Sepsis Onset Prediction Through Federated Learning",
        "link": "/arxiv/2509.20885",
        "arxiv_id": "2509.20885",
        "authors": "Christoph Düsing, Philipp Cimiano",
        "summary": "Early and accurate prediction of sepsis onset remains a major challenge in intensive care, where timely detection and subsequent intervention can significantly improve patient outcomes. While machine learning models have shown promise in this domain, their success is often limited by the amount and diversity of training data available to individual hospitals and Intensive Care Units (ICUs). Federated Learning (FL) addresses this issue by enabling collaborative model training across institutions without requiring data sharing, thus preserving patient privacy. In this work, we propose a federated, attention-enhanced Long Short-Term Memory model for sepsis onset prediction, trained on multi-centric ICU data. Unlike existing approaches that rely on fixed prediction windows, our model supports variable prediction horizons, enabling both short- and long-term forecasting in a single unified model. During analysis, we put particular emphasis on the improvements through our approach in terms of early sepsis detection, i.e., predictions with large prediction windows by conducting an in-depth temporal analysis. Our results prove that using FL does not merely improve overall prediction performance (with performance approaching that of a centralized model), but is particularly beneficial for early sepsis onset prediction. Finally, we show that our choice of employing a variable prediction window rather than a fixed window does not hurt performance significantly but reduces computational, communicational, and organizational overhead.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.300135"
    },
    {
        "index": "#106",
        "title": "Model-Based Reinforcement Learning under Random Observation Delays",
        "link": "/arxiv/2509.20869",
        "arxiv_id": "2509.20869",
        "authors": "Armin Karamzade, Kyungmin Kim, JB Lanier, Davide Corsi, Roy Fox",
        "summary": "Delays frequently occur in real-world environments, yet standard reinforcement learning (RL) algorithms often assume instantaneous perception of the environment. We study random sensor delays in POMDPs, where observations may arrive out-of-sequence, a setting that has not been previously addressed in RL. We analyze the structure of such delays and demonstrate that naive approaches, such as stacking past observations, are insufficient for reliable performance. To address this, we propose a model-based filtering process that sequentially updates the belief state based on an incoming stream of observations. We then introduce a simple delay-aware framework that incorporates this idea into model-based RL, enabling agents to effectively handle random delays. Applying this framework to Dreamer, we compare our approach to delay-aware baselines developed for MDPs. Our method consistently outperforms these baselines and demonstrates robustness to delay distribution shifts during deployment. Additionally, we present experiments on simulated robotic tasks, comparing our method to common practical heuristics and emphasizing the importance of explicitly modeling observation delays.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.300993"
    },
    {
        "index": "#108",
        "title": "Federated Markov Imputation: Privacy-Preserving Temporal Imputation in Multi-Centric ICU Environments",
        "link": "/arxiv/2509.20867",
        "arxiv_id": "2509.20867",
        "authors": "Christoph Düsing, Philipp Cimiano",
        "summary": "Missing data is a persistent challenge in federated learning on electronic health records, particularly when institutions collect time-series data at varying temporal granularities. To address this, we propose Federated Markov Imputation (FMI), a privacy-preserving method that enables Intensive Care Units (ICUs) to collaboratively build global transition models for temporal imputation. We evaluate FMI on a real-world sepsis onset prediction task using the MIMIC-IV dataset and show that it outperforms local imputation baselines, especially in scenarios with irregular sampling intervals across ICUs.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.301361"
    },
    {
        "index": "#111",
        "title": "Robust Multi-Omics Integration from Incomplete Modalities Significantly Improves Prediction of Alzheimer's Disease",
        "link": "/arxiv/2509.20842",
        "arxiv_id": "2509.20842",
        "authors": "Sungjoon Park, Kyungwook Lee, Soorin Yim, Doyeong Hwang, Dongyun Kim, Soonyoung Lee, Amy Dunn, Daniel Gatti, Elissa Chesler, Kristen O'Connell, Kiyoung Kim",
        "summary": "Multi-omics data capture complex biomolecular interactions and provide insights into metabolism and disease. However, missing modalities hinder integrative analysis across heterogeneous omics. To address this, we present MOIRA (Multi-Omics Integration with Robustness to Absent modalities), an early integration method enabling robust learning from incomplete omics data via representation alignment and adaptive aggregation. MOIRA leverages all samples, including those with missing modalities, by projecting each omics dataset onto a shared embedding space where a learnable weighting mechanism fuses them. Evaluated on the Religious Order Study and Memory and Aging Project (ROSMAP) dataset for Alzheimer's Disease (AD), MOIRA outperformed existing approaches, and further ablation studies confirmed modality-wise contributions. Feature importance analysis revealed AD-related biomarkers consistent with prior literature, highlighting the biological relevance of our approach.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.307314"
    },
    {
        "index": "#112",
        "title": "ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation",
        "link": "/arxiv/2509.20841",
        "arxiv_id": "2509.20841",
        "authors": "Dekun Lu, Wei Gao, Kui Jia",
        "summary": "End-to-end robot manipulation policies offer significant potential for enabling embodied agents to understand and interact with the world. Unlike traditional modular pipelines, end-to-end learning mitigates key limitations such as information loss between modules and feature misalignment caused by isolated optimization targets. Despite these advantages, existing end-to-end neural networks for robotic manipulation--including those based on large VLM/VLA models--remain insufficiently performant for large-scale practical deployment. In this paper, we take a step towards an end-to-end manipulation policy that is generalizable, accurate and reliable. To achieve this goal, we propose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for robotic manipulation. Our formulation is used as the action representation of a neural policy, which can be trained in an end-to-end fashion. Such an action representation is general, as it extends the standard end-effector pose action representation and supports a diverse set of manipulation tasks in a unified manner. The oriented keypoint in our method enables natural generalization to objects with different shapes and sizes, while achieving sub-centimeter accuracy. Moreover, our formulation can easily handle multi-stage tasks, multi-modal robot behaviors, and deformable objects. Extensive simulated and hardware experiments demonstrate the effectiveness of our method.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.307514"
    },
    {
        "index": "#114",
        "title": "Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks",
        "link": "/arxiv/2509.20835",
        "arxiv_id": "2509.20835",
        "authors": "Yu Liu, Boxiang He, Fanggang Wang",
        "summary": "This paper proposes a novel and flexible security-aware semantic-driven integrated sensing and communication (ISAC) framework, namely security semantic ISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a pair of pluggable encryption and decryption modules is designed in the proposed SS-ISAC framework. The encryption module is installed after the semantic transmitter, adopting a trainable adversarial residual network (ARN) to create the adversarial attack. Correspondingly, the decryption module before the semantic receiver utilizes another trainable ARN to mitigate the adversarial attack and noise. These two modules can be flexibly assembled considering the system security demands, without drastically modifying the hardware infrastructure. To ensure the sensing and communication (SAC) performance while preventing the eavesdropping threat, the above ARNs are jointly optimized by minimizing a carefully designed loss function that relates to the adversarial attack power, SAC performance, as well as the privacy leakage risk. Simulation results validate the effectiveness of the proposed SS-ISAC framework in terms of both SAC and eavesdropping prevention performance.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.307973"
    },
    {
        "index": "#115",
        "title": "Trustworthy Semantic Communication for Vehicular Networks: Challenges and Solutions",
        "link": "/arxiv/2509.20830",
        "arxiv_id": "2509.20830",
        "authors": "Yanghe Pan, Yuntao Wang, Shaolong Guo, Chengyu Yin, Ruidong Li, Zhou Su, Yuan Wu",
        "summary": "Semantic communication (SemCom) has the potential to significantly reduce communication delay in vehicle-to-everything (V2X) communications within vehicular networks (VNs). However, the deployment of vehicular SemCom networks (VN-SemComNets) faces critical trust challenges in information transmission, semantic encoding, and communication entity reliability. This paper proposes an innovative three-layer trustworthy VN-SemComNet architecture. Specifically, we introduce a semantic camouflage transmission mechanism leveraging defensive adversarial noise for active eavesdropping defense, a robust federated encoder-decoder training framework to mitigate encoder-decoder poisoning attacks, and an audit game-based distributed vehicle trust management mechanism to deter untrustworthy vehicles. A case study validates the effectiveness of the proposed solutions. Lastly, essential future research directions are pointed out to advance this emerging field.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.308202"
    },
    {
        "index": "#117",
        "title": "Even More Kawaii than Real-Person-Driven VTubers? Understanding How Viewers Perceive AI-Driven VTubers",
        "link": "/arxiv/2509.20817",
        "arxiv_id": "2509.20817",
        "authors": "Yiluo Wei, Yupeng He, Gareth Tyson",
        "summary": "VTubers, digital personas represented by animated avatars, have gained massive popularity. Traditionally, VTubers are operated and voiced by human controllers known as Nakanohito. The reliance on Nakanohito, however, poses risks due to potential personal controversies and operational disruptions. The emergence of AI-driven VTubers offers a new model free from these human constraints. While AI-driven VTubers present benefits such as continuous operation and reduced scandal risk, they also raise questions about authenticity and audience engagement. Therefore, to gain deeper insights, we conduct a case study, investigating viewer perceptions of Neuro-sama, the most popular AI-driven VTuber with 845k followers on Twitch and 753k followers on YouTube. We analyze 108k Reddit posts and 136k YouTube comments, aiming to better understand viewer motivations, how AI constructs the virtual persona, and perceptions of the AI as Nakanohito. Our findings enhance the understanding of AI-driven VTubers and their impact on digital streaming culture.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence, Computers and Society",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.308611"
    },
    {
        "index": "#122",
        "title": "IConv: Focusing on Local Variation with Channel Independent Convolution for Multivariate Time Series Forecasting",
        "link": "/arxiv/2509.20783",
        "arxiv_id": "2509.20783",
        "authors": "Gawon Lee, Hanbyeol Park, Minseop Kim, Dohee Kim, Hyerim Bae",
        "summary": "Real-world time-series data often exhibit non-stationarity, including changing trends, irregular seasonality, and residuals. In terms of changing trends, recently proposed multi-layer perceptron (MLP)-based models have shown excellent performance owing to their computational efficiency and ability to capture long-term dependency. However, the linear nature of MLP architectures poses limitations when applied to channels with diverse distributions, resulting in local variations such as seasonal patterns and residual components being ignored. However, convolutional neural networks (CNNs) can effectively incorporate these variations. To resolve the limitations of MLP, we propose combining them with CNNs. The overall trend is modeled using an MLP to consider long-term dependencies. The CNN uses diverse kernels to model fine-grained local patterns in conjunction with MLP trend predictions. To focus on modeling local variation, we propose IConv, a novel convolutional architecture that processes the temporal dependency channel independently and considers the inter-channel relationship through distinct layers. Independent channel processing enables the modeling of diverse local temporal dependencies and the adoption of a large kernel size. Distinct inter-channel considerations reduce computational cost. The proposed model is evaluated through extensive experiments on time-series datasets. The results reveal the superiority of the proposed method for multivariate time-series forecasting.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.309635"
    },
    {
        "index": "#125",
        "title": "Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis",
        "link": "/arxiv/2509.20768",
        "arxiv_id": "2509.20768",
        "authors": "Maria F. Davila R, Azizjon Turaev, Wolfram Wingerath",
        "summary": "Synthetic tabular data is used for privacy-preserving data sharing and data-driven model development. Its effectiveness, however, depends heavily on the used Tabular Data Synthesis (TDS) tool. Recent studies have shown that Transformer-based models outperform other state-of-the-art models such as Generative Adversarial Networks (GANs) and Diffusion models in terms of data quality. However, Transformer-based models also come with high computational costs, making them sometimes unfeasible for end users with prosumer hardware. This study presents a sensitivity assessment on how the choice of hyperparameters, such as number of layers or hidden dimension affects the quality of the resultant synthetic data and the computational performance. It is performed across two tools, GReaT and REaLTabFormer, evaluating 10 model setups that vary in architecture type and depth. We assess the sensitivity on three dimensions: runtime, machine learning (ML) utility, and similarity to real data distributions. Experiments were conducted on four real-world datasets. Our findings reveal that runtime is proportional to the number of hyperparameters, with shallower configurations completing faster. GReaT consistently achieves lower runtimes than REaLTabFormer, and only on the largest dataset they have comparable runtime. For small datasets, both tools achieve synthetic data with high utility and optimal similarity, but on larger datasets only REaLTabFormer sustains strong utility and similarity. As a result, REaLTabFormer with lightweight LLMs provides the best balance, since it preserves data quality while reducing computational requirements. Nonetheless, its runtime remains higher than that of GReaT and other TDS tools, suggesting that efficiency gains are possible but only up to a certain level.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.310211"
    },
    {
        "index": "#129",
        "title": "Imagining Design Workflows in Agentic AI Futures",
        "link": "/arxiv/2509.20731",
        "arxiv_id": "2509.20731",
        "authors": "Samangi Wadinambiarachchi, Jenny Waycott, Yvonne Rogers, Greg Wadley",
        "summary": "As designers become familiar with Generative AI, a new concept is emerging: Agentic AI. While generative AI produces output in response to prompts, agentic AI systems promise to perform mundane tasks autonomously, potentially freeing designers to focus on what they love: being creative. But how do designers feel about integrating agentic AI systems into their workflows? Through design fiction, we investigated how designers want to interact with a collaborative agentic AI platform. Ten professional designers imagined and discussed collaborating with an AI agent to organise inspiration sources and ideate. Our findings highlight the roles AI agents can play in supporting designers, the division of authority between humans and AI, and how designers' intent can be explained to AI agents beyond prompts. We synthesise our findings into a conceptual framework that identifies authority distribution among humans and AI agents and discuss directions for utilising AI agents in future design workflows.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.310956"
    },
    {
        "index": "#130",
        "title": "RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking",
        "link": "/arxiv/2509.20717",
        "arxiv_id": "2509.20717",
        "authors": "Zhenguo Sun, Yibo Peng, Yuan Meng, Xukun Li, Bo-Sheng Huang, Zhenshan Bing, Xinlong Wang, Alois Knoll",
        "summary": "Long-horizon, high-dynamic motion tracking on humanoids remains brittle because absolute joint commands cannot compensate model-plant mismatch, leading to error accumulation. We propose RobotDancing, a simple, scalable framework that predicts residual joint targets to explicitly correct dynamics discrepancies. The pipeline is end-to-end--training, sim-to-sim validation, and zero-shot sim-to-real--and uses a single-stage reinforcement learning (RL) setup with a unified observation, reward, and hyperparameter configuration. We evaluate primarily on Unitree G1 with retargeted LAFAN1 dance sequences and validate transfer on H1/H1-2. RobotDancing can track multi-minute, high-energy behaviors (jumps, spins, cartwheels) and deploys zero-shot to hardware with high motion tracking quality.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.311216"
    },
    {
        "index": "#133",
        "title": "Incorporating LLM Embeddings for Variation Across the Human Genome",
        "link": "/arxiv/2509.20702",
        "arxiv_id": "2509.20702",
        "authors": "Hongqian Niu, Jordan Bryan, Xihao Li, Didong Li",
        "summary": "Recent advances in large language model (LLM) embeddings have enabled powerful representations for biological data, but most applications to date focus only on gene-level information. We present one of the first systematic frameworks to generate variant-level embeddings across the entire human genome. Using curated annotations from FAVOR, ClinVar, and the GWAS Catalog, we constructed semantic text descriptions for 8.9 billion possible variants and generated embeddings at three scales: 1.5 million HapMap3+MEGA variants, ~90 million imputed UK Biobank variants, and ~9 billion all possible variants. Embeddings were produced with both OpenAI's text-embedding-3-large and the open-source Qwen3-Embedding-0.6B models. Baseline experiments demonstrate high predictive accuracy for variant properties, validating the embeddings as structured representations of genomic variation. We outline two downstream applications: embedding-informed hypothesis testing by extending the Frequentist And Bayesian framework to genome-wide association studies, and embedding-augmented genetic risk prediction that enhances standard polygenic risk scores. These resources, publicly available on Hugging Face, provide a foundation for advancing large-scale genomic discovery and precision medicine.",
        "subjects": "Applications, Artificial Intelligence, Genomics",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.317028"
    },
    {
        "index": "#134",
        "title": "Learning to Align Molecules and Proteins: A Geometry-Aware Approach to Binding Affinity",
        "link": "/arxiv/2509.20693",
        "arxiv_id": "2509.20693",
        "authors": "Mohammadsaleh Refahi, Bahrad A. Sokhansanj, James R. Brown, Gail Rosen",
        "summary": "Accurate prediction of drug-target binding affinity can accelerate drug discovery by prioritizing promising compounds before costly wet-lab screening. While deep learning has advanced this task, most models fuse ligand and protein representations via simple concatenation and lack explicit geometric regularization, resulting in poor generalization across chemical space and time. We introduce FIRM-DTI, a lightweight framework that conditions molecular embeddings on protein embeddings through a feature-wise linear modulation (FiLM) layer and enforces metric structure with a triplet loss. An RBF regression head operating on embedding distances yields smooth, interpretable affinity predictions. Despite its modest size, FIRM-DTI achieves state-of-the-art performance on the Therapeutics Data Commons DTI-DG benchmark, as demonstrated by an extensive ablation study and out-of-domain evaluation. Our results underscore the value of conditioning and metric learning for robust drug-target affinity prediction.",
        "subjects": "Machine Learning, Artificial Intelligence, Molecular Networks",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.317308"
    },
    {
        "index": "#135",
        "title": "Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection",
        "link": "/arxiv/2509.20682",
        "arxiv_id": "2509.20682",
        "authors": "Duc-Tuan Truong, Tianchi Liu, Junjie Li, Ruijie Tao, Kong Aik Lee, Eng Siong Chng",
        "summary": "In speech deepfake detection (SDD), data augmentation (DA) is commonly used to improve model generalization across varied speech conditions and spoofing attacks. However, during training, the backpropagated gradients from original and augmented inputs may misalign, which can result in conflicting parameter updates. These conflicts could hinder convergence and push the model toward suboptimal solutions, thereby reducing the benefits of DA. To investigate and address this issue, we design a dual-path data-augmented (DPDA) training framework with gradient alignment for SDD. In our framework, each training utterance is processed through two input paths: one using the original speech and the other with its augmented version. This design allows us to compare and align their backpropagated gradient directions to reduce optimization conflicts. Our analysis shows that approximately 25% of training iterations exhibit gradient conflicts between the original inputs and their augmented counterparts when using RawBoost augmentation. By resolving these conflicts with gradient alignment, our method accelerates convergence by reducing the number of training epochs and achieves up to an 18.69% relative reduction in Equal Error Rate on the In-the-Wild dataset compared to the baseline.",
        "subjects": "Sound, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.317528"
    },
    {
        "index": "#137",
        "title": "QAMO: Quality-aware Multi-centroid One-class Learning For Speech Deepfake Detection",
        "link": "/arxiv/2509.20679",
        "arxiv_id": "2509.20679",
        "authors": "Duc-Tuan Truong, Tianchi Liu, Ruijie Tao, Junjie Li, Kong Aik Lee, Eng Siong Chng",
        "summary": "Recent work shows that one-class learning can detect unseen deepfake attacks by modeling a compact distribution of bona fide speech around a single centroid. However, the single-centroid assumption can oversimplify the bona fide speech representation and overlook useful cues, such as speech quality, which reflects the naturalness of the speech. Speech quality can be easily obtained using existing speech quality assessment models that estimate it through Mean Opinion Score. In this paper, we propose QAMO: Quality-Aware Multi-Centroid One-Class Learning for speech deepfake detection. QAMO extends conventional one-class learning by introducing multiple quality-aware centroids. In QAMO, each centroid is optimized to represent a distinct speech quality subspaces, enabling better modeling of intra-class variability in bona fide speech. In addition, QAMO supports a multi-centroid ensemble scoring strategy, which improves decision thresholding and reduces the need for quality labels during inference. With two centroids to represent high- and low-quality speech, our proposed QAMO achieves an equal error rate of 5.09% in In-the-Wild dataset, outperforming previous one-class and quality-aware systems.",
        "subjects": "Sound, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.318023"
    },
    {
        "index": "#139",
        "title": "Understanding Mode Switching in Human-AI Collaboration: Behavioral Insights and Predictive Modeling",
        "link": "/arxiv/2509.20666",
        "arxiv_id": "2509.20666",
        "authors": "Avinash Ajit Nargund, Arthur Caetano, Kevin Yang, Rose Yiwei Liu, Philip Tezaur, Kriteen Shrestha, Qisen Pan, Tobias Höllerer, Misha Sra",
        "summary": "Human-AI collaboration is typically offered in one of two of user control levels: guidance, where the AI provides suggestions and the human makes the final decision, and delegation, where the AI acts autonomously within user-defined constraints. Systems that integrate both modes, common in robotic surgery or driving assistance, often overlook shifts in user preferences within a task in response to factors like evolving trust, decision complexity, and perceived control. In this work, we investigate how users dynamically switch between higher and lower levels of control during a sequential decision-making task. Using a hand-and-brain chess setup, participants either selected a piece and the AI decided how it moved (brain mode), or the AI selected a piece and the participant decided how it moved (hand mode). We collected over 400 mode-switching decisions from eight participants, along with gaze, emotional state, and subtask difficulty data. Statistical analysis revealed significant differences in gaze patterns and subtask complexity prior to a switch and in the quality of the subsequent move. Based on these results, we engineered behavioral and task-specific features to train a lightweight model that predicted control level switches ($F1 = 0.65$). The model performance suggests that real-time behavioral signals can serve as a complementary input alongside system-driven mode-switching mechanisms currently used. We complement our quantitative results with qualitative factors that influence switching including perceived AI ability, decision complexity, and level of control, identified from post-game interview analysis. The combined behavioral and modeling insights can help inform the design of shared autonomy systems that need dynamic, subtask-level control switches aligned with user intent and evolving task demands.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.318465"
    },
    {
        "index": "#141",
        "title": "A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks",
        "link": "/arxiv/2509.20639",
        "arxiv_id": "2509.20639",
        "authors": "Adam Swanda, Amy Chang, Alexander Chen, Fraser Burch, Paul Kassianik, Konstantin Berlin",
        "summary": "The widespread adoption of Large Language Models (LLMs) has revolutionized AI deployment, enabling autonomous and semi-autonomous applications across industries through intuitive language interfaces and continuous improvements in model development. However, the attendant increase in autonomy and expansion of access permissions among AI applications also make these systems compelling targets for malicious attacks. Their inherent susceptibility to security flaws necessitates robust defenses, yet no known approaches can prevent zero-day or novel attacks against LLMs. This places AI protection systems in a category similar to established malware protection systems: rather than providing guaranteed immunity, they minimize risk through enhanced observability, multi-layered defense, and rapid threat response, supported by a threat intelligence function designed specifically for AI-related threats. Prior work on LLM protection has largely evaluated individual detection models rather than end-to-end systems designed for continuous, rapid adaptation to a changing threat landscape. We present a production-grade defense system rooted in established malware detection and threat intelligence practices. Our platform integrates three components: a threat intelligence system that turns emerging threats into protections; a data platform that aggregates and enriches information while providing observability, monitoring, and ML operations; and a release platform enabling safe, rapid detection updates without disrupting customer workflows. Together, these components deliver layered protection against evolving LLM threats while generating training data for continuous model improvement and deploying updates without interrupting production.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.318851"
    },
    {
        "index": "#142",
        "title": "Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments",
        "link": "/arxiv/2509.20635",
        "arxiv_id": "2509.20635",
        "authors": "Matheus P. Angarola, Francisco Affonso, Marcelo Becker",
        "summary": "Legged robots must exhibit robust and agile locomotion across diverse, unstructured terrains, a challenge exacerbated under blind locomotion settings where terrain information is unavailable. This work introduces a hierarchical reinforcement learning framework that leverages terrain-specialized policies and curriculum learning to enhance agility and tracking performance in complex environments. We validated our method on simulation, where our approach outperforms a generalist policy by up to 16% in success rate and achieves lower tracking errors as the velocity target increases, particularly on low-friction and discontinuous terrains, demonstrating superior adaptability and robustness across mixed-terrain scenarios.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.319031"
    },
    {
        "index": "#143",
        "title": "Recidivism and Peer Influence with LLM Text Embeddings in Low Security Correctional Facilities",
        "link": "/arxiv/2509.20634",
        "arxiv_id": "2509.20634",
        "authors": "Shanjukta Nath, Jiwon Hong, Jae Ho Chang, Keith Warren, Subhadeep Paul",
        "summary": "We find AI embeddings obtained using a pre-trained transformer-based Large Language Model (LLM) of 80,000-120,000 written affirmations and correction exchanges among residents in low-security correctional facilities to be highly predictive of recidivism. The prediction accuracy is 30\\% higher with embedding vectors than with only pre-entry covariates. However, since the text embedding vectors are high-dimensional, we perform Zero-Shot classification of these texts to a low-dimensional vector of user-defined classes to aid interpretation while retaining the predictive power. To shed light on the social dynamics inside the correctional facilities, we estimate peer effects in these LLM-generated numerical representations of language with a multivariate peer effect model, adjusting for network endogeneity. We develop new methodology and theory for peer effect estimation that accommodate sparse networks, multivariate latent variables, and correlated multivariate outcomes. With these new methods, we find significant peer effects in language usage for interaction and feedback.",
        "subjects": "Econometrics, Artificial Intelligence, General Economics, Methodology",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.319288"
    },
    {
        "index": "#144",
        "title": "Personalized Federated Dictionary Learning for Modeling Heterogeneity in Multi-site fMRI Data",
        "link": "/arxiv/2509.20627",
        "arxiv_id": "2509.20627",
        "authors": "Yipu Zhang, Chengshuo Zhang, Ziyu Zhou, Gang Qu, Hao Zheng, Yuping Wang, Hui Shen, Hongwen Deng",
        "summary": "Data privacy constraints pose significant challenges for large-scale neuroimaging analysis, especially in multi-site functional magnetic resonance imaging (fMRI) studies, where site-specific heterogeneity leads to non-independent and identically distributed (non-IID) data. These factors hinder the development of generalizable models. To address these challenges, we propose Personalized Federated Dictionary Learning (PFedDL), a novel federated learning framework that enables collaborative modeling across sites without sharing raw data. PFedDL performs independent dictionary learning at each site, decomposing each site-specific dictionary into a shared global component and a personalized local component. The global atoms are updated via federated aggregation to promote cross-site consistency, while the local atoms are refined independently to capture site-specific variability, thereby enhancing downstream analysis. Experiments on the ABIDE dataset demonstrate that PFedDL outperforms existing methods in accuracy and robustness across non-IID datasets.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.319520"
    },
    {
        "index": "#146",
        "title": "MMG: Mutual Information Estimation via the MMSE Gap in Diffusion",
        "link": "/arxiv/2509.20609",
        "arxiv_id": "2509.20609",
        "authors": "Longxuan Yu, Xing Shi, Xianghao Kong, Tong Jia, Greg Ver Steeg",
        "summary": "Mutual information (MI) is one of the most general ways to measure relationships between random variables, but estimating this quantity for complex systems is challenging. Denoising diffusion models have recently set a new bar for density estimation, so it is natural to consider whether these methods could also be used to improve MI estimation. Using the recently introduced information-theoretic formulation of denoising diffusion models, we show the diffusion models can be used in a straightforward way to estimate MI. In particular, the MI corresponds to half the gap in the Minimum Mean Square Error (MMSE) between conditional and unconditional diffusion, integrated over all Signal-to-Noise-Ratios (SNRs) in the noising process. Our approach not only passes self-consistency tests but also outperforms traditional and score-based diffusion MI estimators. Furthermore, our method leverages adaptive importance sampling to achieve scalable MI estimation, while maintaining strong performance even when the MI is high.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.319927"
    },
    {
        "index": "#147",
        "title": "Experience Deploying Containerized GenAI Services at an HPC Center",
        "link": "/arxiv/2509.20603",
        "arxiv_id": "2509.20603",
        "authors": "Angel M. Beltre, Jeff Ogden, Kevin Pedretti",
        "summary": "Generative Artificial Intelligence (GenAI) applications are built from specialized components -- inference servers, object storage, vector and graph databases, and user interfaces -- interconnected via web-based APIs. While these components are often containerized and deployed in cloud environments, such capabilities are still emerging at High-Performance Computing (HPC) centers. In this paper, we share our experience deploying GenAI workloads within an established HPC center, discussing the integration of HPC and cloud computing environments. We describe our converged computing architecture that integrates HPC and Kubernetes platforms running containerized GenAI workloads, helping with reproducibility. A case study illustrates the deployment of the Llama Large Language Model (LLM) using a containerized inference server (vLLM) across both Kubernetes and HPC platforms using multiple container runtimes. Our experience highlights practical considerations and opportunities for the HPC container community, guiding future research and tool development.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Artificial Intelligence, Hardware Architecture, Emerging Technologies, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.320127"
    },
    {
        "index": "#148",
        "title": "An LLM-based Agentic Framework for Accessible Network Control",
        "link": "/arxiv/2509.20600",
        "arxiv_id": "2509.20600",
        "authors": "Samuel Lin, Jiawei Zhou, Minlan Yu",
        "summary": "Traditional approaches to network management have been accessible only to a handful of highly-trained network operators with significant expert knowledge. This creates barriers for lay users to easily manage their networks without resorting to experts. With recent development of powerful large language models (LLMs) for language comprehension, we design a system to make network management accessible to a broader audience of non-experts by allowing users to converse with networks in natural language. To effectively leverage advancements in LLMs, we propose an agentic framework that uses an intermediate representation to streamline configuration across diverse vendor equipment, retrieves the network state from memory in real-time, and provides an interface for external feedback. We also conduct pilot studies to collect real user data of natural language utterances for network control, and present a visualization interface to facilitate dialogue-driven user interaction and enable large-scale data collection for future development. Preliminary experiments validate the effectiveness of our proposed system components with LLM integration on both synthetic and real user utterances. Through our data collection and visualization efforts, we pave the way for more effective use of LLMs and democratize network control for everyday users.",
        "subjects": "Networking and Internet Architecture, Artificial Intelligence, Machine Learning",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.320311"
    },
    {
        "index": "#152",
        "title": "MechStyle: Augmenting Generative AI with Mechanical Simulation to Create Stylized and Structurally Viable 3D Models",
        "link": "/arxiv/2509.20571",
        "arxiv_id": "2509.20571",
        "authors": "Faraz Faruqi, Amira Abdel-Rahman, Leandra Tejedor, Martin Nisser, Jiaji Li, Vrushank Phadnis, Varun Jampani, Neil Gershenfeld, Megan Hofmann, Stefanie Mueller",
        "summary": "Recent developments in Generative AI enable creators to stylize 3D models based on text prompts. These methods change the 3D model geometry, which can compromise the model's structural integrity once fabricated. We present MechStyle, a system that enables creators to stylize 3D printable models while preserving their structural integrity. MechStyle accomplishes this by augmenting the Generative AI-based stylization process with feedback from a Finite Element Analysis (FEA) simulation. As the stylization process modifies the geometry to approximate the desired style, feedback from the FEA simulation reduces modifications to regions with increased stress. We evaluate the effectiveness of FEA simulation feedback in the augmented stylization process by comparing three stylization control strategies. We also investigate the time efficiency of our approach by comparing three adaptive scheduling strategies. Finally, we demonstrate MechStyle's user interface that allows users to generate stylized and structurally viable 3D models and provide five example applications.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.321188"
    },
    {
        "index": "#153",
        "title": "PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models",
        "link": "/arxiv/2509.20570",
        "arxiv_id": "2509.20570",
        "authors": "Mingze Yuan, Pengfei Jin, Na Li, Quanzheng Li",
        "summary": "Diffusion models have demonstrated strong generative capabilities across scientific domains, but often produce outputs that violate physical laws. We propose a new perspective by framing physics-informed generation as a sparse reward optimization problem, where adherence to physical constraints is treated as a reward signal. This formulation unifies prior approaches under a reward-based paradigm and reveals a shared bottleneck: reliance on diffusion posterior sampling (DPS)-style value function approximations, which introduce non-negligible errors and lead to training instability and inference inefficiency. To overcome this, we introduce Physics-Informed Reward Fine-tuning (PIRF), a method that bypasses value approximation by computing trajectory-level rewards and backpropagating their gradients directly. However, a naive implementation suffers from low sample efficiency and compromised data fidelity. PIRF mitigates these issues through two key strategies: (1) a layer-wise truncated backpropagation method that leverages the spatiotemporally localized nature of physics-based rewards, and (2) a weight-based regularization scheme that improves efficiency over traditional distillation-based methods. Across five PDE benchmarks, PIRF consistently achieves superior physical enforcement under efficient sampling regimes, highlighting the potential of reward fine-tuning for advancing scientific generative modeling.",
        "subjects": "Machine Learning, Artificial Intelligence, Computational Engineering, Finance, and Science, Systems and Control",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.321439"
    },
    {
        "index": "#156",
        "title": "GraspFactory: A Large Object-Centric Grasping Dataset",
        "link": "/arxiv/2509.20550",
        "arxiv_id": "2509.20550",
        "authors": "Srinidhi Kalgundi Srinivas, Yash Shukla, Adam Arnold, Sachin Chitta",
        "summary": "Robotic grasping is a crucial task in industrial automation, where robots are increasingly expected to handle a wide range of objects. However, a significant challenge arises when robot grasping models trained on limited datasets encounter novel objects. In real-world environments such as warehouses or manufacturing plants, the diversity of objects can be vast, and grasping models need to generalize to this diversity. Training large, generalizable robot-grasping models requires geometrically diverse datasets. In this paper, we introduce GraspFactory, a dataset containing over 109 million 6-DoF grasps collectively for the Franka Panda (with 14,690 objects) and Robotiq 2F-85 grippers (with 33,710 objects). GraspFactory is designed for training data-intensive models, and we demonstrate the generalization capabilities of one such model trained on a subset of GraspFactory in both simulated and real-world settings. The dataset and tools are made available for download at https://graspfactory.github.io/.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.327408"
    },
    {
        "index": "#157",
        "title": "Understanding and Improving Adversarial Robustness of Neural Probabilistic Circuits",
        "link": "/arxiv/2509.20549",
        "arxiv_id": "2509.20549",
        "authors": "Weixin Chen, Han Zhao",
        "summary": "Neural Probabilistic Circuits (NPCs), a new class of concept bottleneck models, comprise an attribute recognition model and a probabilistic circuit for reasoning. By integrating the outputs from these two modules, NPCs produce compositional and interpretable predictions. While offering enhanced interpretability and high performance on downstream tasks, the neural-network-based attribute recognition model remains a black box. This vulnerability allows adversarial attacks to manipulate attribute predictions by introducing carefully crafted subtle perturbations to input images, potentially compromising the final predictions. In this paper, we theoretically analyze the adversarial robustness of NPC and demonstrate that it only depends on the robustness of the attribute recognition model and is independent of the robustness of the probabilistic circuit. Moreover, we propose RNPC, the first robust neural probabilistic circuit against adversarial attacks on the recognition module. RNPC introduces a novel class-wise integration for inference, ensuring a robust combination of outputs from the two modules. Our theoretical analysis demonstrates that RNPC exhibits provably improved adversarial robustness compared to NPC. Empirical results on image classification tasks show that RNPC achieves superior adversarial robustness compared to existing concept bottleneck models while maintaining high accuracy on benign inputs.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.327751"
    },
    {
        "index": "#159",
        "title": "CHOIR: A Chatbot-mediated Organizational Memory Leveraging Communication in University Research Labs",
        "link": "/arxiv/2509.20512",
        "arxiv_id": "2509.20512",
        "authors": "Sangwook Lee, Adnan Abbas, Yan Chen, Young-Ho Kim, Sang Won Lee",
        "summary": "University research labs often rely on chat-based platforms for communication and project management, where valuable knowledge surfaces but is easily lost in message streams. Documentation can preserve knowledge, but it requires ongoing maintenance and is challenging to navigate. Drawing on formative interviews that revealed organizational memory challenges in labs, we designed CHOIR, an LLM-based chatbot that supports organizational memory through four key functions: document-grounded Q&A, Q&A sharing for follow-up discussion, knowledge extraction from conversations, and AI-assisted document updates. We deployed CHOIR in four research labs for one month (n=21), where the lab members asked 107 questions and lab directors updated documents 38 times in the organizational memory. Our findings reveal a privacy-awareness tension: questions were asked privately, limiting directors' visibility into documentation gaps. Students often avoided contribution due to challenges in generalizing personal experiences into universal documentation. We contribute design implications for privacy-preserving awareness and supporting context-specific knowledge documentation.",
        "subjects": "Human-Computer Interaction, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.328359"
    },
    {
        "index": "#160",
        "title": "Complexity-Driven Policy Optimization",
        "link": "/arxiv/2509.20509",
        "arxiv_id": "2509.20509",
        "authors": "Luca Serfilippi, Giorgio Franceschelli, Antonio Corradi, Mirco Musolesi",
        "summary": "Policy gradient methods often balance exploitation and exploration via entropy maximization. However, maximizing entropy pushes the policy towards a uniform random distribution, which represents an unstructured and sometimes inefficient exploration strategy. In this work, we propose replacing the entropy bonus with a more robust complexity bonus. In particular, we adopt a measure of complexity, defined as the product of Shannon entropy and disequilibrium, where the latter quantifies the distance from the uniform distribution. This regularizer encourages policies that balance stochasticity (high entropy) with structure (high disequilibrium), guiding agents toward regimes where useful, non-trivial behaviors can emerge. Such behaviors arise because the regularizer suppresses both extremes, e.g., maximal disorder and complete order, creating pressure for agents to discover structured yet adaptable strategies. Starting from Proximal Policy Optimization (PPO), we introduce Complexity-Driven Policy Optimization (CDPO), a new learning algorithm that replaces entropy with complexity. We show empirically across a range of discrete action space tasks that CDPO is more robust to the choice of the complexity coefficient than PPO is with the entropy coefficient, especially in environments requiring greater exploration.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.328551"
    },
    {
        "index": "#162",
        "title": "Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting",
        "link": "/arxiv/2509.20499",
        "arxiv_id": "2509.20499",
        "authors": "Boqi Li, Siyuan Li, Weiyi Wang, Anran Li, Zhong Cao, Henry X. Liu",
        "summary": "With the rapid progress of foundation models and robotics, vision-language navigation (VLN) has emerged as a key task for embodied agents with broad practical applications. We address VLN in continuous environments, a particularly challenging setting where an agent must jointly interpret natural language instructions, perceive its surroundings, and plan low-level actions. We propose a zero-shot framework that integrates a simplified yet effective waypoint predictor with a multimodal large language model (MLLM). The predictor operates on an abstract obstacle map, producing linearly reachable waypoints, which are incorporated into a dynamically updated topological graph with explicit visitation records. The graph and visitation information are encoded into the prompt, enabling reasoning over both spatial structure and exploration history to encourage exploration and equip MLLM with local path planning for error correction. Extensive experiments on R2R-CE and RxR-CE show that our method achieves state-of-the-art zero-shot performance, with success rates of 41% and 36%, respectively, outperforming prior state-of-the-art methods.",
        "subjects": "Robotics, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.329048"
    },
    {
        "index": "#163",
        "title": "AI-Specific Code Smells: From Specification to Detection",
        "link": "/arxiv/2509.20491",
        "arxiv_id": "2509.20491",
        "authors": "Brahim Mahmoudi, Naouel Moha, Quentin Stievenert, Florent Avellaneda",
        "summary": "The rise of Artificial Intelligence (AI) is reshaping how software systems are developed and maintained. However, AI-based systems give rise to new software issues that existing detection tools often miss. Among these, we focus on AI-specific code smells, recurring patterns in the code that may indicate deeper problems such as unreproducibility, silent failures, or poor model generalization. We introduce SpecDetect4AI, a tool-based approach for the specification and detection of these code smells at scale. This approach combines a high-level declarative Domain-Specific Language (DSL) for rule specification with an extensible static analysis tool that interprets and detects these rules for AI-based systems. We specified 22 AI-specific code smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code), achieving a precision of 88.66% and a recall of 88.89%, outperforming other existing detection tools. Our results show that SpecDetect4AI supports the specification and detection of AI-specific code smells through dedicated rules and can effectively analyze large AI-based systems, demonstrating both efficiency and extensibility (SUS 81.7/100).",
        "subjects": "Software Engineering, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.329258"
    },
    {
        "index": "#164",
        "title": "CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification",
        "link": "/arxiv/2509.20489",
        "arxiv_id": "2509.20489",
        "authors": "D. Darankoum, C. Habermacher, J. Volle, S. Grudinin",
        "summary": "Electroencephalography signals (EEGs) contain rich multi-scale information crucial for understanding brain states, with potential applications in diagnosing and advancing the drug development landscape. However, extracting meaningful features from raw EEG signals while handling noise and channel variability remains a major challenge. This work proposes a novel end-to-end deep-learning framework that addresses these issues through several key innovations. First, we designed an encoder capable of explicitly capturing multi-scale frequency oscillations covering a wide range of features for different EEG-related tasks. Secondly, to model complex dependencies and handle the high temporal resolution of EEGs, we introduced an attention-based encoder that simultaneously learns interactions across EEG channels and within localized {\\em patches} of individual channels. We integrated a dedicated gating network on top of the attention encoder to dynamically filter out noisy and non-informative channels, enhancing the reliability of EEG data. The entire encoding process is guided by a novel loss function, which leverages supervised and contrastive learning, significantly improving model generalization. We validated our approach in multiple applications, ranging from the classification of effects across multiple Central Nervous System (CNS) disorders treatments to the diagnosis of Parkinson's and Alzheimer's disease. Our results demonstrate that the proposed learning paradigm can extract biologically meaningful patterns from raw EEG signals across different species, autonomously select high-quality channels, and achieve robust generalization through innovative architectural and loss design.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.329452"
    },
    {
        "index": "#166",
        "title": "Wartime Media Dynamics in Emerging Democracies: Case Study of Pakistani Media in May 2025 Indo-Pak Conflict",
        "link": "/arxiv/2509.20419",
        "arxiv_id": "2509.20419",
        "authors": "Taaha Saleem Bajwa",
        "summary": "Democracies rely on opposition and dissent to function, but in emerging democracies, freedom of speech is often restricted. This effect intensifies during regional conflicts. This study examines how the India-Pakistan conflict of May 2025 influenced Pakistani media coverage. Analyzing approximately 2,600 news articles from three major newspapers using a large language model (LLM), the study found that war-related reporting significantly overshadowed coverage of political opposition and dissent. These findings highlight how conflict can marginalize democratic discourse, reinforcing the need to safeguard press freedom in volatile regions.",
        "subjects": "Computers and Society, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.329823"
    },
    {
        "index": "#167",
        "title": "A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review",
        "link": "/arxiv/2509.20418",
        "arxiv_id": "2509.20418",
        "authors": "Grace Billiris, Asif Gill, Madhushi Bandara",
        "summary": "Quantum Artificial Intelligence (QAI), the integration of Artificial Intelligence (AI) and Quantum Computing (QC), promises transformative advances, including AI-enabled quantum cryptography and quantum-resistant encryption protocols. However, QAI inherits data risks from both AI and QC, creating complex privacy and security vulnerabilities that are not systematically studied. These risks affect the trustworthiness and reliability of AI and QAI systems, making their understanding critical. This study systematically reviews 67 privacy- and security-related studies to expand understanding of QAI data risks. We propose a taxonomy of 22 key data risks, organised into five categories: governance, risk assessment, control implementation, user considerations, and continuous monitoring. Our findings reveal vulnerabilities unique to QAI and identify gaps in holistic risk assessment. This work contributes to trustworthy AI and QAI research and provides a foundation for developing future risk assessment tools.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Emerging Technologies",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.330039"
    },
    {
        "index": "#168",
        "title": "Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation",
        "link": "/arxiv/2509.20411",
        "arxiv_id": "2509.20411",
        "authors": "Tharcisse Ndayipfukamiye, Jianguo Ding, Doreen Sebastian Sarwatt, Adamu Gaston Philipo, Huansheng Ning",
        "summary": "Machine learning-based cybersecurity systems are highly vulnerable to adversarial attacks, while Generative Adversarial Networks (GANs) act as both powerful attack enablers and promising defenses. This survey systematically reviews GAN-based adversarial defenses in cybersecurity (2021--August 31, 2025), consolidating recent progress, identifying gaps, and outlining future directions. Using a PRISMA-compliant systematic literature review protocol, we searched five major digital libraries. From 829 initial records, 185 peer-reviewed studies were retained and synthesized through quantitative trend analysis and thematic taxonomy development. We introduce a four-dimensional taxonomy spanning defensive function, GAN architecture, cybersecurity domain, and adversarial threat model. GANs improve detection accuracy, robustness, and data utility across network intrusion detection, malware analysis, and IoT security. Notable advances include WGAN-GP for stable training, CGANs for targeted synthesis, and hybrid GAN models for improved resilience. Yet, persistent challenges remain such as instability in training, lack of standardized benchmarks, high computational cost, and limited explainability. GAN-based defenses demonstrate strong potential but require advances in stable architectures, benchmarking, transparency, and deployment. We propose a roadmap emphasizing hybrid models, unified evaluation, real-world integration, and defenses against emerging threats such as LLM-driven cyberattacks. This survey establishes the foundation for scalable, trustworthy, and adaptive GAN-powered defenses.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.330327"
    },
    {
        "index": "#169",
        "title": "Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry",
        "link": "/arxiv/2509.20399",
        "arxiv_id": "2509.20399",
        "authors": "Birk Torpmann-Hagen, Michael A. Riegler, Pål Halvorsen, Dag Johansen",
        "summary": "Deep neural networks are being utilized in a growing number of applications, both in production systems and for personal use. Network checkpoints are as a consequence often shared and distributed on various platforms to ease the development process. This work considers the threat of neural network stegomalware, where malware is embedded in neural network checkpoints at a negligible cost to network accuracy. This constitutes a significant security concern, but is nevertheless largely neglected by the deep learning practitioners and security specialists alike. We propose the first effective countermeasure to these attacks. In particular, we show that state-of-the-art neural network stegomalware can be efficiently and effectively neutralized through shuffling the column order of the weight- and bias-matrices, or equivalently the channel-order of convolutional layers. We show that this effectively corrupts payloads that have been embedded by state-of-the-art methods in neural network steganography at no cost to network accuracy, outperforming competing methods by a significant margin. We then discuss possible means by which to bypass this defense, additional defense methods, and advocate for continued research into the security of machine learning systems.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.330531"
    },
    {
        "index": "#170",
        "title": "Variational Low-Rank Adaptation for Personalized Impaired Speech Recognition",
        "link": "/arxiv/2509.20397",
        "arxiv_id": "2509.20397",
        "authors": "Niclas Pokel, Pehuén Moure, Roman Boehringer, Shih-Chii Liu, Yingqiang Gao",
        "summary": "Speech impairments resulting from congenital disorders, such as cerebral palsy, down syndrome, or apert syndrome, as well as acquired brain injuries due to stroke, traumatic accidents, or tumors, present major challenges to automatic speech recognition (ASR) systems. Despite recent advancements, state-of-the-art ASR models like Whisper still struggle with non-normative speech due to limited training data availability and high acoustic variability. Moreover, collecting and annotating non-normative speech is burdensome: speaking is effortful for many affected individuals, while laborious annotation often requires caregivers familiar with the speaker. This work introduces a novel ASR personalization method based on Bayesian Low-rank Adaptation for data-efficient fine-tuning. We validate our method on the English UA-Speech dataset and a newly collected German speech dataset, BF-Sprache, from a child with structural speech impairment. The dataset and approach are designed to reflect the challenges of low-resource settings that include individuals with speech impairments. Our method significantly improves ASR accuracy for impaired speech while maintaining data and annotation efficiency, offering a practical path toward inclusive ASR.",
        "subjects": "Audio and Speech Processing, Artificial Intelligence",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.330726"
    },
    {
        "index": "#171",
        "title": "Data-Efficient ASR Personalization for Non-Normative Speech Using an Uncertainty-Based Phoneme Difficulty Score for Guided Sampling",
        "link": "/arxiv/2509.20396",
        "arxiv_id": "2509.20396",
        "authors": "Niclas Pokel, Pehuén Moure, Roman Boehringer, Yingqiang Gao",
        "summary": "Automatic speech recognition (ASR) systems struggle with non-normative speech from individuals with impairments caused by conditions like cerebral palsy or structural anomalies. The high acoustic variability and scarcity of training data severely degrade model performance. This work introduces a data-efficient personalization method that quantifies phoneme-level uncertainty to guide fine-tuning. We leverage Monte Carlo Dropout to estimate which phonemes a model finds most difficult and use these estimates for a targeted oversampling strategy. We validate our method on English and German datasets. Crucially, we demonstrate that our model-derived uncertainty strongly correlates with phonemes identified as challenging in an expert clinical logopedic report, marking, to our knowledge, the first work to successfully align model uncertainty with expert assessment of speech difficulty. Our results show that this clinically-validated, uncertainty-guided sampling significantly improves ASR accuracy, delivering a practical framework for personalized and inclusive ASR.",
        "subjects": "Audio and Speech Processing, Artificial Intelligence, Sound",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.330965"
    },
    {
        "index": "#172",
        "title": "Centralized vs. Decentralized Security for Space AI Systems? A New Look",
        "link": "/arxiv/2509.20395",
        "arxiv_id": "2509.20395",
        "authors": "Noam Schmitt, Marc Antoine Lacoste",
        "summary": "This paper investigates the trade-off between centralized and decentralized security management in constellations of satellites to balance security and performance. We highlight three key AI architectures for automated security management: (a) centralized, (b) distributed and (c) federated. The centralized architecture is the best option short term, providing fast training, despite the hard challenge of the communication latency overhead across space. Decentralized architectures are better alternatives in the longer term, providing enhanced scalability and security.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.331164"
    },
    {
        "index": "#174",
        "title": "The Secret Agenda: LLMs Strategically Lie and Our Current Safety Tools Are Blind",
        "link": "/arxiv/2509.20393",
        "arxiv_id": "2509.20393",
        "authors": "Caleb DeLeeuw, Gaurav Chawla, Aniket Sharma, Vanessa Dietze",
        "summary": "We investigate strategic deception in large language models using two complementary testbeds: Secret Agenda (across 38 models) and Insider Trading compliance (via SAE architectures). Secret Agenda reliably induced lying when deception advantaged goal achievement across all model families. Analysis revealed that autolabeled SAE features for \"deception\" rarely activated during strategic dishonesty, and feature steering experiments across 100+ deception-related features failed to prevent lying. Conversely, insider trading analysis using unlabeled SAE activations separated deceptive versus compliant responses through discriminative patterns in heatmaps and t-SNE visualizations. These findings suggest autolabel-driven interpretability approaches fail to detect or control behavioral deception, while aggregate unlabeled activations provide population-level structure for risk assessment. Results span Llama 8B/70B SAE implementations and GemmaScope under resource constraints, representing preliminary findings that motivate larger studies on feature discovery, labeling methodology, and causal interventions in realistic deception contexts.",
        "subjects": "Computers and Society, Artificial Intelligence, Machine Learning",
        "date": "2025-09-23",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.331564"
    },
    {
        "index": "#175",
        "title": "Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants",
        "link": "/arxiv/2509.20388",
        "arxiv_id": "2509.20388",
        "authors": "Amir AL-Maamari",
        "summary": "The rapid integration of AI-powered coding assistants into developer workflows has raised significant privacy and trust concerns. As developers entrust proprietary code to services like OpenAI's GPT, Google's Gemini, and GitHub Copilot, the unclear data handling practices of these tools create security and compliance risks. This paper addresses this challenge by introducing and applying a novel, expert-validated privacy scorecard. The methodology involves a detailed analysis of four document types; from legal policies to external audits; to score five leading assistants against 14 weighted criteria. A legal expert and a data protection officer refined these criteria and their weighting. The results reveal a distinct hierarchy of privacy protections, with a 20-point gap between the highest- and lowest-ranked tools. The analysis uncovers common industry weaknesses, including the pervasive use of opt-out consent for model training and a near-universal failure to filter secrets from user prompts proactively. The resulting scorecard provides actionable guidance for developers and organizations, enabling evidence-based tool selection. This work establishes a new benchmark for transparency and advocates for a shift towards more user-centric privacy standards in the AI industry.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.331733"
    },
    {
        "index": "#176",
        "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments",
        "link": "/arxiv/2509.20386",
        "arxiv_id": "2509.20386",
        "authors": "Nishant Gaurav, Adit Akarsh, Ankit Ranjan, Manoj Bajaj",
        "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef- ficiently operate with extensive Model Control Protocol (MCP) tool sets that exceed the contextual memory limitations of large language models. Our approach addresses the fundamental challenge of tool selection in environments containing hundreds or thousands of available tools, where loading all tools simultaneously is computationally infeasible. We propose and evaluate five distinct architectures that progressively refine the tool selection process, culminating in a search-and-load mechanism that achieves intelligent tool selection with minimal computational overhead. Our experimental results demonstrate that the proposed approach reduces tool loading by up to 50% while maintaining task completion accuracy, advancing the path towards truly general-purpose AI agents capable of dynamically adapting to diverse task environments.",
        "subjects": "Software Engineering, Artificial Intelligence, Information Retrieval",
        "date": "2025-09-22",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.331925"
    },
    {
        "index": "#177",
        "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning",
        "link": "/arxiv/2509.20384",
        "arxiv_id": "2509.20384",
        "authors": "Jiayi Lin, Liangcai Su, Junzhe Li, Chenxiong Qian",
        "summary": "Fuzzing is effective for vulnerability discovery but struggles with complex targets such as compilers, interpreters, and database engines, which accept textual input that must satisfy intricate syntactic and semantic constraints. Although language models (LMs) have attracted interest for this task due to their vast latent knowledge and reasoning potential, their practical adoption has been limited. The major challenges stem from insufficient exploration of deep program logic among real-world codebases, and the high cost of leveraging larger models. To overcome these challenges, we propose R1-Fuzz, the first framework that leverages reinforcement learning (RL) to specialize cost-efficient LMs and integrate them for complex textual fuzzing input generation. R1-Fuzz introduces two key designs: coverage-slicing-based question construction and a distance-based reward calculation. Through RL-based post-training of a model with our constructed dataset, R1-Fuzz designs a fuzzing workflow that tightly integrates LMs to reason deep program semantics during fuzzing. Evaluations on diverse real-world targets show that our design enables a small model, named R1-Fuzz-7B, to rival or even outperform much larger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\% higher coverage than state-of-the-art fuzzers and discovers 29 previously unknown vulnerabilities, demonstrating its practicality.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Programming Languages, Software Engineering",
        "date": "2025-09-21",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.332124"
    },
    {
        "index": "#178",
        "title": "MARS: A Malignity-Aware Backdoor Defense in Federated Learning",
        "link": "/arxiv/2509.20383",
        "arxiv_id": "2509.20383",
        "authors": "Wei Wan, Yuxuan Ning, Zhicong Huang, Cheng Hong, Shengshan Hu, Ziqi Zhou, Yechao Zhang, Tianqing Zhu, Wanlei Zhou, Leo Yu Zhang",
        "summary": "Federated Learning (FL) is a distributed paradigm aimed at protecting participant data privacy by exchanging model parameters to achieve high-quality model training. However, this distributed nature also makes FL highly vulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art (SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether the backdoor models have been accepted by the defender and adaptively optimizes backdoor models, rendering existing defenses ineffective. In this paper, we first reveal that the failure of existing defenses lies in the employment of empirical statistical measures that are loosely coupled with backdoor attacks. Motivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that leverages backdoor energy (BE) to indicate the malicious extent of each neuron. To amplify malignity, we further extract the most prominent BE values from each model to form a concentrated backdoor energy (CBE). Finally, a novel Wasserstein distance-based clustering method is introduced to effectively identify backdoor models. Extensive experiments demonstrate that MARS can defend against SOTA backdoor attacks and significantly outperforms existing defenses.",
        "subjects": "Cryptography and Security, Artificial Intelligence",
        "date": "2025-09-21",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.337688"
    },
    {
        "index": "#179",
        "title": "Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation",
        "link": "/arxiv/2509.20382",
        "arxiv_id": "2509.20382",
        "authors": "Dilli Hang Rai, Sabin Kafley",
        "summary": "ECG biometrics offer a unique, secure authentication method, yet their deployment on wearable devices faces real-time processing, privacy, and spoofing vulnerability challenges. This paper proposes a lightweight deep learning model (MobileNetV1+GRU) for ECG-based authentication, injection of 20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and edge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving accuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923, 0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of 0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013, 0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998, while under FGSM adversarial attacks, accuracy drops from 96.82% to as low as 0.80%. This paper highlights federated learning, adversarial testing, and the need for diverse wearable physiological datasets to ensure secure and scalable biometrics.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Machine Learning, Signal Processing",
        "date": "2025-09-21",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.338120"
    },
    {
        "index": "#181",
        "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation",
        "link": "/arxiv/2509.20380",
        "arxiv_id": "2509.20380",
        "authors": "Samyak Jhaveri, Vanessa Klotzmann, Crista Lopes",
        "summary": "The increasing ubiquity of GPUs is accompanied by the increasing complexity of their hardware and parallel programming frameworks. Directive-based parallel programming standards like OpenACC simplify GPU programming to some extent by abstracting away low-level complexities, but a fair amount of expertise is still required in order to use those directives effectively. We introduce ACCeLLiuM, two open weights Large Language Models specifically fine-tuned for generating expert OpenACC directives for data-parallel loops, along with the supervised fine-tuning dataset that was used to train them. The ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for testing. Experimental evaluations show a pronounced performance gap in generating correct OpenACC pragmas between base LLMs and our fine-tuned versions. On the held-out test set, base LLMs fail to consistently generate valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid pragmas with the correct directive type for $87\\%$ of the data-parallel loops, and exact pragmas--including directives, clauses, clause order, and clause variables--for $50\\%$ of the cases. Even when not exact, generated pragmas frequently incorporate the correct clauses in a different order than the ground-truth label, or include additional clauses that enable finer control over parallel execution, data movement, and concurrency, offering practical value beyond strict string-matching. By publicly releasing the code, models, and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU offloading of serially written programs.",
        "subjects": "Software Engineering, Artificial Intelligence, Programming Languages",
        "date": "2025-09-20",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.338851"
    },
    {
        "index": "#187",
        "title": "AI-driven formative assessment and adaptive learning in data-science education: Evaluating an LLM-powered virtual teaching assistant",
        "link": "/arxiv/2509.20369",
        "arxiv_id": "2509.20369",
        "authors": "Fadjimata I Anaroua, Qing Li, Yan Tang, Hong P. Liu",
        "summary": "This paper presents VITA (Virtual Teaching Assistants), an adaptive distributed learning (ADL) platform that embeds a large language model (LLM)-powered chatbot (BotCaptain) to provide dialogic support, interoperable analytics, and integrity-aware assessment for workforce preparation in data science. The platform couples context-aware conversational tutoring with formative-assessment patterns designed to promote reflective reasoning. The paper describes an end-to-end data pipeline that transforms chat logs into Experience API (xAPI) statements, instructor dashboards that surface outliers for just-in-time intervention, and an adaptive pathway engine that routes learners among progression, reinforcement, and remediation content. The paper also benchmarks VITA conceptually against emerging tutoring architectures, including retrieval-augmented generation (RAG)--based assistants and Learning Tools Interoperability (LTI)--integrated hubs, highlighting trade-offs among content grounding, interoperability, and deployment complexity. Contributions include a reusable architecture for interoperable conversational analytics, a catalog of patterns for integrity-preserving formative assessment, and a practical blueprint for integrating adaptive pathways into data-science courses. The paper concludes with implementation lessons and a roadmap (RAG integration, hallucination mitigation, and LTI~1.3 / OpenID Connect) to guide multi-course evaluations and broader adoption. In light of growing demand and scalability constraints in traditional instruction, the approach illustrates how conversational AI can support engagement, timely feedback, and personalized learning at scale. Future work will refine the platform's adaptive intelligence and examine applicability across varied educational settings.",
        "subjects": "Computers and Society, Artificial Intelligence, Human-Computer Interaction",
        "date": "2025-09-17",
        "category": "cs.AI",
        "crawl_time": "2025-09-26T21:01:55.340123"
    }
]