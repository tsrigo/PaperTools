[
    {
        "index": "#1",
        "title": "SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards",
        "link": "/arxiv/2509.26640",
        "arxiv_id": "2509.26640",
        "authors": "João Vitorino, Eva Maia, Isabel Praça, Carlos Soares",
        "summary": "Due to the susceptibility of Artificial Intelligence (AI) to data perturbations and adversarial examples, it is crucial to perform a thorough robustness evaluation before any Machine Learning (ML) model is deployed. However, examining a model's decision boundaries and identifying potential vulnerabilities typically requires access to the training and testing datasets, which may pose risks to data privacy and confidentiality. To improve transparency in organizations that handle confidential data or manage critical infrastructure, it is essential to allow external verification and validation of AI without the disclosure of private datasets. This paper presents Systematic Pattern Analysis (SPATA), a deterministic method that converts any tabular dataset to a domain-independent representation of its statistical patterns, to provide more detailed and transparent data cards. SPATA computes the projection of each data instance into a discrete space where they can be analyzed and compared, without risking data leakage. These projected datasets can be reliably used for the evaluation of how different features affect ML model robustness and for the generation of interpretable explanations of their behavior, contributing to more trustworthy AI.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.842286"
    },
    {
        "index": "#2",
        "title": "AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond",
        "link": "/arxiv/2509.26636",
        "arxiv_id": "2509.26636",
        "authors": "Shangding Gu, Xiaohan Wang, Donghao Ying, Haoyu Zhao, Runing Yang, Ming Jin, Boyi Li, Marco Pavone, Serena Yeung-Levy, Jun Wang, Dawn Song, Costas Spanos",
        "summary": "Rapid advances in multimodal models demand benchmarks that rigorously evaluate understanding and reasoning in safety-critical, dynamic real-world settings. We present AccidentBench, a large-scale benchmark that combines vehicle accident scenarios with Beyond domains, safety-critical settings in air and water that emphasize spatial and temporal reasoning (e.g., navigation, orientation, multi-vehicle motion). The benchmark contains approximately 2000 videos and over 19000 human-annotated question--answer pairs spanning multiple video lengths (short/medium/long) and difficulty levels (easy/medium/hard). Tasks systematically probe core capabilities: temporal, spatial, and intent understanding and reasoning. By unifying accident-centric traffic scenes with broader safety-critical scenarios in air and water, AccidentBench offers a comprehensive, physically grounded testbed for evaluating models under real-world variability. Evaluations of state-of-the-art models (e.g., Gemini-2.5 Pro and GPT-5) show that even the strongest models achieve only about 18% accuracy on the hardest tasks and longest videos, revealing substantial gaps in real-world temporal, spatial, and intent reasoning. AccidentBench is designed to expose these critical gaps and drive the development of multimodal models that are safer, more robust, and better aligned with real-world safety-critical challenges. The code and dataset are available at: https://github.com/SafeRL-Lab/AccidentBench",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.843129"
    },
    {
        "index": "#4",
        "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models",
        "link": "/arxiv/2509.26626",
        "arxiv_id": "2509.26626",
        "authors": "Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain",
        "summary": "Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains -- not just the final answers -- and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the model to combine solutions via a novel aggregation-aware reinforcement learning approach yields significant performance gains. Code available at https://github.com/HyperPotatoNeo/RSA.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.844818"
    },
    {
        "index": "#5",
        "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training",
        "link": "/arxiv/2509.26625",
        "arxiv_id": "2509.26625",
        "authors": "Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos",
        "summary": "Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition, Multimedia",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.845602"
    },
    {
        "index": "#6",
        "title": "Uncertainty Quantification for Regression using Proper Scoring Rules",
        "link": "/arxiv/2509.26610",
        "arxiv_id": "2509.26610",
        "authors": "Alexander Fishkov, Kajetan Schweighofer, Mykyta Ielanskyi, Nikita Kotelevskii, Mohsen Guizani, Maxim Panov",
        "summary": "Quantifying uncertainty of machine learning model predictions is essential for reliable decision-making, especially in safety-critical applications. Recently, uncertainty quantification (UQ) theory has advanced significantly, building on a firm basis of learning with proper scoring rules. However, these advances were focused on classification, while extending these ideas to regression remains challenging. In this work, we introduce a unified UQ framework for regression based on proper scoring rules, such as CRPS, logarithmic, squared error, and quadratic scores. We derive closed-form expressions for the resulting uncertainty measures under practical parametric assumptions and show how to estimate them using ensembles of models. In particular, the derived uncertainty measures naturally decompose into aleatoric and epistemic components. The framework recovers popular regression UQ measures based on predictive variance and differential entropy. Our broad evaluation on synthetic and real-world regression datasets provides guidance for selecting reliable UQ measures.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.846289"
    },
    {
        "index": "#8",
        "title": "Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning",
        "link": "/arxiv/2509.26578",
        "arxiv_id": "2509.26578",
        "authors": "Zheng Zhang, Ziwei Shan, Kaitao Song, Yexin Li, Kan Ren",
        "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning capabilities of large language models (LLMs) by guiding their step-by-step reasoning toward a final answer. However, existing PRMs either treat each reasoning step in isolation, failing to capture inter-step dependencies, or struggle to align process rewards with the final outcome. Consequently, the reward signal fails to respect temporal causality in sequential reasoning and faces ambiguous credit assignment. These limitations make downstream models vulnerable to reward hacking and lead to suboptimal performance. In this work, we propose Conditional Reward Modeling (CRM) that frames LLM reasoning as a temporal process leading to a correct answer. The reward of each reasoning step is not only conditioned on the preceding steps but also explicitly linked to the final outcome of the reasoning trajectory. By enforcing conditional probability rules, our design captures the causal relationships among reasoning steps, with the link to the outcome allowing precise attribution of each intermediate step, thereby resolving credit assignment ambiguity. Further, through this consistent probabilistic modeling, the rewards produced by CRM enable more reliable cross-sample comparison. Experiments across Best-of-N sampling, beam search and reinforcement learning demonstrate that CRM consistently outperforms existing reward models, offering a principled framework for enhancing LLM reasoning. In particular, CRM is more robust to reward hacking and delivers stable downstream improvements without relying on verifiable rewards derived from ground truth.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.852710"
    },
    {
        "index": "#9",
        "title": "Importance of localized dilatation and distensibility in identifying determinants of thoracic aortic aneurysm with neural operators",
        "link": "/arxiv/2509.26576",
        "arxiv_id": "2509.26576",
        "authors": "David S. Li, Somdatta Goswami, Qianying Cao, Vivek Oommen, Roland Assi, Jay D. Humphrey, George E. Karniadakis",
        "summary": "Thoracic aortic aneurysms (TAAs) arise from diverse mechanical and mechanobiological disruptions to the aortic wall that increase the risk of dissection or rupture. Evidence links TAA development to dysfunctions in the aortic mechanotransduction axis, including loss of elastic fiber integrity and cell-matrix connections. Because distinct insults create different mechanical vulnerabilities, there is a critical need to identify interacting factors that drive progression. Here, we use a finite element framework to generate synthetic TAAs from hundreds of heterogeneous insults spanning varying degrees of elastic fiber damage and impaired mechanosensing. From these simulations, we construct spatial maps of localized dilatation and distensibility to train neural networks that predict the initiating combined insult. We compare several architectures (Deep Operator Networks, UNets, and Laplace Neural Operators) and multiple input data formats to define a standard for future subject-specific modeling. We also quantify predictive performance when networks are trained using only geometric data (dilatation) versus both geometric and mechanical data (dilatation plus distensibility). Across all networks, prediction errors are significantly higher when trained on dilatation alone, underscoring the added value of distensibility information. Among the tested models, UNet consistently provides the highest accuracy across all data formats. These findings highlight the importance of acquiring full-field measurements of both dilatation and distensibility in TAA assessment to reveal the mechanobiological drivers of disease and support the development of personalized treatment strategies.",
        "subjects": "Machine Learning, Computational Engineering, Finance, and Science",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.853298"
    },
    {
        "index": "#10",
        "title": "Parametric Neural Amp Modeling with Active Learning",
        "link": "/arxiv/2509.26564",
        "arxiv_id": "2509.26564",
        "authors": "Florian Grötschla, Longxiang Jiao, Luca A. Lanzendörfer, Roger Wattenhofer",
        "summary": "We introduce Panama, an active learning framework to train parametric guitar amp models end-to-end using a combination of an LSTM model and a WaveNet-like architecture. With \\model, one can create a virtual amp by recording samples that are determined through an ensemble-based active learning strategy to minimize the amount of datapoints needed (i.e., amp knob settings). Our strategy uses gradient-based optimization to maximize the disagreement among ensemble models, in order to identify the most informative datapoints. MUSHRA listening tests reveal that, with 75 datapoints, our models are able to match the perceptual quality of NAM, the leading open-source non-parametric amp modeler.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.853803"
    },
    {
        "index": "#11",
        "title": "Bayesian Influence Functions for Hessian-Free Data Attribution",
        "link": "/arxiv/2509.26544",
        "arxiv_id": "2509.26544",
        "authors": "Philipp Alexander Kreer, Wilson Wu, Maxwell Adam, Zach Furman, Jesse Hoogland",
        "summary": "Classical influence functions face significant challenges when applied to deep neural networks, primarily due to non-invertible Hessians and high-dimensional parameter spaces. We propose the local Bayesian influence function (BIF), an extension of classical influence functions that replaces Hessian inversion with loss landscape statistics that can be estimated via stochastic-gradient MCMC sampling. This Hessian-free approach captures higher-order interactions among parameters and scales efficiently to neural networks with billions of parameters. We demonstrate state-of-the-art results on predicting retraining experiments.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.854315"
    },
    {
        "index": "#12",
        "title": "TASP: Topology-aware Sequence Parallelism",
        "link": "/arxiv/2509.26541",
        "arxiv_id": "2509.26541",
        "authors": "Yida Wang, Ke Hong, Xiuhong Li, Yuanchao Xu, Wenxun Wang, Guohao Dai, Yu Wang",
        "summary": "Long-context large language models (LLMs) face constraints due to the quadratic complexity of the self-attention mechanism. The mainstream sequence parallelism (SP) method, Ring Attention, attempts to solve this by distributing the query into multiple query chunks across accelerators and enable each Q tensor to access all KV tensors from other accelerators via the Ring AllGather communication primitive. However, it exhibits low communication efficiency, restricting its practical applicability. This inefficiency stems from the mismatch between the Ring AllGather communication primitive it adopts and the AlltoAll topology of modern accelerators. A Ring AllGather primitive is composed of iterations of ring-styled data transfer, which can only utilize a very limited fraction of an AlltoAll topology. Inspired by the Hamiltonian decomposition of complete directed graphs, we identify that modern accelerator topology can be decomposed into multiple orthogonal ring datapaths which can concurrently transfer data without interference. Based on this, we further observe that the Ring AllGather primitive can also be decomposed into the same number of concurrent ring-styled data transfer at every iteration. Based on these insights, we propose TASP, a topology-aware SP method for long-context LLMs that fully utilizes the communication capacity of modern accelerators via topology decomposition and primitive decomposition. Experimental results on both single-node and multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate that TASP achieves higher communication efficiency than Ring Attention on these modern accelerator topologies and achieves up to 3.58 speedup than Ring Attention and its variant Zigzag-Ring Attention. The code is available at https://github.com/infinigence/HamiltonAttention.",
        "subjects": "Machine Learning, Distributed, Parallel, and Cluster Computing",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.854908"
    },
    {
        "index": "#13",
        "title": "The Loss Kernel: A Geometric Probe for Deep Learning Interpretability",
        "link": "/arxiv/2509.26537",
        "arxiv_id": "2509.26537",
        "authors": "Maxwell Adam, Zach Furman, Jesse Hoogland",
        "summary": "We introduce the loss kernel, an interpretability method for measuring similarity between data points according to a trained neural network. The kernel is the covariance matrix of per-sample losses computed under a distribution of low-loss-preserving parameter perturbations. We first validate our method on a synthetic multitask problem, showing it separates inputs by task as predicted by theory. We then apply this kernel to Inception-v1 to visualize the structure of ImageNet, and we show that the kernel's structure aligns with the WordNet semantic hierarchy. This establishes the loss kernel as a practical tool for interpretability and data attribution.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.855395"
    },
    {
        "index": "#14",
        "title": "Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in Power Grids",
        "link": "/arxiv/2509.26532",
        "arxiv_id": "2509.26532",
        "authors": "Justin Tackett, Benjamin Francis, Luis Garcia, David Grimsman, Sean Warnick",
        "summary": "Every year critical infrastructure becomes more complex and we grow to rely on it more and more. With this reliance, it becomes an attractive target for cyberattacks from sophisticated actors, with one of the most attractive targets being the power grid. One class of attacks, instability attacks, is a newer type of attack that has relatively few protections developed. We present a cost effective, data-driven approach to training a supervised machine learning model to retrofit load shedding decision systems in power grids with the capacity to defend against instability attacks. We show a proof of concept on the IEEE 14 Bus System using the Achilles Heel Technologies Power Grid Analyzer, and show through an implementation of modified Prony analysis (MPA) that MPA is a viable method for detecting instability attacks and triggering defense mechanisms.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.855984"
    },
    {
        "index": "#15",
        "title": "TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning",
        "link": "/arxiv/2509.26524",
        "arxiv_id": "2509.26524",
        "authors": "Seohyun Lee, Wenzhi Fang, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton",
        "summary": "Federated Learning (FL), despite demonstrating impressive capabilities in the training of multiple models in a decentralized manner, has been shown to produce a final model not necessarily well-suited to the needs of each client. While extensive work has been conducted on how to create tailored personalized models, called Personalized Federated Learning (PFL), less attention has been given to personalization via fine-tuning of foundation models with multi-task and multi-modal properties. Moreover, there exists a lack of understanding in the literature on how to fine-tune and personalize such models in a setting that is heterogeneous across clients not only in data, but also in tasks and modalities. To address this gap in the literature, we propose TAP (Two-Stage Adaptive Personalization), which (i) leverages mismatched model architectures between the clients and server to selectively conduct replacement operations when it benefits a client's local tasks and (ii) engages in post-FL knowledge distillation for capturing beneficial general knowledge without compromising personalization. We also introduce the first convergence analysis of the server model under its modality-task pair architecture, and demonstrate that as the number of modality-task pairs increases, its ability to cater to all tasks suffers. Through extensive experiments, we demonstrate the effectiveness of our proposed algorithm across a variety of datasets and tasks in comparison to a multitude of baselines. Implementation code is publicly available at https://github.com/lee3296/TAP.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.861688"
    },
    {
        "index": "#16",
        "title": "Entropy After $\\langle \\texttt{/Think} \\rangle$ for reasoning model early exiting",
        "link": "/arxiv/2509.26522",
        "arxiv_id": "2509.26522",
        "authors": "Xi Wang, James McInerney, Lequn Wang, Nathan Kallus",
        "summary": "Large reasoning models show improved performance with longer chains of thought. However, recent work has highlighted (qualitatively) their tendency to overthink, continuing to revise answers even after reaching the correct solution. We quantitatively confirm this inefficiency by tracking Pass@1 for answers averaged over a large number of rollouts and find that the model often begins to always produce the correct answer early in the reasoning, making extra reasoning a waste of tokens. To detect and prevent overthinking, we propose a simple and inexpensive novel signal -- Entropy After </Think> (EAT) -- for monitoring and deciding whether to exit reasoning early. By appending a stop thinking token (</think>) and monitoring the entropy of the following token as the model reasons, we obtain a trajectory that decreases and stabilizes when Pass@1 plateaus; thresholding its variance under an exponential moving average yields a practical stopping rule. Importantly, our approach enables adaptively allocating compute based on the EAT trajectory, allowing us to spend compute in a more efficient way compared with fixing the token budget for all questions. Empirically, on MATH500 and AIME2025, EAT reduces token usage by 13 - 21% without harming accuracy, and it remains effective in black box settings where logits from the reasoning model are not accessible, and EAT is computed with proxy models.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.862236"
    },
    {
        "index": "#17",
        "title": "Equivariance by Local Canonicalization: A Matter of Representation",
        "link": "/arxiv/2509.26499",
        "arxiv_id": "2509.26499",
        "authors": "Gerrit Gerhartz, Peter Lippmann, Fred A. Hamprecht",
        "summary": "Equivariant neural networks offer strong inductive biases for learning from molecular and geometric data but often rely on specialized, computationally expensive tensor operations. We present a framework to transfers existing tensor field networks into the more efficient local canonicalization paradigm, preserving equivariance while significantly improving the runtime. Within this framework, we systematically compare different equivariant representations in terms of theoretical complexity, empirical runtime, and predictive accuracy. We publish the tensor_frames package, a PyTorchGeometric based implementation for local canonicalization, that enables straightforward integration of equivariance into any standard message passing neural network.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.862720"
    },
    {
        "index": "#18",
        "title": "DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick",
        "link": "/arxiv/2509.26469",
        "arxiv_id": "2509.26469",
        "authors": "Mohammad Hassan Vali, Tom Bäckström, Arno Solin",
        "summary": "Vector quantization is common in deep models, yet its hard assignments block gradients and hinder end-to-end training. We propose DiVeQ, which treats quantization as adding an error vector that mimics the quantization distortion, keeping the forward pass hard while letting gradients flow. We also present a space-filling variant (SF-DiVeQ) that assigns to a curve constructed by the lines connecting codewords, resulting in less quantization error and full codebook usage. Both methods train end-to-end without requiring auxiliary losses or temperature schedules. On VQ-VAE compression and VQGAN generation across various data sets, they improve reconstruction and sample quality over alternative quantization approaches.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.863206"
    },
    {
        "index": "#19",
        "title": "fev-bench: A Realistic Benchmark for Time Series Forecasting",
        "link": "/arxiv/2509.26468",
        "arxiv_id": "2509.26468",
        "authors": "Oleksandr Shchur, Abdul Fatir Ansari, Caner Turkmen, Lorenzo Stella, Nick Erickson, Pablo Guerron, Michael Bohlke-Schneider, Yuyang Wang",
        "summary": "Benchmark quality is critical for meaningful evaluation and sustained progress in time series forecasting, particularly given the recent rise of pretrained models. Existing benchmarks often have narrow domain coverage or overlook important real-world settings, such as tasks with covariates. Additionally, their aggregation procedures often lack statistical rigor, making it unclear whether observed performance differences reflect true improvements or random variation. Many benchmarks also fail to provide infrastructure for consistent evaluation or are too rigid to integrate into existing pipelines. To address these gaps, we propose fev-bench, a benchmark comprising 100 forecasting tasks across seven domains, including 46 tasks with covariates. Supporting the benchmark, we introduce fev, a lightweight Python library for benchmarking forecasting models that emphasizes reproducibility and seamless integration with existing workflows. Usingfev, fev-bench employs principled aggregation methods with bootstrapped confidence intervals to report model performance along two complementary dimensions: win rates and skill scores. We report results on fev-bench for various pretrained, statistical and baseline models, and identify promising directions for future research.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.863815"
    },
    {
        "index": "#20",
        "title": "Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning",
        "link": "/arxiv/2509.26442",
        "arxiv_id": "2509.26442",
        "authors": "Xinyu Liu, Zixuan Xie, Shangtong Zhang",
        "summary": "The Robbins-Siegmund theorem establishes the convergence of stochastic processes that are almost supermartingales and is foundational for analyzing a wide range of stochastic iterative algorithms in stochastic approximation and reinforcement learning (RL). However, its original form has a significant limitation as it requires the zero-order term to be summable. In many important RL applications, this summable condition, however, cannot be met. This limitation motivates us to extend the Robbins-Siegmund theorem for almost supermartingales where the zero-order term is not summable but only square summable. Particularly, we introduce a novel and mild assumption on the increments of the stochastic processes. This together with the square summable condition enables an almost sure convergence to a bounded set. Additionally, we further provide almost sure convergence rates, high probability concentration bounds, and $L^p$ convergence rates. We then apply the new results in stochastic approximation and RL. Notably, we obtain the first almost sure convergence rate, the first high probability concentration bound, and the first $L^p$ convergence rate for $Q$-learning with linear function approximation.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.864343"
    },
    {
        "index": "#21",
        "title": "ACT: Agentic Classification Tree",
        "link": "/arxiv/2509.26433",
        "arxiv_id": "2509.26433",
        "authors": "Vincent Grari, Tim Arni, Thibault Laugel, Sylvain Lamprier, James Zou, Marcin Detyniecki",
        "summary": "When used in high-stakes settings, AI systems are expected to produce decisions that are transparent, interpretable, and auditable, a requirement increasingly expected by regulations. Decision trees such as CART provide clear and verifiable rules, but they are restricted to structured tabular data and cannot operate directly on unstructured inputs such as text. In practice, large language models (LLMs) are widely used for such data, yet prompting strategies such as chain-of-thought or prompt optimization still rely on free-form reasoning, limiting their ability to ensure trustworthy behaviors. We present the Agentic Classification Tree (ACT), which extends decision-tree methodology to unstructured inputs by formulating each split as a natural-language question, refined through impurity-based evaluation and LLM feedback via TextGrad. Experiments on text benchmarks show that ACT matches or surpasses prompting-based baselines while producing transparent and interpretable decision paths.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.864905"
    },
    {
        "index": "#22",
        "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size",
        "link": "/arxiv/2509.26432",
        "arxiv_id": "2509.26432",
        "authors": "Guanxi Lu, Hao, Chen, Yuto Karashima, Zhican Wang, Daichi Fujiki, Hongxiang Fan",
        "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive (semi-AR) approaches are widely adopted due to their natural support for KV caching and their favorable accuracy-speed trade-off. However, this paper identifies two fundamental limitations in the conventional semi-AR decoding approach that applies a fixed block size: i) late decoding overhead, where the unmasking of high-confidence tokens outside the current block is unnecessarily delayed, and ii) premature decoding error, where low-confidence tokens inside the current block are committed too early, leading to incorrect tokens. This paper presents the first systematic investigation challenging the fixed block size assumption in semi-AR decoding. Through a statistical analysis of confidence dynamics during the denoising process, we identify a volatility band (VB) region during dLLM decoding, which encodes local semantic structure and can be used to guide adaptive block sizing. Leveraging these insights, we introduce AdaBlock-dLLM, a training-free, plug-and-play scheduler that adaptively aligns block boundaries with semantic steps by adjusting block size during runtime. Extensive experiments across diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget. Beyond inference-time optimization, we hope our semantics-aware adaptive scheduling approach and confidence-based analysis will inspire future training strategies for dLLMs.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.865490"
    },
    {
        "index": "#23",
        "title": "Ascent Fails to Forget",
        "link": "/arxiv/2509.26427",
        "arxiv_id": "2509.26427",
        "authors": "Ioannis Mavrothalassitis, Pol Puigdemont, Noam Itzhak Levi, Volkan Cevher",
        "summary": "Contrary to common belief, we show that gradient ascent-based unconstrained optimization methods frequently fail to perform machine unlearning, a phenomenon we attribute to the inherent statistical dependence between the forget and retain data sets. This dependence, which can manifest itself even as simple correlations, undermines the misconception that these sets can be independently manipulated during unlearning. We provide empirical and theoretical evidence showing these methods often fail precisely due to this overlooked relationship. For random forget sets, this dependence means that degrading forget set metrics (which, for a retrained model, should mirror test set metrics) inevitably harms overall test performance. Going beyond random sets, we consider logistic regression as an instructive example where a critical failure mode emerges: inter-set dependence causes gradient descent-ascent iterations to progressively diverge from the ideal retrained model. Strikingly, these methods can converge to solutions that are not only far from the retrained ideal but are potentially even further from it than the original model itself, rendering the unlearning process actively detrimental. A toy example further illustrates how this dependence can trap models in inferior local minima, inescapable via finetuning. Our findings highlight that the presence of such statistical dependencies, even when manifest only as correlations, can be sufficient for ascent-based unlearning to fail. Our theoretical insights are corroborated by experiments on complex neural networks, demonstrating that these methods do not perform as expected in practice due to this unaddressed statistical interplay.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.866018"
    },
    {
        "index": "#24",
        "title": "Refine Drugs, Don't Complete Them: Uniform-Source Discrete Flows for Fragment-Based Drug Discovery",
        "link": "/arxiv/2509.26405",
        "arxiv_id": "2509.26405",
        "authors": "Benno Kaech, Luis Wyss, Karsten Borgwardt, Gianvito Grasso",
        "summary": "We introduce InVirtuoGen, a discrete flow generative model for fragmented SMILES for de novo and fragment-constrained generation, and target-property/lead optimization of small molecules. The model learns to transform a uniform source over all possible tokens into the data distribution. Unlike masked models, its training loss accounts for predictions on all sequence positions at every denoising step, shifting the generation paradigm from completion to refinement, and decoupling the number of sampling steps from the sequence length. For \\textit{de novo} generation, InVirtuoGen achieves a stronger quality-diversity pareto frontier than prior fragment-based models and competitive performance on fragment-constrained tasks. For property and lead optimization, we propose a hybrid scheme that combines a genetic algorithm with a Proximal Property Optimization fine-tuning strategy adapted to discrete flows. Our approach sets a new state-of-the-art on the Practical Molecular Optimization benchmark, measured by top-10 AUC across tasks, and yields higher docking scores in lead optimization than previous baselines. InVirtuoGen thus establishes a versatile generative foundation for drug discovery, from early hit finding to multi-objective lead optimization. We further contribute to open science by releasing pretrained checkpoints and code, making our results fully reproducible\\footnote{https://github.com/invirtuolabs/InVirtuoGen_results}.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.866515"
    },
    {
        "index": "#25",
        "title": "Data-to-Energy Stochastic Dynamics",
        "link": "/arxiv/2509.26364",
        "arxiv_id": "2509.26364",
        "authors": "Kirill Tamogashev, Nikolay Malkin",
        "summary": "The Schrödinger bridge problem is concerned with finding a stochastic dynamical system bridging two marginal distributions that minimises a certain transportation cost. This problem, which represents a generalisation of optimal transport to the stochastic case, has received attention due to its connections to diffusion models and flow matching, as well as its applications in the natural sciences. However, all existing algorithms allow to infer such dynamics only for cases where samples from both distributions are available. In this paper, we propose the first general method for modelling Schrödinger bridges when one (or both) distributions are given by their unnormalised densities, with no access to data samples. Our algorithm relies on a generalisation of the iterative proportional fitting (IPF) procedure to the data-free case, inspired by recent developments in off-policy reinforcement learning for training of diffusion samplers. We demonstrate the efficacy of the proposed data-to-energy IPF on synthetic problems, finding that it can successfully learn transports between multimodal distributions. As a secondary consequence of our reinforcement learning formulation, which assumes a fixed time discretisation scheme for the dynamics, we find that existing data-to-data Schrödinger bridge algorithms can be substantially improved by learning the diffusion coefficient of the dynamics. Finally, we apply the newly developed algorithm to the problem of sampling posterior distributions in latent spaces of generative models, thus creating a data-free image-to-image translation method. Code: https://github.com/mmacosha/d2e-stochastic-dynamics",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.872165"
    },
    {
        "index": "#26",
        "title": "LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and MCI-Like Field Simulation",
        "link": "/arxiv/2509.26351",
        "arxiv_id": "2509.26351",
        "authors": "Joshua Sebastian, Karma Tobden, KMA Solaiman",
        "summary": "Research on emergency and mass casualty incident (MCI) triage has been limited by the absence of openly usable, reproducible benchmarks. Yet these scenarios demand rapid identification of the patients most in need, where accurate deterioration prediction can guide timely interventions. While the MIMIC-IV-ED database is openly available to credentialed researchers, transforming it into a triage-focused benchmark requires extensive preprocessing, feature harmonization, and schema alignment -- barriers that restrict accessibility to only highly technical users. We address these gaps by first introducing an open, LLM-assisted emergency triage benchmark for deterioration prediction (ICU transfer, in-hospital mortality). The benchmark then defines two regimes: (i) a hospital-rich setting with vitals, labs, notes, chief complaints, and structured observations, and (ii) an MCI-like field simulation limited to vitals, observations, and notes. Large language models (LLMs) contributed directly to dataset construction by (i) harmonizing noisy fields such as AVPU and breathing devices, (ii) prioritizing clinically relevant vitals and labs, and (iii) guiding schema alignment and efficient merging of disparate tables. We further provide baseline models and SHAP-based interpretability analyses, illustrating predictive gaps between regimes and the features most critical for triage. Together, these contributions make triage prediction research more reproducible and accessible -- a step toward dataset democratization in clinical AI.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.872657"
    },
    {
        "index": "#27",
        "title": "Memory-Driven Self-Improvement for Decision Making with Large Language Models",
        "link": "/arxiv/2509.26340",
        "arxiv_id": "2509.26340",
        "authors": "Xue Yan, Zijing Ou, Mengyue Yang, Yan Song, Haifeng Zhang, Yingzhen Li, Jun Wang",
        "summary": "Large language models (LLMs) have emerged as effective action policies for sequential decision-making (SDM) tasks due to their extensive prior knowledge. However, this broad yet general knowledge is often insufficient for specific decision-making tasks with limited task-related data, making it challenging to efficiently adapt LLMs to specific SDM tasks. To address this challenge, we propose a memory-driven self-improvement framework that combines LLM general prior knowledge with a compact memory of domain-specific experiences. Memory retains past interactions and associated Q-values, thereby capturing decision-relevant knowledge that facilitates accurate value estimation and informs the LLM prior refinement. The refined LLM prior, in turn, generates higher-reward trajectories that further enrich memory, forming a natural self-improvement framework where memory and LLM prior mutually reinforce each other. Experiments show that our memory-driven approach significantly outperforms both traditional RL and LLM-based baselines, e.g., improving performance by over 40\\% on in-distribution tasks and over 75\\% when generalized to unseen tasks in ALFWorld.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.873203"
    },
    {
        "index": "#28",
        "title": "FedMuon: Federated Learning with Bias-corrected LMO-based Optimization",
        "link": "/arxiv/2509.26337",
        "arxiv_id": "2509.26337",
        "authors": "Yuki Takezawa, Anastasia Koloskova, Xiaowen Jiang, Sebastian U. Stich",
        "summary": "Recently, a new optimization method based on the linear minimization oracle (LMO), called Muon, has been attracting increasing attention since it can train neural networks faster than existing adaptive optimization methods, such as Adam. In this paper, we study how Muon can be utilized in federated learning. We first show that straightforwardly using Muon as the local optimizer of FedAvg does not converge to the stationary point since the LMO is a biased operator. We then propose FedMuon which can mitigate this issue. We also analyze how solving the LMO approximately affects the convergence rate and find that, surprisingly, FedMuon can converge for any number of Newton-Schulz iterations, while it can converge faster as we solve the LMO more accurately. Through experiments, we demonstrated that FedMuon can outperform the state-of-the-art federated learning methods.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.873863"
    },
    {
        "index": "#29",
        "title": "A Generalized Information Bottleneck Theory of Deep Learning",
        "link": "/arxiv/2509.26327",
        "arxiv_id": "2509.26327",
        "authors": "Charles Westphal, Stephen Hailes, Mirco Musolesi",
        "summary": "The Information Bottleneck (IB) principle offers a compelling theoretical framework to understand how neural networks (NNs) learn. However, its practical utility has been constrained by unresolved theoretical ambiguities and significant challenges in accurate estimation. In this paper, we present a \\textit{Generalized Information Bottleneck (GIB)} framework that reformulates the original IB principle through the lens of synergy, i.e., the information obtainable only through joint processing of features. We provide theoretical and empirical evidence demonstrating that synergistic functions achieve superior generalization compared to their non-synergistic counterparts. Building on these foundations we re-formulate the IB using a computable definition of synergy based on the average interaction information (II) of each feature with those remaining. We demonstrate that the original IB objective is upper bounded by our GIB in the case of perfect estimation, ensuring compatibility with existing IB theory while addressing its limitations. Our experimental results demonstrate that GIB consistently exhibits compression phases across a wide range of architectures (including those with \\textit{ReLU} activations where the standard IB fails), while yielding interpretable dynamics in both CNNs and Transformers and aligning more closely with our understanding of adversarial robustness.",
        "subjects": "Machine Learning, Information Theory",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.874396"
    },
    {
        "index": "#30",
        "title": "ACE: Adapting sampling for Counterfactual Explanations",
        "link": "/arxiv/2509.26322",
        "arxiv_id": "2509.26322",
        "authors": "Margarita A. Guerrero, Cristian R. Rojas",
        "summary": "Counterfactual Explanations (CFEs) interpret machine learning models by identifying the smallest change to input features needed to change the model's prediction to a desired output. For classification tasks, CFEs determine how close a given sample is to the decision boundary of a trained classifier. Existing methods are often sample-inefficient, requiring numerous evaluations of a black-box model -- an approach that is both costly and impractical when access to the model is limited. We propose Adaptive sampling for Counterfactual Explanations (ACE), a sample-efficient algorithm combining Bayesian estimation and stochastic optimization to approximate the decision boundary with fewer queries. By prioritizing informative points, ACE minimizes evaluations while generating accurate and feasible CFEs. Extensive empirical results show that ACE achieves superior evaluation efficiency compared to state-of-the-art methods, while maintaining effectiveness in identifying minimal and actionable changes.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.874889"
    },
    {
        "index": "#31",
        "title": "A Review on Single-Problem Multi-Attempt Heuristic Optimization",
        "link": "/arxiv/2509.26321",
        "arxiv_id": "2509.26321",
        "authors": "Judith Echevarrieta, Etor Arza, Aritz Pérez, Josu Ceberio",
        "summary": "In certain real-world optimization scenarios, practitioners are not interested in solving multiple problems but rather in finding the best solution to a single, specific problem. When the computational budget is large relative to the cost of evaluating a candidate solution, multiple heuristic alternatives can be tried to solve the same given problem, each possibly with a different algorithm, parameter configuration, initialization, or stopping criterion. The sequential selection of which alternative to try next is crucial for efficiently identifying the one that provides the best possible solution across multiple attempts. Despite the relevance of this problem in practice, it has not yet been the exclusive focus of any existing review. Several sequential alternative selection strategies have been proposed in different research topics, but they have not been comprehensively and systematically unified under a common perspective. This work presents a focused review of single-problem multi-attempt heuristic optimization. It brings together suitable strategies to this problem that have been studied separately through algorithm selection, parameter tuning, multi-start and resource allocation. These strategies are explained using a unified terminology within a common framework, which supports the development of a taxonomy for systematically organizing and classifying them.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.875403"
    },
    {
        "index": "#32",
        "title": "Attribution-Guided Decoding",
        "link": "/arxiv/2509.26307",
        "arxiv_id": "2509.26307",
        "authors": "Piotr Komorowski, Elena Golimblevskaia, Reduan Achtibat, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek",
        "summary": "The capacity of Large Language Models (LLMs) to follow complex instructions and generate factually accurate text is critical for their real-world application. However, standard decoding methods often fail to robustly satisfy these requirements, while existing control techniques frequently degrade general output quality. In this work, we introduce Attribution-Guided Decoding (AGD), an interpretability-based decoding strategy. Instead of directly manipulating model activations, AGD considers a set of high-probability output token candidates and selects the one that exhibits the highest attribution to a user-defined Region of Interest (ROI). This ROI can be flexibly defined over different parts of the model's input or internal components, allowing AGD to steer generation towards various desirable behaviors. We demonstrate AGD's efficacy across three challenging domains. For instruction following, we show that AGD significantly boosts adherence (e.g., improving the overall success rate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show that guiding generation towards usage of internal knowledge components or contextual sources can reduce hallucinations and improve factual accuracy in both closed-book and open-book settings. Furthermore, we propose an adaptive, entropy-based variant of AGD that mitigates quality degradation and reduces computational overhead by applying guidance only when the model is uncertain. Our work presents a versatile, more interpretable, and effective method for enhancing the reliability of modern LLMs.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.875938"
    },
    {
        "index": "#33",
        "title": "NeuroTTT: Bridging Pretraining-Downstream Task Misalignment in EEG Foundation Models via Test-Time Training",
        "link": "/arxiv/2509.26301",
        "arxiv_id": "2509.26301",
        "authors": "Suli Wang, Yangshen Deng, Zhenghua Bao, Xinyu Zhan, Yiqun Duan",
        "summary": "Large-scale foundation models for EEG signals offer a promising path to generalizable brain-computer interface (BCI) applications, but they often suffer from misalignment between pretraining objectives and downstream tasks, as well as significant cross-subject distribution shifts. This paper addresses these challenges by introducing a two-stage alignment strategy that bridges the gap between generic pretraining and specific EEG decoding tasks. First, we propose NeuroTTT: a domain-specific self-supervised fine-tuning paradigm that augments the foundation model with task-relevant self-supervised objectives, aligning latent representations to important spectral, spatial, and temporal EEG features without requiring additional labeled data. Second, we incorporate test-time training (TTT) at inference, we perform (i) self-supervised test-time training on individual unlabeled test samples and (ii) prediction entropy minimization (Tent), which updates only normalization statistics to continually calibrate the model to each new input on the fly. Our approach, which, to our knowledge, is the first to unify domain-tuned self-supervision with test-time training in large-scale EEG foundation models, yields substantially improved robustness and accuracy across diverse BCI tasks (imagined speech, stress detection, motor imagery). Using CBraMod and LaBraM as backbones, our method pushes their performance to a markedly higher level. Results on three diverse tasks demonstrate that the proposed alignment strategy achieves state-of-the-art performance, outperforming conventional fine-tuning and adaptation methods. Our code is available at https://github.com/wsl2000/NeuroTTT.",
        "subjects": "Machine Learning, Human-Computer Interaction",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.876465"
    },
    {
        "index": "#34",
        "title": "Tuning the Tuner: Introducing Hyperparameter Optimization for Auto-Tuning",
        "link": "/arxiv/2509.26300",
        "arxiv_id": "2509.26300",
        "authors": "Floris-Jan Willemsen, Rob V. van Nieuwpoort, Ben van Werkhoven",
        "summary": "Automatic performance tuning (auto-tuning) is widely used to optimize performance-critical applications across many scientific domains by finding the best program variant among many choices. Efficient optimization algorithms are crucial for navigating the vast and complex search spaces in auto-tuning. As is well known in the context of machine learning and similar fields, hyperparameters critically shape optimization algorithm efficiency. Yet for auto-tuning frameworks, these hyperparameters are almost never tuned, and their potential performance impact has not been studied. We present a novel method for general hyperparameter tuning of optimization algorithms for auto-tuning, thus \"tuning the tuner\". In particular, we propose a robust statistical method for evaluating hyperparameter performance across search spaces, publish a FAIR data set and software for reproducibility, and present a simulation mode that replays previously recorded tuning data, lowering the costs of hyperparameter tuning by two orders of magnitude. We show that even limited hyperparameter tuning can improve auto-tuner performance by 94.8% on average, and establish that the hyperparameters themselves can be optimized efficiently with meta-strategies (with an average improvement of 204.7%), demonstrating the often overlooked hyperparameter tuning as a powerful technique for advancing auto-tuning research and practice.",
        "subjects": "Machine Learning, Distributed, Parallel, and Cluster Computing, Performance",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.882133"
    },
    {
        "index": "#35",
        "title": "Noise-Guided Transport for Imitation Learning",
        "link": "/arxiv/2509.26294",
        "arxiv_id": "2509.26294",
        "authors": "Lionel Blondé, Joao A. Candido Ramos, Alexandros Kalousis",
        "summary": "We consider imitation learning in the low-data regime, where only a limited number of expert demonstrations are available. In this setting, methods that rely on large-scale pretraining or high-capacity architectures can be difficult to apply, and efficiency with respect to demonstration data becomes critical. We introduce Noise-Guided Transport (NGT), a lightweight off-policy method that casts imitation as an optimal transport problem solved via adversarial training. NGT requires no pretraining or specialized architectures, incorporates uncertainty estimation by design, and is easy to implement and tune. Despite its simplicity, NGT achieves strong performance on challenging continuous control tasks, including high-dimensional Humanoid tasks, under ultra-low data regimes with as few as 20 transitions. Code is publicly available at: https://github.com/lionelblonde/ngt-pytorch.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.882654"
    },
    {
        "index": "#36",
        "title": "Reframing Generative Models for Physical Systems using Stochastic Interpolants",
        "link": "/arxiv/2509.26282",
        "arxiv_id": "2509.26282",
        "authors": "Anthony Zhou, Alexander Wikner, Amaury Lancelin, Pedram Hassanzadeh, Amir Barati Farimani",
        "summary": "Generative models have recently emerged as powerful surrogates for physical systems, demonstrating increased accuracy, stability, and/or statistical fidelity. Most approaches rely on iteratively denoising a Gaussian, a choice that may not be the most effective for autoregressive prediction tasks in PDEs and dynamical systems such as climate. In this work, we benchmark generative models across diverse physical domains and tasks, and highlight the role of stochastic interpolants. By directly learning a stochastic process between current and future states, stochastic interpolants can leverage the proximity of successive physical distributions. This allows for generative models that can use fewer sampling steps and produce more accurate predictions than models relying on transporting Gaussian noise. Our experiments suggest that generative models need to balance deterministic accuracy, spectral consistency, and probabilistic calibration, and that stochastic interpolants can potentially fulfill these requirements by adjusting their sampling. This study establishes stochastic interpolants as a competitive baseline for physical emulation and gives insight into the abilities of different generative modeling frameworks.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.883149"
    },
    {
        "index": "#37",
        "title": "Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness",
        "link": "/arxiv/2509.26275",
        "arxiv_id": "2509.26275",
        "authors": "Ahmad-Reza Ehyaei, Golnoosh Farnadi, Samira Samadi",
        "summary": "In recent years, Wasserstein Distributionally Robust Optimization (DRO) has garnered substantial interest for its efficacy in data-driven decision-making under distributional uncertainty. However, limited research has explored the application of DRO to address individual fairness concerns, particularly when considering causal structures and sensitive attributes in learning problems. To address this gap, we first formulate the DRO problem from causality and individual fairness perspectives. We then present the DRO dual formulation as an efficient tool to convert the DRO problem into a more tractable and computationally efficient form. Next, we characterize the closed form of the approximate worst-case loss quantity as a regularizer, eliminating the max-step in the min-max DRO problem. We further estimate the regularizer in more general cases and explore the relationship between DRO and classical robust optimization. Finally, by removing the assumption of a known structural causal model, we provide finite sample error bounds when designing DRO with empirical distributions and estimated causal structures to ensure efficiency and robust learning.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.883648"
    },
    {
        "index": "#38",
        "title": "From Fragile to Certified: Wasserstein Audits of Group Fairness Under Distribution Shift",
        "link": "/arxiv/2509.26241",
        "arxiv_id": "2509.26241",
        "authors": "Ahmad-Reza Ehyaei, Golnoosh Farnadi, Samira Samadi",
        "summary": "Group-fairness metrics (e.g., equalized odds) can vary sharply across resamples and are especially brittle under distribution shift, undermining reliable audits. We propose a Wasserstein distributionally robust framework that certifies worst-case group fairness over a ball of plausible test distributions centered at the empirical law. Our formulation unifies common group fairness notions via a generic conditional-probability functional and defines $\\varepsilon$-Wasserstein Distributional Fairness ($\\varepsilon$-WDF) as the audit target. Leveraging strong duality, we derive tractable reformulations and an efficient estimator (DRUNE) for $\\varepsilon$-WDF. We prove feasibility and consistency and establish finite-sample certification guarantees for auditing fairness, along with quantitative bounds under smoothness and margin conditions. Across standard benchmarks and classifiers, $\\varepsilon$-WDF delivers stable fairness assessments under distribution shift, providing a principled basis for auditing and certifying group fairness beyond observational data.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.884151"
    },
    {
        "index": "#39",
        "title": "Sandbagging in a Simple Survival Bandit Problem",
        "link": "/arxiv/2509.26239",
        "arxiv_id": "2509.26239",
        "authors": "Joel Dyer, Daniel Jarne Ornia, Nicholas Bishop, Anisoara Calinescu, Michael Wooldridge",
        "summary": "Evaluating the safety of frontier AI systems is an increasingly important concern, helping to measure the capabilities of such models and identify risks before deployment. However, it has been recognised that if AI agents are aware that they are being evaluated, such agents may deliberately hide dangerous capabilities or intentionally demonstrate suboptimal performance in safety-related tasks in order to be released and to avoid being deactivated or retrained. Such strategic deception - often known as \"sandbagging\" - threatens to undermine the integrity of safety evaluations. For this reason, it is of value to identify methods that enable us to distinguish behavioural patterns that demonstrate a true lack of capability from behavioural patterns that are consistent with sandbagging. In this paper, we develop a simple model of strategic deception in sequential decision-making tasks, inspired by the recently developed survival bandit framework. We demonstrate theoretically that this problem induces sandbagging behaviour in optimal rational agents, and construct a statistical test to distinguish between sandbagging and incompetence from sequences of test scores. In simulation experiments, we investigate the reliability of this test in allowing us to distinguish between such behaviours in bandit models. This work aims to establish a potential avenue for developing robust statistical procedures for use in the science of frontier model evaluations.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.884743"
    },
    {
        "index": "#40",
        "title": "Beyond Linear Probes: Dynamic Safety Monitoring for Language Models",
        "link": "/arxiv/2509.26238",
        "arxiv_id": "2509.26238",
        "authors": "James Oldfield, Philip Torr, Ioannis Patras, Adel Bibi, Fazl Barez",
        "summary": "Monitoring large language models' (LLMs) activations is an effective way to detect harmful requests before they lead to unsafe outputs. However, traditional safety monitors often require the same amount of compute for every query. This creates a trade-off: expensive monitors waste resources on easy inputs, while cheap ones risk missing subtle cases. We argue that safety monitors should be flexible--costs should rise only when inputs are difficult to assess, or when more compute is available. To achieve this, we introduce Truncated Polynomial Classifiers (TPCs), a natural extension of linear probes for dynamic activation monitoring. Our key insight is that polynomials can be trained and evaluated progressively, term-by-term. At test-time, one can early-stop for lightweight monitoring, or use more terms for stronger guardrails when needed. TPCs provide two modes of use. First, as a safety dial: by evaluating more terms, developers and regulators can \"buy\" stronger guardrails from the same model. Second, as an adaptive cascade: clear cases exit early after low-order checks, and higher-order guardrails are evaluated only for ambiguous inputs, reducing overall monitoring costs. On two large-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with up to 30B parameters, we show that TPCs compete with or outperform MLP-based probe baselines of the same size, all the while being more interpretable than their black-box counterparts. Our code is available at http://github.com/james-oldfield/tpc.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.885290"
    },
    {
        "index": "#41",
        "title": "Machine Learning Detection of Lithium Plating in Lithium-ion Cells: A Gaussian Process Approach",
        "link": "/arxiv/2509.26234",
        "arxiv_id": "2509.26234",
        "authors": "Ayush Patnaik, Adam B Zufall, Stephen K Robinson, Xinfan Lin",
        "summary": "Lithium plating during fast charging is a critical degradation mechanism that accelerates capacity fade and can trigger catastrophic safety failures. Recent work has identified a distinctive dQ/dV peak above 4.0 V as a reliable signature of plating onset; however, conventional methods for computing dQ/dV rely on finite differencing with filtering, which amplifies sensor noise and introduces bias in peak location. In this paper, we propose a Gaussian Process (GP) framework for lithium plating detection by directly modeling the charge-voltage relationship Q(V) as a stochastic process with calibrated uncertainty. Leveraging the property that derivatives of GPs remain GPs, we infer dQ/dV analytically and probabilistically from the posterior, enabling robust detection without ad hoc smoothing. The framework provides three key benefits: (i) noise-aware inference with hyperparameters learned from data, (ii) closed-form derivatives with credible intervals for uncertainty quantification, and (iii) scalability to online variants suitable for embedded BMS. Experimental validation on Li-ion coin cells across a range of C-rates (0.2C-1C) and temperatures (0-40\\deg C) demonstrates that the GP-based method reliably detects plating peaks under low-temperature, high-rate charging, while correctly reporting no peaks in baseline cases. The concurrence of GP-identified differential peaks, reduced charge throughput, and capacity fade measured via reference performance tests confirms the method's accuracy and robustness, establishing a practical pathway for real-time lithium plating detection.",
        "subjects": "Machine Learning, Systems and Control",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.885831"
    },
    {
        "index": "#43",
        "title": "Marginal Flow: a flexible and efficient framework for density estimation",
        "link": "/arxiv/2509.26221",
        "arxiv_id": "2509.26221",
        "authors": "Marcello Massimo Negri, Jonathan Aellen, Manuel Jahn, AmirEhsan Khorashadizadeh, Volker Roth",
        "summary": "Current density modeling approaches suffer from at least one of the following shortcomings: expensive training, slow inference, approximate likelihood, mode collapse or architectural constraints like bijective mappings. We propose a simple yet powerful framework that overcomes these limitations altogether. We define our model $q_\\theta(x)$ through a parametric distribution $q(x|w)$ with latent parameters $w$. Instead of directly optimizing the latent variables $w$, our idea is to marginalize them out by sampling $w$ from a learnable distribution $q_\\theta(w)$, hence the name Marginal Flow. In order to evaluate the learned density $q_\\theta(x)$ or to sample from it, we only need to draw samples from $q_\\theta(w)$, which makes both operations efficient. The proposed model allows for exact density evaluation and is orders of magnitude faster than competing models both at training and inference. Furthermore, Marginal Flow is a flexible framework: it does not impose any restrictions on the neural network architecture, it enables learning distributions on lower-dimensional manifolds (either known or to be learned), it can be trained efficiently with any objective (e.g. forward and reverse KL divergence), and it easily handles multi-modal targets. We evaluate Marginal Flow extensively on various tasks including synthetic datasets, simulation-based inference, distributions on positive definite matrices and manifold learning in latent spaces of images.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.886959"
    },
    {
        "index": "#44",
        "title": "Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning",
        "link": "/arxiv/2509.26187",
        "arxiv_id": "2509.26187",
        "authors": "Youssef Sabiri, Walid Houmaidi, Aaya Bougrine, Salmane El Mansour Billah",
        "summary": "Ensuring optimal Indoor Environmental Quality (IEQ) is vital for occupant health and productivity, yet it often comes at a high energy cost in conventional Heating, Ventilation, and Air Conditioning (HVAC) systems. This paper proposes a deep learning driven approach to proactively manage IEQ parameters specifically CO2 concentration, temperature, and humidity while balancing building energy efficiency. Leveraging the ROBOD dataset collected from a net-zero energy academic building, we benchmark three architectures--Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and a hybrid Convolutional Neural Network LSTM (CNN-LSTM)--to forecast IEQ variables across various time horizons. Our results show that GRU achieves the best short-term prediction accuracy with lower computational overhead, whereas CNN-LSTM excels in extracting dominant features for extended forecasting windows. Meanwhile, LSTM offers robust long-range temporal modeling. The comparative analysis highlights that prediction reliability depends on data resolution, sensor placement, and fluctuating occupancy conditions. These findings provide actionable insights for intelligent Building Management Systems (BMS) to implement predictive HVAC control, thereby reducing energy consumption and enhancing occupant comfort in real-world building operations.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.892650"
    },
    {
        "index": "#45",
        "title": "PDE Solvers Should Be Local: Fast, Stable Rollouts with Learned Local Stencils",
        "link": "/arxiv/2509.26186",
        "arxiv_id": "2509.26186",
        "authors": "Chun-Wun Cheng, Bin Dong, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero",
        "summary": "Neural operator models for solving partial differential equations (PDEs) often rely on global mixing mechanisms-such as spectral convolutions or attention-which tend to oversmooth sharp local dynamics and introduce high computational cost. We present FINO, a finite-difference-inspired neural architecture that enforces strict locality while retaining multiscale representational power. FINO replaces fixed finite-difference stencil coefficients with learnable convolutional kernels and evolves states via an explicit, learnable time-stepping scheme. A central Local Operator Block leverage a differential stencil layer, a gating mask, and a linear fuse step to construct adaptive derivative-like local features that propagate forward in time. Embedded in an encoder-decoder with a bottleneck, FINO captures fine-grained local structures while preserving interpretability. We establish (i) a composition error bound linking one-step approximation error to stable long-horizon rollouts under a Lipschitz condition, and (ii) a universal approximation theorem for discrete time-stepped PDE dynamics. (iii) Across six benchmarks and a climate modelling task, FINO achieves up to 44\\% lower error and up to around 2\\times speedups over state-of-the-art operator-learning baselines, demonstrating that strict locality with learnable time-stepping yields an accurate and scalable foundation for neural PDE solvers.",
        "subjects": "Machine Learning, Numerical Analysis",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.893158"
    },
    {
        "index": "#46",
        "title": "Neighbor-aware informal settlement mapping with graph convolutional networks",
        "link": "/arxiv/2509.26171",
        "arxiv_id": "2509.26171",
        "authors": "Thomas Hallopeau, Joris Guérin, Laurent Demagistri, Christovam Barcellos, Nadine Dessay",
        "summary": "Mapping informal settlements is crucial for addressing challenges related to urban planning, public health, and infrastructure in rapidly growing cities. Geospatial machine learning has emerged as a key tool for detecting and mapping these areas from remote sensing data. However, existing approaches often treat spatial units independently, neglecting the relational structure of the urban fabric. We propose a graph-based framework that explicitly incorporates local geographical context into the classification process. Each spatial unit (cell) is embedded in a graph structure along with its adjacent neighbors, and a lightweight Graph Convolutional Network (GCN) is trained to classify whether the central cell belongs to an informal settlement. Experiments are conducted on a case study in Rio de Janeiro using spatial cross-validation across five distinct zones, ensuring robustness and generalizability across heterogeneous urban landscapes. Our method outperforms standard baselines, improving Kappa coefficient by 17 points over individual cell classification. We also show that graph-based modeling surpasses simple feature concatenation of neighboring cells, demonstrating the benefit of encoding spatial structure for urban scene understanding.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.893718"
    },
    {
        "index": "#47",
        "title": "Alignment-Aware Decoding",
        "link": "/arxiv/2509.26169",
        "arxiv_id": "2509.26169",
        "authors": "Frédéric Berdoz, Luca A. Lanzendörfer, René Caky, Roger Wattenhofer",
        "summary": "Alignment of large language models remains a central challenge in natural language processing. Preference optimization has emerged as a popular and effective method for improving alignment, typically through training-time or prompt-based interventions. In this paper, we introduce alignment-aware decoding (AAD), a method to enhance model alignment directly at inference. Theoretically, AAD can be interpreted as implicit reward optimization, yet it requires no specialized training beyond the standard DPO setup. Empirically, AAD consistently outperforms strong baselines across diverse alignment benchmarks and model scales. Moreover, in data-constrained settings, AAD can produce high-quality synthetic data to improve alignment under standard decoding, providing a practical solution when labeled data is limited.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.894233"
    },
    {
        "index": "#48",
        "title": "Leveraging AI modelling for FDS with Simvue: monitor and optimise for more sustainable simulations",
        "link": "/arxiv/2509.26139",
        "arxiv_id": "2509.26139",
        "authors": "James Panayis, Matt Field, Vignesh Gopakumar, Andrew Lahiff, Kristian Zarebski, Aby Abraham, Jonathan L. Hodges",
        "summary": "There is high demand on fire simulations, in both scale and quantity. We present a multi-pronged approach to improving the time and energy required to meet these demands. We show the ability of a custom machine learning surrogate model to predict the dynamics of heat propagation orders of magnitude faster than state-of-the-art CFD software for this application. We also demonstrate how a guided optimisation procedure can decrease the number of simulations required to meet an objective; using lightweight models to decide which simulations to run, we see a tenfold reduction when locating the most dangerous location for a fire to occur within a building based on the impact of smoke on visibility. Finally we present a framework and product, Simvue, through which we access these tools along with a host of automatic organisational and tracking features which enables future reuse of data and more savings through better management of simulations and combating redundancy.",
        "subjects": "Machine Learning, Artificial Intelligence, Computational Physics",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.894838"
    },
    {
        "index": "#49",
        "title": "Accelerating Transformers in Online RL",
        "link": "/arxiv/2509.26137",
        "arxiv_id": "2509.26137",
        "authors": "Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov",
        "summary": "The appearance of transformer-based models in Reinforcement Learning (RL) has expanded the horizons of possibilities in robotics tasks, but it has simultaneously brought a wide range of challenges during its implementation, especially in model-free online RL. Some of the existing learning algorithms cannot be easily implemented with transformer-based models due to the instability of the latter. In this paper, we propose a method that uses the Accelerator policy as a transformer's trainer. The Accelerator, a simpler and more stable model, interacts with the environment independently while simultaneously training the transformer through behavior cloning during the first stage of the proposed algorithm. In the second stage, the pretrained transformer starts to interact with the environment in a fully online setting. As a result, this model-free algorithm accelerates the transformer in terms of its performance and helps it to train online in a more stable and faster way. By conducting experiments on both state-based and image-based ManiSkill environments, as well as on MuJoCo tasks in MDP and POMDP settings, we show that applying our algorithm not only enables stable training of transformers but also reduces training time on image-based environments by up to a factor of two. Moreover, it decreases the required replay buffer size in off-policy methods to 10-20 thousand, which significantly lowers the overall computational demands.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.895332"
    },
    {
        "index": "#50",
        "title": "Domain-Aware Hyperdimensional Computing for Edge Smart Manufacturing",
        "link": "/arxiv/2509.26131",
        "arxiv_id": "2509.26131",
        "authors": "Fardin Jalil Piran, Anandkumar Patel, Rajiv Malhotra, Farhad Imani",
        "summary": "Smart manufacturing requires on-device intelligence that meets strict latency and energy budgets. HyperDimensional Computing (HDC) offers a lightweight alternative by encoding data as high-dimensional hypervectors and computing with simple operations. Prior studies often assume that the qualitative relation between HDC hyperparameters and performance is stable across applications. Our analysis of two representative tasks, signal-based quality monitoring in Computer Numerical Control (CNC) machining and image-based defect detection in Laser Powder Bed Fusion (LPBF), shows that this assumption does not hold. We map how encoder type, projection variance, hypervector dimensionality, and data regime shape accuracy, inference latency, training time, and training energy. A formal complexity model explains predictable trends in encoding and similarity computation and reveals nonmonotonic interactions with retraining that preclude a closed-form optimum. Empirically, signals favor nonlinear Random Fourier Features with more exclusive encodings and saturate in accuracy beyond moderate dimensionality. Images favor linear Random Projection, achieve high accuracy with small dimensionality, and depend more on sample count than on dimensionality. Guided by these insights, we tune HDC under multiobjective constraints that reflect edge deployment and obtain models that match or exceed the accuracy of state-of-the-art deep learning and Transformer models while delivering at least 6x faster inference and more than 40x lower training energy. These results demonstrate that domain-aware HDC encoding is necessary and that tuned HDC offers a practical, scalable path to real-time industrial AI on constrained hardware. Future work will enable adaptive encoder and hyperparameter selection, expand evaluation to additional manufacturing modalities, and validate on low-power accelerators.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.895857"
    },
    {
        "index": "#51",
        "title": "UncertainGen: Uncertainty-Aware Representations of DNA Sequences for Metagenomic Binning",
        "link": "/arxiv/2509.26116",
        "arxiv_id": "2509.26116",
        "authors": "Abdulkadir Celikkanat, Andres R. Masegosa, Mads Albertsen, Thomas D. Nielsen",
        "summary": "Metagenomic binning aims to cluster DNA fragments from mixed microbial samples into their respective genomes, a critical step for downstream analyses of microbial communities. Existing methods rely on deterministic representations, such as k-mer profiles or embeddings from large language models, which fail to capture the uncertainty inherent in DNA sequences arising from inter-species DNA sharing and from fragments with highly similar representations. We present the first probabilistic embedding approach, UncertainGen, for metagenomic binning, representing each DNA fragment as a probability distribution in latent space. Our approach naturally models sequence-level uncertainty, and we provide theoretical guarantees on embedding distinguishability. This probabilistic embedding framework expands the feasible latent space by introducing a data-adaptive metric, which in turn enables more flexible separation of bins/clusters. Experiments on real metagenomic datasets demonstrate the improvements over deterministic k-mer and LLM-based embeddings for the binning task by offering a scalable and lightweight solution for large-scale metagenomic analysis.",
        "subjects": "Machine Learning, Computational Engineering, Finance, and Science",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.896390"
    },
    {
        "index": "#52",
        "title": "Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models",
        "link": "/arxiv/2509.26114",
        "arxiv_id": "2509.26114",
        "authors": "Jaesung R. Park, Junsu Kim, Gyeongman Kim, Jinyoung Jo, Sean Choi, Jaewoong Cho, Ernest K. Ryu",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently emerged as the leading approach for enhancing the reasoning capabilities of large language models (LLMs). However, RLVR is prone to entropy collapse, where the LLM quickly converges to a near-deterministic form, hindering exploration and progress during prolonged RL training. In this work, we reveal that the clipping mechanism in PPO and GRPO induces biases on entropy. Through theoretical and empirical analyses, we show that clip-low increases entropy, while clip-high decreases it. Further, under standard clipping parameters, the effect of clip-high dominates, resulting in an overall entropy reduction even when purely random rewards are provided to the RL algorithm. Our findings highlight an overlooked confounding factor in RLVR: independent of the reward signal, the clipping mechanism influences entropy, which in turn affects the reasoning behavior. Furthermore, our analysis demonstrates that clipping can be deliberately used to control entropy. Specifically, with a more aggressive clip-low value, one can increase entropy, promote exploration, and ultimately prevent entropy collapse in RLVR training.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.896935"
    },
    {
        "index": "#53",
        "title": "Real-time Noise Detection and Classification in Single-Channel EEG: A Lightweight Machine Learning Approach for EMG, White Noise, and EOG Artifacts",
        "link": "/arxiv/2509.26058",
        "arxiv_id": "2509.26058",
        "authors": "Hossein Enshaei, Pariya Jebreili, Sayed Mahmoud Sakahei",
        "summary": "Electroencephalogram (EEG) artifact detection in real-world settings faces significant challenges such as computational inefficiency in multi-channel methods, poor robustness to simultaneous noise, and trade-offs between accuracy and complexity in deep learning models. We propose a hybrid spectral-temporal framework for real-time detection and classification of ocular (EOG), muscular (EMG), and white noise artifacts in single-channel EEG. This method, in contrast to other approaches, combines time-domain low-pass filtering (targeting low-frequency EOG) and frequency-domain power spectral density (PSD) analysis (capturing broad-spectrum EMG), followed by PCA-optimized feature fusion to minimize redundancy while preserving discriminative information. This feature engineering strategy allows a lightweight multi-layer perceptron (MLP) architecture to outperform advanced CNNs and RNNs by achieving 99% accuracy at low SNRs (SNR -7) dB and >90% accuracy in moderate noise (SNR 4 dB). Additionally, this framework addresses the unexplored problem of simultaneous multi-source contamination(EMG+EOG+white noise), where it maintains 96% classification accuracy despite overlapping artifacts. With 30-second training times (97% faster than CNNs) and robust performance across SNR levels, this framework bridges the gap between clinical applicability and computational efficiency, which enables real-time use in wearable brain-computer interfaces. This work also challenges the ubiquitous dependence on model depth for EEG artifact detection by demonstrating that domain-informed feature fusion surpasses complex architecture in noisy scenarios.",
        "subjects": "Machine Learning, Artificial Intelligence, Neural and Evolutionary Computing",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.902616"
    },
    {
        "index": "#55",
        "title": "Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification",
        "link": "/arxiv/2509.26032",
        "arxiv_id": "2509.26032",
        "authors": "Xiaobao Wang, Ruoxiao Sun, Yujun Zhang, Bingdao Feng, Dongxiao He, Luzhi Wang, Di Jin",
        "summary": "Graph Neural Networks (GNNs) have demonstrated strong performance across tasks such as node classification, link prediction, and graph classification, but remain vulnerable to backdoor attacks that implant imperceptible triggers during training to control predictions. While node-level attacks exploit local message passing, graph-level attacks face the harder challenge of manipulating global representations while maintaining stealth. We identify two main sources of anomaly in existing graph classification backdoor methods: structural deviation from rare subgraph triggers and semantic deviation caused by label flipping, both of which make poisoned graphs easily detectable by anomaly detection models. To address this, we propose DPSBA, a clean-label backdoor framework that learns in-distribution triggers via adversarial training guided by anomaly-aware discriminators. DPSBA effectively suppresses both structural and semantic anomalies, achieving high attack success while significantly improving stealth. Extensive experiments on real-world datasets validate that DPSBA achieves a superior balance between effectiveness and detectability compared to state-of-the-art baselines.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.903805"
    },
    {
        "index": "#56",
        "title": "Muon Outperforms Adam in Tail-End Associative Memory Learning",
        "link": "/arxiv/2509.26030",
        "arxiv_id": "2509.26030",
        "authors": "Shuche Wang, Fengzhuo Zhang, Jiaxiang Li, Cunxiao Du, Chao Du, Tianyu Pang, Zhuoran Yang, Mingyi Hong, Vincent Y. F. Tan",
        "summary": "The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon's superiority. Motivated by this associative memory view, we then explain Muon's superiority on real-world corpora, which are intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon's core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.904419"
    },
    {
        "index": "#58",
        "title": "Indirect Attention: Turning Context Misalignment into a Feature",
        "link": "/arxiv/2509.26015",
        "arxiv_id": "2509.26015",
        "authors": "Bissmella Bahaduri, Hicham Talaoubrid, Fangchen Feng, Zuheng Ming, Anissa Mokraoui",
        "summary": "The attention mechanism has become a cornerstone of modern deep learning architectures, where keys and values are typically derived from the same underlying sequence or representation. This work explores a less conventional scenario, when keys and values originate from different sequences or modalities. Specifically, we first analyze the attention mechanism's behavior under noisy value features, establishing a critical noise threshold beyond which signal degradation becomes significant. Furthermore, we model context (key, value) misalignment as an effective form of structured noise within the value features, demonstrating that the noise induced by such misalignment can substantially exceed this critical threshold, thereby compromising standard attention's efficacy. Motivated by this, we introduce Indirect Attention, a modified attention mechanism that infers relevance indirectly in scenarios with misaligned context. We evaluate the performance of Indirect Attention across a range of synthetic tasks and real world applications, showcasing its superior ability to handle misalignment.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.905530"
    },
    {
        "index": "#59",
        "title": "Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access",
        "link": "/arxiv/2509.26000",
        "arxiv_id": "2509.26000",
        "authors": "Daniel Ebi, Gaspard Lambrechts, Damien Ernst, Klemens Böhm",
        "summary": "Reinforcement learning in partially observable environments requires agents to act under uncertainty from noisy, incomplete observations. Asymmetric actor-critic methods leverage privileged information during training to improve learning under these conditions. However, existing approaches typically assume full-state access during training. In this work, we challenge this assumption by proposing a novel actor-critic framework, called informed asymmetric actor-critic, that enables conditioning the critic on arbitrary privileged signals without requiring access to the full state. We show that policy gradients remain unbiased under this formulation, extending the theoretical foundation of asymmetric methods to the more general case of privileged partial information. To quantify the impact of such signals, we propose informativeness measures based on kernel methods and return prediction error, providing practical tools for evaluating training-time signals. We validate our approach empirically on benchmark navigation tasks and synthetic partially observable environments, showing that our informed asymmetric method improves learning efficiency and value estimation when informative privileged inputs are available. Our findings challenge the necessity of full-state access and open new directions for designing asymmetric reinforcement learning methods that are both practical and theoretically sound.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.906053"
    },
    {
        "index": "#61",
        "title": "Exact Solutions to the Quantum Schrödinger Bridge Problem",
        "link": "/arxiv/2509.25980",
        "arxiv_id": "2509.25980",
        "authors": "Mykola Bordyuh, Djork-Arné Clevert, Marco Bertolini",
        "summary": "The Quantum Schrödinger Bridge Problem (QSBP) describes the evolution of a stochastic process between two arbitrary probability distributions, where the dynamics are governed by the Schrödinger equation rather than by the traditional real-valued wave equation. Although the QSBP is known in the mathematical literature, we formulate it here from a Lagrangian perspective and derive its main features in a way that is particularly suited to generative modeling. We show that the resulting evolution equations involve the so-called Bohm (quantum) potential, representing a notion of non-locality in the stochastic process. This distinguishes the QSBP from classical stochastic dynamics and reflects a key characteristic typical of quantum mechanical systems. In this work, we derive exact closed-form solutions for the QSBP between Gaussian distributions. Our derivation is based on solving the Fokker-Planck Equation (FPE) and the Hamilton-Jacobi Equation (HJE) arising from the Lagrangian formulation of dynamical Optimal Transport. We find that, similar to the classical Schrödinger Bridge Problem, the solution to the QSBP between Gaussians is again a Gaussian process; however, the evolution of the covariance differs due to quantum effects. Leveraging these explicit solutions, we present a modified algorithm based on a Gaussian Mixture Model framework, and demonstrate its effectiveness across several experimental settings, including single-cell evolution data, image generation, molecular translation and applications in Mean-Field Games.",
        "subjects": "Machine Learning, Mathematical Physics, Probability, Quantum Physics",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.907128"
    },
    {
        "index": "#62",
        "title": "Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier",
        "link": "/arxiv/2509.25979",
        "arxiv_id": "2509.25979",
        "authors": "Gaojie Jin, Xinping Yi, Xiaowei Huang",
        "summary": "Within the PAC-Bayesian framework, the Gibbs classifier (defined on a posterior $Q$) and the corresponding $Q$-weighted majority vote classifier are commonly used to analyze the generalization performance. However, there exists a notable lack in theoretical research exploring the certified robustness of majority vote classifier and its interplay with generalization. In this study, we develop a generalization error bound that possesses a certified robust radius for the smoothed majority vote classifier (i.e., the $Q$-weighted majority vote classifier with smoothed inputs); In other words, the generalization bound holds under any data perturbation within the certified robust radius. As a byproduct, we find that the underpinnings of both the generalization bound and the certified robust radius draw, in part, upon weight spectral norm, which thereby inspires the adoption of spectral regularization in smooth training to boost certified robustness. Utilizing the dimension-independent property of spherical Gaussian inputs in smooth training, we propose a novel and inexpensive spectral regularizer to enhance the smoothed majority vote classifier. In addition to the theoretical contribution, a set of empirical results is provided to substantiate the effectiveness of our proposed method.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.912773"
    },
    {
        "index": "#63",
        "title": "Data-Free Continual Learning of Server Models in Model-Heterogeneous Federated learning",
        "link": "/arxiv/2509.25977",
        "arxiv_id": "2509.25977",
        "authors": "Xiao Zhang, Zengzhe Chen, Yuan Yuan, Yifei Zou, Fuzhen Zhuang, Wenyu Jiao, Yuke Wang, Dongxiao Yu",
        "summary": "Federated learning (FL) is a distributed learning paradigm across multiple entities while preserving data privacy. However, with the continuous emergence of new data and increasing model diversity, traditional federated learning faces significant challenges, including inherent issues of data heterogeneity, model heterogeneity and catastrophic forgetting, along with new challenge of knowledge misalignment. In this study, we introduce FedDCL, a novel framework designed to enable data-free continual learning of the server model in a model-heterogeneous federated setting. We leverage pre-trained diffusion models to extract lightweight class-specific prototypes, which confer a threefold data-free advantage, enabling: (1) generation of synthetic data for the current task to augment training and counteract non-IID data distributions; (2) exemplar-free generative replay for retaining knowledge from previous tasks; and (3) data-free dynamic knowledge transfer from heterogeneous clients to the server. Experimental results on various datasets demonstrate the effectiveness of FedDCL, showcasing its potential to enhance the generalizability and practical applicability of federated learning in dynamic settings.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.913376"
    },
    {
        "index": "#64",
        "title": "Reevaluating Convolutional Neural Networks for Spectral Analysis: A Focus on Raman Spectroscopy",
        "link": "/arxiv/2509.25964",
        "arxiv_id": "2509.25964",
        "authors": "Deniz Soysal, Xabier García-Andrade, Laura E. Rodriguez, Pablo Sobron, Laura M. Barge, Renaud Detry",
        "summary": "Autonomous Raman instruments on Mars rovers, deep-sea landers, and field robots must interpret raw spectra distorted by fluorescence baselines, peak shifts, and limited ground-truth labels. Using curated subsets of the RRUFF database, we evaluate one-dimensional convolutional neural networks (CNNs) and report four advances: (i) Baseline-independent classification: compact CNNs surpass $k$-nearest-neighbors and support-vector machines on handcrafted features, removing background-correction and peak-picking stages while ensuring reproducibility through released data splits and scripts. (ii) Pooling-controlled robustness: tuning a single pooling parameter accommodates Raman shifts up to $30 \\,\\mathrm{cm}^{-1}$, balancing translational invariance with spectral resolution. (iii) Label-efficient learning: semi-supervised generative adversarial networks and contrastive pretraining raise accuracy by up to $11\\%$ with only $10\\%$ labels, valuable for autonomous deployments with scarce annotation. (iv) Constant-time adaptation: freezing the CNN backbone and retraining only the softmax layer transfers models to unseen minerals at $\\mathcal{O}(1)$ cost, outperforming Siamese networks on resource-limited processors. This workflow, which involves training on raw spectra, tuning pooling, adding semi-supervision when labels are scarce, and fine-tuning lightly for new targets, provides a practical path toward robust, low-footprint Raman classification in autonomous exploration.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.913941"
    },
    {
        "index": "#65",
        "title": "AIM: Adaptive Intervention for Deep Multi-task Learning of Molecular Properties",
        "link": "/arxiv/2509.25955",
        "arxiv_id": "2509.25955",
        "authors": "Mason Minot, Gisbert Schneider",
        "summary": "Simultaneously optimizing multiple, frequently conflicting, molecular properties is a key bottleneck in the development of novel therapeutics. Although a promising approach, the efficacy of multi-task learning is often compromised by destructive gradient interference, especially in the data-scarce regimes common to drug discovery. To address this, we propose AIM, an optimization framework that learns a dynamic policy to mediate gradient conflicts. The policy is trained jointly with the main network using a novel augmented objective composed of dense, differentiable regularizers. This objective guides the policy to produce updates that are geometrically stable and dynamically efficient, prioritizing progress on the most challenging tasks. We demonstrate that AIM achieves statistically significant improvements over multi-task baselines on subsets of the QM9 and targeted protein degraders benchmarks, with its advantage being most pronounced in data-scarce regimes. Beyond performance, AIM's key contribution is its interpretability; the learned policy matrix serves as a diagnostic tool for analyzing inter-task relationships. This combination of data-efficient performance and diagnostic insight highlights the potential of adaptive optimizers to accelerate scientific discovery by creating more robust and insightful models for multi-property molecular design.",
        "subjects": "Machine Learning, Artificial Intelligence, Chemical Physics",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.914466"
    },
    {
        "index": "#66",
        "title": "From MNIST to ImageNet: Understanding the Scalability Boundaries of Differentiable Logic Gate Networks",
        "link": "/arxiv/2509.25933",
        "arxiv_id": "2509.25933",
        "authors": "Sven Brändle, Till Aczel, Andreas Plesner, Roger Wattenhofer",
        "summary": "Differentiable Logic Gate Networks (DLGNs) are a very fast and energy-efficient alternative to conventional feed-forward networks. With learnable combinations of logical gates, DLGNs enable fast inference by hardware-friendly execution. Since the concept of DLGNs has only recently gained attention, these networks are still in their developmental infancy, including the design and scalability of their output layer. To date, this architecture has primarily been tested on datasets with up to ten classes. This work examines the behavior of DLGNs on large multi-class datasets. We investigate its general expressiveness, its scalability, and evaluate alternative output strategies. Using both synthetic and real-world datasets, we provide key insights into the importance of temperature tuning and its impact on output layer performance. We evaluate conditions under which the Group-Sum layer performs well and how it can be applied to large-scale classification of up to 2000 classes.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.915020"
    },
    {
        "index": "#67",
        "title": "ReNF: Rethinking the Design Space of Neural Long-Term Time Series Forecasters",
        "link": "/arxiv/2509.25914",
        "arxiv_id": "2509.25914",
        "authors": "Yihang Lu, Xianwei Meng, Enhong Chen",
        "summary": "Neural Forecasters (NFs) are a cornerstone of Long-term Time Series Forecasting (LTSF). However, progress has been hampered by an overemphasis on architectural complexity at the expense of fundamental forecasting principles. In this work, we return to first principles to redesign the LTSF paradigm. We begin by introducing a Multiple Neural Forecasting Theorem that provides a theoretical basis for our approach. We propose Boosted Direct Output (BDO), a novel forecasting strategy that synergistically combines the advantages of both Auto-Regressive (AR) and Direct Output (DO). In addition, we stabilize the learning process by smoothly tracking the model's parameters. Extensive experiments show that these principled improvements enable a simple MLP to achieve state-of-the-art performance, outperforming recent, complex models in nearly all cases, without any specific considerations in the area. Finally, we empirically verify our theorem, establishing a dynamic performance bound and identifying promising directions for future research. The code for review is available at: .",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.915504"
    },
    {
        "index": "#68",
        "title": "Federated Learning with Enhanced Privacy via Model Splitting and Random Client Participation",
        "link": "/arxiv/2509.25906",
        "arxiv_id": "2509.25906",
        "authors": "Yiwei Li, Shuai Wang, Zhuojun Tian, Xiuhua Wang, Shijian Su",
        "summary": "Federated Learning (FL) often adopts differential privacy (DP) to protect client data, but the added noise required for privacy guarantees can substantially degrade model accuracy. To resolve this challenge, we propose model-splitting privacy-amplified federated learning (MS-PAFL), a novel framework that combines structural model splitting with statistical privacy amplification. In this framework, each client's model is partitioned into a private submodel, retained locally, and a public submodel, shared for global aggregation. The calibrated Gaussian noise is injected only into the public submodel, thereby confining its adverse impact while preserving the utility of the local model. We further present a rigorous theoretical analysis that characterizes the joint privacy amplification achieved through random client participation and local data subsampling under this architecture. The analysis provides tight bounds on both single-round and total privacy loss, demonstrating that MS-PAFL significantly reduces the noise necessary to satisfy a target privacy protection level. Extensive experiments validate our theoretical findings, showing that MS-PAFL consistently attains a superior privacy-utility trade-off and enables the training of highly accurate models under strong privacy guarantees.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.916044"
    },
    {
        "index": "#69",
        "title": "Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space",
        "link": "/arxiv/2509.25876",
        "arxiv_id": "2509.25876",
        "authors": "Xinyu Zhang, Aishik Deb, Klaus Mueller",
        "summary": "Policy-gradient methods such as Proximal Policy Optimization (PPO) are typically updated along a single stochastic gradient direction, leaving the rich local structure of the parameter space unexplored. Previous work has shown that the surrogate gradient is often poorly correlated with the true reward landscape. Building on this insight, we visualize the parameter space spanned by policy checkpoints within an iteration and reveal that higher performing solutions often lie in nearby unexplored regions. To exploit this opportunity, we introduce ExploRLer, a pluggable pipeline that seamlessly integrates with on-policy algorithms such as PPO and TRPO, systematically probing the unexplored neighborhoods of surrogate on-policy gradient updates. Without increasing the number of gradient updates, ExploRLer achieves significant improvements over baselines in complex continuous control environments. Our results demonstrate that iteration-level exploration provides a practical and effective way to strengthen on-policy reinforcement learning and offer a fresh perspective on the limitations of the surrogate objective.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.916538"
    },
    {
        "index": "#70",
        "title": "RL-Guided Data Selection for Language Model Finetuning",
        "link": "/arxiv/2509.25850",
        "arxiv_id": "2509.25850",
        "authors": "Animesh Jha, Harshit Gupta, Ananjan Nandi",
        "summary": "Data selection for finetuning Large Language Models (LLMs) can be framed as a budget-constrained optimization problem: maximizing a model's downstream performance under a strict training data budget. Solving this problem is generally intractable, and existing approximate approaches are pretraining-oriented and transfer poorly to the fine-tuning setting. We reformulate this problem as a tractable Markov Decision Process (MDP) and train agents using various Reinforcement Learning (RL) methods to learn optimal data selection policies, guided by an efficient, proxy-model-based reward signal. Across four datasets, training on a $5\\%$ subset selected by our approach matches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy points, while cutting wall-clock training time by up to $2 \\times$, highlighting the promise of RL-guided data selection.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.917009"
    },
    {
        "index": "#72",
        "title": "S$^2$FS: Spatially-Aware Separability-Driven Feature Selection in Fuzzy Decision Systems",
        "link": "/arxiv/2509.25841",
        "arxiv_id": "2509.25841",
        "authors": "Suping Xu, Chuyi Dai, Ye Liu, Lin Shang, Xibei Yang, Witold Pedrycz",
        "summary": "Feature selection is crucial for fuzzy decision systems (FDSs), as it identifies informative features and eliminates rule redundancy, thereby enhancing predictive performance and interpretability. Most existing methods either fail to directly align evaluation criteria with learning performance or rely solely on non-directional Euclidean distances to capture relationships among decision classes, which limits their ability to clarify decision boundaries. However, the spatial distribution of instances has a potential impact on the clarity of such boundaries. Motivated by this, we propose Spatially-aware Separability-driven Feature Selection (S$^2$FS), a novel framework for FDSs guided by a spatially-aware separability criterion. This criterion jointly considers within-class compactness and between-class separation by integrating scalar-distances with spatial directional information, providing a more comprehensive characterization of class structures. S$^2$FS employs a forward greedy strategy to iteratively select the most discriminative features. Extensive experiments on ten real-world datasets demonstrate that S$^2$FS consistently outperforms eight state-of-the-art feature selection algorithms in both classification accuracy and clustering performance, while feature visualizations further confirm the interpretability of the selected features.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.923297"
    },
    {
        "index": "#73",
        "title": "Distillation of Large Language Models via Concrete Score Matching",
        "link": "/arxiv/2509.25837",
        "arxiv_id": "2509.25837",
        "authors": "Yeongmin Kim, Donghyeok Shin, Mina Kang, Byeonghu Na, Il-Chul Moon",
        "summary": "Large language models (LLMs) deliver remarkable performance but are costly to deploy, motivating knowledge distillation (KD) for efficient inference. Existing KD objectives typically match student and teacher probabilities via softmax, which blurs valuable logit information. While direct logit distillation (DLD) mitigates softmax smoothing, it fails to account for logit shift invariance, thereby restricting the solution space. We propose Concrete Score Distillation (CSD), a discrete score-matching objective that overcomes both softmax-induced smoothing and restrictions on the optimal solution set. We resolve the training instability and quadratic complexity of discrete score-matching in autoregressive LLMs, and the resulting CSD objective aligns relative logit differences across all vocabulary pairs between student and teacher with flexible weighting. We provide both mode-seeking and mode-covering instances within our framework and evaluate CSD on task-agnostic instruction-following and task-specific distillation using GPT-2-1.5B, OpenLLaMA-7B, and GEMMA-7B-IT. Experiments show that CSD consistently surpasses recent KD objectives, achieves favorable fidelity-diversity trade-offs, and yields complementary gains when combined with on-policy techniques, demonstrating its scalability and effectiveness for LLM distillation.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.923854"
    },
    {
        "index": "#74",
        "title": "MIDAS: Misalignment-based Data Augmentation Strategy for Imbalanced Multimodal Learning",
        "link": "/arxiv/2509.25831",
        "arxiv_id": "2509.25831",
        "authors": "Seong-Hyeon Hwang, Soyoung Choi, Steven Euijong Whang",
        "summary": "Multimodal models often over-rely on dominant modalities, failing to achieve optimal performance. While prior work focuses on modifying training objectives or optimization procedures, data-centric solutions remain underexplored. We propose MIDAS, a novel data augmentation strategy that generates misaligned samples with semantically inconsistent cross-modal information, labeled using unimodal confidence scores to compel learning from contradictory signals. However, this confidence-based labeling can still favor the more confident modality. To address this within our misaligned samples, we introduce weak-modality weighting, which dynamically increases the loss weight of the least confident modality, thereby helping the model fully utilize weaker modality. Furthermore, when misaligned features exhibit greater similarity to the aligned features, these misaligned samples pose a greater challenge, thereby enabling the model to better distinguish between classes. To leverage this, we propose hard-sample weighting, which prioritizes such semantically ambiguous misaligned samples. Experiments on multiple multimodal classification benchmarks demonstrate that MIDAS significantly outperforms related baselines in addressing modality imbalance.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.924333"
    },
    {
        "index": "#75",
        "title": "Kairos: Towards Adaptive and Generalizable Time Series Foundation Models",
        "link": "/arxiv/2509.25826",
        "arxiv_id": "2509.25826",
        "authors": "Kun Feng, Shaocheng Lan, Yuchen Fang, Wenchao He, Lintao Ma, Xingyu Lu, Kan Ren",
        "summary": "Time series foundation models (TSFMs) have emerged as a powerful paradigm for time series analysis, driven by large-scale pretraining on diverse data corpora. However, time series inherently exhibit heterogeneous information density over time, influenced by system states and signal complexity, presenting significant modeling challenges especially in a zero-shot scenario. Current TSFMs rely on non-adaptive processing pipelines that fail to capture this dynamic nature. For example, common tokenization strategies such as fixed-size patching enforce rigid observational granularity, limiting their ability to adapt to varying information densities. Similarly, conventional positional encodings impose a uniform temporal scale, making it difficult to model diverse periodicities and trends across series. To overcome these limitations, we propose Kairos, a flexible TSFM framework that integrates a dynamic patching tokenizer and an instance-adaptive positional embedding. Kairos adaptively selects tokenization granularity and tailors positional encodings to the unique characteristics of each time series instance. Trained on a large-scale Predictability-Stratified Time Series (PreSTS) corpus comprising over 300 billion time points and adopting a multi-patch prediction strategy in the inference stage, Kairos achieves superior performance with much fewer parameters on two common zero-shot benchmarks, GIFT-Eval and the Time-Series-Library benchmark, consistently outperforming established methods across diverse tasks. The project page is at https://foundation-model-research.github.io/Kairos .",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.924889"
    },
    {
        "index": "#76",
        "title": "Decentralized Asynchronous Multi-player Bandits",
        "link": "/arxiv/2509.25824",
        "arxiv_id": "2509.25824",
        "authors": "Jingqi Fan, Canzhe Zhao, Shuai Li, Siwei Wang",
        "summary": "In recent years, multi-player multi-armed bandits (MP-MAB) have been extensively studied due to their wide applications in cognitive radio networks and Internet of Things systems. While most existing research on MP-MAB focuses on synchronized settings, real-world systems are often decentralized and asynchronous, where players may enter or leave the system at arbitrary times, and do not have a global clock. This decentralized asynchronous setting introduces two major challenges. First, without a global time, players cannot implicitly coordinate their actions through time, making it difficult to avoid collisions. Second, it is important to detect how many players are in the system, but doing so may cost a lot. In this paper, we address the challenges posed by such a fully asynchronous setting in a decentralized environment. We develop a novel algorithm in which players adaptively change between exploration and exploitation. During exploration, players uniformly pull their arms, reducing the probability of collisions and effectively mitigating the first challenge. Meanwhile, players continue pulling arms currently exploited by others with a small probability, enabling them to detect when a player has left, thereby addressing the second challenge. We prove that our algorithm achieves a regret of $\\mathcal{O}(\\sqrt{T \\log T} + {\\log T}/{\\Delta^2})$, where $\\Delta$ is the minimum expected reward gap between any two arms. To the best of our knowledge, this is the first efficient MP-MAB algorithm in the asynchronous and decentralized environment. Extensive experiments further validate the effectiveness and robustness of our algorithm, demonstrating its applicability to real-world scenarios.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.925415"
    },
    {
        "index": "#78",
        "title": "Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse",
        "link": "/arxiv/2509.25808",
        "arxiv_id": "2509.25808",
        "authors": "Yuheng Zhang, Wenlin Yao, Changlong Yu, Yao Liu, Qingyu Yin, Bing Yin, Hyokun Yun, Lihong Li",
        "summary": "Large language models (LLMs) have achieved impressive reasoning performance, with reinforcement learning with verifiable rewards (RLVR) emerging as a standard paradigm for post-training. A representative algorithm, group relative policy optimization (GRPO) (Shao et al., 2024), computes advantages by normalizing outcome rewards within response groups, but suffers from a vanishing advantage issue when all responses in a group receive identical rewards. To address this issue, we propose Adaptive Rollout and Response Reuse Policy Optimization (AR3PO), a sampling efficient RLVR algorithm that introduces two novel techniques: adaptive rollout, which dynamically allocates more responses to difficult prompts while saving computation on easier ones, and response reuse, which leverages previously generated correct responses to provide useful training signals. We compare AR3PO with strong RLVR baselines on multiple representative benchmarks using two different families of base models. Across the 7B and 8B models, AR3PO consistently outperforms GRPO and matches or surpasses DAPO (Yu et al., 2025), reducing rollout cost by up to 4.2x. On the larger 32B model, AR3PO achieves comparable performance to DAPO at similar training steps while maintaining substantially lower rollout cost.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.926629"
    },
    {
        "index": "#79",
        "title": "CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG",
        "link": "/arxiv/2509.25804",
        "arxiv_id": "2509.25804",
        "authors": "Vaskar Chakma, Ju Xiaolin, Heling Cao, Xue Feng, Ji Xiaodong, Pan Haiyan, Gao Zhan",
        "summary": "This study aims to develop and evaluate an ensemble machine learning-based framework for the automatic detection of Wide QRS Complex Tachycardia (WCT) from ECG signals, emphasizing diagnostic accuracy and interpretability using Explainable AI. The proposed system integrates ensemble learning techniques, i.e., an optimized Random Forest known as CardioForest, and models like XGBoost and LightGBM. The models were trained and tested on ECG data from the publicly available MIMIC-IV dataset. The testing was carried out with the assistance of accuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error rate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations) was used to ascertain model explainability and clinical relevance. The CardioForest model performed best on all metrics, achieving a test accuracy of 94.95%, a balanced accuracy of 88.31%, and high precision and recall metrics. SHAP analysis confirmed the model's ability to rank the most relevant ECG features, such as QRS duration, in accordance with clinical intuitions, thereby fostering trust and usability in clinical practice. The findings recognize CardioForest as an extremely dependable and interpretable WCT detection model. Being able to offer accurate predictions and transparency through explainability makes it a valuable tool to help cardiologists make timely and well-informed diagnoses, especially for high-stakes and emergency scenarios.",
        "subjects": "Machine Learning, Artificial Intelligence, Networking and Internet Architecture",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.927225"
    },
    {
        "index": "#80",
        "title": "Characterization and Learning of Causal Graphs with Latent Confounders and Post-treatment Selection from Interventional Data",
        "link": "/arxiv/2509.25800",
        "arxiv_id": "2509.25800",
        "authors": "Gongxu Luo, Loka Li, Guangyi Chen, Haoyue Dai, Kun Zhang",
        "summary": "Interventional causal discovery seeks to identify causal relations by leveraging distributional changes introduced by interventions, even in the presence of latent confounders. Beyond the spurious dependencies induced by latent confounders, we highlight a common yet often overlooked challenge in the problem due to post-treatment selection, in which samples are selectively included in datasets after interventions. This fundamental challenge widely exists in biological studies; for example, in gene expression analysis, both observational and interventional samples are retained only if they meet quality control criteria (e.g., highly active cells). Neglecting post-treatment selection may introduce spurious dependencies and distributional changes under interventions, which can mimic causal responses, thereby distorting causal discovery results and challenging existing causal formulations. To address this, we introduce a novel causal formulation that explicitly models post-treatment selection and reveals how its differential reactions to interventions can distinguish causal relations from selection patterns, allowing us to go beyond traditional equivalence classes toward the underlying true causal structure. We then characterize its Markov properties and propose a Fine-grained Interventional equivalence class, named FI-Markov equivalence, represented by a new graphical diagram, F-PAG. Finally, we develop a provably sound and complete algorithm, F-FCI, to identify causal relations, latent confounders, and post-treatment selection up to $\\mathcal{FI}$-Markov equivalence, using both observational and interventional data. Experimental results on synthetic and real-world datasets demonstrate that our method recovers causal relations despite the presence of both selection and latent confounders.",
        "subjects": "Machine Learning, Methodology",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.927795"
    },
    {
        "index": "#81",
        "title": "From Cheap Geometry to Expensive Physics: Elevating Neural Operators via Latent Shape Pretraining",
        "link": "/arxiv/2509.25788",
        "arxiv_id": "2509.25788",
        "authors": "Zhizhou Zhang, Youjia Wu, Kaixuan Zhang, Yanjia Wang",
        "summary": "Industrial design evaluation often relies on high-fidelity simulations of governing partial differential equations (PDEs). While accurate, these simulations are computationally expensive, making dense exploration of design spaces impractical. Operator learning has emerged as a promising approach to accelerate PDE solution prediction; however, its effectiveness is often limited by the scarcity of labeled physics-based data. At the same time, large numbers of geometry-only candidate designs are readily available but remain largely untapped. We propose a two-stage framework to better exploit this abundant, physics-agnostic resource and improve supervised operator learning under limited labeled data. In Stage 1, we pretrain an autoencoder on a geometry reconstruction task to learn an expressive latent representation without PDE labels. In Stage 2, the neural operator is trained in a standard supervised manner to predict PDE solutions, using the pretrained latent embeddings as inputs instead of raw point clouds. Transformer-based architectures are adopted for both the autoencoder and the neural operator to handle point cloud data and integrate both stages seamlessly. Across four PDE datasets and three state-of-the-art transformer-based neural operators, our approach consistently improves prediction accuracy compared to models trained directly on raw point cloud inputs. These results demonstrate that representations from physics-agnostic pretraining provide a powerful foundation for data-efficient operator learning.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.933474"
    },
    {
        "index": "#82",
        "title": "A Hamiltonian driven Geometric Construction of Neural Networks on the Lognormal Statistical Manifold",
        "link": "/arxiv/2509.25778",
        "arxiv_id": "2509.25778",
        "authors": "Prosper Rosaire Mama Assandje, Teumsa Aboubakar, Dongho Joseph, Takemi Nakamura",
        "summary": "Bridging information geometry with machine learning, this paper presents a method for constructing neural networks intrinsically on statistical manifolds. We demonstrate this approach by formulating a neural network architecture directly on the lognormal statistical manifold. The construction is driven by the Hamiltonian system that is equivalent to the gradient flow on this manifold. First, we define the network's input values using the coordinate system of this Hamiltonian dynamics, naturally embedded in the Poincare disk. The core of our contribution lies in the derivation of the network's components from geometric principles: the rotation component of the synaptic weight matrix is determined by the Lie group action of SU(1,1) on the disk, while the activation function emerges from the symplectic structure of the system. We subsequently obtain the complete weight matrix, including its translation vector, and the resulting output values. This work shows that the lognormal manifold can be seamlessly viewed as a neural manifold, with its geometric properties dictating a unique and interpretable neural network structure. The proposed method offers a new paradigm for building learning systems grounded in the differential geometry of their underlying parameter spaces.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.933986"
    },
    {
        "index": "#83",
        "title": "Online Decision Making with Generative Action Sets",
        "link": "/arxiv/2509.25777",
        "arxiv_id": "2509.25777",
        "authors": "Jianyu Xu, Vidhi Jain, Bryan Wilder, Aarti Singh",
        "summary": "With advances in generative AI, decision-making agents can now dynamically create new actions during online learning, but action generation typically incurs costs that must be balanced against potential benefits. We study an online learning problem where an agent can generate new actions at any time step by paying a one-time cost, with these actions becoming permanently available for future use. The challenge lies in learning the optimal sequence of two-fold decisions: which action to take and when to generate new ones, further complicated by the triangular tradeoffs among exploitation, exploration and $\\textit{creation}$. To solve this problem, we propose a doubly-optimistic algorithm that employs Lower Confidence Bounds (LCB) for action selection and Upper Confidence Bounds (UCB) for action generation. Empirical evaluation on healthcare question-answering datasets demonstrates that our approach achieves favorable generation-quality tradeoffs compared to baseline strategies. From theoretical perspectives, we prove that our algorithm achieves the optimal regret of $O(T^{\\frac{d}{d+2}}d^{\\frac{d}{d+2}} + d\\sqrt{T\\log T})$, providing the first sublinear regret bound for online learning with expanding action spaces.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.934518"
    },
    {
        "index": "#84",
        "title": "Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions",
        "link": "/arxiv/2509.25775",
        "arxiv_id": "2509.25775",
        "authors": "Amber Srivastava, Salar Basiri, Srinivasa Salapaka",
        "summary": "Clustering arises in a wide range of problem formulations, yet most existing approaches assume that the entities under clustering are passive and strictly conform to their assigned groups. In reality, entities often exhibit local autonomy, overriding prescribed associations in ways not fully captured by feature representations. Such autonomy can substantially reshape clustering outcomes -- altering cluster compositions, geometry, and cardinality -- with significant downstream effects on inference and decision-making. We introduce autonomy-aware clustering, a reinforcement (RL) learning framework that learns and accounts for the influence of local autonomy without requiring prior knowledge of its form. Our approach integrates RL with a deterministic annealing (DA) procedure, where, to determine underlying clusters, DA naturally promotes exploration in early stages of annealing and transitions to exploitation later. We also show that the annealing procedure exhibits phase transitions that enable design of efficient annealing schedules. To further enhance adaptability, we propose the Adaptive Distance Estimation Network (ADEN), a transformer-based attention model that learns dependencies between entities and cluster representatives within the RL loop, accommodates variable-sized inputs and outputs, and enables knowledge transfer across diverse problem instances. Empirical results show that our framework closely aligns with underlying data dynamics: even without explicit autonomy models, it achieves solutions close to the ground truth (gap ~3-4%), whereas ignoring autonomy leads to substantially larger gaps (~35-40%). The code and data are publicly available at https://github.com/salar96/AutonomyAwareClustering.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.935052"
    },
    {
        "index": "#85",
        "title": "OPPO: Accelerating PPO-based RLHF via Pipeline Overlap",
        "link": "/arxiv/2509.25762",
        "arxiv_id": "2509.25762",
        "authors": "Kaizhuo Yan, Yingjie Yu, Yifan Yu, Haizhong Zheng, Fan Lai",
        "summary": "Proximal Policy Optimization (PPO)-based reinforcement learning from human feedback (RLHF) is a widely adopted paradigm for aligning large language models (LLMs) with human preferences. However, its training pipeline suffers from substantial inefficiencies due to sequential multi-model dependencies (e.g., reward model depends on actor outputs) and long-tail response lengths, where a few long responses straggle the stage completion. We present OPPO, a novel, lightweight, and model-agnostic PPO-based RLHF framework that improves training efficiency by overlapping pipeline execution. OPPO introduces two novel techniques: (1) Intra-step overlap, which streams upstream model outputs (e.g., actor model) in right-sized chunks, enabling the downstream model (e.g., reward) to begin prefill while the upstream continues decoding; and (2) Inter-step overlap, which adaptively overcommits a few prompts and defers long generations to future steps, mitigating tail latency without discarding partial work. OPPO integrates easily with existing PPO implementations with a few lines of code change. Extensive evaluations show that OPPO accelerates PPO-based RLHF training by $1.8 \\times-2.8 \\times$ and improves GPU utilization by $1.4 \\times-2.1 \\times$ without compromising training convergence.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.935572"
    },
    {
        "index": "#87",
        "title": "Less is More: Towards Simple Graph Contrastive Learning",
        "link": "/arxiv/2509.25742",
        "arxiv_id": "2509.25742",
        "authors": "Yanan Zhao, Feng Ji, Jingyang Dai, Jiaze Ma, Wee Peng Tay",
        "summary": "Graph Contrastive Learning (GCL) has shown strong promise for unsupervised graph representation learning, yet its effectiveness on heterophilic graphs, where connected nodes often belong to different classes, remains limited. Most existing methods rely on complex augmentation schemes, intricate encoders, or negative sampling, which raises the question of whether such complexity is truly necessary in this challenging setting. In this work, we revisit the foundations of supervised and unsupervised learning on graphs and uncover a simple yet effective principle for GCL: mitigating node feature noise by aggregating it with structural features derived from the graph topology. This observation suggests that the original node features and the graph structure naturally provide two complementary views for contrastive learning. Building on this insight, we propose an embarrassingly simple GCL model that uses a GCN encoder to capture structural features and an MLP encoder to isolate node feature noise. Our design requires neither data augmentation nor negative sampling, yet achieves state-of-the-art results on heterophilic benchmarks with minimal computational and memory overhead, while also offering advantages in homophilic graphs in terms of complexity, scalability, and robustness. We provide theoretical justification for our approach and validate its effectiveness through extensive experiments, including robustness evaluations against both black-box and white-box adversarial attacks.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.936673"
    },
    {
        "index": "#88",
        "title": "A Physics-Guided Probabilistic Surrogate Modeling Framework for Digital Twins of Underwater Radiated Noise",
        "link": "/arxiv/2509.25730",
        "arxiv_id": "2509.25730",
        "authors": "Indu Kant Deo, Akash Venkateshwaran, Rajeev K. Jaiman",
        "summary": "Ship traffic is an increasing source of underwater radiated noise in coastal waters, motivating real-time digital twins of ocean acoustics for operational noise mitigation. We present a physics-guided probabilistic framework to predict three-dimensional transmission loss in realistic ocean environments. As a case study, we consider the Salish Sea along shipping routes from the Pacific Ocean to the Port of Vancouver. A dataset of over 30 million source-receiver pairs was generated with a Gaussian beam solver across seasonal sound speed profiles and one-third-octave frequency bands spanning 12.5 Hz to 8 kHz. We first assess sparse variational Gaussian processes (SVGP) and then incorporate physics-based mean functions combining spherical spreading with frequency-dependent absorption. To capture nonlinear effects, we examine deep sigma-point processes and stochastic variational deep kernel learning. The final framework integrates four components: (i) a learnable physics-informed mean that represents dominant propagation trends, (ii) a convolutional encoder for bathymetry along the source-receiver track, (iii) a neural encoder for source, receiver, and frequency coordinates, and (iv) a residual SVGP layer that provides calibrated predictive uncertainty. This probabilistic digital twin facilitates the construction of sound-exposure bounds and worst-case scenarios for received levels. We further demonstrate the application of the framework to ship speed optimization, where predicted transmission loss combined with near-field source models provides sound exposure level estimates for minimizing acoustic impacts on marine mammals. The proposed framework advances uncertainty-aware digital twins for ocean acoustics and illustrates how physics-guided machine learning can support sustainable maritime operations.",
        "subjects": "Machine Learning, Computational Physics",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.937200"
    },
    {
        "index": "#89",
        "title": "Boundary-to-Region Supervision for Offline Safe Reinforcement Learning",
        "link": "/arxiv/2509.25727",
        "arxiv_id": "2509.25727",
        "authors": "Huikang Su, Dengyun Peng, Zifeng Zhuang, YuHan Liu, Qiguang Chen, Donglin Wang, Qinghe Liu",
        "summary": "Offline safe reinforcement learning aims to learn policies that satisfy predefined safety constraints from static datasets. Existing sequence-model-based methods condition action generation on symmetric input tokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry: return-to-go (RTG) serves as a flexible performance target, while cost-to-go (CTG) should represent a rigid safety boundary. This symmetric conditioning leads to unreliable constraint satisfaction, especially when encountering out-of-distribution cost trajectories. To address this, we propose Boundary-to-Region (B2R), a framework that enables asymmetric conditioning through cost signal realignment . B2R redefines CTG as a boundary constraint under a fixed safety budget, unifying the cost distribution of all feasible trajectories while preserving reward structures. Combined with rotary positional embeddings , it enhances exploration within the safe region. Experimental results show that B2R satisfies safety constraints in 35 out of 38 safety-critical tasks while achieving superior reward performance over baseline methods. This work highlights the limitations of symmetric token conditioning and establishes a new theoretical and practical approach for applying sequence models to safe RL. Our code is available at https://github.com/HuikangSu/B2R.",
        "subjects": "Machine Learning, Artificial Intelligence, Robotics",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.937789"
    },
    {
        "index": "#90",
        "title": "Beyond Point Estimates: Likelihood-Based Full-Posterior Wireless Localization",
        "link": "/arxiv/2509.25719",
        "arxiv_id": "2509.25719",
        "authors": "Haozhe Lei, Hao Guo, Tommy Svensson, Sundeep Rangan",
        "summary": "Modern wireless systems require not only position estimates, but also quantified uncertainty to support planning, control, and radio resource management. We formulate localization as posterior inference of an unknown transmitter location from receiver measurements. We propose Monte Carlo Candidate-Likelihood Estimation (MC-CLE), which trains a neural scoring network using Monte Carlo sampling to compare true and candidate transmitter locations. We show that in line-of-sight simulations with a multi-antenna receiver, MC-CLE learns critical properties including angular ambiguity and front-to-back antenna patterns. MC-CLE also achieves lower cross-entropy loss relative to a uniform baseline and Gaussian posteriors. alternatives under a uniform-loss metric.",
        "subjects": "Machine Learning, Information Theory, Systems and Control",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.943656"
    },
    {
        "index": "#92",
        "title": "Reweighted Flow Matching via Unbalanced OT for Label-free Long-tailed Generation",
        "link": "/arxiv/2509.25713",
        "arxiv_id": "2509.25713",
        "authors": "Hyunsoo Song, Minjung Gim, Jaewoong Choi",
        "summary": "Flow matching has recently emerged as a powerful framework for continuous-time generative modeling. However, when applied to long-tailed distributions, standard flow matching suffers from majority bias, producing minority modes with low fidelity and failing to match the true class proportions. In this work, we propose Unbalanced Optimal Transport Reweighted Flow Matching (UOT-RFM), a novel framework for generative modeling under class-imbalanced (long-tailed) distributions that operates without any class label information. Our method constructs the conditional vector field using mini-batch Unbalanced Optimal Transport (UOT) and mitigates majority bias through a principled inverse reweighting strategy. The reweighting relies on a label-free majority score, defined as the density ratio between the target distribution and the UOT marginal. This score quantifies the degree of majority based on the geometric structure of the data, without requiring class labels. By incorporating this score into the training objective, UOT-RFM theoretically recovers the target distribution with first-order correction ($k=1$) and empirically improves tail-class generation through higher-order corrections ($k > 1$). Our model outperforms existing flow matching baselines on long-tailed benchmarks, while maintaining competitive performance on balanced datasets.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.945087"
    },
    {
        "index": "#93",
        "title": "Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking",
        "link": "/arxiv/2509.25712",
        "arxiv_id": "2509.25712",
        "authors": "Dengming Zhang, Xiaowen Ma, Zhenliang Ni, Zhenkai Wu, Han Shu, Xin Jiang, Xinghao Chen",
        "summary": "Model merging, which combines multiple domain-specialized experts into a single model, offers a practical path to endow Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) with broad capabilities without the cost of joint training or serving many models. However, training-free methods rely on hand-tuned coefficients, whereas training-based methods primarily align parameters rather than downstream task behavior and typically treat all layers uniformly, ignoring inter-layer heterogeneity. We introduce Expert Merging, a training-light method that learns a small set of layer-wise coefficients using only unlabeled calibration data. The coefficients are optimized to explicitly align the merged model's hidden states and logits with those of the corresponding experts, with a coefficient regularizer for stability and task-weighted losses for controllable trade-offs. To capture inter-layer variation, Expert Merging++ augments this design with importance-guided chunking: a normalized layer-importance metric, derived from learned coefficients, task-vector magnitudes, and parameter counts, allocates more chunk-wise coefficients to high-importance layers while keeping low-importance layers lightweight. The result is a label-free, parameter-efficient, and scalable approach to multi-expert model merging across LLMs and MLLMs. Across MLLM backbones (InternVL and Qwen2-VL) and the LLM backbone (Mistral), our method surpasses strong training-free and training-based merging baselines, with Expert Merging++ delivering further gains and, in some cases, even exceeding supervised Mixture Training. The source code is available at https://github.com/Littleor/ExpertMerging.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.945642"
    },
    {
        "index": "#94",
        "title": "Adaptive Graph Coarsening for Efficient GNN Training",
        "link": "/arxiv/2509.25706",
        "arxiv_id": "2509.25706",
        "authors": "Rostyslav Olshevskyi, Madeline Navarro, Santiago Segarra",
        "summary": "We propose an adaptive graph coarsening method to jointly learn graph neural network (GNN) parameters and merge nodes via K-means clustering during training. As real-world graphs grow larger, processing them directly becomes increasingly challenging and sometimes infeasible. Tailoring algorithms to large-scale data may sacrifice performance, so we instead consider graph reduction to decrease the amount of data used during training. In particular, we propose a method to simultaneously train a GNN and coarsen its graph by partitioning nodes via K-means clustering based on their embeddings. Unlike past graph coarsening works, our approach allows us to merge nodes during training. Not only does this preclude coarsening as a preprocessing step, but our node clusters can adapt to the learning task instead of relying solely on graph connectivity and features. Thus, our method is amenable to scenarios that are challenging for other methods, such as heterophilic data. We validate our approach on both homophilic and heterophilic node classification datasets. We further visualize relationships between node embeddings and their corresponding clusters to illustrate that our coarsened graph adapts to the learning task during training.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.946128"
    },
    {
        "index": "#95",
        "title": "Physics-Informed Learning for Human Whole-Body Kinematics Prediction via Sparse IMUs",
        "link": "/arxiv/2509.25704",
        "arxiv_id": "2509.25704",
        "authors": "Cheng Guo, Giuseppe L'Erario, Giulio Romualdi, Mattia Leonori, Marta Lorenzini, Arash Ajoudani, Daniele Pucci",
        "summary": "Accurate and physically feasible human motion prediction is crucial for safe and seamless human-robot collaboration. While recent advancements in human motion capture enable real-time pose estimation, the practical value of many existing approaches is limited by the lack of fu- ture predictions and consideration of physical constraints. Conventional motion prediction schemes rely heavily on past poses, which are not always available in real-world scenarios. To address these limitations, we present a physics-informed learning framework that integrates domain knowledge into both training and inference to predict human motion using inertial measurements from only 5 IMUs. We propose a network that accounts for the spatial characteristics of human movements. During training, we incorporate forward and differential kinematics functions as additional loss components to regularize the learned joint predictions. At the inference stage, we refine the prediction from the previous iteration to update a joint state buffer, which is used as extra inputs to the network. Experimental results demonstrate that our approach achieves high accuracy, smooth transitions between motions, and generalizes well to unseen subjects",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.946695"
    },
    {
        "index": "#97",
        "title": "Annotation-Efficient Active Test-Time Adaptation with Conformal Prediction",
        "link": "/arxiv/2509.25692",
        "arxiv_id": "2509.25692",
        "authors": "Tingyu Shi, Fan Lyu, Shaoliang Peng",
        "summary": "Active Test-Time Adaptation (ATTA) improves model robustness under domain shift by selectively querying human annotations at deployment, but existing methods use heuristic uncertainty measures and suffer from low data selection efficiency, wasting human annotation budget. We propose Conformal Prediction Active TTA (CPATTA), which first brings principled, coverage-guaranteed uncertainty into ATTA. CPATTA employs smoothed conformal scores with a top-K certainty measure, an online weight-update algorithm driven by pseudo coverage, a domain-shift detector that adapts human supervision, and a staged update scheme balances human-labeled and model-labeled data. Extensive experiments demonstrate that CPATTA consistently outperforms the state-of-the-art ATTA methods by around 5% in accuracy. Our code and datasets are available at https://github.com/tingyushi/CPATTA.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.947853"
    },
    {
        "index": "#98",
        "title": "A Unified Probabilistic Framework for Dictionary Learning with Parsimonious Activation",
        "link": "/arxiv/2509.25690",
        "arxiv_id": "2509.25690",
        "authors": "Zihui Zhao, Yuanbo Tang, Jieyu Ren, Xiaoping Zhang, Yang Li",
        "summary": "Dictionary learning is traditionally formulated as an $L_1$-regularized signal reconstruction problem. While recent developments have incorporated discriminative, hierarchical, or generative structures, most approaches rely on encouraging representation sparsity over individual samples that overlook how atoms are shared across samples, resulting in redundant and sub-optimal dictionaries. We introduce a parsimony promoting regularizer based on the row-wise $L_\\infty$ norm of the coefficient matrix. This additional penalty encourages entire rows of the coefficient matrix to vanish, thereby reducing the number of dictionary atoms activated across the dataset. We derive the formulation from a probabilistic model with Beta-Bernoulli priors, which provides a Bayesian interpretation linking the regularization parameters to prior distributions. We further establish theoretical calculation for optimal hyperparameter selection and connect our formulation to both Minimum Description Length, Bayesian model selection and pathlet learning. Extensive experiments on benchmark datasets demonstrate that our method achieves substantially improved reconstruction quality (with a 20\\% reduction in RMSE) and enhanced representation sparsity, utilizing fewer than one-tenth of the available dictionary atoms, while empirically validating our theoretical analysis.",
        "subjects": "Machine Learning, Information Theory",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.948415"
    },
    {
        "index": "#99",
        "title": "Minimalist Explanation Generation and Circuit Discovery",
        "link": "/arxiv/2509.25686",
        "arxiv_id": "2509.25686",
        "authors": "Pirzada Suhail, Aditya Anand, Amit Sethi",
        "summary": "Machine learning models, by virtue of training, learn a large repertoire of decision rules for any given input, and any one of these may suffice to justify a prediction. However, in high-dimensional input spaces, such rules are difficult to identify and interpret. In this paper, we introduce an activation-matching based approach to generate minimal and faithful explanations for the decisions of pre-trained image classifiers. We aim to identify minimal explanations that not only preserve the model's decision but are also concise and human-readable. To achieve this, we train a lightweight autoencoder to produce binary masks that learns to highlight the decision-wise critical regions of an image while discarding irrelevant background. The training objective integrates activation alignment across multiple layers, consistency at the output label, priors that encourage sparsity, and compactness, along with a robustness constraint that enforces faithfulness. The minimal explanations so generated also lead us to mechanistically interpreting the model internals. In this regard we also introduce a circuit readout procedure wherein using the explanation's forward pass and gradients, we identify active channels and construct a channel-level graph, scoring inter-layer edges by ingress weight magnitude times source activation and feature-to-class links by classifier weight magnitude times feature activation. Together, these contributions provide a practical bridge between minimal input-level explanations and a mechanistic understanding of the internal computations driving model decisions.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.948902"
    },
    {
        "index": "#100",
        "title": "Guiding Mixture-of-Experts with Temporal Multimodal Interactions",
        "link": "/arxiv/2509.25678",
        "arxiv_id": "2509.25678",
        "authors": "Xing Han, Hsing-Huan Chung, Joydeep Ghosh, Paul Pu Liang, Suchi Saria",
        "summary": "Mixture-of-Experts (MoE) architectures have become pivotal for large-scale multimodal models. However, their routing mechanisms typically overlook the informative, time-varying interaction dynamics between modalities. This limitation hinders expert specialization, as the model cannot explicitly leverage intrinsic modality relationships for effective reasoning. To address this, we propose a novel framework that guides MoE routing using quantified temporal interaction. A multimodal interaction-aware router learns to dispatch tokens to experts based on the nature of their interactions. This dynamic routing encourages experts to acquire generalizable interaction-processing skills rather than merely learning task-specific features. Our framework builds on a new formulation of temporal multimodal interaction dynamics, which are used to guide expert routing. We first demonstrate that these temporal multimodal interactions reveal meaningful patterns across applications, and then show how they can be leveraged to improve both the design and performance of MoE-based models. Comprehensive experiments on challenging multimodal benchmarks validate our approach, demonstrating both enhanced performance and improved interpretability.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.954543"
    },
    {
        "index": "#101",
        "title": "EEG-based AI-BCI Wheelchair Advancement: Hybrid Deep Learning with Motor Imagery for Brain Computer Interface",
        "link": "/arxiv/2509.25667",
        "arxiv_id": "2509.25667",
        "authors": "Bipul Thapa, Biplov Paneru, Bishwash Paneru, Khem Narayan Poudyal",
        "summary": "This paper presents an Artificial Intelligence (AI) integrated novel approach to Brain-Computer Interface (BCI)-based wheelchair development, utilizing a motor imagery right-left-hand movement mechanism for control. The system is designed to simulate wheelchair navigation based on motor imagery right and left-hand movements using electroencephalogram (EEG) data. A pre-filtered dataset, obtained from an open-source EEG repository, was segmented into arrays of 19x200 to capture the onset of hand movements. The data was acquired at a sampling frequency of 200Hz. The system integrates a Tkinter-based interface for simulating wheelchair movements, offering users a functional and intuitive control system. We propose a BiLSTM-BiGRU model that shows a superior test accuracy of 92.26% as compared with various machine learning baseline models, including XGBoost, EEGNet, and a transformer-based model. The Bi-LSTM-BiGRU attention-based model achieved a mean accuracy of 90.13% through cross-validation, showcasing the potential of attention mechanisms in BCI applications.",
        "subjects": "Machine Learning, Artificial Intelligence, Human-Computer Interaction",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.955096"
    },
    {
        "index": "#103",
        "title": "Growing Winning Subnetworks, Not Pruning Them: A Paradigm for Density Discovery in Sparse Neural Networks",
        "link": "/arxiv/2509.25665",
        "arxiv_id": "2509.25665",
        "authors": "Qihang Yao, Constantine Dovrolis",
        "summary": "The lottery ticket hypothesis suggests that dense networks contain sparse subnetworks that can be trained in isolation to match full-model performance. Existing approaches-iterative pruning, dynamic sparse training, and pruning at initialization-either incur heavy retraining costs or assume the target density is fixed in advance. We introduce Path Weight Magnitude Product-biased Random growth (PWMPR), a constructive sparse-to-dense training paradigm that grows networks rather than pruning them, while automatically discovering their operating density. Starting from a sparse seed, PWMPR adds edges guided by path-kernel-inspired scores, mitigates bottlenecks via randomization, and stops when a logistic-fit rule detects plateauing accuracy. Experiments on CIFAR, TinyImageNet, and ImageNet show that PWMPR approaches the performance of IMP-derived lottery tickets-though at higher density-at substantially lower cost (~1.5x dense vs. 3-4x for IMP). These results establish growth-based density discovery as a promising paradigm that complements pruning and dynamic sparsity.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.956197"
    },
    {
        "index": "#104",
        "title": "BaB-prob: Branch and Bound with Preactivation Splitting for Probabilistic Verification of Neural Networks",
        "link": "/arxiv/2509.25647",
        "arxiv_id": "2509.25647",
        "authors": "Fangji Wang, Panagiotis Tsiotras",
        "summary": "Branch-and-bound with preactivation splitting has been shown highly effective for deterministic verification of neural networks. In this paper, we extend this framework to the probabilistic setting. We propose BaB-prob that iteratively divides the original problem into subproblems by splitting preactivations and leverages linear bounds computed by linear bound propagation to bound the probability for each subproblem. We prove soundness and completeness of BaB-prob for feedforward-ReLU neural networks. Furthermore, we introduce the notion of uncertainty level and design two efficient strategies for preactivation splitting, yielding BaB-prob-ordered and BaB+BaBSR-prob. We evaluate BaB-prob on untrained networks, MNIST and CIFAR-10 models, respectively, and VNN-COMP 2025 benchmarks. Across these settings, our approach consistently outperforms state-of-the-art approaches in medium- to high-dimensional input problems.",
        "subjects": "Machine Learning, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.956741"
    },
    {
        "index": "#105",
        "title": "Deep set based operator learning with uncertainty quantification",
        "link": "/arxiv/2509.25646",
        "arxiv_id": "2509.25646",
        "authors": "Lei Ma, Ling Guo, Hao Wu, Tao Zhou",
        "summary": "Learning operators from data is central to scientific machine learning. While DeepONets are widely used for their ability to handle complex domains, they require fixed sensor numbers and locations, lack mechanisms for uncertainty quantification (UQ), and are thus limited in practical applicability. Recent permutationinvariant extensions, such as the Variable-Input Deep Operator Network (VIDON), relax these sensor constraints but still rely on sufficiently dense observations and cannot capture uncertainties arising from incomplete measurements or from operators with inherent randomness. To address these challenges, we propose UQ-SONet, a permutation-invariant operator learning framework with built-in UQ. Our model integrates a set transformer embedding to handle sparse and variable sensor locations, and employs a conditional variational autoencoder (cVAE) to approximate the conditional distribution of the solution operator. By minimizing the negative ELBO, UQ-SONet provides principled uncertainty estimation while maintaining predictive accuracy. Numerical experiments on deterministic and stochastic PDEs, including the Navier-Stokes equation, demonstrate the robustness and effectiveness of the proposed framework.",
        "subjects": "Machine Learning, Numerical Analysis",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.957245"
    },
    {
        "index": "#106",
        "title": "How Does Preconditioning Guide Feature Learning in Deep Neural Networks?",
        "link": "/arxiv/2509.25637",
        "arxiv_id": "2509.25637",
        "authors": "Kotaro Yoshida, Atsushi Nitanda",
        "summary": "Preconditioning is widely used in machine learning to accelerate convergence on the empirical risk, yet its role on the expected risk remains underexplored. In this work, we investigate how preconditioning affects feature learning and generalization performance. We first show that the input information available to the model is conveyed solely through the Gram matrix defined by the preconditioner's metric, thereby inducing a controllable spectral bias on feature learning. Concretely, instantiating the preconditioner as the $p$-th power of the input covariance matrix and within a single-index teacher model, we prove that in generalization, the exponent $p$ and the alignment between the teacher and the input spectrum are crucial factors. We further investigate how the interplay between these factors influences feature learning from three complementary perspectives: (i) Robustness to noise, (ii) Out-of-distribution generalization, and (iii) Forward knowledge transfer. Our results indicate that the learned feature representations closely mirror the spectral bias introduced by the preconditioner -- favoring components that are emphasized and exhibiting reduced sensitivity to those that are suppressed. Crucially, we demonstrate that generalization is significantly enhanced when this spectral bias is aligned with that of the teacher.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.957691"
    },
    {
        "index": "#107",
        "title": "Swift: An Autoregressive Consistency Model for Efficient Weather Forecasting",
        "link": "/arxiv/2509.25631",
        "arxiv_id": "2509.25631",
        "authors": "Jason Stock, Troy Arcomano, Rao Kotamarthi",
        "summary": "Diffusion models offer a physically grounded framework for probabilistic weather forecasting, but their typical reliance on slow, iterative solvers during inference makes them impractical for subseasonal-to-seasonal (S2S) applications where long lead-times and domain-driven calibration are essential. To address this, we introduce Swift, a single-step consistency model that, for the first time, enables autoregressive finetuning of a probability flow model with a continuous ranked probability score (CRPS) objective. This eliminates the need for multi-model ensembling or parameter perturbations. Results show that Swift produces skillful 6-hourly forecasts that remain stable for up to 75 days, running $39\\times$ faster than state-of-the-art diffusion baselines while achieving forecast skill competitive with the numerical-based, operational IFS ENS. This marks a step toward efficient and reliable ensemble forecasting from medium-range to seasonal-scales.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.958168"
    },
    {
        "index": "#108",
        "title": "Layer-wise dynamic rank for compressing large language models",
        "link": "/arxiv/2509.25622",
        "arxiv_id": "2509.25622",
        "authors": "Zhendong Mi, Bian Sun, Grace Li Zhang, Shaoyi Huang",
        "summary": "Large language models (LLMs) have rapidly scaled in size, bringing severe memory and computational challenges that hinder their deployment. Singular Value Decomposition (SVD)-based compression has emerged as an appealing post-training compression technique for LLMs, yet most existing methods apply a uniform compression ratio across all layers, implicitly assuming homogeneous information included in various layers. This overlooks the substantial intra-layer heterogeneity observed in LLMs, where middle layers tend to encode richer information while early and late layers are more redundant. In this work, we revisit the existing SVD-based compression method and propose D-Rank, a framework with layer-wise balanced Dynamic Rank allocation for LLMs compression. We first introduce effective rank as a principled metric to measure the information density of weight matrices, and then allocate ranks via a Lagrange multiplier-based optimization scheme to adaptively assign more capacity to groups with higher information density under a fixed compression ratio. Moreover, we rebalance the allocated ranks across attention layers to account for their varying importance and extend D-Rank to latest LLMs with grouped-query attention. Extensive experiments on various LLMs with different scales across multiple compression ratios demonstrate that D-Rank consistently outperforms SVD-LLM, ASVD, and Basis Sharing, achieving more than 15 lower perplexity with LLaMA-3-8B model on C4 datasets at 20% compression ratio and up to 5% higher zero-shot reasoning accuracy with LLaMA-7B model at 40% compression ratio while achieving even higher throughput.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.958650"
    },
    {
        "index": "#109",
        "title": "Unsupervised Detection of Spatiotemporal Anomalies in PMU Data Using Transformer-Based BiGAN",
        "link": "/arxiv/2509.25612",
        "arxiv_id": "2509.25612",
        "authors": "Muhammad Imran Hossain, Jignesh Solanki, Sarika Khushlani Solanki",
        "summary": "Ensuring power grid resilience requires the timely and unsupervised detection of anomalies in synchrophasor data streams. We introduce T-BiGAN, a novel framework that integrates window-attention Transformers within a bidirectional Generative Adversarial Network (BiGAN) to address this challenge. Its self-attention encoder-decoder architecture captures complex spatio-temporal dependencies across the grid, while a joint discriminator enforces cycle consistency to align the learned latent space with the true data distribution. Anomalies are flagged in real-time using an adaptive score that combines reconstruction error, latent space drift, and discriminator confidence. Evaluated on a realistic hardware-in-the-loop PMU benchmark, T-BiGAN achieves an ROC-AUC of 0.95 and an average precision of 0.996, significantly outperforming leading supervised and unsupervised methods. It shows particular strength in detecting subtle frequency and voltage deviations, demonstrating its practical value for live, wide-area monitoring without relying on manually labeled fault data.",
        "subjects": "Machine Learning, Artificial Intelligence, Systems and Control",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.959164"
    },
    {
        "index": "#110",
        "title": "Effective Model Pruning",
        "link": "/arxiv/2509.25606",
        "arxiv_id": "2509.25606",
        "authors": "Yixuan Wang, Dan Guralnik, Saiedeh Akbari, Warren Dixon",
        "summary": "We introduce Effective Model Pruning (EMP), a context-agnostic, parameter-free rule addressing a fundamental question about pruning: how many entries to keep. EMP does not prescribe how to score the parameters or prune the models; instead, it supplies a universal adaptive threshold that can be applied to any pruning criterion: weight magnitude, attention score, KAN importance score, or even feature-level signals such as image pixel, and used on structural parts or weights of the models. Given any score vector s, EMP maps s to a built-in effective number N_eff which is inspired by the Inverse Simpson index of contributors. Retaining the N_eff highest scoring entries and zeroing the remainder yields sparse models with performance comparable to the original dense networks across MLPs, CNNs, Transformers/LLMs, and KAN, in our experiments. By leveraging the geometry of the simplex, we derive a tight lower bound on the preserved mass s_eff (the sum of retained scores) over the corresponding ordered probability simplex associated with the score vector s. We further verify the effectiveness of N_eff by pruning the model with a scaled threshold \\b{eta}*N_eff across a variety of criteria and models. Experiments suggest that the default \\b{eta} = 1 yields a robust threshold for model pruning while \\b{eta} not equal to 1 still serves as an optional adjustment to meet specific sparsity requirements.",
        "subjects": "Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.964827"
    },
    {
        "index": "#111",
        "title": "Binary Sparse Coding for Interpretability",
        "link": "/arxiv/2509.25596",
        "arxiv_id": "2509.25596",
        "authors": "Lucia Quirke, Stepan Shabalin, Nora Belrose",
        "summary": "Sparse autoencoders (SAEs) are used to decompose neural network activations into sparsely activating features, but many SAE features are only interpretable at high activation strengths. To address this issue we propose to use binary sparse autoencoders (BAEs) and binary transcoders (BTCs), which constrain all activations to be zero or one. We find that binarisation significantly improves the interpretability and monosemanticity of the discovered features, while increasing reconstruction error. By eliminating the distinction between high and low activation strengths, we prevent uninterpretable information from being smuggled in through the continuous variation in feature activations. However, we also find that binarisation increases the number of uninterpretable ultra-high frequency features, and when interpretability scores are frequency-adjusted, the scores for continuous sparse coders are slightly better than those of binary ones. This suggests that polysemanticity may be an ineliminable property of neural activations.",
        "subjects": "Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.965301"
    },
    {
        "index": "#112",
        "title": "Machine Learning Algorithms for Improving Black Box Optimization Solvers",
        "link": "/arxiv/2509.25592",
        "arxiv_id": "2509.25592",
        "authors": "Morteza Kimiaei, Vyacheslav Kungurtsev",
        "summary": "Black-box optimization (BBO) addresses problems where objectives are accessible only through costly queries without gradients or explicit structure. Classical derivative-free methods -- line search, direct search, and model-based solvers such as Bayesian optimization -- form the backbone of BBO, yet often struggle in high-dimensional, noisy, or mixed-integer settings. Recent advances use machine learning (ML) and reinforcement learning (RL) to enhance BBO: ML provides expressive surrogates, adaptive updates, meta-learning portfolios, and generative models, while RL enables dynamic operator configuration, robustness, and meta-optimization across tasks. This paper surveys these developments, covering representative algorithms such as NNs with the modular model-based optimization framework (mlrMBO), zeroth-order adaptive momentum methods (ZO-AdaMM), automated BBO (ABBO), distributed block-wise optimization (DiBB), partition-based Bayesian optimization (SPBOpt), the transformer-based optimizer (B2Opt), diffusion-model-based BBO, surrogate-assisted RL for differential evolution (Surr-RLDE), robust BBO (RBO), coordinate-ascent model-based optimization with relative entropy (CAS-MORE), log-barrier stochastic gradient descent (LB-SGD), policy improvement with black-box (PIBB), and offline Q-learning with Mamba backbones (Q-Mamba). We also review benchmark efforts such as the NeurIPS 2020 BBO Challenge and the MetaBox framework. Overall, we highlight how ML and RL transform classical inexact solvers into more scalable, robust, and adaptive frameworks for real-world optimization.",
        "subjects": "Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.965768"
    },
    {
        "index": "#113",
        "title": "Safe In-Context Reinforcement Learning",
        "link": "/arxiv/2509.25582",
        "arxiv_id": "2509.25582",
        "authors": "Amir Moeini, Minjae Kwon, Alper Kamil Bozkurt, Yuichi Motai, Rohan Chandra, Lu Feng, Shangtong Zhang",
        "summary": "In-context reinforcement learning (ICRL) is an emerging RL paradigm where the agent, after some pretraining procedure, is able to adapt to out-of-distribution test tasks without any parameter updates. The agent achieves this by continually expanding the input (i.e., the context) to its policy neural networks. For example, the input could be all the history experience that the agent has access to until the current time step. The agent's performance improves as the input grows, without any parameter updates. In this work, we propose the first method that promotes the safety of ICRL's adaptation process in the framework of constrained Markov Decision Processes. In other words, during the parameter-update-free adaptation process, the agent not only maximizes the reward but also minimizes an additional cost function. We also demonstrate that our agent actively reacts to the threshold (i.e., budget) of the cost tolerance. With a higher cost budget, the agent behaves more aggressively, and with a lower cost budget, the agent behaves more conservatively.",
        "subjects": "Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.966306"
    },
    {
        "index": "#114",
        "title": "Lightweight and Robust Federated Data Valuation",
        "link": "/arxiv/2509.25560",
        "arxiv_id": "2509.25560",
        "authors": "Guojun Tang, Jiayu Zhou, Mohammad Mamun, Steve Drew",
        "summary": "Federated learning (FL) faces persistent robustness challenges due to non-IID data distributions and adversarial client behavior. A promising mitigation strategy is contribution evaluation, which enables adaptive aggregation by quantifying each client's utility to the global model. However, state-of-the-art Shapley-value-based approaches incur high computational overhead due to repeated model reweighting and inference, which limits their scalability. We propose FedIF, a novel FL aggregation framework that leverages trajectory-based influence estimation to efficiently compute client contributions. FedIF adapts decentralized FL by introducing normalized and smoothed influence scores computed from lightweight gradient operations on client updates and a public validation set. Theoretical analysis demonstrates that FedIF yields a tighter bound on one-step global loss change under noisy conditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF achieves robustness comparable to or exceeding SV-based methods in the presence of label noise, gradient noise, and adversarial samples, while reducing aggregation overhead by up to 450x. Ablation studies confirm the effectiveness of FedIF's design choices, including local weight normalization and influence smoothing. Our results establish FedIF as a practical, theoretically grounded, and scalable alternative to Shapley-value-based approaches for efficient and robust FL in real-world deployments.",
        "subjects": "Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.966787"
    },
    {
        "index": "#115",
        "title": "Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization",
        "link": "/arxiv/2509.25538",
        "arxiv_id": "2509.25538",
        "authors": "Marcus Schwarting, Logan Ward, Nathaniel Hudson, Xiaoli Yan, Ben Blaiszik, Santanu Chaudhuri, Eliu Huerta, Ian Foster",
        "summary": "Generative AI poses both opportunities and risks for solving inverse design problems in the sciences. Generative tools provide the ability to expand and refine a search space autonomously, but do so at the cost of exploring low-quality regions until sufficiently fine tuned. Here, we propose a queue prioritization algorithm that combines generative modeling and active learning in the context of a distributed workflow for exploring complex design spaces. We find that incorporating an active learning model to prioritize top design candidates can prevent a generative AI workflow from expending resources on nonsensical candidates and halt potential generative model decay. For an existing generative AI workflow for discovering novel molecular structure candidates for carbon capture, our active learning approach significantly increases the number of high-quality candidates identified by the generative model. We find that, out of 1000 novel candidates, our workflow without active learning can generate an average of 281 high-performing candidates, while our proposed prioritization with active learning can generate an average 604 high-performing candidates.",
        "subjects": "Machine Learning, Materials Science, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.967420"
    },
    {
        "index": "#116",
        "title": "Meta-Router: Bridging Gold-standard and Preference-based Evaluations in Large Language Model Routing",
        "link": "/arxiv/2509.25535",
        "arxiv_id": "2509.25535",
        "authors": "Yichi Zhang, Fangzheng Xie, Shu Yang, Chong Wu",
        "summary": "In language tasks that require extensive human--model interaction, deploying a single \"best\" model for every query can be expensive. To reduce inference cost while preserving the quality of the responses, a large language model (LLM) router selects the most appropriate model from a pool of candidates for each query. A central challenge to training a high-quality router is the scarcity of reliable supervision. Gold-standard data (e.g., expert-verified labels or rubric-based scores) provide accurate quality evaluations of LLM responses but are costly and difficult to scale. In contrast, preference-based data, collected via crowdsourcing or LLM-as-a-judge systems, are cheaper and more scalable, yet often biased in reflecting the true quality of responses. We cast the problem of LLM router training with combined gold-standard and preference-based data into a causal inference framework by viewing the response evaluation mechanism as the treatment assignment. This perspective further reveals that the bias in preference-based data corresponds to the well-known causal estimand: the conditional average treatment effect. Based on this new perspective, we develop an integrative causal router training framework that corrects preference-data bias, address imbalances between two data sources, and improve routing robustness and efficiency. Numerical experiments demonstrate that our approach delivers more accurate routing and improves the trade-off between cost and quality.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.967981"
    },
    {
        "index": "#117",
        "title": "Flow Matching with Semidiscrete Couplings",
        "link": "/arxiv/2509.25519",
        "arxiv_id": "2509.25519",
        "authors": "Alireza Mousavi-Hosseini, Stephen Y. Zhang, Michal Klein, Marco Cuturi",
        "summary": "Flow models parameterized as time-dependent velocity fields can generate data from noise by integrating an ODE. These models are often trained using flow matching, i.e. by sampling random pairs of noise and target points $(\\mathbf{x}_0,\\mathbf{x}_1)$ and ensuring that the velocity field is aligned, on average, with $\\mathbf{x}_1-\\mathbf{x}_0$ when evaluated along a segment linking $\\mathbf{x}_0$ to $\\mathbf{x}_1$. While these pairs are sampled independently by default, they can also be selected more carefully by matching batches of $n$ noise to $n$ target points using an optimal transport (OT) solver. Although promising in theory, the OT flow matching (OT-FM) approach is not widely used in practice. Zhang et al. (2025) pointed out recently that OT-FM truly starts paying off when the batch size $n$ grows significantly, which only a multi-GPU implementation of the Sinkhorn algorithm can handle. Unfortunately, the costs of running Sinkhorn can quickly balloon, requiring $O(n^2/\\varepsilon^2)$ operations for every $n$ pairs used to fit the velocity field, where $\\varepsilon$ is a regularization parameter that should be typically small to yield better results. To fulfill the theoretical promises of OT-FM, we propose to move away from batch-OT and rely instead on a semidiscrete formulation that leverages the fact that the target dataset distribution is usually of finite size $N$. The SD-OT problem is solved by estimating a dual potential vector using SGD; using that vector, freshly sampled noise vectors at train time can then be matched with data points at the cost of a maximum inner product search (MIPS). Semidiscrete FM (SD-FM) removes the quadratic dependency on $n/\\varepsilon$ that bottlenecks OT-FM. SD-FM beats both FM and OT-FM on all training metrics and inference budget constraints, across multiple datasets, on unconditional/conditional generation, or when using mean-flow models.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.968505"
    },
    {
        "index": "#118",
        "title": "World Model for AI Autonomous Navigation in Mechanical Thrombectomy",
        "link": "/arxiv/2509.25518",
        "arxiv_id": "2509.25518",
        "authors": "Harry Robertshaw, Han-Ru Wu, Alejandro Granados, Thomas C Booth",
        "summary": "Autonomous navigation for mechanical thrombectomy (MT) remains a critical challenge due to the complexity of vascular anatomy and the need for precise, real-time decision-making. Reinforcement learning (RL)-based approaches have demonstrated potential in automating endovascular navigation, but current methods often struggle with generalization across multiple patient vasculatures and long-horizon tasks. We propose a world model for autonomous endovascular navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL agent across multiple endovascular navigation tasks in ten real patient vasculatures, comparing performance against the state-of-the-art Soft Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly outperforms SAC in multi-task learning, achieving a 65% mean success rate compared to SAC's 37%, with notable improvements in path ratio. TD-MPC2 exhibited increased procedure times, suggesting a trade-off between success rate and execution speed. These findings highlight the potential of world models for improving autonomous endovascular navigation and lay the foundation for future research in generalizable AI-driven robotic interventions.",
        "subjects": "Machine Learning, Robotics, Image and Video Processing",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.969053"
    },
    {
        "index": "#119",
        "title": "EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit",
        "link": "/arxiv/2509.25510",
        "arxiv_id": "2509.25510",
        "authors": "Chang Liu, Danial Chitnis",
        "summary": "The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often involves significant manual effort, especially during the transistor sizing process. While Machine Learning techniques in Electronic Design Automation (EDA) have shown promise in reducing complexity and minimizing human intervention, they still face challenges such as numerous iterations and a lack of knowledge about AMS circuit design. Recently, Large Language Models (LLMs) have demonstrated significant potential across various fields, showing a certain level of knowledge in circuit design and indicating their potential to automate the transistor sizing process. In this work, we propose EEsizer, an LLM-based AI agent that integrates large language models with circuit simulators and custom data analysis functions, enabling fully automated, closed-loop transistor sizing without relying on external knowledge. By employing prompt engineering and Chain-of-Thought reasoning, the agent iteratively explores design directions, evaluates performance, and refines solutions with minimal human intervention. We first benchmarked 8 LLMs on six basic circuits and selected three high-performing models to optimize a 20-transistor CMOS operational amplifier, targeting multiple performance metrics, including rail-to-rail operation from 180 nm to 90 nm technology nodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90 nm across three different test groups, with a maximum of 20 iterations, demonstrating adaptability and robustness at advanced nodes. To assess design robustness, we manually designed a bias circuit and performed a variation analysis using Gaussian-distributed variations on transistor dimensions and threshold voltages.",
        "subjects": "Machine Learning, Hardware Architecture",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.974666"
    },
    {
        "index": "#120",
        "title": "Can Molecular Foundation Models Know What They Don't Know? A Simple Remedy with Preference Optimization",
        "link": "/arxiv/2509.25509",
        "arxiv_id": "2509.25509",
        "authors": "Langzhou He, Junyou Zhu, Fangxin Wang, Junhua Liu, Haoyan Xu, Yue Zhao, Philip S. Yu, Qitian Wu",
        "summary": "Molecular foundation models are rapidly advancing scientific discovery, but their unreliability on out-of-distribution (OOD) samples severely limits their application in high-stakes domains such as drug discovery and protein design. A critical failure mode is chemical hallucination, where models make high-confidence yet entirely incorrect predictions for unknown molecules. To address this challenge, we introduce Molecular Preference-Aligned Instance Ranking (Mole-PAIR), a simple, plug-and-play module that can be flexibly integrated with existing foundation models to improve their reliability on OOD data through cost-effective post-training. Specifically, our method formulates the OOD detection problem as a preference optimization over the estimated OOD affinity between in-distribution (ID) and OOD samples, achieving this goal through a pairwise learning objective. We show that this objective essentially optimizes AUROC, which measures how consistently ID and OOD samples are ranked by the model. Extensive experiments across five real-world molecular datasets demonstrate that our approach significantly improves the OOD detection capabilities of existing molecular foundation models, achieving up to 45.8%, 43.9%, and 24.3% improvements in AUROC under distribution shifts of size, scaffold, and assay, respectively.",
        "subjects": "Machine Learning, Quantitative Methods",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.975284"
    },
    {
        "index": "#121",
        "title": "Scalable Disk-Based Approximate Nearest Neighbor Search with Page-Aligned Graph",
        "link": "/arxiv/2509.25487",
        "arxiv_id": "2509.25487",
        "authors": "Dingyi Kang, Dongming Jiang, Hanshen Yang, Hang Liu, Bingzhe Li",
        "summary": "Approximate Nearest Neighbor Search (ANNS), as the core of vector databases (VectorDBs), has become widely used in modern AI and ML systems, powering applications from information retrieval to bio-informatics. While graph-based ANNS methods achieve high query efficiency, their scalability is constrained by the available host memory. Recent disk-based ANNS approaches mitigate memory usage by offloading data to Solid-State Drives (SSDs). However, they still suffer from issues such as long I/O traversal path, misalignment with storage I/O granularity, and high in-memory indexing overhead, leading to significant I/O latency and ultimately limiting scalability for large-scale vector search. In this paper, we propose PageANN, a disk-based approximate nearest neighbor search (ANNS) framework designed for high performance and scalability. PageANN introduces a page-node graph structure that aligns logical graph nodes with physical SSD pages, thereby shortening I/O traversal paths and reducing I/O operations. Specifically, similar vectors are clustered into page nodes, and a co-designed disk data layout leverages this structure with a merging technique to store only representative vectors and topology information, avoiding unnecessary reads. To further improve efficiency, we design a memory management strategy that combines lightweight indexing with coordinated memory-disk data allocation, maximizing host memory utilization while minimizing query latency and storage overhead. Experimental results show that PageANN significantly outperforms state-of-the-art (SOTA) disk-based ANNS methods, achieving 1.85x-10.83x higher throughput and 51.7%-91.9% lower latency across different datasets and memory budgets, while maintaining comparable high recall accuracy.",
        "subjects": "Machine Learning, Databases, Information Retrieval",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.975937"
    },
    {
        "index": "#122",
        "title": "Translation from Wearable PPG to 12-Lead ECG",
        "link": "/arxiv/2509.25480",
        "arxiv_id": "2509.25480",
        "authors": "Hui Ji, Wei Gao, Pengfei Zhou",
        "summary": "The 12-lead electrocardiogram (ECG) is the gold standard for cardiovascular monitoring, offering superior diagnostic granularity and specificity compared to photoplethysmography (PPG). However, existing 12-lead ECG systems rely on cumbersome multi-electrode setups, limiting sustained monitoring in ambulatory settings, while current PPG-based methods fail to reconstruct multi-lead ECG due to the absence of inter-lead constraints and insufficient modeling of spatial-temporal dependencies across leads. To bridge this gap, we introduce P2Es, an innovative demographic-aware diffusion framework designed to generate clinically valid 12-lead ECG from PPG signals via three key innovations. Specifically, in the forward process, we introduce frequency-domain blurring followed by temporal noise interference to simulate real-world signal distortions. In the reverse process, we design a temporal multi-scale generation module followed by frequency deblurring. In particular, we leverage KNN-based clustering combined with contrastive learning to assign affinity matrices for the reverse process, enabling demographic-specific ECG translation. Extensive experimental results show that P2Es outperforms baseline models in 12-lead ECG reconstruction.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.976451"
    },
    {
        "index": "#123",
        "title": "Conformal Prediction for Signal Temporal Logic Inference",
        "link": "/arxiv/2509.25473",
        "arxiv_id": "2509.25473",
        "authors": "Danyang Li, Yixuan Wang, Matthew Cleaveland, Mingyu Cai, Roberto Tron",
        "summary": "Signal Temporal Logic (STL) inference seeks to extract human-interpretable rules from time-series data, but existing methods lack formal confidence guarantees for the inferred rules. Conformal prediction (CP) is a technique that can provide statistical correctness guarantees, but is typically applied as a post-training wrapper without improving model learning. Instead, we introduce an end-to-end differentiable CP framework for STL inference that enhances both reliability and interpretability of the resulting formulas. We introduce a robustness-based nonconformity score, embed a smooth CP layer directly into training, and employ a new loss function that simultaneously optimizes inference accuracy and CP prediction sets with a single term. Following training, an exact CP procedure delivers statistical guarantees for the learned STL formulas. Experiments on benchmark time-series tasks show that our approach reduces uncertainty in predictions (i.e., it achieves high coverage while reducing prediction set size), and improves accuracy (i.e., the number of misclassifications when using a fixed threshold) over state-of-the-art baselines.",
        "subjects": "Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.976959"
    },
    {
        "index": "#124",
        "title": "Data-Efficient Multitask DAgger",
        "link": "/arxiv/2509.25466",
        "arxiv_id": "2509.25466",
        "authors": "Haotian Fu, Ran Gong, Xiaohan Zhang, Maria Vittoria Minniti, Jigarkumar Patel, Karl Schmeckpeper",
        "summary": "Generalist robot policies that can perform many tasks typically require extensive expert data or simulations for training. In this work, we propose a novel Data-Efficient multitask DAgger framework that distills a single multitask policy from multiple task-specific expert policies. Our approach significantly increases the overall task success rate by actively focusing on tasks where the multitask policy underperforms. The core of our method is a performance-aware scheduling strategy that tracks how much each task's learning process benefits from the amount of data, using a Kalman filter-based estimator to robustly decide how to allocate additional demonstrations across tasks. We validate our approach on MetaWorld, as well as a suite of diverse drawer-opening tasks in IsaacLab. The resulting policy attains high performance across all tasks while using substantially fewer expert demonstrations, and the visual policy learned with our method in simulation shows better performance than naive DAgger and Behavior Cloning when transferring zero-shot to a real robot without using real data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.977490"
    },
    {
        "index": "#125",
        "title": "Joint Embeddings Go Temporal",
        "link": "/arxiv/2509.25449",
        "arxiv_id": "2509.25449",
        "authors": "Sofiane Ennadir, Siavash Golkar, Leopoldo Sarra",
        "summary": "Self-supervised learning has seen great success recently in unsupervised representation learning, enabling breakthroughs in natural language and image processing. However, these methods often rely on autoregressive and masked modeling, which aim to reproduce masked information in the input, which can be vulnerable to the presence of noise or confounding variables. To address this problem, Joint-Embedding Predictive Architectures (JEPA) has been introduced with the aim to perform self-supervised learning in the latent space. To leverage these advancements in the domain of time series, we introduce Time Series JEPA (TS-JEPA), an architecture specifically adapted for time series representation learning. We validate TS-JEPA on both classification and forecasting, showing that it can match or surpass current state-of-the-art baselines on different standard datasets. Notably, our approach demonstrates a strong performance balance across diverse tasks, indicating its potential as a robust foundation for learning general representations. Thus, this work lays the groundwork for developing future time series foundation models based on Joint Embedding.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.977983"
    },
    {
        "index": "#126",
        "title": "Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications",
        "link": "/arxiv/2509.25439",
        "arxiv_id": "2509.25439",
        "authors": "Hanyuan Gao, Xiaoxuan Yang",
        "summary": "Hidden Markov models (HMM) are commonly used in generation tasks and have demonstrated strong capabilities in neuro-symbolic applications for the Markov property. These applications leverage the strengths of neural networks and symbolic reasoning to create robust and interpretable AI systems. However, they may inherit and amplify the shortcomings of both approaches. Both components require dense computation and data transfer, and their communication further hinders performance. This paper proposes Norm-Q, a normalized linear quantization approach for compressing probabilistic symbolic models, such as HMMs. We reduce the bit width of the data with minimal impact, thereby alleviating memory and bandwidth stress and enabling deployment on potential custom hardware. Our method introduces a normalized quantization-aware expectation maximization process for probabilistic model training. The experimental results show that Norm-Q achieves a higher compression rate with reasonable score loss compared to traditional quantization methods. In the case of the constrained generation task of large language models, we successfully quantize an HMM of 4096 hidden states to 8 bits without loss and, at most, 3 bits with acceptable loss. Notably, the Norm-Q method can achieve a compression rate of 99% for the weights of the HMM. The code is open source at https://github.com/superstarghy/Norm-Q.",
        "subjects": "Machine Learning, Neural and Evolutionary Computing",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.978473"
    },
    {
        "index": "#127",
        "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring",
        "link": "/arxiv/2509.25438",
        "arxiv_id": "2509.25438",
        "authors": "Zhibo Hou, Zhiyu An, Wan Du",
        "summary": "When there exists an unlearnable source of randomness (noisy-TV) in the environment, a naively intrinsic reward driven exploring agent gets stuck at that source of randomness and fails at exploration. Intrinsic reward based on uncertainty estimation or distribution similarity, while eventually escapes noisy-TVs as time unfolds, suffers from poor sample efficiency and high computational cost. Inspired by recent findings from neuroscience that humans monitor their improvements during exploration, we propose a novel method for intrinsically-motivated exploration, named Learning Progress Monitoring (LPM). During exploration, LPM rewards model improvements instead of prediction error or novelty, effectively rewards the agent for observing learnable transitions rather than the unlearnable transitions. We introduce a dual-network design that uses an error model to predict the expected prediction error of the dynamics model in its previous iteration, and use the difference between the model errors of the current iteration and previous iteration to guide exploration. We theoretically show that the intrinsic reward of LPM is zero-equivariant and a monotone indicator of Information Gain (IG), and that the error model is necessary to achieve monotonicity correspondence with IG. We empirically compared LPM against state-of-the-art baselines in noisy environments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari. Results show that LPM's intrinsic reward converges faster, explores more states in the maze experiment, and achieves higher extrinsic reward in Atari. This conceptually simple approach marks a shift-of-paradigm of noise-robust exploration. For code to reproduce our experiments, see https://github.com/Akuna23Matata/LPM_exploration",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.978961"
    },
    {
        "index": "#128",
        "title": "Feedback Control for Small Budget Pacing",
        "link": "/arxiv/2509.25429",
        "arxiv_id": "2509.25429",
        "authors": "Sreeja Apparaju, Yichuan Niu, Xixi Qi",
        "summary": "Budget pacing is critical in online advertising to align spend with campaign goals under dynamic auctions. Existing pacing methods often rely on ad-hoc parameter tuning, which can be unstable and inefficient. We propose a principled controller that combines bucketized hysteresis with proportional feedback to provide stable and adaptive spend control. Our method provides a framework and analysis for parameter selection that enables accurate tracking of desired spend rates across campaigns. Experiments in real-world auctions demonstrate significant improvements in pacing accuracy and delivery consistency, reducing pacing error by 13% and $\\lambda$-volatility by 54% compared to baseline method. By bridging control theory with advertising systems, our approach offers a scalable and reliable solution for budget pacing, with particular benefits for small-budget campaigns.",
        "subjects": "Machine Learning, Computer Science and Game Theory",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.979460"
    },
    {
        "index": "#129",
        "title": "Polychromic Objectives for Reinforcement Learning",
        "link": "/arxiv/2509.25424",
        "arxiv_id": "2509.25424",
        "authors": "Jubayer Ibn Hamid, Ifdita Hasan Orney, Ellen Xu, Chelsea Finn, Dorsa Sadigh",
        "summary": "Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for improving pretrained policies for downstream tasks. These pretrained policies, trained on large datasets, produce generations with a broad range of promising but unrefined behaviors. Often, a critical failure mode of RLFT arises when policies lose this diversity and collapse into a handful of easily exploitable outputs. This convergence hinders exploration, which is essential for expanding the capabilities of the pretrained policy and for amplifying the benefits of test-time compute scaling. To address this, we introduce an objective for policy gradient methods that explicitly enforces the exploration and refinement of diverse generations, which we call a polychromic objective. We then show how proximal policy optimization (PPO) can be adapted to optimize this objective. Our method (1) employs vine sampling to collect on-policy rollouts and (2) modifies the advantage function to reflect the advantage under our new objective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show that our method improves success rates by reliably solving a larger set of environment configurations and generalizes better under large perturbations. Moreover, when given multiple attempts in pass@$k$ experiments, the policy achieves substantially higher coverage, demonstrating its ability to maintain and exploit a diverse repertoire of strategies.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.985141"
    },
    {
        "index": "#130",
        "title": "Leveraging Vulnerabilities in Temporal Graph Neural Networks via Strategic High-Impact Assaults",
        "link": "/arxiv/2509.25418",
        "arxiv_id": "2509.25418",
        "authors": "Dong Hyun Jeon, Lijing Zhu, Haifang Li, Pengze Li, Jingna Feng, Tiehang Duan, Houbing Herbert Song, Cui Tao, Shuteng Niu",
        "summary": "Temporal Graph Neural Networks (TGNNs) have become indispensable for analyzing dynamic graphs in critical applications such as social networks, communication systems, and financial networks. However, the robustness of TGNNs against adversarial attacks, particularly sophisticated attacks that exploit the temporal dimension, remains a significant challenge. Existing attack methods for Spatio-Temporal Dynamic Graphs (STDGs) often rely on simplistic, easily detectable perturbations (e.g., random edge additions/deletions) and fail to strategically target the most influential nodes and edges for maximum impact. We introduce the High Impact Attack (HIA), a novel restricted black-box attack framework specifically designed to overcome these limitations and expose critical vulnerabilities in TGNNs. HIA leverages a data-driven surrogate model to identify structurally important nodes (central to network connectivity) and dynamically important nodes (critical for the graph's temporal evolution). It then employs a hybrid perturbation strategy, combining strategic edge injection (to create misleading connections) and targeted edge deletion (to disrupt essential pathways), maximizing TGNN performance degradation. Importantly, HIA minimizes the number of perturbations to enhance stealth, making it more challenging to detect. Comprehensive experiments on five real-world datasets and four representative TGNN architectures (TGN, JODIE, DySAT, and TGAT) demonstrate that HIA significantly reduces TGNN accuracy on the link prediction task, achieving up to a 35.55% decrease in Mean Reciprocal Rank (MRR) - a substantial improvement over state-of-the-art baselines. These results highlight fundamental vulnerabilities in current STDG models and underscore the urgent need for robust defenses that account for both structural and temporal dynamics.",
        "subjects": "Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.985735"
    },
    {
        "index": "#132",
        "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers",
        "link": "/arxiv/2509.25401",
        "arxiv_id": "2509.25401",
        "authors": "Liang Qiao, Yue Dai, Yeqi Huang, Hongyu Kan, Jun Shi, Hong An",
        "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional capabilities in visual synthesis, yet their deployment remains constrained by substantial computational demands. To alleviate this bottleneck, many sparsity-based acceleration methods have been proposed. However, their diverse sparsity patterns often require customized kernels for high-performance inference, limiting universality. We propose FlashOmni, a unified sparse attention engine compatible with arbitrary DiT architectures. FlashOmni introduces flexible sparse symbols to standardize the representation of a wide range of sparsity strategies, such as feature caching and block-sparse skipping. This unified abstraction enables the execution of diverse sparse computations within a single attention kernel. In addition, FlashOmni designs optimized sparse GEMMs for attention blocks, leveraging sparse symbols to eliminate redundant computations and further improve efficiency. Experiments demonstrate that FlashOmni delivers near-linear, closely matching the sparsity ratio speedup (1:1) in attention and GEMM-$Q$, and achieves 2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of the theoretical limit). Applied with a multi-granularity sparsity strategy, it enables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end acceleration without degrading visual quality.",
        "subjects": "Machine Learning, Artificial Intelligence, Performance",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.986836"
    },
    {
        "index": "#133",
        "title": "Multi-Task Equation Discovery",
        "link": "/arxiv/2509.25400",
        "arxiv_id": "2509.25400",
        "authors": "S C Bee, N Dervilis, K Worden, L A Bull",
        "summary": "Equation discovery provides a grey-box approach to system identification by uncovering governing dynamics directly from observed data. However, a persistent challenge lies in ensuring that identified models generalise across operating conditions rather than over-fitting to specific datasets. This work investigates this issue by applying a Bayesian relevance vector machine (RVM) within a multi-task learning (MTL) framework for simultaneous parameter identification across multiple datasets. In this formulation, responses from the same structure under different excitation levels are treated as related tasks that share model parameters but retain task-specific noise characteristics. A simulated single degree-of-freedom oscillator with linear and cubic stiffness provided the case study, with datasets generated under three excitation regimes. Standard single-task RVM models were able to reproduce system responses but often failed to recover the true governing terms when excitations insufficiently stimulated non-linear dynamics. By contrast, the MTL-RVM combined information across tasks, improving parameter recovery for weakly and moderately excited datasets, while maintaining strong performance under high excitation. These findings demonstrate that multi-task Bayesian inference can mitigate over-fitting and promote generalisation in equation discovery. The approach is particularly relevant to structural health monitoring, where varying load conditions reveal complementary aspects of system physics.",
        "subjects": "Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.987339"
    },
    {
        "index": "#134",
        "title": "Crowdsourcing Without People: Modelling Clustering Algorithms as Experts",
        "link": "/arxiv/2509.25395",
        "arxiv_id": "2509.25395",
        "authors": "Jordyn E. A. Lorentz, Katharine M. Clark",
        "summary": "This paper introduces mixsemble, an ensemble method that adapts the Dawid-Skene model to aggregate predictions from multiple model-based clustering algorithms. Unlike traditional crowdsourcing, which relies on human labels, the framework models the outputs of clustering algorithms as noisy annotations. Experiments on both simulated and real-world datasets show that, although the mixsemble is not always the single top performer, it consistently approaches the best result and avoids poor outcomes. This robustness makes it a practical alternative when the true data structure is unknown, especially for non-expert users.",
        "subjects": "Machine Learning, Methodology",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.987857"
    },
    {
        "index": "#135",
        "title": "On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study",
        "link": "/arxiv/2509.25382",
        "arxiv_id": "2509.25382",
        "authors": "Fernanda Zapata Bascuñán",
        "summary": "In this work, we explore the latent space of a denoising variational autoencoder with a mixture-of-Gaussians prior (VAE-MoG), trained on gravitational wave data from event GW150914. To evaluate how well the model captures the underlying structure, we use Hamiltonian Monte Carlo (HMC) to draw posterior samples conditioned on clean inputs, and compare them to the encoder's outputs from noisy data. Although the model reconstructs signals accurately, statistical comparisons reveal a clear mismatch in the latent space. This shows that strong denoising performance doesn't necessarily mean the latent representations are reliable highlighting the importance of using posterior-based validation when evaluating generative models.",
        "subjects": "Machine Learning, Hardware Architecture",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.988346"
    },
    {
        "index": "#136",
        "title": "Deep Survival Analysis for Competing Risk Modeling with Functional Covariates and Missing Data Imputation",
        "link": "/arxiv/2509.25381",
        "arxiv_id": "2509.25381",
        "authors": "Penglei Gao, Yan Zou, Abhijit Duggal, Shuaiqi Huang, Faming Liang, Xiaofeng Wang",
        "summary": "We introduce the Functional Competing Risk Net (FCRN), a unified deep-learning framework for discrete-time survival analysis under competing risks, which seamlessly integrates functional covariates and handles missing data within an end-to-end model. By combining a micro-network Basis Layer for functional data representation with a gradient-based imputation module, FCRN simultaneously learns to impute missing values and predict event-specific hazards. Evaluated on multiple simulated datasets and a real-world ICU case study using the MIMIC-IV and Cleveland Clinic datasets, FCRN demonstrates substantial improvements in prediction accuracy over random survival forests and traditional competing risks models. This approach advances prognostic modeling in critical care by more effectively capturing dynamic risk factors and static predictors while accommodating irregular and incomplete data.",
        "subjects": "Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.988898"
    },
    {
        "index": "#138",
        "title": "Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation",
        "link": "/arxiv/2509.25379",
        "arxiv_id": "2509.25379",
        "authors": "Yogesh Verma, Markus Heinonen, Vikas Garg",
        "summary": "Protein structure prediction and folding are fundamental to understanding biology, with recent deep learning advances reshaping the field. Diffusion-based generative models have revolutionized protein design, enabling the creation of novel proteins. However, these methods often neglect the intrinsic physical realism of proteins, driven by noising dynamics that lack grounding in physical principles. To address this, we first introduce a physically motivated non-linear noising process, grounded in classical physics, that unfolds proteins into secondary structures (e.g., alpha helices, linear beta sheets) while preserving topological integrity--maintaining bonds, and preventing collisions. We then integrate this process with the flow-matching paradigm on SE(3) to model the invariant distribution of protein backbones with high fidelity, incorporating sequence information to enable sequence-conditioned folding and expand the generative capabilities of our model. Experimental results demonstrate that the proposed method achieves state-of-the-art performance in unconditional protein generation, producing more designable and novel protein structures while accurately folding monomer sequences into precise protein conformations.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.995088"
    },
    {
        "index": "#139",
        "title": "Cold-Start Active Correlation Clustering",
        "link": "/arxiv/2509.25376",
        "arxiv_id": "2509.25376",
        "authors": "Linus Aronsson, Han Wu, Morteza Haghir Chehreghani",
        "summary": "We study active correlation clustering where pairwise similarities are not provided upfront and must be queried in a cost-efficient manner through active learning. Specifically, we focus on the cold-start scenario, where no true initial pairwise similarities are available for active learning. To address this challenge, we propose a coverage-aware method that encourages diversity early in the process. We demonstrate the effectiveness of our approach through several synthetic and real-world experiments.",
        "subjects": "Machine Learning, Artificial Intelligence, Social and Information Networks",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.995654"
    },
    {
        "index": "#140",
        "title": "Gradient Descent with Large Step Sizes: Chaos and Fractal Convergence Region",
        "link": "/arxiv/2509.25351",
        "arxiv_id": "2509.25351",
        "authors": "Shuang Liang, Guido Montúfar",
        "summary": "We examine gradient descent in matrix factorization and show that under large step sizes the parameter space develops a fractal structure. We derive the exact critical step size for convergence in scalar-vector factorization and show that near criticality the selected minimizer depends sensitively on the initialization. Moreover, we show that adding regularization amplifies this sensitivity, generating a fractal boundary between initializations that converge and those that diverge. The analysis extends to general matrix factorization with orthogonal initialization. Our findings reveal that near-critical step sizes induce a chaotic regime of gradient descent where the long-term dynamics are unpredictable and there are no simple implicit biases, such as towards balancedness, minimum norm, or flatness.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.996186"
    },
    {
        "index": "#141",
        "title": "Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder",
        "link": "/arxiv/2509.25334",
        "arxiv_id": "2509.25334",
        "authors": "Amirhossein Zare, Amirhessam Zare, Parmida Sadat Pezeshki, Herlock, Rahimi, Ali Ebrahimi, Ignacio Vázquez-García, Leo Anthony Celi",
        "summary": "Class imbalance remains a major challenge in machine learning, especially for high-dimensional biomedical data where nonlinear manifold structures dominate. Traditional oversampling methods such as SMOTE rely on local linear interpolation, often producing implausible synthetic samples. Deep generative models like Conditional Variational Autoencoders (CVAEs) better capture nonlinear distributions, but standard variants treat all minority samples equally, neglecting the importance of uncertain, boundary-region examples emphasized by heuristic methods like Borderline-SMOTE and ADASYN. We propose Local Entropy-Guided Oversampling with a CVAE (LEO-CVAE), a generative oversampling framework that explicitly incorporates local uncertainty into both representation learning and data generation. To quantify uncertainty, we compute Shannon entropy over the class distribution in a sample's neighborhood: high entropy indicates greater class overlap, serving as a proxy for uncertainty. LEO-CVAE leverages this signal through two mechanisms: (i) a Local Entropy-Weighted Loss (LEWL) that emphasizes robust learning in uncertain regions, and (ii) an entropy-guided sampling strategy that concentrates generation in these informative, class-overlapping areas. Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE consistently improves classifier performance, outperforming both traditional oversampling and generative baselines. These results highlight the value of uncertainty-aware generative oversampling for imbalanced learning in domains governed by complex nonlinear structures, such as omics data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.996808"
    },
    {
        "index": "#142",
        "title": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning",
        "link": "/arxiv/2509.25300",
        "arxiv_id": "2509.25300",
        "authors": "Zelin Tan, Hejia Geng, Mulei Zhang, Xiaohang Yu, Guancheng Wan, Yifan Zhou, Qiang He, Xiangyuan Xue, Heng Zhou, Yutao Fan, Zhongzhi Li, Zaibin Zhang, Guibin Zhang, Chen Zhang, Zhenfei Yin, Lei Bai",
        "summary": "While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on 54 experiments across diverse model sizes and training settings, we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: (1). Under a fixed computational budget, larger models trained for fewer steps consistently outperform smaller models trained for more steps. (2). Given a fixed amount of training data, larger models achieve superior sample efficiency, yielding lower loss. (3). In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. (4). These scaling behaviors are robust across both base and instruction-tuned models, which share similar learning dynamics (e.g., larger models show faster convergence) even while differing in absolute accuracy. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.997516"
    },
    {
        "index": "#143",
        "title": "ClustRecNet: A Novel End-to-End Deep Learning Framework for Clustering Algorithm Recommendation",
        "link": "/arxiv/2509.25289",
        "arxiv_id": "2509.25289",
        "authors": "Mohammadreza Bakhtyari, Bogdan Mazoure, Renato Cordeiro de Amorim, Guillaume Rabusseau, Vladimir Makarenkov",
        "summary": "We introduce ClustRecNet - a novel deep learning (DL)-based recommendation framework for determining the most suitable clustering algorithms for a given dataset, addressing the long-standing challenge of clustering algorithm selection in unsupervised learning. To enable supervised learning in this context, we construct a comprehensive data repository comprising 34,000 synthetic datasets with diverse structural properties. Each of them was processed using 10 popular clustering algorithms. The resulting clusterings were assessed via the Adjusted Rand Index (ARI) to establish ground truth labels, used for training and evaluation of our DL model. The proposed network architecture integrates convolutional, residual, and attention mechanisms to capture both local and global structural patterns from the input data. This design supports end-to-end training to learn compact representations of datasets and enables direct recommendation of the most suitable clustering algorithm, reducing reliance on handcrafted meta-features and traditional Cluster Validity Indices (CVIs). Comprehensive experiments across synthetic and real-world benchmarks demonstrate that our DL model consistently outperforms conventional CVIs (e.g. Silhouette, Calinski-Harabasz, Davies-Bouldin, and Dunn) as well as state-of-the-art AutoML clustering recommendation approaches (e.g. ML2DAC, AutoCluster, and AutoML4Clust). Notably, the proposed model achieves a 0.497 ARI improvement over the Calinski-Harabasz index on synthetic data and a 15.3% ARI gain over the best-performing AutoML approach on real-world data.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.998092"
    },
    {
        "index": "#144",
        "title": "Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using Deep Reinforcement Learning",
        "link": "/arxiv/2509.25284",
        "arxiv_id": "2509.25284",
        "authors": "Oluwaseyi Giwa, Jonathan Shock, Jaco Du Toit, Tobi Awodumila",
        "summary": "Dynamic resource allocation in heterogeneous wireless networks (HetNets) is challenging for traditional methods under varying user loads and channel conditions. We propose a deep reinforcement learning (DRL) framework that jointly optimises transmit power, bandwidth, and scheduling via a multi-objective reward balancing throughput, energy efficiency, and fairness. Using real base station coordinates, we compare Proximal Policy Optimisation (PPO) and Twin Delayed Deep Deterministic Policy Gradient (TD3) against three heuristic algorithms in multiple network scenarios. Our results show that DRL frameworks outperform heuristic algorithms in optimising resource allocation in dynamic networks. These findings highlight key trade-offs in DRL design for future HetNets.",
        "subjects": "Machine Learning, Networking and Internet Architecture, Signal Processing",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.998654"
    },
    {
        "index": "#145",
        "title": "MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series",
        "link": "/arxiv/2509.25278",
        "arxiv_id": "2509.25278",
        "authors": "Payal Mohapatra, Yueyuan Sui, Akash Pandey, Stephen Xia, Qi Zhu",
        "summary": "From clinical healthcare to daily living, continuous sensor monitoring across multiple modalities has shown great promise for real-world intelligent decision-making but also faces various challenges. In this work, we introduce MAESTRO, a novel framework that overcomes key limitations of existing multimodal learning approaches: (1) reliance on a single primary modality for alignment, (2) pairwise modeling of modalities, and (3) assumption of complete modality observations. These limitations hinder the applicability of these approaches in real-world multimodal time-series settings, where primary modality priors are often unclear, the number of modalities can be large (making pairwise modeling impractical), and sensor failures often result in arbitrary missing observations. At its core, MAESTRO facilitates dynamic intra- and cross-modal interactions based on task relevance, and leverages symbolic tokenization and adaptive attention budgeting to construct long multimodal sequences, which are processed via sparse cross-modal attention. The resulting cross-modal tokens are routed through a sparse Mixture-of-Experts (MoE) mechanism, enabling black-box specialization under varying modality combinations. We evaluate MAESTRO against 10 baselines on four diverse datasets spanning three applications, and observe average relative improvements of 4% and 8% over the best existing multimodal and multivariate approaches, respectively, under complete observations. Under partial observations -- with up to 40% of missing modalities -- MAESTRO achieves an average 9% improvement. Further analysis also demonstrates the robustness and efficiency of MAESTRO's sparse, modality-aware design for learning from dynamic time series.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.999213"
    },
    {
        "index": "#146",
        "title": "InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions",
        "link": "/arxiv/2509.25270",
        "arxiv_id": "2509.25270",
        "authors": "Liangjian Wen, Qun Dai, Jianzhuang Liu, Jiangtao Zheng, Yong Dai, Dongkai Wang, Zhao Kang, Jun Wang, Zenglin Xu, Jiang Duan",
        "summary": "In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an \\textbf{Inf}inite \\textbf{Masking} strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:19.999884"
    },
    {
        "index": "#147",
        "title": "A Weather Foundation Model for the Power Grid",
        "link": "/arxiv/2509.25268",
        "arxiv_id": "2509.25268",
        "authors": "Cristian Bodnar, Raphaël Rousseau-Rizzi, Nikhil Shankar, James Merleau, Stylianos Flampouris, Guillem Candille, Slavica Antic, François Miralles, Jayesh K. Gupta",
        "summary": "Weather foundation models (WFMs) have recently set new benchmarks in global forecast skill, yet their concrete value for the weather-sensitive infrastructure that powers modern society remains largely unexplored. In this study, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting Transformer (GFT), on a rich archive of Hydro-Québec asset observations--including transmission-line weather stations, wind-farm met-mast streams, and icing sensors--to deliver hyper-local, asset-level forecasts for five grid-critical variables: surface temperature, precipitation, hub-height wind speed, wind-turbine icing risk, and rime-ice accretion on overhead conductors. Across 6-72 h lead times, the tailored model surpasses state-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE) by 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%. Most importantly, it attains an average precision score of 0.72 for day-ahead rime-ice detection, a capability absent from existing operational systems, which affords several hours of actionable warning for potentially catastrophic outage events. These results show that WFMs, when post-trained with small amounts of high-fidelity, can serve as a practical foundation for next-generation grid-resilience intelligence.",
        "subjects": "Machine Learning, Artificial Intelligence, Atmospheric and Oceanic Physics",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.005676"
    },
    {
        "index": "#149",
        "title": "How Effective Are Time-Series Models for Rainfall Nowcasting? A Comprehensive Benchmark for Rainfall Nowcasting Incorporating PWV Data",
        "link": "/arxiv/2509.25263",
        "arxiv_id": "2509.25263",
        "authors": "Yifang Zhang, Pengfei Duan, Henan Wang, Shengwu Xiong",
        "summary": "Rainfall nowcasting, which aims to predict precipitation within the next 0 to 3 hours, is critical for disaster mitigation and real-time response planning. However, most time series forecasting benchmarks in meteorology are evaluated on variables with strong periodicity, such as temperature and humidity, which fail to reflect model capabilities in more complex and practically meteorology scenarios like rainfall nowcasting. To address this gap, we propose RainfallBench, a benchmark designed for rainfall nowcasting, a highly challenging and practically relevant task characterized by zero inflation, temporal decay, and non-stationarity, focused on predicting precipitation within the next 0 to 3 hours. The dataset is derived from five years of meteorological observations, recorded at 15-minute intervals across six essential variables, and collected from more than 12,000 GNSS stations globally. In particular, it incorporates precipitable water vapor (PWV), a crucial indicator of rainfall that is absent in other datasets. We further design specialized evaluation strategies to assess model performance on key meteorological challenges, such as multi-scale prediction and extreme rainfall events, and evaluate over 20 state-of-the-art models across six major architectures on RainfallBench. Additionally, to address the zero-inflation and temporal decay issues overlooked by existing models, we introduce Bi-Focus Precipitation Forecaster (BFPF), a plug-and-play module that incorporates domain-specific priors to enhance rainfall time series forecasting. Statistical analysis and ablation studies validate the comprehensiveness of our dataset as well as the superiority of our methodology. Code and datasets are available at https://anonymous.4open.science/r/RainfallBench-A710.",
        "subjects": "Machine Learning, Artificial Intelligence, Atmospheric and Oceanic Physics, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.006773"
    },
    {
        "index": "#151",
        "title": "Knowledge distillation through geometry-aware representational alignment",
        "link": "/arxiv/2509.25253",
        "arxiv_id": "2509.25253",
        "authors": "Prajjwal Bhattarai, Mohammad Amjad, Dmytro Zhylko, Tuka Alhanai",
        "summary": "Knowledge distillation is a common paradigm for transferring capabilities from larger models to smaller ones. While traditional distillation methods leverage a probabilistic divergence over the output of the teacher and student models, feature-based distillation methods often minimize variants of Euclidean norms between the hidden layer representations. The main goal is for the student to mimic the structure of the feature space of the teacher. In this work, we theoretically show that existing feature distillation methods, such as projection based mean squared loss or Centered Kernel Alignment (CKA), cannot capture the feature structure, even under zero loss. We then motivate the use of Procrustes distance and the Frobenius norm of Feature Gram Matrix, distances already common in the context of measuring representational alignment, as distillation losses. We show that feature distillation through our method showcases statistically significant improvement in distillation performance across language models families (BERT and OPT) in classification and instruction-following tasks by up to 2 percentage points, showcasing the potential of integrating feature geometry into existing distillation methods.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-27",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.007887"
    },
    {
        "index": "#152",
        "title": "Fine-tuning of Large Language Models for Domain-Specific Cybersecurity Knowledge",
        "link": "/arxiv/2509.25241",
        "arxiv_id": "2509.25241",
        "authors": "Yuan Huang",
        "summary": "Recent advancements in training paradigms for Large Language Models (LLMs) have unlocked their remarkable capabilities in natural language processing and cross-domain generalization. While LLMs excel in tasks like programming and mathematical problem-solving, their zero-shot performance in specialized domains requiring expert knowledge, such as cybersecurity, is often suboptimal. This limitation arises because foundational LLMs are designed for general-purpose applications, constraining their ability to encapsulate domain-specific expertise within their parameter space. To address this, we explore fine-tuning strategies to embed cybersecurity knowledge into LLMs, enhancing their performance in cybersecurity question-answering (Q\\&A) tasks while prioritizing computational efficiency. Specifically, we investigate Supervised Fine-Tuning (SFT), Low-Rank Adaptation (LoRA), and Quantized Low-Rank Adaptation (QLoRA) using a cybersecurity Q\\&A dataset. Our results demonstrate that these fine-tuning approaches significantly outperform the foundational model in cybersecurity Q\\&A tasks. Moreover, LoRA and QLoRA achieve comparable performance to SFT with substantially lower computational costs, offering an efficient pathway for adapting LLMs to specialized domains. Our work highlights the potential of low-rank fine-tuning strategies to bridge the gap between general-purpose LLMs and domain-specific applications.",
        "subjects": "Machine Learning, Cryptography and Security",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.008375"
    },
    {
        "index": "#154",
        "title": "PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases",
        "link": "/arxiv/2509.25238",
        "arxiv_id": "2509.25238",
        "authors": "Sri Vatsa Vuddanti, Aarav Shah, Satwik Kumar Chittiprolu, Tony Song, Sunishchal Dev, Kevin Zhu, Maheep Chaudhary",
        "summary": "Tool-augmented language agents frequently fail in real-world deployment due to tool malfunctions--timeouts, API exceptions, or inconsistent outputs--triggering cascading reasoning errors and task abandonment. Existing agent training pipelines optimize only for success trajectories, failing to expose models to the tool failures that dominate real-world usage. We propose \\textbf{PALADIN}, a generalizable framework for equipping language agents with robust failure recovery capabilities. PALADIN trains on 50,000+ recovery-annotated trajectories constructed via systematic failure injection and expert demonstrations on an enhanced ToolBench dataset. Training uses LoRA-based fine-tuning to retain base capabilities while injecting recovery competence. At inference, PALADIN detects execution-time errors and retrieves the most similar case from a curated bank of 55+ failure exemplars aligned with ToolScan's taxonomy, then executes the corresponding recovery action. This approach generalizes to novel failures beyond the training distribution, retaining 95.2\\% recovery performance on unseen tool APIs. Evaluation across PaladinEval and ToolReflectEval demonstrates consistent improvements in Recovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR), and Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57% relative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%) by +13.3%. Against vanilla agents, PALADIN achieves 89.86\\% RR (+66% relative improvement from 23.75%). These results establish PALADIN as an effective method for building fault-tolerant agents capable of robust recovery in real-world tool environments.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.009590"
    },
    {
        "index": "#155",
        "title": "Machine Learning for Pattern Detection in Printhead Nozzle Logging",
        "link": "/arxiv/2509.25235",
        "arxiv_id": "2509.25235",
        "authors": "Nikola Prianikov, Evelyne Janssen-van Dam, Marcin Pietrasik, Charalampos S. Kouzinopoulos",
        "summary": "Correct identification of failure mechanisms is essential for manufacturers to ensure the quality of their products. Certain failures of printheads developed by Canon Production Printing can be identified from the behavior of individual nozzles, the states of which are constantly recorded and can form distinct patterns in terms of the number of failed nozzles over time, and in space in the nozzle grid. In our work, we investigate the problem of printhead failure classification based on a multifaceted dataset of nozzle logging and propose a Machine Learning classification approach for this problem. We follow the feature-based framework of time-series classification, where a set of time-based and spatial features was selected with the guidance of domain experts. Several traditional ML classifiers were evaluated, and the One-vs-Rest Random Forest was found to have the best performance. The proposed model outperformed an in-house rule-based baseline in terms of a weighted F1 score for several failure mechanisms.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.015621"
    },
    {
        "index": "#156",
        "title": "FedCLF - Towards Efficient Participant Selection for Federated Learning in Heterogeneous IoV Networks",
        "link": "/arxiv/2509.25233",
        "arxiv_id": "2509.25233",
        "authors": "Kasun Eranda Wijethilake, Adnan Mahmood, Quan Z. Sheng",
        "summary": "Federated Learning (FL) is a distributed machine learning technique that preserves data privacy by sharing only the trained parameters instead of the client data. This makes FL ideal for highly dynamic, heterogeneous, and time-critical applications, in particular, the Internet of Vehicles (IoV) networks. However, FL encounters considerable challenges in such networks owing to the high data and device heterogeneity. To address these challenges, we propose FedCLF, i.e., FL with Calibrated Loss and Feedback control, which introduces calibrated loss as a utility in the participant selection process and a feedback control mechanism to dynamically adjust the sampling frequency of the clients. The envisaged approach (a) enhances the overall model accuracy in case of highly heterogeneous data and (b) optimizes the resource utilization for resource constrained IoV networks, thereby leading to increased efficiency in the FL process. We evaluated FedCLF vis-à-vis baseline models, i.e., FedAvg, Newt, and Oort, using CIFAR-10 dataset with varying data heterogeneity. Our results depict that FedCLF significantly outperforms the baseline models by up to a 16% improvement in high data heterogeneity-related scenarios with improved efficiency via reduced sampling frequency.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.016181"
    },
    {
        "index": "#157",
        "title": "Sampling via Gaussian Mixture Approximations",
        "link": "/arxiv/2509.25232",
        "arxiv_id": "2509.25232",
        "authors": "Yongchao Huang",
        "summary": "We present a family of \\textit{Gaussian Mixture Approximation} (GMA) samplers for sampling unnormalised target densities, encompassing \\textit{weights-only GMA} (W-GMA), \\textit{Laplace Mixture Approximation} (LMA), \\textit{expectation-maximization GMA} (EM-GMA), and further variants. GMA adopts a simple two-stage paradigm: (i) initialise a finite set of Gaussian components and draw samples from a proposal mixture; (ii) fit the mixture to the target by optimising either only the component weights or also the means and variances, via a sample-based KL divergence objective that requires only evaluations of the unnormalised density, followed by stratified resampling. The method is gradient-free, and computationally efficient: it leverages the ease of sampling from Gaussians, efficient optimisation methods (projected gradient descent, mirror descent, and EM), and the robustness of stratified resampling to produce samples faithful to the target. We show that this optimisation-resampling scheme yields consistent approximations under mild conditions, and we validate this methodology with empirical results demonstrating accuracy and speed across diverse densities.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.016640"
    },
    {
        "index": "#158",
        "title": "WDformer: A Wavelet-based Differential Transformer Model for Time Series Forecasting",
        "link": "/arxiv/2509.25231",
        "arxiv_id": "2509.25231",
        "authors": "Xiaojian Wang, Chaoli Zhang, Zhonglong Zheng, Yunliang Jiang",
        "summary": "Time series forecasting has various applications, such as meteorological rainfall prediction, traffic flow analysis, financial forecasting, and operational load monitoring for various systems. Due to the sparsity of time series data, relying solely on time-domain or frequency-domain modeling limits the model's ability to fully leverage multi-domain information. Moreover, when applied to time series forecasting tasks, traditional attention mechanisms tend to over-focus on irrelevant historical information, which may introduce noise into the prediction process, leading to biased results. We proposed WDformer, a wavelet-based differential Transformer model. This study employs the wavelet transform to conduct a multi-resolution analysis of time series data. By leveraging the advantages of joint representation in the time-frequency domain, it accurately extracts the key information components that reflect the essential characteristics of the data. Furthermore, we apply attention mechanisms on inverted dimensions, allowing the attention mechanism to capture relationships between multiple variables. When performing attention calculations, we introduced the differential attention mechanism, which computes the attention score by taking the difference between two separate softmax attention matrices. This approach enables the model to focus more on important information and reduce noise. WDformer has achieved state-of-the-art (SOTA) results on multiple challenging real-world datasets, demonstrating its accuracy and effectiveness. Code is available at https://github.com/xiaowangbc/WDformer.",
        "subjects": "Machine Learning",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.017165"
    },
    {
        "index": "#159",
        "title": "Energy Guided Geometric Flow Matching",
        "link": "/arxiv/2509.25230",
        "arxiv_id": "2509.25230",
        "authors": "Aaron Zweig, Mingxuan Zhang, Elham Azizi, David Knowles",
        "summary": "A useful inductive bias for temporal data is that trajectories should stay close to the data manifold. Traditional flow matching relies on straight conditional paths, and flow matching methods which learn geodesics rely on RBF kernels or nearest neighbor graphs that suffer from the curse of dimensionality. We propose to use score matching and annealed energy distillation to learn a metric tensor that faithfully captures the underlying data geometry and informs more accurate flows. We demonstrate the efficacy of this strategy on synthetic manifolds with analytic geodesics, and interpolation of cell",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.017714"
    },
    {
        "index": "#160",
        "title": "Simple, Fast and Efficient Injective Manifold Density Estimation with Random Projections",
        "link": "/arxiv/2509.25228",
        "arxiv_id": "2509.25228",
        "authors": "Ahmad Ayaz Amin",
        "summary": "We introduce Random Projection Flows (RPFs), a principled framework for injective normalizing flows that leverages tools from random matrix theory and the geometry of random projections. RPFs employ random semi-orthogonal matrices, drawn from Haar-distributed orthogonal ensembles via QR decomposition of Gaussian matrices, to project data into lower-dimensional latent spaces for the base distribution. Unlike PCA-based flows or learned injective maps, RPFs are plug-and-play, efficient, and yield closed-form expressions for the Riemannian volume correction term. We demonstrate that RPFs are both theoretically grounded and practically effective, providing a strong baseline for generative modeling and a bridge between random projection theory and normalizing flows.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.018176"
    },
    {
        "index": "#161",
        "title": "Integrated Forecasting of Marine Renewable Power: An Adaptively Bayesian-Optimized MVMD-LSTM Framework for Wind-Solar-Wave Energy",
        "link": "/arxiv/2509.25226",
        "arxiv_id": "2509.25226",
        "authors": "Baoyi Xie, Shuiling Shi, Wenqi Liu",
        "summary": "Integrated wind-solar-wave marine energy systems hold broad promise for supplying clean electricity in offshore and coastal regions. By leveraging the spatiotemporal complementarity of multiple resources, such systems can effectively mitigate the intermittency and volatility of single-source outputs, thereby substantially improving overall power-generation efficiency and resource utilization. Accurate ultra-short-term forecasting is crucial for ensuring secure operation and optimizing proactive dispatch. However, most existing forecasting methods construct separate models for each energy source, insufficiently account for the complex couplings among multiple energies, struggle to capture the system's nonlinear and nonstationary dynamics, and typically depend on extensive manual parameter tuning-limitations that constrain both predictive performance and practicality. We address this issue using a Bayesian-optimized Multivariate Variational Mode Decomposition-Long Short-Term Memory (MVMD-LSTM) framework. The framework first applies MVMD to jointly decompose wind, solar and wave power series so as to preserve cross-source couplings; it uses Bayesian optimization to automatically search the number of modes and the penalty parameter in the MVMD process to obtain intrinsic mode functions (IMFs); finally, an LSTM models the resulting IMFs to achieve ultra-short-term power forecasting for the integrated system. Experiments based on field measurements from an offshore integrated energy platform in China show that the proposed framework significantly outperforms benchmark models in terms of MAPE, RMSE and MAE. The results demonstrate superior predictive accuracy, robustness, and degree of automation.",
        "subjects": "Machine Learning, Optimization and Control",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.018735"
    },
    {
        "index": "#162",
        "title": "MSCoD: An Enhanced Bayesian Updating Framework with Multi-Scale Information Bottleneck and Cooperative Attention for Structure-Based Drug Design",
        "link": "/arxiv/2509.25225",
        "arxiv_id": "2509.25225",
        "authors": "Long Xu, Yongcai Chen, Fengshuo Liu, Yuzhong Peng",
        "summary": "Structure-Based Drug Design (SBDD) is a powerful strategy in computational drug discovery, utilizing three-dimensional protein structures to guide the design of molecules with improved binding affinity. However, capturing complex protein-ligand interactions across multiple scales remains challenging, as current methods often overlook the hierarchical organization and intrinsic asymmetry of these interactions. To address these limitations, we propose MSCoD, a novel Bayesian updating-based generative framework for structure-based drug design. In our MSCoD, Multi-Scale Information Bottleneck (MSIB) was developed, which enables semantic compression at multiple abstraction levels for efficient hierarchical feature extraction. Furthermore, a multi-head cooperative attention (MHCA) mechanism was developed, which employs asymmetric protein-to-ligand attention to capture diverse interaction types while addressing the dimensionality disparity between proteins and ligands. Empirical studies showed that MSCoD outperforms state-of-the-art methods on the benchmark dataset. Case studies on challenging targets such as KRAS G12D further demonstrate its applicability in real-world scenarios. The code and data underlying this article are freely available at https://github.com/xulong0826/MSCoD.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.019248"
    },
    {
        "index": "#163",
        "title": "AMLA: MUL by ADD in FlashAttention Rescaling",
        "link": "/arxiv/2509.25224",
        "arxiv_id": "2509.25224",
        "authors": "Qichen Liao, Chengqiu Hu, Fangzheng Miao, Bao Li, Yiyang Liu, Junlong Lyu, Lirui Jiang, Jun Wang, Lingchao Zheng, Jun Li, Yuwei Fan",
        "summary": "Multi-head Latent Attention (MLA) significantly reduces KVCache memory usage in Large Language Models while introducing substantial computational overhead and intermediate variable expansion. This poses challenges for efficient hardware implementation -- especially during the decode phase. This paper introduces Ascend MLA (AMLA), a high-performance kernel specifically optimized for Huawei's Ascend NPUs. AMLA is built on two core innovations: (1) A novel FlashAttention-based algorithm that replaces floating-point multiplications with integer additions for output block rescaling, leveraging binary correspondence between FP32 and INT32 representations; (2) A Preload Pipeline strategy with hierarchical tiling that maximizes FLOPS utilization: the Preload Pipeline achieves Cube-bound performance, while hierarchical tiling overlaps data movement and computation within the Cube core. Experiments show that on Ascend 910 NPUs (integrated in CloudMatrix384), AMLA achieves up to 614 TFLOPS, reaching 86.8% of the theoretical maximum FLOPS, outperforming the state-of-the-art open-source FlashMLA implementation, whose FLOPS utilization is up to 66.7% on NVIDIA H800 SXM5. The AMLA kernel has been integrated into Huawei's CANN and will be released soon.",
        "subjects": "Machine Learning",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.019894"
    },
    {
        "index": "#164",
        "title": "Enhancing Linear Attention with Residual Learning",
        "link": "/arxiv/2509.25223",
        "arxiv_id": "2509.25223",
        "authors": "Xunhao Lai, Jialiang Kang, Jianqiao Lu, Tong Lin, Pengyu Zhao",
        "summary": "Linear attention offers a linear-time alternative to self-attention but often struggles to capture long-range patterns. We revisit linear attention through a prediction-correction lens and show that prevalent variants can be written as a combination of a historical prediction and a single-token correction, which creates an expressivity bottleneck. To address this bottleneck, we introduce Residual Linear Attention (RLA), a framework that equips linear attention with an explicit residual-fitting mechanism. RLA maintains an auxiliary recurrent state that learns to accumulate residual errors over time and correct the base prediction. We further instantiate a delta-rule version, Residual Delta Net (RDN), incorporating adaptive gating and residual clipping for enhanced correction control and stability. Our implementation leverages highly optimized linear attention kernels and preserves linear time and memory. Across language modeling and recall-intensive evaluations, RLA and RDN consistently outperform their respective baselines and other modern linear-attention methods, narrowing the gap to standard Transformers while retaining linear scaling.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.020455"
    },
    {
        "index": "#165",
        "title": "Sensor optimization for urban wind estimation with cluster-based probabilistic framework",
        "link": "/arxiv/2509.25222",
        "arxiv_id": "2509.25222",
        "authors": "Yutong Liang, Chang Hou, Guy Y. Cornejo Maceda, Andrea Ianiro, Stefano Discetti, Andrea Meilán-Vila, Didier Sornette, Sandro Claudio Lera, Jialong Chen, Xiaozhou He, Bernd R. Noack",
        "summary": "We propose a physics-informed machine-learned framework for sensor-based flow estimation for drone trajectories in complex urban terrain. The input is a rich set of flow simulations at many wind conditions. The outputs are velocity and uncertainty estimates for a target domain and subsequent sensor optimization for minimal uncertainty. The framework has three innovations compared to traditional flow estimators. First, the algorithm scales proportionally to the domain complexity, making it suitable for flows that are too complex for any monolithic reduced-order representation. Second, the framework extrapolates beyond the training data, e.g., smaller and larger wind velocities. Last, and perhaps most importantly, the sensor location is a free input, significantly extending the vast majority of the literature. The key enablers are (1) a Reynolds number-based scaling of the flow variables, (2) a physics-based domain decomposition, (3) a cluster-based flow representation for each subdomain, (4) an information entropy correlating the subdomains, and (5) a multi-variate probability function relating sensor input and targeted velocity estimates. This framework is demonstrated using drone flight paths through a three-building cluster as a simple example. We anticipate adaptations and applications for estimating complete cities and incorporating weather input.",
        "subjects": "Machine Learning, Robotics, Fluid Dynamics",
        "date": "2025-09-24",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.026259"
    },
    {
        "index": "#166",
        "title": "On The Dynamic Ensemble Selection for TinyML-based Systems -- a Preliminary Study",
        "link": "/arxiv/2509.25218",
        "arxiv_id": "2509.25218",
        "authors": "Tobiasz Puslecki, Krzysztof Walkowiak",
        "summary": "The recent progress in TinyML technologies triggers the need to address the challenge of balancing inference time and classification quality. TinyML systems are defined by specific constraints in computation, memory and energy. These constraints emphasize the need for specialized optimization techniques when implementing Machine Learning (ML) applications on such platforms. While deep neural networks are widely used in TinyML, the exploration of Dynamic Ensemble Selection (DES) methods is also beneficial. This study examines a DES-Clustering approach for a multi-class computer vision task within TinyML systems. This method allows for adjusting classification accuracy, thereby affecting latency and energy consumption per inference. We implemented the TinyDES-Clustering library, optimized for embedded system limitations. Experiments have shown that a larger pool of classifiers for dynamic selection improves classification accuracy, and thus leads to an increase in average inference time on the TinyML device.",
        "subjects": "Machine Learning",
        "date": "2025-09-22",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.026736"
    },
    {
        "index": "#167",
        "title": "Learning to Condition: A Neural Heuristic for Scalable MPE Inference",
        "link": "/arxiv/2509.25217",
        "arxiv_id": "2509.25217",
        "authors": "Brij Malhotra, Shivvrat Arya, Tahrima Rahman, Vibhav Giridhar Gogate",
        "summary": "We introduce learning to condition (L2C), a scalable, data-driven framework for accelerating Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs), a fundamentally intractable problem. L2C trains a neural network to score variable-value assignments based on their utility for conditioning, given observed evidence. To facilitate supervised learning, we develop a scalable data generation pipeline that extracts training signals from the search traces of existing MPE solvers. The trained network serves as a heuristic that integrates with search algorithms, acting as a conditioning strategy prior to exact inference or as a branching and node selection policy within branch-and-bound solvers. We evaluate L2C on challenging MPE queries involving high-treewidth PGMs. Experiments show that our learned heuristic significantly reduces the search space while maintaining or improving solution quality over state-of-the-art methods.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.027284"
    },
    {
        "index": "#168",
        "title": "Evaluating Double Descent in Machine Learning: Insights from Tree-Based Models Applied to a Genomic Prediction Task",
        "link": "/arxiv/2509.25216",
        "arxiv_id": "2509.25216",
        "authors": "Guillermo Comesaña Cimadevila",
        "summary": "Classical learning theory describes a well-characterised U-shaped relationship between model complexity and prediction error, reflecting a transition from underfitting in underparameterised regimes to overfitting as complexity grows. Recent work, however, has introduced the notion of a second descent in test error beyond the interpolation threshold-giving rise to the so-called double descent phenomenon. While double descent has been studied extensively in the context of deep learning, it has also been reported in simpler models, including decision trees and gradient boosting. In this work, we revisit these claims through the lens of classical machine learning applied to a biological classification task: predicting isoniazid resistance in Mycobacterium tuberculosis using whole-genome sequencing data. We systematically vary model complexity along two orthogonal axes-learner capacity (e.g., Pleaf, Pboost) and ensemble size (i.e., Pens)-and show that double descent consistently emerges only when complexity is scaled jointly across these axes. When either axis is held fixed, generalisation behaviour reverts to classical U- or L-shaped patterns. These results are replicated on a synthetic benchmark and support the unfolding hypothesis, which attributes double descent to the projection of distinct generalisation regimes onto a single complexity axis. Our findings underscore the importance of treating model complexity as a multidimensional construct when analysing generalisation behaviour. All code and reproducibility materials are available at: https://github.com/guillermocomesanacimadevila/Demystifying-Double-Descent-in-ML.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-22",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.027783"
    },
    {
        "index": "#169",
        "title": "Anomaly detection by partitioning of multi-variate time series",
        "link": "/arxiv/2509.25215",
        "arxiv_id": "2509.25215",
        "authors": "Pierre Lotte, André Péninou, Olivier Teste",
        "summary": "In this article, we suggest a novel non-supervised partition based anomaly detection method for anomaly detection in multivariate time series called PARADISE. This methodology creates a partition of the variables of the time series while ensuring that the inter-variable relations remain untouched. This partitioning relies on the clustering of multiple correlation coefficients between variables to identify subsets of variables before executing anomaly detection algorithms locally for each of those subsets. Through multiple experimentations done on both synthetic and real datasets coming from the literature, we show the relevance of our approach with a significant improvement in anomaly detection performance.",
        "subjects": "Machine Learning, Signal Processing, Machine Learning",
        "date": "2025-09-22",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.028316"
    },
    {
        "index": "#170",
        "title": "On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs",
        "link": "/arxiv/2509.25214",
        "arxiv_id": "2509.25214",
        "authors": "Rongguang Ye, Ming Tang, Edith C. H. Ngai",
        "summary": "As increasingly large pre-trained models are released, deploying them on edge devices for privacy-preserving applications requires effective compression. Recent works combine quantization with the fine-tuning of high-precision LoRA adapters, which can substantially reduce model size while mitigating the accuracy loss from quantization. However, edge devices have inherently heterogeneous capabilities, while performing configuration-wise fine-tuning for every quantization setting is computationally prohibitive. In this paper, we propose CoA-LoRA, a method that dynamically adjusts the LoRA adapter to arbitrary quantization configurations (i.e., the per-layer bit-width choices of a pre-trained model) without requiring repeated fine-tuning. This is accomplished via a configuration-aware model that maps each configuration to its low-rank adjustments. The effectiveness of this model critically depends on the training configuration set, a collection of configurations chosen to cover different total bit-width budgets. However, constructing a high-quality configuration set is non-trivial. We therefore design a Pareto-based configuration search that iteratively optimizes the training configuration set, yielding more precise low-rank adjustments. Our experiments demonstrate that, unlike the state-of-the-art methods that require fine-tuning a separate LoRA adapter for each configuration, CoA-LoRA incurs no additional time cost while achieving comparable or even superior performance to those methods.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-22",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.028832"
    },
    {
        "index": "#171",
        "title": "Six Sigma For Neural Networks: Taguchi-based optimization",
        "link": "/arxiv/2509.25213",
        "arxiv_id": "2509.25213",
        "authors": "Sai Varun Kodathala",
        "summary": "The optimization of hyperparameters in convolutional neural networks (CNNs) remains a challenging and computationally expensive process, often requiring extensive trial-and-error approaches or exhaustive grid searches. This study introduces the application of Taguchi Design of Experiments methodology, a statistical optimization technique traditionally used in quality engineering, to systematically optimize CNN hyperparameters for professional boxing action recognition. Using an L12(211) orthogonal array, eight hyperparameters including image size, color mode, activation function, learning rate, rescaling, shuffling, vertical flip, and horizontal flip were systematically evaluated across twelve experimental configurations. To address the multi-objective nature of machine learning optimization, five different approaches were developed to simultaneously optimize training accuracy, validation accuracy, training loss, and validation loss using Signal-to-Noise ratio analysis. The study employed a novel logarithmic scaling technique to unify conflicting metrics and enable comprehensive multi-quality assessment within the Taguchi framework. Results demonstrate that Approach 3, combining weighted accuracy metrics with logarithmically transformed loss functions, achieved optimal performance with 98.84% training accuracy and 86.25% validation accuracy while maintaining minimal loss values. The Taguchi analysis revealed that learning rate emerged as the most influential parameter, followed by image size and activation function, providing clear guidance for hyperparameter prioritization in CNN optimization.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2025-09-22",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.029337"
    },
    {
        "index": "#172",
        "title": "LEMs: A Primer On Large Execution Models",
        "link": "/arxiv/2509.25211",
        "arxiv_id": "2509.25211",
        "authors": "Remi Genet, Hugo Inzirillo",
        "summary": "This paper introduces Large Execution Models (LEMs), a novel deep learning framework that extends transformer-based architectures to address complex execution problems with flexible time boundaries and multiple execution constraints. Building upon recent advances in neural VWAP execution strategies, LEMs generalize the approach from fixed-duration orders to scenarios where execution duration is bounded between minimum and maximum time horizons, similar to share buyback contract structures. The proposed architecture decouples market information processing from execution allocation decisions: a common feature extraction pipeline using Temporal Kolmogorov-Arnold Networks (TKANs), Variable Selection Networks (VSNs), and multi-head attention mechanisms processes market data to create informational context, while independent allocation networks handle the specific execution logic for different scenarios (fixed quantity vs. fixed notional, buy vs. sell orders). This architectural separation enables a unified model to handle diverse execution objectives while leveraging shared market understanding across scenarios. Through comprehensive empirical evaluation on intraday cryptocurrency markets and multi-day equity trading using DOW Jones constituents, we demonstrate that LEMs achieve superior execution performance compared to traditional benchmarks by dynamically optimizing execution paths within flexible time constraints. The unified model architecture enables deployment across different execution scenarios (buy/sell orders, varying duration boundaries, volume/notional targets) through a single framework, providing significant operational advantages over asset-specific approaches.",
        "subjects": "Machine Learning, Computational Finance",
        "date": "2025-09-21",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.029834"
    },
    {
        "index": "#173",
        "title": "STCast: Adaptive Boundary Alignment for Global and Regional Weather Forecasting",
        "link": "/arxiv/2509.25210",
        "arxiv_id": "2509.25210",
        "authors": "Hao Chen, Tao Han, Jie Zhang, Song Guo, Lei Bai",
        "summary": "To gain finer regional forecasts, many works have explored the regional integration from the global atmosphere, e.g., by solving boundary equations in physics-based methods or cropping regions from global forecasts in data-driven methods. However, the effectiveness of these methods is often constrained by static and imprecise regional boundaries, resulting in poor generalization ability. To address this issue, we propose Spatial-Temporal Weather Forecasting (STCast), a novel AI-driven framework for adaptive regional boundary optimization and dynamic monthly forecast allocation. Specifically, our approach employs a Spatial-Aligned Attention (SAA) mechanism, which aligns global and regional spatial distributions to initialize boundaries and adaptively refines them based on attention-derived alignment patterns. Furthermore, we design a Temporal Mixture-of-Experts (TMoE) module, where atmospheric variables from distinct months are dynamically routed to specialized experts using a discrete Gaussian distribution, enhancing the model's ability to capture temporal patterns. Beyond global and regional forecasting, we evaluate our STCast on extreme event prediction and ensemble forecasting. Experimental results demonstrate consistent superiority over state-of-the-art methods across all four tasks.",
        "subjects": "Machine Learning, Artificial Intelligence, Atmospheric and Oceanic Physics",
        "date": "2025-09-21",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.030411"
    },
    {
        "index": "#174",
        "title": "DPSformer: A long-tail-aware model for improving heavy rainfall prediction",
        "link": "/arxiv/2509.25208",
        "arxiv_id": "2509.25208",
        "authors": "Zenghui Huang, Ting Shu, Zhonglei Wang, Yang Lu, Yan Yan, Wei Zhong, Hanzi Wang",
        "summary": "Accurate and timely forecasting of heavy rainfall remains a critical challenge for modern society. Precipitation exhibits a highly imbalanced distribution: most observations record no or light rain, while heavy rainfall events are rare. Such an imbalanced distribution obstructs deep learning models from effectively predicting heavy rainfall events. To address this challenge, we treat rainfall forecasting explicitly as a long-tailed learning problem, identifying the insufficient representation of heavy rainfall events as the primary barrier to forecasting accuracy. Therefore, we introduce DPSformer, a long-tail-aware model that enriches representation of heavy rainfall events through a high-resolution branch. For heavy rainfall events $ \\geq $ 50 mm/6 h, DPSformer lifts the Critical Success Index (CSI) of a baseline Numerical Weather Prediction (NWP) model from 0.012 to 0.067. For the top 1% coverage of heavy rainfall events, its Fraction Skill Score (FSS) exceeds 0.45, surpassing existing methods. Our work establishes an effective long-tailed paradigm for heavy rainfall prediction, offering a practical tool to enhance early warning systems and mitigate the societal impacts of extreme weather events.",
        "subjects": "Machine Learning, Atmospheric and Oceanic Physics",
        "date": "2025-09-20",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.036182"
    },
    {
        "index": "#175",
        "title": "Multi-level Diagnosis and Evaluation for Robust Tabular Feature Engineering with Large Language Models",
        "link": "/arxiv/2509.25207",
        "arxiv_id": "2509.25207",
        "authors": "Yebin Lim, Susik Yoon",
        "summary": "Recent advancements in large language models (LLMs) have shown promise in feature engineering for tabular data, but concerns about their reliability persist, especially due to variability in generated outputs. We introduce a multi-level diagnosis and evaluation framework to assess the robustness of LLMs in feature engineering across diverse domains, focusing on the three main factors: key variables, relationships, and decision boundary values for predicting target classes. We demonstrate that the robustness of LLMs varies significantly over different datasets, and that high-quality LLM-generated features can improve few-shot prediction performance by up to 10.52%. This work opens a new direction for assessing and enhancing the reliability of LLM-driven feature engineering in various domains.",
        "subjects": "Machine Learning, Artificial Intelligence",
        "date": "2025-09-20",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.036709"
    },
    {
        "index": "#176",
        "title": "Hyperbolic Optimization",
        "link": "/arxiv/2509.25206",
        "arxiv_id": "2509.25206",
        "authors": "Yanke Wang, Kyriakos Flouris",
        "summary": "This work explores optimization methods on hyperbolic manifolds. Building on Riemannian optimization principles, we extend the Hyperbolic Stochastic Gradient Descent (a specialization of Riemannian SGD) to a Hyperbolic Adam optimizer. While these methods are particularly relevant for learning on the Poincaré ball, they may also provide benefits in Euclidean and other non-Euclidean settings, as the chosen optimization encourages the learning of Poincaré embeddings. This representation, in turn, accelerates convergence in the early stages of training, when parameters are far from the optimum. As a case study, we train diffusion models using the hyperbolic optimization methods with hyperbolic time-discretization of the Langevin dynamics, and show that they achieve faster convergence on certain datasets without sacrificing generative quality.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2025-09-20",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.037207"
    },
    {
        "index": "#177",
        "title": "Polynomial Contrastive Learning for Privacy-Preserving Representation Learning on Graphs",
        "link": "/arxiv/2509.25205",
        "arxiv_id": "2509.25205",
        "authors": "Daksh Pandey",
        "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations on graph data without requiring manual labels. However, leading SSL methods like GRACE are fundamentally incompatible with privacy-preserving technologies such as Homomorphic Encryption (HE) due to their reliance on non-polynomial operations. This paper introduces Poly-GRACE, a novel framework for HE-compatible self-supervised learning on graphs. Our approach consists of a fully polynomial-friendly Graph Convolutional Network (GCN) encoder and a novel, polynomial-based contrastive loss function. Through experiments on three benchmark datasets -- Cora, CiteSeer, and PubMed -- we demonstrate that Poly-GRACE not only enables private pre-training but also achieves performance that is highly competitive with, and in the case of CiteSeer, superior to the standard non-private baseline. Our work represents a significant step towards practical and high-performance privacy-preserving graph representation learning.",
        "subjects": "Machine Learning, Cryptography and Security, Rings and Algebras",
        "date": "2025-09-19",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.037721"
    },
    {
        "index": "#179",
        "title": "VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving with Eroded Gaps",
        "link": "/arxiv/2509.25202",
        "arxiv_id": "2509.25202",
        "authors": "Zhuoning Xu, Xinyan Liu",
        "summary": "Jigsaw puzzle solving remains challenging in computer vision, requiring an understanding of both local fragment details and global spatial relationships. While most traditional approaches only focus on visual cues like edge matching and visual coherence, few methods explore natural language descriptions for semantic guidance in challenging scenarios, especially for eroded gap puzzles. We propose a vision-language framework that leverages textual context to enhance puzzle assembly performance. Our approach centers on the Vision-Language Hierarchical Semantic Alignment (VLHSA) module, which aligns visual patches with textual descriptions through multi-level semantic matching from local tokens to global context. Also, a multimodal architecture that combines dual visual encoders with language features for cross-modal reasoning is integrated into this module. Experiments demonstrate that our method significantly outperforms state-of-the-art models across various datasets, achieving substantial improvements, including a 14.2 percentage point gain in piece accuracy. Ablation studies confirm the critical role of the VLHSA module in driving improvements over vision-only approaches. Our work establishes a new paradigm for jigsaw puzzle solving by incorporating multimodal semantic insights.",
        "subjects": "Machine Learning",
        "date": "2025-09-17",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.038775"
    },
    {
        "index": "#180",
        "title": "SOLD: SELFIES-based Objective-driven Latent Diffusion",
        "link": "/arxiv/2509.25198",
        "arxiv_id": "2509.25198",
        "authors": "Elbert Ho",
        "summary": "Recently, machine learning has made a significant impact on de novo drug design. However, current approaches to creating novel molecules conditioned on a target protein typically rely on generating molecules directly in the 3D conformational space, which are often slow and overly complex. In this work, we propose SOLD (SELFIES-based Objective-driven Latent Diffusion), a novel latent diffusion model that generates molecules in a latent space derived from 1D SELFIES strings and conditioned on a target protein. In the process, we also train an innovative SELFIES transformer and propose a new way to balance losses when training multi-task machine learning models.Our model generates high-affinity molecules for the target protein in a simple and efficient way, while also leaving room for future improvements through the addition of more data.",
        "subjects": "Machine Learning",
        "date": "2025-09-03",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.039235"
    },
    {
        "index": "#181",
        "title": "Stitch: Training-Free Position Control in Multimodal Diffusion Transformers",
        "link": "/arxiv/2509.26644",
        "arxiv_id": "2509.26644",
        "authors": "Jessica Bader, Mateusz Pach, Maria A. Bravo, Serge Belongie, Zeynep Akata",
        "summary": "Text-to-Image (T2I) generation models have advanced rapidly in recent years, but accurately capturing spatial relationships like \"above\" or \"to the right of\" poses a persistent challenge. Earlier methods improved spatial relationship following with external position control. However, as architectures evolved to enhance image quality, these techniques became incompatible with modern models. We propose Stitch, a training-free method for incorporating external position control into Multi-Modal Diffusion Transformers (MMDiT) via automatically-generated bounding boxes. Stitch produces images that are both spatially accurate and visually appealing by generating individual objects within designated bounding boxes and seamlessly stitching them together. We find that targeted attention heads capture the information necessary to isolate and cut out individual objects mid-generation, without needing to fully complete the image. We evaluate Stitch on PosEval, our benchmark for position-based T2I generation. Featuring five new tasks that extend the concept of Position beyond the basic GenEval task, PosEval demonstrates that even top models still have significant room for improvement in position-based generation. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances base models, even improving FLUX by 218% on GenEval's Position task and by 206% on PosEval. Stitch achieves state-of-the-art results with Qwen-Image on PosEval, improving over previous models by 54%, all accomplished while integrating position control into leading models training-free. Code is available at https://github.com/ExplainableML/Stitch.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.039843"
    },
    {
        "index": "#183",
        "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction",
        "link": "/arxiv/2509.26633",
        "arxiv_id": "2509.26633",
        "authors": "Lujie Yang, Xiaoyu Huang, Zhen Wu, Angjoo Kanazawa, Pieter Abbeel, Carmelo Sferrazza, C. Karen Liu, Rocky Duan, Guanya Shi",
        "summary": "A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum.",
        "subjects": "Robotics, Artificial Intelligence, Machine Learning, Systems and Control",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.041003"
    },
    {
        "index": "#184",
        "title": "TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance",
        "link": "/arxiv/2509.26627",
        "arxiv_id": "2509.26627",
        "authors": "Yuyang Liu, Chuan Wen, Yihang Hu, Dinesh Jayaraman, Yang Gao",
        "summary": "Designing dense rewards is crucial for reinforcement learning (RL), yet in robotics it often demands extensive manual effort and lacks scalability. One promising solution is to view task progress as a dense reward signal, as it quantifies the degree to which actions advance the system toward task completion over time. We present TimeRewarder, a simple yet effective reward learning method that derives progress estimation signals from passive videos, including robot demonstrations and human videos, by modeling temporal distances between frame pairs. We then demonstrate how TimeRewarder can supply step-wise proxy rewards to guide reinforcement learning. In our comprehensive experiments on ten challenging Meta-World tasks, we show that TimeRewarder dramatically improves RL for sparse-reward tasks, achieving nearly perfect success in 9/10 tasks with only 200,000 interactions per task with the environment. This approach outperformed previous methods and even the manually designed environment dense reward on both the final success rate and sample efficiency. Moreover, we show that TimeRewarder pretraining can exploit real-world human videos, highlighting its potential as a scalable approach path to rich reward signals from diverse video sources.",
        "subjects": "Artificial Intelligence, Machine Learning, Robotics",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.046724"
    },
    {
        "index": "#185",
        "title": "Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning",
        "link": "/arxiv/2509.26605",
        "arxiv_id": "2509.26605",
        "authors": "Maël Macuglia, Paul Friedrich, Giorgia Ramponi",
        "summary": "Deploying reinforcement learning (RL) in robotics, industry, and health care is blocked by two obstacles: the difficulty of specifying accurate rewards and the risk of unsafe, data-hungry exploration. We address this by proposing a two-stage framework that first learns a safe initial policy from a reward-free dataset of expert demonstrations, then fine-tunes it online using preference-based human feedback. We provide the first principled analysis of this offline-to-online approach and introduce BRIDGE, a unified algorithm that integrates both signals via an uncertainty-weighted objective. We derive regret bounds that shrink with the number of offline demonstrations, explicitly connecting the quantity of offline data to online sample efficiency. We validate BRIDGE in discrete and continuous control MuJoCo environments, showing it achieves lower regret than both standalone behavioral cloning and online preference-based RL. Our work establishes a theoretical foundation for designing more sample-efficient interactive agents.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.057735"
    },
    {
        "index": "#188",
        "title": "Are Robust LLM Fingerprints Adversarially Robust?",
        "link": "/arxiv/2509.26598",
        "arxiv_id": "2509.26598",
        "authors": "Anshul Nasery, Edoardo Contente, Alkin Kaz, Pramod Viswanath, Sewoong Oh",
        "summary": "Model fingerprinting has emerged as a promising paradigm for claiming model ownership. However, robustness evaluations of these schemes have mostly focused on benign perturbations such as incremental fine-tuning, model merging, and prompting. Lack of systematic investigations into {\\em adversarial robustness} against a malicious model host leaves current systems vulnerable. To bridge this gap, we first define a concrete, practical threat model against model fingerprinting. We then take a critical look at existing model fingerprinting schemes to identify their fundamental vulnerabilities. Based on these, we develop adaptive adversarial attacks tailored for each vulnerability, and demonstrate that these can bypass model authentication completely for ten recently proposed fingerprinting schemes while maintaining high utility of the model for the end users. Our work encourages fingerprint designers to adopt adversarial robustness by design. We end with recommendations for future fingerprinting methods.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.060010"
    },
    {
        "index": "#189",
        "title": "Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models",
        "link": "/arxiv/2509.26584",
        "arxiv_id": "2509.26584",
        "authors": "Matheus Vinicius da Silva de Oliveira, Jonathan de Andrade Silva, Awdren de Lima Fontao",
        "summary": "Large Language Models (LLMs) are widely used across multiple domains but continue to raise concerns regarding security and fairness. Beyond known attack vectors such as data poisoning and prompt injection, LLMs are also vulnerable to fairness bugs. These refer to unintended behaviors influenced by sensitive demographic cues (e.g., race or sexual orientation) that should not affect outcomes. Another key issue is hallucination, where models generate plausible yet false information. Retrieval-Augmented Generation (RAG) has emerged as a strategy to mitigate hallucinations by combining external retrieval with text generation. However, its adoption raises new fairness concerns, as the retrieved content itself may surface or amplify bias. This study conducts fairness testing through metamorphic testing (MT), introducing controlled demographic perturbations in prompts to assess fairness in sentiment analysis performed by three Small Language Models (SLMs) hosted on HuggingFace (Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B), each integrated into a RAG pipeline. Results show that minor demographic variations can break up to one third of metamorphic relations (MRs). A detailed analysis of these failures reveals a consistent bias hierarchy, with perturbations involving racial cues being the predominant cause of the violations. In addition to offering a comparative evaluation, this work reinforces that the retrieval component in RAG must be carefully curated to prevent bias amplification. The findings serve as a practical alert for developers, testers and small organizations aiming to adopt accessible SLMs without compromising fairness or reliability.",
        "subjects": "Artificial Intelligence, Information Retrieval, Machine Learning, Software Engineering",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.060713"
    },
    {
        "index": "#190",
        "title": "Source Separation for A Cappella Music",
        "link": "/arxiv/2509.26580",
        "arxiv_id": "2509.26580",
        "authors": "Luca A. Lanzendörfer, Constantin Pinkl, Florian Grötschla",
        "summary": "In this work, we study the task of multi-singer separation in a cappella music, where the number of active singers varies across mixtures. To address this, we use a power set-based data augmentation strategy that expands limited multi-singer datasets into exponentially more training samples. To separate singers, we introduce SepACap, an adaptation of SepReformer, a state-of-the-art speaker separation model architecture. We adapt the model with periodic activations and a composite loss function that remains effective when stems are silent, enabling robust detection and separation. Experiments on the JaCappella dataset demonstrate that our approach achieves state-of-the-art performance in both full-ensemble and subset singer separation scenarios, outperforming spectrogram-based baselines while generalizing to realistic mixtures with varying numbers of singers.",
        "subjects": "Sound, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.061352"
    },
    {
        "index": "#191",
        "title": "AI-assisted Advanced Propellant Development for Electric Propulsion",
        "link": "/arxiv/2509.26567",
        "arxiv_id": "2509.26567",
        "authors": "Angel Pan Du, Miguel Arana-Catania, Enric Grustan Gutiérrez",
        "summary": "Artificial Intelligence algorithms are introduced in this work as a tool to predict the performance of new chemical compounds as alternative propellants for electric propulsion, focusing on predicting their ionisation characteristics and fragmentation patterns. The chemical properties and structure of the compounds are encoded using a chemical fingerprint, and the training datasets are extracted from the NIST WebBook. The AI-predicted ionisation energy and minimum appearance energy have a mean relative error of 6.87% and 7.99%, respectively, and a predicted ion mass with a 23.89% relative error. In the cases of full mass spectra due to electron ionisation, the predictions have a cosine similarity of 0.6395 and align with the top 10 most similar mass spectra in 78% of instances within a 30 Da range.",
        "subjects": "Instrumentation and Methods for Astrophysics, Artificial Intelligence, Machine Learning, Space Physics",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.062049"
    },
    {
        "index": "#192",
        "title": "DeepProv: Behavioral Characterization and Repair of Neural Networks via Inference Provenance Graph Analysis",
        "link": "/arxiv/2509.26562",
        "arxiv_id": "2509.26562",
        "authors": "Firas Ben Hmida, Abderrahmen Amich, Ata Kaboudi, Birhanu Eshete",
        "summary": "Deep neural networks (DNNs) are increasingly being deployed in high-stakes applications, from self-driving cars to biometric authentication. However, their unpredictable and unreliable behaviors in real-world settings require new approaches to characterize and ensure their reliability. This paper introduces DeepProv, a novel and customizable system designed to capture and characterize the runtime behavior of DNNs during inference by using their underlying graph structure. Inspired by system audit provenance graphs, DeepProv models the computational information flow of a DNN's inference process through Inference Provenance Graphs (IPGs). These graphs provide a detailed structural representation of the behavior of DNN, allowing both empirical and structural analysis. DeepProv uses these insights to systematically repair DNNs for specific objectives, such as improving robustness, privacy, or fairness. We instantiate DeepProv with adversarial robustness as the goal of model repair and conduct extensive case studies to evaluate its effectiveness. Our results demonstrate its effectiveness and scalability across diverse classification tasks, attack scenarios, and model complexities. DeepProv automatically identifies repair actions at the node and edge-level within IPGs, significantly enhancing the robustness of the model. In particular, applying DeepProv repair strategies to just a single layer of a DNN yields an average 55% improvement in adversarial accuracy. Moreover, DeepProv complements existing defenses, achieving substantial gains in adversarial robustness. Beyond robustness, we demonstrate the broader potential of DeepProv as an adaptable system to characterize DNN behavior in other critical areas, such as privacy auditing and fairness analysis.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.067887"
    },
    {
        "index": "#193",
        "title": "Estimating Dimensionality of Neural Representations from Finite Samples",
        "link": "/arxiv/2509.26560",
        "arxiv_id": "2509.26560",
        "authors": "Chanwoo Chun, Abdulkadir Canatar, SueYeon Chung, Daniel Lee",
        "summary": "The global dimensionality of a neural representation manifold provides rich insight into the computational process underlying both artificial and biological neural networks. However, all existing measures of global dimensionality are sensitive to the number of samples, i.e., the number of rows and columns of the sample matrix. We show that, in particular, the participation ratio of eigenvalues, a popular measure of global dimensionality, is highly biased with small sample sizes, and propose a bias-corrected estimator that is more accurate with finite samples and with noise. On synthetic data examples, we demonstrate that our estimator can recover the true known dimensionality. We apply our estimator to neural brain recordings, including calcium imaging, electrophysiological recordings, and fMRI data, and to the neural activations in a large language model and show our estimator is invariant to the sample size. Finally, our estimators can additionally be used to measure the local dimensionalities of curved neural manifolds by weighting the finite samples appropriately.",
        "subjects": "Machine Learning, Machine Learning, Neurons and Cognition",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.068515"
    },
    {
        "index": "#194",
        "title": "Pretrain-Test Task Alignment Governs Generalization in In-Context Learning",
        "link": "/arxiv/2509.26551",
        "arxiv_id": "2509.26551",
        "authors": "Mary I. Letey, Jacob A. Zavatone-Veth, Yue M. Lu, Cengiz Pehlevan",
        "summary": "In-context learning (ICL) is a central capability of Transformer models, but the structures in data that enable its emergence and govern its robustness remain poorly understood. In this work, we study how the structure of pretraining tasks governs generalization in ICL. Using a solvable model for ICL of linear regression by linear attention, we derive an exact expression for ICL generalization error in high dimensions under arbitrary pretraining-testing task covariance mismatch. This leads to a new alignment measure that quantifies how much information about the pretraining task distribution is useful for inference at test time. We show that this measure directly predicts ICL performance not only in the solvable model but also in nonlinear Transformers. Our analysis further reveals a tradeoff between specialization and generalization in ICL: depending on task distribution alignment, increasing pretraining task diversity can either improve or harm test performance. Together, these results identify train-test task alignment as a key determinant of generalization in ICL.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.069052"
    },
    {
        "index": "#195",
        "title": "Towards Verified Code Reasoning by LLMs",
        "link": "/arxiv/2509.26546",
        "arxiv_id": "2509.26546",
        "authors": "Meghana Sistla, Gogul Balakrishnan, Pat Rondon, José Cambronero, Michele Tufano, Satish Chandra",
        "summary": "While LLM-based agents are able to tackle a wide variety of code reasoning questions, the answers are not always correct. This prevents the agent from being useful in situations where high precision is desired: (1) helping a software engineer understand a new code base, (2) helping a software engineer during code review sessions, and (3) ensuring that the code generated by an automated code generation system meets certain requirements (e.g. fixes a bug, improves readability, implements a feature). As a result of this lack of trustworthiness, the agent's answers need to be manually verified before they can be trusted. Manually confirming responses from a code reasoning agent requires human effort and can result in slower developer productivity, which weakens the assistance benefits of the agent. In this paper, we describe a method to automatically validate the answers provided by a code reasoning agent by verifying its reasoning steps. At a very high level, the method consists of extracting a formal representation of the agent's response and, subsequently, using formal verification and program analysis tools to verify the agent's reasoning steps. We applied this approach to a benchmark set of 20 uninitialized variable errors detected by sanitizers and 20 program equivalence queries. For the uninitialized variable errors, the formal verification step was able to validate the agent's reasoning on 13/20 examples, and for the program equivalence queries, the formal verification step successfully caught 6/8 incorrect judgments made by the agent.",
        "subjects": "Software Engineering, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.069616"
    },
    {
        "index": "#198",
        "title": "Signal-Aware Workload Shifting Algorithms with Uncertainty-Quantified Predictors",
        "link": "/arxiv/2509.26511",
        "arxiv_id": "2509.26511",
        "authors": "Ezra Johnson, Adam Lechowicz, Mohammad Hajiesmaili",
        "summary": "A wide range of sustainability and grid-integration strategies depend on workload shifting, which aligns the timing of energy consumption with external signals such as grid curtailment events, carbon intensity, or time-of-use electricity prices. The main challenge lies in the online nature of the problem: operators must make real-time decisions (e.g., whether to consume energy now) without knowledge of the future. While forecasts of signal values are typically available, prior work on learning-augmented online algorithms has relied almost exclusively on simple point forecasts. In parallel, the forecasting research has made significant progress in uncertainty quantification (UQ), which provides richer and more fine-grained predictive information. In this paper, we study how online workload shifting can leverage UQ predictors to improve decision-making. We introduce $\\texttt{UQ-Advice}$, a learning-augmented algorithm that systematically integrates UQ forecasts through a $\\textit{decision uncertainty score}$ that measures how forecast uncertainty affects optimal future decisions. By introducing $\\textit{UQ-robustness}$, a new metric that characterizes how performance degrades with forecast uncertainty, we establish theoretical performance guarantees for $\\texttt{UQ-Advice}$. Finally, using trace-driven experiments on carbon intensity and electricity price data, we demonstrate that $\\texttt{UQ-Advice}$ consistently outperforms robust baselines and existing learning-augmented methods that ignore uncertainty.",
        "subjects": "Data Structures and Algorithms, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.071608"
    },
    {
        "index": "#199",
        "title": "The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain",
        "link": "/arxiv/2509.26507",
        "arxiv_id": "2509.26507",
        "authors": "Adrian Kosowski, Przemysław Uznański, Jan Chorowski, Zuzanna Stamirowska, Michał Bartoszkiewicz",
        "summary": "The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models. We introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of \\$n\\$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance. BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data. BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech. BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.",
        "subjects": "Neural and Evolutionary Computing, Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.072216"
    },
    {
        "index": "#200",
        "title": "Contrastive Diffusion Guidance for Spatial Inverse Problems",
        "link": "/arxiv/2509.26489",
        "arxiv_id": "2509.26489",
        "authors": "Sattwik Basu, Chaitanya Amballa, Zhongweiyang Xu, Jorge Vančo Sampedro, Srihari Nelakuditi, Romit Roy Choudhury",
        "summary": "We consider the inverse problem of reconstructing the spatial layout of a place, a home floorplan for example, from a user`s movements inside that layout. Direct inversion is ill-posed since many floorplans can explain the same movement trajectories. We adopt a diffusion-based posterior sampler to generate layouts consistent with the measurements. While active research is in progress on generative inverse solvers, we find that the forward operator in our problem poses new challenges. The path-planning process inside a floorplan is a non-invertible, non-differentiable function, and causes instability while optimizing using the likelihood score. We break-away from existing approaches and reformulate the likelihood score in a smoother embedding space. The embedding space is trained with a contrastive loss which brings compatible floorplans and trajectories close to each other, while pushing mismatched pairs far apart. We show that a surrogate form of the likelihood score in this embedding space is a valid approximation of the true likelihood score, making it possible to steer the denoising process towards the posterior. Across extensive experiments, our model CoGuide produces more consistent floorplans from trajectories, and is more robust than differentiable-planner baselines and guided-diffusion methods.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning, Signal Processing",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.077910"
    },
    {
        "index": "#203",
        "title": "Zero-Shot Decentralized Federated Learning",
        "link": "/arxiv/2509.26462",
        "arxiv_id": "2509.26462",
        "authors": "Alessio Masano, Matteo Pennisi, Federica Proietto Salanitri, Concetto Spampinato, Giovanni Bellitto",
        "summary": "CLIP has revolutionized zero-shot learning by enabling task generalization without fine-tuning. While prompting techniques like CoOp and CoCoOp enhance CLIP's adaptability, their effectiveness in Federated Learning (FL) remains an open challenge. Existing federated prompt learning approaches, such as FedCoOp and FedTPG, improve performance but face generalization issues, high communication costs, and reliance on a central server, limiting scalability and privacy. We propose Zero-shot Decentralized Federated Learning (ZeroDFL), a fully decentralized framework that enables zero-shot adaptation across distributed clients without a central coordinator. ZeroDFL employs an iterative prompt-sharing mechanism, allowing clients to optimize and exchange textual prompts to enhance generalization while drastically reducing communication overhead. We validate ZeroDFL on nine diverse image classification datasets, demonstrating that it consistently outperforms--or remains on par with--state-of-the-art federated prompt learning methods. More importantly, ZeroDFL achieves this performance in a fully decentralized setting while reducing communication overhead by 118x compared to FedTPG. These results highlight that our approach not only enhances generalization in federated zero-shot learning but also improves scalability, efficiency, and privacy preservation--paving the way for decentralized adaptation of large vision-language models in real-world applications.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.079619"
    },
    {
        "index": "#204",
        "title": "Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification",
        "link": "/arxiv/2509.26457",
        "arxiv_id": "2509.26457",
        "authors": "Artur Barros, Carlos Caetano, João Macedo, Jefersson A. dos Santos, Sandra Avila",
        "summary": "Indoor scene classification is a critical task in computer vision, with wide-ranging applications that go from robotics to sensitive content analysis, such as child sexual abuse imagery (CSAI) classification. The problem is particularly challenging due to the intricate relationships between objects and complex spatial layouts. In this work, we propose the Attention over Scene Graphs for Sensitive Content Analysis (ASGRA), a novel framework that operates on structured graph representations instead of raw pixels. By first converting images into Scene Graphs and then employing a Graph Attention Network for inference, ASGRA directly models the interactions between a scene's components. This approach offers two key benefits: (i) inherent explainability via object and relationship identification, and (ii) privacy preservation, enabling model training without direct access to sensitive images. On Places8, we achieve 81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI evaluation with law enforcement yields 74.27% balanced accuracy. Our results establish structured scene representations as a robust paradigm for indoor scene classification and CSAI classification. Code is publicly available at https://github.com/tutuzeraa/ASGRA.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.080204"
    },
    {
        "index": "#205",
        "title": "Stabilization of nonlinear systems with unknown delays via delay-adaptive neural operator approximate predictors",
        "link": "/arxiv/2509.26443",
        "arxiv_id": "2509.26443",
        "authors": "Luke Bhan, Miroslav Krstic, Yuanyuan Shi",
        "summary": "This work establishes the first rigorous stability guarantees for approximate predictors in delay-adaptive control of nonlinear systems, addressing a key challenge in practical implementations where exact predictors are unavailable. We analyze two scenarios: (i) when the actuated input is directly measurable, and (ii) when it is estimated online. For the measurable input case, we prove semi-global practical asymptotic stability with an explicit bound proportional to the approximation error $\\epsilon$. For the unmeasured input case, we demonstrate local practical asymptotic stability, with the region of attraction explicitly dependent on both the initial delay estimate and the predictor approximation error. To bridge theory and practice, we show that neural operators-a flexible class of neural network-based approximators-can achieve arbitrarily small approximation errors, thus satisfying the conditions of our stability theorems. Numerical experiments on two nonlinear benchmark systems-a biological protein activator/repressor model and a micro-organism growth Chemostat model-validate our theoretical results. In particular, our numerical simulations confirm stability under approximate predictors, highlight the strong generalization capabilities of neural operators, and demonstrate a substantial computational speedup of up to 15x compared to a baseline fixed-point method.",
        "subjects": "Systems and Control, Machine Learning, Dynamical Systems",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.080727"
    },
    {
        "index": "#206",
        "title": "An Orthogonal Learner for Individualized Outcomes in Markov Decision Processes",
        "link": "/arxiv/2509.26429",
        "arxiv_id": "2509.26429",
        "authors": "Emil Javurek, Valentyn Melnychuk, Jonas Schweisthal, Konstantin Hess, Dennis Frauen, Stefan Feuerriegel",
        "summary": "Predicting individualized potential outcomes in sequential decision-making is central for optimizing therapeutic decisions in personalized medicine (e.g., which dosing sequence to give to a cancer patient). However, predicting potential outcomes over long horizons is notoriously difficult. Existing methods that break the curse of the horizon typically lack strong theoretical guarantees such as orthogonality and quasi-oracle efficiency. In this paper, we revisit the problem of predicting individualized potential outcomes in sequential decision-making (i.e., estimating Q-functions in Markov decision processes with observational data) through a causal inference lens. In particular, we develop a comprehensive theoretical foundation for meta-learners in this setting with a focus on beneficial theoretical properties. As a result, we yield a novel meta-learner called DRQ-learner and establish that it is: (1) doubly robust (i.e., valid inference under the misspecification of one of the nuisances), (2) Neyman-orthogonal (i.e., insensitive to first-order estimation errors in the nuisance functions), and (3) achieves quasi-oracle efficiency (i.e., behaves asymptotically as if the ground-truth nuisance functions were known). Our DRQ-learner is applicable to settings with both discrete and continuous state spaces. Further, our DRQ-learner is flexible and can be used together with arbitrary machine learning models (e.g., neural networks). We validate our theoretical results through numerical experiments, thereby showing that our meta-learner outperforms state-of-the-art baselines.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.081293"
    },
    {
        "index": "#207",
        "title": "OntoAligner Meets Knowledge Graph Embedding Aligners",
        "link": "/arxiv/2509.26417",
        "arxiv_id": "2509.26417",
        "authors": "Hamed Babaei Giglou, Jennifer D'Souza, Sören Auer, Mahsa Sanaei",
        "summary": "Ontology Alignment (OA) is essential for enabling semantic interoperability across heterogeneous knowledge systems. While recent advances have focused on large language models (LLMs) for capturing contextual semantics, this work revisits the underexplored potential of Knowledge Graph Embedding (KGE) models, which offer scalable, structure-aware representations well-suited to ontology-based tasks. Despite their effectiveness in link prediction, KGE methods remain underutilized in OA, with most prior work focusing narrowly on a few models. To address this gap, we reformulate OA as a link prediction problem over merged ontologies represented as RDF-style triples and develop a modular framework, integrated into the OntoAligner library, that supports 17 diverse KGE models. The system learns embeddings from a combined ontology and aligns entities by computing cosine similarity between their representations. We evaluate our approach using standard metrics across seven benchmark datasets spanning five domains: Anatomy, Biodiversity, Circular Economy, Material Science and Engineering, and Biomedical Machine Learning. Two key findings emerge: first, KGE models like ConvE and TransF consistently produce high-precision alignments, outperforming traditional systems in structure-rich and multi-relational domains; second, while their recall is moderate, this conservatism makes KGEs well-suited for scenarios demanding high-confidence mappings. Unlike LLM-based methods that excel at contextual reasoning, KGEs directly preserve and exploit ontology structure, offering a complementary and computationally efficient strategy. These results highlight the promise of embedding-based OA and open pathways for further work on hybrid models and adaptive strategies.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.081822"
    },
    {
        "index": "#208",
        "title": "TrackFormers Part 2: Enhanced Transformer-Based Models for High-Energy Physics Track Reconstruction",
        "link": "/arxiv/2509.26411",
        "arxiv_id": "2509.26411",
        "authors": "Sascha Caron, Nadezhda Dobreva, Maarten Kimpel, Uraz Odyurt, Slav Pshenov, Roberto Ruiz de Austri Bazan, Eugene Shalugin, Zef Wolffs, Yue Zhao",
        "summary": "High-Energy Physics experiments are rapidly escalating in generated data volume, a trend that will intensify with the upcoming High-Luminosity LHC upgrade. This surge in data necessitates critical revisions across the data processing pipeline, with particle track reconstruction being a prime candidate for improvement. In our previous work, we introduced \"TrackFormers\", a collection of Transformer-based one-shot encoder-only models that effectively associate hits with expected tracks. In this study, we extend our earlier efforts by incorporating loss functions that account for inter-hit correlations, conducting detailed investigations into (various) Transformer attention mechanisms, and a study on the reconstruction of higher-level objects. Furthermore we discuss new datasets that allow the training on hit level for a range of physics processes. These developments collectively aim to boost both the accuracy, and potentially the efficiency of our tracking models, offering a robust solution to meet the demands of next-generation high-energy physics experiments.",
        "subjects": "High Energy Physics - Experiment, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.082409"
    },
    {
        "index": "#209",
        "title": "Are neural scaling laws leading quantum chemistry astray?",
        "link": "/arxiv/2509.26397",
        "arxiv_id": "2509.26397",
        "authors": "Siwoo Lee, Adji Bousso Dieng",
        "summary": "Neural scaling laws are driving the machine learning community toward training ever-larger foundation models across domains, assuring high accuracy and transferable representations for extrapolative tasks. We test this promise in quantum chemistry by scaling model capacity and training data from quantum chemical calculations. As a generalization task, we evaluate the resulting models' predictions of the bond dissociation energy of neutral H$_2$, the simplest possible molecule. We find that, regardless of dataset size or model capacity, models trained only on stable structures fail dramatically to even qualitatively reproduce the H$_2$ energy curve. Only when compressed and stretched geometries are explicitly included in training do the predictions roughly resemble the correct shape. Nonetheless, the largest foundation models trained on the largest and most diverse datasets containing dissociating diatomics exhibit serious failures on simple diatomic molecules. Most strikingly, they cannot reproduce the trivial repulsive energy curve of two bare protons, revealing their failure to learn the basic Coulomb's law involved in electronic structure theory. These results suggest that scaling alone is insufficient for building reliable quantum chemical models.",
        "subjects": "Chemical Physics, Machine Learning, Computational Physics",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.084865"
    },
    {
        "index": "#210",
        "title": "Vector-Valued Reproducing Kernel Banach Spaces for Neural Networks and Operators",
        "link": "/arxiv/2509.26371",
        "arxiv_id": "2509.26371",
        "authors": "Sven Dummer, Tjeerd Jan Heeringa, José A. Iglesias",
        "summary": "Recently, there has been growing interest in characterizing the function spaces underlying neural networks. While shallow and deep scalar-valued neural networks have been linked to scalar-valued reproducing kernel Banach spaces (RKBS), $\\R^d$-valued neural networks and neural operator models remain less understood in the RKBS setting. To address this gap, we develop a general definition of vector-valued RKBS (vv-RKBS), which inherently includes the associated reproducing kernel. Our construction extends existing definitions by avoiding restrictive assumptions such as symmetric kernel domains, finite-dimensional output spaces, reflexivity, or separability, while still recovering familiar properties of vector-valued reproducing kernel Hilbert spaces (vv-RKHS). We then show that shallow $\\R^d$-valued neural networks are elements of a specific vv-RKBS, namely an instance of the integral and neural vv-RKBS. To also explore the functional structure of neural operators, we analyze the DeepONet and Hypernetwork architectures and demonstrate that they too belong to an integral and neural vv-RKBS. In all cases, we establish a Representer Theorem, showing that optimization over these function spaces recovers the corresponding neural architectures.",
        "subjects": "Functional Analysis, Artificial Intelligence, Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.085431"
    },
    {
        "index": "#212",
        "title": "TrackCore-F: Deploying Transformer-Based Subatomic Particle Tracking on FPGAs",
        "link": "/arxiv/2509.26335",
        "arxiv_id": "2509.26335",
        "authors": "Arjan Blankestijn, Uraz Odyurt, Amirreza Yousefzadeh",
        "summary": "The Transformer Machine Learning (ML) architecture has been gaining considerable momentum in recent years. In particular, computational High-Energy Physics tasks such as jet tagging and particle track reconstruction (tracking), have either achieved proper solutions, or reached considerable milestones using Transformers. On the other hand, the use of specialised hardware accelerators, especially FPGAs, is an effective method to achieve online, or pseudo-online latencies. The development and integration of Transformer-based ML to FPGAs is still ongoing and the support from current tools is very limited to non-existent. Additionally, FPGA resources present a significant constraint. Considering the model size alone, while smaller models can be deployed directly, larger models are to be partitioned in a meaningful and ideally, automated way. We aim to develop methodologies and tools for monolithic, or partitioned Transformer synthesis, specifically targeting inference. Our primary use-case involves two machine learning model designs for tracking, derived from the TrackFormers project. We elaborate our development approach, present preliminary results, and provide comparisons.",
        "subjects": "High Energy Physics - Experiment, Hardware Architecture, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.086623"
    },
    {
        "index": "#214",
        "title": "Ultra-Reliable Risk-Aggregated Sum Rate Maximization via Model-Aided Deep Learning",
        "link": "/arxiv/2509.26311",
        "arxiv_id": "2509.26311",
        "authors": "Hassaan Hashmi, Spyridon Pougkakiotis, Dionysis Kalogerias",
        "summary": "We consider the problem of maximizing weighted sum rate in a multiple-input single-output (MISO) downlink wireless network with emphasis on user rate reliability. We introduce a novel risk-aggregated formulation of the complex WSR maximization problem, which utilizes the Conditional Value-at-Risk (CVaR) as a functional for enforcing rate (ultra)-reliability over channel fading uncertainty/risk. We establish a WMMSE-like equivalence between the proposed precoding problem and a weighted risk-averse MSE problem, enabling us to design a tailored unfolded graph neural network (GNN) policy function approximation (PFA), named {\\alpha}-Robust Graph Neural Network ({\\alpha}RGNN), trained to maximize lower-tail (CVaR) rates resulting from adverse wireless channel realizations (e.g., deep fading, attenuation). We empirically demonstrate that a trained {\\alpha}RGNN fully eliminates per user deep rate fades, and substantially and optimally reduces statistical user rate variability while retaining adequate ergodic performance.",
        "subjects": "Signal Processing, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.087915"
    },
    {
        "index": "#215",
        "title": "Representation-Based Data Quality Audits for Audio",
        "link": "/arxiv/2509.26291",
        "arxiv_id": "2509.26291",
        "authors": "Alvaro Gonzalez-Jimenez, Fabian Gröger, Linda Wermelinger, Andrin Bürli, Iason Kastanis, Simone Lionetti, Marc Pouly",
        "summary": "Data quality issues such as off-topic samples, near duplicates, and label errors often limit the performance of audio-based systems. This paper addresses these issues by adapting SelfClean, a representation-to-rank data auditing framework, from the image to the audio domain. This approach leverages self-supervised audio representations to identify common data quality issues, creating ranked review lists that surface distinct issues within a single, unified process. The method is benchmarked on the ESC-50, GTZAN, and a proprietary industrial dataset, using both synthetic and naturally occurring corruptions. The results demonstrate that this framework achieves state-of-the-art ranking performance, often outperforming issue-specific baselines and enabling significant annotation savings by efficiently guiding human review.",
        "subjects": "Sound, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.088449"
    },
    {
        "index": "#216",
        "title": "FLOWER: A Flow-Matching Solver for Inverse Problems",
        "link": "/arxiv/2509.26287",
        "arxiv_id": "2509.26287",
        "authors": "Mehrsa Pourya, Bassam El Rawas, Michael Unser",
        "summary": "We introduce Flower, a solver for inverse problems. It leverages a pre-trained flow model to produce reconstructions that are consistent with the observed measurements. Flower operates through an iterative procedure over three steps: (i) a flow-consistent destination estimation, where the velocity network predicts a denoised target; (ii) a refinement step that projects the estimated destination onto a feasible set defined by the forward operator; and (iii) a time-progression step that re-projects the refined destination along the flow trajectory. We provide a theoretical analysis that demonstrates how Flower approximates Bayesian posterior sampling, thereby unifying perspectives from plug-and-play methods and generative inverse solvers. On the practical side, Flower achieves state-of-the-art reconstruction quality while using nearly identical hyperparameters across various inverse problems.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.088746"
    },
    {
        "index": "#217",
        "title": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection",
        "link": "/arxiv/2509.26272",
        "arxiv_id": "2509.26272",
        "authors": "Tuan Nguyen, Naseem Khan, Khang Tran, NhatHai Phan, Issa Khalil",
        "summary": "The rapid rise of synthetic media has made deepfake detection a critical challenge for online safety and trust. Progress remains constrained by the scarcity of large, high-quality datasets. Although multimodal large language models (LLMs) exhibit strong reasoning capabilities, their performance on deepfake detection is poor, often producing explanations that are misaligned with visual evidence or hallucinatory. To address this limitation, we introduce a reasoning-annotated dataset for deepfake detection and propose Paragraph-level Relative Policy Optimization (PRPO), a reinforcement learning algorithm that aligns LLM reasoning with image content at the paragraph level. Experiments show that PRPO improves detection accuracy by a wide margin and achieves the highest reasoning score of 4.55/5.0. Ablation studies further demonstrate that PRPO significantly outperforms GRPO under test-time conditions. These results underscore the importance of grounding multimodal reasoning in visual evidence to enable more reliable and interpretable deepfake detection.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.089067"
    },
    {
        "index": "#218",
        "title": "Why is topology hard to learn?",
        "link": "/arxiv/2509.26261",
        "arxiv_id": "2509.26261",
        "authors": "D. O. Oriekhov, Stan Bergkamp, Guliuxin Jin, Juan Daniel Torres Luna, Badr Zouggari, Sibren van der Meer, Naoual El Yazidi, Eliska Greplova",
        "summary": "Much attention has been devoted to the use of machine learning to approximate physical concepts. Yet, due to challenges in interpretability of machine learning techniques, the question of what physics machine learning models are able to learn remains open. Here we bridge the concept a physical quantity and its machine learning approximation in the context of the original application of neural networks in physics: topological phase classification. We construct a hybrid tensor-neural network object that exactly expresses real space topological invariant and rigorously assess its trainability and generalization. Specifically, we benchmark the accuracy and trainability of a tensor-neural network to multiple types of neural networks, thus exemplifying the differences in trainability and representational power. Our work highlights the challenges in learning topological invariants and constitutes a stepping stone towards more accurate and better generalizable machine learning representations in condensed matter physics.",
        "subjects": "Mesoscale and Nanoscale Physics, Disordered Systems and Neural Networks, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.089436"
    },
    {
        "index": "#219",
        "title": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning",
        "link": "/arxiv/2509.26255",
        "arxiv_id": "2509.26255",
        "authors": "Yichao Liang, Dat Nguyen, Cambridge Yang, Tianyang Li, Joshua B. Tenenbaum, Carl Edward Rasmussen, Adrian Weller, Zenna Tavares, Tom Silver, Kevin Ellis",
        "summary": "Long-horizon embodied planning is challenging because the world does not only change through an agent's actions: exogenous processes (e.g., water heating, dominoes cascading) unfold concurrently with the agent's actions. We propose a framework for abstract world models that jointly learns (i) symbolic state representations and (ii) causal processes for both endogenous actions and exogenous mechanisms. Each causal process models the time course of a stochastic causal-effect relation. We learn these world models from limited data via variational Bayesian inference combined with LLM proposals. Across five simulated tabletop robotics environments, the learned models enable fast planning that generalizes to held-out tasks with more objects and more complex goals, outperforming a range of baselines.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.090101"
    },
    {
        "index": "#221",
        "title": "Hybrid Quantum-Classical Optimisation of Traveling Salesperson Problem",
        "link": "/arxiv/2509.26229",
        "arxiv_id": "2509.26229",
        "authors": "Christos Lytrosyngounis, Ioannis Lytrosyngounis",
        "summary": "The Traveling Salesperson Problem (TSP), a quintessential NP-hard combinatorial optimisation challenge, is vital for logistics and network design but limited by exponential complexity in large instances. We propose a hybrid quantum-classical framework integrating variational quantum eigensolver (VQE) optimisation with classical machine learning, using K-means clustering for problem decomposition and a RandomForestRegressor for path refinement. Evaluated on 80 European cities (from 4 to 80 cities, 38,500 samples in total) via Qiskit's AerSimulator and ibm_kyiv 127-qubit backend, the hybrid approach outperforms quantum-only methods, achieving an approximation ratio of 1.0287 at 80 cities, a 47.5% improvement over quantum-only's 1.9614, nearing the classical baseline. Machine learning reduces variability in tour distances (interquartile range, IQR - the spread of the middle 50% of results relative to the median - from 0.06 to 0.04), enhancing stability despite noisy intermediate-scale quantum (NISQ) noise. This framework underscores hybrid strategies' potential for scalable TSP optimisation, with future hardware advancements promising practical quantum advantages.",
        "subjects": "Quantum Physics, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.090757"
    },
    {
        "index": "#222",
        "title": "The silence of the weights: an investigation of structural pruning strategies for attention-based audio signal architectures",
        "link": "/arxiv/2509.26207",
        "arxiv_id": "2509.26207",
        "authors": "Andrea Diecidue, Carlo Alberto Barbano, Piero Fraternali, Mathieu Fontaine, Enzo Tartaglione",
        "summary": "Transformer-based models have become the state of the art across multiple domains, from natural language processing to machine listening, thanks to attention mechanisms. However, the attention layers require a large number of parameters and high-end hardware for both training and inference. We propose a novel pruning technique targeted explicitly at the attention mechanism, where we decouple the pruning of the four layers in the attention block, namely: query, keys, values and outputs' projection matrices. We also investigate pruning strategies to prune along the head and channel dimensions, and compare the performance of the Audio Spectrogram Transformer (AST) model under different pruning scenarios. Our results show that even by pruning 50\\% of the attention parameters we incur in performance degradation of less than 1\\%",
        "subjects": "Sound, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.091089"
    },
    {
        "index": "#223",
        "title": "Self-supervised learning for phase retrieval",
        "link": "/arxiv/2509.26203",
        "arxiv_id": "2509.26203",
        "authors": "Victor Sechaud, Patrice Abry, Laurent Jacques, Julián Tachella",
        "summary": "In recent years, deep neural networks have emerged as a solution for inverse imaging problems. These networks are generally trained using pairs of images: one degraded and the other of high quality, the latter being called 'ground truth'. However, in medical and scientific imaging, the lack of fully sampled data limits supervised learning. Recent advances have made it possible to reconstruct images from measurement data alone, eliminating the need for references. However, these methods remain limited to linear problems, excluding non-linear problems such as phase retrieval. We propose a self-supervised method that overcomes this limitation in the case of phase retrieval by using the natural invariance of images to translations.",
        "subjects": "Information Retrieval, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.091417"
    },
    {
        "index": "#224",
        "title": "AttriGen: Automated Multi-Attribute Annotation for Blood Cell Datasets",
        "link": "/arxiv/2509.26185",
        "arxiv_id": "2509.26185",
        "authors": "Walid Houmaidi, Youssef Sabiri, Fatima Zahra Iguenfer, Amine Abouaomar",
        "summary": "We introduce AttriGen, a novel framework for automated, fine-grained multi-attribute annotation in computer vision, with a particular focus on cell microscopy where multi-attribute classification remains underrepresented compared to traditional cell type categorization. Using two complementary datasets: the Peripheral Blood Cell (PBC) dataset containing eight distinct cell types and the WBC Attribute Dataset (WBCAtt) that contains their corresponding 11 morphological attributes, we propose a dual-model architecture that combines a CNN for cell type classification, as well as a Vision Transformer (ViT) for multi-attribute classification achieving a new benchmark of 94.62\\% accuracy. Our experiments demonstrate that AttriGen significantly enhances model interpretability and offers substantial time and cost efficiency relative to conventional full-scale human annotation. Thus, our framework establishes a new paradigm that can be extended to other computer vision classification tasks by effectively automating the expansion of multi-attribute labels.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.091770"
    },
    {
        "index": "#225",
        "title": "Benchmarking Diarization Models",
        "link": "/arxiv/2509.26177",
        "arxiv_id": "2509.26177",
        "authors": "Luca A. Lanzendörfer, Florian Grötschla, Cesare Blaser, Roger Wattenhofer",
        "summary": "Speaker diarization is the task of partitioning audio into segments according to speaker identity, answering the question of \"who spoke when\" in multi-speaker conversation recordings. While diarization is an essential task for many downstream applications, it remains an unsolved problem. Errors in diarization propagate to downstream systems and cause wide-ranging failures. To this end, we examine exact failure modes by evaluating five state-of-the-art diarization models, across four diarization datasets spanning multiple languages and acoustic conditions. The evaluation datasets consist of 196.6 hours of multilingual audio, including English, Mandarin, German, Japanese, and Spanish. Overall, we find that PyannoteAI achieves the best performance at 11.2% DER, while DiariZen provides a competitive open-source alternative at 13.3% DER. When analyzing failure cases, we find that the primary cause of diarization errors stem from missed speech segments followed by speaker confusion, especially in high-speaker count settings.",
        "subjects": "Sound, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.092098"
    },
    {
        "index": "#226",
        "title": "EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting",
        "link": "/arxiv/2509.26157",
        "arxiv_id": "2509.26157",
        "authors": "Sachith Abeywickrama, Emadeldeen Eldele, Min Wu, Xiaoli Li, Chau Yuen",
        "summary": "Transformer-based models have significantly advanced time series forecasting, with patch-based input strategies offering efficiency and improved long-horizon modeling. Yet, existing approaches rely on temporally-agnostic patch construction, where arbitrary starting positions and fixed lengths fracture temporal coherence by splitting natural transitions across boundaries. This naive segmentation often disrupts short-term dependencies and weakens representation learning. In response, we propose EntroPE (Entropy-Guided Dynamic Patch Encoder), a novel, temporally informed framework that dynamically detects transition points via conditional entropy and dynamically places patch boundaries. This preserves temporal structure while retaining the computational benefits of patching. EntroPE consists of two key modules, namely an Entropy-based Dynamic Patcher (EDP) that applies information-theoretic criteria to locate natural temporal shifts and determine patch boundaries, and an Adaptive Patch Encoder (APE) that employs pooling and cross-attention to capture intra-patch dependencies and produce fixed-size latent representations. These embeddings are then processed by a global transformer to model inter-patch dynamics. Experiments across long-term forecasting benchmarks demonstrate that EntroPE improves both accuracy and efficiency, establishing entropy-guided dynamic patching as a promising new paradigm for time series modeling. Code is available at: https://github.com/Sachithx/EntroPE.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.092454"
    },
    {
        "index": "#227",
        "title": "Non-Vacuous Generalization Bounds: Can Rescaling Invariances Help?",
        "link": "/arxiv/2509.26149",
        "arxiv_id": "2509.26149",
        "authors": "Damien Rouchouse, Antoine Gonon, Rémi Gribonval, Benjamin Guedj",
        "summary": "A central challenge in understanding generalization is to obtain non-vacuous guarantees that go beyond worst-case complexity over data or weight space. Among existing approaches, PAC-Bayes bounds stand out as they can provide tight, data-dependent guarantees even for large networks. However, in ReLU networks, rescaling invariances mean that different weight distributions can represent the same function while leading to arbitrarily different PAC-Bayes complexities. We propose to study PAC-Bayes bounds in an invariant, lifted representation that resolves this discrepancy. This paper explores both the guarantees provided by this approach (invariance, tighter bounds via data processing) and the algorithmic aspects of KL-based rescaling-invariant PAC-Bayes bounds.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.092786"
    },
    {
        "index": "#228",
        "title": "Ordinal Label-Distribution Learning with Constrained Asymmetric Priors for Imbalanced Retinal Grading",
        "link": "/arxiv/2509.26146",
        "arxiv_id": "2509.26146",
        "authors": "Nagur Shareef Shaik, Teja Krishna Cherukuri, Adnan Masood, Ehsan Adeli, Dong Hye Ye",
        "summary": "Diabetic retinopathy grading is inherently ordinal and long-tailed, with minority stages being scarce, heterogeneous, and clinically critical to detect accurately. Conventional methods often rely on isotropic Gaussian priors and symmetric loss functions, misaligning latent representations with the task's asymmetric nature. We propose the Constrained Asymmetric Prior Wasserstein Autoencoder (CAP-WAE), a novel framework that addresses these challenges through three key innovations. Our approach employs a Wasserstein Autoencoder (WAE) that aligns its aggregate posterior with a asymmetric prior, preserving the heavy-tailed and skewed structure of minority classes. The latent space is further structured by a Margin-Aware Orthogonality and Compactness (MAOC) loss to ensure grade-ordered separability. At the supervision level, we introduce a direction-aware ordinal loss, where a lightweight head predicts asymmetric dispersions to generate soft labels that reflect clinical priorities by penalizing under-grading more severely. Stabilized by an adaptive multi-task weighting scheme, our end-to-end model requires minimal tuning. Across public DR benchmarks, CAP-WAE consistently achieves state-of-the-art Quadratic Weighted Kappa, accuracy, and macro-F1, surpassing both ordinal classification and latent generative baselines. t-SNE visualizations further reveal that our method reshapes the latent manifold into compact, grade-ordered clusters with reduced overlap.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.093131"
    },
    {
        "index": "#229",
        "title": "LMILAtt: A Deep Learning Model for Depression Detection from Social Media Users Enhanced by Multi-Instance Learning Based on Attention Mechanism",
        "link": "/arxiv/2509.26145",
        "arxiv_id": "2509.26145",
        "authors": "Yukun Yang",
        "summary": "Depression is a major global public health challenge and its early identification is crucial. Social media data provides a new perspective for depression detection, but existing methods face limitations such as insufficient accuracy, insufficient utilization of time series features, and high annotation costs. To this end, this study proposes the LMILAtt model, which innovatively integrates Long Short-Term Memory autoencoders and attention mechanisms: firstly, the temporal dynamic features of user tweets (such as depressive tendency evolution patterns) are extracted through unsupervised LSTM autoencoders. Secondly, the attention mechanism is used to dynamically weight key texts (such as early depression signals) and construct a multi-example learning architecture to improve the accuracy of user-level detection. Finally, the performance was verified on the WU3D dataset labeled by professional medicine. Experiments show that the model is significantly better than the baseline model in terms of accuracy, recall and F1 score. In addition, the weakly supervised learning strategy significantly reduces the cost of labeling and provides an efficient solution for large-scale social media depression screening.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.093429"
    },
    {
        "index": "#230",
        "title": "EVODiff: Entropy-aware Variance Optimized Diffusion Inference",
        "link": "/arxiv/2509.26096",
        "arxiv_id": "2509.26096",
        "authors": "Shigui Li, Wei Chen, Delu Zeng",
        "summary": "Diffusion models (DMs) excel in image generation, but suffer from slow inference and the training-inference discrepancies. Although gradient-based solvers like DPM-Solver accelerate the denoising inference, they lack theoretical foundations in information transmission efficiency. In this work, we introduce an information-theoretic perspective on the inference processes of DMs, revealing that successful denoising fundamentally reduces conditional entropy in reverse transitions. This principle leads to our key insights into the inference processes: (1) data prediction parameterization outperforms its noise counterpart, and (2) optimizing conditional variance offers a reference-free way to minimize both transition and reconstruction errors. Based on these insights, we propose an entropy-aware variance optimized method for the generative process of DMs, called EVODiff, which systematically reduces uncertainty by optimizing conditional entropy during denoising. Extensive experiments on DMs validate our insights and demonstrate that our method significantly and consistently outperforms state-of-the-art (SOTA) gradient-based solvers. For example, compared to the DPM-Solver++, EVODiff reduces the reconstruction error by up to 45.5\\% (FID improves from 5.10 to 2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25\\% (from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves text-to-image generation while reducing artifacts. Code is available at https://github.com/ShiguiLi/EVODiff.",
        "subjects": "Computer Vision and Pattern Recognition, Information Theory, Machine Learning, Optimization and Control, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.093789"
    },
    {
        "index": "#231",
        "title": "Efficient Distributed Training via Dual Batch Sizes and Cyclic Progressive Learning",
        "link": "/arxiv/2509.26092",
        "arxiv_id": "2509.26092",
        "authors": "Kuan-Wei Lu, Ding-Yong Hong, Pangfeng Liu, Jan-Jan Wu",
        "summary": "Distributed machine learning is critical for training deep learning models on large datasets and with numerous parameters. Current research primarily focuses on leveraging additional hardware resources and powerful computing units to accelerate the training process. As a result, larger batch sizes are often employed to speed up training. However, training with large batch sizes can lead to lower accuracy due to poor generalization. To address this issue, we propose the dual batch size learning scheme, a distributed training method built on the parameter server framework. This approach maximizes training efficiency by utilizing the largest batch size that the hardware can support while incorporating a smaller batch size to enhance model generalization. By using two different batch sizes simultaneously, this method reduces testing loss and enhances generalization, with minimal extra training time. Additionally, to mitigate the time overhead caused by dual batch size learning, we propose the cyclic progressive learning scheme. This technique gradually adjusts image resolution from low to high during training, significantly boosting training speed. By combining cyclic progressive learning with dual batch size learning, our hybrid approach improves both model generalization and training efficiency. Experimental results using ResNet-18 show that, compared to conventional training methods, our method can improve accuracy by 3.3% while reducing training time by 10.6% on CIFAR-100, and improve accuracy by 0.1% while reducing training time by 35.7% on ImageNet.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.094112"
    },
    {
        "index": "#232",
        "title": "Text-to-Scene with Large Reasoning Models",
        "link": "/arxiv/2509.26091",
        "arxiv_id": "2509.26091",
        "authors": "Frédéric Berdoz, Luca A. Lanzendörfer, Nick Tuninga, Roger Wattenhofer",
        "summary": "Prompt-driven scene synthesis allows users to generate complete 3D environments from textual descriptions. Current text-to-scene methods often struggle with complex geometries and object transformations, and tend to show weak adherence to complex instructions. We address these limitations by introducing Reason-3D, a text-to-scene model powered by large reasoning models (LRMs). Reason-3D integrates object retrieval using captions covering physical, functional, and contextual attributes. Reason-3D then places the selected objects based on implicit and explicit layout constraints, and refines their positions with collision-aware spatial reasoning. Evaluated on instructions ranging from simple to complex indoor configurations, Reason-3D significantly outperforms previous methods in human-rated visual fidelity, adherence to constraints, and asset retrieval quality. Beyond its contribution to the field of text-to-scene generation, our work showcases the advanced spatial reasoning abilities of modern LRMs. Additionally, we release the codebase to further the research in object retrieval and placement with LRMs.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.094430"
    },
    {
        "index": "#233",
        "title": "GaussEdit: Adaptive 3D Scene Editing with Text and Image Prompts",
        "link": "/arxiv/2509.26055",
        "arxiv_id": "2509.26055",
        "authors": "Zhenyu Shu, Junlong Yu, Kai Chao, Shiqing Xin, Ligang Liu",
        "summary": "This paper presents GaussEdit, a framework for adaptive 3D scene editing guided by text and image prompts. GaussEdit leverages 3D Gaussian Splatting as its backbone for scene representation, enabling convenient Region of Interest selection and efficient editing through a three-stage process. The first stage involves initializing the 3D Gaussians to ensure high-quality edits. The second stage employs an Adaptive Global-Local Optimization strategy to balance global scene coherence and detailed local edits and a category-guided regularization technique to alleviate the Janus problem. The final stage enhances the texture of the edited objects using a sophisticated image-to-image synthesis technique, ensuring that the results are visually realistic and align closely with the given prompts. Our experimental results demonstrate that GaussEdit surpasses existing methods in editing accuracy, visual fidelity, and processing speed. By successfully embedding user-specified concepts into 3D scenes, GaussEdit is a powerful tool for detailed and user-driven 3D scene editing, offering significant improvements over traditional methods.",
        "subjects": "Graphics, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.094778"
    },
    {
        "index": "#234",
        "title": "CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search",
        "link": "/arxiv/2509.26037",
        "arxiv_id": "2509.26037",
        "authors": "Zhe Li, Zhiwei Lin, Yongtao Wang",
        "summary": "The integration of Large Language Models (LLMs) with Neural Architecture Search (NAS) has introduced new possibilities for automating the design of neural architectures. However, most existing methods face critical limitations, including architectural invalidity, computational inefficiency, and inferior performance compared to traditional NAS. In this work, we present Collaborative LLM-based NAS (CoLLM-NAS), a two-stage NAS framework with knowledge-guided search driven by two complementary LLMs. Specifically, we propose a Navigator LLM to guide search direction and a Generator LLM to synthesize high-quality candidates, with a dedicated Coordinator module to manage their interaction. CoLLM-NAS efficiently guides the search process by combining LLMs' inherent knowledge of structured neural architectures with progressive knowledge from iterative feedback and historical trajectory. Experimental results on ImageNet and NAS-Bench-201 show that CoLLM-NAS surpasses existing NAS methods and conventional search algorithms, achieving new state-of-the-art results. Furthermore, CoLLM-NAS consistently enhances the performance and efficiency of various two-stage NAS methods (e.g., OFA, SPOS, and AutoFormer) across diverse search spaces (e.g., MobileNet, ShuffleNet, and AutoFormer), demonstrating its excellent generalization.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.095364"
    },
    {
        "index": "#235",
        "title": "SeMoBridge: Semantic Modality Bridge for Efficient Few-Shot Adaptation of CLIP",
        "link": "/arxiv/2509.26036",
        "arxiv_id": "2509.26036",
        "authors": "Christoph Timmermann, Hyunse Lee, Woojin Lee",
        "summary": "While Contrastive Language-Image Pretraining (CLIP) excels at zero-shot tasks by aligning image and text embeddings, its performance in few-shot classification is hindered by a critical limitation: intra-modal misalignment. This issue, caused by a persistent modality gap and CLIP's exclusively inter-modal training objective, leaves the embedding spaces uncalibrated, making direct image-to-image comparisons unreliable. Existing methods attempt to address this by refining similarity logits or by computationally expensive per-sample optimization. To overcome these challenges, we introduce SeMoBridge, a lightweight yet powerful approach that directly addresses the misalignment. Our method maps images into the text modality, while keeping their semantic content intact through what we call a Semantic Modality Bridge. SeMoBridge is closed-form and can optionally be trained through multi-modal supervision, combining image and text-alignment losses to optimize the projection. Experiments show that the trained version, SeMoBridge-T, requires only a fraction of the training time while overall outperforming other methods, particularly in low-data scenarios (1, 2, and 4 shots). The code is available at \\href{https://github.com/christti98/semobridge}{github.com/christti98/semobridge}.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.095700"
    },
    {
        "index": "#236",
        "title": "BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories under Spatio-Temporal Vector Fields",
        "link": "/arxiv/2509.26005",
        "arxiv_id": "2509.26005",
        "authors": "Rui-Yang Zhang, Henry B. Moss, Lachlan Astfalck, Edward Cripps, David S. Leslie",
        "summary": "We introduce a formal active learning methodology for guiding the placement of Lagrangian observers to infer time-dependent vector fields -- a key task in oceanography, marine science, and ocean engineering -- using a physics-informed spatio-temporal Gaussian process surrogate model. The majority of existing placement campaigns either follow standard `space-filling' designs or relatively ad-hoc expert opinions. A key challenge to applying principled active learning in this setting is that Lagrangian observers are continuously advected through the vector field, so they make measurements at different locations and times. It is, therefore, important to consider the likely future trajectories of placed observers to account for the utility of candidate placement locations. To this end, we present BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories. We observe noticeable benefits of BALLAST-aided sequential observer placement strategies on both synthetic and high-fidelity ocean current models.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.096050"
    },
    {
        "index": "#237",
        "title": "Scaling Equilibrium Propagation to Deeper Neural Network Architectures",
        "link": "/arxiv/2509.26003",
        "arxiv_id": "2509.26003",
        "authors": "Sankar Vinayak. E. P, Gopalakrishnan Srinivasan",
        "summary": "Equilibrium propagation has been proposed as a biologically plausible alternative to the backpropagation algorithm. The local nature of gradient computations, combined with the use of convergent RNNs to reach equilibrium states, make this approach well-suited for implementation on neuromorphic hardware. However, previous studies on equilibrium propagation have been restricted to networks containing only dense layers or relatively small architectures with a few convolutional layers followed by a final dense layer. These networks have a significant gap in accuracy compared to similarly sized feedforward networks trained with backpropagation. In this work, we introduce the Hopfield-Resnet architecture, which incorporates residual (or skip) connections in Hopfield networks with clipped $\\mathrm{ReLU}$ as the activation function. The proposed architectural enhancements enable the training of networks with nearly twice the number of layers reported in prior works. For example, Hopfield-Resnet13 achieves 93.92\\% accuracy on CIFAR-10, which is $\\approx$3.5\\% higher than the previous best result and comparable to that provided by Resnet13 trained using backpropagation.",
        "subjects": "Neural and Evolutionary Computing, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.096351"
    },
    {
        "index": "#239",
        "title": "CO3: Contrasting Concepts Compose Better",
        "link": "/arxiv/2509.25940",
        "arxiv_id": "2509.25940",
        "authors": "Debottam Dutta, Jianchong Chen, Rajalaxmi Rajagopalan, Yu-Lin Wei, Romit Roy Choudhury",
        "summary": "We propose to improve multi-concept prompt fidelity in text-to-image diffusion models. We begin with common failure cases-prompts like \"a cat and a dog\" that sometimes yields images where one concept is missing, faint, or colliding awkwardly with another. We hypothesize that this happens when the diffusion model drifts into mixed modes that over-emphasize a single concept it learned strongly during training. Instead of re-training, we introduce a corrective sampling strategy that steers away from regions where the joint prompt behavior overlaps too strongly with any single concept in the prompt. The goal is to steer towards \"pure\" joint modes where all concepts can coexist with balanced visual presence. We further show that existing multi-concept guidance schemes can operate in unstable weight regimes that amplify imbalance; we characterize favorable regions and adapt sampling to remain within them. Our approach, CO3, is plug-and-play, requires no model tuning, and complements standard classifier-free guidance. Experiments on diverse multi-concept prompts indicate improvements in concept coverage, balance and robustness, with fewer dropped or distorted concepts compared to standard baselines and prior compositional methods. Results suggest that lightweight corrective guidance can substantially mitigate brittle semantic alignment behavior in modern diffusion systems.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.097061"
    },
    {
        "index": "#240",
        "title": "The Impact of Scaling Training Data on Adversarial Robustness",
        "link": "/arxiv/2509.25927",
        "arxiv_id": "2509.25927",
        "authors": "Marco Zimmerli, Andreas Plesner, Till Aczel, Roger Wattenhofer",
        "summary": "Deep neural networks remain vulnerable to adversarial examples despite advances in architectures and training paradigms. We investigate how training data characteristics affect adversarial robustness across 36 state-of-the-art vision models spanning supervised, self-supervised, and contrastive learning approaches, trained on datasets from 1.2M to 22B images. Models were evaluated under six black-box attack categories: random perturbations, two types of geometric masks, COCO object manipulations, ImageNet-C corruptions, and ImageNet-R style shifts. Robustness follows a logarithmic scaling law with both data volume and model size: a tenfold increase in data reduces attack success rate (ASR) on average by ~3.2%, whereas a tenfold increase in model size reduces ASR on average by ~13.4%. Notably, some self-supervised models trained on curated datasets, such as DINOv2, outperform others trained on much larger but less curated datasets, challenging the assumption that scale alone drives robustness. Adversarial fine-tuning of ResNet50s improves generalization across structural variations but not across color distributions. Human evaluation reveals persistent gaps between human and machine vision. These results show that while scaling improves robustness, data quality, architecture, and training objectives play a more decisive role than raw scale in achieving broad-spectrum adversarial resilience.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Cryptography and Security, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.097420"
    },
    {
        "index": "#241",
        "title": "Better Privilege Separation for Agents by Restricting Data Types",
        "link": "/arxiv/2509.25926",
        "arxiv_id": "2509.25926",
        "authors": "Dennis Jacob, Emad Alghamdi, Zhanhao Hu, Basel Alomair, David Wagner",
        "summary": "Large language models (LLMs) have become increasingly popular due to their ability to interact with unstructured content. As such, LLMs are now a key driver behind the automation of language processing systems, such as AI agents. Unfortunately, these advantages have come with a vulnerability to prompt injections, an attack where an adversary subverts the LLM's intended functionality with an injected task. Past approaches have proposed detectors and finetuning to provide robustness, but these techniques are vulnerable to adaptive attacks or cannot be used with state-of-the-art models. To this end we propose type-directed privilege separation for LLMs, a method that systematically prevents prompt injections. We restrict the ability of an LLM to interact with third-party data by converting untrusted content to a curated set of data types; unlike raw strings, each data type is limited in scope and content, eliminating the possibility for prompt injections. We evaluate our method across several case studies and find that designs leveraging our principles can systematically prevent prompt injection attacks while maintaining high utility.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.097748"
    },
    {
        "index": "#243",
        "title": "Aging Decline in Basketball Career Trend Prediction Based on Machine Learning and LSTM Model",
        "link": "/arxiv/2509.25858",
        "arxiv_id": "2509.25858",
        "authors": "Yi-chen Yao, Jerry Wang, Yi-cheng Lai, Lyn Chao-ling Chen",
        "summary": "The topic of aging decline on performance of NBA players has been discussed in this study. The autoencoder with K-means clustering machine learning method was adopted to career trend classification of NBA players, and the LSTM deep learning method was adopted in performance prediction of each NBA player. The dataset was collected from the basketball game data of veteran NBA players. The contribution of the work performed better than the other methods with generalization ability for evaluating various types of NBA career trend, and can be applied in different types of sports in the field of sport analytics.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.098495"
    },
    {
        "index": "#245",
        "title": "Logo-VGR: Visual Grounded Reasoning for Open-world Logo Recognition",
        "link": "/arxiv/2509.25811",
        "arxiv_id": "2509.25811",
        "authors": "Zichen Liang, Jingjing Fei, Jie Wang, Zheming Yang, Changqing Li, Pei Wu, Minghui Qiu, Fei Yang, Xialei Liu",
        "summary": "Recent advances in multimodal large language models (MLLMs) have been primarily evaluated on general-purpose benchmarks, while their applications in domain-specific scenarios, such as intelligent product moderation, remain underexplored. To address this gap, we introduce an open-world logo recognition benchmark, a core challenge in product moderation. Unlike traditional logo recognition methods that rely on memorizing representations of tens of thousands of brands-an impractical approach in real-world settings-our proposed method, Logo-VGR, enables generalization to large-scale brand recognition with supervision from only a small subset of brands. Specifically, we reformulate logo recognition as a comparison-based task, requiring the model to match product images with candidate logos rather than directly generating brand labels. We further observe that existing models tend to overfit by memorizing brand distributions instead of learning robust multimodal reasoning, which results in poor performance on unseen brands. To overcome this limitation, Logo-VGR introduces a new paradigm of domain-specific multimodal reasoning: Logo Perception Grounding injects domain knowledge, and Logo-Guided Visual Grounded Reasoning enhances the model's reasoning capability. Experimental results show that Logo-VGR outperforms strong baselines by nearly 10 points in OOD settings, demonstrating superior generalization.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.099178"
    },
    {
        "index": "#246",
        "title": "Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding",
        "link": "/arxiv/2509.25803",
        "arxiv_id": "2509.25803",
        "authors": "Wanying Ding, Savinay Narendra, Xiran Shi, Adwait Ratnaparkhi, Chengrui Yang, Nikoo Sabzevar, Ziyan Yin",
        "summary": "Analyzing financial transactions is crucial for ensuring regulatory compliance, detecting fraud, and supporting decisions. The complexity of financial transaction data necessitates advanced techniques to extract meaningful insights and ensure accurate analysis. Since Transformer-based models have shown outstanding performance across multiple domains, this paper seeks to explore their potential in understanding financial transactions. This paper conducts extensive experiments to evaluate three types of Transformer models: Encoder-Only, Decoder-Only, and Encoder-Decoder models. For each type, we explore three options: pretrained LLMs, fine-tuned LLMs, and small proprietary models developed from scratch. Our analysis reveals that while LLMs, such as LLaMA3-8b, Flan-T5, and SBERT, demonstrate impressive capabilities in various natural language processing tasks, they do not significantly outperform small proprietary models in the specific context of financial transaction understanding. This phenomenon is particularly evident in terms of speed and cost efficiency. Proprietary models, tailored to the unique requirements of transaction data, exhibit faster processing times and lower operational costs, making them more suitable for real-time applications in the financial sector. Our findings highlight the importance of model selection based on domain-specific needs and underscore the potential advantages of customized proprietary models over general-purpose LLMs in specialized applications. Ultimately, we chose to implement a proprietary decoder-only model to handle the complex transactions that we previously couldn't manage. This model can help us to improve 14% transaction coverage, and save more than \\$13 million annual cost.",
        "subjects": "Information Retrieval, Artificial Intelligence, Computational Engineering, Finance, and Science, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.099571"
    },
    {
        "index": "#247",
        "title": "Sharpness of Minima in Deep Matrix Factorization: Exact Expressions",
        "link": "/arxiv/2509.25783",
        "arxiv_id": "2509.25783",
        "authors": "Anil Kamber, Rahul Parhi",
        "summary": "Understanding the geometry of the loss landscape near a minimum is key to explaining the implicit bias of gradient-based methods in non-convex optimization problems such as deep neural network training and deep matrix factorization. A central quantity to characterize this geometry is the maximum eigenvalue of the Hessian of the loss, which measures the sharpness of the landscape. Currently, its precise role has been obfuscated because no exact expressions for this sharpness measure were known in general settings. In this paper, we present the first exact expression for the maximum eigenvalue of the Hessian of the squared-error loss at any minimizer in general overparameterized deep matrix factorization (i.e., deep linear neural network training) problems, resolving an open question posed by Mulayoff & Michaeli (2020). To complement our theory, we empirically investigate an escape phenomenon observed during gradient-based training near a minimum that crucially relies on our exact expression of the sharpness.",
        "subjects": "Machine Learning, Machine Learning, Optimization and Control",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.099909"
    },
    {
        "index": "#249",
        "title": "SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling",
        "link": "/arxiv/2509.25756",
        "arxiv_id": "2509.25756",
        "authors": "Yixian Zhang, Shu'ang Yu, Tonghe Zhang, Mo Guang, Haojia Hui, Kaiwen Long, Yu Wang, Chao Yu, Wenbo Ding",
        "summary": "Training expressive flow-based policies with off-policy reinforcement learning is notoriously unstable due to gradient pathologies in the multi-step action sampling process. We trace this instability to a fundamental connection: the flow rollout is algebraically equivalent to a residual recurrent computation, making it susceptible to the same vanishing and exploding gradients as RNNs. To address this, we reparameterize the velocity network using principles from modern sequential models, introducing two stable architectures: Flow-G, which incorporates a gated velocity, and Flow-T, which utilizes a decoded velocity. We then develop a practical SAC-based algorithm, enabled by a noise-augmented rollout, that facilitates direct end-to-end training of these policies. Our approach supports both from-scratch and offline-to-online learning and achieves state-of-the-art performance on continuous control and robotic manipulation benchmarks, eliminating the need for common workarounds like policy distillation or surrogate objectives.",
        "subjects": "Robotics, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.105887"
    },
    {
        "index": "#251",
        "title": "Test time training enhances in-context learning of nonlinear functions",
        "link": "/arxiv/2509.25741",
        "arxiv_id": "2509.25741",
        "authors": "Kento Kuwataka, Taiji Suzuki",
        "summary": "Test-time training (TTT) enhances model performance by explicitly updating designated parameters prior to each prediction to adapt to the test data. While TTT has demonstrated considerable empirical success, its theoretical underpinnings remain limited, particularly for nonlinear models. In this paper, we investigate the combination of TTT with in-context learning (ICL), where the model is given a few examples from the target distribution at inference time. We analyze this framework in the setting of single-index models $y=\\sigma_*(\\langle \\beta, \\mathbf{x} \\rangle)$, where the feature vector $\\beta$ is drawn from a hidden low-dimensional subspace. For single-layer transformers trained with gradient-based algorithms and adopting TTT, we establish an upper bound on the prediction risk. Our theory reveals that TTT enables the single-layer transformers to adapt to both the feature vector $\\beta$ and the link function $\\sigma_*$, which vary across tasks. This creates a sharp contrast with ICL alone, which is theoretically difficult to adapt to shifts in the link function. Moreover, we provide the convergence rate with respect to the data length, showing the predictive error can be driven arbitrarily close to the noise level as the context size and the network width grow.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.106695"
    },
    {
        "index": "#253",
        "title": "Towards A Universally Transferable Acceleration Method for Density Functional Theory",
        "link": "/arxiv/2509.25724",
        "arxiv_id": "2509.25724",
        "authors": "Zhe Liu, Yuyan Ni, Zhichen Pu, Qiming Sun, Siyuan Liu, Wen Yan",
        "summary": "Recently, sophisticated deep learning-based approaches have been developed for generating efficient initial guesses to accelerate the convergence of density functional theory (DFT) calculations. While the actual initial guesses are often density matrices (DM), quantities that can convert into density matrices also qualify as alternative forms of initial guesses. Hence, existing works mostly rely on the prediction of the Hamiltonian matrix for obtaining high-quality initial guesses. However, the Hamiltonian matrix is both numerically difficult to predict and intrinsically non-transferable, hindering the application of such models in real scenarios. In light of this, we propose a method that constructs DFT initial guesses by predicting the electron density in a compact auxiliary basis representation using E(3)-equivariant neural networks. Trained on small molecules with up to 20 atoms, our model is able to achieve an average 33.3% self-consistent field (SCF) step reduction on systems up to 60 atoms, substantially outperforming Hamiltonian-centric and DM-centric models. Critically, this acceleration remains nearly constant with increasing system sizes and exhibits strong transferring behaviors across orbital basis sets and exchange-correlation (XC) functionals. To the best of our knowledge, this work represents the first and robust candidate for a universally transferable DFT acceleration method. We are also releasing the SCFbench dataset and its accompanying code to facilitate future research in this promising direction.",
        "subjects": "Chemical Physics, Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.107744"
    },
    {
        "index": "#254",
        "title": "Transformer-Based Rate Prediction for Multi-Band Cellular Handsets",
        "link": "/arxiv/2509.25722",
        "arxiv_id": "2509.25722",
        "authors": "Ruibin Chen, Haozhe Lei, Hao Guo, Marco Mezzavilla, Hitesh Poddar, Tomoki Yoshimura, Sundeep Rangan",
        "summary": "Cellular wireless systems are witnessing the proliferation of frequency bands over a wide spectrum, particularly with the expansion of new bands in FR3. These bands must be supported in user equipment (UE) handsets with multiple antennas in a constrained form factor. Rapid variations in channel quality across the bands from motion and hand blockage, limited field-of-view of antennas, and hardware and power-constrained measurement sparsity pose significant challenges to reliable multi-band channel tracking. This paper formulates the problem of predicting achievable rates across multiple antenna arrays and bands with sparse historical measurements. We propose a transformer-based neural architecture that takes asynchronous rate histories as input and outputs per-array rate predictions. Evaluated on ray-traced simulations in a dense urban micro-cellular setting with FR1 and FR3 arrays, our method demonstrates superior performance over baseline predictors, enabling more informed band selection under realistic mobility and hardware constraints.",
        "subjects": "Signal Processing, Information Theory, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.108285"
    },
    {
        "index": "#256",
        "title": "Collaborative Compression for Large-Scale MoE Deployment on Edge",
        "link": "/arxiv/2509.25689",
        "arxiv_id": "2509.25689",
        "authors": "Yixiao Chen, Yanyue Xie, Ruining Yang, Wei Jiang, Wei Wang, Yong He, Yue Chen, Pu Zhao, Yanzhi Wang",
        "summary": "The Mixture of Experts (MoE) architecture is an important method for scaling Large Language Models (LLMs). It increases model capacity while keeping computation cost low. However, the ultra-large MoE models still have hundreds of billions of parameters, requiring massive memory/storage and leading to difficulties for deployment on resource-constrained edge platforms. Pruning or quantization alone can hardly address the issue, because of the super-aggressive compression ratio with significantly degraded accuracy and output quality. To facilitate the deployment of ultra-large MoEs on edge platforms, we propose a collaborative compression framework by combining expert pruning, mixed-precision quantization, and activation optimization. It can effectively reduce the storage footprint of the ultra-large MoE DeepSeek-V3 from 1.3TB to 103GB, while preserving high output quality with better accuracy than traditional uniform low-bit quantization methods. To the best of our knowledge, we are the first to deploy a compressed model from the ultra-large DeepSeek-V3 on the platform with a strict 128GB total memory limit. Our comprehensive experiments on multiple benchmarks under various memory constraints demonstrate the effectiveness of our method with smaller model sizes and higher accuracy than uniform low-bit quantization methods.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.109459"
    },
    {
        "index": "#257",
        "title": "Deep Reinforcement Learning-Based Precoding for Multi-RIS-Aided Multiuser Downlink Systems with Practical Phase Shift",
        "link": "/arxiv/2509.25661",
        "arxiv_id": "2509.25661",
        "authors": "Po-Heng Chou, Bo-Ren Zheng, Wan-Jen Huang, Walid Saad, Yu Tsao, Ronald Y. Chang",
        "summary": "This study considers multiple reconfigurable intelligent surfaces (RISs)-aided multiuser downlink systems with the goal of jointly optimizing the transmitter precoding and RIS phase shift matrix to maximize spectrum efficiency. Unlike prior work that assumed ideal RIS reflectivity, a practical coupling effect is considered between reflecting amplitude and phase shift for the RIS elements. This makes the optimization problem non-convex. To address this challenge, we propose a deep deterministic policy gradient (DDPG)-based deep reinforcement learning (DRL) framework. The proposed model is evaluated under both fixed and random numbers of users in practical mmWave channel settings. Simulation results demonstrate that, despite its complexity, the proposed DDPG approach significantly outperforms optimization-based algorithms and double deep Q-learning, particularly in scenarios with random user distributions.",
        "subjects": "Information Theory, Artificial Intelligence, Machine Learning, Networking and Internet Architecture, Signal Processing",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.110043"
    },
    {
        "index": "#258",
        "title": "Capacity-Net-Based RIS Precoding Design without Channel Estimation for mmWave MIMO System",
        "link": "/arxiv/2509.25660",
        "arxiv_id": "2509.25660",
        "authors": "Chun-Yuan Huang, Po-Heng Chou, Wan-Jen Huang, Ying-Ren Chien, Yu Tsao",
        "summary": "In this paper, we propose Capacity-Net, a novel unsupervised learning approach aimed at maximizing the achievable rate in reflecting intelligent surface (RIS)-aided millimeter-wave (mmWave) multiple input multiple output (MIMO) systems. To combat severe channel fading of the mmWave spectrum, we optimize the phase-shifting factors of the reflective elements in the RIS to enhance the achievable rate. However, most optimization algorithms rely heavily on complete and accurate channel state information (CSI), which is often challenging to acquire since the RIS is mostly composed of passive components. To circumvent this challenge, we leverage unsupervised learning techniques with implicit CSI provided by the received pilot signals. Specifically, it usually requires perfect CSI to evaluate the achievable rate as a performance metric of the current optimization result of the unsupervised learning method. Instead of channel estimation, the Capacity-Net is proposed to establish a mapping among the received pilot signals, optimized RIS phase shifts, and the resultant achievable rates. Simulation results demonstrate the superiority of the proposed Capacity-Net-based unsupervised learning approach over learning methods based on traditional channel estimation.",
        "subjects": "Information Theory, Artificial Intelligence, Machine Learning, Networking and Internet Architecture, Signal Processing",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.115793"
    },
    {
        "index": "#259",
        "title": "YOLO-Based Defect Detection for Metal Sheets",
        "link": "/arxiv/2509.25659",
        "arxiv_id": "2509.25659",
        "authors": "Po-Heng Chou, Chun-Chi Wang, Wei-Lung Mao",
        "summary": "In this paper, we propose a YOLO-based deep learning (DL) model for automatic defect detection to solve the time-consuming and labor-intensive tasks in industrial manufacturing. In our experiments, the images of metal sheets are used as the dataset for training the YOLO model to detect the defects on the surfaces and in the holes of metal sheets. However, the lack of metal sheet images significantly degrades the performance of detection accuracy. To address this issue, the ConSinGAN is used to generate a considerable amount of data. Four versions of the YOLO model (i.e., YOLOv3, v4, v7, and v9) are combined with the ConSinGAN for data augmentation. The proposed YOLOv9 model with ConSinGAN outperforms the other YOLO models with an accuracy of 91.3%, and a detection time of 146 ms. The proposed YOLOv9 model is integrated into manufacturing hardware and a supervisory control and data acquisition (SCADA) system to establish a practical automated optical inspection (AOI) system. Additionally, the proposed automated defect detection is easily applied to other components in industrial manufacturing.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning, Image and Video Processing, Signal Processing",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.116350"
    },
    {
        "index": "#260",
        "title": "Using Images from a Video Game to Improve the Detection of Truck Axles",
        "link": "/arxiv/2509.25644",
        "arxiv_id": "2509.25644",
        "authors": "Leandro Arab Marcomini, Andre Luiz Cunha",
        "summary": "Convolutional Neural Networks (CNNs) traditionally require large amounts of data to train models with good performance. However, data collection is an expensive process, both in time and resources. Generated synthetic images are a good alternative, with video games producing realistic 3D models. This paper aims to determine whether images extracted from a video game can be effectively used to train a CNN to detect real-life truck axles. Three different databases were created, with real-life and synthetic trucks, to provide training and testing examples for three different You Only Look Once (YOLO) architectures. Results were evaluated based on four metrics: recall, precision, F1-score, and mean Average Precision (mAP). To evaluate the statistical significance of the results, the Mann-Whitney U test was also applied to the resulting mAP of all models. Synthetic images from trucks extracted from a video game proved to be a reliable source of training data, contributing to the performance of all networks. The highest mAP score reached 99\\%. Results indicate that synthetic images can be used to train neural networks, providing a reliable, low-cost data source for extracting knowledge.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.116790"
    },
    {
        "index": "#261",
        "title": "Generalized Contrastive Learning for Universal Multimodal Retrieval",
        "link": "/arxiv/2509.25638",
        "arxiv_id": "2509.25638",
        "authors": "Jungsoo Lee, Janghoon Cho, Hyojin Park, Munawar Hayat, Kyuwoong Hwang, Fatih Porikli, Sungha Choi",
        "summary": "Despite their consistent performance improvements, cross-modal retrieval models (e.g., CLIP) show degraded performances with retrieving keys composed of fused image-text modality (e.g., Wikipedia pages with both images and text). To address this critical challenge, multimodal retrieval has been recently explored to develop a unified single retrieval model capable of retrieving keys across diverse modality combinations. A common approach involves constructing new composed sets of image-text triplets (e.g., retrieving a pair of image and text given a query image). However, such an approach requires careful curation to ensure the dataset quality and fails to generalize to unseen modality combinations. To overcome these limitations, this paper proposes Generalized Contrastive Learning (GCL), a novel loss formulation that improves multimodal retrieval performance without the burdensome need for new dataset curation. Specifically, GCL operates by enforcing contrastive learning across all modalities within a mini-batch, utilizing existing image-caption paired datasets to learn a unified representation space. We demonstrate the effectiveness of GCL by showing consistent performance improvements on off-the-shelf multimodal retrieval models (e.g., VISTA, CLIP, and TinyCLIP) using the M-BEIR, MMEB, and CoVR benchmarks.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.117325"
    },
    {
        "index": "#262",
        "title": "When Langevin Monte Carlo Meets Randomization: Non-asymptotic Error Bounds beyond Log-Concavity and Gradient Lipschitzness",
        "link": "/arxiv/2509.25630",
        "arxiv_id": "2509.25630",
        "authors": "Xiaojie Wang, Bin Yang",
        "summary": "Efficient sampling from complex and high dimensional target distributions turns out to be a fundamental task in diverse disciplines such as scientific computing, statistics and machine learning. In this paper, we revisit the randomized Langevin Monte Carlo (RLMC) for sampling from high dimensional distributions without log-concavity. Under the gradient Lipschitz condition and the log-Sobolev inequality, we prove a uniform-in-time error bound in $\\mathcal{W}_2$-distance of order $O(\\sqrt{d}h)$ for the RLMC sampling algorithm, which matches the best one in the literature under the log-concavity condition. Moreover, when the gradient of the potential $U$ is non-globally Lipschitz with superlinear growth, modified RLMC algorithms are proposed and analyzed, with non-asymptotic error bounds established. To the best of our knowledge, the modified RLMC algorithms and their non-asymptotic error bounds are new in the non-globally Lipschitz setting.",
        "subjects": "Machine Learning, Machine Learning, Numerical Analysis",
        "date": "2025-09-30",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.117801"
    },
    {
        "index": "#265",
        "title": "Coupling Generative Modeling and an Autoencoder with the Causal Bridge",
        "link": "/arxiv/2509.25599",
        "arxiv_id": "2509.25599",
        "authors": "Ruolin Meng, Ming-Yu Chung, Dhanajit Brahma, Ricardo Henao, Lawrence Carin",
        "summary": "We consider inferring the causal effect of a treatment (intervention) on an outcome of interest in situations where there is potentially an unobserved confounder influencing both the treatment and the outcome. This is achievable by assuming access to two separate sets of control (proxy) measurements associated with treatment and outcomes, which are used to estimate treatment effects through a function termed the em causal bridge (CB). We present a new theoretical perspective, associated assumptions for when estimating treatment effects with the CB is feasible, and a bound on the average error of the treatment effect when the CB assumptions are violated. From this new perspective, we then demonstrate how coupling the CB with an autoencoder architecture allows for the sharing of statistical strength between observed quantities (proxies, treatment, and outcomes), thus improving the quality of the CB estimates. Experiments on synthetic and real-world data demonstrate the effectiveness of the proposed approach in relation to the state-of-the-art methodology for proxy measurements.",
        "subjects": "Machine Learning, Machine Learning, Methodology",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.119409"
    },
    {
        "index": "#266",
        "title": "Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks",
        "link": "/arxiv/2509.25598",
        "arxiv_id": "2509.25598",
        "authors": "Peiran Xu, Zhuohao Li, Xiaoying Xing, Guannan Zhang, Debiao Li, Kunyu Shi",
        "summary": "Large Language Models (LLMs) increasingly rely on external tools such as search engines to solve complex agentic tasks that require reasoning and external knowledge retrieval. Recently, reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in advancing capabilities of LLMs by rewarding the final answers via outcome rewards. While straightforward to supervise, outcome rewards only provide sparse signals and delayed feedback, which limits their effectiveness on long trajectories. Process rewards address this by evaluating intermediate steps, providing fine-grained supervision and encouraging grounded problem solving. However, it is notoriously hard to annotate step-wise labels, especially in non-verifiable process without \"golden\" answers. Furthermore, step-wise judgment requires the balance between local quality with contribution to the final outcome, as optimizing towards higher process reward may not always align with better final outcomes. To address the above challenges, we introduce Principle Process Reward (PPR), an RL approach that unifies principled step-level assessment and outcome verification. We train a principle-based reward model to improve the transparency and reliability of process evaluation, and further introduce a Reward Normalization (ReNorm) strategy to calibrate outcome and process rewards. Experiment results show that PPR achieves state-of-the-art performance across a wide range of benchmarks, demonstrating its impressive robustness and generalization. Our code and model collection is available in this link.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.119931"
    },
    {
        "index": "#267",
        "title": "MetaChest: Generalized few-shot learning of patologies from chest X-rays",
        "link": "/arxiv/2509.25590",
        "arxiv_id": "2509.25590",
        "authors": "Berenice Montalvo-Lezama, Gibran Fuentes-Pineda",
        "summary": "The limited availability of annotated data presents a major challenge for applying deep learning methods to medical image analysis. Few-shot learning methods aim to recognize new classes from only a small number of labeled examples. These methods are typically studied under the standard few-shot learning setting, where all classes in a task are new. However, medical applications such as pathology classification from chest X-rays often require learning new classes while simultaneously leveraging knowledge of previously known ones, a scenario more closely aligned with generalized few-shot classification. Despite its practical relevance, few-shot learning has been scarcely studied in this context. In this work, we present MetaChest, a large-scale dataset of 479,215 chest X-rays collected from four public databases. MetaChest includes a meta-set partition specifically designed for standard few-shot classification, as well as an algorithm for generating multi-label episodes. We conduct extensive experiments evaluating both a standard transfer learning approach and an extension of ProtoNet across a wide range of few-shot multi-label classification tasks. Our results demonstrate that increasing the number of classes per episode and the number of training examples per class improves classification performance. Notably, the transfer learning approach consistently outperforms the ProtoNet extension, despite not being tailored for few-shot learning. We also show that higher-resolution images improve accuracy at the cost of additional computation, while efficient model architectures achieve comparable performance to larger models with significantly reduced resource requirements.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.120386"
    },
    {
        "index": "#268",
        "title": "Conservative Decisions with Risk Scores",
        "link": "/arxiv/2509.25588",
        "arxiv_id": "2509.25588",
        "authors": "Yishu Wei, Wen-Yee Lee, George Ekow Quaye, Xiaogang Su",
        "summary": "In binary classification applications, conservative decision-making that allows for abstention can be advantageous. To this end, we introduce a novel approach that determines the optimal cutoff interval for risk scores, which can be directly available or derived from fitted models. Within this interval, the algorithm refrains from making decisions, while outside the interval, classification accuracy is maximized. Our approach is inspired by support vector machines (SVM), but differs in that it minimizes the classification margin rather than maximizing it. We provide the theoretical optimal solution to this problem, which holds important practical implications. Our proposed method not only supports conservative decision-making but also inherently results in a risk-coverage curve. Together with the area under the curve (AUC), this curve can serve as a comprehensive performance metric for evaluating and comparing classifiers, akin to the receiver operating characteristic (ROC) curve. To investigate and illustrate our approach, we conduct both simulation studies and a real-world case study in the context of diagnosing prostate cancer.",
        "subjects": "Machine Learning, Machine Learning, Methodology",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.126062"
    },
    {
        "index": "#271",
        "title": "Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology",
        "link": "/arxiv/2509.25559",
        "arxiv_id": "2509.25559",
        "authors": "Suvrankar Datta, Divya Buchireddygari, Lakshmi Vennela Chowdary Kaza, Mrudula Bhalke, Kautik Singh, Ayush Pandey, Sonit Sai Vasipalli, Upasana Karnwal, Hakikat Bir Singh Bhatti, Bhavya Ratan Maroo, Sanjana Hebbar, Rahul Joseph, Gurkawal Kaur, Devyani Singh, Akhil V, Dheeksha Devasya Shama Prasad, Nishtha Mahajan, Ayinaparthi Arisha, Rajesh Vanagundi, Reet Nandy, Kartik Vuthoo, Snigdhaa Rajvanshi, Nikhileswar Kondaveeti, Suyash Gunjal, Rishabh Jain, Rajat Jain, Anurag Agrawal",
        "summary": "Generalist multimodal AI systems such as large language models (LLMs) and vision language models (VLMs) are increasingly accessed by clinicians and patients alike for medical image interpretation through widely available consumer-facing chatbots. Most evaluations claiming expert level performance are on public datasets containing common pathologies. Rigorous evaluation of frontier models on difficult diagnostic cases remains limited. We developed a pilot benchmark of 50 expert-level \"spot diagnosis\" cases across multiple imaging modalities to evaluate the performance of frontier AI models against board-certified radiologists and radiology trainees. To mirror real-world usage, the reasoning modes of five popular frontier AI models were tested through their native web interfaces, viz. OpenAI o3, OpenAI GPT-5, Gemini 2.5 Pro, Grok-4, and Claude Opus 4.1. Accuracy was scored by blinded experts, and reproducibility was assessed across three independent runs. GPT-5 was additionally evaluated across various reasoning modes. Reasoning quality errors were assessed and a taxonomy of visual reasoning errors was defined. Board-certified radiologists achieved the highest diagnostic accuracy (83%), outperforming trainees (45%) and all AI models (best performance shown by GPT-5: 30%). Reliability was substantial for GPT-5 and o3, moderate for Gemini 2.5 Pro and Grok-4, and poor for Claude Opus 4.1. These findings demonstrate that advanced frontier models fall far short of radiologists in challenging diagnostic cases. Our benchmark highlights the present limitations of generalist AI in medical imaging and cautions against unsupervised clinical use. We also provide a qualitative analysis of reasoning traces and propose a practical taxonomy of visual reasoning errors by AI models for better understanding their failure modes, informing evaluation standards and guiding more robust model development.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.128041"
    },
    {
        "index": "#272",
        "title": "Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches",
        "link": "/arxiv/2509.25555",
        "arxiv_id": "2509.25555",
        "authors": "Amirreza Sokhankhosh, Khalid Hassan, Sara Rouhani",
        "summary": "Collaborative and distributed learning techniques, such as Federated Learning (FL) and Split Learning (SL), hold significant promise for leveraging sensitive data in privacy-critical domains. However, FL and SL suffer from key limitations -- FL imposes substantial computational demands on clients, while SL leads to prolonged training times. To overcome these challenges, SplitFed Learning (SFL) was introduced as a hybrid approach that combines the strengths of FL and SL. Despite its advantages, SFL inherits scalability, performance, and security issues from SL. In this paper, we propose two novel frameworks: Sharded SplitFed Learning (SSFL) and Blockchain-enabled SplitFed Learning (BSFL). SSFL addresses the scalability and performance constraints of SFL by distributing the workload and communication overhead of the SL server across multiple parallel shards. Building upon SSFL, BSFL replaces the centralized server with a blockchain-based architecture that employs a committee-driven consensus mechanism to enhance fairness and security. BSFL incorporates an evaluation mechanism to exclude poisoned or tampered model updates, thereby mitigating data poisoning and model integrity attacks. Experimental evaluations against baseline SL and SFL approaches show that SSFL improves performance and scalability by 31.2% and 85.2%, respectively. Furthermore, BSFL increases resilience to data poisoning attacks by 62.7% while maintaining superior performance under normal operating conditions. To the best of our knowledge, BSFL is the first blockchain-enabled framework to implement an end-to-end decentralized SplitFed Learning system.",
        "subjects": "Distributed, Parallel, and Cluster Computing, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.128516"
    },
    {
        "index": "#273",
        "title": "Learning to Interact in World Latent for Team Coordination",
        "link": "/arxiv/2509.25550",
        "arxiv_id": "2509.25550",
        "authors": "Dongsu Lee, Daehee Lee, Yaru Niu, Honguk Woo, Amy Zhang, Ding Zhao",
        "summary": "This work presents a novel representation learning framework, interactive world latent (IWoL), to facilitate team coordination in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging from multi-agent interaction and incomplete information induced by local observations. Our key insight is to construct a learnable representation space that jointly captures inter-agent relations and task-specific world information by directly modeling communication protocols. This representation, we maintain fully decentralized execution with implicit coordination, all while avoiding the inherent drawbacks of explicit message passing, e.g., slower decision-making, vulnerability to malicious attackers, and sensitivity to bandwidth constraints. In practice, our representation can be used not only as an implicit latent for each agent, but also as an explicit message for communication. Across four challenging MARL benchmarks, we evaluate both variants and show that IWoL provides a simple yet powerful key for team coordination. Moreover, we demonstrate that our representation can be combined with existing MARL algorithms to further enhance their performance.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.129016"
    },
    {
        "index": "#274",
        "title": "Hybrid Approach for Enhancing Lesion Segmentation in Fundus Images",
        "link": "/arxiv/2509.25549",
        "arxiv_id": "2509.25549",
        "authors": "Mohammadmahdi Eshragh, Emad A. Mohammed, Behrouz Far, Ezekiel Weis, Carol L Shields, Sandor R Ferenczy, Trafford Crump",
        "summary": "Choroidal nevi are common benign pigmented lesions in the eye, with a small risk of transforming into melanoma. Early detection is critical to improving survival rates, but misdiagnosis or delayed diagnosis can lead to poor outcomes. Despite advancements in AI-based image analysis, diagnosing choroidal nevi in colour fundus images remains challenging, particularly for clinicians without specialized expertise. Existing datasets often suffer from low resolution and inconsistent labelling, limiting the effectiveness of segmentation models. This paper addresses the challenge of achieving precise segmentation of fundus lesions, a critical step toward developing robust diagnostic tools. While deep learning models like U-Net have demonstrated effectiveness, their accuracy heavily depends on the quality and quantity of annotated data. Previous mathematical/clustering segmentation methods, though accurate, required extensive human input, making them impractical for medical applications. This paper proposes a novel approach that combines mathematical/clustering segmentation models with insights from U-Net, leveraging the strengths of both methods. This hybrid model improves accuracy, reduces the need for large-scale training data, and achieves significant performance gains on high-resolution fundus images. The proposed model achieves a Dice coefficient of 89.7% and an IoU of 80.01% on 1024*1024 fundus images, outperforming the Attention U-Net model, which achieved 51.3% and 34.2%, respectively. It also demonstrated better generalizability on external datasets. This work forms a part of a broader effort to develop a decision support system for choroidal nevus diagnosis, with potential applications in automated lesion annotation to enhance the speed and accuracy of diagnosis and monitoring.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.129564"
    },
    {
        "index": "#277",
        "title": "Personalized Auto-Grading and Feedback System for Constructive Geometry Tasks Using Large Language Models on an Online Math Platform",
        "link": "/arxiv/2509.25529",
        "arxiv_id": "2509.25529",
        "authors": "Yong Oh Lee, Byeonghun Bang, Joohyun Lee, Sejun Oh",
        "summary": "As personalized learning gains increasing attention in mathematics education, there is a growing demand for intelligent systems that can assess complex student responses and provide individualized feedback in real time. In this study, we present a personalized auto-grading and feedback system for constructive geometry tasks, developed using large language models (LLMs) and deployed on the Algeomath platform, a Korean online tool designed for interactive geometric constructions. The proposed system evaluates student-submitted geometric constructions by analyzing their procedural accuracy and conceptual understanding. It employs a prompt-based grading mechanism using GPT-4, where student answers and model solutions are compared through a few-shot learning approach. Feedback is generated based on teacher-authored examples built from anticipated student responses, and it dynamically adapts to the student's problem-solving history, allowing up to four iterative attempts per question. The system was piloted with 79 middle-school students, where LLM-generated grades and feedback were benchmarked against teacher judgments. Grading closely aligned with teachers, and feedback helped many students revise errors and complete multi-step geometry tasks. While short-term corrections were frequent, longer-term transfer effects were less clear. Overall, the study highlights the potential of LLMs to support scalable, teacher-aligned formative assessment in mathematics, while pointing to improvements needed in terminology handling and feedback design.",
        "subjects": "Computers and Society, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.136561"
    },
    {
        "index": "#278",
        "title": "Defeating Cerberus: Concept-Guided Privacy-Leakage Mitigation in Multimodal Language Models",
        "link": "/arxiv/2509.25525",
        "arxiv_id": "2509.25525",
        "authors": "Boyang Zhang, Istemi Ekin Akkus, Ruichuan Chen, Alice Dethise, Klaus Satzke, Ivica Rimac, Yang Zhang",
        "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in processing and reasoning over diverse modalities, but their advanced abilities also raise significant privacy concerns, particularly regarding Personally Identifiable Information (PII) leakage. While relevant research has been conducted on single-modal language models to some extent, the vulnerabilities in the multimodal setting have yet to be fully investigated. In this work, we investigate these emerging risks with a focus on vision language models (VLMs), a representative subclass of MLLMs that covers the two modalities most relevant for PII leakage, vision and text. We introduce a concept-guided mitigation approach that identifies and modifies the model's internal states associated with PII-related content. Our method guides VLMs to refuse PII-sensitive tasks effectively and efficiently, without requiring re-training or fine-tuning. We also address the current lack of multimodal PII datasets by constructing various ones that simulate real-world scenarios. Experimental results demonstrate that the method can achieve an average refusal rate of 93.3% for various PII-related tasks with minimal impact on unrelated model performances. We further examine the mitigation's performance under various conditions to show the adaptability of our proposed method.",
        "subjects": "Cryptography and Security, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.137082"
    },
    {
        "index": "#279",
        "title": "AGNOMIN - Architecture Agnostic Multi-Label Function Name Prediction",
        "link": "/arxiv/2509.25514",
        "arxiv_id": "2509.25514",
        "authors": "Yonatan Gizachew Achamyeleh, Tongtao Zhang, Joshua Hyunki Kim, Gabriel Garcia, Shih-Yuan Yu, Anton Kocheturov, Mohammad Abdullah Al Faruque",
        "summary": "Function name prediction is crucial for understanding stripped binaries in software reverse engineering, a key step for \\textbf{enabling subsequent vulnerability analysis and patching}. However, existing approaches often struggle with architecture-specific limitations, data scarcity, and diverse naming conventions. We present AGNOMIN, a novel architecture-agnostic approach for multi-label function name prediction in stripped binaries. AGNOMIN builds Feature-Enriched Hierarchical Graphs (FEHGs), combining Control Flow Graphs, Function Call Graphs, and dynamically learned \\texttt{PCode} features. A hierarchical graph neural network processes this enriched structure to generate consistent function representations across architectures, vital for \\textbf{scalable security assessments}. For function name prediction, AGNOMIN employs a Renée-inspired decoder, enhanced with an attention-based head layer and algorithmic improvements. We evaluate AGNOMIN on a comprehensive dataset of 9,000 ELF executable binaries across three architectures, demonstrating its superior performance compared to state-of-the-art approaches, with improvements of up to 27.17\\% in precision and 55.86\\% in recall across the testing dataset. Moreover, AGNOMIN generalizes well to unseen architectures, achieving 5.89\\% higher recall than the closest baseline. AGNOMIN's practical utility has been validated through security hackathons, where it successfully aided reverse engineers in analyzing and patching vulnerable binaries across different architectures.",
        "subjects": "Software Engineering, Cryptography and Security, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.137633"
    },
    {
        "index": "#280",
        "title": "One-shot Conditional Sampling: MMD meets Nearest Neighbors",
        "link": "/arxiv/2509.25507",
        "arxiv_id": "2509.25507",
        "authors": "Anirban Chatterjee, Sayantan Choudhury, Rohan Hore",
        "summary": "How can we generate samples from a conditional distribution that we never fully observe? This question arises across a broad range of applications in both modern machine learning and classical statistics, including image post-processing in computer vision, approximate posterior sampling in simulation-based inference, and conditional distribution modeling in complex data settings. In such settings, compared with unconditional sampling, additional feature information can be leveraged to enable more adaptive and efficient sampling. Building on this, we introduce Conditional Generator using MMD (CGMMD), a novel framework for conditional sampling. Unlike many contemporary approaches, our method frames the training objective as a simple, adversary-free direct minimization problem. A key feature of CGMMD is its ability to produce conditional samples in a single forward pass of the generator, enabling practical one-shot sampling with low test-time complexity. We establish rigorous theoretical bounds on the loss incurred when sampling from the CGMMD sampler, and prove convergence of the estimated distribution to the true conditional distribution. In the process, we also develop a uniform concentration result for nearest-neighbor based functionals, which may be of independent interest. Finally, we show that CGMMD performs competitively on synthetic tasks involving complex conditional densities, as well as on practical applications such as image denoising and image super-resolution.",
        "subjects": "Machine Learning, Machine Learning, Statistics Theory, Methodology",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.138148"
    },
    {
        "index": "#281",
        "title": "Scalable Boltzmann Generators for equilibrium sampling of large-scale materials",
        "link": "/arxiv/2509.25486",
        "arxiv_id": "2509.25486",
        "authors": "Maximilian Schebek, Jutta Rogal",
        "summary": "The use of generative models to sample equilibrium distributions of many-body systems, as first demonstrated by Boltzmann Generators, has attracted substantial interest due to their ability to produce unbiased and uncorrelated samples in `one shot'. Despite their promise and impressive results across the natural sciences, scaling these models to large systems remains a major challenge. In this work, we introduce a Boltzmann Generator architecture that addresses this scalability bottleneck with a focus on applications in materials science. We leverage augmented coupling flows in combination with graph neural networks to base the generation process on local environmental information, while allowing for energy-based training and fast inference. Compared to previous architectures, our model trains significantly faster, requires far less computational resources, and achieves superior sampling efficiencies. Crucially, the architecture is transferable to larger system sizes, which allows for the efficient sampling of materials with simulation cells of unprecedented size. We demonstrate the potential of our approach by applying it to several materials systems, including Lennard-Jones crystals, ice phases of mW water, and the phase diagram of silicon, for system sizes well above one thousand atoms. The trained Boltzmann Generators produce highly accurate equilibrium ensembles for various crystal structures, as well as Helmholtz and Gibbs free energies across a range of system sizes, able to reach scales where finite-size effects become negligible.",
        "subjects": "Statistical Mechanics, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.138612"
    },
    {
        "index": "#282",
        "title": "Message passing-based inference in an autoregressive active inference agent",
        "link": "/arxiv/2509.25482",
        "arxiv_id": "2509.25482",
        "authors": "Wouter M. Kouw, Tim N. Nisslbeck, Wouter L. N. Nuijten",
        "summary": "We present the design of an autoregressive active inference agent in the form of message passing on a factor graph. Expected free energy is derived and distributed across a planning graph. The proposed agent is validated on a robot navigation task, demonstrating exploration and exploitation in a continuous-valued observation space with bounded continuous-valued actions. Compared to a classical optimal controller, the agent modulates action based on predictive uncertainty, arriving later but with a better model of the robot's dynamics.",
        "subjects": "Artificial Intelligence, Machine Learning, Robotics, Systems and Control, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.139136"
    },
    {
        "index": "#283",
        "title": "Fair Classification by Direct Intervention on Operating Characteristics",
        "link": "/arxiv/2509.25481",
        "arxiv_id": "2509.25481",
        "authors": "Kevin Jiang, Edgar Dobriban",
        "summary": "We develop new classifiers under group fairness in the attribute-aware setting for binary classification with multiple group fairness constraints (e.g., demographic parity (DP), equalized odds (EO), and predictive parity (PP)). We propose a novel approach, applicable to linear fractional constraints, based on directly intervening on the operating characteristics of a pre-trained base classifier, by (i) identifying optimal operating characteristics using the base classifier's group-wise ROC convex hulls and (ii) post-processing the base classifier to match those targets. As practical post-processors, we consider randomizing a mixture of group-wise thresholding rules subject to minimizing the expected number of interventions. We further extend our approach to handle multiple protected attributes and multiple linear fractional constraints. On standard datasets (COMPAS and ACSIncome), our methods simultaneously satisfy approximate DP, EO, and PP with few interventions and a near-oracle drop in accuracy; comparing favorably to previous methods.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.139594"
    },
    {
        "index": "#284",
        "title": "TDHook: A Lightweight Framework for Interpretability",
        "link": "/arxiv/2509.25475",
        "arxiv_id": "2509.25475",
        "authors": "Yoann Poupart",
        "summary": "Interpretability of Deep Neural Networks (DNNs) is a growing field driven by the study of vision and language models. Yet, some use cases, like image captioning, or domains like Deep Reinforcement Learning (DRL), require complex modelling, with multiple inputs and outputs or use composable and separated networks. As a consequence, they rarely fit natively into the API of popular interpretability frameworks. We thus present TDHook, an open-source, lightweight, generic interpretability framework based on $\\texttt{tensordict}$ and applicable to any $\\texttt{torch}$ model. It focuses on handling complex composed models which can be trained for Computer Vision, Natural Language Processing, Reinforcement Learning or any other domain. This library features ready-to-use methods for attribution, probing and a flexible get-set API for interventions, and is aiming to bridge the gap between these method classes to make modern interpretability pipelines more accessible. TDHook is designed with minimal dependencies, requiring roughly half as much disk space as $\\texttt{transformer_lens}$, and, in our controlled benchmark, achieves up to a $\\times$2 speed-up over $\\texttt{captum}$ when running integrated gradients for multi-target pipelines on both CPU and GPU. In addition, to value our work, we showcase concrete use cases of our library with composed interpretability pipelines in Computer Vision (CV) and Natural Language Processing (NLP), as well as with complex models in DRL.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.140074"
    },
    {
        "index": "#286",
        "title": "PIPer: On-Device Environment Setup via Online Reinforcement Learning",
        "link": "/arxiv/2509.25455",
        "arxiv_id": "2509.25455",
        "authors": "Alexander Kovrigin, Aleksandra Eliseeva, Konstantin Grotov, Egor Bogomolov, Yaroslav Zharov",
        "summary": "Environment setup-the process of configuring the system to work with a specific software project-represents a persistent challenge in Software Engineering (SE). Automated environment setup methods could assist developers by providing fully configured environments for arbitrary repositories without manual effort. This also helps SE researchers to scale execution-based benchmarks. However, recent studies reveal that even state-of-the-art Large Language Models (LLMs) achieve limited success in automating this task. To address this limitation, we tune a specialized model for environment setup. We combine supervised fine-tuning for generating correct Bash scripts and Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model runnable on consumer hardware) to perform on par with larger models-Qwen3-32B and GPT-4o. The training code and model checkpoints are available online: https://github.com/JetBrains-Research/PIPer.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.146311"
    },
    {
        "index": "#287",
        "title": "Neural Optimal Transport Meets Multivariate Conformal Prediction",
        "link": "/arxiv/2509.25444",
        "arxiv_id": "2509.25444",
        "authors": "Vladimir Kondratyev, Alexander Fishkov, Nikita Kotelevskii, Mahmoud Hegazy, Remi Flamary, Maxim Panov, Eric Moulines",
        "summary": "We propose a framework for conditional vector quantile regression (CVQR) that combines neural optimal transport with amortized optimization, and apply it to multivariate conformal prediction. Classical quantile regression does not extend naturally to multivariate responses, while existing approaches often ignore the geometry of joint distributions. Our method parametrizes the conditional vector quantile function as the gradient of a convex potential implemented by an input-convex neural network, ensuring monotonicity and uniform ranks. To reduce the cost of solving high-dimensional variational problems, we introduced amortized optimization of the dual potentials, yielding efficient training and faster inference. We then exploit the induced multivariate ranks for conformal prediction, constructing distribution-free predictive regions with finite-sample validity. Unlike coordinatewise methods, our approach adapts to the geometry of the conditional distribution, producing tighter and more informative regions. Experiments on benchmark datasets show improved coverage-efficiency trade-offs compared to baselines, highlighting the benefits of integrating neural optimal transport with conformal prediction.",
        "subjects": "Machine Learning, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.146867"
    },
    {
        "index": "#288",
        "title": "Bayesian Transformer for Pan-Arctic Sea Ice Concentration Mapping and Uncertainty Estimation using Sentinel-1, RCM, and AMSR2 Data",
        "link": "/arxiv/2509.25437",
        "arxiv_id": "2509.25437",
        "authors": "Mabel Heffring, Lincoln Linlin Xu",
        "summary": "Although high-resolution mapping of Pan-Arctic sea ice with reliable corresponding uncertainty is essential for operational sea ice concentration (SIC) charting, it is a difficult task due to some key challenges, e.g., the subtle nature of ice signature features, model uncertainty, and data heterogeneity. This letter presents a novel Bayesian Transformer approach for Pan-Arctic SIC mapping and uncertainty quantification using Sentinel-1, RADARSAT Constellation Mission (RCM), and Advanced Microwave Scanning Radiometer 2 (AMSR2) data. First, to improve feature extraction, we design a novel high-resolution Transformer model with both global and local modules that can better discern the subtle differences in sea ice patterns. Second, to improve uncertainty quantification, we design a Bayesian extension of the proposed Transformer model, treating its parameters as random variables to more effectively capture uncertainties. Third, to address data heterogeneity, we fuse three different data types (Sentinel-1, RCM, and AMSR2) at decision-level to improve both SIC mapping and uncertainty quantification. The proposed approach is tested on Pan-Arctic datasets from September 2021, and the results demonstrate that the proposed model can achieve both high-resolution SIC maps and robust uncertainty maps compared to other uncertainty quantification approaches.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.147575"
    },
    {
        "index": "#289",
        "title": "RADAR: Reasoning-Ability and Difficulty-Aware Routing for Reasoning LLMs",
        "link": "/arxiv/2509.25426",
        "arxiv_id": "2509.25426",
        "authors": "Nigel Fernandez, Branislav Kveton, Ryan A. Rossi, Andrew S. Lan, Zichao Wang",
        "summary": "Reasoning language models have demonstrated remarkable performance on many challenging tasks in math, science, and coding. Choosing the right reasoning model for practical deployment involves a performance and cost tradeoff at two key levels: model size and reasoning budget, where larger models and higher reasoning budget lead to better performance but with increased cost and latency. In this work, we tackle this tradeoff from the angle of model configuration routing for different queries, and present RADAR (Reasoning-Ability and Difficulty-Aware Routing), a lightweight, interpretable, and scalable routing framework. Inspired by psychometrics, RADAR learns an item response model from model responses with different budgets to different queries, with interpretable parameters including query difficulties and model-budget abilities. RADAR then routes queries with higher difficulty to model-budget pairs with higher ability, and vice versa. We conduct extensive experiments on 8 widely used challenging reasoning benchmarks, demonstrating the superior performance of RADAR compared to state-of-the-art model routing methods. RADAR also exhibits query generalization capabilities, showing strong performance on out-of-distribution queries in all benchmarks. RADAR is also scalable and can efficiently integrate additional models by dynamically selecting a small set of evaluation queries to estimate their abilities.",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.148102"
    },
    {
        "index": "#291",
        "title": "Boolean Satisfiability via Imitation Learning",
        "link": "/arxiv/2509.25411",
        "arxiv_id": "2509.25411",
        "authors": "Zewei Zhang, Huan Liu, Yuanhao Yu, Jun Chen, Xiangyu Xu",
        "summary": "We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT",
        "subjects": "Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.149270"
    },
    {
        "index": "#293",
        "title": "A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects",
        "link": "/arxiv/2509.25397",
        "arxiv_id": "2509.25397",
        "authors": "Johan Linåker, Cailean Osborne, Jennifer Ding, Ben Burtenshaw",
        "summary": "The proliferation of open large language models (LLMs) is fostering a vibrant ecosystem of research and innovation in artificial intelligence (AI). However, the methods of collaboration used to develop open LLMs both before and after their public release have not yet been comprehensively studied, limiting our understanding of how open LLM projects are initiated, organized, and governed as well as what opportunities there are to foster this ecosystem even further. We address this gap through an exploratory analysis of open collaboration throughout the development and reuse lifecycle of open LLMs, drawing on semi-structured interviews with the developers of 14 open LLMs from grassroots projects, research institutes, startups, and Big Tech companies in North America, Europe, Africa, and Asia. We make three key contributions to research and practice. First, collaboration in open LLM projects extends far beyond the LLMs themselves, encompassing datasets, benchmarks, open source frameworks, leaderboards, knowledge sharing and discussion forums, and compute partnerships, among others. Second, open LLM developers have a variety of social, economic, and technological motivations, from democratizing AI access and promoting open science to building regional ecosystems and expanding language representation. Third, the sampled open LLM projects exhibit five distinct organizational models, ranging from single company projects to non-profit-sponsored grassroots projects, which vary in their centralization of control and community engagement strategies used throughout the open LLM lifecycle. We conclude with practical recommendations for stakeholders seeking to support the global community building a more open future for AI.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.150337"
    },
    {
        "index": "#295",
        "title": "SynthPert: Enhancing LLM Biological Reasoning via Synthetic Reasoning Traces for Cellular Perturbation Prediction",
        "link": "/arxiv/2509.25346",
        "arxiv_id": "2509.25346",
        "authors": "Lawrence Phillips, Marc Boubnovski Martell, Aditya Misra, Josefa Lia Stoisser, Cesar A. Prada-Medina, Rory Donovan-Maiye, Kaspar Märtens",
        "summary": "Predicting cellular responses to genetic perturbations represents a fundamental challenge in systems biology, critical for advancing therapeutic discovery and virtual cell modeling. While large language models (LLMs) show promise for biological reasoning, their application to perturbation prediction remains underexplored due to challenges in adapting them to structured experimental data. We present SynthPert, a novel method that enhances LLM performance through supervised fine-tuning on synthetic reasoning traces generated by frontier models. Using the PerturbQA benchmark, we demonstrate that our approach not only achieves state-of-the-art performance but surpasses the capabilities of the frontier model that generated the training data. Our results reveal three key insights: (1) Synthetic reasoning traces effectively distill biological knowledge even when partially inaccurate, (2) This approach enables cross-cell-type generalization with 87% accuracy on unseen RPE1 cells, and (3) Performance gains persist despite using only 2% of quality-filtered training data. This work shows the effectiveness of synthetic reasoning distillation for enhancing domain-specific reasoning in LLMs.",
        "subjects": "Artificial Intelligence, Machine Learning, Cell Behavior, Genomics",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.151479"
    },
    {
        "index": "#296",
        "title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes",
        "link": "/arxiv/2509.25339",
        "arxiv_id": "2509.25339",
        "authors": "Paul Gavrikov, Wei Lin, M. Jehanzeb Mirza, Soumya Jahagirdar, Muhammad Huzaifa, Sivan Doveh, Serena Yeung-Levy, James Glass, Hilde Kuehne",
        "summary": "Is basic visual understanding really solved in state-of-the-art VLMs? We present VisualOverload, a slightly different visual question answering (VQA) benchmark comprising 2,720 question-answer pairs, with privately held ground-truth responses. Unlike prior VQA datasets that typically focus on near global image understanding, VisualOverload challenges models to perform simple, knowledge-free vision tasks in densely populated (or, overloaded) scenes. Our dataset consists of high-resolution scans of public-domain paintings that are populated with multiple figures, actions, and unfolding subplots set against elaborately detailed backdrops. We manually annotated these images with questions across six task categories to probe for a thorough understanding of the scene. We hypothesize that current benchmarks overestimate the performance of VLMs, and encoding and reasoning over details is still a challenging task for them, especially if they are confronted with densely populated scenes. Indeed, we observe that even the best model (o3) out of 37 tested models only achieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on all questions. Beyond a thorough evaluation, we complement our benchmark with an error analysis that reveals multiple failure modes, including a lack of counting skills, failure in OCR, and striking logical inconsistencies under complex tasks. Altogether, VisualOverload exposes a critical gap in current vision models and offers a crucial resource for the community to develop better models. Benchmark: http://paulgavrikov.github.io/visualoverload",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning, Image and Video Processing",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.152135"
    },
    {
        "index": "#297",
        "title": "Aspects of holographic entanglement using physics-informed-neural-networks",
        "link": "/arxiv/2509.25311",
        "arxiv_id": "2509.25311",
        "authors": "Anirudh Deb, Yaman Sanghavi",
        "summary": "We implement physics-informed-neural-networks (PINNs) to compute holographic entanglement entropy and entanglement wedge cross section. This technique allows us to compute these quantities for arbitrary shapes of the subregions in any asymptotically AdS metric. We test our computations against some known results and further demonstrate the utility of PINNs in examples, where it is not straightforward to perform such computations.",
        "subjects": "High Energy Physics - Theory, Machine Learning, Computational Physics",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.157802"
    },
    {
        "index": "#299",
        "title": "Learning Relationships Between Separate Audio Tracks for Creative Applications",
        "link": "/arxiv/2509.25296",
        "arxiv_id": "2509.25296",
        "authors": "Balthazar Bujard, Jérôme Nika, Fédéric Bevilacqua, Nicolas Obin",
        "summary": "This paper presents the first step in a research project situated within the field of musical agents. The objective is to achieve, through training, the tuning of the desired musical relationship between a live musical input and a real-time generated musical output, through the curation of a database of separated tracks. We propose an architecture integrating a symbolic decision module capable of learning and exploiting musical relationships from such musical corpus. We detail an offline implementation of this architecture employing Transformers as the decision module, associated with a perception module based on Wav2Vec 2.0, and concatenative synthesis as audio renderer. We present a quantitative evaluation of the decision module's ability to reproduce learned relationships extracted during training. We demonstrate that our decision module can predict a coherent track B when conditioned by its corresponding ''guide'' track A, based on a corpus of paired tracks (A, B).",
        "subjects": "Sound, Artificial Intelligence, Human-Computer Interaction, Machine Learning, Audio and Speech Processing",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.158934"
    },
    {
        "index": "#300",
        "title": "Mechanisms of Matter: Language Inferential Benchmark on Physicochemical Hypothesis in Materials Synthesis",
        "link": "/arxiv/2509.25281",
        "arxiv_id": "2509.25281",
        "authors": "Yingming Pu, Tao Lin, Hongyu Chen",
        "summary": "The capacity of Large Language Models (LLMs) to generate valid scientific hypotheses for materials synthesis remains largely unquantified, hindered by the absence of benchmarks probing physicochemical logics reasoning. To address this, we introduce MatterMech, a benchmark for evaluating LLM-generated hypotheses across eight nanomaterial synthesis domains. Our analysis reveals a critical disconnect: LLMs are proficient in abstract logic yet fail to ground their reasoning in fundamental physicochemical principles. We demonstrate that our proposed principle-aware prompting methodology substantially outperforms standard Chain-of-Thought, enhancing both hypothesis accuracy and computational efficiency. This work provides a methodological framework to advance LLMs toward reliable scientific hypothesis generation in materials science. The MatterMech benchmark and associated code is publicly available at \\href{https://github.com/amair-lab/MatterMech}{GitHub}.",
        "subjects": "Materials Science, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.159426"
    },
    {
        "index": "#301",
        "title": "RL in the Wild: Characterizing RLVR Training in LLM Deployment",
        "link": "/arxiv/2509.25279",
        "arxiv_id": "2509.25279",
        "authors": "Jiecheng Zhou, Qinghao Hu, Yuyang Jin, Zerui Wang, Peng Sun, Yuzhe Gu, Wenwei Zhang, Mingshu Zhai, Xingcheng Zhang, Weiming Zhang",
        "summary": "Large Language Models (LLMs) are now widely used across many domains. With their rapid development, Reinforcement Learning with Verifiable Rewards (RLVR) has surged in recent months to enhance their reasoning and understanding abilities. However, its complex data flows and diverse tasks pose substantial challenges to RL training systems, and there is limited understanding of RLVR from a system perspective. To thoroughly understand the system challenges introduced by RLVR, we present a characterization study of RLVR tasks in our LLM deployment. Specifically, we investigate the distribution and variation trends of workloads across different RL tasks across training steps. We identify issues such as GPU idling caused by skewed sequence length distribution, inefficient parallel strategies in dynamically varying workloads, inefficient data management mechanisms, and load imbalance. We describe our observations and call for further investigation into the remaining open challenges. Furthermore, we propose PolyTrace benchmark suite to conduct evaluation with realistic workloads, and a practical use case validates that PolyTrace benchmark suite exhibits 94.7% accuracy.",
        "subjects": "Artificial Intelligence, Distributed, Parallel, and Cluster Computing, Machine Learning",
        "date": "2025-09-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.160066"
    },
    {
        "index": "#302",
        "title": "DNABERT-2: Fine-Tuning a Genomic Language Model for Colorectal Gene Enhancer Classification",
        "link": "/arxiv/2509.25274",
        "arxiv_id": "2509.25274",
        "authors": "Darren King, Yaser Atlasi, Gholamreza Rafiee",
        "summary": "Gene enhancers control when and where genes switch on, yet their sequence diversity and tissue specificity make them hard to pinpoint in colorectal cancer. We take a sequence-only route and fine-tune DNABERT-2, a transformer genomic language model that uses byte-pair encoding to learn variable-length tokens from DNA. Using assays curated via the Johnston Cancer Research Centre at Queen's University Belfast, we assembled a balanced corpus of 2.34 million 1 kb enhancer sequences, applied summit-centered extraction and rigorous de-duplication including reverse-complement collapse, and split the data stratified by class. With a 4096-term vocabulary and a 232-token context chosen empirically, the DNABERT-2-117M classifier was trained with Optuna-tuned hyperparameters and evaluated on 350742 held-out sequences. The model reached PR-AUC 0.759, ROC-AUC 0.743, and best F1 0.704 at an optimized threshold (0.359), with recall 0.835 and precision 0.609. Against a CNN-based EnhancerNet trained on the same data, DNABERT-2 delivered stronger threshold-independent ranking and higher recall, although point accuracy was lower. To our knowledge, this is the first study to apply a second-generation genomic language model with BPE tokenization to enhancer classification in colorectal cancer, demonstrating the feasibility of capturing tumor-associated regulatory signals directly from DNA sequence alone. Overall, our results show that transformer-based genomic models can move beyond motif-level encodings toward holistic classification of regulatory elements, offering a novel path for cancer genomics. Next steps will focus on improving precision, exploring hybrid CNN-transformer designs, and validating across independent datasets to strengthen real-world utility.",
        "subjects": "Genomics, Artificial Intelligence, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.160578"
    },
    {
        "index": "#304",
        "title": "Position-Blind Ptychography: Viability of image reconstruction via data-driven variational inference",
        "link": "/arxiv/2509.25269",
        "arxiv_id": "2509.25269",
        "authors": "Simon Welker, Lorenz Kuger, Tim Roith, Berthy Feng, Martin Burger, Timo Gerkmann, Henry Chapman",
        "summary": "In this work, we present and investigate the novel blind inverse problem of position-blind ptychography, i.e., ptychographic phase retrieval without any knowledge of scan positions, which then must be recovered jointly with the image. The motivation for this problem comes from single-particle diffractive X-ray imaging, where particles in random orientations are illuminated and a set of diffraction patterns is collected. If one uses a highly focused X-ray beam, the measurements would also become sensitive to the beam positions relative to each particle and therefore ptychographic, but these positions are also unknown. We investigate the viability of image reconstruction in a simulated, simplified 2-D variant of this difficult problem, using variational inference with modern data-driven image priors in the form of score-based diffusion models. We find that, with the right illumination structure and a strong prior, one can achieve reliable and successful image reconstructions even under measurement noise, in all except the most difficult evaluated imaging scenario.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition, Machine Learning, Numerical Analysis, Optics",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.161829"
    },
    {
        "index": "#305",
        "title": "Evaluating the Impact of Radiographic Noise on Chest X-ray Semantic Segmentation and Disease Classification Using a Scalable Noise Injection Framework",
        "link": "/arxiv/2509.25265",
        "arxiv_id": "2509.25265",
        "authors": "Derek Jiu, Kiran Nijjer, Nishant Chinta, Ryan Bui, Ben Liu, Kevin Zhu",
        "summary": "Deep learning models are increasingly used for radiographic analysis, but their reliability is challenged by the stochastic noise inherent in clinical imaging. A systematic, cross-task understanding of how different noise types impact these models is lacking. Here, we evaluate the robustness of state-of-the-art convolutional neural networks (CNNs) to simulated quantum (Poisson) and electronic (Gaussian) noise in two key chest X-ray tasks: semantic segmentation and pulmonary disease classification. Using a novel, scalable noise injection framework, we applied controlled, clinically-motivated noise severities to common architectures (UNet, DeepLabV3, FPN; ResNet, DenseNet, EfficientNet) on public datasets (Landmark, ChestX-ray14). Our results reveal a stark dichotomy in task robustness. Semantic segmentation models proved highly vulnerable, with lung segmentation performance collapsing under severe electronic noise (Dice Similarity Coefficient drop of 0.843), signifying a near-total model failure. In contrast, classification tasks demonstrated greater overall resilience, but this robustness was not uniform. We discovered a differential vulnerability: certain tasks, such as distinguishing Pneumothorax from Atelectasis, failed catastrophically under quantum noise (AUROC drop of 0.355), while others were more susceptible to electronic noise. These findings demonstrate that while classification models possess a degree of inherent robustness, pixel-level segmentation tasks are far more brittle. The task- and noise-specific nature of model failure underscores the critical need for targeted validation and mitigation strategies before the safe clinical deployment of diagnostic AI.",
        "subjects": "Image and Video Processing, Machine Learning",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.167478"
    },
    {
        "index": "#306",
        "title": "From NL2SQL to NL2GeoSQL: GeoSQL-Eval for automated evaluation of LLMs on PostGIS queries",
        "link": "/arxiv/2509.25264",
        "arxiv_id": "2509.25264",
        "authors": "Shuyang Hou, Haoyue Jiao, Ziqi Liu, Lutong Xie, Guanyu Chen, Shaowen Wu, Xuefeng Guan, Huayi Wu",
        "summary": "In recent years, large language models (LLMs) have achieved remarkable progress in natural language understanding and structured query generation (NL2SQL). However, extending these advances to GeoSQL tasks in the PostGIS environment remains challenging due to the complexity of spatial functions, geometric data types, and execution semantics. Existing evaluations primarily focus on general relational databases or Google Earth Engine code generation, leaving a lack of systematic benchmarks tailored to spatial databases. To address this gap, this study introduces GeoSQL-Eval, the first end-to-end automated evaluation framework for PostGIS query generation. Built upon Webb's Depth of Knowledge (DOK) model, the framework encompasses four cognitive dimensions, five proficiency levels, and twenty task categories, providing a comprehensive assessment of model performance in terms of knowledge acquisition, syntactic generation, semantic alignment, execution accuracy, and robustness. In parallel, we developed GeoSQL-Bench, a benchmark dataset comprising 14178 questions that span three task types, 340 PostGIS functions, and 82 domain-specific databases. Leveraging this framework, we systematically evaluated 24 representative models across six categories, applying entropy-weighting and statistical analyses to reveal differences in performance, error distributions, and resource consumption patterns. Furthermore, we established a public GeoSQL-Eval leaderboard that enables global research teams to conduct ongoing testing and comparison. These contributions not only extend the boundaries of NL2SQL applications but also provide a standardized, interpretable, and scalable framework for evaluating LLM performance in spatial database contexts, offering valuable insights for model optimization and applications in geographic information science, urban studies, and spatial analysis.",
        "subjects": "Databases, Artificial Intelligence, Machine Learning, Software Engineering",
        "date": "2025-09-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.168112"
    },
    {
        "index": "#308",
        "title": "RANGER -- Repository-Level Agent for Graph-Enhanced Retrieval",
        "link": "/arxiv/2509.25257",
        "arxiv_id": "2509.25257",
        "authors": "Pratik Shah, Rajat Ghosh, Aryan Singhal, Debojyoti Dutta",
        "summary": "General-purpose automated software engineering (ASE) includes tasks such as code completion, retrieval, repair, QA, and summarization. These tasks require a code retrieval system that can handle specific queries about code entities, or code entity queries (for example, locating a specific class or retrieving the dependencies of a function), as well as general queries without explicit code entities, or natural language queries (for example, describing a task and retrieving the corresponding code). We present RANGER, a repository-level code retrieval agent designed to address both query types, filling a gap in recent works that have focused primarily on code-entity queries. We first present a tool that constructs a comprehensive knowledge graph of the entire repository, capturing hierarchical and cross-file dependencies down to the variable level, and augments graph nodes with textual descriptions and embeddings to bridge the gap between code and natural language. RANGER then operates on this graph through a dual-stage retrieval pipeline. Entity-based queries are answered through fast Cypher lookups, while natural language queries are handled by MCTS-guided graph exploration. We evaluate RANGER across four diverse benchmarks that represent core ASE tasks including code search, question answering, cross-file dependency retrieval, and repository-level code completion. On CodeSearchNet and RepoQA it outperforms retrieval baselines that use embeddings from strong models such as Qwen3-8B. On RepoBench, it achieves superior cross-file dependency retrieval over baselines, and on CrossCodeEval, pairing RANGER with BM25 delivers the highest exact match rate in code completion compared to other RAG methods.",
        "subjects": "Software Engineering, Information Retrieval, Machine Learning",
        "date": "2025-09-27",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.169206"
    },
    {
        "index": "#309",
        "title": "Protocode: Prototype-Driven Interpretability for Code Generation in LLMs",
        "link": "/arxiv/2509.25247",
        "arxiv_id": "2509.25247",
        "authors": "Krishna Vamshi Bodla, Haizhao Yang",
        "summary": "Since the introduction of Large Language Models (LLMs), they have been widely adopted for various tasks such as text summarization, question answering, speech-to-text translation, and more. In recent times, the use of LLMs for code generation has gained significant attention, with tools such as Cursor and Windsurf demonstrating the ability to analyze massive code repositories and recommend relevant changes. Big tech companies have also acknowledged the growing reliance on LLMs for code generation within their codebases. Although these advances significantly improve developer productivity, increasing reliance on automated code generation can proportionally increase the risk of suboptimal solutions and insecure code. Our work focuses on automatically sampling In-Context Learning (ICL) demonstrations which can improve model performance and enhance the interpretability of the generated code. Using AST-based analysis on outputs from the MBPP test set, we identify regions of code most influenced by the chosen demonstrations. In our experiments, we show that high-quality ICL demonstrations not only make outputs easier to interpret but also yield a positive performance improvement on the pass@10 metric. Conversely, poorly chosen ICL demonstrations affected the LLM performance on the pass@10 metric negatively compared to the base model. Overall, our approach highlights the importance of efficient sampling strategies for ICL, which can affect the performance of the model on any given task.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning",
        "date": "2025-09-27",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.169681"
    },
    {
        "index": "#310",
        "title": "Comprehensive Analysis of VQC for Financial Fraud Detection: A Comparative Study of Quantum Encoding Techniques and Architectural Optimizations",
        "link": "/arxiv/2509.25245",
        "arxiv_id": "2509.25245",
        "authors": "Fouad Mohammed Abbou, Mohamed Bouhadda, Lamiae Bouanane, Mouna Kettani, Farid Abdi, Abdelouahab Abid",
        "summary": "This paper presents a systematic comparative analysis of Variational Quantum Classifier (VQC) configurations for financial fraud detection, encompassing three distinct quantum encoding techniques and comprehensive architectural variations. Through empirical evaluation across multiple entanglement patterns, circuit depths, and optimization strategies,quantum advantages in fraud classification accuracy are demonstrated, achieving up to 94.3 % accuracy with ZZ encoding schemes. The analysis reveals significant performance variations across entanglement topologies, with circular entanglement consistently outperforming linear (90.7) %) and full connectivity (92.0 %) patterns, achieving optimal performance at 93.3 % accuracy. The study introduces novel visualization methodologies for quantum circuit analysis and provides actionable deployment recommendations for practical quantum machine learning implementations. Notably, systematic entanglement pattern analysis shows that circular connectivity provides superior balance between expressivity and trainability while maintaining computational efficiency. These researches offer initial benchmarks for quantum enhanced fraud detection systems and propose potential benefits of quantum machine learning in financial security applications.",
        "subjects": "Quantum Physics, Artificial Intelligence, Machine Learning",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.170214"
    },
    {
        "index": "#312",
        "title": "The Causal Abstraction Network: Theory and Learning",
        "link": "/arxiv/2509.25236",
        "arxiv_id": "2509.25236",
        "authors": "Gabriele D'Acunto, Paolo Di Lorenzo, Sergio Barbarossa",
        "summary": "Causal artificial intelligence aims to enhance explainability, trustworthiness, and robustness in AI by leveraging structural causal models (SCMs). In this pursuit, recent advances formalize network sheaves of causal knowledge. Pushing in the same direction, we introduce the causal abstraction network (CAN), a specific instance of such sheaves where (i) SCMs are Gaussian, (ii) restriction maps are transposes of constructive linear causal abstractions (CAs), and (iii) edge stalks correspond -- up to rotation -- to the node stalks of more detailed SCMs. We investigate the theoretical properties of CAN, including algebraic invariants, cohomology, consistency, global sections characterized via the Laplacian kernel, and smoothness. We then tackle the learning of consistent CANs. Our problem formulation separates into edge-specific local Riemannian problems and avoids nonconvex, costly objectives. We propose an efficient search procedure as a solution, solving the local problems with SPECTRAL, our iterative method with closed-form updates and suitable for positive definite and semidefinite covariance matrices. Experiments on synthetic data show competitive performance in the CA learning task, and successful recovery of diverse CAN structures.",
        "subjects": "Artificial Intelligence, Machine Learning, Signal Processing",
        "date": "2025-09-25",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.171214"
    },
    {
        "index": "#314",
        "title": "APRIL: API Synthesis with Automatic Prompt Optimization and Reinforcement Learning",
        "link": "/arxiv/2509.25196",
        "arxiv_id": "2509.25196",
        "authors": "Hua Zhong, Shan Jiang, Sarfraz Khurshid",
        "summary": "APIs are central to modern software development, yet composing new APIs from large libraries is difficult due to the exponential search space; traditional component-based synthesis relies on costly exploration and hand-crafted specifications. While large language models (LLMs) can generate implementations from natural language, hallucinations and limited access to up-to-date contextual information often yield incorrect code. In this paper, we present APRIL, an approach that combines LLM-based synthesis with Automatic Prompt Optimization (APO) and Reinforcement Learning from Verifiable Rewards (RLVR): APO iteratively refines prompts for a frozen model, while RLVR fine-tunes the policy toward functional correctness, producing an efficient synthesis pipeline. Evaluated on 81 real-world APIs from widely used scientific Python libraries and benchmarked against instruction-tuned but unfine-tuned LLMs guided by expert prompts, APRIL achieves substantial improvements. These results indicate that integrating APO and RLVR provides a robust, scalable path for component-based API synthesis in large libraries.",
        "subjects": "Software Engineering, Artificial Intelligence, Machine Learning, Programming Languages",
        "date": "2025-08-29",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.172204"
    },
    {
        "index": "#315",
        "title": "Understanding Practitioners Perspectives on Monitoring Machine Learning Systems",
        "link": "/arxiv/2509.25195",
        "arxiv_id": "2509.25195",
        "authors": "Hira Naveed, John Grundy, Chetan Arora, Hourieh Khalajzadeh, Omar Haggag",
        "summary": "Given the inherent non-deterministic nature of machine learning (ML) systems, their behavior in production environments can lead to unforeseen and potentially dangerous outcomes. For a timely detection of unwanted behavior and to prevent organizations from financial and reputational damage, monitoring these systems is essential. This paper explores the strategies, challenges, and improvement opportunities for monitoring ML systems from the practitioners perspective. We conducted a global survey of 91 ML practitioners to collect diverse insights into current monitoring practices for ML systems. We aim to complement existing research through our qualitative and quantitative analyses, focusing on prevalent runtime issues, industrial monitoring and mitigation practices, key challenges, and desired enhancements in future monitoring tools. Our findings reveal that practitioners frequently struggle with runtime issues related to declining model performance, exceeding latency, and security violations. While most prefer automated monitoring for its increased efficiency, many still rely on manual approaches due to the complexity or lack of appropriate automation solutions. Practitioners report that the initial setup and configuration of monitoring tools is often complicated and challenging, particularly when integrating with ML systems and setting alert thresholds. Moreover, practitioners find that monitoring adds extra workload, strains resources, and causes alert fatigue. The desired improvements from the practitioners perspective are: automated generation and deployment of monitors, improved support for performance and fairness monitoring, and recommendations for resolving runtime issues. These insights offer valuable guidance for the future development of ML monitoring tools that are better aligned with practitioners needs.",
        "subjects": "Software Engineering, Machine Learning",
        "date": "2025-08-20",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.177840"
    },
    {
        "index": "#316",
        "title": "UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning",
        "link": "/arxiv/2509.22628",
        "arxiv_id": "2509.22628",
        "authors": "Hongyu Chen, Guangrun Wang",
        "summary": "Chain-of-Thought (CoT) prompting improves reasoning in large language models (LLMs), but its reliance on unstructured text limits interpretability and executability in embodied tasks. Prior work has explored structured CoTs using scene or logic graphs, yet these remain fundamentally limited: they model only low-order relations, lack constructs like inheritance or behavioral abstraction, and provide no standardized semantics for sequential or conditional planning. We propose UML-CoT, a structured reasoning and planning framework that leverages Unified Modeling Language (UML) to generate symbolic CoTs and executable action plans. UML class diagrams capture compositional object semantics, while activity diagrams model procedural control flow. Our three-stage training pipeline combines supervised fine-tuning with Group Relative Policy Optimization (GRPO), including reward learning from answer-only data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in interpretability, planning coherence, and execution success, highlighting UML as a more expressive and actionable structured reasoning formalism.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2025-09-26",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.178286"
    },
    {
        "index": "#317",
        "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and High-Quality Language Model Serving",
        "link": "/arxiv/2509.00105",
        "arxiv_id": "2509.00105",
        "authors": "Shaoting Feng, Hanchen Li, Kuntai Du, Zhuohan Gu, Yuhan Liu, Jiayi Yao, Siddhant Ray, Samuel Shen, Yihua Cheng, Ganesh Ananthanarayanan, Junchen Jiang",
        "summary": "Large language model (LLM) applications often reuse previously processed context, such as chat history and documents, which introduces significant redundant computation. Existing LLM serving systems address such redundant computation by storing the KV caches of processed context and loading the corresponding KV cache when a new request reuses the context. Further, as these LLM applications scale, the total size of KV caches becomes excessively large and requires both DRAM and SSD for full storage. However, prior work that stores KV caches in DRAM and SSD suffers from high loading delays, as most KV cache hits come from SSD, which is slow to load. To increase the KV cache hit rate on DRAM, we identify lossy KV cache compression as a promising approach. We design a lossy compression system that decides the compression algorithm, compression rate and device placement for each KV cache entry to maximise DRAM hits and minimise loading delay without significantly degrading generation quality. Compared to various static compression baselines across three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at the same quality and 6--55% quality improvements at the same delay.",
        "subjects": "Operating Systems",
        "date": "2025-08-28",
        "category": "cs.LG",
        "crawl_time": "2025-10-06T22:50:20.178878"
    }
]