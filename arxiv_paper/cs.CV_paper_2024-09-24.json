[
    {
        "index": "#1",
        "title": "PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions",
        "link": "/arxiv/2409.15278",
        "arxiv_id": "2409.15278",
        "authors": "Weifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shitian Zhao, Siyuan Huang, Junlin Xie, Yu Qiao, Peng Gao, Hongsheng Li",
        "summary": "This paper presents a versatile image-to-image visual assistant, PixWizard, designed for image generation, manipulation, and translation based on free-from language instructions. To this end, we tackle a variety of vision tasks into a unified image-text-to-image generation framework and curate an Omni Pixel-to-Pixel Instruction-Tuning Dataset. By constructing detailed instruction templates in natural language, we comprehensively include a large set of diverse vision tasks such as text-to-image generation, image restoration, image grounding, dense image prediction, image editing, controllable generation, inpainting/outpainting, and more. Furthermore, we adopt Diffusion Transformers (DiT) as our foundation model and extend its capabilities with a flexible any resolution mechanism, enabling the model to dynamically process images based on the aspect ratio of the input, closely aligning with human perceptual processes. The model also incorporates structure-aware and semantic-aware guidance to facilitate effective fusion of information from the input image. Our experiments demonstrate that PixWizard not only shows impressive generative and understanding abilities for images with diverse resolutions but also exhibits promising generalization capabilities with unseen tasks and human instructions. The code and related resources are available at https://github.com/AFeng-x/PixWizard",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 17:59:46 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.975202"
    },
    {
        "index": "#2",
        "title": "MaterialFusion: Enhancing Inverse Rendering with Material Diffusion Priors",
        "link": "/arxiv/2409.15273",
        "arxiv_id": "2409.15273",
        "authors": "Yehonathan Litman, Or Patashnik, Kangle Deng, Aviral Agrawal, Rushikesh Zawar, Fernando De la Torre, Shubham Tulsiani",
        "summary": "Recent works in inverse rendering have shown promise in using multi-view images of an object to recover shape, albedo, and materials. However, the recovered components often fail to render accurately under new lighting conditions due to the intrinsic challenge of disentangling albedo and material properties from input images. To address this challenge, we introduce MaterialFusion, an enhanced conventional 3D inverse rendering pipeline that incorporates a 2D prior on texture and material properties. We present StableMaterial, a 2D diffusion model prior that refines multi-lit data to estimate the most likely albedo and material from given input appearances. This model is trained on albedo, material, and relit image data derived from a curated dataset of approximately ~12K artist-designed synthetic Blender objects called BlenderVault. we incorporate this diffusion prior with an inverse rendering framework where we use score distillation sampling (SDS) to guide the optimization of the albedo and materials, improving relighting performance in comparison with previous work. We validate MaterialFusion's relighting performance on 4 datasets of synthetic and real objects under diverse illumination conditions, showing our diffusion-aided approach significantly improves the appearance of reconstructed objects under novel lighting conditions. We intend to publicly release our BlenderVault dataset to support further research in this field.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 17:59:06 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.975454"
    },
    {
        "index": "#3",
        "title": "ReLoo: Reconstructing Humans Dressed in Loose Garments from Monocular Video in the Wild",
        "link": "/arxiv/2409.15269",
        "arxiv_id": "2409.15269",
        "authors": "Chen Guo, Tianjian Jiang, Manuel Kaufmann, Chengwei Zheng, Julien Valentin, Jie Song, Otmar Hilliges",
        "summary": "While previous years have seen great progress in the 3D reconstruction of humans from monocular videos, few of the state-of-the-art methods are able to handle loose garments that exhibit large non-rigid surface deformations during articulation. This limits the application of such methods to humans that are dressed in standard pants or T-shirts. Our method, ReLoo, overcomes this limitation and reconstructs high-quality 3D models of humans dressed in loose garments from monocular in-the-wild videos. To tackle this problem, we first establish a layered neural human representation that decomposes clothed humans into a neural inner body and outer clothing. On top of the layered neural representation, we further introduce a non-hierarchical virtual bone deformation module for the clothing layer that can freely move, which allows the accurate recovery of non-rigidly deforming loose clothing. A global optimization jointly optimizes the shape, appearance, and deformations of the human body and clothing via multi-layer differentiable volume rendering. To evaluate ReLoo, we record subjects with dynamically deforming garments in a multi-view capture studio. This evaluation, both on existing and our novel dataset, demonstrates ReLoo's clear superiority over prior art on both indoor datasets and in-the-wild videos.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 17:58:39 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.975655"
    },
    {
        "index": "#4",
        "title": "S$^2$AG-Vid: Enhancing Multi-Motion Alignment in Video Diffusion Models via Spatial and Syntactic Attention-Based Guidance",
        "link": "/arxiv/2409.15259",
        "arxiv_id": "2409.15259",
        "authors": "Yuanhang Li, Qi Mao, Lan Chen, Zhen Fang, Lei Tian, Xinyan Xiao, Libiao Jin, Hua Wu",
        "summary": "Recent advancements in text-to-video (T2V) generation using diffusion models have garnered significant attention. However, existing T2V models primarily focus on simple scenes featuring a single object performing a single motion. Challenges arise in scenarios involving multiple objects with distinct motions, often leading to incorrect video-text alignment between subjects and their corresponding motions. To address this challenge, we propose \\textbf{S$^2$AG-Vid}, a training-free inference-stage optimization method that improves the alignment of multiple objects with their corresponding motions in T2V models. S$^2$AG-Vid initially applies a spatial position-based, cross-attention (CA) constraint in the early stages of the denoising process, facilitating multiple nouns distinctly attending to the correct subject regions. To enhance the motion-subject binding, we implement a syntax-guided contrastive constraint in the subsequent denoising phase, aimed at improving the correlations between the CA maps of verbs and their corresponding nouns.Both qualitative and quantitative evaluations demonstrate that the proposed framework significantly outperforms baseline approaches, producing higher-quality videos with improved subject-motion consistency.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-23 17:56:03 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.975867"
    },
    {
        "index": "#5",
        "title": "ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models",
        "link": "/arxiv/2409.15250",
        "arxiv_id": "2409.15250",
        "authors": "Sombit Dey, Jan-Nico Zaech, Nikolay Nikolov, Luc Van Gool, Danda Pani Paudel",
        "summary": "Recent progress in large language models and access to large-scale robotic datasets has sparked a paradigm shift in robotics models transforming them into generalists able to adapt to various tasks, scenes, and robot modalities. A large step for the community are open Vision Language Action models which showcase strong performance in a wide variety of tasks. In this work, we study the visual generalization capabilities of three existing robotic foundation models, and propose a corresponding evaluation framework. Our study shows that the existing models do not exhibit robustness to visual out-of-domain scenarios. This is potentially caused by limited variations in the training data and/or catastrophic forgetting, leading to domain limitations in the vision foundation models. We further explore OpenVLA, which uses two pre-trained vision foundation models and is, therefore, expected to generalize to out-of-domain experiments. However, we showcase catastrophic forgetting by DINO-v2 in OpenVLA through its failure to fulfill the task of depth regression. To overcome the aforementioned issue of visual catastrophic forgetting, we propose a gradual backbone reversal approach founded on model merging. This enables OpenVLA which requires the adaptation of the visual backbones during initial training -- to regain its visual generalization ability. Regaining this capability enables our ReVLA model to improve over OpenVLA by a factor of 77% and 66% for grasping and lifting in visual OOD tasks .",
        "subjects": "Computer Vision and Pattern Recognition, Robotics",
        "date": "2024-09-23 17:47:59 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.976063"
    },
    {
        "index": "#6",
        "title": "Enhancing Pedestrian Trajectory Prediction with Crowd Trip Information",
        "link": "/arxiv/2409.15224",
        "arxiv_id": "2409.15224",
        "authors": "Rei Tamaru, Pei Li, Bin Ran",
        "summary": "Pedestrian trajectory prediction is essential for various applications in active traffic management, urban planning, traffic control, crowd management, and autonomous driving, aiming to enhance traffic safety and efficiency. Accurately predicting pedestrian trajectories requires a deep understanding of individual behaviors, social interactions, and road environments. Existing studies have developed various models to capture the influence of social interactions and road conditions on pedestrian trajectories. However, these approaches are limited by the lack of a comprehensive view of social interactions and road environments. To address these limitations and enhance the accuracy of pedestrian trajectory prediction, we propose a novel approach incorporating trip information as a new modality into pedestrian trajectory models. We propose RNTransformer, a generic model that utilizes crowd trip information to capture global information on social interactions. We incorporated RNTransformer with various socially aware local pedestrian trajectory prediction models to demonstrate its performance. Specifically, by leveraging a pre-trained RNTransformer when training different pedestrian trajectory prediction models, we observed improvements in performance metrics: a 1.3/2.2% enhancement in ADE/FDE on Social-LSTM, a 6.5/28.4% improvement on Social-STGCNN, and an 8.6/4.3% improvement on S-Implicit. Evaluation results demonstrate that RNTransformer significantly enhances the accuracy of various pedestrian trajectory prediction models across multiple datasets. Further investigation reveals that the RNTransformer effectively guides local models to more accurate directions due to the consideration of global information. By exploring crowd behavior within the road network, our approach shows great promise in improving pedestrian safety through accurate trajectory predictions.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 17:11:31 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.976246"
    },
    {
        "index": "#7",
        "title": "HydroVision: LiDAR-Guided Hydrometric Prediction with Vision Transformers and Hybrid Graph Learning",
        "link": "/arxiv/2409.15213",
        "arxiv_id": "2409.15213",
        "authors": "Naghmeh Shafiee Roudbari, Ursula Eicker, Charalambos Poullis, Zachary Patterson",
        "summary": "Hydrometric forecasting is crucial for managing water resources, flood prediction, and environmental protection. Water stations are interconnected, and this connectivity influences the measurements at other stations. However, the dynamic and implicit nature of water flow paths makes it challenging to extract a priori knowledge of the connectivity structure. We hypothesize that terrain elevation significantly affects flow and connectivity. To incorporate this, we use LiDAR terrain elevation data encoded through a Vision Transformer (ViT). The ViT, which has demonstrated excellent performance in image classification by directly applying transformers to sequences of image patches, efficiently captures spatial features of terrain elevation. To account for both spatial and temporal features, we employ GRU blocks enhanced with graph convolution, a method widely used in the literature. We propose a hybrid graph learning structure that combines static and dynamic graph learning. A static graph, derived from transformer-encoded LiDAR data, captures terrain elevation relationships, while a dynamic graph adapts to temporal changes, improving the overall graph representation. We apply graph convolution in two layers through these static and dynamic graphs. Our method makes daily predictions up to 12 days ahead. Empirical results from multiple water stations in Quebec demonstrate that our method significantly reduces prediction error by an average of 10\\% across all days, with greater improvements for longer forecasting horizons.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-23 16:57:43 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.976466"
    },
    {
        "index": "#8",
        "title": "HOTVCOM: Generating Buzzworthy Comments for Videos",
        "link": "/arxiv/2409.15196",
        "arxiv_id": "2409.15196",
        "authors": "Yuyan Chen, Yiwen Qian, Songzhou Yan, Jiyuan Jia, Zhixu Li, Yanghua Xiao, Xiaobo Li, Ming Yang, Qingpei Guo",
        "summary": "In the era of social media video platforms, popular ``hot-comments'' play a crucial role in attracting user impressions of short-form videos, making them vital for marketing and branding purpose. However, existing research predominantly focuses on generating descriptive comments or ``danmaku'' in English, offering immediate reactions to specific video moments. Addressing this gap, our study introduces \\textsc{HotVCom}, the largest Chinese video hot-comment dataset, comprising 94k diverse videos and 137 million comments. We also present the \\texttt{ComHeat} framework, which synergistically integrates visual, auditory, and textual data to generate influential hot-comments on the Chinese video dataset. Empirical evaluations highlight the effectiveness of our framework, demonstrating its excellence on both the newly constructed and existing datasets.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-23 16:45:13 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.976738"
    },
    {
        "index": "#9",
        "title": "Interpretability-Guided Test-Time Adversarial Defense",
        "link": "/arxiv/2409.15190",
        "arxiv_id": "2409.15190",
        "authors": "Akshay Kulkarni, Tsui-Wei Weng",
        "summary": "We propose a novel and low-cost test-time adversarial defense by devising interpretability-guided neuron importance ranking methods to identify neurons important to the output classes. Our method is a training-free approach that can significantly improve the robustness-accuracy tradeoff while incurring minimal computational overhead. While being among the most efficient test-time defenses (4x faster), our method is also robust to a wide range of black-box, white-box, and adaptive attacks that break previous test-time defenses. We demonstrate the efficacy of our method for CIFAR10, CIFAR100, and ImageNet-1k on the standard RobustBench benchmark (with average gains of 2.6%, 4.9%, and 2.8% respectively). We also show improvements (average 1.5%) over the state-of-the-art test-time defenses even under strong adaptive attacks.",
        "subjects": "Computer Vision and Pattern Recognition, Cryptography and Security, Machine Learning",
        "date": "2024-09-23 16:40:10 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.976928"
    },
    {
        "index": "#10",
        "title": "MIMAFace: Face Animation via Motion-Identity Modulated Appearance Feature Learning",
        "link": "/arxiv/2409.15179",
        "arxiv_id": "2409.15179",
        "authors": "Yue Han, Junwei Zhu, Yuxiang Feng, Xiaozhong Ji, Keke He, Xiangtai Li, zhucun xue, Yong Liu",
        "summary": "Current diffusion-based face animation methods generally adopt a ReferenceNet (a copy of U-Net) and a large amount of curated self-acquired data to learn appearance features, as robust appearance features are vital for ensuring temporal stability. However, when trained on public datasets, the results often exhibit a noticeable performance gap in image quality and temporal consistency. To address this issue, we meticulously examine the essential appearance features in the facial animation tasks, which include motion-agnostic (e.g., clothing, background) and motion-related (e.g., facial details) texture components, along with high-level discriminative identity features. Drawing from this analysis, we introduce a Motion-Identity Modulated Appearance Learning Module (MIA) that modulates CLIP features at both motion and identity levels. Additionally, to tackle the semantic/ color discontinuities between clips, we design an Inter-clip Affinity Learning Module (ICA) to model temporal relationships across clips. Our method achieves precise facial motion control (i.e., expressions and gaze), faithful identity preservation, and generates animation videos that maintain both intra/inter-clip temporal consistency. Moreover, it easily adapts to various modalities of driving sources. Extensive experiments demonstrate the superiority of our method.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 16:33:53 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.977128"
    },
    {
        "index": "#11",
        "title": "SpikeGS: Learning 3D Gaussian Fields from Continuous Spike Stream",
        "link": "/arxiv/2409.15176",
        "arxiv_id": "2409.15176",
        "authors": "Jinze Yu, Xi Peng, Zhengda Lu, Laurent Kneip, Yiqun Wang",
        "summary": "A spike camera is a specialized high-speed visual sensor that offers advantages such as high temporal resolution and high dynamic range compared to conventional frame cameras. These features provide the camera with significant advantages in many computer vision tasks. However, the tasks of 3D reconstruction and novel view synthesis based on spike cameras remain underdeveloped. Although there are existing methods for learning neural radiance fields from spike stream, they either lack robustness in extremely noisy, low-quality lighting conditions or suffer from high computational complexity due to the deep fully connected neural networks and ray marching rendering strategies used in neural radiance fields, making it difficult to recover fine texture details. In contrast, the latest advancements in 3DGS have achieved high-quality real-time rendering by optimizing the point cloud representation into Gaussian ellipsoids. Building on this, we introduce SpikeGS, the first method to learn 3D Gaussian fields solely from spike stream. We designed a differentiable spike stream rendering framework based on 3DGS, incorporating noise embedding and spiking neurons. By leveraging the multi-view consistency of 3DGS and the tile-based multi-threaded parallel rendering mechanism, we achieved high-quality real-time rendering results. Additionally, we introduced a spike rendering loss function that generalizes under varying illumination conditions. Our method can reconstruct view synthesis results with fine texture details from a continuous spike stream captured by a moving spike camera, while demonstrating high robustness in extremely noisy low-light scenarios. Experimental results on both real and synthetic datasets demonstrate that our method surpasses existing approaches in terms of rendering quality and speed. Our code will be available at https://github.com/520jz/SpikeGS.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 16:28:41 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.977307"
    },
    {
        "index": "#12",
        "title": "FusionRF: High-Fidelity Satellite Neural Radiance Fields from Multispectral and Panchromatic Acquisitions",
        "link": "/arxiv/2409.15132",
        "arxiv_id": "2409.15132",
        "authors": "Michael Sprintson, Rama Chellappa, Cheng Peng",
        "summary": "We introduce FusionRF, a novel neural rendering terrain reconstruction method from optically unprocessed satellite imagery. While previous methods depend on external pansharpening methods to fuse low resolution multispectral imagery and high resolution panchromatic imagery, FusionRF directly performs reconstruction based on optically unprocessed acquisitions with no prior knowledge. This is accomplished through the addition of a sharpening kernel which models the resolution loss in multispectral images. Additionally, novel modal embeddings allow the model to perform image fusion as a bottleneck to novel view synthesis. We evaluate our method on multispectral and panchromatic satellite images from the WorldView-3 satellite in various locations, and FusionRF outperforms previous State-of-The-Art methods in depth reconstruction on unprocessed imagery, renders sharp training and novel views, and retains multi-spectral information.",
        "subjects": "Computer Vision and Pattern Recognition, Image and Video Processing",
        "date": "2024-09-23 15:38:03 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.977483"
    },
    {
        "index": "#13",
        "title": "Detect, Describe, Discriminate: Moving Beyond VQA for MLLM Evaluation",
        "link": "/arxiv/2409.15125",
        "arxiv_id": "2409.15125",
        "authors": "Manu Gaur, Darshan Singh S, Makarand Tapaswi",
        "summary": "Visual Question Answering (VQA) with multiple choice questions enables a vision-centric evaluation of Multimodal Large Language Models (MLLMs). Although it reliably checks the existence of specific visual abilities, it is easier for the model to select an answer from multiple choices (VQA evaluation) than to generate the answer itself. In this work, we offer a novel perspective: we evaluate how well an MLLM understands a specific visual concept by its ability to uniquely describe two extremely similar images that differ only in the targeted visual concept. Specifically, we assess the ability of MLLMs to capture specific points of visual differences using self-retrieval, i.e., by retrieving the target image using its generated caption against the other image in the pair serving as the distractor. We curate 247 highly similar image pairs as part of the D3 benchmark. For each image pair, the model is prompted to: (1) Detect a specific visual difference, and (2) Describe the target image uniquely such that it (3) Discriminates the target image from the distractor. Self-retrieval within D3 enables whitebox evaluation across six different visual patterns, revealing that current models struggle to independently discern fine-grained visual differences, with open-source models failing to outperform random guess.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 15:31:25 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.977650"
    },
    {
        "index": "#14",
        "title": "Diffusion-based RGB-D Semantic Segmentation with Deformable Attention Transformer",
        "link": "/arxiv/2409.15117",
        "arxiv_id": "2409.15117",
        "authors": "Minh Bui, Kostas Alexis",
        "summary": "Vision-based perception and reasoning is essential for scene understanding in any autonomous system. RGB and depth images are commonly used to capture both the semantic and geometric features of the environment. Developing methods to reliably interpret this data is critical for real-world applications, where noisy measurements are often unavoidable. In this work, we introduce a diffusion-based framework to address the RGB-D semantic segmentation problem. Additionally, we demonstrate that utilizing a Deformable Attention Transformer as the encoder to extract features from depth images effectively captures the characteristics of invalid regions in depth measurements. Our generative framework shows a greater capacity to model the underlying distribution of RGB-D images, achieving robust performance in challenging scenarios with significantly less training time compared to discriminative methods. Experimental results indicate that our approach achieves State-of-the-Art performance on both the NYUv2 and SUN-RGBD datasets in general and especially in the most challenging of their image data. Our project page will be available at https://diffusionmms.github.io/",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 15:23:01 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.977843"
    },
    {
        "index": "#15",
        "title": "The BRAVO Semantic Segmentation Challenge Results in UNCV2024",
        "link": "/arxiv/2409.15107",
        "arxiv_id": "2409.15107",
        "authors": "Tuan-Hung Vu, Eduardo Valle, Andrei Bursuc, Tommie Kerssies, Daan de Geus, Gijs Dubbelman, Long Qian, Bingke Zhu, Yingying Chen, Ming Tang, Jinqiao Wang, Tomáš Vojíř, Jan Šochman, Jiří Matas, Michael Smith, Frank Ferrie, Shamik Basu, Christos Sakaridis, Luc Van Gool",
        "summary": "We propose the unified BRAVO challenge to benchmark the reliability of semantic segmentation models under realistic perturbations and unknown out-of-distribution (OOD) scenarios. We define two categories of reliability: (1) semantic reliability, which reflects the model's accuracy and calibration when exposed to various perturbations; and (2) OOD reliability, which measures the model's ability to detect object classes that are unknown during training. The challenge attracted nearly 100 submissions from international teams representing notable research institutions. The results reveal interesting insights into the importance of large-scale pre-training and minimal architectural design in developing robust and reliable semantic segmentation models.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 15:17:30 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.978161"
    },
    {
        "index": "#16",
        "title": "M2OST: Many-to-one Regression for Predicting Spatial Transcriptomics from Digital Pathology Images",
        "link": "/arxiv/2409.15092",
        "arxiv_id": "2409.15092",
        "authors": "Hongyi Wang, Xiuju Du, Jing Liu, Shuyi Ouyang, Yen-Wei Chen, Lanfen Lin",
        "summary": "The advancement of Spatial Transcriptomics (ST) has facilitated the spatially-aware profiling of gene expressions based on histopathology images. Although ST data offers valuable insights into the micro-environment of tumors, its acquisition cost remains expensive. Therefore, directly predicting the ST expressions from digital pathology images is desired. Current methods usually adopt existing regression backbones along with patch-sampling for this task, which ignores the inherent multi-scale information embedded in the pyramidal data structure of digital pathology images, and wastes the inter-spot visual information crucial for accurate gene expression prediction. To address these limitations, we propose M2OST, a many-to-one regression Transformer that can accommodate the hierarchical structure of the pathology images via a decoupled multi-scale feature extractor. Unlike traditional models that are trained with one-to-one image-label pairs, M2OST uses multiple images from different levels of the digital pathology image to jointly predict the gene expressions in their common corresponding spot. Built upon our many-to-one scheme, M2OST can be easily scaled to fit different numbers of inputs, and its network structure inherently incorporates nearby inter-spot features, enhancing regression performance. We have tested M2OST on three public ST datasets and the experimental results show that M2OST can achieve state-of-the-art performance with fewer parameters and floating-point operations (FLOPs). The code will be released upon acceptance.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-23 15:06:37 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.978359"
    },
    {
        "index": "#17",
        "title": "TSCLIP: Robust CLIP Fine-Tuning for Worldwide Cross-Regional Traffic Sign Recognition",
        "link": "/arxiv/2409.15077",
        "arxiv_id": "2409.15077",
        "authors": "Guoyang Zhao, Fulong Ma, Weiqing Qi, Chenguang Zhang, Yuxuan Liu, Ming Liu, Jun Ma",
        "summary": "Traffic sign is a critical map feature for navigation and traffic control. Nevertheless, current methods for traffic sign recognition rely on traditional deep learning models, which typically suffer from significant performance degradation considering the variations in data distribution across different regions. In this paper, we propose TSCLIP, a robust fine-tuning approach with the contrastive language-image pre-training (CLIP) model for worldwide cross-regional traffic sign recognition. We first curate a cross-regional traffic sign benchmark dataset by combining data from ten different sources. Then, we propose a prompt engineering scheme tailored to the characteristics of traffic signs, which involves specific scene descriptions and corresponding rules to generate targeted text descriptions for optimizing the model training process. During the TSCLIP fine-tuning process, we implement adaptive dynamic weight ensembling (ADWE) to seamlessly incorporate outcomes from each training iteration with the zero-shot CLIP model. This approach ensures that the model retains its ability to generalize while acquiring new knowledge about traffic signs. Our method surpasses conventional classification benchmark models in cross-regional traffic sign evaluations, and it achieves state-of-the-art performance compared to existing CLIP fine-tuning techniques. To the best knowledge of authors, TSCLIP is the first contrastive language-image model used for the worldwide cross-regional traffic sign recognition task. The project website is available at: https://github.com/guoyangzhao/TSCLIP.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 14:51:26 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.978577"
    },
    {
        "index": "#18",
        "title": "FisheyeDepth: A Real Scale Self-Supervised Depth Estimation Model for Fisheye Camera",
        "link": "/arxiv/2409.15054",
        "arxiv_id": "2409.15054",
        "authors": "Guoyang Zhao, Yuxuan Liu, Weiqing Qi, Fulong Ma, Ming Liu, Jun Ma",
        "summary": "Accurate depth estimation is crucial for 3D scene comprehension in robotics and autonomous vehicles. Fisheye cameras, known for their wide field of view, have inherent geometric benefits. However, their use in depth estimation is restricted by a scarcity of ground truth data and image distortions. We present FisheyeDepth, a self-supervised depth estimation model tailored for fisheye cameras. We incorporate a fisheye camera model into the projection and reprojection stages during training to handle image distortions, thereby improving depth estimation accuracy and training stability. Furthermore, we incorporate real-scale pose information into the geometric projection between consecutive frames, replacing the poses estimated by the conventional pose network. Essentially, this method offers the necessary physical depth for robotic tasks, and also streamlines the training and inference procedures. Additionally, we devise a multi-channel output strategy to improve robustness by adaptively fusing features at various scales, which reduces the noise from real pose data. We demonstrate the superior performance and robustness of our model in fisheye image depth estimation through evaluations on public datasets and real-world scenarios. The project website is available at: https://github.com/guoyangzhao/FisheyeDepth.",
        "subjects": "Computer Vision and Pattern Recognition, Robotics",
        "date": "2024-09-23 14:31:42 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.978812"
    },
    {
        "index": "#19",
        "title": "AIM 2024 Sparse Neural Rendering Challenge: Methods and Results",
        "link": "/arxiv/2409.15045",
        "arxiv_id": "2409.15045",
        "authors": "Michal Nazarczuk, Sibi Catley-Chandar, Thomas Tanay, Richard Shaw, Eduardo Pérez-Pellitero, Radu Timofte, Xing Yan, Pan Wang, Yali Guo, Yongxin Wu, Youcheng Cai, Yanan Yang, Junting Li, Yanghong Zhou, P. Y. Mok, Zongqi He, Zhe Xiao, Kin-Chung Chan, Hana Lebeta Goshu, Cuixin Yang, Rongkang Dong, Jun Xiao, Kin-Man Lam, Jiayao Hao, Qiong Gao, Yanyan Zu, Junpei Zhang, Licheng Jiao, Xu Liu, Kuldeep Purohit",
        "summary": "This paper reviews the challenge on Sparse Neural Rendering that was part of the Advances in Image Manipulation (AIM) workshop, held in conjunction with ECCV 2024. This manuscript focuses on the competition set-up, the proposed methods and their respective results. The challenge aims at producing novel camera view synthesis of diverse scenes from sparse image observations. It is composed of two tracks, with differing levels of sparsity; 3 views in Track 1 (very sparse) and 9 views in Track 2 (sparse). Participants are asked to optimise objective fidelity to the ground-truth images as measured via the Peak Signal-to-Noise Ratio (PSNR) metric. For both tracks, we use the newly introduced Sparse Rendering (SpaRe) dataset and the popular DTU MVS dataset. In this challenge, 5 teams submitted final results to Track 1 and 4 teams submitted final results to Track 2. The submitted models are varied and push the boundaries of the current state-of-the-art in sparse neural rendering. A detailed description of all models developed in the challenge is provided in this paper.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 14:17:40 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.979163"
    },
    {
        "index": "#20",
        "title": "AIM 2024 Sparse Neural Rendering Challenge: Dataset and Benchmark",
        "link": "/arxiv/2409.15041",
        "arxiv_id": "2409.15041",
        "authors": "Michal Nazarczuk, Thomas Tanay, Sibi Catley-Chandar, Richard Shaw, Radu Timofte, Eduardo Pérez-Pellitero",
        "summary": "Recent developments in differentiable and neural rendering have made impressive breakthroughs in a variety of 2D and 3D tasks, e.g. novel view synthesis, 3D reconstruction. Typically, differentiable rendering relies on a dense viewpoint coverage of the scene, such that the geometry can be disambiguated from appearance observations alone. Several challenges arise when only a few input views are available, often referred to as sparse or few-shot neural rendering. As this is an underconstrained problem, most existing approaches introduce the use of regularisation, together with a diversity of learnt and hand-crafted priors. A recurring problem in sparse rendering literature is the lack of an homogeneous, up-to-date, dataset and evaluation protocol. While high-resolution datasets are standard in dense reconstruction literature, sparse rendering methods often evaluate with low-resolution images. Additionally, data splits are inconsistent across different manuscripts, and testing ground-truth images are often publicly available, which may lead to over-fitting. In this work, we propose the Sparse Rendering (SpaRe) dataset and benchmark. We introduce a new dataset that follows the setup of the DTU MVS dataset. The dataset is composed of 97 new scenes based on synthetic, high-quality assets. Each scene has up to 64 camera views and 7 lighting configurations, rendered at 1600x1200 resolution. We release a training split of 82 scenes to foster generalizable approaches, and provide an online evaluation platform for the validation and test sets, whose ground-truth images remain hidden. We propose two different sparse configurations (3 and 9 input images respectively). This provides a powerful and convenient tool for reproducible evaluation, and enable researchers easy access to a public leaderboard with the state-of-the-art performance scores. Available at: https://sparebenchmark.github.io/",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 14:10:06 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.979358"
    },
    {
        "index": "#21",
        "title": "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP",
        "link": "/arxiv/2409.15035",
        "arxiv_id": "2409.15035",
        "authors": "Zeliang Zhang, Zhuo Liu, Mingqian Feng, Chenliang Xu",
        "summary": "CLIP has demonstrated great versatility in adapting to various downstream tasks, such as image editing and generation, visual question answering, and video understanding. However, CLIP-based applications often suffer from misunderstandings regarding user intent, leading to discrepancies between the required number of objects and the actual outputs in image generation tasks. In this work, we empirically investigate the quantity bias in CLIP. By carefully designing different experimental settings and datasets, we comprehensively evaluate CLIP's understanding of quantity from text, image, and cross-modal perspectives. Our experimental results reveal a quantity bias in CLIP embeddings, impacting the reliability of downstream tasks.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language",
        "date": "2024-09-23 14:01:16 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.979545"
    },
    {
        "index": "#22",
        "title": "Region Mixup",
        "link": "/arxiv/2409.15028",
        "arxiv_id": "2409.15028",
        "authors": "Saptarshi Saha, Utpal Garain",
        "summary": "This paper introduces a simple extension of mixup (Zhang et al., 2018) data augmentation to enhance generalization in visual recognition tasks. Unlike the vanilla mixup method, which blends entire images, our approach focuses on combining regions from multiple images.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 13:55:16 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.979722"
    },
    {
        "index": "#23",
        "title": "Cross Branch Feature Fusion Decoder for Consistency Regularization-based Semi-Supervised Change Detection",
        "link": "/arxiv/2409.15021",
        "arxiv_id": "2409.15021",
        "authors": "Yan Xing, Qi'ao Xu, Jingcheng Zeng, Rui Huang, Sihua Gao, Weifeng Xu, Yuxiang Zhang, Wei Fan",
        "summary": "Semi-supervised change detection (SSCD) utilizes partially labeled data and a large amount of unlabeled data to detect changes. However, the transformer-based SSCD network does not perform as well as the convolution-based SSCD network due to the lack of labeled data. To overcome this limitation, we introduce a new decoder called Cross Branch Feature Fusion CBFF, which combines the strengths of both local convolutional branch and global transformer branch. The convolutional branch is easy to learn and can produce high-quality features with a small amount of labeled data. The transformer branch, on the other hand, can extract global context features but is hard to learn without a lot of labeled data. Using CBFF, we build our SSCD model based on a strong-to-weak consistency strategy. Through comprehensive experiments on WHU-CD and LEVIR-CD datasets, we have demonstrated the superiority of our method over seven state-of-the-art SSCD methods.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 13:47:59 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.979920"
    },
    {
        "index": "#24",
        "title": "DepthART: Monocular Depth Estimation as Autoregressive Refinement Task",
        "link": "/arxiv/2409.15010",
        "arxiv_id": "2409.15010",
        "authors": "Bulat Gabdullin, Nina Konovalova, Nikolay Patakin, Dmitry Senushkin, Anton Konushin",
        "summary": "Despite recent success in discriminative approaches in monocular depth estimation its quality remains limited by training datasets. Generative approaches mitigate this issue by leveraging strong priors derived from training on internet-scale datasets. Recent studies have demonstrated that large text-to-image diffusion models achieve state-of-the-art results in depth estimation when fine-tuned on small depth datasets. Concurrently, autoregressive generative approaches, such as the Visual AutoRegressive modeling~(VAR), have shown promising results in conditioned image synthesis. Following the visual autoregressive modeling paradigm, we introduce the first autoregressive depth estimation model based on the visual autoregressive transformer. Our primary contribution is DepthART -- a novel training method formulated as Depth Autoregressive Refinement Task. Unlike the original VAR training procedure, which employs static targets, our method utilizes a dynamic target formulation that enables model self-refinement and incorporates multi-modal guidance during training. Specifically, we use model predictions as inputs instead of ground truth token maps during training, framing the objective as residual minimization. Our experiments demonstrate that the proposed training approach significantly outperforms visual autoregressive modeling via next-scale prediction in the depth estimation task. The Visual Autoregressive Transformer trained with our approach on Hypersim achieves superior results on a set of unseen benchmarks compared to other generative and discriminative baselines.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 13:36:34 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.985620"
    },
    {
        "index": "#25",
        "title": "Generalizing monocular colonoscopy image depth estimation by uncertainty-based global and local fusion network",
        "link": "/arxiv/2409.15006",
        "arxiv_id": "2409.15006",
        "authors": "Sijia Du, Chengfeng Zhou, Suncheng Xiang, Jianwei Xu, Dahong Qian",
        "summary": "Objective: Depth estimation is crucial for endoscopic navigation and manipulation, but obtaining ground-truth depth maps in real clinical scenarios, such as the colon, is challenging. This study aims to develop a robust framework that generalizes well to real colonoscopy images, overcoming challenges like non-Lambertian surface reflection and diverse data distributions. Methods: We propose a framework combining a convolutional neural network (CNN) for capturing local features and a Transformer for capturing global information. An uncertainty-based fusion block was designed to enhance generalization by identifying complementary contributions from the CNN and Transformer branches. The network can be trained with simulated datasets and generalize directly to unseen clinical data without any fine-tuning. Results: Our method is validated on multiple datasets and demonstrates an excellent generalization ability across various datasets and anatomical structures. Furthermore, qualitative analysis in real clinical scenarios confirmed the robustness of the proposed method. Conclusion: The integration of local and global features through the CNN-Transformer architecture, along with the uncertainty-based fusion block, improves depth estimation performance and generalization in both simulated and real-world endoscopic environments. Significance: This study offers a novel approach to estimate depth maps for endoscopy images despite the complex conditions in clinic, serving as a foundation for endoscopic automatic navigation and other clinical tasks, such as polyp detection and segmentation.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-23 13:30:59 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.985972"
    },
    {
        "index": "#26",
        "title": "Sparse-to-Dense LiDAR Point Generation by LiDAR-Camera Fusion for 3D Object Detection",
        "link": "/arxiv/2409.14985",
        "arxiv_id": "2409.14985",
        "authors": "Minseung Lee, Seokha Moon, Seung Joon Lee, Jinkyu Kim",
        "summary": "Accurately detecting objects at long distances remains a critical challenge in 3D object detection when relying solely on LiDAR sensors due to the inherent limitations of data sparsity. To address this issue, we propose the LiDAR-Camera Augmentation Network (LCANet), a novel framework that reconstructs LiDAR point cloud data by fusing 2D image features, which contain rich semantic information, generating additional points to improve detection accuracy. LCANet fuses data from LiDAR sensors and cameras by projecting image features into the 3D space, integrating semantic information into the point cloud data. This fused data is then encoded to produce 3D features that contain both semantic and spatial information, which are further refined to reconstruct final points before bounding box prediction. This fusion effectively compensates for LiDAR's weakness in detecting objects at long distances, which are often represented by sparse points. Additionally, due to the sparsity of many objects in the original dataset, which makes effective supervision for point generation challenging, we employ a point cloud completion network to create a complete point cloud dataset that supervises the generation of dense point clouds in our network. Extensive experiments on the KITTI and Waymo datasets demonstrate that LCANet significantly outperforms existing models, particularly in detecting sparse and distant objects.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-23 13:03:31 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.986322"
    },
    {
        "index": "#27",
        "title": "SocialCircle+: Learning the Angle-based Conditioned Interaction Representation for Pedestrian Trajectory Prediction",
        "link": "/arxiv/2409.14984",
        "arxiv_id": "2409.14984",
        "authors": "Conghao Wong, Beihao Xia, Ziqian Zou, Xinge You",
        "summary": "Trajectory prediction is a crucial aspect of understanding human behaviors. Researchers have made efforts to represent socially interactive behaviors among pedestrians and utilize various networks to enhance prediction capability. Unfortunately, they still face challenges not only in fully explaining and measuring how these interactive behaviors work to modify trajectories but also in modeling pedestrians' preferences to plan or participate in social interactions in response to the changeable physical environments as extra conditions. This manuscript mainly focuses on the above explainability and conditionality requirements for trajectory prediction networks. Inspired by marine animals perceiving other companions and the environment underwater by echolocation, this work constructs an angle-based conditioned social interaction representation SocialCircle+ to represent the socially interactive context and its corresponding conditions. It employs a social branch and a conditional branch to describe how pedestrians are positioned in prediction scenes socially and physically in angle-based-cyclic-sequence forms. Then, adaptive fusion is applied to fuse the above conditional clues onto the social ones to learn the final interaction representation. Experiments demonstrate the superiority of SocialCircle+ with different trajectory prediction backbones. Moreover, counterfactual interventions have been made to simultaneously verify the modeling capacity of causalities among interactive variables and the conditioning capability.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 13:02:12 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.986503"
    },
    {
        "index": "#28",
        "title": "Dynamic Integration of Task-Specific Adapters for Class Incremental Learning",
        "link": "/arxiv/2409.14983",
        "arxiv_id": "2409.14983",
        "authors": "Jiashuo Li, Shaokun Wang, Bo Qian, Yuhang He, Xing Wei, Yihong Gong",
        "summary": "Non-exemplar class Incremental Learning (NECIL) enables models to continuously acquire new classes without retraining from scratch and storing old task exemplars, addressing privacy and storage issues. However, the absence of data from earlier tasks exacerbates the challenge of catastrophic forgetting in NECIL. In this paper, we propose a novel framework called Dynamic Integration of task-specific Adapters (DIA), which comprises two key components: Task-Specific Adapter Integration (TSAI) and Patch-Level Model Alignment. TSAI boosts compositionality through a patch-level adapter integration strategy, which provides a more flexible compositional solution while maintaining low computation costs. Patch-Level Model Alignment maintains feature consistency and accurate decision boundaries via two specialized mechanisms: Patch-Level Distillation Loss (PDL) and Patch-Level Feature Reconstruction method (PFR). Specifically, the PDL preserves feature-level consistency between successive models by implementing a distillation loss based on the contributions of patch tokens to new class learning. The PFR facilitates accurate classifier alignment by reconstructing old class features from previous tasks that adapt to new task knowledge. Extensive experiments validate the effectiveness of our DIA, revealing significant improvements on benchmark datasets in the NECIL setting, maintaining an optimal balance between computational complexity and accuracy. The full code implementation will be made publicly available upon the publication of this paper.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 13:01:33 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.986714"
    },
    {
        "index": "#29",
        "title": "A new baseline for edge detection: Make Encoder-Decoder great again",
        "link": "/arxiv/2409.14976",
        "arxiv_id": "2409.14976",
        "authors": "Yachuan Li, Xavier Soria Pomab, Yongke Xi, Guanlin Li, Chaozhi Yang, Qian Xiao, Yun Bai, Zongmin LI",
        "summary": "The performance of deep learning based edge detector has far exceeded that of humans, but the huge computational cost and complex training strategy hinder its further development and application. In this paper, we eliminate these complexities with a vanilla encoder-decoder based detector. Firstly, we design a bilateral encoder to decouple the extraction process of location features and semantic features. Since the location branch no longer provides cues for the semantic branch, the richness of features can be further compressed, which is the key to make our model more compact. We propose a cascaded feature fusion decoder, where the location features are progressively refined by semantic features. The refined location features are the only basis for generating the edge map. The coarse original location features and semantic features are avoided from direct contact with the final result. So the noise in the location features and the location error in the semantic features can be suppressed in the generated edge map. The proposed New Baseline for Edge Detection (NBED) achieves superior performance consistently across multiple edge detection benchmarks, even compared with those methods with huge computational cost and complex training strategy. The ODS of NBED on BSDS500 is 0.838, achieving state-of-the-art performance. Our study shows that what really matters in the current edge detection is high-quality features, and we can make the encoder-decoder based detector great again even without complex training strategies and huge computational cost. The code is available at https://github.com/Li-yachuan/NBED.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 12:54:38 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.986964"
    },
    {
        "index": "#30",
        "title": "Exploring Fine-grained Retail Product Discrimination with Zero-shot Object Classification Using Vision-Language Models",
        "link": "/arxiv/2409.14963",
        "arxiv_id": "2409.14963",
        "authors": "Anil Osman Tur, Alessandro Conti, Cigdem Beyan, Davide Boscaini, Roberto Larcher, Stefano Messelodi, Fabio Poiesi, Elisa Ricci",
        "summary": "In smart retail applications, the large number of products and their frequent turnover necessitate reliable zero-shot object classification methods. The zero-shot assumption is essential to avoid the need for re-training the classifier every time a new product is introduced into stock or an existing product undergoes rebranding. In this paper, we make three key contributions. Firstly, we introduce the MIMEX dataset, comprising 28 distinct product categories. Unlike existing datasets in the literature, MIMEX focuses on fine-grained product classification and includes a diverse range of retail products. Secondly, we benchmark the zero-shot object classification performance of state-of-the-art vision-language models (VLMs) on the proposed MIMEX dataset. Our experiments reveal that these models achieve unsatisfactory fine-grained classification performance, highlighting the need for specialized approaches. Lastly, we propose a novel ensemble approach that integrates embeddings from CLIP and DINOv2 with dimensionality reduction techniques to enhance classification performance. By combining these components, our ensemble approach outperforms VLMs, effectively capturing visual cues crucial for fine-grained product discrimination. Additionally, we introduce a class adaptation method that utilizes visual prototyping with limited samples in scenarios with scarce labeled data, addressing a critical need in retail environments where product variety frequently changes. To encourage further research into zero-shot object classification for smart retail applications, we will release both the MIMEX dataset and benchmark to the research community. Interested researchers can contact the authors for details on the terms and conditions of use. The code is available: https://github.com/AnilOsmanTur/Zero-shot-Retail-Product-Classification.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 12:28:40 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.987208"
    },
    {
        "index": "#31",
        "title": "Improving Adversarial Robustness for 3D Point Cloud Recognition at Test-Time through Purified Self-Training",
        "link": "/arxiv/2409.14940",
        "arxiv_id": "2409.14940",
        "authors": "Jinpeng Lin, Xulei Yang, Tianrui Li, Xun Xu",
        "summary": "Recognizing 3D point cloud plays a pivotal role in many real-world applications. However, deploying 3D point cloud deep learning model is vulnerable to adversarial attacks. Despite many efforts into developing robust model by adversarial training, they may become less effective against emerging attacks. This limitation motivates the development of adversarial purification which employs generative model to mitigate the impact of adversarial attacks. In this work, we highlight the remaining challenges from two perspectives. First, the purification based method requires retraining the classifier on purified samples which introduces additional computation overhead. Moreover, in a more realistic scenario, testing samples arrives in a streaming fashion and adversarial samples are not isolated from clean samples. These challenges motivates us to explore dynamically update model upon observing testing samples. We proposed a test-time purified self-training strategy to achieve this objective. Adaptive thresholding and feature distribution alignment are introduced to improve the robustness of self-training. Extensive results on different adversarial attacks suggest the proposed method is complementary to purification based method in handling continually changing adversarial attacks on the testing data stream.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 11:46:38 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.987398"
    },
    {
        "index": "#32",
        "title": "Deep Cost Ray Fusion for Sparse Depth Video Completion",
        "link": "/arxiv/2409.14935",
        "arxiv_id": "2409.14935",
        "authors": "Jungeon Kim, Soongjin Kim, Jaesik Park, Seungyong Lee",
        "summary": "In this paper, we present a learning-based framework for sparse depth video completion. Given a sparse depth map and a color image at a certain viewpoint, our approach makes a cost volume that is constructed on depth hypothesis planes. To effectively fuse sequential cost volumes of the multiple viewpoints for improved depth completion, we introduce a learning-based cost volume fusion framework, namely RayFusion, that effectively leverages the attention mechanism for each pair of overlapped rays in adjacent cost volumes. As a result of leveraging feature statistics accumulated over time, our proposed framework consistently outperforms or rivals state-of-the-art approaches on diverse indoor and outdoor datasets, including the KITTI Depth Completion benchmark, VOID Depth Completion benchmark, and ScanNetV2 dataset, using much fewer network parameters.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 11:42:16 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.987590"
    },
    {
        "index": "#33",
        "title": "DanceCamAnimator: Keyframe-Based Controllable 3D Dance Camera Synthesis",
        "link": "/arxiv/2409.14925",
        "arxiv_id": "2409.14925",
        "authors": "Zixuan Wang, Jiayi Li, Xiaoyu Qin, Shikun Sun, Songtao Zhou, Jia Jia, Jiebo Luo",
        "summary": "Synthesizing camera movements from music and dance is highly challenging due to the contradicting requirements and complexities of dance cinematography. Unlike human movements, which are always continuous, dance camera movements involve both continuous sequences of variable lengths and sudden drastic changes to simulate the switching of multiple cameras. However, in previous works, every camera frame is equally treated and this causes jittering and unavoidable smoothing in post-processing. To solve these problems, we propose to integrate animator dance cinematography knowledge by formulating this task as a three-stage process: keyframe detection, keyframe synthesis, and tween function prediction. Following this formulation, we design a novel end-to-end dance camera synthesis framework \\textbf{DanceCamAnimator}, which imitates human animation procedures and shows powerful keyframe-based controllability with variable lengths. Extensive experiments on the DCM dataset demonstrate that our method surpasses previous baselines quantitatively and qualitatively. Code will be available at \\url{https://github.com/Carmenw1203/DanceCamAnimator-Official}.",
        "subjects": "Computer Vision and Pattern Recognition, Multimedia",
        "date": "2024-09-23 11:20:44 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.987799"
    },
    {
        "index": "#34",
        "title": "Advancing Video Quality Assessment for AIGC",
        "link": "/arxiv/2409.14888",
        "arxiv_id": "2409.14888",
        "authors": "Xinli Yue, Jianhui Sun, Han Kong, Liangchao Yao, Tianyi Wang, Lei Li, Fengyun Rao, Jing Lv, Fan Xia, Yuetang Deng, Qian Wang, Lingchen Zhao",
        "summary": "In recent years, AI generative models have made remarkable progress across various domains, including text generation, image generation, and video generation. However, assessing the quality of text-to-video generation is still in its infancy, and existing evaluation frameworks fall short when compared to those for natural videos. Current video quality assessment (VQA) methods primarily focus on evaluating the overall quality of natural videos and fail to adequately account for the substantial quality discrepancies between frames in generated videos. To address this issue, we propose a novel loss function that combines mean absolute error with cross-entropy loss to mitigate inter-frame quality inconsistencies. Additionally, we introduce the innovative S2CNet technique to retain critical content, while leveraging adversarial training to enhance the model's generalization capabilities. Experimental results demonstrate that our method outperforms existing VQA techniques on the AIGC Video dataset, surpassing the previous state-of-the-art by 3.1% in terms of PLCC.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 10:36:22 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.988032"
    },
    {
        "index": "#35",
        "title": "Probabilistically Aligned View-unaligned Clustering with Adaptive Template Selection",
        "link": "/arxiv/2409.14882",
        "arxiv_id": "2409.14882",
        "authors": "Wenhua Dong, Xiao-Jun Wu, Zhenhua Feng, Sara Atito, Muhammad Awais, Josef Kittler",
        "summary": "In most existing multi-view modeling scenarios, cross-view correspondence (CVC) between instances of the same target from different views, like paired image-text data, is a crucial prerequisite for effortlessly deriving a consistent representation. Nevertheless, this premise is frequently compromised in certain applications, where each view is organized and transmitted independently, resulting in the view-unaligned problem (VuP). Restoring CVC of unaligned multi-view data is a challenging and highly demanding task that has received limited attention from the research community. To tackle this practical challenge, we propose to integrate the permutation derivation procedure into the bipartite graph paradigm for view-unaligned clustering, termed Probabilistically Aligned View-unaligned Clustering with Adaptive Template Selection (PAVuC-ATS). Specifically, we learn consistent anchors and view-specific graphs by the bipartite graph, and derive permutations applied to the unaligned graphs by reformulating the alignment between two latent representations as a 2-step transition of a Markov chain with adaptive template selection, thereby achieving the probabilistic alignment. The convergence of the resultant optimization problem is validated both experimentally and theoretically. Extensive experiments on six benchmark datasets demonstrate the superiority of the proposed PAVuC-ATS over the baseline methods.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 10:30:09 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.988284"
    },
    {
        "index": "#36",
        "title": "Mammo-Clustering:A Weakly Supervised Multi-view Global-Local Context Clustering Network for Detection and Classification in Mammography",
        "link": "/arxiv/2409.14876",
        "arxiv_id": "2409.14876",
        "authors": "Shilong Yang, Chulong Zhang, Qi Zang, Juan Yu, Liang Zeng, Xiao Luo, Yexuan Xing, Xin Pan, Qi Li, Xiaokun Liang, Yaoqin Xie",
        "summary": "Breast cancer has long posed a significant threat to women's health, making early screening crucial for mitigating its impact. However, mammography, the preferred method for early screening, faces limitations such as the burden of double reading by radiologists, challenges in widespread adoption in remote and underdeveloped areas, and obstacles in intelligent early screening development due to data constraints. To address these challenges, we propose a weakly supervised multi-view mammography early screening model for breast cancer based on context clustering. Context clustering, a feature extraction structure that is neither CNN nor transformer, combined with multi-view learning for information complementation, presents a promising approach. The weak supervision design specifically addresses data limitations. Our model achieves state-of-the-art performance with fewer parameters on two public datasets, with an AUC of 0.828 on the Vindr-Mammo dataset and 0.805 on the CBIS-DDSM dataset. Our model shows potential in reducing the burden on doctors and increasing the feasibility of breast cancer screening for women in underdeveloped regions.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-23 10:17:13 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.988529"
    },
    {
        "index": "#37",
        "title": "FUSED-Net: Enhancing Few-Shot Traffic Sign Detection with Unfrozen Parameters, Pseudo-Support Sets, Embedding Normalization, and Domain Adaptation",
        "link": "/arxiv/2409.14852",
        "arxiv_id": "2409.14852",
        "authors": "Md. Atiqur Rahman, Nahian Ibn Asad, Md. Mushfiqul Haque Omi, Md. Bakhtiar Hasan, Sabbir Ahmed, Md. Hasanul Kabir",
        "summary": "Automatic Traffic Sign Recognition is paramount in modern transportation systems, motivating several research endeavors to focus on performance improvement by utilizing large-scale datasets. As the appearance of traffic signs varies across countries, curating large-scale datasets is often impractical; and requires efficient models that can produce satisfactory performance using limited data. In this connection, we present 'FUSED-Net', built-upon Faster RCNN for traffic sign detection, enhanced by Unfrozen Parameters, Pseudo-Support Sets, Embedding Normalization, and Domain Adaptation while reducing data requirement. Unlike traditional approaches, we keep all parameters unfrozen during training, enabling FUSED-Net to learn from limited samples. The generation of a Pseudo-Support Set through data augmentation further enhances performance by compensating for the scarcity of target domain data. Additionally, Embedding Normalization is incorporated to reduce intra-class variance, standardizing feature representation. Domain Adaptation, achieved by pre-training on a diverse traffic sign dataset distinct from the target domain, improves model generalization. Evaluating FUSED-Net on the BDTSD dataset, we achieved 2.4x, 2.2x, 1.5x, and 1.3x improvements of mAP in 1-shot, 3-shot, 5-shot, and 10-shot scenarios, respectively compared to the state-of-the-art Few-Shot Object Detection (FSOD) models. Additionally, we outperform state-of-the-art works on the cross-domain FSOD benchmark under several scenarios.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-23 09:34:42 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.988738"
    },
    {
        "index": "#38",
        "title": "Disentanglement with Factor Quantized Variational Autoencoders",
        "link": "/arxiv/2409.14851",
        "arxiv_id": "2409.14851",
        "authors": "Gulcin Baykal, Melih Kandemir, Gozde Unal",
        "summary": "Disentangled representation learning aims to represent the underlying generative factors of a dataset in a latent representation independently of one another. In our work, we propose a discrete variational autoencoder (VAE) based model where the ground truth information about the generative factors are not provided to the model. We demonstrate the advantages of learning discrete representations over learning continuous representations in facilitating disentanglement. Furthermore, we propose incorporating an inductive bias into the model to further enhance disentanglement. Precisely, we propose scalar quantization of the latent variables in a latent representation with scalar values from a global codebook, and we add a total correlation term to the optimization as an inductive bias. Our method called FactorQVAE is the first method that combines optimization based disentanglement approaches with discrete representation learning, and it outperforms the former disentanglement methods in terms of two disentanglement metrics (DCI and InfoMEC) while improving the reconstruction performance. Our code can be found at \\url{https://github.com/ituvisionlab/FactorQVAE}.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-23 09:33:53 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.988954"
    },
    {
        "index": "#39",
        "title": "GroCo: Ground Constraint for Metric Self-Supervised Monocular Depth",
        "link": "/arxiv/2409.14850",
        "arxiv_id": "2409.14850",
        "authors": "Aurélien Cecille, Stefan Duffner, Franck Davoine, Thibault Neveu, Rémi Agier",
        "summary": "Monocular depth estimation has greatly improved in the recent years but models predicting metric depth still struggle to generalize across diverse camera poses and datasets. While recent supervised methods mitigate this issue by leveraging ground prior information at inference, their adaptability to self-supervised settings is limited due to the additional challenge of scale recovery. Addressing this gap, we propose in this paper a novel constraint on ground areas designed specifically for the self-supervised paradigm. This mechanism not only allows to accurately recover the scale but also ensures coherence between the depth prediction and the ground prior. Experimental results show that our method surpasses existing scale recovery techniques on the KITTI benchmark and significantly enhances model generalization capabilities. This improvement can be observed by its more robust performance across diverse camera rotations and its adaptability in zero-shot conditions with previously unseen driving datasets such as DDAD.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning, Robotics",
        "date": "2024-09-23 09:30:27 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.989189"
    },
    {
        "index": "#40",
        "title": "Revisiting Video Quality Assessment from the Perspective of Generalization",
        "link": "/arxiv/2409.14847",
        "arxiv_id": "2409.14847",
        "authors": "Xinli Yue, Jianhui Sun, Liangchao Yao, Fan Xia, Yuetang Deng, Tianyi Wang, Lei Li, Fengyun Rao, Jing Lv, Qian Wang, Lingchen Zhao",
        "summary": "The increasing popularity of short video platforms such as YouTube Shorts, TikTok, and Kwai has led to a surge in User-Generated Content (UGC), which presents significant challenges for the generalization performance of Video Quality Assessment (VQA) tasks. These challenges not only affect performance on test sets but also impact the ability to generalize across different datasets. While prior research has primarily focused on enhancing feature extractors, sampling methods, and network branches, it has largely overlooked the generalization capabilities of VQA tasks. In this work, we reevaluate the VQA task from a generalization standpoint. We begin by analyzing the weight loss landscape of VQA models, identifying a strong correlation between this landscape and the generalization gaps. We then investigate various techniques to regularize the weight loss landscape. Our results reveal that adversarial weight perturbations can effectively smooth this landscape, significantly improving the generalization performance, with cross-dataset generalization and fine-tuning performance enhanced by up to 1.8% and 3%, respectively. Through extensive experiments across various VQA methods and datasets, we validate the effectiveness of our approach. Furthermore, by leveraging our insights, we achieve state-of-the-art performance in Image Quality Assessment (IQA) tasks. Our code is available at https://github.com/XinliYue/VQA-Generalization.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 09:24:55 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.989415"
    },
    {
        "index": "#41",
        "title": "Two Deep Learning Solutions for Automatic Blurring of Faces in Videos",
        "link": "/arxiv/2409.14828",
        "arxiv_id": "2409.14828",
        "authors": "Roman Plaud, Jose-Luis Lisani",
        "summary": "The widespread use of cameras in everyday life situations generates a vast amount of data that may contain sensitive information about the people and vehicles moving in front of them (location, license plates, physical characteristics, etc). In particular, people's faces are recorded by surveillance cameras in public spaces. In order to ensure the privacy of individuals, face blurring techniques can be applied to the collected videos. In this paper we present two deep-learning based options to tackle the problem. First, a direct approach, consisting of a classical object detector (based on the YOLO architecture) trained to detect faces, which are subsequently blurred. Second, an indirect approach, in which a Unet-like segmentation network is trained to output a version of the input image in which all the faces have been blurred.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 08:59:44 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.989588"
    },
    {
        "index": "#42",
        "title": "AIM 2024 Challenge on Video Saliency Prediction: Methods and Results",
        "link": "/arxiv/2409.14827",
        "arxiv_id": "2409.14827",
        "authors": "Andrey Moskalenko, Alexey Bryncev, Dmitry Vatolin, Radu Timofte, Gen Zhan, Li Yang, Yunlong Tang, Yiting Liao, Jiongzhi Lin, Baitao Huang, Morteza Moradi, Mohammad Moradi, Francesco Rundo, Concetto Spampinato, Ali Borji, Simone Palazzo, Yuxin Zhu, Yinan Sun, Huiyu Duan, Yuqin Cao, Ziheng Jia, Qiang Hu, Xiongkuo Min, Guangtao Zhai, Hao Fang, Runmin Cong, Xiankai Lu, Xiaofei Zhou, Wei Zhang, Chunyu Zhao, Wentao Mu, Tao Deng, Hamed R. Tavakoli",
        "summary": "This paper reviews the Challenge on Video Saliency Prediction at AIM 2024. The goal of the participants was to develop a method for predicting accurate saliency maps for the provided set of video sequences. Saliency maps are widely exploited in various applications, including video compression, quality assessment, visual perception studies, the advertising industry, etc. For this competition, a previously unused large-scale audio-visual mouse saliency (AViMoS) dataset of 1500 videos with more than 70 observers per video was collected using crowdsourced mouse tracking. The dataset collection methodology has been validated using conventional eye-tracking data and has shown high consistency. Over 30 teams registered in the challenge, and there are 7 teams that submitted the results in the final phase. The final phase solutions were tested and ranked by commonly used quality metrics on a private test subset. The results of this evaluation and the descriptions of the solutions are presented in this report. All data, including the private test subset, is made publicly available on the challenge homepage - https://challenges.videoprocessing.ai/challenges/video-saliency-prediction.html.",
        "subjects": "Computer Vision and Pattern Recognition, Human-Computer Interaction, Multimedia",
        "date": "2024-09-23 08:59:22 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.989974"
    },
    {
        "index": "#43",
        "title": "Advancing Depression Detection on Social Media Platforms Through Fine-Tuned Large Language Models",
        "link": "/arxiv/2409.14794",
        "arxiv_id": "2409.14794",
        "authors": "Shahid Munir Shah, Syeda Anshrah Gillani, Mirza Samad Ahmed Baig, Muhammad Aamer Saleem, Muhammad Hamzah Siddiqui",
        "summary": "This study investigates the use of Large Language Models (LLMs) for improved depression detection from users social media data. Through the use of fine-tuned GPT 3.5 Turbo 1106 and LLaMA2-7B models and a sizable dataset from earlier studies, we were able to identify depressed content in social media posts with a high accuracy of nearly 96.0 percent. The comparative analysis of the obtained results with the relevant studies in the literature shows that the proposed fine-tuned LLMs achieved enhanced performance compared to existing state of the-art systems. This demonstrates the robustness of LLM-based fine-tuned systems to be used as potential depression detection systems. The study describes the approach in depth, including the parameters used and the fine-tuning procedure, and it addresses the important implications of our results for the early diagnosis of depression on several social media platforms.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 08:18:25 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.990168"
    },
    {
        "index": "#44",
        "title": "Human Hair Reconstruction with Strand-Aligned 3D Gaussians",
        "link": "/arxiv/2409.14778",
        "arxiv_id": "2409.14778",
        "authors": "Egor Zakharov, Vanessa Sklyarova, Michael Black, Giljoo Nam, Justus Thies, Otmar Hilliges",
        "summary": "We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data. In contrast to recent approaches that leverage unstructured Gaussians to model human avatars, our method reconstructs the hair using 3D polylines, or strands. This fundamental difference allows the use of the resulting hairstyles out-of-the-box in modern computer graphics engines for editing, rendering, and simulation. Our 3D lifting method relies on unstructured Gaussians to generate multi-view ground truth data to supervise the fitting of hair strands. The hairstyle itself is represented in the form of the so-called strand-aligned 3D Gaussians. This representation allows us to combine strand-based hair priors, which are essential for realistic modeling of the inner structure of hairstyles, with the differentiable rendering capabilities of 3D Gaussian Splatting. Our method, named Gaussian Haircut, is evaluated on synthetic and real scenes and demonstrates state-of-the-art performance in the task of strand-based hair reconstruction.",
        "subjects": "Computer Vision and Pattern Recognition, Graphics",
        "date": "2024-09-23 07:49:46 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.995652"
    },
    {
        "index": "#45",
        "title": "CFVNet: An End-to-End Cancelable Finger Vein Network for Recognition",
        "link": "/arxiv/2409.14774",
        "arxiv_id": "2409.14774",
        "authors": "Yifan Wang, Jie Gui, Yuan Yan Tang, James Tin-Yau Kwok",
        "summary": "Finger vein recognition technology has become one of the primary solutions for high-security identification systems. However, it still has information leakage problems, which seriously jeopardizes users privacy and anonymity and cause great security risks. In addition, there is no work to consider a fully integrated secure finger vein recognition system. So, different from the previous systems, we integrate preprocessing and template protection into an integrated deep learning model. We propose an end-to-end cancelable finger vein network (CFVNet), which can be used to design an secure finger vein recognition system.It includes a plug-and-play BWR-ROIAlign unit, which consists of three sub-modules: Localization, Compression and Transformation. The localization module achieves automated localization of stable and unique finger vein ROI. The compression module losslessly removes spatial and channel redundancies. The transformation module uses the proposed BWR method to introduce unlinkability, irreversibility and revocability to the system. BWR-ROIAlign can directly plug into the model to introduce the above features for DCNN-based finger vein recognition systems. We perform extensive experiments on four public datasets to study the performance and cancelable biometric attributes of the CFVNet-based recognition system. The average accuracy, EERs and Dsys on the four datasets are 99.82%, 0.01% and 0.025, respectively, and achieves competitive performance compared with the state-of-the-arts.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 07:43:44 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.995886"
    },
    {
        "index": "#46",
        "title": "Robust and Flexible Omnidirectional Depth Estimation with Multiple 360° Cameras",
        "link": "/arxiv/2409.14766",
        "arxiv_id": "2409.14766",
        "authors": "Ming Li, Xueqian Jin, Xuejiao Hu, Jinghao Cao, Sidan Du, Yang Li",
        "summary": "Omnidirectional depth estimation has received much attention from researchers in recent years. However, challenges arise due to camera soiling and variations in camera layouts, affecting the robustness and flexibility of the algorithm. In this paper, we use the geometric constraints and redundant information of multiple 360-degree cameras to achieve robust and flexible multi-view omnidirectional depth estimation. We implement two algorithms, in which the two-stage algorithm obtains initial depth maps by pairwise stereo matching of multiple cameras and fuses the multiple depth maps to achieve the final depth estimation; the one-stage algorithm adopts spherical sweeping based on hypothetical depths to construct a uniform spherical matching cost of the multi-camera images and obtain the depth. Additionally, a generalized epipolar equirectangular projection is introduced to simplify the spherical epipolar constraints. To overcome panorama distortion, a spherical feature extractor is implemented. Furthermore, a synthetic 360-degree dataset consisting of 12K road scene panoramas and 3K ground truth depth maps is presented to train and evaluate 360-degree depth estimation algorithms. Our dataset takes soiled camera lenses and glare into consideration, which is more consistent with the real-world environment. Experiments show that our two algorithms achieve state-of-the-art performance, accurately predicting depth maps even when provided with soiled panorama inputs. The flexibility of the algorithms is experimentally validated in terms of camera layouts and numbers.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 07:31:48 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.996211"
    },
    {
        "index": "#47",
        "title": "VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models",
        "link": "/arxiv/2409.14759",
        "arxiv_id": "2409.14759",
        "authors": "Nam Hyeon-Woo, Moon Ye-Bin, Wonseok Choi, Lee Hyun, Tae-Hyun Oh",
        "summary": "Vision language models (VLMs) have shown promising reasoning capabilities across various benchmarks; however, our understanding of their visual perception remains limited. In this work, we propose an eye examination process to investigate how a VLM perceives images, specifically focusing on key elements of visual recognition, from primitive color and shape to semantic levels. To this end, we introduce a dataset named LENS to guide a VLM to follow the examination and check its readiness. Once the model is ready, we conduct the examination. Through this examination, we quantify and visualize VLMs' sensitivities to color and shape, and semantic matching. Our findings reveal that VLMs have varying sensitivity to different colors while consistently showing insensitivity to green across different VLMs. Also, we found different shape sensitivity and semantic recognition depending on LLM's capacity despite using the same fixed visual encoder. Our analyses and findings have potential to inspire the design of VLMs and the pre-processing of visual input to VLMs for improving application performance.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-23 07:15:29 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.996420"
    },
    {
        "index": "#48",
        "title": "BranchPoseNet: Characterizing tree branching with a deep learning-based pose estimation approach",
        "link": "/arxiv/2409.14755",
        "arxiv_id": "2409.14755",
        "authors": "Stefano Puliti, Carolin Fischer, Rasmus Astrup",
        "summary": "This paper presents an automated pipeline for detecting tree whorls in proximally laser scanning data using a pose-estimation deep learning model. Accurate whorl detection provides valuable insights into tree growth patterns, wood quality, and offers potential for use as a biometric marker to track trees throughout the forestry value chain. The workflow processes point cloud data to create sectional images, which are subsequently used to identify keypoints representing tree whorls and branches along the stem. The method was tested on a dataset of destructively sampled individual trees, where the whorls were located along the stems of felled trees. The results demonstrated strong potential, with accurate identification of tree whorls and precise calculation of key structural metrics, unlocking new insights and deeper levels of information from individual tree point clouds.",
        "subjects": "Computer Vision and Pattern Recognition, Quantitative Methods",
        "date": "2024-09-23 07:10:11 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.996609"
    },
    {
        "index": "#49",
        "title": "UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection",
        "link": "/arxiv/2409.14751",
        "arxiv_id": "2409.14751",
        "authors": "Haocheng Zhao, Runwei Guan, Taoyu Wu, Ka Lok Man, Limin Yu, Yutao Yue",
        "summary": "4D millimeter-wave (MMW) radar, which provides both height information and dense point cloud data over 3D MMW radar, has become increasingly popular in 3D object detection. In recent years, radar-vision fusion models have demonstrated performance close to that of LiDAR-based models, offering advantages in terms of lower hardware costs and better resilience in extreme conditions. However, many radar-vision fusion models treat radar as a sparse LiDAR, underutilizing radar-specific information. Additionally, these multi-modal networks are often sensitive to the failure of a single modality, particularly vision. To address these challenges, we propose the Radar Depth Lift-Splat-Shoot (RDL) module, which integrates radar-specific data into the depth prediction process, enhancing the quality of visual Bird-Eye View (BEV) features. We further introduce a Unified Feature Fusion (UFF) approach that extracts BEV features across different modalities using shared module. To assess the robustness of multi-modal models, we develop a novel Failure Test (FT) ablation experiment, which simulates vision modality failure by injecting Gaussian noise. We conduct extensive experiments on the View-of-Delft (VoD) and TJ4D datasets. The results demonstrate that our proposed Unified BEVFusion (UniBEVFusion) network significantly outperforms state-of-the-art models on the TJ4D dataset, with improvements of 1.44 in 3D and 1.72 in BEV object detection accuracy.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-23 06:57:27 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.996815"
    },
    {
        "index": "#50",
        "title": "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension",
        "link": "/arxiv/2409.14750",
        "arxiv_id": "2409.14750",
        "authors": "Junzhuo Liu, Xuzheng Yang, Weiwei Li, Peng Wang",
        "summary": "Referring Expression Comprehension (REC) is a crucial cross-modal task that objectively evaluates the capabilities of language understanding, image comprehension, and language-to-image grounding. Consequently, it serves as an ideal testing ground for Multi-modal Large Language Models (MLLMs). In pursuit of this goal, we have established a new REC dataset characterized by two key features: Firstly, it is designed with controllable varying levels of difficulty, necessitating multi-level fine-grained reasoning across object categories, attributes, and multi-hop relationships. Secondly, it includes negative text and images created through fine-grained editing and generation based on existing data, thereby testing the model's ability to correctly reject scenarios where the target object is not visible in the image--an essential aspect often overlooked in existing datasets and approaches. Utilizing this high-quality dataset, we conducted comprehensive evaluations of both state-of-the-art specialist models and MLLMs. Our findings indicate that there remains a significant gap in achieving satisfactory grounding performance. We anticipate that our dataset will inspire new approaches to enhance visual reasoning and develop more advanced cross-modal interaction strategies, ultimately unlocking the full potential of MLLMs. Our code and the datasets are available at https://github.com/liujunzhuo/FineCops-Ref.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language",
        "date": "2024-09-23 06:56:51 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.997012"
    },
    {
        "index": "#51",
        "title": "Distribution-Level Feature Distancing for Machine Unlearning: Towards a Better Trade-off Between Model Utility and Forgetting",
        "link": "/arxiv/2409.14747",
        "arxiv_id": "2409.14747",
        "authors": "Dasol Choi, Dongbin Na",
        "summary": "With the explosive growth of deep learning applications, the right to be forgotten has become increasingly in demand in various AI industries. For example, given a facial recognition system, some individuals may wish to remove images that might have been used in the training phase from the trained model. Unfortunately, modern deep neural networks sometimes unexpectedly leak personal identities. Recent studies have presented various machine unlearning algorithms to make a trained model unlearn the data to be forgotten. While these methods generally perform well in terms of forgetting scores, we have found that an unexpected modelutility drop can occur. This phenomenon, which we term correlation collapse, happens when the machine unlearning algorithms reduce the useful correlation between image features and the true label. To address this challenge, we propose Distribution-Level Feature Distancing (DLFD), a novel method that efficiently forgets instances while preventing correlation collapse. Our method synthesizes data samples so that the generated data distribution is far from the distribution of samples being forgotten in the feature space, achieving effective results within a single training epoch. Through extensive experiments on facial recognition datasets, we demonstrate that our approach significantly outperforms state-of-the-art machine unlearning methods.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-23 06:51:10 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.997188"
    },
    {
        "index": "#52",
        "title": "Less yet robust: crucial region selection for scene recognition",
        "link": "/arxiv/2409.14741",
        "arxiv_id": "2409.14741",
        "authors": "Jianqi Zhang, Mengxuan Wang, Jingyao Wang, Lingyu Si, Changwen Zheng, Fanjiang Xu",
        "summary": "Scene recognition, particularly for aerial and underwater images, often suffers from various types of degradation, such as blurring or overexposure. Previous works that focus on convolutional neural networks have been shown to be able to extract panoramic semantic features and perform well on scene recognition tasks. However, low-quality images still impede model performance due to the inappropriate use of high-level semantic features. To address these To address these challenges, we propose an adaptive selection mechanism to identify the most important and robust regions with high-level features. Thus, the model can perform learning via these regions to avoid interference. implement a learnable mask in the neural network, which can filter high-level features by assigning weights to different regions of the feature matrix. We also introduce a regularization term to further enhance the significance of key high-level feature regions. Different from previous methods, our learnable matrix pays extra attention to regions that are important to multiple categories but may cause misclassification and sets constraints to reduce the influence of such regions.This is a plug-and-play architecture that can be easily extended to other methods. Additionally, we construct an Underwater Geological Scene Classification dataset to assess the effectiveness of our model. Extensive experimental results demonstrate the superiority and robustness of our proposed method over state-of-the-art techniques on two datasets.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-23 06:39:35 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.997385"
    },
    {
        "index": "#53",
        "title": "EDSNet: Efficient-DSNet for Video Summarization",
        "link": "/arxiv/2409.14724",
        "arxiv_id": "2409.14724",
        "authors": "Ashish Prasad, Pranav Jeevan, Amit Sethi",
        "summary": "Current video summarization methods largely rely on transformer-based architectures, which, due to their quadratic complexity, require substantial computational resources. In this work, we address these inefficiencies by enhancing the Direct-to-Summarize Network (DSNet) with more resource-efficient token mixing mechanisms. We show that replacing traditional attention with alternatives like Fourier, Wavelet transforms, and Nyströmformer improves efficiency and performance. Furthermore, we explore various pooling strategies within the Regional Proposal Network, including ROI pooling, Fast Fourier Transform pooling, and flat pooling. Our experimental results on TVSum and SumMe datasets demonstrate that these modifications significantly reduce computational costs while maintaining competitive summarization performance. Thus, our work offers a more scalable solution for video summarization tasks.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2024-09-23 05:43:37 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.997571"
    },
    {
        "index": "#54",
        "title": "ControlEdit: A MultiModal Local Clothing Image Editing Method",
        "link": "/arxiv/2409.14720",
        "arxiv_id": "2409.14720",
        "authors": "Di Cheng, YingJie Shi, ShiXin Sun, JiaFu Zhang, WeiJing Wang, Yu Liu",
        "summary": "Multimodal clothing image editing refers to the precise adjustment and modification of clothing images using data such as textual descriptions and visual images as control conditions, which effectively improves the work efficiency of designers and reduces the threshold for user design. In this paper, we propose a new image editing method ControlEdit, which transfers clothing image editing to multimodal-guided local inpainting of clothing images. We address the difficulty of collecting real image datasets by leveraging the self-supervised learning approach. Based on this learning approach, we extend the channels of the feature extraction network to ensure consistent clothing image style before and after editing, and we design an inverse latent loss function to achieve soft control over the content of non-edited areas. In addition, we adopt Blended Latent Diffusion as the sampling method to make the editing boundaries transition naturally and enforce consistency of non-edited area content. Extensive experiments demonstrate that ControlEdit surpasses baseline algorithms in both qualitative and quantitative evaluations.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 05:34:59 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.997761"
    },
    {
        "index": "#55",
        "title": "Phantom of Latent for Large Language and Vision Models",
        "link": "/arxiv/2409.14713",
        "arxiv_id": "2409.14713",
        "authors": "Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, Yong Man Ro",
        "summary": "The success of visual instruction tuning has accelerated the development of large language and vision models (LLVMs). Following the scaling laws of instruction-tuned large language models (LLMs), LLVMs either have further increased their sizes, reaching 26B, 34B, and even 80B parameters. While this increase in model size has yielded significant performance gains, it demands substantially more hardware resources for both training and inference. Consequently, there naturally exists a strong need for efficient LLVMs that achieve the performance of larger models while being smaller in size. To achieve this need, we present a new efficient LLVM family with model sizes of 0.5B, 1.8B, 3.8B, and 7B parameters, Phantom, which significantly enhances learning capabilities within limited structures. By temporarily increasing the latent hidden dimension during multi-head self-attention (MHSA), we make LLVMs prepare to look and understand much more vision-language knowledge on the latent, without substantially increasing physical model sizes. To maximize its advantage, we introduce Phantom Optimization (PO) using both autoregressive supervised fine-tuning (SFT) and direct preference optimization (DPO)-like concept, which effectively follows correct answers while eliminating incorrect and ambiguous ones. Phantom outperforms numerous larger open- and closed-source LLVMs, positioning itself as a leading solution in the landscape of efficient LLVMs.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 05:19:06 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.997984"
    },
    {
        "index": "#56",
        "title": "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models",
        "link": "/arxiv/2409.14704",
        "arxiv_id": "2409.14704",
        "authors": "Jingtao Cao, Zheng Zhang, Hongru Wang, Kam-Fai Wong",
        "summary": "Progress in Text-to-Image (T2I) models has significantly improved the generation of images from textual descriptions. However, existing evaluation metrics do not adequately assess the models' ability to handle a diverse range of textual prompts, which is crucial for their generalizability. To address this, we introduce a new metric called Visual Language Evaluation Understudy (VLEU). VLEU uses large language models to sample from the visual text domain, the set of all possible input texts for T2I models, to generate a wide variety of prompts. The images generated from these prompts are evaluated based on their alignment with the input text using the CLIP model.VLEU quantifies a model's generalizability by computing the Kullback-Leibler divergence between the marginal distribution of the visual text and the conditional distribution of the images generated by the model. This metric provides a quantitative way to compare different T2I models and track improvements during model finetuning. Our experiments demonstrate the effectiveness of VLEU in evaluating the generalization capability of various T2I models, positioning it as an essential metric for future research in text-to-image synthesis.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language",
        "date": "2024-09-23 04:50:36 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.998213"
    },
    {
        "index": "#57",
        "title": "Dynamic Realms: 4D Content Analysis, Recovery and Generation with Geometric, Topological and Physical Priors",
        "link": "/arxiv/2409.14692",
        "arxiv_id": "2409.14692",
        "authors": "Zhiyang Dou",
        "summary": "My research focuses on the analysis, recovery, and generation of 4D content, where 4D includes three spatial dimensions (x, y, z) and a temporal dimension t, such as shape and motion. This focus goes beyond static objects to include dynamic changes over time, providing a comprehensive understanding of both spatial and temporal variations. These techniques are critical in applications like AR/VR, embodied AI, and robotics. My research aims to make 4D content generation more efficient, accessible, and higher in quality by incorporating geometric, topological, and physical priors. I also aim to develop effective methods for 4D content recovery and analysis using these priors.",
        "subjects": "Computer Vision and Pattern Recognition, Graphics",
        "date": "2024-09-23 03:46:51 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.998390"
    },
    {
        "index": "#58",
        "title": "Quantifying Context Bias in Domain Adaptation for Object Detection",
        "link": "/arxiv/2409.14679",
        "arxiv_id": "2409.14679",
        "authors": "Hojun Son, Arpan Kusari",
        "summary": "Domain adaptation for object detection (DAOD) aims to transfer a trained model from a source to a target domain. Various DAOD methods exist, some of which minimize context bias between foreground-background associations in various domains. However, no prior work has studied context bias in DAOD by analyzing changes in background features during adaptation and how context bias is represented in different domains. Our research experiment highlights the potential usability of context bias in DAOD. We address the problem by varying activation values over different layers of trained models and by masking the background, both of which impact the number and quality of detections. We then use one synthetic dataset from CARLA and two different versions of real open-source data, Cityscapes and Cityscapes foggy, as separate domains to represent and quantify context bias. We utilize different metrics such as Maximum Mean Discrepancy (MMD) and Maximum Variance Discrepancy (MVD) to find the layer-specific conditional probability estimates of foreground given manipulated background regions for separate domains. We demonstrate through detailed analysis that understanding of the context bias can affect DAOD approach and foc",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Robotics",
        "date": "2024-09-23 03:01:50 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.998577"
    },
    {
        "index": "#59",
        "title": "Reflecting Reality: Enabling Diffusion Models to Produce Faithful Mirror Reflections",
        "link": "/arxiv/2409.14677",
        "arxiv_id": "2409.14677",
        "authors": "Ankit Dhiman, Manan Shah, Rishubh Parihar, Yash Bhalgat, Lokesh R Boregowda, R Venkatesh Babu",
        "summary": "We tackle the problem of generating highly realistic and plausible mirror reflections using diffusion-based generative models. We formulate this problem as an image inpainting task, allowing for more user control over the placement of mirrors during the generation process. To enable this, we create SynMirror, a large-scale dataset of diverse synthetic scenes with objects placed in front of mirrors. SynMirror contains around 198K samples rendered from 66K unique 3D objects, along with their associated depth maps, normal maps and instance-wise segmentation masks, to capture relevant geometric properties of the scene. Using this dataset, we propose a novel depth-conditioned inpainting method called MirrorFusion, which generates high-quality geometrically consistent and photo-realistic mirror reflections given an input image and a mask depicting the mirror region. MirrorFusion outperforms state-of-the-art methods on SynMirror, as demonstrated by extensive quantitative and qualitative analysis. To the best of our knowledge, we are the first to successfully tackle the challenging problem of generating controlled and faithful mirror reflections of an object in a scene using diffusion based models. SynMirror and MirrorFusion open up new avenues for image editing and augmented reality applications for practitioners and researchers alike.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 02:59:07 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.998766"
    },
    {
        "index": "#60",
        "title": "AEANet: Affinity Enhanced Attentional Networks for Arbitrary Style Transfer",
        "link": "/arxiv/2409.14652",
        "arxiv_id": "2409.14652",
        "authors": "Gen Li",
        "summary": "Arbitrary artistic style transfer is a field that integrates rational academic research with emotional artistic creation. It aims to produce an image that not only features artistic characteristics of the target style but also preserves the texture structure of the content image itself. Existing style transfer methods primarily rely either on global statistics-based information or local patch-based. As a result, the generated images often either superficially apply a filter to the content image or capture extraneous semantic information from the style image, leading to a significant deviation from the global style. In this paper, we propose Affinity Enhanced-Attentional Networks (AEANet), which include a content affinity-enhanced attention (CAEA) module, style affinity-enhanced attention (SAEA) module, and hybrid attention (HA) module. The CAEA and SAEA modules first use attention to improve content and style representations with a Detail Enhanced(DE) module to reinforce fine details. Then, it aligns the global statistical information of the content and style features to fine-tune the feature information. Subsequently, the HA module adjusts the distribution of style features based on the distribution of content features. Additionally, we introduce affinity attention-based Local Dissimilarity Loss to preserve the affinities between the content and style images. Experimental results demonstrate that our approach outperforms state-of-the-art methods in arbitrary style transfer.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-23 01:39:11 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.998920"
    },
    {
        "index": "#61",
        "title": "EQ-CBM: A Probabilistic Concept Bottleneck with Energy-based Models and Quantized Vectors",
        "link": "/arxiv/2409.14630",
        "arxiv_id": "2409.14630",
        "authors": "Sangwon Kim, Dasom Ahn, Byoung Chul Ko, In-su Jang, Kwang-Ju Kim",
        "summary": "The demand for reliable AI systems has intensified the need for interpretable deep neural networks. Concept bottleneck models (CBMs) have gained attention as an effective approach by leveraging human-understandable concepts to enhance interpretability. However, existing CBMs face challenges due to deterministic concept encoding and reliance on inconsistent concepts, leading to inaccuracies. We propose EQ-CBM, a novel framework that enhances CBMs through probabilistic concept encoding using energy-based models (EBMs) with quantized concept activation vectors (qCAVs). EQ-CBM effectively captures uncertainties, thereby improving prediction reliability and accuracy. By employing qCAVs, our method selects homogeneous vectors during concept encoding, enabling more decisive task performance and facilitating higher levels of human intervention. Empirical results using benchmark datasets demonstrate that our approach outperforms the state-of-the-art in both concept and task accuracy.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-22 23:43:45 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.999123"
    },
    {
        "index": "#62",
        "title": "SOS: Segment Object System for Open-World Instance Segmentation With Object Priors",
        "link": "/arxiv/2409.14627",
        "arxiv_id": "2409.14627",
        "authors": "Christian Wilms, Tim Rolff, Maris Hillemann, Robert Johanson, Simone Frintrop",
        "summary": "We propose an approach for Open-World Instance Segmentation (OWIS), a task that aims to segment arbitrary unknown objects in images by generalizing from a limited set of annotated object classes during training. Our Segment Object System (SOS) explicitly addresses the generalization ability and the low precision of state-of-the-art systems, which often generate background detections. To this end, we generate high-quality pseudo annotations based on the foundation model SAM. We thoroughly study various object priors to generate prompts for SAM, explicitly focusing the foundation model on objects. The strongest object priors were obtained by self-attention maps from self-supervised Vision Transformers, which we utilize for prompting SAM. Finally, the post-processed segments from SAM are used as pseudo annotations to train a standard instance segmentation system. Our approach shows strong generalization capabilities on COCO, LVIS, and ADE20k datasets and improves on the precision by up to 81.6% compared to the state-of-the-art. Source code is available at: https://github.com/chwilms/SOS",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 23:35:31 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.999316"
    },
    {
        "index": "#63",
        "title": "Secrets of Edge-Informed Contrast Maximization for Event-Based Vision",
        "link": "/arxiv/2409.14611",
        "arxiv_id": "2409.14611",
        "authors": "Pritam P. Karmokar, Quan H. Nguyen, William J. Beksi",
        "summary": "Event cameras capture the motion of intensity gradients (edges) in the image plane in the form of rapid asynchronous events. When accumulated in 2D histograms, these events depict overlays of the edges in motion, consequently obscuring the spatial structure of the generating edges. Contrast maximization (CM) is an optimization framework that can reverse this effect and produce sharp spatial structures that resemble the moving intensity gradients by estimating the motion trajectories of the events. Nonetheless, CM is still an underexplored area of research with avenues for improvement. In this paper, we propose a novel hybrid approach that extends CM from uni-modal (events only) to bi-modal (events and edges). We leverage the underpinning concept that, given a reference time, optimally warped events produce sharp gradients consistent with the moving edge at that time. Specifically, we formalize a correlation-based objective to aid CM and provide key insights into the incorporation of multiscale and multireference techniques. Moreover, our edge-informed CM method yields superior sharpness scores and establishes new state-of-the-art event optical flow benchmarks on the MVSEC, DSEC, and ECD datasets.",
        "subjects": "Computer Vision and Pattern Recognition, Image and Video Processing",
        "date": "2024-09-22 22:22:26 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.999505"
    },
    {
        "index": "#64",
        "title": "Patch Ranking: Efficient CLIP by Learning to Rank Local Patches",
        "link": "/arxiv/2409.14607",
        "arxiv_id": "2409.14607",
        "authors": "Cheng-En Wu, Jinhong Lin, Yu Hen Hu, Pedro Morgado",
        "summary": "Contrastive image-text pre-trained models such as CLIP have shown remarkable adaptability to downstream tasks. However, they face challenges due to the high computational requirements of the Vision Transformer (ViT) backbone. Current strategies to boost ViT efficiency focus on pruning patch tokens but fall short in addressing the multimodal nature of CLIP and identifying the optimal subset of tokens for maximum performance. To address this, we propose greedy search methods to establish a \"Golden Ranking\" and introduce a lightweight predictor specifically trained to approximate this Ranking. To compensate for any performance degradation resulting from token pruning, we incorporate learnable visual tokens that aid in restoring and potentially enhancing the model's performance. Our work presents a comprehensive and systematic investigation of pruning tokens within the ViT backbone of CLIP models. Through our framework, we successfully reduced 40% of patch tokens in CLIP's ViT while only suffering a minimal average accuracy loss of 0.3 across seven datasets. Our study lays the groundwork for building more computationally efficient multimodal models without sacrificing their performance, addressing a key challenge in the application of advanced vision-language models.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-22 22:04:26 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.999701"
    },
    {
        "index": "#65",
        "title": "URSimulator: Human-Perception-Driven Prompt Tuning for Enhanced Virtual Urban Renewal via Diffusion Models",
        "link": "/arxiv/2409.14589",
        "arxiv_id": "2409.14589",
        "authors": "Chuanbo Hu, Shan Jia, Xin Li",
        "summary": "Tackling Urban Physical Disorder (e.g., abandoned buildings, litter, messy vegetation, graffiti) is essential, as it negatively impacts the safety, well-being, and psychological state of communities. Urban Renewal is the process of revitalizing these neglected and decayed areas within a city to improve the physical environment and quality of life for residents. Effective urban renewal efforts can transform these environments, enhancing their appeal and livability. However, current research lacks simulation tools that can quantitatively assess and visualize the impacts of renewal efforts, often relying on subjective judgments. Such tools are crucial for planning and implementing effective strategies by providing a clear visualization of potential changes and their impacts. This paper presents a novel framework addressing this gap by using human perception feedback to simulate street environment enhancement. We develop a prompt tuning approach that integrates text-driven Stable Diffusion with human perception feedback, iteratively editing local areas of street view images to better align with perceptions of beauty, liveliness, and safety. Our experiments show that this framework significantly improves perceptions of urban environments, with increases of 17.60% in safety, 31.15% in beauty, and 28.82% in liveliness. In contrast, advanced methods like DiffEdit achieve only 2.31%, 11.87%, and 15.84% improvements, respectively. We applied this framework across various virtual scenarios, including neighborhood improvement, building redevelopment, green space expansion, and community garden creation. The results demonstrate its effectiveness in simulating urban renewal, offering valuable insights for urban planning and policy-making.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 20:39:32 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:30:59.999880"
    },
    {
        "index": "#66",
        "title": "Space evaluation based on pitch control using drone video in Ultimate",
        "link": "/arxiv/2409.14588",
        "arxiv_id": "2409.14588",
        "authors": "Shunsuke Iwashita, Atom Scott, Rikuhei Umemoto, Ning Ding, Keisuke Fujii",
        "summary": "Ultimate is a sport in which teams of seven players compete for points by passing a disc into the end zone. A distinctive aspect of Ultimate is that the player holding the disc is unable to move, underscoring the significance of creating space to receive passes. Despite extensive research into space evaluation in sports such as football and basketball, there is a paucity of information available for Ultimate. This study focuses on the 3-on-3 format, which is widely practiced in Ultimate, and evaluates space during offensive play. The data collection process entailed the use of drones for filming and the subsequent correction of the angles for the purpose of obtaining positional data. The model is derived from the pitch control model of soccer and adapted to the rules of Ultimate, where the player holding the disc is stationary. The integration of position and distance weights with pitch control values enables the derivation of space evaluation metrics. The findings of this study indicate that movement to create space and accurate passing into that space are both significant factors in scoring. The code is available at https://github.com/shunsuke-iwashita/USO.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-03 01:19:02 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.000126"
    },
    {
        "index": "#67",
        "title": "Deep Learning Techniques for Atmospheric Turbulence Removal: A Review",
        "link": "/arxiv/2409.14587",
        "arxiv_id": "2409.14587",
        "authors": "Paul Hill, Nantheera Anantrasirichai, Alin Achim, David Bull",
        "summary": "The influence of atmospheric turbulence on acquired imagery makes image interpretation and scene analysis extremely difficult and reduces the effectiveness of conventional approaches for classifying and tracking objects of interest in the scene. Restoring a scene distorted by atmospheric turbulence is also a challenging problem. The effect, which is caused by random, spatially varying perturbations, makes conventional model-based approaches difficult and, in most cases, impractical due to complexity and memory requirements. Deep learning approaches offer faster operation and are capable of implementation on small devices. This paper reviews the characteristics of atmospheric turbulence and its impact on acquired imagery. It compares the performance of various state-of-the-art deep neural networks, including Transformers, SWIN and Mamba, when used to mitigate spatio-temporal image distortions.",
        "subjects": "Computer Vision and Pattern Recognition, Instrumentation and Methods for Astrophysics",
        "date": "2024-09-03 15:53:22 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.000373"
    },
    {
        "index": "#68",
        "title": "AR Overlay: Training Image Pose Estimation on Curved Surface in a Synthetic Way",
        "link": "/arxiv/2409.14577",
        "arxiv_id": "2409.14577",
        "authors": "Sining Huang, Yukun Song, Yixiao Kang, Chang Yu",
        "summary": "In the field of spatial computing, one of the most essential tasks is the pose estimation of 3D objects. While rigid transformations of arbitrary 3D objects are relatively hard to detect due to varying environment introducing factors like insufficient lighting or even occlusion, objects with pre-defined shapes are often easy to track, leveraging geometric constraints. Curved images, with flexible dimensions but a confined shape, are essential shapes often targeted in 3D tracking. Traditionally, proprietary algorithms often require specific curvature measures as the input along with the original flattened images to enable pose estimation for a single image target. In this paper, we propose a pipeline that can detect several logo images simultaneously and only requires the original images as the input, unlocking more effects in downstream fields such as Augmented Reality (AR).",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 19:44:46 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.010904"
    },
    {
        "index": "#69",
        "title": "Event-ECC: Asynchronous Tracking of Events with Continuous Optimization",
        "link": "/arxiv/2409.14564",
        "arxiv_id": "2409.14564",
        "authors": "Maria Zafeiri, Georgios Evangelidis, Emmanouil Psarakis",
        "summary": "In this paper, an event-based tracker is presented. Inspired by recent advances in asynchronous processing of individual events, we develop a direct matching scheme that aligns spatial distributions of events at different times. More specifically, we adopt the Enhanced Correlation Coefficient (ECC) criterion and propose a tracking algorithm that computes a 2D motion warp per single event, called event-ECC (eECC). The complete tracking of a feature along time is cast as a \\emph{single} iterative continuous optimization problem, whereby every single iteration is executed per event. The computational burden of event-wise processing is alleviated through a lightweight version that benefits from incremental processing and updating scheme. We test the proposed algorithm on publicly available datasets and we report improvements in tracking accuracy and feature age over state-of-the-art event-based asynchronous trackers.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 19:03:19 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.011333"
    },
    {
        "index": "#70",
        "title": "GlamTry: Advancing Virtual Try-On for High-End Accessories",
        "link": "/arxiv/2409.14553",
        "arxiv_id": "2409.14553",
        "authors": "Ting-Yu Chang, Seretsi Khabane Lekena",
        "summary": "The paper aims to address the lack of photorealistic virtual try-on models for accessories such as jewelry and watches, which are particularly relevant for online retail applications. While existing virtual try-on models focus primarily on clothing items, there is a gap in the market for accessories. This research explores the application of techniques from 2D virtual try-on models for clothing, such as VITON-HD, and integrates them with other computer vision models, notably MediaPipe Hand Landmarker. Drawing on existing literature, the study customizes and retrains a unique model using accessory-specific data and network architecture modifications to assess the feasibility of extending virtual try-on technology to accessories. Results demonstrate improved location prediction compared to the original model for clothes, even with a small dataset. This underscores the model's potential with larger datasets exceeding 10,000 images, paving the way for future research in virtual accessory try-on applications.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 18:29:32 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.011575"
    },
    {
        "index": "#71",
        "title": "TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps",
        "link": "/arxiv/2409.14543",
        "arxiv_id": "2409.14543",
        "authors": "Arjun Raj, Lei Wang, Tom Gedeon",
        "summary": "Accurately detecting and tracking high-speed, small objects, such as balls in sports videos, is challenging due to factors like motion blur and occlusion. Although recent deep learning frameworks like TrackNetV1, V2, and V3 have advanced tennis ball and shuttlecock tracking, they often struggle in scenarios with partial occlusion or low visibility. This is primarily because these models rely heavily on visual features without explicitly incorporating motion information, which is crucial for precise tracking and trajectory prediction. In this paper, we introduce an enhancement to the TrackNet family by fusing high-level visual features with learnable motion attention maps through a motion-aware fusion mechanism, effectively emphasizing the moving ball's location and improving tracking performance. Our approach leverages frame differencing maps, modulated by a motion prompt layer, to highlight key motion regions over time. Experimental results on the tennis ball and shuttlecock datasets show that our method enhances the tracking performance of both TrackNetV2 and V3. We refer to our lightweight, plug-and-play solution, built on top of the existing TrackNet, as TrackNetV4.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2024-09-22 17:58:09 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.011765"
    },
    {
        "index": "#72",
        "title": "Towards Model-Agnostic Dataset Condensation by Heterogeneous Models",
        "link": "/arxiv/2409.14538",
        "arxiv_id": "2409.14538",
        "authors": "Jun-Yeong Moon, Jung Uk Kim, Gyeong-Moon Park",
        "summary": "Abstract. The advancement of deep learning has coincided with the proliferation of both models and available data. The surge in dataset sizes and the subsequent surge in computational requirements have led to the development of the Dataset Condensation (DC). While prior studies have delved into generating synthetic images through methods like distribution alignment and training trajectory tracking for more efficient model training, a significant challenge arises when employing these condensed images practically. Notably, these condensed images tend to be specific to particular models, constraining their versatility and practicality. In response to this limitation, we introduce a novel method, Heterogeneous Model Dataset Condensation (HMDC), designed to produce universally applicable condensed images through cross-model interactions. To address the issues of gradient magnitude difference and semantic distance in models when utilizing heterogeneous models, we propose the Gradient Balance Module (GBM) and Mutual Distillation (MD) with the SpatialSemantic Decomposition method. By balancing the contribution of each model and maintaining their semantic meaning closely, our approach overcomes the limitations associated with model-specific condensed images and enhances the broader utility. The source code is available in https://github.com/KHU-AGI/HMDC.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 17:13:07 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.011936"
    },
    {
        "index": "#73",
        "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding",
        "link": "/arxiv/2409.14485",
        "arxiv_id": "2409.14485",
        "authors": "Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, Bo Zhao",
        "summary": "Although current Multi-modal Large Language Models (MLLMs) demonstrate promising results in video understanding, processing extremely long videos remains an ongoing challenge. Typically, MLLMs struggle with handling thousands of tokens that exceed the maximum context length of LLMs, and they experience reduced visual clarity due to token aggregation. Another challenge is the high computational cost stemming from the large number of video tokens. To tackle these issues, we propose Video-XL, an extra-long vision language model designed for efficient hour-scale video understanding. Specifically, we argue that LLMs can be adapted as effective visual condensers and introduce Visual Context Latent Summarization, which condenses visual contexts into highly compact forms. Extensive experiments demonstrate that our model achieves promising results on popular long video understanding benchmarks, despite being trained on limited image data. Moreover, Video-XL strikes a promising balance between efficiency and effectiveness, processing 1024 frames on a single 80GB GPU while achieving nearly 100\\% accuracy in the Needle-in-a-Haystack evaluation. We envision Video-XL becoming a valuable tool for long video applications such as video summarization, surveillance anomaly detection, and Ad placement identification.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 15:13:31 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.012134"
    },
    {
        "index": "#74",
        "title": "Effectively Enhancing Vision Language Large Models by Prompt Augmentation and Caption Utilization",
        "link": "/arxiv/2409.14484",
        "arxiv_id": "2409.14484",
        "authors": "Minyi Zhao, Jie Wang, Zhaoyang Li, Jiyuan Zhang, Zhenbang Sun, Shuigeng Zhou",
        "summary": "Recent studies have shown that Vision Language Large Models (VLLMs) may output content not relevant to the input images. This problem, called the hallucination phenomenon, undoubtedly degrades VLLM performance. Therefore, various anti-hallucination techniques have been proposed to make model output more reasonable and accurate. Despite their successes, from extensive tests we found that augmenting the prompt (e.g. word appending, rewriting, and spell error etc.) may change model output and make the output hallucinate again. To cure this drawback, we propose a new instruct-tuning framework called Prompt Augmentation and Caption Utilization (PACU) to boost VLLM's generation ability under the augmented prompt scenario. Concretely, on the one hand, PACU exploits existing LLMs to augment and evaluate diverse prompts automatically. The resulting high-quality prompts are utilized to enhance VLLM's ability to process different prompts. On the other hand, PACU exploits image captions to jointly work with image features as well as the prompts for response generation. When the visual feature is inaccurate, LLM can capture useful information from the image captions for response generation. Extensive experiments on hallucination evaluation and prompt-augmented datasets demonstrate that our PACU method can work well with existing schemes to effectively boost VLLM model performance. Code is available in https://github.com/zhaominyiz/PACU.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 15:07:18 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.012322"
    },
    {
        "index": "#75",
        "title": "One Model for Two Tasks: Cooperatively Recognizing and Recovering Low-Resolution Scene Text Images by Iterative Mutual Guidance",
        "link": "/arxiv/2409.14483",
        "arxiv_id": "2409.14483",
        "authors": "Minyi Zhao, Yang Wang, Jihong Guan, Shuigeng Zhou",
        "summary": "Scene text recognition (STR) from high-resolution (HR) images has been significantly successful, however text reading on low-resolution (LR) images is still challenging due to insufficient visual information. Therefore, recently many scene text image super-resolution (STISR) models have been proposed to generate super-resolution (SR) images for the LR ones, then STR is done on the SR images, which thus boosts recognition performance. Nevertheless, these methods have two major weaknesses. On the one hand, STISR approaches may generate imperfect or even erroneous SR images, which mislead the subsequent recognition of STR models. On the other hand, as the STISR and STR models are jointly optimized, to pursue high recognition accuracy, the fidelity of SR images may be spoiled. As a result, neither the recognition performance nor the fidelity of STISR models are desirable. Then, can we achieve both high recognition performance and good fidelity? To this end, in this paper we propose a novel method called IMAGE (the abbreviation of Iterative MutuAl GuidancE) to effectively recognize and recover LR scene text images simultaneously. Concretely, IMAGE consists of a specialized STR model for recognition and a tailored STISR model to recover LR images, which are optimized separately. And we develop an iterative mutual guidance mechanism, with which the STR model provides high-level semantic information as clue to the STISR model for better super-resolution, meanwhile the STISR model offers essential low-level pixel clue to the STR model for more accurate recognition. Extensive experiments on two LR datasets demonstrate the superiority of our method over the existing works on both recognition performance and super-resolution fidelity.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 15:05:25 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.012500"
    },
    {
        "index": "#76",
        "title": "SynBench: A Synthetic Benchmark for Non-rigid 3D Point Cloud Registration",
        "link": "/arxiv/2409.14474",
        "arxiv_id": "2409.14474",
        "authors": "Sara Monji-Azad, Marvin Kinz, Claudia Scherl, David Männle, Jürgen Hesser, Nikolas Löw",
        "summary": "Non-rigid point cloud registration is a crucial task in computer vision. Evaluating a non-rigid point cloud registration method requires a dataset with challenges such as large deformation levels, noise, outliers, and incompleteness. Despite the existence of several datasets for deformable point cloud registration, the absence of a comprehensive benchmark with all challenges makes it difficult to achieve fair evaluations among different methods. This paper introduces SynBench, a new non-rigid point cloud registration dataset created using SimTool, a toolset for soft body simulation in Flex and Unreal Engine. SynBench provides the ground truth of corresponding points between two point sets and encompasses key registration challenges, including varying levels of deformation, noise, outliers, and incompleteness. To the best of the authors' knowledge, compared to existing datasets, SynBench possesses three particular characteristics: (1) it is the first benchmark that provides various challenges for non-rigid point cloud registration, (2) SynBench encompasses challenges of varying difficulty levels, and (3) it includes ground truth corresponding points both before and after deformation. The authors believe that SynBench enables future non-rigid point cloud registration methods to present a fair comparison of their achievements. SynBench is publicly available at: https://doi.org/10.11588/data/R9IKCF.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Graphics, Machine Learning",
        "date": "2024-09-22 14:46:20 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.012780"
    },
    {
        "index": "#77",
        "title": "Low-Light Enhancement Effect on Classification and Detection: An Empirical Study",
        "link": "/arxiv/2409.14461",
        "arxiv_id": "2409.14461",
        "authors": "Xu Wu, Zhihui Lai, Zhou Jie, Can Gao, Xianxu Hou, Ya-nan Zhang, Linlin Shen",
        "summary": "Low-light images are commonly encountered in real-world scenarios, and numerous low-light image enhancement (LLIE) methods have been proposed to improve the visibility of these images. The primary goal of LLIE is to generate clearer images that are more visually pleasing to humans. However, the impact of LLIE methods in high-level vision tasks, such as image classification and object detection, which rely on high-quality image datasets, is not well {explored}. To explore the impact, we comprehensively evaluate LLIE methods on these high-level vision tasks by utilizing an empirical investigation comprising image classification and object detection experiments. The evaluation reveals a dichotomy: {\\textit{While Low-Light Image Enhancement (LLIE) methods enhance human visual interpretation, their effect on computer vision tasks is inconsistent and can sometimes be harmful. }} Our findings suggest a disconnect between image enhancement for human visual perception and for machine analysis, indicating a need for LLIE methods tailored to support high-level vision tasks effectively. This insight is crucial for the development of LLIE techniques that align with the needs of both human and machine vision.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 14:21:31 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.012986"
    },
    {
        "index": "#78",
        "title": "Fake It till You Make It: Curricular Dynamic Forgery Augmentations towards General Deepfake Detection",
        "link": "/arxiv/2409.14444",
        "arxiv_id": "2409.14444",
        "authors": "Yuzhen Lin, Wentang Song, Bin Li, Yuezun Li, Jiangqun Ni, Han Chen, Qiushi Li",
        "summary": "Previous studies in deepfake detection have shown promising results when testing face forgeries from the same dataset as the training. However, the problem remains challenging when one tries to generalize the detector to forgeries from unseen datasets and created by unseen methods. In this work, we present a novel general deepfake detection method, called \\textbf{C}urricular \\textbf{D}ynamic \\textbf{F}orgery \\textbf{A}ugmentation (CDFA), which jointly trains a deepfake detector with a forgery augmentation policy network. Unlike the previous works, we propose to progressively apply forgery augmentations following a monotonic curriculum during the training. We further propose a dynamic forgery searching strategy to select one suitable forgery augmentation operation for each image varying between training stages, producing a forgery augmentation policy optimized for better generalization. In addition, we propose a novel forgery augmentation named self-shifted blending image to simply imitate the temporal inconsistency of deepfake generation. Comprehensive experiments show that CDFA can significantly improve both cross-datasets and cross-manipulations performances of various naive deepfake detectors in a plug-and-play way, and make them attain superior performances over the existing methods in several benchmark datasets.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 13:51:22 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.013191"
    },
    {
        "index": "#79",
        "title": "EM-DARTS: Hierarchical Differentiable Architecture Search for Eye Movement Recognition",
        "link": "/arxiv/2409.14432",
        "arxiv_id": "2409.14432",
        "authors": "Huafeng Qin, Hongyu Zhu, Xin Jin, Xin Yu, Mounim A. El-Yacoubi, Xinbo Gao",
        "summary": "Eye movement biometrics has received increasing attention thanks to its high secure identification. Although deep learning (DL) models have been recently successfully applied for eye movement recognition, the DL architecture still is determined by human prior knowledge. Differentiable Neural Architecture Search (DARTS) automates the manual process of architecture design with high search efficiency. DARTS, however, usually stacks the same multiple learned cells to form a final neural network for evaluation, limiting therefore the diversity of the network. Incidentally, DARTS usually searches the architecture in a shallow network while evaluating it in a deeper one, which results in a large gap between the architecture depths in the search and evaluation scenarios. To address this issue, we propose EM-DARTS, a hierarchical differentiable architecture search algorithm to automatically design the DL architecture for eye movement recognition. First, we define a supernet and propose a global and local alternate Neural Architecture Search method to search the optimal architecture alternately with an differentiable neural architecture search. The local search strategy aims to find an optimal architecture for different cells while the global search strategy is responsible for optimizing the architecture of the target network. To further reduce redundancy, a transfer entropy is proposed to compute the information amount of each layer, so as to further simplify search network. Our experiments on three public databases demonstrate that the proposed EM-DARTS is capable of producing an optimal architecture that leads to state-of-the-art recognition performance.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 13:11:08 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.013432"
    },
    {
        "index": "#80",
        "title": "Pomo3D: 3D-Aware Portrait Accessorizing and More",
        "link": "/arxiv/2409.14430",
        "arxiv_id": "2409.14430",
        "authors": "Tzu-Chieh Liu, Chih-Ting Liu, Shao-Yi Chien",
        "summary": "We propose Pomo3D, a 3D portrait manipulation framework that allows free accessorizing by decomposing and recomposing portraits and accessories. It enables the avatars to attain out-of-distribution (OOD) appearances of simultaneously wearing multiple accessories. Existing methods still struggle to offer such explicit and fine-grained editing; they either fail to generate additional objects on given portraits or cause alterations to portraits (e.g., identity shift) when generating accessories. This restriction presents a noteworthy obstacle as people typically seek to create charming appearances with diverse and fashionable accessories in the virtual universe. Our approach provides an effective solution to this less-addressed issue. We further introduce the Scribble2Accessories module, enabling Pomo3D to create 3D accessories from user-drawn accessory scribble maps. Moreover, we design a bias-conscious mapper to mitigate biased associations present in real-world datasets. In addition to object-level manipulation above, Pomo3D also offers extensive editing options on portraits, including global or local editing of geometry and texture and avatar stylization, elevating 3D editing of neural portraits to a more comprehensive level.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-22 13:03:24 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.013642"
    },
    {
        "index": "#81",
        "title": "Prior Knowledge Distillation Network for Face Super-Resolution",
        "link": "/arxiv/2409.14385",
        "arxiv_id": "2409.14385",
        "authors": "Qiu Yang, Xiao Sun, Xin-yu Li, Feng-Qi Cui, Yu-Tong Guo, Shuang-Zhen Hu, Ping Luo, Si-Ying Li",
        "summary": "The purpose of face super-resolution (FSR) is to reconstruct high-resolution (HR) face images from low-resolution (LR) inputs. With the continuous advancement of deep learning technologies, contemporary prior-guided FSR methods initially estimate facial priors and then use this information to assist in the super-resolution reconstruction process. However, ensuring the accuracy of prior estimation remains challenging, and straightforward cascading and convolutional operations often fail to fully leverage prior knowledge. Inaccurate or insufficiently utilized prior information inevitably degrades FSR performance. To address this issue, we propose a prior knowledge distillation network (PKDN) for FSR, which involves transferring prior information from the teacher network to the student network. This approach enables the network to learn priors during the training stage while relying solely on low-resolution facial images during the testing stage, thus mitigating the adverse effects of prior estimation inaccuracies. Additionally, we incorporate robust attention mechanisms to design a parsing map fusion block that effectively utilizes prior information. To prevent feature loss, we retain multi-scale features during the feature extraction stage and employ them in the subsequent super-resolution reconstruction process. Experimental results on benchmark datasets demonstrate that our PKDN approach surpasses existing FSR methods in generating high-quality face images.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 09:58:20 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.013907"
    },
    {
        "index": "#82",
        "title": "GroupDiff: Diffusion-based Group Portrait Editing",
        "link": "/arxiv/2409.14379",
        "arxiv_id": "2409.14379",
        "authors": "Yuming Jiang, Nanxuan Zhao, Qing Liu, Krishna Kumar Singh, Shuai Yang, Chen Change Loy, Ziwei Liu",
        "summary": "Group portrait editing is highly desirable since users constantly want to add a person, delete a person, or manipulate existing persons. It is also challenging due to the intricate dynamics of human interactions and the diverse gestures. In this work, we present GroupDiff, a pioneering effort to tackle group photo editing with three dedicated contributions: 1) Data Engine: Since there is no labeled data for group photo editing, we create a data engine to generate paired data for training. The training data engine covers the diverse needs of group portrait editing. 2) Appearance Preservation: To keep the appearance consistent after editing, we inject the images of persons from the group photo into the attention modules and employ skeletons to provide intra-person guidance. 3) Control Flexibility: Bounding boxes indicating the locations of each person are used to reweight the attention matrix so that the features of each person can be injected into the correct places. This inter-person guidance provides flexible manners for manipulation. Extensive experiments demonstrate that GroupDiff exhibits state-of-the-art performance compared to existing methods. GroupDiff offers controllability for editing and maintains the fidelity of the original photos.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 09:50:28 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.014169"
    },
    {
        "index": "#83",
        "title": "Memory Matching is not Enough: Jointly Improving Memory Matching and Decoding for Video Object Segmentation",
        "link": "/arxiv/2409.14343",
        "arxiv_id": "2409.14343",
        "authors": "Jintu Zheng, Yun Liang, Yuqing Zhang, Wanchao Su",
        "summary": "Memory-based video object segmentation methods model multiple objects over long temporal-spatial spans by establishing memory bank, which achieve the remarkable performance. However, they struggle to overcome the false matching and are prone to lose critical information, resulting in confusion among different objects. In this paper, we propose an effective approach which jointly improving the matching and decoding stages to alleviate the false matching issue.For the memory matching stage, we present a cost aware mechanism that suppresses the slight errors for short-term memory and a shunted cross-scale matching for long-term memory which establish a wide filed matching spaces for various object scales. For the readout decoding stage, we implement a compensatory mechanism aims at recovering the essential information where missing at the matching stage. Our approach achieves the outstanding performance in several popular benchmarks (i.e., DAVIS 2016&2017 Val (92.4%&88.1%), and DAVIS 2017 Test (83.9%)), and achieves 84.8%&84.6% on YouTubeVOS 2018&2019 Val.",
        "subjects": "Computer Vision and Pattern Recognition, Image and Video Processing",
        "date": "2024-09-22 07:08:59 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.014364"
    },
    {
        "index": "#84",
        "title": "Self-Supervised Audio-Visual Soundscape Stylization",
        "link": "/arxiv/2409.14340",
        "arxiv_id": "2409.14340",
        "authors": "Tingle Li, Renhao Wang, Po-Yao Huang, Andrew Owens, Gopala Anumanchipalli",
        "summary": "Speech sounds convey a great deal of information about the scenes, resulting in a variety of effects ranging from reverberation to additional ambient sounds. In this paper, we manipulate input speech to sound as though it was recorded within a different scene, given an audio-visual conditional example recorded from that scene. Our model learns through self-supervision, taking advantage of the fact that natural video contains recurring sound events and textures. We extract an audio clip from a video and apply speech enhancement. We then train a latent diffusion model to recover the original speech, using another audio-visual clip taken from elsewhere in the video as a conditional hint. Through this process, the model learns to transfer the conditional example's sound properties to the input speech. We show that our model can be successfully trained using unlabeled, in-the-wild videos, and that an additional visual signal can improve its sound prediction abilities. Please see our project webpage for video results: https://tinglok.netlify.app/files/avsoundscape/",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning, Multimedia, Sound, Audio and Speech Processing",
        "date": "2024-09-22 06:57:33 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.014624"
    },
    {
        "index": "#85",
        "title": "Zero-Shot Skeleton-based Action Recognition with Dual Visual-Text Alignment",
        "link": "/arxiv/2409.14336",
        "arxiv_id": "2409.14336",
        "authors": "Jidong Kuang, Hongsong Wang, Chaolei Han, Jie Gui",
        "summary": "Zero-shot action recognition, which addresses the issue of scalability and generalization in action recognition and allows the models to adapt to new and unseen actions dynamically, is an important research topic in computer vision communities. The key to zero-shot action recognition lies in aligning visual features with semantic vectors representing action categories. Most existing methods either directly project visual features onto the semantic space of text category or learn a shared embedding space between the two modalities. However, a direct projection cannot accurately align the two modalities, and learning robust and discriminative embedding space between visual and text representations is often difficult. To address these issues, we introduce Dual Visual-Text Alignment (DVTA) for skeleton-based zero-shot action recognition. The DVTA consists of two alignment modules-Direct Alignment (DA) and Augmented Alignment (AA)-along with a designed Semantic Description Enhancement (SDE). The DA module maps the skeleton features to the semantic space through a specially designed visual projector, followed by the SDE, which is based on cross-attention to enhance the connection between skeleton and text, thereby reducing the gap between modalities. The AA module further strengthens the learning of the embedding space by utilizing deep metric learning to learn the similarity between skeleton and text. Our approach achieves state-of-the-art performances on several popular zero-shot skeleton-based action recognition benchmarks.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 06:44:58 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.014847"
    },
    {
        "index": "#86",
        "title": "PISR: Polarimetric Neural Implicit Surface Reconstruction for Textureless and Specular Objects",
        "link": "/arxiv/2409.14331",
        "arxiv_id": "2409.14331",
        "authors": "Guangcheng Chen, Yicheng He, Li He, Hong Zhang",
        "summary": "Neural implicit surface reconstruction has achieved remarkable progress recently. Despite resorting to complex radiance modeling, state-of-the-art methods still struggle with textureless and specular surfaces. Different from RGB images, polarization images can provide direct constraints on the azimuth angles of the surface normals. In this paper, we present PISR, a novel method that utilizes a geometrically accurate polarimetric loss to refine shape independently of appearance. In addition, PISR smooths surface normals in image space to eliminate severe shape distortions and leverages the hash-grid-based neural signed distance function to accelerate the reconstruction. Experimental results demonstrate that PISR achieves higher accuracy and robustness, with an L1 Chamfer distance of 0.5 mm and an F-score of 99.5% at 1 mm, while converging 4~30 times faster than previous polarimetric surface reconstruction methods.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 06:31:11 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.015034"
    },
    {
        "index": "#87",
        "title": "Scene-Text Grounding for Text-Based Video Question Answering",
        "link": "/arxiv/2409.14319",
        "arxiv_id": "2409.14319",
        "authors": "Sheng Zhou, Junbin Xiao, Xun Yang, Peipei Song, Dan Guo, Angela Yao, Meng Wang, Tat-Seng Chua",
        "summary": "Existing efforts in text-based video question answering (TextVideoQA) are criticized for their opaque decisionmaking and heavy reliance on scene-text recognition. In this paper, we propose to study Grounded TextVideoQA by forcing models to answer questions and spatio-temporally localize the relevant scene-text regions, thus decoupling QA from scenetext recognition and promoting research towards interpretable QA. The task has three-fold significance. First, it encourages scene-text evidence versus other short-cuts for answer predictions. Second, it directly accepts scene-text regions as visual answers, thus circumventing the problem of ineffective answer evaluation by stringent string matching. Third, it isolates the challenges inherited in VideoQA and scene-text recognition. This enables the diagnosis of the root causes for failure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve Grounded TextVideoQA, we propose the T2S-QA model that highlights a disentangled temporal-to-spatial contrastive learning strategy for weakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate evaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text bounding boxes within 2.2K temporal segments related to 2K questions and 729 videos. With ViTXT-GQA, we perform extensive experiments and demonstrate the severe limitations of existing techniques in Grounded TextVideoQA. While T2S-QA achieves superior results, the large performance gap with human leaves ample space for improvement. Our further analysis of oracle scene-text inputs posits that the major challenge is scene-text recognition. To advance the research of Grounded TextVideoQA, our dataset and code are at \\url{https://github.com/zhousheng97/ViTXT-GQA.git}",
        "subjects": "Computer Vision and Pattern Recognition, Multimedia",
        "date": "2024-09-22 05:13:11 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.015257"
    },
    {
        "index": "#88",
        "title": "MVPGS: Excavating Multi-view Priors for Gaussian Splatting from Sparse Input Views",
        "link": "/arxiv/2409.14316",
        "arxiv_id": "2409.14316",
        "authors": "Wangze Xu, Huachen Gao, Shihe Shen, Rui Peng, Jianbo Jiao, Ronggang Wang",
        "summary": "Recently, the Neural Radiance Field (NeRF) advancement has facilitated few-shot Novel View Synthesis (NVS), which is a significant challenge in 3D vision applications. Despite numerous attempts to reduce the dense input requirement in NeRF, it still suffers from time-consumed training and rendering processes. More recently, 3D Gaussian Splatting (3DGS) achieves real-time high-quality rendering with an explicit point-based representation. However, similar to NeRF, it tends to overfit the train views for lack of constraints. In this paper, we propose \\textbf{MVPGS}, a few-shot NVS method that excavates the multi-view priors based on 3D Gaussian Splatting. We leverage the recent learning-based Multi-view Stereo (MVS) to enhance the quality of geometric initialization for 3DGS. To mitigate overfitting, we propose a forward-warping method for additional appearance constraints conforming to scenes based on the computed geometry. Furthermore, we introduce a view-consistent geometry constraint for Gaussian parameters to facilitate proper optimization convergence and utilize a monocular depth regularization as compensation. Experiments show that the proposed method achieves state-of-the-art performance with real-time rendering speed. Project page: https://zezeaaa.github.io/projects/MVPGS/",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 05:07:20 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.015476"
    },
    {
        "index": "#89",
        "title": "Anisotropic Diffusion Probabilistic Model for Imbalanced Image Classification",
        "link": "/arxiv/2409.14313",
        "arxiv_id": "2409.14313",
        "authors": "Jingyu Kong, Yuan Guo, Yu Wang, Yuping Duan",
        "summary": "Real-world data often has a long-tailed distribution, where the scarcity of tail samples significantly limits the model's generalization ability. Denoising Diffusion Probabilistic Models (DDPM) are generative models based on stochastic differential equation theory and have demonstrated impressive performance in image classification tasks. However, existing diffusion probabilistic models do not perform satisfactorily in classifying tail classes. In this work, we propose the Anisotropic Diffusion Probabilistic Model (ADPM) for imbalanced image classification problems. We utilize the data distribution to control the diffusion speed of different class samples during the forward process, effectively improving the classification accuracy of the denoiser in the reverse process. Specifically, we provide a theoretical strategy for selecting noise levels for different categories in the diffusion process based on error analysis theory to address the imbalanced classification problem. Furthermore, we integrate global and local image prior in the forward process to enhance the model's discriminative ability in the spatial dimension, while incorporate semantic-level contextual information in the reverse process to boost the model's discriminative power and robustness. Through comparisons with state-of-the-art methods on four medical benchmark datasets, we validate the effectiveness of the proposed method in handling long-tail data. Our results confirm that the anisotropic diffusion model significantly improves the classification accuracy of rare classes while maintaining the accuracy of head classes. On the skin lesion datasets, PAD-UFES and HAM10000, the F1-scores of our method improved by 4% and 3%, respectively compared to the original diffusion probabilistic model.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 04:42:52 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.015718"
    },
    {
        "index": "#90",
        "title": "DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation",
        "link": "/arxiv/2409.14307",
        "arxiv_id": "2409.14307",
        "authors": "Xuewen Liu, Zhikai Li, Qingyi Gu",
        "summary": "Diffusion models have shown excellent performance on various image generation tasks, but the substantial computational costs and huge memory footprint hinder their low-latency applications in real-world scenarios. Quantization is a promising way to compress and accelerate models. Nevertheless, due to the wide range and time-varying activations in diffusion models, existing methods cannot maintain both accuracy and efficiency simultaneously for low-bit quantization. To tackle this issue, we propose DilateQuant, a novel quantization framework for diffusion models that offers comparable accuracy and high efficiency. Specifically, we keenly aware of numerous unsaturated in-channel weights, which can be cleverly exploited to reduce the range of activations without additional computation cost. Based on this insight, we propose Weight Dilation (WD) that maximally dilates the unsaturated in-channel weights to a constrained range through a mathematically equivalent scaling. WD costlessly absorbs the activation quantization errors into weight quantization. The range of activations decreases, which makes activations quantization easy. The range of weights remains constant, which makes model easy to converge in training stage. Considering the temporal network leads to time-varying activations, we design a Temporal Parallel Quantizer (TPQ), which sets time-step quantization parameters and supports parallel quantization for different time steps, significantly improving the performance and reducing time cost. To further enhance performance while preserving efficiency, we introduce a Block-wise Knowledge Distillation (BKD) to align the quantized models with the full-precision models at a block level. The simultaneous training of time-step quantization parameters and weights minimizes the time required, and the shorter backpropagation paths decreases the memory footprint of the quantization process.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-22 04:21:29 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.015934"
    },
    {
        "index": "#91",
        "title": "Deep Learning Technology for Face Forgery Detection: A Survey",
        "link": "/arxiv/2409.14289",
        "arxiv_id": "2409.14289",
        "authors": "Lixia Ma, Puning Yang, Yuting Xu, Ziming Yang, Peipei Li, Huaibo Huang",
        "summary": "Currently, the rapid development of computer vision and deep learning has enabled the creation or manipulation of high-fidelity facial images and videos via deep generative approaches. This technology, also known as deepfake, has achieved dramatic progress and become increasingly popular in social media. However, the technology can generate threats to personal privacy and national security by spreading misinformation. To diminish the risks of deepfake, it is desirable to develop powerful forgery detection methods to distinguish fake faces from real faces. This paper presents a comprehensive survey of recent deep learning-based approaches for facial forgery detection. We attempt to provide the reader with a deeper understanding of the current advances as well as the major challenges for deepfake detection based on deep learning. We present an overview of deepfake techniques and analyse the characteristics of various deepfake datasets. We then provide a systematic review of different categories of deepfake detection and state-of-the-art deepfake detection methods. The drawbacks of existing detection methods are analyzed, and future research directions are discussed to address the challenges in improving both the performance and generalization of deepfake detection.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 01:42:01 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.021369"
    },
    {
        "index": "#92",
        "title": "Lidar Panoptic Segmentation in an Open World",
        "link": "/arxiv/2409.14273",
        "arxiv_id": "2409.14273",
        "authors": "Anirudh S Chakravarthy, Meghana Reddy Ganesina, Peiyun Hu, Laura Leal-Taixe, Shu Kong, Deva Ramanan, Aljosa Osep",
        "summary": "Addressing Lidar Panoptic Segmentation (LPS ) is crucial for safe deployment of autonomous vehicles. LPS aims to recognize and segment lidar points w.r.t. a pre-defined vocabulary of semantic classes, including thing classes of countable objects (e.g., pedestrians and vehicles) and stuff classes of amorphous regions (e.g., vegetation and road). Importantly, LPS requires segmenting individual thing instances (e.g., every single vehicle). Current LPS methods make an unrealistic assumption that the semantic class vocabulary is fixed in the real open world, but in fact, class ontologies usually evolve over time as robots encounter instances of novel classes that are considered to be unknowns w.r.t. the pre-defined class vocabulary. To address this unrealistic assumption, we study LPS in the Open World (LiPSOW): we train models on a dataset with a pre-defined semantic class vocabulary and study their generalization to a larger dataset where novel instances of thing and stuff classes can appear. This experimental setting leads to interesting conclusions. While prior art train class-specific instance segmentation methods and obtain state-of-the-art results on known classes, methods based on class-agnostic bottom-up grouping perform favorably on classes outside of the initial class vocabulary (i.e., unknown classes). Unfortunately, these methods do not perform on-par with fully data-driven methods on known classes. Our work suggests a middle ground: we perform class-agnostic point clustering and over-segment the input cloud in a hierarchical fashion, followed by binary point segment classification, akin to Region Proposal Network [1]. We obtain the final point cloud segmentation by computing a cut in the weighted hierarchical tree of point segments, independently of semantic classification. Remarkably, this unified approach leads to strong performance on both known and unknown classes.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-22 00:10:20 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.021629"
    },
    {
        "index": "#93",
        "title": "Combining Absolute and Semi-Generalized Relative Poses for Visual Localization",
        "link": "/arxiv/2409.14269",
        "arxiv_id": "2409.14269",
        "authors": "Vojtech Panek, Torsten Sattler, Zuzana Kukelova",
        "summary": "Visual localization is the problem of estimating the camera pose of a given query image within a known scene. Most state-of-the-art localization approaches follow the structure-based paradigm and use 2D-3D matches between pixels in a query image and 3D points in the scene for pose estimation. These approaches assume an accurate 3D model of the scene, which might not always be available, especially if only a few images are available to compute the scene representation. In contrast, structure-less methods rely on 2D-2D matches and do not require any 3D scene model. However, they are also less accurate than structure-based methods. Although one prior work proposed to combine structure-based and structure-less pose estimation strategies, its practical relevance has not been shown. We analyze combining structure-based and structure-less strategies while exploring how to select between poses obtained from 2D-2D and 2D-3D matches, respectively. We show that combining both strategies improves localization performance in multiple practically relevant scenarios.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 23:55:42 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.021825"
    },
    {
        "index": "#94",
        "title": "End to End Face Reconstruction via Differentiable PnP",
        "link": "/arxiv/2409.14249",
        "arxiv_id": "2409.14249",
        "authors": "Yiren Lu, Huawei Wei",
        "summary": "This is a challenge report of the ECCV 2022 WCPA Challenge, Face Reconstruction Track. Inside this report is a brief explanation of how we accomplish this challenge. We design a two-branch network to accomplish this task, whose roles are Face Reconstruction and Face Landmark Detection. The former outputs canonical 3D face coordinates. The latter outputs pixel coordinates, i.e. 2D mapping of 3D coordinates with head pose and perspective projection. In addition, we utilize a differentiable PnP (Perspective-n-Points) layer to finetune the outputs of the two branch. Our method achieves very competitive quantitative results on the MVP-Human dataset and wins a $3^{rd}$ prize in the challenge.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 21:30:24 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.021995"
    },
    {
        "index": "#95",
        "title": "Cloud Adversarial Example Generation for Remote Sensing Image Classification",
        "link": "/arxiv/2409.14240",
        "arxiv_id": "2409.14240",
        "authors": "Fei Ma, Yuqiang Feng, Fan Zhang, Yongsheng Zhou",
        "summary": "Most existing adversarial attack methods for remote sensing images merely add adversarial perturbations or patches, resulting in unnatural modifications. Clouds are common atmospheric effects in remote sensing images. Generating clouds on these images can produce adversarial examples better aligning with human perception. In this paper, we propose a Perlin noise based cloud generation attack method. Common Perlin noise based cloud generation is a random, non-optimizable process, which cannot be directly used to attack the target models. We design a Perlin Gradient Generator Network (PGGN), which takes a gradient parameter vector as input and outputs the grids of Perlin noise gradient vectors at different scales. After a series of computations based on the gradient vectors, cloud masks at corresponding scales can be produced. These cloud masks are then weighted and summed depending on a mixing coefficient vector and a scaling factor to produce the final cloud masks. The gradient vector, coefficient vector and scaling factor are collectively represented as a cloud parameter vector, transforming the cloud generation into a black-box optimization problem. The Differential Evolution (DE) algorithm is employed to solve for the optimal solution of the cloud parameter vector, achieving a query-based black-box attack. Detailed experiments confirm that this method has strong attack capabilities and achieves high query efficiency. Additionally, we analyze the transferability of the generated adversarial examples and their robustness in adversarial defense scenarios.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 20:15:22 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.022181"
    },
    {
        "index": "#96",
        "title": "Masks and Boxes: Combining the Best of Both Worlds for Multi-Object Tracking",
        "link": "/arxiv/2409.14220",
        "arxiv_id": "2409.14220",
        "authors": "Tomasz Stanczyk, Francois Bremond",
        "summary": "Multi-object tracking (MOT) involves identifying and consistently tracking objects across video sequences. Traditional tracking-by-detection methods, while effective, often require extensive tuning and lack generalizability. On the other hand, segmentation mask-based methods are more generic but struggle with tracking management, making them unsuitable for MOT. We propose a novel approach, McByte, which incorporates a temporally propagated segmentation mask as a strong association cue within a tracking-by-detection framework. By combining bounding box and mask information, McByte enhances robustness and generalizability without per-sequence tuning. Evaluated on four benchmark datasets - DanceTrack, MOT17, SoccerNet-tracking 2022, and KITTI-tracking - McByte demonstrates performance gain in all cases examined. At the same time, it outperforms existing mask-based methods. Implementation code will be provided upon acceptance.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 18:52:07 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.022350"
    },
    {
        "index": "#97",
        "title": "@Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology",
        "link": "/arxiv/2409.14215",
        "arxiv_id": "2409.14215",
        "authors": "Xin Jiang, Junwei Zheng, Ruiping Liu, Jiahang Li, Jiaming Zhang, Sven Matthiesen, Rainer Stiefelhagen",
        "summary": "As Vision-Language Models (VLMs) advance, human-centered Assistive Technologies (ATs) for helping People with Visual Impairments (PVIs) are evolving into generalists, capable of performing multiple tasks simultaneously. However, benchmarking VLMs for ATs remains under-explored. To bridge this gap, we first create a novel AT benchmark (@Bench). Guided by a pre-design user study with PVIs, our benchmark includes the five most crucial vision-language tasks: Panoptic Segmentation, Depth Estimation, Optical Character Recognition (OCR), Image Captioning, and Visual Question Answering (VQA). Besides, we propose a novel AT model (@Model) that addresses all tasks simultaneously and can be expanded to more assistive functions for helping PVIs. Our framework exhibits outstanding performance across tasks by integrating multi-modal information, and it offers PVIs a more comprehensive assistance. Extensive experiments prove the effectiveness and generalizability of our framework.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 18:30:17 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.022551"
    },
    {
        "index": "#98",
        "title": "Egocentric zone-aware action recognition across environments",
        "link": "/arxiv/2409.14205",
        "arxiv_id": "2409.14205",
        "authors": "Simone Alberto Peirone, Gabriele Goletto, Mirco Planamente, Andrea Bottino, Barbara Caputo, Giuseppe Averta",
        "summary": "Human activities exhibit a strong correlation between actions and the places where these are performed, such as washing something at a sink. More specifically, in daily living environments we may identify particular locations, hereinafter named activity-centric zones, which may afford a set of homogeneous actions. Their knowledge can serve as a prior to favor vision models to recognize human activities. However, the appearance of these zones is scene-specific, limiting the transferability of this prior information to unfamiliar areas and domains. This problem is particularly relevant in egocentric vision, where the environment takes up most of the image, making it even more difficult to separate the action from the context. In this paper, we discuss the importance of decoupling the domain-specific appearance of activity-centric zones from their universal, domain-agnostic representations, and show how the latter can improve the cross-domain transferability of Egocentric Action Recognition (EAR) models. We validate our solution on the EPIC-Kitchens-100 and Argo1M datasets",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 17:40:48 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.022744"
    },
    {
        "index": "#99",
        "title": "LATTE: Improving Latex Recognition for Tables and Formulae with Iterative Refinement",
        "link": "/arxiv/2409.14201",
        "arxiv_id": "2409.14201",
        "authors": "Nan Jiang, Shanchao Liang, Chengxiao Wang, Jiannan Wang, Lin Tan",
        "summary": "Portable Document Format (PDF) files are dominantly used for storing and disseminating scientific research, legal documents, and tax information. LaTeX is a popular application for creating PDF documents. Despite its advantages, LaTeX is not WYSWYG -- what you see is what you get, i.e., the LaTeX source and rendered PDF images look drastically different, especially for formulae and tables. This gap makes it hard to modify or export LaTeX sources for formulae and tables from PDF images, and existing work is still limited. First, prior work generates LaTeX sources in a single iteration and struggles with complex LaTeX formulae. Second, existing work mainly recognizes and extracts LaTeX sources for formulae; and is incapable or ineffective for tables. This paper proposes LATTE, the first iterative refinement framework for LaTeX recognition. Specifically, we propose delta-view as feedback, which compares and pinpoints the differences between a pair of rendered images of the extracted LaTeX source and the expected correct image. Such delta-view feedback enables our fault localization model to localize the faulty parts of the incorrect recognition more accurately and enables our LaTeX refinement model to repair the incorrect extraction more accurately. LATTE improves the LaTeX source extraction accuracy of both LaTeX formulae and tables, outperforming existing techniques as well as GPT-4V by at least 7.07% of exact match, with a success refinement rate of 46.08% (formula) and 25.51% (table).",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 17:18:49 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.022926"
    },
    {
        "index": "#100",
        "title": "Content-aware Tile Generation using Exterior Boundary Inpainting",
        "link": "/arxiv/2409.14184",
        "arxiv_id": "2409.14184",
        "authors": "Sam Sartor, Pieter Peers",
        "summary": "We present a novel and flexible learning-based method for generating tileable image sets. Our method goes beyond simple self-tiling, supporting sets of mutually tileable images that exhibit a high degree of diversity. To promote diversity we decouple structure from content by foregoing explicit copying of patches from an exemplar image. Instead we leverage the prior knowledge of natural images and textures embedded in large-scale pretrained diffusion models to guide tile generation constrained by exterior boundary conditions and a text prompt to specify the content. By carefully designing and selecting the exterior boundary conditions, we can reformulate the tile generation process as an inpainting problem, allowing us to directly employ existing diffusion-based inpainting models without the need to retrain a model on a custom training set. We demonstrate the flexibility and efficacy of our content-aware tile generation method on different tiling schemes, such as Wang tiles, from only a text prompt. Furthermore, we introduce a novel Dual Wang tiling scheme that provides greater texture continuity and diversity than existing Wang tile variants.",
        "subjects": "Computer Vision and Pattern Recognition, Graphics",
        "date": "2024-09-21 16:04:13 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.023132"
    },
    {
        "index": "#101",
        "title": "LFP: Efficient and Accurate End-to-End Lane-Level Planning via Camera-LiDAR Fusion",
        "link": "/arxiv/2409.14170",
        "arxiv_id": "2409.14170",
        "authors": "Guoliang You, Xiaomeng Chu, Yifan Duan, Xingchen Li, Sha Zhang, Jianmin Ji, Yanyong Zhang",
        "summary": "Multi-modal systems enhance performance in autonomous driving but face inefficiencies due to indiscriminate processing within each modality. Additionally, the independent feature learning of each modality lacks interaction, which results in extracted features that do not possess the complementary characteristics. These issue increases the cost of fusing redundant information across modalities. To address these challenges, we propose targeting driving-relevant elements, which reduces the volume of LiDAR features while preserving critical information. This approach enhances lane level interaction between the image and LiDAR branches, allowing for the extraction and fusion of their respective advantageous features. Building upon the camera-only framework PHP, we introduce the Lane-level camera-LiDAR Fusion Planning (LFP) method, which balances efficiency with performance by using lanes as the unit for sensor fusion. Specifically, we design three modules to enhance efficiency and performance. For efficiency, we propose an image-guided coarse lane prior generation module that forecasts the region of interest (ROI) for lanes and assigns a confidence score, guiding LiDAR processing. The LiDAR feature extraction modules leverages lane-aware priors from the image branch to guide sampling for pillar, retaining essential pillars. For performance, the lane-level cross-modal query integration and feature enhancement module uses confidence score from ROI to combine low-confidence image queries with LiDAR queries, extracting complementary depth features. These features enhance the low-confidence image features, compensating for the lack of depth. Experiments on the Carla benchmarks show that our method achieves state-of-the-art performance in both driving score and infraction score, with maximum improvement of 15% and 14% over existing algorithms, respectively, maintaining high frame rate of 19.27 FPS.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 15:22:01 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.023348"
    },
    {
        "index": "#102",
        "title": "PromptTA: Prompt-driven Text Adapter for Source-free Domain Generalization",
        "link": "/arxiv/2409.14163",
        "arxiv_id": "2409.14163",
        "authors": "Haoran Zhang, Shuanghao Bai, Wanqi Zhou, Jingwen Fu, Badong Chen",
        "summary": "Source-free domain generalization (SFDG) tackles the challenge of adapting models to unseen target domains without access to source domain data. To deal with this challenging task, recent advances in SFDG have primarily focused on leveraging the text modality of vision-language models such as CLIP. These methods involve developing a transferable linear classifier based on diverse style features extracted from the text and learned prompts or deriving domain-unified text representations from domain banks. However, both style features and domain banks have limitations in capturing comprehensive domain knowledge. In this work, we propose Prompt-Driven Text Adapter (PromptTA) method, which is designed to better capture the distribution of style features and employ resampling to ensure thorough coverage of domain knowledge. To further leverage this rich domain information, we introduce a text adapter that learns from these style features for efficient domain information storage. Extensive experiments conducted on four benchmark datasets demonstrate that PromptTA achieves state-of-the-art performance. The code is available at https://github.com/zhanghr2001/PromptTA.",
        "subjects": "Computer Vision and Pattern Recognition, Computation and Language, Machine Learning",
        "date": "2024-09-21 15:02:13 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.023575"
    },
    {
        "index": "#103",
        "title": "MSSDA: Multi-Sub-Source Adaptation for Diabetic Foot Neuropathy Recognition",
        "link": "/arxiv/2409.14154",
        "arxiv_id": "2409.14154",
        "authors": "Yan Zhong, Zhixin Yan, Yi Xie, Shibin Wu, Huaidong Zhang, Lin Shu, Peiru Zhou",
        "summary": "Diabetic foot neuropathy (DFN) is a critical factor leading to diabetic foot ulcers, which is one of the most common and severe complications of diabetes mellitus (DM) and is associated with high risks of amputation and mortality. Despite its significance, existing datasets do not directly derive from plantar data and lack continuous, long-term foot-specific information. To advance DFN research, we have collected a novel dataset comprising continuous plantar pressure data to recognize diabetic foot neuropathy. This dataset includes data from 94 DM patients with DFN and 41 DM patients without DFN. Moreover, traditional methods divide datasets by individuals, potentially leading to significant domain discrepancies in some feature spaces due to the absence of mid-domain data. In this paper, we propose an effective domain adaptation method to address this proplem. We split the dataset based on convolutional feature statistics and select appropriate sub-source domains to enhance efficiency and avoid negative transfer. We then align the distributions of each source and target domain pair in specific feature spaces to minimize the domain gap. Comprehensive results validate the effectiveness of our method on both the newly proposed dataset for DFN recognition and an existing dataset.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-21 14:16:20 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.023904"
    },
    {
        "index": "#104",
        "title": "JVID: Joint Video-Image Diffusion for Visual-Quality and Temporal-Consistency in Video Generation",
        "link": "/arxiv/2409.14149",
        "arxiv_id": "2409.14149",
        "authors": "Hadrien Reynaud, Matthew Baugh, Mischa Dombrowski, Sarah Cechnicka, Qingjie Meng, Bernhard Kainz",
        "summary": "We introduce the Joint Video-Image Diffusion model (JVID), a novel approach to generating high-quality and temporally coherent videos. We achieve this by integrating two diffusion models: a Latent Image Diffusion Model (LIDM) trained on images and a Latent Video Diffusion Model (LVDM) trained on video data. Our method combines these models in the reverse diffusion process, where the LIDM enhances image quality and the LVDM ensures temporal consistency. This unique combination allows us to effectively handle the complex spatio-temporal dynamics in video generation. Our results demonstrate quantitative and qualitative improvements in producing realistic and coherent videos.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 13:59:50 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.024122"
    },
    {
        "index": "#105",
        "title": "A Feature Generator for Few-Shot Learning",
        "link": "/arxiv/2409.14141",
        "arxiv_id": "2409.14141",
        "authors": "Heethanjan Kanagalingam, Thenukan Pathmanathan, Navaneethan Ketheeswaran, Mokeeshan Vathanakumar, Mohamed Afham, Ranga Rodrigo",
        "summary": "Few-shot learning (FSL) aims to enable models to recognize novel objects or classes with limited labelled data. Feature generators, which synthesize new data points to augment limited datasets, have emerged as a promising solution to this challenge. This paper investigates the effectiveness of feature generators in enhancing the embedding process for FSL tasks. To address the issue of inaccurate embeddings due to the scarcity of images per class, we introduce a feature generator that creates visual features from class-level textual descriptions. By training the generator with a combination of classifier loss, discriminator loss, and distance loss between the generated features and true class embeddings, we ensure the generation of accurate same-class features and enhance the overall feature representation. Our results show a significant improvement in accuracy over baseline methods, with our approach outperforming the baseline model by 10% in 1-shot and around 5% in 5-shot approaches. Additionally, both visual-only and visual + textual generators have also been tested in this paper.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 13:31:12 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.024344"
    },
    {
        "index": "#106",
        "title": "Present and Future Generalization of Synthetic Image Detectors",
        "link": "/arxiv/2409.14128",
        "arxiv_id": "2409.14128",
        "authors": "Pablo Bernabeu-Perez, Enrique Lopez-Cuena, Dario Garcia-Gasulla",
        "summary": "The continued release of new and better image generation models increases the demand for synthetic image detectors. In such a dynamic field, detectors need to be able to generalize widely and be robust to uncontrolled alterations. The present work is motivated by this setting, when looking at the role of time, image transformations and data sources, for detector generalization. In these experiments, none of the evaluated detectors is found universal, but results indicate an ensemble could be. Experiments on data collected in the wild show this task to be more challenging than the one defined by large-scale datasets, pointing to a gap between experimentation and actual practice. Finally, we observe a race equilibrium effect, where better generators lead to better detectors, and vice versa. We hypothesize this pushes the field towards a perpetually close race between generators and detectors.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning",
        "date": "2024-09-21 12:46:17 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.024546"
    },
    {
        "index": "#107",
        "title": "Local Patterns Generalize Better for Novel Anomalies",
        "link": "/arxiv/2409.14109",
        "arxiv_id": "2409.14109",
        "authors": "Yalong Jiang, Liquan Mao",
        "summary": "Video anomaly detection (VAD) aims at identifying novel actions or events which are unseen during training. Existing mainstream VAD techniques focus on the global patterns of events and cannot properly generalize to novel samples. In this paper, we propose a framework to identify the spatial local patterns which generalize to novel samples and model the dynamics of local patterns. In spatial part of the framework, the capability of extracting local patterns is gained from image-text contrastive learning with Image-Text Alignment Module (ITAM). To detect different types of anomalies, a two-branch framework is proposed for representing the local patterns in both actions and appearances. In temporal part of the framework, a State Machine Module (SMM) is proposed to model the dynamics of local patterns by decomposing their temporal variations into motion components. Different dynamics are represented with different weighted sums of a fixed set of motion components. The video sequences with either novel spatial distributions of local patterns or distinctive dynamics of local patterns are deemed as anomalies. Extensive experiments on popular benchmark datasets demonstrate that state-of-the-art performance can be achieved.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 11:48:54 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.024718"
    },
    {
        "index": "#108",
        "title": "ExFMan: Rendering 3D Dynamic Humans with Hybrid Monocular Blurry Frames and Events",
        "link": "/arxiv/2409.14103",
        "arxiv_id": "2409.14103",
        "authors": "Kanghao Chen, Zeyu Wang, Lin Wang",
        "summary": "Recent years have witnessed tremendous progress in the 3D reconstruction of dynamic humans from a monocular video with the advent of neural rendering techniques. This task has a wide range of applications, including the creation of virtual characters for virtual reality (VR) environments. However, it is still challenging to reconstruct clear humans when the monocular video is affected by motion blur, particularly caused by rapid human motion (e.g., running, dancing), as often occurs in the wild. This leads to distinct inconsistency of shape and appearance for the rendered 3D humans, especially in the blurry regions with rapid motion, e.g., hands and legs. In this paper, we propose ExFMan, the first neural rendering framework that unveils the possibility of rendering high-quality humans in rapid motion with a hybrid frame-based RGB and bio-inspired event camera. The ``out-of-the-box'' insight is to leverage the high temporal information of event data in a complementary manner and adaptively reweight the effect of losses for both RGB frames and events in the local regions, according to the velocity of the rendered human. This significantly mitigates the inconsistency associated with motion blur in the RGB frames. Specifically, we first formulate a velocity field of the 3D body in the canonical space and render it to image space to identify the body parts with motion blur. We then propose two novel losses, i.e., velocity-aware photometric loss and velocity-relative event loss, to optimize the neural human for both modalities under the guidance of the estimated velocity. In addition, we incorporate novel pose regularization and alpha losses to facilitate continuous pose and clear boundary. Extensive experiments on synthetic and real-world datasets demonstrate that ExFMan can reconstruct sharper and higher quality humans.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 10:58:01 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.024898"
    },
    {
        "index": "#109",
        "title": "PoseAugment: Generative Human Pose Data Augmentation with Physical Plausibility for IMU-based Motion Capture",
        "link": "/arxiv/2409.14101",
        "arxiv_id": "2409.14101",
        "authors": "Zhuojun Li, Chun Yu, Chen Liang, Yuanchun Shi",
        "summary": "The data scarcity problem is a crucial factor that hampers the model performance of IMU-based human motion capture. However, effective data augmentation for IMU-based motion capture is challenging, since it has to capture the physical relations and constraints of the human body, while maintaining the data distribution and quality. We propose PoseAugment, a novel pipeline incorporating VAE-based pose generation and physical optimization. Given a pose sequence, the VAE module generates infinite poses with both high fidelity and diversity, while keeping the data distribution. The physical module optimizes poses to satisfy physical constraints with minimal motion restrictions. High-quality IMU data are then synthesized from the augmented poses for training motion capture models. Experiments show that PoseAugment outperforms previous data augmentation and pose generation methods in terms of motion capture accuracy, revealing a strong potential of our method to alleviate the data collection burden for IMU-based motion capture and related tasks driven by human poses.",
        "subjects": "Computer Vision and Pattern Recognition, Human-Computer Interaction",
        "date": "2024-09-21 10:51:16 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.025082"
    },
    {
        "index": "#110",
        "title": "Foundation Models for Amodal Video Instance Segmentation in Automated Driving",
        "link": "/arxiv/2409.14095",
        "arxiv_id": "2409.14095",
        "authors": "Jasmin Breitenstein, Franz Jünger, Andreas Bär, Tim Fingscheidt",
        "summary": "In this work, we study amodal video instance segmentation for automated driving. Previous works perform amodal video instance segmentation relying on methods trained on entirely labeled video data with techniques borrowed from standard video instance segmentation. Such amodally labeled video data is difficult and expensive to obtain and the resulting methods suffer from a trade-off between instance segmentation and tracking performance. To largely solve this issue, we propose to study the application of foundation models for this task. More precisely, we exploit the extensive knowledge of the Segment Anything Model (SAM), while fine-tuning it to the amodal instance segmentation task. Given an initial video instance segmentation, we sample points from the visible masks to prompt our amodal SAM. We use a point memory to store those points. If a previously observed instance is not predicted in a following frame, we retrieve its most recent points from the point memory and use a point tracking method to follow those points to the current frame, together with the corresponding last amodal instance mask. This way, while basing our method on an amodal instance segmentation, we nevertheless obtain video-level amodal instance segmentation results. Our resulting S-AModal method achieves state-of-the-art results in amodal video instance segmentation while resolving the need for amodal video-based labels. Code for S-AModal is available at https://github.com/ifnspaml/S-AModal.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 10:31:46 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.025336"
    },
    {
        "index": "#111",
        "title": "BRep Boundary and Junction Detection for CAD Reverse Engineering",
        "link": "/arxiv/2409.14087",
        "arxiv_id": "2409.14087",
        "authors": "Sk Aziz Ali, Mohammad Sadil Khan, Didier Stricker",
        "summary": "In machining process, 3D reverse engineering of the mechanical system is an integral, highly important, and yet time consuming step to obtain parametric CAD models from 3D scans. Therefore, deep learning-based Scan-to-CAD modeling can offer designers enormous editability to quickly modify CAD model, being able to parse all its structural compositions and design steps. In this paper, we propose a supervised boundary representation (BRep) detection network BRepDetNet from 3D scans of CC3D and ABC dataset. We have carefully annotated the 50K and 45K scans of both the datasets with appropriate topological relations (e.g., next, mate, previous) between the geometrical primitives (i.e., boundaries, junctions, loops, faces) of their BRep data structures. The proposed solution decomposes the Scan-to-CAD problem in Scan-to-BRep ensuring the right step towards feature-based modeling, and therefore, leveraging other existing BRep-to-CAD modeling methods. Our proposed Scan-to-BRep neural network learns to detect BRep boundaries and junctions by minimizing focal-loss and non-maximal suppression (NMS) during training time. Experimental results show that our BRepDetNet with NMS-Loss achieves impressive results.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning, Multimedia",
        "date": "2024-09-21 09:53:11 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.025562"
    },
    {
        "index": "#112",
        "title": "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information",
        "link": "/arxiv/2409.14083",
        "arxiv_id": "2409.14083",
        "authors": "Jiashuo Sun, Jihai Zhang, Yucheng Zhou, Zhaochen Su, Xiaoye Qu, Yu Cheng",
        "summary": "Large Vision-Language Models (LVLMs) have become pivotal at the intersection of computer vision and natural language processing. However, the full potential of LVLMs Retrieval-Augmented Generation (RAG) capabilities remains underutilized. Existing works either focus solely on the text modality or are limited to specific tasks. Moreover, most LVLMs struggle to selectively utilize retrieved information and are sensitive to irrelevant or misleading references. To address these challenges, we propose a self-refinement framework designed to teach LVLMs to Selectively Utilize Retrieved Information (SURf). Specifically, when given questions that are incorrectly answered by the LVLM backbone, we obtain references that help correct the answers (positive references) and those that do not (negative references). We then fine-tune the LVLM backbone using a combination of these positive and negative references. Our experiments across three tasks and seven datasets demonstrate that our framework significantly enhances LVLMs ability to effectively utilize retrieved multimodal references and improves their robustness against irrelevant or misleading information. The source code is available at https://github.com/GasolSun36/SURf.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 09:36:14 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.025793"
    },
    {
        "index": "#113",
        "title": "Dynamic 2D Gaussians: Geometrically accurate radiance fields for dynamic objects",
        "link": "/arxiv/2409.14072",
        "arxiv_id": "2409.14072",
        "authors": "Shuai Zhang, Guanjun Wu, Xinggang Wang, Bin Feng, Wenyu Liu",
        "summary": "Reconstructing objects and extracting high-quality surfaces play a vital role in the real world. Current 4D representations show the ability to render high-quality novel views for dynamic objects but cannot reconstruct high-quality meshes due to their implicit or geometrically inaccurate representations. In this paper, we propose a novel representation that can reconstruct accurate meshes from sparse image input, named Dynamic 2D Gaussians (D-2DGS). We adopt 2D Gaussians for basic geometry representation and use sparse-controlled points to capture 2D Gaussian's deformation. By extracting the object mask from the rendered high-quality image and masking the rendered depth map, a high-quality dynamic mesh sequence of the object can be extracted. Experiments demonstrate that our D-2DGS is outstanding in reconstructing high-quality meshes from sparse input. More demos and code are available at https://github.com/hustvl/Dynamic-2DGS.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 09:01:49 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.025985"
    },
    {
        "index": "#114",
        "title": "SplatLoc: 3D Gaussian Splatting-based Visual Localization for Augmented Reality",
        "link": "/arxiv/2409.14067",
        "arxiv_id": "2409.14067",
        "authors": "Hongjia Zhai, Xiyu Zhang, Boming Zhao, Hai Li, Yijia He, Zhaopeng Cui, Hujun Bao, Guofeng Zhang",
        "summary": "Visual localization plays an important role in the applications of Augmented Reality (AR), which enable AR devices to obtain their 6-DoF pose in the pre-build map in order to render virtual content in real scenes. However, most existing approaches can not perform novel view rendering and require large storage capacities for maps. To overcome these limitations, we propose an efficient visual localization method capable of high-quality rendering with fewer parameters. Specifically, our approach leverages 3D Gaussian primitives as the scene representation. To ensure precise 2D-3D correspondences for pose estimation, we develop an unbiased 3D scene-specific descriptor decoder for Gaussian primitives, distilled from a constructed feature volume. Additionally, we introduce a salient 3D landmark selection algorithm that selects a suitable primitive subset based on the saliency score for localization. We further regularize key Gaussian primitives to prevent anisotropic effects, which also improves localization performance. Extensive experiments on two widely used datasets demonstrate that our method achieves superior or comparable rendering and localization performance to state-of-the-art implicit-based visual localization approaches. Project page: \\href{https://zju3dv.github.io/splatloc}{https://zju3dv.github.io/splatloc}.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 08:46:16 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.036625"
    },
    {
        "index": "#115",
        "title": "Soft Segmented Randomization: Enhancing Domain Generalization in SAR-ATR for Synthetic-to-Measured",
        "link": "/arxiv/2409.14060",
        "arxiv_id": "2409.14060",
        "authors": "Minjun Kim, Ohtae Jang, Haekang Song, Heesub Shin, Jaewoo Ok, Minyoung Back, Jaehyuk Youn, Sungho Kim",
        "summary": "Synthetic aperture radar technology is crucial for high-resolution imaging under various conditions; however, the acquisition of real-world synthetic aperture radar data for deep learning-based automatic target recognition remains challenging due to high costs and data availability issues. To overcome these challenges, synthetic data generated through simulations have been employed, although discrepancies between synthetic and real data can degrade model performance. In this study, we introduce a novel framework, soft segmented randomization, designed to reduce domain discrepancy and improve the generalize ability of synthetic aperture radar automatic target recognition models. The soft segmented randomization framework applies a Gaussian mixture model to segment target and clutter regions softly, introducing randomized variations that align the synthetic data's statistical properties more closely with those of real-world data. Experimental results demonstrate that the proposed soft segmented randomization framework significantly enhances model performance on measured synthetic aperture radar data, making it a promising approach for robust automatic target recognition in scenarios with limited or no access to measured data.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 08:24:51 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.036948"
    },
    {
        "index": "#116",
        "title": "BrainDreamer: Reasoning-Coherent and Controllable Image Generation from EEG Brain Signals via Language Guidance",
        "link": "/arxiv/2409.14021",
        "arxiv_id": "2409.14021",
        "authors": "Ling Wang, Chen Wu, Lin Wang",
        "summary": "Can we directly visualize what we imagine in our brain together with what we describe? The inherent nature of human perception reveals that, when we think, our body can combine language description and build a vivid picture in our brain. Intuitively, generative models should also hold such versatility. In this paper, we introduce BrainDreamer, a novel end-to-end language-guided generative framework that can mimic human reasoning and generate high-quality images from electroencephalogram (EEG) brain signals. Our method is superior in its capacity to eliminate the noise introduced by non-invasive EEG data acquisition and meanwhile achieve a more precise mapping between the EEG and image modality, thus leading to significantly better-generated images. Specifically, BrainDreamer consists of two key learning stages: 1) modality alignment and 2) image generation. In the alignment stage, we propose a novel mask-based triple contrastive learning strategy to effectively align EEG, text, and image embeddings to learn a unified representation. In the generation stage, we inject the EEG embeddings into the pre-trained Stable Diffusion model by designing a learnable EEG adapter to generate high-quality reasoning-coherent images. Moreover, BrainDreamer can accept textual descriptions (e.g., color, position, etc.) to achieve controllable image generation. Extensive experiments show that our method significantly outperforms prior arts in terms of generating quality and quantitative performance.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-21 05:16:31 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.037150"
    },
    {
        "index": "#117",
        "title": "MOSE: Monocular Semantic Reconstruction Using NeRF-Lifted Noisy Priors",
        "link": "/arxiv/2409.14019",
        "arxiv_id": "2409.14019",
        "authors": "Zhenhua Du, Binbin Xu, Haoyu Zhang, Kai Huo, Shuaifeng Zhi",
        "summary": "Accurately reconstructing dense and semantically annotated 3D meshes from monocular images remains a challenging task due to the lack of geometry guidance and imperfect view-dependent 2D priors. Though we have witnessed recent advancements in implicit neural scene representations enabling precise 2D rendering simply from multi-view images, there have been few works addressing 3D scene understanding with monocular priors alone. In this paper, we propose MOSE, a neural field semantic reconstruction approach to lift inferred image-level noisy priors to 3D, producing accurate semantics and geometry in both 3D and 2D space. The key motivation for our method is to leverage generic class-agnostic segment masks as guidance to promote local consistency of rendered semantics during training. With the help of semantics, we further apply a smoothness regularization to texture-less regions for better geometric quality, thus achieving mutual benefits of geometry and semantics. Experiments on the ScanNet dataset show that our MOSE outperforms relevant baselines across all metrics on tasks of 3D semantic segmentation, 2D semantic segmentation and 3D surface reconstruction.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence, Robotics",
        "date": "2024-09-21 05:12:13 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.037368"
    },
    {
        "index": "#118",
        "title": "Generalizable Non-Line-of-Sight Imaging with Learnable Physical Priors",
        "link": "/arxiv/2409.14011",
        "arxiv_id": "2409.14011",
        "authors": "Shida Sun, Yue Li, Yueyi Zhang, Zhiwei Xiong",
        "summary": "Non-line-of-sight (NLOS) imaging, recovering the hidden volume from indirect reflections, has attracted increasing attention due to its potential applications. Despite promising results, existing NLOS reconstruction approaches are constrained by the reliance on empirical physical priors, e.g., single fixed path compensation. Moreover, these approaches still possess limited generalization ability, particularly when dealing with scenes at a low signal-to-noise ratio (SNR). To overcome the above problems, we introduce a novel learning-based solution, comprising two key designs: Learnable Path Compensation (LPC) and Adaptive Phasor Field (APF). The LPC applies tailored path compensation coefficients to adapt to different objects in the scene, effectively reducing light wave attenuation, especially in distant regions. Meanwhile, the APF learns the precise Gaussian window of the illumination function for the phasor field, dynamically selecting the relevant spectrum band of the transient measurement. Experimental validations demonstrate that our proposed approach, only trained on synthetic data, exhibits the capability to seamlessly generalize across various real-world datasets captured by different imaging systems and characterized by low SNRs.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 04:39:45 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.037553"
    },
    {
        "index": "#119",
        "title": "Multiple-Exit Tuning: Towards Inference-Efficient Adaptation for Vision Transformer",
        "link": "/arxiv/2409.13999",
        "arxiv_id": "2409.13999",
        "authors": "Zheng Liu, Jinchao Zhu, Nannan Li, Gao Huang",
        "summary": "Parameter-efficient transfer learning (PETL) has shown great potential in adapting a vision transformer (ViT) pre-trained on large-scale datasets to various downstream tasks. Existing studies primarily focus on minimizing the number of learnable parameters. Although these methods are storage-efficient, they allocate excessive computational resources to easy samples, leading to inefficient inference. To address this issue, we introduce an inference-efficient tuning method termed multiple-exit tuning (MET). MET integrates multiple exits into the pre-trained ViT backbone. Since the predictions in ViT are made by a linear classifier, each exit is equipped with a linear prediction head. In inference stage, easy samples will exit at early exits and only hard enough samples will flow to the last exit, thus saving the computational cost for easy samples. MET consists of exit-specific adapters (E-adapters) and graph regularization. E-adapters are designed to extract suitable representations for different exits. To ensure parameter efficiency, all E-adapters share the same down-projection and up-projection matrices. As the performances of linear classifiers are influenced by the relationship among samples, we employ graph regularization to improve the representations fed into the classifiers at early exits. Finally, we conduct extensive experiments to verify the performance of MET. Experimental results show that MET has an obvious advantage over the state-of-the-art methods in terms of both accuracy and inference efficiency.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 03:25:18 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.037735"
    },
    {
        "index": "#120",
        "title": "GAInS: Gradient Anomaly-aware Biomedical Instance Segmentation",
        "link": "/arxiv/2409.13988",
        "arxiv_id": "2409.13988",
        "authors": "Runsheng Liu, Hao Jiang, Yanning Zhou, Huangjing Lin, Liansheng Wang, Hao Chen",
        "summary": "Instance segmentation plays a vital role in the morphological quantification of biomedical entities such as tissues and cells, enabling precise identification and delineation of different structures. Current methods often address the challenges of touching, overlapping or crossing instances through individual modeling, while neglecting the intrinsic interrelation between these conditions. In this work, we propose a Gradient Anomaly-aware Biomedical Instance Segmentation approach (GAInS), which leverages instance gradient information to perceive local gradient anomaly regions, thus modeling the spatial relationship between instances and refining local region segmentation. Specifically, GAInS is firstly built on a Gradient Anomaly Mapping Module (GAMM), which encodes the radial fields of instances through window sliding to obtain instance gradient anomaly maps. To efficiently refine boundaries and regions with gradient anomaly attention, we propose an Adaptive Local Refinement Module (ALRM) with a gradient anomaly-aware loss function. Extensive comparisons and ablation experiments in three biomedical scenarios demonstrate that our proposed GAInS outperforms other state-of-the-art (SOTA) instance segmentation methods. The code is available at https://github.com/DeepGAInS/GAInS.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 02:36:46 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.037924"
    },
    {
        "index": "#121",
        "title": "Holistic and Historical Instance Comparison for Cervical Cell Detection",
        "link": "/arxiv/2409.13987",
        "arxiv_id": "2409.13987",
        "authors": "Hao Jiang, Runsheng Liu, Yanning Zhou, Huangjing Lin, Hao Chen",
        "summary": "Cytology screening from Papanicolaou (Pap) smears is a common and effective tool for the preventive clinical management of cervical cancer, where abnormal cell detection from whole slide images serves as the foundation for reporting cervical cytology. However, cervical cell detection remains challenging due to 1) hazily-defined cell types (e.g., ASC-US) with subtle morphological discrepancies caused by the dynamic cancerization process, i.e., cell class ambiguity, and 2) imbalanced class distributions of clinical data may cause missed detection, especially for minor categories, i.e., cell class imbalance. To this end, we propose a holistic and historical instance comparison approach for cervical cell detection. Specifically, we first develop a holistic instance comparison scheme enforcing both RoI-level and class-level cell discrimination. This coarse-to-fine cell comparison encourages the model to learn foreground-distinguishable and class-wise representations. To emphatically improve the distinguishability of minor classes, we then introduce a historical instance comparison scheme with a confident sample selection-based memory bank, which involves comparing current embeddings with historical embeddings for better cell instance discrimination. Extensive experiments and analysis on two large-scale cytology datasets including 42,592 and 114,513 cervical cells demonstrate the effectiveness of our method. The code is available at https://github.com/hjiangaz/HERO.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 02:36:19 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.038124"
    },
    {
        "index": "#122",
        "title": "Cycle-Consistency Uncertainty Estimation for Visual Prompting based One-Shot Defect Segmentation",
        "link": "/arxiv/2409.13984",
        "arxiv_id": "2409.13984",
        "authors": "Geonuk Kim",
        "summary": "Industrial defect detection traditionally relies on supervised learning models trained on fixed datasets of known defect types. While effective within a closed set, these models struggle with new, unseen defects, necessitating frequent re-labeling and re-training. Recent advances in visual prompting offer a solution by allowing models to adaptively infer novel categories based on provided visual cues. However, a prevalent issue in these methods is the over-confdence problem, where models can mis-classify unknown objects as known objects with high certainty. To addresssing the fundamental concerns about the adaptability, we propose a solution to estimate uncertainty of the visual prompting process by cycle-consistency. We designed to check whether it can accurately restore the original prompt from its predictions. To quantify this, we measure the mean Intersection over Union (mIoU) between the restored prompt mask and the originally provided prompt mask. Without using complex designs or ensemble methods with multiple networks, our approach achieved a yield rate of 0.9175 in the VISION24 one-shot industrial challenge.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 02:25:32 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.038359"
    },
    {
        "index": "#123",
        "title": "Enhanced Semantic Segmentation for Large-Scale and Imbalanced Point Clouds",
        "link": "/arxiv/2409.13983",
        "arxiv_id": "2409.13983",
        "authors": "Haoran Gong, Haodong Wang, Di Wang",
        "summary": "Semantic segmentation of large-scale point clouds is of significant importance in environment perception and scene understanding. However, point clouds collected from real-world environments are usually imbalanced and small-sized objects are prone to be under-sampled or misclassified due to their low occurrence frequency, thereby reducing the overall accuracy of semantic segmentation. In this study, we propose the Multilateral Cascading Network (MCNet) for large-scale and sample-imbalanced point cloud scenes. To increase the frequency of small-sized objects, we introduce the semantic-weighted sampling module, which incorporates a probability parameter into the collected data group. To facilitate feature learning, we propose a Multilateral Cascading Attention Enhancement (MCAE) module to learn complex local features through multilateral cascading operations and attention mechanisms. To promote feature fusion, we propose a Point Cross Stage Partial (P-CSP) module to combine global and local features, optimizing the integration of valuable feature information across multiple scales. Finally, we introduce the neighborhood voting module to integrate results at the output layer. Our proposed method demonstrates either competitive or superior performance relative to state-of-the-art approaches across three widely recognized benchmark datasets: S3DIS, Toronto3D, and SensatUrban with mIoU scores of 74.0\\%, 82.9\\% and 64.5\\%, respectively. Notably, our work yielded consistent optimal results on the under-sampled semantic categories, thereby demonstrating exceptional performance in the recognition of small-sized objects.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 02:23:01 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.038546"
    },
    {
        "index": "#124",
        "title": "CUS3D :CLIP-based Unsupervised 3D Segmentation via Object-level Denoise",
        "link": "/arxiv/2409.13982",
        "arxiv_id": "2409.13982",
        "authors": "Fuyang Yu, Runze Tian, Zhen Wang, Xiaochuan Wang, Xiaohui Liang",
        "summary": "To ease the difficulty of acquiring annotation labels in 3D data, a common method is using unsupervised and open-vocabulary semantic segmentation, which leverage 2D CLIP semantic knowledge. In this paper, unlike previous research that ignores the ``noise'' raised during feature projection from 2D to 3D, we propose a novel distillation learning framework named CUS3D. In our approach, an object-level denosing projection module is designed to screen out the ``noise'' and ensure more accurate 3D feature. Based on the obtained features, a multimodal distillation learning module is designed to align the 3D feature with CLIP semantic feature space with object-centered constrains to achieve advanced unsupervised semantic segmentation. We conduct comprehensive experiments in both unsupervised and open-vocabulary segmentation, and the results consistently showcase the superiority of our model in achieving advanced unsupervised segmentation results and its effectiveness in open-vocabulary segmentation.",
        "subjects": "Computer Vision and Pattern Recognition, Multimedia",
        "date": "2024-09-21 02:17:35 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.038751"
    },
    {
        "index": "#125",
        "title": "Enhancing Advanced Visual Reasoning Ability of Large Language Models",
        "link": "/arxiv/2409.13980",
        "arxiv_id": "2409.13980",
        "authors": "Zhiyuan Li, Dongnan Liu, Chaoyi Zhang, Heng Wang, Tengfei Xue, Weidong Cai",
        "summary": "Recent advancements in Vision-Language (VL) research have sparked new benchmarks for complex visual reasoning, challenging models' advanced reasoning ability. Traditional Vision-Language Models (VLMs) perform well in visual perception tasks while struggling with complex reasoning scenarios. Conversely, Large Language Models (LLMs) demonstrate robust text reasoning capabilities; however, they lack visual acuity. To bridge this gap, we propose Complex Visual Reasoning Large Language Models (CVR-LLM), capitalizing on VLMs' visual perception proficiency and LLMs' extensive reasoning capability. Unlike recent multimodal large language models (MLLMs) that require a projection layer, our approach transforms images into detailed, context-aware descriptions using an iterative self-refinement loop and leverages LLMs' text knowledge for accurate predictions without extra training. We also introduce a novel multi-modal in-context learning (ICL) methodology to enhance LLMs' contextual understanding and reasoning. Additionally, we introduce Chain-of-Comparison (CoC), a step-by-step comparison technique enabling contrasting various aspects of predictions. Our CVR-LLM presents the first comprehensive study across a wide array of complex visual reasoning tasks and achieves SOTA performance among all.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-21 02:10:19 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.038957"
    },
    {
        "index": "#126",
        "title": "FracGM: A Fast Fractional Programming Technique for Geman-McClure Robust Estimator",
        "link": "/arxiv/2409.13978",
        "arxiv_id": "2409.13978",
        "authors": "Bang-Shien Chen, Yu-Kai Lin, Jian-Yu Chen, Chih-Wei Huang, Jann-Long Chern, Ching-Cherng Sun",
        "summary": "Robust estimation is essential in computer vision, robotics, and navigation, aiming to minimize the impact of outlier measurements for improved accuracy. We present a fast algorithm for Geman-McClure robust estimation, FracGM, leveraging fractional programming techniques. This solver reformulates the original non-convex fractional problem to a convex dual problem and a linear equation system, iteratively solving them in an alternating optimization pattern. Compared to graduated non-convexity approaches, this strategy exhibits a faster convergence rate and better outlier rejection capability. In addition, the global optimality of the proposed solver can be guaranteed under given conditions. We demonstrate the proposed FracGM solver with Wahba's rotation problem and 3-D point-cloud registration along with relaxation pre-processing and projection post-processing. Compared to state-of-the-art algorithms, when the outlier rates increase from 20\\% to 80\\%, FracGM shows 53\\% and 88\\% lower rotation and translation increases. In real-world scenarios, FracGM achieves better results in 13 out of 18 outcomes, while having a 19.43\\% improvement in the computation time.",
        "subjects": "Computer Vision and Pattern Recognition, Robotics, Optimization and Control",
        "date": "2024-09-21 02:01:55 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.039162"
    },
    {
        "index": "#127",
        "title": "Improving 3D Semi-supervised Learning by Effectively Utilizing All Unlabelled Data",
        "link": "/arxiv/2409.13977",
        "arxiv_id": "2409.13977",
        "authors": "Sneha Paul, Zachary Patterson, Nizar Bouguila",
        "summary": "Semi-supervised learning (SSL) has shown its effectiveness in learning effective 3D representation from a small amount of labelled data while utilizing large unlabelled data. Traditional semi-supervised approaches rely on the fundamental concept of predicting pseudo-labels for unlabelled data and incorporating them into the learning process. However, we identify that the existing methods do not fully utilize all the unlabelled samples and consequently limit their potential performance. To address this issue, we propose AllMatch, a novel SSL-based 3D classification framework that effectively utilizes all the unlabelled samples. AllMatch comprises three modules: (1) an adaptive hard augmentation module that applies relatively hard augmentations to the high-confident unlabelled samples with lower loss values, thereby enhancing the contribution of such samples, (2) an inverse learning module that further improves the utilization of unlabelled data by learning what not to learn, and (3) a contrastive learning module that ensures learning from all the samples in both supervised and unsupervised settings. Comprehensive experiments on two popular 3D datasets demonstrate a performance improvement of up to 11.2% with 1% labelled data, surpassing the SOTA by a significant margin. Furthermore, AllMatch exhibits its efficiency in effectively leveraging all the unlabelled data, demonstrated by the fact that only 10% of labelled data reaches nearly the same performance as fully-supervised learning with all labelled data. The code of our work is available at: https://github.com/snehaputul/AllMatch.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-21 01:53:52 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.039338"
    },
    {
        "index": "#128",
        "title": "Detecting Inpainted Video with Frequency Domain Insights",
        "link": "/arxiv/2409.13976",
        "arxiv_id": "2409.13976",
        "authors": "Quanhui Tang, Jingtao Cao",
        "summary": "Video inpainting enables seamless content removal and replacement within frames, posing ethical and legal risks when misused. To mitigate these risks, detecting manipulated regions in inpainted videos is critical. Previous detection methods often focus solely on the characteristics derived from spatial and temporal dimensions, which limits their effectiveness by overlooking the unique frequency characteristics of different inpainting algorithms. In this paper, we propose the Frequency Domain Insights Network (FDIN), which significantly enhances detection accuracy by incorporating insights from the frequency domain. Our network features an Adaptive Band Selective Response module to discern frequency characteristics specific to various inpainting techniques and a Fast Fourier Convolution-based Attention module for identifying periodic artifacts in inpainted regions. Utilizing 3D ResBlocks for spatiotemporal analysis, FDIN progressively refines detection precision from broad assessments to detailed localization. Experimental evaluations on public datasets demonstrate that FDIN achieves state-of-the-art performance, setting a new benchmark in video inpainting detection.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-21 01:51:07 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.039524"
    },
    {
        "index": "#129",
        "title": "Monocular Event-Inertial Odometry with Adaptive decay-based Time Surface and Polarity-aware Tracking",
        "link": "/arxiv/2409.13971",
        "arxiv_id": "2409.13971",
        "authors": "Kai Tang, Xiaolei Lang, Yukai Ma, Yuehao Huang, Laijian Li, Yong Liu, Jiajun Lv",
        "summary": "Event cameras have garnered considerable attention due to their advantages over traditional cameras in low power consumption, high dynamic range, and no motion blur. This paper proposes a monocular event-inertial odometry incorporating an adaptive decay kernel-based time surface with polarity-aware tracking. We utilize an adaptive decay-based Time Surface to extract texture information from asynchronous events, which adapts to the dynamic characteristics of the event stream and enhances the representation of environmental textures. However, polarity-weighted time surfaces suffer from event polarity shifts during changes in motion direction. To mitigate its adverse effects on feature tracking, we optimize the feature tracking by incorporating an additional polarity-inverted time surface to enhance the robustness. Comparative analysis with visual-inertial and event-inertial odometry methods shows that our approach outperforms state-of-the-art techniques, with competitive results across various datasets.",
        "subjects": "Computer Vision and Pattern Recognition, Robotics",
        "date": "2024-09-21 01:35:12 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.039789"
    },
    {
        "index": "#130",
        "title": "Deep learning for fast segmentation and critical dimension metrology & characterization enabling AR/VR design and fabrication",
        "link": "/arxiv/2409.13951",
        "arxiv_id": "2409.13951",
        "authors": "Kundan Chaudhary, Subhei Shaar, Raja Muthinti",
        "summary": "Quantitative analysis of microscopy images is essential in the design and fabrication of components used in augmented reality/virtual reality (AR/VR) modules. However, segmenting regions of interest (ROIs) from these complex images and extracting critical dimensions (CDs) requires novel techniques, such as deep learning models which are key for actionable decisions on process, material and device optimization. In this study, we report on the fine-tuning of a pre-trained Segment Anything Model (SAM) using a diverse dataset of electron microscopy images. We employed methods such as low-rank adaptation (LoRA) to reduce training time and enhance the accuracy of ROI extraction. The model's ability to generalize to unseen images facilitates zero-shot learning and supports a CD extraction model that precisely extracts CDs from the segmented ROIs. We demonstrate the accurate extraction of binary images from cross-sectional images of surface relief gratings (SRGs) and Fresnel lenses in both single and multiclass modes. Furthermore, these binary images are used to identify transition points, aiding in the extraction of relevant CDs. The combined use of the fine-tuned segmentation model and the CD extraction model offers substantial advantages to various industrial applications by enhancing analytical capabilities, time to data and insights, and optimizing manufacturing processes.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning, Image and Video Processing",
        "date": "2024-09-20 23:54:58 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.039993"
    },
    {
        "index": "#131",
        "title": "TalkMosaic: Interactive PhotoMosaic with Multi-modal LLM Q&A Interactions",
        "link": "/arxiv/2409.13941",
        "arxiv_id": "2409.13941",
        "authors": "Kevin Li, Fulu Li",
        "summary": "We use images of cars of a wide range of varieties to compose an image of an animal such as a bird or a lion for the theme of environmental protection to maximize the information about cars in a single composed image and to raise the awareness about environmental challenges. We present a novel way of image interaction with an artistically-composed photomosaic image, in which a simple operation of \"click and display\" is used to demonstrate the interactive switch between a tile image in a photomosaic image and the corresponding original car image, which will be automatically saved on the Desktop. We build a multimodal custom GPT named TalkMosaic by incorporating car images information and the related knowledge to ChatGPT. By uploading the original car image to TalkMosaic, we can ask questions about the given car image and get the corresponding answers efficiently and effectively such as where to buy the tire in the car image that satisfies high environmental standards. We give an in-depth analysis on how to speed up the inference of multimodal LLM using sparse attention and quantization techniques with presented probabilistic FlashAttention (PrFlashAttention) and Staircase Adaptive Quantization (SAQ) methods. The implemented prototype demonstrates the feasibility and effectiveness of the presented approach.",
        "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence",
        "date": "2024-09-20 23:04:21 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.040167"
    },
    {
        "index": "#132",
        "title": "Data Pruning via Separability, Integrity, and Model Uncertainty-Aware Importance Sampling",
        "link": "/arxiv/2409.13915",
        "arxiv_id": "2409.13915",
        "authors": "Steven Grosz, Rui Zhao, Rajeev Ranjan, Hongcheng Wang, Manoj Aggarwal, Gerard Medioni, Anil Jain",
        "summary": "This paper improves upon existing data pruning methods for image classification by introducing a novel pruning metric and pruning procedure based on importance sampling. The proposed pruning metric explicitly accounts for data separability, data integrity, and model uncertainty, while the sampling procedure is adaptive to the pruning ratio and considers both intra-class and inter-class separation to further enhance the effectiveness of pruning. Furthermore, the sampling method can readily be applied to other pruning metrics to improve their performance. Overall, the proposed approach scales well to high pruning ratio and generalizes better across different classification models, as demonstrated by experiments on four benchmark datasets, including the fine-grained classification scenario.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-20 21:45:02 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.040447"
    },
    {
        "index": "#133",
        "title": "OneBEV: Using One Panoramic Image for Bird's-Eye-View Semantic Mapping",
        "link": "/arxiv/2409.13912",
        "arxiv_id": "2409.13912",
        "authors": "Jiale Wei, Junwei Zheng, Ruiping Liu, Jie Hu, Jiaming Zhang, Rainer Stiefelhagen",
        "summary": "In the field of autonomous driving, Bird's-Eye-View (BEV) perception has attracted increasing attention in the community since it provides more comprehensive information compared with pinhole front-view images and panoramas. Traditional BEV methods, which rely on multiple narrow-field cameras and complex pose estimations, often face calibration and synchronization issues. To break the wall of the aforementioned challenges, in this work, we introduce OneBEV, a novel BEV semantic mapping approach using merely a single panoramic image as input, simplifying the mapping process and reducing computational complexities. A distortion-aware module termed Mamba View Transformation (MVT) is specifically designed to handle the spatial distortions in panoramas, transforming front-view features into BEV features without leveraging traditional attention mechanisms. Apart from the efficient framework, we contribute two datasets, i.e., nuScenes-360 and DeepAccident-360, tailored for the OneBEV task. Experimental results showcase that OneBEV achieves state-of-the-art performance with 51.1% and 36.1% mIoU on nuScenes-360 and DeepAccident-360, respectively. This work advances BEV semantic mapping in autonomous driving, paving the way for more advanced and reliable autonomous systems.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-20 21:33:53 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.040735"
    },
    {
        "index": "#134",
        "title": "Brain-Cognition Fingerprinting via Graph-GCCA with Contrastive Learning",
        "link": "/arxiv/2409.13887",
        "arxiv_id": "2409.13887",
        "authors": "Yixin Wang, Wei Peng, Yu Zhang, Ehsan Adeli, Qingyu Zhao, Kilian M. Pohl",
        "summary": "Many longitudinal neuroimaging studies aim to improve the understanding of brain aging and diseases by studying the dynamic interactions between brain function and cognition. Doing so requires accurate encoding of their multidimensional relationship while accounting for individual variability over time. For this purpose, we propose an unsupervised learning model (called \\underline{\\textbf{Co}}ntrastive Learning-based \\underline{\\textbf{Gra}}ph Generalized \\underline{\\textbf{Ca}}nonical Correlation Analysis (CoGraCa)) that encodes their relationship via Graph Attention Networks and generalized Canonical Correlational Analysis. To create brain-cognition fingerprints reflecting unique neural and cognitive phenotype of each person, the model also relies on individualized and multimodal contrastive learning. We apply CoGraCa to longitudinal dataset of healthy individuals consisting of resting-state functional MRI and cognitive measures acquired at multiple visits for each participant. The generated fingerprints effectively capture significant individual differences and outperform current single-modal and CCA-based multimodal models in identifying sex and age. More importantly, our encoding provides interpretable interactions between those two modalities.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-20 20:36:20 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.040944"
    },
    {
        "index": "#135",
        "title": "SSE: Multimodal Semantic Data Selection and Enrichment for Industrial-scale Data Assimilation",
        "link": "/arxiv/2409.13860",
        "arxiv_id": "2409.13860",
        "authors": "Maying Shen, Nadine Chang, Sifei Liu, Jose M. Alvarez",
        "summary": "In recent years, the data collected for artificial intelligence has grown to an unmanageable amount. Particularly within industrial applications, such as autonomous vehicles, model training computation budgets are being exceeded while model performance is saturating -- and yet more data continues to pour in. To navigate the flood of data, we propose a framework to select the most semantically diverse and important dataset portion. Then, we further semantically enrich it by discovering meaningful new data from a massive unlabeled data pool. Importantly, we can provide explainability by leveraging foundation models to generate semantics for every data point. We quantitatively show that our Semantic Selection and Enrichment framework (SSE) can a) successfully maintain model performance with a smaller training dataset and b) improve model performance by enriching the smaller dataset without exceeding the original dataset size. Consequently, we demonstrate that semantic diversity is imperative for optimal data selection and model performance.",
        "subjects": "Computer Vision and Pattern Recognition",
        "date": "2024-09-20 19:17:52 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.041129"
    },
    {
        "index": "#136",
        "title": "Multi-Modality Conditioned Variational U-Net for Field-of-View Extension in Brain Diffusion MRI",
        "link": "/arxiv/2409.13846",
        "arxiv_id": "2409.13846",
        "authors": "Zhiyuan Li, Tianyuan Yao, Praitayini Kanakaraj, Chenyu Gao, Shunxing Bao, Lianrui Zuo, Michael E. Kim, Nancy R. Newlin, Gaurav Rudravaram, Nazirah M. Khairi, Yuankai Huo, Kurt G. Schilling, Walter A. Kukull, Arthur W. Toga, Derek B. Archer, Timothy J. Hohman, Bennett A. Landman",
        "summary": "An incomplete field-of-view (FOV) in diffusion magnetic resonance imaging (dMRI) can severely hinder the volumetric and bundle analyses of whole-brain white matter connectivity. Although existing works have investigated imputing the missing regions using deep generative models, it remains unclear how to specifically utilize additional information from paired multi-modality data and whether this can enhance the imputation quality and be useful for downstream tractography. To fill this gap, we propose a novel framework for imputing dMRI scans in the incomplete part of the FOV by integrating the learned diffusion features in the acquired part of the FOV to the complete brain anatomical structure. We hypothesize that by this design the proposed framework can enhance the imputation performance of the dMRI scans and therefore be useful for repairing whole-brain tractography in corrupted dMRI scans with incomplete FOV. We tested our framework on two cohorts from different sites with a total of 96 subjects and compared it with a baseline imputation method that treats the information from T1w and dMRI scans equally. The proposed framework achieved significant improvements in imputation performance, as demonstrated by angular correlation coefficient (p < 1E-5), and in downstream tractography accuracy, as demonstrated by Dice score (p < 0.01). Results suggest that the proposed framework improved imputation performance in dMRI scans by specifically utilizing additional information from paired multi-modality data, compared with the baseline method. The imputation achieved by the proposed framework enhances whole brain tractography, and therefore reduces the uncertainty when analyzing bundles associated with neurodegenerative.",
        "subjects": "Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-20 18:41:29 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.041401"
    },
    {
        "index": "#137",
        "title": "ViTGuard: Attention-aware Detection against Adversarial Examples for Vision Transformer",
        "link": "/arxiv/2409.13828",
        "arxiv_id": "2409.13828",
        "authors": "Shihua Sun, Kenechukwu Nwodo, Shridatt Sugrim, Angelos Stavrou, Haining Wang",
        "summary": "The use of transformers for vision tasks has challenged the traditional dominant role of convolutional neural networks (CNN) in computer vision (CV). For image classification tasks, Vision Transformer (ViT) effectively establishes spatial relationships between patches within images, directing attention to important areas for accurate predictions. However, similar to CNNs, ViTs are vulnerable to adversarial attacks, which mislead the image classifier into making incorrect decisions on images with carefully designed perturbations. Moreover, adversarial patch attacks, which introduce arbitrary perturbations within a small area, pose a more serious threat to ViTs. Even worse, traditional detection methods, originally designed for CNN models, are impractical or suffer significant performance degradation when applied to ViTs, and they generally overlook patch attacks. In this paper, we propose ViTGuard as a general detection method for defending ViT models against adversarial attacks, including typical attacks where perturbations spread over the entire input and patch attacks. ViTGuard uses a Masked Autoencoder (MAE) model to recover randomly masked patches from the unmasked regions, providing a flexible image reconstruction strategy. Then, threshold-based detectors leverage distinctive ViT features, including attention maps and classification (CLS) token representations, to distinguish between normal and adversarial samples. The MAE model does not involve any adversarial samples during training, ensuring the effectiveness of our detectors against unseen attacks. ViTGuard is compared with seven existing detection methods under nine attacks across three datasets. The evaluation results show the superiority of ViTGuard over existing detectors. Finally, considering the potential detection evasion, we further demonstrate ViTGuard's robustness against adaptive attacks for evasion.",
        "subjects": "Computer Vision and Pattern Recognition, Cryptography and Security",
        "date": "2024-09-20 18:11:56 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.041598"
    },
    {
        "index": "#138",
        "title": "Intrinsic Single-Image HDR Reconstruction",
        "link": "/arxiv/2409.13803",
        "arxiv_id": "2409.13803",
        "authors": "Sebastian Dille, Chris Careaga, Yağız Aksoy",
        "summary": "The low dynamic range (LDR) of common cameras fails to capture the rich contrast in natural scenes, resulting in loss of color and details in saturated pixels. Reconstructing the high dynamic range (HDR) of luminance present in the scene from single LDR photographs is an important task with many applications in computational photography and realistic display of images. The HDR reconstruction task aims to infer the lost details using the context present in the scene, requiring neural networks to understand high-level geometric and illumination cues. This makes it challenging for data-driven algorithms to generate accurate and high-resolution results. In this work, we introduce a physically-inspired remodeling of the HDR reconstruction problem in the intrinsic domain. The intrinsic model allows us to train separate networks to extend the dynamic range in the shading domain and to recover lost color details in the albedo domain. We show that dividing the problem into two simpler sub-tasks improves performance in a wide variety of photographs.",
        "subjects": "Computer Vision and Pattern Recognition, Graphics, Image and Video Processing",
        "date": "2024-09-20 17:56:51 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.047119"
    },
    {
        "index": "#139",
        "title": "OmniBench: Towards The Future of Universal Omni-Language Models",
        "link": "/arxiv/2409.15272",
        "arxiv_id": "2409.15272",
        "authors": "Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Xiangzhou Wang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin",
        "summary": "Recent advancements in multimodal large language models (MLLMs) have aimed to integrate and interpret data across diverse modalities. However, the capacity of these models to concurrently process and reason about multiple modalities remains inadequately explored, partly due to the lack of comprehensive modality-wise benchmarks. We introduce OmniBench, a novel benchmark designed to rigorously evaluate models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define models capable of such tri-modal processing as omni-language models (OLMs). OmniBench is distinguished by high-quality human annotations, ensuring that accurate responses require integrated understanding and reasoning across all three modalities. Our main findings reveal that: i) open-source OLMs exhibit critical limitations in instruction-following and reasoning capabilities within tri-modal contexts; and ii) the baseline models perform poorly (below 50% accuracy) even when provided with alternative textual representations of images and audio. These results suggest that the ability to construct a consistent context from text, image, and audio is often overlooked in existing MLLM training paradigms. We advocate for future research to focus on developing more robust tri-modal integration techniques and training strategies to enhance OLM performance across diverse modalities. The codes and live leaderboard could be found at https://m-a-p.ai/OmniBench.",
        "subjects": "Computation and Language, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 17:59:05 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.047496"
    },
    {
        "index": "#140",
        "title": "UDA-Bench: Revisiting Common Assumptions in Unsupervised Domain Adaptation Using a Standardized Framework",
        "link": "/arxiv/2409.15264",
        "arxiv_id": "2409.15264",
        "authors": "Tarun Kalluri, Sreyas Ravichandran, Manmohan Chandraker",
        "summary": "In this work, we take a deeper look into the diverse factors that influence the efficacy of modern unsupervised domain adaptation (UDA) methods using a large-scale, controlled empirical study. To facilitate our analysis, we first develop UDA-Bench, a novel PyTorch framework that standardizes training and evaluation for domain adaptation enabling fair comparisons across several UDA methods. Using UDA-Bench, our comprehensive empirical study into the impact of backbone architectures, unlabeled data quantity, and pre-training datasets reveals that: (i) the benefits of adaptation methods diminish with advanced backbones, (ii) current methods underutilize unlabeled data, and (iii) pre-training data significantly affects downstream adaptation in both supervised and self-supervised settings. In the context of unsupervised adaptation, these observations uncover several novel and surprising properties, while scientifically validating several others that were often considered empirical heuristics or practitioner intuitions in the absence of a standardized training and evaluation framework. The UDA-Bench framework and trained models are publicly available at https://github.com/ViLab-UCSD/UDABench_ECCV2024.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 17:57:07 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.047703"
    },
    {
        "index": "#141",
        "title": "ZeroSCD: Zero-Shot Street Scene Change Detection",
        "link": "/arxiv/2409.15255",
        "arxiv_id": "2409.15255",
        "authors": "Shyam Sundar Kannan, Byung-Cheol Min",
        "summary": "Scene Change Detection is a challenging task in computer vision and robotics that aims to identify differences between two images of the same scene captured at different times. Traditional change detection methods rely on training models that take these image pairs as input and estimate the changes, which requires large amounts of annotated data, a costly and time-consuming process. To overcome this, we propose ZeroSCD, a zero-shot scene change detection framework that eliminates the need for training. ZeroSCD leverages pre-existing models for place recognition and semantic segmentation, utilizing their features and outputs to perform change detection. In this framework, features extracted from the place recognition model are used to estimate correspondences and detect changes between the two images. These are then combined with segmentation results from the semantic segmentation model to precisely delineate the boundaries of the detected changes. Extensive experiments on benchmark datasets demonstrate that ZeroSCD outperforms several state-of-the-art methods in change detection accuracy, despite not being trained on any of the benchmark datasets, proving its effectiveness and adaptability across different scenarios.",
        "subjects": "Robotics, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 17:53:44 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.047885"
    },
    {
        "index": "#142",
        "title": "Investigating Robot Dogs for Construction Monitoring: A Comparative Analysis of Specifications and On-site Requirements",
        "link": "/arxiv/2409.15253",
        "arxiv_id": "2409.15253",
        "authors": "Miguel Arturo Vega Torres, Fabian Pfitzner",
        "summary": "Robot dogs are receiving increasing attention in various fields of research. However, the number of studies investigating their potential usability on construction sites is scarce. The construction industry implies several human resource-demanding tasks such as safety monitoring, material transportation, and site inspections. Robot dogs can address some of these challenges by providing automated support and lowering manual effort. In this paper, we investigate the potential usability of currently available robot dogs on construction sites in terms of focusing on their different specifications and on-site requirements to support data acquisition. In addition, we conducted a real-world experiment on a large-scale construction site using a quadruped robot. In conclusion, we consider robot dogs to be a valuable asset for monitoring intricate construction environments in the future, particularly as their limitations are mitigated through technical advancements.",
        "subjects": "Robotics, Hardware Architecture, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 17:51:31 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.048073"
    },
    {
        "index": "#143",
        "title": "Semantic Inference-Based Deep Learning and Modeling for Earth Observation: Cognitive Semantic Augmentation Satellite Networks",
        "link": "/arxiv/2409.15246",
        "arxiv_id": "2409.15246",
        "authors": "Hong-fu Chou, Vu Nguyen Ha, Prabhu Thiruvasagam, Thanh-Dung Le, Geoffrey Eappen, Ti Ti Nguyen, Luis M. Garces-Socarras, Jorge L. Gonzalez-Rios, Juan Carlos Merlano-Duncan, Symeon Chatzinotas",
        "summary": "Earth Observation (EO) systems play a crucial role in achieving Sustainable Development Goals by collecting and analyzing vital global data through satellite networks. These systems are essential for tasks like mapping, disaster monitoring, and resource management, but they face challenges in processing and transmitting large volumes of EO data, especially in specialized fields such as agriculture and real-time disaster response. Domain-adapted Large Language Models (LLMs) provide a promising solution by facilitating data fusion between extensive EO data and semantic EO data. By improving integration and interpretation of diverse datasets, LLMs address the challenges of processing specialized information in agriculture and disaster response applications. This fusion enhances the accuracy and relevance of transmitted data. This paper presents a framework for semantic communication in EO satellite networks, aimed at improving data transmission efficiency and overall system performance through cognitive processing techniques. The proposed system employs Discrete-Task-Oriented Source-Channel Coding (DT-JSCC) and Semantic Data Augmentation (SA) to focus on relevant information while minimizing communication overhead. By integrating cognitive semantic processing and inter-satellite links, the framework enhances the analysis and transmission of multispectral satellite imagery, improving object detection, pattern recognition, and real-time decision-making. The introduction of Cognitive Semantic Augmentation (CSA) allows satellites to process and transmit semantic information, boosting adaptability to changing environments and application needs. This end-to-end architecture is tailored for next-generation satellite networks, such as those supporting 6G, and demonstrates significant improvements in efficiency and accuracy.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition, Networking and Internet Architecture",
        "date": "2024-09-23 17:42:05 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.048322"
    },
    {
        "index": "#144",
        "title": "FLeNS: Federated Learning with Enhanced Nesterov-Newton Sketch",
        "link": "/arxiv/2409.15216",
        "arxiv_id": "2409.15216",
        "authors": "Sunny Gupta, Mohit, Pankhi Kashyap, Pranav Jeevan, Amit Sethi",
        "summary": "Federated learning faces a critical challenge in balancing communication efficiency with rapid convergence, especially for second-order methods. While Newton-type algorithms achieve linear convergence in communication rounds, transmitting full Hessian matrices is often impractical due to quadratic complexity. We introduce Federated Learning with Enhanced Nesterov-Newton Sketch (FLeNS), a novel method that harnesses both the acceleration capabilities of Nesterov's method and the dimensionality reduction benefits of Hessian sketching. FLeNS approximates the centralized Newton's method without relying on the exact Hessian, significantly reducing communication overhead. By combining Nesterov's acceleration with adaptive Hessian sketching, FLeNS preserves crucial second-order information while preserving the rapid convergence characteristics. Our theoretical analysis, grounded in statistical learning, demonstrates that FLeNS achieves super-linear convergence rates in communication rounds - a notable advancement in federated optimization. We provide rigorous convergence guarantees and characterize tradeoffs between acceleration, sketch size, and convergence speed. Extensive empirical evaluation validates our theoretical findings, showcasing FLeNS's state-of-the-art performance with reduced communication requirements, particularly in privacy-sensitive and edge-computing scenarios. The code is available at https://github.com/sunnyinAI/FLeNS",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition, Distributed, Parallel, and Cluster Computing, Optimization and Control",
        "date": "2024-09-23 17:00:35 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.048532"
    },
    {
        "index": "#145",
        "title": "MAR-DTN: Metal Artifact Reduction using Domain Transformation Network for Radiotherapy Planning",
        "link": "/arxiv/2409.15155",
        "arxiv_id": "2409.15155",
        "authors": "Belén Serrano-Antón, Mubashara Rehman, Niki Martinel, Michele Avanzo, Riccardo Spizzo, Giuseppe Fanetti, Alberto P. Muñuzuri, Christian Micheloni",
        "summary": "For the planning of radiotherapy treatments for head and neck cancers, Computed Tomography (CT) scans of the patients are typically employed. However, in patients with head and neck cancer, the quality of standard CT scans generated using kilo-Voltage (kVCT) tube potentials is severely degraded by streak artifacts occurring in the presence of metallic implants such as dental fillings. Some radiotherapy devices offer the possibility of acquiring Mega-Voltage CT (MVCT) for daily patient setup verification, due to the higher energy of X-rays used, MVCT scans are almost entirely free from artifacts making them more suitable for radiotherapy treatment planning. In this study, we leverage the advantages of kVCT scans with those of MVCT scans (artifact-free). We propose a deep learning-based approach capable of generating artifact-free MVCT images from acquired kVCT images. The outcome offers the benefits of artifact-free MVCT images with enhanced soft tissue contrast, harnessing valuable information obtained through kVCT technology for precise therapy calibration. Our proposed method employs UNet-inspired model, and is compared with adversarial learning and transformer networks. This first and unique approach achieves remarkable success, with PSNR of 30.02 dB across the entire patient volume and 27.47 dB in artifact-affected regions exclusively. It is worth noting that the PSNR calculation excludes the background, concentrating solely on the region of interest.",
        "subjects": "Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 16:04:00 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.048759"
    },
    {
        "index": "#146",
        "title": "Towards Accountable AI-Assisted Eye Disease Diagnosis: Workflow Design, External Validation, and Continual Learning",
        "link": "/arxiv/2409.15087",
        "arxiv_id": "2409.15087",
        "authors": "Qingyu Chen, Tiarnan D L Keenan, Elvira Agron, Alexis Allot, Emily Guan, Bryant Duong, Amr Elsawy, Benjamin Hou, Cancan Xue, Sanjeeb Bhandari, Geoffrey Broadhead, Chantal Cousineau-Krieger, Ellen Davis, William G Gensheimer, David Grasic, Seema Gupta, Luis Haddock, Eleni Konstantinou, Tania Lamba, Michele Maiberger, Dimosthenis Mantopoulos, Mitul C Mehta, Ayman G Nahri, Mutaz AL-Nawaflh, Arnold Oshinsky, Brittany E Powell, Boonkit Purt, Soo Shin, Hillary Stiefel, Alisa T Thavikulwat, Keith James Wroblewski, Tham Yih Chung, Chui Ming Gemmy Cheung, Ching-Yu Cheng, Emily Y Chew, Michelle R. Hribar, Michael F. Chiang, Zhiyong Lu",
        "summary": "Timely disease diagnosis is challenging due to increasing disease burdens and limited clinician availability. AI shows promise in diagnosis accuracy but faces real-world application issues due to insufficient validation in clinical workflows and diverse populations. This study addresses gaps in medical AI downstream accountability through a case study on age-related macular degeneration (AMD) diagnosis and severity classification. We designed and implemented an AI-assisted diagnostic workflow for AMD, comparing diagnostic performance with and without AI assistance among 24 clinicians from 12 institutions with real patient data sampled from the Age-Related Eye Disease Study (AREDS). Additionally, we demonstrated continual enhancement of an existing AI model by incorporating approximately 40,000 additional medical images (named AREDS2 dataset). The improved model was then systematically evaluated using both AREDS and AREDS2 test sets, as well as an external test set from Singapore. AI assistance markedly enhanced diagnostic accuracy and classification for 23 out of 24 clinicians, with the average F1-score increasing by 20% from 37.71 (Manual) to 45.52 (Manual + AI) (P-value < 0.0001), achieving an improvement of over 50% in some cases. In terms of efficiency, AI assistance reduced diagnostic times for 17 out of the 19 clinicians tracked, with time savings of up to 40%. Furthermore, a model equipped with continual learning showed robust performance across three independent datasets, recording a 29% increase in accuracy, and elevating the F1-score from 42 to 54 in the Singapore population.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-23 15:01:09 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.049177"
    },
    {
        "index": "#147",
        "title": "ViBERTgrid BiLSTM-CRF: Multimodal Key Information Extraction from Unstructured Financial Documents",
        "link": "/arxiv/2409.15004",
        "arxiv_id": "2409.15004",
        "authors": "Furkan Pala, Mehmet Yasin Akpınar, Onur Deniz, Gülşen Eryiğit",
        "summary": "Multimodal key information extraction (KIE) models have been studied extensively on semi-structured documents. However, their investigation on unstructured documents is an emerging research topic. The paper presents an approach to adapt a multimodal transformer (i.e., ViBERTgrid previously explored on semi-structured documents) for unstructured financial documents, by incorporating a BiLSTM-CRF layer. The proposed ViBERTgrid BiLSTM-CRF model demonstrates a significant improvement in performance (up to 2 percentage points) on named entity recognition from unstructured documents in financial domain, while maintaining its KIE performance on semi-structured documents. As an additional contribution, we publicly released token-level annotations for the SROIE dataset in order to pave the way for its use in multimodal sequence labeling models.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Information Retrieval",
        "date": "2024-09-23 13:28:06 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.049436"
    },
    {
        "index": "#148",
        "title": "Multi-Modal Generative AI: Multi-modal LLM, Diffusion and Beyond",
        "link": "/arxiv/2409.14993",
        "arxiv_id": "2409.14993",
        "authors": "Hong Chen, Xin Wang, Yuwei Zhou, Bin Huang, Yipeng Zhang, Wei Feng, Houlun Chen, Zeyang Zhang, Siao Tang, Wenwu Zhu",
        "summary": "Multi-modal generative AI has received increasing attention in both academia and industry. Particularly, two dominant families of techniques are: i) The multi-modal large language model (MLLM) such as GPT-4V, which shows impressive ability for multi-modal understanding; ii) The diffusion model such as Sora, which exhibits remarkable multi-modal powers, especially with respect to visual generation. As such, one natural question arises: Is it possible to have a unified model for both understanding and generation? To answer this question, in this paper, we first provide a detailed review of both MLLM and diffusion models, including their probabilistic modeling procedure, multi-modal architecture design, and advanced applications to image/video large language models as well as text-to-image/video generation. Then, we discuss the two important questions on the unified model: i) whether the unified model should adopt the auto-regressive or diffusion probabilistic modeling, and ii) whether the model should utilize a dense architecture or the Mixture of Experts(MoE) architectures to better support generation and understanding, two objectives. We further provide several possible strategies for building a unified model and analyze their potential advantages and disadvantages. We also summarize existing large-scale multi-modal datasets for better model pretraining in the future. To conclude the paper, we present several challenging future directions, which we believe can contribute to the ongoing advancement of multi-modal generative AI.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 13:16:09 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.049676"
    },
    {
        "index": "#149",
        "title": "CON: Continual Object Navigation via Data-Free Inter-Agent Knowledge Transfer in Unseen and Unfamiliar Places",
        "link": "/arxiv/2409.14899",
        "arxiv_id": "2409.14899",
        "authors": "Kouki Terashima, Daiki Iwata, Kanji Tanaka",
        "summary": "This work explores the potential of brief inter-agent knowledge transfer (KT) to enhance the robotic object goal navigation (ON) in unseen and unfamiliar environments. Drawing on the analogy of human travelers acquiring local knowledge, we propose a framework in which a traveler robot (student) communicates with local robots (teachers) to obtain ON knowledge through minimal interactions. We frame this process as a data-free continual learning (CL) challenge, aiming to transfer knowledge from a black-box model (teacher) to a new model (student). In contrast to approaches like zero-shot ON using large language models (LLMs), which utilize inherently communication-friendly natural language for knowledge representation, the other two major ON approaches -- frontier-driven methods using object feature maps and learning-based ON using neural state-action maps -- present complex challenges where data-free KT remains largely uncharted. To address this gap, we propose a lightweight, plug-and-play KT module targeting non-cooperative black-box teachers in open-world settings. Using the universal assumption that every teacher robot has vision and mobility capabilities, we define state-action history as the primary knowledge base. Our formulation leads to the development of a query-based occupancy map that dynamically represents target object locations, serving as an effective and communication-friendly knowledge representation. We validate the effectiveness of our method through experiments conducted in the Habitat environment.",
        "subjects": "Robotics, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-23 10:50:11 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.049869"
    },
    {
        "index": "#150",
        "title": "Observe Then Act: Asynchronous Active Vision-Action Model for Robotic Manipulation",
        "link": "/arxiv/2409.14891",
        "arxiv_id": "2409.14891",
        "authors": "Guokang Wang, Hang Li, Shuyuan Zhang, Yanhong Liu, Huaping Liu",
        "summary": "In real-world scenarios, many robotic manipulation tasks are hindered by occlusions and limited fields of view, posing significant challenges for passive observation-based models that rely on fixed or wrist-mounted cameras. In this paper, we investigate the problem of robotic manipulation under limited visual observation and propose a task-driven asynchronous active vision-action model.Our model serially connects a camera Next-Best-View (NBV) policy with a gripper Next-Best Pose (NBP) policy, and trains them in a sensor-motor coordination framework using few-shot reinforcement learning. This approach allows the agent to adjust a third-person camera to actively observe the environment based on the task goal, and subsequently infer the appropriate manipulation actions.We trained and evaluated our model on 8 viewpoint-constrained tasks in RLBench. The results demonstrate that our model consistently outperforms baseline algorithms, showcasing its effectiveness in handling visual constraints in manipulation tasks.",
        "subjects": "Robotics, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 10:38:20 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.050064"
    },
    {
        "index": "#151",
        "title": "Towards Ground-truth-free Evaluation of Any Segmentation in Medical Images",
        "link": "/arxiv/2409.14874",
        "arxiv_id": "2409.14874",
        "authors": "Ahjol Senbi, Tianyu Huang, Fei Lyu, Qing Li, Yuhui Tao, Wei Shao, Qiang Chen, Chengyan Wang, Shuo Wang, Tao Zhou, Yizhe Zhang",
        "summary": "We are interested in building a ground-truth-free evaluation model to assess the quality of segmentations produced by SAM (Segment Anything Model) and its variants in medical images. This model estimates segmentation quality scores by comparing input images with their corresponding segmentation maps. Building on prior research, we frame this as a regression problem within a supervised learning framework, using Dice scores (and optionally other metrics) to compute the training loss. The model is trained using a large collection of public datasets of medical images with segmentation predictions from SAM and its variants. We name this model EvanySeg (Evaluation of Any Segmentation in Medical Images). Our exploration of convolution-based models (e.g., ResNet) and transformer-based models (e.g., ViT) revealed that ViT offers superior performance for EvanySeg. This model can be employed for various tasks, including: (1) identifying poorly segmented samples by detecting low-percentile segmentation quality scores; (2) benchmark segmentation models without ground truth by averaging scores across test samples; (3) alerting human experts during human-AI collaboration by applying a threshold within the score space; and (4) selecting the best segmentation prediction for each test sample at test time when multiple segmentation models are available, by choosing the prediction with the highest score. Models and code will be made available at https://github.com/ahjolsenbics/EvanySeg.",
        "subjects": "Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-23 10:12:08 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.050307"
    },
    {
        "index": "#152",
        "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
        "link": "/arxiv/2409.14846",
        "arxiv_id": "2409.14846",
        "authors": "Junyang Zhang, Mu Yuan, Ruiguang Zhong, Puhan Luo, Huiyou Zhan, Ningkang Zhang, Chengchen Hu, Xiangyang Li",
        "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural language processing techniques, offering substantial application potential. However, these models demand extensive resources during inference. Adaptive attention techniques can dynamically reduce computational redundancy and thus improve efficiency. Although current adaptive attention methods significantly reduce the memory requirements of Transformer-based language models, they are not tailored for LVLMs. We observe that LVLMs generate responses from both remote image tokens and local text tokens, and different modalities have different attention patterns. This observation inspires us to manage the attention for each modality separately. Specifically, for visual input, we store the cache of potentially useful information but only compute the most critical parts. For language input, we care more about local information. Based on our observation and analysis of vision-language attention patterns, we develop A-VL, a plug-and-play adaptive attention tailored for LVLM inference. Extensive evaluations on three vision-language tasks and five datasets show the effectiveness of our designs. Our approach A-VL outperforms existing adaptive attention methods in reducing memory usage and computational load without compromising performance.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 09:22:59 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.050520"
    },
    {
        "index": "#153",
        "title": "RoWSFormer: A Robust Watermarking Framework with Swin Transformer for Enhanced Geometric Attack Resilience",
        "link": "/arxiv/2409.14829",
        "arxiv_id": "2409.14829",
        "authors": "Weitong Chen, Yuheng Li",
        "summary": "In recent years, digital watermarking techniques based on deep learning have been widely studied. To achieve both imperceptibility and robustness of image watermarks, most current methods employ convolutional neural networks to build robust watermarking frameworks. However, despite the success of CNN-based watermarking models, they struggle to achieve robustness against geometric attacks due to the limitations of convolutional neural networks in capturing global and long-range relationships. To address this limitation, we propose a robust watermarking framework based on the Swin Transformer, named RoWSFormer. Specifically, we design the Locally-Channel Enhanced Swin Transformer Block as the core of both the encoder and decoder. This block utilizes the self-attention mechanism to capture global and long-range information, thereby significantly improving adaptation to geometric distortions. Additionally, we construct the Frequency-Enhanced Transformer Block to extract frequency domain information, which further strengthens the robustness of the watermarking framework. Experimental results demonstrate that our RoWSFormer surpasses existing state-of-the-art watermarking methods. For most non-geometric attacks, RoWSFormer improves the PSNR by 3 dB while maintaining the same extraction accuracy. In the case of geometric attacks (such as rotation, scaling, and affine transformations), RoWSFormer achieves over a 6 dB improvement in PSNR, with extraction accuracy exceeding 97\\%.",
        "subjects": "Multimedia, Computer Vision and Pattern Recognition, Image and Video Processing",
        "date": "2024-09-23 08:59:55 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.050701"
    },
    {
        "index": "#154",
        "title": "Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models",
        "link": "/arxiv/2409.14785",
        "arxiv_id": "2409.14785",
        "authors": "Patrick Amadeus Irawan, Genta Indra Winata, Samuel Cahyawijaya, Ayu Purwarianti",
        "summary": "Natural Language Explanation (NLE) aims to elucidate the decision-making process by providing detailed, human-friendly explanations in natural language. It helps demystify the decision-making processes of large vision-language models (LVLMs) through the use of language models. While existing methods for creating a Vision Question-Answering with Natural Language Explanation (VQA-NLE) datasets can provide explanations, they heavily rely on human annotations that are time-consuming and costly. In this study, we propose a novel approach that leverages LVLMs to efficiently generate high-quality synthetic VQA-NLE datasets. By evaluating our synthetic data, we showcase how advanced prompting techniques can lead to the production of high-quality VQA-NLE data. Our findings indicate that this proposed method achieves up to 20x faster than human annotation, with only a minimal decrease in qualitative metrics, achieving robust quality that is nearly equivalent to human-annotated data. Furthermore, we show that incorporating visual prompts significantly enhances the relevance of text generation. Our study paves the way for a more efficient and robust automated generation of multi-modal NLE data, offering a promising solution to the problem.",
        "subjects": "Computation and Language, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 07:59:50 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.050884"
    },
    {
        "index": "#155",
        "title": "TransUKAN:Computing-Efficient Hybrid KAN-Transformer for Enhanced Medical Image Segmentation",
        "link": "/arxiv/2409.14676",
        "arxiv_id": "2409.14676",
        "authors": "Yanlin Wu, Tao Li, Zhihong Wang, Hong Kang, Along He",
        "summary": "U-Net is currently the most widely used architecture for medical image segmentation. Benefiting from its unique encoder-decoder architecture and skip connections, it can effectively extract features from input images to segment target regions. The commonly used U-Net is typically based on convolutional operations or Transformers, modeling the dependencies between local or global information to accomplish medical image analysis tasks. However, convolutional layers, fully connected layers, and attention mechanisms used in this process introduce a significant number of parameters, often requiring the stacking of network layers to model complex nonlinear relationships, which can impact the training process. To address these issues, we propose TransUKAN. Specifically, we have improved the KAN to reduce memory usage and computational load. On this basis, we explored an effective combination of KAN, Transformer, and U-Net structures. This approach enhances the model's capability to capture nonlinear relationships by introducing only a small number of additional parameters and compensates for the Transformer structure's deficiency in local information extraction. We validated TransUKAN on multiple medical image segmentation tasks. Experimental results demonstrate that TransUKAN achieves excellent performance with significantly reduced parameters. The code will be available athttps://github.com/wuyanlin-wyl/TransUKAN.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 02:52:49 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.051078"
    },
    {
        "index": "#156",
        "title": "RACER: Rich Language-Guided Failure Recovery Policies for Imitation Learning",
        "link": "/arxiv/2409.14674",
        "arxiv_id": "2409.14674",
        "authors": "Yinpei Dai, Jayjun Lee, Nima Fazeli, Joyce Chai",
        "summary": "Developing robust and correctable visuomotor policies for robotic manipulation is challenging due to the lack of self-recovery mechanisms from failures and the limitations of simple language instructions in guiding robot actions. To address these issues, we propose a scalable data generation pipeline that automatically augments expert demonstrations with failure recovery trajectories and fine-grained language annotations for training. We then introduce Rich languAge-guided failure reCovERy (RACER), a supervisor-actor framework, which combines failure recovery data with rich language descriptions to enhance robot control. RACER features a vision-language model (VLM) that acts as an online supervisor, providing detailed language guidance for error correction and task execution, and a language-conditioned visuomotor policy as an actor to predict the next actions. Our experimental results show that RACER outperforms the state-of-the-art Robotic View Transformer (RVT) on RLbench across various evaluation settings, including standard long-horizon tasks, dynamic goal-change tasks and zero-shot unseen tasks, achieving superior performance in both simulated and real world environments. Videos and code are available at: https://rich-language-failure-recovery.github.io.",
        "subjects": "Robotics, Computation and Language, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 02:50:33 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.051292"
    },
    {
        "index": "#157",
        "title": "FedGCA: Global Consistent Augmentation Based Single-Source Federated Domain Generalization",
        "link": "/arxiv/2409.14671",
        "arxiv_id": "2409.14671",
        "authors": "Yuan Liu, Shu Wang, Zhe Qu, Xingyu Li, Shichao Kan, Jianxin Wang",
        "summary": "Federated Domain Generalization (FedDG) aims to train the global model for generalization ability to unseen domains with multi-domain training samples. However, clients in federated learning networks are often confined to a single, non-IID domain due to inherent sampling and temporal limitations. The lack of cross-domain interaction and the in-domain divergence impede the learning of domain-common features and limit the effectiveness of existing FedDG, referred to as the single-source FedDG (sFedDG) problem. To address this, we introduce the Federated Global Consistent Augmentation (FedGCA) method, which incorporates a style-complement module to augment data samples with diverse domain styles. To ensure the effective integration of augmented samples, FedGCA employs both global guided semantic consistency and class consistency, mitigating inconsistencies from local semantics within individual clients and classes across multiple clients. The conducted extensive experiments demonstrate the superiority of FedGCA.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2024-09-23 02:24:46 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.051562"
    },
    {
        "index": "#158",
        "title": "RobotFingerPrint: Unified Gripper Coordinate Space for Multi-Gripper Grasp Synthesis",
        "link": "/arxiv/2409.14519",
        "arxiv_id": "2409.14519",
        "authors": "Ninad Khargonkar, Luis Felipe Casas, Balakrishnan Prabhakaran, Yu Xiang",
        "summary": "We introduce a novel representation named as the unified gripper coordinate space for grasp synthesis of multiple grippers. The space is a 2D surface of a sphere in 3D using longitude and latitude as its coordinates, and it is shared for all robotic grippers. We propose a new algorithm to map the palm surface of a gripper into the unified gripper coordinate space, and design a conditional variational autoencoder to predict the unified gripper coordinates given an input object. The predicted unified gripper coordinates establish correspondences between the gripper and the object, which can be used in an optimization problem to solve the grasp pose and the finger joints for grasp synthesis. We demonstrate that using the unified gripper coordinate space improves the success rate and diversity in the grasp synthesis of multiple grippers.",
        "subjects": "Robotics, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-22 16:25:31 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.051775"
    },
    {
        "index": "#159",
        "title": "SPAQ-DL-SLAM: Towards Optimizing Deep Learning-based SLAM for Resource-Constrained Embedded Platforms",
        "link": "/arxiv/2409.14515",
        "arxiv_id": "2409.14515",
        "authors": "Niraj Pudasaini, Muhammad Abdullah Hanif, Muhammad Shafique",
        "summary": "Optimizing Deep Learning-based Simultaneous Localization and Mapping (DL-SLAM) algorithms is essential for efficient implementation on resource-constrained embedded platforms, enabling real-time on-board computation in autonomous mobile robots. This paper presents SPAQ-DL-SLAM, a framework that strategically applies Structured Pruning and Quantization (SPAQ) to the architecture of one of the state-ofthe-art DL-SLAM algorithms, DROID-SLAM, for resource and energy-efficiency. Specifically, we perform structured pruning with fine-tuning based on layer-wise sensitivity analysis followed by 8-bit post-training static quantization (PTQ) on the deep learning modules within DROID-SLAM. Our SPAQ-DROIDSLAM model, optimized version of DROID-SLAM model using our SPAQ-DL-SLAM framework with 20% structured pruning and 8-bit PTQ, achieves an 18.9% reduction in FLOPs and a 79.8% reduction in overall model size compared to the DROID-SLAM model. Our evaluations on the TUM-RGBD benchmark shows that SPAQ-DROID-SLAM model surpasses the DROID-SLAM model by an average of 10.5% on absolute trajectory error (ATE) metric. Additionally, our results on the ETH3D SLAM training benchmark demonstrate enhanced generalization capabilities of the SPAQ-DROID-SLAM model, seen by a higher Area Under the Curve (AUC) score and success in 2 additional data sequences compared to the DROIDSLAM model. Despite these improvements, the model exhibits performance variance on the distinct Vicon Room sequences from the EuRoC dataset, which are captured at high angular velocities. This varying performance at some distinct scenarios suggests that designing DL-SLAM algorithms taking operating environments and tasks in consideration can achieve optimal performance and resource efficiency for deployment in resource-constrained embedded platforms.",
        "subjects": "Robotics, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-22 16:19:47 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.114103"
    },
    {
        "index": "#160",
        "title": "Lesion Segmentation in Whole-Body Multi-Tracer PET-CT Images; a Contribution to AutoPET 2024 Challenge",
        "link": "/arxiv/2409.14475",
        "arxiv_id": "2409.14475",
        "authors": "Mehdi Astaraki, Simone Bendazzoli",
        "summary": "The automatic segmentation of pathological regions within whole-body PET-CT volumes has the potential to streamline various clinical applications such as diagno-sis, prognosis, and treatment planning. This study aims to address this challenge by contributing to the AutoPET MICCAI 2024 challenge through a proposed workflow that incorporates image preprocessing, tracer classification, and lesion segmentation steps. The implementation of this pipeline led to a significant enhancement in the segmentation accuracy of the models. This improvement is evidenced by an average overall Dice score of 0.548 across 1611 training subjects, 0.631 and 0.559 for classi-fied FDG and PSMA subjects of the training set, and 0.792 on the preliminary testing phase dataset.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition",
        "date": "2024-09-22 14:50:46 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.228970"
    },
    {
        "index": "#161",
        "title": "Detection of pulmonary pathologies using convolutional neural networks, Data Augmentation, ResNet50 and Vision Transformers",
        "link": "/arxiv/2409.14446",
        "arxiv_id": "2409.14446",
        "authors": "Pablo Ramirez Amador, Dinarle Milagro Ortega, Arnold Cesarano",
        "summary": "Pulmonary diseases are a public health problem that requires accurate and fast diagnostic techniques. In this paper, a method based on convolutional neural networks (CNN), Data Augmentation, ResNet50 and Vision Transformers (ViT) is proposed to detect lung pathologies from medical images. A dataset of X-ray images and CT scans of patients with different lung diseases, such as cancer, pneumonia, tuberculosis and fibrosis, is used. The results obtained by the proposed method are compared with those of other existing methods, using performance metrics such as accuracy, sensitivity, specificity and area under the ROC curve. The results show that the proposed method outperforms the other methods in all metrics, achieving an accuracy of 98% and an area under the ROC curve of 99%. It is concluded that the proposed method is an effective and promising tool for the diagnosis of pulmonary pathologies by medical imaging.",
        "subjects": "Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2024-09-22 13:54:28 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.229207"
    },
    {
        "index": "#162",
        "title": "Dormant: Defending against Pose-driven Human Image Animation",
        "link": "/arxiv/2409.14424",
        "arxiv_id": "2409.14424",
        "authors": "Jiachen Zhou, Mingsi Wang, Tianlin Li, Guozhu Meng, Kai Chen",
        "summary": "Pose-driven human image animation has achieved tremendous progress, enabling the generation of vivid and realistic human videos from just one single photo. However, it conversely exacerbates the risk of image misuse, as attackers may use one available image to create videos involving politics, violence and other illegal content. To counter this threat, we propose Dormant, a novel protection approach tailored to defend against pose-driven human image animation techniques. Dormant applies protective perturbation to one human image, preserving the visual similarity to the original but resulting in poor-quality video generation. The protective perturbation is optimized to induce misextraction of appearance features from the image and create incoherence among the generated video frames. Our extensive evaluation across 8 animation methods and 4 datasets demonstrates the superiority of Dormant over 6 baseline protection methods, leading to misaligned identities, visual distortions, noticeable artifacts, and inconsistent frames in the generated videos. Moreover, Dormant shows effectiveness on 6 real-world commercial services, even with fully black-box access.",
        "subjects": "Cryptography and Security, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2024-09-22 12:51:32 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.229421"
    },
    {
        "index": "#163",
        "title": "GraspMamba: A Mamba-based Language-driven Grasp Detection Framework with Hierarchical Feature Learning",
        "link": "/arxiv/2409.14403",
        "arxiv_id": "2409.14403",
        "authors": "Huy Hoang Nguyen, An Vuong, Anh Nguyen, Ian Reid, Minh Nhat Vu",
        "summary": "Grasp detection is a fundamental robotic task critical to the success of many industrial applications. However, current language-driven models for this task often struggle with cluttered images, lengthy textual descriptions, or slow inference speed. We introduce GraspMamba, a new language-driven grasp detection method that employs hierarchical feature fusion with Mamba vision to tackle these challenges. By leveraging rich visual features of the Mamba-based backbone alongside textual information, our approach effectively enhances the fusion of multimodal features. GraspMamba represents the first Mamba-based grasp detection model to extract vision and language features at multiple scales, delivering robust performance and rapid inference time. Intensive experiments show that GraspMamba outperforms recent methods by a clear margin. We validate our approach through real-world robotic experiments, highlighting its fast inference speed.",
        "subjects": "Robotics, Computer Vision and Pattern Recognition",
        "date": "2024-09-22 11:45:48 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.229621"
    },
    {
        "index": "#164",
        "title": "Frequency-regularized Neural Representation Method for Sparse-view Tomographic Reconstruction",
        "link": "/arxiv/2409.14394",
        "arxiv_id": "2409.14394",
        "authors": "Jingmou Xian, Jian Zhu, Haolin Liao, Si Li",
        "summary": "Sparse-view tomographic reconstruction is a pivotal direction for reducing radiation dose and augmenting clinical applicability. While many research works have proposed the reconstruction of tomographic images from sparse 2D projections, existing models tend to excessively focus on high-frequency information while overlooking low-frequency components within the sparse input images. This bias towards high-frequency information often leads to overfitting, particularly intense at edges and boundaries in the reconstructed slices. In this paper, we introduce the Frequency Regularized Neural Attenuation/Activity Field (Freq-NAF) for self-supervised sparse-view tomographic reconstruction. Freq-NAF mitigates overfitting by incorporating frequency regularization, directly controlling the visible frequency bands in the neural network input. This approach effectively balances high-frequency and low-frequency information. We conducted numerical experiments on CBCT and SPECT datasets, and our method demonstrates state-of-the-art accuracy.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition",
        "date": "2024-09-22 11:19:38 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.229816"
    },
    {
        "index": "#165",
        "title": "Thinking in Granularity: Dynamic Quantization for Image Super-Resolution by Intriguing Multi-Granularity Clues",
        "link": "/arxiv/2409.14330",
        "arxiv_id": "2409.14330",
        "authors": "Mingshen Wang, Zhao Zhang, Feng Li, Ke Xu, Kang Miao, Meng Wang",
        "summary": "Dynamic quantization has attracted rising attention in image super-resolution (SR) as it expands the potential of heavy SR models onto mobile devices while preserving competitive performance. Existing methods explore layer-to-bit configuration upon varying local regions, adaptively allocating the bit to each layer and patch. Despite the benefits, they still fall short in the trade-off of SR accuracy and quantization efficiency. Apart from this, adapting the quantization level for each layer individually can disturb the original inter-layer relationships, thus diminishing the representation capability of quantized models. In this work, we propose Granular-DQ, which capitalizes on the intrinsic characteristics of images while dispensing with the previous consideration for layer sensitivity in quantization. Granular-DQ conducts a multi-granularity analysis of local patches with further exploration of their information densities, achieving a distinctive patch-wise and layer-invariant dynamic quantization paradigm. Specifically, Granular-DQ initiates by developing a granularity-bit controller (GBC) to apprehend the coarse-to-fine granular representations of different patches, matching their proportional contribution to the entire image to determine the proper bit-width allocation. On this premise, we investigate the relation between bit-width and information density, devising an entropy-to-bit (E2B) mechanism that enables further fine-grained dynamic bit adaption of high-bit patches. Extensive experiments validate the superiority and generalization ability of Granular-DQ over recent state-of-the-art methods on various SR models. Code will be available at \\url{https://github.com/MmmingS/Granular-DQ.git}.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition",
        "date": "2024-09-22 06:29:54 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.230098"
    },
    {
        "index": "#166",
        "title": "Can-Do! A Dataset and Neuro-Symbolic Grounded Framework for Embodied Planning with Large Multimodal Models",
        "link": "/arxiv/2409.14277",
        "arxiv_id": "2409.14277",
        "authors": "Yew Ken Chia, Qi Sun, Lidong Bing, Soujanya Poria",
        "summary": "Large multimodal models have demonstrated impressive problem-solving abilities in vision and language tasks, and have the potential to encode extensive world knowledge. However, it remains an open challenge for these models to perceive, reason, plan, and act in realistic environments. In this work, we introduce Can-Do, a benchmark dataset designed to evaluate embodied planning abilities through more diverse and complex scenarios than previous datasets. Our dataset includes 400 multimodal samples, each consisting of natural language user instructions, visual images depicting the environment, state changes, and corresponding action plans. The data encompasses diverse aspects of commonsense knowledge, physical understanding, and safety awareness. Our fine-grained analysis reveals that state-of-the-art models, including GPT-4V, face bottlenecks in visual perception, comprehension, and reasoning abilities. To address these challenges, we propose NeuroGround, a neurosymbolic framework that first grounds the plan generation in the perceived environment states and then leverages symbolic planning engines to augment the model-generated plans. Experimental results demonstrate the effectiveness of our framework compared to strong baselines. Our code and dataset are available at https://embodied-planning.github.io.",
        "subjects": "Artificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Robotics",
        "date": "2024-09-22 00:30:11 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.230358"
    },
    {
        "index": "#167",
        "title": "FeDETR: a Federated Approach for Stenosis Detection in Coronary Angiography",
        "link": "/arxiv/2409.14268",
        "arxiv_id": "2409.14268",
        "authors": "Raffaele Mineo, Amelia Sorrenti, Federica Proietto Salanitri",
        "summary": "Assessing the severity of stenoses in coronary angiography is critical to the patient's health, as coronary stenosis is an underlying factor in heart failure. Current practice for grading coronary lesions, i.e. fractional flow reserve (FFR) or instantaneous wave-free ratio (iFR), suffers from several drawbacks, including time, cost and invasiveness, alongside potential interobserver variability. In this context, some deep learning methods have emerged to assist cardiologists in automating the estimation of FFR/iFR values. Despite the effectiveness of these methods, their reliance on large datasets is challenging due to the distributed nature of sensitive medical data. Federated learning addresses this challenge by aggregating knowledge from multiple nodes to improve model generalization, while preserving data privacy. We propose the first federated detection transformer approach, FeDETR, to assess stenosis severity in angiography videos based on FFR/iFR values estimation. In our approach, each node trains a detection transformer (DETR) on its local dataset, with the central server federating the backbone part of the network. The proposed method is trained and evaluated on a dataset collected from five hospitals, consisting of 1001 angiographic examinations, and its performance is compared with state-of-the-art federated learning methods.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-21 23:52:05 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.230574"
    },
    {
        "index": "#168",
        "title": "R-AIF: Solving Sparse-Reward Robotic Tasks from Pixels with Active Inference and World Models",
        "link": "/arxiv/2409.14216",
        "arxiv_id": "2409.14216",
        "authors": "Viet Dung Nguyen, Zhizhuo Yang, Christopher L. Buckley, Alexander Ororbia",
        "summary": "Although research has produced promising results demonstrating the utility of active inference (AIF) in Markov decision processes (MDPs), there is relatively less work that builds AIF models in the context of environments and problems that take the form of partially observable Markov decision processes (POMDPs). In POMDP scenarios, the agent must infer the unobserved environmental state from raw sensory observations, e.g., pixels in an image. Additionally, less work exists in examining the most difficult form of POMDP-centered control: continuous action space POMDPs under sparse reward signals. In this work, we address issues facing the AIF modeling paradigm by introducing novel prior preference learning techniques and self-revision schedules to help the agent excel in sparse-reward, continuous action, goal-based robotic control POMDP environments. Empirically, we show that our agents offer improved performance over state-of-the-art models in terms of cumulative rewards, relative stability, and success rate. The code in support of this work can be found at https://github.com/NACLab/robust-active-inference.",
        "subjects": "Robotics, Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-21 18:32:44 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.230791"
    },
    {
        "index": "#169",
        "title": "UniMo: Universal Motion Correction For Medical Images without Network Retraining",
        "link": "/arxiv/2409.14204",
        "arxiv_id": "2409.14204",
        "authors": "Jian Wang, Razieh Faghihpirayesh, Danny Joca, Polina Golland, Ali Gholipour",
        "summary": "In this paper, we introduce a Universal Motion Correction (UniMo) framework, leveraging deep neural networks to tackle the challenges of motion correction across diverse imaging modalities. Our approach employs advanced neural network architectures with equivariant filters, overcoming the limitations of current models that require iterative inference or retraining for new image modalities. UniMo enables one-time training on a single modality while maintaining high stability and adaptability for inference across multiple unseen image modalities. We developed a joint learning framework that integrates multimodal knowledge from both shape and images that faithfully improve motion correction accuracy despite image appearance variations. UniMo features a geometric deformation augmenter that enhances the robustness of global motion correction by addressing any local deformations whether they are caused by object deformations or geometric distortions, and also generates augmented data to improve the training process. Our experimental results, conducted on various datasets with four different image modalities, demonstrate that UniMo surpasses existing motion correction methods in terms of accuracy. By offering a comprehensive solution to motion correction, UniMo marks a significant advancement in medical imaging, especially in challenging applications with wide ranges of motion, such as fetal imaging. The code for this work is available online, https://github.com/IntelligentImaging/UNIMO/.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition",
        "date": "2024-09-21 17:36:11 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.231050"
    },
    {
        "index": "#170",
        "title": "A Sinkhorn Regularized Adversarial Network for Image Guided DEM Super-resolution using Frequency Selective Hybrid Graph Transformer",
        "link": "/arxiv/2409.14198",
        "arxiv_id": "2409.14198",
        "authors": "Subhajit Paul, Ashutosh Gupta",
        "summary": "Digital Elevation Model (DEM) is an essential aspect in the remote sensing (RS) domain to analyze various applications related to surface elevations. Here, we address the generation of high-resolution (HR) DEMs using HR multi-spectral (MX) satellite imagery as a guide by introducing a novel hybrid transformer model consisting of Densely connected Multi-Residual Block (DMRB) and multi-headed Frequency Selective Graph Attention (M-FSGA). To promptly regulate this process, we utilize the notion of discriminator spatial maps as the conditional attention to the MX guide. Further, we present a novel adversarial objective related to optimizing Sinkhorn distance with classical GAN. In this regard, we provide both theoretical and empirical substantiation of better performance in terms of vanishing gradient issues and numerical convergence. Based on our experiments on 4 different DEM datasets, we demonstrate both qualitative and quantitative comparisons with available baseline methods and show that the performance of our proposed model is superior to others with sharper details and minimal errors.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition",
        "date": "2024-09-21 16:59:08 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.231241"
    },
    {
        "index": "#171",
        "title": "Accelerated Multi-Contrast MRI Reconstruction via Frequency and Spatial Mutual Learning",
        "link": "/arxiv/2409.14113",
        "arxiv_id": "2409.14113",
        "authors": "Qi Chen, Xiaohan Xing, Zhen Chen, Zhiwei Xiong",
        "summary": "To accelerate Magnetic Resonance (MR) imaging procedures, Multi-Contrast MR Reconstruction (MCMR) has become a prevalent trend that utilizes an easily obtainable modality as an auxiliary to support high-quality reconstruction of the target modality with under-sampled k-space measurements. The exploration of global dependency and complementary information across different modalities is essential for MCMR. However, existing methods either struggle to capture global dependency due to the limited receptive field or suffer from quadratic computational complexity. To tackle this dilemma, we propose a novel Frequency and Spatial Mutual Learning Network (FSMNet), which efficiently explores global dependencies across different modalities. Specifically, the features for each modality are extracted by the Frequency-Spatial Feature Extraction (FSFE) module, featuring a frequency branch and a spatial branch. Benefiting from the global property of the Fourier transform, the frequency branch can efficiently capture global dependency with an image-size receptive field, while the spatial branch can extract local features. To exploit complementary information from the auxiliary modality, we propose a Cross-Modal Selective fusion (CMS-fusion) module that selectively incorporate the frequency and spatial features from the auxiliary modality to enhance the corresponding branch of the target modality. To further integrate the enhanced global features from the frequency branch and the enhanced local features from the spatial branch, we develop a Frequency-Spatial fusion (FS-fusion) module, resulting in a comprehensive feature representation for the target modality. Extensive experiments on the BraTS and fastMRI datasets demonstrate that the proposed FSMNet achieves state-of-the-art performance for the MCMR task with different acceleration factors. The code is available at: https://github.com/qic999/FSMNet.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition",
        "date": "2024-09-21 12:02:47 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.231440"
    },
    {
        "index": "#172",
        "title": "Window-based Channel Attention for Wavelet-enhanced Learned Image Compression",
        "link": "/arxiv/2409.14090",
        "arxiv_id": "2409.14090",
        "authors": "Heng Xu, Bowen Hai, Yushun Tang, Zhihai He",
        "summary": "Learned Image Compression (LIC) models have achieved superior rate-distortion performance than traditional codecs. Existing LIC models use CNN, Transformer, or Mixed CNN-Transformer as basic blocks. However, limited by the shifted window attention, Swin-Transformer-based LIC exhibits a restricted growth of receptive fields, affecting the ability to model large objects in the image. To address this issue, we incorporate window partition into channel attention for the first time to obtain large receptive fields and capture more global information. Since channel attention hinders local information learning, it is important to extend existing attention mechanisms in Transformer codecs to the space-channel attention to establish multiple receptive fields, being able to capture global correlations with large receptive fields while maintaining detailed characterization of local correlations with small receptive fields. We also incorporate the discrete wavelet transform into our Spatial-Channel Hybrid (SCH) framework for efficient frequency-dependent down-sampling and further enlarging receptive fields. Experiment results demonstrate that our method achieves state-of-the-art performances, reducing BD-rate by 18.54%, 23.98%, 22.33%, and 24.71% on four standard datasets compared to VTM-23.1.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition",
        "date": "2024-09-21 10:08:52 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.231637"
    },
    {
        "index": "#173",
        "title": "Recovering Global Data Distribution Locally in Federated Learning",
        "link": "/arxiv/2409.14063",
        "arxiv_id": "2409.14063",
        "authors": "Ziyu Yao",
        "summary": "Federated Learning (FL) is a distributed machine learning paradigm that enables collaboration among multiple clients to train a shared model without sharing raw data. However, a major challenge in FL is the label imbalance, where clients may exclusively possess certain classes while having numerous minority and missing classes. Previous works focus on optimizing local updates or global aggregation but ignore the underlying imbalanced label distribution across clients. In this paper, we propose a novel approach ReGL to address this challenge, whose key idea is to Recover the Global data distribution Locally. Specifically, each client uses generative models to synthesize images that complement the minority and missing classes, thereby alleviating label imbalance. Moreover, we adaptively fine-tune the image generation process using local real data, which makes the synthetic images align more closely with the global distribution. Importantly, both the generation and fine-tuning processes are conducted at the client-side without leaking data privacy. Through comprehensive experiments on various image classification datasets, we demonstrate the remarkable superiority of our approach over existing state-of-the-art works in fundamentally tackling label imbalance in FL.",
        "subjects": "Machine Learning, Computer Vision and Pattern Recognition",
        "date": "2024-09-21 08:35:04 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.231816"
    },
    {
        "index": "#174",
        "title": "ECHO: Environmental Sound Classification with Hierarchical Ontology-guided Semi-Supervised Learning",
        "link": "/arxiv/2409.14043",
        "arxiv_id": "2409.14043",
        "authors": "Pranav Gupta, Raunak Sharma, Rashmi Kumari, Sri Krishna Aditya, Shwetank Choudhary, Sumit Kumar, Kanchana M, Thilagavathy R",
        "summary": "Environment Sound Classification has been a well-studied research problem in the field of signal processing and up till now more focus has been laid on fully supervised approaches. Over the last few years, focus has moved towards semi-supervised methods which concentrate on the utilization of unlabeled data, and self-supervised methods which learn the intermediate representation through pretext task or contrastive learning. However, both approaches require a vast amount of unlabelled data to improve performance. In this work, we propose a novel framework called Environmental Sound Classification with Hierarchical Ontology-guided semi-supervised Learning (ECHO) that utilizes label ontology-based hierarchy to learn semantic representation by defining a novel pretext task. In the pretext task, the model tries to predict coarse labels defined by the Large Language Model (LLM) based on ground truth label ontology. The trained model is further fine-tuned in a supervised way to predict the actual task. Our proposed novel semi-supervised framework achieves an accuracy improvement in the range of 1\\% to 8\\% over baseline systems across three datasets namely UrbanSound8K, ESC-10, and ESC-50.",
        "subjects": "Sound, Computer Vision and Pattern Recognition, Audio and Speech Processing",
        "date": "2024-09-21 07:08:57 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.232046"
    },
    {
        "index": "#175",
        "title": "MSDet: Receptive Field Enhanced Multiscale Detection for Tiny Pulmonary Nodule",
        "link": "/arxiv/2409.14028",
        "arxiv_id": "2409.14028",
        "authors": "Guohui Cai, Ying Cai, Zeyu Zhang, Daji Ergu, Yuanzhouhan Cao, Binbin Hu, Zhibin Liao, Yang Zhao",
        "summary": "Pulmonary nodules are critical indicators for the early diagnosis of lung cancer, making their detection essential for timely treatment. However, traditional CT imaging methods suffered from cumbersome procedures, low detection rates, and poor localization accuracy. The subtle differences between pulmonary nodules and surrounding tissues in complex lung CT images, combined with repeated downsampling in feature extraction networks, often lead to missed or false detections of small nodules. Existing methods such as FPN, with its fixed feature fusion and limited receptive field, struggle to effectively overcome these issues. To address these challenges, our paper proposed three key contributions: Firstly, we proposed MSDet, a multiscale attention and receptive field network for detecting tiny pulmonary nodules. Secondly, we proposed the extended receptive domain (ERD) strategy to capture richer contextual information and reduce false positives caused by nodule occlusion. We also proposed the position channel attention mechanism (PCAM) to optimize feature learning and reduce multiscale detection errors, and designed the tiny object detection block (TODB) to enhance the detection of tiny nodules. Lastly, we conducted thorough experiments on the public LUNA16 dataset, achieving state-of-the-art performance, with an mAP improvement of 8.8% over the previous state-of-the-art method YOLOv8. These advancements significantly boosted detection accuracy and reliability, providing a more effective solution for early lung cancer diagnosis. The code will be available at https://github.com/CaiGuoHui123/MSDet",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition",
        "date": "2024-09-21 06:08:23 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.232273"
    },
    {
        "index": "#176",
        "title": "Simple Unsupervised Knowledge Distillation With Space Similarity",
        "link": "/arxiv/2409.13939",
        "arxiv_id": "2409.13939",
        "authors": "Aditya Singh, Haohan Wang",
        "summary": "As per recent studies, Self-supervised learning (SSL) does not readily extend to smaller architectures. One direction to mitigate this shortcoming while simultaneously training a smaller network without labels is to adopt unsupervised knowledge distillation (UKD). Existing UKD approaches handcraft preservation worthy inter/intra sample relationships between the teacher and its student. However, this may overlook/ignore other key relationships present in the mapping of a teacher. In this paper, instead of heuristically constructing preservation worthy relationships between samples, we directly motivate the student to model the teacher's embedding manifold. If the mapped manifold is similar, all inter/intra sample relationships are indirectly conserved. We first demonstrate that prior methods cannot preserve teacher's latent manifold due to their sole reliance on $L_2$ normalised embedding features. Subsequently, we propose a simple objective to capture the lost information due to normalisation. Our proposed loss component, termed \\textbf{space similarity}, motivates each dimension of a student's feature space to be similar to the corresponding dimension of its teacher. We perform extensive experiments demonstrating strong performance of our proposed approach on various benchmarks.",
        "subjects": "Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2024-09-20 22:54:39 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.232501"
    },
    {
        "index": "#177",
        "title": "RN-SDEs: Limited-Angle CT Reconstruction with Residual Null-Space Diffusion Stochastic Differential Equations",
        "link": "/arxiv/2409.13930",
        "arxiv_id": "2409.13930",
        "authors": "Jiaqi Guo, Santiago Lopez-Tapia, Wing Shun Li, Yunnan Wu, Marcelo Carignano, Vadim Backman, Vinayak P. Dravid, Aggelos K. Katsaggelos",
        "summary": "Computed tomography is a widely used imaging modality with applications ranging from medical imaging to material analysis. One major challenge arises from the lack of scanning information at certain angles, leading to distorted CT images with artifacts. This results in an ill-posed problem known as the Limited Angle Computed Tomography (LACT) reconstruction problem. To address this problem, we propose Residual Null-Space Diffusion Stochastic Differential Equations (RN-SDEs), which are a variant of diffusion models that characterize the diffusion process with mean-reverting (MR) stochastic differential equations. To demonstrate the generalizability of RN-SDEs, our experiments are conducted on two different LACT datasets, i.e., ChromSTEM and C4KC-KiTS. Through extensive experiments, we show that by leveraging learned Mean-Reverting SDEs as a prior and emphasizing data consistency using Range-Null Space Decomposition (RNSD) based rectification, RN-SDEs can restore high-quality images from severe degradation and achieve state-of-the-art performance in most LACT tasks. Additionally, we present a quantitative comparison of computational complexity and runtime efficiency, highlighting the superior effectiveness of our proposed approach.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition",
        "date": "2024-09-20 22:33:36 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.232733"
    },
    {
        "index": "#178",
        "title": "Learning to Play Video Games with Intuitive Physics Priors",
        "link": "/arxiv/2409.13886",
        "arxiv_id": "2409.13886",
        "authors": "Abhishek Jaiswal, Nisheeth Srivastava",
        "summary": "Video game playing is an extremely structured domain where algorithmic decision-making can be tested without adverse real-world consequences. While prevailing methods rely on image inputs to avoid the problem of hand-crafting state space representations, this approach systematically diverges from the way humans actually learn to play games. In this paper, we design object-based input representations that generalize well across a number of video games. Using these representations, we evaluate an agent's ability to learn games similar to an infant - with limited world experience, employing simple inductive biases derived from intuitive representations of physics from the real world. Using such biases, we construct an object category representation to be used by a Q-learning algorithm and assess how well it learns to play multiple games based on observed object affordances. Our results suggest that a human-like object interaction setup capably learns to play several video games, and demonstrates superior generalizability, particularly for unfamiliar objects. Further exploring such methods will allow machines to learn in a human-centric way, thus incorporating more human-like learning benefits.",
        "subjects": "Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2024-09-20 20:30:27 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.232989"
    },
    {
        "index": "#179",
        "title": "Deep Learning-Based Channel Squeeze U-Structure for Lung Nodule Detection and Segmentation",
        "link": "/arxiv/2409.13868",
        "arxiv_id": "2409.13868",
        "authors": "Mingxiu Sui, Jiacheng Hu, Tong Zhou, Zibo Liu, Likang Wen, Junliang Du",
        "summary": "This paper introduces a novel deep-learning method for the automatic detection and segmentation of lung nodules, aimed at advancing the accuracy of early-stage lung cancer diagnosis. The proposed approach leverages a unique \"Channel Squeeze U-Structure\" that optimizes feature extraction and information integration across multiple semantic levels of the network. This architecture includes three key modules: shallow information processing, channel residual structure, and channel squeeze integration. These modules enhance the model's ability to detect and segment small, imperceptible, or ground-glass nodules, which are critical for early diagnosis. The method demonstrates superior performance in terms of sensitivity, Dice similarity coefficient, precision, and mean Intersection over Union (IoU). Extensive experiments were conducted on the Lung Image Database Consortium (LIDC) dataset using five-fold cross-validation, showing excellent stability and robustness. The results indicate that this approach holds significant potential for improving computer-aided diagnosis systems, providing reliable support for radiologists in clinical practice and aiding in the early detection of lung cancer, especially in resource-limited settings",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition, Machine Learning",
        "date": "2024-09-20 19:47:07 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.233342"
    },
    {
        "index": "#180",
        "title": "AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble",
        "link": "/arxiv/2409.13779",
        "arxiv_id": "2409.13779",
        "authors": "Tanya Chutani, Saikiran Bonthu, Pranab Samanta, Nitin Singhal",
        "summary": "Positron Emission Tomography (PET) /Computed Tomography (CT) is crucial for diagnosing, managing, and planning treatment for various cancers. Developing reliable deep learning models for the segmentation of tumor lesions in PET/CT scans in a multi-tracer multicenter environment, is a critical area of research. Different tracers, such as Fluorodeoxyglucose (FDG) and Prostate-Specific Membrane Antigen (PSMA), have distinct physiological uptake patterns and data from different centers often vary in terms of acquisition protocols, scanner types, and patient populations. Because of this variability, it becomes more difficult to design reliable segmentation algorithms and generalization techniques due to variations in image quality and lesion detectability. To address this challenge, We trained a 3D Residual encoder U-Net within the no new U-Net framework, aiming to generalize the performance of automatic lesion segmentation of whole body PET/CT scans, across different tracers and clinical sites. Further, We explored several preprocessing techniques and ultimately settled on using the Total Segmentator to crop our training data. Additionally, we applied resampling during this process. During inference, we leveraged test-time augmentations and other post-processing techniques to enhance tumor lesion segmentation. Our team currently hold the top position in the Auto-PET III challenge and outperformed the challenge baseline model in the preliminary test set with Dice score of 0.9627.",
        "subjects": "Image and Video Processing, Artificial Intelligence, Computer Vision and Pattern Recognition",
        "date": "2024-09-19 20:18:39 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.233590"
    },
    {
        "index": "#181",
        "title": "Efficient Classification of Histopathology Images",
        "link": "/arxiv/2409.13720",
        "arxiv_id": "2409.13720",
        "authors": "Mohammad Iqbal Nouyed, Mary-Anne Hartley, Gianfranco Doretto, Donald A. Adjeroh",
        "summary": "This work addresses how to efficiently classify challenging histopathology images, such as gigapixel whole-slide images for cancer diagnostics with image-level annotation. We use images with annotated tumor regions to identify a set of tumor patches and a set of benign patches in a cancerous slide. Due to the variable nature of region of interest the tumor positive regions may refer to an extreme minority of the pixels. This creates an important problem during patch-level classification, where the majority of patches from an image labeled as 'cancerous' are actually tumor-free. This problem is different from semantic segmentation which associates a label to every pixel in an image, because after patch extraction we are only dealing with patch-level labels.Most existing approaches address the data imbalance issue by mitigating the data shortage in minority classes in order to prevent the model from being dominated by the majority classes. These methods include data re-sampling, loss re-weighting, margin modification, and data augmentation. In this work, we mitigate the patch-level class imbalance problem by taking a divide-and-conquer approach. First, we partition the data into sub-groups, and define three separate classification sub-problems based on these data partitions. Then, using an information-theoretic cluster-based sampling of deep image patch features, we sample discriminative patches from the sub-groups. Using these sampled patches, we build corresponding deep models to solve the new classification sub-problems. Finally, we integrate information learned from the respective models to make a final decision on the patches. Our result shows that the proposed approach can perform competitively using a very low percentage of the available patches in a given whole-slide image.",
        "subjects": "Image and Video Processing, Computer Vision and Pattern Recognition",
        "date": "2024-09-08 17:41:04 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.239110"
    },
    {
        "index": "#182",
        "title": "A Stochastic Geo-spatiotemporal Bipartite Network to Optimize GCOOS Sensor Placement Strategies",
        "link": "/arxiv/2404.14357",
        "arxiv_id": "2404.14357",
        "authors": "Ted Edward Holmberg, Elias Ioup, Mahdi Abdelguerfi",
        "summary": "This paper proposes two new measures applicable in a spatial bipartite network model: coverage and coverage robustness. The bipartite network must consist of observer nodes, observable nodes, and edges that connect observer nodes to observable nodes. The coverage and coverage robustness scores evaluate the effectiveness of the observer node placements. This measure is beneficial for stochastic data as it may be coupled with Monte Carlo simulations to identify optimal placements for new observer nodes. In this paper, we construct a Geo-SpatioTemporal Bipartite Network (GSTBN) within the stochastic and dynamical environment of the Gulf of Mexico. This GSTBN consists of GCOOS sensor nodes and HYCOM Region of Interest (RoI) event nodes. The goal is to identify optimal placements to expand GCOOS to improve the forecasting outcomes by the HYCOM ocean prediction model.",
        "subjects": "Multiagent Systems",
        "date": "2024-04-22 17:12:06 UTC",
        "category": "cs.CV",
        "crawl_time": "2025-09-24T16:31:00.239315"
    }
]