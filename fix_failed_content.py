#!/usr/bin/env python3
"""
ä¿®å¤å·²æœ‰æ•°æ®ä¸­å¤±è´¥çš„å†…å®¹ï¼ˆç¿»è¯‘ã€æ‘˜è¦ã€çµæ„Ÿæº¯æºç­‰ï¼‰
Fix failed content in existing data
"""

import os
import sys
import json
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent
SUMMARY_DIR = PROJECT_ROOT / "summary"

# å¤±è´¥æ ‡è®°æ¨¡å¼
FAILURE_PATTERNS = {
    "summary_translation": [
        "ç¿»è¯‘å¤±è´¥",
        "Translation failed",
    ],
    "summary2": [
        "æ€»ç»“ç”Ÿæˆå¤±è´¥",
        "Summary generation failed",
    ],
    "inspiration_trace": [
        "ç”Ÿæˆçµæ„Ÿæº¯æºæ—¶å‘ç”Ÿé”™è¯¯",
        "çµæ„Ÿæº¯æºåˆ†æç”Ÿæˆå¤±è´¥",
    ],
    "research_insights": [
        "ç ”ç©¶æ´å¯Ÿåˆ†æç”Ÿæˆå¤±è´¥",
    ],
    "critical_evaluation": [
        "æ‰¹åˆ¤æ€§è¯„ä¼°ç”Ÿæˆå¤±è´¥",
    ],
}

# æ¯æ—¥é€Ÿè§ˆå¤±è´¥æ¨¡å¼
OVERVIEW_FAILURE_PATTERNS = [
    "ç”Ÿæˆæ¯æ—¥é€Ÿè§ˆæ—¶å‘ç”Ÿé”™è¯¯",
    "Connection error",
]


def is_failed_content(value: str, field: str) -> bool:
    """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºå¤±è´¥çŠ¶æ€"""
    if not value:
        return True
    patterns = FAILURE_PATTERNS.get(field, [])
    for pattern in patterns:
        if pattern in value:
            return True
    return False


def is_overview_failed(content: str) -> bool:
    """æ£€æŸ¥æ¯æ—¥é€Ÿè§ˆæ˜¯å¦å¤±è´¥"""
    if not content:
        return True
    for pattern in OVERVIEW_FAILURE_PATTERNS:
        if pattern in content:
            return True
    return False


def scan_failed_papers(date_str: str = None) -> Dict:
    """æ‰«æå¤±è´¥çš„è®ºæ–‡å†…å®¹"""
    results = {
        "papers": {},  # {date: [{arxiv_id, failed_fields}, ...]}
        "overviews": [],  # [date, ...]
    }

    # è·å–è¦æ‰«æçš„æ–‡ä»¶
    if date_str:
        pattern = f"filtered_papers_{date_str}_with_summary2.json"
        files = list(SUMMARY_DIR.glob(pattern))
    else:
        files = list(SUMMARY_DIR.glob("filtered_papers_*_with_summary2.json"))

    for json_file in sorted(files):
        # æå–æ—¥æœŸ
        match = re.search(r'(\d{4}-\d{2}-\d{2})', json_file.stem)
        if not match:
            continue
        date = match.group(1)

        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                papers = json.load(f)
        except Exception as e:
            print(f"âŒ è¯»å– {json_file} å¤±è´¥: {e}")
            continue

        failed_papers = []
        for paper in papers:
            failed_fields = []
            for field in FAILURE_PATTERNS.keys():
                value = paper.get(field, "")
                if is_failed_content(value, field):
                    failed_fields.append(field)

            if failed_fields:
                failed_papers.append({
                    "arxiv_id": paper.get("arxiv_id", "unknown"),
                    "title": paper.get("title", "")[:50],
                    "failed_fields": failed_fields,
                })

        if failed_papers:
            results["papers"][date] = failed_papers

    # æ‰«ææ¯æ—¥é€Ÿè§ˆ
    for md_file in sorted(SUMMARY_DIR.glob("daily_overview_*.md")):
        match = re.search(r'(\d{4}-\d{2}-\d{2})', md_file.stem)
        if not match:
            continue
        date = match.group(1)

        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            if is_overview_failed(content):
                results["overviews"].append(date)
        except Exception:
            results["overviews"].append(date)

    return results


def print_scan_results(results: Dict):
    """æ‰“å°æ‰«æç»“æœ"""
    total_papers = sum(len(v) for v in results["papers"].values())
    total_dates = len(results["papers"])

    print(f"\n{'='*60}")
    print("ğŸ“Š æ‰«æç»“æœ")
    print(f"{'='*60}")

    if results["papers"]:
        print(f"\nğŸ“„ è®ºæ–‡å†…å®¹å¤±è´¥: {total_papers} ç¯‡ (è·¨ {total_dates} å¤©)")
        for date, papers in sorted(results["papers"].items(), reverse=True):
            print(f"\n  ğŸ“… {date} ({len(papers)} ç¯‡):")
            for p in papers[:5]:  # åªæ˜¾ç¤ºå‰5ç¯‡
                fields = ", ".join(p["failed_fields"])
                print(f"     - {p['arxiv_id']}: {fields}")
            if len(papers) > 5:
                print(f"     ... è¿˜æœ‰ {len(papers) - 5} ç¯‡")
    else:
        print("\nâœ… æ²¡æœ‰å‘ç°è®ºæ–‡å†…å®¹å¤±è´¥")

    if results["overviews"]:
        print(f"\nğŸ“ æ¯æ—¥é€Ÿè§ˆå¤±è´¥: {len(results['overviews'])} å¤©")
        for date in results["overviews"][:10]:
            print(f"   - {date}")
        if len(results["overviews"]) > 10:
            print(f"   ... è¿˜æœ‰ {len(results['overviews']) - 10} å¤©")
    else:
        print("\nâœ… æ²¡æœ‰å‘ç°æ¯æ—¥é€Ÿè§ˆå¤±è´¥")


def fix_papers_for_date(date_str: str, dry_run: bool = False) -> int:
    """ä¿®å¤æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡å†…å®¹"""
    # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨æ‰«ææ—¶åŠ è½½
    from src.core.generate_summary import (
        translate_summary,
        generate_summary,
        generate_inspiration_trace,
        generate_research_insights,
        generate_critical_evaluation,
        CacheManager,
    )
    from openai import OpenAI
    from dotenv import load_dotenv

    load_dotenv()

    json_file = SUMMARY_DIR / f"filtered_papers_{date_str}_with_summary2.json"
    if not json_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        return 0

    with open(json_file, 'r', encoding='utf-8') as f:
        papers = json.load(f)

    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    client = OpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL"),
        timeout=180.0,
    )
    model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.7"))
    cache_manager = CacheManager()

    fixed_count = 0
    modified = False

    for i, paper in enumerate(papers):
        arxiv_id = paper.get("arxiv_id", "unknown")
        title = paper.get("title", "")
        summary = paper.get("summary", "")

        needs_fix = []
        for field in FAILURE_PATTERNS.keys():
            if is_failed_content(paper.get(field, ""), field):
                needs_fix.append(field)

        if not needs_fix:
            continue

        print(f"\nğŸ”§ ä¿®å¤ {arxiv_id}: {', '.join(needs_fix)}")

        if dry_run:
            continue

        try:
            # ä¿®å¤ç¿»è¯‘
            if "summary_translation" in needs_fix and summary:
                print(f"   - ç¿»è¯‘æ‘˜è¦...")
                translation = translate_summary(
                    summary, client, model, temperature,
                    title, cache_manager
                )
                if not is_failed_content(translation, "summary_translation"):
                    paper["summary_translation"] = translation
                    modified = True
                    print(f"   âœ… ç¿»è¯‘æˆåŠŸ")

            # ä¿®å¤çµæ„Ÿæº¯æº
            if "inspiration_trace" in needs_fix and summary:
                print(f"   - ç”Ÿæˆçµæ„Ÿæº¯æº...")
                trace = generate_inspiration_trace(
                    summary, client, model, temperature,
                    title, cache_manager
                )
                if not is_failed_content(trace, "inspiration_trace"):
                    paper["inspiration_trace"] = trace
                    modified = True
                    print(f"   âœ… çµæ„Ÿæº¯æºæˆåŠŸ")

            # ä¿®å¤ç ”ç©¶æ´å¯Ÿ
            if "research_insights" in needs_fix and summary:
                print(f"   - ç”Ÿæˆç ”ç©¶æ´å¯Ÿ...")
                insights = generate_research_insights(
                    summary, client, model, temperature,
                    title, cache_manager
                )
                if not is_failed_content(insights, "research_insights"):
                    paper["research_insights"] = insights
                    modified = True
                    print(f"   âœ… ç ”ç©¶æ´å¯ŸæˆåŠŸ")

            # ä¿®å¤æ‰¹åˆ¤æ€§è¯„ä¼°
            if "critical_evaluation" in needs_fix and summary:
                print(f"   - ç”Ÿæˆæ‰¹åˆ¤æ€§è¯„ä¼°...")
                evaluation = generate_critical_evaluation(
                    summary, client, model, temperature,
                    title, cache_manager
                )
                if not is_failed_content(evaluation, "critical_evaluation"):
                    paper["critical_evaluation"] = evaluation
                    modified = True
                    print(f"   âœ… æ‰¹åˆ¤æ€§è¯„ä¼°æˆåŠŸ")

            fixed_count += 1

        except Exception as e:
            print(f"   âŒ ä¿®å¤å¤±è´¥: {e}")

    # ä¿å­˜ä¿®æ”¹
    if modified and not dry_run:
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(papers, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ å·²ä¿å­˜: {json_file}")

    return fixed_count


def fix_overview_for_date(date_str: str, dry_run: bool = False) -> bool:
    """ä¿®å¤æŒ‡å®šæ—¥æœŸçš„æ¯æ—¥é€Ÿè§ˆ"""
    from src.core.generate_summary import (
        generate_daily_overview,
        CacheManager,
    )
    from openai import OpenAI
    from dotenv import load_dotenv

    load_dotenv()

    json_file = SUMMARY_DIR / f"filtered_papers_{date_str}_with_summary2.json"
    md_file = SUMMARY_DIR / f"daily_overview_{date_str}.md"

    if not json_file.exists():
        print(f"âŒ è®ºæ–‡æ•°æ®ä¸å­˜åœ¨: {json_file}")
        return False

    with open(json_file, 'r', encoding='utf-8') as f:
        papers = json.load(f)

    print(f"\nğŸ”§ é‡æ–°ç”Ÿæˆæ¯æ—¥é€Ÿè§ˆ: {date_str}")

    if dry_run:
        return True

    client = OpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL"),
    )
    model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.7"))
    cache_manager = CacheManager()

    try:
        overview = generate_daily_overview(
            papers, client, model, temperature,
            date_str, cache_manager
        )

        if not is_overview_failed(overview):
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(overview)
            print(f"âœ… æ¯æ—¥é€Ÿè§ˆç”ŸæˆæˆåŠŸ")
            return True
        else:
            print(f"âŒ æ¯æ—¥é€Ÿè§ˆä»ç„¶å¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        return False


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="ä¿®å¤å·²æœ‰æ•°æ®ä¸­å¤±è´¥çš„å†…å®¹"
    )
    parser.add_argument(
        "--scan",
        action="store_true",
        help="æ‰«æå¤±è´¥çš„å†…å®¹ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰"
    )
    parser.add_argument(
        "--fix",
        action="store_true",
        help="ä¿®å¤å¤±è´¥çš„å†…å®¹"
    )
    parser.add_argument(
        "--date",
        help="æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="åªæ˜¾ç¤ºè¦ä¿®å¤çš„å†…å®¹ï¼Œä¸å®é™…æ‰§è¡Œ"
    )
    parser.add_argument(
        "--papers-only",
        action="store_true",
        help="åªä¿®å¤è®ºæ–‡å†…å®¹"
    )
    parser.add_argument(
        "--overview-only",
        action="store_true",
        help="åªä¿®å¤æ¯æ—¥é€Ÿè§ˆ"
    )
    parser.add_argument(
        "--regenerate-index",
        action="store_true",
        help="ä¿®å¤åé‡æ–°ç”Ÿæˆç»Ÿä¸€é¡µé¢"
    )

    args = parser.parse_args()

    # é»˜è®¤è¡Œä¸ºæ˜¯æ‰«æ
    if not args.fix:
        args.scan = True

    if args.scan and not args.fix:
        print("ğŸ” æ‰«æå¤±è´¥çš„å†…å®¹...")
        results = scan_failed_papers(args.date)
        print_scan_results(results)

        if results["papers"] or results["overviews"]:
            print(f"\nğŸ’¡ ä½¿ç”¨ --fix å‚æ•°æ¥ä¿®å¤è¿™äº›å†…å®¹")
            print(f"   python fix_failed_content.py --fix")
        return

    if args.fix:
        print("ğŸ”§ å¼€å§‹ä¿®å¤å¤±è´¥çš„å†…å®¹...")

        if args.dry_run:
            print("âš ï¸  Dry-run æ¨¡å¼ï¼Œä¸ä¼šå®é™…ä¿®æ”¹æ–‡ä»¶\n")

        results = scan_failed_papers(args.date)

        fixed_papers = 0
        fixed_overviews = 0

        # ä¿®å¤è®ºæ–‡å†…å®¹
        if not args.overview_only and results["papers"]:
            for date in sorted(results["papers"].keys()):
                print(f"\n{'='*50}")
                print(f"ğŸ“… å¤„ç†æ—¥æœŸ: {date}")
                print(f"{'='*50}")
                fixed_papers += fix_papers_for_date(date, args.dry_run)

        # ä¿®å¤æ¯æ—¥é€Ÿè§ˆ
        if not args.papers_only and results["overviews"]:
            for date in results["overviews"]:
                if fix_overview_for_date(date, args.dry_run):
                    fixed_overviews += 1

        # é‡æ–°ç”Ÿæˆç»Ÿä¸€é¡µé¢
        if args.regenerate_index and not args.dry_run:
            print(f"\n{'='*50}")
            print("ğŸ”„ é‡æ–°ç”Ÿæˆç»Ÿä¸€é¡µé¢...")
            print(f"{'='*50}")
            import subprocess
            subprocess.run([
                sys.executable,
                "src/core/generate_unified_index.py"
            ], cwd=PROJECT_ROOT)

        # æ€»ç»“
        print(f"\n{'='*50}")
        print("ğŸ“Š ä¿®å¤å®Œæˆæ€»ç»“")
        print(f"{'='*50}")
        print(f"âœ… ä¿®å¤è®ºæ–‡: {fixed_papers} ç¯‡")
        print(f"âœ… ä¿®å¤é€Ÿè§ˆ: {fixed_overviews} å¤©")


if __name__ == "__main__":
    main()
