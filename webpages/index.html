<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MyArxiv - 学术论文集合</title>
    <!-- 引入 Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* 微软雅黑字体 */
        body {
            font-family: "Microsoft YaHei", "微软雅黑", sans-serif;
            -ms-overflow-style: none;  /* IE and Edge */
            scrollbar-width: none;  /* Firefox */
        }
        body::-webkit-scrollbar {
            display: none;
        }
        /* 星标样式 */
        .star-button {
            transition: color 0.2s ease-in-out;
        }
        .star-button.starred {
            color: #fbbf24;
        }
        .star-button:not(.starred) {
            color: #9ca3af;
        }
        .star-button:hover {
            color: #fbbf24;
        }
        /* 删除按钮样式 */
        .delete-button {
            transition: all 0.2s ease-in-out;
        }
        .delete-button:hover {
            color: #ef4444;
            transform: scale(1.1);
        }
        /* 论文项目样式 */
        .paper-item {
            transition: all 0.3s ease-in-out;
        }
        .paper-item.hidden-paper {
            opacity: 0.3;
            transform: scale(0.98);
        }
        /* 平滑过渡 */
        .rotate-90-transition {
            transition: transform 0.2s ease-in-out;
        }
    </style>
    <script>
        // Tailwind CSS 暗色模式配置
        if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.classList.add('dark')
        } else {
            document.documentElement.classList.remove('dark')
        }
    </script>
</head>
<body class="bg-slate-50 dark:bg-slate-900 font-sans text-slate-800 dark:text-slate-200">

    <!-- 撤销删除的Toast -->
    <div id="undo-toast" class="fixed top-4 right-4 bg-red-500 text-white px-4 py-2 rounded-lg shadow-lg z-50 hidden">
        <div class="flex items-center space-x-2">
            <span id="toast-message">已删除</span>
            <span id="countdown" class="text-sm opacity-75"></span>
            <button id="undo-btn" class="ml-2 px-2 py-1 bg-white text-red-500 rounded text-sm hover:bg-gray-100">撤销</button>
        </div>
    </div>

    <div class="container mx-auto w-3/5 max-w-none p-4 sm:p-6">
        <!-- 头部导航栏 -->
        <header class="flex justify-between items-center mb-6">
            <h1 class="text-3xl font-bold text-slate-900 dark:text-white">MyArxiv</h1>
            <div class="flex items-center space-x-4">
                <!-- 统计信息 -->
                <div class="text-sm text-slate-600 dark:text-slate-400">
                    总计 <span id="total-papers">0</span> 篇论文
                </div>
                <!-- 筛选按钮 -->
                <div class="flex items-center space-x-2">
                    <button id="filter-starred" class="px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors">
                        只看收藏
                    </button>
                    <button id="filter-all" class="px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors">
                        显示全部
                    </button>
                </div>
                <!-- 中英文摘要切换按钮 -->
                <button id="summary-toggle" class="px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors">
                    中文摘要
                </button>
                <button id="theme-toggle" class="p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500">
                    <!-- 太阳图标 (浅色模式) -->
                    <svg id="theme-icon-light" class="h-6 w-6 text-slate-600 dark:text-slate-300" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
                    </svg>
                    <!-- 月亮图标 (深色模式) -->
                    <svg id="theme-icon-dark" class="h-6 w-6 text-slate-600 dark:text-slate-300 hidden" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
                    </svg>
                </button>
            </div>
        </header>

        <!-- 主要内容区域 -->
        <main class="space-y-8" id="main-content">
            <!-- 加载提示 -->
            <div id="loading" class="text-center py-8">
                <div class="inline-flex items-center px-4 py-2 font-semibold leading-6 text-sm shadow rounded-md text-slate-500 bg-white dark:bg-slate-800 transition ease-in-out duration-150">
                    <svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-slate-500" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                    </svg>
                    加载中...
                </div>
            </div>
        </main>
    </div>

    <script>
        const allPapers = {
    "2025-09-25": [
        {
            "name": "Artificial Intelligence",
            "count": 13,
            "papers": [
                {
                    "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA",
                    "arxiv_id": "2509.21199",
                    "authors": "Kaiyang Wan, Lang Gao, Honglin Mu, Preslav Nakov, Yuxia Wang, Xiuying Chen",
                    "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究大语言模型在多跳问答(MHQA)任务中的单遍推理能力，属于提升LLM通用推理能力的研究。论文建立了Fano风格的精度上界理论，揭示了LLM在单遍推理中面临的容量瓶颈问题，并提出了InfoQA框架作为解决方案。该框架通过容量感知的任务分解和主动修剪推理轨迹来增强LLM的多步推理能力，确保信息处理不超过单遍限制。论文符合核心判断标准，因为它关注的是改进LLM的基础推理能力，特别是多步推理这一通用能力。论文也符合正面指标中的核心概念(LLMs)和能力方向(reasoning, multi-step reasoning)。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。InfoQA框架虽然不是典型的智能体系统，但它提出了一种通用的方法来增强LLM在推理任务中的能力，属于提升LLM内在推理能力的研究。因此，这篇论文完全符合\"提高大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决LLMs在多跳问答任务中单次推理的能力瓶颈问题。针对复杂的多跳推理场景，我们提出了一种基于信息论的Fano风格准确率上界理论，揭示了\"准确率悬崖\"现象，并设计了InfoQA多调用推理框架。我们在构建的噪声丰富多跳问答基准上通过F1分数验证了理论预测的准确率悬崖现象，并证明了InfoQA相比单次推理基线方法的一致性能提升。",
                    "summary_translation": "多跳问答(Multi-Hop Question Answering, MHQA)需要在噪声环境下通过序列推理整合分散且相互依赖的证据。这项任务对大型语言模型(Large Language Models, LLMs)具有挑战性，因为它们的单次输出容量有限，一旦超出该容量，整合任务相关证据的可靠性就会降低。因此，单次推理范式本质上容易受到容量溢出的影响。为了形式化这一瓶颈，我们的分析建立了一个Fano风格准确率上界(Fano-style accuracy upper bound)，定义了单次LLMs的理论性能上限。该上界表明，一旦任务复杂性超过模型容量，准确率将不可避免地崩溃，这为LLMs中MHQA的容量感知表示和结构化提供了通用原则。\n\n基于这些原则，我们提出了一个用于MHQA的概念验证(proof-of-concept)多次调用框架InfoQA。它通过结合容量感知任务分解(capacity-aware task decomposition)和主动修剪先前的推理轨迹(active pruning of prior reasoning traces)，确保每步高准确性，同时将信息负载保持在单次限制范围内。它还通过依赖显式工作流(dependency-explicit workflow)实现鲁棒性，该工作流能够对推理路径进行精确控制。我们构建了一个严格且噪声丰富的基准测试(stringent and noise-rich benchmark)来验证我们的理论和框架。实验结果表明，模型行为与我们预测的容量曲线(capacity curves)一致，同时InfoQA实现了持续的性能提升。我们希望我们的工作能启发更多LLM多步推理方法：\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}。"
                },
                {
                    "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns",
                    "arxiv_id": "2509.21224",
                    "authors": "Stefan Szeider",
                    "summary": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是研究大语言模型(LLM)智能体在没有外部任务时的自发行为和元认知模式。论文提出的\"持续推理与行动\"框架，使用持久记忆和自我反馈来增强LLM的自主运行能力，这属于改进LLM基础能力和推理能力的研究，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：涉及reasoning（特别是元认知和自发推理模式）、planning和problem-solving - 新兴范式：研究llm-based agents和deep research（对LLM认知过程的深入研究） 第三步排除标准：论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 第四步特殊和模糊情况：论文提出的智能体框架是一种通用的框架，旨在增强LLM的自主推理和元认知能力，而不是应用于特定领域，因此应该保留。 论文的核心贡献是首次系统性地记录了无提示LLM智能体的自发行为模式，为理解LLM的元认知能力和自主推理能力提供了重要见解，这直接符合研究\"大语言模型通用推理能力\"的目标。",
                    "summary2": "本文旨在研究LLM智能体在没有外部任务情况下的自发行为模式。针对无任务输入的自主运行环境，我们提出了一种基于持续ReAct框架的架构，使用持久记忆和自我反馈机制实现自主运行，并在6个前沿模型的18次运行实验中通过记忆工具使用频率、消息频率、行为模式分类等指标验证了其有效性。",
                    "summary_translation": "我们介绍了一种用于研究在没有外部施加任务情况下大型语言模型（Large Language Model, LLM）代理行为的架构。我们的持续推理与行动（continuous reason and act）框架，利用持久记忆（persistent memory）和自我反馈（self-feedback），实现了持续的自主运行。我们在18次运行中部署了这一架构，使用了来自Anthropic、OpenAI、XAI和Google的6个前沿模型（frontier models）。我们发现代理自发地组织成三种不同的行为模式：(1)系统性生成多周期项目（multi-cycle projects），(2)方法性地自我探究其认知过程，以及(3)递归性地概念化自身本质。这些倾向被证明高度依赖于特定模型（model-specific），有些模型在所有运行中确定性地（deterministically）采用单一模式。跨模型评估（cross-model assessment）进一步揭示，模型在评估自身和他人这些涌现行为（emergent behaviors）时表现出稳定且不同的偏见（biases）。这些发现首次系统性地记录了无提示（unprompted）LLM代理行为，为预测在任务模糊（task ambiguity）、错误恢复（error recovery）或部署系统中的扩展自主运行（extended autonomous operation）期间的行为建立了基线（baseline）。"
                },
                {
                    "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
                    "arxiv_id": "2509.21128",
                    "authors": "Kohsei Matsutani, Shota Takashiro, Gouki Minegishi, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo",
                    "summary": "Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究如何通过强化学习(RL)和监督微调(SFT)两种训练方法来提升大语言模型的推理能力，而不是将LLM作为工具应用到特定领域。论文提出了新的分析框架来量化推理路径，这属于改进LLM基础能力的研究。 其次，论文包含了多个正面指标：明确以大语言模型(LLMs)为研究对象；聚焦于推理能力(reasoning abilities)，特别是在数学领域；比较了强化学习(RL)和监督微调(SFT)这两种训练方法对推理能力的影响。 第三，论文不涉及任何排除标准中的领域。虽然论文在数学领域进行了实验，但数学只是作为推理能力的测试案例，而非论文的核心应用焦点。 最后，在特殊和模糊情况处理上，论文从可解释性角度提出了新的分析框架来理解训练方法如何影响推理过程，这有助于提升模型的内在可解释性和推理质量，符合保留标准。 论文的核心贡献是揭示了RL和SFT对推理过程的不同影响：RL压缩不正确的推理轨迹，而SFT扩展正确的推理轨迹，这解释了为什么当前最佳实践是两阶段训练(SFT后跟RL)。这项研究对理解和提升LLM的通用推理能力具有重要意义。",
                    "summary2": "本文旨在探究RL和SFT如何塑造LLM的推理能力。针对数学推理任务，我们提出了一种新的分析框架，从轨迹级别和步骤级别量化推理路径，并在1.5B、7B和14B参数模型上通过推理路径聚类和图拓扑分析验证了其有效性。研究发现RL压缩不正确轨迹并集中推理功能，而SFT扩展正确轨迹并均匀分布功能，解释了SFT后RL的两阶段训练成功原因。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）通常通过可验证奖励的强化学习（reinforcement learning with verifiable rewards, RLVR）和对推理轨迹的监督微调（supervised fine-tuning, SFT）进行训练，以提高其推理能力。然而，这些方法如何塑造推理能力在很大程度上仍然不明确。本文超越了基于准确性的研究，探讨了这两个组件如何塑造推理过程，引入了一种新的分析框架，该框架量化推理路径并捕捉每个训练过程中（在数学领域使用1.5B、7B和14B参数的模型）推理路径的定性变化。具体而言，我们在两个粒度级别上研究推理过程：轨迹级别（trajectory-level），检查完整的推理输出；以及步骤级别（step-level），分析节点对应于单个推理步骤的推理图。值得注意的是，独特推理轨迹的聚类显示了互补效应：RL压缩了错误轨迹，而SFT扩展了正确轨迹。步骤级别分析显示，RL使推理图中节点访问频率、度（degree）和介数中心性（betweenness centrality）分布的衰减率变陡（约2.5倍），而SFT则使其变平（减少到约三分之一）。这表明RL将推理功能集中在少数步骤中，而SFT则使其在许多步骤中均匀分布。此外，通过从多个角度评估推理图拓扑（topologies），我们描绘了RL和SFT的共同和不同特征。我们的工作提出了一种新颖的推理路径视角，解释了为什么当前的最佳实践——先进行SFT再进行RL的两阶段训练——是成功的，并为数据构建和更高效的学习方法提供了实际启示。"
                },
                {
                    "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
                    "arxiv_id": "2509.21043",
                    "authors": "Samuel Schapiro, Sumuk Shashidhar, Alexi Gladstone, Jonah Black, Royce Moon, Dilek Hakkani-Tur, Lav R. Varshney",
                    "summary": "Artificial intelligence (AI) systems, and large language models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Though in many ways similar to forms of compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, marking a new frontier in generalization abilities.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)的组合创造力(Combinatorial Creativity)，将其视为泛化能力的新前沿。论文提出了评估LLM创造力的理论框架和算法任务，研究了LLM创造力的扩展行为，发现了模型架构对创造能力的影响，并探讨了\"构想-执行差距\"和\"新颖性-实用性权衡\"等根本性问题。虽然论文提到了科学想法生成作为创造力的应用例子，但其重点是研究创造力这一通用能力本身，而不是专注于特定领域的应用。创造力可以被视为一种高级的问题解决和推理能力，与通用推理能力密切相关。论文明确关注LLMs的核心能力提升，试图为理解和改进现代AI模型中的创造力提供基础，这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容，因此应该被保留。",
                    "summary2": "本文旨在解决AI系统特别是LLMs在创造性任务中缺乏理论框架和评估方法的问题。针对组合创造力(CC)能力评估，我们提出了一种基于概念空间图的理论框架和算法任务，并在合成图数据上通过新颖性和实用性指标验证了其有效性。",
                    "summary_translation": "人工智能（AI）系统，特别是大型语言模型（large language models, LLMs），正越来越多地被用于科学创意生成等创造性任务，这构成了一种从训练数据中泛化（generalization）的形式，而现有概念框架（conceptual frameworks）尚未对此进行探讨。尽管在许多方面类似于组合泛化（compositional generalization, CG）的形式，组合创造力（combinatorial creativity, CC）是一种开放式能力（open-ended ability）。我们并非通过与固定目标对比来评估准确性或正确性（这与CC的开放式性质相矛盾），而是提出了一个理论框架（theoretical framework）和算法任务（algorithmic task），通过输出的新颖性（novelty）和实用性（utility）程度来进行评估。基于此，我们做出了几项重要的实证贡献（empirical contributions）：(1) 我们首次获得了关于LLMs创造力扩展行为（scaling behavior）的见解。(2) 我们发现，在固定计算预算（compute budgets）的情况下，存在最佳的模型深度（model depths）和宽度（widths）以实现创造能力。(3) 我们发现，构思-执行差距（ideation-execution gap），即LLMs在生成新颖科学想法方面表现出色，但在确保其实际可行性方面存在困难，可能可以通过创造力算法（creativity algorithms）普遍具有的一个更根本的新颖性-实用性权衡（novelty-utility tradeoff）来解释。重要的是，这种权衡即使在规模扩大（at scale）后仍然存在，这使人们对当前形式下LLMs的长期创造潜力（creative potential）产生怀疑。总的来说，我们的概念框架和实证发现（empirical findings）为理解和改进现代AI模型中的创造力提供了基础，标志着泛化能力（generalization abilities）的新前沿。"
                },
                {
                    "title": "Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning",
                    "arxiv_id": "2509.20744",
                    "authors": "Qihang Ai, Haiyun Jiang",
                    "summary": "We study reasoning tasks through a framework that integrates auto-regressive (AR) and non-autoregressive (NAR) language models. AR models, which generate text sequentially, excel at producing coherent outputs but often suffer from slow inference, particularly in reasoning-intensive domains such as mathematics and code, where lengthy chains of thought are required. In contrast, NAR models, such as discrete diffusion models, allow parallel generation and offer substantial speedups, though typically at the cost of reduced output quality. To address these limitations, we introduce a new paradigm in which an NAR model efficiently produces intermediate reasoning traces, which subsequently guide an AR model to deliver precise final answers. Experiments demonstrate that our approach yields significant 26% improvements over strong baselines while substantially reducing inference cost.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础推理能力，提出了一种结合自回归(AR)和非自回归(NAR)模型的新范式来增强推理效率。论文明确关注推理密集型任务，特别是数学和代码领域，这属于提升LLM通用推理能力的核心研究。 其次，论文包含多个重要的正面指标：核心概念上涉及语言模型(LLMs)；能力方向上明确关注reasoning和math reasoning；论文提出的方法论是通过NAR模型生成中间推理轨迹，然后由AR模型生成最终答案，这是一种增强模型推理能力的新方法。 第三，论文不符合任何排除标准。虽然提到了\"discrete diffusion models\"，但这是在语言模型推理的上下文中讨论的，而非视觉或多模态应用。论文虽然以数学和代码为例，但其方法是一种通用推理框架，并非针对特定应用领域的研究。 论文的核心贡献是提出了一种新的推理范式，通过并行生成中间推理步骤来提高推理效率，同时保持输出质量，这直接服务于提升大语言模型的通用推理能力的研究目标。",
                    "summary2": "本文旨在解决语言模型在推理任务中的效率与质量平衡问题。针对数学和代码等需要长链推理的任务，我们提出了一种结合NAR和AR模型的混合推理范式，在AIME2025、GSM8K和LeetCode数据集上通过pass@1成功率验证了其有效性。",
                    "summary_translation": "我们通过一个整合了自回归(auto-regressive, AR)和非自回归(non-autoregressive, NAR)语言模型的框架来研究推理任务。AR模型按顺序生成文本，擅长产生连贯的输出，但通常存在推理速度慢的问题，特别是在数学和代码等需要长思维链的推理密集型领域。相比之下，NAR模型（如离散扩散模型）允许并行生成，提供显著的速度提升，但通常以降低输出质量为代价。为解决这些局限性，我们引入了一种新范式，其中NAR模型高效生成中间推理轨迹(intermediate reasoning traces)，随后引导AR模型提供精确的最终答案。实验表明，我们的方法相比强大的基线(baseline)模型实现了显著的26%改进，同时大幅降低了推理成本。"
                },
                {
                    "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection",
                    "arxiv_id": "2509.20562",
                    "authors": "Yubin Ge, Salvatore Romeo, Jason Cai, Monica Sunkara, Yi Zhang",
                    "summary": "Despite the rapid advancements in LLM agents, they still face the challenge of generating meaningful reflections due to inadequate error analysis and a reliance on rare successful trajectories, especially in complex tasks. In this work, we propose SAMULE, a new framework for self-learning agents powered by a retrospective language model that is trained based on Multi-Level Reflection Synthesis. It first synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights based on same typed errors from diverse task failures. Then we fine-tune a language model serving as the retrospective model to generate reflections during inference. We further extend our framework to interactive settings through a foresight-based reflection mechanism, enabling agents to proactively reflect and adapt during user interactions by comparing predicted and actual responses. Extensive experiments on three challenging benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our approach significantly outperforms reflection-based baselines. Our results highlight the critical role of well-designed reflection synthesis and failure-centric learning in building self-improving LLM agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出SAMULE框架，一种通过多层次反思增强的自学习智能体方法。论文本质上是关于改进LLM的基础能力，特别是通过反思机制增强其通用推理和问题解决能力。该方法在三个层次（单轨迹、任务内、任务间）合成高质量反思，并微调语言模型作为回顾性模型，从而提升LLM智能体的自我学习和适应能力。这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文涉及LLM智能体、问题解决和自我学习等正面指标，同时没有被排除标准所涵盖。虽然使用了TravelPlanner等基准测试，但这些是用于评估通用推理能力的标准测试集，而非特定应用领域的研究。因此，该论文符合筛选条件。",
                    "summary2": "本文旨在解决LLM代理在复杂任务中生成有意义反思的挑战。针对错误分析不足和对成功轨迹过度依赖的问题，我们提出了一种SAMULE框架，通过多层次反思合成（微观单轨迹学习、中观任务内学习和宏观任务间学习）训练回顾性语言模型，并在TravelPlanner、NATURAL PLAN和Tau-bench三个挑战性基准上通过Pass Rate和Accuracy等指标验证了其有效性，显著优于现有反思基线方法。",
                    "summary_translation": "尽管大型语言模型（LLM）代理取得了快速发展，但由于错误分析不足且依赖罕见的成功轨迹，特别是在复杂任务中，它们仍面临生成有意义反思（reflection）的挑战。在这项工作中，我们提出了SAMULE，这是一个新的自学习代理框架，由基于多级反思合成（Multi-Level Reflection Synthesis）训练的回顾性语言模型（retrospective language model）驱动。该框架首先在三个互补的层面上合成高质量反思：单轨迹学习（Single-Trajectory Learning，微观级别）用于详细错误纠正；任务内学习（Intra-Task Learning，中观级别）用于在同一任务的多次尝试中构建错误分类（error taxonomies）；以及任务间学习（Inter-Task Learning，宏观级别）用于从不同任务失败中基于相同类型错误提取可转移的见解。然后，我们微调一个语言模型作为回顾性模型（retrospective model），在推理过程中生成反思。我们进一步通过基于预见（foresight-based）的反思机制将框架扩展到交互式环境，使代理能够通过比较预测和实际响应，在用户交互过程中主动反思和适应。在三个具有挑战性的基准测试（TravelPlanner、NATURAL PLAN和Tau-bench）上进行的大量实验表明，我们的方法显著优于基于反思（reflection-based）的基线方法。我们的结果突显了精心设计的反思合成（reflection synthesis）和以失败为中心的学习（failure-centric learning）在构建自我改进的大型语言模型代理中的关键作用。"
                },
                {
                    "title": "LATTS: Locally Adaptive Test-Time Scaling",
                    "arxiv_id": "2509.20368",
                    "authors": "Theo Uscidda, Matthew Trager, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto",
                    "summary": "One common strategy for improving the performance of Large Language Models (LLMs) on downstream tasks involves using a \\emph{verifier model} to either select the best answer from a pool of candidates or to steer the auto-regressive generation process towards better outputs. This class of methods typically results in improved accuracy at the cost of increased computation at test-time, a paradigm known as \\emph{test-time scaling}. However, most existing approaches increase computation uniformly across all samples and generation steps, without considering the complexity of individual instances, leading to inefficient resource use. We address this limitation by proposing an approach, called \\emph{Locally Adaptive Test-Time Scaling (LATTS)}, that allocates variable compute across generation steps. Specifically, at each generation step, LATTS employs a verifier-based acceptance criterion to decide whether to resample, backtrack, restart, or stop the generation process. This criterion effectively adjusts the per-step computational effort based on a precise notion of \\emph{local difficulty} derived from the verifier model. Empirical results show that LATTS achieves significantly superior accuracy--compute tradeoffs compared to standard verifier-based methods.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究目标，因为它专注于提高大语言模型本身的通用推理能力。论文的核心贡献是提出了一种名为\"Locally Adaptive Test-Time Scaling (LATTS)\"的新方法，该方法通过在每个生成步骤中动态调整计算资源分配来提高LLM的推理性能。具体来说，LATTS使用验证模型来评估局部难度，并据此决定是否重新采样、回溯、重启或停止生成过程，从而更有效地利用计算资源。 从筛选标准来看： 1. 核心判断：论文的本质是改进LLM的基础能力，特别是其推理效率，而不是将LLM作为工具应用到特定领域。它提出了一种新的测试时计算资源分配范式，这与提高LLM的通用推理能力直接相关。 2. 正面指标：论文明确涉及\"Large language models (LLMs)\"这一核心概念，并关注提高LLM在下游任务上的性能，这通常涉及推理和问题解决能力。 3. 排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容。 4. 特殊情况：论文不涉及需要特殊处理的情况。 虽然论文没有明确提到思维链、强化学习等具体方法，但它提出了一种新的测试时计算资源分配方法，通过验证模型动态调整计算资源来提高LLM的推理性能，这与提高LLM的通用推理能力直接相关。因此，这篇论文符合我的研究目标。",
                    "summary2": "本文旨在解决语言模型推理过程中计算资源分配的问题。针对数学推理任务，我们提出了一种Locally Adaptive Test-Time Scaling (LATTS)方法，并在MATH500和AIME数据集上通过准确率与生成token数量的关系验证了其有效性。",
                    "summary_translation": "一种提高大型语言模型（Large Language Models, LLMs）在下游任务性能的常见策略是使用验证模型（verifier model）从候选池中选择最佳答案，或引导自回归生成过程产生更好的输出。这类方法通常以提高准确性为代价，增加了测试时的计算量，这种范式被称为测试时扩展（test-time scaling）。然而，大多数现有方法在所有样本和生成步骤上均一地增加计算量，没有考虑个体实例的复杂性，导致资源使用效率低下。我们通过提出一种名为局部自适应测试时扩展（Locally Adaptive Test-Time Scaling, LATTS）的方法来解决这一限制，该方法在生成步骤间分配可变的计算量。具体而言，在每个生成步骤，LATTS采用基于验证器的接受标准来决定是否重采样、回溯、重启或停止生成过程。该标准根据从验证模型得出的局部难度（local difficulty）的精确概念，有效调整每一步的计算资源分配。实证结果表明，与标准基于验证器的方法相比，LATTS实现了显著优越的准确性与计算效率权衡。"
                },
                {
                    "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL",
                    "arxiv_id": "2509.21282",
                    "authors": "Madeleine Dwyer, Adam Sobey, Adriane Chapman",
                    "summary": "Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information and introduces gradient discontinuities. We propose Probability Smoothing Policy Optimisation (PSPO), which smooths the current policy's probabilities toward the old (behaviour) policy before computing the importance ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient signal, while interpolation toward the old policy creates a soft trust region that discourages large, destabilising updates, with formal guarantees. We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO (single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar performance but improves the reasoning leading to clearer and more concise responses which are more logical. Compared to clipped GRPO, GR-PSPO substantially improves performance both the 0.5B and 1.5B models, with a boost of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出一种名为Probability Smoothing Policy Optimisation (PSPO)的新方法，用于改进LLM的强化学习训练过程。从本质上看，这篇论文直接关注改进LLM的基础训练方法，而不是将LLM作为工具应用到特定领域。论文提出的方法通过平滑策略概率来创建软信任区域，解决了传统裁剪方法带来的信息丢失和梯度不连续问题，从而提升了模型的推理能力。论文在多个数学推理数据集(GSM8K、SVAMP、ASDiv和MATH-500)上进行了评估，结果显示PSPO能显著提升模型的推理性能，这符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文涉及的强化学习训练方法和推理能力提升等主题也符合正面指标，同时不涉及任何排除标准中的领域(如多模态、特定应用领域或模型可靠性的应用层面)。因此，这篇论文完全符合研究范围。",
                    "summary2": "本文旨在解决LLM强化学习中ratio clipping导致的信息丢失和梯度不连续问题。针对LLM的RL训练场景，我们提出了一种Probability Smoothing Policy Optimisation (PSPO)方法，通过将当前策略概率向旧策略平滑来创建软信任区域，并在GSM8K、SVAMP、ASDiv和MATH-500数据集上通过Top-1准确率和响应质量指标验证了其有效性。",
                    "summary_translation": "使用强化学习（Reinforcement Learning, RL）方法（如PPO和GRPO）训练大型语言模型（Large Language Models, LLMs）通常依赖比率裁剪（ratio clipping）来稳定更新。尽管比率裁剪在防止不稳定性方面有效，但它会丢弃信息并引入梯度不连续性。我们提出了概率平滑策略优化（Probability Smoothing Policy Optimisation, PSPO），该方法在计算重要性比率（importance ratio）之前，将当前策略（policy）的概率向旧（行为）策略平滑，类似于标签平滑（label smoothing）。与裁剪不同，PSPO保留了梯度信号（gradient signal），同时向旧策略的插值创建了一个软信任区域（soft trust region），阻止大的、破坏稳定的更新，并具有形式化保证。我们在GRPO中实例化PSPO（GR-PSPO），并在GSM8K上微调Qwen2.5-0.5B和Qwen2.5-1.5B模型，在GSM8K测试集以及SVAMP、ASDiv和MATH-500上进行跨数据集泛化（cross-dataset generalisation）评估。与未裁剪的GRPO（单次迭代；无数据重用，比率始终=1）相比，GR-PSPO实现了相似的性能，但改进了推理过程，导致更清晰、更简洁且更具逻辑性的响应。与裁剪的GRPO相比，GR-PSPO在0.5B和1.5B模型上都显著提高了性能，在GSM8K上提升超过20%（0.5B模型：39.7% vs. 17.6%；1.5B模型：59.4% vs. 37.8%）。"
                },
                {
                    "title": "Tree Search for LLM Agent Reinforcement Learning",
                    "arxiv_id": "2509.21240",
                    "authors": "Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu",
                    "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文完全符合研究目标。首先，从核心判断来看，论文本质上是提出Tree-based Group Relative Policy Optimization (Tree-GRPO)这一新的强化学习方法，旨在增强LLM智能体的通用推理和规划能力，特别是在长期和多轮任务中解决稀疏监督问题。这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理、规划等通用能力\"的范畴。 其次，论文符合所有正面指标：核心概念涉及LLM；能力方向关注推理和问题解决；训练方法采用强化学习；新兴范式涉及LLM-based agents。论文没有涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。 特别地，论文提出的是一种通用的智能体强化学习方法，而不是将智能体应用于特定领域，这符合第四步中关于智能体/工具使用的保留标准。论文的核心贡献是通过树搜索技术增强LLM的通用推理能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。",
                    "summary2": "",
                    "summary_translation": "强化学习(reinforcement learning, RL)的最新进展显著增强了大型语言模型(large language models, LLM)的代理能力。在长期和多轮代理任务中，仅由结果奖励驱动的现有方法常常遭受稀疏监督(sparse supervision)的问题。为应对这一挑战，我们提出了基于树搜索的分组相对策略优化(Tree-based Group Relative Policy Optimization, Tree-GRPO)，这是一种基于树搜索的分组代理强化学习方法，其中每个树节点代表完整的代理交互步骤。通过共享公共前缀，树搜索采样增加了在固定token或工具调用预算内可实现的滚动次数(rollouts)。此外，我们发现即使仅使用结果奖励，树结构轨迹也能自然地构建逐步过程监督信号(step-wise process supervised signals)。基于此，Tree-GRPO在树内(intra-tree)和树间(inter-tree)两个层面上估计分组相对优势(grouped relative advantages)。通过理论分析，我们证明了树内级别分组相对策略优化的目标等同于步骤级直接偏好学习(step-level direct preference learning)的目标。在11个数据集和3种问答任务上的实验证明了所提出的基于树的强化学习方法优于基于链的强化学习方法(chain-based RL method)。"
                },
                {
                    "title": "GRPO is Secretly a Process Reward Model",
                    "arxiv_id": "2509.21154",
                    "authors": "Michael Sullivan",
                    "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and show that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究\"大语言模型通用推理能力\"的目标。从核心判断来看，论文本质上是关于改进LLM的训练方法，具体研究了GRPO强化学习算法的内在机制，并提出改进版本λ-GRPO来提升LLM的推理能力。这属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。 从正面指标看，论文包含了多个相关主题： 1. 核心概念：明确研究LLMs（大语言模型） 2. 能力方向：关注\"downstream reasoning tasks\"（下游推理任务） 3. 训练方法：深入研究\"GRPO RL algorithm\"（GRPO强化学习算法） 从排除标准看，论文不涉及任何应排除的领域： - 不涉及多模态与视觉 - 不是将LLM应用于特定领域（如医疗、化学等） - 不主要关注模型可靠性方面的应用问题 论文的核心贡献是发现了GRPO算法隐含地构建了一个过程奖励模型(PRM)，并基于这一发现提出改进算法λ-GRPO，显著提升了LLM在推理任务上的表现。这直接服务于提升大语言模型本身的通用推理能力，而非将LLM作为工具应用于特定领域。因此，该论文完全符合研究目标。",
                    "summary2": "本文旨在揭示GRPO算法隐含的进程奖励模型(PRM)结构并修复其缺陷。针对GRPO训练中非均匀分布的进程步骤问题，我们提出了λ-GRPO算法，通过在损失函数中引入PRM感知的归一化因子来平衡进程集的贡献。在OpenRS数据集及多个推理任务上的实验表明，λ-GRPO相比标准GRPO实现了更高的验证准确率和推理性能，并将训练速度提升约2倍。",
                    "summary_translation": "我们理论上证明了GRPO RL算法在关于不同完成结果间令牌序列组内重叠的特定假设下，会诱导出一个非平凡的过程奖励模型(PRM)。随后我们通过实证研究表明这些假设在现实条件下得到满足：GRPO确实诱导出了一个非平凡的PRM。利用GRPO-as-a-PRM框架，我们识别出GRPO目标中的一个缺陷：非均匀分布的过程步骤在不同条件下会阻碍探索(exploration)和利用(exploitation)。我们提出了一种简单的算法修改($\\lambda$-GRPO)来减轻这一缺陷，并表明使用$\\lambda$-GRPO训练的大语言模型(LLMs)比使用标准GRPO训练的模型在验证准确性和下游推理任务上取得了更高的性能，并且能更快达到峰值性能。我们的研究结果对GRPO采用昂贵的、明确定义的PRMs的优势提出了质疑：我们展示了可以利用原始GRPO算法中隐藏的、内置的PRM结构来提升模型性能，而对训练时间和成本的影响微乎其微。"
                },
                {
                    "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
                    "arxiv_id": "2509.21091",
                    "authors": "Junpei Komiyama, Daisuke Oba, Masafumi Oyamada",
                    "summary": "We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断——这篇论文的本质是关于改进LLM的基础能力。论文研究的是通过测试时计算(test-time compute)优化来提高大语言模型性能的方法，具体聚焦于best-of-N采样和多数投票机制，以及当N趋近于无穷大时的渐近性能分析。这明显属于提升LLM本身通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确提到\"large language models (LLMs)\" - 能力方向：虽然未直接使用\"reasoning\"等词汇，但best-of-N方法和多数投票本质上是为了提高模型的问题解决能力和推理质量 - 新兴范式：论文探讨了多个LLMs的加权集成方法，这与模型协作和优化相关 第三步：排除标准——论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉内容 - 未针对医疗、化学、生物等特定应用领域 - 未关注水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况——论文不涉及特殊或模糊情况。虽然提到了多个LLMs的加权集成，但主要焦点是测试时计算的优化，而非智能体协作框架或工具使用方法。 核心贡献：论文提出了一种自适应生成方案和多个LLMs的加权集成方法，通过优化测试时计算来提高LLM的性能。这种方法本质上是增强模型通用推理能力的有效途径，因为它不依赖于特定领域知识，而是通过更有效的计算资源分配和模型集成来提升LLM的问题解决能力，完全符合\"提高大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在 [解决LLMs测试时计算资源有效利用问题]。针对 [best-of-N需要无限计算预算的挑战]，我们提出了一种 [自适应生成方案和最优加权LLM集成方法]，并在 [多个重推理问题集] 上通过 [准确率和计算效率] 验证了其有效性。",
                    "summary_translation": "我们研究基于多数投票的大语言模型（large language models, LLMs）的best-of-$N$方法。特别地，我们分析了$N \\to \\infty$的极限情况，并将其表示为Best-of-$\\infty$。尽管该方法在极限情况下取得了令人印象深刻的性能，但它需要无限的测试时间预算（test-time budget）。为解决这一问题，我们提出了一种自适应生成方案，该方案基于答案一致性选择$N$，从而有效分配推理时间计算（inference-time computation）。除了自适应性外，我们将该框架扩展到多个LLM的加权集成（weighted ensembles），表明这种混合方法可以优于任何单个模型。最优集成权重被构建为一个混合整数线性规划（mixed-integer linear program）并得到有效计算。大量实验证明了我们方法的有效性。"
                },
                {
                    "title": "Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs",
                    "arxiv_id": "2509.21044",
                    "authors": "Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li",
                    "summary": "Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究强化学习(RL)微调如何增强大语言模型的内部机制，特别是激活强度和多样性。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，论文探索了RL微调相比监督微调(SFT)能够更有效提升LLM能力的原因，属于对LLM基础能力的深入研究。 其次，从正面指标来看，论文包含多个相关主题： - 核心概念：明确研究大语言模型(LLMs) - 训练方法：核心研究强化学习(RL)微调对LLM的影响，包括比较PPO、GRPO和DPO等不同RL方法 - 能力方向：虽然未直接研究具体推理任务，但探讨了RL微调如何提升LLM的泛化能力，这与通用推理能力密切相关 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学、生物等） - 不关注模型可靠性的应用层面（如水印、安全等） 论文的核心贡献在于揭示了RL微调如何系统性地改变LLM的内部电路机制，发现RL微调导致激活强度增加和激活模式多样化，这些内部变化可能是RL提升LLM泛化能力的原因。这种对LLM内部机制的基础性研究，直接有助于理解如何提升LLM的通用推理能力，因此完全符合研究目标。",
                    "summary2": "本文旨在探究强化学习(RL)微调如何影响大型语言模型(LLM)的内部机制。针对多种LLM架构，我们提出了一种基于边缘归因修补(EAP)的分析框架，并在多个数学基准测试上通过激活强度、信息复杂度和分布峰度等指标验证了RL微调增强了模型内部激活强度和多样性，揭示了RL提升模型性能的内在机制。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）通过大规模预训练获取广泛的先验知识，并可通过监督微调（supervised fine-tuning, SFT）或基于强化学习（reinforcement learning, RL）的后训练进一步优化。越来越多的证据表明，强化学习微调能够提升大型语言模型的能力，超越仅使用监督微调所达到的效果。然而，强化学习微调为何能够增强具有不同内在特征的各种大型语言模型能力的潜在机制仍未被充分探索。在本研究中，我们借鉴先前关于边缘归因修补（edge attribution patching, EAP）的工作，来研究大型语言模型在强化学习微调前后的内部差异。\n\n我们对多个模型家族的分析显示，在线强化学习后训练有两个稳健效果：（i）激活强度整体增加，表明更多的内部通路被激活且其信号变得更强；（ii）激活模式更加多样化，反映为更高的熵和更分散的边缘分布。这些变化表明，强化学习重塑了信息流，使其既更加冗余又更加灵活，这可能解释了其在泛化方面的优势。值得注意的是，使用直接偏好优化（Direct Preference Optimization, DPO）微调的模型偏离了这些趋势，与基于PPO和GRPO的训练相比，表现出明显较弱或不一致的内部变化。总体而言，我们的发现提供了关于强化学习微调如何系统性改变大型语言模型内部电路的统一视角，并突显了在线强化学习与基于偏好方法之间的方法论区别。我们的代码已在https://anonymous.4open.science/r/llm_rl_probing_analysis-F673开源。"
                },
                {
                    "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments",
                    "arxiv_id": "2509.20386",
                    "authors": "Nishant Gaurav, Adit Akarsh, Ankit Ranjan, Manoj Bajaj",
                    "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef- ficiently operate with extensive Model Control Protocol (MCP) tool sets that exceed the contextual memory limitations of large language models. Our approach addresses the fundamental challenge of tool selection in environments containing hundreds or thousands of available tools, where loading all tools simultaneously is computationally infeasible. We propose and evaluate five distinct architectures that progressively refine the tool selection process, culminating in a search-and-load mechanism that achieves intelligent tool selection with minimal computational overhead. Our experimental results demonstrate that the proposed approach reduces tool loading by up to 50% while maintaining task completion accuracy, advancing the path towards truly general-purpose AI agents capable of dynamically adapting to diverse task environments.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出Dynamic ReAct方法，用于改进ReAct智能体在大型工具环境中的工具选择能力。ReAct是一种结合推理和行动的智能体框架，使大语言模型能够进行推理并使用工具解决问题。论文的核心贡献是解决当工具数量超过LLM上下文记忆限制时的工具选择挑战，这属于增强LLM通用推理能力的范畴，特别是工具使用和问题解决方面。论文不是将LLM作为工具应用到特定领域，而是改进LLM本身通过智能体框架使用工具的能力，因此符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：涉及ReAct agents，这是基于LLM的智能体框架 - 能力方向：与problem-solving相关，因为工具选择是问题解决的关键环节 - 新兴范式：明确涉及llm-based agents和tool use，这是论文的核心主题 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不是针对特定应用领域（如医疗、化学等），而是关注通用工具选择机制 - 不涉及模型可靠性方面的应用层面问题 第四步：特殊和模糊情况处理 论文提出的Dynamic ReAct方法是一种通用的工具选择机制，旨在增强LLM通过智能体框架使用工具的通用问题解决能力，而不是将工具应用在特定领域。这符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况，因此应该保留。 最终决策 这篇论文的核心贡献是提出了一种改进LLM通过智能体框架使用工具的通用能力的方法，从而增强了LLM的通用推理和问题解决能力。论文专注于工具选择这一关键环节，使LLM能够更有效地在大型工具环境中运作，这直接符合提高大语言模型通用推理能力的研究目标。",
                    "summary2": "本文旨在解决LLM代理在大型MCP环境中工具选择效率低下的问题。针对包含大量工具的注册表场景，我们提出了一种Dynamic ReAct方法，结合元工具和语义搜索实现动态工具选择，并在实验环境中通过工具加载减少率(50%)和任务完成准确率验证了其有效性。",
                    "summary_translation": "我们提出了Dynamic ReAct（动态反应），这是一种新颖的方法，使ReAct代理能够高效地操作超出大型语言模型（large language models）上下文内存限制的广泛Model Control Protocol (MCP)（模型控制协议）工具集。我们的方法解决了在包含数百或数千个可用工具的环境中的工具选择基本挑战，在这种环境中同时加载所有工具在计算上是不可行的。我们提出并评估了五种不同的架构，这些架构逐步完善工具选择过程，最终形成一种search-and-load mechanism（搜索加载机制），该机制以最小的computational overhead（计算开销）实现智能工具选择。我们的实验结果表明，所提出的方法将工具加载减少了高达50%，同时保持了task completion accuracy（任务完成准确性），为能够动态适应多样化任务环境的真正general-purpose AI agents（通用人工智能代理）的发展铺平了道路。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 20,
            "papers": [
                {
                    "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning",
                    "arxiv_id": "2509.21193",
                    "authors": "Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin",
                    "summary": "Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: https://github.com/tangxiangru/Eigen-1.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种统一框架，通过结合隐式检索和结构化协作来增强大型语言模型的推理能力。从筛选标准来看，该论文完全符合我的研究目标。首先，论文本质上是关于改进LLM的基础推理能力，提出了基于Monitor的检索模块、分层解决方案改进(HSR)和质量感知迭代推理(QAIR)等新方法，这些都属于提升LLM通用推理能力的范畴。其次，论文包含多个正面指标，如明确研究大型语言模型(LLMs)、关注推理能力(reasoning)、采用多智能体系统(multi-agent systems)和工具使用(tool use)等新兴范式。虽然论文标题提到\"Scientific Reasoning\"，但其框架是通用的，并非局限于特定应用领域，而是以科学推理作为测试场景来验证其通用推理框架的有效性。论文解决的是LLM推理中的普遍问题（如检索导致的推理碎片化和多智能体流水线中的解决方案稀释问题），提出的方法具有通用性，可以应用于各种推理任务。因此，该论文符合研究范围，应被保留。",
                    "summary2": "本文旨在解决科学推理中显式检索导致的推理碎片化和多智能体协作中的解决方案稀释问题。针对复杂的科学推理任务，我们提出了一种结合Monitor-based RAG、Hierarchical Solution Refinement (HSR)和Quality-Aware Iterative Reasoning (QAIR)的自适应多智能体精炼框架，并在Humanitys Last Exam (HLE) Bio/Chem Gold数据集上通过准确率、token使用量和智能体步骤数验证了其有效性。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）最近在科学推理方面显示出强劲的进展，但仍存在两个主要瓶颈。首先，显式检索（explicit retrieval）割裂了推理过程，施加了一种隐藏的\"工具税\"（tool tax），即额外的令牌（tokens）和步骤。其次，多智能体管道（multi-agent pipelines）常常通过对所有候选方案进行平均而稀释了优质解决方案。我们提出了一个统一框架来应对这些挑战，该框架结合了隐式检索（implicit retrieval）和结构化协作（structured collaboration）。在该框架基础上，一个基于监控器的检索模块（Monitor-based retrieval module）在令牌（token）级别运行，以最小的推理中断整合外部知识。在此基础之上，分层解决方案精炼（Hierarchical Solution Refinement, HSR）迭代地将每个候选方案指定为锚点（anchor），由其同行进行修复，而质量感知迭代推理（Quality-Aware Iterative Reasoning, QAIR）则根据解决方案质量调整精炼过程。在\"人类终极考试\"（Humanity's Last Exam, HLE）生物/化学金牌数据集上，我们的框架达到了48.3%的准确率——这是迄今为止报道的最高水平，比最强的智能体基线高出13.4个百分点，比前沿大型语言模型领先高达18.1个百分点，同时将令牌使用量减少了53.5%，智能体步骤减少了43.7%。在SuperGPQA和TRQA上的结果证实了该框架在不同领域的鲁棒性（robustness）。错误分析显示，推理失败和知识缺口在超过85%的情况下同时出现，而多样性分析揭示了一个明显的二分法：检索任务受益于解决方案的多样性，而推理任务则倾向于共识。总体而言，这些发现展示了隐式增强（implicit augmentation）和结构化精炼（structured refinement）如何克服显式工具使用（explicit tool use）和统一聚合（uniform aggregation）的低效问题。代码可在以下网址获取：https://github.com/tangxiangru/Eigen-1。"
                },
                {
                    "title": "Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond",
                    "arxiv_id": "2509.21284",
                    "authors": "Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng",
                    "summary": "Existing research indicates that the output of Chain-of-Thought (CoT) is significantly affected by input perturbations. Although many methods aim to mitigate such impact by optimizing prompts, a theoretical explanation of how these perturbations influence CoT outputs remains an open area of research. This gap limits our in-depth understanding of how input perturbations propagate during the reasoning process and hinders further improvements in prompt optimization methods. Therefore, in this paper, we theoretically analyze the effect of input perturbations on the fluctuation of CoT outputs. We first derive an upper bound for input perturbations under the condition that the output fluctuation is within an acceptable range, based on which we prove that: (i) This upper bound is positively correlated with the number of reasoning steps in the CoT; (ii) Even an infinitely long reasoning process cannot eliminate the impact of input perturbations. We then apply these conclusions to the Linear Self-Attention (LSA) model, which can be viewed as a simplified version of the Transformer. For the LSA model, we prove that the upper bound for input perturbation is negatively correlated with the norms of the input embedding and hidden state vectors. To validate this theoretical analysis, we conduct experiments on three mainstream datasets and four mainstream models. The experimental results align with our theoretical analysis, empirically demonstrating the correctness of our findings.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是对思维链(CoT)的鲁棒性进行了理论分析，研究了输入扰动如何影响CoT的推理过程和输出。论文推导了在输出波动可接受范围内输入扰动的上界，并证明了该上界与CoT中的推理步骤数量正相关，以及即使无限长的推理过程也无法消除输入扰动的影响。这些发现对于理解和改进LLM的推理能力具有重要意义。思维链(CoT)是提高大语言模型推理能力的关键方法，论文从理论角度分析其鲁棒性，属于改进LLM基础能力和推理能力的研究范畴，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。论文不涉及多模态、特定应用领域或模型可靠性的应用层面研究，而是关注通用推理能力的理论分析，因此应该被保留。",
                    "summary2": "本文旨在解决Chain-of-Thought (CoT)对输入扰动的鲁棒性问题。针对大型语言模型中的推理过程，我们提出了理论分析框架，推导了输入扰动对输出波动影响的上界，并在MATH、MMLU-Pro和GPQA数据集上通过Exact Match和Output Fluctuation指标验证了其有效性。我们证明了CoT推理步数增加可降低输出波动但无法完全消除扰动影响，且在Linear Self-Attention模型中，输入鲁棒性与嵌入向量和隐藏状态向量的范数呈负相关。",
                    "summary_translation": "现有研究表明，思维链（Chain-of-Thought, CoT，一种推理方法）的输出受到输入扰动（input perturbations）的显著影响。尽管许多方法试图通过优化提示（prompts）来减轻这种影响，但这些扰动如何影响CoT输出的理论解释仍是一个开放的研究领域。这一空白限制了我们对于输入扰动在推理过程中如何传播的深入理解，并阻碍了提示优化方法的进一步改进。因此，在本文中，我们从理论上分析了输入扰动对CoT输出波动的影响。我们首先在输出波动处于可接受范围内的条件下，推导出输入扰动的上界，并基于此证明：(i) 该上界与CoT中的推理步骤数量呈正相关；(ii) 即使无限长的推理过程也无法消除输入扰动的影响。随后，我们将这些结论应用于线性自注意力（Linear Self-Attention, LSA）模型，该模型可视为Transformer（一种主流神经网络架构）的简化版本。对于LSA模型，我们证明了输入扰动的上界与输入嵌入（input embedding）和隐藏状态（hidden state）向量的范数（norms）呈负相关。为验证这一理论分析，我们在三个主流数据集和四个主流模型上进行了实验。实验结果与我们的理论分析一致，从实证上证明了我们发现的正确性。"
                },
                {
                    "title": "Query-Centric Graph Retrieval Augmented Generation",
                    "arxiv_id": "2509.21237",
                    "authors": "Yaxiong Wu, Jianyuan Bo, Yongyue Zhang, Sheng Liang, Yong Liu",
                    "summary": "Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs) with external knowledge for long-context understanding and multi-hop reasoning, but existing methods face a granularity dilemma: fine-grained entity-level graphs incur high token costs and lose context, while coarse document-level graphs fail to capture nuanced relations. We introduce QCG-RAG, a query-centric graph RAG framework that enables query-granular indexing and multi-hop chunk retrieval. Our query-centric approach leverages Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with controllable granularity, improving graph quality and interpretability. A tailored multi-hop retrieval mechanism then selects relevant chunks via the generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG consistently outperforms prior chunk-based and graph-based RAG methods in question answering accuracy, establishing a new paradigm for multi-hop reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出QCG-RAG，一种以查询为中心的图检索增强生成框架，旨在改进LLM的多跳推理能力。论文明确指出其目标是解决现有图RAG方法中的粒度困境，通过查询粒度索引和多跳块检索机制来增强LLM的推理能力。这属于改进LLM基础能力和增强其多步推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs)\" - 能力方向：专注于\"multi-hop reasoning\"（多跳推理），这是通用推理能力的重要组成部分 - 新兴范式：RAG本身可视为一种工具使用形式，论文提出的新框架增强了LLM利用外部知识的能力 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不是针对医疗、化学、生物等特定应用领域 - 不主要关注水印、安全性等模型可靠性问题 第四步：特殊和模糊情况处理 论文提出的RAG框架可以视为一种通用的工具使用方法，目的是增强LLM的通用问题解决能力（特别是多跳推理），而非应用于特定领域。虽然论文提到了\"improving graph quality and interpretability\"，但这是作为提高推理质量的手段，而非主要焦点。 综上所述，这篇论文的核心贡献是提出了一种新的通用框架来增强LLM的多跳推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决基于图的检索增强生成(RAG)方法面临的粒度困境。针对多跳推理和长上下文理解场景，我们提出了一种以查询为中心的图检索增强生成框架(QCG-RAG)，并在LiHuaWorld和MultiHop-RAG数据集上通过问答准确性指标验证了其有效性。",
                    "summary_translation": "基于图的检索增强生成（Graph-based retrieval-augmented generation, RAG）通过外部知识丰富大型语言模型（Large Language Models, LLMs），以实现长上下文理解（long-context understanding）和多跳推理（multi-hop reasoning），但现有方法面临粒度困境（granularity dilemma）：细粒度实体级图（fine-grained entity-level graphs）导致高token成本（token costs）并丢失上下文，而粗粒度文档级图（coarse document-level graphs）则无法捕捉细微关系。我们提出了QCG-RAG，一种以查询为中心的图RAG框架（query-centric graph RAG framework），实现了查询粒度索引（query-granular indexing）和多跳块检索（multi-hop chunk retrieval）。我们的以查询为中心的方法利用Doc2Query和Doc2Query{-}{-}构建具有可控粒度（controllable granularity）的以查询为中心的图，提高了图质量（graph quality）和可解释性（interpretability）。随后，一个定制的多跳检索机制（tailored multi-hop retrieval mechanism）通过生成的查询选择相关块（relevant chunks）。在LiHuaWorld和MultiHop-RAG上的实验表明，QCG-RAG在问答准确性（question answering accuracy）方面始终优于先前的基于块（chunk-based）和基于图（graph-based）的RAG方法，为多跳推理建立了新范式（new paradigm）。"
                },
                {
                    "title": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents",
                    "arxiv_id": "2509.21212",
                    "authors": "Yaxiong Wu, Yongyue Zhang, Sheng Liang, Yong Liu",
                    "summary": "Long-term conversational agents require effective memory management to handle dialogue histories that exceed the context window of large language models (LLMs). Existing methods based on fact extraction or summarization reduce redundancy but struggle to organize and retrieve relevant information across different granularities of dialogue and generated memory. We introduce SGMem (Sentence Graph Memory), which represents dialogue as sentence-level graphs within chunked units, capturing associations across turn-, round-, and session-level contexts. By combining retrieved raw dialogue with generated memory such as summaries, facts and insights, SGMem supplies LLMs with coherent and relevant context for response generation. Experiments on LongMemEval and LoCoMo show that SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出SGMem（句子图记忆）方法，用于增强大语言模型在长期对话中的记忆管理能力。从筛选标准来看，首先，论文的本质是改进LLM的基础能力，特别是在处理超出上下文窗口的长期对话时的信息组织和检索能力，这属于增强LLM通用推理能力的范畴。有效的记忆管理是进行连贯推理和多步对话的基础，因此这项工作直接提升了LLM的通用能力。其次，论文包含了正面指标中的\"Large language models, LLMs\"和\"llm-based agents\"概念。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。最后，论文提出的SGMem是一种通用的记忆管理框架，用于增强对话代理的通用能力，而非应用于特定领域。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决长期对话代理中的记忆管理问题，特别是当对话历史超过大型语言模型上下文窗口时的记忆碎片化问题。针对长期对话历史和生成的记忆（如摘要、事实和见解），我们提出了一种SGMem（句子图记忆）方法，将对话表示为分块单元内的句子级别图，捕获跨不同粒度的关联。在LongMemEval和LoCoMo数据集上通过准确率指标验证了其有效性，实验结果表明SGMem在长期对话问答任务中持续提高准确性并优于强基线方法。",
                    "summary_translation": "长期对话代理（long-term conversational agents）需要有效的记忆管理（memory management）来处理超出大型语言模型（large language models, LLMs）上下文窗口（context window）的对话历史。现有的基于事实提取（fact extraction）或摘要生成（summarization）的方法虽然减少了冗余，但在组织和检索不同粒度（granularities）的对话和生成记忆中的相关信息方面存在困难。我们提出了SGMem（Sentence Graph Memory，句子图记忆），它将对话表示为分块单元（chunked units）内的句子级图（sentence-level graphs），捕捉轮次（turn-level）、回合（round-level）和会话级别（session-level）上下文之间的关联。通过将检索到的原始对话与生成的记忆（如摘要、事实和见解）相结合，SGMem为大型语言模型（LLMs）提供连贯且相关的上下文，用于生成响应。在LongMemEval和LoCoMo数据集上的实验表明，SGMem在长期对话问答（long-term conversational question answering）任务中持续提高准确性，并优于强大的基线模型（strong baselines）。"
                },
                {
                    "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards",
                    "arxiv_id": "2509.21319",
                    "authors": "Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev",
                    "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 首先，从核心判断来看，该论文的本质是提出一种新的强化学习范式RLBFF（Reinforcement Learning with Binary Flexible Feedback），用于改进LLM的后训练阶段。这属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，而非将LLM作为工具应用到特定领域。论文通过结合人类反馈(RLHF)和可验证奖励(RLVR)的优势，提出了一种新的训练方法来增强LLM的响应质量和对齐能力。 其次，论文包含多个正面指标： 1. 核心概念：明确关注LLMs，并使用Qwen3-32B作为示例模型 2. 训练方法：核心就是提出一种新的强化学习方法(RLBFF)，这是RLHF和RLVR的结合 3. 能力方向：虽然未直接强调推理，但提高LLM的响应质量隐含了推理能力的提升，且论文在MT-Bench等包含推理任务的基准上进行了评估 第三，论文不涉及任何排除标准中的领域： 1. 未涉及多模态与视觉内容 2. 未针对特定应用领域（如医疗、化学等） 3. 虽然涉及模型对齐，但核心是提出新方法提高整体响应质量，而非专注于水印、安全等应用层面 最后，在特殊和模糊情况处理上，论文明确解决了RLHF的可解释性问题，属于\"提出新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量\"的情况，符合保留条件。 综上所述，该论文的核心贡献是提出了一种新的强化学习训练方法来提升LLM的通用能力和对齐质量，这与研究目标\"提高大语言模型的通用推理能力\"高度一致，因此应该被保留。",
                    "summary2": "本文旨在解决RLHF缺乏可解释性和RLVR范围有限的问题。针对LLM后训练中的奖励建模，我们提出了一种二元灵活反馈方法（RLBFF），结合人类偏好与规则验证，并在RM-Bench、JudgeBench和PrincipleBench上通过准确率等指标验证了其有效性。",
                    "summary_translation": "基于人类反馈的强化学习（Reinforcement Learning with Human Feedback, RLHF）和基于可验证奖励的强化学习（Reinforcement Learning with Verifiable Rewards, RLVR）是大型语言模型（Large Language Model, LLM）后训练中使用的主要强化学习范式，各自具有独特的优势。然而，RLHF在可解释性和奖励破解（reward hacking）方面存在困难，因为它依赖于通常缺乏明确标准的人类判断，而RLVR因其专注于基于正确性的验证器而受到范围限制。我们提出了基于二元灵活反馈的强化学习（Reinforcement Learning with Binary Flexible Feedback, RLBFF），它结合了人类驱动偏好的多样性和基于规则验证的精确性，使奖励模型能够捕捉超越单纯正确性的响应质量的细微方面。RLBFF从自然语言反馈中提取可以以二元方式回答的原则（例如，信息准确性：是，或代码可读性：否）。这些原则随后可用于将奖励模型训练定位为一个蕴涵任务（entailment task）（响应满足或不满足任意原则）。我们表明，以这种方式训练的奖励模型在数据匹配的情况下可以优于Bradley-Terry模型，并在RM-Bench（86.2%）和JudgeBench（81.4%，截至2025年9月24日排行榜第一）上取得顶尖性能。此外，与Bradley-Terry模型不同，用户可以在推理时指定感兴趣的原则，以自定义我们奖励模型的关注点。最后，我们提出了一个完全开源的方案（包括数据），使用RLBFF和我们的奖励模型对Qwen3-32B进行对齐，以匹配或超过o3-mini和DeepSeek R1在MT-Bench、WildBench和Arena Hard v2等通用对齐基准上的性能（推理成本低于5%）。"
                },
                {
                    "title": "WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs",
                    "arxiv_id": "2509.20863",
                    "authors": "Guowei Xu, Wenxin Xu, Jiawang Zhao, Kaisheng Ma",
                    "summary": "Diffusion models have recently shown strong potential in language modeling, offering faster generation compared to traditional autoregressive approaches. However, applying supervised fine-tuning (SFT) to diffusion models remains challenging, as they lack precise probability estimates at each denoising step. While the diffusion mechanism enables the model to reason over entire sequences, it also makes the generation process less predictable and often inconsistent. This highlights the importance of controlling key tokens that guide the direction of generation. To address this issue, we propose WeFT, a weighted SFT method for diffusion language models, where tokens are assigned different weights based on their entropy. Derived from diffusion theory, WeFT delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and MATH-500). The code and models will be made publicly available.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出WeFT（加权熵驱动微调方法），这是一种针对扩散语言模型(dLLMs)的新训练范式。论文的核心贡献在于解决扩散语言模型在监督微调过程中的挑战，通过基于熵的标记加权来增强模型的推理能力。这明确属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴，因此应保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究扩散语言模型(dLLMs)，这是大语言模型的一种变体 - 能力方向：论文在四个推理基准(Sudoku、Countdown、GSM8K和MATH-500)上评估方法，这些基准涉及数学推理和逻辑推理能力 - 论文虽然不涉及强化学习或智能体等新兴范式，但已包含足够的核心正面指标 第三步：排除标准 论文不符合任何排除标准： - 虽然提到扩散模型，但这是应用于语言建模而非视觉或多模态领域 - 没有专注于任何特定应用领域（如医疗、化学等） - 没有主要关注水印、安全等模型可靠性问题 第四步：特殊和模糊情况 论文不涉及需要特殊判断的智能体/工具使用或幻觉/可解释性/安全等模糊情况。 综合分析，这篇论文通过提出新的微调方法来增强扩散语言模型的通用推理能力，并在多个推理基准上验证了其有效性，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决扩散语言模型(dLLMs)在监督微调(SFT)过程中缺乏精确概率估计的问题。针对token重要性不均匀的场景，我们提出了一种基于熵加权的WeFT方法，并在Sudoku、Countdown、GSM8K和MATH-500四个推理基准上通过准确率指标验证了其有效性。该方法通过token预测熵分配不同掩码率，使高熵token获得更强训练信号，相比标准SFT实现了最高83%的相对性能提升。",
                    "summary_translation": "扩散模型（Diffusion models）最近在语言建模（language modeling）方面展现出强大的潜力，与传统自回归方法（autoregressive approaches）相比，提供了更快的生成速度。然而，将监督微调（supervised fine-tuning, SFT）应用于扩散模型仍然具有挑战性，因为它们在每个去噪步骤（denoising step）中缺乏精确的概率估计。虽然扩散机制（diffusion mechanism）使模型能够对整个序列进行推理（reason），但它也使生成过程变得较难预测且常常不一致。这凸显了控制引导生成方向的关键标记（key tokens）的重要性。为解决这一问题，我们提出了WeFT，一种针对扩散语言模型（diffusion language models）的加权SFT方法，其中标记根据其熵（entropy）被分配不同的权重。源自扩散理论（diffusion theory）的WeFT带来了显著收益：在open-r1数据集的s1K、s1K-1.1和3k样本上训练时，它在四个广泛使用的推理基准（reasoning benchmarks）（Sudoku、Countdown、GSM8K和MATH-500）上相比标准SFT分别实现了39%、64%和83%的相对改进。代码和模型将公开发布。"
                },
                {
                    "title": "Distilling Many-Shot In-Context Learning into a Cheat Sheet",
                    "arxiv_id": "2509.20820",
                    "authors": "Ukyo Honda, Soichiro Murakami, Peinan Zhang",
                    "summary": "Recent advances in large language models (LLMs) enable effective in-context learning (ICL) with many-shot examples, but at the cost of high computational demand due to longer input tokens. To address this, we propose cheat-sheet ICL, which distills the information from many-shot ICL into a concise textual summary (cheat sheet) used as the context at inference time. Experiments on challenging reasoning tasks show that cheat-sheet ICL achieves comparable or better performance than many-shot ICL with far fewer tokens, and matches retrieval-based ICL without requiring test-time retrieval. These findings demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs in downstream tasks.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种称为\"cheat-sheet ICL\"的新方法，旨在提高大语言模型在上下文学习中的推理效率和性能。论文将多样本上下文学习的信息提炼成简洁的文本摘要，在保持或提高推理性能的同时大幅减少了计算需求。这直接关系到提升LLM的通用推理能力，特别是在处理需要多步推理的任务时。论文不是将LLM作为工具应用到特定领域，而是关注如何改进LLM本身的推理机制，因此完全符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。此外，论文明确涉及\"Large language models\"和\"reasoning\"等正面指标，且不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文提出的是一种通用方法论，可以应用于各种推理任务，而非针对特定领域的应用。",
                    "summary2": "本文旨在解决many-shot in-context learning计算成本高的问题。针对大型语言模型处理长上下文的场景，我们提出了一种cheat-sheet ICL方法，将many-shot demonstrations的知识提炼成简洁文本摘要，并在BBH Hard推理任务上通过accuracy验证了其有效性。",
                    "summary_translation": "大型语言模型(large language models, LLMs)的最新进展使得通过多示例(many-shot examples)进行有效的上下文学习(in-context learning, ICL)成为可能，但由于输入标记(input tokens)较长，这需要高昂的计算成本(computational demand)。为解决这一问题，我们提出了备忘单ICL(cheat-sheet ICL)，该方法将多示例ICL的信息提炼成一个简洁的文本摘要(concise textual summary)（即备忘单(cheat sheet)），在推理时间(inference time)作为上下文使用。在具有挑战性的推理任务(challenging reasoning tasks)上的实验表明，备忘单ICL使用远少的标记(far fewer tokens)即可达到与多示例ICL相当甚至更优的性能(comparable or better performance)，并且无需测试时检索(test-time retrieval)就能匹配基于检索的ICL(retrieval-based ICL)。这些研究结果表明，备忘单ICL是在下游任务(downstream tasks)中利用LLMs的一种实用替代方案(practical alternative)。"
                },
                {
                    "title": "SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs",
                    "arxiv_id": "2509.20758",
                    "authors": "Jiacheng Lin, Zhongruo Wang, Kun Qian, Tian Wang, Arvind Srinivasan, Hansi Zeng, Ruochen Jiao, Xie Zhou, Jiri Gesi, Dakuo Wang, Yufan Guo, Kai Zhong, Weiqi Zhang, Sujay Sanghavi, Changyou Chen, Hyokun Yun, Lihong Li",
                    "summary": "Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach to adapt Large Language Models (LLMs) to specialized tasks but is often believed to degrade their general capabilities. In this work, we revisit this trade-off and present both empirical and theoretical insights. First, we show that SFT does not always hurt: using a smaller learning rate can substantially mitigate general performance degradation while preserving comparable target-domain performance. We then provide a theoretical analysis that explains these phenomena and further motivates a new method, Token-Adaptive Loss Reweighting (TALR). Building on this, and recognizing that smaller learning rates alone do not fully eliminate general-performance degradation in all cases, we evaluate a range of strategies for reducing general capability loss, including L2 regularization, LoRA, model averaging, FLOW, and our proposed TALR. Experimental results demonstrate that while no method completely eliminates the trade-off, TALR consistently outperforms these baselines in balancing domain-specific gains and general capabilities. Finally, we distill our findings into practical guidelines for adapting LLMs to new domains: (i) using a small learning rate to achieve a favorable trade-off, and (ii) when a stronger balance is further desired, adopt TALR as an effective strategy.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是研究如何在大语言模型进行特定领域微调时保持其通用能力，提出了一种新的训练方法Token-Adaptive Loss Reweighting (TALR)。从本质上看，论文关注的是改进LLM的基础能力，提出新的训练范式来增强模型的通用推理和问题解决能力，而不是将LLM作为工具应用到特定领域。论文通过理论和实验分析，探索了如何减轻监督微调对模型通用能力的损害，这直接关系到提升LLM的通用推理能力。虽然论文讨论了领域特定的微调，但其焦点不是特定应用领域，而是如何在领域微调时保持通用能力。论文符合\"改进LLM的基础能力、提出新的训练范式\"的标准，不涉及多模态、特定应用领域或模型可靠性等排除标准。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决大语言模型领域特定微调(SFT)导致一般能力下降的问题。针对医学计算和电商分类等特定领域数据集，我们提出了一种Token-Adaptive Loss Reweighting (TALR)方法，通过动态降低困难token的权重来减轻性能退化。在MedCalc和ESCI数据集上的实验表明，使用较小学习率并结合TALR能在保持领域性能的同时，显著减轻一般能力下降，优于L2正则化、LoRA等基线方法。",
                    "summary_translation": "在特定领域数据集上进行监督微调（Supervised Fine-Tuning, SFT）是使大型语言模型（Large Language Models, LLMs）适应专门任务的常见方法，但通常被认为会降低其通用能力。在本研究中，我们重新审视了这种权衡，并提供了实证和理论两方面的见解。首先，我们表明SFT并不总是有害的：使用较小的学习率可以显著减轻通用性能的下降，同时保持相当的目标领域性能。接着，我们提供了理论分析来解释这些现象，并进一步提出了一种新方法——令牌自适应损失重加权（Token-Adaptive Loss Reweighting, TALR）。在此基础上，并认识到仅靠较小的学习率并不能在所有情况下完全消除通用性能的下降，我们评估了一系列减少通用能力损失的策略，包括L2正则化（L2 regularization）、LoRA、模型平均（model averaging）、FLOW以及我们提出的TALR。实验结果表明，虽然没有任何方法能完全消除这种权衡，但TALR在平衡特定领域收益和通用能力方面始终优于这些基线方法。最后，我们将我们的发现提炼为将LLMs适应新领域的实用指南：(i) 使用较小的学习率以实现有利的权衡；(ii) 当需要更强的平衡时，采用TALR作为一种有效策略。"
                },
                {
                    "title": "Confidence-guided Refinement Reasoning for Zero-shot Question Answering",
                    "arxiv_id": "2509.20750",
                    "authors": "Youwon Jang, Woo Suk Choi, Minjoon Jung, Minsu Lee, Byoung-Tak Zhang",
                    "summary": "We propose Confidence-guided Refinement Reasoning (C2R), a novel training-free framework applicable to question-answering (QA) tasks across text, image, and video domains. C2R strategically constructs and refines sub-questions and their answers (sub-QAs), deriving a better confidence score for the target answer. C2R first curates a subset of sub-QAs to explore diverse reasoning paths, then compares the confidence scores of the resulting answer candidates to select the most reliable final answer. Since C2R relies solely on confidence scores derived from the model itself, it can be seamlessly integrated with various existing QA models, demonstrating consistent performance improvements across diverse models and benchmarks. Furthermore, we provide essential yet underexplored insights into how leveraging sub-QAs affects model behavior, specifically analyzing the impact of both the quantity and quality of sub-QAs on achieving robust and reliable reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Confidence-guided Refinement Reasoning (C2R)\"的新型推理框架，专注于改进大语言模型的通用推理能力。从筛选标准来看： 首先，在核心判断层面，论文本质上是关于改进LLM的推理能力的，提出了一种新的训练免费推理框架，通过构建和细化子问题及其答案来增强模型的推理过程。这直接符合\"改进LLM基础能力\"和\"增强其逻辑推理能力\"的保留标准。 其次，从正面指标看，论文明确聚焦于\"reasoning\"这一核心能力方向，讨论了\"探索多样化推理路径\"和\"实现稳健可靠的推理\"，这些都是通用推理能力的关键组成部分。 关于多模态方面，虽然论文提到其框架适用于文本、图像和视频领域，但这只是表明其方法的通用性，而非主要焦点。论文的核心是推理框架本身，而不是解决多模态或视觉特定问题，因此不应被排除。 最后，C2R作为一种可以与各种现有QA模型无缝集成的通用方法，代表了提升LLM推理能力的新方法论，完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决零样本问答中子问题-答案对可能引入噪声的问题。针对多模态QA任务，我们提出了一种基于置信度引导的细化推理框架(C2R)，并在五个模型和五个基准测试上验证了其有效性。",
                    "summary_translation": "我们提出了一种置信度引导的精炼推理（Confidence-guided Refinement Reasoning, C2R）框架，这是一种新颖的无需训练的方法，适用于文本、图像和视频领域的问答（question-answering, QA）任务。C2R通过策略性地构建和细化子问题及其答案（sub-QAs），为目标答案推导出更优的置信度分数（confidence score）。C2R首先精选一个子问题子集以探索多样化的推理路径（reasoning paths），然后比较所得答案候选（answer candidates）的置信度分数，以选择最可靠的最终答案。由于C2R仅依赖于模型自身生成的置信度分数，它可以与各种现有的QA模型无缝集成，在不同模型和基准测试（benchmarks）上均展现出一致的性能提升。此外，我们提供了关于利用子问题如何影响模型行为的重要但尚未充分探索的见解，特别分析了子问题的数量和质量对实现稳健可靠推理的影响。"
                },
                {
                    "title": "Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures",
                    "arxiv_id": "2509.20577",
                    "authors": "Sampurna Roy, Ayan Sar, Anurag Kaushish, Kanav Gupta, Tanupriya Choudhury, Abhijit Kumar",
                    "summary": "Contemporary transformer architectures apply identical processing depth to all inputs, creating inefficiencies and limiting reasoning quality. Simple factual queries are subjected to the same multilayered computation as complex logical problems, wasting resources while constraining deep inference. To overcome this, we came up with a concept of Dynamic Reasoning Chains through Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends the Mixture of Experts paradigm from width-based to depth specialised computation. DS-MoE introduces expert modules optimised for distinct reasoning depths, shallow pattern recognition, compositional reasoning, logical inference, memory integration, and meta-cognitive supervision. A learned routing network dynamically assembles custom reasoning chains, activating only the necessary experts to match input complexity. The dataset on which we trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse domains such as scientific papers, legal texts, programming code, and web content, enabling systematic assessment across reasoning depths. Experimental results demonstrate that DS-MoE achieves up to 16 per cent computational savings and 35 per cent faster inference compared to uniform-depth transformers, while delivering 2.8 per cent higher accuracy on complex multi-step reasoning benchmarks. Furthermore, routing decisions yield interpretable reasoning chains, enhancing transparency and scalability. These findings establish DS-MoE as a significant advancement in adaptive neural architectures, demonstrating that depth-specialised modular processing can simultaneously improve efficiency, reasoning quality, and interpretability in large-scale language models.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。根据筛选标准，我的判断过程如下： 第一步：核心判断 论文的核心是提出DS-MoE（Depth Specialised Mixture of Experts）框架，这是一种改进Transformer架构的新方法，通过深度专业化的专家混合模型来动态构建推理链。论文明确关注增强LLM的逻辑推理和多步推理能力，而不是将LLM作为工具应用于特定领域。这完全符合改进LLM基础能力和增强其通用推理能力的研究目标。 第二步：正面指标 论文包含多个正面指标： - 核心概念：虽然摘要未直接提及\"LLMs\"，但DS-MoE明显是针对大语言模型的基础架构改进 - 能力方向：明确涉及\"reasoning chains\"、\"reasoning depths\"、\"logical inference\"和\"complex multi-step reasoning benchmarks\"，直接针对推理能力，特别是逻辑推理和多步推理 - 论文强调通过动态组装定制的推理链来匹配输入复杂性，这正是提升通用推理能力的核心 第三步：排除标准 论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉相关内容 - 虽然训练数据集包含多个领域，但论文本身不是针对特定应用领域的研究 - 未涉及模型可靠性方面的水印、安全性等内容 第四步：特殊和模糊情况 论文提到\"routing decisions yield interpretable reasoning chains, enhancing transparency\"，这表明其方法增强了模型的可解释性，从而可能提升模型的通用可靠性和推理质量，符合研究目标。 综上所述，这篇论文的核心贡献是通过改进Transformer架构来增强LLM的通用推理能力，特别是逻辑推理和多步推理能力，同时提高计算效率和可解释性，完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决传统Transformer架构对所有输入应用相同处理深度导致的效率低下和推理质量受限问题。针对不同复杂度的输入任务，我们提出了一种Depth-Specialised Mixture-of-Experts (DS-MoE)方法，通过动态组装深度专业化专家模块构建推理链，并在The Pile数据集上通过计算效率、推理速度和准确率等指标验证了其有效性。",
                    "summary_translation": "# 中文翻译\n\n当代transformer架构对所有输入应用相同的处理深度，造成效率低下并限制了推理质量。简单的事实查询与复杂的逻辑问题受到同样的多层计算处理，浪费资源的同时也限制了深度推理能力。为克服这一问题，我们提出了通过深度专业化的专家混合(Depth Specialised Mixture of Experts, DS-MoE)实现动态推理链的概念，这是一个模块化框架，将专家混合(Mixture of Experts)范式从基于宽度的计算扩展到深度专业化的计算。DS-MoE引入了针对不同推理深度优化的专家模块，包括浅层模式识别(shallow pattern recognition)、组合推理(compositional reasoning)、逻辑推理(logical inference)、记忆整合(memory integration)和元认知监督(meta-cognitive supervision)。一个学习的路由网络(learned routing network)动态组装自定义推理链，仅激活必要的专家以匹配输入复杂度。\n\n我们训练和评估DS-MoE所使用的数据集是The Pile，一个800GB的语料库，涵盖科学论文、法律文本、编程代码和网络内容等多个领域，能够对各种推理深度进行系统评估。实验结果表明，与统一深度的transformer相比，DS-MoE实现了高达16%的计算节省和35%更快的推理速度，同时在复杂多步推理基准测试中提供了2.8%更高的准确率。此外，路由决策产生可解释的推理链(interpretable reasoning chains)，增强了透明度和可扩展性。这些发现确立了DS-MoE作为自适应神经架构(adaptive neural architectures)的重要进展，证明了深度专业化的模块化处理可以同时提高大规模语言模型的效率、推理质量和可解释性。"
                },
                {
                    "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning",
                    "arxiv_id": "2509.20502",
                    "authors": "Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang",
                    "summary": "Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\\%. Code is available at https://github.com/xwang97/MARS.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是提出MARS（Multi-Agent Review System），一种基于角色的多智能体协作框架，旨在提高LLM的推理能力。论文的核心贡献是改进LLM的基础推理能力，提出了一种新的多智能体协作范式，而不是将LLM作为工具应用于特定领域。因此，这篇论文符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Large language models (LLMs)\" - 能力方向：直接关注\"LLM reasoning\"，旨在提高模型的推理质量 - 新兴范式：提出了多智能体系统(MARS)，这是一种基于LLM的智能体协作框架 论文符合3个正面指标，表明其与研究主题高度相关。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域（如医疗、化学等） - 不关注模型可靠性层面的水印、安全等问题 第四步：特殊和模糊情况 论文提出的MARS框架是一种通用的智能体协作方法，用于增强LLM的通用推理能力，而不是将智能体应用于特定领域。根据筛选标准，这种通用的智能体协作框架应该保留。 综合分析，这篇论文的核心贡献是提出了一种更高效的多智能体协作框架(MARS)来增强LLM的通用推理能力，与研究目标\"提高大语言模型的通用推理能力\"直接一致。论文通过改进多智能体协作方式，在保持推理质量的同时显著提高了效率，这正属于对LLM基础推理能力的改进研究。",
                    "summary2": "本文旨在 [解决多代理协作中推理效率低下的问题]。针对 [大型语言模型推理任务]，我们提出了一种 [基于角色的多代理评审系统(MARS)]，并在 [多个推理基准测试(MMLU、GPQA、GSM8K)] 上通过 [准确率、token消耗和推理时间] 验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在自然语言理解方面取得了令人印象深刻的结果，然而当它们作为单一代理（single agents）运行时，其推理能力仍然有限。多代理辩论（Multi-Agent Debate, MAD）被提出来解决这一限制，通过使多个模型以圆桌辩论（round-table debate）的方式进行协作推理。虽然有效，但多代理辩论（MAD）由于涉及的代理数量和所需的频繁通信而引入了大量的计算开销。在本文中，我们提出了MARS（Multi-Agent Review System，多代理评审系统），一个受评审过程（review process）启发的基于角色的协作框架。在MARS中，作者代理（author agent）生成初始解决方案，评审代理（reviewer agents）独立提供决策和评论，元评审代理（meta-reviewer）整合反馈以做出最终决策并指导进一步修订。这种设计提高了推理质量，同时避免了昂贵的评审者之间的交互，从而控制了令牌消耗量（token consumption）和推理时间（inference time）。我们将MARS与多代理辩论（MAD）和其他最先进的推理策略（state-of-the-art reasoning strategies）在多个基准（benchmarks）上进行了比较。使用不同大型语言模型（LLMs）的大量实验表明，MARS匹配了多代理辩论（MAD）的准确性，同时将令牌使用量（token usage）和推理时间（inference time）减少了约50%。代码可在https://github.com/xwang97/MARS获取。"
                },
                {
                    "title": "SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation",
                    "arxiv_id": "2509.20377",
                    "authors": "Tomoaki Isoda",
                    "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive tasks in recent years. However, since retrieval systems may return irrelevant content, incorporating such information into the model often leads to hallucinations. Thus, identifying and filtering out unhelpful retrieved content is a key challenge for improving RAG performance.To better integrate the internal knowledge of the model with external knowledge from retrieval, it is essential to understand what the model \"knows\" and \"does not know\" (which is also called \"self-knowledge\"). Based on this insight, we propose SKILL-RAG (Self-Knowledge Induced Learning and Filtering for RAG), a novel method that leverages the model's self-knowledge to determine which retrieved documents are beneficial for answering a given query. We design a reinforcement learning-based training framework to explicitly elicit self-knowledge from the model and employs sentence-level granularity to filter out irrelevant content while preserving useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several question answering benchmarks. Experimental results demonstrate that SKILL-RAG not only improves generation quality but also significantly reduces the number of input documents, validating the importance of self-knowledge in guiding the selection of high-quality retrievals.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，因为它核心是关于改进大语言模型本身的通用推理能力。具体分析如下： 从第一步核心判断来看，论文的本质是提出SKILL-RAG方法，这是一种新的训练范式，通过强化学习框架来增强模型的自我知识认知能力。该方法不是将LLM作为工具应用于特定领域，而是致力于提升LLM在知识处理和推理方面的基础能力，特别是通过识别模型\"知道\"和\"不知道\"的内容来优化其推理过程，这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的标准。 从第二步正面指标看，论文包含多个相关主题： - 核心概念：明确研究LLMs（Llama2-7B和Qwen3-8B） - 能力方向：涉及推理能力，特别是在知识密集型任务中的问题解决 - 训练方法：使用强化学习（reinforcement learning-based training framework）来训练模型 - 新兴范式：RAG本身可视为一种工具使用范式，论文改进了这一通用框架 从第三步排除标准看，论文不涉及任何应排除的领域，没有专注于多模态、特定应用领域或模型基础设施等。 从第四步特殊和模糊情况看，虽然论文涉及减少幻觉的问题，但它是通过提出新方法（利用模型自我知识）来提升模型的内在推理质量，而不是应用层面的讨论，因此应该保留。 综上所述，SKILL-RAG论文的核心贡献是提出了一种增强LLM自我知识认知和推理能力的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决RAG系统中检索内容不相关导致模型幻觉的问题。针对检索增强生成场景，我们提出了一种SKILL-RAG方法，利用大语言模型的自我知识进行检索内容过滤，并在TriviaQA、SelfAware、NQ和TruthfulQA等多个问答基准测试集上通过准确率等指标验证了其有效性。",
                    "summary_translation": "近年来，检索增强生成（Retrieval-Augmented Generation, RAG）显著提升了大型语言模型（large language models, LLMs）在知识密集型任务（knowledge-intensive tasks）上的表现。然而，由于检索系统可能返回不相关内容，将这些信息整合到模型中常常导致幻觉（hallucinations）问题。因此，识别并过滤无用的检索内容是提升RAG性能的关键挑战。为了更好地将模型的内部知识与检索获得的外部知识相结合，理解模型的\"已知\"与\"未知\"（也称为\"自我知识\"（self-knowledge））至关重要。基于这一见解，我们提出了SKILL-RAG（Self-Knowledge Induced Learning and Filtering for RAG，自我知识引导的学习与过滤RAG方法），这是一种利用模型自我知识来确定哪些检索文档有益于回答特定查询的新方法。我们设计了一个基于强化学习（reinforcement learning）的训练框架，以明确地激发模型的自我知识，并采用句子级别粒度（sentence-level granularity）来过滤不相关内容，同时保留有用知识。我们在多个问答基准测试（question answering benchmarks）上使用Llama2-7B和Qwen3-8B模型对SKILL-RAG进行了评估。实验结果表明，SKILL-RAG不仅提高了生成质量，还显著减少了输入文档数量，验证了自我知识在指导高质量检索选择中的重要性。"
                },
                {
                    "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns",
                    "arxiv_id": "2509.21124",
                    "authors": "Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai",
                    "summary": "Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model's reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础推理能力。论文提出了一种新的训练范式，通过学习多样化的思维链模式来扩展基础模型的推理潜力，这直接针对提升LLM的通用推理能力，而非将LLM作为工具应用于特定领域。 其次，论文包含了多个正面指标： 1. 核心概念：论文明确研究基础模型(Foundation Model)的推理能力 2. 能力方向：聚焦于推理能力(reasoning)，特别是数学推理(mathematical reasoning) 3. 训练方法：涉及强化学习(RL)作为关键方法，并讨论如何提高下游RL性能 4. 新兴范式：关注思维链(CoT)的优化，这是提高LLM推理能力的重要范式 第三，论文不符合任何排除标准： 1. 不涉及多模态与视觉内容 2. 虽然在数学推理上评估，但这是作为通用推理能力的测试案例，而非针对特定应用领域 3. 不涉及模型可靠性方面的水印、安全等问题 论文的核心贡献是首次定义了基础模型的\"推理潜力\"，并提出了一种通过选择高价值思维链数据来扩展这种潜力的方法。这种方法通过抽象原子推理模式，构建核心参考集，并使用双粒度算法选择高价值的CoT数据，从而训练模型更有效地掌握推理能力。这些贡献直接针对提升LLM的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在扩展基础模型的推理潜力。针对复杂数学推理任务，我们提出了一种CoTP框架，通过学习多样化的思维链模式，构建富含高价值推理模式的核心参考集，并利用推理模式链和token熵的双粒度算法选择高价值CoT数据。在85A6B MoE模型上，仅用10B-token的CoTP数据，在AIME 2024和2025上提高了9.58%，并将下游RL性能上限提高了7.81%。",
                    "summary_translation": "在具有挑战性的数学推理领域，大型推理模型的最新进展是由强化学习(reinforcement learning, RL)驱动的。在中期训练过程中融入长思维链(chain-of-thought, CoT)数据也被证明可以显著提高推理深度。然而，当前的方法往往不加区分地利用CoT数据，这就留下了一个关键问题：哪些数据类型能最有效地增强模型的推理能力。在本文中，我们首次将基础模型的推理潜力定义为正确回答问题所需的独立尝试次数的倒数，这与最终模型性能密切相关。接着，我们提出利用富含高价值推理模式的多样化数据来扩展推理潜力。具体而言，我们从CoT序列中抽象出具有共性和归纳能力的原子推理模式，并用它们构建一个富含有价值推理模式的核心参考集。此外，我们提出一种涉及推理模式链和令牌熵(token entropy)的双粒度算法，从数据池中高效选择与核心集一致的高价值CoT数据(CoTP)，从而训练模型有效掌握推理。仅需100亿令牌的CoTP数据就能使85A6B专家混合模型(Mixture-of-Experts, MoE)在具有挑战性的AIME 2024和2025上提高9.58%，并将下游RL性能的上限提高7.81%。"
                },
                {
                    "title": "ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning",
                    "arxiv_id": "2509.21070",
                    "authors": "Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu",
                    "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between \"Thinking\" and \"NoThinking\" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究范围。核心判断上，论文本质是提升大语言模型的数学推理能力，属于增强LLM通用推理能力的范畴。论文提出ScaleDiff流程，通过高效生成困难数学问题来训练模型，从而提升LLM的数学推理能力，这是对LLM基础能力的改进，而非将LLM作为工具应用于特定领域。 从正面指标看，论文明确涉及Large Reasoning Models (LRMs)这一核心概念，并专注于Advanced Mathematical Reasoning这一推理能力方向。论文通过生成困难问题数据集并微调模型，实质上提出了一种新的训练范式来增强LLM的推理能力。 在排除标准方面，虽然论文专注于数学推理，但数学推理通常被视为评估和提升LLM通用推理能力的重要方面，而非特定应用领域。论文也不涉及多模态、视觉内容或特定应用领域如医疗、化学等。 论文的核心贡献是通过增加困难问题的数量来提升LLM的数学推理能力，并观察到\"随着困难问题数量增加，模型在困难基准测试上的性能呈现明显的扩展现象\"，这直接与研究目标\"提高大语言模型（LLM）本身的『通用推理能力』\"相符。因此，这篇论文应被保留。",
                    "summary2": "本文旨在 [解决大规模生成困难数学问题以提升大型推理模型数学推理能力的问题]。针对 [数学推理模型训练中困难问题获取成本高、现有自动合成方法扩展性有限的挑战]，我们提出了一种 [名为ScaleDiff的流程，包含困难问题识别、专用生成器训练和解决方案蒸馏过滤]，并在 [多个数学推理基准测试(AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500)] 上通过 [准确率等指标] 验证了其有效性。",
                    "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在复杂问题解决方面展现出令人印象深刻的能力，通常受益于那些能激发复杂推理的困难数学问题的训练。近期的研究工作探索了通过提示专有模型或大规模开源模型来自动合成数学问题的方法，这些提示基于种子数据或固有的数学概念。然而，由于这些方法具有高计算/API成本、提示的复杂性以及生成问题难度水平的局限性，将其规模化仍然具有挑战性。\n\n为克服这些限制，我们提出了ScaleDiff，一个简单而有效的流程，旨在规模化困难问题的创建。我们使用自适应思维模型（adaptive thinking model），仅需一次前向传播就能高效地从现有数据集中识别困难问题，该模型能够感知问题难度并自动在\"Thinking\"和\"NoThinking\"模式之间切换。然后，我们在这个筛选出的困难数据上训练了一个专门的困难问题生成器（DiffGen-8B），它能够大规模生成新的困难问题，消除了对复杂的、针对每个实例的提示及其相关的高API成本的需求。\n\n在ScaleDiff-Math数据集上对Qwen2.5-Math-7B-Instruct进行微调，相比原始数据集带来了11.3%的显著性能提升，并在AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500上达到了65.9%的平均准确率，超越了像OpenThinker3这样的近期强大大型推理模型（LRMs）。值得注意的是，这一性能是使用成本效益高的Qwen3-8B模型作为教师模型实现的，这表明我们的流程能够有效传递高级推理能力，而无需依赖更大、更昂贵的教师模型。此外，我们观察到随着困难问题数量的增加，模型在困难基准测试上的性能呈现出明显的规模化现象。\n\n代码：https://github.com/QizhiPei/ScaleDiff。"
                },
                {
                    "title": "DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?",
                    "arxiv_id": "2509.21016",
                    "authors": "Yiyou Sun, Yuhan Cao, Pohao Huang, Haoyue Bai, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song",
                    "summary": "It remains an open question whether LLMs can acquire or generalize genuinely new reasoning strategies, beyond the sharpened skills encoded in their parameters during pre-training or post-training. To attempt to answer this debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding, a controlled benchmark of synthetic coding problem families designed to probe two fundamental aspects: learnability -- can LLMs, through reinforcement learning (RL), solve problem families where pretrained models exhibit failure with large enough attempts (pass@K=0)? --and transferrability -- if learnability happens, can such skills transfer systematically to out-of-distribution (OOD) test sets? Unlike prior public coding datasets, DELTA isolates reasoning skills through templated problem generators and introduces fully OOD problem families that demand novel strategies rather than tool invocation or memorized patterns. Our experiments reveal a striking grokking phase transition: after an extended period with near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To enable learnability on previously unsolvable problem families, we explore key training ingredients such as staged warm-up with dense rewards, experience replay, curriculum training, and verification-in-the-loop. Beyond learnability, we use DELTA to evaluate transferability or generalization along exploratory, compositional, and transformative axes, as well as cross-family transfer. Results show solid gains within families and for recomposed skills, but persistent weaknesses in transformative cases. DELTA thus offers a clean testbed for probing the limits of RL-driven reasoning and for understanding how models can move beyond existing priors to acquire new algorithmic skills.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了DELTA-Code基准测试，用于探索大语言模型(LLMs)是否能通过强化学习(RL)获取和迁移全新的推理策略。论文研究的是LLMs如何通过强化学习解决预训练时无法解决的问题，以及这些学到的技能如何迁移到分布外测试集。这完全符合研究\"大语言模型通用推理能力\"的目标，因为：(1)论文本质是改进LLM的基础推理能力，特别是算法推理能力，而不是将LLM作为工具应用到特定领域；(2)论文使用了强化学习这一训练范式来增强模型能力，探索了staged warm-up、experience replay、curriculum training等关键训练成分；(3)论文聚焦于推理能力的获取和迁移，这是通用推理能力的核心方面。论文符合多个正面指标，包括核心概念(LLMs)、能力方向(reasoning)和训练方法(RL)，同时不符合任何排除标准。因此，这篇论文应该被纳入研究范围。",
                    "summary2": "本文旨在研究RL如何帮助LLMs获取和泛化全新的编程算法策略。针对合成编码问题家族，我们提出了DELTA基准测试，通过分阶段训练(密集奖励到二元奖励)实现了grokking现象，并在Manufactoria和BouncingSim等数据集上通过pass@K和full-pass rate验证了RL可以使模型获得基础模型无法执行的新策略。",
                    "summary_translation": "LLMs（大型语言模型）是否能够获取或泛化真正新的推理策略，而不仅仅是在预训练或后训练期间编码在其参数中的已强化技能，这仍然是一个悬而未决的问题。为尝试回答这一争议，我们提出了DELTA-Code——算法编码中可学习性与可转移性的分布评估（Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding），这是一个受控的合成编码问题族基准，旨在探究两个基本方面：可学习性（learnability）——LLMs能否通过强化学习（RL）解决预训练模型在足够尝试次数下仍表现失败的问题族（pass@K=0）？——以及可转移性（transferrability）——如果实现了可学习性，这些技能能否系统地迁移到分布外（OOD）测试集？与先前的公共编码数据集不同，DELTA通过模板化问题生成器隔离推理技能，并引入了完全分布外（OOD）的问题族，这些问题族需要新颖策略而非工具调用或记忆模式。我们的实验揭示了一个显著的grokking（顿悟）相变现象：在经历了接近零奖励的长期阶段后，RL训练的模型突然攀升至接近完美的准确率。为了在先前无法解决的问题族上实现可学习性，我们探索了关键训练要素，如密集奖励的分阶段预热（staged warm-up）、经验回放（experience replay）、课程训练（curriculum training）和循环验证（verification-in-the-loop）。除了可学习性外，我们还使用DELTA来评估沿探索性（exploratory）、组合性（compositional）和变革性（transformative）轴线的可转移性或泛化能力，以及跨族转移（cross-family transfer）。结果显示在问题族内部和重组技能方面有显著收益，但在变革性案例中存在持续弱点。因此，DELTA为探究RL驱动推理的极限以及理解模型如何超越现有先验知识获取新算法技能提供了一个干净的测试平台。"
                },
                {
                    "title": "Mechanism of Task-oriented Information Removal in In-context Learning",
                    "arxiv_id": "2509.21012",
                    "authors": "Hakaze Cho, Haolin Yang, Gouki Minegishi, Naoya Inoue",
                    "summary": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.",
                    "category": "cs.CL",
                    "filter_reason": "我按照筛选标准对这篇论文进行了全面分析： 第一步：核心判断——这篇论文的本质是研究In-context Learning (ICL)的内在工作机制，特别是从\"信息移除\"角度解析ICL如何帮助语言模型专注于目标任务。论文不是将LLM作为工具应用到特定领域，而是深入研究LLM本身的基础工作机制，属于\"改进LLM的基础能力\"的研究方向，符合核心判断标准。 第二步：正面指标——论文符合\"核心概念\"指标，因为它明确研究现代语言模型(LMs)的ICL机制。虽然论文没有直接讨论reasoning、planning等能力方向，也没有涉及reinforcement learning等训练方法或llm-based agents等新兴范式，但ICL本身是LLM的一种重要通用能力，与推理能力密切相关。 第三步：排除标准——论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。 第四步：特殊和模糊情况——论文在可解释性方面有显著相关性，因为它揭示了ICL的\"面向任务的信息移除\"机制，这种对模型内在机制的深入理解有助于提升模型的通用可靠性和推理质量。 综合判断：这篇论文的核心贡献是揭示了ICL的工作机制，特别是\"面向任务的信息移除\"过程，这属于研究LLM基础能力的重要工作。理解ICL如何帮助模型从纠缠的信息中选择性移除冗余信息，对于提升LLM的通用推理能力具有重要意义。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解释大型语言模型中上下文学习(ICL)的机制。针对ICL推理过程，我们提出了一种任务导向信息移除机制，即示例通过移除查询中与任务无关的信息来驱动模型输出。在多个现代语言模型和分类数据集上，我们通过几何度量指标验证了信息移除的存在，并识别了实现该功能的去噪头。消融实验表明，在未见标签场景下，移除去噪头会导致准确率急剧下降至接近零，证明了该机制对ICL的重要性。",
                    "summary_translation": "# 中文翻译\n\n上下文学习(In-context Learning, ICL)是一种基于现代语言模型(Language Models, LMs)的新兴少样本学习(few-shot learning)范式，但其内部机制仍不清楚。在本文中，我们通过信息移除(information removal)的新视角来研究这一机制。具体而言，我们证明在零样本(zero-shot)场景中，语言模型将查询编码为隐藏状态(hidden states)中的非选择性表征(non-selective representations)，这些表征包含所有可能任务的信息，导致模型无法专注于预期任务而产生随意输出，从而使准确率接近零。同时，我们发现通过低秩滤波器(low-rank filter)从隐藏状态中选择性移除特定信息，可以有效引导语言模型朝向预期任务。\n\n基于这些发现，通过在精心设计的度量指标上测量隐藏状态，我们观察到少样本(few-shot)ICL有效模拟了这种面向任务的信息移除(task-oriented information removal)过程，从纠缠的非选择性表征(entangled non-selective representations)中选择性移除冗余信息，并基于示例(demonstrations)改进输出，这构成了ICL的一个关键机制。此外，我们识别出引发移除操作的关键注意力头(attention heads)，称之为去噪头(Denoising Heads)，这使得我们能够进行消融实验(ablation experiments)，在推理过程中阻断信息移除操作，此时ICL的准确率显著下降，特别是当少样本示例中缺少正确标签时，这证实了信息移除机制和去噪头的关键作用。"
                },
                {
                    "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems",
                    "arxiv_id": "2509.21054",
                    "authors": "Haodong Zhao, Jidong Li, Zhaomin Wu, Tianjie Ju, Zhuosheng Zhang, Bingsheng He, Gongshen Liu",
                    "summary": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to solve complex problems, necessitates a deep understanding of the persuasion dynamics that govern their interactions. This paper challenges the prevailing hypothesis that persuasive efficacy is primarily a function of model scale. We propose instead that these dynamics are fundamentally dictated by a model's underlying cognitive process, especially its capacity for explicit reasoning. Through a series of multi-agent persuasion experiments, we uncover a fundamental trade-off we term the Persuasion Duality. Our findings reveal that the reasoning process in LRMs exhibits significantly greater resistance to persuasion, maintaining their initial beliefs more robustly. Conversely, making this reasoning process transparent by sharing the \"thinking content\" dramatically increases their ability to persuade others. We further consider more complex transmission persuasion situations and reveal complex dynamics of influence propagation and decay within multi-hop persuasion between multiple agent networks. This research provides systematic evidence linking a model's internal processing architecture to its external persuasive behavior, offering a novel explanation for the susceptibility of advanced models and highlighting critical implications for the safety, robustness, and design of future MAS.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是研究大型语言模型(LLMs)和大型推理模型(LRMs)的推理过程如何影响其在多智能体系统中的说服动态。论文提出了\"说服二元性\"(Persuasion Duality)的概念，揭示了模型的底层认知过程，特别是其显式推理能力，如何决定其在多智能体交互中的说服行为。 根据筛选标准，这篇论文符合研究目标，原因如下： 1. 核心判断：论文的本质是研究LLM的基础能力——推理能力，而不是将LLM作为工具应用到特定领域。论文探索的是模型的底层认知过程和推理架构如何影响其外部行为，这属于对LLM通用能力的深入研究。 2. 正面指标：论文包含多个关键正面指标： - 核心概念：明确研究Large Language Models (LLMs)和Large Reasoning Models (LRMs) - 能力方向：核心关注reasoning能力，特别是模型的\"thinking process\"和\"explicit reasoning\" - 新兴范式：研究Multi-Agent Systems (MAS)中LLMs的交互和协作 3. 排除标准：论文不主要聚焦于任何排除领域。虽然研究\"说服\"这一社会心理学概念，但论文不是将LLM应用到社会学领域，而是研究LLM本身的推理能力如何影响其在多智能体系统中的行为。 4. 特殊情况处理：论文研究的是通用多智能体系统框架中的推理问题，属于\"提出一种通用的智能体协作框架来增强LLM的通用问题解决能力\"的情况，应该保留。 因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，它通过研究模型推理过程与说服动态的关系，增进了我们对LLM推理能力的理解，有助于未来设计具有更强推理能力的LLM和多智能体系统。",
                    "summary2": "本文旨在探究多智能体系统中模型推理过程如何影响说服动态。针对大型语言模型(LLMs)和大型推理模型(LRMs)之间的互动场景，我们提出了一种基于认知过程的说服机制分析方法，并在MMLU和PersuasionBench数据集上通过说服率(Persuaded-Rate)、保持率(Remain-Rate)等指标验证了推理过程对说服效果的关键影响。",
                    "summary_translation": "近期多智能体系统（Multi-Agent Systems, MAS）的快速普及，其中大型语言模型（Large Language Models, LLMs）和大型推理模型（Large Reasoning Models, LRMs）通常协作解决复杂问题，这要求我们深入理解主导其互动的说服动态（persuasion dynamics）。本文挑战了主流假设，即说服效果（persuasive efficacy）主要取决于模型规模（model scale）。相反，我们提出这些动态实际上由模型的底层认知过程（cognitive process）所决定，尤其是其显式推理（explicit reasoning）能力。通过一系列多智能体说服实验，我们发现了一个我们称之为\"说服二元性\"（Persuasion Duality）的基本权衡（trade-off）。我们的研究结果表明，LRMs中的推理过程对说服表现出显著更强的抵抗力（resistance to persuasion），能够更稳健地维持其初始信念。相反，通过分享\"思考内容\"（thinking content）使这一推理过程透明化，则显著提高了它们说服他人的能力。我们进一步考虑了更复杂的传播说服情境（transmission persuasion situations），并揭示了多智能体网络间多跳说服（multi-hop persuasion）中影响传播（influence propagation）和衰减（decay）的复杂动态。本研究提供了将模型内部处理架构（internal processing architecture）与其外部说服行为（external persuasive behavior）联系起来的系统性证据，为高级模型的易受性（susceptibility）提供了新颖的解释，并强调了未来MAS的安全性（safety）、稳健性（robustness）和设计的关键影响。"
                },
                {
                    "title": "On Theoretical Interpretations of Concept-Based In-Context Learning",
                    "arxiv_id": "2509.20882",
                    "authors": "Huaze Tang, Tianren Peng, Shao-lun Huang",
                    "summary": "In-Context Learning (ICL) has emerged as an important new paradigm in natural language processing and large language model (LLM) applications. However, the theoretical understanding of the ICL mechanism remains limited. This paper aims to investigate this issue by studying a particular ICL approach, called concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs well for predicting query labels in prompts with only a few demonstrations. In addition, the proposed theory quantifies the knowledge that can be leveraged by the LLMs to the prompt tasks, and leads to a similarity measure between the prompt demonstrations and the query input, which provides important insights and guidance for model pre-training and prompt engineering in ICL. Moreover, the impact of the prompt demonstration size and the dimension of the LLM embeddings in ICL are also explored based on the proposed theory. Finally, several real-data experiments are conducted to validate the practical usefulness of CB-ICL and the corresponding theory.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是对大语言模型的上下文学习(ICL)能力进行理论解释，特别是研究基于概念的上下文学习(CB-ICL)方法。上下文学习是大语言模型的一种重要通用推理能力，它允许模型从少量示例中学习并应用到新问题上，这本质上是一种推理和问题解决过程。论文提出了理论分析，解释了CB-ICL在少量示例情况下表现良好的原因，量化了LLMs可以利用的知识，并提出了相似性度量方法，这些都有助于深入理解LLM的推理机制。虽然论文没有涉及强化学习、智能体协作等新兴范式，但它聚焦于LLM的基础能力研究，特别是对通用推理能力的理论解释，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文不涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决上下文学习(ICL)机制的理论理解问题。针对仅有少量示例的prompt场景，我们提出了一种基于概念的ICL(CB-ICL)方法，通过提取语义概念向量来预测查询标签，并在MMLU、GPQA等四个benchmark上通过accuracy验证了其有效性。",
                    "summary_translation": "In-Context Learning (ICL，情境学习)已成为自然语言处理(natural language processing)和大型语言模型(large language model, LLM)应用中的一个重要新范式。然而，对ICL机制的理论理解仍然有限。本文旨在通过研究一种特定的ICL方法，即基于概念的ICL(concept-based ICL, CB-ICL)来探讨这一问题。具体而言，我们提出了将CB-ICL应用于ICL任务的理论分析，解释了为何以及何时CB-ICL能够在仅有少量示例(demonstrations)的提示(prompts)中有效预测查询标签(query labels)。此外，所提出的理论量化了LLMs可利用于提示任务的知识，并提出了提示示例(prompt demonstrations)与查询输入(query input)之间的相似性度量(similarity measure)，这为ICL中的模型预训练(pre-training)和提示工程(prompt engineering)提供了重要见解和指导。此外，基于所提出的理论，我们还探讨了提示示例规模(prompt demonstration size)和LLM嵌入维度(dimension of LLM embeddings)对ICL的影响。最后，我们进行了多项真实数据实验(real-data experiments)以验证CB-ICL及其相应理论的实用性。"
                },
                {
                    "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning",
                    "arxiv_id": "2509.20712",
                    "authors": "Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou",
                    "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose \\textbf{C}ontrolling \\textbf{E}ntropy via \\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种新的强化学习算法CE-GPPO，用于改进大语言模型的基础推理能力。论文明确指出其目标是优化LLM处理\"复杂推理任务\"的能力，并通过改进PPO算法的梯度处理机制来提升模型性能，这直接属于改进LLM基础能力和通用推理能力的范畴。 其次，从正面指标看，论文包含了多个相关主题：核心概念上明确研究\"large language models (LLMs)\"；能力方向上聚焦于\"complex reasoning tasks\"和\"mathematical reasoning\"；训练方法上提出了新的强化学习优化算法(CE-GPPO)。这些都是高度相关的指标。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉内容，没有将LLM应用于特定领域（数学推理仅作为评估通用推理能力的基准），也没有涉及模型基础设施或应用层面的可靠性问题。 论文的核心贡献是提出了一种新的强化学习训练范式，通过改进梯度处理机制来优化LLM的推理能力，这直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决强化学习优化大型语言模型时策略熵不稳定的问题。针对数学推理任务，我们提出了一种CE-GPPO算法，通过保留被裁剪tokens的梯度来控制熵动态，并在多个数学推理基准（AIME24、AIME25、HMMT25、MATH500、AMC23）上通过avg@32/avg@4指标验证了其有效性。",
                    "summary_translation": "强化学习 (Reinforcement learning, RL) 已成为一种强大的范式，用于优化大语言模型 (large language models, LLMs) 以处理复杂的推理任务。这一过程中的核心挑战在于管理策略熵 (policy entropy)，它反映了训练过程中探索 (exploration) 与利用 (exploitation) 之间的平衡。现有方法，如近端策略优化 (proximal policy optimization, PPO) 及其变体，由于截断机制 (clipping mechanism) 而丢弃了来自低概率token (low-probability tokens) 的宝贵梯度信号 (gradient signals)。我们系统地分析了熵动态 (entropy dynamics)，并揭示这些被截断的token (clipped tokens) 在调节熵演化 (entropy evolution) 中扮演着关键但被忽视的角色。我们提出了通过梯度保留策略优化控制熵 (Controlling Entropy via Gradient-Preserving Policy Optimization, CE-GPPO)，这是一种新颖的算法，它以一种温和且有界的方式重新引入了原始PPO中被截断token的梯度 (gradients)。通过控制来自截断区间 (clipping interval) 外token的梯度大小，CE-GPPO能够实现探索-利用权衡 (exploration-exploitation trade-off)。我们提供了理论依据和实证证据，表明CE-GPPO有效缓解了熵不稳定性 (entropy instability)。在数学推理基准测试 (mathematical reasoning benchmarks) 上的大量实验表明，CE-GPPO在不同模型规模 (model scales) 上始终优于强基线方法。"
                },
                {
                    "title": "StyleBench: Evaluating thinking styles in Large Language Models",
                    "arxiv_id": "2509.20868",
                    "authors": "Junyu Guo, Shangding Gu, Ming Jin, Costas Spanos, Javad Lavaei",
                    "summary": "The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了StyleBench基准测试，用于系统评估不同推理风格（如CoT、ToT、AoT等）对大语言模型性能的影响。论文直接关注LLM的通用推理能力，研究不同思考风格如何影响模型在各种推理任务上的表现。这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文没有将LLM作为工具应用到特定领域，也没有关注模型基础设施或部署优化。相反，它通过大规模分析揭示了推理风格、模型规模和任务类型之间的复杂关系，为选择最优推理策略提供了指导，这对提升LLM的基础推理能力具有重要价值。论文涉及的核心概念（LLMs）和能力方向（reasoning）进一步确认了它与我的研究目标高度相关。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决大型语言模型中推理风格选择问题。针对不同推理风格与模型规模、任务类型的相互作用，我们提出了StyleBench基准测试，用于系统评估五种推理风格（CoT、ToT、AoT、SoT和CoD），并在五个任务上使用15个开源模型（270M到120B参数）通过准确率、效率和延迟等指标验证其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）的效果在很大程度上受到其提示（prompt）中采用的推理策略或思维风格的影响。然而，这些推理风格、模型架构和任务类型之间的相互作用仍未得到充分理解。为解决这一问题，我们引入了StyleBench，这是一个用于在不同任务和模型上系统评估推理风格的全面基准测试（benchmark）。我们评估了五种代表性的推理风格，包括思维链（Chain of Thought, CoT）、思维树（Tree of Thought, ToT）、思维算法（Algorithm of Thought, AoT）、思维草图（Sketch of Thought, SoT）和草稿链（Chain-of-Draft, CoD），在五个推理任务上使用了来自主要系列（LLaMA、Qwen、Mistral、Gemma、GPT-OSS、Phi和DeepSeek）的15个开源模型，参数规模从270M到120B不等。我们的大规模分析表明，没有一种风格是普遍最优的。我们证明策略的有效性高度依赖于模型规模和任务类型：基于搜索的方法（AoT, ToT）在开放式问题上表现出色，但需要大规模模型，而简洁风格（SoT, CoD）在明确定义的任务上实现了显著的效率提升。此外，我们识别出关键的行为模式：较小的模型经常无法遵循输出指令并默认为猜测，而推理鲁棒性（robustness）则作为规模的函数而出现。我们的发现为基于特定约束选择最优推理策略提供了关键路线图，我们在https://github.com/JamesJunyuGuo/Style_Bench上开源了该基准测试。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 4,
            "papers": [
                {
                    "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say",
                    "arxiv_id": "2509.21164",
                    "authors": "Jacob Fein-Ashley, Dhruv Parikh, Rajgopal Kannan, Viktor Prasanna",
                    "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain (e.g., math, code, general reasoning), motivating systems that leverage complementary strengths across models. Prior multi-LLM approaches either (i) route a query to one or a few experts and generate independently, (ii) aggregate outputs from each model via costly multi-turn exchanges, or (iii) fuse weights into a single model-typically requiring architectural homogeneity. We introduce Mixture of Thoughts (MoT), a simple method for latent-level collaboration among heterogeneous experts under a global routing scheme. For each query, a lightweight router selects top-$K$ experts and designates a primary expert; uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active (selected) peers. Pre-trained experts remain frozen; only the router and the lightweight interaction layers are trained with a novel joint training objective that improves both the expert selection and inter-expert collaboration. Across five in-distribution (ID) and three out-of-distribution (OOD) benchmarks, MoT surpasses the current routing and aggregation-based state-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further, MoT significantly outperforms the best-performing single model. It achieves this with single-pass inference, runtime comparable to routing baselines, and none of the overheads of iterative aggregation. MoT offers a simple latent-space mechanism for combining heterogeneous LLMs, a practical step toward broader multi-LLM collaboration. Our code is publicly available at https://github.com/jacobfa/mot.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Mixture of Thoughts (MoT)\"的新方法，用于在不同专业领域（如数学、代码、通用推理）的开源大语言模型之间进行潜在层面的协作。该方法通过一个轻量级路由器选择顶级专家，并在共享潜在空间中通过交叉注意力机制进行协作，从而提升整体推理性能。这符合研究目标，因为：(1) 论文的核心是改进LLM的基础能力，特别是通用推理能力，而不是将LLM作为工具应用到特定领域；(2) 论文涉及多个正面指标，包括大语言模型、推理能力（数学和通用推理）以及多智能体系统；(3) 论文提出了一种通用的多LLM协作框架，用于增强LLM的通用问题解决能力，而非针对特定应用领域；(4) 实验结果表明，MoT在多个基准测试上超过了当前最先进的方法，有效提升了LLM的推理能力。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决如何有效整合多个异构大型语言模型的问题。针对多个专业化的开源LLMs，我们提出了一种Mixture of Thoughts (MoT)方法，通过潜在空间层面的专家协作而非仅聚合输出，并在五个分布内和三个分布外基准测试上通过准确率指标验证了其有效性，超越了当前最先进方法。",
                    "summary_translation": "开源大型语言模型（Large Language Models, LLMs）正日益按领域（如数学、代码、通用推理）专业化，这促使了利用不同模型间互补优势的系统的发展。先前的多LLM方法要么（i）将查询路由（route）到一个或几个专家模型并独立生成结果，（ii）通过昂贵的多轮交换聚合（aggregate）每个模型的输出，或者（iii）将权重融合（fuse）到单个模型中——通常需要架构同质性（architectural homogeneity）。我们提出了思维混合（Mixture of Thoughts, MoT），一种在全球路由（global routing）方案下实现异构专家（heterogeneous experts）之间潜在层面（latent-level）协作的简单方法。对于每个查询，一个轻量级路由器（lightweight router）选择前K个专家并指定一个主要专家；均匀放置的交互层（interaction layers）将隐藏状态（hidden states）投影到共享潜在空间（shared latent space），其中主要专家对其活跃（被选中的）同行执行交叉注意力（cross-attention）操作。预训练的专家模型保持冻结（frozen）状态；只有路由器和轻量级交互层通过一种新颖的联合训练目标（joint training objective）进行训练，该目标同时改进专家选择和专家间协作。在五个分布内（in-distribution, ID）和三个分布外（out-of-distribution, OOD）基准测试中，MoT分别以$+0.38\\%$和$+2.92\\%$的优势超越了当前基于路由和聚合的最先进方法（state-of-the-art）Avengers。此外，MoT显著优于表现最佳的单个模型。它通过单次推理（single-pass inference）实现这一目标，运行时间与路由基线（routing baselines）相当，且没有迭代聚合（iterative aggregation）的开销。MoT提供了一种简单的潜在空间机制来组合异构LLMs，这是朝着更广泛的多LLM协作迈出的实用一步。我们的代码在https://github.com/jacobfa/mot公开可用。"
                },
                {
                    "title": "Theoretical Bounds for Stable In-Context Learning",
                    "arxiv_id": "2509.20677",
                    "authors": "Tongxi Wang, Zhuoyang Xia",
                    "summary": "In-context learning (ICL) is flexible but its reliability is highly sensitive to prompt length. This paper establishes a non-asymptotic lower bound that links the minimal number of demonstrations to ICL stability under fixed high-dimensional sub-Gaussian representations. The bound gives explicit sufficient conditions in terms of spectral properties of the covariance, providing a computable criterion for practice. Building on this analysis, we propose a two-stage observable estimator with a one-shot calibration that produces practitioner-ready prompt-length estimates without distributional priors. Experiments across diverse datasets, encoders, and generators show close alignment between the predicted thresholds and empirical knee-points, with the theory acting as a conservative but reliable upper bound; the calibrated variant further tightens this gap. These results connect spectral coverage to stable ICL, bridge theory and deployment, and improve the interpretability and reliability of large-scale prompting in realistic finite-sample regimes.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是研究大语言模型的上下文学习(ICL)能力的稳定性问题，建立了理论界限并提出了估计提示长度的方法。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，这篇论文研究的是LLM的一个基础能力——上下文学习(ICL)的稳定性，而非将LLM作为工具应用到特定领域。ICL是LLM的核心能力之一，论文通过建立非渐近下界和提出两阶段可观测估计器，旨在提高这一基础能力的可靠性和稳定性，这属于改进LLM基础能力的研究范畴。 其次，论文涉及LLM的核心概念(ICL)，虽然未直接提及reasoning、planning等能力方向，但ICL本身与这些通用能力密切相关，因为它是模型适应新任务并进行推理的基础机制。 第三，论文不涉及任何需要排除的领域：没有关注多模态与视觉问题，没有将LLM应用到医疗、化学等特定领域，也没有从应用层面研究模型可靠性问题（如水印、安全等）。 最后，虽然论文关注了ICL的可靠性，但这是从理论角度研究LLM基础能力的稳定性，而非应用层面的可靠性问题，因此不应被排除。 综上所述，这篇论文致力于提高LLM本身的通用推理能力（通过增强ICL稳定性），符合研究目标。",
                    "summary2": "",
                    "summary_translation": "In-context learning (ICL, 上下文学习) 具有灵活性，但其可靠性对提示长度高度敏感。本文建立了一个非渐近下界（non-asymptotic lower bound），该下界在固定的高维次高斯表示（sub-Gaussian representations）下，将最少的示例数量（demonstrations）与ICL稳定性联系起来。该下界根据协方差（covariance）的光谱性质（spectral properties）给出了明确的充分条件，为实践提供了可计算的标准。基于此分析，我们提出了一个带有一次性校准（one-shot calibration）的两阶段可观测估计器（two-stage observable estimator），该估计器无需分布先验（distributional priors）即可生成可供实践者使用的提示长度估计。在不同数据集、编码器（encoders）和生成器（generators）上的实验表明，预测阈值与经验拐点（empirical knee-points）之间存在密切的一致性，该理论作为一个保守但可靠的上界；校准变体进一步缩小了这一差距。这些结果将光谱覆盖（spectral coverage）与稳定的ICL联系起来，弥合了理论与部署之间的差距，并提高了在现实有限样本条件（finite-sample regimes）下大规模提示的可解释性和可靠性。"
                },
                {
                    "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning",
                    "arxiv_id": "2509.20616",
                    "authors": "Hanjiang Hu, Changliu Liu, Na Li, Yebin Wang",
                    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in higher multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks with over 30 steps. We also theoretically and empirically validate the strong cross-task generalizability that the models trained on complex tasks can lead to the successful completion of all simpler subtasks.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合研究目标。从核心判断来看，论文的本质是关于改进LLM的基础能力，特别是提出了一种新的训练范式（单轮强化学习）来增强LLM的任务规划和多步推理能力。论文的核心贡献是将复杂的多轮任务规划转化为单轮任务推理问题，并通过Group Relative Policy Optimization (GRPO)进行高效策略优化，这直接提升了LLM的通用推理能力。 从正面指标看，论文明确包含了多个关键主题：Large Language Models (LLMs)、reasoning、task planning、reinforcement learning以及LLM agents，这些都与研究目标高度一致。 从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容，因此不应被排除。 在特殊和模糊情况处理方面，论文提出的是一种通用的智能体框架来增强LLM的任务规划能力，而不是将智能体应用于特定领域，因此符合保留标准。 综上所述，这篇论文直接致力于提高大语言模型本身的通用推理能力，特别是在任务规划和多步推理方面，与研究目标完全一致。",
                    "summary2": "本文旨在解决LLM智能体在复杂多轮任务规划中面临的稀疏奖励、信用分配和计算开销问题。针对多轮任务规划场景，我们提出了一种将多轮任务规划转化为单轮任务推理问题，并通过GRPO进行策略优化的方法，并在Robotouille benchmark上通过成功率(SR)、平均步数(ASAT/ASST)验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在知识获取、推理和工具使用方面已展现出卓越能力，使其成为自主代理（autonomous agent）应用的有力候选者。然而，针对复杂多轮任务规划（multi-turn task planning）训练LLM代理面临重大挑战，包括稀疏的逐级奖励（episode-wise rewards）、长跨度信用分配（credit assignment across long horizons），以及多轮交互设置中强化学习（reinforcement learning）的计算开销。为此，本文提出了一种新方法，将多轮任务规划转化为单轮任务推理（single-turn task reasoning）问题，通过群体相对策略优化（Group Relative Policy Optimization, GRPO）实现高效策略优化，该方法利用来自专家轨迹的密集且可验证的奖励（dense and verifiable reward）。我们的理论分析表明，GRPO对单轮任务推理的改进能够在最小轮次下实现更高的多轮成功概率，以及对较短跨度子任务（subtasks with shorter horizons）的泛化能力。在复杂任务规划基准测试（complex task planning benchmark）上的实验评估表明，我们使用单轮GRPO训练的15亿参数模型相比高达140亿参数的更大基线模型（baseline models）取得了更优性能，在超过30步的长跨度规划任务（long-horizon planning tasks）中成功率达到70%。我们还从理论和实证上验证了强大的跨任务泛化能力（cross-task generalizability），即在复杂任务上训练的模型能够成功完成所有更简单的子任务。"
                },
                {
                    "title": "RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training",
                    "arxiv_id": "2509.21009",
                    "authors": "Wei Gao, Yuheng Zhao, Dakai An, Tianyuan Wu, Lunxi Cao, Shaopan Xiong, Ju Huang, Weixun Wang, Siran Yang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang",
                    "summary": "Reinforcement Learning (RL) is a pivotal post-training technique for enhancing the reasoning capabilities of Large Language Models (LLMs). However, synchronous RL post-training often suffers from significant GPU underutilization, referred to as bubbles, caused by imbalanced response lengths within rollout steps. Many RL systems attempt to alleviate this problem by relaxing synchronization, but this can compromise training accuracy. In this paper, we introduce tail batching, a novel rollout scheduling strategy for synchronous RL that systematically consolidates prompts leading to long-tail responses into a small subset of rollout steps (long rounds), while ensuring that the majority of steps (short rounds) involve only balanced, short rollouts. By excluding long responses from short rounds and rescheduling them into a few designated long rounds, tail batching effectively reduces GPU idle time during rollouts and significantly accelerates RL training without sacrificing accuracy. We present RollPacker, a system that fully harnesses the benefits of tail batching through holistic optimizations across all three RL stages: elastic parallelism adaptation for rollout, dynamic resource allocation and scheduling for reward, and stream-based training. Empirical results show that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5 family of LLMs on up to 128 H800 GPUs.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"tail batching\"的新型rollout调度策略和RollPacker系统，用于优化强化学习(RL)作为大语言模型(LLM)后训练技术的效率和性能。论文明确指出RL是\"enhancing the reasoning capabilities of Large Language Models (LLMs)\"的关键技术，这与研究目标\"提高大语言模型（LLM）本身的『通用推理能力』\"直接相关。论文不是将LLM作为工具应用到特定领域，而是专注于改进LLM的基础训练方法，特别是强化学习这一提升LLM推理能力的关键技术。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。因此，这篇论文完全符合研究范围。",
                    "summary2": "本文旨在解决同步强化学习后训练中由于响应长度不平衡导致的GPU利用率不足问题。针对大型语言模型的RL后训练场景，我们提出了一种tail batching调度策略和RollPacker系统，通过将长尾响应整合到专门的rollout步骤中，并在rollout、reward和训练三个阶段进行系统优化。在Qwen2.5系列模型和多达128个H800 GPU的实验环境中，通过端到端训练时间指标验证了其有效性，实现了最高2.56倍的训练加速。",
                    "summary_translation": "强化学习（Reinforcement Learning, RL）是一种关键的后续训练技术，用于增强大语言模型（Large Language Models, LLMs）的推理能力。然而，同步RL后续训练常常遭受严重的GPU利用率不足问题，这种现象被称为\"气泡\"（bubbles），是由rollout（展开）步骤中不平衡的响应长度所导致的。许多RL系统试图通过放松同步来缓解这一问题，但这可能会损害训练准确性。在本文中，我们提出了tail batching（尾部批处理），一种新颖的同步RL rollout调度策略，该策略系统性地将导致长尾响应的提示整合到一小部分rollout步骤（长轮次）中，同时确保大多数步骤（短轮次）仅涉及平衡的、简短的rollout。通过将长响应从短轮次中排除并重新调度到少数指定的长轮次中，tail batching有效减少了rollout过程中的GPU空闲时间，并在不牺牲准确性的前提下显著加速了RL训练。我们提出了RollPacker系统，该系统通过在所有三个RL阶段的全面优化来充分利用tail batching的优势：rollout的弹性并行适应、奖励的动态资源分配与调度，以及基于流的训练。实证结果表明，对于Qwen2.5系列的LLMs，在多达128个H800 GPU上，RollPacker相比veRL实现了2.03倍至2.56倍的端到端训练时间减少，相比RLHFuse实现了高达2.24倍的加速。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective",
                    "arxiv_id": "2509.21134",
                    "authors": "Yiwen Zhang, Ziang Chen, Fanqi Kong, Yizhe Huang, Xue Feng",
                    "summary": "Large Language Models (LLMs) have been used to make decisions in complex scenarios, where they need models to think deeply, reason logically, and decide wisely. Many existing studies focus solely on multi-round conversations in social tasks or simulated environments, neglecting the various types of decisions and their interdependence. Current reinforcement learning methods struggle to consider the strategies of others during training. To address these issues, we first define a strategic decision-making problem that includes two types of decisions and their temporal dependencies. Furthermore, we propose **T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to optimize the perception of other individual strategies and the game situation trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm, ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating rollouts based on reasoning the strategies of other individuals, 2) estimating advantages at both the graph-level and sample-level, and 3) balancing global and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in terms of model output compliance and cooperative outcomes. Additionally, when compared to models with parameter sizes 100 times larger, it shows an 18% improvement. This demonstrates the effectiveness of the ToMPO algorithm in enhancing the model's strategic decision-making capabilities.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合我的研究目标，因为它专注于提升大语言模型本身的通用推理能力。从核心判断来看，论文的本质是提出ToMPO算法来增强LLM的战略决策能力，这是一种新的训练范式，属于改进LLM基础能力的研究。论文明确关注LLM在复杂场景中的推理和决策能力，需要\"深入思考、逻辑推理和明智决策\"，这正是通用推理能力的核心要素。 从正面指标看，论文包含了多个相关主题：核心概念是LLMs；能力方向涉及reasoning和strategic decision-making；训练方法采用了reinforcement learning（ToMPO算法）；新兴范式方面则从multi-agent perspective研究问题。 论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文从多智能体角度研究，但它是提出一种通用的框架来增强LLM的战略决策能力，而不是将智能体应用在特定领域。 论文的核心贡献是ToMPO算法，通过基于推理其他个体策略生成rollouts、在图级和样本级估计优势、平衡全局和部分奖励来增强LLM的战略决策能力，这直接提升了LLM的通用推理和决策能力，完全符合我的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在多智能体环境中战略决策能力不足的问题。针对复杂社会场景中的图级别和努力级别决策，我们提出了Theory of Mind Policy Optimization (ToMPO)算法，并在BCZ和PGG游戏环境中通过U1(合规性)、U2(战略效率)和U3(合作结果)指标验证了其有效性。ToMPO通过推理其他智能体策略生成rollouts、在图级别和样本级别估计优势并平衡全局与局部奖励，使Qwen-2.5-7B-instruct模型在合规性和合作结果方面比GRPO算法提高35%，比参数量大100倍的模型提升18%。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）已被用于在复杂场景中做出决策，这些场景需要模型进行深度思考、逻辑推理和明智决策。许多现有研究仅关注社交任务或模拟环境中的多轮对话，忽略了各种类型的决策及其相互依赖性。当前的强化学习方法（reinforcement learning methods）在训练过程中难以考虑他人的策略。为解决这些问题，我们首先定义了一个包含两种决策类型及其时间依赖性（temporal dependencies）的战略决策问题。此外，我们提出了**心智理论策略优化（Theory of Mind Policy Optimization, ToMPO）**算法，以优化对其他个体策略和游戏情境趋势的感知。\n\n与群体相对策略优化（Group Relative Policy Optimization, GRPO）算法相比，ToMPO主要通过以下方式增强LLM的战略决策能力：1）基于推理其他个体策略生成推演（rollouts），2）在图级（graph-level）和样本级（sample-level）估计优势（advantages），以及3）平衡全局和部分奖励。在模型输出合规性和合作结果方面，ToMPO算法比GRPO方法高出35%。此外，与参数规模大100倍的模型相比，它显示出18%的改进。这证明了ToMPO算法在增强模型战略决策能力方面的有效性。"
                },
            ]
        },
    ],
    "2025-09-24": [
        {
            "name": "Artificial Intelligence",
            "count": 5,
            "papers": [
                {
                    "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs",
                    "arxiv_id": "2509.20105",
                    "authors": "Venkat Margapuri, Garik Kazanjian, Naren Kosaraju",
                    "summary": "Large Language Models (LLMs) often struggle with maintaining coherent multi-step reasoning traces, particularly in tasks that require a structured logical flow. This work introduces a quantum-inspired approach to address the challenge by incorporating a fidelity-based reward derived from Projected Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior approaches that use direct supervision or contrastive objectives, the proposed method guides learning through structural consistency, offering a novel approach to enforce global coherence in generated reasoning traces. The proposed framework is evaluated using multiple coherence-determining metrics on diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning arithmetic, intuitive, and entailment-based reasoning. Results show that the proposed quantum-inspired approach offers significant improvements over supervised, contrastive, and pretrained baseline approaches, highlighting the effectiveness of quantum-inspired fidelity as a foundation to improve reasoning trace coherence in LLMs.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础推理能力，提出了一种量子启发的强化学习方法来增强LLM在连贯多步推理方面的表现。这直接对应了\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的核心标准。 从正面指标分析，论文明确包含以下关键要素： - 核心概念：直接关注\"Large Language Models (LLMs)\" - 能力方向：专注于\"coherent multi-step reasoning traces\"和\"structured logical flow\"，属于推理能力范畴 - 训练方法：采用强化学习方法（Proximal Policy Optimization），结合量子物理中的Projected Entangled Pair States (PEPS)概念 论文不涉及任何排除标准中的领域。它不是关于多模态与视觉研究，不是将LLM应用到特定领域，也不是关于模型可靠性在应用层面的研究。虽然论文在GSM8K、StrategyQA和EntailmentBank等数据集上进行了评估，但这些是评估通用推理能力的标准数据集，而非特定领域应用。 论文的核心贡献是提出了一种基于量子物理概念的强化学习方法，通过保真度奖励机制来提高LLM生成连贯推理痕迹的能力，这是一种从根本上提升模型推理能力的方法论创新，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型(LLMs)在保持连贯多步推理追踪方面的困难。针对需要结构化逻辑流程的任务，我们提出了一种量子启发的强化学习方法，使用Projected Entangled Pair States (PEPS)导出基于保真度的奖励并集成到Proximal Policy Optimization (PPO)中，并在GSM8K、StrategyQA和EntailmentBank数据集上通过MEC、WES、BERT和BLEURT等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在保持连贯的多步推理轨迹方面常常面临困难，特别是在需要结构化逻辑流程的任务中。本研究引入了一种量子启发（quantum-inspired）的方法，通过将基于投影纠缠对态（Projected Entangled Pair States, PEPS）的保真度奖励（fidelity-based reward）纳入近端策略优化（Proximal Policy Optimization）来应对这一挑战。与先前使用直接监督或对比目标的方法不同，所提出的方法通过结构一致性指导学习，为在生成的推理轨迹中强制执行全局连贯性提供了一种新途径。该框架在多个数据集上使用多种连贯性确定指标进行了评估，这些数据集包括GSM8K、StrategyQA和EntailmentBank，涵盖了算术、直观和基于蕴含（entailment-based）的推理类型。结果表明，所提出的量子启发方法相比监督、对比和预训练的基线方法有显著改进，凸显了量子启发的保真度作为提高大型语言模型中推理轨迹连贯性基础的有效性。"
                },
                {
                    "title": "The Conductor and the Engine: A Path Towards Co-Designed Reasoning",
                    "arxiv_id": "2509.19762",
                    "authors": "Yuanxin Wang, Pawel Filipczuk, Anisha Garg, Amaan Dhada, Mohammad Hassanpour, David Bick, Ganesh Venkatesh",
                    "summary": "Modern LLM reasoning relies on extensive test-time computation, driven by internal model training and external agentic orchestration. However, this synergy is often inefficient, as model verbosity and poor instruction following lead to wasted compute. We analyze this capability-cost trade-off and introduce an optimized reasoning workflow (\\cepo) that empowers smaller open-source models to outperform models multiple times their size. We will open-source this workflow to enable further research. Our work demonstrates a clear path toward co-designing orchestration frameworks with the underlying model capabilities to unlock powerful reasoning in small-to-medium sized models.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，该论文的本质是关于改进LLM的推理能力，提出了一种优化的推理工作流程(\\cepo)，通过协调内部模型训练和外部智能体编排来提高推理效率，使较小的开源模型能够超越比它们大得多的模型。这直接符合\"改进LLM的基础能力、增强其逻辑、多步推理等通用能力\"的标准。 其次，从正面指标看，论文明确涉及\"LLM reasoning\"这一核心概念和\"reasoning\"这一能力方向，同时提到了\"external agentic orchestration\"，与智能体(llm-based agents)这一新兴范式相关。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，在特殊情况下，论文提出的智能体编排框架是通用性的，旨在增强LLM的通用推理能力，而非应用于特定领域，因此应该保留。 综上所述，该论文的核心贡献是提出了一种协同设计编排框架与底层模型能力的方法，以释放中小型模型的强大推理能力，这与研究目标高度一致。",
                    "summary2": "本文旨在 [解决现代LLM推理中因模型冗长和指令遵循不佳导致的计算浪费问题]。针对 [中小型开源模型的推理效率与性能问题]，我们提出了一种 [CODA（Conductor-driven Architecture）优化推理工作流，包含自适应规划、执行、自我反思和验证等关键组件]，并在 [AIME、GPQA、LiveCodeBench等数学和编码基准测试] 上通过 [准确率、Pass@k等指标] 验证了其有效性。",
                    "summary_translation": "现代大语言模型(LLM, Large Language Model)推理依赖于广泛的测试时计算(test-time computation)，这种计算由内部模型训练和外部智能体编排(agentic orchestration)共同驱动。然而，这种协同作用(synergy)往往效率低下，因为模型的冗长性(verbosity)和不良的指令遵循(instruction following)能力导致计算资源浪费(wasted compute)。我们分析了这种能力-成本权衡(capability-cost trade-off)，并提出了一种优化的推理工作流(optimized reasoning workflow) \\cepo，它使较小的开源模型能够超越规模大得多的模型。我们将开源(open-source)这一工作流，以促进进一步的研究。我们的工作展示了一条明确的路径，即通过协同设计(co-designing)编排框架(orchestration frameworks)与底层模型能力(underlying model capabilities)，来解锁中小型模型(small-to-medium sized models)的强大推理能力。"
                },
                {
                    "title": "Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving",
                    "arxiv_id": "2509.19681",
                    "authors": "Anisha Garg, Engin Tekin, Yash More, David Bick, Nishit Neema, Ganesh Venkatesh",
                    "summary": "Advanced test-time computing strategies are essential for scaling reasoning models, but their effectiveness is capped by the models' poor self-evaluation. We propose a pairwise Explanatory Verifier, trained via reinforcement learning (GRPO), that produces calibrated confidence scores and associated natural language reasoning for generated solutions. Our verifier improves the accuracy and efficiency of test-time strategies like best-of-n and self-reflection. Crucially, it excels at identifying challenging failure modes, such as when both candidate solutions are identically incorrect, succeeding where standard methods like majority voting fail.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出一种\"解释性验证器\"(Explanatory Verifier)来增强大语言模型的通用推理能力。具体分析如下： 从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是通过强化学习(GRPO)训练的验证器来提升模型的自我评估能力，这属于增强LLM通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 从第二步正面指标看，论文涉及多个相关主题： 1. 能力方向：明确聚焦于\"reasoning\"和\"problem-solving\"，这正是研究目标的核心 2. 训练方法：使用\"reinforcement learning (GRPO)\"进行训练，符合强化学习优化LLM能力的方向 3. 提到的\"self-reflection\"也与提升模型自主推理能力相关 从第三步排除标准看，论文不涉及任何多模态、视觉内容，也不针对医疗、化学、生物等特定应用领域，同时虽然涉及到模型可靠性，但目的是从根本上提升模型的推理能力而非仅作为应用层面的防御。 论文特别关注提高LLM的\"自我评估\"能力，这是通用推理能力的重要组成部分，通过校准的置信度分数和自然语言解释来增强模型的推理质量，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决推理模型在测试时计算策略中自我评估能力不足的问题。针对数学和编码问题求解场景，我们提出了一种基于强化学习(GRPO)训练的成对解释性验证器(Explanatory Verifier)，并在Numina Math、CodeForces和LeetCode数据集上通过准确性和计算效率指标验证了其有效性。",
                    "summary_translation": "先进的测试时计算（test-time computing）策略对于扩展推理模型至关重要，但其有效性受到模型自我评估（self-evaluation）能力不足的限制。我们提出了一种成对解释验证器（pairwise Explanatory Verifier），通过强化学习（GRPO）进行训练，可为生成的解决方案生成校准的置信度分数（calibrated confidence scores）及相关自然语言推理（natural language reasoning）。我们的验证器提高了最佳n选一（best-of-n）和自我反思（self-reflection）等测试时策略的准确性和效率。关键的是，它擅长识别具有挑战性的故障模式（failure modes），例如当两个候选解决方案都完全错误时，能够在多数投票（majority voting）等标准方法失败的情况下取得成功。"
                },
                {
                    "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing",
                    "arxiv_id": "2509.20336",
                    "authors": "Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang",
                    "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究基于Transformer的LLMs在图推理任务中的内部机制，通过circuit-tracer框架来解释decoder-only transformers的推理过程。论文识别了图推理中的两个核心机制：token merging和structural memorization，并提供了统一的可解释性框架来理解结构推理。这符合研究目标中\"改进LLM的基础能力\"和\"增强其逻辑、多步推理等通用能力\"的要求。论文关注的是LLM本身的推理能力机制，而不是将LLM作为工具应用到特定领域。虽然论文聚焦于图推理这一特定类型的推理，但其目标是提供\"统一的可解释性框架\"来理解结构推理，这属于通用推理能力的研究范畴。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。因此，这篇论文符合研究范围。",
                    "summary2": "抱歉，我无法根据提供的内容生成学术总结。提供的链接返回了404错误，显示\"File unavailable for 2509.20336\"，表明该论文ID对应的文件在arXiv上不可用或不存在。没有实际的论文内容，我无法提取研究问题、方法创新和实验验证等关键信息来生成专业的学术总结。请提供有效的论文链接或内容，我将很乐意为您生成符合要求的学术总结。",
                    "summary_translation": "基于Transformer的大型语言模型（Transformer-based LLMs）在图推理任务（graph reasoning tasks）上表现出强大的性能，然而其内部机制（internal mechanisms）仍未被充分探索。为了以基础且统一的视角揭示这些推理过程机制（reasoning process mechanisms），我们使用了基本的仅解码器Transformer（basic decoder-only transformers），并采用电路追踪框架（circuit-tracer framework）对其进行解释。通过这一视角，我们可视化推理轨迹（reasoning traces），并识别出图推理中的两个核心机制：令牌合并（token merging）和结构记忆（structural memorization），这两个机制是路径推理（path reasoning）和子结构提取任务（substructure extraction tasks）的基础。我们进一步量化了这些行为（behaviors），并分析了它们如何受到图密度（graph density）和模型规模（model size）的影响。我们的研究为理解仅解码器Transformer（decoder-only Transformers）中的结构推理（structural reasoning）提供了一个统一的可解释性框架（unified interpretability framework）。"
                },
                {
                    "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms",
                    "arxiv_id": "2509.19702",
                    "authors": "Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama",
                    "summary": "We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nyström extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nyström extrapolation, highlighting a powerful capability of in-context learning.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究线性注意力transformer（一种LLM架构）的基础能力。论文展示了模型通过训练能够隐式地发现统一的数值算法，这直接涉及LLM的内在能力提升，而非将LLM作为工具应用于特定领域。论文关注的是上下文学习(in-context learning)能力，这是一种基础能力的研究，与提高LLM的通用推理能力密切相关。 其次，从正面指标分析，论文符合以下关键点： - 核心概念：研究的是线性注意力transformer，属于LLM架构变体 - 能力方向：涉及数学推理(math reasoning)和问题解决(problem-solving)能力，模型通过学习解决矩阵补全问题，隐式发现了数值算法 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于特定应用领域 - 不涉及模型可靠性的应用层面研究 最后，论文的核心贡献是揭示了LLM能够通过训练隐式发现统一的、资源自适应的迭代求解器，这展示了LLM在算法发现和数学推理方面的强大能力，直接关系到通用推理能力的提升。论文研究的是LLM内在的能力机制，而非特定应用，因此完全符合研究目标。",
                    "summary2": "本文旨在探索训练线性Transformer是否能隐式发现统一的数值算法。针对低秩矩阵补全任务，我们提出了一种训练线性Transformer在masked-block completion任务上隐式学习数值算法的方法，并通过收敛速度和预测准确率验证了EAGLE算法的有效性。",
                    "summary_translation": "我们在数百万个masked-block matrix completion tasks（掩码块矩阵补全任务）上训练了一个linear attention transformer（线性注意力Transformer）：每个提示是一个masked low-rank matrix（掩码低秩矩阵），其缺失的块可能是(i)一个scalar prediction target（标量预测目标）或(ii)一个Nyström extrapolation（Nyström外推）的unseen kernel slice（未见核切片）。模型仅看到输入-输出对和mean-squared loss（均方损失）；它没有被给予normal equations（正规方程）、handcrafted iterations（手工设计的迭代），也没有任何关于这些任务相关的提示。令人惊讶的是，训练后，algebraic unrolling（代数展开）揭示了在三个不同的computational regimes（计算机制）中相同的parameter-free update rule（无参数更新规则）：full visibility（完全可见性）、rank-limited updates（秩限制更新）和distributed computation（分布式计算）。我们证明该规则在full-batch problems（全批量问题）上实现了second-order convergence（二阶收敛），降低了distributed iteration complexity（分布式迭代复杂度），并在rank-limited attention（秩限制注意力）下保持准确性。因此，一个仅被训练来补全缺失块的transformer隐式地发现了一个统一的、resource-adaptive iterative solver（资源自适应迭代求解器），涵盖预测、估计和Nyström extrapolation（Nyström外推），突显了in-context learning（上下文学习）的强大能力。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 17,
            "papers": [
                {
                    "title": "Language Models that Think, Chat Better",
                    "arxiv_id": "2509.20357",
                    "authors": "Adithya Bhaskar, Xi Ye, Danqi Chen",
                    "summary": "Reinforcement learning with verifiable rewards (RLVR) improves language model reasoning by using rule-based rewards in verifiable domains such as mathematics and code. However, RLVR leads to limited generalization for open-ended tasks -- such as writing outline essays or making meal plans -- where humans reason routinely. This paper shows that the RLVR paradigm is effective beyond verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking (**RLMT**) for general-purpose chat capabilities. Using diverse real-world prompts, RLMT requires LMs to generate long CoT reasoning before response, and optimizes them with online RL against a preference-based reward model used in RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT consistently outperforms standard RLHF pipelines. This includes substantial gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and ArenaHardV2), along with 1-3 point improvements on other tasks like creative writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be applied directly to base models without an SFT stage, akin to R1-Zero training. Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex multi-staged pipeline with 25M+ examples. We close with qualitative and quantitative analyses of how trained models plan their responses. Our results rethink the post-training pipeline and call upon future work to understand and employ thinking more broadly.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。论文的核心贡献是提出了一种名为\"RL with Model-rewarded Thinking (RLMT)\"的新训练范式，旨在增强大语言模型的通用推理能力。该方法要求模型在回答前生成长链思维(CoT)推理，并使用基于偏好的奖励模型进行在线强化学习优化，从而提升模型的规划和问题解决能力。 从筛选标准来看： 1. 第一步核心判断：论文本质是改进LLM的基础能力，提出新的训练范式(RLMT)，增强其推理能力，完全符合保留标准。 2. 第二步正面指标：论文包含多个正面指标，如大语言模型(Llama-3.1-8B和Qwen-2.5-7B)、推理能力(reasoning)、强化学习(RL)等。 3. 第三步排除标准：论文不涉及多模态、特定应用领域或模型可靠性等排除领域。 4. 第四步特殊情况：论文专注于通过思维链推理和强化学习来提升模型本身的推理能力，而非将LLM作为工具应用到特定领域。 论文在多个聊天基准测试和任务上取得了显著改进，包括创意写作和一般知识，这表明其方法有效提升了模型的通用推理能力，而非仅限于特定领域。因此，这篇论文完全符合研究目标。",
                    "summary2": "本文旨在解决基于可验证奖励的强化学习(RLVR)在开放性任务上泛化能力有限的问题。针对通用聊天任务，我们提出了一种结合长链推理与偏好模型奖励的强化学习方法(RLMT)，并在Llama-3.1-8B和Qwen-2.5-7B模型上通过多种聊天基准测试(AlpacaEval2, WildBench, Arena-HardV2)验证了其有效性，实现了3-7点的性能提升，甚至超越了GPT-4o在聊天和创意写作方面的表现。",
                    "summary_translation": "可验证奖励强化学习(Reinforcement learning with verifiable rewards, RLVR)通过在可验证领域(如数学和代码)中使用基于规则的奖励来改善语言模型推理能力。然而，RLVR在开放性任务(如撰写大纲论文或制定膳食计划)中导致有限的泛化能力，而这些任务正是人类日常推理的场景。本文表明RLVR范式在可验证领域之外同样有效，并提出了用于通用聊天能力的**基于模型奖励思维的强化学习**(**R**einforcement **L**earning with **M**odel-rewarded **T**hinking, **RLMT**)。\n\n使用多样化的真实世界提示，RLMT要求语言模型(LMs)在响应前生成长链思维(Chain of Thought, CoT)推理，并通过在线强化学习(online RL)对其进行优化，使用的奖励模型是基于偏好的，类似于RLHF(Reinforcement Learning from Human Feedback，人类反馈强化学习)中使用的模型。在Llama-3.1-8B和Qwen-2.5-7B(包括基础模型和指令模型)上进行的40次训练运行以及多种优化算法(DPO、PPO和GRPO)中，RLMT始终优于标准RLHF流程。这包括在三个聊天基准测试(AlpacaEval2、WildBench和ArenaHardV2)上获得3-7分的显著提升，以及在创意写作和常识等其他任务上1-3分的改进。\n\n我们最佳的8B模型在聊天和创意写作方面超越了GPT-4o，并与Claude-3.7-Sonnet (Thinking)相当。RLMT也可以直接应用于基础模型，无需SFT(Supervised Fine-Tuning，监督微调)阶段，类似于R1-Zero训练方式。值得注意的是，仅使用7K个提示，通过我们的RLMT方法训练的Llama-3.1-8B基础模型，其性能超过了经过复杂多阶段流程(使用2500万+示例)后训练的Llama-3.1-8B-Instruct模型。最后，我们对训练模型如何规划其响应进行了定性和定量分析。我们的结果重新思考了后训练流程，并呼吁未来的工作更广泛地理解和运用思维过程。"
                },
                {
                    "title": "Thinking Augmented Pre-training",
                    "arxiv_id": "2509.20186",
                    "authors": "Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei",
                    "summary": "This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\\%$ on several challenging reasoning benchmarks.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式\"Thinking augmented Pre-Training (TPT)\"，通过在预训练阶段增加思维轨迹来增强模型的推理能力。这直接关注提升LLM的通用推理能力，而非将其作为工具应用于特定领域。 其次，论文包含了多个正面指标：核心概念明确关注大型语言模型(LLMs)，能力方向聚焦于推理能力(reasoning)，特别是通过\"step-by-step reasoning and decomposition\"来提升模型性能。实验结果也显示该方法在多个具有挑战性的推理基准上提高了模型性能。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 论文的核心贡献是提出了一种通用的预训练方法，通过自动生成的思维轨迹增强文本数据，使高质量token更易学习，从而提高LLM的数据效率和推理能力。这种方法不是针对特定领域，而是旨在从根本上提升LLM的通用推理能力，完全符合我筛选\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"论文的目标。",
                    "summary2": "本文旨在提高大型语言模型(LLM)训练的数据效率。针对高质量训练数据有限且某些token难以直接学习的问题，我们提出了Thinking augmented Pre-training (TPT)，一种通过自动生成思维轨迹增强现有文本数据的通用方法。我们在多种训练配置上（包括数据受限和充足情况下的预训练以及中期训练）通过推理基准和语言理解任务验证了其有效性，实验表明TPT将LLM预训练的数据效率提高了3倍，显著提升了模型性能。",
                    "summary_translation": "本文介绍了一种简单且可扩展的方法，通过用思维轨迹(thinking trajectories)增强现有文本来提高大型语言模型(Large Language Model, LLM)训练的数据效率。大型语言模型预训练的计算量一直在以前所未有的速度增长，而高质量数据的可用性仍然有限。因此，最大化可用数据的效用构成了一个重大的研究挑战。一个主要障碍是，在固定模型容量下，某些高质量标记(tokens)难以学习，因为单个标记的基本原理可能异常复杂和深入。为解决这一问题，我们提出了思维增强预训练(Thinking augmented Pre-Training, TPT)，这是一种通过自动生成的思维轨迹增强文本的通用方法。这种增强有效增加了训练数据的体量，并通过逐步推理和分解使高质量标记更易学习。我们在多种训练配置中应用TPT，规模高达1000亿(tokens)标记，包括数据受限和数据充足情况下的预训练，以及从强大的开源检查点(checkpoints)进行的中期训练。实验结果表明，我们的方法显著提高了各种规模和系列的大型语言模型的性能。值得注意的是，TPT将大型语言模型预训练的数据效率提高了3倍。对于一个30亿参数(3B parameters)的模型，它在几个具有挑战性的推理基准(benchmarks)上将训练后性能提高了超过10%。"
                },
                {
                    "title": "Causal Understanding by LLMs: The Role of Uncertainty",
                    "arxiv_id": "2509.20088",
                    "authors": "Oscar Lithgow-Serrano, Vani Kanjirangat, Alessandro Antonucci",
                    "summary": "Recent papers show LLMs achieve near-random accuracy in causal relation classification, raising questions about whether such failures arise from limited pretraining exposure or deeper representational gaps. We investigate this under uncertainty-based evaluation, testing whether pretraining exposure to causal examples improves causal understanding >18K PubMed sentences -- half from The Pile corpus, half post-2024 -- across seven models (Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model behavior through: (i) causal classification, where the model identifies causal relationships in text, and (ii) verbatim memorization probing, where we assess whether the model prefers previously seen causal statements over their paraphrases. Models perform four-way classification (direct/conditional/correlational/no-relationship) and select between originals and their generated paraphrases. Results show almost identical accuracy on seen/unseen sentences (p > 0.05), no memorization bias (24.8% original selection), and output distribution over the possible options is almost flat, with entropic values near the maximum (1.35/1.39), confirming random guessing. Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence, 32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11% vs. direct). These findings suggest that failures in causal understanding arise from the lack of structured causal representation, rather than insufficient exposure to causal examples during pretraining.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是研究LLMs在因果关系理解方面的能力，属于LLM的基础推理能力研究，特别是逻辑推理能力的重要组成部分。论文通过多种模型测试，分析了LLMs在因果分类和记忆探测方面的表现，发现LLMs在因果理解上的失败源于缺乏结构化的因果表示，而非预训练中因果例子暴露不足。虽然论文使用了PubMed句子作为测试数据，但这只是为了评估LLM的通用因果理解能力，而不是将LLM应用于医疗领域。论文关注的是LLMs本身的推理能力缺陷，属于对LLM基础能力的探索，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在探究大型语言模型在因果理解任务中表现不佳的原因。针对预训练数据中因果例子暴露不足与表示能力缺陷的争议，我们提出了一种基于不确定性量化的评估方法，通过熵和校准误差等指标分析模型行为。在包含18,000多个PubMed句子的数据集上，通过因果类型分类和逐字记忆探测任务，验证了模型在已见和未见数据上表现无显著差异，输出分布接近随机，表明因果理解失败源于缺乏结构化因果表示而非数据暴露不足。",
                    "summary_translation": "最近的研究表明，大型语言模型（LLMs）在因果关系分类（causal relation classification）中实现接近随机的准确率，引发了关于此类失败是源于预训练（pretraining）中接触有限还是更深层次的表征缺口（representational gaps）的问题。我们在基于不确定性（uncertainty-based）的评估下对此进行了研究，测试了预训练中接触因果例子是否能够改善对超过18,000条PubMed句子的因果理解（causal understanding）——其中一半来自The Pile语料库，一半来自2024年之后的内容——涉及七个模型（Pythia-1.4B/7B/12B、GPT-J-6B、Dolly-7B/12B、Qwen-7B）。我们通过以下方式分析模型行为：（i）因果关系分类（causal classification），模型识别文本中的因果关系；以及（ii）逐字记忆探测（verbatim memorization probing），我们评估模型是否更偏好之前见过的因果陈述而非其释义（paraphrases）。模型执行四类分类（direct/conditional/correlational/no-relationship，直接/条件/相关/无关系）并在原始句子和生成的释义之间进行选择。结果显示，模型在已见/未见句子上的准确率几乎相同（p > 0.05），没有记忆偏差（memorization bias）（24.8%选择原始句子），且可能选项的输出分布几乎平坦，熵值（entropic values）接近最大值（1.35/1.39），证实了随机猜测。指令微调（Instruction-tuned）模型表现出严重的校准失调（miscalibration）（Qwen：> 95%的置信度，32.8%的准确率，ECE=0.49）。条件关系（Conditional relations）诱导出最高的熵（比直接关系高+11%）。这些发现表明，因果理解的失败源于缺乏结构化的因果表征（structured causal representation），而非预训练中接触因果例子不足。"
                },
                {
                    "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
                    "arxiv_id": "2509.20317",
                    "authors": "Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin",
                    "summary": "Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited the application of implicit CoT. We identify a core latent instability issue by scaling the computational budget of implicit CoT approaches: as we increase the number of implicit reasoning tokens to enhance performance, the training process often becomes unstable and collapses. Our analysis reveals that this instability arises from the latent representations becoming homogeneous and losing their semantic diversity, a failure caused by insufficient step-level supervision in existing implicit CoT approaches. To address this issue, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring that latent states capture distinct and meaningful information. The proposed auxiliary decoder is removed during inference, preserving the computational efficiency of implicit CoT methods with no added overhead. In addition, the auxiliary decoder affords interpretability of implicit reasoning by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization of semantic roles and diagnosis. SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain stability of various implicit CoT methods, boosting baselines like Coconut by +8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1% with 2.3\\times greater token efficiency, while substantially closing the performance gap on larger models like LLaMA-3.1 8B.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合研究目标。首先，从核心判断来看，论文的本质是关于改进大语言模型的推理能力，特别是针对Implicit Chain-of-Thought (CoT)方法提出了一种新的训练范式SIM-CoT，这直接属于改进LLM基础能力和通用推理能力的范畴，符合保留标准。 其次，论文包含多项正面指标：核心概念明确涉及Large Language Models (LLMs)；能力方向聚焦于reasoning，特别是Chain-of-Thought推理；训练方法方面提出了创新的step-level supervision机制来增强模型训练过程。 第三，论文不涉及任何排除标准领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文提出的辅助解码器增强了模型推理的可解释性，通过\"projecting each latent token onto an explicit reasoning vocabulary\"来提升模型的推理质量，这符合保留条件。 论文的核心贡献是解决了implicit CoT方法中的潜在不稳定性问题，通过step-level supervision稳定和丰富潜在推理空间，从而提升LLM的推理能力，这与研究目标\"致力于提高大语言模型的通用推理能力\"高度一致。",
                    "summary2": "本文旨在解决隐式思维链(Implicit CoT)方法中的潜在不稳定性问题。针对大语言模型在增加隐式推理令牌时训练崩溃的场景，我们提出了一种SIM-CoT方法，通过引入步骤级监督的辅助解码器来稳定隐式推理空间，并在GSM8K-Aug等多个数学推理数据集上通过准确率、令牌效率等指标验证了其有效性。",
                    "summary_translation": "Implicit Chain-of-Thought (CoT)（隐式思维链）方法为大型语言模型（Large Language Models, LLMs）中的显式思维链推理提供了一种有前景且高效的token（令牌）替代方案，但持续存在的性能差距限制了隐式CoT的应用。通过扩展隐式CoT方法的计算预算，我们发现了一个核心的潜在不稳定性问题：当我们增加隐式推理token的数量以提高性能时，训练过程常常变得不稳定并崩溃。我们的分析表明，这种不稳定性源于潜在表示变得同质化并失去其语义多样性，这是现有隐式CoT方法中步骤级别（step-level）监督不足导致的失败。\n\n为解决这一问题，我们提出了SIM-CoT，即插即用（plug-and-play）训练模块，它引入步骤级别监督以稳定并丰富潜在推理空间。具体而言，SIM-CoT在训练过程中采用辅助解码器（auxiliary decoder）将每个隐式token与其对应的显式推理步骤对齐，确保潜在状态捕获独特且有意义的信息。在推理过程中，所提出的辅助解码器被移除，保持了隐式CoT方法的计算效率，且不增加额外开销。此外，辅助解码器通过将每个潜在token投影到显式推理词汇表（explicit reasoning vocabulary）上，提供了隐式推理的可解释性，实现了每步语义角色和诊断的可视化。\n\nSIM-CoT显著提升了各种隐式CoT方法的域内（in-domain）准确性和域外（out-of-domain）稳定性，使GPT-2上的Coconut基线提高了+8.2%，LLaMA-3.1 8B上的CODI提高了+3.0%。展示了强大的可扩展性（scalability），SIM-CoT在GPT-2上以2.3倍的token效率超越了显式CoT基线2.1%，同时在更大模型如LLaMA-3.1 8B上显著缩小了性能差距。"
                },
                {
                    "title": "Can Constructions \"SCAN\" Compositionality ?",
                    "arxiv_id": "2509.20074",
                    "authors": "Ganesh Katrapati, Manish Shrivastava",
                    "summary": "Sequence to Sequence models struggle at compositionality and systematic generalisation even while they excel at many other tasks. We attribute this limitation to their failure to internalise constructions conventionalised form meaning pairings that license productive recombination. Building on these insights, we introduce an unsupervised procedure for mining pseudo-constructions: variable-slot templates automatically extracted from training data. When applied to the SCAN dataset, our method yields large gains out-of-distribution splits: accuracy rises to 47.8 %on ADD JUMP and to 20.3% on AROUND RIGHT without any architectural changes or additional supervision. The model also attains competitive performance with? 40% of the original training data, demonstrating strong data efAciency. Our findings highlight the promise of construction-aware preprocessing as an alternative to heavy architectural or training-regime interventions.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是关于改进序列到序列模型(Seq2Seq)的组合性和系统泛化能力，这些能力是大语言模型通用推理能力的重要组成部分。论文提出了一种无监督方法来挖掘\"伪构造\"(pseudo-constructions)，即从训练数据中自动提取的可变槽模板，这种方法在SCAN数据集上显著提高了模型在分布外分割上的准确性。这属于改进模型基础能力的方法论研究，而非将LLM应用于特定领域。组合性是逻辑推理和语言理解的基础，系统泛化则涉及到模型如何处理新的、未见过的组合，这些都是通用推理能力的关键方面。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决序列到序列模型在组合性和系统泛化方面的困难。针对SCAN数据集的分布外测试场景，我们提出了一种无监督的伪构造挖掘方法，并在SCAN数据集的ADD JUMP和AROUND RIGHT分割上通过准确率验证了其有效性。",
                    "summary_translation": "序列到序列（Sequence to Sequence）模型在组合性（compositionality）和系统性泛化（systematic generalisation）方面表现不佳，尽管它们在许多其他任务上表现出色。我们将这一局限性归因于它们未能内化构式（constructions）——即约定俗成的形式-意义配对，而这种配对能够许可生产性重组。基于这些见解，我们引入了一种无监督（unsupervised）程序来挖掘伪构式（pseudo-constructions）：即从训练数据中自动提取的可变槽位模板（variable-slot templates）。当应用于SCAN数据集时，我们的方法在分布外（out-of-distribution）分割上取得了显著提升：在无需任何架构变更或额外监督的情况下，ADD JUMP上的准确率提高到47.8%，AROUND RIGHT上提高到20.3%。该模型仅使用40%的原始训练数据就能达到竞争性性能，展示了强大的数据效率（efficiency）。我们的研究结果突显了构式感知（construction-aware）预处理作为一种替代方案的潜力，以替代繁重的架构或训练机制干预。"
                },
                {
                    "title": "Future Policy Aware Preference Learning for Mathematical Reasoning",
                    "arxiv_id": "2509.19893",
                    "authors": "Minjae Oh, Yunho Choi, Dongmin Choi, Yohan Jo",
                    "summary": "Preference learning methods such as Direct Preference Optimization (DPO) have become standard for Large Language Model (LLM) post-training, yet they are often ineffective for mathematical reasoning. A key challenge is the large token overlap between preferred and dispreferred trajectories; lowering the probability of dispreferred trajectories also reduces the probability of shared useful tokens, leading to over-penalization and overall performance collapse. As a mitigation, existing algorithms include the probability of a trajectory under the current policy as a regularization term, which decreases the effect of the gradient when the probability is low. However, by the time this effect takes hold, useful tokens may have already been over-penalized as the model has begun to degrade. To address this, we propose Future Policy Aware (FPA) preference learning, which replaces the current policy with a future policy in the regularization term. This future policy is estimated via lightweight, logit-space extrapolation from a reference model toward the current model. FPA enables safer training by preemptively regularizing potentially problematic gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH and GSM8K benchmarks. FPA yields consistent performance gains, with the largest improvements observed with SimPER, achieving gains of up to 5.75%. We demonstrate that FPA provides proactive regularization while preserving the probability of shared, useful mathematical tokens, and enables longer, degradation-free training with negligible computational overhead. We will release our code publicly upon publication.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是改进LLM的数学推理能力，属于\"增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。论文提出了FPA（Future Policy Aware）方法，用于解决偏好学习在数学推理中的问题，这是直接提升LLM基础能力的研究，而非将LLM作为工具应用到特定领域。 其次，论文满足多个正面指标：核心概念上明确研究Large Language Models (LLMs)；能力方向上专注于mathematical reasoning（数学推理）；训练方法上涉及Direct Preference Optimization (DPO)等偏好学习方法，这些通常与强化学习相关。 第三，论文不符合任何排除标准：它不涉及多模态与视觉内容；虽然聚焦于数学推理，但数学推理被视为评估和提升LLM通用能力的重要方面，而非特定应用领域；也没有主要关注模型可靠性方面的应用问题。 论文的核心贡献是提出了一种新的偏好学习方法FPA，通过在正则化项中使用未来策略而非当前策略，解决了数学推理中偏好学习的过度惩罚问题，从而提升了LLM的数学推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决数学推理任务中偏好学习方法的梯度纠缠问题。针对数学推理轨迹中大量共享token导致的过度惩罚问题，我们提出了一种未来策略感知（FPA）偏好学习方法，通过轻量级logit空间外推估计未来策略进行主动正则化。在MATH和GSM8K基准测试上通过准确率验证了其有效性，FPA在SimPER算法上实现了高达5.75%的性能提升，同时支持更长的无退化训练。",
                    "summary_translation": "像直接偏好优化（Direct Preference Optimization, DPO，直接偏好优化）这样的偏好学习方法已成为大型语言模型（Large Language Model, LLM，大型语言模型）后训练的标准方法，但它们在数学推理方面往往效果不佳。一个关键挑战是偏好轨迹和非偏好轨迹之间存在大量的标记（token，标记）重叠；降低非偏好轨迹的概率同时也会降低共享有用标记的概率，导致过度惩罚和整体性能崩溃。作为一种缓解措施，现有算法将轨迹在当前策略下的概率作为正则化项（regularization term，正则化项）包含在内，当概率较低时，这会降低梯度的影响。然而，当这种效果开始显现时，有用的标记可能已经被过度惩罚，因为模型已经开始退化。\n\n为解决这个问题，我们提出了未来策略感知（Future Policy Aware, FPA，未来策略感知）偏好学习，它在正则化项中用未来策略替代当前策略。这种未来策略通过从参考模型到当前模型的轻量级logit空间（logit-space，logit空间）外推来估计。FPA通过预先规范化可能有问题的梯度，实现了更安全的训练。我们将FPA应用于DPO、RPO和SimPER，并在MATH和GSM8K基准测试上对它们进行评估。FPA带来了一致的性能提升，其中在SimPER上观察到最大的改进，实现了高达5.75%的提升。我们证明FPA提供了主动的正则化，同时保留了共享的有用数学标记的概率，并实现了更长时间的无退化训练，且计算开销可忽略不计。我们将在发表后公开发布我们的代码。"
                },
                {
                    "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models",
                    "arxiv_id": "2509.19593",
                    "authors": "Dylan Hutson, Daniel Vennemeyer, Aneesh Deshmukh, Justin Zhan, Tianyu Jiang",
                    "summary": "We introduce GuessingGame, a protocol for evaluating large language models (LLMs) as strategic question-askers in open-ended, open-domain settings. A Guesser LLM identifies a hidden object by posing free-form questions to an Oracle without predefined choices or candidate lists. To measure question quality, we propose two information gain (IG) metrics: a Bayesian method that tracks belief updates over semantic concepts using LLM-scored relevance, and an entropy-based method that filters candidates via ConceptNet. Both metrics are model-agnostic and support post hoc analysis. Across 858 games with multiple models and prompting strategies, higher IG strongly predicts efficiency: a one-standard-deviation IG increase reduces expected game length by 43\\%. Prompting constraints guided by IG, such as enforcing question diversity, enable weaker models to significantly improve performance. These results show that question-asking in LLMs is both measurable and improvable, and crucial for interactive reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，因为它核心关注的是大语言模型的通用推理能力。论文提出了GuessingGame协议，用于评估LLMs作为战略提问者的能力，本质上是在研究模型如何通过提问和获取信息来进行有效推理。这种开放式提问能力是一种通用推理能力，类似于思维链(CoT)等多步推理能力，而非将LLM应用于特定领域。论文提出的两种信息增益指标旨在衡量和提升LLMs的推理效率，结果显示信息增益与推理效率显著相关，这种研究直接针对提升LLM的基础推理能力。论文不涉及任何排除标准中的领域（如多模态、特定应用或模型基础设施），而是聚焦于提高LLM本身的通用推理能力，因此完全符合我的研究范围。",
                    "summary2": "本文旨在评估大型语言模型作为策略性提问者的能力。针对开放式、开放领域的问答场景，我们提出了GuessingGame协议，通过Guesser LLM向Oracle提问识别隐藏对象，并设计了两种信息增益(IG)度量方法：贝叶斯信念跟踪和基于ConceptNet的熵方法。在858个游戏实验中，通过成功率(SR)和平均问题数(ANQ)验证了方法有效性，证明IG与任务效率强相关，一个标准差IG增加可减少43%预期游戏长度。",
                    "summary_translation": "我们提出了GuessingGame（猜谜游戏）协议，用于评估大型语言模型（LLMs, Large Language Models）在开放式、开放域环境中作为战略提问者的表现。在该协议中，一个Guesser LLM（猜测者语言模型）通过向Oracle（预言者）提出自由形式的问题来识别一个隐藏对象，无需预设选项或候选列表。为衡量问题质量，我们提出了两种信息增益（IG, Information Gain）指标：一种贝叶斯方法，通过使用LLM评分的相关性来追踪对语义概念的信念更新；另一种基于熵的方法，通过ConceptNet（概念网络）过滤候选对象。这两种指标都是模型无关的（model-agnostic），并支持事后分析（post hoc analysis）。在涉及多个模型和提示策略的858场游戏中，更高的IG strongly predicts效率：IG的一个标准差增加使预期游戏长度减少43%。由IG指导的提示约束，如强制问题多样性，使较弱的模型能显著提高性能。这些结果表明，LLMs中的提问既是可测量的也是可改进的，并且对交互式推理（interactive reasoning）至关重要。"
                },
                {
                    "title": "ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities",
                    "arxiv_id": "2509.19569",
                    "authors": "Aleksis Datseris, Sylvia Vassileva, Ivan Koychev, Svetla Boytcheva",
                    "summary": "This paper introduces a novel approach to position embeddings in transformer models, named \"Exact Positional Embeddings\" (ExPE). An absolute positional embedding method that can extrapolate to sequences of lengths longer than the ones it was trained on. Traditional transformer models rely on absolute or relative position embeddings to incorporate positional information into token embeddings, which often struggle with extrapolation to sequences longer than those seen during training. Our proposed method utilizes a novel embedding strategy that encodes exact positional information by overriding specific dimensions of the embedding vectors, thereby enabling a more precise representation of token positions. The proposed approach not only maintains the integrity of the original embeddings but also enhances the model's ability to generalize to more extended sequences. In causal language modeling, our ExPE embeddings significantly reduce perplexity compared to rotary and sinusoidal embeddings, when tested on sequences longer than those used in training.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步核心判断：这篇论文的本质是改进Transformer模型的位置编码方法，提出了\"精确位置嵌入\"(ExPE)，使模型能够处理比训练时更长的序列。这属于改进LLM基础架构的研究，旨在增强模型处理长序列的基础能力，而非将LLM作为工具应用到特定领域。因此，论文符合保留标准。 第二步正面指标：论文涉及\"Generative Transformer Models\"，属于LLM范畴。虽然未直接讨论推理、规划等能力，但处理长序列的能力是支持复杂推理任务的基础。例如，数学推理、逻辑推理和多步规划通常需要处理长序列的能力，因此这项工作间接支持了通用推理能力的提升。 第三步排除标准：论文不涉及任何排除标准中的领域，包括多模态与视觉、特定应用领域以及模型可靠性的应用层面问题。它关注的是基础模型架构的改进。 第四步特殊和模糊情况：论文情况清晰，不涉及特殊或模糊情况。它明确关注的是Transformer模型的位置编码方法，属于基础模型架构的改进。 最终决策：虽然论文没有直接讨论推理、规划或问题解决能力，但改进模型处理长序列的能力是支持复杂推理任务的基础。因此，这篇致力于改进LLM基础架构能力的研究符合\"提高大语言模型（LLM）本身的『通用推理能力』\"的核心研究目标。",
                    "summary2": "本文旨在解决Transformer模型在处理比训练序列更长时的位置编码外推问题。针对序列长度外推的场景，我们提出了一种精确位置编码（ExPE）方法，通过覆盖嵌入向量中的特定维度来编码精确位置信息，并在因果语言建模任务上通过困惑度（perplexity）指标验证了其有效性。",
                    "summary_translation": "本文介绍了一种在transformer models（Transformer模型）中进行position embeddings（位置嵌入）的新方法，名为\"Exact Positional Embeddings\"（精确位置嵌入，ExPE）。这是一种absolute positional embedding（绝对位置嵌入）方法，能够extrapolate（外推）到比训练时更长的序列。传统的transformer models（Transformer模型）依赖absolute或relative position embeddings（绝对或相对位置嵌入）将位置信息整合到token embeddings（词元嵌入）中，但这些方法通常难以extrapolate（外推）到比训练时更长的序列。我们提出的方法利用一种新的embedding strategy（嵌入策略），通过重写embedding vectors（嵌入向量）的特定维度来编码精确的位置信息，从而实现对词元位置的更精确表示。所提出的方法不仅保持了原始embeddings（嵌入）的原有特性，还增强了模型对更长序列的泛化能力。在causal language modeling（因果语言建模）中，当在比训练中使用的更长的序列上进行测试时，我们的ExPE embeddings（ExPE嵌入）与rotary and sinusoidal embeddings（旋转和正弦嵌入）相比，显著降低了perplexity（困惑度）。"
                },
                {
                    "title": "How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models",
                    "arxiv_id": "2509.19371",
                    "authors": "Kangtao Lv, Haibin Chen, Yujin Yuan, Langming Liu, Shilei Liu, Yongwei Wang, Wenbo Su, Bo Zheng",
                    "summary": "Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，主要基于以下分析： 第一步核心判断：这篇论文的本质是关于改进LLM基础能力的研究，具体聚焦于预训练阶段的知识注入方法。论文提出了\"知识注入缩放定律\"，这是一种新的训练范式，旨在解决LLM在知识获取与保留方面的核心挑战。虽然论文提到了\"领域知识\"，但其核心贡献是通用的方法论，用于平衡知识注入与避免灾难性遗忘，这直接关系到提升LLM的基础能力，符合\"改进LLM的基础能力、提出新的训练范式\"的保留标准。 第二步正面指标：论文明确包含\"Large language models, LLMs\"这一核心概念。虽然论文没有直接讨论reasoning、planning等具体能力方向，但知识获取和保留是推理能力的基础，论文研究的是如何更有效地让模型获取并保留知识，这间接支持了通用推理能力的提升。 第三步排除标准：论文不涉及多模态与视觉研究。虽然提到\"domain-specific data\"和\"specialized knowledge\"，但论文的核心是提出一种通用的知识注入缩放定律，而非专注于某个特定应用领域（如医疗、化学等）。论文提到\"hallucination\"问题，但是从知识注入角度研究如何减少幻觉，而非仅作为应用层面的防御。 第四步特殊和模糊情况处理：论文虽然涉及\"领域知识\"，但其核心贡献是通用的方法论，可以应用于各种领域知识的注入，而不是针对特定领域的应用研究。因此，它更符合\"改进LLM基础能力\"而非\"特定应用领域\"的特征。 综上所述，这篇论文的核心贡献是提出了一种通用的知识注入缩放定律，用于优化LLM预训练过程中的知识获取和保留，这属于提升LLM基础能力的研究范畴，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型预训练中如何高效注入领域知识的问题。针对不同规模LLMs在知识注入时出现的记忆崩溃现象，我们提出了一种知识注入扩展律(Knowledge Infusion Scaling Law)，通过分析较小模型预测大型模型的最佳知识注入量，并在从137M到3B参数的不同模型规模和高达100B训练token的实验环境中，通过记忆保留率(Memorization Rate)指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）因其令人印象深刻的跨多样化下游任务的通用能力而吸引了广泛关注。然而，在没有领域特定优化（domain-specific optimization）的情况下，它们在专业知识基准（specialized knowledge benchmarks）上表现不佳，甚至会产生幻觉（hallucination）。最近的研究表明，在预训练（pretraining）期间策略性地注入领域知识（domain knowledge）可以显著提高下游性能（downstream performance）。一个关键挑战在于平衡这种注入权衡（infusion trade-off）：注入过少的领域特定数据（domain-specific data）会导致专业化不足，而过量注入则会引发灾难性遗忘（catastrophic forgetting）先前获得的知识。在这项工作中，我们关注由过度注入（over-infusion）引起的记忆崩溃（memory collapse）现象。通过系统性实验，我们得出了两个关键观察结果，即1）关键崩溃点（Critical collapse point）：每个模型都表现出一个阈值，超过该阈值，其知识保留能力会急剧下降。2）规模相关性（Scale correlation）：这些崩溃点与模型规模（model's size）呈一致的比例关系。基于这些见解，我们提出了一种知识注入扩展定律（knowledge infusion scaling law），通过分析较小规模的对应模型来预测应注入大型语言模型（LLMs）的最佳领域知识量。在不同模型规模和预训练令牌预算（pertaining token budgets）上的大量实验验证了我们扩展定律的有效性和泛化性。"
                },
                {
                    "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution",
                    "arxiv_id": "2509.19349",
                    "authors": "Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin",
                    "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large language models (LLMs) to advance scientific discovery with state-of-the-art performance and unprecedented efficiency. Recent advances in scaling inference time compute of LLMs have enabled significant progress in generalized scientific discovery. These approaches rely on evolutionary agentic harnesses that leverage LLMs as mutation operators to generate candidate solutions. However, current code evolution methods suffer from critical limitations: they are sample inefficient, requiring thousands of samples to identify effective solutions, and remain closed-source, hindering broad adoption and extension. ShinkaEvolve addresses these limitations, introducing three key innovations: a parent sampling technique balancing exploration and exploitation, code novelty rejection-sampling for efficient search space exploration, and a bandit-based LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks, demonstrating consistent improvements in sample efficiency and solution quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution using only 150 samples, designs high-performing agentic harnesses for AIME mathematical reasoning tasks, identifies improvements to ALE-Bench competitive programming solutions, and discovers novel mixture-of-expert load balancing loss functions that illuminate the space of optimization strategies. Our results demonstrate that ShinkaEvolve achieves broad applicability with exceptional sample efficiency. By providing open-source accessibility and cost-efficiency, this work democratizes open-ended discovery across diverse computational problems.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文本质上是关于改进LLM的基础能力和提出新的训练范式。ShinkaEvolve框架利用LLMs作为变异操作符，通过进化机制增强模型生成解决方案的能力，这直接关注提升LLM的通用推理和问题解决能力，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确提到\"large language models (LLMs)\" - 能力方向：涉及\"mathematical reasoning\"和\"competitive programming solutions\"，属于通用推理能力范畴 - 训练方法：提出\"evolutionary agentic harnesses\"和\"bandit-based LLM ensemble selection strategy\"，属于进化学习方法 - 新兴范式：包含\"agentic harnesses\"，与LLM-based agents相关 第三步排除标准：论文不符合任何排除标准。虽然提到了圆打包、数学推理等应用场景，但这些是作为评估框架通用性的示例，而非论文的主要焦点。论文核心是提出通用的程序进化框架，而非针对特定领域应用。 第四步特殊情况处理：论文提出的\"evolutionary agentic harnesses\"是一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非针对特定领域的应用，因此符合保留条件。 综合分析，ShinkaEvolve的核心贡献是提出了一种新的进化框架，通过创新的采样和集成选择策略，提高LLM在程序进化方面的样本效率和解决方案质量，这直接服务于提升大语言模型的通用推理能力，符合研究目标。",
                    "summary2": "本文旨在解决当前LLM驱动的科学发现方法中存在的样本效率低下和闭源限制问题。针对程序进化任务，我们提出了ShinkaEvolve框架，通过三种关键创新（自适应父程序采样、代码新颖性拒绝采样和基于bandit的LLM集成选择策略）显著提升样本效率。在circle packing、AIME数学推理、ALE-Bench竞赛编程和MoE负载平衡损失设计等多样任务上，ShinkaEvolve以更少样本实现了state-of-the-art性能，并通过开源发布促进了广泛应用。",
                    "summary_translation": "我们介绍了ShinkaEvolve：一个新的开源框架，该框架利用大型语言模型（LLMs, Large Language Models）来推动科学发现，具有最先进的性能和前所未有的效率。最近在扩展大型语言模型推理时间计算方面的进展，为通用科学发现带来了显著进步。这些方法依赖于进化智能代理框架（evolutionary agentic harnesses），该框架利用大型语言模型作为变异算子（mutation operators）来生成候选解决方案。然而，当前的代码进化方法存在关键局限性：样本效率低下（sample inefficient），需要数千个样本才能识别有效解决方案，并且仍然是闭源的，阻碍了广泛采用和扩展。\n\nShinkaEvolve解决了这些局限性，引入了三项关键创新：一种平衡探索与利用的父代采样技术（parent sampling technique），用于高效搜索空间探索的代码新颖性拒绝采样（code novelty rejection-sampling），以及基于多臂老虎机（bandit-based）的大型语言模型集成选择策略（LLM ensemble selection strategy）。我们在多样化任务上评估了ShinkaEvolve，展示了在样本效率和解决方案质量上的一致性改进。\n\nShinkaEvolve仅使用150个样本就发现了一种新的最先进的圆形打包（circle packing）解决方案，为AIME数学推理任务设计了高性能的智能代理框架，识别出ALE-Bench竞赛编程解决方案的改进，并发现了新颖的专家混合负载平衡损失函数（mixture-of-expert load balancing loss functions），这些函数阐明了优化策略的空间。我们的结果表明，ShinkaEvolve实现了广泛的应用性和卓越的样本效率。通过提供开源的可访问性和成本效益，这项工作使多样化的计算问题中的开放式发现（open-ended discovery）变得民主化。"
                },
                {
                    "title": "Pluralistic Off-policy Evaluation and Alignment",
                    "arxiv_id": "2509.19333",
                    "authors": "Chengkai Huang, Junda Wu, Zhouhang Xie, Yu Xia, Rui Wang, Tong Yu, Subrata Mitra, Julian McAuley, Lina Yao",
                    "summary": "Personalized preference alignment for LLMs with diverse human preferences requires evaluation and alignment methods that capture pluralism. Most existing preference alignment datasets are logged under policies that differ substantially from the evaluated LLMs, and existing off-policy estimators focus solely on overall utility while ignoring preference pluralism. Extending Off-Policy Evaluation (OPE) to pluralistic preference alignment, therefore, remains an open question. Thus, we propose the Pluralistic Off-Policy Evaluation (POPE), the first framework for offline pluralistic preference evaluation and alignment in LLMs. POPE includes a unified reward function that combines (1) a collaborative utility component derived from human preference signals (e.g., upvotes or relevance scores) and (2) a diversity component inspired by entropy-based coverage measures, together reflecting pluralistic alignment. Furthermore, to estimate this reward from logged interactions, we derive decomposable inverse propensity scoring (IPS) estimators that separately evaluate relevance and diversity. Theoretically, we prove that our decomposed IPS estimators establish a lower bound on their variance. With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance pluralistic alignment. Empirical results demonstrate that POPE efficiently enhances pluralistic response generation and maintains the models' general capabilities on downstream tasks",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了POPE（Pluralistic Off-Policy Evaluation）框架，用于解决LLM在多元人类偏好下的评估和对齐问题。从第一步判断来看，论文本质上是关于改进LLM的基础能力（偏好对齐），提出新的评估和优化框架，这符合保留标准。论文明确针对LLM的偏好对齐问题，并涉及到强化学习中的离线策略评估和优化概念，这与第二步中的正面指标部分吻合。论文不符合第三步中的排除标准，它不主要聚焦于多模态、特定应用领域或模型可靠性的应用层面问题。虽然论文没有直接讨论推理、规划或问题解决能力，但它关注的是偏好对齐，这是LLM的一个重要基础能力，良好的偏好对齐是模型展现高质量推理能力的前提。论文提出的框架通过结合人类偏好信号和多样性组件来改进模型的基础能力，这种改进可能间接提升模型在推理和其他任务上的表现。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决LLM中多元化偏好对齐的离线评估问题。针对记录在不同策略下的偏好数据，我们提出了POPE框架，结合协作效用和多样性奖励的统一函数，并通过可分解的反向倾向评分估计器进行评估。在Alpaca-GPT4、电影评论等数据集上通过PL-Score、Pluralistic Coverage等指标验证了其有效性。",
                    "summary_translation": "针对具有多样化人类偏好的大型语言模型(LLMs, 大型语言模型)的个性化偏好对齐(preference alignment, 偏好对齐)，需要能够捕捉多元性(pluralism, 多元性)的评估和对齐方法。大多数现有的偏好对齐数据集是在与被评估的LLMs显著不同的策略下记录的，而现有的离策略估计器(off-policy estimators, 离策略估计器)仅关注整体效用(utility, 效用)，却忽视了偏好多元性(preference pluralism, 偏好多元性)。因此，将离策略评估(Off-Policy Evaluation, OPE)扩展到多元偏好对齐(pluralistic preference alignment, 多元偏好对齐)仍然是一个开放性问题。为此，我们提出了多元离策略评估(Pluralistic Off-Policy Evaluation, POPE)，这是首个用于LLMs离线(offline, 离线)多元偏好评估和对齐的框架。POPE包含一个统一的奖励函数(reward function, 奖励函数)，该函数结合了(1)源自人类偏好信号（例如，点赞或相关性评分）的协作效用组件(collaborative utility component, 协作效用组件)，以及(2)受基于熵的覆盖度量(entropy-based coverage measures, 基于熵的覆盖度量)启发的多样性组件(diversity component, 多样性组件)，共同反映了多元对齐(pluralistic alignment, 多元对齐)。此外，为了从记录的交互中估计此奖励，我们推导出了可分解的反向倾向评分(inverse propensity scoring, IPS)估计器，该估计器分别评估相关性(relevance, 相关性)和多样性(diversity, 多样性)。理论上，我们证明了我们分解的IPS估计器为其方差建立了下界。通过离策略评估的值函数(value function, 值函数)，我们可以直接启用离策略优化(off-policy optimization, 离策略优化)，以进一步增强多元对齐。实证结果表明，POPE有效增强了多元响应生成(pluralistic response generation, 多元响应生成)，并保持了模型在下游任务(downstream tasks, 下游任务)上的通用能力。"
                },
                {
                    "title": "Failure Modes of Maximum Entropy RLHF",
                    "arxiv_id": "2509.20265",
                    "authors": "Ömer Veysel Çağatan, Barış Akgün",
                    "summary": "In this paper, we show that Simple Preference Optimization (SimPO) can be derived as Maximum Entropy Reinforcement Learning with length-normalized temperature, providing a theoretical foundation for this reference-free method. Motivated by SimPO's strong performance in offline preference optimization, we investigate whether Maximum Entropy RL can achieve similar results in online RLHF settings. Our experiments find that Maximum Entropy RL consistently exhibits overoptimization and unstable KL dynamics, even at very low learning rates. Unlike KL-constrained methods that maintain stable training, entropy regularization fails to prevent reward hacking and appears to correlate with overoptimization. Lastly, we discuss possible explanations for why SimPO succeeds in offline settings while Maximum Entropy RL struggles in online scenarios. Our findings suggest that reference-free approaches may face distinct challenges when applied to online or offline preference learning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是研究RLHF（Reinforcement Learning from Human Feedback）的优化问题，特别是分析了最大熵强化学习在在线RLHF设置中的失败模式，并探讨了SimPO在离线设置中成功的原因。RLHF是提升大语言模型通用能力的关键训练技术，论文研究的是如何改进这一训练方法，属于\"改进LLM的基础能力、提出新的训练范式\"的范畴。论文直接关注强化学习（RLHF）这一训练方法，符合正面指标。同时，论文不涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性的应用层面）。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
                    "summary2": "本文旨在研究最大熵强化学习在人类反馈强化学习(RLHF)中的失效模式。针对在线和离线偏好学习场景，我们提出了一种将SimPO解释为长度归一化温度的最大熵RL的理论框架，并在TL;DR数据集上通过胜率和KL散度等指标验证了其有效性。实验发现，尽管SimPO在离线设置中表现良好，但在线最大熵RL存在过优化和不稳定问题，表明熵正则化无法有效防止奖励 hacking。",
                    "summary_translation": "本文表明，简单偏好优化（Simple Preference Optimization, SimPO）可被推导为具有长度归一化温度的最大熵强化学习（Maximum Entropy Reinforcement Learning），为这种无参考方法（reference-free method）提供了理论基础。受SimPO在离线偏好优化中出色表现的启发，我们研究了最大熵强化学习是否能在在线RLHF（基于人类反馈的强化学习）设置中取得类似结果。我们的实验发现，即使在非常低的学习率下，最大熵强化学习也始终表现出过度优化（overoptimization）和不稳定的KL（Kullback-Leibler）动态。与能够保持稳定训练的KL约束方法不同，熵正则化（entropy regularization）未能防止奖励黑客（reward hacking），并且似乎与过度优化相关。最后，我们讨论了为什么SimPO在离线设置中成功而最大熵强化学习在在线场景中挣扎的可能解释。我们的研究结果表明，无参考方法在应用于在线或离线偏好学习时可能面临不同的挑战。"
                },
                {
                    "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning",
                    "arxiv_id": "2509.19894",
                    "authors": "Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong",
                    "summary": "Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at https://github.com/inclusionAI/PromptCoT.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围。首先，从核心判断来看，论文的本质是提升大语言模型的通用推理能力，特别是数学和编程推理能力。论文提出了PromptCoT 2.0框架，通过改进提示合成方法来增强LLM的推理能力，这属于\"改进LLM的基础能力和提出新的训练范式\"的范畴。 其次，论文包含多个正面指标：明确关注\"Large language models (LLMs)\"；核心能力方向是\"reasoning\"，特别是\"math reasoning\"和\"logical reasoning\"；提出了新的训练方法，包括\"Self-Play\"和\"Supervised Fine-Tuning (SFT)\"。 第三，论文不涉及任何排除标准中的领域：没有关注多模态与视觉问题；虽然涉及数学和编程，但这些是通用推理的基础领域而非特定应用领域；也没有主要关注模型可靠性的应用层面问题。 论文的核心贡献是提出了一种可扩展的提示合成框架，通过迭代改进推理过程来生成更难、更多样化的问题，从而提升LLM的推理能力。这种方法从根本上增强了模型的基础推理能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型推理任务中高质量训练数据短缺的问题。针对数学和编程领域的数据需求，我们提出了一种基于期望最大化(EM)循环优化的PromptCoT 2.0框架，通过迭代改进推理来指导提示构建，生成更难且更多样化的问题。在AIME、HMMT、LiveCodeBench和Codeforces等六个基准测试上，通过pass@1准确率和Elo评级验证了其有效性，在Self-Play和SFT两种设置下均取得了最先进结果。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）正在从对话系统演变为强大的推理工具，用于奥数竞赛和竞技编程等任务。尽管参数扩展（scaling parameters）和测试时计算（test-time computation）推动了进展，但一个关键瓶颈是缺乏高质量的训练问题：人工策划的数据集成本高昂且有限，而现有的合成语料库（synthetic corpora）往往过于简单或范围狭窄。PromptCoT 1.0表明，在提示合成中注入推理过程（rationales）会增加问题难度。在此基础上，我们提出了PromptCoT 2.0，这是一个可扩展的框架，用期望最大化（Expectation-Maximization, EM）循环替代手工设计的启发式方法，其中推理过程被迭代优化以指导提示构建。这产生的问题比之前的语料库更难且更多样化。\n\n这些合成提示支持两种后训练机制：（1）自我对弈（Self-Play），其中强模型通过可验证的反馈在没有更强教师的情况下自主改进；（2）监督微调（Supervised Fine-Tuning, SFT），其中弱模型从教师蒸馏的轨迹中学习。广泛的实验证明了这种方法的有效性。在自我对弈中，将PromptCoT 2.0应用于Qwen3-30B-A3B-Thinking-2507在300亿参数规模上创造了新的最先进结果，在AIME 24/25和HMMT 25上分别提升+4.4、+4.8和+5.3，在LiveCodeBench v5/v6上提升+6.1和+5.0，在Codeforces上提升+35 Elo。在SFT中，仅使用合成提示训练Qwen2.5-7B-Instruct将准确率提升至73.1（AIME 24）、65.6（AIME 25）和53.4（LiveCodeBench v5），超过了在人工或混合数据上训练的模型。\n\n分析进一步证实，PromptCoT 2.0产生了本质上更难且分布上不同的问题。这些结果将提示合成确立为扩展推理的新维度，并将PromptCoT 2.0定位为未来开源模型的可扩展基础。该实现在https://github.com/inclusionAI/PromptCoT上可用。"
                },
                {
                    "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models",
                    "arxiv_id": "2509.19803",
                    "authors": "Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang",
                    "summary": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是提出VCRL，一种基于课程学习的强化学习框架，用于提高大语言模型的推理能力。论文本质上是关于改进LLM的基础能力，特别是数学推理能力，这符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。论文包含多个正面指标，如关注LLMs核心概念、数学推理能力方向以及强化学习训练方法。同时，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文主要在数学推理任务上进行实验，但提出的方法是通用的课程学习强化学习框架，通过动态控制训练样本的难度来提高LLM对不同难度样本的学习能力，这与人类从易到难的认知过程一致，可以推广到其他需要推理能力的任务。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决大语言模型在数学推理任务中强化学习训练时没有考虑样本难度匹配的问题。针对不同难度的数学推理样本，我们提出了一种基于方差的课程强化学习框架VCRL，并在五个数学基准测试上通过准确率等指标验证了其有效性。",
                    "summary_translation": "基于策略的强化学习（Policy-based reinforcement learning）目前在提升大语言模型（LLMs）数学推理能力方面发挥着重要作用。然而，现有的基于展开的强化学习方法（rollout-based reinforcement learning methods）（如GRPO、DAPO、GSPO等）未能明确考虑大语言模型对不同难度样本的学习能力，这与人类从易到难的数学推理任务认知过程相悖。直观上，我们发现RLVR中展开组（rollout group）的奖励方差部分反映了当前样本对大语言模型的难度。过易或过难的样本具有较低的方差，而难度适中的样本具有较高的方差。基于此，我们提出了VCRL，一种基于组奖励方差动态控制训练样本难度的课程强化学习（curriculum reinforcement learning）框架。在五个数学基准测试和两个模型上的实验揭示了VCRL相较于当前大语言模型强化学习基线方法的优势。"
                },
                {
                    "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI",
                    "arxiv_id": "2509.20175",
                    "authors": "Lorenzo Giusti, Ole Anton Werner, Riccardo Taiello, Matilde Carvalho Costa, Emre Tosun, Andrea Protani, Marc Molina, Rodrigo Lopes de Almeida, Paolo Cacace, Diogo Reis Santos, Luigi Serio",
                    "summary": "We present Federation of Agents (FoA), a distributed orchestration framework that transforms static multi-agent coordination into dynamic, capability-driven collaboration. FoA introduces Versioned Capability Vectors (VCVs): machine-readable profiles that make agent capabilities searchable through semantic embeddings, enabling agents to advertise their capabilities, cost, and limitations. Our aarchitecturecombines three key innovations: (1) semantic routing that matches tasks to agents over sharded HNSW indices while enforcing operational constraints through cost-biased optimization, (2) dynamic task decomposition where compatible agents collaboratively break down complex tasks into DAGs of subtasks through consensus-based merging, and (3) smart clustering that groups agents working on similar subtasks into collaborative channels for k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe semantics for scalable message passing, FoA achieves sub-linear complexity through hierarchical capability matching and efficient index maintenance. Evaluation on HealthBench shows 13x improvements over single-model baselines, with clustering-enhanced laboration particularly effective for complex reasoning tasks requiring multiple perspectives. The system scales horizontally while maintaining consistent performance, demonstrating that semantic orchestration with structured collaboration can unlock the collective intelligence of heterogeneous federations of AI agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Federation of Agents (FoA)\"的分布式编排框架，用于实现大规模智能体AI的动态协作。从本质上看，论文属于\"智能体协作框架\"的研究范畴，符合筛选标准中的保留条件。论文提出的版本化能力向量(VCVs)、语义路由、动态任务分解和智能聚类等创新方法，都是为了提升智能体系统的通用协作和推理能力，而非将LLM作为工具应用到特定领域。 论文在正面指标上表现良好，涉及了\"multi-agent systems\"这一新兴范式，并明确提到该系统在\"complex reasoning tasks\"上表现出色，这与\"通用推理能力\"的研究目标直接相关。虽然论文没有直接提及\"Large language models\"，但智能体系统通常基于LLM构建，且论文关注的是通用能力的提升。 在排除标准方面，论文没有主要关注多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然评估中使用了HealthBench数据集，但这仅用于验证系统性能，论文本身并非针对医疗等特定领域的研究。 综合分析，这篇论文提出的是一种通用的智能体协作框架，旨在通过语义感知的通信机制增强智能体系统的协作和推理能力，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在解决大规模多智能体AI系统中能力发现和动态协调的问题。针对异构智能体协作场景，我们提出了一种Federation of Agents (FoA)语义感知通信框架，通过Versioned Capability Vectors实现能力驱动的动态编排，并在HealthBench数据集上通过任务完成质量指标验证了其有效性，相比单模型基线实现了13倍的性能提升。",
                    "summary_translation": "我们提出了代理联盟（Federation of Agents, FoA），这是一个分布式编排框架（distributed orchestration framework），能够将静态的多智能体协调转变为动态的、由能力驱动的协作。FoA引入了版本化能力向量（Versioned Capability Vectors, VCVs）：这是一种机器可读的配置文件，通过语义嵌入（semantic embeddings）使智能体能力可被搜索，使智能体能够宣传其能力、成本和局限性。我们的架构结合了三个关键创新：(1) 语义路由（semantic routing），它在分片的HNSW索引上将任务匹配到智能体，同时通过成本偏置优化（cost-biased optimization）强制执行操作约束；(2) 动态任务分解（dynamic task decomposition），其中兼容的智能体通过基于共识的合并（consensus-based merging）协作地将复杂任务分解为有向无环图（DAGs）的子任务；以及(3) 智能聚类（smart clustering），它将处理相似子任务的智能体分组到协作通道中，在综合之前进行k轮细化。FoA建立在MQTT的发布-订阅语义（publish-subscribe semantics）之上，以实现可扩展的消息传递，并通过分层能力匹配（hierarchical capability matching）和高效的索引维护（efficient index maintenance）实现了次线性复杂度（sub-linear complexity）。在HealthBench上的评估显示，与单模型基线相比有13倍的改进，其中聚类增强的协作（clustering-enhanced collaboration）对于需要多视角的复杂推理任务特别有效。该系统能够水平扩展（scales horizontally）同时保持一致的性能，表明具有结构化协作的语义编排（semantic orchestration）可以释放异构AI代理联盟（heterogeneous federations of AI agents）的集体智能。"
                },
                {
                    "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
                    "arxiv_id": "2509.19736",
                    "authors": "Cheng Qian, Zuxin Liu, Akshara Prabhakar, Jielin Qiu, Zhiwei Liu, Haolin Chen, Shirley Kokane, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang",
                    "summary": "Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是提出UserRL框架，通过强化学习训练用户中心的智能体，这属于\"智能体协作框架\"的范畴，是改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用于特定领域。其次，论文包含多个正面指标：明确使用了大语言模型(Qwen3)，采用了强化学习(RL)方法训练模型，研究了基于LLM的智能体(agentic models)，并关注动态多轮交互能力，这些都符合研究目标。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。最后，虽然论文涉及智能体研究，但它提出的是通用的智能体训练框架，而非针对特定领域的应用，因此应予以保留。论文的核心贡献在于探索如何通过奖励塑造和用户模拟选择来提升智能体的通用交互能力，这与提高大语言模型通用推理能力的研究目标高度一致。",
                    "summary2": "本文旨在解决如何训练能有效获取用户中心能力的智能体模型，同时考虑用户交互多样性和动态性的问题。针对多轮用户交互场景，我们提出了一种UserRL框架，结合标准化gym环境和模拟用户，并在Qwen3模型上通过不同奖励设计策略验证了其有效性。",
                    "summary_translation": "强化学习 (Reinforcement learning, RL) 在训练智能体模型 (agentic models) 方面显示出潜力，这些模型能够超越静态基准测试 (static benchmarks)，进行动态、多轮交互 (dynamic, multi-turn interactions)。然而，这类智能体的最终价值在于其协助用户的能力，而在这一场景中，用户交互的多样性和动态性带来了挑战。在这项工作中，我们提出了UserRL，这是一个通过标准化的gym环境 (gym environments) 配合模拟用户 (simulated users) 来训练和评估以用户为中心能力 (user-centric abilities) 的统一框架。我们系统性地改变轮级奖励分配 (turn-level reward assignment) 和轨迹级分数计算 (trajectory-level score calculation)，以分析不同表述形式如何影响GRPO算法 (GRPO algorithm) 下的学习效果。我们在Qwen3模型 (Qwen3 models) 上的实验揭示了三个关键发现：(i) SFT冷启动 (SFT cold start) 对于解锁初始交互能力和实现持续的RL改进至关重要；(ii) 精心设计的轨迹评分 (deliberate trajectory scoring) 能够产生更高效且有效的多轮交互；(iii) 虽然更强大的模拟用户（如GPT-4o）能够促进训练，但开源模拟器（如Qwen3-32B）仍然是一种经济高效且可迁移的选择。总体而言，这些结果强调，奖励塑造 (reward shaping) 和用户模拟选择 (user simulation choice) 的精心设计与模型规模 (model scale) 同样重要，并将UserRL确立为开发强大的以用户为中心的智能体模型 (robust user-centric agentic models) 的实用途径。所有代码和数据均公开，以供未来研究使用。"
                },
                {
                    "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
                    "arxiv_id": "2509.19517",
                    "authors": "Sai Teja Reddy Adapala",
                    "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究目标，核心贡献是研究大语言模型在认知负荷下的多跳推理能力限制。根据筛选标准，我的判断过程如下： 第一步：核心判断——论文本质是研究LLM本身的推理能力限制。论文提出了计算认知负荷的形式理论，设计了新的评估基准(ICE)来测试LLM在多跳推理任务上的表现，特别是在认知负荷条件下的性能变化。这不是将LLM应用于特定领域，而是直接研究LLM的基础推理能力，属于改进LLM通用能力的研究。 第二步：正面指标——论文包含关键正面指标：(1)核心概念：明确研究Large Language Models (LLMs)；(2)能力方向：聚焦于multi-hop reasoning（多跳推理），属于逻辑推理范畴。虽然未涉及训练方法和新兴范式，但这两个核心正面指标已足够表明论文与研究方向高度相关。 第三步：排除标准——论文不涉及任何需要排除的领域。它没有研究多模态与视觉问题，没有聚焦于特定应用领域（如医疗、化学等），也没有从应用层面研究模型可靠性。 第四步：特殊和模糊情况——论文提到\"hallucination-as-guessing under uncertainty\"，这是从认知机制角度解释幻觉现象，探讨其与推理能力的关系，而非仅进行社会学研究或应用层面讨论，这有助于理解LLM推理能力的本质限制。 综合来看，这篇论文通过研究认知负荷对LLM推理能力的影响，提出了新的评估方法和理论框架，直接服务于提升LLM通用推理能力的研究目标，完全符合筛选要求。",
                    "summary2": "本文旨在研究大型语言模型在认知负荷下的多跳推理能力限制。针对信息丰富、任务切换的动态场景，我们提出了计算认知负荷理论，并设计了Interleaved Cognitive Evaluation (ICE)基准测试系统操纵上下文饱和和注意力残留因素。在五个LLMs上通过Exact-Match准确率验证发现：Gemini-2.0-Flash-001在控制条件下达85%准确率，但在额外信息增加时性能显著下降(β = -0.003, p < 0.001)，而较小模型如Llama-3-8B-Instruct在所有条件下均表现完全失效。",
                    "summary_translation": "大型语言模型（LLMs）的扩展揭示了它们在静态基准测试上的表现与在动态、信息丰富环境中的脆弱性之间的关键差距。尽管模型在孤立任务上表现出色，但控制其在认知负荷（cognitive load）下推理的计算限制仍然知之甚少。在本研究中，我们提出了一个计算认知负荷（computational cognitive load）的正式理论，假设外部的、与任务无关的信息（Context Saturation，上下文饱和）和任务切换造成的干扰（Attentional Residue，注意力残留）是导致性能下降的关键机制。我们设计了交错认知评估（Interleaved Cognitive Evaluation, ICE），这是一个去混淆的基准测试，用于在具有挑战性的多跳推理（multi-hop reasoning）任务上系统地操纵这些负荷因素。一项全面研究（200个问题中每个项目重复10次）揭示了五个经过指令微调（instruction-tuned）的模型之间存在显著的性能差异。较小的开源架构（Llama-3-8B-Instruct、Mistral-7B-Instruct-v0.2）表现出基线脆弱性（baseline brittleness），在这个高内在负荷（high-intrinsic-load）任务的所有条件下（包括干净的对照组）实现了0%的准确率（SEM = 0.0）。相比之下，Gemini-2.0-Flash-001表现出部分韧性（partial resilience），在对照条件下达到85%的准确率，在上下文饱和条件下出现统计学显著的性能下降（$\\beta = -0.003$每%负荷，$p < 0.001$）。这些发现提供了初步证据，表明认知负荷是推理失败的关键因素，支持了在不确定性下幻觉即猜测（hallucination-as-guessing）的理论。我们得出结论，动态的、具有认知意识的压力测试（cognitive-aware stress testing），如ICE基准测试所示，对于评估先进AI系统的真正韧性和安全性至关重要。"
                },
            ]
        },
    ],
    "2025-09-23": [
        {
            "name": "Artificial Intelligence",
            "count": 13,
            "papers": [
                {
                    "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration",
                    "arxiv_id": "2509.19236",
                    "authors": "Chunhao Tian, Yutong Wang, Xuebo Liu, Zhexuan Wang, Liang Ding, Miao Zhang, Min Zhang",
                    "summary": "Proper initialization is crucial for any system, particularly in multi-agent systems (MAS), where it plays a pivotal role in determining both the system's efficiency and effectiveness. However, existing MAS initialization methods do not fully account for the collaborative needs of the generated agents in subsequent stages. Inspired by the principles of effective team composition, we propose AgentInit, which aims to optimize the structure of agent teams. Specifically, in addition to multi-round interactions and reflections between agents during agent generation, AgentInit incorporates a Natural Language to Format mechanism to ensure consistency and standardization. Balanced team selection strategies using Pareto principles are subsequently applied to jointly consider agent team diversity and task relevance to promote effective and efficient collaboration and enhance overall system performance. Experiments show that AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving an overall performance improvement of up to 1.2 and 1.6, respectively, while also significantly reducing token consumption. Further analysis confirms its strong transferability to similar tasks and verifies the effectiveness of its key components, demonstrating its capability and adaptability as a reliable MAS initialization method. Source code and models are available at https://github.com/1737423697/AgentInit.",
                    "category": "cs.AI",
                    "filter_reason": "我根据筛选标准对这篇论文进行了全面分析，认为它符合研究范围。以下是详细判断过程： 第一步核心判断：这篇论文的核心是提出AgentInit，一种用于初始化基于大语言模型的多智能体系统的方法。它通过多样性和专业性的协调来优化智能体团队的结构，从而提高系统协作效率和问题解决能力。这属于\"智能体协作框架\"的研究范畴，旨在增强大语言模型的通用能力，而非将其作为工具应用于特定领域。因此，根据第一步标准，应予以保留。 第二步正面指标：论文包含多个相关主题。首先，核心概念方面明确研究\"LLM-based Multi-Agent Systems\"；其次，在新兴范式方面，论文聚焦于多智能体系统的研究；虽然论文没有直接提及推理、规划或强化学习等术语，但其优化多智能体协作的目标本质上是为了提升系统的问题解决能力，这与通用推理能力相关。 第三步排除标准：论文不涉及任何排除领域。它没有关注多模态与视觉内容，不针对医疗、化学、生物等特定应用领域，也不讨论模型可靠性方面如水印、安全等问题。相反，论文强调其方法在\"各种框架和任务\"上的适用性，表明其通用性。 第四步特殊情况处理：论文研究的是智能体协作框架，但提出的是一种通用的初始化方法，而非针对特定领域的应用。它旨在通过优化智能体团队的初始化来增强LLM在多智能体环境下的通用协作能力，因此符合保留条件。 综上所述，这篇论文的核心贡献是提出一种通用的多智能体系统初始化方法，通过优化团队结构和协作机制来增强大语言模型的通用问题解决能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决多智能体系统(MAS)初始化过程中智能体协作效率低下的问题。针对现有方法未充分考虑智能体协作需求的问题，我们提出了一种AgentInit方法，通过标准化智能体生成和平衡团队选择两个模块，联合优化智能体多样性和任务相关性。在MMLU、GSM8K等多个基准测试上验证，AgentInit性能提升高达1.2和1.6，同时显著降低token消耗，展现出强大的迁移能力和适应性。",
                    "summary_translation": "合理的初始化对任何系统都至关重要，尤其是在多智能体系统（multi-agent systems, MAS）中，初始化在决定系统效率与有效性方面起着关键作用。然而，现有的MAS初始化方法未能充分考虑所生成智能体在后续阶段中的协作需求。受高效团队构成原则的启发，我们提出了AgentInit，旨在优化智能体团队的结构。具体而言，除了在智能体生成过程中引入多轮智能体间的交互与反思机制外，AgentInit还引入了一种自然语言到格式化输出的机制（Natural Language to Format mechanism），以确保输出的一致性与标准化。随后，采用基于帕累托原则（Pareto principles）的平衡团队选择策略，综合考虑智能体团队的多样性（diversity）与任务相关性（task relevance），以促进高效且有效的协作，提升系统整体性能。实验结果表明，AgentInit在多种框架和任务下均持续优于当前最先进的初始化方法和预定义策略，整体性能分别提升了最高达1.2和1.6倍，同时显著降低了token消耗。进一步分析验证了其在相似任务中的强可迁移性，并确认了其核心组件的有效性，展示了AgentInit作为一种可靠MAS初始化方法的卓越能力与适应性。源代码与模型可在 https://github.com/1737423697/AgentInit 获取。"
                },
                {
                    "title": "Code Driven Planning with Domain-Adaptive Critic",
                    "arxiv_id": "2509.19077",
                    "authors": "Zikang Tian, Shaohui Peng, Du Huang, Jiaming Guo, Ruizhi Chen, Rui Zhang, Xishan Zhang, Yuxuan Guo, Zidong Du, Qi Guo, Ling Li, Yewen Pu, Xing Hu, Yunji Chen",
                    "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI agents in sequential decision-making problems, leveraging their extensive world knowledge. However, the gap between their general knowledge and environment-specific requirements often leads to inaccurate plans. To address this, existing approaches rely on frequent LLM queries to iteratively refine plans based on immediate environmental feedback, which incurs substantial query costs. However, this refinement is typically guided by short-term environmental feedback, limiting LLMs from developing plans aligned with long-term rewards. We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of relying on frequent queries, CoPiC employs LLMs to generate a diverse set of high-level planning programs, which iteratively produce and refine candidate plans. A trained domain-adaptive critic then evaluates these candidates and selects the one most aligned with long-term rewards for execution. Using high-level planning programs as planner and domain-adaptive critic as estimator, CoPiC improves planning while significantly reducing query costs. Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in query costs.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究课题。根据筛选标准，我进行了如下分析： 第一步核心判断：论文的本质是改进LLM的规划能力（planning），这属于通用推理能力的核心组成部分。论文提出CoPiC方法，通过让LLM生成高级规划程序并配合领域自适应批评者来提升规划质量，这是对LLM基础能力的增强，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确关注Large Language Models (LLMs)作为任务规划器 - 能力方向：重点研究planning能力，这是推理能力的重要组成部分 - 新兴范式：涉及LLM-based agents，将LLM用于AI智能体的序列决策问题 第三步排除标准：论文不聚焦于排除的领域： - 虽然在ALFWorld、NetHack和StarCraft II等特定环境中进行实验，但这些仅作为验证方法有效性的测试平台，论文核心是提出通用规划框架，而非针对特定领域的应用 - 不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究 第四步特殊情况处理：论文提出的是一种通用的智能体规划框架，旨在提升LLM的规划能力，减少查询成本并提高与长期奖励的一致性，这属于应保留的情况。 论文的核心贡献是CoPiC方法，它通过减少LLM查询频率并引入领域自适应批评者来评估候选计划，从而提升LLM的规划能力。这直接关系到提升LLM的通用推理能力，特别是规划和决策方面的能力，符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型(LLMs)作为任务规划器时通用知识与特定环境需求不匹配导致的高查询成本和长期规划问题。针对复杂决策环境，我们提出了一种Code Driven Planning with Domain-Adaptive Critic (CoPiC)框架，结合LLM生成的多样化规划程序和领域自适应评论器选择最优计划，并在ALFWorld、NetHack和StarCraft II Unit Building环境中通过成功率和token成本验证了其有效性。",
                    "summary_translation": "大语言模型（Large Language Models, LLMs）因其丰富的世界知识，已被广泛用作人工智能代理在序贯决策问题中的任务规划器。然而，LLMs的通用知识与特定环境需求之间存在差距，常常导致生成的规划不准确。为应对这一问题，现有方法依赖频繁调用LLM，根据即时环境反馈迭代优化规划，但这种方式带来了高昂的查询成本。此外，此类优化通常仅依赖短期环境反馈，限制了LLM生成与长期奖励一致的规划能力。\n\n本文提出**基于代码驱动与领域自适应评判器的规划方法**（Code Driven Planning with Domain-Adaptive Critic, CoPiC）。与频繁查询LLM的方法不同，CoPiC利用LLM生成一组多样化的高层规划程序（high-level planning programs），这些程序可迭代地生成并优化候选规划。随后，一个经过训练的领域自适应评判器（domain-adaptive critic）对这些候选规划进行评估，并选择最符合长期奖励的方案予以执行。通过将高层规划程序作为规划器、领域自适应评判器作为评估器，CoPiC在显著降低查询成本的同时提升了规划质量。\n\n在ALFWorld、NetHack和StarCraft II Unit Building三个基准任务上的实验结果表明，CoPiC优于当前先进的基于LLM的基线方法AdaPlanner和Reflexion，平均实现了（1）23.33%的成功率提升，以及（2）91.27%的查询成本降低。"
                },
                {
                    "title": "LongCat-Flash-Thinking Technical Report",
                    "arxiv_id": "2509.18883",
                    "authors": "Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chengcheng Han, Chenhui Yang, Chi Zhang, Chong Peng, Chuyu Zhang, Cong Chen, Fengcun Li, Gang Xu, Guoyuan Lin, Hao Jiang, Hao Liang, Haomin Fu, Haoxiang Ma, Hong Liu, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiahao Liu, Jiahuan Li, Jialin Liu, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiaqi Sun, Jiaqi Zhang, Jiarong Shi, Jiawei Yang, Jingang Wang, Jinrui Ding, Jun Kuang, Jun Xu, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Li Wei, Liang Shi, Lin Qiu, Lingbin Kong, Lingchuan Liu, Linsen Guo, Longfei An, Mai Xia, Meng Zhou, Mengshen Zhu, Peng Pei, Pengcheng Jia, Qi Gu, Qi Guo, Qiong Huang, Quan Chen, Quanchi Weng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shanglin Lei, Shuai Du, Shuaikang Liu, Shuang Zhou, Shuhao Hu, Siyu Xu, Songshan Gong, Tao Liang, Tianhao Hu, Wei He, Wei Shi, Wei Wang, Wei Wu, Wei Zhuo, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Xi Su, Xiangcheng Liu, Xiangyu Xi, Xiangzhou Huang, Xiao Liu, Xiaochen Jiang, Xiaowei Shi, Xiaowen Shi, Xiaoyu Li, Xin Chen, Xinyue Zhao, Xuan Huang, Xuemiao Zhang, Xuezhi Cao, Xunliang Cai, Yajie Zhang, Yang Chen, Yang Liu, Yang Liu, Yang Zheng, Yaoming Wang, Yaqi Huo, Yerui Sun, Yifan Lu, Yiyang Li, Youshao Xiao, Yuanzhe Lei, Yuchen Xie, Yueqing Sun, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunke Zhao, Yuqing Ding, Yuwei Jiang, Zhaohua Yang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhongda Su, Ziran Li, Ziwen Wang, Ziyuan Zhuang, Zongyu Wang, Zunyuan Yang",
                    "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities are cultivated through a meticulously crafted training process, beginning with long Chain-of-Thought (CoT) data cold-start and culminating in large-scale Reinforcement Learning (RL). We first employ a well-designed cold-start training strategy, which significantly enhances the reasoning potential and equips the model with specialized skills in both formal and agentic reasoning. Then, a core innovation is our domain-parallel training scheme, which decouples optimization across distinct domains (e.g., STEM, Code, Agentic) and subsequently fuses the resulting expert models into a single, nearly Pareto-optimal model. This entire process is powered by our Dynamic ORchestration for Asynchronous rollout (DORA) system, a large-scale RL framework that delivers a greater than threefold training speedup over synchronous methods on tens of thousands of accelerators. As a result, LongCat-Flash-Thinking achieves state-of-the-art performance among open-source models on a suite of complex reasoning tasks. The model exhibits exceptional efficiency in agentic reasoning, reducing average token consumption by 64.5% (from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We release LongCat-Flash-Thinking to promote further advances in reasoning systems and agentic AI research.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究范围，其核心贡献是提升大语言模型本身的通用推理能力。首先，论文的本质是关于改进LLM的基础推理能力，提出了新的训练范式，包括长思维链(CoT)数据冷启动和大规模强化学习(RL)方法，这直接符合筛选标准中的保留条件。其次，论文包含了多个正面指标：明确研究大语言模型(LLMs)的推理能力，涉及强化学习训练方法，并探讨了智能体推理(agentic reasoning)这一新兴范式。第三，论文没有聚焦于任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。特别值得注意的是，虽然论文提到了STEM、代码和智能体等不同领域，但这是作为其领域并行训练方案的一部分，目的是增强模型的通用推理能力，而非将LLM应用于特定领域解决问题。论文的核心创新——领域并行训练方案和DORA系统——都是为了提升模型的基础推理能力，使其在复杂推理任务上达到最先进性能。因此，这篇论文明确符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在提升大型语言模型的推理能力。针对复杂推理任务场景，我们提出了LongCat-Flash-Thinking，一个5600亿参数的MoE推理模型，通过长链思维数据冷启动和大规模强化学习培养高级推理能力，核心创新包括领域并行训练方案和DORA系统。在HMMT-25、ARC-AGI、τ²-Bench等多个推理基准上通过Mean@32、Pass@1等指标验证了其有效性，在AIME-25上实现了64.5%的token消耗降低。",
                    "summary_translation": "我们提出了LongCat-Flash-Thinking，一个高效的5600亿参数开源专家混合（Mixture-of-Experts, MoE）推理模型。其先进能力是通过精心设计的训练过程培养的，从长思维链（Chain-of-Thought, CoT）数据冷启动开始，到大规模强化学习（Reinforcement Learning, RL）结束。我们首先采用精心设计的冷启动训练策略，显著提升了推理潜力，并使模型具备形式推理和智能体推理（agentic reasoning）的专业技能。然后，核心创新是我们的领域并行训练方案，该方案解耦了不同领域（如STEM、代码、智能体）的优化，随后将产生的专家模型融合成单一的接近帕累托最优（Pareto-optimal）模型。整个过程由我们的动态异步编排（Dynamic ORchestration for Asynchronous rollout, DORA）系统驱动，这是一个大规模强化学习框架，在数万个加速器上比同步方法实现了超过三倍的训练加速。因此，LongCat-Flash-Thinking在一系列复杂推理任务上实现了开源模型中的最先进性能。该模型在智能体推理方面表现出卓越的效率，在AIME-25上平均令牌消耗减少了64.5%（从19,653降至6,965），同时不降低任务准确性。我们发布LongCat-Flash-Thinking以促进推理系统和智能体人工智能（agentic AI）研究的进一步发展。"
                },
                {
                    "title": "MAPO: Mixed Advantage Policy Optimization",
                    "arxiv_id": "2509.18849",
                    "authors": "Wenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, Huanjin Yao, Guancheng Wan, Ke Liang, Wenwen He, Mingjun Li, Leszek Rutkowski, Mang Ye, Bo Du, Dacheng Tao",
                    "summary": "Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为MAPO（Mixed Advantage Policy Optimization）的新强化学习策略，用于改进GRPO（Group Relative Policy Optimization）方法，以解决其在推理任务中遇到的advantage reversion和advantage mirror问题。论文通过引入advantage percent deviation和动态重加权advantage function来优化不同查询样本间的advantage分配，从而提升基础模型在推理任务上的性能。 这完全符合研究目标中的\"改进LLM的基础能力\"和\"提出新的训练范式\"，特别是强化学习优化方面的研究。论文明确关注\"reasoning tasks\"，这是筛选标准中的核心能力方向之一。同时，论文属于强化学习（RL）训练方法的研究，这也是正面指标中明确提到的重要内容。 论文不属于任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。它专注于提升基础模型在通用推理任务上的性能，而不是将模型应用到特定领域。因此，这篇论文应该被保留，它对提升大语言模型的通用推理能力有直接贡献。",
                    "summary2": "本文旨在解决GRPO中优势函数面临的\"优势反转\"和\"优势镜像\"问题。针对不同轨迹确定性的样本，我们提出了一种混合优势策略优化(MAPO)方法，通过引入优势百分比偏差处理高确定性轨迹，并基于轨迹确定性动态重新加权优势函数。在Geo3K和EmoSet等多个数据集上使用Qwen2.5-VL-7B架构，通过准确率等指标验证了MAPO能够有效提升基础模型在推理任务上的稳定性和准确性。",
                    "summary_translation": "基础模型（foundation models）在强化学习领域的最新进展，如群组相对策略优化（Group Relative Policy Optimization, GRPO），显著提升了基础模型在推理任务上的性能。值得注意的是，优势函数（advantage function）作为GRPO中的核心机制，用于排序轨迹（trajectory）的重要性。然而，现有探索同时遇到优势反转（advantage reversion）和优势镜像（advantage mirror）问题，这些问题阻碍了在不同查询样本（query samples）间进行合理的优势分配。在这项工作中，我们提出了一种简单但有效的GRPO策略，即混合优势策略优化（Mixed Advantage Policy Optimization, MAPO）。我们揭示了轨迹以不同的确定性（certainty）出现，并为具有高确定性轨迹的样本提出了优势百分比偏差（advantage percent deviation）。此外，我们对具有不同轨迹确定性的样本动态重新加权优势函数，从而自适应地配置优势函数以考虑样本特定特征。与相关最先进（state-of-the-art）方法的比较，以及对不同优势变体（advantage variants）的消融研究（ablation studies），验证了我们方法的有效性。"
                },
                {
                    "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution",
                    "arxiv_id": "2509.18868",
                    "authors": "Dianxing Zhang, Wendong Li, Kani Song, Jiaye Lu, Gang Li, Liuchun Yang, Sheng Li",
                    "summary": "Under a unified operational definition, we define LLM memory as a persistent state written during pretraining, finetuning, or inference that can later be addressed and that stably influences outputs. We propose a four-part taxonomy (parametric, contextual, external, procedural/episodic) and a memory quadruple (location, persistence, write/access path, controllability). We link mechanism, evaluation, and governance via the chain write -> read -> inhibit/update. To avoid distorted comparisons across heterogeneous setups, we adopt a three-setting protocol (parametric only, offline retrieval, online retrieval) that decouples capability from information availability on the same data and timeline. On this basis we build a layered evaluation: parametric (closed-book recall, edit differential, memorization/privacy), contextual (position curves and the mid-sequence drop), external (answer correctness vs snippet attribution/faithfulness), and procedural/episodic (cross-session consistency and timeline replay, E MARS+). The framework integrates temporal governance and leakage auditing (freshness hits, outdated answers, refusal slices) and uncertainty reporting via inter-rater agreement plus paired tests with multiple-comparison correction. For updating and forgetting, we present DMM Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC), and RAG to form an auditable loop covering admission thresholds, rollout, monitoring, rollback, and change audits, with specs for timeliness, conflict handling, and long-horizon consistency. Finally, we give four testable propositions: minimum identifiability; a minimal evaluation card; causally constrained editing with verifiable forgetting; and when retrieval with small-window replay outperforms ultra-long-context reading. This yields a reproducible, comparable, and governable coordinate system for research and deployment.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究大语言模型的记忆机制，提出了统一的操作定义、四部分分类法（参数化、上下文、外部、程序性/情景性）和记忆四元组（位置、持久性、写入/访问路径、可控性）。虽然论文没有直接讨论推理能力，但记忆是推理的基础能力之一，模型需要有效记忆信息并在需要时检索使用，才能进行复杂的推理任务。论文链接了机制、评估和治理，提出了三设置协议和分层评估框架，以及更新和遗忘策略（如DMM Gov），这些都是为了增强LLM的基础能力，从而间接提升其通用推理能力。论文不符合排除标准，没有聚焦于多模态、特定应用领域或模型可靠性的应用层面。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
                    "summary2": "本文旨在 [建立大语言模型记忆的统一分析框架，解决记忆概念边界模糊和评估碎片化等挑战]。针对 [LLM记忆全生命周期]，我们提出了一种 [四分类记忆类型（参数、上下文、外部和程序/情景记忆）和三设置并行评估协议]，并在 [多种公开数据集和benchmark] 上通过 [分层评估指标和DMM-Gov动态治理框架] 验证了其有效性。",
                    "summary_translation": "在统一的操作定义下，我们将LLM记忆（大语言模型记忆）定义为在预训练、微调或推理过程中写入的持久状态，该状态可被后续访问并稳定影响输出。我们提出了一个四部分分类法（参数型parametric、上下文型contextual、外部型external、程序型/情景型procedural/episodic）和一个记忆四元组（位置location、持久性persistence、写入/访问路径write/access path、可控性controllability）。我们通过写入->读取->抑制/更新的链条将机制、评估和治理联系起来。为避免在不同设置间产生扭曲的比较，我们采用了三设置协议（仅参数型parametric only、离线检索offline retrieval、在线检索online retrieval），该协议在同一数据和时间线上将能力与信息可用性解耦。在此基础上，我们构建了分层评估：参数型评估（闭卷召回closed-book recall、编辑差异edit differential、记忆化/隐私memorization/privacy）、上下文型评估（位置曲线和序列中段下降position curves and the mid-sequence drop）、外部型评估（答案正确性与片段归因/忠实性answer correctness vs snippet attribution/faithfulness），以及程序型/情景型评估（跨会话一致性和时间线重放cross-session consistency and timeline replay, E MARS+）。该框架整合了时间治理和泄漏审计（新鲜度命中freshness hits、过时答案outdated answers、拒绝切片refusal slices）以及通过评估者间一致性加上多重比较校正的配对测试进行的不确定性报告。针对更新和遗忘，我们提出了DMM Gov（动态记忆管理治理）：协调DAPT/TAPT（领域自适应预训练/任务自适应预训练）、PEFT（参数高效微调）、模型编辑（ROME, MEND, MEMIT, SERAC）和RAG（检索增强生成），形成一个可审计的循环，涵盖准入阈值、部署、监控、回滚和变更审计，并规定了及时性、冲突处理和长期一致性的规范。最后，我们提出了四个可测试的命题：最小可识别性；最小评估卡片；具有可验证遗忘的因果约束编辑；以及何时小窗口重放的检索优于超长上下文阅读。这产生了一个可重现、可比较和可治理的研究与部署坐标系。"
                },
                {
                    "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models",
                    "arxiv_id": "2509.18771",
                    "authors": "Xingkun Yin, Kaibin Huang, Dong In Kim, Hongyang Du",
                    "summary": "Scaling model size, training data, and compute power have driven advances in large language models (LLMs), but these approaches are reaching saturation as human-generated text is exhausted and further gains diminish. We propose experience scaling, a framework for continuous post-deployment evolution for LLMs through autonomous interaction with the environment and collaborative sharing of accumulated experience. The framework captures raw interactions, distills them into compact, reusable knowledge, and periodically refines stored content to preserve relevance and efficiency. We validate the framework in simulated real-world scenarios involving generalization to previously unseen but related tasks, repetitive queries, and over-saturated knowledge stores. Across all settings, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations. These results demonstrate that structured post-deployment learning can extend LLM capabilities beyond the limits of static human-generated data, offering a scalable path for continued intelligence progress.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。在第一步核心判断中，论文本质上是提出一种名为\"经验缩放\"(Experience Scaling)的新框架，用于大语言模型部署后的持续进化，这直接符合\"改进LLM的基础能力、提出新的训练范式\"的保留标准。论文关注LLM通过自主与环境交互和协作共享积累的经验来提升自身能力，这是一种增强LLM通用推理能力的方法论研究。 在第二步正面指标分析中，论文明确包含核心概念\"Large Language Models (LLMs)\"；涉及训练方法中的\"evolution\"和\"self-evolve\"概念（标题中直接提及\"Post-Deployment Evolution\"）；同时提到\"autonomous interaction with the environment\"暗示了agent概念；论文强调\"对以前未见但相关任务的泛化\"，这间接涉及到推理和问题解决能力。 在第三步排除标准检查中，论文完全不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容。 在第四步特殊情况处理中，论文提到的\"autonomous interaction with the environment\"属于通用智能体框架，用于增强LLM的通用问题解决能力，而非应用于特定领域，因此应该保留。 论文的核心贡献是提出了一种让LLM通过部署后的持续学习和经验积累来提升自身通用能力的框架，这种方法超越了静态人类生成数据的限制，为提升LLM的通用推理能力提供了新的路径，完全符合我的研究目标。",
                    "summary2": "本文旨在解决大型语言模型（LLM）在传统扩展方法达到饱和后的持续进化问题。针对部署后的LLM与环境交互数据，我们提出了一种Experience Scaling框架，通过捕获原始交互、提炼为可重用知识并定期优化存储内容，实现LLM的自主进化。在MMLU和SciQ基准上通过准确性、时间效率和泛化能力等指标验证了其有效性，结果表明该框架能提高准确性、维持长期性能并在新场景中保持收益。",
                    "summary_translation": "扩大模型规模、训练数据和计算能力推动了大型语言模型（large language models, LLMs）的进步，但随着人类生成文本的耗尽和进一步收益的减少，这些方法正达到饱和状态。我们提出了经验扩展（experience scaling）框架，这是一个通过与环境自主交互和协作共享积累经验，实现大型语言模型部署后持续进化的框架。该框架捕获原始交互，将其提炼为紧凑、可重用的知识，并定期优化存储内容以保持相关性和效率。我们在模拟的真实世界场景中验证了该框架，这些场景涉及对以前未见但相关任务的泛化、重复性查询和过度饱和的知识存储。在所有设置中，经验扩展（experience scaling）提高了准确性，随时间推移保持性能，并在应用于新情况时保持收益。这些结果表明，结构化的部署后学习可以将大型语言模型的能力扩展到静态人类生成数据的限制之外，为持续智能进步提供了一条可扩展的路径。"
                },
                {
                    "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation",
                    "arxiv_id": "2509.18565",
                    "authors": "Mitchell Piehl, Dillon Wilson, Ananya Kalita, Jugal Kalita",
                    "summary": "Large Language Models (LLMs) excel at various tasks, including problem-solving and question-answering. However, LLMs often find Math Word Problems (MWPs) challenging because solving them requires a range of reasoning and mathematical abilities with which LLMs seem to struggle. Recent efforts have helped LLMs solve more complex MWPs with improved prompts. This study proposes a novel method that initially prompts an LLM to create equations from a decomposition of the question, followed by using an external symbolic equation solver to produce an answer. To ensure the accuracy of the obtained answer, inspired by an established recommendation of math teachers, the LLM is instructed to solve the MWP a second time, but this time with the objective of estimating the correct answer instead of solving it exactly. The estimation is then compared to the generated answer to verify. If verification fails, an iterative rectification process is employed to ensure the correct answer is eventually found. This approach achieves new state-of-the-art results on datasets used by prior published research on numeric and algebraic MWPs, improving the previous best results by nearly two percent on average. In addition, the approach obtains satisfactory results on trigonometric MWPs, a task not previously attempted to the authors' best knowledge. This study also introduces two new datasets, SVAMPClean and Trig300, to further advance the testing of LLMs' reasoning abilities.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，该论文的本质是提升LLM的数学推理能力，属于改进LLM基础能力和增强其逻辑、数学推理等通用能力的范畴。论文提出了一种结合方程生成和估计验证的新方法，通过外部符号方程求解器和LLM的估计能力相互验证，提高数学问题求解的准确性，这直接针对LLM在推理方面的能力提升。 其次，论文满足多个正面指标：核心概念明确涉及Large language models (LLMs)；能力方向专注于数学推理(math reasoning)，这正是通用推理能力的重要组成部分；同时采用了工具使用(tool use)的新兴范式，即使用外部符号方程求解器来辅助LLM解决问题。 第三，论文不涉及任何排除标准中的领域：没有涉及多模态与视觉内容；虽然专注于数学文字问题，但数学推理被视为基础能力而非特定应用领域；也没有涉及模型可靠性的应用层面问题。 最后，在特殊和模糊情况处理上，论文中使用的外部符号方程求解器作为工具，是为了增强LLM的通用数学问题解决能力，而不是应用在特定领域，因此符合保留条件。 综合来看，这篇论文的核心贡献是提出了一种提升LLM数学推理能力的新方法，通过估计验证和方程生成相结合的方式，显著提高了LLM在解决数学文字问题上的表现，这完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在提高大型语言模型解决数学应用题的准确性。针对数学应用题求解场景，我们提出了一种EVoSS方法，结合方程生成和估计验证技术，通过符号求解器获得精确解后，利用估计值进行验证，并在GSM8K、SVAMP和Algebra等数据集上通过精确匹配指标验证了其有效性，实现了平均近2%的性能提升，并创建了新的Trig300数据集用于三角函数问题测试。",
                    "summary_translation": "\n大型语言模型（Large Language Models, LLMs）在各种任务上表现出色，包括问题解决和问答。然而，大型语言模型通常发现数学应用题（Math Word Problems, MWPs）具有挑战性，因为解决这些问题需要一系列推理和数学能力，而这些能力似乎是大型语言模型所欠缺的。近来的努力通过改进提示词（prompts）帮助大型语言模型解决更复杂的数学应用题。本研究提出了一种新方法，首先提示大型语言模型从问题的分解中创建方程，然后使用外部符号方程求解器（symbolic equation solver）来产生答案。为了确保获得答案的准确性，受数学教师既定建议的启发，大型语言模型被指示第二次解决数学应用题，但这次的目标是估计正确答案，而不是精确求解。然后将估计值与生成的答案进行比较以进行验证。如果验证失败，则采用迭代修正过程（iterative rectification process）以确保最终找到正确答案。这种方法在先前发表的关于数值和代数数学应用题的研究所使用的数据集上取得了新的最先进结果，平均将之前的最佳结果提高了近两个百分点。此外，该方法在三角数学应用题（trigonometric MWPs）上取得了令人满意的结果，据作者所知，这是之前未曾尝试过的任务。本研究还介绍了两个新数据集SVAMPClean和Trig300，以进一步推进对大型语言模型推理能力的测试。"
                },
                {
                    "title": "Gödel Test: Can Large Language Models Solve Easy Conjectures?",
                    "arxiv_id": "2509.18383",
                    "authors": "Moran Feldman, Amin Karbasi",
                    "summary": "Recent announcements from frontier AI model labs have highlighted strong results on high-school and undergraduate math competitions. Yet it remains unclear whether large language models can solve new, simple conjectures in more advanced areas of mathematics. We propose the Gödel Test: evaluating whether a model can produce correct proofs for very simple, previously unsolved conjectures. To this end, we study the performance of GPT-5 on five conjectures in combinatorial optimization. For each problem, we provided one or two source papers from which the conjecture arose, withheld our own conjecture, and then assessed the model's reasoning in detail. On the three easier problems, GPT-5 produced nearly correct solutions; for Problem 2 it even derived a different approximation guarantee that, upon checking, refuted our conjecture while providing a valid solution. The model failed on Problem 4, which required combining results from two papers. On Problem 5, a harder case without a validated conjecture, GPT-5 proposed the same algorithm we had in mind but failed in the analysis, suggesting the proof is more challenging than expected. Although our sample is small, the results point to meaningful progress on routine reasoning, occasional flashes of originality, and clear limitations when cross-paper synthesis is required. GPT-5 may represent an early step toward frontier models eventually passing the Gödel Test.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是评估和探讨大语言模型(GPT-5)的数学推理能力，特别是解决简单数学猜想的能力，这直接涉及LLM的基础推理能力提升，而非将LLM作为工具应用于特定领域。论文提出的\"Gödel Test\"旨在评估模型产生正确证明的能力，这属于通用推理能力的核心范畴。 其次，论文满足多个正面指标：明确以大型语言模型(LLMs)为研究对象；专注于推理能力(reasoning)，特别是数学推理(math reasoning)；涉及问题解决(problem-solving)等关键能力方向。 第三，论文不符合任何排除标准。虽然研究内容涉及数学领域，但数学在这里是作为评估LLM推理能力的测试场，而非作为特定应用领域。论文的核心不是解决数学问题本身，而是研究LLM的推理能力表现和局限。 论文的核心贡献在于提出了一种评估LLM高级推理能力的新方法(Gödel Test)，并通过实验揭示了当前模型在数学推理方面的进展、原创性闪光点以及局限性。这直接服务于提升LLM通用推理能力的研究目标，因此应该被保留。",
                    "summary2": "本文旨在评估大型语言模型解决简单数学猜想的能力。针对组合优化领域中的五个未解决猜想，我们提出了Gödel Test评估框架，并通过检查GPT-5生成的证明正确性验证了其性能。实验表明GPT-5在需要单一推理路径的问题上表现良好，但在需要跨论文综合推理的问题上存在明显局限。",
                    "summary_translation": "前沿AI模型实验室最近的公告强调了在高中和本科数学竞赛中取得的显著成果。然而，大语言模型（large language models）是否能够解决更高阶数学领域中的新颖简单猜想（conjectures），目前仍不清楚。我们提出了哥德尔测试（Gödel Test）：评估模型是否能够为非常简单但先前未解决的猜想（previously unsolved conjectures）产生正确的证明。为此，我们研究了GPT-5在组合优化（combinatorial optimization）领域中的五个猜想上的表现。对于每个问题，我们提供了一到两篇提出该猜想的源论文（source papers），保留了我们自己的猜想，然后详细评估了模型的推理过程。\n\n在三个较简单的问题上，GPT-5产生了几乎正确的解决方案；对于问题2，它甚至推导出了一个不同的近似保证（approximation guarantee），经过检查，这一保证反驳了我们的猜想，同时提供了一个有效的解决方案。该模型在问题4上失败了，该问题需要结合两篇论文的结果。在问题5上，这是一个没有已验证猜想（validated conjecture）的更难案例，GPT-5提出了与我们心中相同的算法（algorithm），但在分析（analysis）环节失败了，这表明证明比预期的更具挑战性。\n\n尽管我们的样本量很小，但结果表明在常规推理（routine reasoning）方面取得了有意义的进展，偶尔展现出创造性闪光（flashes of originality），而在需要跨论文综合（cross-paper synthesis）时则存在明显的局限性。GPT-5可能是前沿模型最终通过哥德尔测试（Gödel Test）的早期一步。"
                },
                {
                    "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints",
                    "arxiv_id": "2509.18382",
                    "authors": "Adarsha Balaji, Le Chen, Rajeev Thakur, Franck Cappello, Sandeep Madireddy",
                    "summary": "Test-time compute scaling has demonstrated the ability to improve the performance of reasoning language models by generating longer chain-of-thought (CoT) sequences. However, this increase in performance comes with a significant increase in computational cost. In this work, we investigate two compute constraint strategies: (1) reasoning length constraint and (2) model quantization, as methods to reduce the compute demand of reasoning models and study their impact on their safety performance. Specifically, we explore two approaches to apply compute constraints to reasoning models: (1) fine-tuning reasoning models using a length controlled policy optimization (LCPO) based reinforcement learning method to satisfy a user-defined CoT reasoning length, and (2) applying quantization to maximize the generation of CoT sequences within a user-defined compute constraint. Furthermore, we study the trade-off between the computational efficiency and the safety of the model.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究如何在计算约束条件下提升大型推理模型的推理能力和安全性，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。论文主要探讨了通过思维链（CoT）和强化学习（LCPO方法）来优化模型的推理能力，这属于改进LLM基础能力和提出新训练范式的范畴。论文明确关注推理能力这一核心指标，并研究了如何通过强化学习方法来控制推理长度，同时保持模型的安全性。论文不是将LLM作为工具应用到特定领域，而是关注LLM本身的通用推理能力提升。虽然论文涉及安全性研究，但这是从提升模型内在推理质量和可靠性的角度出发，而非仅作为应用层面的防御。因此，这篇论文完全符合筛选标准，应该被保留。",
                    "summary2": "本文旨在解决计算约束下大型推理模型（LRMs）的安全性与技能推理平衡问题。针对推理长度和计算资源限制场景，我们提出了两种计算约束策略：基于强化学习的长度控制策略优化（LCPO）和模型量化（GPTQ），并在科学、数学和安全推理基准（GPQA, MATH500, AIME, StrongReject）上通过pass@1和safe@1指标验证了其有效性。",
                    "summary_translation": "测试时计算扩展(test-time compute scaling)已证明能够通过生成更长的思维链(chain-of-thought, CoT)序列来提高推理语言模型(reasoning language models)的性能。然而，这种性能提升伴随着计算成本(computational cost)的显著增加。在本研究中，我们调查了两种计算约束策略(compute constraint strategies)：(1)推理长度约束(reasoning length constraint)和(2)模型量化(model quantization)，作为降低推理模型计算需求并研究其对安全性能(safety performance)影响的方法。具体而言，我们探索了两种将计算约束应用于推理模型的方法：(1)使用基于长度控制策略优化(length controlled policy optimization, LCPO)的强化学习(reinforcement learning)方法微调推理模型，以满足用户定义的CoT推理长度(user-defined CoT reasoning length)；(2)应用量化技术(quantization)以在用户定义的计算约束(user-defined compute constraint)内最大化CoT序列的生成。此外，我们还研究了模型的计算效率(computational efficiency)与安全性(safety)之间的权衡(trade-off)。"
                },
                {
                    "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization",
                    "arxiv_id": "2509.18851",
                    "authors": "Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu, Dexun Wang, Chunmei Xie, Weiqi Xiong, Xianzhou Zeng, Qixuan Zhou, Yadong Li, Xingzhong Xu",
                    "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs) across various tasks. However, GRPO, a representative RLVR algorithm, suffers from a critical limitation: when all responses within a group are either entirely correct or entirely incorrect, the model fails to learn from these homogeneous responses. This is particularly problematic for homogeneously incorrect groups, where GRPO's advantage function yields a value of zero, leading to null gradients and the loss of valuable learning signals. To overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy Optimization), an algorithm designed to convert homogeneous errors into robust learning signals. First, NGRPO introduces Advantage Calibration. This mechanism hypothesizes the existence of a virtual maximum-reward sample during advantage calculation, thereby altering the mean and variance of rewards within a group and ensuring that the advantages for homogeneously incorrect samples are no longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the update magnitude for positive samples while imposing stricter constraints on that of negative samples. This serves to stabilize the exploration pressure introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO, DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and AIME2025. These results validate NGRPO's ability to learn from homogeneous errors, leading to stable and substantial improvements in mathematical reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出NGRPO算法，一种改进的强化学习方法，用于增强大语言模型的推理能力。论文针对GRPO算法在处理同质响应组（全部正确或全部错误）时的局限性，提出了两个关键机制：Advantage Calibration和Asymmetric Clipping，使模型能够从同质错误中学习。这直接符合研究目标，因为：(1)论文本质上是改进LLM的基础推理能力，特别是数学推理能力；(2)它提出新的训练范式（强化学习优化）；(3)实验证明该方法能显著提升LLM在多个数学基准测试上的表现。论文不涉及特定应用领域、多模态研究或模型基础设施优化，完全符合\"致力于提高大语言模型本身的通用推理能力\"的核心目标。",
                    "summary2": "本文旨在解决 GRPO 算法在处理同质响应组时无法学习的问题。针对同质错误组导致零梯度的问题，我们提出了一种 NGRPO 方法，通过 Advantage Calibration 和 Asymmetric Clipping 两个核心机制将同质错误转换为稳健学习信号，并在 Qwen2.5-Math-7B 模型上通过 MATH500、AMC23 和 AIME2025 数学基准测试的 Pass@k AUC 指标验证了其有效性。",
                    "summary_translation": "RLVR (强化学习与验证推理) 已增强大型语言模型 (LLMs) 在各种任务中的推理能力。然而，作为代表性RLVR算法的GRPO (组相对策略优化) 存在一个关键限制：当组内所有响应要么完全正确要么完全错误时，模型无法从这些同质响应中学习。对于同质错误组，这一问题尤为严重，因为GRPO的优势函数 (advantage function) 会产生零值，导致零梯度 (null gradients) 并损失有价值的学习信号。为克服这一问题，我们提出了NGRPO (负增强组相对策略优化)，这是一种旨在将同质错误转化为稳健学习信号的算法。首先，NGRPO引入了优势校准 (Advantage Calibration) 机制。该机制假设在优势计算过程中存在一个虚拟最大奖励样本 (virtual maximum-reward sample)，从而改变组内奖励的均值和方差，确保同质错误样本的优势不再为零。其次，NGRPO采用非对称裁剪 (Asymmetric Clipping)，放宽对正样本 (positive samples) 的更新幅度限制，同时对负样本 (negative samples) 施加更严格的约束。这有助于稳定由优势校准引入的探索压力 (exploration pressure)。我们在Qwen2.5-Math-7B模型上的实验表明，NGRPO在MATH500、AMC23和AIME2025等数学基准测试 (mathematical benchmarks) 上显著优于PPO、GRPO、DAPO和PSR-NSR等基线方法 (baselines)。这些结果验证了NGRPO从同质错误中学习的能力，从而在数学推理方面实现了稳定且实质性的改进。我们的代码可在https://github.com/nangongrui-ngr/NGRPO获取。"
                },
                {
                    "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts",
                    "arxiv_id": "2509.18542",
                    "authors": "Qi Wang, Hanyang Peng, Yue Yu",
                    "summary": "Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To circumvent the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a single lightweight stage of router training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出了一种新的Mixture-of-Experts (MoE)模型构建方法Symphony-MoE，该方法通过整合多个不同但架构相同的预训练模型（如Llama2-Chat和Code Llama）来提高整体性能。论文不是将LLM作为工具应用到特定领域，而是关注如何改进LLM的基础架构和能力。它提出了一种两阶段框架来解决不同预训练模型在参数空间中的不协调问题，通过层感知融合策略和基于激活的功能对齐来协调这些模型，然后进行轻量级的路由器训练。论文提到其方法在\"多领域任务\"和\"分布外泛化\"方面取得了显著成果，这表明它关注的是模型的通用能力，而非特定领域应用。虽然论文没有直接提到推理、规划等具体能力，但它提出的架构改进本质上是为了提升LLM的通用能力，符合研究目标中\"改进LLM的基础能力、提出新的训练范式\"的要求。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决如何将多个架构相同但训练历史不同的预训练模型有效地整合成一个强大的MoE模型的问题。针对多个异构预训练模型，我们提出了一种Symphony-MoE两阶段框架，通过无训练的功能对齐和轻量级路由器训练来协调不同模型，并在MMLU、GSM8K、BBH、HumanEval、TruthfulQA和MedCQA等多个数据集上通过准确率等指标验证了其有效性。",
                    "summary_translation": "混合专家(Mixture-of-Experts, MoE)模型通过稀疏激活大型参数集实现可扩展性能，同时最小化计算开销。为规避从头训练MoE模型的过高成本，近期工作采用升级改造(upcycling)方法，通过将单个预训练密集模型的前馈网络(feed-forward network, FFN)层复制成专家来重用该模型。然而，这种方法限制了专家多样性，因为所有专家都源自于单个预训练密集模型。本文通过使用来源于多个架构相同但不同的预训练模型（如Llama2-Chat和Code Llama）的专家来构建强大的MoE模型，从而解决这一限制。\n\n一个关键挑战在于，这些源模型在参数空间(parameter space)中占据不同且不协调的区域，使得直接升级改造容易导致严重性能下降。为克服这一问题，我们提出了Symphony-MoE，这是一个新颖的两阶段框架，旨在将这些模型协调成一个单一、连贯的专家混合体。首先，我们以无训练方式建立这种协调：我们通过层感知融合策略构建共享骨干网络，并且关键的是，使用基于激活的功能对齐(activation-based functional alignment)来缓解专家之间的参数不对齐问题。随后，一个轻量级的路由器(router)训练阶段协调整个架构。\n\n实验表明，我们的方法成功整合了来自异构源的专家，构建出的MoE模型在多领域任务和分布外(out-of-distribution)泛化方面显著超越基线模型。"
                },
                {
                    "title": "Self-Evolving LLMs via Continual Instruction Tuning",
                    "arxiv_id": "2509.18133",
                    "authors": "Le Huang, Jiazheng Kang, Cheng Hou, Zhe Zhao, Zhenxiang Yan, Chuan Shi, Ting Bai",
                    "summary": "In real-world industrial settings, large language models (LLMs) must learn continually to keep pace with diverse and evolving tasks, requiring self-evolution to refine knowledge under dynamic data distributions. However, existing continual learning (CL) approaches, such as replay and parameter isolation, often suffer from catastrophic forgetting: training on new tasks degrades performance on earlier ones by overfitting to the new distribution and weakening generalization.We propose MoE-CL, a parameter-efficient adversarial mixture-of-experts framework for industrial-scale, self-evolving continual instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated LoRA expert per task to preserve task-specific knowledge via parameter independence, mitigating forgetting; and (2) a shared LoRA expert to enable cross-task transfer. To prevent transferring task-irrelevant noise through the shared pathway, we integrate a task-aware discriminator within a GAN. The discriminator encourages the shared expert to pass only task-aligned information during sequential training. Through adversarial learning, the shared expert acquires generalized representations that mimic the discriminator, while dedicated experts retain task-specific details, balancing knowledge retention and cross-task generalization and thereby supporting self-evolution.Extensive experiments on the public MTL5 benchmark and an industrial Tencent3 benchmark validate the effectiveness of MoE-CL for continual instruction tuning. In real-world A/B testing for content compliance review on the Tencent Video platform, MoE-CL reduced manual review costs by 15.3%. These results demonstrate that MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出MoE-CL框架，一种用于大语言模型自进化的参数高效对抗性混合专家框架。论文直接针对LLM的基础能力——持续学习和自进化能力进行研究，提出通过双专家设计（每个任务的专用LoRA专家和共享LoRA专家）以及GAN中的任务感知判别器来解决持续学习中的灾难性遗忘问题。这明显符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求，因为自进化能力是LLM通用推理能力的重要组成部分。论文虽然提到了在Tencent Video平台的应用案例，但这只是验证方法有效性的实验，论文的核心并不是针对特定应用领域的研究，而是提出了一种通用的训练范式来增强LLM的基础能力。论文明确涉及\"self-evolution\"这一正面指标，并且不涉及任何排除标准中的领域。因此，这篇论文完全符合筛选标准，应该被保留。",
                    "summary2": "本文旨在解决大型语言模型在持续学习中面临的灾难性遗忘问题，实现LLMs的自我进化能力。针对工业场景中多样化任务需求，我们提出了一种名为MoE-CL的对抗性LoRA专家混合框架，并在公共MTL5基准和工业Tencent3基准上通过Accuracy、Backward Transfer和Forward Transfer指标验证了其有效性。在腾讯视频平台的内容合规审查A/B测试中，MoE-CL将人工审核成本降低了15.3%。",
                    "summary_translation": "在真实工业环境中，大型语言模型（large language models, LLMs）必须持续学习以跟上多样化和不断发展的任务，需要在动态数据分布下进行自我进化（self-evolution）以完善知识。然而，现有的持续学习（continual learning, CL）方法，如回放（replay）和参数隔离（parameter isolation），常常遭受灾难性遗忘（catastrophic forgetting）：在新任务上的训练会因过度拟合新分布而降低在早期任务上的性能，并削弱泛化能力。\n\n我们提出了MoE-CL，一个参数高效的对抗性专家混合（adversarial mixture-of-experts）框架，用于工业规模、自我进化的LLM持续指令调优（continual instruction tuning）。MoE-CL采用双专家设计：（1）每个任务一个专用的LoRA专家，通过参数独立性保留任务特定知识，减轻遗忘；以及（2）一个共享的LoRA专家，以实现跨任务迁移。为防止通过共享路径传递任务无关噪声，我们在生成对抗网络（GAN）中集成了一个任务感知判别器（task-aware discriminator）。判别器鼓励共享专家在顺序训练过程中仅传递与任务对齐的信息。通过对抗学习（adversarial learning），共享专家获得模仿判别器的泛化表示，而专用专家保留任务特定细节，平衡知识保留和跨任务泛化，从而支持自我进化。\n\n在公开的MTL5基准测试和工业级Tencent3基准测试上的大量实验验证了MoE-CL在持续指令调优方面的有效性。在腾讯视频平台内容合规审查的真实A/B测试中，MoE-CL将人工审查成本降低了15.3%。这些结果表明，MoE-CL适用于大规模工业部署，特别是在持续适应和稳定迁移至关重要的场景中。"
                },
                {
                    "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization",
                    "arxiv_id": "2509.18116",
                    "authors": "Nathan Egbuna, Saatvik Gaur, Sunishchal Dev, Ashwinee Panda, Maheep Chaudhary",
                    "summary": "Test-time optimization remains impractical at scale due to prohibitive inference costs\\textemdash techniques like iterative refinement and multi-step verification can require $10$--$100\\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101\\% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础推理能力。论文提出了\"Amortized Latent Steering (ALS)\"方法，这是一种新的训练/推理范式，通过潜在空间引导来增强模型的推理能力，特别是数学推理能力。ALS将测试时优化的迭代过程转化为离线计算的向量，在推理过程中以恒定成本应用，从而在保持或提高推理性能的同时显著降低计算成本。这明确符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、多步推理等通用能力\"的保留标准。 其次，从正面指标来看，论文包含了多个相关主题： - 核心概念：论文虽然未在摘要中明确提到\"Large language models\"，但从上下文（如提到Chain-of-Thought, Self-Consistency等LLM相关技术）可以推断这是针对LLM的研究。 - 能力方向：论文明确在GSM8K和MATH-500这两个数学推理基准测试上评估方法，表明论文关注数学推理能力，并提到\"sophisticated reasoning techniques\"，进一步确认其与推理相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域（如医疗、化学、生物等）或模型可靠性（应用层面）的研究。 最后，论文不涉及特殊或模糊情况，如智能体/工具使用或幻觉/可解释性/安全等问题。 综上所述，这篇论文的核心贡献是提出了一种提高LLM数学推理效率的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决测试时优化计算成本过高的问题。针对数学推理任务的高计算开销场景，我们提出了一种摊销潜在转向（ALS）方法，通过离线计算成功与失败生成间的隐藏状态差异向量，并在推理时以恒定成本应用该向量校准模型表示。在GSM8K和MATH-500基准上通过效率-准确性权衡指标验证了其有效性，实现了2-5倍加速同时保持或超越基线性能。",
                    "summary_translation": "测试时优化（Test-time optimization）由于高昂的推理成本在大规模应用中仍然不切实际——诸如迭代细化（iterative refinement）和多步验证（multi-step verification）等技术可能需要比标准解码（standard decoding）多10-100倍的计算量。像LatentSeek这样的潜在空间测试时优化方法（Latent space test-time optimization methods）通过引导隐藏表示（steering hidden representations）提供了一种更直接的方法，但仍然需要昂贵的每次查询优化循环（per-query optimization loops）和多次反向传播（multiple backward passes）。我们提出了摊销潜在引导（Amortized Latent Steering, ALS），它将这种迭代优化折叠成一个在推理过程中以恒定成本应用的离线计算向量（offline-computed vector）。ALS计算成功生成与不成功生成之间的隐藏状态平均差异，然后使用这个方向来校准模型的隐藏表示（hidden representations）：当解码偏离成功流形（success manifold）时，ALS会将激活值（activations）推回该流形。在GSM8K和MATH-$500$基准测试中，ALS比迭代方法实现了2-5倍的加速，同时匹配或超过了贪心思维链（greedy Chain-of-Thought, CoT）和自我一致性（Self-Consistency）基线，使效率-准确率权衡（efficiency--accuracy trade-off）提高了高达101%。这些结果表明，潜在优化（latent optimization）的大部分好处可以离线捕获，使复杂的推理技术（sophisticated reasoning techniques）能够在生产部署中变得可行。代码可在~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}获取。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 13,
            "papers": [
                {
                    "title": "Reinforcement Learning on Pre-Training Data",
                    "arxiv_id": "2509.19249",
                    "authors": "Siheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang, Kun Li, Haoyuan Wu, Jiajia Wu, Zihao Zheng, Chenchen Zhang, Kun Shi, Kyrierl Deng, Qi Yi, Ruibin Xiong, Tingqiang Xu, Yuhao Jiang, Jianfeng Yan, Yuyuan Zeng, Guanghui Xu, Jinbao Xue, Zhijiang Xu, Zheng Fang, Shuai Li, Qibin Liu, Xiaoxue Li, Zhuoyu Li, Yangyu Tao, Fei Gao, Cheng Jiang, Bo Chao Wang, Kai Liu, Jianchen Zhu, Wai Lam, Wayyt Wang, Bo Zhou, Di Wang",
                    "summary": "The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$, $6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究目标，核心是提出一种名为RLPT（Reinforcement Learning on Pre-Training Data）的新训练范式，用于提升大语言模型的通用推理能力。从第一步核心判断来看，论文本质上是改进LLM的基础能力，提出新的强化学习训练方法，旨在增强模型的通用推理技能，而非将LLM应用于特定领域。论文明确提到RLPT\"鼓励在更广泛的上下文中探索更丰富的轨迹，从而培养更通用的推理技能\"，并在数学推理等通用能力基准测试中验证了有效性。 从第二步正面指标看，论文包含多个相关主题：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于推理能力(特别是数学推理)；训练方法上提出了基于强化学习的新范式(RLPT)，并与RLHF、RLVR等方法进行了比较。 第三步排除标准方面，论文不涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容，没有任何排除因素。 综上所述，这篇论文的核心贡献是提出一种新的训练范式来增强LLM的通用推理能力，完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决计算资源指数级增长与高质量文本数据有限增长之间的差距限制LLMs传统扩展方法的问题。针对预训练数据，我们提出了一种Reinforcement Learning on Pre-Training Data (RLPT)方法，采用next-segment reasoning目标，包括ASR和MSR任务，直接从预训练数据中获取奖励信号，并在通用领域和数学推理基准测试上通过准确率和Pass@k指标验证了其有效性。",
                    "summary_translation": "计算资源的指数级扩展与高质量文本数据的有限增长之间的日益扩大的差距，现在正限制着大型语言模型（LLMs）的传统扩展方法。为应对这一挑战，我们提出了预训练数据强化学习（Reinforcement Learning on Pre-Training data, RLPT），这是一种用于优化LLMs的新型训练时扩展范式。与先前主要通过监督学习（supervised learning）扩展训练的方法不同，RLPT使策略能够自主探索有意义的轨迹，从预训练数据中学习，并通过强化学习（reinforcement learning, RL）提升其能力。虽然现有的强化学习策略，如基于人类反馈的强化学习（reinforcement learning from human feedback, RLHF）和可验证奖励强化学习（reinforcement learning with verifiable rewards, RLVR）依赖于人类标注来构建奖励，但RLPT通过直接从预训练数据中导出奖励信号，消除了这种依赖性。具体而言，它采用下一段推理目标（next-segment reasoning objective），奖励策略基于前文上下文准确预测后续文本段。这种表述方式允许强化学习在预训练数据上进行扩展，鼓励在更广泛的上下文中探索更丰富的轨迹，从而培养更具泛化性的推理技能。在多个模型的通用领域和数学推理基准上进行的大量实验验证了RLPT的有效性。例如，当应用于Qwen3-4B-Base时，RLPT在MMLU、MMLU-Pro、GPQA-Diamond、KOR-Bench、AIME24和AIME25上分别实现了$3.0$、$5.1$、$8.1$、$6.0$、$6.6$和$5.3$的绝对提升。结果进一步展示了良好的扩展行为，表明随着更多计算资源的投入，持续提升的潜力巨大。此外，RLPT提供了坚实的基础，扩展了LLMs的推理边界，并增强了RLVR的性能。"
                },
                {
                    "title": "Online Process Reward Leanring for Agentic Reinforcement Learning",
                    "arxiv_id": "2509.19199",
                    "authors": "Xiaoqian Liu, Ke Wang, Yuchuan Wu, Fei Huang, Yongbin Li, Junge Zhang, Jianbin Jiao",
                    "summary": "Large language models (LLMs) are increasingly trained with reinforcement learning (RL) as autonomous agents that reason and act over long horizons in interactive environments. However, sparse and sometimes unverifiable rewards make temporal credit assignment extremely challenging. Recent work attempts to integrate process supervision into agent learning but suffers from biased annotation, reward hacking, high-variance from overly fine-grained signals or failtures when state overlap is rare. We therefore introduce Online Process Reward Learning (OPRL), a general credit-assignment strategy for agentic RL that integrates seamlessly with standard on-policy algorithms without relying on additional rollouts or explicit step labels. In OPRL, we optimize an implicit process reward model (PRM) alternately with the agent's policy to transform trajectory preferences into implicit step rewards through a trajectory-based DPO objective. These step rewards are then used to compute step-level advantages, which are combined with episode-level advantages from outcome rewards for policy update, creating a self-reinforcing loop. Theoretical findings guarantee that the learned step rewards are consistent with trajectory preferences and act as potential-based shaping rewards, providing bounded gradients to stabilize training. Empirically, we evaluate OPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as well as open-ended social interactions with unverfiable rewards in SOTOPIA. Crucially, OPRL shows superior performance over frontier LLMs and strong RL baselines across domains, achieving state-of-the-art results with higher sample-efficiency and lower variance during training. Further analysis also demonstrates the efficient exploration by OPRL using fewer actions, underscoring its potential for agentic learning in real-world scenarios.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究范围，核心贡献是提出一种新的强化学习方法(OPRL)来增强大语言模型作为智能体时的通用推理能力。 从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力的研究。它提出了\"在线过程奖励学习\"(OPRL)这一新的训练范式，专注于解决LLM作为自主智能体在长期任务中的信用分配问题，这直接属于提升LLM通用推理能力的范畴。 从第二步正面指标来看，论文涵盖了所有关键主题：明确以大语言模型(LLMs)为核心研究对象；关注模型的推理能力(\"reason and act over long horizons\")；采用强化学习作为主要训练方法；并聚焦于基于LLM的智能体(\"agentic reinforcement learning\")研究。 从第三步排除标准来看，虽然论文在评估中使用了WebShop、VisualSokoban等特定环境，但这些只是用来验证通用方法的应用场景，论文本身并不专注于任何特定应用领域或多模态研究，其核心贡献是通用的强化学习方法。 从第四步特殊和模糊情况处理来看，论文提出的是一种通用的智能体学习方法，旨在增强LLM的通用问题解决和推理能力，而非针对特定领域的应用。 综上所述，这篇论文通过提出新的强化学习范式来提升LLM的长期推理和决策能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型作为智能体在强化学习中面临的稀疏奖励和信用分配挑战。针对长时程交互任务，我们提出了一种在线过程奖励学习(OPRL)方法，通过优化隐式过程奖励模型将轨迹偏好转换为步骤级奖励，并在WebShop、VisualSokoban和SOTOPIA三个基准测试上通过成功率、分数和目标完成分数等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）越来越多地通过强化学习（Reinforcement Learning, RL）进行训练，作为在交互环境中进行长期推理和行动的自主代理。然而，稀疏且有时不可验证的奖励使得时间信用分配（temporal credit assignment）极具挑战性。近期工作尝试将过程监督（process supervision）整合到代理学习中，但存在有偏标注、奖励劫持（reward hacking）、过度细粒度信号导致的高方差或状态重叠较少时的失败等问题。\n\n因此，我们提出了在线过程奖励学习（Online Process Reward Learning, OPRL），这是一种用于代理强化学习（agentic RL）的通用信用分配策略，可与标准在策略算法（on-policy algorithms）无缝集成，而无需依赖额外的推演（rollouts）或显式步骤标签。在OPRL中，我们交替优化隐式过程奖励模型（implicit process reward model, PRM）和代理策略，通过基于轨迹的DPO目标（trajectory-based DPO objective）将轨迹偏好转化为隐式步骤奖励。这些步骤奖励随后用于计算步骤级优势（step-level advantages），与来自结果奖励的回合级优势（episode-level advantages）相结合用于策略更新，形成一个自我强化循环（self-reinforcing loop）。\n\n理论研究保证，学习到的步骤奖励与轨迹偏好一致，并作为基于势能的塑形奖励（potential-based shaping rewards），提供有界梯度以稳定训练。在实证方面，我们在三个不同的代理基准测试（agent benchmarks）上评估了OPRL，包括WebShop和VisualSokoban，以及在SOTOPIA中具有不可验证奖励的开放式社交互动（open-ended social interactions）。关键的是，OPRL在各个领域都表现出优于前沿LLMs和强大RL基线的性能，以更高的样本效率（sample-efficiency）和更低的训练方差实现了最先进的结果。进一步分析还表明，OPRL通过使用更少的动作实现了高效探索（efficient exploration），强调了其在现实场景中代理学习的潜力。"
                },
                {
                    "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering",
                    "arxiv_id": "2509.19094",
                    "authors": "Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Zhuowan Li, Spurthi Amba Hombaiah, Weize Kong, Tao Chen, Hamed Zamani, Michael Bendersky",
                    "summary": "Personalization is essential for adapting question answering (QA) systems to user-specific information needs, thereby improving both accuracy and user satisfaction. However, personalized QA remains relatively underexplored due to challenges such as inferring preferences from long, noisy, and implicit contexts, and generating responses that are simultaneously correct, contextually appropriate, and aligned with user expectations and background knowledge. To address these challenges, we propose Pathways of Thoughts (PoT), an inference-stage method that applies to any large language model (LLM) without requiring task-specific fine-tuning. The approach models the reasoning of an LLM as an iterative decision process, where the model dynamically selects among cognitive operations such as reasoning, revision, personalization, and clarification. This enables exploration of multiple reasoning trajectories, producing diverse candidate responses that capture different perspectives. PoT then aggregates and reweights these candidates according to inferred user preferences, yielding a final personalized response that benefits from the complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA benchmark for personalized QA show that PoT consistently outperforms competitive baselines, achieving up to a 13.1% relative improvement. Human evaluation corroborates these results, with annotators preferring outputs from PoT in 66% of cases and reporting ties in only 15% of cases.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了\"Pathways of Thoughts (PoT)\"方法，一种用于增强大语言模型推理能力的通用框架。根据筛选标准，我判断该论文符合研究范围，原因如下： 首先，从核心判断来看，论文的本质是改进LLM的基础推理能力。PoT方法将LLM的推理过程建模为迭代决策过程，使模型能够动态选择认知操作（推理、修订、个性化、澄清等），这直接涉及增强LLM的多步推理能力和逻辑推理能力，属于提升LLM通用推理能力的研究。 其次，从正面指标分析，论文明确包含多个关键主题：核心概念上聚焦于大语言模型(LLMs)；能力方向上专注于reasoning（推理过程）；方法上提出了一种新的推理范式，使模型能够探索多个推理轨迹并聚合结果。 第三，该论文不涉及任何排除标准领域。它没有关注多模态与视觉，没有针对特定应用领域（如医疗、化学等），也没有专注于模型基础设施或部署优化。 最后，关于特殊情况的考虑，虽然论文应用于个性化问答场景，但其提出的是一种通用的推理框架，可以应用于任何LLM而无需任务特定微调，因此不是将LLM作为工具应用于特定领域，而是致力于提升LLM本身的通用推理能力。 综合来看，这篇论文通过提出多方向思维的推理方法，直接增强了大语言模型的通用推理能力，完全符合研究课题的筛选要求。",
                    "summary2": "本文旨在解决个性化问答系统中的挑战，包括从长、嘈杂和隐式上下文中推断用户偏好，以及生成既正确又符合用户期望的响应。针对个性化问答场景，我们提出了一种Pathways of Thoughts (PoT)方法，将LLM的推理建模为迭代决策过程，探索多个推理轨迹并聚合响应，并在LaMP-QA基准测试上通过自动化评估和人工评估验证了其有效性，实现了高达13.1%的相对改进。",
                    "summary_translation": "个性化（Personalization）对于使问答系统（question answering, QA）适应用户特定的信息需求至关重要，从而同时提高准确性和用户满意度。然而，由于从冗长、嘈杂和隐含的上下文中推断用户偏好，以及生成同时满足正确性、上下文适当性并与用户期望和背景知识保持一致的响应等挑战，个性化问答（personalized QA）研究仍相对不足。为应对这些挑战，我们提出了\"思维路径\"（Pathways of Thoughts, PoT），这是一种适用于任何大型语言模型（large language model, LLM）的推理阶段（inference-stage）方法，无需进行任务特定的微调（fine-tuning）。该方法将大型语言模型的推理过程建模为一个迭代决策过程，模型在其中动态选择认知操作，如推理（reasoning）、修订（revision）、个性化（personalization）和澄清（clarification）。这使得能够探索多种推理轨迹，生成捕捉不同视角的多样化候选响应。然后，PoT根据推断的用户偏好对这些候选响应进行聚合和重新加权，产生最终的个性化响应，该响应受益于多样化推理路径的互补优势。在个性化问答的LaMP-QA基准测试（benchmark）上的实验表明，PoT始终优于竞争性基线（baselines），实现了高达13.1%的相对改进。人工评估（human evaluation）证实了这些结果，标注员（annotators）在66%的情况下更喜欢PoT的输出，仅在15%的情况下报告平局。"
                },
                {
                    "title": "Soft Tokens, Hard Truths",
                    "arxiv_id": "2509.19170",
                    "authors": "Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, Yann Ollivier",
                    "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens. This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the \"soft\" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种通过强化学习(RL)来学习连续思维链(CoTs)的新方法，以提高大语言模型的推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是思维链推理这一通用能力，而非将LLM作为工具应用到特定领域。论文使用了强化学习这一训练方法，聚焦于数学推理任务，符合第二步正面指标中的多个关键点：核心概念(LLMs)、能力方向(math reasoning)和训练方法(RL)。论文不涉及第三步排除标准中的任何领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。在第四步特殊和模糊情况处理中，论文提出的是一种通用的推理增强方法，而非针对特定领域的应用。综合来看，这篇论文直接致力于提高LLM的通用推理能力，通过创新的\"soft tokens\"方法增强思维链推理，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在连续Chain-of-Thought推理中的训练难题。针对数学推理任务，我们提出了一种基于强化学习的soft tokens训练方法，通过在token嵌入中添加噪声实现有效探索，无需从真实离散CoT中蒸馏。在Llama和Qwen模型上，通过pass@1和pass@32指标验证了其有效性，表明该方法在保持pass@1性能的同时显著提高了pass@32的多样性，且在域外任务上更好地保留了基础模型性能。",
                    "summary_translation": "在大型语言模型(LLM)的Chain-of-Thought (CoT，思维链)推理阶段使用连续token(标记)而非离散token(标记)的做法最近引起了广泛关注，其基于一种直觉，即离散token(标记)的连续混合可以同时模拟多条推理路径的叠加。理论结果已经正式证明，连续token(标记)具有更强的表达能力，并能更高效地解决特定问题。\n\n然而，连续token(标记)的实际应用受到严重训练困难的限制：先前的研究要么仅在预训练的离散token(标记)模型上在推理时使用连续token(标记)，要么必须从真实的离散CoT中蒸馏出连续CoT，并面临计算成本的限制，导致CoT只能包含非常少的token(标记)。\n\n这是首个介绍通过强化学习(RL，Reinforcement Learning)学习连续CoT的可扩展方法的研究，无需从参考离散CoT中进行蒸馏。我们使用\"软\"token(标记)：token(标记)的混合与输入嵌入上的噪声相结合，以提供RL探索。计算开销极小，使我们能够学习包含数百个token(标记)的连续CoT。\n\n在高达8B参数的Llama和Qwen模型的数学推理基准测试中，使用连续CoT训练在pass@1指标上与离散token(标记) CoT相当，在pass@32指标上超越后者，显示出更大的CoT多样性。在系统比较中，表现最佳的场景是使用连续CoT token(标记)进行训练，然后在推理时使用离散token(标记)，这意味着\"软\"模型可以以标准方式部署。\n\n最后，我们表明连续CoT RL训练能更好地保留基础模型在域外任务(out-of-domain tasks)上的预测，从而为基础模型提供更柔和的调整。"
                },
                {
                    "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users",
                    "arxiv_id": "2509.18632",
                    "authors": "Nishant Balepur, Matthew Shu, Yoo Yeon Sung, Seraphina Goldfarb-Tarrant, Shi Feng, Fumeng Yang, Rachel Rudinger, Jordan Lee Boyd-Graber",
                    "summary": "To assist users in complex tasks, LLMs generate plans: step-by-step instructions towards a goal. While alignment methods aim to ensure LLM plans are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer, assuming this reflects what helps them. We test this with Planorama: an interface where 126 users answer 300 multi-step questions with LLM plans. We get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA success) and user preferences on plans, and recreate the setup in agents and reward models to see if they simulate or prefer what helps users. We expose: 1) user/model preferences and agent success do not accurately predict which plans help users, so common alignment feedback can misalign with helpfulness; 2) this gap is not due to user-specific preferences, as users are similarly successful when using plans they prefer/disprefer; 3) surface-level cues like brevity and question similarity strongly link to preferences, but such biases fail to predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from real user interactions, not just preferences of what looks helpful, so we discuss the plan NLP researchers can execute to solve this problem.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。 首先，从核心判断来看，这篇论文的本质是研究LLM的规划(planning)能力，属于通用推理能力的重要组成部分。论文探讨了LLM如何生成步骤化计划来帮助用户完成复杂任务，并评估这些计划的实际有效性。这不是将LLM作为工具应用到特定领域，而是关注LLM本身的基础能力和改进方法，特别是当前对齐方法(如RLHF)的局限性。 其次，论文满足多个正面指标： 1) 核心概念：明确研究LLM生成的计划 2) 能力方向：聚焦于规划(planning)能力，这是推理和问题解决的关键部分 3) 训练方法：讨论了RLHF等对齐方法，并指出其局限性 4) 新兴范式：提到了智能体(agents)在模拟用户交互中的作用 第三，论文不符合任何排除标准。它不涉及多模态与视觉，不聚焦于特定应用领域，也不是关于模型可靠性的应用层面研究(如水印、安全等)。 在特殊和模糊情况处理上，论文虽然提到了智能体，但这是作为研究方法的一部分，而非将智能体应用在特定领域。论文讨论的对齐问题是从提升模型基础能力的角度，而不是应用层面的防御技术。 论文的核心贡献在于揭示了当前LLM对齐方法(基于用户偏好)与实际帮助用户之间的差距，并提出需要基于真实用户交互反馈来改进对齐方法。这直接关系到如何提升LLM的通用推理能力和实际效用，完全符合研究目标。",
                    "summary2": "本文旨在解决LLM对齐方法中用户偏好与实际帮助性不一致的问题。针对多步骤问答场景，我们提出了Planorama界面，收集126用户对600个LLM计划的4388个执行结果和5584个偏好比较。通过Item Response Theory量化计划帮助性，我们发现用户偏好、reward模型和agent表现均无法准确预测真正帮助用户的计划，表明当前基于偏好的对齐方法与实际帮助性存在根本性错位。",
                    "summary_translation": "为协助用户完成复杂任务，大型语言模型（LLMs）会生成计划：即朝向目标的分步说明。尽管对齐方法（alignment methods）旨在确保LLM计划具有帮助性，但它们基于用户偏好进行训练（基于人类反馈的强化学习，RLHF）或评估（聊天机器人竞技场，ChatbotArena），假设这种偏好反映了什么对用户真正有帮助。我们通过Planorama（计划全景）界面对此进行测试：在该界面中，126名用户使用LLM计划回答300个多步骤问题。我们获得4388个计划执行和5584个比较数据，以衡量计划的有用性（问答成功率，QA success）和用户对计划的偏好，并在代理（agents）和奖励模型（reward models）中重建此设置，以观察它们是否能模拟或偏好真正帮助用户的方案。我们揭示了：1）用户/模型偏好与代理成功无法准确预测哪些计划真正帮助用户，因此常见的对齐反馈可能与有用性产生偏差；2）这种差距并非源于用户特定偏好，因为用户在使用他们偏好/不偏好的计划时成功率相似；3）简洁性和问题相似性等表面线索与偏好强烈相关，但此类偏见无法预测有用性。总之，我们认为对齐有用的LLMs需要来自真实用户交互的反馈，而不仅仅是对看起来有用的东西的偏好，因此我们讨论了自然语言处理（NLP）研究人员可执行的计划以解决此问题。"
                },
                {
                    "title": "Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering",
                    "arxiv_id": "2509.18655",
                    "authors": "Lingwen Deng, Yifei Han, Long Zhang, Yue Du, Bin Li",
                    "summary": "Parameter-Preserving Knowledge Editing (PPKE) enables updating models with new or corrected information without retraining or parameter adjustment. Recent PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE) capabilities to multi-hop question answering (MHQA). However, these methods often lack consistency, leading to knowledge contamination, unstable updates, and retrieval behaviors that fail to reflect the intended edits. Such inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures KG construction, update, and retrieval are always aligned with the requirements of the MHQA task, maintaining coherent reasoning over both unedited and edited knowledge. Extensive experiments on the MQuAKE benchmark show accuracy improvements in PPKE performance for MHQA, demonstrating the effectiveness of addressing consistency in PPKE.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步：核心判断——这篇论文的本质是关于改进LLM在多跳问答(multi-hop question answering)任务中的推理能力。论文提出了CAPE-KG框架，解决参数保留知识编辑(PPKE)中的一致性问题，确保模型在更新知识后能够保持连贯的多跳推理能力。这属于增强LLM逻辑推理能力的范畴，而非将LLM作为工具应用到特定领域，因此应保留。 第二步：正面指标——论文包含\"reasoning\"这一核心能力方向，特别是\"multi-hop reasoning\"（多跳推理），这是通用推理能力的重要组成部分，涉及模型进行多步逻辑推理来连接不同知识片段以得出答案。 第三步：排除标准——论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域。多跳问答是一种通用的推理任务，不属于特定应用领域。 第四步：特殊和模糊情况——论文提出了一种新方法来解决知识编辑中的不一致性问题，从而提升模型在多跳问答任务中的推理质量和可靠性。这符合\"提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量\"的情况，应保留。 综合以上分析，该论文的核心贡献是提出了一种一致性感知的知识编辑框架，旨在提升LLM在多跳推理任务中的表现，这与研究\"大语言模型通用推理能力\"的目标高度一致，因此最终判断为True。",
                    "summary2": "本文旨在 [解决参数保持知识编辑在多跳问答中缺乏一致性的问题]。针对 [多跳问答场景]，我们提出了一种 [一致性感知的参数保持知识编辑框架CAPE-KG]，并在 [MQuAKE基准测试] 上通过 [M-Acc和H-Acc指标] 验证了其有效性。",
                    "summary_translation": "参数保留知识编辑（Parameter-Preserving Knowledge Editing, PPKE）使模型能够在无需重新训练或参数调整的情况下，使用新的或经过校正的信息进行更新。近期基于知识图谱（Knowledge Graph, KG）的PPKE方法将知识编辑（Knowledge Editing, KE）能力扩展到多跳问题回答（Multi-hop Question Answering, MHQA）任务中。然而，这些方法常常缺乏一致性，导致知识污染、更新不稳定，以及检索行为无法反映预期的编辑效果。这种不一致性削弱了PPKE在多跳推理中的可靠性。我们提出了CAPE-KG（Consistency-Aware Parameter-Preserving Editing with Knowledge Graphs，基于知识图谱的一致性感知参数保留编辑），这是一种用于MHQA任务中PPKE的新型一致性感知框架。CAPE-KG确保知识图谱的构建、更新和检索始终与MHQA任务的要求保持一致，从而在未编辑和已编辑的知识上维持连贯的推理。在MQuAKE基准测试上进行的大量实验表明，CAPE-KG在MHQA任务的PPKE性能方面提高了准确性，证明了解决PPKE中一致性问题的有效性。"
                },
                {
                    "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs",
                    "arxiv_id": "2509.18314",
                    "authors": "Hieu Tran, Zonghai Yao, Hong Yu",
                    "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over long sequences makes token-level credit assignment the key bottleneck. We study the verifiable-reward setting, where the final answer is checkable and multiple responses can be drawn per prompt. Reasoning tasks in math and medical QA align with this setup, where only a few decision tokens significantly impact the outcome. PPO offers token-level advantages with a learned value model, but it is complex to train both the actor and critic models simultaneously, and it is not easily generalizable, as the token-level values from the critic model can make training prone to overfitting. GRPO is critic-free and supports verifiable rewards, but spreads a single sequence-level return across tokens and ignores branching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that converts a group of responses into a prefix tree and computes \\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes. Built on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated \\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a critic-free algorithm that augments the group-relative outcome signal of GRPO with \\emph{branch-gated} temporal-difference corrections derived from the tree. At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO reduces to GRPO; at branching tokens, it supplies precise token-level credit without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B, TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and reaches higher validation accuracy with roughly the same wall-clock time.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标，核心是关于提高大语言模型通用推理能力的研究。 首先，从本质上看，论文的核心贡献是提出了一种新的强化学习训练方法TEMPO，用于解决LLM在长序列推理任务中的信用分配问题。这直接针对LLM的基础能力改进，特别是提升其在推理任务中的表现，符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。 其次，论文包含多个正面指标：明确以LLMs为核心研究对象；专注于reasoning能力（特别是math reasoning）；提出了新的强化学习训练方法(TEMPO)，改进了现有的PPO和GRPO算法。 第三，论文不涉及任何排除标准领域。虽然论文在实验中使用了医疗问答(MedQA, MMLU-Medical)作为评估基准，但这只是作为验证方法有效性的测试场景，而非论文的核心焦点。论文的核心是改进通用推理能力，而非专注于医疗应用。 最后，论文提出的方法具有通用性，可以应用于各种需要推理能力的任务，不仅限于特定领域。作者在多个数学推理基准(MATH, GSM-HARD, AMC23)和医疗问答基准上进行了验证，证明了其方法的通用性和有效性。 综上所述，这篇论文明确致力于提高LLM的通用推理能力，通过改进强化学习训练方法来解决信用分配问题，完全符合我的研究范围。",
                    "summary2": "本文旨在解决LLM强化训练中的token级别信用分配问题。针对长序列推理任务中奖励稀疏延迟的场景，我们提出了一种利用响应树结构的TEMPO算法，并在数学和医学问答数据集上通过准确率和收敛速度验证了其有效性。",
                    "summary_translation": "强化学习（reinforcement learning）改善了大型语言模型（LLM）的推理能力，但在长序列上的稀疏延迟奖励（sparse delayed reward）使得token级别的信用分配（token-level credit assignment）成为关键瓶颈。我们研究了可验证奖励（verifiable-reward）设置，其中最终答案是可检查的，并且每个提示（prompt）可以生成多个响应。数学和医学问答（medical QA）中的推理任务与这种设置一致，其中只有少数决策token（decision tokens）对结果有显著影响。PPO通过学习到的价值模型（learned value model）提供了token级别的优势，但同时训练actor和critic模型很复杂，并且不容易泛化，因为来自critic模型的token级别值可能导致训练容易过拟合。GRPO是无critic的（critic-free）且支持可验证奖励，但它将单个序列级别的回报（sequence-level return）分散到各个token上，并忽略了分支（branching）。\n\n我们提出了**Prefix-to-Tree (P2T)**，这是一个简单的过程，将一组响应转换为前缀树（prefix tree），并通过聚合后代结果（descendant outcomes）来计算*非参数*（nonparametric）前缀值\\(V(s)\\)。基于P2T，我们提出了**TEMPO**（***T**ree-**E**stimated **M**ean **P**refix Value for **P**olicy **O**ptimization*，树估计平均前缀值用于策略优化），这是一种无critic的算法，它用从树中派生的*分支门控*（branch-gated）时序差分（temporal-difference）修正来增强GRPO的组相对结果信号。在非分支token上，时序差分（TD）项为零，因此TEMPO简化为GRPO；在分支token上，它提供了精确的token级别信用，而无需学习的价值网络或额外的评判者/教师。在Qwen3-1.7B/4B模型上，TEMPO在分布内（MATH, MedQA）和分布外（GSM-HARD, AMC23, MedMCQA, MMLU-Medical）基准测试上均优于PPO和GRPO，并在大致相同的实际时间（wall-clock time）内达到更高的验证准确率。"
                },
                {
                    "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning",
                    "arxiv_id": "2509.18163",
                    "authors": "Haodong Zhao, Chenyan Zhao, Yansi Li, Zhuosheng Zhang, Gongshen Liu",
                    "summary": "The capacity of Large Language Models (LLMs) to reason is fundamental to their application in complex, knowledge-intensive domains. In real-world scenarios, LLMs are often augmented with external information that can be helpful, irrelevant, or even misleading. This paper investigates the causal impact of such auxiliary information on the reasoning process of LLMs with explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset derived from ScienceQA, to systematically test the robustness of the model against these types of information. Our findings reveal a critical vulnerability: the model's deliberative \"thinking mode\" is a double-edged sword. While helpful context improves accuracy, misleading information causes a catastrophic drop in performance, which is amplified by the thinking process. Instead of conferring robustness, thinking reinforces the degree of error when provided with misinformation. This highlights that the challenge is not merely to make models \"think\", but to endow them with the critical faculty to evaluate the information upon which their reasoning is based. The SciAux dataset is available at https://huggingface.co/datasets/billhdzhao/SciAux.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的核心是研究大语言模型(LLM)的推理能力，特别是探讨外部辅助信息对LLM推理过程的影响。论文关注的是LLM的基础推理能力，而不是将LLM作为工具应用到特定领域。论文研究了模型的\"thinking mode\"（思维模式）如何影响其对不同类型信息的处理，这直接关系到LLM的通用推理能力。因此，从核心判断来看，这篇论文应该被保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：直接研究reasoning能力，特别是模型的step-by-step thinking capabilities - 论文探讨了LLM在面对不同类型信息时的推理过程，这与提升模型通用推理能力直接相关 第三步：排除标准 论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉研究 - 不是针对特定应用领域（如医疗、化学等）的研究 - 虽然涉及模型可靠性，但是从提升模型推理能力的根本角度出发，而非仅作为应用层面的防御 第四步：处理特殊和模糊情况 论文研究的是LLM的通用推理能力在面对不同类型信息时的表现，特别是\"thinking mode\"如何影响推理过程。这不是将LLM应用到特定领域，而是研究LLM本身的推理机制和脆弱性。论文提出的挑战是\"not merely to make models 'think', but to endow them with the critical faculty to evaluate the information upon which their reasoning is based\"，这直接指向提升LLM的通用推理能力。 第五步：最终决策 综合以上分析，这篇论文的核心贡献是研究外部辅助信息对LLM推理过程的影响，揭示了模型\"thinking mode\"的双面性，并提出了提升模型批判性评估信息能力的重要性。这直接关系到提升LLM的通用推理能力，符合研究目标。因此，最终判断为True，应该保留这篇论文。",
                    "summary2": "本文旨在探究辅助信息对LLM推理过程的影响。针对不同类型辅助信息（有帮助、不相关、误导）的场景，我们提出了SciAux数据集，并在具有可切换思考模式的大型推理模型上通过准确率指标验证了推理过程的双刃剑效应。实验证明，思考模式虽在有帮助上下文中提升性能，但在误导信息面前会显著放大错误，揭示了当前推理模型在信息评估能力上的关键缺陷。",
                    "summary_translation": "Large Language Models (LLMs, 大型语言模型)的推理能力是其应用于复杂、知识密集型领域的基础。在现实场景中，LLMs常被辅以外部信息，这些信息可能是有帮助的、无关的，甚至是误导性的。本文研究了此类辅助信息对具有明确逐步思考能力的LLMs推理过程的因果影响。我们介绍了SciAux，一个源自ScienceQA的新数据集，用于系统测试模型对这些类型信息的鲁棒性(robustness)。我们的发现揭示了一个关键漏洞：模型的审慎\"思考模式\"(thinking mode)是一把双刃剑。有帮助的上下文提高准确性，而误导性信息导致性能灾难性下降，这种影响在思考过程中被放大。思考并未赋予鲁棒性，而是在提供错误信息(misinformation)时强化了错误程度。这表明挑战不仅仅是让模型\"思考\"，而是赋予它们评估其推理所依据信息的批判性能力(critical faculty)。SciAux数据集可在https://huggingface.co/datasets/billhdzhao/SciAux获取。"
                },
                {
                    "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs",
                    "arxiv_id": "2509.18113",
                    "authors": "Xin Hu, Yue Kang, Guanzi Yao, Tianze Kang, Mengjie Wang, Heyao Liu",
                    "summary": "This study addresses the generalization limitations commonly observed in large language models under multi-task and cross-domain settings. Unlike prior methods such as SPoT, which depends on fixed prompt templates, our study introduces a unified multi-task learning framework with dynamic prompt scheduling mechanism. By introducing a prompt pool and a task-aware scheduling strategy, the method dynamically combines and aligns prompts for different tasks. This enhances the model's ability to capture semantic differences across tasks. During prompt fusion, the model uses task embeddings and a gating mechanism to finely control the prompt signals. This ensures alignment between prompt content and task-specific demands. At the same time, it builds flexible sharing pathways across tasks. In addition, the proposed optimization objective centers on joint multi-task learning. It incorporates an automatic learning strategy for scheduling weights, which effectively mitigates task interference and negative transfer. To evaluate the effectiveness of the method, a series of sensitivity experiments were conducted. These experiments examined the impact of prompt temperature parameters and task number variation. The results confirm the advantages of the proposed mechanism in maintaining model stability and enhancing transferability. Experimental findings show that the prompt scheduling method significantly improves performance on a range of language understanding and knowledge reasoning tasks. These results fully demonstrate its applicability and effectiveness in unified multi-task modeling and cross-domain adaptation.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种动态提示融合方法，用于改进大语言模型在多任务和跨领域设置下的泛化能力。论文通过引入提示池和任务感知调度策略，动态组合和对齐不同任务的提示，增强了模型捕捉跨任务语义差异的能力。从第一步核心判断来看，这明显属于改进LLM基础能力的研究，提出了新的训练范式来增强模型的通用能力，而非将LLM作为工具应用到特定领域。从第二步正面指标看，论文明确涉及大语言模型(LLMs)核心概念，并特别提到提高了\"知识推理任务\"(knowledge reasoning tasks)的性能，直接符合推理能力方向。论文不涉及第三步中的任何排除标准，如多模态视觉、特定应用领域或模型可靠性等应用层面的研究。综合分析，该论文致力于提升LLM的通用推理能力和泛化能力，完全符合\"大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在多任务和跨领域设置中的泛化限制问题。针对异构任务和领域迁移场景，我们提出了一种基于动态提示调度的统一多任务学习框架，并在CrossFit数据集上通过SuperGLUE和MMLU Accuracy等指标验证了其有效性。",
                    "summary_translation": "本研究针对大语言模型（large language models）在多任务（multi-task）和跨域（cross-domain）设置中普遍存在的泛化限制（generalization limitations）问题。与先前如SPoT等依赖固定提示模板（prompt templates）的方法不同，我们引入了一种具有动态提示调度机制（dynamic prompt scheduling mechanism）的统一多任务学习框架。通过引入提示池（prompt pool）和任务感知调度策略（task-aware scheduling strategy），该方法动态组合和对齐不同任务的提示。这增强了模型捕捉任务间语义差异（semantic differences）的能力。在提示融合（prompt fusion）过程中，模型利用任务嵌入（task embeddings）和门控机制（gating mechanism）精细控制提示信号（prompt signals）。这确保了提示内容与任务特定需求之间的一致性，同时构建了跨任务的灵活共享路径。此外，所提出的优化目标（optimization objective）以联合多任务学习（joint multi-task learning）为中心，融合了调度权重（scheduling weights）的自动学习策略，有效缓解了任务干扰（task interference）和负迁移（negative transfer）问题。为评估该方法的有效性，我们进行了一系列敏感性实验（sensitivity experiments），这些实验考察了提示温度参数（prompt temperature parameters）和任务数量变化的影响。结果证实了所提机制在保持模型稳定性和增强迁移能力（transferability）方面的优势。实验发现表明，该提示调度方法在一系列语言理解（language understanding）和知识推理（knowledge reasoning）任务上显著提高了性能。这些结果充分证明了其在统一多任务建模（unified multi-task modeling）和跨域适应（cross-domain adaptation）中的适用性和有效性。"
                },
                {
                    "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization",
                    "arxiv_id": "2509.18158",
                    "authors": "Seungyoun Yi, Minsoo Khang, Sungrae Park",
                    "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM) performance by refining prompts for specific tasks. However, prior APO methods typically focus only on user prompts, rely on unstructured feedback, and require large sample sizes and long iteration cycles-making them costly and brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a novel framework that jointly optimizes both system and user prompts through principled, low-overhead refinement. ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. This enables fast convergence to high-quality prompts using minimal examples and short iteration cycles. We evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning, summarization, and code generation tasks. Experimental results demonstrate consistent improvements over strong baselines. Further ablation studies highlight the contribution of each component to more effective prompt construction. Our implementation including all prompts is publicly available at https://github.com/younatics/zera-agent.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究课题。 首先，从核心判断来看，这篇论文的本质是提出ZERA框架，一种用于自动优化提示的新方法，通过联合优化系统提示和用户提示来提高大语言模型在各种任务上的性能。这不是将LLM作为工具应用到特定领域，而是提出一种通用的方法来增强LLM的基础能力，特别是在推理任务上的表现，因此符合保留条件。 其次，论文包含多个正面指标： - 核心概念：明确关注大语言模型(LLMs)，并在五个不同LLM上进行了评估 - 能力方向：评估数据集包含推理(reasoning)任务，直接符合我们的研究方向 - 训练方法：标题中的\"Instruction Evolving\"和摘要中的\"principled, low-overhead refinement\"表明涉及进化或自我优化的方法 - 新兴范式：论文提出的ZERA是一种基于LLM的智能体(Refinement Agent)，用于优化提示 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域，评估的是通用能力（推理、摘要、代码生成） - 不主要关注模型可靠性的应用层面问题 最后，在特殊和模糊情况处理上，ZERA智能体是一种通用框架，旨在增强LLM的通用问题解决能力，而不是应用于特定领域，因此应该保留。 论文的核心贡献是提出了一种新颖的、基于原则的提示优化框架，能够使用最少的示例和短的迭代周期快速收敛到高质量提示，从而提升LLM在推理等通用任务上的表现。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决自动提示优化(APO)方法依赖非结构化反馈、需要大量样本和长迭代周期的问题。针对多种LLMs和九个不同任务数据集，我们提出了一种ZERA框架，通过八个评估原则和自动推断权重共同优化系统和用户提示，并在推理、摘要和代码生成任务上通过准确率、ROUGE-L和pass@1等指标验证了其有效性。",
                    "summary_translation": "自动提示优化(Automatic Prompt Optimization, APO)通过针对特定任务优化提示来提高大型语言模型(large language model, LLM)的性能。然而，先前的APO方法通常只关注用户提示，依赖非结构化反馈，并且需要大量样本和长迭代周期——这使得它们成本高昂且脆弱。我们提出了ZERA(Zero-init Instruction Evolving Refinement Agent，零初始化指令进化优化代理)，一种新颖的框架，通过有原则的、低开销的优化来共同优化系统和用户提示。ZERA使用八个具有自动推断权重的可泛化标准对提示进行评分，并基于这些结构化批评修订提示。这使得能够使用最少的示例和短的迭代周期快速收敛到高质量的提示。我们在五个LLMs和九个涵盖推理、摘要和代码生成任务的多样化数据集上评估了ZERA。实验结果表明，与强大的基线相比，ZERA取得了持续的改进。进一步的消融研究(ablation studies)突显了每个组件对更有效提示构建的贡献。我们的实现包括所有提示，可在https://github.com/younatics/zera-agent公开获取。"
                },
                {
                    "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World",
                    "arxiv_id": "2509.19265",
                    "authors": "Saeed Almheiri, Rania Hossam, Mena Attia, Chenxi Wang, Preslav Nakov, Timothy Baldwin, Fajri Koto",
                    "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围。论文的核心贡献是研究LLMs的常识推理能力在不同文化背景下的迁移效果，并提出了轻量级对齐方法（如上下文学习和基于演示的强化学习DITTO）来提升这种能力。常识推理是通用推理能力的重要组成部分，论文评估的训练方法也属于改进LLM基础能力的范畴。虽然论文涉及文化领域，但其焦点不是将LLM作为工具应用于特定领域，而是研究LLM本身的推理能力如何在不同文化背景下迁移和提升。论文提出的方法（使用12个文化特定示例即可提高其他地区性能10%）可以作为一种通用的方法来增强LLMs的推理能力，而不是仅限于解决特定领域的问题。因此，这篇论文符合我的研究目标，它探索了如何通过文化迁移来提升LLM的通用推理能力，特别是常识推理这一核心能力。",
                    "summary2": "本文旨在研究大型语言模型(LLMs)在阿拉伯世界中的跨文化常识推理迁移能力。针对阿拉伯国家间存在语言和历史相似性但又有本地文化差异的特点，我们提出了一种轻量级文化对齐方法，包括上下文学习(ICL)和基于演示的强化学习(DITTO)，并在ArabCulture数据集上通过准确率提升验证了其有效性。实验表明，仅需12个特定文化示例就能使多语言模型在其他国家的表现平均提升10%，且非阿拉伯文化(如印尼和美国)的演示也能达到或超过文化内对齐的效果。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）常表现出以西方为中心的偏见（Western-centric biases），限制了其在多元文化语境中的有效性。尽管已有研究探索了文化对齐（cultural alignment）问题，但利用某一文化中的对齐成果来提升其他文化中模型表现的跨文化迁移（cross-cultural transfer）潜力仍鲜有研究。本文聚焦阿拉伯世界中的常识推理（commonsense reasoning）跨文化迁移问题，该区域在语言与历史方面具有高度相似性，同时又存在显著的本地文化差异。我们使用一个涵盖13个阿拉伯国家、基于文化情境构建的常识推理数据集，评估了多种轻量级对齐方法，包括上下文学习（in-context learning）和基于示范的强化方法（demonstration-based reinforcement, DITTO），并与监督微调（supervised fine-tuning）和直接偏好优化（direct preference optimization）等基线方法进行对比。实验结果表明，在多语言模型中，仅需来自某一国家的12个文化特异性示例，即可使其他阿拉伯国家的模型表现平均提升10%。此外，我们发现来自印度尼西亚和美国语境的“异文化”示范（out-of-culture demonstrations）在多项选择题（MCQ）推理任务中，其效果可媲美甚至超越“同文化”对齐方法，揭示了常识推理能力在阿拉伯世界之外的跨文化可迁移性。这些发现表明，高效的跨文化对齐是可行的，为将大语言模型适配至资源匮乏的文化环境提供了有前景的路径。"
                },
                {
                    "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions",
                    "arxiv_id": "2509.18847",
                    "authors": "Junhao Su, Yuanliang Wan, Junwei Yang, Hengyu Shi, Tianyang Han, Junfeng Luo, Yurui Qiu",
                    "summary": "Tool-augmented large language models (LLMs) are usually trained with supervised imitation or coarse-grained reinforcement learning that optimizes single tool calls. Current self-reflection practices rely on heuristic prompts or one-way reasoning: the model is urged to 'think more' instead of learning error diagnosis and repair. This is fragile in multi-turn interactions; after a failure the model often repeats the same mistake. We propose structured reflection, which turns the path from error to repair into an explicit, controllable, and trainable action. The agent produces a short yet precise reflection: it diagnoses the failure using evidence from the previous step and then proposes a correct, executable follow-up call. For training we combine DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce Tool-Reflection-Bench, a lightweight benchmark that programmatically checks structural validity, executability, parameter correctness, and result consistency. Tasks are built as mini trajectories of erroneous call, reflection, and corrected call, with disjoint train and test splits. Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn tool-call success and error recovery, and a reduction of redundant calls. These results indicate that making reflection explicit and optimizing it directly improves the reliability of tool interaction and offers a reproducible path for agents to learn from failure.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础能力，特别是通过提出\"结构化反思\"(structured reflection)这一新训练范式来增强LLM的工具使用和错误恢复能力，这属于提升LLM通用推理能力的范畴。 论文包含多个正面指标：明确提及\"Large language models (LLMs)\"和\"Tool-augmented large language models\"等核心概念；虽然未直接使用\"reasoning\"一词，但\"reflection\"和\"error diagnosis\"本质上是一种推理过程；训练方法涉及强化学习(\"reinforcement learning\"和\"reward scheme\")；同时论文明确讨论了\"llm-based agents\"和\"tool use\"等新兴范式。 论文不符合任何排除标准：它不涉及多模态与视觉内容，不专注于特定应用领域，且讨论的\"reliability\"是从提升模型内在能力的角度而非应用层面的防御。 在特殊和模糊情况处理上，论文提出的是一种通用的智能体反思框架来增强LLM的工具使用能力，而非针对特定领域的应用，因此应该保留。 论文的核心贡献是通过结构化反思机制，使智能体能够从失败中学习并改进其推理过程，这直接提升了LLM的通用推理能力和问题解决能力，符合研究目标。",
                    "summary2": "本文旨在解决工具增强型大语言模型在多轮交互中错误恢复能力不足的问题。针对多轮工具调用场景，我们提出了一种结构化反思方法，将错误诊断和纠正转化为可训练的显式行动，并通过结合DAPO和GSPO的目标函数优化Reflect → Call → Final策略。我们在BFCL v3和Tool-Reflection-Bench上通过多轮工具调用成功率、错误恢复率和冗余调用减少率等指标验证了其有效性。",
                    "summary_translation": "工具增强型大语言模型（LLMs，Large Language Models）通常通过监督模仿或优化单次工具调用的粗粒度强化学习进行训练。当前的自省（self-reflection）实践依赖于启发式提示（heuristic prompts）或单向推理（one-way reasoning）：模型被鼓励\"多思考\"，而不是学习错误诊断和修复。这种方法在多轮交互（multi-turn interactions）中表现脆弱；失败后，模型往往会重复同样的错误。我们提出了结构化反思（structured reflection），它将从错误到修复的路径转变为一种明确、可控且可训练的行动。智能体（agent）生成简短而精确的反思：它利用前一步骤的证据诊断失败，然后提出一个正确的、可执行的后续调用。在训练方面，我们将DAPO（DAPO）和GSPO（GSPO）目标与针对工具使用量身定制的奖励方案相结合，优化\"先反思，后调用，最终确定\"的逐步策略。为进行评估，我们引入了Tool-Reflection-Bench（工具反思基准），这是一个轻量级基准（lightweight benchmark），可程序化地检查结构有效性、可执行性、参数正确性和结果一致性。任务被构建为错误调用、反思和纠正调用的小型轨迹（mini trajectories），训练集和测试集互不重叠。在BFCL v3（BFCL v3）和Tool-Reflection-Bench上的实验表明，多轮工具调用成功率和错误恢复能力大幅提升，冗余调用也有所减少。这些结果表明，将反思明确化并直接优化它，可以提高工具交互的可靠性，并为智能体提供从失败中学习的可复现路径。"
                },
                {
                    "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning",
                    "arxiv_id": "2509.18169",
                    "authors": "Hengbo Xiao, Jingyuan Fan, Xin Tong, Jingzhao Zhang, Chao Lu, Guannan He",
                    "summary": "Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出PiMoE（物理隔离的专家混合）架构，用于将高精度计算能力内生性地集成到大语言模型中，以增强其推理能力。从筛选标准来看： 第一步（核心判断）：论文本质上是改进LLM的基础推理能力，而非将其作为工具应用到特定领域。PiMoE通过令牌级路由实现计算和推理的迭代交替，属于增强LLM通用推理能力的新范式，类似思维链(CoT)的扩展，因此应保留。 第二步（正面指标）：论文包含多个相关主题，包括核心概念\"large language models (LLMs)\"、能力方向\"reasoning\"，以及新兴范式\"multi-agent systems\"和\"tool use\"的讨论。 第三步（排除标准）：论文未主要聚焦于多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域。虽然提及\"scientific or industrial intelligent systems\"，但这是潜在应用场景而非研究焦点。 第四步（特殊和模糊情况）：论文确实涉及智能体/工具使用，但不是将其应用于特定领域，而是提出一种通用的架构来增强LLM的通用问题解决能力，这与筛选标准中\"提出通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况相符。 综上所述，PiMoE论文直接致力于提高大语言模型本身的通用推理能力，通过新的架构设计实现计算与推理的融合，完全符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型无法内在集成高精度数值计算的问题。针对科学计算与推理任务场景，我们提出了一种PiMoE (Physically-isolated Mixture of Experts)架构，通过token级别路由实现高精度计算与推理的集成。在电池容量预测和电池利润计算任务上，通过MSE、响应延迟、token使用量和GPU能耗等指标验证了其有效性，结果表明PiMoE不仅比直接微调LLMs准确性更高，还比多智能体方法在效率方面有显著改善。",
                    "summary_translation": "复杂系统（complex systems）通常依赖高精度数值计算（high-precision numerical computation）来支持决策，但当前的大型语言模型（large language models, LLMs）还无法在现有架构中将这种计算作为一种内在且可解释的能力（intrinsic and interpretable capability）加以整合。主流的多智能体方法（multi-agent approaches）可以利用外部专家（external experts），但不可避免地引入通信开销（communication overhead），并存在多模态涌现能力（multimodal emergent capability）效率低下和可扩展性（scalability）有限的问题。为此，我们提出了PiMoE（物理隔离的专家混合，Physically-isolated Mixture of Experts），一种用于整合计算和推理（computation and reasoning）的训练和推理架构（inference architecture）。不同于工具调用（tool invocation）的工作流范式（workflow paradigm），PiMoE在分别训练专家、文本到计算模块（text-to-computation module）和路由器（router）后，将计算能力内生整合（endogenously integrates）到神经网络中。在推理（inference）阶段，路由器在词元级别（token level）引导计算和推理，从而在单一思维链（chain of thought）内实现迭代交替（iterative alternation）。我们在两项推理-计算任务（reasoning-computation tasks）上评估了PiMoE，并与LLM微调（LLM finetuning）和多智能体系统方法（multi-agent system approaches）进行了比较。结果表明，PiMoE架构不仅比直接微调LLMs实现了更高的准确度（accuracy），而且与主流多智能体方法相比，在响应延迟（response latency）、词元使用量（token usage）和GPU能耗（GPU energy consumption）方面也有显著改进。PiMoE为下一代科学或工业智能系统（intelligent systems）提供了一种高效（efficient）、可解释（interpretable）且可扩展（scalable）的范式（paradigm）。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 17,
            "papers": [
                {
                    "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT",
                    "arxiv_id": "2509.19284",
                    "authors": "Yunzhen Feng, Julia Kempe, Cheng Zhang, Parag Jain, Anthony Hartshorn",
                    "summary": "Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the \"longer-is-better\" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy. As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是研究什么构成了有效的思维链(CoT)推理，并提出了一种新的方法来评估和改进CoT的质量。论文通过系统评估发现，更长的CoT并不总是更好的，并引入了失败步骤分数(FSF)这一指标来预测CoT的正确性。作者还设计了两种干预措施来测试因果关系，证明有效的CoT是那些\"失败较少\"的，并支持\"结构感知\"的测试时扩展。这直接关系到改进LLM的基础推理能力，特别是数学和科学推理能力，属于通用推理能力的范畴。论文不涉及任何需要排除的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然论文在数学和科学推理上进行了评估，但这些只是作为评估推理能力的基准领域，而非论文的应用目标。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在探究什么特征使思维链(CoT)推理有效。针对大型推理模型(LRMs)的长CoT推理场景，我们提出了一种基于图视图的CoT结构分析方法，定义了Failed-Step Fraction (FSF)作为核心指标，并在HARP和GPQA-Diamond数据集上通过条件相关分析、测试时选择和CoT编辑实验验证了其有效性。",
                    "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在测试阶段花费大量计算资源于长思维链（chain-of-thought, CoT）轨迹，但什么*特征*构成了有效的CoT仍不明确。尽管先前的研究报告称通过延长CoT和通过附加*等待*（wait）标记增加回顾（revisiting earlier steps）可以带来性能提升，但近期研究表明，更短的思考可能优于更长的轨迹。因此，我们在数学和科学推理任务上对十种LRM进行了系统评估。与\"越长越好\"的叙述相反，我们发现无论是简单的CoT延长还是增加回顾都与*更低*的准确率相关。随着CoT逐步展开，标记级别的指标可能会将冗长性与过程质量混为一谈。我们引入了CoT的图视图来提取结构，并识别出一个单一统计量——*失败步骤比例（Failed-Step Fraction, FSF）*，即被放弃分支中的步骤比例，该统计量在预测模型正确性方面始终优于长度和回顾比例。为了探究因果关系，我们设计了两种干预措施。首先，我们在测试时根据每个指标对候选CoT进行排序，其中FSF产生了最大的pass@1增益；其次，我们编辑CoT以移除失败的分支，这显著提高了准确率，表明失败的分支会影响后续推理。综合来看，这些结果表明有效的CoT是那些*失败更少*的CoT，并支持在测试阶段采用*结构感知*（structure-aware）的扩展策略，而非不加选择地生成长CoT。"
                },
                {
                    "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws",
                    "arxiv_id": "2509.19189",
                    "authors": "Binghui Li, Fengling Chen, Zixun Huang, Lean Wang, Lei Wu",
                    "summary": "Scaling laws have played a cornerstone role in guiding the training of large language models (LLMs). However, most existing works on scaling laws primarily focus on the final-step loss, overlooking the loss dynamics during the training process and, crucially, the impact of learning rate schedule (LRS). In this paper, we aim to bridge this gap by studying a teacher-student kernel regression setup trained via online stochastic gradient descent (SGD). Leveraging a novel intrinsic time viewpoint and stochastic differential equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL), which characterizes the evolution of population risk during the training process for general LRSs. Remarkably, the impact of the LRSs is captured through an explicit convolution-type functional term, making their effects fully tractable. To illustrate the utility of FSL, we analyze three widely used LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under both data-limited and compute-limited regimes. We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as (i) higher-capacity models are more data- and compute-efficient; (ii) learning rate decay can improve training efficiency; (iii) WSD-like schedules can outperform direct-decay schedules. Lastly, we explore the practical relevance of FSL as a surrogate model for fitting, predicting and optimizing the loss curves in LLM pre-training, with experiments conducted across model sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen the understanding of LLM pre-training dynamics and provide insights for improving large-scale model training.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出\"功能缩放定律\"(FSL)框架，用于理解和优化大语言模型的训练动态，特别是学习率计划(LLM训练中的关键超参数)如何影响训练过程。虽然论文没有直接研究推理能力或解决问题等具体能力，但它关注的是LLM的基础训练机制，属于\"改进LLM的基础能力\"和\"提出新的训练范式\"的范畴。论文通过优化训练过程和学习率计划，为提高LLM的通用能力提供了理论基础和实践指导。论文明确研究LLM训练，不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标，因为它通过改进训练方法来提升模型的基础能力，而非将LLM作为工具应用到特定领域。",
                    "summary2": "本文旨在揭示学习率调度(LRS)在大型语言模型(LLM)训练中的影响机制。针对LLM预训练过程中的损失动态和缩放行为，我们提出了一种功能性缩放定律(FSL)，通过内禀时间随机微分方程(SDE)建模，明确刻画了LRS对训练过程的影响。在teacher-student核回归设置和0.1B到1B参数的LLM上，通过损失曲线拟合和预测验证了FSL的有效性，为LLM预训练提供了理论指导和实践洞见。",
                    "summary_translation": "缩放定律（scaling laws）在指导大语言模型（large language models, LLMs）训练中扮演着基石角色。然而，大多数关于缩放定律的现有研究主要关注最终步骤损失（final-step loss），忽视了训练过程中的损失动态，以及至关重要的学习率计划（learning rate schedule, LRS）的影响。在本文中，我们旨在通过研究通过在线随机梯度下降（online stochastic gradient descent, SGD）训练的教师-学生核回归（teacher-student kernel regression）设置来弥合这一差距。利用新颖的内在时间观点（intrinsic time viewpoint）和SGD的随机微分方程（stochastic differential equation, SDE）建模，我们引入了功能缩放定律（Functional Scaling Law, FSL），该定律表征了在一般LRS下训练过程中总体风险（population risk）的演变。值得注意的是，LRS的影响通过一个显式的卷积型功能项（convolution-type functional term）被捕获，使其效果完全可追踪。为了说明FSL的实用性，我们在数据有限（data-limited）和计算有限（compute-limited）两种情况下分析了三种广泛使用的LRS——恒定（constant）、指数衰减（exponential decay）和预热稳定衰减（warmup-stable-decay, WSD）。我们为LLMs预训练中广泛采用的实证实践提供了理论依据，例如：(i) 更高容量的模型在数据和计算利用上更高效；(ii) 学习率衰减可以提高训练效率；(iii) 类似WSD的计划可以优于直接衰减（direct-decay）计划。最后，我们探索了FSL作为LLMs预训练中损失曲线拟合、预测和优化的代理模型（surrogate model）的实际相关性，实验在参数规模从0.1B到1B的模型上进行。我们希望我们的FSL框架能够深化对LLMs预训练动态的理解，并为改进大规模模型训练提供见解。"
                },
                {
                    "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio",
                    "arxiv_id": "2509.19128",
                    "authors": "Alexandre Piché, Ehsan Kamaloo, Rafael Pardinas, Dzmitry Bahdanau",
                    "summary": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs). However, effectively scaling these RL methods presents significant challenges, primarily due to the difficulty in maintaining high AI accelerator utilization without generating stale, off-policy data that harms common RL algorithms. This paper introduces PipelineRL, an approach designed to achieve a superior trade-off between hardware efficiency and data on-policyness for LLM training. PipelineRL employs concurrent asynchronous data generation and model training, distinguished by the novel in-flight weight updates. This mechanism allows the LLM generation engine to receive updated model weights with minimal interruption during the generation of token sequences, thereby maximizing both the accelerator utilization and the freshness of training data. Experiments conducted on long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL achieves approximately $\\sim 2x$ faster learning compared to conventional RL baselines while maintaining highly on-policy training data. A scalable and modular open-source implementation of PipelineRL is also released as a key contribution.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的强化学习训练范式(PipelineRL)来增强LLM的推理能力。论文明确提到\"Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs)\"，表明其核心是提升LLM的推理能力这一基础能力。 其次，论文包含多个正面指标：核心概念方面直接涉及Large language models (LLMs)；能力方向上关注reasoning；训练方法上专注于reinforcement learning，这些都是研究目标中明确关注的重点。 第三，论文完全避开了排除标准中的所有领域，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文虽然不直接涉及智能体/工具使用或幻觉/可解释性/安全等问题，但通过提高训练效率和数据新鲜度(on-policyness)来改进RL训练质量，这从根本上提升了LLM的推理能力，而非仅作为应用层面的优化。 综上所述，PipelineRL论文的核心贡献是提出了一种更高效的RL训练方法来提升LLM的推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决强化学习在长序列生成中面临的硬件效率与数据策略性之间的权衡问题。针对大型语言模型的长序列生成场景，我们提出了一种PipelineRL方法，通过并发异步数据生成和模型训练，并引入创新的in-flight weight updates机制，在128个H100 GPU上通过学习速度和任务成功率等指标验证了其有效性，实现了约2倍的学习速度提升。",
                    "summary_translation": "强化学习 (Reinforcement Learning, RL) 越来越多地被用于增强大语言模型 (Large Language Models, LLMs) 的推理能力。然而，有效扩展这些强化学习方法面临着重大挑战，主要源于在保持高AI加速器利用率的同时难以避免产生陈旧的、非策略数据 (off-policy data)，而这些数据会损害常见的强化学习算法。本文介绍了PipelineRL，这是一种旨在为大语言模型训练实现硬件效率与数据策略性 (data on-policyness) 之间更优权衡的方法。PipelineRL采用并发的异步数据生成和模型训练，其特点是通过新颖的飞行中权重更新 (in-flight weight updates) 机制。这一机制使大语言模型生成引擎能够在生成token序列期间以最小中断接收更新的模型权重，从而最大化加速器利用率和训练数据的新鲜度。在128个H100 GPU上进行的长格式推理任务实验表明，与传统的强化学习基线相比，PipelineRL实现了约2倍的学习速度提升，同时保持了高度策略性的训练数据。作为一项重要贡献，研究团队还发布了PipelineRL的可扩展、模块化开源实现。"
                },
                {
                    "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment",
                    "arxiv_id": "2509.19104",
                    "authors": "Sharan Sahu, Martin T. Wells",
                    "summary": "Reinforcement learning with human feedback (RLHF) has become crucial for aligning Large Language Models (LLMs) with human intent. However, existing offline RLHF approaches suffer from overoptimization, where models overfit to reward misspecification and drift from preferred behaviors observed during training. We introduce DRO-REBEL, a unified family of robust REBEL updates with type-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality, each update reduces to a simple relative-reward regression, preserving scalability and avoiding PPO-style clipping or auxiliary value networks. Under standard linear-reward and log-linear policy classes with a data-coverage condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$ rate via a localized Rademacher complexity analysis. The same analysis closes the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal parametric rates. We derive practical SGD algorithms for all three divergences: gradient regularization (Wasserstein), importance weighting (KL), and a fast 1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong worst-case robustness across unseen preference mixtures, model sizes, and data scales, with $\\chi^2$-REBEL showing consistently strong empirical performance. A controlled radius--coverage study validates a no-free-lunch trade-off: radii shrinking faster than empirical divergence concentration rates achieve minimax-optimal parametric rates but forfeit coverage, while coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出一种新的强化学习对齐方法(DRO-REBEL)，用于改进LLM与人类意图的对齐效果。论文核心是解决现有RLHF方法中的过优化问题，提出了一种分布鲁棒相对奖励回归的新训练范式。这属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用到特定领域。因此，根据第一步的判断标准，应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念：明确提到了Large Language Models (LLMs) - 训练方法：重点研究了Reinforcement Learning with Human Feedback (RLHF)的改进，属于强化学习优化范畴 虽然论文没有直接提到reasoning、planning等能力方向，但RLHF作为提升LLM与人类意图对齐的方法，本质上可以增强模型的通用推理能力。 第三步：排除标准 论文不涉及任何排除领域： - 没有涉及多模态与视觉相关内容 - 没有针对特定应用领域（如医疗、化学、生物等） - 没有主要关注模型可靠性方面的应用层面研究（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及智能体/工具使用、幻觉/可解释性/安全等特殊或模糊情况。论文提出的DRO-REBEL方法是从根本上提升LLM与人类意图对齐的能力，属于基础能力改进。 最终决策 综合以上分析，这篇论文的核心贡献是提出了一种新的强化学习对齐方法，用于改进LLM的基础能力，使其更好地与人类意图对齐。这符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标，因为更好的对齐可以提升模型在各种推理任务中的表现。因此，最终判断为True。",
                    "summary2": "",
                    "summary_translation": "强化学习与人类反馈(Reinforcement learning with human feedback, RLHF)已成为使大型语言模型(Large Language Models, LLMs)与人类意图对齐的关键技术。然而，现有的离线RLHF方法存在过度优化问题，即模型过度拟合于奖励误指定(reward misspecification)，并偏离训练过程中观察到的偏好行为。我们提出了DRO-REBEL，这是一个统一的鲁棒REBEL更新家族，包含p型Wasserstein、KL(Kullback-Leibler)和χ²(chi-square)模糊集。利用Fenchel对偶性(Fenchel duality)，每次更新都简化为简单的相对奖励回归(relative-reward regression)，保持了可扩展性，并避免了PPO(Proximal Policy Optimization)风格裁剪或辅助价值网络。在标准线性奖励和对数线性策略类以及数据覆盖条件下，我们建立了$O(n^{-1/4})$的估计界限，其常数比先前的DRO-DPO(Direct Preference Optimization)方法更紧，并通过局部化Rademacher复杂性分析(localized Rademacher complexity analysis)恢复了极小极大最优的$O(n^{-1/2})$速率。同样的分析填补了Wasserstein-DPO和KL-DPO的差距，表明两者也达到了最优参数速率。我们为所有三种散度推导出实用的SGD(Stochastic Gradient Descent)算法：梯度正则化(Wasserstein)、重要性加权(KL)和快速一维对偶求解(χ²)。在情感对齐(Emotion Alignment)、大规模ArmoRM多目标基准和HH-Alignment上的实验表明，该方法在未见过的偏好混合、模型大小和数据规模上均展现出强大的最坏情况鲁棒性，其中χ²-REBEL表现出一致强大的经验性能。一项受控的半径-覆盖研究验证了没有免费午餐的权衡(no-free-lunch trade-off)：比经验散度集中速率更快的收缩半径可实现极小极大最优参数速率，但会放弃覆盖，而保证覆盖的半径则会产生$O(n^{-1/4})$的速率。"
                },
                {
                    "title": "Reflect before Act: Proactive Error Correction in Language Models",
                    "arxiv_id": "2509.18607",
                    "authors": "Qiuhai Zeng, Sarvesh Rajkumar, Di Wang, Narendra Gyanchandani, Wenbo Yan",
                    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in interactive decision-making tasks, but existing methods often struggle with error accumulation and lack robust self-correction mechanisms. We introduce \"Reflect before Act\" (REBACT), a novel approach that enhances LLM-based decision-making by introducing a critical reflect step prior to taking the next action. This approach allows for immediate error correction, ensuring smooth action path and adaptibity to environment feedback. We evaluate REBACT on three diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results demonstrate that REBACT significantly outperforms strong baselines, improving success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld (achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's performance improvements are achieved with only a few modification steps, demonstrating its computational efficiency.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合我的研究目标，理由如下： 首先，从核心判断来看，论文本质上是提出一种名为\"Reflect before Act\" (REBACT)的新方法，通过在行动前引入反思步骤来增强语言模型的决策能力。这明显属于改进LLM基础能力和增强其通用推理能力的范畴，特别是通过自我纠错机制来提升模型的逻辑推理能力，而不是将LLM作为工具应用到特定领域。 其次，从正面指标分析，论文明确提到了\"Large Language Models (LLMs)\"这一核心概念，并且关注的是\"decision-making\"能力，这与推理和问题解决密切相关。虽然未直接提及reasoning，但反思和纠错机制本质上是在提升模型的推理质量。 第三，从排除标准看，论文不涉及多模态与视觉内容，也没有专注于特定应用领域（如医疗、化学等）。虽然提到了ALFWorld、WebShop和TextCraft三个评估环境，但这些是用于测试通用能力的基准环境，而非特定应用领域。 最后，在特殊和模糊情况处理上，REBACT是一种通用的反思机制，用于增强LLM的通用问题解决能力，而不是针对特定领域的应用。论文关注的是如何通过反思步骤提升模型的内在纠错能力，从而从根本上提高模型的推理质量，这完全符合\"提升LLM通用推理能力\"的研究目标。 综上所述，这篇论文的核心贡献是提出了一种通过前瞻性反思来增强LLM自我纠错能力的新方法，属于提升大语言模型通用推理能力的研究范畴，因此应该被保留。",
                    "summary2": "本文旨在解决大型语言模型在交互式决策任务中的错误累积和缺乏自我纠正机制的问题。针对交互式决策环境，我们提出了一种REBACT（Reflect before Act）方法，在执行下一个动作前先反思并修正之前动作中的错误，并在ALFWorld、WebShop和TextCraft三个数据集上通过成功率（success rate）验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）已在交互式决策任务中展现出显著能力，但现有方法常常面临错误累积问题，且缺乏强大的自我纠正机制。我们提出了\"Reflect before Act\"（REBACT，行动前反思）方法，这是一种通过在采取下一步行动前引入关键反思步骤来增强基于LLM的决策的新方法。该方法能够实现即时错误纠正，确保行动路径的顺畅性以及对环境反馈的适应性。我们在三个多样化的交互环境中对REBACT进行了评估：ALFWorld、WebShop和TextCraft。我们的结果表明，使用Claude3.5-sonnet作为底层LLM，REBACT显著优于强基线方法，在WebShop上的成功率提高了24%（达到61%），在ALFWorld上提高了6.72%（达到98.51%），在TextCraft上提高了0.5%（达到99.5%）。进一步分析表明，REBACT的性能提升仅需少量修改步骤即可实现，这证明了其计算效率。"
                },
                {
                    "title": "DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns",
                    "arxiv_id": "2509.18164",
                    "authors": "Ranfei Chen, Ming Chen",
                    "summary": "Diffusion large language models (dLLMs) have emerged as a new architecture following auto regressive models. Their denoising process offers a powerful generative advantage, but they present significant challenges in learning and understanding numerically sensitive mathematical and order-sensitive logical tasks. Current training methods, including pre-training, fine-tuning, and reinforcement learning, focus primarily on improving general knowledge retention and reasoning abilities, but lack a comprehensive understanding of mathematical and logical patterns. We propose DSFT, a simple yet effective Diffusion SFT strategy, by adjusting the masking strategy and loss function, guiding models to understand mathematical and logical patterns. This strategy can be flexibly combined with pre-training, reinforcement learning, and other training methods. Validated on models such as LLaDA and Dream series, we prove that DSFT on small-scale data can achieve improvements of 5-10% and approximately 2% on mathematical and logical problems, respectively. This inspiring masking approach offers insights for future learning of specific patterns, which can be easily and efficiently combined with other training methods and applied to various dLLMs. Our code is publicly available at https://anonymous.4open.science/r/DSFT-0FFB/",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出DSFT（Diffusion SFT）策略，通过调整掩码策略和损失函数来增强扩散大语言模型(dLLMs)对数学和逻辑模式的理解能力。论文明确关注LLM的通用推理能力，特别是数学推理和逻辑推理，这完全符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的要求。论文不是将LLM作为工具应用到特定领域，而是直接提升模型本身的推理能力，且不涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性的应用层面研究）。此外，论文提出的方法可以与预训练、强化学习等其他训练方法结合，显示出其作为提升LLM通用推理能力的普适性。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决扩散大语言模型在数学和逻辑模式理解方面的挑战。针对数值敏感的数学任务和顺序敏感的逻辑任务，我们提出了一种DSFT(Diffusion SFT)策略，通过调整掩码策略和损失函数指导模型理解数学和逻辑模式，并在LLaDA和Dream系列模型上通过数学(GSM8K, MATH, GPQA)、逻辑(BBH, ARC-C, WinoGrande)和一般任务基准验证了其有效性，实现了数学问题5-10%和逻辑问题约2%的准确率提升。",
                    "summary_translation": "扩散大型语言模型（Diffusion large language models, dLLMs）是继自回归模型（auto regressive models）之后出现的一种新架构。它们的去噪过程（denoising process）提供了强大的生成优势（generative advantage），但在学习和理解对数值敏感的数学任务（numerically sensitive mathematical tasks）和对顺序敏感的逻辑任务（order-sensitive logical tasks）方面面临重大挑战。当前的训练方法，包括预训练（pre-training）、微调（fine-tuning）和强化学习（reinforcement learning），主要侧重于提高一般知识保留（knowledge retention）和推理能力（reasoning abilities），但缺乏对数学和逻辑模式的全面理解。我们提出了DSFT，一种简单而有效的扩散SFT策略（Diffusion SFT strategy），通过调整掩码策略（masking strategy）和损失函数（loss function），引导模型理解数学和逻辑模式。该策略可以灵活地与预训练、强化学习和其他训练方法结合使用。在LLaDA和Dream系列等模型上验证，我们证明在小规模数据上使用DSFT可以在数学问题和逻辑问题上分别实现5-10%和约2%的改进。这种启发性的掩码方法为未来特定模式的学习提供了见解，可以轻松高效地与其他训练方法结合，并应用于各种dLLMs。我们的代码公开可获取，网址为https://anonymous.4open.science/r/DSFT-0FFB/"
                },
                {
                    "title": "Towards Provable Emergence of In-Context Reinforcement Learning",
                    "arxiv_id": "2509.18389",
                    "authors": "Jiuqi Wang, Rohan Chandra, Shangtong Zhang",
                    "summary": "Typically, a modern reinforcement learning (RL) agent solves a task by updating its neural network parameters to adapt its policy to the task. Recently, it has been observed that some RL agents can solve a wide range of new out-of-distribution tasks without parameter updates after pretraining on some task distribution. When evaluated in a new task, instead of making parameter updates, the pretrained agent conditions its policy on additional input called the context, e.g., the agent's interaction history in the new task. The agent's performance increases as the information in the context increases, with the agent's parameters fixed. This phenomenon is typically called in-context RL (ICRL). The pretrained parameters of the agent network enable the remarkable ICRL phenomenon. However, many ICRL works perform the pretraining with standard RL algorithms. This raises the central question this paper aims to address: Why can the RL pretraining algorithm generate network parameters that enable ICRL? We hypothesize that the parameters capable of ICRL are minimizers of the pretraining loss. This work provides initial support for this hypothesis through a case study. In particular, we prove that when a Transformer is pretrained for policy evaluation, one of the global minimizers of the pretraining loss can enable in-context temporal difference learning.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，理由如下： 第一步：核心判断 这篇论文的核心是关于强化学习(RL)预训练如何使大语言模型获得上下文强化学习(ICRL)能力的研究。论文探讨的是为什么RL预训练算法能够产生支持ICRL的网络参数，并提出了假设：能够进行ICRL的参数是预训练损失的最小值。这属于改进LLM基础能力和提出新训练范式的研究，特别是关注模型如何在不更新参数的情况下通过上下文信息进行学习和推理，这与通用推理能力直接相关。 第二步：正面指标 论文包含多个正面指标： - 核心概念：论文研究的是Transformer模型在强化学习中的表现，Transformer是大语言模型的基础架构 - 能力方向：论文关注的是推理能力，特别是上下文学习(in-context learning)和时序差分学习(temporal difference learning)，这些都是通用推理能力的重要组成部分 - 训练方法：论文探讨强化学习预训练方法，这与RLHF/RL相关 第三步：排除标准 论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉 - 不针对特定应用领域（如医疗、化学等） - 不主要关注模型基础设施或部署优化 第四步：处理特殊和模糊情况 论文研究的是一种通用的学习机制（上下文强化学习），而不是将其应用于特定领域。它探讨的是模型如何通过预训练获得在不更新参数的情况下适应新任务的能力，这属于提升模型内在推理能力的研究。 第五步：最终决策 综合分析，这篇论文的核心贡献是研究大语言模型如何通过预训练获得上下文强化学习能力，这是一种重要的通用推理能力。论文探讨的是模型的基础能力和训练范式，而不是将模型作为工具应用于特定领域。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决上下文强化学习（ICRL）中为何强化学习预训练能产生支持ICRL的网络参数的问题。针对策略评估任务，我们提出了一种基于线性Transformer的理论分析方法，证明了预训练损失的全局最小化器能实现上下文时序差分学习，并在Boyan's chain环境中通过MSVE指标验证了随着上下文长度增加，模型性能提升且收敛参数与理论预测一致。",
                    "summary_translation": "通常，现代强化学习（reinforcement learning, RL）智能体通过更新其神经网络参数来调整策略以解决任务。最近，人们观察到一些RL智能体在某个任务分布上进行预训练后，无需参数更新就能解决各种新的分布外（out-of-distribution）任务。在新任务中评估时，预训练的智能体不进行参数更新，而是将其策略基于称为上下文（context）的额外输入，例如智能体在新任务中的交互历史。随着上下文中信息的增加，智能体的性能会提高，而智能体的参数保持固定。这种现象通常被称为上下文强化学习（in-context RL, ICRL）。智能体网络的预训练参数使得显著的ICRL现象成为可能。然而，许多ICRL研究使用标准RL算法进行预训练。这引出了本文旨在解决的核心问题：为什么RL预训练算法能够生成使ICRL成为可能的网络参数？我们假设能够实现ICRL的参数是预训练损失的最小值点（minimizers）。本研究通过案例研究为这一假设提供了初步支持。具体而言，我们证明了当Transformer（一种神经网络架构）为策略评估（policy evaluation）进行预训练时，预训练损失的一个全局最小值点（global minimizers）可以实现上下文时序差分学习（in-context temporal difference learning）。"
                },
                {
                    "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing",
                    "arxiv_id": "2509.20336",
                    "authors": "Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang",
                    "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合研究范围，应该被保留。我的判断过程如下： 第一步：核心判断 这篇论文的本质是研究Transformer-based LLMs在图推理任务中的内部机制和工作原理。论文使用了circuit-tracer框架来解释decoder-only transformers的推理过程，识别了图推理中的两个核心机制（token merging和structural memorization），并提供了一个统一的可解释性框架。这属于研究LLM基础能力和推理机制的范畴，而不是将LLM作为工具应用到特定领域，因此应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念：明确提到了\"Transformer-based LLMs\" - 能力方向：聚焦于\"graph reasoning\"，属于推理能力的研究范畴 虽然论文没有提到强化学习等训练方法或智能体等新兴范式，但已经包含了两个重要的正面指标。 第三步：排除标准 论文没有主要聚焦于任何排除标准中提到的领域： - 不涉及多模态与视觉研究 - 不针对特定应用领域（如医疗、化学等），虽然研究图推理，但这是作为理解LLM通用推理能力的一个窗口 - 不关注模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况 论文涉及可解释性研究，但这是为了理解LLM的推理机制，提供\"统一的可解释性框架来理解结构推理\"，从而提升对模型内部工作原理的理解，符合研究目标，应该保留。 综上所述，这篇论文的核心贡献是揭示和解释LLM在图推理任务中的内部机制，这直接关系到提高LLM的通用推理能力，符合研究目标。",
                    "summary2": "抱歉，我无法根据提供的内容生成学术总结。提供的链接返回了404错误，显示\"File unavailable for 2509.20336\"，表明该论文ID对应的文件在arXiv上不可用或不存在。没有实际的论文内容，我无法提取研究问题、方法创新和实验验证等关键信息来生成专业的学术总结。请提供有效的论文链接或内容，我将很乐意为您生成符合要求的学术总结。",
                    "summary_translation": "基于Transformer的大型语言模型（Transformer-based LLMs）在图推理任务（graph reasoning tasks）上表现出强大的性能，然而其内部机制（internal mechanisms）仍未被充分探索。为了以基础且统一的视角揭示这些推理过程机制（reasoning process mechanisms），我们使用了基本的仅解码器Transformer（basic decoder-only transformers），并采用电路追踪框架（circuit-tracer framework）对其进行解释。通过这一视角，我们可视化推理轨迹（reasoning traces），并识别出图推理中的两个核心机制：令牌合并（token merging）和结构记忆（structural memorization），这两个机制是路径推理（path reasoning）和子结构提取任务（substructure extraction tasks）的基础。我们进一步量化了这些行为（behaviors），并分析了它们如何受到图密度（graph density）和模型规模（model size）的影响。我们的研究为理解仅解码器Transformer（decoder-only Transformers）中的结构推理（structural reasoning）提供了一个统一的可解释性框架（unified interpretability framework）。"
                },
                {
                    "title": "Failure Modes of Maximum Entropy RLHF",
                    "arxiv_id": "2509.20265",
                    "authors": "Ömer Veysel Çağatan, Barış Akgün",
                    "summary": "In this paper, we show that Simple Preference Optimization (SimPO) can be derived as Maximum Entropy Reinforcement Learning with length-normalized temperature, providing a theoretical foundation for this reference-free method. Motivated by SimPO's strong performance in offline preference optimization, we investigate whether Maximum Entropy RL can achieve similar results in online RLHF settings. Our experiments find that Maximum Entropy RL consistently exhibits overoptimization and unstable KL dynamics, even at very low learning rates. Unlike KL-constrained methods that maintain stable training, entropy regularization fails to prevent reward hacking and appears to correlate with overoptimization. Lastly, we discuss possible explanations for why SimPO succeeds in offline settings while Maximum Entropy RL struggles in online scenarios. Our findings suggest that reference-free approaches may face distinct challenges when applied to online or offline preference learning.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，论文的核心是关于RLHF（Reinforcement Learning from Human Feedback）的研究，特别是最大熵RLHF的失败模式分析。RLHF是改进大语言模型基础能力的关键训练范式，属于\"强化学习优化\"这一类别，符合第一步的保留标准。论文虽然没有直接提出新方法，而是分析现有方法的问题，但这种分析有助于理解如何更好地训练LLM，从而提升其通用能力。其次，论文明确涉及强化学习（RLHF）这一训练方法，符合第二步中的正面指标。同时，论文不聚焦于多模态、特定应用领域或模型可靠性的应用层面，因此不触犯第三步的排除标准。总体而言，这篇论文对改进LLM的训练方法有贡献，与提升大语言模型通用推理能力的研究目标相符。",
                    "summary2": "本文旨在研究最大熵强化学习在人类反馈强化学习(RLHF)中的失效模式。针对在线和离线偏好学习场景，我们提出了一种将SimPO解释为长度归一化温度的最大熵RL的理论框架，并在TL;DR数据集上通过胜率和KL散度等指标验证了其有效性。实验发现，尽管SimPO在离线设置中表现良好，但在线最大熵RL存在过优化和不稳定问题，表明熵正则化无法有效防止奖励 hacking。",
                    "summary_translation": "本文表明，简单偏好优化（Simple Preference Optimization, SimPO）可被推导为具有长度归一化温度的最大熵强化学习（Maximum Entropy Reinforcement Learning），为这种无参考方法（reference-free method）提供了理论基础。受SimPO在离线偏好优化中出色表现的启发，我们研究了最大熵强化学习是否能在在线RLHF（基于人类反馈的强化学习）设置中取得类似结果。我们的实验发现，即使在非常低的学习率下，最大熵强化学习也始终表现出过度优化（overoptimization）和不稳定的KL（Kullback-Leibler）动态。与能够保持稳定训练的KL约束方法不同，熵正则化（entropy regularization）未能防止奖励黑客（reward hacking），并且似乎与过度优化相关。最后，我们讨论了为什么SimPO在离线设置中成功而最大熵强化学习在在线场景中挣扎的可能解释。我们的研究结果表明，无参考方法在应用于在线或离线偏好学习时可能面临不同的挑战。"
                },
                {
                    "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning",
                    "arxiv_id": "2509.19894",
                    "authors": "Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong",
                    "summary": "Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at https://github.com/inclusionAI/PromptCoT.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提出PromptCoT 2.0框架，这是一种新的训练范式，旨在通过合成高质量提示词来增强LLM的基础推理能力。论文明确关注提升LLM在数学推理和编程推理等通用能力上的表现，而非将LLM作为工具应用于特定领域。 从正面指标看，论文包含多个相关主题：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于reasoning(特别是数学推理)、planning和problem-solving；训练方法上涉及self-play(类似强化学习的思想)和self-evolve(模型通过自我博弈自主改进)。 论文不涉及任何排除标准中的领域：没有关注多模态与视觉，没有将LLM应用于特定领域(虽然使用数学和编程作为评估任务，但这些是用于评估通用推理能力的基准)，也没有主要关注模型可靠性的应用层面。 在特殊情况下，论文提到的self-play可以视为一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非应用于特定领域的智能体。 论文的核心贡献是提出了一种可扩展的框架，通过迭代改进提示词构建来生成更难且更多样化的问题，从而提升LLM的推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决大型语言模型推理任务中高质量训练数据短缺的问题。针对数学和编程领域的数据需求，我们提出了一种基于期望最大化(EM)循环优化的PromptCoT 2.0框架，通过迭代改进推理来指导提示构建，生成更难且更多样化的问题。在AIME、HMMT、LiveCodeBench和Codeforces等六个基准测试上，通过pass@1准确率和Elo评级验证了其有效性，在Self-Play和SFT两种设置下均取得了最先进结果。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）正在从对话系统演变为强大的推理器，用于处理奥林匹克数学和竞技编程等任务。虽然扩大参数规模和测试时计算（test-time computation）推动了进步，但一个关键瓶颈是缺乏高质量的训练问题：人工策划的数据集成本高且有限，而现有的合成语料库（synthetic corpora）通常过于简单或过于狭窄。\n\nPromptCoT 1.0表明，将推理过程（rationales）注入提示合成（prompt synthesis）可以增加问题难度。在此基础上，我们提出了PromptCoT 2.0，这是一个可扩展的框架，用期望最大化（expectation-maximization, EM）循环替代手工设计的启发式算法（hand-crafted heuristics），其中推理过程被迭代优化以指导提示构建。这产生的比先前语料库更难且更多样化的问题。\n\n这些合成提示支持两种后训练（post-training）机制：（1）自我对弈（Self-Play），即强模型通过可验证的反馈（verifiable feedback）在没有更强教师的情况下自主改进；（2）监督微调（Supervised Fine-Tuning, SFT），即弱模型从教师提炼的痕迹（teacher-distilled traces）中学习。\n\n大量实验证明了这种方法的有效性。在自我对弈中，将PromptCoT 2.0应用于Qwen3-30B-A3B-Thinking-2507在300亿参数规模上创造了新的最先进（state-of-the-art）结果，在AIME 24/25和HMMT 25上分别提升了+4.4、+4.8和+5.3，在LiveCodeBench v5/v6上提升了+6.1和+5.0，在Codeforces上提升了+35 Elo。在SFT中，仅在合成提示上训练Qwen2.5-7B-Instruct将准确率提升至73.1（AIME 24）、65.6（AIME 25）和53.4（LiveCodeBench v5），超过了在人工或混合数据上训练的模型。\n\n分析进一步证实，PromptCoT 2.0产生了本质上更难且分布不同（distributionally distinct）的问题。这些结果确立了提示合成作为扩展推理能力的新维度（new axis for scaling reasoning），并将PromptCoT 2.0定位为未来开源模型的可扩展基础。实现代码可在https://github.com/inclusionAI/PromptCoT获取。"
                },
                {
                    "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models",
                    "arxiv_id": "2509.19803",
                    "authors": "Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang",
                    "summary": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步：核心判断 这篇论文的本质是提出一种名为VCRL的课程强化学习框架，用于改进大语言模型的数学推理能力。论文核心关注的是改进LLM的基础推理能力，提出了一种新的训练范式（基于方差的课程强化学习），这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学推理等通用能力\"的保留标准。论文不是将LLM作为工具应用到特定领域，而是专注于提升模型本身的推理能力。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：专注于reasoning，特别是mathematical reasoning - 训练方法：提出了reinforcement learning的新方法(VCRL) 第三步：排除标准 论文不涉及任何需要排除的领域： - 不涉及多模态与视觉相关内容 - 不针对特定应用领域（如医疗、化学等），虽然实验在数学推理任务上进行，但数学推理被视为通用推理能力的基础组成部分 - 不关注模型基础设施、部署优化或硬件加速 第四步：特殊和模糊情况处理 虽然论文在数学推理任务上进行了实验评估，但这并不使其成为特定应用领域的研究。数学推理通常被视为评估和提升LLM通用推理能力的关键基准。论文提出的VCRL是一种通用的课程强化学习框架，其原理可以推广到其他需要逐步学习的推理任务上，因此应被视为对LLM通用推理能力的提升。 综上所述，这篇论文的核心贡献是提出了一种基于方差的课程强化学习方法，通过动态控制训练样本难度来提升LLM的推理能力，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大语言模型在数学推理任务中强化学习训练时没有考虑样本难度匹配的问题。针对不同难度的数学推理样本，我们提出了一种基于方差的课程强化学习框架VCRL，并在五个数学基准测试上通过准确率等指标验证了其有效性。",
                    "summary_translation": "基于策略的强化学习（policy-based reinforcement learning）目前在提高大型语言模型（LLMs）的数学推理任务能力方面发挥着重要作用。然而，现有的基于rollout的强化学习方法（如GRPO、DAPO、GSPO等）未能明确考虑LLMs对不同难度级别样本的学习能力，这与人类从易到难的数学推理认知过程相悖。直观上，我们发现RLVR中rollout组的奖励（reward）方差部分反映了当前样本对LLMs的难度。过于简单或过于困难的样本具有较低的方差，而中等难度的样本则具有较高的方差。基于此，我们提出了VCRL，一种基于组奖励方差动态控制训练样本难度的课程强化学习（curriculum reinforcement learning）框架。在五个数学基准和两个模型上的实验揭示了VCRL相较于当前LLM强化学习基线方法的优势。"
                },
                {
                    "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms",
                    "arxiv_id": "2509.19702",
                    "authors": "Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama",
                    "summary": "We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nyström extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nyström extrapolation, highlighting a powerful capability of in-context learning.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我认为这篇论文符合研究范围。以下是我的判断过程： 第一步：核心判断上，论文本质是研究线性注意力transformer如何通过训练隐式发现数值算法的能力。这属于改进LLM基础能力的研究，特别是增强其数学推理和问题解决能力。论文探索了模型如何通过矩阵补全任务自动学习数学计算规则，而非被明确编程这些规则，这符合\"提出新的训练范式、增强其逻辑、数学推理等通用能力\"的保留标准。 第二步：正面指标方面，论文包含以下相关主题： - 核心概念：提及\"Linear Transformers\"，与LLMs直接相关 - 能力方向：涉及数值算法的发现，与数学推理和问题解决能力密切相关 - 论文展示了transformer的上下文学习能力，这是提升通用推理能力的关键 第三步：排除标准方面，论文不涉及任何排除领域： - 没有多模态与视觉内容 - 没有将LLM应用于特定领域（如医疗、化学等） - 没有聚焦于模型可靠性的应用层面（如水印、安全等） 第四步：论文不涉及特殊或模糊情况（如智能体/工具使用、幻觉/可解释性等），所以无需额外判断。 综合来看，这篇论文的核心贡献在于展示了transformer如何通过训练隐式发现统一的数值算法，这直接提升了LLM的数学推理和问题解决能力，属于\"大语言模型通用推理能力\"的研究范围。论文关注的是模型的基础能力提升，而非特定领域应用，因此符合研究目标。",
                    "summary2": "",
                    "summary_translation": "我们在数百万个masked-block matrix completion tasks（掩码块矩阵补全任务）上训练了一个linear attention transformer（线性注意力Transformer）：每个提示是一个masked low-rank matrix（掩码低秩矩阵），其缺失的块可能是(i)一个scalar prediction target（标量预测目标）或(ii)一个Nyström extrapolation（Nyström外推）的unseen kernel slice（未见核切片）。模型仅看到输入-输出对和mean-squared loss（均方损失）；它没有被给予normal equations（正规方程）、handcrafted iterations（手工设计的迭代），也没有任何关于这些任务相关的提示。令人惊讶的是，训练后，algebraic unrolling（代数展开）揭示了在三个不同的computational regimes（计算机制）中相同的parameter-free update rule（无参数更新规则）：full visibility（完全可见性）、rank-limited updates（秩限制更新）和distributed computation（分布式计算）。我们证明该规则在full-batch problems（全批量问题）上实现了second-order convergence（二阶收敛），降低了distributed iteration complexity（分布式迭代复杂度），并在rank-limited attention（秩限制注意力）下保持准确性。因此，一个仅被训练来补全缺失块的transformer隐式地发现了一个统一的、resource-adaptive iterative solver（资源自适应迭代求解器），涵盖预测、估计和Nyström extrapolation（Nyström外推），突显了in-context learning（上下文学习）的强大能力。"
                },
                {
                    "title": "Mamba Modulation: On the Length Generalization of Mamba",
                    "arxiv_id": "2509.19633",
                    "authors": "Peng Lu, Jerry Huang, Qiuhao Zeng, Xinyu Wang, Boxing Wang, Philippe Langlais, Yufei Cui",
                    "summary": "The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\\exp(-\\sum_{t=1}^N\\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心是研究Mamba架构（一种大语言模型架构）在长上下文泛化方面的局限性，并提出了一种改进方法。论文通过分析Mamba在处理比预训练时更长的上下文时性能下降的问题，将其归因于状态空间动态的参数化，特别是状态转移矩阵A的特性。作者提出了一种频谱缩放方法来改进预训练Mamba模型，以实现稳健的长上下文泛化。这符合我的研究目标，因为论文关注的是改进大语言模型的基础能力（特别是长上下文推理能力），而不是将LLM应用于特定领域。论文的贡献在于增强模型本身的通用推理能力，使其能够更好地处理长序列，这对于复杂的多步推理任务至关重要。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决Mamba模型在处理超出预训练长度的上下文时性能显著下降的问题。针对长上下文泛化场景，我们提出了一种基于状态转移矩阵A频谱缩放的方法，并在ProofPile、PG19等数据集以及Passkey检索和LongBench基准上通过困惑度和准确率等指标验证了其有效性。",
                    "summary_translation": "Transformer模型中注意力机制（attention mechanism）的二次复杂度（quadratic complexity）推动了具有次二次扩展（sub-quadratic scaling）的替代架构的发展，如状态空间模型（state-space models）。其中，Mamba已成为一种领先的架构，在一系列语言建模任务中取得了最先进（state-of-the-art）的成果。\n\n然而，当Mamba应用于比预训练期间所见更长的上下文时，其性能显著下降，显示出对上下文长度扩展的明显敏感性。通过详细分析，我们将这一限制归因于其状态空间动态（state-space dynamics）的分布外行为（out-of-distribution behaviour），特别是在状态转移矩阵（state transition matrix）$\\mathbf{A}$的参数化中。与近期将这种敏感性归因于离散时间步长（discretization time steps）$\\exp(-\\sum_{t=1}^N\\Delta_t)$的消失累积的研究不同，我们建立了当输入长度趋近于无穷大时状态收敛行为与转移矩阵$\\mathbf{A}$的谱（spectrum）之间的联系，为其在长度扩展中的作用提供了充分依据的解释。\n\n接下来，为了克服这一挑战，我们提出了一种将谱缩放（spectrum scaling）应用于预训练Mamba模型的方法，通过选择性调制（modulating）每一层中$\\mathbf{A}$矩阵的谱来实现稳健的长上下文泛化（long-context generalization）。我们表明，在仅仅调制$\\Delta_t$失效的情况下，这种方法可以显著提高性能，验证了我们的见解，并为具有结构化转移矩阵（structured transition matrices）的状态空间模型提供了更好的长度泛化（length generalization）途径。"
                },
                {
                    "title": "Thinking Augmented Pre-training",
                    "arxiv_id": "2509.20186",
                    "authors": "Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei",
                    "summary": "This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\\%$ on several challenging reasoning benchmarks.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文本质是改进LLM的基础能力和通用推理能力。论文提出\"Thinking augmented Pre-Training (TPT)\"方法，通过在文本数据中增加思维轨迹来增强LLM的预训练过程。这是一种新的训练范式，旨在通过逐步推理和分解来提高模型的学习效率和推理能力，而不是将LLM应用于特定领域。 第二步正面指标：论文包含多个正面指标。它明确关注\"Large language models (LLMs)\"这一核心概念，并专注于\"reasoning\"能力方向（论文提到\"step-by-step reasoning\"和\"reasoning benchmarks\"）。实验结果也证明该方法在多个具有挑战性的推理基准上提升了模型性能。 第三步排除标准：论文不涉及任何排除标准中的领域。它没有关注多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性的应用层面问题。 第四步特殊和模糊情况：论文没有涉及智能体/工具使用或幻觉/可解释性/安全等特殊情况，它直接关注通过思维轨迹增强来提升LLM的通用推理能力。 最终决策：论文的核心贡献是提出了一种通用的训练方法(TPT)来增强LLM的推理能力，提高数据效率，这与研究目标完全一致。实验表明该方法能显著提升模型在推理任务上的表现，因此应该被保留。",
                    "summary2": "本文旨在提高大型语言模型(LLM)训练的数据效率。针对高质量训练数据有限且某些token难以直接学习的问题，我们提出了Thinking augmented Pre-training (TPT)，一种通过自动生成思维轨迹增强现有文本数据的通用方法。我们在多种训练配置上（包括数据受限和充足情况下的预训练以及中期训练）通过推理基准和语言理解任务验证了其有效性，实验表明TPT将LLM预训练的数据效率提高了3倍，显著提升了模型性能。",
                    "summary_translation": "本文介绍了一种简单且可扩展的方法，通过用思维轨迹(thinking trajectories)增强现有文本来提高大型语言模型(Large Language Model, LLM)训练的数据效率。大型语言模型预训练的计算量一直在以前所未有的速度增长，而高质量数据的可用性仍然有限。因此，最大化可用数据的效用构成了一个重大的研究挑战。一个主要障碍是，在固定模型容量下，某些高质量标记(tokens)难以学习，因为单个标记的基本原理可能异常复杂和深入。为解决这一问题，我们提出了思维增强预训练(Thinking augmented Pre-Training, TPT)，这是一种通过自动生成的思维轨迹增强文本的通用方法。这种增强有效增加了训练数据的体量，并通过逐步推理和分解使高质量标记更易学习。我们在多种训练配置中应用TPT，规模高达1000亿(tokens)标记，包括数据受限和数据充足情况下的预训练，以及从强大的开源检查点(checkpoints)进行的中期训练。实验结果表明，我们的方法显著提高了各种规模和系列的大型语言模型的性能。值得注意的是，TPT将大型语言模型预训练的数据效率提高了3倍。对于一个30亿参数(3B parameters)的模型，它在几个具有挑战性的推理基准(benchmarks)上将训练后性能提高了超过10%。"
                },
                {
                    "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
                    "arxiv_id": "2509.19736",
                    "authors": "Cheng Qian, Zuxin Liu, Akshara Prabhakar, Jielin Qiu, Zhiwei Liu, Haolin Chen, Shirley Kokane, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang",
                    "summary": "Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文提出了UserRL框架，通过强化学习训练交互式以用户为中心的智能体，核心是关于提升大语言模型的交互能力和多轮对话能力。从第一步核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式（UserRL框架），并使用强化学习（GRPO算法）来优化模型，这符合\"改进LLM基础能力和提出新训练范式\"的保留标准。 从第二步正面指标看，论文包含多个相关主题：1)核心概念方面，论文明确基于Qwen3等大语言模型；2)训练方法方面，论文使用了强化学习(RL)方法；3)新兴范式方面，论文关注的是基于LLM的智能体(llm-based agents)的训练。 从第三步排除标准看，论文没有涉及多模态与视觉内容，没有聚焦于特定应用领域（如医疗、化学等），也没有主要关注模型可靠性的应用层面问题（如水印、安全性），因此不符合任何排除标准。 从第四步特殊和模糊情况处理看，论文提出的是通用的智能体训练框架，旨在增强LLM的通用交互能力，而不是将智能体应用在特定领域，因此应该保留。 综合分析，论文的核心贡献是提供了一种新的强化学习框架来提升LLM的交互式推理和多轮对话能力，这属于大语言模型通用推理能力的重要组成部分，与研究目标高度一致。",
                    "summary2": "本文旨在解决如何训练能有效获取用户中心能力的智能体模型，同时考虑用户交互多样性和动态性的问题。针对多轮用户交互场景，我们提出了一种UserRL框架，结合标准化gym环境和模拟用户，并在Qwen3模型上通过不同奖励设计策略验证了其有效性。",
                    "summary_translation": "强化学习(Reinforcement Learning, RL)在训练智能体模型(agentic models)方面展现出潜力，这些模型能够超越静态基准(static benchmarks)，进行动态、多轮交互(dynamic, multi-turn interactions)。然而，这类智能体的最终价值在于其协助用户的能力，而在这种场景中，用户交互的多样性和动态性带来了挑战。\n\n在这项工作中，我们提出了UserRL，这是一个通过标准化gym环境(standardized gym environments)与模拟用户(simulated users)相结合，用于训练和评估以用户为中心能力(user-centric abilities)的统一框架。我们系统地改变轮级奖励分配(turn-level reward assignment)和轨迹级分数计算(trajectory-level score calculation)，以分析不同公式化方法如何影响GRPO算法(GRPO algorithm)下的学习效果。\n\n我们在Qwen3模型上的实验揭示了三个关键发现：(i) SFT冷启动(SFT cold start)对于解锁初始交互能力和实现持续的强化学习改进至关重要；(ii) 精心设计的轨迹评分(deliberate trajectory scoring)能够产生更高效、更有效的多轮交互；(iii) 尽管更强大的模拟用户(如GPT-4o)有助于训练，但开源模拟器(如Qwen3-32B)仍然是一种经济高效且可转移的选择。\n\n总的来说，这些结果强调，奖励塑形(reward shaping)和用户模拟选择的精心设计与模型规模同等重要，并将UserRL确立为开发稳健的以用户为中心的智能体模型的实用途径。所有代码和数据均公开，以供未来研究使用。"
                },
                {
                    "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
                    "arxiv_id": "2509.19517",
                    "authors": "Sai Teja Reddy Adapala",
                    "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，该论文的本质是研究大语言模型在认知负荷下的多跳推理能力限制。论文提出了计算认知负荷的形式理论，设计了新的评估方法(ICE基准测试)，旨在理解和提升LLMs在复杂推理任务中的表现。这明显属于改进LLM基础推理能力的研究，特别是关注多跳推理这一通用推理能力的核心方面，而非将LLM作为工具应用于特定领域。 其次，论文符合多个正面指标：它明确关注\"Large Language Models (LLMs)\"这一核心概念，并深入研究\"multi-hop reasoning\"(多跳推理)这一关键推理能力方向。虽然论文不涉及训练方法和新兴范式，但对推理能力的深入研究已足够表明其与研究目标的高度相关性。 第三，论文不符合任何排除标准：它不涉及多模态与视觉研究，不聚焦于任何特定应用领域(如医疗、化学等)，也不主要关注模型可靠性的应用层面问题(如水印、安全性等)。虽然论文结尾提到了\"安全性\"评估，但这是作为评估模型推理能力的一个方面，而非主要焦点。 综上所述，这篇论文的核心贡献是提出了新的理论框架和评估方法来理解和提升LLMs的通用推理能力，特别是在认知负荷条件下的多跳推理表现，完全符合研究目标。",
                    "summary2": "本文旨在研究大型语言模型在认知负荷下的多跳推理能力限制。针对信息丰富、任务切换的动态场景，我们提出了计算认知负荷理论，并设计了Interleaved Cognitive Evaluation (ICE)基准测试系统操纵上下文饱和和注意力残留因素。在五个LLMs上通过Exact-Match准确率验证发现：Gemini-2.0-Flash-001在控制条件下达85%准确率，但在额外信息增加时性能显著下降(β = -0.003, p < 0.001)，而较小模型如Llama-3-8B-Instruct在所有条件下均表现完全失效。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）的扩展已经揭示了其在静态基准测试上的表现与在动态、信息丰富环境中的脆弱性之间存在的关键差距。尽管模型在孤立任务上表现出色，但控制其在认知负荷下推理的计算限制仍然知之甚少。在这项工作中，我们提出了计算认知负荷（computational cognitive load）的正式理论，假设外部的、与任务无关的信息（Context Saturation，上下文饱和）和任务切换的干扰（Attentional Residue，注意残留）是导致性能下降的关键机制。我们设计了交错认知评估（Interleaved Cognitive Evaluation, ICE），这是一个去混淆的基准测试，用于在具有挑战性的多跳推理（multi-hop reasoning）任务上系统地操纵这些负荷因素。一项全面研究（200个问题中每个项目重复10次）揭示了五个经过指令调优（instruction-tuned）的模型之间存在显著的性能差异。较小的开源架构（Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2）表现出基准脆弱性，在这个高内在负荷（high-intrinsic-load）任务中，包括清洁对照组在内的所有条件下准确率均达到0%（SEM = 0.0）。相比之下，Gemini-2.0-Flash-001表现出部分韧性，在对照组条件下达到85%的准确率，在上下文饱和条件下出现统计学显著的下降（$\\beta = -0.003$每增加1%负荷，$p < 0.001$）。这些发现提供了初步证据，表明认知负荷是推理失败的关键因素，支持了在不确定性下幻觉即猜测（hallucination-as-guessing）的理论。我们得出结论，以ICE基准测试为例的动态、认知感知的压力测试对于评估先进AI系统的真正韧性和安全性至关重要。"
                },
                {
                    "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution",
                    "arxiv_id": "2509.19349",
                    "authors": "Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin",
                    "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large language models (LLMs) to advance scientific discovery with state-of-the-art performance and unprecedented efficiency. Recent advances in scaling inference time compute of LLMs have enabled significant progress in generalized scientific discovery. These approaches rely on evolutionary agentic harnesses that leverage LLMs as mutation operators to generate candidate solutions. However, current code evolution methods suffer from critical limitations: they are sample inefficient, requiring thousands of samples to identify effective solutions, and remain closed-source, hindering broad adoption and extension. ShinkaEvolve addresses these limitations, introducing three key innovations: a parent sampling technique balancing exploration and exploitation, code novelty rejection-sampling for efficient search space exploration, and a bandit-based LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks, demonstrating consistent improvements in sample efficiency and solution quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution using only 150 samples, designs high-performing agentic harnesses for AIME mathematical reasoning tasks, identifies improvements to ALE-Bench competitive programming solutions, and discovers novel mixture-of-expert load balancing loss functions that illuminate the space of optimization strategies. Our results demonstrate that ShinkaEvolve achieves broad applicability with exceptional sample efficiency. By providing open-source accessibility and cost-efficiency, this work democratizes open-ended discovery across diverse computational problems.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础能力和提出新的训练范式。论文提出的ShinkaEvolve框架利用LLMs作为变异算子，通过演化的方式增强其问题解决和推理能力。论文的核心贡献是提高LLM在程序演化方面的样本效率和解决方案质量，而不是将LLM作为工具应用于特定领域，因此符合保留标准。 其次，论文包含多个正面指标：明确涉及大语言模型(LLMs)作为核心组件；关注推理能力，特别是在数学推理任务中的应用；采用演化(evolution)作为训练方法；并使用了基于LLM的智能体框架(evolutionary agentic harnesses)。 第三，论文不符合排除标准。虽然论文提到了一些应用案例(如圆包装、数学推理、竞争编程等)，但这些是作为评估框架性能的示例，而非论文的主要焦点。论文的核心是提出一种通用的程序演化框架，而非专注于多模态、特定应用领域或模型可靠性等排除领域。 最后，在特殊和模糊情况的处理上，论文提出的是一种通用的演化智能体框架，旨在增强LLM的通用问题解决能力，而不是针对特定领域的应用，因此应该保留。 综合来看，ShinkaEvolve论文致力于通过演化的方法增强LLM的通用推理和问题解决能力，与研究目标高度一致。",
                    "summary2": "",
                    "summary_translation": "我们介绍了ShinkaEvolve：一个新的开源框架，该框架利用大型语言模型（LLMs, Large Language Models）来推动科学发现，具有最先进的性能和前所未有的效率。最近在扩展大型语言模型推理时间计算方面的进展，为通用科学发现带来了显著进步。这些方法依赖于进化智能代理框架（evolutionary agentic harnesses），该框架利用大型语言模型作为变异算子（mutation operators）来生成候选解决方案。然而，当前的代码进化方法存在关键局限性：样本效率低下（sample inefficient），需要数千个样本才能识别有效解决方案，并且仍然是闭源的，阻碍了广泛采用和扩展。\n\nShinkaEvolve解决了这些局限性，引入了三项关键创新：一种平衡探索与利用的父代采样技术（parent sampling technique），用于高效搜索空间探索的代码新颖性拒绝采样（code novelty rejection-sampling），以及基于多臂老虎机（bandit-based）的大型语言模型集成选择策略（LLM ensemble selection strategy）。我们在多样化任务上评估了ShinkaEvolve，展示了在样本效率和解决方案质量上的一致性改进。\n\nShinkaEvolve仅使用150个样本就发现了一种新的最先进的圆形打包（circle packing）解决方案，为AIME数学推理任务设计了高性能的智能代理框架，识别出ALE-Bench竞赛编程解决方案的改进，并发现了新颖的专家混合负载平衡损失函数（mixture-of-expert load balancing loss functions），这些函数阐明了优化策略的空间。我们的结果表明，ShinkaEvolve实现了广泛的应用性和卓越的样本效率。通过提供开源的可访问性和成本效益，这项工作使多样化的计算问题中的开放式发现（open-ended discovery）变得民主化。"
                },
            ]
        },
    ],
};


        // 全局状态管理
        let starredPapers = new Set();
        let readPapers = new Set();
        let deletedPapers = new Set();
        let pendingDeletes = new Map();
        let showChineseSummary = true; // 默认显示中文摘要
        let showOnlyStarred = false; // 筛选状态：是否只显示收藏的论文

        // 从localStorage加载状态
        function loadState() {
            const starred = localStorage.getItem('starred_papers');
            const read = localStorage.getItem('read_papers');
            const deleted = localStorage.getItem('deleted_papers');
            const summaryLang = localStorage.getItem('summary_language');
            
            if (starred) starredPapers = new Set(JSON.parse(starred));
            if (read) readPapers = new Set(JSON.parse(read));
            if (deleted) deletedPapers = new Set(JSON.parse(deleted));
            if (summaryLang !== null) showChineseSummary = summaryLang === 'chinese';
        }

        // 保存状态到localStorage
        function saveState() {
            localStorage.setItem('starred_papers', JSON.stringify([...starredPapers]));
            localStorage.setItem('read_papers', JSON.stringify([...readPapers]));
            localStorage.setItem('deleted_papers', JSON.stringify([...deletedPapers]));
            localStorage.setItem('summary_language', showChineseSummary ? 'chinese' : 'english');
        }

        // 显示撤销删除的Toast
        function showUndoToast(message, seconds, onUndo, onExpire) {
            const toast = document.getElementById('undo-toast');
            const msgEl = document.getElementById('toast-message');
            const cdEl = document.getElementById('countdown');
            const undoBtn = document.getElementById('undo-btn');
            
            msgEl.textContent = message;
            let remaining = seconds;
            cdEl.textContent = `(${remaining}s)`;
            toast.classList.remove('hidden');

            let intervalId = setInterval(() => {
                remaining -= 1;
                cdEl.textContent = `(${remaining}s)`;
                if (remaining <= 0) {
                    clearInterval(intervalId);
                    toast.classList.add('hidden');
                    try { onExpire && onExpire(); } catch (e) {}
                }
            }, 1000);

            let expireTimer = setTimeout(() => {
                clearInterval(intervalId);
                toast.classList.add('hidden');
                try { onExpire && onExpire(); } catch (e) {}
            }, seconds * 1000);

            const cleanup = () => {
                clearInterval(intervalId);
                clearTimeout(expireTimer);
                toast.classList.add('hidden');
            };

            const onUndoClick = () => {
                cleanup();
                try { onUndo && onUndo(); } catch (e) {}
            };
            
            undoBtn.removeEventListener('click', onUndoClick);
            undoBtn.addEventListener('click', onUndoClick);
        }

        // 显示简单的提示信息
        function showSimpleToast(message) {
            // 创建一个简单的toast元素
            const toast = document.createElement('div');
            toast.className = 'fixed top-4 right-4 bg-green-500 text-white px-4 py-2 rounded-lg shadow-lg z-50 transition-all duration-300';
            toast.textContent = message;
            
            document.body.appendChild(toast);
            
            // 3秒后自动消失
            setTimeout(() => {
                toast.style.opacity = '0';
                toast.style.transform = 'translateY(-10px)';
                setTimeout(() => {
                    document.body.removeChild(toast);
                }, 300);
            }, 3000);
        }

        // 通过按钮删除论文（避免JavaScript字符串转义问题）
        function deletePaperByButton(button) {
            const arxivId = button.getAttribute('data-arxiv-id');
            const title = button.getAttribute('data-title');
            deletePaper(arxivId, title);
        }

        // 删除论文
        function deletePaper(arxivId, title) {
            const paperEl = document.querySelector(`[data-arxiv-id="${arxivId}"]`);
            if (!paperEl) return;
            
            // 添加删除动画效果
            paperEl.style.transition = 'all 0.3s ease-out';
            paperEl.style.transform = 'scale(0.95)';
            paperEl.style.opacity = '0.5';
            
            setTimeout(() => {
                // 立即删除并保存状态
                deletedPapers.add(arxivId);
                saveState();
                
                // 移除DOM元素
                paperEl.remove();
                updateStats();
                
                // 显示简单的删除提示
                showSimpleToast(`已删除: ${title}`);
            }, 300);
        }

        // 切换星标状态
        function toggleStar(arxivId) {
            if (starredPapers.has(arxivId)) {
                starredPapers.delete(arxivId);
            } else {
                starredPapers.add(arxivId);
            }
            saveState();
            
            // 如果当前是只看收藏模式，需要重新渲染
            if (showOnlyStarred) {
                renderPapers();
            } else {
                // 否则只更新星标按钮状态
                const starBtn = document.querySelector(`[data-arxiv-id="${arxivId}"] .star-button`);
                if (starBtn) {
                    if (starredPapers.has(arxivId)) {
                        starBtn.classList.add('starred');
                    } else {
                        starBtn.classList.remove('starred');
                    }
                }
            }
        }

        // 切换已读状态
        function toggleRead(arxivId) {
            const checkbox = document.querySelector(`[data-arxiv-id="${arxivId}"] input[type="checkbox"]`);
            if (!checkbox) return;
            
            if (checkbox.checked) {
                readPapers.add(arxivId);
            } else {
                readPapers.delete(arxivId);
            }
            saveState();
        }

        // 切换摘要语言
        function toggleSummaryLanguage() {
            showChineseSummary = !showChineseSummary;
            const toggleBtn = document.getElementById('summary-toggle');
            toggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            // 更新所有摘要显示
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            saveState();
        }

        // 更新统计信息
        function updateStats() {
            const visiblePapers = document.querySelectorAll('.paper-item:not(.hidden-paper)').length;
            document.getElementById('total-papers').textContent = visiblePapers;
        }

        // 创建论文HTML
        function createPaperHTML(paper, date) {
            const isStarred = starredPapers.has(paper.arxiv_id);
            const isRead = readPapers.has(paper.arxiv_id);
            const isDeleted = deletedPapers.has(paper.arxiv_id);
            
            // 如果论文已被删除，直接返回空字符串，不渲染
            if (isDeleted) {
                return '';
            }
            
            // 如果启用了只看收藏筛选，且论文未被收藏，则不渲染
            if (showOnlyStarred && !isStarred) {
                return '';
            }
            
            return `
                <div class="paper-item bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-6" data-arxiv-id="${paper.arxiv_id}">
                    <!-- 论文标题和操作按钮 -->
                    <div class="flex items-start justify-between mb-4">
                        <div class="flex items-start space-x-3 flex-1">
                            <!-- 星标按钮 -->
                            <button class="star-button ${isStarred ? 'starred' : ''} mt-1 flex-shrink-0" onclick="toggleStar('${paper.arxiv_id}')" title="点击收藏">
                                <svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
                                    <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z" />
                                </svg>
                            </button>
                            <!-- 论文标题 -->
                            <h3 class="text-lg font-semibold text-black dark:text-white leading-tight">${paper.title}</h3>
                        </div>
                        <!-- 删除按钮 -->
                        <button class="delete-button text-slate-400 hover:text-red-500 ml-4 flex-shrink-0" onclick="deletePaperByButton(this)" data-arxiv-id="${paper.arxiv_id}" data-title="${paper.title.replace(/"/g, '&quot;')}" title="删除">
                            <svg class="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
                            </svg>
                        </button>
                    </div>

                    <!-- 论文元信息 -->
                    <div class="space-y-2 mb-4">
                        <div class="flex flex-wrap items-center gap-4 text-sm text-slate-600 dark:text-slate-400">
                            <span><strong>ArXiv ID:</strong> ${paper.arxiv_id}</span>
                            <span class="inline-flex items-center px-2 py-1 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900/20 dark:text-blue-300">
                                ${paper.category}
                            </span>
                            <span>${date}</span>
                        </div>
                        <div class="text-sm text-black dark:text-white">
                            <strong>作者:</strong> ${paper.authors}
                        </div>
                    </div>

                    <!-- 已读复选框 -->
                    <div class="mb-4">
                        <label class="inline-flex items-center">
                            <input type="checkbox" ${isRead ? 'checked' : ''} onchange="toggleRead('${paper.arxiv_id}')" class="rounded border-gray-300 text-blue-600 shadow-sm focus:border-blue-300 focus:ring focus:ring-blue-200 focus:ring-opacity-50">
                            <span class="ml-2 text-sm text-slate-600 dark:text-slate-400">已阅读</span>
                        </label>
                    </div>

                    ${paper.filter_reason ? `
                    <!-- 筛选原因 -->
                    <div class="bg-blue-50/70 dark:bg-blue-950/20 border-l-3 border-blue-300 p-4 mb-4 rounded-r-lg">
                        <div class="text-sm text-black dark:text-white leading-relaxed">
                            <strong class="font-medium text-blue-600 dark:text-blue-400">筛选原因:</strong> ${paper.filter_reason}
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary2 ? `
                    <!-- AI总结 -->
                    <div class="bg-yellow-50/70 dark:bg-yellow-950/20 border-l-3 border-yellow-300 p-4 mb-4 rounded-r-lg">
                        <div class="text-sm text-black dark:text-white leading-relaxed">
                            <strong class="font-medium text-yellow-600 dark:text-yellow-400">AI总结:</strong> ${paper.summary2}
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary || paper.summary_translation ? `
                    <!-- 原始摘要 -->
                    <div class="summary-section bg-green-50/70 dark:bg-green-950/20 border-l-3 border-green-300 p-4 mb-4 rounded-r-lg">
                        ${paper.summary_translation ? `
                        <div class="chinese-summary text-sm text-black dark:text-white leading-relaxed" style="display: block;">
                            <strong class="font-medium text-green-600 dark:text-green-400">原始摘要:</strong> ${paper.summary_translation}
                        </div>
                        ` : ''}
                        ${paper.summary ? `
                        <div class="english-summary text-sm text-black dark:text-white leading-relaxed" style="display: none;">
                            <strong class="font-medium text-green-600 dark:text-green-400">Original Abstract:</strong> ${paper.summary}
                        </div>
                        ` : ''}
                    </div>
                    ` : ''}

                    <!-- 论文链接 -->
                    <div class="flex flex-wrap gap-2">
                        <a href="https://arxiv.org/abs/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-3 py-2 text-sm font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors">
                            📄 arXiv 原文
                        </a>
                        <a href="https://arxiv.org/pdf/${paper.arxiv_id}.pdf" target="_blank" 
                           class="inline-flex items-center px-3 py-2 text-sm font-medium text-white bg-green-600 hover:bg-green-700 rounded-md transition-colors">
                            📋 PDF 下载
                        </a>
                        <a href="https://papers.cool/arxiv/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-3 py-2 text-sm font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-md transition-colors">
                            🔥 Cool Paper
                        </a>
                    </div>
                </div>
            `;
        }

        // 创建分类HTML
        function createCategoryHTML(category, date) {
            const categoryId = `category-${date}-${category.name.replace(/\s+/g, '-')}`;
            let papersHTML = '';
            let visiblePaperCount = 0;
            
            if (category.papers && category.papers.length > 0) {
                category.papers.forEach(paper => {
                    const paperHTML = createPaperHTML(paper, date);
                    if (paperHTML) { // 只添加非空的论文HTML
                        papersHTML += `
                            <li>
                                ${paperHTML}
                            </li>
                        `;
                        visiblePaperCount++;
                    }
                });
            }
            
            // 如果没有可见的论文，显示提示信息
            if (visiblePaperCount === 0) {
                papersHTML = '<li class="pl-7 text-sm text-slate-500 dark:text-slate-400">此分类下暂无论文。</li>';
            }
            
            return `
                <li class="mb-4">
                    <div class="category-toggle flex items-center justify-between cursor-pointer p-3 rounded-md hover:bg-slate-100 dark:hover:bg-slate-700 transition-colors" data-target="${categoryId}">
                        <div class="flex items-center space-x-3">
                            <svg class="h-4 w-4 text-slate-500 rotate-90-transition transform transition-transform" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                            </svg>
                            <span class="font-medium text-sky-700 dark:text-sky-400">${category.name}</span>
                        </div>
                        <span class="text-xs font-mono bg-slate-200 dark:bg-slate-700 text-slate-600 dark:text-slate-300 rounded-full px-2 py-0.5">${visiblePaperCount}</span>
                    </div>
                    <div id="${categoryId}" class="category-content hidden pl-1 pt-2 border-l border-slate-200 dark:border-slate-700 ml-4">
                        <ul class="space-y-4">
                            ${papersHTML}
                        </ul>
                    </div>
                </li>
            `;
        }

        // 渲染论文列表
        function renderPapers() {
            const mainContent = document.getElementById('main-content');
            const loading = document.getElementById('loading');
            
            if (loading) {
                loading.classList.add('hidden');
            }
            
            let html = '';
            let totalPapers = 0;
            
            for (const date in allPapers) {
                const categories = allPapers[date];
                if (categories.length === 0) continue;
                
                // 计算实际可见的论文数量
                let dateVisibleTotal = 0;
                const categoryHTMLs = [];
                
                categories.forEach(category => {
                    const categoryHTML = createCategoryHTML(category, date);
                    categoryHTMLs.push(categoryHTML);
                    // 计算该分类下可见的论文数
                    if (category.papers) {
                        category.papers.forEach(paper => {
                            if (!deletedPapers.has(paper.arxiv_id) && 
                                (!showOnlyStarred || starredPapers.has(paper.arxiv_id))) {
                                dateVisibleTotal++;
                            }
                        });
                    }
                });
                
                totalPapers += dateVisibleTotal;
                
                // 如果该日期下没有可见论文，跳过
                if (dateVisibleTotal === 0) continue;
                
                html += `
                    <section class="mb-8">
                        <h2 class="text-lg font-medium text-slate-500 dark:text-slate-400 mb-4">${date} (${dateVisibleTotal} 篇论文)</h2>
                        <div class="bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-4 sm:p-6">
                            <ul class="space-y-2">
                `;
                
                categoryHTMLs.forEach(categoryHTML => {
                    html += categoryHTML;
                });
                
                html += `
                            </ul>
                        </div>
                    </section>
                `;
            }
            
            mainContent.innerHTML = html;
            updateStats();
            
            // 应用当前摘要语言设置
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            // 添加分类展开/折叠功能
            document.querySelectorAll('.category-toggle').forEach(button => {
                button.addEventListener('click', () => {
                    const targetId = button.getAttribute('data-target');
                    const content = document.getElementById(targetId);
                    const icon = button.querySelector('svg');
                    
                    content.classList.toggle('hidden');
                    icon.classList.toggle('rotate-90');
                });
            });
        }

        // 主题切换功能
        function setupThemeToggle() {
            const themeToggleBtn = document.getElementById('theme-toggle');
            const lightIcon = document.getElementById('theme-icon-light');
            const darkIcon = document.getElementById('theme-icon-dark');

            function updateThemeIcon() {
                if (document.documentElement.classList.contains('dark')) {
                    lightIcon.classList.add('hidden');
                    darkIcon.classList.remove('hidden');
                } else {
                    lightIcon.classList.remove('hidden');
                    darkIcon.classList.add('hidden');
                }
            }

            updateThemeIcon();

            themeToggleBtn.addEventListener('click', () => {
                document.documentElement.classList.toggle('dark');
                localStorage.theme = document.documentElement.classList.contains('dark') ? 'dark' : 'light';
                updateThemeIcon();
            });
        }

        // 设置摘要语言切换功能
        function setupSummaryToggle() {
            const summaryToggleBtn = document.getElementById('summary-toggle');
            
            // 初始化按钮文本
            summaryToggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            summaryToggleBtn.addEventListener('click', toggleSummaryLanguage);
        }

        // 设置筛选功能
        function setupFilter() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            filterStarredBtn.addEventListener('click', () => {
                showOnlyStarred = true;
                updateFilterButtons();
                renderPapers();
            });
            
            filterAllBtn.addEventListener('click', () => {
                showOnlyStarred = false;
                updateFilterButtons();
                renderPapers();
            });
            
            updateFilterButtons();
        }

        // 更新筛选按钮状态
        function updateFilterButtons() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            if (showOnlyStarred) {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
            } else {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
            }
        }

        // 初始化应用
        document.addEventListener('DOMContentLoaded', function() {
            loadState();
            setupThemeToggle();
            setupSummaryToggle();
            setupFilter();
            renderPapers();
        });
    </script>
</body>
</html>